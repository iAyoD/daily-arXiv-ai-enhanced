{"id": "2510.20884", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20884", "abs": "https://arxiv.org/abs/2510.20884", "authors": ["Pranamya Kulkarni", "Puranjay Datta", "Burak Var\u0131c\u0131", "Emre Acart\u00fcrk", "Karthikeyan Shanmugam", "Ali Tajer"], "title": "ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning", "comment": "A preliminary version of this paper appeared at NeurIPS 2025 Workshop\n  on Embodied World Models for Decision Making", "summary": "Causal representation learning (CRL) has emerged as a powerful unsupervised\nframework that (i) disentangles the latent generative factors underlying\nhigh-dimensional data, and (ii) learns the cause-and-effect interactions among\nthe disentangled variables. Despite extensive recent advances in\nidentifiability and some practical progress, a substantial gap remains between\ntheory and real-world practice. This paper takes a step toward closing that gap\nby bringing CRL to robotics, a domain that has motivated CRL. Specifically,\nthis paper addresses the well-defined robot pose estimation -- the recovery of\nposition and orientation from raw images -- by introducing Robotic Pose\nEstimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPES\nembodies the essence of interventional CRL by identifying those generative\nfactors that are actuated: images are generated by intrinsic and extrinsic\nlatent factors (e.g., joint angles, arm/limb geometry, lighting, background,\nand camera configuration) and the objective is to disentangle and recover the\ncontrollable latent variables, i.e., those that can be directly manipulated\n(intervened upon) through actuation. Interventional CRL theory shows that\nvariables that undergo variations via interventions can be identified. In\nrobotics, such interventions arise naturally by commanding actuators of various\njoints and recording images under varied controls. Empirical evaluations in\nsemi-synthetic manipulator experiments demonstrate that ROPES successfully\ndisentangles latent generative factors with high fidelity with respect to the\nground truth. Crucially, this is achieved by leveraging only distributional\nchanges, without using any labeled data. The paper also includes a comparison\nwith a baseline based on a recently proposed semi-supervised framework. This\npaper concludes by positioning robot pose estimation as a near-practical\ntestbed for CRL.", "AI": {"tldr": "ROPES\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u6570\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u673a\u5668\u4eba\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u673a\u5668\u4eba\u81ea\u7136\u5e72\u9884\uff08\u5173\u8282\u63a7\u5236\uff09\u6765\u8bc6\u522b\u53ef\u63a7\u7684\u6f5c\u5728\u53d8\u91cf\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u9ad8\u4fdd\u771f\u5730\u89e3\u8026\u751f\u6210\u56e0\u7d20\u3002", "motivation": "\u56e0\u679c\u8868\u793a\u5b66\u4e60\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5c06CRL\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u59ff\u6001\u4f30\u8ba1\u8fd9\u4e00\u5177\u4f53\u9886\u57df\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5229\u7528\u673a\u5668\u4eba\u81ea\u7136\u63d0\u4f9b\u7684\u5e72\u9884\u673a\u5236\u3002", "method": "\u63d0\u51faROPES\u6846\u67b6\uff0c\u5229\u7528\u673a\u5668\u4eba\u6267\u884c\u5668\u547d\u4ee4\u4f5c\u4e3a\u81ea\u7136\u5e72\u9884\uff0c\u901a\u8fc7\u5206\u6570\u56e0\u679c\u8868\u793a\u5b66\u4e60\u8bc6\u522b\u53ef\u63a7\u6f5c\u5728\u53d8\u91cf\uff08\u5982\u5173\u8282\u89d2\u5ea6\uff09\uff0c\u4ec5\u4f7f\u7528\u5206\u5e03\u53d8\u5316\u800c\u65e0\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u534a\u5408\u6210\u673a\u68b0\u81c2\u5b9e\u9a8c\u4e2d\uff0cROPES\u6210\u529f\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u89e3\u8026\u6f5c\u5728\u751f\u6210\u56e0\u7d20\uff0c\u4e0e\u771f\u5b9e\u503c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u4f18\u4e8e\u6700\u8fd1\u63d0\u51fa\u7684\u534a\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u673a\u5668\u4eba\u59ff\u6001\u4f30\u8ba1\u53ef\u4f5c\u4e3a\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u8fd1\u5b9e\u7528\u6d4b\u8bd5\u5e73\u53f0\uff0cROPES\u5c55\u793a\u4e86\u65e0\u76d1\u7763CRL\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.20916", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20916", "abs": "https://arxiv.org/abs/2510.20916", "authors": ["Sydney M. Katz", "Robert J. Moss", "Dylan M. Asmar", "Wesley A. Olson", "James K. Kuchar", "Mykel J. Kochenderfer"], "title": "Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance", "comment": "32 pages, 9 figures", "summary": "Aircraft collision avoidance systems is critical to modern aviation. These\nsystems are designed to predict potential collisions between aircraft and\nrecommend appropriate avoidance actions. Creating effective collision avoidance\nsystems requires solutions to a variety of technical challenges related to\nsurveillance, decision making, and validation. These challenges have sparked\nsignificant research and development efforts over the past several decades that\nhave resulted in a variety of proposed solutions. This article provides an\noverview of these challenges and solutions with an emphasis on those that have\nbeen put through a rigorous validation process and accepted by regulatory\nbodies. The challenges posed by the collision avoidance problem are often\npresent in other domains, and aircraft collision avoidance systems can serve as\ncase studies that provide valuable insights for a wide range of safety-critical\nsystems.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u98de\u673a\u9632\u649e\u7cfb\u7edf\u7684\u6280\u672f\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u91cd\u70b9\u5173\u6ce8\u7ecf\u8fc7\u4e25\u683c\u9a8c\u8bc1\u5e76\u88ab\u76d1\u7ba1\u673a\u6784\u63a5\u53d7\u7684\u7cfb\u7edf\u3002", "motivation": "\u98de\u673a\u9632\u649e\u7cfb\u7edf\u5bf9\u73b0\u4ee3\u822a\u7a7a\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u89e3\u51b3\u76d1\u89c6\u3001\u51b3\u7b56\u548c\u9a8c\u8bc1\u7b49\u6280\u672f\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u5728\u5176\u4ed6\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u4e5f\u666e\u904d\u5b58\u5728\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u8fc7\u53bb\u51e0\u5341\u5e74\u7684\u7814\u7a76\u548c\u5f00\u53d1\u6210\u679c\uff0c\u5206\u6790\u5404\u79cd\u9632\u649e\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u5173\u6ce8\u7ecf\u8fc7\u4e25\u683c\u9a8c\u8bc1\u7684\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u6709\u6548\u7684\u9632\u649e\u7cfb\u7edf\u6280\u672f\u65b9\u6848\uff0c\u8fd9\u4e9b\u65b9\u6848\u5df2\u901a\u8fc7\u76d1\u7ba1\u673a\u6784\u8ba4\u8bc1\uff0c\u53ef\u4f5c\u4e3a\u5176\u4ed6\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u6848\u4f8b\u7814\u7a76\u3002", "conclusion": "\u98de\u673a\u9632\u649e\u7cfb\u7edf\u7684\u53d1\u5c55\u4e3a\u89e3\u51b3\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u6280\u672f\u6311\u6218\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7ecf\u9a8c\uff0c\u5bf9\u5176\u4ed6\u9886\u57df\u5177\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2510.20965", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20965", "abs": "https://arxiv.org/abs/2510.20965", "authors": ["Jesse Haworth", "Juo-Tung Chen", "Nigel Nelson", "Ji Woong Kim", "Masoud Moghani", "Chelsea Finn", "Axel Krieger"], "title": "SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing", "comment": "10 pages, 5 figures, 4 tables, NeurIPS 2025", "summary": "Robotic suturing is a prototypical long-horizon dexterous manipulation task,\nrequiring coordinated needle grasping, precise tissue penetration, and secure\nknot tying. Despite numerous efforts toward end-to-end autonomy, a fully\nautonomous suturing pipeline has yet to be demonstrated on physical hardware.\nWe introduce SutureBot: an autonomous suturing benchmark on the da Vinci\nResearch Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying.\nTo ensure repeatability, we release a high-fidelity dataset comprising 1,890\nsuturing demonstrations. Furthermore, we propose a goal-conditioned framework\nthat explicitly optimizes insertion-point precision, improving targeting\naccuracy by 59\\%-74\\% over a task-only baseline. To establish this task as a\nbenchmark for dexterous imitation learning, we evaluate state-of-the-art\nvision-language-action (VLA) models, including $\\pi_0$, GR00T N1, OpenVLA-OFT,\nand multitask ACT, each augmented with a high-level task-prediction policy.\nAutonomous suturing is a key milestone toward achieving robotic autonomy in\nsurgery. These contributions support reproducible evaluation and development of\nprecision-focused, long-horizon dexterous manipulation policies necessary for\nend-to-end suturing. Dataset is available at:\nhttps://huggingface.co/datasets/jchen396/suturebot", "AI": {"tldr": "SutureBot\u662f\u4e00\u4e2a\u5728da Vinci\u7814\u7a76\u5957\u4ef6\u4e0a\u7684\u81ea\u4e3b\u7f1d\u5408\u57fa\u51c6\uff0c\u5305\u62ec\u9488\u5934\u62fe\u53d6\u3001\u7ec4\u7ec7\u63d2\u5165\u548c\u6253\u7ed3\u3002\u4f5c\u8005\u53d1\u5e03\u4e86\u5305\u542b1890\u4e2a\u7f1d\u5408\u6f14\u793a\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u63d2\u5165\u70b9\u7cbe\u5ea6\u7684\u76ee\u6807\u6761\u4ef6\u6846\u67b6\u3002", "motivation": "\u7f1d\u5408\u662f\u5178\u578b\u7684\u957f\u65f6\u7a0b\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\uff0c\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u7aef\u5230\u7aef\u81ea\u4e3b\u5316\u7684\u52aa\u529b\uff0c\u4f46\u5c1a\u672a\u5728\u7269\u7406\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u7f1d\u5408\u6d41\u7a0b\u3002\u81ea\u4e3b\u7f1d\u5408\u662f\u5b9e\u73b0\u624b\u672f\u673a\u5668\u4eba\u81ea\u4e3b\u5316\u7684\u5173\u952e\u91cc\u7a0b\u7891\u3002", "method": "\u63d0\u51fa\u4e86\u76ee\u6807\u6761\u4ef6\u6846\u67b6\uff0c\u663e\u5f0f\u4f18\u5316\u63d2\u5165\u70b9\u7cbe\u5ea6\u3002\u8bc4\u4f30\u4e86\u5305\u62ec\u03c0\u2080\u3001GR00T N1\u3001OpenVLA-OFT\u548c\u591a\u4efb\u52a1ACT\u5728\u5185\u7684\u6700\u5148\u8fdb\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u6bcf\u4e2a\u90fd\u914d\u5907\u4e86\u9ad8\u7ea7\u4efb\u52a1\u9884\u6d4b\u7b56\u7565\u3002", "result": "\u76ee\u6807\u6761\u4ef6\u6846\u67b6\u6bd4\u4ec5\u4efb\u52a1\u57fa\u7ebf\u63d0\u9ad8\u4e8659%-74%\u7684\u76ee\u6807\u7cbe\u5ea6\u3002\u5efa\u7acb\u4e86\u53ef\u91cd\u590d\u7684\u7f1d\u5408\u57fa\u51c6\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b1890\u4e2a\u6f14\u793a\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u652f\u6301\u53ef\u91cd\u590d\u8bc4\u4f30\u548c\u5f00\u53d1\u4e13\u6ce8\u4e8e\u7cbe\u5ea6\u7684\u957f\u65f6\u7a0b\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u5bf9\u4e8e\u7aef\u5230\u7aef\u7f1d\u5408\u662f\u5fc5\u8981\u7684\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2510.20974", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20974", "abs": "https://arxiv.org/abs/2510.20974", "authors": ["Michael Bezick", "Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization", "comment": null, "summary": "Reinforcement Learning (RL) from raw visual input has achieved impressive\nsuccesses in recent years, yet it remains fragile to out-of-distribution\nvariations such as changes in lighting, color, and viewpoint. Point Cloud\nReinforcement Learning (PC-RL) offers a promising alternative by mitigating\nappearance-based brittleness, but its sensitivity to camera pose mismatches\ncontinues to undermine reliability in realistic settings. To address this\nchallenge, we propose PCA Point Cloud (PPC), a canonicalization framework\nspecifically tailored for downstream robotic control. PPC maps point clouds\nunder arbitrary rigid-body transformations to a unique canonical pose, aligning\nobservations to a consistent frame, thereby substantially decreasing\nviewpoint-induced inconsistencies. In our experiments, we show that PPC\nimproves robustness to unseen camera poses across challenging robotic tasks,\nproviding a principled alternative to domain randomization.", "AI": {"tldr": "\u63d0\u51faPPC\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4efb\u610f\u521a\u6027\u53d8\u6362\u7684\u70b9\u4e91\u6620\u5c04\u5230\u89c4\u8303\u59ff\u6001\uff0c\u63d0\u5347\u70b9\u4e91\u5f3a\u5316\u5b66\u4e60\u5bf9\u76f8\u673a\u4f4d\u59ff\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u70b9\u4e91\u5f3a\u5316\u5b66\u4e60\u5bf9\u76f8\u673a\u4f4d\u59ff\u4e0d\u5339\u914d\u7684\u654f\u611f\u6027\uff0c\u63d0\u9ad8\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790(PCA)\u5c06\u4efb\u610f\u521a\u6027\u53d8\u6362\u7684\u70b9\u4e91\u6620\u5c04\u5230\u552f\u4e00\u7684\u89c4\u8303\u59ff\u6001\uff0c\u4f7f\u89c2\u6d4b\u5bf9\u9f50\u5230\u4e00\u81f4\u5750\u6807\u7cfb\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPPC\u5728\u6311\u6218\u6027\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u5bf9\u672a\u89c1\u76f8\u673a\u4f4d\u59ff\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "PPC\u4e3a\u70b9\u4e91\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5bf9\u76f8\u673a\u4f4d\u59ff\u53d8\u5316\u7684\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\uff0c\u662f\u9886\u57df\u968f\u673a\u5316\u7684\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6cd5\u3002"}}
{"id": "2510.21026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21026", "abs": "https://arxiv.org/abs/2510.21026", "authors": ["Sai Haneesh Allu", "Jishnu Jaykumar P", "Ninad Khargonkar", "Tyler Summers", "Jian Yao", "Yu Xiang"], "title": "HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation", "comment": "14 pages, 11 figures and 3 tables. Project page is available at\n  \\url{https://irvlutd.github.io/HRT1/}", "summary": "We introduce a novel system for human-to-robot trajectory transfer that\nenables robots to manipulate objects by learning from human demonstration\nvideos. The system consists of four modules. The first module is a data\ncollection module that is designed to collect human demonstration videos from\nthe point of view of a robot using an AR headset. The second module is a video\nunderstanding module that detects objects and extracts 3D human-hand\ntrajectories from demonstration videos. The third module transfers a human-hand\ntrajectory into a reference trajectory of a robot end-effector in 3D space. The\nlast module utilizes a trajectory optimization algorithm to solve a trajectory\nin the robot configuration space that can follow the end-effector trajectory\ntransferred from the human demonstration. Consequently, these modules enable a\nrobot to watch a human demonstration video once and then repeat the same mobile\nmanipulation task in different environments, even when objects are placed\ndifferently from the demonstrations. Experiments of different manipulation\ntasks are conducted on a mobile manipulator to verify the effectiveness of our\nsystem", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u8f68\u8ff9\u7684\u7cfb\u7edf\uff0c\u5305\u542b\u6570\u636e\u6536\u96c6\u3001\u89c6\u9891\u7406\u89e3\u3001\u8f68\u8ff9\u8f6c\u6362\u548c\u8f68\u8ff9\u4f18\u5316\u56db\u4e2a\u6a21\u5757\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u89c2\u770b\u4e00\u6b21\u6f14\u793a\u89c6\u9891\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u91cd\u590d\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u89c2\u770b\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u5b66\u4e60\u64cd\u4f5c\u6280\u80fd\uff0c\u5e76\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u91cd\u590d\u6267\u884c\u76f8\u540c\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u5373\u4f7f\u7269\u4f53\u6446\u653e\u4f4d\u7f6e\u4e0e\u6f14\u793a\u65f6\u4e0d\u540c", "method": "\u7cfb\u7edf\u5305\u542b\u56db\u4e2a\u6a21\u5757\uff1a1) \u4f7f\u7528AR\u5934\u663e\u4ece\u673a\u5668\u4eba\u89c6\u89d2\u6536\u96c6\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\uff1b2) \u89c6\u9891\u7406\u89e3\u6a21\u5757\u68c0\u6d4b\u7269\u4f53\u5e76\u63d0\u53d63D\u4eba\u624b\u8f68\u8ff9\uff1b3) \u5c06\u4eba\u624b\u8f68\u8ff9\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u53c2\u8003\u8f68\u8ff9\uff1b4) \u4f7f\u7528\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u5728\u673a\u5668\u4eba\u914d\u7f6e\u7a7a\u95f4\u4e2d\u6c42\u89e3\u80fd\u591f\u8ddf\u968f\u8f6c\u6362\u540e\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u7684\u8def\u5f84", "result": "\u5728\u4e0d\u540c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5bf9\u79fb\u52a8\u673a\u68b0\u81c2\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u7cfb\u7edf\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u4e00\u6b21\u89c2\u770b\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\uff0c\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u91cd\u590d\u6267\u884c\u76f8\u540c\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1"}}
{"id": "2510.21046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21046", "abs": "https://arxiv.org/abs/2510.21046", "authors": ["Zlatan Ajanovi\u0107", "Ravi Prakash", "Leandro de Souza Rosa", "Jens Kober"], "title": "Sequentially Teaching Sequential Tasks $(ST)^2$: Teaching Robots Long-horizon Manipulation Skills", "comment": null, "summary": "Learning from demonstration is effective for teaching robots complex skills\nwith high sample efficiency. However, teaching long-horizon tasks with multiple\nskills is difficult, as deviations accumulate, distributional shift increases,\nand human teachers become fatigued, raising the chance of failure. In this\nwork, we study user responses to two teaching frameworks: (i) a traditional\nmonolithic approach, where users demonstrate the entire trajectory of a\nlong-horizon task; and (ii) a sequential approach, where the task is segmented\nby the user and demonstrations are provided step by step. To support this\nstudy, we introduce $(ST)^2$, a sequential method for learning long-horizon\nmanipulation tasks that allows users to control the teaching flow by defining\nkey points, enabling incremental and structured demonstrations. We conducted a\nuser study on a restocking task with 16 participants in a realistic retail\nenvironment to evaluate both user preference and method effectiveness. Our\nobjective and subjective results show that both methods achieve similar\ntrajectory quality and success rates. Some participants preferred the\nsequential approach for its iterative control, while others favored the\nmonolithic approach for its simplicity.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u673a\u5668\u4eba\u793a\u6559\u6846\u67b6\uff1a\u4f20\u7edf\u6574\u4f53\u65b9\u6cd5\u548c\u987a\u5e8f\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86(ST)^2\u987a\u5e8f\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u53d1\u73b0\u5728\u8f68\u8ff9\u8d28\u91cf\u548c\u6210\u529f\u7387\u4e0a\u4e24\u79cd\u65b9\u6cd5\u76f8\u4f3c\uff0c\u4f46\u7528\u6237\u504f\u597d\u4e0d\u540c\u3002", "motivation": "\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u80fd\u9ad8\u6548\u6559\u6388\u673a\u5668\u4eba\u590d\u6742\u6280\u80fd\uff0c\u4f46\u6559\u6388\u5305\u542b\u591a\u4e2a\u6280\u80fd\u7684\u957f\u671f\u4efb\u52a1\u5f88\u56f0\u96be\uff0c\u56e0\u4e3a\u504f\u5dee\u4f1a\u7d2f\u79ef\u3001\u5206\u5e03\u504f\u79fb\u589e\u52a0\uff0c\u4eba\u7c7b\u6559\u5e08\u4f1a\u75b2\u52b3\uff0c\u589e\u52a0\u4e86\u5931\u8d25\u51e0\u7387\u3002", "method": "\u63d0\u51fa\u4e86(ST)^2\u987a\u5e8f\u5b66\u4e60\u65b9\u6cd5\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u5b9a\u4e49\u5173\u952e\u70b9\u6765\u63a7\u5236\u6559\u5b66\u6d41\u7a0b\uff0c\u652f\u6301\u589e\u91cf\u5f0f\u548c\u7ed3\u6784\u5316\u6f14\u793a\u3002\u901a\u8fc716\u540d\u53c2\u4e0e\u8005\u5728\u96f6\u552e\u73af\u5883\u4e2d\u7684\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u4f20\u7edf\u6574\u4f53\u65b9\u6cd5\u548c\u987a\u5e8f\u65b9\u6cd5\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728\u8f68\u8ff9\u8d28\u91cf\u548c\u6210\u529f\u7387\u4e0a\u8868\u73b0\u76f8\u4f3c\u3002\u90e8\u5206\u53c2\u4e0e\u8005\u504f\u597d\u987a\u5e8f\u65b9\u6cd5\u7684\u8fed\u4ee3\u63a7\u5236\uff0c\u800c\u5176\u4ed6\u53c2\u4e0e\u8005\u5219\u559c\u6b22\u6574\u4f53\u65b9\u6cd5\u7684\u7b80\u5355\u6027\u3002", "conclusion": "\u4e24\u79cd\u793a\u6559\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u76f8\u5f53\uff0c\u4f46\u7528\u6237\u504f\u597d\u5b58\u5728\u5dee\u5f02\uff0c\u8868\u660e\u5e94\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u548c\u7528\u6237\u504f\u597d\u9009\u62e9\u5408\u9002\u7684\u6559\u5b66\u6846\u67b6\u3002"}}
{"id": "2510.21074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21074", "abs": "https://arxiv.org/abs/2510.21074", "authors": ["Mitchell E. C. Sabbadini", "Andrew H. Liu", "Joseph Ruan", "Tyler S. Wilson", "Zachary Kingston", "Jonathan D. Gammell"], "title": "Revisiting Replanning from Scratch: Real-Time Incremental Planning with Fast Almost-Surely Asymptotically Optimal Planners", "comment": "Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2026, 8 pages, 5 figures, 1 table. A video of this work can be found\n  at https://www.youtube.com/watch?v=XaZrFy8wGZs", "summary": "Robots operating in changing environments either predict obstacle changes\nand/or plan quickly enough to react to them. Predictive approaches require a\nstrong prior about the position and motion of obstacles. Reactive approaches\nrequire no assumptions about their environment but must replan quickly and find\nhigh-quality paths to navigate effectively.\n  Reactive approaches often reuse information between queries to reduce\nplanning cost. These techniques are conceptually sound but updating dense\nplanning graphs when information changes can be computationally prohibitive. It\ncan also require significant effort to detect the changes in some applications.\n  This paper revisits the long-held assumption that reactive replanning\nrequires updating existing plans. It shows that the incremental planning\nproblem can alternatively be solved more efficiently as a series of independent\nproblems using fast almost-surely asymptotically optimal (ASAO) planning\nalgorithms. These ASAO algorithms quickly find an initial solution and converge\ntowards an optimal solution which allows them to find consistent global plans\nin the presence of changing obstacles without requiring explicit plan reuse.\nThis is demonstrated with simulated experiments where Effort Informed Trees\n(EIT*) finds shorter median solution paths than the tested reactive planning\nalgorithms and is further validated using Asymptotically Optimal RRT-Connect\n(AORRTC) on a real-world planning problem on a robot arm.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u53cd\u5e94\u5f0f\u91cd\u89c4\u5212\u5fc5\u987b\u66f4\u65b0\u73b0\u6709\u8ba1\u5212\u7684\u4f20\u7edf\u5047\u8bbe\uff0c\u63d0\u51fa\u4f7f\u7528\u5feb\u901f\u6e10\u8fd1\u6700\u4f18\u89c4\u5212\u7b97\u6cd5\u5c06\u589e\u91cf\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u4e00\u7cfb\u5217\u72ec\u7acb\u95ee\u9898\u6c42\u89e3\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u65e0\u9700\u663e\u5f0f\u8ba1\u5212\u91cd\u7528\u5373\u53ef\u83b7\u5f97\u9ad8\u8d28\u91cf\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u53cd\u5e94\u5f0f\u89c4\u5212\u65b9\u6cd5\u9700\u8981\u91cd\u7528\u4fe1\u606f\u5e76\u66f4\u65b0\u5bc6\u96c6\u89c4\u5212\u56fe\uff0c\u8fd9\u5728\u4fe1\u606f\u53d8\u5316\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5728\u67d0\u4e9b\u5e94\u7528\u4e2d\u68c0\u6d4b\u53d8\u5316\u9700\u8981\u5927\u91cf\u52aa\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5feb\u901f\u51e0\u4e4e\u5fc5\u7136\u6e10\u8fd1\u6700\u4f18(ASAO)\u89c4\u5212\u7b97\u6cd5\uff0c\u5c06\u589e\u91cf\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u4e00\u7cfb\u5217\u72ec\u7acb\u95ee\u9898\u6c42\u89e3\u3002\u8fd9\u4e9b\u7b97\u6cd5\u80fd\u5feb\u901f\u627e\u5230\u521d\u59cb\u89e3\u5e76\u6536\u655b\u5230\u6700\u4f18\u89e3\uff0c\u65e0\u9700\u663e\u5f0f\u8ba1\u5212\u91cd\u7528\u3002\u901a\u8fc7Effort Informed Trees (EIT*)\u548cAsymptotically Optimal RRT-Connect (AORRTC)\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793aEIT*\u627e\u5230\u7684\u4e2d\u4f4d\u89e3\u8def\u5f84\u6bd4\u6d4b\u8bd5\u7684\u53cd\u5e94\u5f0f\u89c4\u5212\u7b97\u6cd5\u66f4\u77ed\u3002\u5728\u771f\u5b9e\u673a\u5668\u4eba\u81c2\u89c4\u5212\u95ee\u9898\u4e0a\uff0cAORRTC\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53cd\u5e94\u5f0f\u91cd\u89c4\u5212\u4e0d\u4e00\u5b9a\u9700\u8981\u66f4\u65b0\u73b0\u6709\u8ba1\u5212\uff0c\u901a\u8fc7ASAO\u7b97\u6cd5\u5c06\u589e\u91cf\u89c4\u5212\u4f5c\u4e3a\u72ec\u7acb\u95ee\u9898\u5e8f\u5217\u6c42\u89e3\uff0c\u53ef\u4ee5\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e2d\u627e\u5230\u4e00\u81f4\u7684\u5168\u5c40\u8ba1\u5212\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2510.21121", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21121", "abs": "https://arxiv.org/abs/2510.21121", "authors": ["Haibo Zhao", "Yu Qi", "Boce Hu", "Yizhe Zhu", "Ziyan Chen", "Heng Tian", "Xupeng Zhu", "Owen Howell", "Haojie Huang", "Robin Walters", "Dian Wang", "Robert Platt"], "title": "Generalizable Hierarchical Skill Learning via Object-Centric Representation", "comment": null, "summary": "We present Generalizable Hierarchical Skill Learning (GSL), a novel framework\nfor hierarchical policy learning that significantly improves policy\ngeneralization and sample efficiency in robot manipulation. One core idea of\nGSL is to use object-centric skills as an interface that bridges the high-level\nvision-language model and the low-level visual-motor policy. Specifically, GSL\ndecomposes demonstrations into transferable and object-canonicalized skill\nprimitives using foundation models, ensuring efficient low-level skill learning\nin the object frame. At test time, the skill-object pairs predicted by the\nhigh-level agent are fed to the low-level module, where the inferred canonical\nactions are mapped back to the world frame for execution. This structured yet\nflexible design leads to substantial improvements in sample efficiency and\ngeneralization of our method across unseen spatial arrangements, object\nappearances, and task compositions. In simulation, GSL trained with only 3\ndemonstrations per task outperforms baselines trained with 30 times more data\nby 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses\nthe baseline trained with 10 times more data.", "AI": {"tldr": "GSL\u662f\u4e00\u4e2a\u5206\u5c42\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u4f53\u4e2d\u5fc3\u6280\u80fd\u4f5c\u4e3a\u9ad8\u5c42\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4f4e\u5c42\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u63a5\u53e3\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u7a7a\u95f4\u5e03\u5c40\u3001\u7269\u4f53\u5916\u89c2\u548c\u4efb\u52a1\u7ec4\u5408\u65f6\u3002", "method": "\u5c06\u6f14\u793a\u5206\u89e3\u4e3a\u53ef\u8fc1\u79fb\u7684\u7269\u4f53\u89c4\u8303\u5316\u6280\u80fd\u539f\u8bed\uff0c\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u786e\u4fdd\u5728\u7269\u4f53\u5750\u6807\u7cfb\u4e2d\u7684\u9ad8\u6548\u4f4e\u5c42\u6280\u80fd\u5b66\u4e60\u3002\u6d4b\u8bd5\u65f6\uff0c\u9ad8\u5c42\u4ee3\u7406\u9884\u6d4b\u7684\u6280\u80fd-\u7269\u4f53\u5bf9\u88ab\u9001\u5165\u4f4e\u5c42\u6a21\u5757\uff0c\u63a8\u65ad\u51fa\u7684\u89c4\u8303\u52a8\u4f5c\u88ab\u6620\u5c04\u56de\u4e16\u754c\u5750\u6807\u7cfb\u6267\u884c\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cGSL\u4ec5\u7528\u6bcf\u4e2a\u4efb\u52a13\u4e2a\u6f14\u793a\u5c31\u6bd4\u4f7f\u752830\u500d\u6570\u636e\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u534715.5%\u3002\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cGSL\u4e5f\u4f18\u4e8e\u4f7f\u752810\u500d\u6570\u636e\u7684\u57fa\u7ebf\u3002", "conclusion": "GSL\u7684\u7ed3\u6784\u5316\u4f46\u7075\u6d3b\u8bbe\u8ba1\u5728\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5404\u79cd\u672a\u89c1\u573a\u666f\u3002"}}
{"id": "2510.21164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21164", "abs": "https://arxiv.org/abs/2510.21164", "authors": ["Shamistan Karimov", "Elian Neppel", "Shreya Santra", "Kentaro Uno", "Kazuya Yoshida"], "title": "An Agnostic End-Effector Alignment Controller for Robust Assembly of Modular Space Robots", "comment": "6 pages, 12 figures. Accepted at iSparo 2025 | Video:\n  https://youtu.be/BW0YgSrvuDo", "summary": "Modular robots offer reconfigurability and fault tolerance essential for\nlunar missions, but require controllers that adapt safely to real-world\ndisturbances. We build on our previous hardware-agnostic actuator\nsynchronization in Motion Stack to develop a new controller enforcing adaptive\nvelocity bounds via a dynamic hypersphere clamp. Using only real-time\nend-effector and target pose measurements, the controller adjusts its\ntranslational and rotational speed limits to ensure smooth, stable alignment\nwithout abrupt motions. We implemented two variants, a discrete, step-based\nversion and a continuous, velocity-based version, and tested them on two\nMoonBot limbs in JAXA's lunar environment simulator. Field trials demonstrate\nthat the step-based variant produces highly predictable, low-wobble motions,\nwhile the continuous variant converges more quickly and maintains\nmillimeter-level positional accuracy, and both remain robust across limbs with\ndiffering mechanical imperfections and sensing noise (e.g., backlash and flex).\nThese results highlight the flexibility and robustness of our robot-agnostic\nframework for autonomous self-assembly and reconfiguration under harsh\nconditions.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7684\u81ea\u9002\u5e94\u901f\u5ea6\u8fb9\u754c\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8d85\u7403\u9762\u94b3\u5236\u786e\u4fdd\u5728\u6708\u7403\u73af\u5883\u4e2d\u7684\u5e73\u7a33\u7a33\u5b9a\u8fd0\u52a8\uff0c\u6d4b\u8bd5\u663e\u793a\u6b65\u8fdb\u7248\u672c\u8fd0\u52a8\u66f4\u53ef\u9884\u6d4b\uff0c\u8fde\u7eed\u7248\u672c\u6536\u655b\u66f4\u5feb\u4e14\u7cbe\u5ea6\u9ad8\u3002", "motivation": "\u6a21\u5757\u5316\u673a\u5668\u4eba\u5728\u6708\u7403\u4efb\u52a1\u4e2d\u9700\u8981\u53ef\u91cd\u6784\u6027\u548c\u5bb9\u9519\u6027\uff0c\u4f46\u9700\u8981\u80fd\u591f\u5b89\u5168\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u5e72\u6270\u7684\u63a7\u5236\u5668\u3002", "method": "\u5728Motion Stack\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u901a\u8fc7\u52a8\u6001\u8d85\u7403\u9762\u94b3\u5236\u5b9e\u65bd\u81ea\u9002\u5e94\u901f\u5ea6\u8fb9\u754c\u7684\u63a7\u5236\u5668\uff0c\u4ec5\u4f7f\u7528\u5b9e\u65f6\u672b\u7aef\u6267\u884c\u5668\u548c\u76ee\u6807\u4f4d\u59ff\u6d4b\u91cf\uff0c\u5b9e\u73b0\u5e73\u79fb\u548c\u65cb\u8f6c\u901f\u5ea6\u9650\u5236\u7684\u8c03\u6574\u3002\u5b9e\u73b0\u4e86\u79bb\u6563\u6b65\u8fdb\u548c\u8fde\u7eed\u901f\u5ea6\u4e24\u79cd\u53d8\u4f53\u3002", "result": "\u5728JAXA\u6708\u7403\u73af\u5883\u6a21\u62df\u5668\u4e2d\u6d4b\u8bd5\u4e24\u4e2aMoonBot\u80a2\u4f53\uff0c\u6b65\u8fdb\u53d8\u4f53\u4ea7\u751f\u9ad8\u5ea6\u53ef\u9884\u6d4b\u3001\u4f4e\u6446\u52a8\u7684\u8fd0\u52a8\uff0c\u8fde\u7eed\u53d8\u4f53\u6536\u655b\u66f4\u5feb\u4e14\u4fdd\u6301\u6beb\u7c73\u7ea7\u4f4d\u7f6e\u7cbe\u5ea6\uff0c\u4e24\u8005\u90fd\u5bf9\u673a\u68b0\u7f3a\u9677\u548c\u4f20\u611f\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u679c\u7a81\u51fa\u4e86\u673a\u5668\u4eba\u65e0\u5173\u6846\u67b6\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u81ea\u4e3b\u81ea\u7ec4\u88c5\u548c\u91cd\u6784\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.21215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21215", "abs": "https://arxiv.org/abs/2510.21215", "authors": ["Shuoshuo Ding", "Tiedong Zhang", "Dapeng Jiang", "Ming Lei"], "title": "Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments", "comment": "10 pages, 10 figures", "summary": "Visual degradation caused by limited visibility, insufficient lighting, and\nfeature scarcity in underwater environments presents significant challenges to\nvisual-inertial simultaneous localization and mapping (SLAM) systems. To\naddress these challenges, this paper proposes a graph-based\nvisual-inertial-acoustic-depth SLAM system that integrates a stereo camera, an\ninertial measurement unit (IMU), the Doppler velocity log (DVL), and a pressure\nsensor. The key innovation lies in the tight integration of four distinct\nsensor modalities to ensure reliable operation, even under degraded visual\nconditions. To mitigate DVL drift and improve measurement efficiency, we\npropose a novel velocity-bias-based DVL preintegration strategy. At the\nfrontend, hybrid tracking strategies and acoustic-inertial-depth joint\noptimization enhance system stability. Additionally, multi-source hybrid\nresiduals are incorporated into a graph optimization framework. Extensive\nquantitative and qualitative analyses of the proposed system are conducted in\nboth simulated and real-world underwater scenarios. The results demonstrate\nthat our approach outperforms current state-of-the-art stereo visual-inertial\nSLAM systems in both stability and localization accuracy, exhibiting\nexceptional robustness, particularly in visually challenging environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u4f18\u5316\u7684\u89c6\u89c9-\u60ef\u6027-\u58f0\u5b66-\u6df1\u5ea6SLAM\u7cfb\u7edf\uff0c\u96c6\u6210\u56db\u79cd\u4f20\u611f\u5668\u6a21\u6001\uff0c\u5728\u6c34\u4e0b\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u53ef\u9760\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u5b58\u5728\u80fd\u89c1\u5ea6\u53d7\u9650\u3001\u5149\u7167\u4e0d\u8db3\u548c\u7279\u5f81\u7a00\u7f3a\u7b49\u89c6\u89c9\u9000\u5316\u95ee\u9898\uff0c\u7ed9\u89c6\u89c9-\u60ef\u6027SLAM\u7cfb\u7edf\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u63d0\u51fa\u901f\u5ea6\u504f\u5deeDVL\u9884\u79ef\u5206\u7b56\u7565\uff0c\u524d\u7aef\u91c7\u7528\u6df7\u5408\u8ddf\u8e2a\u7b56\u7565\u548c\u58f0\u5b66-\u60ef\u6027-\u6df1\u5ea6\u8054\u5408\u4f18\u5316\uff0c\u5e76\u878d\u5165\u591a\u6e90\u6df7\u5408\u6b8b\u5dee\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6c34\u4e0b\u573a\u666f\u4e2d\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u7a33\u5b9a\u6027\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7acb\u4f53\u89c6\u89c9-\u60ef\u6027SLAM\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u89c6\u89c9\u6311\u6218\u6027\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u7d27\u5bc6\u96c6\u6210\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0bSLAM\u7684\u89c6\u89c9\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2510.21357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21357", "abs": "https://arxiv.org/abs/2510.21357", "authors": ["Daniel Schleich", "Jan Quenzel", "Sven Behnke"], "title": "Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations", "comment": "Accepted final version. IEEE International Symposium on Safety,\n  Security, and Rescue Robotics (SSRR), Galway, Ireland, 2025", "summary": "In recent years, consumer-grade UAVs have been widely adopted by first\nresponders. In general, they are operated manually, which requires trained\npilots, especially in unknown GNSS-denied environments and in the vicinity of\nstructures. Autonomous flight can facilitate the application of UAVs and reduce\noperator strain. However, autonomous systems usually require special\nprogramming interfaces, custom sensor setups, and strong onboard computers,\nwhich limits a broader deployment.\n  We present a system for autonomous flight using lightweight consumer-grade\nDJI drones. They are controlled by an Android app for state estimation and\nobstacle avoidance directly running on the UAV's remote control. Our ground\ncontrol station enables a single operator to configure and supervise multiple\nheterogeneous UAVs at once. Furthermore, it combines the observations of all\nUAVs into a joint 3D environment model for improved situational awareness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4f7f\u7528\u6d88\u8d39\u7ea7DJI\u65e0\u4eba\u673a\u5b9e\u73b0\u81ea\u4e3b\u98de\u884c\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7Android\u5e94\u7528\u5728\u65e0\u4eba\u673a\u9065\u63a7\u5668\u4e0a\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u548c\u907f\u969c\uff0c\u652f\u6301\u5355\u4e2a\u64cd\u4f5c\u5458\u540c\u65f6\u76d1\u63a7\u591a\u67b6\u5f02\u6784\u65e0\u4eba\u673a\uff0c\u5e76\u5c06\u6240\u6709\u89c2\u6d4b\u6570\u636e\u6574\u5408\u4e3a\u8054\u54083D\u73af\u5883\u6a21\u578b\u3002", "motivation": "\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\u5df2\u88ab\u6025\u6551\u4eba\u5458\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u901a\u5e38\u9700\u8981\u624b\u52a8\u64cd\u4f5c\uff0c\u7279\u522b\u662f\u5728\u672a\u77e5GNSS\u53d7\u9650\u73af\u5883\u548c\u7ed3\u6784\u9644\u8fd1\u9700\u8981\u8bad\u7ec3\u6709\u7d20\u7684\u98de\u884c\u5458\u3002\u81ea\u4e3b\u98de\u884c\u53ef\u4ee5\u7b80\u5316\u65e0\u4eba\u673a\u5e94\u7528\u5e76\u51cf\u8f7b\u64cd\u4f5c\u5458\u8d1f\u62c5\uff0c\u4f46\u73b0\u6709\u81ea\u4e3b\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u7279\u6b8a\u7f16\u7a0b\u63a5\u53e3\u3001\u5b9a\u5236\u4f20\u611f\u5668\u8bbe\u7f6e\u548c\u5f3a\u5927\u7684\u673a\u8f7d\u8ba1\u7b97\u673a\uff0c\u9650\u5236\u4e86\u66f4\u5e7f\u6cdb\u90e8\u7f72\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7cfb\u7edf\uff0c\u4f7f\u7528\u6d88\u8d39\u7ea7DJI\u65e0\u4eba\u673a\uff0c\u901a\u8fc7Android\u5e94\u7528\u5728\u65e0\u4eba\u673a\u9065\u63a7\u5668\u4e0a\u76f4\u63a5\u8fd0\u884c\u72b6\u6001\u4f30\u8ba1\u548c\u907f\u969c\u7b97\u6cd5\u3002\u5730\u9762\u63a7\u5236\u7ad9\u5141\u8bb8\u5355\u4e2a\u64cd\u4f5c\u5458\u914d\u7f6e\u548c\u76d1\u63a7\u591a\u67b6\u5f02\u6784\u65e0\u4eba\u673a\uff0c\u5e76\u5c06\u6240\u6709\u65e0\u4eba\u673a\u7684\u89c2\u6d4b\u6570\u636e\u6574\u5408\u4e3a\u8054\u54083D\u73af\u5883\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u4f7f\u7528\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\u8fdb\u884c\u81ea\u4e3b\u98de\u884c\uff0c\u65e0\u9700\u7279\u6b8a\u786c\u4ef6\u6216\u590d\u6742\u8bbe\u7f6e\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u591a\u65e0\u4eba\u673a\u534f\u540c\u64cd\u4f5c\u548c\u589e\u5f3a\u7684\u60c5\u5883\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u8bc1\u660e\u4e86\u4f7f\u7528\u73b0\u6210\u7684\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\u5b9e\u73b0\u81ea\u4e3b\u98de\u884c\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\u514b\u670d\u4e86\u4f20\u7edf\u81ea\u4e3b\u7cfb\u7edf\u7684\u786c\u4ef6\u9650\u5236\uff0c\u4e3a\u6025\u6551\u7b49\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u6613\u90e8\u7f72\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.21369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21369", "abs": "https://arxiv.org/abs/2510.21369", "authors": ["Vivian S. Medeiros", "Giovanni B. Dessy", "Thiago Boaventura", "Marcelo Becker", "Claudio Semini", "Victor Barasuol"], "title": "Load-bearing Assessment for Safe Locomotion of Quadruped Robots on Collapsing Terrain", "comment": null, "summary": "Collapsing terrains, often present in search and rescue missions or planetary\nexploration, pose significant challenges for quadruped robots. This paper\nintroduces a robust locomotion framework for safe navigation over unstable\nsurfaces by integrating terrain probing, load-bearing analysis, motion\nplanning, and control strategies. Unlike traditional methods that rely on\nspecialized sensors or external terrain mapping alone, our approach leverages\njoint measurements to assess terrain stability without hardware modifications.\nA Model Predictive Control (MPC) system optimizes robot motion, balancing\nstability and probing constraints, while a state machine coordinates terrain\nprobing actions, enabling the robot to detect collapsible regions and\ndynamically adjust its footholds. Experimental results on custom-made\ncollapsing platforms and rocky terrains demonstrate the framework's ability to\ntraverse collapsing terrain while maintaining stability and prioritizing\nsafety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u7a33\u5b9a\u5730\u5f62\u4e0a\u5b89\u5168\u5bfc\u822a\u7684\u9c81\u68d2\u8fd0\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5730\u5f62\u63a2\u6d4b\u3001\u627f\u91cd\u5206\u6790\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u7b56\u7565\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u68c0\u6d4b\u53ef\u574d\u584c\u533a\u57df\u5e76\u52a8\u6001\u8c03\u6574\u843d\u811a\u70b9\u3002", "motivation": "\u574d\u584c\u5730\u5f62\u5728\u641c\u6551\u4efb\u52a1\u6216\u884c\u661f\u63a2\u7d22\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u7ed9\u56db\u8db3\u673a\u5668\u4eba\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u7528\u4f20\u611f\u5668\u6216\u5916\u90e8\u5730\u5f62\u6d4b\u7ed8\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u4e0d\u7a33\u5b9a\u8868\u9762\u3002", "method": "\u5229\u7528\u5173\u8282\u6d4b\u91cf\u8bc4\u4f30\u5730\u5f62\u7a33\u5b9a\u6027\u800c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\uff0c\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u4f18\u5316\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u901a\u8fc7\u72b6\u6001\u673a\u534f\u8c03\u5730\u5f62\u63a2\u6d4b\u52a8\u4f5c\u3002", "result": "\u5728\u5b9a\u5236\u574d\u584c\u5e73\u53f0\u548c\u5ca9\u77f3\u5730\u5f62\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u4f18\u5148\u8003\u8651\u5b89\u5168\u6027\u7684\u540c\u65f6\u7a7f\u8d8a\u574d\u584c\u5730\u5f62\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u7a33\u5b9a\u5730\u5f62\u4e0a\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u96c6\u6210\u63a2\u6d4b\u3001\u5206\u6790\u548c\u63a7\u5236\u7b56\u7565\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u8fd0\u52a8\u6027\u80fd\u3002"}}
{"id": "2510.21438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21438", "abs": "https://arxiv.org/abs/2510.21438", "authors": ["Satheeshkumar Veeramani", "Zhengxue Zhou", "Francisco Munguia-Galeano", "Hatem Fakhruldeen", "Thomas Roddelkopf", "Mohammed Faeik Ruzaij Al-Okby", "Kerstin Thurow", "Andrew Ian Cooper"], "title": "PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees", "comment": "25 pages, 8 figures, paper submitted to Robotics and Autonomous\n  Systems Journal", "summary": "Mobile robotic chemists are a fast growing trend in the field of chemistry\nand materials research. However, so far these mobile robots lack workflow\nawareness skills. This poses the risk that even a small anomaly, such as an\nimproperly capped sample vial could disrupt the entire workflow. This wastes\ntime, and resources, and could pose risks to human researchers, such as\nexposure to toxic materials. Existing perception mechanisms can be used to\npredict anomalies but they often generate excessive false positives. This may\nhalt workflow execution unnecessarily, requiring researchers to intervene and\nto resume the workflow when no problem actually exists, negating the benefits\nof autonomous operation. To address this problem, we propose PREVENT a system\ncomprising navigation and manipulation skills based on a multimodal Behavior\nTree (BT) approach that can be integrated into existing software architectures\nwith minimal modifications. Our approach involves a hierarchical perception\nmechanism that exploits AI techniques and sensory feedback through Dexterous\nVision and Navigational Vision cameras and an IoT gas sensor module for\nexecution-related decision-making. Experimental evaluations show that the\nproposed approach is comparatively efficient and completely avoids both false\nnegatives and false positives when tested in simulated risk scenarios within\nour robotic chemistry workflow. The results also show that the proposed\nmulti-modal perception skills achieved deployment accuracies that were higher\nthan the average of the corresponding uni-modal skills, both for navigation and\nfor manipulation.", "AI": {"tldr": "\u63d0\u51faPREVENT\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u884c\u4e3a\u6811\u65b9\u6cd5\u89e3\u51b3\u79fb\u52a8\u5316\u5b66\u673a\u5668\u4eba\u7f3a\u4e4f\u5de5\u4f5c\u6d41\u610f\u8bc6\u7684\u95ee\u9898\uff0c\u907f\u514d\u8bef\u62a5\u548c\u6f0f\u62a5", "motivation": "\u73b0\u6709\u79fb\u52a8\u5316\u5b66\u673a\u5668\u4eba\u7f3a\u4e4f\u5de5\u4f5c\u6d41\u610f\u8bc6\uff0c\u5c0f\u5f02\u5e38\u5982\u672a\u76d6\u7d27\u6837\u54c1\u74f6\u53ef\u80fd\u4e2d\u65ad\u6574\u4e2a\u5de5\u4f5c\u6d41\uff0c\u6d6a\u8d39\u65f6\u95f4\u548c\u8d44\u6e90\uff0c\u4e14\u73b0\u6709\u611f\u77e5\u673a\u5236\u8bef\u62a5\u8fc7\u591a", "method": "\u57fa\u4e8e\u591a\u6a21\u6001\u884c\u4e3a\u6811\u7684\u5bfc\u822a\u548c\u64cd\u4f5c\u6280\u80fd\uff0c\u91c7\u7528\u5206\u5c42\u611f\u77e5\u673a\u5236\uff0c\u7ed3\u5408AI\u6280\u672f\u548c\u4f20\u611f\u5668\u53cd\u9988\uff08\u7075\u5de7\u89c6\u89c9\u3001\u5bfc\u822a\u89c6\u89c9\u6444\u50cf\u5934\u3001\u7269\u8054\u7f51\u6c14\u4f53\u4f20\u611f\u5668\uff09", "result": "\u5728\u6a21\u62df\u98ce\u9669\u573a\u666f\u4e2d\u5b8c\u5168\u907f\u514d\u8bef\u62a5\u548c\u6f0f\u62a5\uff0c\u591a\u6a21\u6001\u611f\u77e5\u6280\u80fd\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u4e2d\u7684\u90e8\u7f72\u51c6\u786e\u7387\u9ad8\u4e8e\u76f8\u5e94\u5355\u6a21\u6001\u6280\u80fd\u7684\u5e73\u5747\u6c34\u5e73", "conclusion": "PREVENT\u7cfb\u7edf\u80fd\u6709\u6548\u96c6\u6210\u5230\u73b0\u6709\u8f6f\u4ef6\u67b6\u6784\u4e2d\uff0c\u63d0\u9ad8\u79fb\u52a8\u5316\u5b66\u673a\u5668\u4eba\u7684\u5de5\u4f5c\u6d41\u610f\u8bc6\u548c\u5b89\u5168\u6027"}}
{"id": "2510.21469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21469", "abs": "https://arxiv.org/abs/2510.21469", "authors": ["Domenico Palmisano", "Giuseppe Palestra", "Berardina Nadja De Carolis"], "title": "Enhancing Social Robots through Resilient AI", "comment": "8 pages, Workshop on Adaptive Social Interaction based on user's\n  Mental mOdels and behaVior in HRI, The 17th International Conference on\n  Social Robotics, 10-12 September 2025, Naples (IT)", "summary": "As artificial intelligence continues to advance and becomes more integrated\ninto sensitive areas like healthcare, education, and everyday life, it's\ncrucial for these systems to be both resilient and robust. This paper shows how\nresilience is a fundamental characteristic of social robots, which, through it,\nensure trust in the robot itself-an essential element especially when operating\nin contexts with elderly people, who often have low trust in these systems.\nResilience is therefore the ability to operate under adverse or stressful\nconditions, even when degraded or weakened, while maintaining essential\noperational capabilities.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u793e\u4f1a\u673a\u5668\u4eba\u4e2d\u97e7\u6027\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u4fdd\u5065\u7b49\u654f\u611f\u9886\u57df\uff0c\u5f3a\u8c03\u97e7\u6027\u662f\u786e\u4fdd\u7528\u6237\u4fe1\u4efb\u7684\u5173\u952e\u7279\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u3001\u6559\u80b2\u7b49\u654f\u611f\u9886\u57df\u7684\u6df1\u5165\u5e94\u7528\uff0c\u9700\u8981\u786e\u4fdd\u8fd9\u4e9b\u7cfb\u7edf\u5177\u6709\u97e7\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u4e0e\u8001\u5e74\u4eba\u4e92\u52a8\u65f6\u5efa\u7acb\u4fe1\u4efb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u793e\u4f1a\u673a\u5668\u4eba\u5728\u538b\u529b\u6761\u4ef6\u4e0b\u7684\u8fd0\u884c\u80fd\u529b\uff0c\u7814\u7a76\u97e7\u6027\u5982\u4f55\u5e2e\u52a9\u673a\u5668\u4eba\u5728\u6027\u80fd\u4e0b\u964d\u65f6\u4ecd\u4fdd\u6301\u57fa\u672c\u529f\u80fd\u3002", "result": "\u7814\u7a76\u8868\u660e\u97e7\u6027\u662f\u793e\u4f1a\u673a\u5668\u4eba\u7684\u57fa\u672c\u7279\u5f81\uff0c\u80fd\u591f\u786e\u4fdd\u5728\u4e0d\u5229\u6761\u4ef6\u4e0b\u7ef4\u6301\u8fd0\u884c\u80fd\u529b\uff0c\u4ece\u800c\u589e\u5f3a\u7528\u6237\u7279\u522b\u662f\u8001\u5e74\u4eba\u5bf9\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u3002", "conclusion": "\u97e7\u6027\u662f\u6784\u5efa\u53ef\u4fe1\u793e\u4f1a\u673a\u5668\u4eba\u7684\u5173\u952e\u8981\u7d20\uff0c\u7279\u522b\u662f\u5728\u654f\u611f\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u80fd\u591f\u786e\u4fdd\u7cfb\u7edf\u5728\u538b\u529b\u6761\u4ef6\u4e0b\u6301\u7eed\u53ef\u9760\u8fd0\u884c\u3002"}}
{"id": "2510.21536", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21536", "abs": "https://arxiv.org/abs/2510.21536", "authors": ["Narendhiran Vijayakumar", "Sridevi. M"], "title": "AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive Refinement for Drivable-Area Segmentation", "comment": "10 pages, 5 figures, 4 tables", "summary": "Free space ground segmentation is essential to navigate robots and autonomous\nvehicles, recognize drivable zones, and traverse efficiently. Fine-grained\nfeatures remain challenging for existing segmentation models, particularly for\nrobots in indoor and structured environments. These difficulties arise from\nineffective multi-scale processing, suboptimal boundary refinement, and limited\nfeature representation. In order to overcome these limitations, we propose\nAttention-Guided Upsampling with Residual Boundary-Assistive Refinement\n(AURASeg), a ground-plane semantic segmentation model that maintains high\nsegmentation accuracy while improving border precision. Our method uses\nCSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for\naccurate edge delineation and an Attention Progressive Upsampling Decoder\n(APUD) for strong feature integration. We also incorporate a lightweight Atrous\nSpatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context\nextraction without compromising real-time performance. The proposed model beats\nbenchmark segmentation architectures in mIoU and F1 metrics when tested on the\nGround Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor\ndataset. Our approach achieves an improvement in mean Intersection-over-Union\n(mIoU) of +1.26% and segmentation precision of +1.65% compared to\nstate-of-the-art models. These results show that our technique is feasible for\nautonomous perception in both indoor and outdoor environments, enabling precise\nborder refinement with minimal effect on inference speed.", "AI": {"tldr": "\u63d0\u51faAURASeg\u5730\u9762\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u4e0a\u91c7\u6837\u548c\u8fb9\u754c\u7ec6\u5316\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u9ad8\u5206\u5272\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u8fb9\u754c\u7cbe\u5ea6\uff0c\u5728\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5206\u5272\u6a21\u578b\u5728\u5ba4\u5185\u548c\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u96be\u4ee5\u5904\u7406\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u5b58\u5728\u591a\u5c3a\u5ea6\u5904\u7406\u6548\u7387\u4f4e\u3001\u8fb9\u754c\u7ec6\u5316\u4e0d\u8db3\u548c\u7279\u5f81\u8868\u793a\u6709\u9650\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528CSP-Darknet\u9aa8\u5e72\u7f51\u7edc\uff0c\u6dfb\u52a0\u6b8b\u5dee\u8fb9\u754c\u7ec6\u5316\u6a21\u5757(RBRM)\u8fdb\u884c\u7cbe\u786e\u8fb9\u7f18\u5212\u5206\uff0c\u6ce8\u610f\u529b\u6e10\u8fdb\u4e0a\u91c7\u6837\u89e3\u7801\u5668(APUD)\u8fdb\u884c\u7279\u5f81\u6574\u5408\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7ASPP-Lite\u6a21\u5757\u8fdb\u884c\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u63d0\u53d6\u3002", "result": "\u5728GMRP\u6570\u636e\u96c6\u548c\u81ea\u5b9a\u4e49Gazebo\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u6a21\u578b\uff0cmIoU\u63d0\u53471.26%\uff0c\u5206\u5272\u7cbe\u5ea6\u63d0\u53471.65%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5ba4\u5185\u5916\u81ea\u4e3b\u611f\u77e5\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u8fb9\u754c\u7ec6\u5316\u4e14\u5bf9\u63a8\u7406\u901f\u5ea6\u5f71\u54cd\u6700\u5c0f\u3002"}}
{"id": "2510.21571", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21571", "abs": "https://arxiv.org/abs/2510.21571", "authors": ["Qixiu Li", "Yu Deng", "Yaobo Liang", "Lin Luo", "Lei Zhou", "Chengtang Yao", "Lingqi Zeng", "Zhiyuan Feng", "Huizhi Liang", "Sicheng Xu", "Yizhong Zhang", "Xi Chen", "Hao Chen", "Lily Sun", "Dong Chen", "Jiaolong Yang", "Baining Guo"], "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos", "comment": "Project page: https://microsoft.github.io/VITRA/", "summary": "This paper presents a novel approach for pretraining robotic manipulation\nVision-Language-Action (VLA) models using a large corpus of unscripted\nreal-life video recordings of human hand activities. Treating human hand as\ndexterous robot end-effector, we show that \"in-the-wild\" egocentric human\nvideos without any annotations can be transformed into data formats fully\naligned with existing robotic V-L-A training data in terms of task granularity\nand labels. This is achieved by the development of a fully-automated holistic\nhuman activity analysis approach for arbitrary human hand videos. This approach\ncan generate atomic-level hand activity segments and their language\ndescriptions, each accompanied with framewise 3D hand motion and camera motion.\nWe process a large volume of egocentric videos and create a hand-VLA training\ndataset containing 1M episodes and 26M frames. This training data covers a wide\nrange of objects and concepts, dexterous manipulation tasks, and environment\nvariations in real life, vastly exceeding the coverage of existing robot data.\nWe design a dexterous hand VLA model architecture and pretrain the model on\nthis dataset. The model exhibits strong zero-shot capabilities on completely\nunseen real-world observations. Additionally, fine-tuning it on a small amount\nof real robot action data significantly improves task success rates and\ngeneralization to novel objects in real robotic experiments. We also\ndemonstrate the appealing scaling behavior of the model's task performance with\nrespect to pretraining data scale. We believe this work lays a solid foundation\nfor scalable VLA pretraining, advancing robots toward truly generalizable\nembodied intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u672a\u6807\u6ce8\u7684\u771f\u5b9e\u4eba\u7c7b\u624b\u90e8\u6d3b\u52a8\u89c6\u9891\u9884\u8bad\u7ec3\u673a\u5668\u4eba\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u624b\u90e8\u89c6\u4e3a\u7075\u5de7\u7684\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21VLA\u6a21\u578b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u8986\u76d6\u8303\u56f4\u6709\u9650\uff0c\u65e0\u6cd5\u652f\u6301\u901a\u7528\u673a\u5668\u4eba\u667a\u80fd\u7684\u53d1\u5c55\u3002\u5229\u7528\u5927\u91cf\u65e0\u6807\u6ce8\u7684\u4eba\u7c7b\u624b\u90e8\u6d3b\u52a8\u89c6\u9891\u53ef\u4ee5\u521b\u5efa\u8986\u76d6\u66f4\u5e7f\u6cdb\u5bf9\u8c61\u3001\u6982\u5ff5\u548c\u4efb\u52a1\u7684\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u5f00\u53d1\u4e86\u5168\u81ea\u52a8\u7684\u4eba\u7c7b\u6d3b\u52a8\u5206\u6790\u65b9\u6cd5\uff0c\u5c06\u4efb\u610f\u4eba\u7c7b\u624b\u90e8\u89c6\u9891\u8f6c\u6362\u4e3a\u4e0e\u673a\u5668\u4ebaVLA\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u5bf9\u9f50\u7684\u7247\u6bb5\uff0c\u5305\u542b\u539f\u5b50\u7ea7\u624b\u90e8\u6d3b\u52a8\u5206\u5272\u3001\u8bed\u8a00\u63cf\u8ff0\u30013D\u624b\u90e8\u8fd0\u52a8\u548c\u76f8\u673a\u8fd0\u52a8\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b100\u4e07\u7247\u6bb5\u30012600\u4e07\u5e27\u7684\u624b\u90e8VLA\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4e16\u754c\u89c2\u5bdf\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u7ecf\u8fc7\u5c11\u91cf\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u5fae\u8c03\u540e\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684VLA\u9884\u8bad\u7ec3\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u5411\u771f\u6b63\u53ef\u6cdb\u5316\u7684\u5177\u8eab\u667a\u80fd\u53d1\u5c55\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u6027\u80fd\u968f\u9884\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u6269\u5c55\u7684\u826f\u597d\u7279\u6027\u3002"}}
{"id": "2510.21609", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21609", "abs": "https://arxiv.org/abs/2510.21609", "authors": ["Elle Miller", "Trevor McInroe", "David Abel", "Oisin Mac Aodha", "Sethu Vijayakumar"], "title": "Enhancing Tactile-based Reinforcement Learning for Robotic Control", "comment": null, "summary": "Achieving safe, reliable real-world robotic manipulation requires agents to\nevolve beyond vision and incorporate tactile sensing to overcome sensory\ndeficits and reliance on idealised state information. Despite its potential,\nthe efficacy of tactile sensing in reinforcement learning (RL) remains\ninconsistent. We address this by developing self-supervised learning (SSL)\nmethodologies to more effectively harness tactile observations, focusing on a\nscalable setup of proprioception and sparse binary contacts. We empirically\ndemonstrate that sparse binary tactile signals are critical for dexterity,\nparticularly for interactions that proprioceptive control errors do not\nregister, such as decoupled robot-object motions. Our agents achieve superhuman\ndexterity in complex contact tasks (ball bouncing and Baoding ball rotation).\nFurthermore, we find that decoupling the SSL memory from the on-policy memory\ncan improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark\nto standardise and promote future research in tactile-based manipulation.\nProject page: https://elle-miller.github.io/tactile_rl", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u5229\u7528\u7a00\u758f\u4e8c\u8fdb\u5236\u89e6\u89c9\u4fe1\u53f7\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7075\u5de7\u6027\uff0c\u5728\u590d\u6742\u63a5\u89e6\u4efb\u52a1\u4e2d\u8fbe\u5230\u8d85\u4eba\u7c7b\u6c34\u5e73\uff0c\u5e76\u53d1\u5e03\u4e86RoTO\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u89c6\u89c9\u611f\u77e5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u89e6\u89c9\u4f20\u611f\u514b\u670d\u611f\u5b98\u7f3a\u9677\u548c\u5bf9\u7406\u60f3\u72b6\u6001\u4fe1\u606f\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4e2d\u89e6\u89c9\u4f20\u611f\u7684\u6548\u80fd\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u672c\u4f53\u611f\u89c9\u548c\u7a00\u758f\u4e8c\u8fdb\u5236\u63a5\u89e6\u4fe1\u53f7\uff0c\u5c06SSL\u8bb0\u5fc6\u4e0e\u7b56\u7565\u8bb0\u5fc6\u89e3\u8026\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u7a00\u758f\u4e8c\u8fdb\u5236\u89e6\u89c9\u4fe1\u53f7\u5bf9\u7075\u5de7\u6027\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u89e3\u8026\u7684\u673a\u5668\u4eba-\u7269\u4f53\u8fd0\u52a8\u4e2d\u3002\u667a\u80fd\u4f53\u5728\u590d\u6742\u63a5\u89e6\u4efb\u52a1\uff08\u7403\u5f39\u8df3\u548c\u4fdd\u5b9a\u7403\u65cb\u8f6c\uff09\u4e2d\u8fbe\u5230\u8d85\u4eba\u7c7b\u7075\u5de7\u6027\u3002", "conclusion": "\u89e6\u89c9\u4f20\u611f\u5bf9\u4e8e\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u89e6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0cRoTO\u57fa\u51c6\u5c06\u4fc3\u8fdb\u672a\u6765\u89e6\u89c9\u64cd\u4f5c\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.21648", "categories": ["cs.RO", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.21648", "abs": "https://arxiv.org/abs/2510.21648", "authors": ["Inbazhagan Ravikumar", "Ram Sundhar", "Narendhiran Vijayakumar"], "title": "Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning", "comment": "8 pages, 4 figures, 4 tables", "summary": "Micro aerial vehicles are becoming increasingly important in search and\nrescue operations due to their agility, speed, and ability to access confined\nspaces or hazardous areas. However, designing lightweight aerial systems\npresents significant structural, aerodynamic, and computational challenges.\nThis work addresses two key limitations in many low-cost aerial systems under\ntwo kilograms: their lack of structural durability during flight through rough\nterrains and inability to replan paths dynamically when new victims or\nobstacles are detected. We present a fully customised drone built from scratch\nusing only commonly available components and materials, emphasising modularity,\nlow cost, and ease of assembly. The structural frame is reinforced with\nlightweight yet durable materials to withstand impact, while the onboard\ncontrol system is powered entirely by free, open-source software solutions. The\nproposed system demonstrates real-time perception and adaptive navigation\ncapabilities without relying on expensive hardware accelerators, offering an\naffordable and practical solution for real-world search and rescue missions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6a21\u5757\u5316\u7684\u5b9a\u5236\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u7528\u4e8e\u641c\u6551\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u8f7b\u578b\u65e0\u4eba\u673a\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u7ed3\u6784\u8010\u7528\u6027\u548c\u52a8\u6001\u8def\u5f84\u91cd\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4f4e\u6210\u672c\u8f7b\u578b\u65e0\u4eba\u673a\u5728\u641c\u6551\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5728\u590d\u6742\u5730\u5f62\u98de\u884c\u65f6\u7f3a\u4e4f\u7ed3\u6784\u8010\u7528\u6027\uff0c\u4ee5\u53ca\u5728\u68c0\u6d4b\u5230\u65b0\u53d7\u5bb3\u8005\u6216\u969c\u788d\u7269\u65f6\u65e0\u6cd5\u52a8\u6001\u91cd\u89c4\u5212\u8def\u5f84\u3002", "method": "\u4f7f\u7528\u5e38\u89c1\u7ec4\u4ef6\u548c\u6750\u6599\u4ece\u5934\u6784\u5efa\u5b8c\u5168\u5b9a\u5236\u7684\u65e0\u4eba\u673a\uff0c\u5f3a\u8c03\u6a21\u5757\u5316\u3001\u4f4e\u6210\u672c\u548c\u6613\u7ec4\u88c5\u6027\u3002\u7ed3\u6784\u6846\u67b6\u91c7\u7528\u8f7b\u91cf\u8010\u7528\u7684\u6750\u6599\u52a0\u56fa\uff0c\u673a\u8f7d\u63a7\u5236\u7cfb\u7edf\u5b8c\u5168\u57fa\u4e8e\u514d\u8d39\u5f00\u6e90\u8f6f\u4ef6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5c55\u793a\u4e86\u5b9e\u65f6\u611f\u77e5\u548c\u81ea\u9002\u5e94\u5bfc\u822a\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u4e3a\u5b9e\u9645\u641c\u6551\u4efb\u52a1\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u5b9a\u5236\u65e0\u4eba\u673a\u7cfb\u7edf\u901a\u8fc7\u7ed3\u6784\u52a0\u56fa\u548c\u5f00\u6e90\u8f6f\u4ef6\u63a7\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u6210\u672c\u65e0\u4eba\u673a\u5728\u641c\u6551\u5e94\u7528\u4e2d\u7684\u8010\u7528\u6027\u548c\u52a8\u6001\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
