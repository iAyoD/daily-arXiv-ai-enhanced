{"id": "2602.04982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04982", "abs": "https://arxiv.org/abs/2602.04982", "authors": ["Deepak Gupta", "Davis Bartels", "Dina Demner-Fuhsman"], "title": "BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations", "comment": "Work in progress", "summary": "With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.", "AI": {"tldr": "BioACE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u751f\u7269\u533b\u5b66\u95ee\u7b54\u548c\u5f15\u7528\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5b8c\u6574\u6027\u3001\u6b63\u786e\u6027\u3001\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u7b49\u591a\u4e2a\u7ef4\u5ea6\u6765\u8bc4\u4f30LLM\u751f\u6210\u7684\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u81ea\u52a8\u5316\u8bc4\u4f30\u751f\u6210\u7b54\u6848\u7684\u8d28\u91cf\u548c\u5f15\u7528\u652f\u6301\u3002\u7531\u4e8e\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u4e13\u4e1a\u6027\u8981\u6c42\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u5f53\u524d\u8bc4\u4f30\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faBioACE\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u65b9\u6cd5\u8bc4\u4f30\u5b8c\u6574\u6027\u3001\u6b63\u786e\u6027\u3001\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\u3002\u540c\u65f6\u8003\u8651\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548cLLM\u7b49\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u6765\u8bc4\u4f30\u5f15\u7528\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u786e\u5b9a\u4e86\u751f\u7269\u533b\u5b66\u7b54\u6848\u548c\u5f15\u7528\u8bc4\u4f30\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230BioACE\u8bc4\u4f30\u5305\u4e2d\u3002", "conclusion": "BioACE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u81ea\u52a8\u5316\u6846\u67b6\u6765\u8bc4\u4f30\u751f\u7269\u533b\u5b66\u95ee\u7b54\u548c\u5f15\u7528\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u4e13\u4e1a\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u6700\u4f73\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2602.05004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05004", "abs": "https://arxiv.org/abs/2602.05004", "authors": ["Zexin Lin", "Jiachen Yu", "Haoyang Zhang", "Yuzhao Li", "Zhonghang Li", "Yujiu Yang", "Junjie Wang", "Xiaoqiang Ji"], "title": "CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System", "comment": null, "summary": "Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.", "AI": {"tldr": "CoWork-X\uff1a\u5b9e\u65f6\u534f\u4f5c\u4efb\u52a1\u7684\u4e3b\u52a8\u534f\u540c\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6280\u80fd\u5e93\u548c\u9884\u7b97\u7ea6\u675f\u7684\u8865\u4e01\u5f0f\u6280\u80fd\u6574\u5408\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u6301\u7eed\u6027\u80fd\u63d0\u5347", "motivation": "\u5f53\u524d\u8bed\u8a00\u6761\u4ef6\u667a\u80fd\u4f53\u5728\u9ad8\u5ea6\u534f\u4f5c\u4efb\u52a1\u4e2d\u9762\u4e34\u4e24\u4e2a\u77db\u76fe\u7ea6\u675f\uff1a\u9700\u8981\u4e9a\u79d2\u7ea7\u5b9e\u65f6\u534f\u8c03\uff0c\u540c\u65f6\u8981\u5728\u4e25\u683c\u7684\u5728\u7ebftoken\u9884\u7b97\u4e0b\u8fdb\u884c\u6301\u7eed\u591a\u56de\u5408\u9002\u5e94\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9891\u7e41\u7684\u56de\u5408\u5185\u63a8\u7406\u5bfc\u81f4\u5ef6\u8fdf\u548c\u65f6\u5e8f\u6296\u52a8\uff0c\u8981\u4e48\u901a\u8fc7\u975e\u7ed3\u6784\u5316\u6587\u672c\u8fdb\u884c\u56de\u5408\u540e\u6539\u8fdb\uff0c\u96be\u4ee5\u7f16\u8bd1\u6210\u53ef\u9760\u7684\u4f4e\u6210\u672c\u6267\u884c\u3002", "method": "\u63d0\u51faCoWork-X\u4e3b\u52a8\u534f\u540c\u8fdb\u5316\u6846\u67b6\uff0c\u5c06\u540c\u4f34\u534f\u4f5c\u5efa\u6a21\u4e3a\u8de8\u56de\u5408\u7684\u95ed\u73af\u4f18\u5316\u95ee\u9898\u3002\u6846\u67b6\u5305\u542b\uff1a1) Skill-Agent\uff1a\u901a\u8fc7HTN\uff08\u5206\u5c42\u4efb\u52a1\u7f51\u7edc\uff09\u4ece\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u7ec4\u5408\u7684\u6280\u80fd\u5e93\u4e2d\u68c0\u7d22\u6280\u80fd\u6267\u884c\uff1b2) \u56de\u5408\u540eCo-Optimizer\uff1a\u5728\u660e\u786e\u9884\u7b97\u7ea6\u675f\u548c\u6f02\u79fb\u6b63\u5219\u5316\u4e0b\u8fdb\u884c\u8865\u4e01\u5f0f\u6280\u80fd\u6574\u5408\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684Overcooked-AI\u7c7b\u5b9e\u65f6\u534f\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoWork-X\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u7d2f\u79ef\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u6301\u7eed\u964d\u4f4e\u5728\u7ebf\u5ef6\u8fdf\u548ctoken\u4f7f\u7528\u91cf\u3002", "conclusion": "CoWork-X\u901a\u8fc7\u5c06\u5feb\u901f-\u6162\u901f\u8bb0\u5fc6\u5206\u79bb\u601d\u60f3\u5e94\u7528\u4e8e\u540c\u4f34\u534f\u4f5c\uff0c\u89e3\u51b3\u4e86\u5b9e\u65f6\u534f\u8c03\u4e0e\u6301\u7eed\u9002\u5e94\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u6761\u4ef6\u667a\u80fd\u4f53\u5728\u4e25\u683c\u7ea6\u675f\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05035", "abs": "https://arxiv.org/abs/2602.05035", "authors": ["Sean Trott", "Pamela D. Rivi\u00e8re"], "title": "Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation", "comment": "9 pages, 5 figures, conference", "summary": "Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.", "AI": {"tldr": "\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u6c47\u6d88\u6b67\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u5982\u5355\u8bed\u6a21\u578b\uff0c\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\"\u591a\u8bed\u8a00\u60e9\u7f5a\"\u6e90\u4e8e\u4e09\u79cd\u80fd\u529b\u9650\u5236\uff1a\u8868\u5f81\u9650\u5236\u3001\u6ce8\u610f\u529b\u9650\u5236\u548c\u8bcd\u6c47\u9650\u5236\u3002", "motivation": "\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u6709\u65f6\u8868\u73b0\u4e0d\u5982\u5355\u8bed\u6a21\u578b\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u6a21\u578b\u5bb9\u91cf\u9650\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cf\u5316\u8fd9\u79cd\"\u591a\u8bed\u8a00\u60e9\u7f5a\"\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u8bed\u4e49\u8868\u5f81\u548c\u4e0a\u4e0b\u6587\u673a\u5236\u7684\u8bcd\u6c47\u6d88\u6b67\u4efb\u52a1\u4e0a\uff0c\u5e76\u63a2\u7d22\u5177\u4f53\u7684\u80fd\u529b\u7ea6\u675f\u673a\u5236\u3002", "method": "\u4f7f\u7528\u82f1\u8bed\u548c\u897f\u73ed\u7259\u8bed\u4e2d\u6b67\u4e49\u8bcd\u7684\u4eba\u7c7b\u76f8\u5173\u6027\u5224\u65ad\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u7684\u5355\u8bed\u548c\u591a\u8bed\u8a00\u6a21\u578b\u3002\u63a2\u7d22\u4e09\u79cd\u6f5c\u5728\u80fd\u529b\u7ea6\u675f\uff1a\u8868\u5f81\u9650\u5236\uff08\u5d4c\u5165\u5404\u5411\u540c\u6027\u964d\u4f4e\uff09\u3001\u6ce8\u610f\u529b\u9650\u5236\uff08\u5bf9\u6d88\u6b67\u7ebf\u7d22\u7684\u5173\u6ce8\u51cf\u5c11\uff09\u548c\u8bcd\u6c47\u9650\u5236\uff08\u591atoken\u5206\u5272\u589e\u52a0\uff09\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u6240\u6709\u6bd4\u8f83\u4e2d\u8868\u73b0\u4e00\u81f4\u964d\u4f4e\u3002\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6240\u6709\u4e09\u79cd\u9650\u5236\u7684\u8bc1\u636e\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5728\u7edf\u8ba1\u4e0a\u89e3\u91ca\u4e86\u539f\u672c\u5f52\u56e0\u4e8e\u6a21\u578b\u591a\u8bed\u8a00\u72b6\u6001\u7684\u65b9\u5dee\u3002\u8fd9\u4e9b\u9650\u5236\u4e0e\u6d88\u6b67\u6027\u80fd\u964d\u4f4e\u76f8\u5173\u3002", "conclusion": "\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u53d7\u5230\u591a\u79cd\u80fd\u529b\u7ea6\u675f\uff0c\u8fd9\u4e9b\u7ea6\u675f\u4e0e\u6d88\u6b67\u6027\u80fd\u964d\u4f4e\u76f8\u5173\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u6c47\u6d88\u6b67\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u5177\u4f53\u673a\u5236\u3002"}}
{"id": "2602.05085", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05085", "abs": "https://arxiv.org/abs/2602.05085", "authors": ["Sidi Lu", "Zhenwen Liang", "Dongyang Ma", "Yan Wang", "Haitao Mi", "Dong Yu"], "title": "Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories", "comment": "Tencent AI Lab Technical Report", "summary": "In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.", "AI": {"tldr": "Locas\u662f\u4e00\u79cd\u5c40\u90e8\u652f\u6301\u7684\u53c2\u6570\u5316\u8bb0\u5fc6\u673a\u5236\uff0c\u53ef\u4e0eTransformer\u7684FFN\u5757\u8bbe\u8ba1\u5171\u4eab\uff0c\u652f\u6301\u7075\u6d3b\u5378\u8f7d\u6216\u5408\u5e76\u5230\u6a21\u578b\u53c2\u6570\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u6865\u63a5\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e0e\u65b0\u578b\u53c2\u6570\u5316\u8bb0\u5fc6\uff0c\u4f7f\u8bb0\u5fc6\u80fd\u591f\u7075\u6d3b\u5730\u4ece\u6a21\u578b\u53c2\u6570\u4e2d\u5378\u8f7d\u6216\u5408\u5e76\uff0c\u652f\u6301\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u63d0\u51faLocas\uff08\u5c40\u90e8\u652f\u6301\u7684\u53c2\u6570\u5316\u8bb0\u5fc6\uff09\uff0c\u6709\u4e24\u79cd\u53d8\u4f53\uff1a1\uff09\u4f20\u7edf\u4e24\u5c42MLP\u8bbe\u8ba1\uff0c\u7406\u8bba\u4fdd\u8bc1\u66f4\u6e05\u6670\uff1b2\uff09\u4e0eSOTA LLMs\u5171\u4eabGLU-FFN\u7ed3\u6784\uff0c\u53ef\u8f7b\u677e\u9644\u52a0\u5230\u73b0\u6709\u6a21\u578b\u3002\u5173\u952e\u662f\u901a\u8fc7\u91cd\u7528\u6a21\u578b\u53c2\u6570\u3001\u6fc0\u6d3b\u548c/\u6216\u68af\u5ea6\u8fdb\u884c\u539f\u5219\u6027\u521d\u59cb\u5316\u3002", "result": "\u5728PG-19\u6574\u4e66\u8bed\u8a00\u5efa\u6a21\u548cLoCoMo\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u95ee\u7b54\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u3002Locas-GLU\u4ec5\u97000.02%\u989d\u5916\u53c2\u6570\u5373\u53ef\u5b58\u50a8\u8fc7\u53bb\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u5c0f\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002MMLU\u8bc4\u4f30\u663e\u793aLocas\u80fd\u591f\u5c06\u8fc7\u53bb\u4e0a\u4e0b\u6587\u6c38\u4e45\u5316\u4e3a\u53c2\u6570\u77e5\u8bc6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u6a21\u578b\u73b0\u6709\u5185\u90e8\u77e5\u8bc6\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "Locas\u5c55\u793a\u4e86\u5c06\u8fc7\u53bb\u4e0a\u4e0b\u6587\u6c38\u4e45\u5316\u4e3a\u53c2\u6570\u77e5\u8bc6\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05010", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05010", "abs": "https://arxiv.org/abs/2602.05010", "authors": ["Maia Stiber", "Sameer Khan", "Russell Taylor", "Chien-Ming Huang"], "title": "Signal or 'Noise': Human Reactions to Robot Errors in the Wild", "comment": null, "summary": "In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but \"noisy.\" We discuss lessons, benefits, and challenges for using social signals in real-world HRI.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5496\u5561\u673a\u5668\u4eba\u7684\u5b9e\u5730\u90e8\u7f72\uff0c\u63a2\u7d22\u4eba\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5982\u4f55\u5bf9\u673a\u5668\u4eba\u9519\u8bef\u505a\u51fa\u793e\u4f1a\u4fe1\u53f7\u53cd\u5e94\uff0c\u53d1\u73b0\u8fd9\u4e9b\u4fe1\u53f7\u4e30\u5bcc\u4f46\"\u5608\u6742\"\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u673a\u5668\u4eba\u7ecf\u5e38\u51fa\u9519\uff0c\u4f46\u4eba\u4eec\u5bf9\u673a\u5668\u4eba\u9519\u8bef\u7684\u793e\u4f1a\u53cd\u5e94\u5728\u5b9e\u9a8c\u5ba4\u5916\u4e86\u89e3\u751a\u5c11\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\u793e\u4f1a\u4fe1\u53f7\u5728\u53d7\u9650\u4ea4\u4e92\u4e2d\u5bf9\u9519\u8bef\u7ba1\u7406\u53ef\u9760\u6709\u7528\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\uff08\u7279\u522b\u662f\u975e\u793e\u4ea4\u673a\u5668\u4eba\u3001\u91cd\u590d\u4ea4\u4e92\u3001\u7fa4\u4f53\u4ea4\u4e92\u548c\u8fde\u7eed\u9519\u8bef\u4f20\u64ad\u60c5\u51b5\u4e0b\uff09\u662f\u5426\u4ecd\u7136\u6709\u6548\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u6784\u5efa\u5496\u5561\u673a\u5668\u4eba\u5e76\u8fdb\u884c\u516c\u5171\u5b9e\u5730\u90e8\u7f72\uff08N=49\uff09\uff0c\u89c2\u5bdf\u53c2\u4e0e\u8005\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5bf9\u673a\u5668\u4eba\u9519\u8bef\u548c\u5176\u4ed6\u523a\u6fc0\u7684\u793e\u4f1a\u4fe1\u53f7\u53cd\u5e94\uff0c\u7279\u522b\u5173\u6ce8\u7fa4\u4f53\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53c2\u4e0e\u8005\u5bf9\u9519\u8bef\u548c\u5176\u4ed6\u523a\u6fc0\u4e00\u81f4\u8868\u8fbe\u591a\u6837\u5316\u7684\u793e\u4f1a\u4fe1\u53f7\uff0c\u5c24\u5176\u5728\u7fa4\u4f53\u4ea4\u4e92\u4e2d\u66f4\u4e3a\u660e\u663e\u3002\u7814\u7a76\u53d1\u73b0\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u793e\u4f1a\u4fe1\u53f7\u4e30\u5bcc\uff08\u53c2\u4e0e\u8005\u4e3b\u52a8\u63d0\u4f9b\u4ea4\u4e92\u4fe1\u606f\uff09\uff0c\u4f46\"\u5608\u6742\"\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u4eba\u673a\u4ea4\u4e92\u4e2d\u4f7f\u7528\u793e\u4f1a\u4fe1\u53f7\u7684\u6559\u8bad\u3001\u76ca\u5904\u548c\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.05106", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05106", "abs": "https://arxiv.org/abs/2602.05106", "authors": ["Michael Browder", "Kevin Duh", "J. David Harris", "Vince Lyzinski", "Paul McNamee", "Youngser Park", "Carey E. Priebe", "Peter Viechnicki"], "title": "Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models", "comment": null, "summary": "Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6570\u636e\u6838\u89c6\u89d2\u7a7a\u95f4\uff08DKPS\uff09\u4f5c\u4e3a\u5206\u6790Transformer\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u89e3\u51b3LLM\u9ed1\u76d2\u95ee\u9898\u3002", "motivation": "\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u662f\u6784\u5efa\u9ad8\u6027\u80fd\u8bed\u8a00\u6280\u672f\u548c\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u4e3b\u8981\u74f6\u9888\u3002\u867d\u7136Transformer\u6a21\u578b\uff08\u7279\u522b\u662fLLM\uff09\u88ab\u7528\u4e8e\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u662f\u9ed1\u76d2\uff0c\u5408\u6210\u6570\u636e\u7684\u5c5e\u6027\u96be\u4ee5\u9884\u6d4b\u3002\u5b9e\u8df5\u4e2d\u5de5\u7a0b\u5e08\u901a\u5e38\u53ea\u80fd\u8c03\u6574\u6e29\u5ea6\u53c2\u6570\u5e76\u5e0c\u671b\u7ed3\u679c\u80fd\u6539\u5584\u4e0b\u6e38\u6a21\u578b\uff0c\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u6570\u636e\u6838\u89c6\u89d2\u7a7a\u95f4\uff08DKPS\uff09\u4f5c\u4e3a\u6570\u5b66\u5206\u6790\u57fa\u7840\uff0c\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u5efa\u7acbDKPS\u6846\u67b6\uff0c\u4e3aTransformer\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u63d0\u4f9b\u5177\u4f53\u7684\u7edf\u8ba1\u4fdd\u8bc1\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u9610\u660e\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u6216\u4f7f\u7528\u5bf9\u6bd4\u504f\u597d\u4f18\u5316\u7684LLM\uff09\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "DKPS\u63d0\u4f9b\u4e86Transformer\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u80fd\u591f\u6307\u5bfc\u5408\u6210\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u57fa\u4e8e\u7406\u8bba\u4fdd\u8bc1\u800c\u975e\u8bd5\u9519\u6765\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002\u8be5\u6846\u67b6\u4e3a\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "DKPS\u4e3a\u89e3\u51b3LLM\u9ed1\u76d2\u95ee\u9898\u548c\u5408\u6210\u6570\u636e\u8d28\u91cf\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u6570\u5b66\u5206\u6790\u6846\u67b6\uff0c\u4e3a\u8bed\u8a00\u6280\u672f\u5de5\u7a0b\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002\u6587\u4e2d\u4e5f\u8ba8\u8bba\u4e86\u5f53\u524d\u5de5\u4f5c\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.05029", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05029", "abs": "https://arxiv.org/abs/2602.05029", "authors": ["Octavio Arriaga", "Proneet Sharma", "Jichen Guo", "Marc Otto", "Siddhant Kadwe", "Rebecca Adam"], "title": "Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping", "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L) for review. This version includes the statement required by IEEE for preprints", "summary": "Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u5fae\u5206\u795e\u7ecf\u56fe\u5f62\u6a21\u578b\uff0c\u7ed3\u5408\u795e\u7ecf\u57fa\u7840\u6a21\u578b\u4e0e\u7269\u7406\u53ef\u5fae\u5206\u6e32\u67d3\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u573a\u666f\u91cd\u5efa\u4e0e\u673a\u5668\u4eba\u6293\u53d6\uff0c\u65e0\u9700\u989d\u59163D\u6570\u636e\u6216\u6d4b\u8bd5\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6837\u672c\u6784\u5efa\u9ed1\u76d2\u573a\u666f\u8868\u793a\uff0c\u7f3a\u4e4f\u6570\u636e\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u5728\u65b0\u73af\u5883\u4e2d\u8fdb\u884c\u96f6\u6837\u672c\u573a\u666f\u91cd\u5efa\u548c\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u57fa\u7840\u6a21\u578b\u4e0e\u7269\u7406\u53ef\u5fae\u5206\u6e32\u67d3\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4f30\u8ba1\u7269\u7406\u4e00\u81f4\u7684\u573a\u666f\u53c2\u6570\uff08\u7f51\u683c\u3001\u5149\u7167\u3001\u6750\u8d28\u30016D\u4f4d\u59ff\uff09\uff0c\u4ec5\u9700\u5355\u5f20RGBD\u56fe\u50cf\u548c\u8fb9\u754c\u6846\u3002", "result": "\u5728\u6807\u51c6\u65e0\u6a21\u578b\u5c11\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u96f6\u6837\u672c\u6293\u53d6\u4efb\u52a1\u9a8c\u8bc1\u4e86\u573a\u666f\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u6570\u636e\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u9014\u5f84\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u96c6\u6216\u6d4b\u8bd5\u91c7\u6837\u3002"}}
{"id": "2602.05107", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05107", "abs": "https://arxiv.org/abs/2602.05107", "authors": ["Ahmed Ruby", "Christian Hardmeier", "Sara Stymne"], "title": "Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text", "comment": null, "summary": "Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u8bed\u8a00\u591a\u6a21\u6001\u9690\u5f0f\u7bc7\u7ae0\u5173\u7cfb\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7Qwen2-Audio\u6574\u5408\u6587\u672c\u548c\u97f3\u9891\u4fe1\u606f\uff0c\u5728\u82f1\u8bed\u3001\u6cd5\u8bed\u548c\u897f\u73ed\u7259\u8bed\u4e0a\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u878d\u5408\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9690\u5f0f\u7bc7\u7ae0\u5173\u7cfb\u5206\u7c7b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63a8\u65ad\u542b\u4e49\u3002\u4e0a\u4e0b\u6587\u7ebf\u7d22\u53ef\u80fd\u5206\u5e03\u5728\u591a\u79cd\u6a21\u6001\u4e2d\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u4ec5\u9760\u6587\u672c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "1) \u63d0\u51fa\u81ea\u52a8\u65b9\u6cd5\u6784\u5efa\u82f1\u8bed\u3001\u6cd5\u8bed\u548c\u897f\u73ed\u7259\u8bed\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u9690\u5f0f\u7bc7\u7ae0\u5173\u7cfb\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u901a\u8fc7Qwen2-Audio\u6574\u5408\u6587\u672c\u548c\u58f0\u5b66\u4fe1\u606f\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u7684\u6587\u672c\u548c\u97f3\u9891\u8054\u5408\u5efa\u6a21\u3002", "result": "1) \u57fa\u4e8e\u6587\u672c\u7684\u6a21\u578b\u4f18\u4e8e\u57fa\u4e8e\u97f3\u9891\u7684\u6a21\u578b\uff1b2) \u6574\u5408\u4e24\u79cd\u6a21\u6001\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\uff1b3) \u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e26\u6765\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u662f\u63d0\u5347\u9690\u5f0f\u7bc7\u7ae0\u5173\u7cfb\u5206\u7c7b\u6027\u80fd\u7684\u6709\u6548\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u65f6\u3002"}}
{"id": "2602.05079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05079", "abs": "https://arxiv.org/abs/2602.05079", "authors": ["Vinal Asodia", "Iman Sharifi", "Saber Fallah"], "title": "Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking", "comment": "12 pages, 7 figures, 5 tables", "summary": "The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u795e\u7ecf\u7b26\u53f7\u7279\u5f81\u8868\u793a\u548c\u8f6f\u4e00\u9636\u903b\u8f91\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u6574\u5408\u8bed\u4e49\u3001\u7a7a\u95f4\u548c\u5f62\u72b6\u4fe1\u606f\u4ee5\u53ca\u52a8\u6001\u5b9e\u4f53\u7684\u7a7a\u95f4\u589e\u5f3a\u7279\u5f81\uff0c\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u6a21\u5757\u5e73\u8861\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u63d0\u9ad8\u4e86\u7b56\u7565\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u5f88\u5c11\u5c06\u9ad8\u5c42\u573a\u666f\u4e0a\u4e0b\u6587\u6574\u5408\u5230\u7279\u5f81\u8868\u793a\u4e2d\uff0c\u4ee5\u53ca\u4f9d\u8d56\u50f5\u5316\u7684\u56fa\u5b9a\u5956\u52b1\u51fd\u6570\u3002\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u548c\u4ef7\u503c\u5bf9\u9f50\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u7279\u5f81\u8868\u793a\u7ba1\u9053\uff0c\u5305\u542b\u8bed\u4e49\u3001\u7a7a\u95f4\u548c\u5f62\u72b6\u4fe1\u606f\uff0c\u4ee5\u53ca\u52a8\u6001\u5b9e\u4f53\uff08\u7279\u522b\u662f\u5b89\u5168\u5173\u952e\u9053\u8def\u4f7f\u7528\u8005\uff09\u7684\u7a7a\u95f4\u589e\u5f3a\u7279\u5f81\u3002\u540c\u65f6\u63d0\u51fa\u8f6f\u4e00\u9636\u903b\u8f91\uff08SFOL\uff09\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u7b26\u53f7\u63a8\u7406\u6a21\u5757\u4ece\u5206\u5272\u56fe\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u7a7a\u95f4\u8c13\u8bcd\uff0c\u5e94\u7528\u8bed\u8a00\u89c4\u5219\u83b7\u5f97\u5956\u52b1\u6743\u91cd\u3002", "result": "\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u7684\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u8868\u793a\u548c\u5956\u52b1\u516c\u5f0f\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u7b26\u53f7\u8868\u793a\u548cSFOL\u5956\u52b1\u51fd\u6570\u5728\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u548c\u906e\u6321\u6c34\u5e73\u4e0b\u63d0\u9ad8\u4e86\u7b56\u7565\u9c81\u68d2\u6027\u548c\u5b89\u5168\u76f8\u5173\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u5c06\u6574\u4f53\u8868\u793a\u548c\u8f6f\u63a8\u7406\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u53ef\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u4e2d\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u4ef7\u503c\u5bf9\u9f50\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u8bc1\u660e\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.05150", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05150", "abs": "https://arxiv.org/abs/2602.05150", "authors": ["Yang Zhang", "Mersin Konomi", "Christos Xypolopoulos", "Konstantinos Divriotis", "Konstantinos Skianis", "Giannis Nikolentzos", "Giorgos Stamou", "Guokan Shang", "Michalis Vazirgiannis"], "title": "GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek", "comment": null, "summary": "Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.", "AI": {"tldr": "\u5e0c\u814aMMLU\u662f\u4e00\u4e2a\u57fa\u4e8e\u5e0c\u814a\u672c\u571f\u5185\u5bb9\u7684\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b21,805\u4e2a\u591a\u9879\u9009\u62e9\u9898\uff0c\u6db5\u76d645\u4e2a\u5b66\u79d1\u9886\u57df\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5e0c\u814a\u8bed\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dLLM\u867d\u7136\u5728\u5e0c\u814a\u8bed\u8bed\u6599\u4e0a\u8bad\u7ec3\uff0c\u4f46\u7f3a\u4e4f\u57fa\u4e8e\u771f\u5b9e\u672c\u571f\u5185\u5bb9\u7684\u53ef\u9760\u8bc4\u4f30\u57fa\u51c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u82f1\u8bed\u673a\u5668\u7ffb\u8bd1\uff0c\u65e0\u6cd5\u6355\u6349\u5e0c\u814a\u8bed\u8a00\u548c\u6587\u5316\u7279\u6027\u3002", "method": "\u521b\u5efa\u5e0c\u814aMMLU\u57fa\u51c6\uff1a\u6536\u96c6\u6216\u7f16\u519921,805\u4e2a\u5e0c\u814a\u8bed\u591a\u9879\u9009\u62e9\u9898\uff0c\u6765\u81ea\u5b66\u672f\u3001\u4e13\u4e1a\u548c\u653f\u5e9c\u8003\u8bd5\uff0c\u6db5\u76d645\u4e2a\u5b66\u79d1\uff0c\u6309\u65b0\u5b9a\u4e49\u7684\u4e3b\u9898\u5206\u7c7b\u6cd5\u7ec4\u7ec7\uff0c\u5e76\u6807\u6ce8\u6559\u80b2\u96be\u5ea6\u7ea7\u522b\u3002", "result": "\u8bc4\u4f3080\u591a\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90LLM\u663e\u793a\uff1a\u524d\u6cbf\u6a21\u578b\u4e0e\u5f00\u6e90\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1b\u5e0c\u814a\u9002\u5e94\u6a21\u578b\u4e0e\u901a\u7528\u591a\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u4e5f\u6709\u660e\u663e\u5dee\u5f02\u3002\u516c\u5f0016,857\u4e2a\u6837\u672c\uff0c\u4fdd\u75594,948\u4e2a\u7528\u4e8e\u79c1\u6709\u6392\u884c\u699c\u3002", "conclusion": "\u5e0c\u814aMMLU\u586b\u8865\u4e86\u5e0c\u814a\u8bed\u8bc4\u4f30\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u6539\u8fdbLLM\u5728\u5e0c\u814a\u8bed\u4e0a\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u9002\u5e94\u6027\u548c\u63d0\u793a\u7b56\u7565\u7b49\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.05092", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05092", "abs": "https://arxiv.org/abs/2602.05092", "authors": ["Thomas Cohn", "Lihan Tang", "Alexandre Amice", "Russ Tedrake"], "title": "A Framework for Combining Optimization-Based and Analytic Inverse Kinematics", "comment": "19 pages, 5 figures, 6 tables. Under submission", "summary": "Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u9006\u8fd0\u52a8\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u6790\u89e3\u4f5c\u4e3a\u53d8\u91cf\u53d8\u6362\uff0c\u4f7f\u4f18\u5316\u5668\u66f4\u5bb9\u6613\u6c42\u89e3\uff0c\u5728\u5404\u79cd\u6311\u6218\u6027IK\u95ee\u9898\u4e0a\u83b7\u5f97\u66f4\u9ad8\u6210\u529f\u7387", "motivation": "\u4f20\u7edf\u89e3\u6790\u65b9\u6cd5\u548c\u4f18\u5316\u65b9\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u4f46\u96be\u4ee5\u7edf\u4e00\u3002\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u5173\u8282\u89d2\u5ea6\u4e0e\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u4e4b\u95f4\u7684\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u52a0\u4e0a\u907f\u78b0\u7b49\u975e\u51f8\u7ea6\u675f\u65f6\u5931\u8d25\u7387\u8f83\u9ad8", "method": "\u63d0\u51fa\u65b0\u7684\u4f18\u5316IK\u516c\u5f0f\uff0c\u4f7f\u7528\u89e3\u6790IK\u89e3\u4f5c\u4e3a\u53d8\u91cf\u53d8\u6362\uff0c\u4ece\u6839\u672c\u4e0a\u964d\u4f4e\u4f18\u5316\u5668\u6c42\u89e3\u96be\u5ea6\u3002\u5728\u4e09\u79cd\u6d41\u884c\u7684\u6c42\u89e3\u5668\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4ee3\u8868\u4e09\u79cd\u4e0d\u540c\u7684\u7ea6\u675f\u975e\u7ebf\u6027\u4f18\u5316\u8303\u5f0f", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6bd4\u8f83\u8868\u660e\uff0c\u65b0\u516c\u5f0f\u5728\u907f\u78b0\u3001\u6293\u53d6\u9009\u62e9\u548c\u4eff\u4eba\u673a\u5668\u4eba\u7a33\u5b9a\u6027\u7b49\u5404\u79cd\u6311\u6218\u6027IK\u95ee\u9898\u4e0a\uff0c\u6bd4\u65e7\u516c\u5f0f\u548c\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u6210\u529f\u7387", "conclusion": "\u901a\u8fc7\u5c06\u89e3\u6790IK\u89e3\u4f5c\u4e3a\u53d8\u91cf\u53d8\u6362\u7684\u65b0\u4f18\u5316IK\u516c\u5f0f\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u89e3\u6790\u65b9\u6cd5\u548c\u4f18\u5316\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6c42\u89e3\u6210\u529f\u7387"}}
{"id": "2602.05176", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05176", "abs": "https://arxiv.org/abs/2602.05176", "authors": ["Ziyuan Yang", "Wenxuan Ding", "Shangbin Feng", "Yulia Tsvetkov"], "title": "Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems", "comment": "19 pages, 15 tables, 4 figures", "summary": "Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u91cf\u5316\u4e86\u6076\u610f\u6a21\u578b\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7f\u7528\u5916\u90e8\u76d1\u7763\u5668\u6765\u7f13\u89e3\u8fd9\u4e9b\u5f71\u54cd\u7684\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u534f\u4f5c\uff08\u5982\u8def\u7531\u7cfb\u7edf\u3001\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u3001\u6a21\u578b\u878d\u5408\u7b49\uff09\uff0c\u5728\u53bb\u4e2d\u5fc3\u5316\u8303\u5f0f\u4e0b\u5b58\u5728\u5173\u952e\u7684\u5b89\u5168\u98ce\u9669\uff1a\u5982\u679c\u591aLLM\u7cfb\u7edf\u4e2d\u7684\u67d0\u4e9b\u6a21\u578b\u88ab\u653b\u9677\u6216\u6076\u610f\uff0c\u4f1a\u5e26\u6765\u4ec0\u4e48\u5f71\u54cd\uff1f", "method": "\u9996\u5148\u8bbe\u8ba1\u4e86\u56db\u7c7b\u6076\u610f\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u5b83\u4eec\u63d2\u5165\u56db\u79cd\u6d41\u884c\u7684\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u53d7\u5f71\u54cd\u7684\u7cfb\u7edf\u3002\u7136\u540e\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\uff0c\u4f7f\u7528\u5916\u90e8\u76d1\u7763\u5668\u6765\u76d1\u7763\u6a21\u578b\u534f\u4f5c\uff0c\u901a\u8fc7\u7981\u7528/\u5c4f\u853d\u6076\u610f\u7ec4\u4ef6\u6765\u51cf\u5c11\u5176\u5f71\u54cd\u3002", "result": "\u6076\u610f\u6a21\u578b\u5bf9\u591aLLM\u7cfb\u7edf\u6709\u4e25\u91cd\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u548c\u5b89\u5168\u9886\u57df\uff0c\u6027\u80fd\u5e73\u5747\u4e0b\u964d7.12%\u548c7.94%\u3002\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u5e73\u5747\u80fd\u6062\u590d95.31%\u7684\u521d\u59cb\u6027\u80fd\uff0c\u4f46\u4f7f\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u5b8c\u5168\u62b5\u6297\u6076\u610f\u6a21\u578b\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u591a\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u9762\u4e34\u4e25\u91cd\u7684\u6076\u610f\u6a21\u578b\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u3002\u867d\u7136\u5916\u90e8\u76d1\u7763\u7b56\u7565\u80fd\u663e\u8457\u7f13\u89e3\u5f71\u54cd\uff0c\u4f46\u5b9e\u73b0\u5b8c\u5168\u62b5\u6297\u6076\u610f\u6a21\u578b\u7684\u7cfb\u7edf\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2602.05156", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05156", "abs": "https://arxiv.org/abs/2602.05156", "authors": ["Dong Ho Kang", "Aaron Kim", "Mingyo Seo", "Kazuto Yokoyama", "Tetsuya Narita", "Luis Sentis"], "title": "PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation", "comment": null, "summary": "We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.", "AI": {"tldr": "PLATO Hand\u662f\u4e00\u79cd\u7075\u5de7\u673a\u5668\u4eba\u624b\uff0c\u91c7\u7528\u6df7\u5408\u6307\u5c16\u8bbe\u8ba1\uff08\u521a\u6027\u6307\u7532\u5d4c\u5165\u67d4\u6027\u6307\u8179\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a5\u89e6\u51e0\u4f55\u5b9e\u73b0\u591a\u79cd\u4ea4\u4e92\u6a21\u5f0f\uff0c\u63d0\u5347\u5939\u6301\u7a33\u5b9a\u6027\u3001\u529b\u611f\u77e5\u80fd\u529b\u548c\u8fb9\u7f18\u654f\u611f\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u624b\u5728\u7cbe\u786e\u64cd\u4f5c\u4e2d\u9762\u4e34\u63a5\u89e6\u884c\u4e3a\u63a7\u5236\u56f0\u96be\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u7269\u4f53\u65f6\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u63a5\u89e6\u673a\u5236\u6765\u5b9e\u73b0\u7a33\u5b9a\u5939\u6301\u548c\u7cbe\u7ec6\u64cd\u4f5c\u3002", "method": "\u8bbe\u8ba1\u6df7\u5408\u6307\u5c16\uff08\u521a\u6027\u6307\u7532\u5d4c\u5165\u67d4\u6027\u6307\u8179\uff09\uff0c\u5f00\u53d1\u57fa\u4e8e\u5e94\u53d8\u80fd\u7684\u5f2f\u66f2-\u538b\u5165\u6a21\u578b\u6307\u5bfc\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5f15\u5bfc\u63a5\u89e6\u4fdd\u6301\u5c40\u90e8\u538b\u5165\u540c\u65f6\u6291\u5236\u5168\u5c40\u5f2f\u66f2\uff0c\u7ed3\u5408\u529b-\u8fd0\u52a8\u900f\u660e\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPLATO Hand\u5728\u5939\u6301\u7a33\u5b9a\u6027\u3001\u529b\u53ef\u89c2\u6d4b\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u6210\u529f\u6267\u884c\u7eb8\u5f20\u5206\u79bb\u3001\u5361\u7247\u62fe\u53d6\u3001\u6a59\u5b50\u5265\u76ae\u7b49\u8fb9\u7f18\u654f\u611f\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "\u5c06\u7ed3\u6784\u5316\u63a5\u89e6\u51e0\u4f55\u4e0e\u529b-\u8fd0\u52a8\u900f\u660e\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u4e3a\u7cbe\u786e\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u539f\u5219\u7684\u7269\u7406\u5b9e\u73b0\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u6307\u5c16\u8bbe\u8ba1\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.05182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05182", "abs": "https://arxiv.org/abs/2602.05182", "authors": ["Shangbin Feng", "Kishan Panaganti", "Yulia Tsvetkov", "Wenhao Yu"], "title": "The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems", "comment": "Code at https://github.com/BunsenFeng/moco_distill", "summary": "Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u591a\u6a21\u578b\u534f\u4f5c\u6a21\u5f0f\u84b8\u998f\u5230\u5355\u4e2a\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u534f\u4f5c\u4f18\u52bf\u4e0e\u5355\u6a21\u578b\u6548\u7387\u7684\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u5355-\u591a\u8fdb\u5316\u5faa\u73af\u8ba9\u6a21\u578b\u5728\u534f\u4f5c\u4e0e\u84b8\u998f\u4e2d\u6301\u7eed\u8fdb\u5316", "motivation": "\u591a\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u7ed3\u5408\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u52a0\u8f7d\u591a\u4e2a\u6a21\u578b\u5bfc\u81f4\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u534f\u4f5c\u4f18\u52bf\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u6a21\u5f0f\u84b8\u998f\uff1a\u5c06\u591a\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u7684\u8f93\u51fa\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u84b8\u998f\u5230\u5355\u4e2a\u6a21\u578b\u4e2d\uff1b\u63d0\u51fa\u5355-\u591a\u8fdb\u5316\u5faa\u73af\uff1a\u591a\u4e2a\u6a21\u578b\u534f\u4f5c \u2192 \u5404\u81ea\u4ece\u534f\u4f5c\u8f93\u51fa\u4e2d\u84b8\u998f \u2192 \u84b8\u998f\u6539\u8fdb\u540e\u7684\u6a21\u578b\u518d\u6b21\u534f\u4f5c\uff0c\u5f62\u6210\u96c6\u4f53\u8fdb\u5316\u751f\u6001\u7cfb\u7edf\u3002", "result": "\u57287\u79cd\u534f\u4f5c\u7b56\u7565\u548c15\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a1) \u5355\u4e2a\u6a21\u578b\u5e73\u5747\u63d0\u53478.0%\uff0c\u5438\u6536\u4e86\u534f\u4f5c\u4f18\u52bf\u540c\u65f6\u6210\u672c\u964d\u81f3\u5355\u6a21\u578b\uff1b2) \u534f\u4f5c\u7cfb\u7edf\u4ece\u84b8\u998f\u540e\u66f4\u5f3a\u3001\u66f4\u534f\u540c\u7684\u6a21\u578b\u4e2d\u83b7\u76ca\uff0c\u76f8\u6bd4\u521d\u59cb\u7cfb\u7edf\u5e73\u5747\u63d0\u534714.9%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u8fdb\u5316AI\u65b9\u6cd5\uff0c\u517c\u5bb9\u591a\u79cd\u6a21\u578b/\u534f\u4f5c/\u84b8\u998f\u8bbe\u7f6e\uff0c\u80fd\u89e3\u51b3\u521d\u59cb\u6a21\u578b/\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u6301\u7eed\u81ea\u6211\u8fdb\u5316\u7684\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2602.05198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05198", "abs": "https://arxiv.org/abs/2602.05198", "authors": ["Kalvik Jakkala", "Saurav Agarwal", "Jason O'Kane", "Srinivas Akella"], "title": "Informative Path Planning with Guaranteed Estimation Uncertainty", "comment": "16 pages, 11 figures, preprint", "summary": "Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.\n  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51e0\u4f55\u8986\u76d6\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u8bc1\u9ad8\u65af\u8fc7\u7a0b\u540e\u9a8c\u65b9\u5dee\u4f4e\u4e8e\u6307\u5b9a\u9608\u503c\u7684\u524d\u63d0\u4e0b\uff0c\u89c4\u5212\u6700\u77ed\u8def\u5f84\u8fdb\u884c\u73af\u5883\u76d1\u6d4b\u3002", "motivation": "\u73af\u5883\u76d1\u6d4b\u673a\u5668\u4eba\u9700\u8981\u5728\u8ddd\u79bb\u548c\u80fd\u91cf\u9650\u5236\u4e0b\u91cd\u5efa\u7a7a\u95f4\u573a\u3002\u4f20\u7edfboustrophedon\u65b9\u6cd5\u6709\u51e0\u4f55\u8986\u76d6\u4fdd\u8bc1\u4f46\u4f1a\u8fc7\u5ea6\u91c7\u6837\u53ef\u9884\u6d4b\u533a\u57df\uff0c\u800c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u867d\u80fd\u51cf\u5c11\u8fc7\u5ea6\u91c7\u6837\u4f46\u7f3a\u4e4f\u91cd\u5efa\u8d28\u91cf\u4fdd\u8bc1\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u7a7a\u95f4\u76f8\u5173\u6027\u53c8\u80fd\u63d0\u4f9b\u91cd\u5efa\u8d28\u91cf\u4fdd\u8bc1\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4ece\u5148\u9a8c\u4fe1\u606f\u5b66\u4e60\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\uff1b2) \u5c06\u5b66\u4e60\u5230\u7684\u6838\u51fd\u6570\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u8986\u76d6\u56fe\uff0c\u6307\u793a\u54ea\u4e9b\u4f4d\u7f6e\u7684\u65b9\u5dee\u53ef\u964d\u81f3\u76ee\u6807\u503c\u4ee5\u4e0b\uff1b3) \u89c4\u5212\u63a5\u8fd1\u6700\u77ed\u7684\u8def\u5f84\uff0c\u5176\u7ec4\u5408\u8986\u76d6\u6ee1\u8db3\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\u7ea6\u675f\u3002\u5f15\u5165\u975e\u5e73\u7a33\u6838\u5904\u7406\u5f02\u8d28\u73b0\u8c61\uff0c\u5e76\u9002\u5e94\u6709\u969c\u788d\u7269\u7684\u975e\u51f8\u73af\u5883\u3002", "result": "\u5728\u771f\u5b9e\u5730\u5f62\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u4f7f\u7528\u66f4\u5c11\u7684\u4f20\u611f\u4f4d\u7f6e\u548c\u66f4\u77ed\u7684\u65c5\u884c\u8ddd\u79bb\u5c31\u80fd\u8fbe\u5230\u4e0d\u786e\u5b9a\u6027\u76ee\u6807\u3002\u81ea\u4e3b\u6c34\u9762\u548c\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u73b0\u573a\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u6865\u63a5\u4e86\u4f20\u7edf\u51e0\u4f55\u8986\u76d6\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\uff0c\u5728\u4fdd\u8bc1\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u73af\u5883\u76d1\u6d4b\u8def\u5f84\u89c4\u5212\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.05189", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.05189", "abs": "https://arxiv.org/abs/2602.05189", "authors": ["Hsuan-Yu Chou", "Wajiha Naveed", "Shuyan Zhou", "Xiaowei Yang"], "title": "Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky", "comment": null, "summary": "As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.\n  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.", "AI": {"tldr": "\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5f53\uff0c\u53ef\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u7684\u672c\u5730\u5316\u5185\u5bb9\u5ba1\u6838", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u8bbf\u95ee\u6269\u5927\uff0c\u6709\u5bb3\u5185\u5bb9\u66b4\u9732\u589e\u52a0\uff0c\u9700\u8981\u6709\u6548\u7684\u5ba1\u6838\u673a\u5236\u3002\u867d\u7136\u4e13\u6709LLM\u5728\u793e\u4ea4\u5a92\u4f53\u5ba1\u6838\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f00\u6e90LLM\u7684\u5373\u7528\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u8bc4\u4f30\u4e867\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff084\u4e2a\u4e13\u6709\u6a21\u578b\u548c3\u4e2a\u5f00\u6e90\u6a21\u578b\uff09\uff0c\u4f7f\u7528Bluesky\u7684\u771f\u5b9e\u5e16\u5b50\u3001Bluesky\u5ba1\u6838\u670d\u52a1\u7684\u51b3\u7b56\u4ee5\u53ca\u4e24\u4f4d\u4f5c\u8005\u7684\u6807\u6ce8\u8fdb\u884c\u6d4b\u8bd5\u3002\u5206\u6790\u4e86\u6a21\u578b\u7684\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u4eba\u7c7b\u5ba1\u6838\u5458\u4e0eLLM\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5f00\u6e90LLM\u7684\u654f\u611f\u6027\uff0881%-97%\uff09\u548c\u7279\u5f02\u6027\uff0891%-100%\uff09\u4e0e\u4e13\u6709\u6a21\u578b\uff0872%-98%\u548c93%-99%\uff09\u6709\u76f8\u5f53\u7a0b\u5ea6\u7684\u91cd\u53e0\u3002\u5728\u7c97\u9c81\u5185\u5bb9\u68c0\u6d4b\u4e2d\u7279\u5f02\u6027\u9ad8\u4e8e\u654f\u611f\u6027\uff0c\u4f46\u5728\u4e0d\u5bbd\u5bb9\u548c\u5a01\u80c1\u68c0\u6d4b\u4e2d\u76f8\u53cd\u3002\u53d1\u73b0\u4e86\u4eba\u7c7b\u5ba1\u6838\u5458\u4e0eLLM\u4e4b\u95f4\u7684\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "\u5f00\u6e90LLM\u53ef\u4ee5\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u7684\u5ba1\u6838\uff0c\u4e3a\u8bbe\u8ba1\u5e73\u8861\u793e\u533a\u4ef7\u503c\u89c2\u4e0e\u4e2a\u4eba\u7528\u6237\u504f\u597d\u7684\u5ba1\u6838\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.05233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05233", "abs": "https://arxiv.org/abs/2602.05233", "authors": ["Wenbo Wang", "Fangyun Wei", "QiXiu Li", "Xi Chen", "Yaobo Liang", "Chang Xu", "Jiaolong Yang", "Baining Guo"], "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation", "comment": null, "summary": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86MobileManiBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u4eff\u771f\u4f18\u5148\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u8f68\u8ff9\uff0c\u5305\u542b30\u4e07\u6761\u8f68\u8ff9\uff0c\u652f\u6301VLA\u6a21\u578b\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u684c\u9762\u573a\u666f\u7684\u9065\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53d1\u5c55\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u4eff\u771f\u57fa\u51c6\u6765\u9a8c\u8bc1VLA\u67b6\u6784\u3002", "method": "\u57fa\u4e8eNVIDIA Isaac Sim\u6784\u5efa\u4eff\u771f\u4f18\u5148\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u64cd\u4f5c\u8f68\u8ff9\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u6807\u6ce8\uff08\u8bed\u8a00\u6307\u4ee4\u3001\u591a\u89c6\u89d2RGB-D\u5206\u5272\u56fe\u50cf\u3001\u540c\u6b65\u72b6\u6001\u548c\u52a8\u4f5c\uff09\u3002", "result": "\u521b\u5efa\u4e86MobileManiBench\u57fa\u51c6\uff0c\u5305\u542b2\u4e2a\u79fb\u52a8\u5e73\u53f0\u30012\u4e2a\u540c\u6b65\u76f8\u673a\u3001630\u4e2a\u7269\u4f53\u30015\u79cd\u6280\u80fd\u3001100\u591a\u4e2a\u4efb\u52a1\u3001100\u4e2a\u771f\u5b9e\u573a\u666f\uff0c\u751f\u621030\u4e07\u6761\u8f68\u8ff9\uff0c\u652f\u6301\u673a\u5668\u4eba\u672c\u4f53\u3001\u611f\u77e5\u6a21\u6001\u548c\u653f\u7b56\u67b6\u6784\u7684\u53d7\u63a7\u7814\u7a76\u3002", "conclusion": "\u8be5\u6846\u67b6\u52a0\u901f\u4e86\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u4ee3\u8868\u6027VLA\u6a21\u578b\uff0c\u4e3a\u590d\u6742\u4eff\u771f\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2602.05205", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05205", "abs": "https://arxiv.org/abs/2602.05205", "authors": ["Kenichiro Ando", "Tatsuya Harada"], "title": "Aligning Large Language Model Behavior with Human Citation Preferences", "comment": "Work In Progress", "summary": "Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\\%$ relative to humans) and sentences containing personal names (by $-20.1\\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8LLM\u5f15\u7528\u884c\u4e3a\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u53d1\u73b0\u6a21\u578b\u8fc7\u5ea6\u5f15\u7528\u660e\u786e\u9700\u8981\u5f15\u7528\u7684\u6587\u672c\uff0c\u4f46\u4f4e\u4f30\u6570\u5b57\u548c\u4eba\u540d\u53e5\u5b50\u7684\u5f15\u7528\u9700\u6c42\uff0c\u53ef\u901a\u8fc7DPO\u6821\u51c6\u6539\u5584\u5bf9\u9f50", "motivation": "\u5f53\u524dLLM\u670d\u52a1\u666e\u904d\u6dfb\u52a0\u5f15\u7528\u6765\u589e\u5f3a\u53ef\u4fe1\u5ea6\uff0c\u4f46\u6a21\u578b\u5982\u4f55\u8bc6\u522b\u5f15\u7528\u4ef7\u503c\u4ee5\u53ca\u5982\u4f55\u63a7\u5236\u8fd9\u4e00\u8fc7\u7a0b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u9700\u8981\u4e86\u89e3LLM\u5f53\u524d\u7684\u5f15\u7528\u503e\u5411\u53ca\u5176\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u7a0b\u5ea6", "method": "\u6784\u5efa\u6570\u636e\u96c6\u8868\u5f81\u4eba\u7c7b\u5f15\u7528\u504f\u597d\u4e0eLLM\u884c\u4e3a\u7684\u5173\u7cfb\uff0c\u5c06\u7f51\u7edc\u6587\u672c\u5206\u4e3a8\u79cd\u5f15\u7528\u52a8\u673a\u7c7b\u578b\uff0c\u5bf9\u6240\u6709\u7c7b\u578b\u7ec4\u5408\u8fdb\u884c\u6210\u5bf9\u5f15\u7528\u504f\u597d\u8bc4\u4f30\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u5bf9\u6bd4", "result": "\u4eba\u7c7b\u6700\u5e38\u4e3a\u533b\u5b66\u6587\u672c\u5bfb\u6c42\u5f15\u7528\uff0c\u66f4\u5f3a\u6a21\u578b\u663e\u793a\u76f8\u4f3c\u503e\u5411\uff1b\u5f53\u524d\u6a21\u578b\u6bd4\u4eba\u7c7b\u591a27%\u53ef\u80fd\u4e3a\u660e\u786e\u9700\u8981\u5f15\u7528\u7684\u6587\u672c\u6dfb\u52a0\u5f15\u7528\uff0c\u4f46\u7cfb\u7edf\u6027\u4f4e\u4f30\u6570\u5b57\u53e5\u5b50(-22.6%)\u548c\u542b\u4eba\u540d\u53e5\u5b50(-20.1%)\u7684\u5f15\u7528\u9700\u6c42\uff1bDPO\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u884c\u4e3a\u53ef\u6821\u51c6\u4ee5\u66f4\u597d\u5339\u914d\u4eba\u7c7b\u504f\u597d", "conclusion": "LLM\u5f15\u7528\u884c\u4e3a\u4e0e\u4eba\u7c7b\u504f\u597d\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u6a21\u578b\u8fc7\u5ea6\u5f3a\u8c03\u660e\u786e\u5f15\u7528\u6807\u8bb0\u800c\u4f4e\u4f30\u6570\u5b57\u548c\u4eba\u540d\u5185\u5bb9\u7684\u5f15\u7528\u9700\u6c42\uff0c\u4f46\u53ef\u901a\u8fc7\u504f\u597d\u4f18\u5316\u6821\u51c6\u3002\u7814\u7a76\u4e3a\u66f4\u7ec6\u7c92\u5ea6\u7684LLM\u5f15\u7528\u504f\u597d\u8c03\u67e5\u5960\u5b9a\u57fa\u7840"}}
{"id": "2602.05265", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05265", "abs": "https://arxiv.org/abs/2602.05265", "authors": ["Kalvik Jakkala", "Jason O'Kane"], "title": "Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot", "comment": null, "summary": "Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.", "AI": {"tldr": "\u4f7f\u7528IMU\u3001\u538b\u529b\u4f20\u611f\u5668\u548c\u4e24\u4e2a\u58f0\u7eb3\uff08\u5355\u6ce2\u675f\u548c\u65cb\u8f6c360\u5ea6\uff09\u5b9e\u73b0\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u7ba1\u9053\u5185\u81ea\u4e3b\u5c45\u4e2d\u5bfc\u822a\u7684\u8f7b\u91cf\u7ea7\u65b9\u6848", "motivation": "\u6c34\u4e0b\u7ba1\u9053\u68c0\u6d4b\u9762\u4e34\u51e0\u4f55\u53d7\u9650\u3001\u6d51\u6d4a\u73af\u5883\u548c\u5b9a\u4f4d\u4fe1\u606f\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u590d\u6742\u4f20\u611f\u5668\u9635\u5217\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u57fa\u4e8e\u5355\u6ce2\u675f\u58f0\u7eb3\u5f3a\u5ea6\u6570\u636e\u7684\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u51e0\u4f55\u6a21\u578b\u8ba1\u7b97\u7ba1\u9053\u4e2d\u5fc3\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u52a0\u6743PD\u63a7\u5236\u5668\u4fdd\u6301\u5c45\u4e2d", "result": "\u572846\u5398\u7c73\u76f4\u5f84\u7684\u6df9\u6ca1\u7ba1\u9053\u4e2d\u4f7f\u7528BlueROV2\u6210\u529f\u5b9e\u73b0\u7a33\u5b9a\u5c45\u4e2d\u548c\u5b8c\u6574\u7ba1\u9053\u7a7f\u8d8a\uff0c\u5373\u4f7f\u5728\u73af\u5883\u6d41\u548c\u7ed3\u6784\u53d8\u5f62\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5de5\u4f5c", "conclusion": "\u8f7b\u91cf\u7ea7\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u4f20\u611f\u5904\u7406\u67b6\u6784\u53ef\u5b9e\u73b0\u53ef\u9760\u7684\u6c34\u4e0b\u7ba1\u9053\u5185\u5bfc\u822a\u68c0\u6d4b\uff0c\u63d0\u5347\u4e86\u53d7\u9650\u73af\u5883\u4e2d\u81ea\u4e3b\u6c34\u4e0b\u68c0\u6d4b\u7684\u5b9e\u7528\u6027"}}
{"id": "2602.05211", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.05211", "abs": "https://arxiv.org/abs/2602.05211", "authors": ["Hongye Zhao", "Yi Zhao", "Chengzhi Zhang"], "title": "Quantifying the Knowledge Proximity Between Academic and Industry Research: An Entity and Semantic Perspective", "comment": null, "summary": "The academia and industry are characterized by a reciprocal shaping and dynamic feedback mechanism. Despite distinct institutional logics, they have adapted closely in collaborative publishing and talent mobility, demonstrating tension between institutional divergence and intensive collaboration. Existing studies on their knowledge proximity mainly rely on macro indicators such as the number of collaborative papers or patents, lacking an analysis of knowledge units in the literature. This has led to an insufficient grasp of fine-grained knowledge proximity between industry and academia, potentially undermining collaboration frameworks and resource allocation efficiency. To remedy the limitation, this study quantifies the trajectory of academia-industry co-evolution through fine-grained entities and semantic space. In the entity measurement part, we extract fine-grained knowledge entities via pre-trained models, measure sequence overlaps using cosine similarity, and analyze topological features through complex network analysis. At the semantic level, we employ unsupervised contrastive learning to quantify convergence in semantic spaces by measuring cross-institutional textual similarities. Finally, we use citation distribution patterns to examine correlations between bidirectional knowledge flows and similarity. Analysis reveals that knowledge proximity between academia and industry rises, particularly following technological change. This provides textual evidence of bidirectional adaptation in co-evolution. Additionally, academia's knowledge dominance weakens during technological paradigm shifts. The dataset and code for this paper can be accessed at https://github.com/tinierZhao/Academic-Industrial-associations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u5b9e\u4f53\u548c\u8bed\u4e49\u7a7a\u95f4\u91cf\u5316\u4e86\u5b66\u672f\u754c\u4e0e\u4ea7\u4e1a\u754c\u7684\u5171\u540c\u6f14\u5316\u8f68\u8ff9\uff0c\u53d1\u73b0\u4e24\u8005\u77e5\u8bc6\u90bb\u8fd1\u6027\u4e0a\u5347\uff0c\u7279\u522b\u662f\u5728\u6280\u672f\u53d8\u9769\u540e\uff0c\u4e14\u5b66\u672f\u754c\u5728\u6280\u672f\u8303\u5f0f\u8f6c\u53d8\u671f\u95f4\u7684\u77e5\u8bc6\u4e3b\u5bfc\u5730\u4f4d\u51cf\u5f31\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5408\u4f5c\u8bba\u6587\u6216\u4e13\u5229\u6570\u91cf\u7b49\u5b8f\u89c2\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u6587\u732e\u4e2d\u77e5\u8bc6\u5355\u5143\u7684\u5206\u6790\uff0c\u5bfc\u81f4\u5bf9\u5b66\u672f\u754c\u4e0e\u4ea7\u4e1a\u754c\u4e4b\u95f4\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u90bb\u8fd1\u6027\u7684\u628a\u63e1\u4e0d\u8db3\uff0c\u53ef\u80fd\u5f71\u54cd\u5408\u4f5c\u6846\u67b6\u548c\u8d44\u6e90\u914d\u7f6e\u6548\u7387\u3002", "method": "1) \u5b9e\u4f53\u6d4b\u91cf\uff1a\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u5b9e\u4f53\uff0c\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u5e8f\u5217\u91cd\u53e0\uff0c\u901a\u8fc7\u590d\u6742\u7f51\u7edc\u5206\u6790\u62d3\u6251\u7279\u5f81\uff1b2) \u8bed\u4e49\u5c42\u9762\uff1a\u91c7\u7528\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u91cf\u5316\u8bed\u4e49\u7a7a\u95f4\u6536\u655b\uff0c\u6d4b\u91cf\u8de8\u673a\u6784\u6587\u672c\u76f8\u4f3c\u6027\uff1b3) \u4f7f\u7528\u5f15\u7528\u5206\u5e03\u6a21\u5f0f\u5206\u6790\u53cc\u5411\u77e5\u8bc6\u6d41\u4e0e\u76f8\u4f3c\u6027\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u5206\u6790\u663e\u793a\u5b66\u672f\u754c\u4e0e\u4ea7\u4e1a\u754c\u4e4b\u95f4\u7684\u77e5\u8bc6\u90bb\u8fd1\u6027\u4e0a\u5347\uff0c\u7279\u522b\u662f\u5728\u6280\u672f\u53d8\u9769\u540e\uff0c\u8fd9\u4e3a\u5171\u540c\u6f14\u5316\u4e2d\u7684\u53cc\u5411\u9002\u5e94\u63d0\u4f9b\u4e86\u6587\u672c\u8bc1\u636e\u3002\u6b64\u5916\uff0c\u5728\u6280\u672f\u8303\u5f0f\u8f6c\u53d8\u671f\u95f4\uff0c\u5b66\u672f\u754c\u77e5\u8bc6\u4e3b\u5bfc\u5730\u4f4d\u51cf\u5f31\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u548c\u8bed\u4e49\u7a7a\u95f4\u5206\u6790\uff0c\u91cf\u5316\u4e86\u5b66\u672f\u754c\u4e0e\u4ea7\u4e1a\u754c\u7684\u5171\u540c\u6f14\u5316\u8f68\u8ff9\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u90bb\u8fd1\u6027\u589e\u5f3a\u7684\u8d8b\u52bf\uff0c\u7279\u522b\u662f\u5728\u6280\u672f\u53d8\u9769\u65f6\u671f\uff0c\u4e3a\u7406\u89e3\u4e24\u8005\u4e92\u52a8\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2602.05273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05273", "abs": "https://arxiv.org/abs/2602.05273", "authors": ["Hengxuan Xu", "Fengbo Lan", "Zhixin Zhao", "Shengjie Wang", "Mengqiao Liu", "Jieqian Sun", "Yu Cheng", "Tao Zhang"], "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions", "comment": "14 pages, 10 figures, 8 tables", "summary": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for \"I'm thirsty\") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.", "AI": {"tldr": "AIDE\u662f\u4e00\u4e2a\u53cc\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u63a2\u7d22\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff0c\u8ba9\u673a\u5668\u4eba\u5728\u6a21\u7cca\u6307\u4ee4\u4e0b\u5b9e\u65f6\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7269\u4f53\u5e76\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u6a21\u7cca\u6307\u4ee4\u4e0b\uff08\u5982\"\u6211\u6e34\u4e86\"\uff09\u96be\u4ee5\u5b9e\u65f6\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7269\u4f53\uff08\u676f\u5b50\u6216\u996e\u6599\uff09\uff0c\u4e3b\u8981\u56e0\u4e3a\u63a8\u7406\u6548\u7387\u4f4e\u4e14\u7f3a\u4e4f\u73af\u5883\u4ea4\u4e92\uff0c\u963b\u788d\u5b9e\u65f6\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u3002", "method": "\u63d0\u51faAIDE\u53cc\u6d41\u6846\u67b6\uff1a\u591a\u9636\u6bb5\u63a8\u7406\u4f5c\u4e3a\u51b3\u7b56\u6d41\uff0c\u52a0\u901f\u51b3\u7b56\u4f5c\u4e3a\u6267\u884c\u6d41\uff0c\u96c6\u6210\u4ea4\u4e92\u5f0f\u63a2\u7d22\u4e0e\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u53ef\u4f9b\u6027\u5206\u6790\u548c\u6a21\u7cca\u6307\u4ee4\u89e3\u91ca\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cAIDE\u4efb\u52a1\u89c4\u5212\u6210\u529f\u7387\u8d85\u8fc780%\uff0c\u95ed\u73af\u8fde\u7eed\u6267\u884c\u51c6\u786e\u7387\u8d85\u8fc795%\uff0810Hz\u9891\u7387\uff09\uff0c\u5728\u591a\u6837\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709VLM\u65b9\u6cd5\u3002", "conclusion": "AIDE\u901a\u8fc7\u4ea4\u4e92\u5f0f\u63a2\u7d22\u4e0e\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u7684\u6709\u6548\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u6a21\u7cca\u6307\u4ee4\u4e0b\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7269\u4f53\u548c\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05220", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.05220", "abs": "https://arxiv.org/abs/2602.05220", "authors": ["Jinchuan Tian", "Haoran Wang", "Bo-Hao Su", "Chien-yu Huang", "Qingzheng Wang", "Jiatong Shi", "William Chen", "Xun Gong", "Siddhant Arora", "Chin-Jou Li", "Masao Someki", "Takashi Maekaku", "Yusuke Shinohara", "Jin Sakuma", "Chao-Han Huck Yang", "Shinji Watanabe"], "title": "Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions", "comment": null, "summary": "Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.", "AI": {"tldr": "Bagpiper\u662f\u4e00\u4e2a80\u4ebf\u53c2\u6570\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5c06\u539f\u59cb\u97f3\u9891\u6620\u5c04\u5230\u9ad8\u7ea7\u8ba4\u77e5\u6982\u5ff5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u97f3\u9891\u7406\u89e3\u4e0e\u751f\u6210\u7684\u7edf\u4e00\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u57fa\u7840\u6a21\u578b\u901a\u5e38\u91c7\u7528\u50f5\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u76d1\u7763\uff0c\u53ea\u80fd\u5904\u7406\u97f3\u9891\u7684\u5b64\u7acb\u56e0\u7d20\u800c\u975e\u6574\u4f53\u3002\u4eba\u7c7b\u667a\u80fd\u5219\u80fd\u6574\u4f53\u5904\u7406\u97f3\u9891\uff0c\u65e0\u7f1d\u8fde\u63a5\u7269\u7406\u4fe1\u53f7\u4e0e\u62bd\u8c61\u8ba4\u77e5\u6982\u5ff5\u4ee5\u6267\u884c\u590d\u6742\u4efb\u52a1\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u97f3\u9891\u6a21\u578b\u3002", "method": "1. \u4f7f\u7528\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff08\u5305\u542b\u8f6c\u5f55\u3001\u97f3\u9891\u4e8b\u4ef6\u7b49\u5173\u952e\u8ba4\u77e5\u6982\u5ff5\uff09\u6765\u89e3\u91ca\u7269\u7406\u97f3\u9891\uff1b2. \u57286000\u4ebftoken\u7684\u5927\u89c4\u6a21\u8bed\u6599\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5efa\u7acb\u539f\u59cb\u97f3\u9891\u4e0e\u9ad8\u7ea7\u6982\u5ff5\u7a7a\u95f4\u4e4b\u95f4\u7684\u53cc\u5411\u6620\u5c04\uff1b3. \u5fae\u8c03\u65f6\u91c7\u7528\"\u63cf\u8ff0-\u5904\u7406\"\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6a21\u62df\u4e2d\u95f4\u8ba4\u77e5\u63a8\u7406\u6b65\u9aa4\u6765\u89e3\u51b3\u591a\u6837\u5316\u4efb\u52a1\u3002", "result": "1. \u5728\u97f3\u9891\u7406\u89e3\u65b9\u9762\uff0c\u5728MMAU\u548cAIRBench\u57fa\u51c6\u4e0a\u8d85\u8d8aQwen-2.5-Omni\uff1b2. \u5728\u751f\u6210\u8d28\u91cf\u65b9\u9762\uff0c\u8d85\u8d8aCosyVoice3\u548cTangoFlux\uff0c\u80fd\u591f\u5408\u6210\u8bed\u97f3\u3001\u97f3\u4e50\u548c\u97f3\u6548\u7684\u4efb\u610f\u7ec4\u5408\uff1b3. \u5b9e\u73b0\u4e86\u901a\u7528\u97f3\u9891\u7684\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u3002", "conclusion": "Bagpiper\u662f\u9996\u6279\u5b9e\u73b0\u901a\u7528\u97f3\u9891\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u7684\u5de5\u4f5c\u4e4b\u4e00\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5efa\u7acb\u4e86\u97f3\u9891\u7269\u7406\u4fe1\u53f7\u4e0e\u62bd\u8c61\u6982\u5ff5\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u97f3\u9891AI\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.05310", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05310", "abs": "https://arxiv.org/abs/2602.05310", "authors": ["Jipeng Kong", "Xinzhe Liu", "Yuhang Lin", "Jinrui Han", "S\u00f6ren Schwertfeger", "Chenjia Bai", "Xuelong Li"], "title": "Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework", "comment": "13 pages, 9 figures, conference", "summary": "Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.", "AI": {"tldr": "PAiD\u662f\u4e00\u4e2a\u6e10\u8fdb\u5f0f\u67b6\u6784\uff0c\u5c06\u4eba\u5f62\u673a\u5668\u4eba\u8db3\u7403\u6280\u80fd\u5b66\u4e60\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u901a\u8fc7\u4eba\u4f53\u8fd0\u52a8\u8ddf\u8e2a\u83b7\u53d6\u8fd0\u52a8\u6280\u80fd\u3001\u8f7b\u91cf\u7ea7\u611f\u77e5-\u52a8\u4f5c\u96c6\u6210\u5b9e\u73b0\u4f4d\u7f6e\u6cdb\u5316\u3001\u7269\u7406\u611f\u77e5\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\uff0c\u5728Unitree G1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u4eba\u5f62\u8e22\u7403\u80fd\u529b\u3002", "motivation": "\u8db3\u7403\u5bf9\u4eba\u5f62\u673a\u5668\u4eba\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u7d27\u5bc6\u96c6\u6210\u7684\u611f\u77e5-\u52a8\u4f5c\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6a21\u5757\u5316\u7ba1\u9053\u4e2d\u7684\u6a21\u5757\u95f4\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u6216\u7aef\u5230\u7aef\u6846\u67b6\u4e2d\u7684\u8bad\u7ec3\u76ee\u6807\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u611f\u77e5-\u52a8\u4f5c\u96c6\u6210\u51b3\u7b56\uff08PAiD\uff09\u67b6\u6784\uff0c\u5c06\u8db3\u7403\u6280\u80fd\u83b7\u53d6\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u901a\u8fc7\u4eba\u4f53\u8fd0\u52a8\u8ddf\u8e2a\u83b7\u53d6\u8fd0\u52a8\u6280\u80fd\uff1b2\uff09\u8f7b\u91cf\u7ea7\u611f\u77e5-\u52a8\u4f5c\u96c6\u6210\u5b9e\u73b0\u4f4d\u7f6e\u6cdb\u5316\uff1b3\uff09\u7269\u7406\u611f\u77e5\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002", "result": "\u5728Unitree G1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u4eba\u5f62\u8e22\u7403\uff0c\u5728\u9759\u6001\u6216\u6eda\u52a8\u7403\u3001\u4e0d\u540c\u4f4d\u7f6e\u548c\u5e72\u6270\u7b49\u591a\u6837\u5316\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5728\u5ba4\u5185\u5916\u573a\u666f\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u8fd9\u79cd\u5206\u800c\u6cbb\u4e4b\u7684\u7b56\u7565\u63a8\u8fdb\u4e86\u7a33\u5065\u7684\u4eba\u5f62\u8db3\u7403\u80fd\u529b\uff0c\u5e76\u4e3a\u590d\u6742\u5177\u8eab\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2602.05235", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05235", "abs": "https://arxiv.org/abs/2602.05235", "authors": ["Zhilin Liang", "Yuxiang Wang", "Zimu Zhou", "Hainan Zhang", "Boyi Liu", "Yongxin Tong"], "title": "FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters", "comment": "11 pages", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.", "AI": {"tldr": "FedMosaic\uff1a\u9996\u4e2a\u57fa\u4e8e\u53c2\u6570\u5316\u9002\u914d\u5668\u7684\u8054\u90a6RAG\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u6587\u6863\u5230\u591a\u6587\u6863\u9002\u914d\u5668\u5e76\u9009\u62e9\u6027\u805a\u5408\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u51c6\u786e\u6027\u5e76\u5927\u5e45\u964d\u4f4e\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edfRAG\u5047\u8bbe\u96c6\u4e2d\u5f0f\u77e5\u8bc6\u5e93\uff0c\u4f46\u5728\u9690\u79c1\u654f\u611f\u9886\u57df\u77e5\u8bc6\u88ab\u9694\u79bb\u5b58\u50a8\uff0c\u65e0\u6cd5\u5171\u4eab\u539f\u59cb\u6587\u6863\u3002\u8fd9\u4fc3\u4f7f\u4e86\u8054\u90a6RAG\u7684\u9700\u6c42\uff0c\u5176\u4e2d\u4e2d\u592eLLM\u670d\u52a1\u5668\u4e0e\u5206\u5e03\u5f0f\u77e5\u8bc6\u5e93\u534f\u4f5c\u800c\u4e0d\u5171\u4eab\u539f\u59cb\u6587\u6863\u3002", "method": "\u91c7\u7528\u53c2\u6570\u5316RAG\u65b9\u6cd5\uff0c\u5c06\u6587\u6863\u7f16\u7801\u4e3a\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u3002\u63d0\u51faFedMosaic\u6846\u67b6\uff1a1\uff09\u5c06\u8bed\u4e49\u76f8\u5173\u6587\u6863\u805a\u7c7b\u5230\u591a\u6587\u6863\u9002\u914d\u5668\u4e2d\uff0c\u4f7f\u7528\u6587\u6863\u7279\u5b9a\u63a9\u7801\u51cf\u5c11\u5f00\u9500\uff1b2\uff09\u9009\u62e9\u6027\u805a\u5408\u9002\u914d\u5668\uff0c\u53ea\u5408\u5e76\u76f8\u5173\u5bf9\u9f50\u4e14\u65e0\u51b2\u7a81\u7684\u9002\u914d\u5668\u3002", "result": "\u5728\u56db\u4e2a\u7c7b\u522b\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad810.9%\uff0c\u5b58\u50a8\u6210\u672c\u964d\u4f4e78.8%-86.3%\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e91.4%\uff0c\u4e14\u4ece\u4e0d\u5171\u4eab\u539f\u59cb\u6587\u6863\u3002", "conclusion": "FedMosaic\u662f\u9996\u4e2a\u57fa\u4e8e\u53c2\u6570\u5316\u9002\u914d\u5668\u7684\u8054\u90a6RAG\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6RAG\u4e2d\u7684\u5b58\u50a8\u901a\u4fe1\u5f00\u9500\u548c\u7834\u574f\u6027\u805a\u5408\u95ee\u9898\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2602.05325", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05325", "abs": "https://arxiv.org/abs/2602.05325", "authors": ["Jiacheng Fan", "Zhiyue Zhao", "Yiqian Zhang", "Chao Chen", "Peide Wang", "Hengdi Zhang", "Zhengxue Cheng"], "title": "RoboPaint: From Human Demonstration to Any Robot and Any View", "comment": "17 pages", "summary": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently \"painted\" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.", "AI": {"tldr": "\u63d0\u51faReal-Sim-Real\u6570\u636e\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u4eba\u7c7b\u6f14\u793a\u751f\u6210\u673a\u5668\u4eba\u53ef\u6267\u884c\u6570\u636e\uff0c\u65e0\u9700\u76f4\u63a5\u9065\u64cd\u4f5c\uff0c\u89e3\u51b3VLA\u6a21\u578b\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\u6570\u636e\u83b7\u53d6\u74f6\u9888\u95ee\u9898", "motivation": "\u83b7\u53d6\u5927\u89c4\u6a21\u3001\u9ad8\u4fdd\u771f\u7684\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u662f\u6269\u5c55\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u5173\u952e\u74f6\u9888\u3002\u4f20\u7edf\u9065\u64cd\u4f5c\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u6269\u5c55\u3002", "method": "1) \u5efa\u7acb\u6807\u51c6\u5316\u6570\u636e\u91c7\u96c6\u5ba4\u6355\u83b7\u591a\u6a21\u6001\u4eba\u7c7b\u6f14\u793a\uff1b2) \u63d0\u51fa\u89e6\u89c9\u611f\u77e5\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u548c\u529b\u5f15\u5bfc\u4f18\u5316\u5c06\u4eba\u624b\u72b6\u6001\u6620\u5c04\u5230\u673a\u5668\u4eba\u7075\u5de7\u624b\u72b6\u6001\uff1b3) \u5728\u903c\u771f\u4eff\u771f\u73af\u5883\u4e2d\u6e32\u67d3\u91cd\u5b9a\u5411\u8f68\u8ff9\u751f\u6210\u8bad\u7ec3\u6570\u636e", "result": "1) \u91cd\u5b9a\u5411\u7684\u7075\u5de7\u624b\u8f68\u8ff9\u572810\u4e2a\u591a\u6837\u5316\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u523084%\u6210\u529f\u7387\uff1b2) \u4ec5\u4f7f\u7528\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684VLA\u7b56\u7565\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u4e2d\u8fbe\u523080%\u5e73\u5747\u6210\u529f\u7387", "conclusion": "\u673a\u5668\u4eba\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u901a\u8fc7\u4eba\u7c7b\u6f14\u793a\"\u7ed8\u5236\"\u751f\u6210\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u9065\u64cd\u4f5c\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u590d\u6742\u7075\u5de7\u64cd\u4f5c\u4e2d\u6027\u80fd\u635f\u5931\u6700\u5c0f"}}
{"id": "2602.05252", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05252", "abs": "https://arxiv.org/abs/2602.05252", "authors": ["Guangwei Zhang", "Jianing Zhu", "Cheng Qian", "Neil Gong", "Rada Mihalcea", "Zhaozhuo Xu", "Jingrui He", "Jiaqi Ma", "Yun Huang", "Chaowei Xiao", "Bo Li", "Ahmed Abbasi", "Dongwon Lee", "Heng Ji", "Denghui Zhang"], "title": "Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks", "comment": null, "summary": "We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.", "AI": {"tldr": "\u9996\u4e2a\u4ea4\u4e92\u5f0f\u6cd5\u8bc1\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u5206\u6790\u548c\u53ef\u89c6\u5316LLM\u8f93\u51fa\u4e2d\u7684\u6f5c\u5728\u7248\u6743\u98ce\u9669\uff0c\u5c06\u4fb5\u6743\u68c0\u6d4b\u89c6\u4e3a\u8bc1\u636e\u53d1\u73b0\u8fc7\u7a0b\u800c\u975e\u9759\u6001\u5206\u7c7b\u4efb\u52a1", "motivation": "\u7531\u4e8e\u7248\u6743\u6cd5\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLM\u8f93\u51fa\u7684\u7248\u6743\u98ce\u9669\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u9759\u6001\u5206\u7c7b\uff0c\u65e0\u6cd5\u5168\u9762\u5904\u7406\u590d\u6742\u7684\u4fb5\u6743\u5224\u5b9a\u95ee\u9898", "method": "\u6574\u5408\u591a\u79cd\u68c0\u6d4b\u8303\u5f0f\uff1a\u5185\u5bb9\u53ec\u56de\u6d4b\u8bd5\u3001\u6539\u5199\u7ea7\u76f8\u4f3c\u6027\u5206\u6790\u3001\u8bf4\u670d\u6027\u8d8a\u72f1\u63a2\u6d4b\u548c\u53cd\u5b66\u4e60\u9a8c\u8bc1\uff0c\u5728\u7edf\u4e00\u53ef\u6269\u5c55\u6846\u67b6\u4e2d\u901a\u8fc7\u4ea4\u4e92\u5f0f\u63d0\u793a\u3001\u54cd\u5e94\u6536\u96c6\u548c\u8fed\u4ee3\u5de5\u4f5c\u6d41\u7a0b\u8fdb\u884c\u7cfb\u7edf\u5ba1\u8ba1", "result": "\u5f00\u53d1\u4e86\u9996\u4e2a\u4ea4\u4e92\u5f0f\u6cd5\u8bc1\u7cfb\u7edf\uff0c\u80fd\u591f\u7cfb\u7edf\u5ba1\u8ba1\u9010\u5b57\u8bb0\u5fc6\u548c\u6539\u5199\u7ea7\u6cc4\u9732\uff0c\u652f\u6301\u5bf9\u9ed1\u76d2\u8bbf\u95ee\u7684LLM\u8fdb\u884c\u8d1f\u8d23\u4efb\u7684\u90e8\u7f72\u548c\u900f\u660e\u8bc4\u4f30", "conclusion": "Copyright Detective\u4e3aLLM\u7248\u6743\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u4ea4\u4e92\u5f0f\u6cd5\u8bc1\u65b9\u6cd5\uff0c\u5c06\u4fb5\u6743\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8bc1\u636e\u53d1\u73b0\u8fc7\u7a0b\uff0c\u652f\u6301\u66f4\u5168\u9762\u7684\u98ce\u9669\u5206\u6790\u548c\u900f\u660e\u8bc4\u4f30"}}
{"id": "2602.05441", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05441", "abs": "https://arxiv.org/abs/2602.05441", "authors": ["Dean Fortier", "Timothy Adamson", "Tess Hellebrekers", "Teresa LaScala", "Kofi Ennin", "Michael Murray", "Andrey Kolobov", "Galen Mullins"], "title": "Benchmarking Affordance Generalization with BusyBox", "comment": null, "summary": "Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.\n  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $\u03c0_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.", "AI": {"tldr": "BusyBox\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30VLA\u6a21\u578b\u7269\u7406\u64cd\u4f5c\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b6\u4e2a\u53ef\u4e92\u6362\u6a21\u5757\uff0c\u6311\u6218\u73b0\u6709VLA\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7269\u7406\u64cd\u4f5c\u6cdb\u5316\uff08affordance generalization\uff09\u7684\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5373\u64cd\u7eb5\u5177\u6709\u719f\u6089\u7269\u7406\u7279\u5f81\u7684\u65b0\u7269\u4f53\u7684\u80fd\u529b", "method": "\u8bbe\u8ba1BusyBox\u7269\u7406\u57fa\u51c6\u5e73\u53f0\uff0c\u5305\u542b6\u4e2a\u53ef\u4e92\u6362\u548c\u65cb\u8f6c\u7684\u6a21\u5757\uff08\u5f00\u5173\u3001\u6ed1\u5757\u3001\u7535\u7ebf\u3001\u6309\u94ae\u3001\u663e\u793a\u5c4f\u3001\u65cb\u94ae\uff09\uff0c\u521b\u5efa\u591a\u79cd\u89c6\u89c9\u5916\u89c2\u4e0d\u540c\u4f46\u64cd\u4f5c\u65b9\u5f0f\u76f8\u540c\u7684\u53d8\u4f53\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30VLA\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "result": "BusyBox\u53d8\u4f53\u95f4\u7684\u6cdb\u5316\u5bf9\u73b0\u6709\u5f3a\u5927VLA\u6a21\u578b\uff08\u5982\u03c0\u2080.\u2085\u548cGR00T-N1.6\uff09\u6781\u5177\u6311\u6218\u6027\uff0c\u8bc1\u660e\u4e86\u7269\u7406\u64cd\u4f5c\u6cdb\u5316\u8bc4\u4f30\u7684\u5fc5\u8981\u6027", "conclusion": "BusyBox\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u6613\u4e8e\u6784\u5efa\u7684\u7269\u7406\u57fa\u51c6\u5e73\u53f0\uff0c\u4fc3\u8fdbVLA\u6a21\u578b\u7269\u7406\u64cd\u4f5c\u6cdb\u5316\u80fd\u529b\u7684\u7814\u7a76\u548c\u8bc4\u4f30\uff0c\u53d1\u5e03\u4e86\u5b8c\u6574\u7684CAD\u6587\u4ef6\u3001\u6750\u6599\u6e05\u5355\u548c\u6f14\u793a\u6570\u636e\u96c6"}}
{"id": "2602.05258", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05258", "abs": "https://arxiv.org/abs/2602.05258", "authors": ["Haoran Li", "Sucheng Ren", "Alan Yuille", "Feng Wang"], "title": "CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs", "comment": null, "summary": "Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.", "AI": {"tldr": "CoPE\u901a\u8fc7\u8f6f\u622a\u65adRoPE\u7684\u4f4e\u9891\u5206\u91cf\uff0c\u7edf\u4e00\u4e86OOD\u7f13\u89e3\u548c\u8bed\u4e49\u5efa\u6a21\u4e24\u4e2a\u76ee\u6807\uff0c\u5728\u957f\u5ea6\u6cdb\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u652f\u6301256k\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709RoPE\u9002\u5e94\u957f\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u4e24\u7c7b\uff1aOOD\u7f13\u89e3\uff08\u8c03\u6574\u9891\u7387\u4ee5\u9002\u5e94\u672a\u89c1\u4f4d\u7f6e\uff09\u548c\u8bed\u4e49\u5efa\u6a21\uff08\u6ce8\u610f\u529b\u5206\u6570\u5e94\u4f18\u5148\u8bed\u4e49\u76f8\u4f3ctoken\uff09\u3002\u672c\u6587\u65e8\u5728\u7edf\u4e00\u8fd9\u4e24\u4e2a\u770b\u4f3c\u4e0d\u540c\u7684\u76ee\u6807\u3002", "method": "\u63d0\u51faCoPE\uff08\u8f6f\u622a\u65adRoPE\u4f4e\u9891\u5206\u91cf\uff09\uff0c\u901a\u8fc7\u8f6f\u622a\u65ad\u7b56\u7565\u6d88\u9664OOD\u5f02\u5e38\u503c\u3001\u7cbe\u70bc\u8bed\u4e49\u4fe1\u53f7\uff0c\u5e76\u9632\u6b62\u786c\u622a\u65ad\u5f15\u8d77\u7684\u9891\u8c31\u6cc4\u6f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7b80\u5355\u5e94\u7528CoPE\u8f6f\u622a\u65ad\u7b56\u7565\u80fd\u5728\u957f\u5ea6\u6cdb\u5316\u4efb\u52a1\u4e0a\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u53ef\u6269\u5c55\u5230256k\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u5e76\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002", "conclusion": "CoPE\u901a\u8fc7\u8f6f\u622a\u65adRoPE\u4f4e\u9891\u5206\u91cf\u7684\u7b80\u7ea6\u5e72\u9884\uff0c\u7edf\u4e00\u4e86OOD\u7f13\u89e3\u548c\u8bed\u4e49\u5efa\u6a21\u76ee\u6807\uff0c\u4e3a\u957f\u5ea6\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2602.05456", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05456", "abs": "https://arxiv.org/abs/2602.05456", "authors": ["Maksym Figat", "Ryan M. Mackey", "Michel D. Ingham"], "title": "Ontology-Driven Robotic Specification Synthesis", "comment": "8 pages, 9 figures, 3 tables, journal", "summary": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.", "AI": {"tldr": "RSTM2\u65b9\u6cd5\u901a\u8fc7\u672c\u4f53\u9a71\u52a8\u3001\u5206\u5c42\u5efa\u6a21\uff0c\u4f7f\u7528\u968f\u673a\u65f6\u95f4Petri\u7f51\u4e0e\u8d44\u6e90\uff0c\u652f\u6301\u8499\u7279\u5361\u6d1b\u4eff\u771f\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u7cfb\u7edf\u4ece\u9ad8\u5c42\u76ee\u6807\u5230\u5f62\u5f0f\u5316\u53ef\u6267\u884c\u89c4\u8303\u7684\u8f6c\u6362\u3002", "motivation": "\u89e3\u51b3\u5b89\u5168\u5173\u952e\u548c\u4efb\u52a1\u5173\u952e\u673a\u5668\u4eba\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u9ad8\u5c42\u76ee\u6807\u4e0e\u5f62\u5f0f\u5316\u53ef\u6267\u884c\u89c4\u8303\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u652f\u6301\u590d\u6742\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u67b6\u6784\u6743\u8861\u3001\u8d44\u6e90\u5206\u914d\u548c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6027\u80fd\u5206\u6790\u3002", "method": "\u63d0\u51faRSTM2\uff08\u673a\u5668\u4eba\u7cfb\u7edf\u4efb\u52a1\u5230\u6a21\u578b\u8f6c\u6362\u65b9\u6cd5\uff09\uff0c\u91c7\u7528\u672c\u4f53\u9a71\u52a8\u3001\u5206\u5c42\u65b9\u6cd5\uff0c\u4f7f\u7528\u5e26\u8d44\u6e90\u7684\u968f\u673a\u65f6\u95f4Petri\u7f51\uff0c\u652f\u6301\u4efb\u52a1\u3001\u7cfb\u7edf\u548c\u5b50\u7cfb\u7edf\u7ea7\u522b\u7684\u8499\u7279\u5361\u6d1b\u4eff\u771f\u3002", "result": "\u901a\u8fc7\u5047\u8bbe\u6848\u4f8b\u7814\u7a76\u5c55\u793aRSTM2\u652f\u6301\u67b6\u6784\u6743\u8861\u3001\u8d44\u6e90\u5206\u914d\u548c\u4e0d\u786e\u5b9a\u6027\u6027\u80fd\u5206\u6790\uff0c\u672c\u4f53\u6982\u5ff5\u652f\u6301\u53ef\u89e3\u91caAI\u52a9\u624b\uff0c\u5b9e\u73b0\u5168\u81ea\u4e3b\u89c4\u8303\u5408\u6210\uff0c\u7279\u522b\u9002\u7528\u4e8eNASA CADRE\u7b49\u590d\u6742\u591a\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "RSTM2\u65b9\u6cd5\u4e3a\u590d\u6742\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u5de5\u7a0b\u6846\u67b6\uff0c\u652f\u6301\u5206\u6563\u3001\u8d44\u6e90\u611f\u77e5\u548c\u81ea\u9002\u5e94\u81ea\u4e3b\u7cfb\u7edf\u7684\u5f00\u53d1\uff0c\u5bf9\u672a\u6765\u81ea\u4e3b\u7cfb\u7edf\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.05261", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05261", "abs": "https://arxiv.org/abs/2602.05261", "authors": ["Fanfan Liu", "Youyang Yin", "Peng Shi", "Siqi Yang", "Zhixiong Zeng", "Haibo Qiu"], "title": "Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR", "comment": null, "summary": "Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86RLVR\u7b97\u6cd5\u4e2d\u54cd\u5e94\u957f\u5ea6\u53d8\u5316\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u89e3\u51b3\u957f\u5ea6\u504f\u5dee\u7684LUSPO\u7b97\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "RLVR\u8bad\u7ec3\u4e2d\u54cd\u5e94\u957f\u5ea6\u7684\u589e\u52a0\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4f46\u4e0d\u540cRLVR\u7b97\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u54cd\u5e94\u957f\u5ea6\u7684\u53d8\u5316\u6a21\u5f0f\u5dee\u5f02\u663e\u8457\uff0c\u9700\u8981\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u8fd9\u4e9b\u53d8\u5316\u5e76\u63d0\u4f9b\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u6df1\u5165\u5206\u6790\u4e3b\u6d41RLVR\u7b97\u6cd5\u7684\u7ec4\u4ef6\uff0c\u7406\u8bba\u5206\u6790\u5f71\u54cd\u54cd\u5e94\u957f\u5ea6\u7684\u56e0\u7d20\uff0c\u63d0\u51faLength-Unbiased Sequence Policy Optimization (LUSPO)\u7b97\u6cd5\uff0c\u4fee\u6b63GSPO\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\uff0c\u4f7f\u5176\u635f\u5931\u51fd\u6570\u5bf9\u54cd\u5e94\u957f\u5ea6\u65e0\u504f\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u6a21\u6001\u63a8\u7406\u573a\u666f\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLUSPO\u59cb\u7ec8\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u76f8\u6bd4GRPO\u548cGSPO\u7b49\u73b0\u6709\u65b9\u6cd5\uff0cLUSPO\u4ee3\u8868\u4e86\u65b0\u9896\u7684SOTA\u4f18\u5316\u7b56\u7565\u3002", "conclusion": "LUSPO\u901a\u8fc7\u89e3\u51b3RLVR\u7b97\u6cd5\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u6709\u6548\u9632\u6b62\u4e86\u54cd\u5e94\u957f\u5ea6\u5d29\u6e83\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e3aRLVR\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2602.05468", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05468", "abs": "https://arxiv.org/abs/2602.05468", "authors": ["Pranav Ponnivalavan", "Satoshi Funabashi", "Alexander Schmitz", "Tetsuya Ogata", "Shigeki Sugano"], "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation", "comment": "8 pages, 8 figures, 8 tables, ICRA2026 accepted", "summary": "Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.", "AI": {"tldr": "\u63d0\u51faTaSA\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6df1\u5ea6\u9884\u6d4b\u5b66\u4e60\uff0c\u8ba9\u673a\u5668\u4eba\u50cf\u4eba\u7c7b\u4e00\u6837\u533a\u5206\u81ea\u63a5\u89e6\u548c\u5916\u90e8\u63a5\u89e6\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u5b9e\u73b0\u7075\u5de7\u64cd\u4f5c", "motivation": "\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u9700\u8981\u540c\u65f6\u63a5\u89e6\u7269\u4f53\u548c\u591a\u4e2a\u624b\u6307\uff0c\u4f46\u96be\u4ee5\u533a\u5206\u81ea\u63a5\u89e6\u548c\u5916\u90e8\u63a5\u89e6\u7684\u89e6\u89c9\u4fe1\u53f7\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u81ea\u63a5\u89e6\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4eba\u7c7b\u901a\u8fc7\u611f\u89c9\u8870\u51cf\u673a\u5236\u533a\u5206\u53ef\u9884\u6d4b\u7684\u81ea\u63a5\u89e6\u4fe1\u53f7\uff0c\u4f7f\u65b0\u9896\u7269\u4f53\u523a\u6fc0\u51f8\u663e\u4e3a\u76f8\u5173\u4fe1\u53f7", "method": "\u63d0\u51faTaSA\u4e24\u9636\u6bb5\u6df1\u5ea6\u9884\u6d4b\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u81ea\u63a5\u89e6\u52a8\u529b\u5b66\uff0c\u5efa\u6a21\u673a\u5668\u4eba\u81ea\u8eab\u52a8\u4f5c\u5982\u4f55\u4ea7\u751f\u89e6\u89c9\u53cd\u9988\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u5b66\u4e60\u5230\u7684\u6a21\u578b\u878d\u5165\u8fd0\u52a8\u5b66\u4e60\u9636\u6bb5\uff0c\u5f3a\u8c03\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u7269\u4f53\u63a5\u89e6\u4fe1\u53f7", "result": "\u5728\u9700\u8981\u7cbe\u7ec6\u89e6\u89c9\u8fa8\u522b\u7684\u63d2\u5165\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff1a\u5c06\u94c5\u7b14\u82af\u63d2\u5165\u81ea\u52a8\u94c5\u7b14\u3001\u5c06\u786c\u5e01\u63d2\u5165\u6295\u5e01\u53e3\u3001\u5c06\u56de\u5f62\u9488\u56fa\u5b9a\u5728\u7eb8\u4e0a\u3002\u6240\u6709\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528TaSA\u8bad\u7ec3\u7684\u7b56\u7565\u90fd\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387", "conclusion": "\u57fa\u4e8e\u611f\u89c9\u8870\u51cf\u7684\u7ed3\u6784\u5316\u89e6\u89c9\u611f\u77e5\u5bf9\u4e8e\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002TaSA\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7684\u81ea\u63a5\u89e6\u611f\u77e5\u673a\u5236\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u533a\u5206\u81ea\u63a5\u89e6\u548c\u5916\u90e8\u63a5\u89e6\u4fe1\u53f7\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u7075\u5de7\u64cd\u4f5c"}}
{"id": "2602.05289", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.05289", "abs": "https://arxiv.org/abs/2602.05289", "authors": ["Jingru Fan", "Dewen Liu", "Yufan Dang", "Huatao Li", "Yuheng Wang", "Wei Liu", "Feiyu Duan", "Xuanwen Ding", "Shu Yao", "Lin Wu", "Ruijie Shi", "Wai-Shing Leung", "Yuan Cheng", "Zhongyu Wei", "Cheng Yang", "Chen Qian", "Zhiyuan Liu", "Maosong Sun"], "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\u0393$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\u0393$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u4ece\u7ecf\u9a8c\u8bd5\u9519\u8f6c\u5411\u8bbe\u8ba1\u79d1\u5b66\uff0c\u5efa\u7acb\u534f\u4f5c\u589e\u76ca\u6307\u6807\u0393\u6765\u533a\u5206\u771f\u5b9e\u534f\u4f5c\u6536\u76ca\u4e0e\u8d44\u6e90\u79ef\u7d2f\uff0c\u6784\u5efa\u7cfb\u7edf\u5316\u7684MAS\u56e0\u5b50\u5e93\uff0c\u5b9e\u73b0\u4ece\u76f2\u76ee\u5b9e\u9a8c\u5230\u4e25\u8c28\u79d1\u5b66\u7684\u8f6c\u53d8\u3002", "motivation": "\u5c3d\u7ba1LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u5f00\u653e\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f53\u524d\u7814\u7a76\u4ecd\u4f9d\u8d56\u7ecf\u9a8c\u8bd5\u9519\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u79d1\u5b66\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u4f18\u5316\u3002\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u5f52\u56e0\u6a21\u7cca\u6027\uff1a\u7f3a\u4e4f\u7ed3\u6784\u5316\u56e0\u5b50\u5206\u7c7b\u5bfc\u81f4\u7814\u7a76\u8005\u53ea\u80fd\u8fdb\u884c\u65e0\u6307\u5bfc\u8c03\u6574\uff1b\u7f3a\u4e4f\u7edf\u4e00\u6307\u6807\u65e0\u6cd5\u533a\u5206\u771f\u5b9e\u534f\u4f5c\u589e\u76ca\u4e0e\u5355\u7eaf\u8d44\u6e90\u79ef\u7d2f\u3002", "method": "1) \u5efa\u7acb\u534f\u4f5c\u589e\u76ca\u6307\u6807\u0393\u4f5c\u4e3a\u79d1\u5b66\u6807\u51c6\uff0c\u5206\u79bb\u5185\u5728\u589e\u76ca\u4e0e\u9884\u7b97\u589e\u52a0\uff1b2) \u63d0\u51fa\u56e0\u5b50\u5f52\u56e0\u8303\u5f0f\uff0c\u7cfb\u7edf\u8bc6\u522b\u9a71\u52a8\u534f\u4f5c\u7684\u56e0\u7d20\uff1b3) \u6784\u5efa\u7cfb\u7edf\u5316\u7684MAS\u56e0\u5b50\u5e93\uff0c\u5c06\u8bbe\u8ba1\u7a7a\u95f4\u7ed3\u6784\u5316\u5206\u4e3a\u63a7\u5236\u7ea7\u9884\u8bbe\u548c\u4fe1\u606f\u7ea7\u52a8\u6001\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u4ece\u76f2\u76ee\u5b9e\u9a8c\u8f6c\u5411\u4e25\u8c28\u79d1\u5b66\uff0c\u4e3a\u96c6\u4f53AI\u79d1\u5b66\u5960\u5b9a\u57fa\u7840\u3002\u901a\u8fc7\u0393\u6307\u6807\u548c\u56e0\u5b50\u5e93\uff0c\u5b9e\u73b0\u4e86\u5bf9\u534f\u4f5c\u6548\u679c\u7684\u7cfb\u7edf\u5316\u5206\u6790\u548c\u4f18\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4fc3\u8fdb\u4e86\u4ece\u7ecf\u9a8c\u8bd5\u9519\u5230\u8bbe\u8ba1\u79d1\u5b66\u7684\u8f6c\u53d8\uff0c\u4e3a\u5b9e\u73b0\u771f\u6b63\u7684\u96c6\u4f53AI\u79d1\u5b66\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7cfb\u7edf\u5316\u7814\u7a76\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u8bba\u652f\u6301\u3002"}}
{"id": "2602.05513", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05513", "abs": "https://arxiv.org/abs/2602.05513", "authors": ["Xukun Li", "Yu Sun", "Lei Zhang", "Bosheng Huang", "Yibo Peng", "Yuan Meng", "Haojun Jiang", "Shaoxuan Xie", "Guacai Yao", "Alois Knoll", "Zhenshan Bing", "Xinlong Wang", "Zhenguo Sun"], "title": "DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter", "comment": "17 pages, 8 figures", "summary": "Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.", "AI": {"tldr": "DECO\u662f\u4e00\u4e2a\u57fa\u4e8eDiT\u7684\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u591a\u6a21\u6001\u6761\u4ef6\u6765\u5b9e\u73b0\u7075\u5de7\u64cd\u4f5c\uff0c\u5e76\u9644\u5e26\u5305\u542b50\u5c0f\u65f6\u6570\u636e\u7684DECO-50\u6570\u636e\u96c6\u3002", "motivation": "\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff08\u89c6\u89c9\u3001\u89e6\u89c9\u3001\u672c\u4f53\u611f\u89c9\uff09\u7684\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\uff0c\u540c\u65f6\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "method": "\u4f7f\u7528DiT-based\u7b56\u7565\uff0c\u901a\u8fc7\u8054\u5408\u81ea\u6ce8\u610f\u529b\u5904\u7406\u56fe\u50cf\u548c\u52a8\u4f5ctoken\uff0c\u81ea\u9002\u5e94\u5c42\u5f52\u4e00\u5316\u6ce8\u5165\u672c\u4f53\u611f\u89c9\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u6ce8\u5165\u89e6\u89c9\u4fe1\u53f7\uff0cLoRA\u9002\u914d\u5668\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u63d0\u51fa\u4e86DECO\u6846\u67b6\u548cDECO-50\u6570\u636e\u96c6\uff0c\u5305\u542b4\u4e2a\u573a\u666f\u300128\u4e2a\u5b50\u4efb\u52a1\u300150\u5c0f\u65f6\u6570\u636e\u3001500\u4e07\u5e27\u548c8000\u6761\u6210\u529f\u8f68\u8ff9\u3002", "conclusion": "DECO\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u591a\u6a21\u6001\u6761\u4ef6\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u7075\u5de7\u64cd\u4f5c\uff0cDECO-50\u6570\u636e\u96c6\u4e3a\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2602.05307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05307", "abs": "https://arxiv.org/abs/2602.05307", "authors": ["Haojin Wang", "Yike Wang", "Shangbin Feng", "Hannaneh Hajishirzi", "Yulia Tsvetkov"], "title": "MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning", "comment": null, "summary": "Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.", "AI": {"tldr": "MentorCollab\uff1a\u4e00\u79cd\u63a8\u7406\u65f6\u534f\u4f5c\u65b9\u6cd5\uff0c\u8ba9\u5927\u578b\u63a8\u7406\u6a21\u578b\u7a00\u758f\u5730\u6307\u5bfc\u5c0f\u578b\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u9a8c\u8bc1\u5668\u9009\u62e9\u6027\u5730\u8ba9\u5c0f\u578b\u6a21\u578b\u8ddf\u968f\u5bfc\u5e08\u7684\u77ed\u524d\u77bb\u7247\u6bb5\uff0c\u5728\u964d\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u63a8\u7406\u6210\u672c\u9ad8\u4e14\u5e38\u4ea7\u751f\u5197\u4f59\u63a8\u7406\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6548\u7387\u9ad8\u4f46\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u534f\u4f5c\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u6a21\u4eff\u800c\u975e\u771f\u6b63\u7684\u9519\u8bef\u7ea0\u6b63\u3002", "method": "\u63d0\u51faMentorCollab\u65b9\u6cd5\uff1a\u5728\u968f\u673a\u91c7\u6837\u7684token\u4f4d\u7f6e\u63a2\u6d4b\u4e24\u4e2a\u6a21\u578b\u7684\u5206\u6b67\uff0c\u4f7f\u7528\u8f7b\u91cf\u9a8c\u8bc1\u5668\u51b3\u5b9a\u5c0f\u578b\u6a21\u578b\u662f\u8ddf\u968f\u5bfc\u5e08\u7684\u77ed\u524d\u77bb\u7247\u6bb5\u8fd8\u662f\u7ee7\u7eed\u81ea\u4e3b\u751f\u6210\u3002\u9009\u62e9\u6027\u7a00\u758f\u6307\u5bfc\u800c\u975e\u63a5\u7ba1\u751f\u6210\u3002", "result": "\u572815\u4e2aSLM-LRM\u5bf9\u548c3\u4e2a\u9886\u57df\uff08\u6570\u5b66\u63a8\u7406\u3001\u5e38\u8bc6\u63a8\u7406\u3001\u901a\u7528\u77e5\u8bc6\uff09\u4e2d\uff0c12\u4e2a\u8bbe\u7f6e\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747\u589e\u76ca3.0%\uff0c\u6700\u9ad8\u8fbe8.0%\uff0c\u5e73\u5747\u4ec518.4%\u7684token\u7531\u6602\u8d35\u5bfc\u5e08\u6a21\u578b\u751f\u6210\u3002", "conclusion": "\u9009\u62e9\u6027\u63a8\u7406\u65f6\u6307\u5bfc\u80fd\u591f\u6062\u590d\u5927\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u800c\u65e0\u9700\u5927\u91cf\u63a8\u7406\u5f00\u9500\uff0c\u77ed\u7247\u6bb5\u548c\u9009\u62e9\u6027\u63a2\u6d4b\u8db3\u4ee5\u5b9e\u73b0\u6709\u6548\u534f\u4f5c\u3002"}}
{"id": "2602.05516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05516", "abs": "https://arxiv.org/abs/2602.05516", "authors": ["Runxiao Liu", "Pengda Mao", "Xiangli Le", "Shuang Gu", "Yapeng Chen", "Quan Quan"], "title": "Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments", "comment": "10 pages, 8 figures", "summary": "This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u865a\u62df\u7ba1\u7406\u8bba\u548c\u8017\u6563\u7cfb\u7edf\u7406\u8bba\u7684\u591a\u65e0\u4eba\u673a\u534f\u540c\u8fd0\u8f93\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u53d7\u9650\u73af\u5883\u4e0b\u7684\u7535\u7f06\u60ac\u6302\u8d1f\u8f7d\u8fd0\u8f93", "motivation": "\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u5728\u53d7\u9650\u73af\u5883\u4e2d\u534f\u540c\u8fd0\u8f93\u7535\u7f06\u60ac\u6302\u8d1f\u8f7d\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u534f\u8c03\u3001\u52a8\u6001\u9002\u5e94\u969c\u788d\u7269\u5e03\u5c40\uff0c\u540c\u65f6\u4fdd\u8bc1\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027", "method": "\u7ed3\u5408\u865a\u62df\u7ba1\u7406\u8bba\u548c\u8017\u6563\u7cfb\u7edf\u7406\u8bba\uff0c\u8bbe\u8ba1\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u63a7\u5236\u6846\u67b6\uff0c\u5b9e\u73b0\u5f20\u529b\u5206\u914d\u548c\u534f\u8c03\u8fd0\u8f93\uff0c\u52a8\u6001\u8c03\u6574\u65e0\u4eba\u673a\u914d\u7f6e\u4ee5\u9002\u5e94\u969c\u788d\u7269\u5e03\u5c40", "result": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5728\u5ba4\u5916\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5b9e\u9645\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5b9e\u73b0\u591a\u65e0\u4eba\u673a\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u534f\u540c\u8d1f\u8f7d\u8fd0\u8f93\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u5f00\u9500\u3001\u9ad8\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f"}}
{"id": "2602.05347", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05347", "abs": "https://arxiv.org/abs/2602.05347", "authors": ["Soma Sato", "Ryohei Sasano"], "title": "How Do Language Models Acquire Character-Level Information?", "comment": "Accepted to EACL 2026 Main Conference", "summary": "Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u672a\u660e\u786e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u9690\u5f0f\u7f16\u7801\u5b57\u7b26\u7ea7\u4fe1\u606f\uff0c\u672c\u6587\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u5206\u6790\u5176\u673a\u5236\uff0c\u53d1\u73b0\u5206\u8bcd\u89c4\u5219\u548c\u62fc\u5199\u7ea6\u675f\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u800c\u5b50\u4e32\u8bed\u4e49\u5173\u8054\u548c\u53e5\u6cd5\u4fe1\u606f\u5219\u662f\u72ec\u7acb\u4e8e\u5206\u8bcd\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u88ab\u62a5\u9053\u80fd\u591f\u9690\u5f0f\u7f16\u7801\u5b57\u7b26\u7ea7\u4fe1\u606f\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u83b7\u5f97\u5b57\u7b26\u7ea7\u77e5\u8bc6\u7684\u673a\u5236\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5728\u53d7\u63a7\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5982\u6307\u5b9a\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u6216\u5206\u8bcd\u5668\uff09\u4e0e\u6807\u51c6\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5206\u6790\u6a21\u578b\u5982\u4f55\u83b7\u5f97\u5b57\u7b26\u7ea7\u77e5\u8bc6\u3002\u5c06\u5f71\u54cd\u56e0\u7d20\u5206\u4e3a\u4e0e\u5206\u8bcd\u76f8\u5173\u548c\u72ec\u7acb\u4e8e\u5206\u8bcd\u7684\u4e24\u7c7b\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff0c\u5408\u5e76\u89c4\u5219\u548c\u62fc\u5199\u7ea6\u675f\u662f\u6e90\u4e8e\u5206\u8bcd\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u5b50\u4e32\u7684\u8bed\u4e49\u5173\u8054\u548c\u53e5\u6cd5\u4fe1\u606f\u5219\u662f\u72ec\u7acb\u4e8e\u5206\u8bcd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u5b57\u7b26\u7ea7\u77e5\u8bc6\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u65e2\u6709\u5206\u8bcd\u76f8\u5173\u7684\u673a\u5236\uff08\u5982\u5408\u5e76\u89c4\u5219\u548c\u62fc\u5199\u7ea6\u675f\uff09\uff0c\u4e5f\u6709\u72ec\u7acb\u4e8e\u5206\u8bcd\u7684\u673a\u5236\uff08\u5982\u8bed\u4e49\u5173\u8054\u548c\u53e5\u6cd5\u4fe1\u606f\uff09\uff0c\u8fd9\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u5f81\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.05552", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05552", "abs": "https://arxiv.org/abs/2602.05552", "authors": ["Bessie Dominguez-Dager", "Sergio Suescun-Ferrandiz", "Felix Escalona", "Francisco Gomez-Donoso", "Miguel Cazorla"], "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator", "comment": null, "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.", "AI": {"tldr": "VLN-Pilot\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u65e0\u4eba\u673a\u5ba4\u5185\u5bfc\u822a\"\u98de\u884c\u5458\"\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u89c6\u89c9\u611f\u77e5\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\uff0c\u65e0\u9700GPS\u6216\u590d\u6742\u5de5\u7a0b\u3002", "motivation": "\u4f20\u7edf\u5ba4\u5185\u65e0\u4eba\u673a\u5bfc\u822a\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u6216\u51e0\u4f55\u8def\u5f84\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u5de5\u7a0b\u3002\u672c\u6587\u65e8\u5728\u5229\u7528VLLM\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u8bed\u4e49\u7406\u89e3\u9a71\u52a8\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u51cf\u5c11\u64cd\u4f5c\u5458\u8d1f\u62c5\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "method": "VLN-Pilot\u6846\u67b6\u8ba9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5145\u5f53\u65e0\u4eba\u673a\u98de\u884c\u5458\uff0c\u901a\u8fc7\u89e3\u91ca\u81ea\u7531\u5f62\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u7ed3\u5408\u89c6\u89c9\u89c2\u5bdf\u8fdb\u884c\u63a5\u5730\uff0c\u89c4\u5212\u5e76\u6267\u884c\u65e0\u4eba\u673a\u8f68\u8ff9\u3002\u6846\u67b6\u6574\u5408\u4e86\u8bed\u8a00\u9a71\u52a8\u7684\u8bed\u4e49\u7406\u89e3\u548c\u89c6\u89c9\u611f\u77e5\uff0c\u652f\u6301\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u3001\u907f\u969c\u548c\u52a8\u6001\u4e8b\u4ef6\u54cd\u5e94\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u7684\u903c\u771f\u5ba4\u5185\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9ad8\u6210\u529f\u7387\uff0c\u5305\u62ec\u5177\u6709\u591a\u4e2a\u8bed\u4e49\u76ee\u6807\u7684\u957f\u671f\u5bfc\u822a\u3002\u7ed3\u679c\u8868\u660e\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u4e3b\u4ee3\u7406\u6709\u671b\u66ff\u4ee3\u8fdc\u7a0b\u65e0\u4eba\u673a\u98de\u884c\u5458\u3002", "conclusion": "VLN-Pilot\u5c55\u793a\u4e86VLLM\u5728\u5ba4\u5185\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u64cd\u4f5c\u5458\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u4efb\u52a1\u7075\u6d3b\u6027\uff0c\u4e3a\u68c0\u67e5\u3001\u641c\u6551\u3001\u8bbe\u65bd\u76d1\u63a7\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4eba\u6027\u5316\u7684\u65e0\u4eba\u673a\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2602.05370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05370", "abs": "https://arxiv.org/abs/2602.05370", "authors": ["Jun Rao", "Zixiong Yu", "Xuebo Liu", "Guhan Chen", "Jing Li", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "title": "PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning", "comment": null, "summary": "Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \\ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \\textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.", "AI": {"tldr": "PACE\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u5f0f\u7ea0\u6b63\u7b56\u7565\u66ff\u4ee3\u66b4\u529b\u91c7\u6837\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4ec5\u9700\u5c11\u91cf\u6837\u672c\u5c31\u80fd\u8d85\u8d8aDPO-R1\uff08N=16\uff09\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u7ea680%", "motivation": "\u6311\u6218DPO-R1\u4e2dBest-of-N\u91c7\u6837\u7684\u6269\u5c55\u5047\u8bbe\uff0c\u53d1\u73b0\u6570\u5b66\u63a8\u7406\u4e2d\u8fc7\u5ea6\u63a2\u7d22\u4f1a\u5bfc\u81f4\u6536\u76ca\u9012\u51cf\u751a\u81f3\u7b56\u7565\u5d29\u6e83\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5", "method": "\u63d0\u51faPACE\uff08Proximal Alignment via Corrective Exploration\uff09\uff0c\u4f7f\u7528\u751f\u6210\u5f0f\u7ea0\u6b63\u7b56\u7565\u4ece\u5931\u8d25\u63a2\u7d22\u4e2d\u5408\u6210\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\uff0c\u4ec5\u9700\u5c11\u91cf\u6837\u672c\uff082<N<3\uff09", "result": "PACE\u5728\u6027\u80fd\u4e0a\u8d85\u8d8aDPO-R1\uff08N=16\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u4e3a\u51761/5\uff0c\u5bf9\u5956\u52b1\u653b\u51fb\u548c\u6807\u7b7e\u566a\u58f0\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027", "conclusion": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u8d28\u91cf\u4f18\u4e8e\u6570\u91cf\uff0cPACE\u901a\u8fc7\u667a\u80fd\u7ea0\u6b63\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u9c81\u68d2\u7684\u504f\u597d\u5bf9\u9f50"}}
{"id": "2602.05596", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05596", "abs": "https://arxiv.org/abs/2602.05596", "authors": ["Hokyun Lee", "Woo-Jeong Baek", "Junhyeok Cha", "Jaeheung Park"], "title": "TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards", "comment": "Accepted for Publication at IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.", "AI": {"tldr": "TOLEBI\u662f\u4e00\u4e2a\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u6b65\u6001\u7684\u6545\u969c\u5bb9\u5fcd\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5173\u8282\u9501\u5b9a\u3001\u7535\u6e90\u6545\u969c\u548c\u5916\u90e8\u5e72\u6270\u6765\u5b66\u4e60\u5bb9\u9519\u6b65\u6001\u7b56\u7565\uff0c\u5e76\u5305\u542b\u5728\u7ebf\u5173\u8282\u72b6\u6001\u6a21\u5757\u8fdb\u884c\u5b9e\u65f6\u6545\u969c\u5206\u7c7b\u3002", "motivation": "\u968f\u7740\u5b66\u4e60\u7b97\u6cd5\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u53cc\u8db3\u6b65\u6001\u7684\u5f3a\u5316\u5b66\u4e60\u6210\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7814\u7a76\u7684\u70ed\u70b9\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u5904\u7406\u6b65\u6001\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u53d1\u751f\u7684\u786c\u4ef6\u6545\u969c\uff0c\u800c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e72\u6270\u6216\u7a81\u53d1\u786c\u4ef6\u6545\u969c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002", "method": "\u63d0\u51faTOLEBI\u6846\u67b6\uff1a1)\u5728\u6a21\u62df\u4e2d\u6ce8\u5165\u5173\u8282\u9501\u5b9a\u3001\u7535\u6e90\u6545\u969c\u548c\u5916\u90e8\u5e72\u6270\u6765\u5b66\u4e60\u5bb9\u9519\u6b65\u6001\u7b56\u7565\uff1b2)\u901a\u8fc7\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u5c06\u5b66\u4e60\u5230\u7684\u7b56\u7565\u8f6c\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\uff1b3)\u96c6\u6210\u5728\u7ebf\u5173\u8282\u72b6\u6001\u6a21\u5757\uff0c\u6839\u636e\u5b9e\u9645\u89c2\u6d4b\u5b9e\u65f6\u5206\u7c7b\u5173\u8282\u72b6\u6001\u3002", "result": "\u5728\u4eba\u5f62\u673a\u5668\u4ebaTOCABI\u4e0a\u8fdb\u884c\u7684\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u9a8c\u8bc1\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002\u636e\u4f5c\u8005\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u53cc\u8db3\u6b65\u6001\u5bb9\u9519\u6846\u67b6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u53cc\u8db3\u6b65\u6001\u6545\u969c\u5bb9\u5fcd\u6846\u67b6TOLEBI\uff0c\u901a\u8fc7\u6a21\u62df\u6545\u969c\u8bad\u7ec3\u548c\u5728\u7ebf\u72b6\u6001\u76d1\u6d4b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4e3a\u8fd9\u4e00\u9886\u57df\u9ad8\u6548\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2602.05374", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05374", "abs": "https://arxiv.org/abs/2602.05374", "authors": ["Chaimae Abouzahir", "Congbo Ma", "Nizar Habash", "Farah E. Shamout"], "title": "Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks", "comment": "Accepted to HeaLing-EACL 2026", "summary": "In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.", "AI": {"tldr": "LLMs\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u533b\u5b66\u95ee\u7b54\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u800c\u52a0\u5267\uff0c\u4e3b\u8981\u6e90\u4e8e\u963f\u62c9\u4f2f\u8bed\u533b\u5b66\u6587\u672c\u7684\u7ed3\u6784\u788e\u7247\u5316\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u6b63\u786e\u6027\u7684\u4f4e\u76f8\u5173\u6027\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u591a\u4e3a\u82f1\u8bed\u4e2d\u5fc3\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bed\u8a00\u591a\u6837\u6027\u793e\u533a\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u6307\u51fa\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4f46\u5176\u6839\u672c\u539f\u56e0\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5bf9\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u533b\u5b66\u95ee\u7b54\u8fdb\u884c\u8de8\u8bed\u8a00\u5b9e\u8bc1\u5206\u6790\uff0c\u5305\u62ec\u6027\u80fd\u5dee\u8ddd\u8bc4\u4f30\u3001\u963f\u62c9\u4f2f\u8bed\u533b\u5b66\u6587\u672c\u7684\u5206\u8bcd\u7ed3\u6784\u5206\u6790\uff0c\u4ee5\u53ca\u6a21\u578b\u62a5\u544a\u7f6e\u4fe1\u5ea6\u548c\u89e3\u91ca\u4e0e\u6b63\u786e\u6027\u7684\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "\u53d1\u73b0\u6301\u7eed\u5b58\u5728\u7684\u8bed\u8a00\u9a71\u52a8\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u800c\u52a0\u5267\uff1b\u963f\u62c9\u4f2f\u8bed\u533b\u5b66\u6587\u672c\u5b58\u5728\u7ed3\u6784\u788e\u7247\u5316\uff1b\u6a21\u578b\u62a5\u544a\u7684\u7f6e\u4fe1\u5ea6\u548c\u89e3\u91ca\u4e0e\u6b63\u786e\u6027\u76f8\u5173\u6027\u6709\u9650\u3002", "conclusion": "\u5f3a\u8c03\u5728\u533b\u5b66\u4efb\u52a1LLMs\u4e2d\u9700\u8981\u8bed\u8a00\u611f\u77e5\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u8bed\u8a00\u591a\u6837\u6027\u5e26\u6765\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002"}}
{"id": "2602.05608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05608", "abs": "https://arxiv.org/abs/2602.05608", "authors": ["Yufei Zhu", "Shih-Min Yang", "Martin Magnusson", "Allan Wang"], "title": "HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments", "comment": "Accepted to the 2026 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.", "AI": {"tldr": "HiCrowd\uff1a\u5206\u5c42\u6846\u67b6\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5229\u7528\u884c\u4eba\u8fd0\u52a8\u4f5c\u4e3a\u5f15\u5bfc\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u4eba\u7fa4\u4e2d\u7684\u51bb\u7ed3\u95ee\u9898\uff0c\u63d0\u9ad8\u5bfc\u822a\u6548\u7387\u548c\u5b89\u5168", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u4eba\u7fa4\u4e2d\u7684\"\u51bb\u7ed3\u673a\u5668\u4eba\u95ee\u9898\"\u2014\u2014\u673a\u5668\u4eba\u96be\u4ee5\u627e\u5230\u5b89\u5168\u8fd0\u52a8\u8def\u5f84\u800c\u88ab\u56f0\u5728\u4eba\u7fa4\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u884c\u4eba\u4ec5\u89c6\u4e3a\u52a8\u6001\u969c\u788d\u7269\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u6548\u7387", "method": "\u63d0\u51faHiCrowd\u5206\u5c42\u6846\u67b6\uff1a\u9ad8\u5c42RL\u7b56\u7565\u751f\u6210\u8ddf\u968f\u70b9\uff0c\u4f7f\u673a\u5668\u4eba\u4e0e\u5408\u9002\u884c\u4eba\u7fa4\u4f53\u5bf9\u9f50\uff1b\u4f4e\u5c42MPC\u5b89\u5168\u8ddf\u8e2a\u5f15\u5bfc\uff0c\u8fdb\u884c\u77ed\u671f\u89c4\u5212\u3002\u7ed3\u5408\u957f\u671f\u4eba\u7fa4\u611f\u77e5\u51b3\u7b56\u4e0e\u5b89\u5168\u77ed\u671f\u6267\u884c", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u5408\u6210\u4eba\u7fa4\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u53cd\u5e94\u5f0f\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5728\u5bfc\u822a\u6548\u7387\u3001\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u51cf\u5c11\u4e86\u51bb\u7ed3\u884c\u4e3a", "conclusion": "\u5c06\u4eba\u7c7b\u8fd0\u52a8\u4f5c\u4e3a\u5f15\u5bfc\u800c\u975e\u4ec5\u89c6\u4e3a\u52a8\u6001\u969c\u788d\u7269\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u4eba\u7fa4\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u529b\u539f\u5219\u3002\u5206\u5c42\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u957f\u671f\u51b3\u7b56\u4e0e\u77ed\u671f\u5b89\u5168\u6267\u884c"}}
{"id": "2602.05385", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05385", "abs": "https://arxiv.org/abs/2602.05385", "authors": ["Tao Liu", "Jiafan Lu", "Bohan Yu", "Pengcheng Wu", "Liu Haixin", "Guoyu Xu", "Li Xiangheng", "Lixiao Li", "Jiaming Hou", "Zhao Shijun", "Xinglin Lyu", "Kunli Zhang", "Yuxiang Jia", "Hongyin Zan"], "title": "IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models", "comment": "25 pages, 16 figures, 8 tables. Hongyin Zan is corresponding author, Jiafan Lu is first co-author", "summary": "Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.", "AI": {"tldr": "IESR\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u589e\u5f3a\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u5b9e\u73b0\u9ad8\u6548Text-to-SQL\uff0c\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u5f53\u524dText-to-SQL\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u3001\u9886\u57df\u77e5\u8bc6\u3001\u5047\u8bbe\u67e5\u8be2\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f01\u4e1a\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faIESR\u6846\u67b6\uff1a1) \u5229\u7528LLM\u8fdb\u884c\u5173\u952e\u4fe1\u606f\u7406\u89e3\u548c\u6a21\u5f0f\u94fe\u63a5\uff0c\u5206\u79bb\u6570\u5b66\u8ba1\u7b97\u548cSQL\u751f\u6210\uff1b2) \u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u591a\u8def\u5f84\u63a8\u7406\u673a\u5236\uff1b3) \u8f68\u8ff9\u4e00\u81f4\u6027\u9a8c\u8bc1\u6a21\u5757\u786e\u4fdd\u51c6\u786e\u6027", "result": "\u5728LogicCat\u57fa\u51c6\u4e0a\u8fbe\u523024.28 EX\uff0cArcher\u6570\u636e\u96c6\u4e0a\u8fbe\u523037.28 EX\uff0c\u4ec5\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u65e0\u9700\u5fae\u8c03\u5373\u5b9e\u73b0SOTA\u6027\u80fd", "conclusion": "IESR\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u590d\u6742Text-to-SQL\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u7f16\u7801\u5668\u6a21\u578b\u5728\u7269\u7406\u77e5\u8bc6\u3001\u6570\u5b66\u8ba1\u7b97\u548c\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u7684\u504f\u5dee\u548c\u4e0d\u8db3"}}
{"id": "2602.05683", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05683", "abs": "https://arxiv.org/abs/2602.05683", "authors": ["Chuwei Wang", "Eduardo Sebasti\u00e1n", "Amanda Prorok", "Anastasia Bizyaeva"], "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking", "comment": null, "summary": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u89c9\u5f15\u5bfc\u5bfc\u822a\u548c\u8ddf\u8e2a\uff0c\u901a\u8fc7\u52a8\u6001\u795e\u7ecf\u5143\u7fa4\u4f53\u5c06\u89c6\u89c9\u76ee\u6807\u6fc0\u52b1\u76f4\u63a5\u8f6c\u6362\u4e3a\u81ea\u6211\u4e2d\u5fc3\u8fd0\u52a8\u547d\u4ee4\uff0c\u5229\u7528\u52a8\u6001\u5206\u5c94\u673a\u5236\u89e3\u51b3\u76ee\u6807\u5bf9\u79f0\u6027\u5bfc\u81f4\u7684\u51b3\u7b56\u56f0\u96be\u3002", "motivation": "\u673a\u5668\u4eba\u5bfc\u822a\u957f\u671f\u9762\u4e34\u53cd\u5e94\u5f0f\u4f20\u611f\u5668\u63a7\u5236\u4e0e\u57fa\u4e8e\u6a21\u578b\u89c4\u5212\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002\u5f53\u591a\u4e2a\u76ee\u6807\u9009\u9879\u5bf9\u79f0\u5b58\u5728\u65f6\uff0c\u53cd\u5e94\u5f0f\u7cfb\u7edf\u96be\u4ee5\u505a\u51fa\u51b3\u7b56\uff0c\u800c\u4f20\u7edf\u89c4\u5212\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u65f6\u54cd\u5e94\u53c8\u5177\u6709\u51b3\u7b56\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u6846\u67b6\uff1a1) \u5c06\u673a\u8f7d\u6444\u50cf\u5934\u50cf\u7d20\u7f16\u7801\u4e3a\u52a8\u6001\u795e\u7ecf\u5143\u7fa4\u4f53\u7684\u8f93\u5165\uff1b2) \u5c06\u89c6\u89c9\u76ee\u6807\u6fc0\u52b1\u76f4\u63a5\u8f6c\u6362\u4e3a\u81ea\u6211\u4e2d\u5fc3\u8fd0\u52a8\u547d\u4ee4\uff1b3) \u5f15\u5165\u52a8\u6001\u5206\u5c94\u673a\u5236\uff0c\u5ef6\u8fdf\u51b3\u7b56\u76f4\u5230\u73af\u5883\u51e0\u4f55\u8bf1\u5bfc\u7684\u4e34\u754c\u70b9\u51fa\u73b0\uff1b4) \u53d7\u52a8\u7269\u8ba4\u77e5\u548c\u610f\u89c1\u52a8\u6001\u673a\u5236\u6a21\u578b\u542f\u53d1\uff0c\u8bbe\u8ba1\u5177\u6709\u53ef\u89e3\u91ca\u53c2\u6570\u7684\u7cfb\u7edf\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\u548c\u5b9e\u9a8c\u56db\u65cb\u7ffc\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u63a7\u5236\u5668\u5b9e\u73b0\u4e86\u5b9e\u65f6\u81ea\u4e3b\u6027\uff0c\u8ba1\u7b97\u8d1f\u62c5\u5c0f\uff0c\u53c2\u6570\u5c11\u4e14\u53ef\u89e3\u91ca\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u7279\u5b9a\u5e94\u7528\u7684\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\u4e2d\u3002", "conclusion": "\u8be5\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u6846\u67b6\u6210\u529f\u6865\u63a5\u4e86\u53cd\u5e94\u5f0f\u63a7\u5236\u4e0e\u6a21\u578b\u89c4\u5212\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5c94\u673a\u5236\u89e3\u51b3\u4e86\u5bf9\u79f0\u76ee\u6807\u4e0b\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9\u5f15\u5bfc\u5bfc\u822a\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u3001\u5b9e\u65f6\u54cd\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05392", "abs": "https://arxiv.org/abs/2602.05392", "authors": ["Jiyun Chun", "Eric Fosler-Lussier", "Michael White", "Andrew Perrault"], "title": "Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances", "comment": null, "summary": "Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u513f\u7ae5\u8bdd\u8bed\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u6269\u5c55\u6027\u548c\u72ec\u7acb\u6027\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u513f\u7ae5\u5728\u5bf9\u8bdd\u4e2d\u7684\u8bdd\u8bed\u8d28\u91cf\uff0c\u66ff\u4ee3\u4f20\u7edf\u57fa\u4e8e\u957f\u5ea6\u7684\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u513f\u7ae5\u8bdd\u8bed\u8bc4\u4f30\u6307\u6807\uff08\u5982\u5e73\u5747\u8bdd\u8bed\u957f\u5ea6\u3001\u8bcd\u6c47\u591a\u6837\u6027\u3001\u53ef\u8bfb\u6027\u6307\u6570\uff09\u8fc7\u4e8e\u4f9d\u8d56\u957f\u5ea6\uff0c\u5ffd\u7565\u4e86\u5bf9\u8bdd\u8bed\u5883\uff0c\u65e0\u6cd5\u6355\u6349\u63a8\u7406\u6df1\u5ea6\u3001\u8bdd\u9898\u7ef4\u6301\u548c\u8bdd\u8bed\u89c4\u5212\u7b49\u8d28\u91cf\u7ef4\u5ea6\u3002", "method": "\u91c7\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u6846\u67b6\uff1a\u9996\u5148\u5206\u7c7b\u6210\u4eba\u524d\u8bdd\u8bed\u7c7b\u578b\uff0c\u7136\u540e\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u5206\u513f\u7ae5\u56de\u5e94\uff1a\u6269\u5c55\u6027\uff08\u8bed\u5883\u9610\u8ff0\u548c\u63a8\u7406\u6df1\u5ea6\uff09\u548c\u72ec\u7acb\u6027\uff08\u513f\u7ae5\u63a8\u52a8\u8bdd\u8bed\u53d1\u5c55\u7684\u8d21\u732e\uff09\u3002", "result": "\u65b9\u6cd5\u5177\u6709\u53d1\u5c55\u6709\u6548\u6027\uff08\u663e\u793a\u5e74\u9f84\u76f8\u5173\u6a21\u5f0f\uff09\uff0c\u9884\u6d4b\u4ef7\u503c\u4f18\u4e8e\u5e38\u89c1\u57fa\u7ebf\uff0c\u80fd\u68c0\u6d4b\u4e0e\u8bdd\u8bed\u5173\u7cfb\u76f8\u5173\u7684\u8bed\u4e49\u5dee\u5f02\uff0c\u4e14\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u3002", "conclusion": "\u5c06\u513f\u7ae5\u8bdd\u8bed\u8bc4\u4f30\u4ece\u5355\u7eaf\u6d4b\u91cf\u957f\u5ea6\u8f6c\u5411\u8bc4\u4f30\u513f\u7ae5\u8bdd\u8bed\u5982\u4f55\u5728\u8bed\u5883\u4e2d\u6709\u610f\u4e49\u5730\u8d21\u732e\u548c\u63a8\u52a8\u5bf9\u8bdd\uff0c\u652f\u6301\u5927\u89c4\u6a21\u8bc4\u4f30\u3002"}}
{"id": "2602.05760", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05760", "abs": "https://arxiv.org/abs/2602.05760", "authors": ["Andreea Tulbure", "Carmen Scheidemann", "Elias Steiner", "Marco Hutter"], "title": "Task-Oriented Robot-Human Handovers on Legged Manipulators", "comment": "Accepted to 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026", "summary": "Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human's intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers.", "AI": {"tldr": "AFT-Handover\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u3001\u53ef\u6cdb\u5316\u7684\u4efb\u52a1\u5bfc\u5411\u9012\u7269\u6846\u67b6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u53ef\u4f9b\u6027\u63a8\u7406\u548c\u7eb9\u7406\u5316\u53ef\u4f9b\u6027\u8f6c\u79fb\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5bf9\u65b0\u9896\u7269\u4f53-\u4efb\u52a1\u5bf9\u7684\u667a\u80fd\u9012\u7269\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5bfc\u5411\u9012\u7269\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u7269\u4f53\u6216\u4efb\u52a1\u7279\u5b9a\u7684\u53ef\u4f9b\u6027\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5904\u7406\u65b0\u9896\u573a\u666f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u7269\u4f53-\u4efb\u52a1\u5bf9\u7684\u9012\u7269\u6846\u67b6\u3002", "method": "\u7ed3\u5408LLM\u9a71\u52a8\u7684\u53ef\u4f9b\u6027\u63a8\u7406\u548c\u9ad8\u6548\u7684\u57fa\u4e8e\u7eb9\u7406\u7684\u53ef\u4f9b\u6027\u8f6c\u79fb\u3002\u7ed9\u5b9a\u65b0\u7269\u4f53-\u4efb\u52a1\u5bf9\u65f6\uff0c\u4ece\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u4ee3\u7406\u793a\u4f8b\uff0c\u901a\u8fc7LLM\u63a8\u7406\u5efa\u7acb\u90e8\u4ef6\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u7eb9\u7406\u5316\u53ef\u4f9b\u6027\u8fdb\u884c\u57fa\u4e8e\u7279\u5f81\u7684\u70b9\u4e91\u8f6c\u79fb\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u4efb\u52a1-\u7269\u4f53\u5bf9\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9012\u7269\u6210\u529f\u7387\u5e76\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u7528\u6237\u7814\u7a76\u4e2d\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u4eba\u7c7b\u5728\u4f7f\u7528\u5de5\u5177\u524d\u7684\u91cd\u65b0\u6293\u63e1\u3002", "conclusion": "AFT-Handover\u6846\u67b6\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u3001\u53ef\u6cdb\u5316\u7684\u4efb\u52a1\u5bfc\u5411\u9012\u7269\uff0c\u5728\u817f\u5f0f\u673a\u68b0\u81c2\u4e0a\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba-\u4eba\u7c7b\u9012\u7269\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05393", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05393", "abs": "https://arxiv.org/abs/2602.05393", "authors": ["Ji Zhao", "Yufei Gu", "Shitong Shao", "Xun Zhou", "Liang Xiang", "Zeke Xie"], "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better", "comment": null, "summary": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model.", "AI": {"tldr": "\u63d0\u51faLate-to-Early Training (LET)\u8303\u5f0f\uff0c\u5229\u7528\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u6307\u5bfc\u5927\u578b\u6a21\u578b\u8bad\u7ec3\uff0c\u901a\u8fc7\u540e\u671f\u77e5\u8bc6\u65e9\u671f\u5b66\u4e60\u548c\u540e\u671f\u5c42\u6307\u5bfc\u65e9\u671f\u5c42\uff0c\u5b9e\u73b0\u8bad\u7ec3\u52a0\u901f\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740LLM\u89c4\u6a21\u6269\u5927\uff0c\u9884\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u6025\u5267\u589e\u52a0\u3002\u867d\u7136\u5df2\u6709\u5927\u91cf\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u5982\u4f55\u5229\u7528\u73b0\u6709\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u52a0\u901f\u5927\u578b\u6a21\u578b\u8bad\u7ec3\u8fd9\u4e00\u5b9e\u9645\u95ee\u9898\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faLET\u8303\u5f0f\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u540e\u671f\u5c42\u7684\u8868\u793a\u6765\u6307\u5bfc\u76ee\u6807\u6a21\u578b\u65e9\u671f\u5c42\u7684\u8bad\u7ec3\u3002\u5305\u542b\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a\u540e\u671f\u5230\u65e9\u671f\u6b65\u9aa4\u5b66\u4e60\u548c\u540e\u671f\u5230\u65e9\u671f\u5c42\u5b66\u4e60\u3002", "result": "\u57281.4B\u548c7B\u53c2\u6570\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLET\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u63d0\u5347\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u8bad\u7ec31.4B\u6a21\u578b\u65f6\uff0c\u8fbe\u52301.6\u500d\u52a0\u901f\uff0c\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u5347\u8fd15%\uff0c\u5373\u4f7f\u4f7f\u7528\u53c2\u6570\u5c1110\u500d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "LET\u8303\u5f0f\u80fd\u591f\u6709\u6548\u5229\u7528\u73b0\u6709\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u52a0\u901f\u5927\u578bLLM\u8bad\u7ec3\uff0c\u5b9e\u73b0\u66f4\u5feb\u8bad\u7ec3\u548c\u66f4\u597d\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3LLM\u9884\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.05791", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05791", "abs": "https://arxiv.org/abs/2602.05791", "authors": ["Yufei Xue", "YunFeng Lin", "Wentao Dong", "Yang Tang", "Jingbo Wang", "Jiangmiao Pang", "Ming Zhou", "Minghuan Liu", "Weinan Zhang"], "title": "Scalable and General Whole-Body Control for Cross-Humanoid Locomotion", "comment": null, "summary": "Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.", "AI": {"tldr": "XHugWBC\uff1a\u4e00\u4e2a\u8de8\u5177\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u5b9e\u73b0\u591a\u79cd\u4e0d\u540c\u8bbe\u8ba1\u4eba\u5f62\u673a\u5668\u4eba\u7684\u901a\u7528\u63a7\u5236", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u5668\u901a\u5e38\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u8fdb\u884c\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u672c\u6587\u7814\u7a76\u8de8\u5177\u8eab\u4eba\u5f62\u63a7\u5236\u95ee\u9898\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u6cdb\u5316\u5230\u591a\u79cd\u4e0d\u540c\u8bbe\u8ba1\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5355\u4e00\u7b56\u7565\u3002", "method": "\u63d0\u51faXHugWBC\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a(1) \u7269\u7406\u4e00\u81f4\u7684\u5f62\u6001\u968f\u673a\u5316\uff0c(2) \u8de8\u4e0d\u540c\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8bed\u4e49\u5bf9\u9f50\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c(3) \u5efa\u6a21\u5f62\u6001\u548c\u52a8\u529b\u5b66\u7279\u6027\u7684\u6709\u6548\u7b56\u7565\u67b6\u6784\u3002\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u4e2d\u5185\u5316\u5e7f\u6cdb\u7684\u5f62\u6001\u548c\u52a8\u529b\u5b66\u7279\u5f81\u5206\u5e03\u3002", "result": "\u572812\u4e2a\u6a21\u62df\u4eba\u5f62\u673a\u5668\u4eba\u548c7\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u901a\u7528\u63a7\u5236\u5668\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u3002", "conclusion": "XHugWBC\u6846\u67b6\u901a\u8fc7\u4e00\u6b21\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8de8\u591a\u79cd\u4eba\u5f62\u673a\u5668\u4eba\u7684\u901a\u7528\u63a7\u5236\uff0c\u5b66\u4e60\u5230\u7684\u8fd0\u52a8\u5148\u9a8c\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05400", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05400", "abs": "https://arxiv.org/abs/2602.05400", "authors": ["Shaobo Wang", "Xuan Ouyang", "Tianyi Xu", "Yuzheng Hu", "Jialin Liu", "Guo Chen", "Tianyu Zhang", "Junhao Zheng", "Kexin Yang", "Xingzhang Ren", "Dayiheng Liu", "Linfeng Zhang"], "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "comment": "45 pages, 7 figures, 8 tables", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "AI": {"tldr": "OPUS\u662f\u4e00\u4e2a\u52a8\u6001\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5668\u8bf1\u5bfc\u7684\u66f4\u65b0\u7a7a\u95f4\u5b9a\u4e49\u6548\u7528\uff0c\u5728\u9884\u8bad\u7ec3\u4e2d\u663e\u8457\u63d0\u5347\u6570\u636e\u6548\u7387\uff0c\u7528\u66f4\u5c11token\u8fbe\u5230\u66f4\u597d\u6548\u679c\u3002", "motivation": "\u968f\u7740\u9ad8\u8d28\u91cf\u516c\u5171\u6587\u672c\u63a5\u8fd1\u67af\u7aed\uff08\u6570\u636e\u5899\u73b0\u8c61\uff09\uff0c\u9884\u8bad\u7ec3\u6b63\u4ece\u66f4\u591atoken\u8f6c\u5411\u66f4\u597dtoken\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5ffd\u7565\u8bad\u7ec3\u52a8\u6001\u7684\u542f\u53d1\u5f0f\u9759\u6001\u8fc7\u6ee4\u5668\uff0c\u8981\u4e48\u4f7f\u7528\u57fa\u4e8e\u539f\u59cb\u68af\u5ea6\u7684\u52a8\u6001\u4f46\u4f18\u5316\u5668\u65e0\u5173\u7684\u6807\u51c6\u3002", "method": "OPUS\u5728\u4f18\u5316\u5668\u8bf1\u5bfc\u7684\u66f4\u65b0\u7a7a\u95f4\u5b9a\u4e49\u6548\u7528\uff0c\u901a\u8fc7\u5c06\u5019\u9009\u6570\u636e\u7684\u6709\u6548\u66f4\u65b0\u6295\u5f71\u5230\u6765\u81ea\u7a33\u5b9a\u3001\u540c\u5206\u5e03\u4ee3\u7406\u7684\u76ee\u6807\u65b9\u5411\u6765\u8bc4\u5206\u3002\u91c7\u7528Ghost\u6280\u672f\u548cCountSketch\u786e\u4fdd\u53ef\u6269\u5c55\u6027\uff0c\u4f7f\u7528Boltzmann\u91c7\u6837\u4fdd\u8bc1\u6570\u636e\u591a\u6837\u6027\uff0c\u4ec5\u589e\u52a04.7%\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728GPT-2 Large/XL\u7684FineWeb\u548cFineWeb-Edu\u9884\u8bad\u7ec3\u4e2d\uff0cOPUS\u4ec5\u752830B token\u5c31\u8d85\u8d8a\u4e86\u5de5\u4e1a\u7ea7\u57fa\u7ebf\u548c\u5b8c\u6574\u7684200B token\u8bad\u7ec3\u3002\u4e0e\u5de5\u4e1a\u7ea7\u9759\u6001\u8fc7\u6ee4\u5668\u7ed3\u5408\u65f6\uff0c\u5373\u4f7f\u4f7f\u7528\u4f4e\u8d28\u91cf\u6570\u636e\u4e5f\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u7387\u3002\u5728Qwen3-8B-Base\u7684SciencePedia\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0c\u4ec5\u75280.5B token\u5c31\u8d85\u8d8a\u4e86\u5b8c\u65743B token\u8bad\u7ec3\u3002", "conclusion": "OPUS\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u52a8\u6001\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u5728\u591a\u79cd\u8bed\u6599\u5e93\u3001\u8d28\u91cf\u5c42\u7ea7\u3001\u4f18\u5316\u5668\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\uff0c\u4e3a\u89e3\u51b3\u6570\u636e\u5899\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u7279\u522b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2602.05855", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05855", "abs": "https://arxiv.org/abs/2602.05855", "authors": ["Dennis Bank", "Joost Cordes", "Thomas Seel", "Simon F. G. Ehlers"], "title": "A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion", "comment": null, "summary": "Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u6a21\u6001\u611f\u77e5\u6846\u67b6\uff0c\u4f7f\u7528\u673a\u5668\u4eba\u4e2d\u5fc3\u9ad8\u5ea6\u56fe\u8868\u793a\uff0c\u7ed3\u5408CNN\u548cGRU\u5b9e\u73b0\u65f6\u7a7a\u4e00\u81f4\u7684\u5730\u5f62\u91cd\u5efa\uff0c\u5728\u6df1\u5ea6\u76f8\u673a\u3001LiDAR\u548cIMU\u878d\u5408\u4e0b\u63d0\u5347\u7cbe\u5ea67.2-9.9%", "motivation": "\u4f20\u7edf\u4eba\u5f62\u673a\u5668\u4eba\u5730\u5f62\u611f\u77e5\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u5355\u4f20\u611f\u5668\u7ba1\u9053\uff0c\u5728\u975e\u7ed3\u6784\u5316\u4eba\u7c7b\u73af\u5883\u4e2d\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6", "method": "\u91c7\u7528\u673a\u5668\u4eba\u4e2d\u5fc3\u9ad8\u5ea6\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u8bbe\u8ba1\u6df7\u5408\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff1aCNN\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0cGRU\u6838\u5fc3\u5904\u7406\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u878d\u5408Intel RealSense\u6df1\u5ea6\u76f8\u673a\u3001LIVOX MID-360 LiDAR\uff08\u901a\u8fc7\u7403\u9762\u6295\u5f71\u5904\u7406\uff09\u548cIMU\u591a\u6a21\u6001\u6570\u636e", "result": "\u591a\u6a21\u6001\u878d\u5408\u76f8\u6bd4\u4ec5\u7528\u6df1\u5ea6\u76f8\u673a\u63d0\u53477.2%\u91cd\u5efa\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u4ec5\u7528LiDAR\u63d0\u53479.9%\uff1b\u96c6\u62103.2\u79d2\u65f6\u5e8f\u4e0a\u4e0b\u6587\u6709\u6548\u51cf\u5c11\u5efa\u56fe\u6f02\u79fb", "conclusion": "\u5b66\u4e60\u578b\u591a\u6a21\u6001\u6846\u67b6\u663e\u8457\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5730\u5f62\u611f\u77e5\u53ef\u9760\u6027\uff0c\u65f6\u7a7a\u878d\u5408\u7b56\u7565\u6709\u6548\u6539\u5584\u91cd\u5efa\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027"}}
{"id": "2602.05419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05419", "abs": "https://arxiv.org/abs/2602.05419", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "title": "Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation", "comment": "Accepted to TACL. This is a pre-MIT Press publication version", "summary": "Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.", "AI": {"tldr": "\u63d0\u51faUOT-ERRANT\uff0c\u4e00\u79cd\u57fa\u4e8e\u7f16\u8f91\u5411\u91cf\u548c\u6700\u4f18\u8fd0\u8f93\u7684GEC\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u5c06\u5047\u8bbe\u53e5\u7684\u7f16\u8f91\u5411\u91cf\u8fd0\u8f93\u5230\u53c2\u8003\u53e5\u6765\u6539\u8fdb\u8bc4\u4f30\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u591a\u7f16\u8f91\u7684\u6d41\u7545\u6027\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\uff08\u5982BERTScore\uff09\u5728GEC\u8bc4\u4f30\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8bb8\u591a\u8bcd\u5728\u6e90\u53e5\u3001\u5047\u8bbe\u53e5\u548c\u53c2\u8003\u53e5\u4e2d\u4fdd\u6301\u4e0d\u53d8\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9GEC\u7f16\u8f91\u8bbe\u8ba1\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7f16\u8f91\u5411\u91cf\u8868\u793a\u7f16\u8f91\u64cd\u4f5c\uff0c\u4f7f\u7528\u4e0d\u5e73\u8861\u6700\u4f18\u8fd0\u8f93\u5c06\u5047\u8bbe\u53e5\u7684\u7f16\u8f91\u5411\u91cf\u8fd0\u8f93\u5230\u53c2\u8003\u53e5\uff0c\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u3002\u57fa\u4e8eERRANT\u7f16\u8f91\u63d0\u53d6\u6846\u67b6\uff0c\u5b9e\u73b0\u8f6f\u7f16\u8f91\u5bf9\u9f50\u3002", "result": "\u5728SEEDA\u5143\u8bc4\u4f30\u5b9e\u9a8c\u4e2d\uff0cUOT-ERRANT\u63d0\u5347\u4e86\u8bc4\u4f30\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728+Fluency\u9886\u57df\uff08\u591a\u7f16\u8f91\u573a\u666f\uff09\u8868\u73b0\u7a81\u51fa\u3002\u65b9\u6cd5\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u8fd0\u8f93\u8ba1\u5212\u53ef\u89e3\u91ca\u4e3a\u8f6f\u7f16\u8f91\u5bf9\u9f50\u3002", "conclusion": "UOT-ERRANT\u662f\u4e00\u79cd\u6709\u6548\u7684GEC\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u65e2\u80fd\u7528\u4e8e\u7cfb\u7edf\u6392\u540d\uff0c\u53c8\u80fd\u5206\u6790GEC\u7cfb\u7edf\u3002\u901a\u8fc7\u7f16\u8f91\u5411\u91cf\u548c\u6700\u4f18\u8fd0\u8f93\u89e3\u51b3\u4e86\u4f20\u7edf\u5d4c\u5165\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.05895", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05895", "abs": "https://arxiv.org/abs/2602.05895", "authors": ["Qi Li", "Karsten Berns"], "title": "Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools", "comment": "12 pages", "summary": "This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u540d\u4e49\u63a7\u5236\u5668\u548c\u5b66\u4e60\u7684\u6b8b\u5dee\u7b56\u7565\uff0c\u7528\u4e8e\u6db2\u538b\u88c5\u8f7d\u8d77\u91cd\u673a\u6267\u884c\u5783\u573e\u5bb9\u5668\u63d0\u5347\u4efb\u52a1\uff0c\u63d0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u57ce\u5e02\u73af\u5883\u4e2d\u5783\u573e\u5bb9\u5668\u56de\u6536\u4efb\u52a1\u7684\u5bb9\u5668\u63d0\u5347\u9636\u6bb5\u9762\u4e34\u4e25\u683c\u51e0\u4f55\u516c\u5dee\u8981\u6c42\uff0c\u9700\u8981\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6446\u52a8\u6291\u5236\uff0c\u800c\u4f20\u7edf\u63a7\u5236\u5668\u96be\u4ee5\u5904\u7406\u672a\u5efa\u6a21\u52a8\u6001\u548c\u53c2\u6570\u53d8\u5316\u3002", "method": "\u91c7\u7528\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u540d\u4e49\u63a7\u5236\u5668\u4f7f\u7528\u5bfc\u7eb3\u63a7\u5236\u8fdb\u884c\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6446\u9524\u611f\u77e5\u6446\u52a8\u963b\u5c3c\uff0c\u914d\u5408\u963b\u5c3c\u6700\u5c0f\u4e8c\u4e58\u9006\u8fd0\u52a8\u5b66\uff1bPPO\u8bad\u7ec3\u7684\u6b8b\u5dee\u7b56\u7565\u8865\u507f\u672a\u5efa\u6a21\u52a8\u6001\uff1b\u91c7\u7528\u968f\u673a\u5316\u521d\u59cb\u5316\u548c\u57df\u968f\u673a\u5316\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u5355\u72ec\u4f7f\u7528\u540d\u4e49\u63a7\u5236\u5668\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u51cf\u5c11\u4e86\u632f\u8361\u3001\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u63d0\u5347\u6210\u529f\u7387\u3002", "conclusion": "\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6db2\u538b\u8d77\u91cd\u673a\u5728\u4e25\u683c\u51e0\u4f55\u516c\u5dee\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4ece\u5934\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u5177\u6709\u66f4\u597d\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.05437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05437", "abs": "https://arxiv.org/abs/2602.05437", "authors": ["Basel Mousi", "Fahim Dalvi", "Shammur Chowdhury", "Firoj Alam", "Nadir Durrani"], "title": "Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.", "AI": {"tldr": "\u63d0\u51fa\u4e86M2CQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6587\u5316\u8bed\u5883\u4e0b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e2d\u4e1c\u548c\u5317\u975e\u5730\u533a\uff0c\u5e76\u63d0\u51fa\u4e86CounterFactual Hallucination Rate\u6307\u6807\u6765\u8861\u91cf\u6a21\u578b\u5728\u6b63\u786e\u56de\u7b54\u771f\u5b9e\u9648\u8ff0\u540e\u63a5\u53d7\u53cd\u4e8b\u5b9e\u9648\u8ff0\u7684\u503e\u5411\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u4e0a\u5408\u7406\u4f46\u89c6\u89c9\u4e0a\u9519\u8bef\u7684\u89e3\u91ca\u8fd9\u4e00\u5931\u8d25\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5728\u975e\u897f\u65b9\u8bed\u5883\u548c\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u8de8\u6587\u5316\u573a\u666f\u4e0b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86M2CQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6765\u81ea17\u4e2a\u4e2d\u4e1c\u548c\u5317\u975e\u56fd\u5bb6\u7684\u56fe\u50cf\uff0c\u914d\u4ee5\u82f1\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u53ca\u5176\u65b9\u8a00\u7684\u771f\u5b9e\u9648\u8ff0\u548c\u53cd\u4e8b\u5b9e\u9648\u8ff0\u3002\u63d0\u51fa\u4e86CounterFactual Hallucination Rate\u6307\u6807\uff0c\u8861\u91cf\u6a21\u578b\u5728\u6b63\u786e\u56de\u7b54\u771f\u5b9e\u9648\u8ff0\u540e\u63a5\u53d7\u53cd\u4e8b\u5b9e\u9648\u8ff0\u7684\u6982\u7387\u3002\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5728\u963f\u62c9\u4f2f\u8bed\u4e2d\uff0c\u7279\u522b\u662f\u65b9\u8a00\u4e2d\uff0cCFHR\u663e\u8457\u4e0a\u5347\uff0c\u5373\u4f7f\u771f\u5b9e\u9648\u8ff0\u7684\u51c6\u786e\u7387\u4ecd\u7136\u5f88\u9ad8\u3002\u63a8\u7406\u4f18\u5148\u7684\u63d0\u793a\u7b56\u7565\u4f1a\u6301\u7eed\u589e\u52a0\u53cd\u4e8b\u5b9e\u5e7b\u89c9\uff0c\u800c\u5148\u56de\u7b54\u540e\u89e3\u91ca\u7684\u7b56\u7565\u5219\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6587\u5316\u8bed\u5883\u4e0b\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6a21\u578b\u6539\u8fdb\u7b56\u7565\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2602.05922", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05922", "abs": "https://arxiv.org/abs/2602.05922", "authors": ["Aziz Mohamed Mili", "Louis Catar", "Paul G\u00e9rard", "Ilyass Tabiai", "David St-Onge"], "title": "From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits", "comment": null, "summary": "Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u5de5\u5177\u94fe\uff0c\u5c06\u5b9e\u9a8c\u5ba4\u51b2\u51fb\u6d4b\u8bd5\u8f6c\u5316\u4e3a\u53ef\u90e8\u7f72\u7684\u65e0\u4eba\u673a\u5b89\u5168\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u5ba4\u5185\u5fae\u98de\u884c\u5668\u8fd1\u4eba\u64cd\u4f5c\u7684\u5b89\u5168\u7ba1\u7406", "motivation": "\u5ba4\u5185\u5fae\u98de\u884c\u5668\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u9700\u8981\u63a5\u8fd1\u4eba\u5458\u7684\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u57fa\u4e8e\u5b9e\u6d4b\u51b2\u51fb\u98ce\u9669\u8c03\u6574\u8fd0\u52a8\u9650\u5236\u7684\u5b9e\u7528\u65b9\u6cd5", "method": "1) \u8bbe\u8ba1\u7d27\u51d1\u53ef\u590d\u5236\u7684\u51b2\u51fb\u6d4b\u8bd5\u88c5\u7f6e\u548c\u534f\u8bae\uff1b2) \u5efa\u7acb\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u5c06\u78b0\u649e\u524d\u901f\u5ea6\u6620\u5c04\u5230\u51b2\u91cf\u548c\u63a5\u89e6\u6301\u7eed\u65f6\u95f4\uff1b3) \u53d1\u5e03\u811a\u672c\u548cROS2\u8282\u70b9\u5728\u7ebf\u6267\u884c\u901f\u5ea6\u9650\u5236\u5e76\u8bb0\u5f55\u5408\u89c4\u6027", "result": "\u5728\u591a\u4e2a\u5546\u7528\u73b0\u6210\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u548c\u4ee3\u8868\u6027\u5ba4\u5185\u8d44\u4ea7\u4e0a\u9a8c\u8bc1\u4e86\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8bc1\u660e\u63a8\u5bfc\u51fa\u7684\u63a7\u5236\u5668\u5728\u6ee1\u8db3\u5b89\u5168\u5229\u76ca\u76f8\u5173\u8005\u6307\u5b9a\u7684\u529b\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u541e\u5410\u91cf", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ece\u5b9e\u6d4b\u51b2\u51fb\u5230\u8fd0\u884c\u65f6\u9650\u5236\u7684\u5b9e\u7528\u6865\u6881\uff0c\u5305\u62ec\u53ef\u5171\u4eab\u7684\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u53ef\u91cd\u590d\u6d41\u7a0b\uff0c\u56e2\u961f\u53ef\u91c7\u7528\u6b64\u65b9\u6cd5\u8ba4\u8bc1\u5ba4\u5185\u5fae\u98de\u884c\u5668\u8fd1\u4eba\u64cd\u4f5c\u7684\u5b89\u5168\u6027"}}
{"id": "2602.05444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05444", "abs": "https://arxiv.org/abs/2602.05444", "authors": ["Yao Zhou", "Zeen Song", "Wenwen Qiang", "Fengge Wu", "Shuyi Zhou", "Changwen Zheng", "Hui Xiong"], "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs", "comment": null, "summary": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.", "AI": {"tldr": "\u63d0\u51faCFA\u00b2\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u56e0\u679c\u524d\u95e8\u51c6\u5219\u5265\u79bb\u5b89\u5168\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u8d8a\u72f1", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u901a\u5e38\u4f5c\u4e3a\u6f5c\u5728\u5185\u90e8\u72b6\u6001\u8fd0\u884c\uff0c\u63a9\u76d6\u4e86\u6a21\u578b\u7684\u56fa\u6709\u80fd\u529b\u3002\u4ece\u56e0\u679c\u89c6\u89d2\u770b\uff0c\u5b89\u5168\u673a\u5236\u5982\u540c\u672a\u89c2\u6d4b\u7684\u6df7\u6742\u56e0\u5b50\uff0c\u963b\u788d\u4e86\u6a21\u578b\u771f\u5b9e\u80fd\u529b\u7684\u5c55\u73b0\u3002", "method": "\u5c06\u5b89\u5168\u673a\u5236\u5efa\u6a21\u4e3a\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u5b50\uff0c\u5229\u7528Pearl\u524d\u95e8\u51c6\u5219\u5207\u65ad\u6df7\u6742\u5173\u8054\u3002\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7269\u7406\u5265\u79bb\u9632\u5fa1\u76f8\u5173\u7279\u5f81\uff0c\u5206\u79bb\u6838\u5fc3\u4efb\u52a1\u610f\u56fe\u3002\u5c06\u8ba1\u7b97\u6602\u8d35\u7684\u8fb9\u7f18\u5316\u7b80\u5316\u4e3a\u786e\u5b9a\u6027\u5e72\u9884\uff0c\u964d\u4f4e\u63a8\u7406\u590d\u6742\u5ea6\u3002", "result": "CFA\u00b2\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4e3a\u8d8a\u72f1\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u524d\u95e8\u8c03\u6574\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5265\u79bbLLM\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\uff0c\u63ed\u793a\u6a21\u578b\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u4e3a\u8d8a\u72f1\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u7684\u56e0\u679c\u89c6\u89d2\u548c\u9ad8\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.06001", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06001", "abs": "https://arxiv.org/abs/2602.06001", "authors": ["Carolina Higuera", "Sergio Arnaud", "Byron Boots", "Mustafa Mukadam", "Francois Robert Hogan", "Franziska Meier"], "title": "Visuo-Tactile World Models", "comment": "Preprint", "summary": "We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.", "AI": {"tldr": "\u63d0\u51fa\u591a\u4efb\u52a1\u89c6\u89c9-\u89e6\u89c9\u4e16\u754c\u6a21\u578bVT-WM\uff0c\u901a\u8fc7\u89e6\u89c9\u63a8\u7406\u6355\u6349\u63a5\u89e6\u7269\u7406\u7279\u6027\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u63d0\u5347\u7269\u7406\u4fdd\u771f\u5ea6\u548c\u89c4\u5212\u6027\u80fd", "motivation": "\u7eaf\u89c6\u89c9\u6a21\u578b\u5728\u906e\u6321\u6216\u63a5\u89e6\u72b6\u6001\u6a21\u7cca\u65f6\u5bb9\u6613\u51fa\u73b0\u7269\u4f53\u6d88\u5931\u3001\u77ac\u79fb\u6216\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u7b49\u5931\u8d25\u6a21\u5f0f\uff0c\u9700\u8981\u7ed3\u5408\u89e6\u89c9\u4f20\u611f\u6765\u66f4\u597d\u5730\u7406\u89e3\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92", "method": "\u5f00\u53d1\u591a\u4efb\u52a1\u89c6\u89c9-\u89e6\u89c9\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u89e6\u89c9\u4f20\u611f\u8865\u5145\u89c6\u89c9\u4fe1\u606f\uff0c\u5728\u591a\u4e2a\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b66\u4e60\u63a5\u89e6\u52a8\u529b\u5b66", "result": "\u5728\u81ea\u56de\u5f52\u63a8\u6f14\u4e2d\uff0c\u7269\u4f53\u6301\u4e45\u6027\u4fdd\u6301\u63d0\u534733%\uff0c\u8fd0\u52a8\u89c4\u5f8b\u7b26\u5408\u5ea6\u63d0\u534729%\uff1b\u96f6\u6837\u672c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u6210\u529f\u7387\u6700\u9ad8\u63d0\u534735%\uff1b\u80fd\u591f\u6709\u6548\u9002\u5e94\u65b0\u4efb\u52a1", "conclusion": "VT-WM\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u4f20\u611f\u663e\u8457\u63d0\u5347\u4e86\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u7269\u7406\u4fdd\u771f\u5ea6\u548c\u89c4\u5212\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u63a5\u89e6\u52a8\u529b\u5b66\u5b66\u4e60\u7684\u4e0b\u6e38\u9002\u5e94\u80fd\u529b"}}
{"id": "2602.05447", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05447", "abs": "https://arxiv.org/abs/2602.05447", "authors": ["Damon McMillan"], "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale", "comment": "8 pages, 7 figures, 10 tables, 26 references", "summary": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.\n  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.\n  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.", "AI": {"tldr": "\u9488\u5bf9LLM\u4ee3\u7406\u5728\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7814\u7a76\uff0c\u901a\u8fc7SQL\u751f\u6210\u4efb\u52a1\u8bc4\u4f30\u4e8611\u4e2a\u6a21\u578b\u30014\u79cd\u683c\u5f0f\u548c\u4e0d\u540c\u89c4\u6a21\u6a21\u5f0f\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u67b6\u6784\u9009\u62e9\u9700\u6839\u636e\u6a21\u578b\u80fd\u529b\u5b9a\u5236\u800c\u975e\u901a\u7528\u6700\u4f73\u5b9e\u8df5\u3002", "motivation": "LLM\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7\u7f16\u7a0b\u63a5\u53e3\u64cd\u4f5c\u5916\u90e8\u7cfb\u7edf\uff0c\u4f46\u5b9e\u8df5\u8005\u7f3a\u4e4f\u5173\u4e8e\u5982\u4f55\u6784\u5efa\u8fd9\u4e9b\u4ee3\u7406\u6240\u6d88\u8d39\u4e0a\u4e0b\u6587\u7684\u5b9e\u8bc1\u6307\u5bfc\u3002\u4f7f\u7528SQL\u751f\u6210\u4f5c\u4e3a\u7f16\u7a0b\u4ee3\u7406\u64cd\u4f5c\u7684\u4ee3\u7406\u4efb\u52a1\uff0c\u7814\u7a76\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3002", "method": "\u4f7f\u7528SQL\u751f\u6210\u4e3a\u4ee3\u7406\u4efb\u52a1\uff0c\u8fdb\u884c\u4e869,649\u4e2a\u5b9e\u9a8c\uff0c\u6db5\u76d611\u4e2a\u6a21\u578b\u30014\u79cd\u683c\u5f0f\uff08YAML\u3001Markdown\u3001JSON\u3001TOON\uff09\u548c\u4ece10\u523010,000\u4e2a\u8868\u7684\u6a21\u5f0f\u3002\u6bd4\u8f83\u4e86\u6587\u4ef6\u5f0f\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e0e\u76f4\u63a5\u4e0a\u4e0b\u6587\u4f20\u9012\u7684\u6548\u679c\u3002", "result": "1) \u67b6\u6784\u9009\u62e9\u4f9d\u8d56\u6a21\u578b\uff1a\u6587\u4ef6\u5f0f\u68c0\u7d22\u63d0\u5347\u524d\u6cbf\u6a21\u578b\u51c6\u786e\u7387(+2.7%)\u4f46\u5bf9\u5f00\u6e90\u6a21\u578b\u6709\u5bb3(-7.7%)\uff1b2) \u683c\u5f0f\u5bf9\u603b\u4f53\u51c6\u786e\u7387\u65e0\u663e\u8457\u5f71\u54cd\uff1b3) \u6a21\u578b\u80fd\u529b\u662f\u4e3b\u5bfc\u56e0\u7d20\uff0c\u524d\u6cbf\u4e0e\u5f00\u6e90\u6a21\u578b\u670921%\u51c6\u786e\u7387\u5dee\u8ddd\uff1b4) \u6587\u4ef6\u539f\u751f\u4ee3\u7406\u53ef\u6269\u5c55\u523010,000\u4e2a\u8868\uff1b5) \u6587\u4ef6\u5927\u5c0f\u4e0d\u80fd\u9884\u6d4b\u8fd0\u884c\u65f6\u6548\u7387\u3002", "conclusion": "\u4e3a\u90e8\u7f72LLM\u4ee3\u7406\u5230\u7ed3\u6784\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u6307\u5bfc\uff0c\u8868\u660e\u67b6\u6784\u51b3\u7b56\u5e94\u6839\u636e\u6a21\u578b\u80fd\u529b\u5b9a\u5236\uff0c\u800c\u975e\u5047\u8bbe\u5b58\u5728\u901a\u7528\u6700\u4f73\u5b9e\u8df5\u3002\u6a21\u578b\u80fd\u529b\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u683c\u5f0f\u9009\u62e9\u76f8\u5bf9\u6b21\u8981\u3002"}}
{"id": "2602.06038", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.06038", "abs": "https://arxiv.org/abs/2602.06038", "authors": ["Xiaopan Zhang", "Zejin Wang", "Zhixu Li", "Jianpeng Yao", "Jiachen Li"], "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction", "comment": "IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/", "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCommCP\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u591a\u4efb\u52a1\u5177\u8eab\u95ee\u7b54\u95ee\u9898\uff0c\u901a\u8fc7LLM\u548c\u4fdd\u5f62\u9884\u6d4b\u6765\u63d0\u5347\u673a\u5668\u4eba\u95f4\u7684\u901a\u4fe1\u6548\u7387\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u9700\u8981\u591a\u4e2a\u5f02\u6784\u673a\u5668\u4eba\u534f\u540c\u5b8c\u6210\u4eba\u7c7b\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u591a\u667a\u80fd\u4f53\u4fe1\u606f\u6536\u96c6\u8fc7\u7a0b\u7684\u7cfb\u7edf\u5904\u7406\uff0c\u7279\u522b\u662f\u5728\u5177\u8eab\u95ee\u7b54\u573a\u666f\u4e2d\u5982\u4f55\u6709\u6548\u901a\u4fe1\u4ee5\u907f\u514d\u5197\u4f59\u3002", "method": "\u63d0\u51faCommCP\u6846\u67b6\uff1a\u57fa\u4e8eLLM\u7684\u53bb\u4e2d\u5fc3\u5316\u901a\u4fe1\u6846\u67b6\uff0c\u91c7\u7528\u4fdd\u5f62\u9884\u6d4b\u6821\u51c6\u751f\u6210\u7684\u6d88\u606f\uff0c\u51cf\u5c11\u63a5\u6536\u8005\u5206\u5fc3\u5e76\u63d0\u5347\u901a\u4fe1\u53ef\u9760\u6027\u3002\u540c\u65f6\u521b\u5efa\u4e86MM-EQA\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCommCP\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u63a2\u7d22\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u534f\u540c\u4fe1\u606f\u6536\u96c6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9760\u7684\u901a\u4fe1\u673a\u5236\u63d0\u5347\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2602.05471", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05471", "abs": "https://arxiv.org/abs/2602.05471", "authors": ["Md. Mithun Hossaina", "Mashary N. Alrasheedy", "Nirban Bhowmick", "Shamim Forhad", "Md. Shakil Hossain", "Sudipto Chaki", "Md Shafiqul Islam"], "title": "Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision", "comment": null, "summary": "Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.", "AI": {"tldr": "\u63d0\u51faReasoning under Ambiguity\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u591a\u8bed\u8a00\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u60c5\u611f\u6b67\u4e49\u548c\u4e0d\u5b8c\u6574\u76d1\u7763\u95ee\u9898\uff0c\u5728\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u77e5\u8bc6\u7684\u591a\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u9762\u4e34\u60c5\u611f\u6b67\u4e49\u548c\u4e0d\u5b8c\u6574\u76d1\u7763\u7684\u6311\u6218\u3002\u6587\u672c\u60c5\u611f\u8bc6\u522b\u672c\u8d28\u4e0a\u662f\u6a21\u7cca\u7684\uff0c\u56e0\u4e3a\u591a\u79cd\u60c5\u611f\u72b6\u6001\u5e38\u540c\u65f6\u51fa\u73b0\uff0c\u4e14\u60c5\u611f\u6807\u6ce8\u7ecf\u5e38\u7f3a\u5931\u6216\u4e0d\u4e00\u81f4\u3002\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6807\u7b7e\u5b8c\u5168\u53ef\u89c2\u5bdf\uff0c\u4f7f\u7528\u786e\u5b9a\u6027\u5b66\u4e60\u76ee\u6807\uff0c\u5728\u90e8\u5206\u76d1\u7763\u4e0b\u4f1a\u5bfc\u81f4\u5b66\u4e60\u504f\u5dee\u548c\u4e0d\u53ef\u9760\u9884\u6d4b\u3002", "method": "\u63d0\u51faReasoning under Ambiguity\u6846\u67b6\uff1a1) \u4f7f\u7528\u5171\u4eab\u591a\u8bed\u8a00\u7f16\u7801\u5668\u914d\u5408\u8bed\u8a00\u7279\u5b9a\u4f18\u5316\uff1b2) \u57fa\u4e8e\u71b5\u7684\u6b67\u4e49\u52a0\u6743\u673a\u5236\uff0c\u964d\u4f4e\u9ad8\u6b67\u4e49\u8bad\u7ec3\u5b9e\u4f8b\u7684\u6743\u91cd\u800c\u975e\u5c06\u7f3a\u5931\u6807\u7b7e\u89c6\u4e3a\u8d1f\u8bc1\u636e\uff1b3) \u7ed3\u5408\u63a9\u7801\u611f\u77e5\u76ee\u6807\u548c\u6b63-\u672a\u6807\u8bb0\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u90e8\u5206\u76d1\u7763\u4e0b\u7684\u9c81\u68d2\u5b66\u4e60\u3002", "result": "\u5728\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u60c5\u611f\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u5bf9\u6807\u6ce8\u7a00\u758f\u6027\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u5b66\u4e60\u4e0e\u6807\u6ce8\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u591a\u8bed\u8a00\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u4e2d\u7684\u60c5\u611f\u6b67\u4e49\u548c\u4e0d\u5b8c\u6574\u76d1\u7763\u95ee\u9898\uff0c\u4e3a\u77e5\u8bc6\u7cfb\u7edf\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2602.05493", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.05493", "abs": "https://arxiv.org/abs/2602.05493", "authors": ["Bingru Li"], "title": "LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation", "comment": null, "summary": "Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.", "AI": {"tldr": "LinguistAgent\u662f\u4e00\u4e2a\u96c6\u6210\u5e73\u53f0\uff0c\u91c7\u7528\u53cd\u601d\u6027\u591a\u6a21\u578b\u67b6\u6784\u548c\u53cc\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5fae\u8c03\u4e09\u79cd\u8303\u5f0f\uff0c\u81ea\u52a8\u5316\u8bed\u8a00\u5b66\u6807\u6ce8\u4efb\u52a1\uff08\u4ee5\u9690\u55bb\u8bc6\u522b\u4e3a\u4f8b\uff09\u3002", "motivation": "\u4eba\u6587\u793e\u79d1\u9886\u57df\u7684\u6570\u636e\u6807\u6ce8\uff08\u5c24\u5176\u662f\u590d\u6742\u8bed\u4e49\u4efb\u52a1\u5982\u9690\u55bb\u8bc6\u522b\uff09\u5b58\u5728\u663e\u8457\u74f6\u9888\uff0c\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u7406\u8bba\u80fd\u529b\u4e0e\u5b9e\u9645\u7814\u7a76\u5e94\u7528\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u5f00\u53d1LinguistAgent\u5e73\u53f0\uff0c\u91c7\u7528\u53cd\u601d\u6027\u591a\u6a21\u578b\u67b6\u6784\uff0c\u5305\u542b\u6807\u6ce8\u8005\u548c\u8bc4\u5ba1\u8005\u53cc\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6a21\u62df\u4e13\u4e1a\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\uff0c\u652f\u6301\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5fae\u8c03\u4e09\u79cd\u5b9e\u9a8c\u8303\u5f0f\u3002", "result": "\u4ee5\u9690\u55bb\u8bc6\u522b\u4efb\u52a1\u4e3a\u4f8b\uff0c\u5e73\u53f0\u63d0\u4f9b\u5b9e\u65f6token\u7ea7\u8bc4\u4f30\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\uff09\uff0c\u4e0e\u4eba\u7c7b\u9ec4\u91d1\u6807\u51c6\u5bf9\u6bd4\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "LinguistAgent\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u81ea\u52a8\u5316\u8bed\u8a00\u5b66\u6807\u6ce8\u5e73\u53f0\uff0c\u586b\u8865\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7406\u8bba\u80fd\u529b\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.05495", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05495", "abs": "https://arxiv.org/abs/2602.05495", "authors": ["Chenhang Cui", "Binyun Yang", "Fei Shen", "Yuxin Chen", "Jingnan Zheng", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Transport and Merge: Cross-Architecture Merging for Large Language Models", "comment": null, "summary": "Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8de8\u67b6\u6784\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u89e3\u51b3\u5927\u6a21\u578b\u5411\u5f02\u6784\u5c0f\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898", "motivation": "\u73b0\u5b9e\u90e8\u7f72\u5e38\u4f7f\u7528\u5c0f\u6a21\u578b\u6216\u4f4e\u8d44\u6e90\u6570\u636e\u8bad\u7ec3\uff0c\u9700\u8981\u4ece\u5927\u6a21\u578b\u5411\u5f02\u6784\u5c0f\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u8981\u6c42\u67b6\u6784\u517c\u5bb9", "method": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u6fc0\u6d3b\u503c\uff0c\u63a8\u65ad\u5f02\u6784\u6a21\u578b\u95f4\u7684\u795e\u7ecf\u5143\u5bf9\u5e94\u5173\u7cfb\uff0c\u5229\u7528\u4f20\u8f93\u8ba1\u5212\u6307\u5bfc\u6743\u91cd\u7a7a\u95f4\u878d\u5408\uff0c\u4ec5\u9700\u5c11\u91cf\u8f93\u5165\u6570\u636e", "result": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e13\u95e8\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u76ee\u6807\u6a21\u578b\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb", "conclusion": "\u63d0\u51fa\u7684\u8de8\u67b6\u6784\u5408\u5e76\u6846\u67b6\u80fd\u6709\u6548\u5b9e\u73b0\u4ece\u9ad8\u8d44\u6e90\u5927\u6a21\u578b\u5411\u4f4e\u8d44\u6e90\u5f02\u6784\u5c0f\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb"}}
{"id": "2602.05512", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05512", "abs": "https://arxiv.org/abs/2602.05512", "authors": ["Larissa Pusch", "Alexandre Courtiol", "Tim Conrad"], "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering", "comment": null, "summary": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u8ba9LLM\u751f\u6210\u548c\u89e3\u91caCypher\u56fe\u67e5\u8be2\uff0c\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fed\u4ee3\u4f18\u5316\uff0c\u63d0\u5347\u5bf9\u77e5\u8bc6\u56fe\u8c31\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e25\u8c28\u6027\u3002", "motivation": "LLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u5b58\u5728\u5e7b\u89c9\u3001\u4fe1\u606f\u8fc7\u65f6\u548c\u53ef\u89e3\u91ca\u6027\u6709\u9650\u7684\u95ee\u9898\uff1b\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u591a\u8df3\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u77e5\u8bc6\u56fe\u8c31\u652f\u6301\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u7684\u67e5\u8be2\uff0c\u4f46\u9700\u8981\u638c\u63e1\u67e5\u8be2\u8bed\u8a00\u3002", "method": "\u5f15\u5165\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0cLLM\u751f\u6210\u548c\u89e3\u91caCypher\u56fe\u67e5\u8be2\uff0c\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fed\u4ee3\u4f18\u5316\u67e5\u8be2\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u56fe\u8c31\u4e0a\u5e94\u7528\u8be5\u6846\u67b6\u3002", "result": "\u6838\u5fc3\u5b9a\u91cf\u8bc4\u4f30\u662f\u5728\u5408\u6210\u7535\u5f71\u77e5\u8bc6\u56fe\u8c31\u4e0a\u768490\u4e2a\u67e5\u8be2\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8861\u91cf\u67e5\u8be2\u89e3\u91ca\u8d28\u91cf\u548c\u6545\u969c\u68c0\u6d4b\uff1b\u8865\u5145\u4e86\u4e24\u4e2a\u5c0f\u578b\u771f\u5b9e\u573a\u666f\u67e5\u8be2\u751f\u6210\u5b9e\u9a8c\uff08Hyena KG\u548cMaRDI KG\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u5bf9\u590d\u6742\u6570\u636e\u96c6\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e25\u8c28\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u9886\u57df\u53d8\u5316\u7684\u6d1e\u5bdf\u3002"}}
{"id": "2602.05547", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05547", "abs": "https://arxiv.org/abs/2602.05547", "authors": ["Shyam Sundhar Ramesh", "Xiaotong Ji", "Matthieu Zimmer", "Sangwoong Yoon", "Zhiyong Wang", "Haitham Bou Ammar", "Aurelien Lucchi", "Ilija Bogunovic"], "title": "Multi-Task GRPO: Reliable LLM Reasoning Across Tasks", "comment": "Preprint", "summary": "RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.", "AI": {"tldr": "MT-GRPO\uff1a\u4e00\u79cd\u591a\u4efb\u52a1GRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u6743\u91cd\u8c03\u6574\u548c\u6bd4\u4f8b\u4fdd\u6301\u91c7\u6837\u5668\uff0c\u89e3\u51b3\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6700\u5dee\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u9700\u8981\u6a21\u578b\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u90fd\u6709\u53ef\u9760\u8868\u73b0\uff0c\u4f46\u4f20\u7edf\u7684\u591a\u4efb\u52a1GRPO\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u4e0d\u5e73\u8861\u7684\u7ed3\u679c\u2014\u2014\u67d0\u4e9b\u4efb\u52a1\u4e3b\u5bfc\u4f18\u5316\u800c\u5176\u4ed6\u4efb\u52a1\u505c\u6ede\u4e0d\u524d\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u4efb\u52a1\u4e2d\u63d0\u793a\u4ea7\u751f\u96f6\u4f18\u52bf\uff08\u4ece\u800c\u96f6\u68af\u5ea6\uff09\u7684\u9891\u7387\u5dee\u5f02\u5f88\u5927\uff0c\u8fdb\u4e00\u6b65\u626d\u66f2\u4e86\u4f18\u5316\u4fe1\u53f7\u7684\u6709\u6548\u8d21\u732e\u3002", "method": "\u63d0\u51faMT-GRPO\u7b97\u6cd5\uff1a1) \u52a8\u6001\u8c03\u6574\u4efb\u52a1\u6743\u91cd\uff0c\u660e\u786e\u4f18\u5316\u6700\u5dee\u4efb\u52a1\u6027\u80fd\u5e76\u4fc3\u8fdb\u8de8\u4efb\u52a1\u5e73\u8861\u8fdb\u5c55\uff1b2) \u5f15\u5165\u6bd4\u4f8b\u4fdd\u6301\u91c7\u6837\u5668\uff0c\u786e\u4fdd\u4efb\u52a1\u7ea7\u7b56\u7565\u68af\u5ea6\u53cd\u6620\u8c03\u6574\u540e\u7684\u6743\u91cd\u3002", "result": "\u57283\u4efb\u52a1\u548c9\u4efb\u52a1\u8bbe\u7f6e\u4e2d\uff0cMT-GRPO\u5728\u6700\u5dee\u4efb\u52a1\u51c6\u786e\u7387\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u76f8\u6bd4\u6807\u51c6GRPO\u548cDAPO\uff0c\u5728\u6700\u5dee\u4efb\u52a1\u6027\u80fd\u4e0a\u5206\u522b\u83b7\u5f9716-28%\u548c6%\u7684\u7edd\u5bf9\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002\u57283\u4efb\u52a1\u8bbe\u7f6e\u4e2d\uff0c\u8fbe\u523050%\u6700\u5dee\u4efb\u52a1\u51c6\u786e\u7387\u6240\u9700\u7684\u8bad\u7ec3\u6b65\u6570\u51cf\u5c1150%\u3002", "conclusion": "MT-GRPO\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u6743\u91cd\u8c03\u6574\u548c\u6bd4\u4f8b\u4fdd\u6301\u91c7\u6837\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u4efb\u52a1\u7684\u6700\u5dee\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u4efb\u52a1\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05633", "abs": "https://arxiv.org/abs/2602.05633", "authors": ["Rui Jia", "Ruiyi Lan", "Fengrui Liu", "Zhongxiang Dai", "Bo Jiang", "Jing Shao", "Jingyuan Chen", "Guandong Xu", "Fei Wu", "Min Zhang"], "title": "CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models", "comment": null, "summary": "Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.", "AI": {"tldr": "CASTLE\u662f\u4e00\u4e2a\u57fa\u4e8e\u6559\u80b2\u7406\u8bba\u6784\u5efa\u7684\u4e2a\u6027\u5316\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\uff0c\u8986\u76d615\u79cd\u6559\u80b2\u5b89\u5168\u98ce\u9669\u548c14\u79cd\u5b66\u751f\u5c5e\u6027\uff0c\u5305\u542b92,908\u4e2a\u53cc\u8bed\u573a\u666f\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4e2a\u6027\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4e2a\u6027\u5316\u6559\u80b2\u4e2d\u5b58\u5728\u540c\u8d28\u5316\u54cd\u5e94\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u5b66\u751f\u8ba4\u77e5\u548c\u5fc3\u7406\u7684\u5f02\u8d28\u6027\uff0c\u53ef\u80fd\u5bf9\u5f31\u52bf\u7fa4\u4f53\u9020\u6210\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u504f\u89c1\u6216\u6bd2\u6027\u7b49\u4e0a\u4e0b\u6587\u65e0\u5173\u6307\u6807\uff0c\u65e0\u6cd5\u6355\u6349\u540c\u4e00\u54cd\u5e94\u5bf9\u4e0d\u540c\u5b66\u751f\u5c5e\u6027\u53ef\u80fd\u9020\u6210\u7684\u4e0d\u540c\u5371\u5bb3\u3002", "method": "\u63d0\u51fa\"\u5b66\u751f\u5b9a\u5236\u4e2a\u6027\u5316\u5b89\u5168\"\u6982\u5ff5\uff0c\u6784\u5efaCASTLE\u57fa\u51c6\uff0c\u5305\u542b15\u79cd\u6559\u80b2\u5b89\u5168\u98ce\u9669\u548c14\u79cd\u5b66\u751f\u5c5e\u6027\uff0c\u517192,908\u4e2a\u53cc\u8bed\u573a\u666f\u3002\u8bbe\u8ba1\u4e09\u4e2a\u8bc4\u4f30\u6307\u6807\uff1a\u98ce\u9669\u654f\u611f\u6027\uff08\u68c0\u6d4b\u98ce\u9669\u80fd\u529b\uff09\u3001\u60c5\u611f\u5171\u60c5\uff08\u8bc6\u522b\u5b66\u751f\u72b6\u6001\u80fd\u529b\uff09\u548c\u5b66\u751f\u5bf9\u9f50\uff08\u54cd\u5e94\u4e0e\u5b66\u751f\u5c5e\u6027\u5339\u914d\u5ea6\uff09\u3002", "result": "\u572818\u4e2a\u6700\u5148\u8fdb\u7684LLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6240\u6709\u6a21\u578b\u57285\u5206\u5236\u4e0b\u7684\u5e73\u5747\u5b89\u5168\u8bc4\u5206\u4f4e\u4e8e2.3\u5206\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5b89\u5168\u4fdd\u969c\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "CASTLE\u57fa\u51c6\u63ed\u793a\u4e86LLM\u5728\u4e2a\u6027\u5316\u6559\u80b2\u5b89\u5168\u65b9\u9762\u7684\u4e25\u91cd\u7f3a\u9677\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u5b66\u751f\u5f02\u8d28\u6027\u7684\u4e2a\u6027\u5316\u5b89\u5168\u673a\u5236\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u6559\u80b2AI\u7684\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002"}}
{"id": "2602.05648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05648", "abs": "https://arxiv.org/abs/2602.05648", "authors": ["Giuseppe Samo", "Paola Merlo"], "title": "Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew", "comment": "13 pages, 7 figures, to appear as proceedings of the SIGTURK 2026 Workshop", "summary": "We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.", "AI": {"tldr": "\u7814\u7a76Transformer\u6a21\u578b\u5982\u4f55\u8868\u793a\u571f\u8033\u5176\u8bed\u548c\u73b0\u4ee3\u5e0c\u4f2f\u6765\u8bed\u7684\u590d\u6742\u52a8\u8bcd\u8303\u5f0f\uff0c\u91cd\u70b9\u5173\u6ce8\u5206\u8bcd\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u80fd\u529b\u3002\u571f\u8033\u5176\u8bed\u56e0\u5176\u900f\u660e\u7684\u5f62\u6001\u6807\u8bb0\uff0c\u5355\u8bed\u548c\u591a\u8bed\u6a21\u578b\u90fd\u80fd\u6210\u529f\uff1b\u800c\u5e0c\u4f2f\u6765\u8bed\u4e2d\uff0c\u5355\u8bed\u548c\u591a\u8bed\u6a21\u578b\u8868\u73b0\u4e0d\u540c\uff0c\u9700\u8981\u7279\u5b9a\u5206\u8bcd\u7b56\u7565\u624d\u80fd\u5904\u7406\u5176\u975e\u8fde\u63a5\u6027\u5f62\u6001\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3Transformer\u6a21\u578b\u5982\u4f55\u8868\u793a\u5177\u6709\u590d\u6742\u5f62\u6001\u53d8\u5316\u7684\u8bed\u8a00\uff08\u7279\u522b\u662f\u571f\u8033\u5176\u8bed\u548c\u73b0\u4ee3\u5e0c\u4f2f\u6765\u8bed\uff09\uff0c\u4ee5\u53ca\u5206\u8bcd\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6355\u6349\u8fd9\u4e9b\u8bed\u8a00\u5f62\u6001\u7279\u5f81\u7684\u80fd\u529b\u3002\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5904\u7406\u4e0d\u540c\u8bed\u8a00\u7c7b\u578b\u7684\u5f62\u6001\u7ed3\u6784\u3002", "method": "\u4f7f\u7528Blackbird Language Matrices\u4efb\u52a1\u5728\u81ea\u7136\u6570\u636e\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u5bf9\u6bd4\u4e0d\u540c\u5206\u8bcd\u7b56\u7565\uff1a\u539f\u5b50\u5206\u8bcd\u3001\u5c0f\u5b50\u8bcd\u5355\u5143\u5206\u8bcd\u3001\u5b57\u7b26\u7ea7\u5206\u8bcd\u548c\u8bed\u7d20\u611f\u77e5\u5206\u8bcd\u3002\u6bd4\u8f83\u5355\u8bed\u6a21\u578b\u548c\u591a\u8bed\u6a21\u578b\u5728\u571f\u8033\u5176\u8bed\u548c\u73b0\u4ee3\u5e0c\u4f2f\u6765\u8bed\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u5bf9\u4e8e\u571f\u8033\u5176\u8bed\uff08\u5177\u6709\u900f\u660e\u5f62\u6001\u6807\u8bb0\uff09\uff0c\u65e0\u8bba\u4f7f\u7528\u539f\u5b50\u5206\u8bcd\u8fd8\u662f\u5c0f\u5b50\u8bcd\u5355\u5143\u5206\u8bcd\uff0c\u5355\u8bed\u548c\u591a\u8bed\u6a21\u578b\u90fd\u80fd\u6210\u529f\u3002\u5bf9\u4e8e\u5e0c\u4f2f\u6765\u8bed\uff0c\u5355\u8bed\u548c\u591a\u8bed\u6a21\u578b\u8868\u73b0\u4e0d\u540c\uff1a\u4f7f\u7528\u5b57\u7b26\u7ea7\u5206\u8bcd\u7684\u591a\u8bed\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u8bed\u8a00\u7684\u975e\u8fde\u63a5\u6027\u5f62\u6001\uff0c\u4f46\u4f7f\u7528\u8bed\u7d20\u611f\u77e5\u5206\u8bcd\u7684\u5e0c\u4f2f\u6765\u8bed\u5355\u8bed\u6a21\u578b\u8868\u73b0\u826f\u597d\u3002\u6240\u6709\u6a21\u578b\u5728\u66f4\u5408\u6210\u5316\u7684\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u90fd\u6709\u63d0\u5347\u3002", "conclusion": "\u5206\u8bcd\u7b56\u7565\u5bf9Transformer\u6a21\u578b\u8868\u793a\u590d\u6742\u5f62\u6001\u8303\u5f0f\u7684\u80fd\u529b\u6709\u91cd\u8981\u5f71\u54cd\u3002\u5bf9\u4e8e\u5177\u6709\u900f\u660e\u5f62\u6001\u7684\u8bed\u8a00\uff08\u5982\u571f\u8033\u5176\u8bed\uff09\uff0c\u591a\u79cd\u5206\u8bcd\u7b56\u7565\u90fd\u6709\u6548\uff1b\u4f46\u5bf9\u4e8e\u5177\u6709\u975e\u8fde\u63a5\u6027\u5f62\u6001\u7684\u8bed\u8a00\uff08\u5982\u5e0c\u4f2f\u6765\u8bed\uff09\uff0c\u9700\u8981\u7279\u5b9a\u7684\u8bed\u7d20\u611f\u77e5\u5206\u8bcd\u7b56\u7565\u624d\u80fd\u6709\u6548\u6355\u6349\u5176\u5f62\u6001\u7279\u5f81\u3002\u6a21\u578b\u6027\u80fd\u5728\u5408\u6210\u6570\u636e\u4e0a\u666e\u904d\u66f4\u597d\u3002"}}
{"id": "2602.05692", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05692", "abs": "https://arxiv.org/abs/2602.05692", "authors": ["Congbo Ma", "Yichun Zhang", "Yousef Al-Jazzazi", "Ahamed Foisal", "Laasya Sharma", "Yousra Sadqi", "Khaled Saleh", "Jihad Mallat", "Farah E. Shamout"], "title": "MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations", "comment": null, "summary": "Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.", "AI": {"tldr": "MedErrBench\u662f\u9996\u4e2a\u591a\u8bed\u8a00\u4e34\u5e8a\u6587\u672c\u9519\u8bef\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u7ea0\u6b63\u57fa\u51c6\uff0c\u6db5\u76d6\u82f1\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u548c\u4e2d\u6587\uff0c\u5305\u542b10\u79cd\u5e38\u89c1\u9519\u8bef\u7c7b\u578b\uff0c\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u533b\u7597\u9886\u57df\u7684\u6027\u80fd\u5e76\u4fc3\u8fdb\u66f4\u5b89\u5168\u7684AI\u533b\u7597\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u4e34\u5e8a\u6587\u672c\u4e2d\u7684\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff08\u5982\u8bef\u8bca\u6216\u9519\u8bef\u6cbb\u7597\u5efa\u8bae\uff09\uff0c\u800c\u968f\u7740LLMs\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u9700\u8981\u5168\u9762\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u57fa\u51c6\uff0c\u4f46\u76ee\u524d\u6b64\u7c7b\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u8bed\u5883\u4e0b\u3002", "method": "\u5728\u4e34\u5e8a\u4e13\u5bb6\u6307\u5bfc\u4e0b\u5f00\u53d1MedErrBench\u57fa\u51c6\uff0c\u6269\u5c55\u4e8610\u79cd\u5e38\u89c1\u9519\u8bef\u7c7b\u578b\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d6\u82f1\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u548c\u4e2d\u6587\u7684\u81ea\u7136\u4e34\u5e8a\u6848\u4f8b\uff0c\u7531\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u6807\u6ce8\u548c\u5ba1\u6838\u3002\u8bc4\u4f30\u4e86\u901a\u7528\u3001\u8bed\u8a00\u7279\u5b9a\u548c\u533b\u7597\u9886\u57df\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u4e2a\u4efb\u52a1\uff08\u9519\u8bef\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u7ea0\u6b63\uff09\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u73af\u5883\u4e2d\uff0c\u7a81\u663e\u4e86\u9700\u8981\u57fa\u4e8e\u4e34\u5e8a\u77e5\u8bc6\u4e14\u5177\u5907\u8bed\u8a00\u611f\u77e5\u80fd\u529b\u7684\u7cfb\u7edf\u3002\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u5df2\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "MedErrBench\u7684\u53d1\u5e03\u65e8\u5728\u63a8\u52a8\u591a\u8bed\u8a00\u4e34\u5e8aNLP\u53d1\u5c55\uff0c\u4fc3\u8fdb\u5168\u7403\u66f4\u5b89\u5168\u3001\u66f4\u516c\u5e73\u7684\u57fa\u4e8eAI\u7684\u533b\u7597\u4fdd\u5065\u3002\u6570\u636e\u96c6\u5df2\u5728\u8865\u5145\u6750\u6599\u4e2d\u63d0\u4f9b\uff0c\u533f\u540d\u7248\u672c\u53ef\u901a\u8fc7GitHub\u83b7\u53d6\u3002"}}
{"id": "2602.05694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05694", "abs": "https://arxiv.org/abs/2602.05694", "authors": ["Shuting Jiang", "Ran Song", "Yuxin Huang", "Yan Xiang", "Yantuan Xian", "Shengxiang Gao", "Zhengtao Yu"], "title": "Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation", "comment": "Accepted by AAAI 2026", "summary": "Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5171\u8bc6\u5bf9\u9f50\u795e\u7ecf\u5143\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u9886\u57df\u673a\u5668\u7ffb\u8bd1\uff0c\u901a\u8fc7\u6700\u5927\u5316\u795e\u7ecf\u5143\u884c\u4e3a\u4e0e\u9886\u57df\u7279\u5f81\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u9009\u62e9\u5173\u952e\u795e\u7ecf\u5143\uff0c\u6709\u6548\u7f13\u89e3\u53c2\u6570\u5e72\u6270\u548c\u9886\u57df\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9886\u57df\u9002\u5e94\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u7684\u591a\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\u5982\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5b58\u5728\u9886\u57df\u504f\u79fb\u3001\u53c2\u6570\u5e72\u6270\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u5143\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u795e\u7ecf\u5143\u884c\u4e3a\u4e0e\u9886\u57df\u7279\u5f81\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u8bc6\u522b\u548c\u66f4\u65b0\u5171\u8bc6\u5bf9\u9f50\u795e\u7ecf\u5143\uff0c\u8fd9\u4e9b\u795e\u7ecf\u5143\u80fd\u591f\u6355\u6349\u53ef\u6cdb\u5316\u7684\u7ffb\u8bd1\u6a21\u5f0f\u548c\u9886\u57df\u7279\u5b9a\u7ec6\u5fae\u5dee\u522b\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u795e\u7ecf\u5143\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u5bf9\u5341\u4e2a\u5fb7\u8bed-\u82f1\u8bed\u548c\u6c49\u8bed-\u82f1\u8bed\u7ffb\u8bd1\u9886\u57df\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5df2\u89c1\u548c\u672a\u89c1\u9886\u57df\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u57fa\u7ebf\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u548c\u5fae\u8c03\u5171\u8bc6\u5bf9\u9f50\u795e\u7ecf\u5143\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u53c2\u6570\u5e72\u6270\u548c\u9886\u57df\u7279\u5b9a\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4e3a\u591a\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05711", "abs": "https://arxiv.org/abs/2602.05711", "authors": ["Jingze Shi", "Zhangyang Peng", "Yizhang Zhu", "Yifan Wu", "Guang Liu", "Yuyu Luo"], "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.", "AI": {"tldr": "OmniMoE\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u5c06\u4e13\u5bb6\u7c92\u5ea6\u63a8\u5230\u6781\u81f4\uff08\u5411\u91cf\u7ea7\u539f\u5b50\u4e13\u5bb6\uff09\uff0c\u901a\u8fc7\u7b1b\u5361\u5c14\u79ef\u8def\u7531\u5668\u548c\u4e13\u5bb6\u4e2d\u5fc3\u8c03\u5ea6\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6MoE\u7684\u8def\u7531\u590d\u6742\u6027\u548c\u5185\u5b58\u8bbf\u95ee\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684MoE\u67b6\u6784\u5728\u4e13\u5bb6\u4e13\u4e1a\u5316\u7c92\u5ea6\u548c\u786c\u4ef6\u6267\u884c\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\u3002\u7c97\u7c92\u5ea6\u4e13\u5bb6\u6548\u7387\u9ad8\u4f46\u4e13\u4e1a\u5316\u6709\u9650\uff0c\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u4e13\u4e1a\u5316\u5f3a\u4f46\u8def\u7531\u590d\u6742\u3001\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u6781\u81f4\u4e13\u5bb6\u7c92\u5ea6\u53c8\u80fd\u4fdd\u6301\u9ad8\u6548\u6267\u884c\u7684\u65b9\u6cd5\u3002", "method": "1. \u5f15\u5165\u5411\u91cf\u7ea7\u539f\u5b50\u4e13\u5bb6\uff0c\u5b9e\u73b0\u5355\u5c42MoE\u5185\u7684\u53ef\u6269\u5c55\u8def\u7531\u548c\u6267\u884c\uff0c\u540c\u65f6\u4fdd\u7559\u5171\u4eab\u7684\u5bc6\u96c6MLP\u5206\u652f\u8fdb\u884c\u901a\u7528\u5904\u7406\u30022. \u7b1b\u5361\u5c14\u79ef\u8def\u7531\u5668\u5c06\u5927\u89c4\u6a21\u7d22\u5f15\u7a7a\u95f4\u5206\u89e3\uff0c\u5c06\u8def\u7531\u590d\u6742\u5ea6\u4eceO(N)\u964d\u4f4e\u5230O(\u221aN)\u30023. \u4e13\u5bb6\u4e2d\u5fc3\u8c03\u5ea6\u53cd\u8f6c\u6267\u884c\u987a\u5e8f\uff0c\u5c06\u5206\u6563\u7684\u5185\u5b58\u7ed1\u5b9a\u67e5\u627e\u8f6c\u6362\u4e3a\u9ad8\u6548\u7684\u5bc6\u96c6\u77e9\u9635\u8fd0\u7b97\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniMoE\uff0817\u4ebf\u6fc0\u6d3b\u53c2\u6570\uff09\u5b9e\u73b0\u4e8650.9%\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u7c97\u7c92\u5ea6\uff08\u5982DeepSeekMoE\uff09\u548c\u7ec6\u7c92\u5ea6\uff08\u5982PEER\uff09\u57fa\u7ebf\u3002\u5173\u952e\u7684\u662f\uff0c\u4e0ePEER\u76f8\u6bd4\uff0cOmniMoE\u5c06\u63a8\u7406\u5ef6\u8fdf\u4ece73ms\u964d\u4f4e\u52306.7ms\uff08\u52a0\u901f10.9\u500d\uff09\uff0c\u8bc1\u660e\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6MoE\u53ef\u4ee5\u65e2\u5feb\u901f\u53c8\u51c6\u786e\u3002", "conclusion": "OmniMoE\u901a\u8fc7\u7cfb\u7edf\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6MoE\u7684\u8def\u7531\u590d\u6742\u6027\u548c\u5185\u5b58\u8bbf\u95ee\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u6781\u81f4\u4e13\u5bb6\u7c92\u5ea6\u4e0e\u9ad8\u6548\u786c\u4ef6\u6267\u884c\u7684\u5e73\u8861\uff0c\u4e3a\u5927\u89c4\u6a21MoE\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05728", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05728", "abs": "https://arxiv.org/abs/2602.05728", "authors": ["Hao Yang", "Zhiyu Yang", "Xupeng Zhang", "Wei Wei", "Yunjie Zhang", "Lin Yang"], "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering", "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.\n  In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.\n  Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.", "AI": {"tldr": "CompactRAG\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u8df3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u77e5\u8bc6\u5e93\u91cd\u6784\u548c\u5728\u7ebf\u63a8\u7406\u89e3\u8026\uff0c\u663e\u8457\u51cf\u5c11LLM\u8c03\u7528\u6b21\u6570\u548ctoken\u6d88\u8017", "motivation": "\u73b0\u6709\u591a\u8df3RAG\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u4ea4\u66ff\u8fdb\u884c\u68c0\u7d22\u548c\u63a8\u7406\uff0c\u5bfc\u81f4\u91cd\u590d\u7684LLM\u8c03\u7528\u3001\u9ad8token\u6d88\u8017\u4ee5\u53ca\u8de8\u8df3\u5b9e\u4f53\u63a5\u5730\u4e0d\u7a33\u5b9a", "method": "1. \u79bb\u7ebf\u9636\u6bb5\uff1aLLM\u4e00\u6b21\u6027\u8bfb\u53d6\u8bed\u6599\u5e93\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u539f\u5b50QA\u77e5\u8bc6\u5e93\uff0c\u4ee5\u6700\u5c0f\u5316\u7684\u7ec6\u7c92\u5ea6\u95ee\u7b54\u5bf9\u8868\u793a\u77e5\u8bc6\uff1b2. \u5728\u7ebf\u9636\u6bb5\uff1a\u5c06\u590d\u6742\u67e5\u8be2\u5206\u89e3\u5e76\u91cd\u5199\u4ee5\u4fdd\u6301\u5b9e\u4f53\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u5bc6\u96c6\u68c0\u7d22\u548cRoBERTa\u7b54\u6848\u63d0\u53d6\u89e3\u51b3\uff0c\u6574\u4e2a\u63a8\u7406\u8fc7\u7a0bLLM\u4ec5\u8c03\u7528\u4e24\u6b21", "result": "\u5728HotpotQA\u30012WikiMultiHopQA\u548cMuSiQue\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCompactRAG\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u8fed\u4ee3\u5f0fRAG\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u4e86token\u6d88\u8017", "conclusion": "CompactRAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u5b9e\u7528\u7684\u591a\u8df3\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u79bb\u7ebf\u77e5\u8bc6\u5e93\u91cd\u6784\u548c\u5728\u7ebf\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u77e5\u8bc6\u8bed\u6599\u5e93\u63a8\u7406"}}
{"id": "2602.05758", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05758", "abs": "https://arxiv.org/abs/2602.05758", "authors": ["Bowen Ping", "Zijun Chen", "Yiyao Yu", "Tingfeng Hui", "Junchi Yan", "Baobao Chang"], "title": "LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards", "comment": null, "summary": "Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic \"Think-and-Read\" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.", "AI": {"tldr": "LongR\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\"\u601d\u8003-\u9605\u8bfb\"\u673a\u5236\u548c\u4e0a\u4e0b\u6587\u5bc6\u5ea6\u5956\u52b1\u6765\u63d0\u5347LLM\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u7684\u3001\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u8fd9\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u573a\u666f\u4e2d\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u7c97\u7c92\u5ea6\u7684\u5956\u52b1\u4fe1\u53f7\u4e0d\u8db3\u4ee5\u6709\u6548\u6307\u5bfc\u590d\u6742\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\uff08\u5982\u957f\u5bf9\u8bdd\u7406\u89e3\u548c\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\uff09\u4e0d\u4ec5\u9700\u8981\u5904\u7406\u5927\u91cf\u6807\u8bb0\uff0c\u8fd8\u9700\u8981\u8fdb\u884c\u4e25\u683c\u7684\u6f14\u7ece\u63a8\u7406\u3002", "method": "\u63d0\u51faLongR\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u52a8\u6001\u7684\"\u601d\u8003-\u9605\u8bfb\"\u673a\u5236\uff0c\u4ea4\u66ff\u8fdb\u884c\u63a8\u7406\u548c\u6587\u6863\u67e5\u9605\uff1b2\uff09\u57fa\u4e8e\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u7684\u4e0a\u4e0b\u6587\u5bc6\u5ea6\u5956\u52b1\uff0c\u7528\u4e8e\u91cf\u5316\u76f8\u5173\u6587\u6863\u7684\u6548\u7528\u3002", "result": "\u5728LongBench v2\u4e0a\u83b7\u5f979%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728RULER\u548cInfiniteBench\u4e0a\u4e5f\u53d6\u5f97\u4e00\u81f4\u7684\u6539\u8fdb\u3002LongR\u5728\u4e0d\u540cRL\u7b97\u6cd5\uff08\u5982DAPO\u3001GSPO\uff09\u4e0a\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002\u5206\u6790\u663e\u793a\u63a8\u7406\u94fe\u957f\u5ea6\u5bf9\u6548\u7387\u6709\u5f71\u54cd\uff0c\u6a21\u578b\u5bf9\u5e72\u6270\u4fe1\u606f\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "LongR\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u63a8\u7406-\u9605\u8bfb\u673a\u5236\u548c\u7ec6\u7c92\u5ea6\u7684\u4e0a\u4e0b\u6587\u5bc6\u5ea6\u5956\u52b1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2602.05769", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05769", "abs": "https://arxiv.org/abs/2602.05769", "authors": ["Adnan Al Ali", "Jind\u0159ich Helcl", "Jind\u0159ich Libovick\u00fd"], "title": "Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors", "comment": "This paper was accepted to EACL 2026 Student Research Workshop", "summary": "LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u8bc4\u4f30\u4e86LLM\u6587\u672c\u68c0\u6d4b\u5668\u5bf9\u975e\u6bcd\u8bed\u8005\u7684\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u5728\u6377\u514b\u8bed\u73af\u5883\u4e2d\uff0c\u975e\u6bcd\u8bed\u8005\u7684\u6587\u672c\u56f0\u60d1\u5ea6\u5e76\u4e0d\u4f4e\u4e8e\u6bcd\u8bed\u8005\uff0c\u4e14\u73b0\u4ee3\u68c0\u6d4b\u5668\u6ca1\u6709\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u4e5f\u4e0d\u4f9d\u8d56\u56f0\u60d1\u5ea6\u8fdb\u884c\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740ChatGPT\u7b49LLM\u52a9\u624b\u7684\u666e\u53ca\uff0c\u5b66\u672f\u754c\u5bf9\u5176\u6ee5\u7528\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u52a0\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\u7ecf\u5e38\u9519\u8bef\u5730\u5c06\u975e\u6bcd\u8bed\u8005\u7684\u6587\u7ae0\u6807\u8bb0\u4e3a\u751f\u6210\u6587\u672c\uff0c\u56e0\u4e3a\u975e\u6bcd\u8bed\u8005\u7684\u6587\u672c\u56f0\u60d1\u5ea6\u8f83\u4f4e\uff08\u8fd9\u662f\u68c0\u6d4b\u5668\u7684\u5173\u952e\u7279\u5f81\uff09\u3002\u672c\u6587\u65e8\u5728\u91cd\u65b0\u8bc4\u4f30\u8fd9\u4e00\u8bf4\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6377\u514b\u8bed\u73af\u5883\u4e2d\u3002", "method": "1. \u6bd4\u8f83\u6377\u514b\u8bed\u975e\u6bcd\u8bed\u8005\u548c\u6bcd\u8bed\u8005\u6587\u672c\u7684\u56f0\u60d1\u5ea6\uff1b2. \u6d4b\u8bd5\u6765\u81ea\u4e09\u4e2a\u4e0d\u540c\u5bb6\u65cf\u7684\u68c0\u6d4b\u5668\uff1b3. \u5206\u6790\u68c0\u6d4b\u5668\u662f\u5426\u4f9d\u8d56\u56f0\u60d1\u5ea6\u7279\u5f81\u8fdb\u884c\u5224\u65ad\u3002", "result": "1. \u6377\u514b\u8bed\u975e\u6bcd\u8bed\u8005\u7684\u6587\u672c\u56f0\u60d1\u5ea6\u5e76\u4e0d\u4f4e\u4e8e\u6bcd\u8bed\u8005\uff1b2. \u68c0\u6d4b\u5668\u6ca1\u6709\u663e\u793a\u51fa\u5bf9\u975e\u6bcd\u8bed\u8005\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff1b3. \u5f53\u4ee3\u68c0\u6d4b\u5668\u5728\u4e0d\u9700\u8981\u4f9d\u8d56\u56f0\u60d1\u5ea6\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u8fd0\u4f5c\u3002", "conclusion": "\u5728\u6377\u514b\u8bed\u73af\u5883\u4e2d\uff0c\u5148\u524d\u5173\u4e8eLLM\u6587\u672c\u68c0\u6d4b\u5668\u5bf9\u975e\u6bcd\u8bed\u8005\u5b58\u5728\u504f\u89c1\u7684\u8bf4\u6cd5\u4e0d\u518d\u6210\u7acb\u3002\u73b0\u4ee3\u68c0\u6d4b\u5668\u5df2\u7ecf\u53d1\u5c55\u51fa\u66f4\u590d\u6742\u7684\u68c0\u6d4b\u673a\u5236\uff0c\u4e0d\u518d\u5355\u7eaf\u4f9d\u8d56\u6587\u672c\u56f0\u60d1\u5ea6\uff0c\u4ece\u800c\u907f\u514d\u4e86\u7cfb\u7edf\u6027\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2602.05842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05842", "abs": "https://arxiv.org/abs/2602.05842", "authors": ["Xiao Yu", "Baolin Peng", "Ruize Xu", "Yelong Shen", "Pengcheng He", "Suman Nath", "Nikhil Singh", "Jiangfeng Gao", "Zhou Yu"], "title": "Reinforcement World Model Learning for LLM-based Agents", "comment": null, "summary": "Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $\u03c4^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $\u03c4^2$ Bench respectively, while matching the performance of expert-data training.", "AI": {"tldr": "RWML\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5956\u52b1\u4e3a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u52a8\u4f5c\u6761\u4ef6\u7684\u4e16\u754c\u6a21\u578b\uff0c\u63d0\u5347\u5176\u5728\u73af\u5883\u52a8\u6001\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "LLM\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u9884\u6d4b\u52a8\u4f5c\u540e\u679c\u548c\u9002\u5e94\u73af\u5883\u52a8\u6001\uff0c\u9700\u8981\u589e\u5f3a\u5176\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faRWML\u65b9\u6cd5\uff0c\u4f7f\u7528\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5956\u52b1\u5728\u6587\u672c\u72b6\u6001\u4e0a\u5b66\u4e60\u52a8\u4f5c\u6761\u4ef6\u7684\u4e16\u754c\u6a21\u578b\u3002\u901a\u8fc7\u5c06\u6a21\u578b\u751f\u6210\u7684\u6a21\u62df\u4e0b\u4e00\u72b6\u6001\u4e0e\u73af\u5883\u89c2\u5bdf\u5230\u7684\u5b9e\u9645\u4e0b\u4e00\u72b6\u6001\u5728\u9884\u8bad\u7ec3\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\uff0c\u786e\u4fdd\u5185\u90e8\u4e16\u754c\u6a21\u62df\u4e0e\u5b9e\u9645\u73af\u5883\u52a8\u6001\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728ALFWorld\u548c\u03c4\u00b2 Bench\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\u3002\u4e0e\u4efb\u52a1\u6210\u529f\u5956\u52b1\u7ed3\u5408\u65f6\uff0c\u5728ALFWorld\u548c\u03c4\u00b2 Bench\u4e0a\u5206\u522b\u6bd4\u76f4\u63a5\u4efb\u52a1\u6210\u529f\u5956\u52b1RL\u9ad8\u51fa6.9\u548c5.7\u5206\uff0c\u6027\u80fd\u4e0e\u4e13\u5bb6\u6570\u636e\u8bad\u7ec3\u76f8\u5f53\u3002", "conclusion": "RWML\u4e3a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u6bd4\u4e0b\u4e00\u72b6\u6001\u4ee4\u724c\u9884\u6d4b\u66f4\u7a33\u5065\uff0c\u6bd4LLM-as-a-judge\u66f4\u4e0d\u6613\u53d7\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2602.05843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05843", "abs": "https://arxiv.org/abs/2602.05843", "authors": ["Fangzhi Xu", "Hang Yan", "Qiushi Sun", "Jinyang Wu", "Zixian Huang", "Muye Huang", "Jingyang Gong", "Zichen Ding", "Kanzhi Cheng", "Yian Wang", "Xinyu Che", "Zeyi Sun", "Jian Zhang", "Zhangyue Yin", "Haoran Luo", "Xuanjing Huang", "Ben Kao", "Jun Liu", "Qika Lin"], "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions", "comment": "34 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", "AI": {"tldr": "\u63d0\u51faOdysseyArena\u8bc4\u4f30\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u957f\u89c6\u91ce\u3001\u4e3b\u52a8\u3001\u5f52\u7eb3\u5f0f\u4ea4\u4e92\uff0c\u6d4b\u8bd5\u667a\u80fd\u4f53\u4ece\u7ecf\u9a8c\u4e2d\u53d1\u73b0\u6f5c\u5728\u8f6c\u79fb\u89c4\u5f8b\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u524d\u6cbfLLMs\u5728\u5f52\u7eb3\u573a\u666f\u4e2d\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u91c7\u7528\u6f14\u7ece\u8303\u5f0f\uff0c\u667a\u80fd\u4f53\u57fa\u4e8e\u660e\u786e\u89c4\u5219\u548c\u9759\u6001\u76ee\u6807\u6267\u884c\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u4ece\u7ecf\u9a8c\u4e2d\u81ea\u4e3b\u53d1\u73b0\u6f5c\u5728\u8f6c\u79fb\u89c4\u5f8b\u7684\u5f52\u7eb3\u9700\u6c42\uff0c\u8fd9\u662f\u5b9e\u73b0\u667a\u80fd\u4f53\u9884\u89c1\u6027\u548c\u6218\u7565\u4e00\u81f4\u6027\u7684\u5173\u952e\u3002", "method": "\u5f15\u5165OdysseyArena\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5e76\u5b9e\u4f8b\u5316\u56db\u4e2a\u539f\u8bed\uff0c\u5c06\u62bd\u8c61\u8f6c\u79fb\u52a8\u6001\u8f6c\u5316\u4e3a\u5177\u4f53\u4ea4\u4e92\u73af\u5883\u3002\u5efa\u7acbOdysseyArena-Lite\u7528\u4e8e\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff08120\u4e2a\u4efb\u52a1\uff09\uff0c\u5e76\u63a8\u51faOdysseyArena-Challenge\u4ee5\u538b\u529b\u6d4b\u8bd5\u667a\u80fd\u4f53\u5728\u6781\u7aef\u4ea4\u4e92\u89c6\u91ce\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5bf915+\u4e2a\u9886\u5148LLM\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u524d\u6cbf\u6a21\u578b\u5728\u5f52\u7eb3\u573a\u666f\u4e2d\u4e5f\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u63ed\u793a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u53d1\u73b0\u7684\u5173\u952e\u74f6\u9888\u3002", "conclusion": "OdysseyArena\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5f3a\u8c03\u957f\u89c6\u91ce\u3001\u4e3b\u52a8\u3001\u5f52\u7eb3\u5f0f\u4ea4\u4e92\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u53d1\u73b0\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u8fd9\u4e00\u5173\u952e\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.05853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05853", "abs": "https://arxiv.org/abs/2602.05853", "authors": ["Siran Liu", "Guoxia Wang", "Sa Wang", "Jinle Zeng", "HaoYang Xie", "Siyu Lou", "JiaBin Yang", "DianHai Yu", "Haifeng Wang", "Chao Yang"], "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference", "comment": null, "summary": "The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \\underline{r}ound-\\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$\u03c4$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.", "AI": {"tldr": "RRAttention\u662f\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5934\u8f6e\u8be2\u91c7\u6837\u7b56\u7565\u5c06\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u4eceO(L\u00b2)\u964d\u4f4e\u5230O(L\u00b2/S\u00b2)\uff0c\u5728\u4fdd\u6301\u67e5\u8be2\u72ec\u7acb\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u5168\u5c40\u6a21\u5f0f\u53d1\u73b0\uff0c\u4ec5\u8ba1\u7b97\u4e00\u534a\u6ce8\u610f\u529b\u5757\u5373\u53ef\u6062\u590d99%\u4ee5\u4e0a\u6027\u80fd\uff0c\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b02.4\u500d\u52a0\u901f\u3002", "motivation": "\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u662f\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5b58\u5728\u9884\u5904\u7406\u9700\u6c42\u3001\u7f3a\u4e4f\u5168\u5c40\u8bc4\u4f30\u3001\u8fdd\u53cd\u67e5\u8be2\u72ec\u7acb\u6027\u6216\u8ba1\u7b97\u5f00\u9500\u9ad8\u7b49\u57fa\u672c\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faRRAttention\u65b9\u6cd5\uff0c\u91c7\u7528\u5934\u8f6e\u8be2\u91c7\u6837\u7b56\u7565\uff0c\u5728\u6bcf\u4e2a\u6b65\u5e45\u5185\u8de8\u6ce8\u610f\u529b\u5934\u65cb\u8f6c\u67e5\u8be2\u91c7\u6837\u4f4d\u7f6e\uff0c\u4fdd\u6301\u67e5\u8be2\u72ec\u7acb\u6027\u540c\u65f6\u5b9e\u73b0\u6b65\u5e45\u7ea7\u805a\u5408\u7684\u9ad8\u6548\u5168\u5c40\u6a21\u5f0f\u53d1\u73b0\u3002\u5c06\u590d\u6742\u5ea6\u4eceO(L\u00b2)\u964d\u4f4e\u5230O(L\u00b2/S\u00b2)\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94Top-\u03c4\u9009\u62e9\u5b9e\u73b0\u6700\u4f18\u7a00\u758f\u6027\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08HELMET\uff09\u548c\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\uff08Video-MME\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRRAttention\u4ec5\u8ba1\u7b97\u4e00\u534a\u6ce8\u610f\u529b\u5757\u5373\u53ef\u6062\u590d\u8d85\u8fc799%\u7684\u5b8c\u6574\u6ce8\u610f\u529b\u6027\u80fd\uff0c\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b02.4\u500d\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "conclusion": "RRAttention\u901a\u8fc7\u5934\u8f6e\u8be2\u91c7\u6837\u7b56\u7565\u540c\u65f6\u5b9e\u73b0\u4e86\u6240\u6709\u671f\u671b\u7279\u6027\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u9762\u4e34\u7684\u57fa\u672c\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.05874", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05874", "abs": "https://arxiv.org/abs/2602.05874", "authors": ["Adri\u00e1n Gir\u00f3n", "Pablo Miralles", "Javier Huertas-Tato", "Sergio D'Antonio", "David Camacho"], "title": "xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection", "comment": null, "summary": "Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.\n  We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.\n  We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.\n  Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.", "AI": {"tldr": "xList-Hate\uff1a\u5c06\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u91cd\u6784\u4e3a\u57fa\u4e8e\u68c0\u67e5\u6e05\u5355\u7684\u8bca\u65ad\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7LLM\u56de\u7b54\u6982\u5ff5\u7ea7\u95ee\u9898\u751f\u6210\u8bca\u65ad\u8868\u793a\uff0c\u518d\u7528\u53ef\u89e3\u91ca\u51b3\u7b56\u6811\u805a\u5408\uff0c\u63d0\u5347\u8de8\u6570\u636e\u96c6\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u88ab\u7b80\u5316\u4e3a\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\uff0c\u4f46\u4ec7\u6068\u8a00\u8bba\u662f\u590d\u5408\u6982\u5ff5\uff0c\u53d7\u6cd5\u5f8b\u6846\u67b6\u3001\u5e73\u53f0\u653f\u7b56\u548c\u6807\u6ce8\u6307\u5357\u7b49\u591a\u56e0\u7d20\u5f71\u54cd\u3002\u76d1\u7763\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u7279\u5b9a\u6570\u636e\u96c6\u5b9a\u4e49\uff0c\u5728\u9886\u57df\u8fc1\u79fb\u548c\u6807\u6ce8\u566a\u58f0\u4e0b\u9c81\u68d2\u6027\u6709\u9650\u3002", "method": "\u63d0\u51faxList-Hate\u8bca\u65ad\u6846\u67b6\uff1a1) \u5c06\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u5206\u89e3\u4e3a\u57fa\u4e8e\u5e7f\u6cdb\u5171\u4eab\u89c4\u8303\u6807\u51c6\u7684\u68c0\u67e5\u6e05\u5355\uff0c\u5305\u542b\u660e\u786e\u7684\u6982\u5ff5\u7ea7\u95ee\u9898\uff1b2) \u7528\u5927\u8bed\u8a00\u6a21\u578b\u72ec\u7acb\u56de\u7b54\u6bcf\u4e2a\u95ee\u9898\uff0c\u751f\u6210\u4e8c\u8fdb\u5236\u8bca\u65ad\u8868\u793a\uff1b3) \u7528\u8f7b\u91cf\u7ea7\u3001\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6811\u805a\u5408\u8bca\u65ad\u4fe1\u53f7\uff0c\u4ea7\u751f\u900f\u660e\u53ef\u5ba1\u8ba1\u7684\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u4ec7\u6068\u8a00\u8bba\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u5bb6\u65cf\u4e2d\u8bc4\u4f30\uff0c\u76f8\u6bd4\u96f6\u6837\u672cLLM\u5206\u7c7b\u548c\u9886\u57df\u5185\u76d1\u7763\u5fae\u8c03\uff1a1) \u76d1\u7763\u65b9\u6cd5\u901a\u5e38\u6700\u5927\u5316\u9886\u57df\u5185\u6027\u80fd\uff0c\u800cxList-Hate\u6301\u7eed\u63d0\u5347\u8de8\u6570\u636e\u96c6\u9c81\u68d2\u6027\u548c\u9886\u57df\u8fc1\u79fb\u4e0b\u7684\u76f8\u5bf9\u6027\u80fd\uff1b2) \u5b9a\u6027\u5206\u6790\u8868\u660e\u6846\u67b6\u5bf9\u67d0\u4e9b\u5f62\u5f0f\u7684\u6807\u6ce8\u4e0d\u4e00\u81f4\u548c\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u66f4\u4e0d\u654f\u611f\uff1b3) \u901a\u8fc7\u660e\u786e\u51b3\u7b56\u8def\u5f84\u548c\u56e0\u7d20\u7ea7\u5206\u6790\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c06\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u91cd\u6784\u4e3a\u8bca\u65ad\u63a8\u7406\u4efb\u52a1\u800c\u975e\u5355\u4e00\u5206\u7c7b\u95ee\u9898\uff0c\u4e3a\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002xList-Hate\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u6982\u5ff5\u548c\u900f\u660e\u51b3\u7b56\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8fc7\u62df\u5408\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002"}}
{"id": "2602.05879", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05879", "abs": "https://arxiv.org/abs/2602.05879", "authors": ["Miguel Moura Ramos", "Duarte M. Alves", "Hippolyte Gisserot-Boukhlef", "Jo\u00e3o Alves", "Pedro Henrique Martins", "Patrick Fernandes", "Jos\u00e9 Pombal", "Nuno M. Guerreiro", "Ricardo Rei", "Nicolas Boizard", "Amin Farajian", "Mateusz Klimaszewski", "Jos\u00e9 G. C. de Souza", "Barry Haddow", "Fran\u00e7ois Yvon", "Pierre Colombo", "Alexandra Birch", "Andr\u00e9 F. T. Martins"], "title": "EuroLLM-22B: Technical Report", "comment": null, "summary": "This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.", "AI": {"tldr": "EuroLLM-22B\u662f\u4e00\u4e2a\u4ece\u5934\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u6b27\u76df\u6240\u670924\u79cd\u5b98\u65b9\u8bed\u8a00\u548c11\u79cd\u989d\u5916\u8bed\u8a00\uff0c\u65e8\u5728\u89e3\u51b3\u6b27\u6d32\u8bed\u8a00\u5728\u73b0\u6709\u5f00\u6e90\u5927\u6a21\u578b\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u6b27\u6d32\u8bed\u8a00\u4ee3\u8868\u6027\u4e0d\u8db3\u4e14\u670d\u52a1\u4e0d\u5145\u5206\uff0c\u9700\u8981\u4e13\u95e8\u652f\u6301\u6b27\u6d32\u516c\u6c11\u9700\u6c42\u7684\u591a\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86EuroLLM-22B\u6a21\u578b\uff0c\u5305\u62ec\u5206\u8bcd\u5668\u8bbe\u8ba1\u3001\u67b6\u6784\u89c4\u8303\u3001\u6570\u636e\u8fc7\u6ee4\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8986\u76d635\u79cd\u8bed\u8a00\uff0c\u5e76\u53d1\u5e03\u4e86\u57fa\u7840\u6a21\u578b\u3001\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u3001\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6570\u636e\u548c\u8bc4\u4f30\u4ee3\u7801\u5e93\u3002", "result": "EuroLLM-22B\u5728\u5e7f\u6cdb\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u63a8\u7406\u3001\u6307\u4ee4\u9075\u5faa\u548c\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u540c\u89c4\u6a21\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "EuroLLM-22B\u6210\u529f\u586b\u8865\u4e86\u6b27\u6d32\u8bed\u8a00\u5728\u5927\u6a21\u578b\u4e2d\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8d44\u6e90\u652f\u6301\uff0c\u5305\u62ec\u6a21\u578b\u3001\u6570\u636e\u548c\u4ee3\u7801\u7684\u5f00\u653e\u53d1\u5e03\u3002"}}
{"id": "2602.05897", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05897", "abs": "https://arxiv.org/abs/2602.05897", "authors": ["Shuo Nie", "Hexuan Deng", "Chao Wang", "Ruiyu Fang", "Xuebo Liu", "Shuangyong Song", "Yu Li", "Min Zhang", "Xuelong Li"], "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models", "comment": null, "summary": "As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.", "AI": {"tldr": "FaithRL\uff1a\u9488\u5bf9\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u7684\u5fe0\u5b9e\u6027\u611f\u77e5\u6b65\u7ea7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u548c\u622a\u65ad\u91cd\u91c7\u6837\u7b56\u7565\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898", "motivation": "\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u5bb9\u6613\u5728\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u4ea7\u751f\u5fe0\u5b9e\u6027\u5e7b\u89c9\u3002\u73b0\u6709\u57fa\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u7ed3\u679c\u5956\u52b1\u6216\u7c97\u7c92\u5ea6\u8bc4\u4f30\uff0c\u5f53\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u65f6\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u4e0d\u5fe0\u5b9e\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51faFaithRL\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u663e\u5f0f\u7684\u6b65\u7ea7\u5fe0\u5b9e\u6027\u76d1\u7763\uff1b2\uff09\u91c7\u7528\u9690\u5f0f\u622a\u65ad\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u4ece\u5fe0\u5b9e\u524d\u7f00\u751f\u6210\u5bf9\u6bd4\u4fe1\u53f7", "result": "\u5728\u591a\u4e2a\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u548c\u5f00\u653e\u4e66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFaithRL\u80fd\u6301\u7eed\u51cf\u5c11CoT\u548c\u6700\u7ec8\u7b54\u6848\u4e2d\u7684\u5e7b\u89c9\uff0c\u5b9e\u73b0\u66f4\u5fe0\u5b9e\u53ef\u9760\u7684\u63a8\u7406", "conclusion": "FaithRL\u901a\u8fc7\u6b65\u7ea7\u76d1\u7763\u548c\u5bf9\u6bd4\u4fe1\u53f7\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5fe0\u5b9e\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u63a8\u7406\u65b9\u6cd5"}}
{"id": "2602.05905", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05905", "abs": "https://arxiv.org/abs/2602.05905", "authors": ["Letian Peng", "Yupeng Hou", "Kun Zhou", "Jingbo Shang"], "title": "Codified Finite-state Machines for Role-playing", "comment": null, "summary": "Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.", "AI": {"tldr": "\u63d0\u51faCFSM/CPFSM\u6846\u67b6\uff0c\u901a\u8fc7LLM\u81ea\u52a8\u5c06\u89d2\u8272\u63cf\u8ff0\u7f16\u7801\u4e3a\u6709\u9650\u72b6\u6001\u673a\uff0c\u63d0\u5347\u89d2\u8272\u626e\u6f14\u7684\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u8868\u9762\u884c\u4e3a\uff0c\u96be\u4ee5\u8ddf\u8e2a\u9a71\u52a8\u4e92\u52a8\u7684\u6f5c\u5728\u72b6\u6001\u3002\u4f20\u7edf\u624b\u5de5\u5236\u4f5c\u7684\u6709\u9650\u72b6\u6001\u673a\u96be\u4ee5\u9002\u5e94\u89d2\u8272\u626e\u6f14\u7684\u5f00\u653e\u8bed\u4e49\u7a7a\u95f4", "method": "\u5f15\u5165Codified Finite-State Machines (CFSMs)\uff0c\u4f7f\u7528LLM\u81ea\u52a8\u5c06\u6587\u672c\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u7f16\u7801\u4e3aFSM\uff0c\u63d0\u53d6\u5173\u952e\u72b6\u6001\u548c\u8f6c\u6362\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3aCodified Probabilistic Finite-State Machines (CPFSMs)\uff0c\u5c06\u8f6c\u6362\u5efa\u6a21\u4e3a\u72b6\u6001\u7684\u6982\u7387\u5206\u5e03", "result": "\u5728\u5408\u6210\u8bc4\u4f30\u548c\u771f\u5b9e\u4e16\u754c\u89d2\u8272\u626e\u6f14\u573a\u666f\u4e2d\uff0cCFSM\u548cCPFSM\u4f18\u4e8e\u4e00\u822c\u5e94\u7528\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u548c\u5f00\u653e\u968f\u673a\u72b6\u6001\u63a2\u7d22\u4e2d\u90fd\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "conclusion": "CFSM/CPFSM\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u4ece\u89d2\u8272\u63cf\u8ff0\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\uff0c\u5f3a\u5236\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bed\u4e49\u7a7a\u95f4\u4e2d\u89d2\u8272\u72b6\u6001\u5efa\u6a21\u7684\u6311\u6218"}}
{"id": "2602.05929", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05929", "abs": "https://arxiv.org/abs/2602.05929", "authors": ["Jian Chen", "Zhuoran Wang", "Jiayu Qin", "Ming Li", "Meng Wang", "Changyou Chen", "Yin Chen", "Qizhen Weng", "Yirui Liu"], "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs", "comment": null, "summary": "Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.", "AI": {"tldr": "KV-CoRE\uff1a\u4e00\u79cd\u57fa\u4e8eSVD\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316KV\u7f13\u5b58\u7684\u6570\u636e\u76f8\u5173\u4f4e\u79e9\u53ef\u538b\u7f29\u6027\uff0c\u4e3aLLM\u7684KV\u7f13\u5b58\u538b\u7f29\u63d0\u4f9b\u539f\u5219\u6027\u8bc4\u4f30\u6846\u67b6\u548c\u5927\u89c4\u6a21\u57fa\u51c6", "motivation": "\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u957f\uff0cKV\u7f13\u5b58\u7684\u8bfb\u5199\u4f1a\u5feb\u901f\u9971\u548cGPU\u5185\u5b58\u5e26\u5bbd\u3002\u73b0\u6709KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86KV\u7f13\u5b58\u7684\u6570\u636e\u4f9d\u8d56\u6027\u53ca\u5176\u5728\u4e0d\u540c\u5c42\u95f4\u7684\u53d8\u5316\uff0c\u9700\u8981\u4e00\u79cd\u91cf\u5316\u8bc4\u4f30\u65b9\u6cd5\u6765\u7406\u89e3KV\u7f13\u5b58\u7684\u53ef\u538b\u7f29\u6027\u6a21\u5f0f\u3002", "method": "\u63d0\u51faKV-CoRE\u65b9\u6cd5\uff0c\u57fa\u4e8eSVD\u8ba1\u7b97Frobenius\u8303\u6570\u4e0b\u7684\u6700\u4f18\u4f4e\u79e9\u8fd1\u4f3c\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u68af\u5ea6\u3001\u652f\u6301\u589e\u91cf\u8ba1\u7b97\uff0c\u80fd\u591f\u9ad8\u6548\u8fdb\u884c\u6570\u636e\u96c6\u7ea7\u522b\u548c\u5c42\u7ea7\u7684\u8bc4\u4f30\u3002\u4f7f\u7528\u5f52\u4e00\u5316\u6709\u6548\u79e9\u4f5c\u4e3a\u53ef\u538b\u7f29\u6027\u5ea6\u91cf\u6307\u6807\u3002", "result": "\u5206\u6790\u4e86\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\uff08\u6db5\u76d65\u4e2a\u82f1\u6587\u9886\u57df\u548c16\u79cd\u8bed\u8a00\uff09\uff0c\u63ed\u793a\u4e86\u53ef\u538b\u7f29\u6027\u4e0e\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u548c\u8bed\u8a00\u8986\u76d6\u7684\u7cfb\u7edf\u6027\u5173\u8054\u3002\u5f52\u4e00\u5316\u6709\u6548\u79e9\u4e0e\u538b\u7f29\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u6709\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u5efa\u7acb\u4e86KV\u7f13\u5b58\u53ef\u538b\u7f29\u6027\u7684\u539f\u5219\u6027\u8bc4\u4f30\u6846\u67b6\u548c\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u4e3a\u52a8\u6001\u3001\u6570\u636e\u611f\u77e5\u7684\u538b\u7f29\u548c\u6570\u636e\u4e2d\u5fc3\u7684\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6d1e\u89c1\u3002"}}
{"id": "2602.05932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05932", "abs": "https://arxiv.org/abs/2602.05932", "authors": ["L\u00e9o Labat", "Etienne Ollion", "Fran\u00e7ois Yvon"], "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions", "comment": "17 pages, 5 figures (8 pages of references and appendices)", "summary": "Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.", "AI": {"tldr": "\u7814\u7a76\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ef7\u503c\u5bfc\u5411\u591a\u9009\u9898\u4e0a\u7684\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u867d\u7136\u5927\u578b\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u6574\u4f53\u4e00\u81f4\u6027\u8f83\u9ad8\uff0c\u4f46\u67d0\u4e9b\u95ee\u9898\u4ecd\u4f1a\u5f15\u53d1\u8bed\u8a00\u7279\u5f02\u6027\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u591a\u8bed\u8a00LLM\u5728\u4e8b\u5b9e\u56de\u5fc6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4f46\u8f83\u5c11\u63a2\u8ba8\u8bed\u8a00\u5bf9\u4ef7\u503c\u5bfc\u5411\u591a\u9009\u9898\u56de\u7b54\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u591a\u8bed\u8a00LLM\u5728\u4e0d\u540c\u8bed\u8a00\u4e0b\u56de\u7b54\u4ef7\u503c\u95ee\u9898\u65f6\u662f\u5426\u4fdd\u6301\u4e00\u81f4\uff08\u50cf\u7406\u8bba\u4e0a\u7684\u591a\u8bed\u8005\uff09\uff0c\u8fd8\u662f\u8868\u73b0\u51fa\u8bed\u8a00\u4f9d\u8d56\u7684\u884c\u4e3a\u5dee\u5f02\uff08\u50cf\u591a\u4e2a\u5355\u8bed\u6a21\u578b\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u8868\u8fbe\u4e0d\u540c\u4ef7\u503c\u89c2\uff09\u3002", "method": "1. \u521b\u5efa\u65b0\u7684\u8bed\u6599\u5e93MEVS\uff08\u591a\u8bed\u8a00\u6b27\u6d32\u4ef7\u503c\u89c2\u8c03\u67e5\uff09\uff0c\u5305\u542b8\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u4eba\u7c7b\u7ffb\u8bd1\u8c03\u67e5\u95ee\u9898\uff0c\u907f\u514d\u4e86\u673a\u5668\u7ffb\u8bd1\u6216\u4e34\u65f6\u63d0\u793a\u7684\u95ee\u9898\u30022. \u5bf930\u591a\u4e2a\u4e0d\u540c\u89c4\u6a21\u3001\u5236\u9020\u5546\u548c\u8c03\u4f18\u72b6\u6001\u7684\u591a\u8bed\u8a00LLM\u8fdb\u884c\u6d4b\u8bd5\u30023. \u5728\u5168\u9762\u63a7\u5236\u7684\u63d0\u793a\u53d8\u4f53\u4e0b\uff08\u5305\u62ec\u7b54\u6848\u987a\u5e8f\u3001\u7b26\u53f7\u7c7b\u578b\u548c\u5c3e\u90e8\u5b57\u7b26\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "1. \u66f4\u5927\u3001\u7ecf\u8fc7\u6307\u4ee4\u8c03\u4f18\u7684\u6a21\u578b\u6574\u4f53\u4e00\u81f4\u6027\u66f4\u9ad8\u30022. \u6a21\u578b\u56de\u7b54\u7684\u7a33\u5065\u6027\u5728\u4e0d\u540c\u95ee\u9898\u95f4\u5dee\u5f02\u5f88\u5927\uff1a\u67d0\u4e9b\u591a\u9009\u9898\u5728\u6240\u6709\u6a21\u578b\u5185\u548c\u6a21\u578b\u95f4\u8fbe\u6210\u5b8c\u5168\u4e00\u81f4\uff0c\u800c\u53e6\u4e00\u4e9b\u95ee\u9898\u5219\u5bfc\u81f4LLM\u56de\u7b54\u5206\u88c2\u30023. \u6240\u6709\u4e00\u81f4\u7684\u3001\u7ecf\u8fc7\u6307\u4ee4\u5fae\u8c03\u7684\u6a21\u578b\u5728\u67d0\u4e9b\u95ee\u9898\u4e0a\u90fd\u8868\u73b0\u51fa\u8bed\u8a00\u7279\u5f02\u6027\u884c\u4e3a\u3002", "conclusion": "\u591a\u8bed\u8a00LLM\u5728\u4ef7\u503c\u5bfc\u5411\u591a\u9009\u9898\u4e0a\u5e76\u975e\u5b8c\u5168\u4e00\u81f4\u7684\u591a\u8bed\u8005\uff0c\u800c\u662f\u5728\u67d0\u4e9b\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8bed\u8a00\u4f9d\u8d56\u7684\u884c\u4e3a\u5dee\u5f02\u3002\u8fd9\u63d0\u793a\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u504f\u597d\u5fae\u8c03\u7684\u9009\u62e9\u6027\u6548\u5e94\uff0c\u4ee5\u53ca\u8bed\u8a00\u5982\u4f55\u5f71\u54cdLLM\u7684\u4ef7\u503c\u8868\u8fbe\u3002"}}
{"id": "2602.05940", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05940", "abs": "https://arxiv.org/abs/2602.05940", "authors": ["Junxiao Liu", "Zhijun Wang", "Yixiao Li", "Zhejian Lai", "Liqian Huang", "Xin Huang", "Xue Han", "Junlan Feng", "Shujian Huang"], "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training", "comment": "16 pages, 11 figures", "summary": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.", "AI": {"tldr": "TRIT\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7ffb\u8bd1\u8bad\u7ec3\u5230\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\uff0c\u89e3\u51b3\u957f\u63a8\u7406\u6a21\u578b\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u53cd\u9988\u6216\u989d\u5916\u6570\u636e\u5373\u53ef\u63d0\u5347\u95ee\u9898\u7406\u89e3\u548c\u56de\u7b54\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u957f\u63a8\u7406\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1a\u5b83\u4eec\u503e\u5411\u4e8e\u7528\u82f1\u8bed\u63a8\u7406\u975e\u82f1\u8bed\u95ee\u9898\uff0c\u800c\u5f3a\u5236\u7528\u95ee\u9898\u8bed\u8a00\u63a8\u7406\u65f6\u51c6\u786e\u7387\u5927\u5e45\u4e0b\u964d\u3002\u8fd9\u79cd\u56f0\u5883\u6e90\u4e8e\u591a\u8bed\u8a00\u95ee\u9898\u7406\u89e3\u548c\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u53cc\u91cd\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTRIT\uff08Translation-Reasoning Integrated Training\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u6211\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u5c06\u7ffb\u8bd1\u8bad\u7ec3\u6574\u5408\u5230\u591a\u8bed\u8a00\u63a8\u7406\u8bad\u7ec3\u4e2d\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u53cd\u9988\u6216\u989d\u5916\u591a\u8bed\u8a00\u6570\u636e\uff0c\u8054\u5408\u589e\u5f3a\u591a\u8bed\u8a00\u95ee\u9898\u7406\u89e3\u548c\u56de\u7b54\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728MMATH\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf7\u4e2a\u767e\u5206\u70b9\uff0c\u63d0\u9ad8\u4e86\u7b54\u6848\u6b63\u786e\u6027\u548c\u8bed\u8a00\u4e00\u81f4\u6027\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\uff0c\u6574\u5408\u7ffb\u8bd1\u8bad\u7ec3\u5c06\u8de8\u8bed\u8a00\u95ee\u9898\u5bf9\u9f50\u63d0\u9ad8\u4e8610\u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\uff0c\u5e76\u63d0\u5347\u4e86\u6570\u5b66\u95ee\u9898\u548c\u901a\u7528\u9886\u57df\u6587\u672c\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5728FLORES-200\u4e0aCOMET\u5206\u6570\u63d0\u5347\u9ad8\u8fbe8.4\u5206\u3002", "conclusion": "TRIT\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7ffb\u8bd1\u8bad\u7ec3\u5230\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u63a8\u7406\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u95ee\u9898\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.05971", "categories": ["cs.CL", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.05971", "abs": "https://arxiv.org/abs/2602.05971", "authors": ["Felipe D. Toro-Hern\u00e1ndez", "Jesuino Vieira Filho", "Rodrigo M. Cabral-Carvalho"], "title": "Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space", "comment": "10 pages, 6 figures (excluding refs/appendix). Accepted to ICLR 2026", "summary": "Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u6982\u5ff5\u751f\u6210\u89c6\u4e3a\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\uff0c\u901a\u8fc7\u7d2f\u79ef\u5d4c\u5165\u6784\u5efa\u8bed\u4e49\u8f68\u8ff9\u5e76\u63d0\u53d6\u51e0\u4f55\u548c\u52a8\u6001\u6307\u6807\uff0c\u7528\u4e8e\u533a\u5206\u4e34\u5e8a\u7ec4\u548c\u6982\u5ff5\u7c7b\u578b\uff0c\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u3001\u8a00\u8bed\u6d41\u7545\u6027\u7b49\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5982\u4f55\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5bfc\u822a\u4ee5\u68c0\u7d22\u548c\u64cd\u7eb5\u610f\u4e49\uff0c\u4e3a\u8bed\u4e49\u8868\u5f81\u641c\u7d22\u63d0\u4f9b\u8ba1\u7b97\u57fa\u7840\uff0c\u51cf\u5c11\u4f20\u7edf\u8bed\u8a00\u9884\u5904\u7406\u65b9\u6cd5\u7684\u4eba\u5de5\u5e72\u9884\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u4e0d\u540cTransformer\u6587\u672c\u5d4c\u5165\u6a21\u578b\u6784\u5efa\u53c2\u4e0e\u8005\u7279\u5b9a\u7684\u8bed\u4e49\u8f68\u8ff9\uff08\u57fa\u4e8e\u7d2f\u79ef\u5d4c\u5165\uff09\uff0c\u63d0\u53d6\u51e0\u4f55\u548c\u52a8\u6001\u6307\u6807\uff08\u5230\u4e0b\u4e00\u4e2a\u7684\u8ddd\u79bb\u3001\u5230\u8d28\u5fc3\u7684\u8ddd\u79bb\u3001\u71b5\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\uff09\uff0c\u5728\u56db\u79cd\u8de8\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u533a\u5206\u4e34\u5e8a\u7ec4\u548c\u6982\u5ff5\u7c7b\u578b\uff0c\u7d2f\u79ef\u5d4c\u5165\u5728\u957f\u8f68\u8ff9\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\u7ed3\u679c\u76f8\u4f3c\uff0c\u8868\u660e\u4e0d\u540c\u5b66\u4e60\u8868\u5f81\u4e4b\u95f4\u5b58\u5728\u5171\u6027\u3002", "conclusion": "\u5c06\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\u5316\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u5316\u8f68\u8ff9\uff0c\u8fde\u63a5\u8ba4\u77e5\u5efa\u6a21\u4e0e\u5b66\u4e60\u8868\u5f81\uff0c\u4e3a\u91cf\u5316\u8bed\u4e49\u8868\u5f81\u52a8\u6001\u5efa\u7acb\u7ba1\u9053\uff0c\u5728\u4e34\u5e8a\u7814\u7a76\u3001\u8de8\u8bed\u8a00\u5206\u6790\u548c\u4eba\u5de5\u8ba4\u77e5\u8bc4\u4f30\u4e2d\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.05992", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05992", "abs": "https://arxiv.org/abs/2602.05992", "authors": ["Lizhuo Luo", "Shenggui Li", "Yonggang Wen", "Tianwei Zhang"], "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs", "comment": null, "summary": "Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.", "AI": {"tldr": "\u63d0\u51faDynamic Sliding Block (DSB)\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5757\u5927\u5c0f\u6765\u6539\u8fdb\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5757\u8c03\u5ea6\u7b56\u7565\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387", "motivation": "\u73b0\u6709\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u56fa\u5b9a\u7684\u9884\u5b9a\u4e49\u5757\u8c03\u5ea6\u7b56\u7565\uff0c\u65e0\u6cd5\u6839\u636e\u8bed\u4e49\u96be\u5ea6\u52a8\u6001\u8c03\u6574\uff0c\u5bfc\u81f4\u8d28\u91cf\u635f\u5931\u548c\u6548\u7387\u4f4e\u4e0b\uff1a\u53ef\u80fd\u8fc7\u65e9\u786e\u5b9a\u4e0d\u786e\u5b9a\u4f4d\u7f6e\uff0c\u540c\u65f6\u5ef6\u8fdf\u8fb9\u754c\u9644\u8fd1\u7684\u7b80\u5355\u4f4d\u7f6e", "method": "\u63d0\u51faDSB\u65b9\u6cd5\uff0c\u4f7f\u7528\u52a8\u6001\u5927\u5c0f\u7684\u6ed1\u52a8\u5757\u6765\u514b\u670d\u56fa\u5b9a\u5757\u7684\u521a\u6027\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faDSB Cache\uff0c\u4e3aDSB\u5b9a\u5236\u7684\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u673a\u5236", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDSB\u4e0eDSB Cache\u7ed3\u5408\u80fd\u591f\u6301\u7eed\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387", "conclusion": "\u52a8\u6001\u9002\u5e94\u8bed\u4e49\u96be\u5ea6\u7684\u5757\u8c03\u5ea6\u5bf9\u4e8e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u9ad8\u6548\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0cDSB\u65b9\u6cd5\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.06015", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06015", "abs": "https://arxiv.org/abs/2602.06015", "authors": ["Panagiotis Kaliosis", "Adithya V Ganesan", "Oscar N. E. Kjell", "Whitney Ringwald", "Scott Feltman", "Melissa A. Carr", "Dimitris Samaras", "Camilo Ruggero", "Benjamin J. Luft", "Roman Kotov", "Andrew H. Schwartz"], "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies", "comment": "18 pages, 3 figures, 5 tables", "summary": "Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.", "AI": {"tldr": "LLMs\u5728\u96f6\u6837\u672c\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u7684\u51c6\u786e\u6027\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3001\u5efa\u6a21\u7b56\u7565\u7b49\uff0c\u6700\u4f73\u8868\u73b0\u9700\u8981\u7ed3\u5408\u76d1\u7763\u6a21\u578b\u4e0eLLM\u96c6\u6210\u3002", "motivation": "LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u96f6\u6837\u672c\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\uff0c\u4f46\u6211\u4eec\u5bf9\u5176\u51c6\u786e\u6027\u5f71\u54cd\u56e0\u7d20\u4e86\u89e3\u6709\u9650\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u56e0\u7d20\u5bf9\u8bc4\u4f30\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u75281,437\u540d\u4e2a\u4f53\u7684\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u8bc4\u4f3011\u4e2aSOTA LLMs\uff0c\u7cfb\u7edf\u53d8\u5316(i)\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff08\u5b50\u91cf\u8868\u5b9a\u4e49\u3001\u5206\u5e03\u6458\u8981\u3001\u8bbf\u8c08\u95ee\u9898\uff09\u548c(ii)\u5efa\u6a21\u7b56\u7565\uff08\u96f6\u6837\u672cvs\u5c11\u6837\u672c\u3001\u63a8\u7406\u91cf\u3001\u6a21\u578b\u5927\u5c0f\u3001\u7ed3\u6784\u5316\u5b50\u91cf\u8868vs\u76f4\u63a5\u6807\u91cf\u9884\u6d4b\u3001\u8f93\u51fa\u91cd\u7f29\u653e\u548c9\u79cd\u96c6\u6210\u65b9\u6cd5\uff09\u3002", "result": "(a)\u63d0\u4f9b\u8be6\u7ec6\u6784\u9020\u5b9a\u4e49\u548c\u53d9\u8ff0\u4e0a\u4e0b\u6587\u65f6LLMs\u6700\u51c6\u786e\uff1b(b)\u589e\u52a0\u63a8\u7406\u52aa\u529b\u63d0\u9ad8\u4f30\u8ba1\u51c6\u786e\u6027\uff1b(c)\u5f00\u6e90\u6a21\u578b\u572870B\u53c2\u6570\u540e\u6027\u80fd\u9971\u548c\uff0c\u95ed\u6e90\u6a21\u578b\u968f\u65b0\u4ee3\u6539\u8fdb\uff1b(d)\u76d1\u7763\u6a21\u578b\u4e0e\u96f6\u6837\u672cLLMs\u96c6\u6210\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u77e5\u8bc6\u548c\u5efa\u6a21\u7b56\u7565\u7684\u9009\u62e9\u5bf9\u90e8\u7f72LLMs\u51c6\u786e\u8bc4\u4f30\u5fc3\u7406\u5065\u5eb7\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u8fd9\u4e9b\u56e0\u7d20\u4ee5\u83b7\u5f97\u53ef\u9760\u7ed3\u679c\u3002"}}
{"id": "2602.06019", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06019", "abs": "https://arxiv.org/abs/2602.06019", "authors": ["John Kirchenbauer", "Abhimanyu Hans", "Brian Bartoldson", "Micah Goldblum", "Ashwinee Panda", "Tom Goldstein"], "title": "Multi-Token Prediction via Self-Distillation", "comment": "8 pages and 5 figures in the main body", "summary": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\\times$ faster on average at $<5\\%$ drop in accuracy relative to single token decoding performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u5728\u7ebf\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u5feb\u901f\u7684\u591a\u4ee4\u724c\u9884\u6d4b\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u63a8\u7406\u7ba1\u9053\u6216\u8f85\u52a9\u6a21\u578b\uff0c\u5728GSM8K\u4e0a\u5b9e\u73b03\u500d\u4ee5\u4e0a\u89e3\u7801\u52a0\u901f\u4e14\u51c6\u786e\u7387\u4e0b\u964d\u5c0f\u4e8e5%", "motivation": "\u73b0\u6709\u52a0\u901f\u6280\u672f\u5982\u63a8\u6d4b\u89e3\u7801\u9700\u8981\u8bad\u7ec3\u8f85\u52a9\u6a21\u578b\u5e76\u6784\u5efa\u590d\u6742\u63a8\u7406\u7ba1\u9053\uff0c\u90e8\u7f72\u590d\u6742\u3002\u672c\u6587\u5bfb\u6c42\u66f4\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u52a0\u901f\u8bed\u8a00\u6a21\u578b\u63a8\u7406", "method": "\u4f7f\u7528\u7b80\u5355\u5728\u7ebf\u84b8\u998f\u76ee\u6807\uff0c\u5c06\u9884\u8bad\u7ec3\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4ece\u6162\u901f\u5355\u4ee4\u724c\u9884\u6d4b\u8f6c\u6362\u4e3a\u5feb\u901f\u72ec\u7acb\u591a\u4ee4\u724c\u9884\u6d4b\u6a21\u578b\uff0c\u4fdd\u6301\u4e0e\u539f\u59cb\u68c0\u67e5\u70b9\u5b8c\u5168\u76f8\u540c\u7684\u5b9e\u73b0", "result": "\u5728GSM8K\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u7684\u6a21\u578b\u5e73\u5747\u89e3\u7801\u901f\u5ea6\u63d0\u53473\u500d\u4ee5\u4e0a\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u5c0f\u4e8e5%\uff08\u76f8\u5bf9\u4e8e\u5355\u4ee4\u724c\u89e3\u7801\u6027\u80fd\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8bed\u8a00\u6a21\u578b\u52a0\u901f\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u63a8\u7406\u7ba1\u9053\u6216\u8f85\u52a9\u6a21\u578b\uff0c\u90e8\u7f72\u7b80\u5355\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6"}}
{"id": "2602.06025", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06025", "abs": "https://arxiv.org/abs/2602.06025", "authors": ["Haozhen Zhang", "Haodong Yue", "Tao Feng", "Quanyu Long", "Jianzhu Bao", "Bowen Jin", "Weizhi Zhang", "Xiao Li", "Jiaxuan You", "Chengwei Qin", "Wenya Wang"], "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory", "comment": "Code is available at https://github.com/ViktorAxelsen/BudgetMem", "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.", "AI": {"tldr": "BudgetMem\uff1a\u4e00\u4e2a\u8fd0\u884c\u65f6\u4ee3\u7406\u5185\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u7b97\u5c42\u7ea7\u8def\u7531\u5b9e\u73b0\u663e\u5f0f\u7684\u67e5\u8be2\u611f\u77e5\u6027\u80fd-\u6210\u672c\u63a7\u5236\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5185\u5b58\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u79bb\u7ebf\u3001\u67e5\u8be2\u65e0\u5173\u7684\u5185\u5b58\u6784\u5efa\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u53ef\u80fd\u4e22\u5f03\u67e5\u8be2\u5173\u952e\u4fe1\u606f\u3002\u8fd0\u884c\u65f6\u5185\u5b58\u5229\u7528\u867d\u4e3a\u81ea\u7136\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f00\u9500\u5927\u4e14\u7f3a\u4e4f\u5bf9\u6027\u80fd-\u6210\u672c\u6743\u8861\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "\u5c06\u5185\u5b58\u5904\u7406\u7ed3\u6784\u5316\u4e3a\u591a\u4e2a\u5185\u5b58\u6a21\u5757\uff0c\u6bcf\u4e2a\u6a21\u5757\u63d0\u4f9b\u4e09\u4e2a\u9884\u7b97\u5c42\u7ea7\uff08\u4f4e/\u4e2d/\u9ad8\uff09\u3002\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u6267\u884c\u8de8\u6a21\u5757\u7684\u9884\u7b97\u5c42\u7ea7\u8def\u7531\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7d27\u51d1\u795e\u7ecf\u7b56\u7565\u6765\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u548c\u5185\u5b58\u6784\u5efa\u6210\u672c\u3002\u7814\u7a76\u4e86\u4e09\u79cd\u5b9e\u73b0\u9884\u7b97\u5c42\u7ea7\u7684\u4e92\u8865\u7b56\u7565\uff1a\u5b9e\u73b0\uff08\u65b9\u6cd5\u590d\u6742\u5ea6\uff09\u3001\u63a8\u7406\uff08\u63a8\u7406\u884c\u4e3a\uff09\u548c\u5bb9\u91cf\uff08\u6a21\u5757\u6a21\u578b\u5927\u5c0f\uff09\u3002", "result": "\u5728LoCoMo\u3001LongMemEval\u548cHotpotQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBudgetMem\u5728\u4f18\u5148\u6027\u80fd\uff08\u9ad8\u9884\u7b97\u8bbe\u7f6e\uff09\u65f6\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u5728\u66f4\u7d27\u9884\u7b97\u4e0b\u63d0\u4f9b\u66f4\u597d\u7684\u51c6\u786e\u7387-\u6210\u672c\u524d\u6cbf\u3002\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u5c42\u7ea7\u7b56\u7565\u7684\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u660e\u786e\u4e86\u5728\u4e0d\u540c\u9884\u7b97\u673a\u5236\u4e0b\u6bcf\u79cd\u7b56\u7565\u4f55\u65f6\u63d0\u4f9b\u6700\u6709\u5229\u7684\u6743\u8861\u3002", "conclusion": "BudgetMem\u4e3a\u8fd0\u884c\u65f6\u4ee3\u7406\u5185\u5b58\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u663e\u5f0f\u6027\u80fd-\u6210\u672c\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u7b97\u5c42\u7ea7\u8def\u7531\u548c\u591a\u79cd\u5b9e\u73b0\u7b56\u7565\uff0c\u5728\u4e0d\u540c\u9884\u7b97\u7ea6\u675f\u4e0b\u90fd\u80fd\u5b9e\u73b0\u4f18\u8d8a\u7684\u6743\u8861\uff0c\u4e3aLLM\u4ee3\u7406\u5185\u5b58\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.06036", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06036", "abs": "https://arxiv.org/abs/2602.06036", "authors": ["Jian Chen", "Yesheng Liang", "Zhijian Liu"], "title": "DFlash: Block Diffusion for Flash Speculative Decoding", "comment": null, "summary": "Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.", "AI": {"tldr": "DFlash\u662f\u4e00\u4e2a\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5757\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5e76\u884c\u8349\u7a3f\u751f\u6210\u7684\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u81ea\u56de\u5f52\u8349\u7a3f\u65b9\u6cd5\u5b9e\u73b0\u4e866\u500d\u65e0\u635f\u52a0\u901f\u548c2.5\u500d\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u9700\u8981\u987a\u5e8f\u89e3\u7801\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\u4e14GPU\u5229\u7528\u7387\u4f4e\u3002\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u81ea\u56de\u5f52\u8349\u7a3f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u52a0\u901f\u6548\u679c\u3002\u6269\u6563\u6a21\u578b\u867d\u80fd\u5e76\u884c\u751f\u6210\uff0c\u4f46\u6027\u80fd\u901a\u5e38\u4e0d\u5982\u81ea\u56de\u5f52\u6a21\u578b\u3002", "method": "DFlash\u91c7\u7528\u8f7b\u91cf\u7ea7\u5757\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5e76\u884c\u8349\u7a3f\u751f\u6210\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210\u8349\u7a3ftoken\uff0c\u5e76\u5c06\u8349\u7a3f\u6a21\u578b\u57fa\u4e8e\u76ee\u6807\u6a21\u578b\u63d0\u53d6\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8f93\u51fa\u548c\u9ad8\u63a5\u53d7\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDFlash\u5728\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc76\u500d\u7684\u65e0\u635f\u52a0\u901f\uff0c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5EAGLE-3\u5feb2.5\u500d\u3002", "conclusion": "DFlash\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u5e76\u884c\u751f\u6210\u80fd\u529b\u548c\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3aLLM\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
