{"id": "2509.08859", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08859", "abs": "https://arxiv.org/abs/2509.08859", "authors": ["Vincenzo Suriani", "Daniele Affinita", "Domenico D. Bloisi", "Daniele Nardi"], "title": "Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication", "comment": "The 19th International Conference on Intelligent Autonomous Systems\n  (IAS 19), 2025, Genoa", "summary": "Coordinating a fully distributed multi-agent system (MAS) can be challenging\nwhen the communication channel has very limited capabilities in terms of\nsending rate and packet payload. When the MAS has to deal with active obstacles\nin a highly partially observable environment, the communication channel\nacquires considerable relevance. In this paper, we present an approach to deal\nwith task assignments in extremely active scenarios, where tasks need to be\nfrequently reallocated among the agents participating in the coordination\nprocess. Inspired by market-based task assignments, we introduce a novel\ndistributed coordination method to orchestrate autonomous agents' actions\nefficiently in low communication scenarios. In particular, our algorithm takes\ninto account asymmetric obstacles. While in the real world, the majority of\nobstacles are asymmetric, they are usually treated as symmetric ones, thus\nlimiting the applicability of existing methods. To summarize, the presented\narchitecture is designed to tackle scenarios where the obstacles are active and\nasymmetric, the communication channel is poor and the environment is partially\nobservable. Our approach has been validated in simulation and in the real\nworld, using a team of NAO robots during official RoboCup competitions.\nExperimental results show a notable reduction in task overlaps in limited\ncommunication settings, with a decrease of 52% in the most frequent reallocated\ntask."}
{"id": "2509.09024", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.09024", "abs": "https://arxiv.org/abs/2509.09024", "authors": ["Md Habib Ullah Khan", "Kaiyue Deng", "Ismail Mujtaba Khan", "Kelvin Fu"], "title": "Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites", "comment": "23 pages, 5 figures", "summary": "The demand for lightweight and high-strength composite structures is rapidly\ngrowing in aerospace and robotics, particularly for optimized drone frames.\nHowever, conventional composite manufacturing methods struggle to achieve\ncomplex 3D architectures for weight savings and rely on assembling separate\ncomponents, which introduce weak points at the joints. Additionally,\nmaintaining continuous fiber reinforcement remains challenging, limiting\nstructural efficiency. In this study, we demonstrate the lightweight Face\nCentered Cubic (FFC) lattice structured conceptualization of drone frames for\nweight reduction and complex topology fabrication through 3D Fiber Tethering\n(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,\neliminating weak points associated with traditional composite assembly.\nMechanical testing demonstrates that the fabricated drone frame exhibits a high\nspecific strength of around four to eight times the metal and thermoplastic,\noutperforming other conventional 3D printing methods. The drone frame weighs\nonly 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing\nstructural integrity and contributing to an extended flight time of three\nminutes, while flight testing confirms its stability and durability under\noperational conditions. The findings demonstrate the potential of single tow\nlattice truss-based drone frames, with 3DFiT serving as a scalable and\nefficient manufacturing method."}
{"id": "2509.09074", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09074", "abs": "https://arxiv.org/abs/2509.09074", "authors": ["Alice Kate Li", "Thales C Silva", "Victoria Edwards", "Vijay Kumar", "M. Ani Hsieh"], "title": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning", "comment": "Accepted to CoRL 2025 (Conference on Robot Learning). 15 pages 11\n  figures", "summary": "In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such\nthat it converges to the trajectory's end point. Despite demonstrated efficacy\nin using Koopman operator theory for modeling dynamical systems, Koopman does\nnot inherently enforce convergence to desired trajectories nor to specified\ngoals -- a requirement when learning from demonstrations (LfD). We present\nKoopMotion which represents motion flow fields as dynamical systems,\nparameterized by Koopman Operators to mimic desired trajectories, and leverages\nthe divergence properties of the learnt flow fields to obtain smooth motion\nfields that converge to a desired reference trajectory when a robot is placed\naway from the desired trajectory, and tracks the trajectory until the end\npoint. To demonstrate the effectiveness of our approach, we show evaluations of\nKoopMotion on the LASA human handwriting dataset and a 3D manipulator\nend-effector trajectory dataset, including spectral analysis. We also perform\nexperiments on a physical robot, verifying KoopMotion on a miniature autonomous\nsurface vehicle operating in a non-static fluid flow environment. Our approach\nis highly sample efficient in both space and time, requiring only 3\\% of the\nLASA dataset to generate dense motion plans. Additionally, KoopMotion provides\na significant improvement over baselines when comparing metrics that measure\nspatial and temporal dynamics modeling efficacy."}
{"id": "2509.09093", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09093", "abs": "https://arxiv.org/abs/2509.09093", "authors": ["Nan Mao", "Guanglu Jia", "Junpeng Chen", "Emmanouil Spyrakos-Papastavridis", "Jian S. Dai"], "title": "Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators", "comment": "50 pages, 19 figures", "summary": "Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive\nactuators, complex control, and limited adaptability to dynamic tasks. This\nstudy proposes an innovative mechanism of underactuated metamorphic loading\nmanipulators (UMLM), integrating a metamorphic arm with a passively adaptive\ngripper. The metamorphic arm exploits geometric constraints, enabling the\ntopology reconfiguration and flexible motion trajectories without additional\nactuators. The adaptive gripper, driven entirely by the arm, conforms to\ndiverse objects through passive compliance. A structural model is developed,\nand a kinetostatics analysis is conducted to investigate isomorphic grasping\nconfigurations. To optimize performance, Particle-Swarm Optimization (PSO) is\nutilized to refine the gripper's dimensional parameters, ensuring robust\nadaptability across various applications. Simulation results validate the\nUMLM's easily implemented control strategy, operational versatility, and\neffectiveness in grasping diverse objects in dynamic environments. This work\nunderscores the practical potential of underactuated metamorphic mechanisms in\napplications requiring efficient and adaptable loading solutions. Beyond the\nspecific design, this generalized modeling and optimization framework extends\nto a broader class of manipulators, offering a scalable approach to the\ndevelopment of robotic systems that require efficiency, flexibility, and robust\nperformance."}
{"id": "2509.09106", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09106", "abs": "https://arxiv.org/abs/2509.09106", "authors": ["Haokai Su", "Haoxiang Luo", "Shunpeng Yang", "Kaiwen Jiang", "Wei Zhang", "Hua Chen"], "title": "LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots", "comment": null, "summary": "Achieving stable and robust perceptive locomotion for bipedal robots in\nunstructured outdoor environments remains a critical challenge due to complex\nterrain geometry and susceptibility to external disturbances. In this work, we\npropose a novel reward design inspired by the Linear Inverted Pendulum Model\n(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM\nprovides theoretical guidance for dynamic balance by regulating the center of\nmass (CoM) height and the torso orientation. These are key factors for\nterrain-aware locomotion, as they help ensure a stable viewpoint for the\nrobot's camera. Building on this insight, we design a reward function that\npromotes balance and dynamic stability while encouraging accurate CoM\ntrajectory tracking. To adaptively trade off between velocity tracking and\nstability, we leverage the Reward Fusion Module (RFM) approach that prioritizes\nstability when needed. A double-critic architecture is adopted to separately\nevaluate stability and locomotion objectives, improving training efficiency and\nrobustness. We validate our approach through extensive experiments on a bipedal\nrobot in both simulation and real-world outdoor environments. The results\ndemonstrate superior terrain adaptability, disturbance rejection, and\nconsistent performance across a wide range of speeds and perceptual conditions."}
{"id": "2509.09141", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09141", "abs": "https://arxiv.org/abs/2509.09141", "authors": ["Jianping Li", "Xinhang Xu", "Zhongyuan Liu", "Shenghai Yuan", "Muqing Cao", "Lihua Xie"], "title": "AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes", "comment": null, "summary": "LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)\nare fundamentally limited by the narrow field of view (FoV) of compact LiDAR\nsensors and the payload constraints that preclude multi-sensor configurations.\nTraditional motorized scanning systems with fixed-speed rotations lack scene\nawareness and task-level adaptability, leading to degraded odometry and mapping\nperformance in complex, occluded environments. Inspired by the active sensing\nbehavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),\na biologically inspired and computationally efficient framework for adaptive\nLiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model\npredictive control (MPC) and reinforcement learning (RL) in a hybrid\narchitecture: an analytical uncertainty model predicts future pose\nobservability for exploitation, while a lightweight neural network learns an\nimplicit cost map from panoramic depth representations to guide exploration. To\nsupport scalable training and generalization, we develop a point cloud-based\nsimulation environment with real-world LiDAR maps across diverse scenes,\nenabling sim-to-real transfer. Extensive experiments in both simulation and\nreal-world environments demonstrate that AEOS significantly improves odometry\naccuracy compared to fixed-rate, optimization-only, and fully learned\nbaselines, while maintaining real-time performance under onboard computational\nconstraints. The project page can be found at\nhttps://kafeiyin00.github.io/AEOS/."}
{"id": "2509.09206", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.09206", "abs": "https://arxiv.org/abs/2509.09206", "authors": ["Farhad Nawaz", "Faizan M. Tariq", "Sangjae Bae", "David Isele", "Avinash Singh", "Nadia Figueroa", "Nikolai Matni", "Jovin D'sa"], "title": "Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments", "comment": null, "summary": "Accurately reasoning about future parking spot availability and integrated\nplanning is critical for enabling safe and efficient autonomous valet parking\nin dynamic, uncertain environments. Unlike existing methods that rely solely on\ninstantaneous observations or static assumptions, we present an approach that\npredicts future parking spot occupancy by explicitly distinguishing between\ninitially vacant and occupied spots, and by leveraging the predicted motion of\ndynamic agents. We introduce a probabilistic spot occupancy estimator that\nincorporates partial and noisy observations within a limited Field-of-View\n(FoV) model and accounts for the evolving uncertainty of unobserved regions.\nCoupled with this, we design a strategy planner that adaptively balances\ngoal-directed parking maneuvers with exploratory navigation based on\ninformation gain, and intelligently incorporates wait-and-go behaviors at\npromising spots. Through randomized simulations emulating large parking lots,\nwe demonstrate that our framework significantly improves parking efficiency,\nsafety margins, and trajectory smoothness compared to existing approaches."}
{"id": "2509.09283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09283", "abs": "https://arxiv.org/abs/2509.09283", "authors": ["Yueqi Zhang", "Quancheng Qian", "Taixian Hou", "Peng Zhai", "Xiaoyi Wei", "Kangmai Hu", "Jiafu Yi", "Lihua Zhang"], "title": "RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse", "comment": "Accepted for IEEE Robotics and Automation Letters (RA-L)", "summary": "Vision-based locomotion in outdoor environments presents significant\nchallenges for quadruped robots. Accurate environmental prediction and\neffective handling of depth sensor noise during real-world deployment remain\ndifficult, severely restricting the outdoor applications of such algorithms. To\naddress these deployment challenges in vision-based motion control, this letter\nproposes the Redundant Estimator Network (RENet) framework. The framework\nemploys a dual-estimator architecture that ensures robust motion performance\nwhile maintaining deployment stability during onboard vision failures. Through\nan online estimator adaptation, our method enables seamless transitions between\nestimation modules when handling visual perception uncertainties. Experimental\nvalidation on a real-world robot demonstrates the framework's effectiveness in\ncomplex outdoor environments, showing particular advantages in scenarios with\ndegraded visual perception. This framework demonstrates its potential as a\npractical solution for reliable robotic deployment in challenging field\nconditions. Project website: https://RENet-Loco.github.io/"}
{"id": "2509.09332", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09332", "abs": "https://arxiv.org/abs/2509.09332", "authors": ["Yuecheng Liu", "Dafeng Chi", "Shiguang Wu", "Zhanguang Zhang", "Yuzheng Zhuang", "Bowen Yang", "He Zhu", "Lingfeng Zhang", "Pengwei Xie", "David Gamaliel Arcos Bravo", "Yingxue Zhang", "Jianye Hao", "Xingyue Quan"], "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io"}
{"id": "2509.09364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09364", "abs": "https://arxiv.org/abs/2509.09364", "authors": ["Grzegorz Ficht", "Luis Denninger", "Sven Behnke"], "title": "AGILOped: Agile Open-Source Humanoid Robot for Research", "comment": "10th IEEE International Conference on Advanced Robotics and\n  Mechatronics (ARM), Portsmouth, UK, August 2025", "summary": "With academic and commercial interest for humanoid robots peaking, multiple\nplatforms are being developed. Through a high level of customization, they\nshowcase impressive performance. Most of these systems remain closed-source or\nhave high acquisition and maintenance costs, however. In this work, we present\nAGILOped - an open-source humanoid robot that closes the gap between high\nperformance and accessibility. Our robot is driven by off-the-shelf\nbackdrivable actuators with high power density and uses standard electronic\ncomponents. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be\noperated without a gantry by a single person. Experiments in walking, jumping,\nimpact mitigation and getting-up demonstrate its viability for use in research."}
{"id": "2509.09372", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09372", "abs": "https://arxiv.org/abs/2509.09372", "authors": ["Yihao Wang", "Pengxiang Ding", "Lingxiao Li", "Can Cui", "Zirui Ge", "Xinyang Tong", "Wenxuan Song", "Han Zhao", "Wei Zhao", "Pengxu Hou", "Siteng Huang", "Yifan Tang", "Wenhui Wang", "Ru Zhang", "Jianyi Liu", "Donglin Wang"], "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model", "comment": null, "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/."}
{"id": "2509.09404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09404", "abs": "https://arxiv.org/abs/2509.09404", "authors": ["Tongshun Chen", "Zezhou Sun", "Yanhan Sun", "Yuhao Wang", "Dezhen Song", "Ke Wu"], "title": "A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness", "comment": null, "summary": "Cable-driven continuum robots offer high flexibility and lightweight design,\nmaking them well-suited for tasks in constrained and unstructured environments.\nHowever, prolonged use can induce mechanical fatigue from plastic deformation\nand material degradation, compromising performance and risking structural\nfailure. In the state of the art, fatigue estimation of continuum robots\nremains underexplored, limiting long-term operation. To address this, we\npropose a fatigue-aware continuum robot with three key innovations: (1) a\nHybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and\nbending: passive revolute joints in the BendBeam mitigate stress concentration,\nwhile TwistBeam's limited torsional deformation reduces BendBeam stress\nmagnitude, enhancing durability; (2) a Passive Stopper that safely constrains\nmotion via mechanical constraints and employs motor torque sensing to detect\ncorresponding limit torque, ensuring safety and enabling data collection; and\n(3) a real-time fatigue-awareness method that estimates stiffness from motor\ntorque at the limit pose, enabling online fatigue estimation without additional\nsensors. Experiments show that the proposed design reduces fatigue accumulation\nby about 49% compared with a conventional design, while passive mechanical\nlimiting combined with motor-side sensing allows accurate estimation of\nstructural fatigue and damage. These results confirm the effectiveness of the\nproposed architecture for safe and reliable long-term operation."}
{"id": "2509.09484", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.09484", "abs": "https://arxiv.org/abs/2509.09484", "authors": ["Peng Zhou", "Jiaming Qi", "Hongmin Wu", "Chen Wang", "Yizhou Chen", "Zeqing Zhang"], "title": "BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging", "comment": null, "summary": "Bagging tasks, commonly found in industrial scenarios, are challenging\nconsidering deformable bags' complicated and unpredictable nature. This paper\npresents an automated bagging system from the proposed adaptive\nStructure-of-Interest (SOI) manipulation strategy for dual robot arms. The\nsystem dynamically adjusts its actions based on real-time visual feedback,\nremoving the need for pre-existing knowledge of bag properties. Our framework\nincorporates Gaussian Mixture Models (GMM) for estimating SOI states,\noptimization techniques for SOI generation, motion planning via Constrained\nBidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination\nusing Model Predictive Control (MPC). Extensive experiments validate the\ncapability of our system to perform precise and robust bagging across various\nobjects, showcasing its adaptability. This work offers a new solution for\nrobotic deformable object manipulation (DOM), particularly in automated bagging\ntasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ."}
{"id": "2509.09509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09509", "abs": "https://arxiv.org/abs/2509.09509", "authors": ["Pedro Miguel Bastos Soares", "Ali Tourani", "Miguel Fernandez-Cortizas", "Asier Bikandi Noya", "Jose Luis Sanchez-Lopez", "Holger Voos"], "title": "SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking", "comment": "12 pages, 6 figures, 5 tables", "summary": "Advancing research in fields like Simultaneous Localization and Mapping\n(SLAM) and autonomous navigation critically depends on reliable and\nreproducible multimodal datasets. While several influential datasets have\ndriven progress in these domains, they often suffer from limitations in sensing\nmodalities, environmental diversity, and the reproducibility of the underlying\nhardware setups. To address these challenges, this paper introduces SMapper, a\nnovel open-hardware, multi-sensor platform designed explicitly for, though not\nlimited to, SLAM research. The device integrates synchronized LiDAR,\nmulti-camera, and inertial sensing, supported by a robust calibration and\nsynchronization pipeline that ensures precise spatio-temporal alignment across\nmodalities. Its open and replicable design allows researchers to extend its\ncapabilities and reproduce experiments across both handheld and robot-mounted\nscenarios. To demonstrate its practicality, we additionally release\nSMapper-light, a publicly available SLAM dataset containing representative\nindoor and outdoor sequences. The dataset includes tightly synchronized\nmultimodal data and ground-truth trajectories derived from offline LiDAR-based\nSLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.\nFurthermore, the paper contains benchmarking results on state-of-the-art LiDAR\nand visual SLAM frameworks using the SMapper-light dataset. By combining\nopen-hardware design, reproducible data collection, and comprehensive\nbenchmarking, SMapper establishes a robust foundation for advancing SLAM\nalgorithm development, evaluation, and reproducibility."}
{"id": "2509.09546", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09546", "abs": "https://arxiv.org/abs/2509.09546", "authors": ["Yanhui Lu", "Zeyu Deng", "Stephen J. Redmond", "Efi Psomopoulou", "Benjamin Ward-Cherrier"], "title": "A Neuromorphic Incipient Slip Detection System using Papillae Morphology", "comment": "7 pages, 12 figures. Submitted to IEEE Robotics and Automation\n  Letters (RAL), under review", "summary": "Detecting incipient slip enables early intervention to prevent object\nslippage and enhance robotic manipulation safety. However, deploying such\nsystems on edge platforms remains challenging, particularly due to energy\nconstraints. This work presents a neuromorphic tactile sensing system based on\nthe NeuroTac sensor with an extruding papillae-based skin and a spiking\nconvolutional neural network (SCNN) for slip-state classification. The SCNN\nmodel achieves 94.33% classification accuracy across three classes (no slip,\nincipient slip, and gross slip) in slip conditions induced by sensor motion.\nUnder the dynamic gravity-induced slip validation conditions, after temporal\nsmoothing of the SCNN's final-layer spike counts, the system detects incipient\nslip at least 360 ms prior to gross slip across all trials, consistently\nidentifying incipient slip before gross slip occurs. These results demonstrate\nthat this neuromorphic system has stable and responsive incipient slip\ndetection capability."}
{"id": "2509.09594", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.09594", "abs": "https://arxiv.org/abs/2509.09594", "authors": ["Sourav Garg", "Dustin Craggs", "Vineeth Bhat", "Lachlan Mares", "Stefan Podgorski", "Madhava Krishna", "Feras Dayoub", "Ian Reid"], "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation", "comment": "CoRL 2025; 23 pages including appendix", "summary": "Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/"}
{"id": "2509.09613", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09613", "abs": "https://arxiv.org/abs/2509.09613", "authors": ["Taisei Mogi", "Mari Saito", "Yoshihiro Nakata"], "title": "MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements", "comment": null, "summary": "Robots for therapy and social interaction are often intended to evoke\n\"animacy\" in humans. While many robots imitate appearance and joint movements,\nlittle attention has been given to whole-body expansion-contraction,\nvolume-changing movements observed in living organisms, and their effect on\nanimacy perception. We developed a mobile robot called \"MOFU (Morphing Fluffy\nUnit),\" capable of whole-body expansion-contraction with a single motor and\ncovered with a fluffy exterior. MOFU employs a \"Jitterbug\" structure, a\ngeometric transformation mechanism that enables smooth volume change in\ndiameter from 210 to 280 mm using one actuator. It is also equipped with a\ndifferential two-wheel drive mechanism for locomotion. To evaluate the effect\nof expansion-contraction movements, we conducted an online survey using videos\nof MOFU's behavior. Participants rated impressions with the Godspeed\nQuestionnaire Series. First, we compared videos of MOFU in a stationary state\nwith and without expansion-contraction and turning, finding that\nexpansion-contraction significantly increased perceived animacy. Second, we\nhypothesized that presenting two MOFUs would increase animacy compared with a\nsingle robot; however, this was not supported, as no significant difference\nemerged. Exploratory analyses further compared four dual-robot motion\nconditions. Third, when expansion-contraction was combined with locomotion,\nanimacy ratings were higher than locomotion alone. These results suggest that\nvolume-changing movements such as expansion and contraction enhance perceived\nanimacy in robots and should be considered an important design element in\nfuture robot development aimed at shaping human impressions."}
{"id": "2509.09671", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09671", "abs": "https://arxiv.org/abs/2509.09671", "authors": ["Sirui Xu", "Yu-Wei Chao", "Liuyu Bian", "Arsalan Mousavian", "Yu-Xiong Wang", "Liang-Yan Gui", "Wei Yang"], "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration", "comment": "CoRL 2025", "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation."}
{"id": "2509.09674", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09674", "abs": "https://arxiv.org/abs/2509.09674", "authors": ["Haozhan Li", "Yuxin Zuo", "Jiale Yu", "Yuhao Zhang", "Zhaohui Yang", "Kaiyan Zhang", "Xuekai Zhu", "Yuchen Zhang", "Tianxing Chen", "Ganqu Cui", "Dehui Wang", "Dingxiang Luo", "Yuchen Fan", "Youbang Sun", "Jia Zeng", "Jiangmiao Pang", "Shanghang Zhang", "Yu Wang", "Yao Mu", "Bowen Zhou", "Ning Ding"], "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL"}
