{"id": "2510.09786", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09786", "abs": "https://arxiv.org/abs/2510.09786", "authors": ["Yuang Lu", "Song Wang", "Xiao Han", "Xuri Zhang", "Yucong Wu", "Zhicheng He"], "title": "Enhancing Diffusion Policy with Classifier-Free Guidance for Temporal Robotic Tasks", "comment": "7 pages, 7 figures", "summary": "Temporal sequential tasks challenge humanoid robots, as existing Diffusion\nPolicy (DP) and Action Chunking with Transformers (ACT) methods often lack\ntemporal context, resulting in local optima traps and excessive repetitive\nactions. To address these issues, this paper introduces a Classifier-Free\nGuidance-Based Diffusion Policy (CFG-DP), a novel framework to enhance DP by\nintegrating Classifier-Free Guidance (CFG) with conditional and unconditional\nmodels. Specifically, CFG leverages timestep inputs to track task progression\nand ensure precise cycle termination. It dynamically adjusts action predictions\nbased on task phase, using a guidance factor tuned to balance temporal\ncoherence and action accuracy. Real-world experiments on a humanoid robot\ndemonstrate high success rates and minimal repetitive actions. Furthermore, we\nassessed the model's ability to terminate actions and examined how different\ncomponents and parameter adjustments affect its performance. This framework\nsignificantly enhances deterministic control and execution reliability for\nsequential robotic tasks."}
{"id": "2510.09817", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09817", "abs": "https://arxiv.org/abs/2510.09817", "authors": ["Samanta Rodriguez", "Yiming Dou", "Miquel Oller", "Andrew Owens", "Nima Fazeli"], "title": "Cross-Sensor Touch Generation", "comment": "CoRL 2025", "summary": "Today's visuo-tactile sensors come in many shapes and sizes, making it\nchallenging to develop general-purpose tactile representations. This is because\nmost models are tied to a specific sensor design. To address this challenge, we\npropose two approaches to cross-sensor image generation. The first is an\nend-to-end method that leverages paired data (Touch2Touch). The second method\nbuilds an intermediate depth representation and does not require paired data\n(T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific\nmodels across multiple sensors via the cross-sensor touch generation process.\nTogether, these models offer flexible solutions for sensor translation,\ndepending on data availability and application needs. We demonstrate their\neffectiveness on downstream tasks such as in-hand pose estimation and behavior\ncloning, successfully transferring models trained on one sensor to another.\nProject page: https://samantabelen.github.io/cross_sensor_touch_generation."}
{"id": "2510.09962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09962", "abs": "https://arxiv.org/abs/2510.09962", "authors": ["Yicheng He", "Jingwen Yu", "Guangcheng Chen", "Hong Zhang"], "title": "VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene Mapping", "comment": null, "summary": "Maintaining an up-to-date map that accurately reflects recent changes in the\nenvironment is crucial, especially for robots that repeatedly traverse the same\nspace. Failing to promptly update the changed regions can degrade map quality,\nresulting in poor localization, inefficient operations, and even lost robots.\n3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online\nmap reconstruction due to its dense, differentiable, and photorealistic\nproperties, yet accurately and efficiently updating the regions of change\nremains a challenge. In this paper, we propose VG-Mapping, a novel online\n3DGS-based mapping system tailored for such semi-static scenes. Our approach\nintroduces a hybrid representation that augments 3DGS with a TSDF-based voxel\nmap to efficiently identify changed regions in a scene, along with a\nvariation-aware density control strategy that inserts or deletes Gaussian\nprimitives in regions undergoing change. Furthermore, to address the absence of\npublic benchmarks for this task, we construct a RGB-D dataset comprising both\nsynthetic and real-world semi-static environments. Experimental results\ndemonstrate that our method substantially improves the rendering quality and\nmap update efficiency in semi-static scenes. The code and dataset are available\nat https://github.com/heyicheng-never/VG-Mapping."}
{"id": "2510.09963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09963", "abs": "https://arxiv.org/abs/2510.09963", "authors": ["Chaoran Wang", "Jingyuan Sun", "Yanhui Zhang", "Mingyu Zhang", "Changju Wu"], "title": "LLM-HBT: Dynamic Behavior Tree Construction for Adaptive Coordination in Heterogeneous Robots", "comment": "It contains 8 pages, 7 figures and 4 tables. This paper is submitted\n  to ICRA 2026", "summary": "We introduce a novel framework for automatic behavior tree (BT) construction\nin heterogeneous multi-robot systems, designed to address the challenges of\nadaptability and robustness in dynamic environments. Traditional robots are\nlimited by fixed functional attributes and cannot efficiently reconfigure their\nstrategies in response to task failures or environmental changes. To overcome\nthis limitation, we leverage large language models (LLMs) to generate and\nextend BTs dynamically, combining the reasoning and generalization power of\nLLMs with the modularity and recovery capability of BTs. The proposed framework\nconsists of four interconnected modules task initialization, task assignment,\nBT update, and failure node detection which operate in a closed loop. Robots\ntick their BTs during execution, and upon encountering a failure node, they can\neither extend the tree locally or invoke a centralized virtual coordinator\n(Alex) to reassign subtasks and synchronize BTs across peers. This design\nenables long-term cooperative execution in heterogeneous teams. We validate the\nframework on 60 tasks across three simulated scenarios and in a real-world cafe\nenvironment with a robotic arm and a wheeled-legged robot. Results show that\nour method consistently outperforms baseline approaches in task success rate,\nrobustness, and scalability, demonstrating its effectiveness for multi-robot\ncollaboration in complex scenarios."}
{"id": "2510.09966", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09966", "abs": "https://arxiv.org/abs/2510.09966", "authors": ["Easton R. Potokar", "Taylor Pool", "Daniel McGann", "Michael Kaess"], "title": "FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors", "comment": "Submitted to ICRA 2026", "summary": "Light Detection and Ranging (LiDAR) sensors have become a de-facto sensor for\nmany robot state estimation tasks, spurring development of many LiDAR Odometry\n(LO) methods in recent years. While some smoothing-based LO methods have been\nproposed, most require matching against multiple scans, resulting in\nsub-real-time performance. Due to this, most prior works estimate a single\nstate at a time and are ``submap''-based. This architecture propagates any\nerror in pose estimation to the fixed submap and can cause jittery trajectories\nand degrade future registrations. We propose Fixed-Lag Odometry with Reparative\nMapping (FORM), a LO method that performs smoothing over a densely connected\nfactor graph while utilizing a single iterative map for matching. This allows\nfor both real-time performance and active correction of the local map as pose\nestimates are further refined. We evaluate on a wide variety of datasets to\nshow that FORM is robust, accurate, real-time, and provides smooth trajectory\nestimates when compared to prior state-of-the-art LO methods."}
{"id": "2510.09980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09980", "abs": "https://arxiv.org/abs/2510.09980", "authors": ["Jingyuan Sun", "Hongyu Ji", "Zihan Qu", "Chaoran Wang", "Mingyu Zhang"], "title": "ATRos: Learning Energy-Efficient Agile Locomotion for Wheeled-legged Robots", "comment": "4 pages, 2 figures, submitted to IROS 2025 wheeled-legged workshop", "summary": "Hybrid locomotion of wheeled-legged robots has recently attracted increasing\nattention due to their advantages of combining the agility of legged locomotion\nand the efficiency of wheeled motion. But along with expanded performance, the\nwhole-body control of wheeled-legged robots remains challenging for hybrid\nlocomotion. In this paper, we present ATRos, a reinforcement learning\n(RL)-based hybrid locomotion framework to achieve hybrid walking-driving\nmotions on the wheeled-legged robot. Without giving predefined gait patterns,\nour planner aims to intelligently coordinate simultaneous wheel and leg\nmovements, thereby achieving improved terrain adaptability and improved energy\nefficiency. Based on RL techniques, our approach constructs a prediction policy\nnetwork that could estimate external environmental states from proprioceptive\nsensory information, and the outputs are then fed into an actor critic network\nto produce optimal joint commands. The feasibility of the proposed framework is\nvalidated through both simulations and real-world experiments across diverse\nterrains, including flat ground, stairs, and grassy surfaces. The hybrid\nlocomotion framework shows robust performance over various unseen terrains,\nhighlighting its generalization capability."}
{"id": "2510.10016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10016", "abs": "https://arxiv.org/abs/2510.10016", "authors": ["Shahid Ansari", "Vivek Gupta", "Bishakh Bhattacharya"], "title": "Hybrid Robotic Meta-gripper for Tomato Harvesting: Analysis of Auxetic Structures with Lattice Orientation Variations", "comment": null, "summary": "The agricultural sector is rapidly evolving to meet growing global food\ndemands, yet tasks like fruit and vegetable handling remain labor-intensive,\ncausing inefficiencies and post-harvest losses. Automation, particularly\nselective harvesting, offers a viable solution, with soft robotics emerging as\na key enabler. This study introduces a novel hybrid gripper for tomato\nharvesting, incorporating a rigid outer frame with a soft auxetic internal\nlattice. The six-finger, 3D caging-effect design enables gentle yet secure\ngrasping in unstructured environments. Uniquely, the work investigates the\neffect of auxetic lattice orientation on grasping conformability, combining\nexperimental validation with 2D Digital Image Correlation (DIC) and nonlinear\nfinite element analysis (FEA). Auxetic configurations with unit cell\ninclinations of 0 deg, 30 deg, 45 deg, and 60 deg are evaluated, and their\ngrasping forces, deformation responses, and motor torque requirements are\nsystematically compared. Results demonstrate that lattice orientation strongly\ninfluences compliance, contact forces, and energy efficiency, with distinct\nadvantages across configurations. This comparative framework highlights the\nnovelty of tailoring auxetic geometries to optimize robotic gripper\nperformance. The findings provide new insights into soft-rigid hybrid gripper\ndesign, advancing automation strategies for precision agriculture while\nminimizing crop damage."}
{"id": "2510.10046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10046", "abs": "https://arxiv.org/abs/2510.10046", "authors": ["Mingke Lu", "Shuaikang Wang", "Meng Guo"], "title": "LOMORO: Long-term Monitoring of Dynamic Targets with Minimum Robotic Fleet under Resource Constraints", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2025)", "summary": "Long-term monitoring of numerous dynamic targets can be tedious for a human\noperator and infeasible for a single robot, e.g., to monitor wild flocks,\ndetect intruders, search and rescue. Fleets of autonomous robots can be\neffective by acting collaboratively and concurrently. However, the online\ncoordination is challenging due to the unknown behaviors of the targets and the\nlimited perception of each robot. Existing work often deploys all robots\navailable without minimizing the fleet size, or neglects the constraints on\ntheir resources such as battery and memory. This work proposes an online\ncoordination scheme called LOMORO for collaborative target monitoring, path\nrouting and resource charging. It includes three core components: (I) the\nmodeling of multi-robot task assignment problem under the constraints on\nresources and monitoring intervals; (II) the resource-aware task coordination\nalgorithm iterates between the high-level assignment of dynamic targets and the\nlow-level multi-objective routing via the Martin's algorithm; (III) the online\nadaptation algorithm in case of unpredictable target behaviors and robot\nfailures. It ensures the explicitly upper-bounded monitoring intervals for all\ntargets and the lower-bounded resource levels for all robots, while minimizing\nthe average number of active robots. The proposed methods are validated\nextensively via large-scale simulations against several baselines, under\ndifferent road networks, robot velocities, charging rates and monitoring\nintervals."}
{"id": "2510.10059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10059", "abs": "https://arxiv.org/abs/2510.10059", "authors": ["Keidai Iiyama", "Grace Gao"], "title": "Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model", "comment": "Submitted NAVIGATION: Journal of the Institute of Navigation", "summary": "Recent advancements in lunar positioning, navigation, and timing (PNT) have\ndemonstrated that terrestrial GNSS signals, including weak sidelobe\ntransmissions, can be exploited for lunar spacecraft positioning and timing.\nWhile GNSS-based navigation at the Moon has been validated recently, unmodeled\nionospheric and plasmaspheric delays remain a significant error source,\nparticularly given the unique signal geometry and extended propagation paths.\nThis paper characterizes these delays using the Global Core Plasma Model (GCPM)\nand a custom low-cost ray-tracing algorithm that iteratively solves for bent\nsignal paths. We simulate first-, second-, and third-order group delays, as\nwell as excess path length from ray bending, for GNSS signals received at both\nlunar orbit and the lunar south pole under varying solar and geomagnetic\nconditions. Results show that mean group delays are typically on the order of 1\nm, but can exceed 100 m for low-altitude ray paths during high solar activity,\nwhile bending delays are generally smaller but non-negligible for low-altitude\nray paths. We also quantify the influence of signal frequency, geomagnetic\n$K_p$ index, and solar R12 index. These findings inform the design of robust\npositioning and timing algorithms that utilize terrestrial GNSS signals."}
{"id": "2510.10086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10086", "abs": "https://arxiv.org/abs/2510.10086", "authors": ["Feifei Liu", "Haozhe Wang", "Zejun Wei", "Qirong Lu", "Yiyang Wen", "Xiaoyu Tang", "Jingyan Jiang", "Zhijian He"], "title": "Beyond ADE and FDE: A Comprehensive Evaluation Framework for Safety-Critical Prediction in Multi-Agent Autonomous Driving Scenarios", "comment": null, "summary": "Current evaluation methods for autonomous driving prediction models rely\nheavily on simplistic metrics such as Average Displacement Error (ADE) and\nFinal Displacement Error (FDE). While these metrics offer basic performance\nassessments, they fail to capture the nuanced behavior of prediction modules\nunder complex, interactive, and safety-critical driving scenarios. For\ninstance, existing benchmarks do not distinguish the influence of nearby versus\ndistant agents, nor systematically test model robustness across varying\nmulti-agent interactions. This paper addresses this critical gap by proposing a\nnovel testing framework that evaluates prediction performance under diverse\nscene structures, saying, map context, agent density and spatial distribution.\nThrough extensive empirical analysis, we quantify the differential impact of\nagent proximity on target trajectory prediction and identify scenario-specific\nfailure cases that are not exposed by traditional metrics. Our findings\nhighlight key vulnerabilities in current state-of-the-art prediction models and\ndemonstrate the importance of scenario-aware evaluation. The proposed framework\nlays the groundwork for rigorous, safety-driven prediction validation,\ncontributing significantly to the identification of failure-prone corner cases\nand the development of robust, certifiable prediction systems for autonomous\nvehicles."}
{"id": "2510.10125", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10125", "abs": "https://arxiv.org/abs/2510.10125", "authors": ["Yanjiang Guo", "Lucy Xiaoyang Shi", "Jianyu Chen", "Chelsea Finn"], "title": "Ctrl-World: A Controllable Generative World Model for Robot Manipulation", "comment": "17 pages", "summary": "Generalist robot policies can now perform a wide range of manipulation\nskills, but evaluating and improving their ability with unfamiliar objects and\ninstructions remains a significant challenge. Rigorous evaluation requires a\nlarge number of real-world rollouts, while systematic improvement demands\nadditional corrective data with expert labels. Both of these processes are\nslow, costly, and difficult to scale. World models offer a promising, scalable\nalternative by enabling policies to rollout within imagination space. However,\na key challenge is building a controllable world model that can handle\nmulti-step interactions with generalist robot policies. This requires a world\nmodel compatible with modern generalist policies by supporting multi-view\nprediction, fine-grained action control, and consistent long-horizon\ninteractions, which is not achieved by previous works. In this paper, we make a\nstep forward by introducing a controllable multi-view world model that can be\nused to evaluate and improve the instruction-following ability of generalist\nrobot policies. Our model maintains long-horizon consistency with a\npose-conditioned memory retrieval mechanism and achieves precise action control\nthrough frame-level action conditioning. Trained on the DROID dataset (95k\ntrajectories, 564 scenes), our model generates spatially and temporally\nconsistent trajectories under novel scenarios and new camera placements for\nover 20 seconds. We show that our method can accurately rank policy performance\nwithout real-world robot rollouts. Moreover, by synthesizing successful\ntrajectories in imagination and using them for supervised fine-tuning, our\napproach can improve policy success by 44.7\\%."}
{"id": "2510.10154", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10154", "abs": "https://arxiv.org/abs/2510.10154", "authors": ["LinFeng Li", "Jian Zhao", "Yuan Xie", "Xin Tan", "Xuelong Li"], "title": "CompassNav: Steering From Path Imitation To Decision Understanding In Navigation", "comment": null, "summary": "The dominant paradigm for training Large Vision-Language Models (LVLMs) in\nnavigation relies on imitating expert trajectories. This approach reduces the\ncomplex navigation task to a sequence-to-sequence replication of a single\ncorrect path, fundamentally limiting the agent's ability to explore and\ngeneralize. In this work, we argue for and introduce a new paradigm: a shift\nfrom Path Imitation to Decision Understanding. The goal of this paradigm is to\nbuild agents that do not just follow, but truly understand how to navigate. We\nmaterialize this through two core contributions: first, we introduce\nCompass-Data-22k, a novel 22k-trajectory dataset.Its Reinforcement Fine-Tuning\n(RFT) subset provides a panoramic view of the decision landscape by annotating\nall feasible actions with A* geodesic distances. Second, we design a novel\ngap-aware hybrid reward function that dynamically adapts its feedback to\ndecision certainty, shifting between decisive signals for optimal actions and\nnuanced scores to encourage exploration. Integrated into an SFT-then-RFT\nrecipe, our CompassNav agent is trained not to memorize static routes, but to\ndevelop an internal ``compass'' that constantly intuits the direction to the\ngoal by evaluating the relative quality of all possible moves. This approach\nenables our 7B agent to set a new state-of-the-art on Goal navigation\nbenchmarks, outperforming even larger proprietary models, and achieve robust\nreal-world goal navigation on a physical robot."}
{"id": "2510.10181", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10181", "abs": "https://arxiv.org/abs/2510.10181", "authors": ["Shaokai Wu", "Yanbiao Ji", "Qiuchang Li", "Zhiyi Zhang", "Qichen He", "Wenyuan Xie", "Guodong Zhang", "Bayram Bayramli", "Yue Ding", "Hongtao Lu"], "title": "Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback", "comment": null, "summary": "Embodied agents face a fundamental limitation: once deployed in real-world\nenvironments to perform specific tasks, they are unable to acquire new useful\nknowledge to enhance task performance. In this paper, we propose a general\npost-deployment learning framework called Dejavu, which employs an Experience\nFeedback Network (EFN) and augments the frozen Vision-Language-Action (VLA)\npolicy with retrieved execution memories. EFN automatically identifies\ncontextually successful prior action experiences and conditions action\nprediction on this retrieved guidance. We adopt reinforcement learning with\nsemantic similarity rewards on EFN to ensure that the predicted actions align\nwith past successful behaviors under current observations. During deployment,\nEFN continually enriches its memory with new trajectories, enabling the agent\nto exhibit \"learning from experience\" despite fixed weights. Experiments across\ndiverse embodied tasks show that EFN significantly improves adaptability,\nrobustness, and success rates over frozen baselines. These results highlight a\npromising path toward embodied agents that continually refine their behavior\nafter deployment."}
{"id": "2510.10206", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10206", "abs": "https://arxiv.org/abs/2510.10206", "authors": ["Zuhong Liu", "Junhao Ge", "Minhao Xiong", "Jiahao Gu", "Bowei Tang", "Wei Jing", "Siheng Chen"], "title": "It Takes Two: Learning Interactive Whole-Body Control Between Humanoid Robots", "comment": null, "summary": "The true promise of humanoid robotics lies beyond single-agent autonomy: two\nor more humanoids must engage in physically grounded, socially meaningful\nwhole-body interactions that echo the richness of human social interaction.\nHowever, single-humanoid methods suffer from the isolation issue, ignoring\ninter-agent dynamics and causing misaligned contacts, interpenetrations, and\nunrealistic motions. To address this, we present Harmanoid , a dual-humanoid\nmotion imitation framework that transfers interacting human motions to two\nrobots while preserving both kinematic fidelity and physical realism. Harmanoid\ncomprises two key components: (i) contact-aware motion retargeting, which\nrestores inter-body coordination by aligning SMPL contacts with robot vertices,\nand (ii) interaction-driven motion controller, which leverages\ninteraction-specific rewards to enforce coordinated keypoints and physically\nplausible contacts. By explicitly modeling inter-agent contacts and\ninteraction-aware dynamics, Harmanoid captures the coupled behaviors between\nhumanoids that single-humanoid frameworks inherently overlook. Experiments\ndemonstrate that Harmanoid significantly improves interactive motion imitation,\nsurpassing existing single-humanoid frameworks that largely fail in such\nscenarios."}
{"id": "2510.10217", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10217", "abs": "https://arxiv.org/abs/2510.10217", "authors": ["Hyogo Hiruma", "Hiroshi Ito", "Tetsuya Ogata"], "title": "UF-RNN: Real-Time Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction", "comment": "8 pages, 6 figures", "summary": "Training robots to operate effectively in environments with uncertain states,\nsuch as ambiguous object properties or unpredictable interactions, remains a\nlongstanding challenge in robotics. Imitation learning methods typically rely\non successful examples and often neglect failure scenarios where uncertainty is\nmost pronounced. To address this limitation, we propose the Uncertainty-driven\nForesight Recurrent Neural Network (UF-RNN), a model that combines standard\ntime-series prediction with an active \"Foresight\" module. This module performs\ninternal simulations of multiple future trajectories and refines the hidden\nstate to minimize predicted variance, enabling the model to selectively explore\nactions under high uncertainty. We evaluate UF-RNN on a door-opening task in\nboth simulation and a real-robot setting, demonstrating that, despite the\nabsence of explicit failure demonstrations, the model exhibits robust\nadaptation by leveraging self-induced chaotic dynamics in its latent space.\nWhen guided by the Foresight module, these chaotic properties stimulate\nexploratory behaviors precisely when the environment is ambiguous, yielding\nimproved success rates compared to conventional stochastic RNN baselines. These\nfindings suggest that integrating uncertainty-driven foresight into imitation\nlearning pipelines can significantly enhance a robot's ability to handle\nunpredictable real-world conditions."}
{"id": "2510.10221", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10221", "abs": "https://arxiv.org/abs/2510.10221", "authors": ["Hyogo Hiruma", "Hiroshi Ito", "Hiroki Mori", "Tetsuya Ogata"], "title": "A3RNN: Bi-directional Fusion of Bottom-up and Top-down Process for Developmental Visual Attention in Robots", "comment": "8 pages, 5 figures", "summary": "This study investigates the developmental interaction between top-down (TD)\nand bottom-up (BU) visual attention in robotic learning. Our goal is to\nunderstand how structured, human-like attentional behavior emerges through the\nmutual adaptation of TD and BU mechanisms over time. To this end, we propose a\nnovel attention model $A^3 RNN$ that integrates predictive TD signals and\nsaliency-based BU cues through a bi-directional attention architecture.\n  We evaluate our model in robotic manipulation tasks using imitation learning.\nExperimental results show that attention behaviors evolve throughout training,\nfrom saliency-driven exploration to prediction-driven direction. Initially, BU\nattention highlights visually salient regions, which guide TD processes, while\nas learning progresses, TD attention stabilizes and begins to reshape what is\nperceived as salient. This trajectory reflects principles from cognitive\nscience and the free-energy framework, suggesting the importance of\nself-organizing attention through interaction between perception and internal\nprediction. Although not explicitly optimized for stability, our model exhibits\nmore coherent and interpretable attention patterns than baselines, supporting\nthe idea that developmental mechanisms contribute to robust attention\nformation."}
{"id": "2510.10273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10273", "abs": "https://arxiv.org/abs/2510.10273", "authors": ["Vincent Schoenbach", "Marvin Wiedemann", "Raphael Memmesheimer", "Malte Mosbach", "Sven Behnke"], "title": "Integration of the TIAGo Robot into Isaac Sim with Mecanum Drive Modeling and Learned S-Curve Velocity Profiles", "comment": "In Proceedings of IEEE 21st International Conference on Automation\n  Science and Engineering (CASE), Los Angeles, USA, August 2025", "summary": "Efficient physics simulation has significantly accelerated research progress\nin robotics applications such as grasping and assembly. The advent of\nGPU-accelerated simulation frameworks like Isaac Sim has particularly empowered\nlearning-based methods, enabling them to tackle increasingly complex tasks. The\nPAL Robotics TIAGo++ Omni is a versatile mobile manipulator equipped with a\nmecanum-wheeled base, allowing omnidirectional movement and a wide range of\ntask capabilities. However, until now, no model of the robot has been available\nin Isaac Sim. In this paper, we introduce such a model, calibrated to\napproximate the behavior of the real robot, with a focus on its omnidirectional\ndrive dynamics. We present two control models for the omnidirectional drive: a\nphysically accurate model that replicates real-world wheel dynamics and a\nlightweight velocity-based model optimized for learning-based applications.\nWith these models, we introduce a learning-based calibration approach to\napproximate the real robot's S-shaped velocity profile using minimal trajectory\ndata recordings. This simulation should allow researchers to experiment with\nthe robot and perform efficient learning-based control in diverse environments.\nWe provide the integration publicly at https://github.com/AIS-Bonn/tiago_isaac."}
{"id": "2510.10274", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10274", "abs": "https://arxiv.org/abs/2510.10274", "authors": ["Jinliang Zheng", "Jianxiong Li", "Zhihao Wang", "Dongxiu Liu", "Xirui Kang", "Yuchun Feng", "Yinan Zheng", "Jiayin Zou", "Yilun Chen", "Jia Zeng", "Ya-Qin Zhang", "Jiangmiao Pang", "Jingjing Liu", "Tai Wang", "Xianyuan Zhan"], "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model", "comment": "preprint, technical report, 33 pages", "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/"}
{"id": "2510.10332", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10332", "abs": "https://arxiv.org/abs/2510.10332", "authors": ["Kohio Deflesselle", "Mélodie Daniel", "Aly Magassouba", "Miguel Aranda", "Olivier Ly"], "title": "Towards Safe Maneuvering of Double-Ackermann-Steering Robots with a Soft Actor-Critic Framework", "comment": "4 pages, 3 figures, 2 tables, Accepted for Safety of Intelligent and\n  Autonomous Vehicles: Formal Methods vs. Machine Learning approaches for\n  reliable navigation (SIAV-FM2L) an IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025) workshop", "summary": "We present a deep reinforcement learning framework based on Soft Actor-Critic\n(SAC) for safe and precise maneuvering of double-Ackermann-steering mobile\nrobots (DASMRs). Unlike holonomic or simpler non-holonomic robots such as\ndifferential-drive robots, DASMRs face strong kinematic constraints that make\nclassical planners brittle in cluttered environments. Our framework leverages\nthe Hindsight Experience Replay (HER) and the CrossQ overlay to encourage\nmaneuvering efficiency while avoiding obstacles. Simulation results with a\nheavy four-wheel-steering rover show that the learned policy can robustly reach\nup to 97% of target positions while avoiding obstacles. Our framework does not\nrely on handcrafted trajectories or expert demonstrations."}
{"id": "2510.10337", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10337", "abs": "https://arxiv.org/abs/2510.10337", "authors": ["Jihong Zhu", "Kefeng Huang", "Jonathon Pipe", "Chris Horbaczewsky", "Andy Tyrrell", "Ian J. S. Fairlamb"], "title": "Rise of the Robochemist", "comment": "This article was originally published in the IEEE Systems, Man, and\n  Cybernetics Society eNewsletter, September 2025 issue:\n  https://www.ieeesmc.org/wp-content/uploads/2024/10/FeatureArticle_Sept25.pdf", "summary": "Chemistry, a long-standing discipline, has historically relied on manual and\noften time-consuming processes. While some automation exists, the field is now\non the cusp of a significant evolution driven by the integration of robotics\nand artificial intelligence (AI), giving rise to the concept of the\nrobochemist: a new paradigm where autonomous systems assist in designing,\nexecuting, and analyzing experiments. Robochemists integrate mobile\nmanipulators, advanced perception, teleoperation, and data-driven protocols to\nexecute experiments with greater adaptability, reproducibility, and safety.\nRather than a fully automated replacement for human chemists, we envisioned the\nrobochemist as a complementary partner that works collaboratively to enhance\ndiscovery, enabling a more efficient exploration of chemical space and\naccelerating innovation in pharmaceuticals, materials science, and sustainable\nmanufacturing. This article traces the technologies, applications, and\nchallenges that define this transformation, highlighting both the opportunities\nand the responsibilities that accompany the emergence of the robochemist.\nUltimately, the future of chemistry is argued to lie in a symbiotic partnership\nwhere human intuition and expertise is amplified by robotic precision and\nAI-driven insight."}
{"id": "2510.10346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10346", "abs": "https://arxiv.org/abs/2510.10346", "authors": ["Yuxiang Peng", "Chuchu Chen", "Kejian Wu", "Guoquan Huang"], "title": "sqrtVINS: Robust and Ultrafast Square-Root Filter-based 3D Motion Tracking", "comment": null, "summary": "In this paper, we develop and open-source, for the first time, a square-root\nfilter (SRF)-based visual-inertial navigation system (VINS), termed sqrtVINS,\nwhich is ultra-fast, numerically stable, and capable of dynamic initialization\neven under extreme conditions (i.e., extremely small time window). Despite\nrecent advancements in VINS, resource constraints and numerical instability on\nembedded (robotic) systems with limited precision remain critical challenges. A\nsquare-root covariance-based filter offers a promising solution by providing\nnumerical stability, efficient memory usage, and guaranteed positive\nsemi-definiteness. However, canonical SRFs suffer from inefficiencies caused by\ndisruptions in the triangular structure of the covariance matrix during\nupdates. The proposed method significantly improves VINS efficiency with a\nnovel Cholesky decomposition (LLT)-based SRF update, by fully exploiting the\nsystem structure to preserve the structure. Moreover, we design a fast, robust,\ndynamic initialization method, which first recovers the minimal states without\ntriangulating 3D features and then efficiently performs iterative SRF update to\nrefine the full states, enabling seamless VINS operation. The proposed\nLLT-based SRF is extensively verified through numerical studies, demonstrating\nsuperior numerical stability and achieving robust efficient performance on\n32-bit single-precision floats, operating at twice the speed of\nstate-of-the-art (SOTA) methods. Our initialization method, tested on both\nmobile workstations and Jetson Nano computers, achieving a high success rate of\ninitialization even within a 100 ms window under minimal conditions. Finally,\nthe proposed sqrtVINS is extensively validated across diverse scenarios,\ndemonstrating strong efficiency, robustness, and reliability. The full\nopen-source implementation is released to support future research and\napplications."}
{"id": "2510.10357", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10357", "abs": "https://arxiv.org/abs/2510.10357", "authors": ["Yang Liu", "Bruno Da Costa", "Aude Billard"], "title": "Learning to Throw-Flip", "comment": "Accepted to IROS 2025. Video Summary: https://youtu.be/txYc9b1oflU", "summary": "Dynamic manipulation, such as robot tossing or throwing objects, has recently\ngained attention as a novel paradigm to speed up logistic operations. However,\nthe focus has predominantly been on the object's landing location, irrespective\nof its final orientation. In this work, we present a method enabling a robot to\naccurately \"throw-flip\" objects to a desired landing pose (position and\norientation). Conventionally, objects thrown by revolute robots suffer from\nparasitic rotation, resulting in highly restricted and uncontrollable landing\nposes. Our approach is based on two key design choices: first, leveraging the\nimpulse-momentum principle, we design a family of throwing motions that\neffectively decouple the parasitic rotation, significantly expanding the\nfeasible set of landing poses. Second, we combine a physics-based model of free\nflight with regression-based learning methods to account for unmodeled effects.\nReal robot experiments demonstrate that our framework can learn to throw-flip\nobjects to a pose target within ($\\pm$5 cm, $\\pm$45 degrees) threshold in\ndozens of trials. Thanks to data assimilation, incorporating projectile\ndynamics reduces sample complexity by an average of 40% when throw-flipping to\nunseen poses compared to end-to-end learning methods. Additionally, we show\nthat past knowledge on in-hand object spinning can be effectively reused,\naccelerating learning by 70% when throwing a new object with a Center of Mass\n(CoM) shift. A video summarizing the proposed method and the hardware\nexperiments is available at https://youtu.be/txYc9b1oflU."}
{"id": "2510.10379", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10379", "abs": "https://arxiv.org/abs/2510.10379", "authors": ["Rohan Gupta", "Trevor Asbery", "Zain Merchant", "Abrar Anwar", "Jesse Thomason"], "title": "RobotFleet: An Open-Source Framework for Centralized Multi-Robot Task Planning", "comment": null, "summary": "Coordinating heterogeneous robot fleets to achieve multiple goals is\nchallenging in multi-robot systems. We introduce an open-source and extensible\nframework for centralized multi-robot task planning and scheduling that\nleverages LLMs to enable fleets of heterogeneous robots to accomplish multiple\ntasks. RobotFleet provides abstractions for planning, scheduling, and execution\nacross robots deployed as containerized services to simplify fleet scaling and\nmanagement. The framework maintains a shared declarative world state and\ntwo-way communication for task execution and replanning. By modularizing each\nlayer of the autonomy stack and using LLMs for open-world reasoning, RobotFleet\nlowers the barrier to building scalable multi-robot systems. The code can be\nfound here: https://github.com/therohangupta/robot-fleet."}
{"id": "2510.10392", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10392", "abs": "https://arxiv.org/abs/2510.10392", "authors": ["Max Sokolich", "Yanda Yang", "Subrahmanyam Cherukumilli", "Fatma Ceren Kirmizitas", "Sambeeta Das"], "title": "MicroRoboScope: A Portable and Integrated Mechatronic Platform for Magnetic and Acoustic Microrobotic Experimentation", "comment": null, "summary": "This paper presents MicroRoboScope, a portable, compact, and versatile\nmicrorobotic experimentation platform designed for real-time, closed-loop\ncontrol of both magnetic and acoustic microrobots. The system integrates an\nembedded computer, microscope, power supplies, and control circuitry into a\nsingle, low-cost and fully integrated apparatus. Custom control software\ndeveloped in Python and Arduino C++ handles live video acquisition, microrobot\ntracking, and generation of control signals for electromagnetic coils and\nacoustic transducers. The platform's multi-modal actuation, accessibility, and\nportability make it suitable not only for specialized research laboratories but\nalso for educational and outreach settings. By lowering the barrier to entry\nfor microrobotic experimentation, this system enables new opportunities for\nresearch, education, and translational applications in biomedicine, tissue\nengineering, and robotics."}
{"id": "2510.10421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10421", "abs": "https://arxiv.org/abs/2510.10421", "authors": ["Junbin Yuan", "Brady Moon", "Muqing Cao", "Sebastian Scherer"], "title": "Hierarchical Planning for Long-Horizon Multi-Target Tracking Under Target Motion Uncertainty", "comment": "8 pages, 7 figures. Accepted to IEEE Robotics and Automation Letters\n  (RAL), 2025", "summary": "Achieving persistent tracking of multiple dynamic targets over a large\nspatial area poses significant challenges for a single-robot system with\nconstrained sensing capabilities. As the robot moves to track different\ntargets, the ones outside the field of view accumulate uncertainty, making them\nprogressively harder to track. An effective path planning algorithm must manage\nuncertainty over a long horizon and account for the risk of permanently losing\ntrack of targets that remain unseen for too long. However, most existing\napproaches rely on short planning horizons and assume small, bounded\nenvironments, resulting in poor tracking performance and target loss in\nlarge-scale scenarios. In this paper, we present a hierarchical planner for\ntracking multiple moving targets with an aerial vehicle. To address the\nchallenge of tracking non-static targets, our method incorporates motion models\nand uncertainty propagation during path execution, allowing for more informed\ndecision-making. We decompose the multi-target tracking task into sub-tasks of\nsingle target search and detection, and our proposed pipeline consists a novel\nlow-level coverage planner that enables searching for a target in an evolving\nbelief area, and an estimation method to assess the likelihood of success for\neach sub-task, making it possible to convert the active target tracking task to\na Markov decision process (MDP) that we solve with a tree-based algorithm to\ndetermine the sequence of sub-tasks. We validate our approach in simulation,\ndemonstrating its effectiveness compared to existing planners for active target\ntracking tasks, and our proposed planner outperforms existing approaches,\nachieving a reduction of 11-70% in final uncertainty across different\nenvironments."}
{"id": "2510.10455", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10455", "abs": "https://arxiv.org/abs/2510.10455", "authors": ["Jiayu Ding", "Xulin Chen", "Garrett E. Katz", "Zhenyu Gan"], "title": "Towards Dynamic Quadrupedal Gaits: A Symmetry-Guided RL Hierarchy Enables Free Gait Transitions at Varying Speeds", "comment": null, "summary": "Quadrupedal robots exhibit a wide range of viable gaits, but generating\nspecific footfall sequences often requires laborious expert tuning of numerous\nvariables, such as touch-down and lift-off events and holonomic constraints for\neach leg. This paper presents a unified reinforcement learning framework for\ngenerating versatile quadrupedal gaits by leveraging the intrinsic symmetries\nand velocity-period relationship of dynamic legged systems. We propose a\nsymmetry-guided reward function design that incorporates temporal,\nmorphological, and time-reversal symmetries. By focusing on preserved\nsymmetries and natural dynamics, our approach eliminates the need for\npredefined trajectories, enabling smooth transitions between diverse locomotion\npatterns such as trotting, bounding, half-bounding, and galloping. Implemented\non the Unitree Go2 robot, our method demonstrates robust performance across a\nrange of speeds in both simulations and hardware tests, significantly improving\ngait adaptability without extensive reward tuning or explicit foot placement\ncontrol. This work provides insights into dynamic locomotion strategies and\nunderscores the crucial role of symmetries in robotic gait design."}
{"id": "2510.10468", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10468", "abs": "https://arxiv.org/abs/2510.10468", "authors": ["Robert Mahony", "Jonathan Kelly", "Stephan Weiss"], "title": "Galilean Symmetry in Robotics", "comment": "Under Review", "summary": "Galilean symmetry is the natural symmetry of inertial motion that underpins\nNewtonian physics. Although rigid-body symmetry is one of the most established\nand fundamental tools in robotics, there appears to be no comparable treatment\nof Galilean symmetry for a robotics audience. In this paper, we present a\nrobotics-tailored exposition of Galilean symmetry that leverages the\ncommunity's familiarity with and understanding of rigid-body transformations\nand pose representations. Our approach contrasts with common treatments in the\nphysics literature that introduce Galilean symmetry as a stepping stone to\nEinstein's relativity. A key insight is that the Galilean matrix Lie group can\nbe used to describe two different pose representations, Galilean frames, that\nuse inertial velocity in the state definition, and extended poses, that use\ncoordinate velocity. We provide three examples where applying the Galilean\nmatrix Lie-group algebra to robotics problems is straightforward and yields\nsignificant insights: inertial navigation above the rotating Earth, manipulator\nkinematics, and sensor data fusion under temporal uncertainty. We believe that\nthe time is right for the robotics community to benefit from rediscovering and\nextending this classical material and applying it to modern problems."}
{"id": "2510.10506", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10506", "abs": "https://arxiv.org/abs/2510.10506", "authors": ["Kush Garg", "Akshat Dave"], "title": "SuperEx: Enhancing Indoor Mapping and Exploration using Non-Line-of-Sight Perception", "comment": "8 pages, 9 Figures , Project webpage: https://super-ex.github.io/", "summary": "Efficient exploration and mapping in unknown indoor environments is a\nfundamental challenge, with high stakes in time-critical settings. In current\nsystems, robot perception remains confined to line-of-sight; occluded regions\nremain unknown until physically traversed, leading to inefficient exploration\nwhen layouts deviate from prior assumptions. In this work, we bring\nnon-line-of-sight (NLOS) sensing to robotic exploration. We leverage\nsingle-photon LiDARs, which capture time-of-flight histograms that encode the\npresence of hidden objects - allowing robots to look around blind corners.\nRecent single-photon LiDARs have become practical and portable, enabling\ndeployment beyond controlled lab settings. Prior NLOS works target 3D\nreconstruction in static, lab-based scenarios, and initial efforts toward\nNLOS-aided navigation consider simplified geometries. We introduce SuperEx, a\nframework that integrates NLOS sensing directly into the mapping-exploration\nloop. SuperEx augments global map prediction with beyond-line-of-sight cues by\n(i) carving empty NLOS regions from timing histograms and (ii) reconstructing\noccupied structure via a two-step physics-based and data-driven approach that\nleverages structural regularities. Evaluations on complex simulated maps and\nthe real-world KTH Floorplan dataset show a 12% gain in mapping accuracy under\n< 30% coverage and improved exploration efficiency compared to line-of-sight\nbaselines, opening a path to reliable mapping beyond direct visibility."}
{"id": "2510.10516", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10516", "abs": "https://arxiv.org/abs/2510.10516", "authors": ["Kanishkha Jaisankar", "Xiaoyang Jiang", "Feifan Liao", "Jeethu Sreenivas Amuthan"], "title": "Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control", "comment": null, "summary": "Energy-efficient and high-performance motor control remains a critical\nchallenge in robotics, particularly for high-dimensional continuous control\ntasks with limited onboard resources. While Deep Reinforcement Learning (DRL)\nhas achieved remarkable results, its computational demands and energy\nconsumption limit deployment in resource-constrained environments. This paper\nintroduces a novel framework combining population-coded Spiking Neural Networks\n(SNNs) with DRL to address these challenges. Our approach leverages the\nevent-driven, asynchronous computation of SNNs alongside the robust policy\noptimization capabilities of DRL, achieving a balance between energy efficiency\nand control performance. Central to this framework is the Population-coded\nSpiking Actor Network (PopSAN), which encodes high-dimensional observations\ninto neuronal population activities and enables optimal policy learning through\ngradient-based updates. We evaluate our method on the Isaac Gym platform using\nthe PixMC benchmark with complex robotic manipulation tasks. Experimental\nresults on the Franka robotic arm demonstrate that our approach achieves energy\nsavings of up to 96.10% compared to traditional Artificial Neural Networks\n(ANNs) while maintaining comparable control performance. The trained SNN\npolicies exhibit robust finger position tracking with minimal deviation from\ncommanded trajectories and stable target height maintenance during\npick-and-place operations. These results position population-coded SNNs as a\npromising solution for energy-efficient, high-performance robotic control in\nresource-constrained applications, paving the way for scalable deployment in\nreal-world robotics systems."}
{"id": "2510.10545", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10545", "abs": "https://arxiv.org/abs/2510.10545", "authors": ["Koki Yamane", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Decoupled Scaling 4ch Bilateral Control on the Cartesian coordinate by 6-DoF Manipulator using Rotation Matrix", "comment": "6 pages, 4 figures, Accepted at SAMCON 2025", "summary": "Four-channel bilateral control is a method for achieving remote control with\nforce feedback and adjustment operability by synchronizing the positions and\nforces of two manipulators. This is expected to significantly improve the\noperability of the remote control in contact-rich tasks. Among these, 4-channel\nbilateral control on the Cartesian coordinate system is advantageous owing to\nits suitability for manipulators with different structures and because it\nallows the dynamics in the Cartesian coordinate system to be adjusted by\nadjusting the control parameters, thus achieving intuitive operability for\nhumans. This paper proposes a 4-channel bilateral control method that achieves\nthe desired dynamics by decoupling each dimension in the Cartesian coordinate\nsystem regardless of the scaling factor."}
{"id": "2510.10567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10567", "abs": "https://arxiv.org/abs/2510.10567", "authors": ["Alexander Langmann", "Yevhenii Tokarev", "Mattia Piccinini", "Korbinian Moller", "Johannes Betz"], "title": "Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving", "comment": "8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria", "summary": "Sampling-based trajectory planners are widely used for agile autonomous\ndriving due to their ability to generate fast, smooth, and kinodynamically\nfeasible trajectories. However, their behavior is often governed by a cost\nfunction with manually tuned, static weights, which forces a tactical\ncompromise that is suboptimal across the wide range of scenarios encountered in\na race. To address this shortcoming, we propose using a Reinforcement Learning\n(RL) agent as a high-level behavioral selector that dynamically switches the\ncost function parameters of an analytical, low-level trajectory planner during\nruntime. We show the effectiveness of our approach in simulation in an\nautonomous racing environment where our RL-based planner achieved 0% collision\nrate while reducing overtaking time by up to 60% compared to state-of-the-art\nstatic planners. Our new agent now dynamically switches between aggressive and\nconservative behaviors, enabling interactive maneuvers unattainable with static\nconfigurations. These results demonstrate that integrating reinforcement\nlearning as a high-level selector resolves the inherent trade-off between\nsafety and competitiveness in autonomous racing planners. The proposed\nmethodology offers a pathway toward adaptive yet interpretable motion planning\nfor broader autonomous driving applications."}
{"id": "2510.10597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10597", "abs": "https://arxiv.org/abs/2510.10597", "authors": ["David Rodríguez-Martínez", "C. J. Pérez del Pulgar"], "title": "Fast Vision in the Dark: A Case for Single-Photon Imaging in Planetary Navigation", "comment": "9 pages, 6 figures, conference paper", "summary": "Improving robotic navigation is critical for extending exploration range and\nenhancing operational efficiency. Vision-based navigation relying on\ntraditional CCD or CMOS cameras faces major challenges when complex\nillumination conditions are paired with motion, limiting the range and\naccessibility of mobile planetary robots. In this study, we propose a novel\napproach to planetary navigation that leverages the unique imaging capabilities\nof Single-Photon Avalanche Diode (SPAD) cameras. We present the first\ncomprehensive evaluation of single-photon imaging as an alternative passive\nsensing technology for robotic exploration missions targeting perceptually\nchallenging locations, with a special emphasis on high-latitude lunar regions.\nWe detail the operating principles and performance characteristics of SPAD\ncameras, assess their advantages and limitations in addressing key perception\nchallenges of upcoming exploration missions to the Moon, and benchmark their\nperformance under representative illumination conditions."}
{"id": "2510.10602", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10602", "abs": "https://arxiv.org/abs/2510.10602", "authors": ["Zhuoheng Gao", "Jiyao Zhang", "Zhiyong Xie", "Hao Dong", "Zhaofei Yu", "Rongmei Chen", "Guozhang Chen", "Tiejun Huang"], "title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams", "comment": null, "summary": "Most robotic grasping systems rely on converting sensor data into explicit 3D\npoint clouds, which is a computational step not found in biological\nintelligence. This paper explores a fundamentally different, neuro-inspired\nparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that\nmimics the biological visuomotor pathway, processing raw, asynchronous events\nfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.\nOur model fuses these stereo spike streams and uses a recurrent spiking neural\nnetwork, analogous to high-level visual processing, to iteratively refine grasp\nhypotheses without ever reconstructing a point cloud. To validate this\napproach, we built a large-scale synthetic benchmark dataset. Experiments show\nthat SpikeGrasp surpasses traditional point-cloud-based baselines, especially\nin cluttered and textureless scenes, and demonstrates remarkable data\nefficiency. By establishing the viability of this end-to-end, neuro-inspired\napproach, SpikeGrasp paves the way for future systems capable of the fluid and\nefficient manipulation seen in nature, particularly for dynamic objects."}
{"id": "2510.10637", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10637", "abs": "https://arxiv.org/abs/2510.10637", "authors": ["Haoyu Zhao", "Cheng Zeng", "Linghao Zhuang", "Yaxi Zhao", "Shengke Xue", "Hao Wang", "Xingyue Zhao", "Zhongyu Li", "Kehan Li", "Siteng Huang", "Mingxiu Chen", "Xin Li", "Deli Zhao", "Hua Zou"], "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting", "comment": "13 pages, 6 figures", "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap."}
{"id": "2510.10642", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10642", "abs": "https://arxiv.org/abs/2510.10642", "authors": ["Jianke Zhang", "Yucheng Hu", "Yanjiang Guo", "Xiaoyu Chen", "Yichen Liu", "Wenna Chen", "Chaochao Lu", "Jianyu Chen"], "title": "UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning", "comment": null, "summary": "Building generalist robot policies that can handle diverse tasks in\nopen-ended environments is a central challenge in robotics. To leverage\nknowledge from large-scale pretraining, prior work has typically built\ngeneralist policies either on top of vision-language understanding models\n(VLMs) or generative models. However, both semantic understanding from\nvision-language pretraining and visual dynamics modeling from visual-generation\npretraining are crucial for embodied robots. Recent unified models of\ngeneration and understanding have demonstrated strong capabilities in both\ncomprehension and generation through large-scale pretraining. We posit that\nrobotic policy learning can likewise benefit from the combined strengths of\nunderstanding, planning and continuous future representation learning. Building\non this insight, we introduce UniCoD, which acquires the ability to dynamically\nmodel high-dimensional visual features through pretraining on over 1M\ninternet-scale instructional manipulation videos. Subsequently, UniCoD is\nfine-tuned on data collected from the robot embodiment, enabling the learning\nof mappings from predictive representations to action tokens. Extensive\nexperiments show our approach consistently outperforms baseline methods in\nterms of 9\\% and 12\\% across simulation environments and real-world\nout-of-distribution tasks."}
{"id": "2510.10716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10716", "abs": "https://arxiv.org/abs/2510.10716", "authors": ["Christopher Thierauf"], "title": "Deployment and Development of a Cognitive Teleoreactive Framework for Deep Sea Autonomy", "comment": null, "summary": "A new AUV mission planning and execution software has been tested on AUV\nSentry. Dubbed DINOS-R, it draws inspiration from cognitive architectures and\nAUV control systems to replace the legacy MC architecture. Unlike these\nexisting architectures, however, DINOS-R is built from the ground-up to unify\nsymbolic decision making (for understandable, repeatable, provable behavior)\nwith machine learning techniques and reactive behaviors, for field-readiness\nacross oceanographic platforms. Implemented primarily in Python3, DINOS-R is\nextensible, modular, and reusable, with an emphasis on non-expert use as well\nas growth for future research in oceanography and robot algorithms. Mission\nspecification is flexible, and can be specified declaratively. Behavior\nspecification is similarly flexible, supporting simultaneous use of real-time\ntask planning and hard-coded user specified plans. These features were\ndemonstrated in the field on Sentry, in addition to a variety of simulated\ncases. These results are discussed, and future work is outlined."}
{"id": "2510.10731", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10731", "abs": "https://arxiv.org/abs/2510.10731", "authors": ["Yongxi Cao", "Julian F. Schumann", "Jens Kober", "Joni Pajarinen", "Arkady Zgonnikov"], "title": "Controllable Generative Trajectory Prediction via Weak Preference Alignment", "comment": null, "summary": "Deep generative models such as conditional variational autoencoders (CVAEs)\nhave shown great promise for predicting trajectories of surrounding agents in\nautonomous vehicle planning. State-of-the-art models have achieved remarkable\naccuracy in such prediction tasks. Besides accuracy, diversity is also crucial\nfor safe planning because human behaviors are inherently uncertain and\nmultimodal. However, existing methods generally lack a scheme to generate\ncontrollably diverse trajectories, which is arguably more useful than randomly\ndiversified trajectories, to the end of safe planning. To address this, we\npropose PrefCVAE, an augmented CVAE framework that uses weakly labeled\npreference pairs to imbue latent variables with semantic attributes. Using\naverage velocity as an example attribute, we demonstrate that PrefCVAE enables\ncontrollable, semantically meaningful predictions without degrading baseline\naccuracy. Our results show the effectiveness of preference supervision as a\ncost-effective way to enhance sampling-based generative models."}
{"id": "2510.10759", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10759", "abs": "https://arxiv.org/abs/2510.10759", "authors": ["Arthicha Srisuchinnawong", "Poramate Manoonpong"], "title": "Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning", "comment": "RSS 2025", "summary": "Existing robot locomotion learning techniques rely heavily on the offline\nselection of proper reward weighting gains and cannot guarantee constraint\nsatisfaction (i.e., constraint violation) during training. Thus, this work aims\nto address both issues by proposing Reward-Oriented Gains via Embodied\nRegulation (ROGER), which adapts reward-weighting gains online based on\npenalties received throughout the embodied interaction process. The ratio\nbetween the positive reward (primary reward) and negative reward (penalty)\ngains is automatically reduced as the learning approaches the constraint\nthresholds to avoid violation. Conversely, the ratio is increased when learning\nis in safe states to prioritize performance. With a 60-kg quadruped robot,\nROGER achieved near-zero constraint violation throughout multiple learning\ntrials. It also achieved up to 50% more primary reward than the equivalent\nstate-of-the-art techniques. In MuJoCo continuous locomotion benchmarks,\nincluding a single-leg hopper, ROGER exhibited comparable or up to 100% higher\nperformance and 60% less torque usage and orientation deviation compared to\nthose trained with the default reward function. Finally, real-world locomotion\nlearning of a physical quadruped robot was achieved from scratch within one\nhour without any falls. Therefore, this work contributes to\nconstraint-satisfying real-world continual robot locomotion learning and\nsimplifies reward weighting gain tuning, potentially facilitating the\ndevelopment of physical robots and those that learn in the real world."}
{"id": "2510.10778", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10778", "abs": "https://arxiv.org/abs/2510.10778", "authors": ["Christopher D. Hsu", "Pratik Chaudhari"], "title": "Real2USD: Scene Representations in Universal Scene Description Language", "comment": "8 pages, 10 figures, 1 table", "summary": "Large Language Models (LLMs) can help robots reason about abstract task\nspecifications. This requires augmenting classical representations of the\nenvironment used by robots with natural language-based priors. There are a\nnumber of existing approaches to doing so, but they are tailored to specific\ntasks, e.g., visual-language models for navigation, language-guided neural\nradiance fields for mapping, etc. This paper argues that the Universal Scene\nDescription (USD) language is an effective and general representation of\ngeometric, photometric and semantic information in the environment for\nLLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene\ngraph, readable by LLMs and humans alike, and rich enough to support\nessentially any task -- Pixar developed this language to store assets, scenes\nand even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2\nquadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD\nrepresentation of indoor environments with diverse objects and challenging\nsettings with lots of glass, and (ii) parses the USD using Google's Gemini to\ndemonstrate scene understanding, complex inferences, and planning. We also\nstudy different aspects of this system in simulated warehouse and hospital\nsettings using Nvidia's Issac Sim. Code is available at\nhttps://github.com/grasp-lyrl/Real2USD ."}
{"id": "2510.10781", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.10781", "abs": "https://arxiv.org/abs/2510.10781", "authors": ["Douglas Hutchings", "Luai Abuelsamen", "Karthik Rajgopal"], "title": "Two-Layer Voronoi Coverage Control for Hybrid Aerial-Ground Robot Teams in Emergency Response: Implementation and Analysis", "comment": "23 pages, 7 figures. Technical report with complete implementation\n  details and open-source code", "summary": "We present a comprehensive two-layer Voronoi coverage control approach for\ncoordinating hybrid aerial-ground robot teams in hazardous material emergency\nresponse scenarios. Traditional Voronoi coverage control methods face three\ncritical limitations in emergency contexts: heterogeneous agent capabilities\nwith vastly different velocities, clustered initial deployment configurations,\nand urgent time constraints requiring rapid response rather than eventual\nconvergence. Our method addresses these challenges through a decoupled\ntwo-layer architecture that separately optimizes aerial and ground robot\npositioning, with aerial agents delivering ground sensors via airdrop to\nhigh-priority locations. We provide detailed implementation of bounded Voronoi\ncell computation, efficient numerical integration techniques for\nimportance-weighted centroids, and robust control strategies that prevent agent\ntrapping. Simulation results demonstrate an 88% reduction in response time,\nachieving target sensor coverage (18.5% of initial sensor loss) in 25 seconds\ncompared to 220 seconds for ground-only deployment. Complete implementation\ncode is available at https://github.com/dHutchings/ME292B."}
{"id": "2510.10804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10804", "abs": "https://arxiv.org/abs/2510.10804", "authors": ["Alessandro Albini", "Mohsen Kaboli", "Giorgio Cannata", "Perla Maiolino"], "title": "Representing Data in Robotic Tactile Perception -- A Review", "comment": null, "summary": "Robotic tactile perception is a complex process involving several\ncomputational steps performed at different levels. Tactile information is\nshaped by the interplay of robot actions, the mechanical properties of its\nbody, and the software that processes the data. In this respect, high-level\ncomputation, required to process and extract information, is commonly performed\nby adapting existing techniques from other domains, such as computer vision,\nwhich expects input data to be properly structured. Therefore, it is necessary\nto transform tactile sensor data to match a specific data structure. This\noperation directly affects the tactile information encoded and, as a\nconsequence, the task execution. This survey aims to address this specific\naspect of the tactile perception pipeline, namely Data Representation. The\npaper first clearly defines its contributions to the perception pipeline and\nthen reviews how previous studies have dealt with the problem of representing\ntactile information, investigating the relationships among hardware,\nrepresentations, and high-level computation methods. The analysis has led to\nthe identification of six structures commonly used in the literature to\nrepresent data. The manuscript provides discussions and guidelines for properly\nselecting a representation depending on operating conditions, including the\navailable hardware, the tactile information required to be encoded, and the\ntask at hand."}
{"id": "2510.10843", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10843", "abs": "https://arxiv.org/abs/2510.10843", "authors": ["Jared Grinberg", "Yanran Ding"], "title": "Contact Sensing via Joint Torque Sensors and a Force/Torque Sensor for Legged Robots", "comment": "Proc. IEEE 21st International Conference on Automation Science and\n  Engineering (CASE), Los Angeles, CA, USA, Aug. 17-21, 2025, pp. 1-7,\n  doi:10.1109/CASE58245.2025.11164031", "summary": "This paper presents a method for detecting and localizing contact along robot\nlegs using distributed joint torque sensors and a single hip-mounted\nforce-torque (FT) sensor using a generalized momentum-based observer framework.\nWe designed a low-cost strain-gauge-based joint torque sensor that can be\ninstalled on every joint to provide direct torque measurements, eliminating the\nneed for complex friction models and providing more accurate torque readings\nthan estimation based on motor current. Simulation studies on a floating-based\n2-DoF robot leg verified that the proposed framework accurately recovers\ncontact force and location along the thigh and shin links. Through a\ncalibration procedure, our torque sensor achieved an average 96.4% accuracy\nrelative to ground truth measurements. Building upon the torque sensor, we\nperformed hardware experiments on a 2-DoF manipulator, which showed\nsub-centimeter contact localization accuracy and force errors below 0.2 N."}
{"id": "2510.10851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10851", "abs": "https://arxiv.org/abs/2510.10851", "authors": ["Tingxuan Leng", "Yushi Wang", "Tinglong Zheng", "Changsheng Luo", "Mingguo Zhao"], "title": "Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion", "comment": null, "summary": "Humanoid locomotion requires not only accurate command tracking for\nnavigation but also compliant responses to external forces during human\ninteraction. Despite significant progress, existing RL approaches mainly\nemphasize robustness, yielding policies that resist external forces but lack\ncompliance-particularly challenging for inherently unstable humanoids. In this\nwork, we address this by formulating humanoid locomotion as a multi-objective\noptimization problem that balances command tracking and external force\ncompliance. We introduce a preference-conditioned multi-objective RL (MORL)\nframework that integrates rigid command following and compliant behaviors\nwithin a single omnidirectional locomotion policy. External forces are modeled\nvia velocity-resistance factor for consistent reward design, and training\nleverages an encoder-decoder structure that infers task-relevant privileged\nfeatures from deployable observations. We validate our approach in both\nsimulation and real-world experiments on a humanoid robot. Experimental results\nindicate that our framework not only improves adaptability and convergence over\nstandard pipelines, but also realizes deployable preference-conditioned\nhumanoid locomotion."}
{"id": "2510.10865", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.10865", "abs": "https://arxiv.org/abs/2510.10865", "authors": ["Ahmed Alanazi", "Duy Ho", "Yugyung Lee"], "title": "GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments", "comment": "17 pages, 5 figures, 8 tables", "summary": "Robots navigating dynamic, cluttered, and semantically complex environments\nmust integrate perception, symbolic reasoning, and spatial planning to\ngeneralize across diverse layouts and object categories. Existing methods often\nrely on static priors or limited memory, constraining adaptability under\npartial observability and semantic ambiguity. We present GRIP, Grid-based Relay\nwith Intermediate Planning, a unified, modular framework with three scalable\nvariants: GRIP-L (Lightweight), optimized for symbolic navigation via semantic\noccupancy grids; GRIP-F (Full), supporting multi-hop anchor chaining and\nLLM-based introspection; and GRIP-R (Real-World), enabling physical robot\ndeployment under perceptual uncertainty. GRIP integrates dynamic 2D grid\nconstruction, open-vocabulary object grounding, co-occurrence-aware symbolic\nplanning, and hybrid policy execution using behavioral cloning, D* search, and\ngrid-conditioned control. Empirical results on AI2-THOR and RoboTHOR benchmarks\nshow that GRIP achieves up to 9.6% higher success rates and over $2\\times$\nimprovement in path efficiency (SPL and SAE) on long-horizon tasks. Qualitative\nanalyses reveal interpretable symbolic plans in ambiguous scenes. Real-world\ndeployment on a Jetbot further validates GRIP's generalization under sensor\nnoise and environmental variation. These results position GRIP as a robust,\nscalable, and explainable framework bridging simulation and real-world\nnavigation."}
{"id": "2510.10886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10886", "abs": "https://arxiv.org/abs/2510.10886", "authors": ["Yashom Dighe", "Youngjin Kim", "Karthik Dantu"], "title": "QuayPoints: A Reasoning Framework to Bridge the Information Gap Between Global and Local Planning in Autonomous Racing", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Autonomous racing requires tight integration between perception, planning and\ncontrol to minimize latency as well as timely decision making. A standard\nautonomy pipeline comprising a global planner, local planner, and controller\nloses information as the higher-level racing context is sequentially propagated\ndownstream into specific task-oriented context. In particular, the global\nplanner's understanding of optimality is typically reduced to a sparse set of\nwaypoints, leaving the local planner to make reactive decisions with limited\ncontext. This paper investigates whether additional global insights,\nspecifically time-optimality information, can be meaningfully passed to the\nlocal planner to improve downstream decisions. We introduce a framework that\npreserves essential global knowledge and conveys it to the local planner\nthrough QuayPoints regions where deviations from the optimal raceline result in\nsignificant compromises to optimality. QuayPoints enable local planners to make\nmore informed global decisions when deviating from the raceline, such as during\nstrategic overtaking. To demonstrate this, we integrate QuayPoints into an\nexisting planner and show that it consistently overtakes opponents traveling at\nup to 75% of the ego vehicle's speed across four distinct race tracks."}
{"id": "2510.10893", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10893", "abs": "https://arxiv.org/abs/2510.10893", "authors": ["Dikshant Shehmar", "Matthew E. Taylor", "Ehsan Hashemi"], "title": "An Adaptive Transition Framework for Game-Theoretic Based Takeover", "comment": null, "summary": "The transition of control from autonomous systems to human drivers is\ncritical in automated driving systems, particularly due to the out-of-the-loop\n(OOTL) circumstances that reduce driver readiness and increase reaction times.\nExisting takeover strategies are based on fixed time-based transitions, which\nfail to account for real-time driver performance variations. This paper\nproposes an adaptive transition strategy that dynamically adjusts the control\nauthority based on both the time and tracking ability of the driver trajectory.\nShared control is modeled as a cooperative differential game, where control\nauthority is modulated through time-varying objective functions instead of\nblending control torques directly. To ensure a more natural takeover, a\ndriver-specific state-tracking matrix is introduced, allowing the transition to\nalign with individual control preferences. Multiple transition strategies are\nevaluated using a cumulative trajectory error metric. Human-in-the-loop control\nscenarios of the standardized ISO lane change maneuvers demonstrate that\nadaptive transitions reduce trajectory deviations and driver control effort\ncompared to conventional strategies. Experiments also confirm that continuously\nadjusting control authority based on real-time deviations enhances vehicle\nstability while reducing driver effort during takeover."}
{"id": "2510.10903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10903", "abs": "https://arxiv.org/abs/2510.10903", "authors": ["Shuanghao Bai", "Wenxuan Song", "Jiayi Chen", "Yuheng Ji", "Zhide Zhong", "Jin Yang", "Han Zhao", "Wanqi Zhou", "Wei Zhao", "Zhe Li", "Pengxiang Ding", "Cheng Chi", "Haoang Li", "Chang Xu", "Xiaolong Zheng", "Donglin Wang", "Shanghang Zhang", "Badong Chen"], "title": "Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey", "comment": null, "summary": "Embodied intelligence has witnessed remarkable progress in recent years,\ndriven by advances in computer vision, natural language processing, and the\nrise of large-scale multimodal models. Among its core challenges, robot\nmanipulation stands out as a fundamental yet intricate problem, requiring the\nseamless integration of perception, planning, and control to enable interaction\nwithin diverse and unstructured environments. This survey presents a\ncomprehensive overview of robotic manipulation, encompassing foundational\nbackground, task-organized benchmarks and datasets, and a unified taxonomy of\nexisting methods. We extend the classical division between high-level planning\nand low-level control by broadening high-level planning to include language,\ncode, motion, affordance, and 3D representations, while introducing a new\ntaxonomy of low-level learning-based control grounded in training paradigms\nsuch as input modeling, latent learning, and policy learning. Furthermore, we\nprovide the first dedicated taxonomy of key bottlenecks, focusing on data\ncollection, utilization, and generalization, and conclude with an extensive\nreview of real-world applications. Compared with prior surveys, our work offers\nboth a broader scope and deeper insight, serving as an accessible roadmap for\nnewcomers and a structured reference for experienced researchers. All related\nresources, including research papers, open-source datasets, and projects, are\ncurated for the community at\nhttps://github.com/BaiShuanghao/Awesome-Robotics-Manipulation."}
{"id": "2510.10912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10912", "abs": "https://arxiv.org/abs/2510.10912", "authors": ["Xinyu Shao", "Yanzhe Tang", "Pengwei Xie", "Kaiwen Zhou", "Yuzheng Zhuang", "Xingyue Quan", "Jianye Hao", "Long Zeng", "Xiu Li"], "title": "More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks", "comment": "More details and videos can be found at https://robo-map.github.io.\n  Xiu Li (Corresponding author: Xiu Li)", "summary": "Many language-guided robotic systems rely on collapsing spatial reasoning\ninto discrete points, making them brittle to perceptual noise and semantic\nambiguity. To address this challenge, we propose RoboMAP, a framework that\nrepresents spatial targets as continuous, adaptive affordance heatmaps. This\ndense representation captures the uncertainty in spatial grounding and provides\nricher information for downstream policies, thereby significantly enhancing\ntask success and interpretability. RoboMAP surpasses the previous\nstate-of-the-art on a majority of grounding benchmarks with up to a 50x speed\nimprovement, and achieves an 82\\% success rate in real-world manipulation.\nAcross extensive simulated and physical experiments, it demonstrates robust\nperformance and shows strong zero-shot generalization to navigation. More\ndetails and videos can be found at https://robo-map.github.io."}
{"id": "2510.10960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10960", "abs": "https://arxiv.org/abs/2510.10960", "authors": ["Dong Hu", "Fenqing Hu", "Lidong Yang", "Chao Huang"], "title": "Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving", "comment": null, "summary": "Ensuring safety in autonomous driving (AD) remains a significant challenge,\nespecially in highly dynamic and complex traffic environments where diverse\nagents interact and unexpected hazards frequently emerge. Traditional\nreinforcement learning (RL) methods often struggle to balance safety,\nefficiency, and adaptability, as they primarily focus on reward maximization\nwithout explicitly modeling risk or safety constraints. To address these\nlimitations, this study proposes a novel game-theoretic risk-shaped RL (GTR2L)\nframework for safe AD. GTR2L incorporates a multi-level game-theoretic world\nmodel that jointly predicts the interactive behaviors of surrounding vehicles\nand their associated risks, along with an adaptive rollout horizon that adjusts\ndynamically based on predictive uncertainty. Furthermore, an uncertainty-aware\nbarrier mechanism enables flexible modulation of safety boundaries. A dedicated\nrisk modeling approach is also proposed, explicitly capturing both epistemic\nand aleatoric uncertainty to guide constrained policy optimization and enhance\ndecision-making in complex environments. Extensive evaluations across diverse\nand safety-critical traffic scenarios show that GTR2L significantly outperforms\nstate-of-the-art baselines, including human drivers, in terms of success rate,\ncollision and violation reduction, and driving efficiency. The code is\navailable at https://github.com/DanielHu197/GTR2L."}
{"id": "2510.10975", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10975", "abs": "https://arxiv.org/abs/2510.10975", "authors": ["Mingtong Dai", "Lingbo Liu", "Yongjie Bai", "Yang Liu", "Zhouxia Wang", "Rui SU", "Chunjie Chen", "Liang Lin", "Xinyu Wu"], "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model", "comment": null, "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs.We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."}
{"id": "2510.10979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10979", "abs": "https://arxiv.org/abs/2510.10979", "authors": ["Qizhi Guo", "Siyuan Yang", "Junning Lyu", "Jianjun Sun", "Defu Lin", "Shaoming He"], "title": "AMO-HEAD: Adaptive MARG-Only Heading Estimation for UAVs under Magnetic Disturbances", "comment": null, "summary": "Accurate and robust heading estimation is crucial for unmanned aerial\nvehicles (UAVs) when conducting indoor inspection tasks. However, the cluttered\nnature of indoor environments often introduces severe magnetic disturbances,\nwhich can significantly degrade heading accuracy. To address this challenge,\nthis paper presents an Adaptive MARG-Only Heading (AMO-HEAD) estimation\napproach for UAVs operating in magnetically disturbed environments. AMO-HEAD is\na lightweight and computationally efficient Extended Kalman Filter (EKF)\nframework that leverages inertial and magnetic sensors to achieve reliable\nheading estimation. In the proposed approach, gyroscope angular rate\nmeasurements are integrated to propagate the quaternion state, which is\nsubsequently corrected using accelerometer and magnetometer data. The corrected\nquaternion is then used to compute the UAV's heading. An adaptive process noise\ncovariance method is introduced to model and compensate for gyroscope\nmeasurement noise, bias drift, and discretization errors arising from the Euler\nmethod integration. To mitigate the effects of external magnetic disturbances,\na scaling factor is applied based on real-time magnetic deviation detection. A\ntheoretical observability analysis of the proposed AMO-HEAD is performed using\nthe Lie derivative. Extensive experiments were conducted in real world indoor\nenvironments with customized UAV platforms. The results demonstrate the\neffectiveness of the proposed algorithm in providing precise heading estimation\nunder magnetically disturbed conditions."}
{"id": "2510.11014", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11014", "abs": "https://arxiv.org/abs/2510.11014", "authors": ["Subhransu S. Bhattacharjee", "Hao Lu", "Dylan Campbell", "Rahul Shome"], "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces", "comment": "Under Review", "summary": "Priors are vital for planning under partial observability, yet difficult to\nobtain in practice. We present a sampling-based pipeline that leverages\nlarge-scale pretrained generative models to produce probabilistic priors\ncapturing environmental uncertainty and spatio-semantic relationships in a\nzero-shot manner. Conditioned on partial observations, the pipeline recovers\ncomplete RGB-D point cloud samples with occupancy and target semantics,\nformulated to be directly useful in configuration-space planning. We establish\na Matterport3D benchmark of rooms partially visible through doorways, where a\nrobot must navigate to an unobserved target object. Effective priors for this\nsetting must represent both occupancy and target-location uncertainty in\nunobserved regions. Experiments show that our approach recovers commonsense\nspatial semantics consistent with ground truth, yielding diverse, clean 3D\npoint clouds usable in motion planning, highlight the promise of generative\nmodels as a rich source of priors for robotic planning."}
{"id": "2510.11019", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11019", "abs": "https://arxiv.org/abs/2510.11019", "authors": ["Bingjie Tang", "Iretiayo Akinola", "Jie Xu", "Bowen Wen", "Dieter Fox", "Gaurav S. Sukhatme", "Fabio Ramos", "Abhishek Gupta", "Yashraj Narang"], "title": "Refinery: Active Fine-tuning and Deployment-time Optimization for Contact-Rich Policies", "comment": "in submission. 8 pages, 6 figures. Website:\n  https://refinery-2025.github.io/refinery/", "summary": "Simulation-based learning has enabled policies for precise, contact-rich\ntasks (e.g., robotic assembly) to reach high success rates (~80%) under high\nlevels of observation noise and control error. Although such performance may be\nsufficient for research applications, it falls short of industry standards and\nmakes policy chaining exceptionally brittle. A key limitation is the high\nvariance in individual policy performance across diverse initial conditions. We\nintroduce Refinery, an effective framework that bridges this performance gap,\nrobustifying policy performance across initial conditions. We propose Bayesian\nOptimization-guided fine-tuning to improve individual policies, and Gaussian\nMixture Model-based sampling during deployment to select initializations that\nmaximize execution success. Using Refinery, we improve mean success rates by\n10.98% over state-of-the-art methods in simulation-based learning for robotic\nassembly, reaching 91.51% in simulation and comparable performance in the real\nworld. Furthermore, we demonstrate that these fine-tuned policies can be\nchained to accomplish long-horizon, multi-part\nassembly$\\unicode{x2013}$successfully assembling up to 8 parts without\nrequiring explicit multi-step training."}
{"id": "2510.11036", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11036", "abs": "https://arxiv.org/abs/2510.11036", "authors": ["Yeonseo Lee", "Jungwook Mun", "Hyosup Shin", "Guebin Hwang", "Junhee Nam", "Taeyeop Lee", "Sungho Jo"], "title": "XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation", "comment": null, "summary": "Most robotic grasping methods are typically designed for single gripper\ntypes, which limits their applicability in real-world scenarios requiring\ndiverse end-effectors. We propose XGrasp, a real-time gripper-aware grasp\ndetection framework that efficiently handles multiple gripper configurations.\nThe proposed method addresses data scarcity by systematically augmenting\nexisting datasets with multi-gripper annotations. XGrasp employs a hierarchical\ntwo-stage architecture. In the first stage, a Grasp Point Predictor (GPP)\nidentifies optimal locations using global scene information and gripper\nspecifications. In the second stage, an Angle-Width Predictor (AWP) refines the\ngrasp angle and width using local features. Contrastive learning in the AWP\nmodule enables zero-shot generalization to unseen grippers by learning\nfundamental grasping characteristics. The modular framework integrates\nseamlessly with vision foundation models, providing pathways for future\nvision-language capabilities. The experimental results demonstrate competitive\ngrasp success rates across various gripper types, while achieving substantial\nimprovements in inference speed compared to existing gripper-aware methods.\nProject page: https://sites.google.com/view/xgrasp"}
{"id": "2510.11041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11041", "abs": "https://arxiv.org/abs/2510.11041", "authors": ["Shiyao Zhang", "Liwei Deng", "Shuyu Zhang", "Weijie Yuan", "Hong Zhang"], "title": "Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy", "comment": "Accepted by IEEE RA-L", "summary": "In future intelligent transportation systems, autonomous cooperative planning\n(ACP), becomes a promising technique to increase the effectiveness and security\nof multi-vehicle interactions. However, multiple uncertainties cannot be fully\naddressed for existing ACP strategies, e.g. perception, planning, and\ncommunication uncertainties. To address these, a novel deep reinforcement\nlearning-based autonomous cooperative planning (DRLACP) framework is proposed\nto tackle various uncertainties on cooperative motion planning schemes.\nSpecifically, the soft actor-critic (SAC) with the implementation of gate\nrecurrent units (GRUs) is adopted to learn the deterministic optimal\ntime-varying actions with imperfect state information occurred by planning,\ncommunication, and perception uncertainties. In addition, the real-time actions\nof autonomous vehicles (AVs) are demonstrated via the Car Learning to Act\n(CARLA) simulation platform. Evaluation results show that the proposed DRLACP\nlearns and performs cooperative planning effectively, which outperforms other\nbaseline methods under different scenarios with imperfect AV state information."}
{"id": "2510.11072", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11072", "abs": "https://arxiv.org/abs/2510.11072", "authors": ["Huayi Wang", "Wentao Zhang", "Runyi Yu", "Tao Huang", "Junli Ren", "Feiyu Jia", "Zirui Wang", "Xiaojie Niu", "Xiao Chen", "Jiahe Chen", "Qifeng Chen", "Jingbo Wang", "Jiangmiao Pang"], "title": "PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System", "comment": "Project website: https://why618188.github.io/physhsi/", "summary": "Deploying humanoid robots to interact with real-world environments--such as\ncarrying objects or sitting on chairs--requires generalizable, lifelike motions\nand robust scene perception. Although prior approaches have advanced each\ncapability individually, combining them in a unified system is still an ongoing\nchallenge. In this work, we present a physical-world humanoid-scene interaction\nsystem, PhysHSI, that enables humanoids to autonomously perform diverse\ninteraction tasks while maintaining natural and lifelike behaviors. PhysHSI\ncomprises a simulation training pipeline and a real-world deployment system. In\nsimulation, we adopt adversarial motion prior-based policy learning to imitate\nnatural humanoid-scene interaction data across diverse scenarios, achieving\nboth generalization and lifelike behaviors. For real-world deployment, we\nintroduce a coarse-to-fine object localization module that combines LiDAR and\ncamera inputs to provide continuous and robust scene perception. We validate\nPhysHSI on four representative interactive tasks--box carrying, sitting, lying,\nand standing up--in both simulation and real-world settings, demonstrating\nconsistently high success rates, strong generalization across diverse task\ngoals, and natural motion patterns."}
{"id": "2510.11083", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11083", "abs": "https://arxiv.org/abs/2510.11083", "authors": ["Tianyi Tan", "Yinan Zheng", "Ruiming Liang", "Zexu Wang", "Kexin Zheng", "Jinliang Zheng", "Jianxiong Li", "Xianyuan Zhan", "Jingjing Liu"], "title": "Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling", "comment": "26 pages, 6 figures. Accepted at NeurIPS 2025", "summary": "Modeling interactive driving behaviors in complex scenarios remains a\nfundamental challenge for autonomous driving planning. Learning-based\napproaches attempt to address this challenge with advanced generative models,\nremoving the dependency on over-engineered architectures for representation\nfusion. However, brute-force implementation by simply stacking transformer\nblocks lacks a dedicated mechanism for modeling interactive behaviors that are\ncommon in real driving scenarios. The scarcity of interactive driving data\nfurther exacerbates this problem, leaving conventional imitation learning\nmethods ill-equipped to capture high-value interactive behaviors. We propose\nFlow Planner, which tackles these problems through coordinated innovations in\ndata modeling, model architecture, and learning scheme. Specifically, we first\nintroduce fine-grained trajectory tokenization, which decomposes the trajectory\ninto overlapping segments to decrease the complexity of whole trajectory\nmodeling. With a sophisticatedly designed architecture, we achieve efficient\ntemporal and spatial fusion of planning and scene information, to better\ncapture interactive behaviors. In addition, the framework incorporates flow\nmatching with classifier-free guidance for multi-modal behavior generation,\nwhich dynamically reweights agent interactions during inference to maintain\ncoherent response strategies, providing a critical boost for interactive\nscenario understanding. Experimental results on the large-scale nuPlan dataset\nand challenging interactive interPlan dataset demonstrate that Flow Planner\nachieves state-of-the-art performance among learning-based approaches while\neffectively modeling interactive behaviors in complex driving scenarios."}
{"id": "2510.11094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11094", "abs": "https://arxiv.org/abs/2510.11094", "authors": ["Junxiang Wang", "Han Zhang", "Zehao Wang", "Huaiyuan Chen", "Pu Wang", "Weidong Chen"], "title": "Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation", "comment": null, "summary": "Effective rehabilitation methods are essential for the recovery of lower limb\ndysfunction caused by stroke. Nowadays, robotic exoskeletons have shown great\npotentials in rehabilitation. Nevertheless, traditional rigid exoskeletons are\nusually heavy and need a lot of work to help the patients to put them on.\nMoreover, it also requires extra compliance control to guarantee the safety. In\ncontrast, soft exoskeletons are easy and comfortable to wear and have intrinsic\ncompliance, but their complex nonlinear human-robot interaction dynamics would\npose significant challenges for control. In this work, based on the pneumatic\nactuators inspired by origami, we design a rehabilitation exoskeleton for knee\nthat is easy and comfortable to wear. To guarantee the control performance and\nenable a nice human-robot interaction, we first use Deep Koopman Network to\nmodel the human-robot interaction dynamics. In particular, by viewing the\nelectromyography (EMG) signals and the duty cycle of the PWM wave that controls\nthe pneumatic robot's valves and pump as the inputs, the linear Koopman model\naccurately captures the complex human-robot interaction dynamics. Next, based\non the obtained Koopman model, we further use Model Predictive Control (MPC) to\ncontrol the soft robot and help the user to do rehabilitation training in\nreal-time. The goal of the rehabilitation training is to track a given\nreference signal shown on the screen. Experiments show that by integrating the\nEMG signals into the Koopman model, we have improved the model accuracy to\ngreat extent. In addition, a personalized Koopman model trained from the\nindividual's own data performs better than the non-personalized model.\nConsequently, our control framework outperforms the traditional PID control in\nboth passive and active training modes. Hence the proposed method provides a\nnew control framework for soft rehabilitation robots."}
{"id": "2510.11103", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11103", "abs": "https://arxiv.org/abs/2510.11103", "authors": ["Martin Schuck", "Sherif Samy", "Angela P. Schoellig"], "title": "A Primer on SO(3) Action Representations in Deep Reinforcement Learning", "comment": null, "summary": "Many robotic control tasks require policies to act on orientations, yet the\ngeometry of SO(3) makes this nontrivial. Because SO(3) admits no global,\nsmooth, minimal parameterization, common representations such as Euler angles,\nquaternions, rotation matrices, and Lie algebra coordinates introduce distinct\nconstraints and failure modes. While these trade-offs are well studied for\nsupervised learning, their implications for actions in reinforcement learning\nremain unclear. We systematically evaluate SO(3) action representations across\nthree standard continuous control algorithms, PPO, SAC, and TD3, under dense\nand sparse rewards. We compare how representations shape exploration, interact\nwith entropy regularization, and affect training stability through empirical\nstudies and analyze the implications of different projections for obtaining\nvalid rotations from Euclidean network outputs. Across a suite of robotics\nbenchmarks, we quantify the practical impact of these choices and distill\nsimple, implementation-ready guidelines for selecting and using rotation\nactions. Our results highlight that representation-induced geometry strongly\ninfluences exploration and optimization and show that representing actions as\ntangent vectors in the local frame yields the most reliable results across\nalgorithms."}
{"id": "2510.11258", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11258", "abs": "https://arxiv.org/abs/2510.11258", "authors": ["Yuhui Fu", "Feiyang Xie", "Chaoyi Xu", "Jing Xiong", "Haoqi Yuan", "Zongqing Lu"], "title": "DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation", "comment": null, "summary": "Loco-manipulation is a fundamental challenge for humanoid robots to achieve\nversatile interactions in human environments. Although recent studies have made\nsignificant progress in humanoid whole-body control, loco-manipulation remains\nunderexplored and often relies on hard-coded task definitions or costly\nreal-world data collection, which limits autonomy and generalization. We\npresent DemoHLM, a framework for humanoid loco-manipulation that enables\ngeneralizable loco-manipulation on a real humanoid robot from a single\ndemonstration in simulation. DemoHLM adopts a hierarchy that integrates a\nlow-level universal whole-body controller with high-level manipulation policies\nfor multiple tasks. The whole-body controller maps whole-body motion commands\nto joint torques and provides omnidirectional mobility for the humanoid robot.\nThe manipulation policies, learned in simulation via our data generation and\nimitation learning pipeline, command the whole-body controller with closed-loop\nvisual feedback to execute challenging loco-manipulation tasks. Experiments\nshow a positive correlation between the amount of synthetic data and policy\nperformance, underscoring the effectiveness of our data generation pipeline and\nthe data efficiency of our approach. Real-world experiments on a Unitree G1\nrobot equipped with an RGB-D camera validate the sim-to-real transferability of\nDemoHLM, demonstrating robust performance under spatial variations across ten\nloco-manipulation tasks."}
{"id": "2510.11306", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11306", "abs": "https://arxiv.org/abs/2510.11306", "authors": ["Xiaobin Zhou", "Miao Wang", "Chengao Li", "Can Cui", "Ruibin Zhang", "Yongchao Wang", "Chao Xu", "Fei Gao"], "title": "Rotor-Failure-Aware Quadrotors Flight in Unknown Environments", "comment": null, "summary": "Rotor failures in quadrotors may result in high-speed rotation and vibration\ndue to rotor imbalance, which introduces significant challenges for autonomous\nflight in unknown environments. The mainstream approaches against rotor\nfailures rely on fault-tolerant control (FTC) and predefined trajectory\ntracking. To the best of our knowledge, online failure detection and diagnosis\n(FDD), trajectory planning, and FTC of the post-failure quadrotors in unknown\nand complex environments have not yet been achieved. This paper presents a\nrotor-failure-aware quadrotor navigation system designed to mitigate the\nimpacts of rotor imbalance. First, a composite FDD-based nonlinear model\npredictive controller (NMPC), incorporating motor dynamics, is designed to\nensure fast failure detection and flight stability. Second, a\nrotor-failure-aware planner is designed to leverage FDD results and\nspatial-temporal joint optimization, while a LiDAR-based quadrotor platform\nwith four anti-torque plates is designed to enable reliable perception under\nhigh-speed rotation. Lastly, extensive benchmarks against state-of-the-art\nmethods highlight the superior performance of the proposed approach in\naddressing rotor failures, including propeller unloading and motor stoppage.\nThe experimental results demonstrate, for the first time, that our approach\nenables autonomous quadrotor flight with rotor failures in challenging\nenvironments, including cluttered rooms and unknown forests."}
{"id": "2510.11308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11308", "abs": "https://arxiv.org/abs/2510.11308", "authors": ["Weixi Situ", "Hanjing Ye", "Jianwei Peng", "Yu Zhan", "Hong Zhang"], "title": "Adap-RPF: Adaptive Trajectory Sampling for Robot Person Following in Dynamic Crowded Environments", "comment": "https://adap-rpf.github.io/", "summary": "Robot person following (RPF) is a core capability in human-robot interaction,\nenabling robots to assist users in daily activities, collaborative work, and\nother service scenarios. However, achieving practical RPF remains challenging\ndue to frequent occlusions, particularly in dynamic and crowded environments.\nExisting approaches often rely on fixed-point following or sparse\ncandidate-point selection with oversimplified heuristics, which cannot\nadequately handle complex occlusions caused by moving obstacles such as\npedestrians. To address these limitations, we propose an adaptive trajectory\nsampling method that generates dense candidate points within socially aware\nzones and evaluates them using a multi-objective cost function. Based on the\noptimal point, a person-following trajectory is estimated relative to the\npredicted motion of the target. We further design a prediction-aware model\npredictive path integral (MPPI) controller that simultaneously tracks this\ntrajectory and proactively avoids collisions using predicted pedestrian\nmotions. Extensive experiments show that our method outperforms\nstate-of-the-art baselines in smoothness, safety, robustness, and human\ncomfort, with its effectiveness further demonstrated on a mobile robot in\nreal-world scenarios."}
{"id": "2510.11321", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11321", "abs": "https://arxiv.org/abs/2510.11321", "authors": ["Ruizhe Liu", "Pei Zhou", "Qian Luo", "Li Sun", "Jun Cen", "Yibing Song", "Yanchao Yang"], "title": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios."}
{"id": "2510.11401", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11401", "abs": "https://arxiv.org/abs/2510.11401", "authors": ["Jiayang Wu", "Jiongye Li", "Shibowen Zhang", "Zhicheng He", "Zaijin Wang", "Xiaokun Leng", "Hangxin Liu", "Jingwen Zhang", "Jiayi Wang", "Song-Chun Zhu", "Yao Su"], "title": "Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots", "comment": null, "summary": "This paper proposes a novel framework for humanoid robots to execute\ninspection tasks with high efficiency and millimeter-level precision. The\napproach combines hierarchical planning, time-optimal standing position\ngeneration, and integrated \\ac{mpc} to achieve high speed and precision. A\nhierarchical planning strategy, leveraging \\ac{ik} and \\ac{mip}, reduces\ncomputational complexity by decoupling the high-dimensional planning problem. A\nnovel MIP formulation optimizes standing position selection and trajectory\nlength, minimizing task completion time. Furthermore, an MPC system with\nsimplified kinematics and single-step position correction ensures\nmillimeter-level end-effector tracking accuracy. Validated through simulations\nand experiments on the Kuavo 4Pro humanoid platform, the framework demonstrates\nlow time cost and a high success rate in multi-location tasks, enabling\nefficient and precise execution of complex industrial operations."}
{"id": "2510.11421", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11421", "abs": "https://arxiv.org/abs/2510.11421", "authors": ["Shih-Chieh Sun", "Yun-Cheng Tsai"], "title": "A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation in Smart Cities", "comment": null, "summary": "This paper presents an AI-driven IoT robotic teleoperation system designed\nfor real-time remote manipulation and intelligent visual monitoring, tailored\nfor smart city applications. The architecture integrates a Flutter-based\ncross-platform mobile interface with MQTT-based control signaling and WebRTC\nvideo streaming via the LiveKit framework. A YOLOv11-nano model is deployed for\nlightweight object detection, enabling real-time perception with annotated\nvisual overlays delivered to the user interface. Control commands are\ntransmitted via MQTT to an ESP8266-based actuator node, which coordinates\nmulti-axis robotic arm motion through an Arduino Mega2560 controller. The\nbackend infrastructure is hosted on DigitalOcean, ensuring scalable cloud\norchestration and stable global communication. Latency evaluations conducted\nunder both local and international VPN scenarios (including Hong Kong, Japan,\nand Belgium) demonstrate actuator response times as low as 0.2 seconds and\ntotal video latency under 1.2 seconds, even across high-latency networks. This\nlow-latency dual-protocol design ensures responsive closed-loop interaction and\nrobust performance in distributed environments. Unlike conventional\nteleoperation platforms, the proposed system emphasizes modular deployment,\nreal-time AI sensing, and adaptable communication strategies, making it\nwell-suited for smart city scenarios such as remote infrastructure inspection,\npublic equipment servicing, and urban automation. Future enhancements will\nfocus on edge-device deployment, adaptive routing, and integration with\ncity-scale IoT networks to enhance resilience and scalability."}
{"id": "2510.11448", "categories": ["cs.RO", "cs.SY", "eess.SY", "C.3; D.4.1; D.4.4; D.4.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.11448", "abs": "https://arxiv.org/abs/2510.11448", "authors": ["Yuankai He", "Hanlin Chen", "Weisong Shi"], "title": "A Faster and More Reliable Middleware for Autonomous Driving Systems", "comment": "8 pages,7 figures, 8 tables", "summary": "Ensuring safety in high-speed autonomous vehicles requires rapid control\nloops and tightly bounded delays from perception to actuation. Many open-source\nautonomy systems rely on ROS 2 middleware; when multiple sensor and control\nnodes share one compute unit, ROS 2 and its DDS transports add significant\n(de)serialization, copying, and discovery overheads, shrinking the available\ntime budget. We present Sensor-in-Memory (SIM), a shared-memory transport\ndesigned for intra-host pipelines in autonomous vehicles. SIM keeps sensor data\nin native memory layouts (e.g., cv::Mat, PCL), uses lock-free bounded double\nbuffers that overwrite old data to prioritize freshness, and integrates into\nROS 2 nodes with four lines of code. Unlike traditional middleware, SIM\noperates beside ROS 2 and is optimized for applications where data freshness\nand minimal latency outweigh guaranteed completeness. SIM provides sequence\nnumbers, a writer heartbeat, and optional checksums to ensure ordering,\nliveness, and basic integrity. On an NVIDIA Jetson Orin Nano, SIM reduces\ndata-transport latency by up to 98% compared to ROS 2 zero-copy transports such\nas FastRTPS and Zenoh, lowers mean latency by about 95%, and narrows\n95th/99th-percentile tail latencies by around 96%. In tests on a\nproduction-ready Level 4 vehicle running Autoware.Universe, SIM increased\nlocalization frequency from 7.5 Hz to 9.5 Hz. Applied across all\nlatency-critical modules, SIM cut average perception-to-decision latency from\n521.91 ms to 290.26 ms, reducing emergency braking distance at 40 mph (64 km/h)\non dry concrete by 13.6 ft (4.14 m)."}
{"id": "2510.11474", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11474", "abs": "https://arxiv.org/abs/2510.11474", "authors": ["Ardian Selmonaj", "Giacomo Del Rio", "Adrian Schneider", "Alessandro Antonucci"], "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning", "comment": "2025 IEEE International Conference on Agentic AI (ICA)", "summary": "Achieving mission objectives in a realistic simulation of aerial combat is\nhighly challenging due to imperfect situational awareness and nonlinear flight\ndynamics. In this work, we introduce a novel 3D multi-agent air combat\nenvironment and a Hierarchical Multi-Agent Reinforcement Learning framework to\ntackle these challenges. Our approach combines heterogeneous agent dynamics,\ncurriculum learning, league-play, and a newly adapted training algorithm. To\nthis end, the decision-making process is organized into two abstraction levels:\nlow-level policies learn precise control maneuvers, while high-level policies\nissue tactical commands based on mission objectives. Empirical results show\nthat our hierarchical approach improves both learning efficiency and combat\nperformance in complex dogfight scenarios."}
{"id": "2510.11491", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11491", "abs": "https://arxiv.org/abs/2510.11491", "authors": ["Murad Dawood", "Usama Ahmed Siddiquie", "Shahram Khorshidi", "Maren Bennewitz"], "title": "Constraint-Aware Reinforcement Learning via Adaptive Action Scaling", "comment": null, "summary": "Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that\narise from exploration during training by reducing constraint violations while\nmaintaining task performance. Existing approaches typically rely on a single\npolicy to jointly optimize reward and safety, which can cause instability due\nto conflicting objectives, or they use external safety filters that override\nactions and require prior system knowledge. In this paper, we propose a modular\ncost-aware regulator that scales the agent's actions based on predicted\nconstraint violations, preserving exploration through smooth action modulation\nrather than overriding the policy. The regulator is trained to minimize\nconstraint violations while avoiding degenerate suppression of actions. Our\napproach integrates seamlessly with off-policy RL methods such as SAC and TD3,\nand achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion\ntasks with sparse costs, reducing constraint violations by up to 126 times\nwhile increasing returns by over an order of magnitude compared to prior\nmethods."}
{"id": "2510.11525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11525", "abs": "https://arxiv.org/abs/2510.11525", "authors": ["Luis F. Recalde", "Dhruv Agrawal", "Jon Arrizabalaga", "Guanrui Li"], "title": "DQ-NMPC: Dual-Quaternion NMPC for Quadrotor Flight", "comment": "Accepted to IEEE Robotics and Automation Letters", "summary": "MAVs have great potential to assist humans in complex tasks, with\napplications ranging from logistics to emergency response. Their agility makes\nthem ideal for operations in complex and dynamic environments. However,\nachieving precise control in agile flights remains a significant challenge,\nparticularly due to the underactuated nature of quadrotors and the strong\ncoupling between their translational and rotational dynamics. In this work, we\npropose a novel NMPC framework based on dual-quaternions (DQ-NMPC) for\nquadrotor flight. By representing both quadrotor dynamics and the pose error\ndirectly on the dual-quaternion manifold, our approach enables a compact and\nglobally non-singular formulation that captures the quadrotor coupled dynamics.\nWe validate our approach through simulations and real-world experiments,\ndemonstrating better numerical conditioning and significantly improved tracking\nperformance, with reductions in position and orientation errors of up to 56.11%\nand 56.77%, compared to a conventional baseline NMPC method. Furthermore, our\ncontroller successfully handles aggressive trajectories, reaching maximum\nspeeds up to 13.66 m/s and accelerations reaching 4.2 g within confined space\nconditions of dimensions 11m x 4.5m x 3.65m under which the baseline controller\nfails."}
{"id": "2510.11534", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11534", "abs": "https://arxiv.org/abs/2510.11534", "authors": ["Enli Lin", "Ziyuan Yang", "Qiujing Lu", "Jianming Hu", "Shuo Feng"], "title": "IntersectioNDE: Learning Complex Urban Traffic Dynamics based on Interaction Decoupling Strategy", "comment": "Accepted by ITSC 2025", "summary": "Realistic traffic simulation is critical for ensuring the safety and\nreliability of autonomous vehicles (AVs), especially in complex and diverse\nurban traffic environments. However, existing data-driven simulators face two\nkey challenges: a limited focus on modeling dense, heterogeneous interactions\nat urban intersections - which are prevalent, crucial, and practically\nsignificant in countries like China, featuring diverse agents including\nmotorized vehicles (MVs), non-motorized vehicles (NMVs), and pedestrians - and\nthe inherent difficulty in robustly learning high-dimensional joint\ndistributions for such high-density scenes, often leading to mode collapse and\nlong-term simulation instability. We introduce City Crossings Dataset\n(CiCross), a large-scale dataset collected from a real-world urban\nintersection, uniquely capturing dense, heterogeneous multi-agent interactions,\nparticularly with a substantial proportion of MVs, NMVs and pedestrians. Based\non this dataset, we propose IntersectioNDE (Intersection Naturalistic Driving\nEnvironment), a data-driven simulator tailored for complex urban intersection\nscenarios. Its core component is the Interaction Decoupling Strategy (IDS), a\ntraining paradigm that learns compositional dynamics from agent subsets,\nenabling the marginal-to-joint simulation. Integrated into a scene-aware\nTransformer network with specialized training techniques, IDS significantly\nenhances simulation robustness and long-term stability for modeling\nheterogeneous interactions. Experiments on CiCross show that IntersectioNDE\noutperforms baseline methods in simulation fidelity, stability, and its ability\nto replicate complex, distribution-level urban traffic dynamics."}
{"id": "2510.11539", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.11539", "abs": "https://arxiv.org/abs/2510.11539", "authors": ["Denglin Cheng", "Jiarong Kang", "Xiaobin Xiong"], "title": "Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization", "comment": null, "summary": "Accurate state estimation is critical for legged and aerial robots operating\nin dynamic, uncertain environments. A key challenge lies in specifying process\nand measurement noise covariances, which are typically unknown or manually\ntuned. In this work, we introduce a bi-level optimization framework that\njointly calibrates covariance matrices and kinematic parameters in an\nestimator-in-the-loop manner. The upper level treats noise covariances and\nmodel parameters as optimization variables, while the lower level executes a\nfull-information estimator. Differentiating through the estimator allows direct\noptimization of trajectory-level objectives, resulting in accurate and\nconsistent state estimates. We validate our approach on quadrupedal and\nhumanoid robots, demonstrating significantly improved estimation accuracy and\nuncertainty calibration compared to hand-tuned baselines. Our method unifies\nstate estimation, sensor, and kinematics calibration into a principled,\ndata-driven framework applicable across diverse robotic platforms."}
{"id": "2510.11542", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11542", "abs": "https://arxiv.org/abs/2510.11542", "authors": ["Neil C. Janwani", "Varun Madabushi", "Maegan Tucker"], "title": "NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful method to learn robust\ncontrol policies for bipedal locomotion. Yet, it can be difficult to tune\ndesired robot behaviors due to unintuitive and complex reward design. In\ncomparison, offline trajectory optimization methods, like Hybrid Zero Dynamics,\noffer more tuneable, interpretable, and mathematically grounded motion plans\nfor high-dimensional legged systems. However, these methods often remain\nbrittle to real-world disturbances like external perturbations.\n  In this work, we present NaviGait, a hierarchical framework that combines the\nstructure of trajectory optimization with the adaptability of RL for robust and\nintuitive locomotion control. NaviGait leverages a library of offline-optimized\ngaits and smoothly interpolates between them to produce continuous reference\nmotions in response to high-level commands. The policy provides both\njoint-level and velocity command residual corrections to modulate and stabilize\nthe reference trajectories in the gait library. One notable advantage of\nNaviGait is that it dramatically simplifies reward design by encoding rich\nmotion priors from trajectory optimization, reducing the need for finely tuned\nshaping terms and enabling more stable and interpretable learning. Our\nexperimental results demonstrate that NaviGait enables faster training compared\nto conventional and imitation-based RL, and produces motions that remain\nclosest to the original reference. Overall, by decoupling high-level motion\ngeneration from low-level correction, NaviGait offers a more scalable and\ngeneralizable approach for achieving dynamic and robust locomotion."}
{"id": "2510.11552", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11552", "abs": "https://arxiv.org/abs/2510.11552", "authors": ["Gregoire Passault", "Clement Gaspard", "Olivier Ly"], "title": "Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education", "comment": null, "summary": "Recent developments of low cost off-the-shelf programmable components, their\nmodularity, and also rapid prototyping made educational robotics flourish, as\nit is accessible in most schools today. They allow to illustrate and embody\ntheoretical problems in practical and tangible applications, and gather\nmultidisciplinary skills. They also give a rich natural context for\nproject-oriented pedagogy. However, most current robot kits all are limited to\negocentric aspect of the robots perception. This makes it difficult to access\nmore high-level problems involving e.g. coordinates or navigation. In this\npaper we introduce an educational holonomous robot kit that comes with an\nexternal tracking system, which lightens the constraint on embedded systems,\nbut allows in the same time to discover high-level aspects of robotics,\notherwise unreachable."}
{"id": "2510.11566", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11566", "abs": "https://arxiv.org/abs/2510.11566", "authors": ["Kuanning Wang", "Yongchong Gu", "Yuqian Fu", "Zeyu Shangguan", "Sicheng He", "Xiangyang Xue", "Yanwei Fu", "Daniel Seita"], "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy", "comment": "Project page is at https://scoopdiff.github.io/", "summary": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/."}
{"id": "2510.11574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11574", "abs": "https://arxiv.org/abs/2510.11574", "authors": ["Lennart Werner", "Pol Eyschen", "Sean Costello", "Pierluigi Micarelli", "Marco Hutter"], "title": "Calibrated Dynamic Modeling for Force and Payload Estimation in Hydraulic Machinery", "comment": null, "summary": "Accurate real-time estimation of end effector interaction forces in hydraulic\nexcavators is a key enabler for advanced automation in heavy machinery.\nAccurate knowledge of these forces allows improved, precise grading and digging\nmaneuvers. To address these challenges, we introduce a high-accuracy,\nretrofittable 2D force- and payload estimation algorithm that does not impose\nadditional requirements on the operator regarding trajectory, acceleration or\nthe use of the slew joint. The approach is designed for retrofittability,\nrequires minimal calibration and no prior knowledge of machine-specific dynamic\ncharacteristics. Specifically, we propose a method for identifying a dynamic\nmodel, necessary to estimate both end effector interaction forces and bucket\npayload during normal operation. Our optimization-based payload estimation\nachieves a full-scale payload accuracy of 1%. On a standard 25 t excavator, the\nonline force measurement from pressure and inertial measurements achieves a\ndirection accuracy of 13 degree and a magnitude accuracy of 383 N. The method's\naccuracy and generalization capability are validated on two excavator platforms\nof different type and weight classes. We benchmark our payload estimation\nagainst a classical quasistatic method and a commercially available system. Our\nsystem outperforms both in accuracy and precision."}
{"id": "2510.11660", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11660", "abs": "https://arxiv.org/abs/2510.11660", "authors": ["Yi Yang", "Kefan Gu", "Yuqing Wen", "Hebei Li", "Yucheng Zhao", "Tiancai Wang", "Xudong Liu"], "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation", "comment": "8 pages, 6 figures, conference", "summary": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets.The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/."}
{"id": "2510.11682", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11682", "abs": "https://arxiv.org/abs/2510.11682", "authors": ["Hang Liu", "Yuman Gao", "Sangli Teng", "Yufeng Chi", "Yakun Sophia Shao", "Zhongyu Li", "Maani Ghaffari", "Koushil Sreenath"], "title": "Ego-Vision World Model for Humanoid Contact Planning", "comment": null, "summary": "Enabling humanoid robots to exploit physical contact, rather than simply\navoid collisions, is crucial for autonomy in unstructured environments.\nTraditional optimization-based planners struggle with contact complexity, while\non-policy reinforcement learning (RL) is sample-inefficient and has limited\nmulti-task ability. We propose a framework combining a learned world model with\nsampling-based Model Predictive Control (MPC), trained on a demonstration-free\noffline dataset to predict future outcomes in a compressed latent space. To\naddress sparse contact rewards and sensor noise, the MPC uses a learned\nsurrogate value function for dense, robust planning. Our single, scalable model\nsupports contact-aware tasks, including wall support after perturbation,\nblocking incoming objects, and traversing height-limited arches, with improved\ndata efficiency and multi-task capability over on-policy RL. Deployed on a\nphysical humanoid, our system achieves robust, real-time contact planning from\nproprioception and ego-centric depth images. Website:\nhttps://ego-vcp.github.io/"}
{"id": "2510.11689", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11689", "abs": "https://arxiv.org/abs/2510.11689", "authors": ["Maggie Wang", "Stephen Tian", "Aiden Swann", "Ola Shorinwa", "Jiajun Wu", "Mac Schwager"], "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation", "comment": null, "summary": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ ."}
