{"id": "2510.15114", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15114", "abs": "https://arxiv.org/abs/2510.15114", "authors": ["Marios-Nektarios Stamatopoulos", "Elias Small", "Shridhar Velhal", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Autonomous Reactive Masonry Construction using Collaborative Heterogeneous Aerial Robots with Experimental Demonstration", "comment": null, "summary": "This article presents a fully autonomous aerial masonry construction\nframework using heterogeneous unmanned aerial vehicles (UAVs), supported by\nexperimental validation. Two specialized UAVs were developed for the task: (i)\na brick-carrier UAV equipped with a ball-joint actuation mechanism for precise\nbrick manipulation, and (ii) an adhesion UAV integrating a servo-controlled\nvalve and extruder nozzle for accurate adhesion application. The proposed\nframework employs a reactive mission planning unit that combines a dependency\ngraph of the construction layout with a conflict graph to manage simultaneous\ntask execution, while hierarchical state machines ensure robust operation and\nsafe transitions during task execution. Dynamic task allocation allows\nreal-time adaptation to environmental feedback, while minimum-jerk trajectory\ngeneration ensures smooth and precise UAV motion during brick pickup and\nplacement. Additionally, the brick-carrier UAV employs an onboard vision system\nthat estimates brick poses in real time using ArUco markers and a least-squares\noptimization filter, enabling accurate alignment during construction. To the\nbest of the authors' knowledge, this work represents the first experimental\ndemonstration of fully autonomous aerial masonry construction using\nheterogeneous UAVs, where one UAV precisely places the bricks while another\nautonomously applies adhesion material between them. The experimental results\nsupported by the video showcase the effectiveness of the proposed framework and\ndemonstrate its potential to serve as a foundation for future developments in\nautonomous aerial robotic construction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5f02\u6784\u65e0\u4eba\u673a\u8fdb\u884c\u5168\u81ea\u4e3b\u7a7a\u4e2d\u780c\u4f53\u5efa\u9020\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e13\u95e8\u7684\u7816\u5757\u642c\u8fd0\u65e0\u4eba\u673a\u548c\u7c98\u5408\u5242\u5e94\u7528\u65e0\u4eba\u673a\uff0c\u91c7\u7528\u53cd\u5e94\u5f0f\u4efb\u52a1\u89c4\u5212\u548c\u5206\u5c42\u72b6\u6001\u673a\u786e\u4fdd\u9c81\u68d2\u64cd\u4f5c\u3002", "motivation": "\u5f00\u53d1\u5168\u81ea\u4e3b\u7684\u7a7a\u4e2d\u780c\u4f53\u5efa\u9020\u7cfb\u7edf\uff0c\u5229\u7528\u5f02\u6784\u65e0\u4eba\u673a\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u7816\u5757\u7cbe\u786e\u653e\u7f6e\u548c\u7c98\u5408\u5242\u81ea\u52a8\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u81ea\u4e3b\u7a7a\u4e2d\u673a\u5668\u4eba\u5efa\u9020\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u4e13\u7528\u65e0\u4eba\u673a\uff1a\u7816\u5757\u642c\u8fd0\u65e0\u4eba\u673a\uff08\u914d\u5907\u7403\u5173\u8282\u9a71\u52a8\u673a\u5236\uff09\u548c\u7c98\u5408\u5242\u5e94\u7528\u65e0\u4eba\u673a\uff08\u914d\u5907\u4f3a\u670d\u63a7\u5236\u9600\u548c\u6324\u51fa\u55b7\u5634\uff09\u3002\u91c7\u7528\u53cd\u5e94\u5f0f\u4efb\u52a1\u89c4\u5212\u3001\u5206\u5c42\u72b6\u6001\u673a\u3001\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u6700\u5c0f\u6025\u52a8\u8f68\u8ff9\u751f\u6210\u6280\u672f\u3002\u7816\u5757\u642c\u8fd0\u65e0\u4eba\u673a\u4f7f\u7528\u673a\u8f7d\u89c6\u89c9\u7cfb\u7edf\u5b9e\u65f6\u4f30\u8ba1\u7816\u5757\u4f4d\u59ff\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u6709\u6548\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5168\u81ea\u4e3b\u7a7a\u4e2d\u780c\u4f53\u5efa\u9020\uff0c\u5176\u4e2d\u4e00\u67b6\u65e0\u4eba\u673a\u7cbe\u786e\u653e\u7f6e\u7816\u5757\uff0c\u53e6\u4e00\u67b6\u81ea\u52a8\u5728\u7816\u5757\u95f4\u5e94\u7528\u7c98\u5408\u5242\u6750\u6599\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4f7f\u7528\u5f02\u6784\u65e0\u4eba\u673a\u8fdb\u884c\u5168\u81ea\u4e3b\u7a7a\u4e2d\u780c\u4f53\u5efa\u9020\u7684\u5b9e\u9a8c\u6f14\u793a\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u81ea\u4e3b\u7a7a\u4e2d\u673a\u5668\u4eba\u5efa\u9020\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.15189", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15189", "abs": "https://arxiv.org/abs/2510.15189", "authors": ["Xiangyu Chen", "Chuhao Zhou", "Yuxi Liu", "Jianfei Yang"], "title": "RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation", "comment": null, "summary": "Precise robot manipulation is critical for fine-grained applications such as\nchemical and biological experiments, where even small errors (e.g., reagent\nspillage) can invalidate an entire task. Existing approaches often rely on\npre-collected expert demonstrations and train policies via imitation learning\n(IL) or offline reinforcement learning (RL). However, obtaining high-quality\ndemonstrations for precision tasks is difficult and time-consuming, while\noffline RL commonly suffers from distribution shifts and low data efficiency.\nWe introduce a Role-Model Reinforcement Learning (RM-RL) framework that unifies\nonline and offline training in real-world environments. The key idea is a\nrole-model strategy that automatically generates labels for online training\ndata using approximately optimal actions, eliminating the need for human\ndemonstrations. RM-RL reformulates policy learning as supervised training,\nreducing instability from distribution mismatch and improving efficiency. A\nhybrid training scheme further leverages online role-model data for offline\nreuse, enhancing data efficiency through repeated sampling. Extensive\nexperiments show that RM-RL converges faster and more stably than existing RL\nmethods, yielding significant gains in real-world manipulation: 53% improvement\nin translation accuracy and 20% in rotation accuracy. Finally, we demonstrate\nthe successful execution of a challenging task, precisely placing a cell plate\nonto a shelf, highlighting the framework's effectiveness where prior methods\nfail.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u89d2\u8272\u6a21\u578b\u5f3a\u5316\u5b66\u4e60(RM-RL)\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u8fd1\u4f3c\u6700\u4f18\u52a8\u4f5c\u6807\u7b7e\u6765\u66ff\u4ee3\u4eba\u5de5\u6f14\u793a\uff0c\u7edf\u4e00\u4e86\u5728\u7ebf\u548c\u79bb\u7ebf\u8bad\u7ec3\uff0c\u5728\u7cbe\u5bc6\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7cbe\u5bc6\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u4eba\u5de5\u6f14\u793a\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u56f0\u96be\u4e14\u8017\u65f6\uff1b\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u5206\u5e03\u504f\u79fb\u548c\u4f4e\u6570\u636e\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u89d2\u8272\u6a21\u578b\u7b56\u7565\u81ea\u52a8\u4e3a\u5728\u7ebf\u8bad\u7ec3\u6570\u636e\u751f\u6210\u6807\u7b7e\uff0c\u5c06\u7b56\u7565\u5b66\u4e60\u91cd\u65b0\u8868\u8ff0\u4e3a\u76d1\u7763\u8bad\u7ec3\uff0c\u7ed3\u5408\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\u91cd\u590d\u5229\u7528\u5728\u7ebf\u6570\u636e\u3002", "result": "\u76f8\u6bd4\u73b0\u6709RL\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\u66f4\u7a33\u5b9a\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4e2d\u7ffb\u8bd1\u7cbe\u5ea6\u63d0\u534753%\uff0c\u65cb\u8f6c\u7cbe\u5ea6\u63d0\u534720%\uff0c\u6210\u529f\u5b8c\u6210\u7ec6\u80de\u677f\u7cbe\u786e\u653e\u7f6e\u7b49\u6311\u6218\u6027\u4efb\u52a1\u3002", "conclusion": "RM-RL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7cbe\u5bc6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u83b7\u53d6\u548c\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u5728\u65e0\u9700\u4eba\u5de5\u6f14\u793a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.15199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15199", "abs": "https://arxiv.org/abs/2510.15199", "authors": ["Borna Monazzah Moghaddam", "Robin Chhabra"], "title": "Lagrange-Poincar\u00e9-Kepler Equations of Disturbed Space-Manipulator Systems in Orbit", "comment": null, "summary": "This article presents an extension of the Lagrange-Poincare Equations (LPE)\nto model the dynamics of spacecraft-manipulator systems operating within a\nnon-inertial orbital reference frame. Building upon prior formulations of LPE\nfor vehicle-manipulator systems, the proposed framework, termed the\nLagrange-Poincare-Kepler Equations (LPKE), incorporates the coupling between\nspacecraft attitude dynamics, orbital motion, and manipulator kinematics. The\nformalism combines the Euler-Poincare equations for the base spacecraft,\nKeplerian orbital dynamics for the reference frame, and reduced Euler-Lagrange\nequations for the manipulator's shape space, using an exponential joint\nparametrization. Leveraging the Lagrange-d'Alembert principle on principal\nbundles, we derive novel closed-form structural matrices that explicitly\ncapture the effects of orbital disturbances and their dynamic coupling with the\nmanipulator system. The LPKE framework also systematically includes externally\napplied, symmetry-breaking wrenches, allowing for immediate integration into\nhardware-in-the-loop simulations and model-based control architectures for\nautonomous robotic operations in the orbital environment. To illustrate the\neffectiveness of the proposed model and its numerical superiority, we present a\nsimulation study analyzing orbital effects on a 7-degree-of-freedom manipulator\nmounted on a spacecraft.", "AI": {"tldr": "\u63d0\u51fa\u4e86Lagrange-Poincare-Kepler\u65b9\u7a0b(LPKE)\uff0c\u7528\u4e8e\u5728\u975e\u60ef\u6027\u8f68\u9053\u53c2\u8003\u7cfb\u4e2d\u5efa\u6a21\u822a\u5929\u5668-\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u4e86\u822a\u5929\u5668\u59ff\u6001\u3001\u8f68\u9053\u8fd0\u52a8\u548c\u673a\u68b0\u81c2\u8fd0\u52a8\u5b66\u7684\u8026\u5408\u6548\u5e94\u3002", "motivation": "\u6269\u5c55Lagrange-Poincare\u65b9\u7a0b\uff0c\u4ee5\u66f4\u597d\u5730\u5efa\u6a21\u5728\u8f68\u9053\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u822a\u5929\u5668-\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u7279\u522b\u662f\u8003\u8651\u975e\u60ef\u6027\u8f68\u9053\u53c2\u8003\u7cfb\u7684\u5f71\u54cd\u3002", "method": "\u7ed3\u5408Euler-Poincare\u65b9\u7a0b\uff08\u822a\u5929\u5668\u57fa\u5ea7\uff09\u3001\u5f00\u666e\u52d2\u8f68\u9053\u52a8\u529b\u5b66\uff08\u53c2\u8003\u7cfb\uff09\u548c\u7b80\u5316Euler-Lagrange\u65b9\u7a0b\uff08\u673a\u68b0\u81c2\u5f62\u72b6\u7a7a\u95f4\uff09\uff0c\u4f7f\u7528\u6307\u6570\u5173\u8282\u53c2\u6570\u5316\uff0c\u57fa\u4e8e\u4e3b\u4e1b\u4e0a\u7684Lagrange-d'Alembert\u539f\u7406\u63a8\u5bfc\u7ed3\u6784\u77e9\u9635\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86\u65b0\u7684\u5c01\u95ed\u5f62\u5f0f\u7ed3\u6784\u77e9\u9635\uff0c\u660e\u786e\u6355\u6349\u8f68\u9053\u6270\u52a8\u6548\u5e94\u53ca\u5176\u4e0e\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u52a8\u6001\u8026\u5408\uff0c\u80fd\u591f\u7cfb\u7edf\u5305\u542b\u5916\u90e8\u65bd\u52a0\u7684\u5bf9\u79f0\u7834\u7f3a\u529b\u3002", "conclusion": "LPKE\u6846\u67b6\u5728\u5efa\u6a21\u8f68\u9053\u73af\u5883\u4e2d\u7684\u822a\u5929\u5668-\u673a\u68b0\u81c2\u7cfb\u7edf\u52a8\u529b\u5b66\u65b9\u9762\u5177\u6709\u6570\u503c\u4f18\u52bf\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u786c\u4ef6\u5728\u73af\u4eff\u771f\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u67b6\u6784\u4e2d\u3002"}}
{"id": "2510.15220", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15220", "abs": "https://arxiv.org/abs/2510.15220", "authors": ["Kevin Christiansen Marsim", "Minho Oh", "Byeongho Yu", "Seungjae Lee", "I Made Aswin Nahrendra", "Hyungtae Lim", "Hyun Myung"], "title": "LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization", "comment": "8 Pages, 9 Figures", "summary": "Autonomous navigation for legged robots in complex and dynamic environments\nrelies on robust simultaneous localization and mapping (SLAM) systems to\naccurately map surroundings and localize the robot, ensuring safe and efficient\noperation. While prior sensor fusion-based SLAM approaches have integrated\nvarious sensor modalities to improve their robustness, these algorithms are\nstill susceptible to estimation drift in challenging environments due to their\nreliance on unsuitable fusion strategies. Therefore, we propose a robust\nLiDAR-visual-inertial-kinematic odometry system that integrates information\nfrom multiple sensors, such as a camera, LiDAR, inertial measurement unit\n(IMU), and joint encoders, for visual and LiDAR-based odometry estimation. Our\nsystem employs a fusion-based pose estimation approach that runs\noptimization-based visual-inertial-kinematic odometry (VIKO) and filter-based\nLiDAR-inertial-kinematic odometry (LIKO) based on measurement availability. In\nVIKO, we utilize the footpreintegration technique and robust LiDAR-visual depth\nconsistency using superpixel clusters in a sliding window optimization. In\nLIKO, we incorporate foot kinematics and employ a point-toplane residual in an\nerror-state iterative Kalman filter (ESIKF). Compared with other sensor\nfusion-based SLAM algorithms, our approach shows robust performance across\npublic and longterm datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684LiDAR-\u89c6\u89c9-\u60ef\u6027-\u8fd0\u52a8\u5b66\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\uff08\u76f8\u673a\u3001LiDAR\u3001IMU\u3001\u5173\u8282\u7f16\u7801\u5668\uff09\u6765\u63d0\u5347\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4f20\u611f\u5668\u878d\u5408SLAM\u65b9\u6cd5\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u4ecd\u5b58\u5728\u4f30\u8ba1\u6f02\u79fb\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u91c7\u7528\u4e86\u4e0d\u5408\u9002\u7684\u878d\u5408\u7b56\u7565\u3002", "method": "\u91c7\u7528\u878d\u5408\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6d4b\u91cf\u53ef\u7528\u6027\u8fd0\u884c\u4f18\u5316\u578b\u89c6\u89c9-\u60ef\u6027-\u8fd0\u52a8\u5b66\u91cc\u7a0b\u8ba1(VIKO)\u548c\u6ee4\u6ce2\u578bLiDAR-\u60ef\u6027-\u8fd0\u52a8\u5b66\u91cc\u7a0b\u8ba1(LIKO)\uff0c\u5206\u522b\u4f7f\u7528\u8db3\u90e8\u9884\u79ef\u5206\u6280\u672f\u548c\u9c81\u68d2\u7684LiDAR-\u89c6\u89c9\u6df1\u5ea6\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u8db3\u90e8\u8fd0\u52a8\u5b66\u548c\u70b9\u5bf9\u9762\u6b8b\u5dee\u3002", "result": "\u4e0e\u5176\u5b83\u4f20\u611f\u5668\u878d\u5408SLAM\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u548c\u957f\u671f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u5347\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u548c\u5efa\u56fe\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.15226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15226", "abs": "https://arxiv.org/abs/2510.15226", "authors": ["Mrunal Sarvaiya", "Guanrui Li", "Giuseppe Loianno"], "title": "PolyFly: Polytopic Optimal Planning for Collision-Free Cable-Suspended Aerial Payload Transportation", "comment": null, "summary": "Aerial transportation robots using suspended cables have emerged as versatile\nplatforms for disaster response and rescue operations. To maximize the\ncapabilities of these systems, robots need to aggressively fly through tightly\nconstrained environments, such as dense forests and structurally unsafe\nbuildings, while minimizing flight time and avoiding obstacles. Existing\nmethods geometrically over-approximate the vehicle and obstacles, leading to\nconservative maneuvers and increased flight times. We eliminate these\nrestrictions by proposing PolyFly, an optimal global planner which considers a\nnon-conservative representation for aerial transportation by modeling each\nphysical component of the environment, and the robot (quadrotor, cable and\npayload), as independent polytopes. We further increase the model accuracy by\nincorporating the attitude of the physical components by constructing\norientation-aware polytopes. The resulting optimal control problem is\nefficiently solved by converting the polytope constraints into smooth\ndifferentiable constraints via duality theory. We compare our method against\nthe existing state-of-the-art approach in eight maze-like environments and show\nthat PolyFly produces faster trajectories in each scenario. We also\nexperimentally validate our proposed approach on a real quadrotor with a\nsuspended payload, demonstrating the practical reliability and accuracy of our\nmethod.", "AI": {"tldr": "PolyFly\u662f\u4e00\u79cd\u7528\u4e8e\u7a7a\u4e2d\u8fd0\u8f93\u673a\u5668\u4eba\u7684\u5168\u5c40\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5c06\u673a\u5668\u4eba\u7ec4\u4ef6\u548c\u73af\u5883\u5efa\u6a21\u4e3a\u72ec\u7acb\u591a\u9762\u4f53\uff0c\u6d88\u9664\u51e0\u4f55\u4fdd\u5b88\u8fd1\u4f3c\uff0c\u5b9e\u73b0\u66f4\u6fc0\u8fdb\u7684\u98de\u884c\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u8f66\u8f86\u548c\u969c\u788d\u7269\u8fdb\u884c\u51e0\u4f55\u4fdd\u5b88\u8fd1\u4f3c\uff0c\u5bfc\u81f4\u98de\u884c\u8f68\u8ff9\u4fdd\u5b88\u4e14\u98de\u884c\u65f6\u95f4\u589e\u52a0\u3002\u9700\u8981\u6d88\u9664\u8fd9\u4e9b\u9650\u5236\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5728\u53d7\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u66f4\u6fc0\u8fdb\u7684\u98de\u884c\u3002", "method": "\u5c06\u673a\u5668\u4eba\uff08\u56db\u65cb\u7ffc\u3001\u7535\u7f06\u548c\u8d1f\u8f7d\uff09\u548c\u73af\u5883\u7269\u7406\u7ec4\u4ef6\u5efa\u6a21\u4e3a\u72ec\u7acb\u591a\u9762\u4f53\uff0c\u6784\u5efa\u65b9\u5411\u611f\u77e5\u591a\u9762\u4f53\u4ee5\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\uff0c\u901a\u8fc7\u5bf9\u5076\u7406\u8bba\u5c06\u591a\u9762\u4f53\u7ea6\u675f\u8f6c\u6362\u4e3a\u5e73\u6ed1\u53ef\u5fae\u7ea6\u675f\u6765\u6c42\u89e3\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002", "result": "\u5728\u516b\u4e2a\u8ff7\u5bab\u5f0f\u73af\u5883\u4e2d\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u6bd4\u8f83\uff0cPolyFly\u5728\u6bcf\u4e2a\u573a\u666f\u4e2d\u90fd\u4ea7\u751f\u66f4\u5feb\u7684\u8f68\u8ff9\u3002\u5728\u771f\u5b9e\u56db\u65cb\u7ffc\u5e26\u60ac\u6302\u8d1f\u8f7d\u7684\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "PolyFly\u901a\u8fc7\u975e\u4fdd\u5b88\u7684\u591a\u9762\u4f53\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a7a\u4e2d\u8fd0\u8f93\u673a\u5668\u4eba\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u98de\u884c\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u66f4\u6fc0\u8fdb\u7684\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2510.15229", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.15229", "abs": "https://arxiv.org/abs/2510.15229", "authors": ["Sina Kazemdehbashi", "Yanchao Liu", "Boris S. Mordukhovich"], "title": "A Generalized Sylvester-Fermat-Torricelli problem with application in disaster relief operations by UAVs", "comment": null, "summary": "Natural and human-made disasters can cause severe devastation and claim\nthousands of lives worldwide. Therefore, developing efficient methods for\ndisaster response and management is a critical task for relief teams. One of\nthe most essential components of effective response is the rapid collection of\ninformation about affected areas, damages, and victims. More data translates\ninto better coordination, faster rescue operations, and ultimately, more lives\nsaved. However, in some disasters, such as earthquakes, the communication\ninfrastructure is often partially or completely destroyed, making it extremely\ndifficult for victims to send distress signals and for rescue teams to locate\nand assist them in time. Unmanned Aerial Vehicles (UAVs) have emerged as\nvaluable tools in such scenarios. In particular, a fleet of UAVs can be\ndispatched from a mobile station to the affected area to facilitate data\ncollection and establish temporary communication networks. Nevertheless,\nreal-world deployment of UAVs faces several challenges, with adverse weather\nconditions--especially wind--being among the most significant. To address this,\nwe develop a novel mathematical framework to determine the optimal location of\na mobile UAV station while explicitly accounting for the heterogeneity of the\nUAVs and the effect of wind. In particular, we generalize the Sylvester problem\nto introduce the Sylvester-Fermat-Torricelli (SFT) problem, which captures\ncomplex factors such as wind influence, UAV heterogeneity, and back-and-forth\nmotion within a unified framework. The proposed framework enhances the\npracticality of UAV-based disaster response planning by accounting for\nreal-world factors such as wind and UAV heterogeneity. Experimental results\ndemonstrate that it can reduce wasted operational time by up to 84%, making\npost-disaster missions significantly more efficient and effective.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u98ce\u529b\u548c\u65e0\u4eba\u673a\u5f02\u8d28\u6027\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u79fb\u52a8\u65e0\u4eba\u673a\u7ad9\u7684\u4f4d\u7f6e\uff0c\u53ef\u5c06\u64cd\u4f5c\u65f6\u95f4\u6d6a\u8d39\u51cf\u5c1184%\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u4e2d\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u5e38\u88ab\u7834\u574f\uff0c\u65e0\u4eba\u673a\u53ef\u7528\u4e8e\u6570\u636e\u6536\u96c6\u548c\u4e34\u65f6\u901a\u4fe1\uff0c\u4f46\u98ce\u529b\u7b49\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5c06Sylvester\u95ee\u9898\u63a8\u5e7f\u4e3aSylvester-Fermat-Torricelli\u95ee\u9898\uff0c\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u8003\u8651\u98ce\u529b\u5f71\u54cd\u3001\u65e0\u4eba\u673a\u5f02\u8d28\u6027\u548c\u5f80\u8fd4\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u53ef\u5c06\u64cd\u4f5c\u65f6\u95f4\u6d6a\u8d39\u51cf\u5c11\u9ad8\u8fbe84%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u8003\u8651\u98ce\u529b\u548c\u65e0\u4eba\u673a\u5f02\u8d28\u6027\u7b49\u73b0\u5b9e\u56e0\u7d20\uff0c\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u707e\u5bb3\u54cd\u5e94\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.15319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15319", "abs": "https://arxiv.org/abs/2510.15319", "authors": ["Jeewon Kim", "Minho Oh", "Hyun Myung"], "title": "Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping", "comment": "Accepted by RiTA 2024", "summary": "Scene graphs enhance 3D mapping capabilities in robotics by understanding the\nrelationships between different spatial elements, such as rooms and objects.\nRecent research extends scene graphs to hierarchical layers, adding and\nleveraging constraints across these levels. This approach is tightly integrated\nwith pose-graph optimization, improving both localization and mapping accuracy\nsimultaneously. However, when segmenting spatial characteristics, consistently\nrecognizing rooms becomes challenging due to variations in viewpoints and\nlimited field of view (FOV) of sensors. For example, existing real-time\napproaches often over-segment large rooms into smaller, non-functional spaces\nthat are not useful for localization and mapping due to the time-dependent\nmethod. Conversely, their voxel-based room segmentation method often\nunder-segment in complex cases like not fully enclosed 3D space that are\nnon-traversable for ground robots or humans, leading to false constraints in\npose-graph optimization. We propose a traversability-aware room segmentation\nmethod that considers the interaction between robots and surroundings, with\nconsistent feasibility of traversability information. This enhances both the\nsemantic coherence and computational efficiency of pose-graph optimization.\nImproved performance is demonstrated through the re-detection frequency of the\nsame rooms in a dataset involving repeated traversals of the same space along\nthe same path, as well as the optimization time consumption.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u901a\u884c\u6027\u611f\u77e5\u7684\u623f\u95f4\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u63d0\u9ad8\u573a\u666f\u56fe\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u4f4d\u59ff\u56fe\u4f18\u5316\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5272\u7a7a\u95f4\u7279\u5f81\u65f6\uff0c\u7531\u4e8e\u89c6\u89d2\u53d8\u5316\u548c\u4f20\u611f\u5668\u89c6\u91ce\u9650\u5236\uff0c\u96be\u4ee5\u4e00\u81f4\u8bc6\u522b\u623f\u95f4\u3002\u5b9e\u65f6\u65b9\u6cd5\u5bb9\u6613\u5c06\u5927\u623f\u95f4\u8fc7\u5ea6\u5206\u5272\u6210\u65e0\u7528\u7684\u5c0f\u7a7a\u95f4\uff0c\u800c\u57fa\u4e8e\u4f53\u7d20\u7684\u65b9\u6cd5\u5728\u590d\u6742\u60c5\u51b5\u4e0b\uff08\u5982\u975e\u5b8c\u5168\u5c01\u95ed\u76843D\u7a7a\u95f4\uff09\u53c8\u5bb9\u6613\u5206\u5272\u4e0d\u8db3\uff0c\u5bfc\u81f4\u4f4d\u59ff\u56fe\u4f18\u5316\u4e2d\u51fa\u73b0\u9519\u8bef\u7ea6\u675f\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u901a\u884c\u6027\u611f\u77e5\u7684\u623f\u95f4\u5206\u5272\u65b9\u6cd5\uff0c\u8003\u8651\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u4fdd\u6301\u53ef\u901a\u884c\u6027\u4fe1\u606f\u7684\u4e00\u81f4\u6027\u53ef\u884c\u6027\u3002", "result": "\u901a\u8fc7\u5728\u91cd\u590d\u904d\u5386\u76f8\u540c\u8def\u5f84\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u76f8\u540c\u623f\u95f4\u7684\u91cd\u65b0\u68c0\u6d4b\u9891\u7387\u63d0\u9ad8\uff0c\u4ee5\u53ca\u4f18\u5316\u65f6\u95f4\u6d88\u8017\u7684\u51cf\u5c11\uff0c\u8bc1\u660e\u4e86\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u53ef\u901a\u884c\u6027\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u56fe\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u4f4d\u59ff\u56fe\u4f18\u5316\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.15331", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15331", "abs": "https://arxiv.org/abs/2510.15331", "authors": ["Gahee Kim", "Takamitsu Matsubara"], "title": "ASBI: Leveraging Informative Real-World Data for Active Black-Box Simulator Tuning", "comment": null, "summary": "Black-box simulators are widely used in robotics, but optimizing their\nparameters remains challenging due to inaccessible likelihoods.\nSimulation-Based Inference (SBI) tackles this issue using simulation-driven\napproaches, estimating the posterior from offline real observations and forward\nsimulations. However, in black-box scenarios, preparing observations that\ncontain sufficient information for parameter estimation is difficult due to the\nunknown relationship between parameters and observations. In this work, we\npresent Active Simulation-Based Inference (ASBI), a parameter estimation\nframework that uses robots to actively collect real-world online data to\nachieve accurate black-box simulator tuning. Our framework optimizes robot\nactions to collect informative observations by maximizing information gain,\nwhich is defined as the expected reduction in Shannon entropy between the\nposterior and the prior. While calculating information gain requires the\nlikelihood, which is inaccessible in black-box simulators, our method solves\nthis problem by leveraging Neural Posterior Estimation (NPE), which leverages a\nneural network to learn the posterior estimator. Three simulation experiments\nquantitatively verify that our method achieves accurate parameter estimation,\nwith posteriors sharply concentrated around the true parameters. Moreover, we\nshow a practical application using a real robot to estimate the simulation\nparameters of cubic particles corresponding to two real objects, beads and\ngravel, with a bucket pouring action.", "AI": {"tldr": "\u63d0\u51fa\u4e3b\u52a8\u4eff\u771f\u63a8\u7406(ASBI)\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u4e3b\u52a8\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6765\u4f18\u5316\u9ed1\u76d2\u4eff\u771f\u5668\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4eff\u771f\u63a8\u7406\u4e2d\u89c2\u6d4b\u6570\u636e\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u9ed1\u76d2\u4eff\u771f\u5668\u5728\u673a\u5668\u4eba\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u65e0\u6cd5\u83b7\u53d6\u4f3c\u7136\u51fd\u6570\uff0c\u53c2\u6570\u4f18\u5316\u56f0\u96be\u3002\u4f20\u7edf\u4eff\u771f\u63a8\u7406\u9700\u8981\u79bb\u7ebf\u89c2\u6d4b\u6570\u636e\uff0c\u4f46\u96be\u4ee5\u51c6\u5907\u5305\u542b\u8db3\u591f\u53c2\u6570\u4f30\u8ba1\u4fe1\u606f\u7684\u89c2\u6d4b\u6570\u636e\u3002", "method": "\u4f7f\u7528\u4e3b\u52a8\u6570\u636e\u6536\u96c6\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\u6765\u4f18\u5316\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u5229\u7528\u795e\u7ecf\u540e\u9a8c\u4f30\u8ba1(NPE)\u89e3\u51b3\u9ed1\u76d2\u4eff\u771f\u5668\u4e2d\u4f3c\u7136\u51fd\u6570\u4e0d\u53ef\u8bbf\u95ee\u7684\u95ee\u9898\u3002", "result": "\u4e09\u4e2a\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u540e\u9a8c\u5206\u5e03\u96c6\u4e2d\u5728\u771f\u5b9e\u53c2\u6570\u5468\u56f4\u3002\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u6210\u529f\u4f30\u8ba1\u4e86\u7acb\u65b9\u9897\u7c92\u7684\u4eff\u771f\u53c2\u6570\u3002", "conclusion": "ASBI\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ed1\u76d2\u4eff\u771f\u5668\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u4e3b\u52a8\u6570\u636e\u6536\u96c6\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u53c2\u6570\u8c03\u4f18\u3002"}}
{"id": "2510.15336", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15336", "abs": "https://arxiv.org/abs/2510.15336", "authors": ["Liviu-Mihai Stan", "Ranulfo Bezerra", "Shotaro Kojima", "Tsige Tadesse Alemayoh", "Satoshi Tadokoro", "Masashi Konyo", "Kazunori Ohno"], "title": "Adaptive Cost-Map-based Path Planning in Partially Unknown Environments with Movable Obstacles", "comment": null, "summary": "Reliable navigation in disaster-response and other unstructured indoor\nsettings requires robots not only to avoid obstacles but also to recognise when\nthose obstacles can be pushed aside. We present an adaptive, LiDAR and\nodometry-based path-planning framework that embeds this capability into the\nROS2 Nav2 stack. A new Movable Obstacles Layer labels all LiDAR returns missing\nfrom a prior static map as tentatively movable and assigns a reduced traversal\ncost. A companion Slow-Pose Progress Checker monitors the ratio of commanded to\nactual velocity; when the robot slows appreciably, the local cost is raised\nfrom light to heavy, and on a stall to lethal, prompting the global planner to\nback out and re-route. Gazebo evaluations on a Scout Mini, spanning isolated\nobjects and cluttered corridors, show higher goal-reach rates and fewer\ndeadlocks than a no-layer baseline, with traversal times broadly comparable.\nBecause the method relies only on planar scans and CPU-level computation, it\nsuits resource-constrained search and rescue robots and integrates into\nheterogeneous platforms with minimal engineering. Overall, the results indicate\nthat interaction-aware cost maps are a lightweight, ROS2-native extension for\nnavigating among potentially movable obstacles in unstructured settings. The\nfull implementation will be released as open source\nathttps://costmap-namo.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLiDAR\u548c\u91cc\u7a0b\u8ba1\u7684\u81ea\u9002\u5e94\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u5728ROS2 Nav2\u5806\u6808\u4e2d\u8bc6\u522b\u548c\u63a8\u5f00\u53ef\u79fb\u52a8\u969c\u788d\u7269\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u5728\u707e\u96be\u54cd\u5e94\u548c\u975e\u7ed3\u6784\u5316\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u4e0d\u4ec5\u9700\u8981\u907f\u5f00\u969c\u788d\u7269\uff0c\u8fd8\u9700\u8981\u8bc6\u522b\u54ea\u4e9b\u969c\u788d\u7269\u53ef\u4ee5\u88ab\u63a8\u5f00\uff0c\u4ee5\u63d0\u9ad8\u5bfc\u822a\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u79fb\u52a8\u969c\u788d\u7269\u5c42\uff0c\u5c06\u9759\u6001\u5730\u56fe\u4e2d\u4e0d\u5b58\u5728\u7684LiDAR\u8fd4\u56de\u6807\u8bb0\u4e3a\u53ef\u79fb\u52a8\u969c\u788d\u7269\u5e76\u5206\u914d\u8f83\u4f4e\u904d\u5386\u6210\u672c\uff1b\u914d\u5408\u6162\u901f\u59ff\u6001\u8fdb\u5ea6\u68c0\u67e5\u5668\u76d1\u63a7\u901f\u5ea6\u6bd4\uff0c\u5728\u673a\u5668\u4eba\u51cf\u901f\u65f6\u63d0\u9ad8\u5c40\u90e8\u6210\u672c\uff0c\u4fc3\u4f7f\u5168\u5c40\u89c4\u5212\u5668\u91cd\u65b0\u89c4\u5212\u8def\u5f84\u3002", "result": "\u5728Gazebo\u6a21\u62df\u73af\u5883\u4e2d\u5bf9Scout Mini\u673a\u5668\u4eba\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u65e0\u5c42\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u76ee\u6807\u5230\u8fbe\u7387\u548c\u66f4\u5c11\u7684\u6b7b\u9501\uff0c\u904d\u5386\u65f6\u95f4\u5927\u81f4\u76f8\u5f53\u3002", "conclusion": "\u4ea4\u4e92\u611f\u77e5\u6210\u672c\u5730\u56fe\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684ROS2\u539f\u751f\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5bfc\u822a\u53ef\u79fb\u52a8\u969c\u788d\u7269\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u641c\u6551\u673a\u5668\u4eba\u3002"}}
{"id": "2510.15350", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.15350", "abs": "https://arxiv.org/abs/2510.15350", "authors": ["Shyalan Ramesh", "Scott Mann", "Alex Stumpf"], "title": "Nauplius Optimisation for Autonomous Hydrodynamics", "comment": null, "summary": "Autonomous Underwater vehicles must operate in strong currents, limited\nacoustic bandwidth, and persistent sensing requirements where conventional\nswarm optimisation methods are unreliable. This paper presents NOAH, a novel\nnature-inspired swarm optimisation algorithm that combines current-aware drift,\nirreversible settlement in persistent sensing nodes, and colony-based\ncommunication. Drawing inspiration from the behaviour of barnacle nauplii, NOAH\naddresses the critical limitations of existing swarm algorithms by providing\nhydrodynamic awareness, irreversible anchoring mechanisms, and colony-based\ncommunication capabilities essential for underwater exploration missions. The\nalgorithm establishes a comprehensive foundation for scalable and\nenergy-efficient underwater swarm robotics with validated performance analysis.\nValidation studies demonstrate an 86% success rate for permanent anchoring\nscenarios, providing a unified formulation for hydrodynamic constraints and\nirreversible settlement behaviours with an empirical study under flow.", "AI": {"tldr": "NOAH\u662f\u4e00\u79cd\u53d7\u85e4\u58f6\u5e7c\u4f53\u884c\u4e3a\u542f\u53d1\u7684\u4eff\u751f\u7fa4\u4f18\u5316\u7b97\u6cd5\uff0c\u4e13\u4e3a\u6c34\u4e0b\u81ea\u4e3b\u8f66\u8f86\u5728\u5f3a\u6c34\u6d41\u3001\u6709\u9650\u5e26\u5bbd\u548c\u6301\u7eed\u611f\u77e5\u9700\u6c42\u73af\u5883\u4e2d\u7684\u53ef\u9760\u8fd0\u884c\u800c\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u7fa4\u4f18\u5316\u65b9\u6cd5\u5728\u5f3a\u6c34\u6d41\u3001\u6709\u9650\u58f0\u5b66\u5e26\u5bbd\u548c\u6301\u7eed\u611f\u77e5\u9700\u6c42\u7684\u6c34\u4e0b\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u65b0\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u9650\u5236\u3002", "method": "\u7ed3\u5408\u6c34\u6d41\u611f\u77e5\u6f02\u79fb\u3001\u6301\u7eed\u611f\u77e5\u8282\u70b9\u4e2d\u7684\u4e0d\u53ef\u9006\u6c89\u964d\u548c\u57fa\u4e8e\u7fa4\u4f53\u7684\u901a\u4fe1\uff0c\u6a21\u4eff\u85e4\u58f6\u5e7c\u4f53\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u9a8c\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u5728\u6c38\u4e45\u951a\u5b9a\u573a\u666f\u4e2d\u8fbe\u523086%\u7684\u6210\u529f\u7387\uff0c\u4e3a\u6c34\u52a8\u529b\u7ea6\u675f\u548c\u4e0d\u53ef\u9006\u6c89\u964d\u884c\u4e3a\u63d0\u4f9b\u4e86\u7edf\u4e00\u516c\u5f0f\u3002", "conclusion": "NOAH\u4e3a\u53ef\u6269\u5c55\u548c\u8282\u80fd\u7684\u6c34\u4e0b\u7fa4\u673a\u5668\u4eba\u5efa\u7acb\u4e86\u5168\u9762\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7fa4\u7b97\u6cd5\u5728\u6c34\u4e0b\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u9650\u5236\u3002"}}
{"id": "2510.15352", "categories": ["cs.RO", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.15352", "abs": "https://arxiv.org/abs/2510.15352", "authors": ["Alejandro Escontrela", "Justin Kerr", "Arthur Allshire", "Jonas Frey", "Rocky Duan", "Carmelo Sferrazza", "Pieter Abbeel"], "title": "GaussGym: An open-source real-to-sim framework for learning locomotion from pixels", "comment": null, "summary": "We present a novel approach for photorealistic robot simulation that\nintegrates 3D Gaussian Splatting as a drop-in renderer within vectorized\nphysics simulators such as IsaacGym. This enables unprecedented speed --\nexceeding 100,000 steps per second on consumer GPUs -- while maintaining high\nvisual fidelity, which we showcase across diverse tasks. We additionally\ndemonstrate its applicability in a sim-to-real robotics setting. Beyond\ndepth-based sensing, our results highlight how rich visual semantics improve\nnavigation and decision-making, such as avoiding undesirable regions. We\nfurther showcase the ease of incorporating thousands of environments from\niPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs\nfrom generative video models like Veo, enabling rapid creation of realistic\ntraining worlds. This work bridges high-throughput simulation and high-fidelity\nperception, advancing scalable and generalizable robot learning. All code and\ndata will be open-sourced for the community to build upon. Videos, code, and\ndata available at https://escontrela.me/gauss_gym/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c063D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u6e32\u67d3\u5668\u96c6\u6210\u5230\u5411\u91cf\u5316\u7269\u7406\u6a21\u62df\u5668\u4e2d\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u8d85\u8fc710\u4e07\u6b65/\u79d2\u7684\u9ad8\u901f\u6a21\u62df\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u6865\u63a5\u9ad8\u541e\u5410\u91cf\u6a21\u62df\u548c\u9ad8\u4fdd\u771f\u611f\u77e5\uff0c\u63a8\u8fdb\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5feb\u901f\u8fd0\u884c\u53c8\u5177\u6709\u903c\u771f\u89c6\u89c9\u6548\u679c\u7684\u673a\u5668\u4eba\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u5c063D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u6e32\u67d3\u5668\u96c6\u6210\u5230IsaacGym\u7b49\u5411\u91cf\u5316\u7269\u7406\u6a21\u62df\u5668\u4e2d\uff0c\u652f\u6301\u4eceiPhone\u626b\u63cf\u3001\u5927\u89c4\u6a21\u573a\u666f\u6570\u636e\u96c6\u548c\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u4e2d\u5feb\u901f\u521b\u5efa\u903c\u771f\u8bad\u7ec3\u73af\u5883\u3002", "result": "\u5b9e\u73b0\u4e86\u8d85\u8fc710\u4e07\u6b65/\u79d2\u7684\u6a21\u62df\u901f\u5ea6\uff0c\u5c55\u793a\u4e86\u5728\u6a21\u62df\u5230\u771f\u5b9e\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4e30\u5bcc\u7684\u89c6\u89c9\u8bed\u4e49\u6539\u5584\u4e86\u5bfc\u822a\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u5982\u907f\u514d\u4e0d\u826f\u533a\u57df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u6865\u63a5\u4e86\u9ad8\u541e\u5410\u91cf\u6a21\u62df\u548c\u9ad8\u4fdd\u771f\u611f\u77e5\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2510.15376", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15376", "abs": "https://arxiv.org/abs/2510.15376", "authors": ["Zhaodong Yang", "Ai-Ping Hu", "Harish Ravichandar"], "title": "Towards Automated Chicken Deboning via Learning-based Dynamically-Adaptive 6-DoF Multi-Material Cutting", "comment": "8 Pages, 8 figures", "summary": "Automating chicken shoulder deboning requires precise 6-DoF cutting through a\npartially occluded, deformable, multi-material joint, since contact with the\nbones presents serious health and safety risks. Our work makes both\nsystems-level and algorithmic contributions to train and deploy a reactive\nforce-feedback cutting policy that dynamically adapts a nominal trajectory and\nenables full 6-DoF knife control to traverse the narrow joint gap while\navoiding contact with the bones. First, we introduce an open-source\ncustom-built simulator for multi-material cutting that models coupling,\nfracture, and cutting forces, and supports reinforcement learning, enabling\nefficient training and rapid prototyping. Second, we design a reusable physical\ntestbed to emulate the chicken shoulder: two rigid \"bone\" spheres with\ncontrollable pose embedded in a softer block, enabling rigorous and repeatable\nevaluation while preserving essential multi-material characteristics of the\ntarget problem. Third, we train and deploy a residual RL policy, with\ndiscretized force observations and domain randomization, enabling robust\nzero-shot sim-to-real transfer and the first demonstration of a learned policy\nthat debones a real chicken shoulder. Our experiments in our simulator, on our\nphysical testbed, and on real chicken shoulders show that our learned policy\nreliably navigates the joint gap and reduces undesired bone/cartilage contact,\nresulting in up to a 4x improvement over existing open-loop cutting baselines\nin terms of success rate and bone avoidance. Our results also illustrate the\nnecessity of force feedback for safe and effective multi-material cutting. The\nproject website is at https://sites.google.com/view/chickendeboning-2026.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529b\u53cd\u9988\u7684\u81ea\u52a8\u5316\u9e21\u80a9\u53bb\u9aa8\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u53cd\u5e94\u5f0f\u5207\u5272\u7b56\u7565\uff0c\u5b9e\u73b06\u81ea\u7531\u5ea6\u5200\u5177\u63a7\u5236\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6210\u529f\u5b8c\u6210\u9e21\u80a9\u53bb\u9aa8\u4efb\u52a1\u3002", "motivation": "\u81ea\u52a8\u5316\u9e21\u80a9\u53bb\u9aa8\u9700\u8981\u7cbe\u786e\u76846\u81ea\u7531\u5ea6\u5207\u5272\uff0c\u4f46\u7531\u4e8e\u5173\u8282\u90e8\u5206\u906e\u6321\u3001\u53ef\u53d8\u5f62\u4e14\u591a\u6750\u6599\u7279\u6027\uff0c\u63a5\u89e6\u9aa8\u9abc\u4f1a\u5e26\u6765\u4e25\u91cd\u7684\u5065\u5eb7\u548c\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e86\u5f00\u6e90\u591a\u6750\u6599\u5207\u5272\u4eff\u771f\u5668\uff0c\u8bbe\u8ba1\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u7269\u7406\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bad\u7ec3\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4f7f\u7528\u79bb\u6563\u5316\u529b\u89c2\u6d4b\u548c\u9886\u57df\u968f\u673a\u5316\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u8fc1\u79fb\u3002", "result": "\u5b66\u4e60\u7b56\u7565\u53ef\u9760\u5730\u5bfc\u822a\u5173\u8282\u95f4\u9699\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u9aa8\u9abc/\u8f6f\u9aa8\u63a5\u89e6\uff0c\u76f8\u6bd4\u73b0\u6709\u5f00\u73af\u5207\u5272\u57fa\u7ebf\uff0c\u6210\u529f\u7387\u63d0\u9ad84\u500d\uff0c\u9aa8\u9abc\u907f\u514d\u6548\u679c\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u529b\u53cd\u9988\u5bf9\u4e8e\u5b89\u5168\u6709\u6548\u7684\u591a\u6750\u6599\u5207\u5272\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u9996\u6b21\u5c55\u793a\u4e86\u5b66\u4e60\u7b56\u7565\u5728\u771f\u5b9e\u9e21\u80a9\u53bb\u9aa8\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.15446", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15446", "abs": "https://arxiv.org/abs/2510.15446", "authors": ["Ziang Guo", "Zufeng Zhang"], "title": "VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving", "comment": "1st version", "summary": "In autonomous driving, dynamic environment and corner cases pose significant\nchallenges to the robustness of ego vehicle's state understanding and decision\nmaking. We introduce VDRive, a novel pipeline for end-to-end autonomous driving\nthat explicitly models state-action mapping to address these challenges,\nenabling interpretable and robust decision making. By leveraging the\nadvancement of the state understanding of the Vision Language Action Model\n(VLA) with generative diffusion policy-based action head, our VDRive guides the\ndriving contextually and geometrically. Contextually, VLA predicts future\nobservations through token generation pre-training, where the observations are\nrepresented as discrete codes by a Conditional Vector Quantized Variational\nAutoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning\nfine-tuning of the VLA to predict future trajectories and actions based on\ncurrent driving conditions. VLA supplies the current state tokens and predicted\nstate tokens for the action policy head to generate hierarchical actions and\ntrajectories. During policy training, a learned critic evaluates the actions\ngenerated by the policy and provides gradient-based feedback, forming an\nactor-critic framework that enables a reinforcement-based policy learning\npipeline. Experiments show that our VDRive achieves state-of-the-art\nperformance in the Bench2Drive closed-loop benchmark and nuScenes open-loop\nplanning.", "AI": {"tldr": "VDRive\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001-\u52a8\u4f5c\u6620\u5c04\u5efa\u6a21\u6765\u89e3\u51b3\u52a8\u6001\u73af\u5883\u548c\u6781\u7aef\u60c5\u51b5\u7684\u6311\u6218\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u548c\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u9c81\u68d2\u51b3\u7b56\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u52a8\u6001\u73af\u5883\u548c\u6781\u7aef\u60c5\u51b5\u5bf9\u8f66\u8f86\u72b6\u6001\u7406\u89e3\u548c\u51b3\u7b56\u7684\u9c81\u68d2\u6027\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u8fdb\u884c\u72b6\u6001\u7406\u89e3\uff0c\u901a\u8fc7\u6761\u4ef6\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5c06\u89c2\u6d4b\u8868\u793a\u4e3a\u79bb\u6563\u4ee3\u7801\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u751f\u6210\u52a8\u4f5c\uff0c\u5e76\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6765\u9884\u6d4b\u8f68\u8ff9\u548c\u52a8\u4f5c\u3002", "result": "\u5728Bench2Drive\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u548cnuScenes\u5f00\u73af\u89c4\u5212\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "VDRive\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u51e0\u4f55\u5efa\u6a21\uff0c\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15505", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15505", "abs": "https://arxiv.org/abs/2510.15505", "authors": ["Aron Distelzweig", "Faris Janjo\u0161", "Oliver Scheel", "Sirish Reddy Varra", "Raghu Rajan", "Joschka Boedecker"], "title": "Perfect Prediction or Plenty of Proposals? What Matters Most in Planning for Autonomous Driving", "comment": "8 pages, 5 figures", "summary": "Traditionally, prediction and planning in autonomous driving (AD) have been\ntreated as separate, sequential modules. Recently, there has been a growing\nshift towards tighter integration of these components, known as Integrated\nPrediction and Planning (IPP), with the aim of enabling more informed and\nadaptive decision-making. However, it remains unclear to what extent this\nintegration actually improves planning performance. In this work, we\ninvestigate the role of prediction in IPP approaches, drawing on the widely\nadopted Val14 benchmark, which encompasses more common driving scenarios with\nrelatively low interaction complexity, and the interPlan benchmark, which\nincludes highly interactive and out-of-distribution driving situations. Our\nanalysis reveals that even access to perfect future predictions does not lead\nto better planning outcomes, indicating that current IPP methods often fail to\nfully exploit future behavior information. Instead, we focus on high-quality\nproposal generation, while using predictions primarily for collision checks. We\nfind that many imitation learning-based planners struggle to generate realistic\nand plausible proposals, performing worse than PDM - a simple lane-following\napproach. Motivated by this observation, we build on PDM with an enhanced\nproposal generation method, shifting the emphasis towards producing diverse but\nrealistic and high-quality proposals. This proposal-centric approach\nsignificantly outperforms existing methods, especially in out-of-distribution\nand highly interactive settings, where it sets new state-of-the-art results.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u96c6\u6210\u9884\u6d4b\u4e0e\u89c4\u5212\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u9884\u6d4b\u4fe1\u606f\uff0c\u63d0\u51fa\u4ee5\u9ad8\u8d28\u91cf\u63d0\u6848\u751f\u6210\u4e3a\u6838\u5fc3\u7684\u65b9\u6cd5\uff0c\u5728\u4ea4\u4e92\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u63a2\u8ba8\u96c6\u6210\u9884\u6d4b\u4e0e\u89c4\u5212\u65b9\u6cd5\u4e2d\u9884\u6d4b\u7684\u5b9e\u9645\u4f5c\u7528\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u771f\u6b63\u63d0\u5347\u89c4\u5212\u6027\u80fd", "method": "\u57fa\u4e8ePDM\u589e\u5f3a\u63d0\u6848\u751f\u6210\uff0c\u5f3a\u8c03\u751f\u6210\u591a\u6837\u4f46\u771f\u5b9e\u7684\u9ad8\u8d28\u91cf\u63d0\u6848\uff0c\u4e3b\u8981\u4f7f\u7528\u9884\u6d4b\u8fdb\u884c\u78b0\u649e\u68c0\u67e5", "result": "\u63d0\u6848\u4e2d\u5fc3\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u548c\u9ad8\u5ea6\u4ea4\u4e92\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73", "conclusion": "\u5f53\u524dIPP\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u9884\u6d4b\u4fe1\u606f\uff0c\u9ad8\u8d28\u91cf\u63d0\u6848\u751f\u6210\u6bd4\u9884\u6d4b\u96c6\u6210\u66f4\u91cd\u8981"}}
{"id": "2510.15530", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15530", "abs": "https://arxiv.org/abs/2510.15530", "authors": ["Zehao Ni", "Yonghao He", "Lingfeng Qian", "Jilei Mao", "Fa Fu", "Wei Sui", "Hu Su", "Junran Peng", "Zhipeng Wang", "Bin He"], "title": "VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation", "comment": null, "summary": "In the context of imitation learning, visuomotor-based diffusion policy\nlearning is one of the main directions in robotic manipulation. Most of these\napproaches rely on point clouds as observation inputs and construct scene\nrepresentations through point clouds feature learning, which enables them to\nachieve remarkable accuracy. However, the existing literature lacks an in-depth\nexploration of vision-only solutions that have significant potential. In this\npaper, we propose a Vision-Only and single-view Diffusion Policy learning\nmethod (VO-DP) that leverages pretrained visual foundation models to achieve\neffective fusion of semantic and geometric features. We utilize intermediate\nfeatures from VGGT incorporating semantic features from DINOv2 and geometric\nfeatures from Alternating Attention blocks. Features are fused via\ncross-attention and spatially compressed with a CNN to form the input to the\npolicy head. Extensive experiments demonstrate that VO-DP not only outperforms\nthe vision-only baseline DP significantly but also exhibits distinct\nperformance trends against the point cloud-based method DP3: in simulation\ntasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%\nand far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,\noutperforming both DP3 67.5% and DP 11.2% by a notable margin. Further\nrobustness evaluations confirm that VO-DP remains highly stable under varying\nconditions including color, size, background, and lighting. Lastly, we\nopen-source a training library for robotic manipulation. Built on Accelerate,\nthis library supports multi-machine and multi-GPU parallel training, as well as\nmixed precision training. It is compatible with visuomotor policies such as DP,\nDP3 and VO-DP, and also supports the RoboTwin simulator.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u89c6\u89c9\u8f93\u5165\u7684\u5355\u89c6\u56fe\u6269\u6563\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5VO-DP\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u878d\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u70b9\u4e91\u4f5c\u4e3a\u89c2\u6d4b\u8f93\u5165\uff0c\u7f3a\u4e4f\u5bf9\u4ec5\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u7684\u6df1\u5165\u63a2\u7d22\uff0c\u800c\u4ec5\u89c6\u89c9\u65b9\u6cd5\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002", "method": "\u5229\u7528VGGT\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u7ed3\u5408DINOv2\u7684\u8bed\u4e49\u7279\u5f81\u548c\u4ea4\u66ff\u6ce8\u610f\u529b\u5757\u7684\u51e0\u4f55\u7279\u5f81\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u7279\u5f81\uff0c\u5e76\u7528CNN\u8fdb\u884c\u7a7a\u95f4\u538b\u7f29\u4f5c\u4e3a\u7b56\u7565\u5934\u8f93\u5165\u3002", "result": "\u5728\u4eff\u771f\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u738764.6%\uff0c\u4e0eDP3\u768464.0%\u76f8\u5f53\uff0c\u8fdc\u9ad8\u4e8eDP\u768434.8%\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8fbe\u523087.9%\uff0c\u663e\u8457\u4f18\u4e8eDP3\u768467.5%\u548cDP\u768411.2%\u3002", "conclusion": "VO-DP\u65b9\u6cd5\u5728\u4ec5\u89c6\u89c9\u8f93\u5165\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u70b9\u4e91\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5f00\u6e90\u4e86\u652f\u6301\u591a\u673a\u591aGPU\u8bad\u7ec3\u7684\u5de5\u5177\u5e93\u3002"}}
{"id": "2510.15533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15533", "abs": "https://arxiv.org/abs/2510.15533", "authors": ["Shilei Li", "Dawei Shi", "Makoto Iwasaki", "Yan Ning", "Hongpeng Zhou", "Ling Shi"], "title": "Improved Extended Kalman Filter-Based Disturbance Observers for Exoskeletons", "comment": null, "summary": "The nominal performance of mechanical systems is often degraded by unknown\ndisturbances. A two-degree-of-freedom control structure can decouple nominal\nperformance from disturbance rejection. However, perfect disturbance rejection\nis unattainable when the disturbance dynamic is unknown. In this work, we\nreveal an inherent trade-off in disturbance estimation subject to tracking\nspeed and tracking uncertainty. Then, we propose two novel methods to enhance\ndisturbance estimation: an interacting multiple model extended Kalman\nfilter-based disturbance observer and a multi-kernel correntropy extended\nKalman filter-based disturbance observer. Experiments on an exoskeleton verify\nthat the proposed two methods improve the tracking accuracy $36.3\\%$ and\n$16.2\\%$ in hip joint error, and $46.3\\%$ and $24.4\\%$ in knee joint error,\nrespectively, compared to the extended Kalman filter-based disturbance\nobserver, in a time-varying interaction force scenario, demonstrating the\nsuperiority of the proposed method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u6270\u52a8\u89c2\u6d4b\u5668\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u673a\u68b0\u7cfb\u7edf\u7684\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5728\u5916\u9aa8\u9abc\u5b9e\u9a8c\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u9acb\u5173\u8282\u548c\u819d\u5173\u8282\u7684\u8ddf\u8e2a\u8bef\u5dee\u3002", "motivation": "\u673a\u68b0\u7cfb\u7edf\u7684\u6807\u79f0\u6027\u80fd\u5e38\u56e0\u672a\u77e5\u6270\u52a8\u800c\u4e0b\u964d\uff0c\u867d\u7136\u4e8c\u81ea\u7531\u5ea6\u63a7\u5236\u7ed3\u6784\u80fd\u5c06\u6807\u79f0\u6027\u80fd\u4e0e\u6270\u52a8\u6291\u5236\u89e3\u8026\uff0c\u4f46\u5f53\u6270\u52a8\u52a8\u6001\u672a\u77e5\u65f6\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b8c\u7f8e\u7684\u6270\u52a8\u6291\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u6270\u52a8\u4f30\u8ba1\u65b9\u6cd5\uff1a\u57fa\u4e8e\u4ea4\u4e92\u591a\u6a21\u578b\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u6270\u52a8\u89c2\u6d4b\u5668\u548c\u57fa\u4e8e\u591a\u6838\u76f8\u5173\u71b5\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u6270\u52a8\u89c2\u6d4b\u5668\u3002", "result": "\u5728\u5916\u9aa8\u9abc\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u57fa\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u6270\u52a8\u89c2\u6d4b\u5668\u76f8\u6bd4\uff0c\u6240\u63d0\u4e24\u79cd\u65b9\u6cd5\u5728\u65f6\u53d8\u4ea4\u4e92\u529b\u573a\u666f\u4e0b\u5206\u522b\u5c06\u9acb\u5173\u8282\u8bef\u5dee\u63d0\u9ad8\u4e8636.3%\u548c16.2%\uff0c\u819d\u5173\u8282\u8bef\u5dee\u63d0\u9ad8\u4e8646.3%\u548c24.4%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6270\u52a8\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5916\u9aa8\u9abc\u7cfb\u7edf\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u3002"}}
{"id": "2510.15626", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15626", "abs": "https://arxiv.org/abs/2510.15626", "authors": ["Hongyu Zhou", "Xiaoyu Zhang", "Vasileios Tzoumas"], "title": "Adaptive Legged Locomotion via Online Learning for Model Predictive Control", "comment": "9 pages", "summary": "We provide an algorithm for adaptive legged locomotion via online learning\nand model predictive control. The algorithm is composed of two interacting\nmodules: model predictive control (MPC) and online learning of residual\ndynamics. The residual dynamics can represent modeling errors and external\ndisturbances. We are motivated by the future of autonomy where quadrupeds will\nautonomously perform complex tasks despite real-world unknown uncertainty, such\nas unknown payload and uneven terrains. The algorithm uses random Fourier\nfeatures to approximate the residual dynamics in reproducing kernel Hilbert\nspaces. Then, it employs MPC based on the current learned model of the residual\ndynamics. The model is updated online in a self-supervised manner using least\nsquares based on the data collected while controlling the quadruped. The\nalgorithm enjoys sublinear \\textit{dynamic regret}, defined as the\nsuboptimality against an optimal clairvoyant controller that knows how the\nresidual dynamics. We validate our algorithm in Gazebo and MuJoCo simulations,\nwhere the quadruped aims to track reference trajectories. The Gazebo\nsimulations include constant unknown external forces up to $12\\boldsymbol{g}$,\nwhere $\\boldsymbol{g}$ is the gravity vector, in flat terrain, slope terrain\nwith $20\\degree$ inclination, and rough terrain with $0.25m$ height variation.\nThe MuJoCo simulations include time-varying unknown disturbances with payload\nup to $8~kg$ and time-varying ground friction coefficients in flat terrain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5b9e\u73b0\u81ea\u9002\u5e94\u817f\u5f0f\u8fd0\u52a8\u7684\u7b97\u6cd5\uff0c\u5305\u542bMPC\u548c\u6b8b\u5dee\u52a8\u529b\u5b66\u5728\u7ebf\u5b66\u4e60\u4e24\u4e2a\u4ea4\u4e92\u6a21\u5757\uff0c\u80fd\u591f\u5904\u7406\u5efa\u6a21\u8bef\u5dee\u548c\u5916\u90e8\u5e72\u6270\u3002", "motivation": "\u8ba9\u56db\u8db3\u673a\u5668\u4eba\u5728\u5b58\u5728\u672a\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u81ea\u4e3b\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0c\u5982\u672a\u77e5\u8d1f\u8f7d\u548c\u4e0d\u5e73\u5766\u5730\u5f62\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u8fd1\u4f3c\u6b8b\u5dee\u52a8\u529b\u5b66\uff0c\u57fa\u4e8e\u5f53\u524d\u5b66\u4e60\u6a21\u578b\u8fdb\u884cMPC\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5728\u7ebf\u81ea\u76d1\u7763\u66f4\u65b0\u6a21\u578b\u3002", "result": "\u7b97\u6cd5\u5177\u6709\u6b21\u7ebf\u6027\u52a8\u6001\u9057\u61be\u6027\u80fd\uff0c\u5728Gazebo\u548cMuJoCo\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u8fbe12\u500d\u91cd\u529b\u7684\u5916\u529b\u300120\u5ea6\u659c\u5761\u30010.25\u7c73\u9ad8\u5ea6\u53d8\u5316\u5730\u5f62\u30018kg\u8d1f\u8f7d\u548c\u65f6\u53d8\u5730\u9762\u6469\u64e6\u7cfb\u6570\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u672a\u77e5\u5efa\u6a21\u8bef\u5dee\u548c\u5916\u90e8\u5e72\u6270\uff0c\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u63a7\u5236\u3002"}}
{"id": "2510.15638", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15638", "abs": "https://arxiv.org/abs/2510.15638", "authors": ["Jared K. Lepora", "Haoran Li", "Efi Psomopoulou", "Nathan F. Lepora"], "title": "Educational SoftHand-A: Building an Anthropomorphic Hand with Soft Synergies using LEGO MINDSTORMS", "comment": "6 pages. Accepted at IROS 2025", "summary": "This paper introduces an anthropomorphic robot hand built entirely using LEGO\nMINDSTORMS: the Educational SoftHand-A, a tendon-driven, highly-underactuated\nrobot hand based on the Pisa/IIT SoftHand and related hands. To be suitable for\nan educational context, the design is constrained to use only standard LEGO\npieces with tests using common equipment available at home. The hand features\ndual motors driving an agonist/antagonist opposing pair of tendons on each\nfinger, which are shown to result in reactive fine control. The finger motions\nare synchonized through soft synergies, implemented with a differential\nmechanism using clutch gears. Altogether, this design results in an\nanthropomorphic hand that can adaptively grasp a broad range of objects using a\nsimple actuation and control mechanism. Since the hand can be constructed from\nLEGO pieces and uses state-of-the-art design concepts for robotic hands, it has\nthe potential to educate and inspire children to learn about the frontiers of\nmodern robotics.", "AI": {"tldr": "\u4f7f\u7528\u4e50\u9ad8MINDSTORMS\u6784\u5efa\u7684\u4eff\u4eba\u673a\u5668\u4eba\u624b\uff0c\u91c7\u7528\u808c\u8171\u9a71\u52a8\u548c\u9ad8\u5ea6\u6b20\u9a71\u52a8\u8bbe\u8ba1\uff0c\u5177\u6709\u81ea\u9002\u5e94\u6293\u63e1\u80fd\u529b\uff0c\u9002\u5408\u6559\u80b2\u7528\u9014\u3002", "motivation": "\u4e3a\u6559\u80b2\u73af\u5883\u8bbe\u8ba1\u4e00\u4e2a\u4ec5\u4f7f\u7528\u6807\u51c6\u4e50\u9ad8\u96f6\u4ef6\u548c\u5bb6\u7528\u8bbe\u5907\u7684\u673a\u5668\u4eba\u624b\uff0c\u8ba9\u513f\u7ae5\u80fd\u591f\u63a5\u89e6\u548c\u5b66\u4e60\u73b0\u4ee3\u673a\u5668\u4eba\u6280\u672f\u7684\u524d\u6cbf\u6982\u5ff5\u3002", "method": "\u91c7\u7528\u808c\u8171\u9a71\u52a8\u8bbe\u8ba1\uff0c\u6bcf\u4e2a\u624b\u6307\u4f7f\u7528\u53cc\u7535\u673a\u9a71\u52a8\u62ee\u6297\u808c\u8171\u5bf9\uff0c\u901a\u8fc7\u79bb\u5408\u5668\u9f7f\u8f6e\u5b9e\u73b0\u8f6f\u534f\u540c\u8fd0\u52a8\u7684\u5dee\u52a8\u673a\u5236\u3002", "result": "\u5b9e\u73b0\u4e86\u5177\u6709\u7cbe\u7ec6\u53cd\u5e94\u63a7\u5236\u7684\u4eff\u4eba\u624b\u578b\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u6293\u63e1\u5404\u79cd\u7269\u4f53\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u9a71\u52a8\u548c\u63a7\u5236\u673a\u5236\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u6210\u529f\u5c55\u793a\u4e86\u4f7f\u7528\u4e50\u9ad8\u96f6\u4ef6\u6784\u5efa\u5148\u8fdb\u673a\u5668\u4eba\u624b\u7684\u53ef\u884c\u6027\uff0c\u5177\u6709\u6559\u80b2\u548c\u542f\u53d1\u513f\u7ae5\u5b66\u4e60\u73b0\u4ee3\u673a\u5668\u4eba\u6280\u672f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.15639", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15639", "abs": "https://arxiv.org/abs/2510.15639", "authors": ["Manuel J. Fernandez", "Alejandro Suarez", "Anibal Ollero", "Matteo Fumagalli"], "title": "Integration of a Variable Stiffness Link for Long-Reach Aerial Manipulation", "comment": null, "summary": "This paper presents the integration of a Variable Stiffness Link (VSL) for\nlong-reach aerial manipulation, enabling adaptable mechanical coupling between\nan aerial multirotor platform and a dual-arm manipulator. Conventional\nlong-reach manipulation systems rely on rigid or cable connections, which limit\nprecision or transmit disturbances to the aerial vehicle. The proposed VSL\nintroduces an adjustable stiffness mechanism that allows the link to behave\neither as a flexible rope or as a rigid rod, depending on task requirements.\n  The system is mounted on a quadrotor equipped with the LiCAS dual-arm\nmanipulator and evaluated through teleoperated experiments, involving external\ndisturbances and parcel transportation tasks. Results demonstrate that varying\nthe link stiffness significantly modifies the dynamic interaction between the\nUAV and the payload. The flexible configuration attenuates external impacts and\naerodynamic perturbations, while the rigid configuration improves positional\naccuracy during manipulation phases.\n  These results confirm that VSL enhances versatility and safety, providing a\ncontrollable trade-off between compliance and precision. Future work will focus\non autonomous stiffness regulation, multi-rope configurations, cooperative\naerial manipulation and user studies to further assess its impact on\nteleoperated and semi-autonomous aerial tasks.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u53d8\u521a\u5ea6\u8fde\u6746(VSL)\u7528\u4e8e\u957f\u8ddd\u79bb\u7a7a\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7\u8c03\u8282\u673a\u68b0\u8026\u5408\u521a\u5ea6\u5728\u67d4\u6027\u7ef3\u7d22\u548c\u521a\u6027\u6746\u4e4b\u95f4\u5207\u6362\uff0c\u5e73\u8861\u6297\u5e72\u6270\u80fd\u529b\u548c\u64cd\u4f5c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u957f\u8ddd\u79bb\u7a7a\u4e2d\u64cd\u4f5c\u7cfb\u7edf\u4f7f\u7528\u521a\u6027\u6216\u7f06\u7ef3\u8fde\u63a5\uff0c\u9650\u5236\u4e86\u64cd\u4f5c\u7cbe\u5ea6\u6216\u4f1a\u5c06\u6270\u52a8\u4f20\u9012\u7ed9\u98de\u884c\u5668\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u8c03\u8282\u521a\u5ea6\u7684\u8fde\u63a5\u673a\u5236\u3002", "method": "\u5728\u914d\u5907LiCAS\u53cc\u81c2\u673a\u68b0\u624b\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u96c6\u6210\u53ef\u53d8\u521a\u5ea6\u8fde\u6746\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u5176\u5728\u5916\u90e8\u5e72\u6270\u548c\u5305\u88f9\u8fd0\u8f93\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53ef\u53d8\u521a\u5ea6\u663e\u8457\u6539\u53d8\u4e86\u65e0\u4eba\u673a\u4e0e\u8d1f\u8f7d\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\uff1a\u67d4\u6027\u914d\u7f6e\u8870\u51cf\u5916\u90e8\u51b2\u51fb\u548c\u6c14\u52a8\u6270\u52a8\uff0c\u521a\u6027\u914d\u7f6e\u63d0\u9ad8\u64cd\u4f5c\u9636\u6bb5\u7684\u4f4d\u7f6e\u7cbe\u5ea6\u3002", "conclusion": "VSL\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u591a\u529f\u80fd\u6027\u548c\u5b89\u5168\u6027\uff0c\u5728\u67d4\u987a\u6027\u548c\u7cbe\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u4e86\u53ef\u63a7\u6743\u8861\u3002\u672a\u6765\u5c06\u7814\u7a76\u81ea\u4e3b\u521a\u5ea6\u8c03\u8282\u3001\u591a\u7ef3\u914d\u7f6e\u548c\u534f\u4f5c\u7a7a\u4e2d\u64cd\u4f5c\u3002"}}
{"id": "2510.15668", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15668", "abs": "https://arxiv.org/abs/2510.15668", "authors": ["Yameng Zhang", "Dianye Huang", "Max Q. -H. Meng", "Nassir Navab", "Zhongliang Jiang"], "title": "Freehand 3D Ultrasound Imaging: Sim-in-the-Loop Probe Pose Optimization via Visual Servoing", "comment": null, "summary": "Freehand 3D ultrasound (US) imaging using conventional 2D probes offers\nflexibility and accessibility for diverse clinical applications but faces\nchallenges in accurate probe pose estimation. Traditional methods depend on\ncostly tracking systems, while neural network-based methods struggle with image\nnoise and error accumulation, compromising reconstruction precision. We propose\na cost-effective and versatile solution that leverages lightweight cameras and\nvisual servoing in simulated environments for precise 3D US imaging. These\ncameras capture visual feedback from a textured planar workspace. To counter\nocclusions and lighting issues, we introduce an image restoration method that\nreconstructs occluded regions by matching surrounding texture patterns. For\npose estimation, we develop a simulation-in-the-loop approach, which replicates\nthe system setup in simulation and iteratively minimizes pose errors between\nsimulated and real-world observations. A visual servoing controller refines the\nalignment of camera views, improving translational estimation by optimizing\nimage alignment. Validations on a soft vascular phantom, a 3D-printed conical\nmodel, and a human arm demonstrate the robustness and accuracy of our approach,\nwith Hausdorff distances to the reference reconstructions of 0.359 mm, 1.171\nmm, and 0.858 mm, respectively. These results confirm the method's potential\nfor reliable freehand 3D US reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8f7b\u91cf\u7ea7\u6444\u50cf\u5934\u548c\u89c6\u89c9\u4f3a\u670d\u7684\u4f4e\u6210\u672c3D\u8d85\u58f0\u6210\u50cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u4fee\u590d\u548c\u4eff\u771f\u5faa\u73af\u6280\u672f\u89e3\u51b3\u906e\u6321\u548c\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf3D\u8d85\u58f0\u6210\u50cf\u4f9d\u8d56\u6602\u8d35\u7684\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u800c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u53d7\u56fe\u50cf\u566a\u58f0\u548c\u8bef\u5dee\u7d2f\u79ef\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6444\u50cf\u5934\u6355\u6349\u89c6\u89c9\u53cd\u9988\uff0c\u5f15\u5165\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5904\u7406\u906e\u6321\uff0c\u91c7\u7528\u4eff\u771f\u5faa\u73af\u65b9\u6cd5\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u5668\u4f18\u5316\u56fe\u50cf\u5bf9\u9f50\u3002", "result": "\u5728\u8840\u7ba1\u6a21\u578b\u3001\u5706\u9525\u6a21\u578b\u548c\u4eba\u4f53\u624b\u81c2\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u4e0e\u53c2\u8003\u91cd\u5efa\u7684Hausdorff\u8ddd\u79bb\u5206\u522b\u4e3a0.359 mm\u30011.171 mm\u548c0.858 mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u53ef\u9760\u7684\u5f92\u624b3D\u8d85\u58f0\u91cd\u5efa\u6f5c\u529b\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.15679", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15679", "abs": "https://arxiv.org/abs/2510.15679", "authors": ["Yuhong Cao", "Yizhuo Wang", "Jingsong Liang", "Shuhao Liao", "Yifeng Zhang", "Peizhuo Li", "Guillaume Sartoretti"], "title": "HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward", "comment": null, "summary": "This work pushes the boundaries of learning-based methods in autonomous robot\nexploration in terms of environmental scale and exploration efficiency. We\npresent HEADER, an attention-based reinforcement learning approach with\nhierarchical graphs for efficient exploration in large-scale environments.\nHEADER follows existing conventional methods to construct hierarchical\nrepresentations for the robot belief/map, but further designs a novel\ncommunity-based algorithm to construct and update a global graph, which remains\nfully incremental, shape-adaptive, and operates with linear complexity.\nBuilding upon attention-based networks, our planner finely reasons about the\nnearby belief within the local range while coarsely leveraging distant\ninformation at the global scale, enabling next-best-viewpoint decisions that\nconsider multi-scale spatial dependencies. Beyond novel map representation, we\nintroduce a parameter-free privileged reward that significantly improves model\nperformance and produces near-optimal exploration behaviors, by avoiding\ntraining objective bias caused by handcrafted reward shaping. In simulated\nchallenging, large-scale exploration scenarios, HEADER demonstrates better\nscalability than most existing learning and non-learning methods, while\nachieving a significant improvement in exploration efficiency (up to 20%) over\nstate-of-the-art baselines. We also deploy HEADER on hardware and validate it\nin complex, large-scale real-life scenarios, including a 300m*230m campus\nenvironment.", "AI": {"tldr": "HEADER\u662f\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u5206\u5c42\u56fe\u7ed3\u6784\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u6548\u81ea\u4e3b\u63a2\u7d22\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22\u6548\u7387\u4e0a\u63d0\u5347\u9ad8\u8fbe20%\u3002", "motivation": "\u63a8\u52a8\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u81ea\u4e3b\u673a\u5668\u4eba\u63a2\u7d22\u4e2d\u7684\u8fb9\u754c\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u63a2\u7d22\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u5206\u5c42\u56fe\u7ed3\u6784\uff0c\u8bbe\u8ba1\u65b0\u9896\u7684\u57fa\u4e8e\u793e\u533a\u7684\u5168\u5c40\u56fe\u6784\u5efa\u66f4\u65b0\u7b97\u6cd5\uff0c\u7ed3\u5408\u53c2\u6570\u81ea\u7531\u7684\u7279\u6743\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728\u6a21\u62df\u5927\u89c4\u6a21\u63a2\u7d22\u573a\u666f\u4e2d\uff0cHEADER\u6bd4\u5927\u591a\u6570\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u548c\u975e\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u63a2\u7d22\u6548\u7387\u63d0\u5347\u9ad8\u8fbe20%\u3002\u5728\u771f\u5b9e300m*230m\u6821\u56ed\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\u9a8c\u8bc1\u3002", "conclusion": "HEADER\u901a\u8fc7\u5206\u5c42\u56fe\u8868\u793a\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u7279\u6743\u5956\u52b1\u7684\u6709\u6548\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u81ea\u4e3b\u63a2\u7d22\u3002"}}
{"id": "2510.15686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15686", "abs": "https://arxiv.org/abs/2510.15686", "authors": ["Taehyeon Kim", "Vishnunandan L. N. Venkatesh", "Byung-Cheol Min"], "title": "Few-Shot Demonstration-Driven Task Coordination and Trajectory Execution for Multi-Robot Systems", "comment": null, "summary": "In this paper, we propose a novel few-shot learning framework for multi-robot\nsystems that integrate both spatial and temporal elements: Few-Shot\nDemonstration-Driven Task Coordination and Trajectory Execution (DDACE). Our\napproach leverages temporal graph networks for learning task-agnostic temporal\nsequencing and Gaussian Processes for spatial trajectory modeling, ensuring\nmodularity and generalization across various tasks. By decoupling temporal and\nspatial aspects, DDACE requires only a small number of demonstrations,\nsignificantly reducing data requirements compared to traditional learning from\ndemonstration approaches. To validate our proposed framework, we conducted\nextensive experiments in task environments designed to assess various aspects\nof multi-robot coordination-such as multi-sequence execution, multi-action\ndynamics, complex trajectory generation, and heterogeneous configurations. The\nexperimental results demonstrate that our approach successfully achieves task\nexecution under few-shot learning conditions and generalizes effectively across\ndynamic and diverse settings. This work underscores the potential of modular\narchitectures in enhancing the practicality and scalability of multi-robot\nsystems in real-world applications. Additional materials are available at\nhttps://sites.google.com/view/ddace.", "AI": {"tldr": "\u63d0\u51fa\u4e86DDACE\u6846\u67b6\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u65f6\u7a7a\u8981\u7d20\u5b9e\u73b0\u4efb\u52a1\u534f\u8c03\u548c\u8f68\u8ff9\u6267\u884c", "motivation": "\u4f20\u7edf\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u65b9\u6cd5", "method": "\u4f7f\u7528\u65f6\u95f4\u56fe\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u65e0\u5173\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u8fdb\u884c\u7a7a\u95f4\u8f68\u8ff9\u5efa\u6a21\uff0c\u5b9e\u73b0\u65f6\u7a7a\u8981\u7d20\u7684\u6a21\u5757\u5316\u89e3\u8026", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u5305\u62ec\u591a\u5e8f\u5217\u6267\u884c\u3001\u591a\u52a8\u4f5c\u52a8\u6001\u3001\u590d\u6742\u8f68\u8ff9\u751f\u6210\u548c\u5f02\u6784\u914d\u7f6e\uff0c\u6210\u529f\u5b9e\u73b0\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u4efb\u52a1\u6267\u884c", "conclusion": "\u6a21\u5757\u5316\u67b6\u6784\u80fd\u663e\u8457\u63d0\u5347\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2510.15786", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15786", "abs": "https://arxiv.org/abs/2510.15786", "authors": ["Xinyue Xu", "Jieqiang Sun", "Jing", "Dai", "Siyuan Chen", "Lanjie Ma", "Ke Sun", "Bin Zhao", "Jianbo Yuan", "Yiwen Lu"], "title": "DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation", "comment": null, "summary": "We present DexCanvas, a large-scale hybrid real-synthetic human manipulation\ndataset containing 7,000 hours of dexterous hand-object interactions seeded\nfrom 70 hours of real human demonstrations, organized across 21 fundamental\nmanipulation types based on the Cutkosky taxonomy. Each entry combines\nsynchronized multi-view RGB-D, high-precision mocap with MANO hand parameters,\nand per-frame contact points with physically consistent force profiles. Our\nreal-to-sim pipeline uses reinforcement learning to train policies that control\nan actuated MANO hand in physics simulation, reproducing human demonstrations\nwhile discovering the underlying contact forces that generate the observed\nobject motion. DexCanvas is the first manipulation dataset to combine\nlarge-scale real demonstrations, systematic skill coverage based on established\ntaxonomies, and physics-validated contact annotations. The dataset can\nfacilitate research in robotic manipulation learning, contact-rich control, and\nskill transfer across different hand morphologies.", "AI": {"tldr": "DexCanvas\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6df7\u5408\u73b0\u5b9e-\u5408\u6210\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u5305\u542b7000\u5c0f\u65f6\u7075\u5de7\u624b-\u7269\u4f53\u4ea4\u4e92\uff0c\u57fa\u4e8e70\u5c0f\u65f6\u771f\u5b9e\u4eba\u7c7b\u6f14\u793a\uff0c\u6db5\u76d621\u79cd\u57fa\u672c\u64cd\u4f5c\u7c7b\u578b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7ed3\u5408\u5927\u89c4\u6a21\u771f\u5b9e\u6f14\u793a\u3001\u7cfb\u7edf\u6027\u6280\u80fd\u8986\u76d6\u548c\u7269\u7406\u9a8c\u8bc1\u63a5\u89e6\u6807\u6ce8\u7684\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u3001\u63a5\u89e6\u4e30\u5bcc\u63a7\u5236\u548c\u6280\u80fd\u8fc1\u79fb\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u5230\u865a\u62df\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u63a7\u5236\u9a71\u52a8MANO\u624b\u5728\u7269\u7406\u6a21\u62df\u4e2d\u91cd\u73b0\u4eba\u7c7b\u6f14\u793a\uff0c\u540c\u65f6\u53d1\u73b0\u4ea7\u751f\u89c2\u5bdf\u7269\u4f53\u8fd0\u52a8\u7684\u6f5c\u5728\u63a5\u89e6\u529b\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u7ed3\u5408\u5927\u89c4\u6a21\u771f\u5b9e\u6f14\u793a\u3001\u57fa\u4e8e\u65e2\u5b9a\u5206\u7c7b\u6cd5\u7684\u7cfb\u7edf\u6027\u6280\u80fd\u8986\u76d6\u548c\u7269\u7406\u9a8c\u8bc1\u63a5\u89e6\u6807\u6ce8\u7684\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u5305\u542b\u540c\u6b65\u591a\u89c6\u89d2RGB-D\u3001\u9ad8\u7cbe\u5ea6\u52a8\u4f5c\u6355\u6349\u548c\u9010\u5e27\u63a5\u89e6\u70b9\u3002", "conclusion": "DexCanvas\u6570\u636e\u96c6\u80fd\u591f\u4fc3\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u3001\u63a5\u89e6\u4e30\u5bcc\u63a7\u5236\u548c\u4e0d\u540c\u624b\u5f62\u6001\u95f4\u6280\u80fd\u8fc1\u79fb\u7684\u7814\u7a76\u3002"}}
{"id": "2510.15803", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15803", "abs": "https://arxiv.org/abs/2510.15803", "authors": ["Zahra Arjmandi", "Gunho Sohn"], "title": "Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion", "comment": "9 pages, 9 figures", "summary": "This paper presents a novel fusion technique for LiDAR Simultaneous\nLocalization and Mapping (SLAM), aimed at improving localization and 3D mapping\nusing LiDAR sensor. Our approach centers on the Inferred Attention Fusion\n(INAF) module, which integrates AI with geometric odometry. Utilizing the KITTI\ndataset's LiDAR data, INAF dynamically adjusts attention weights based on\nenvironmental feedback, enhancing the system's adaptability and measurement\naccuracy. This method advances the precision of both localization and 3D\nmapping, demonstrating the potential of our fusion technique to enhance\nautonomous navigation systems in complex scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684LiDAR SLAM\u878d\u5408\u6280\u672f\uff0c\u901a\u8fc7\u63a8\u65ad\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u7ed3\u5408AI\u4e0e\u51e0\u4f55\u91cc\u7a0b\u8ba1\uff0c\u63d0\u9ad8\u5b9a\u4f4d\u548c3D\u5efa\u56fe\u7cbe\u5ea6\u3002", "motivation": "\u6539\u8fdbLiDAR\u4f20\u611f\u5668\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5b9a\u4f4d\u548c3D\u5efa\u56fe\u80fd\u529b\uff0c\u589e\u5f3a\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528INAF\u6a21\u5757\uff0c\u57fa\u4e8eKITTI\u6570\u636e\u96c6\u7684LiDAR\u6570\u636e\uff0c\u6839\u636e\u73af\u5883\u53cd\u9988\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\u3002", "result": "\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u5728\u5b9a\u4f4d\u548c3D\u5efa\u56fe\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u7cbe\u786e\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u878d\u5408\u6280\u672f\u5c55\u793a\u4e86\u5728\u590d\u6742\u573a\u666f\u4e2d\u589e\u5f3a\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
