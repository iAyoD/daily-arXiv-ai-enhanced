{"id": "2509.22693", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22693", "abs": "https://arxiv.org/abs/2509.22693", "authors": ["Muhammad Hafil Nugraha", "Fauzi Abdul", "Lastiko Bramantyo", "Estiko Rijanto", "Roni Permana Saputra", "Oka Mahendra"], "title": "Mobile Robot Localization via Indoor Positioning System and Odometry Fusion", "comment": "6 pages, 7 figures,", "summary": "Accurate localization is crucial for effectively operating mobile robots in\nindoor environments. This paper presents a comprehensive approach to mobile\nrobot localization by integrating an ultrasound-based indoor positioning system\n(IPS) with wheel odometry data via sensor fusion techniques. The fusion\nmethodology leverages the strengths of both IPS and wheel odometry,\ncompensating for the individual limitations of each method. The Extended Kalman\nFilter (EKF) fusion method combines the data from the IPS sensors and the\nrobot's wheel odometry, providing a robust and reliable localization solution.\nExtensive experiments in a controlled indoor environment reveal that the\nfusion-based localization system significantly enhances accuracy and precision\ncompared to standalone systems. The results demonstrate significant\nimprovements in trajectory tracking, with the EKF-based approach reducing\nerrors associated with wheel slippage and sensor noise.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u5c06\u8d85\u58f0\u6ce2\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u4e0e\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u6570\u636e\u76f8\u7ed3\u5408\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u6765\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u7684\u7cbe\u786e\u5b9a\u4f4d\u5bf9\u4e8e\u6709\u6548\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u89e3\u51b3\u5355\u72ec\u4f7f\u7528\u8d85\u58f0\u6ce2\u5b9a\u4f4d\u7cfb\u7edf\u6216\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u65f6\u5404\u81ea\u7684\u5c40\u9650\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u878d\u5408\u65b9\u6cd5\uff0c\u5c06IPS\u4f20\u611f\u5668\u6570\u636e\u548c\u673a\u5668\u4eba\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u6570\u636e\u8fdb\u884c\u878d\u5408\uff0c\u5229\u7528\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\u4e92\u8865\u5404\u81ea\u7684\u4e0d\u8db3\u3002", "result": "\u5728\u53d7\u63a7\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u878d\u5408\u5b9a\u4f4d\u7cfb\u7edf\u76f8\u6bd4\u72ec\u7acb\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u51c6\u786e\u6027\uff0cEKF\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u8f6e\u5b50\u6253\u6ed1\u548c\u4f20\u611f\u5668\u566a\u58f0\u76f8\u5173\u7684\u8bef\u5dee\u3002", "conclusion": "\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8d85\u58f0\u6ce2IPS\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u7684\u5ba4\u5185\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22694", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22694", "abs": "https://arxiv.org/abs/2509.22694", "authors": ["Rakha Rahmadani Pratama", "Catur Hilman A. H. B. Baskoro", "Joga Dharma Setiawan", "Dyah Kusuma Dewi", "P Paryanto", "Mochammad Ariyanto", "Roni Permana Saputra"], "title": "Nonlinear Model Predictive Control with Single-Shooting Method for Autonomous Personal Mobility Vehicle", "comment": "15 pages, 3 figures, 4 tables", "summary": "This paper introduces a proposed control method for autonomous personal\nmobility vehicles, specifically the Single-passenger Electric Autonomous\nTransporter (SEATER), using Nonlinear Model Predictive Control (NMPC). The\nproposed method leverages a single-shooting approach to solve the optimal\ncontrol problem (OCP) via non-linear programming (NLP). The proposed NMPC is\nimplemented to a non-holonomic vehicle with a differential drive system, using\nodometry data as localization feedback to guide the vehicle towards its target\npose while achieving objectives and adhering to constraints, such as obstacle\navoidance. To evaluate the performance of the proposed method, a number of\nsimulations have been conducted in both obstacle-free and static obstacle\nenvironments. The SEATER model and testing environment have been developed in\nthe Gazebo Simulation and the NMPC are implemented within the Robot Operating\nSystem (ROS) framework. The simulation results demonstrate that the NMPC-based\napproach successfully controls the vehicle to reach the desired target location\nwhile satisfying the imposed constraints. Furthermore, this study highlights\nthe robustness and real-time effectiveness of NMPC with a single-shooting\napproach for autonomous vehicle control in the evaluated scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(NMPC)\u7684\u5355\u4eba\u7535\u52a8\u81ea\u4e3b\u8fd0\u8f93\u8f66(SEATER)\u63a7\u5236\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u5c04\u51fb\u65b9\u6cd5\u901a\u8fc7\u975e\u7ebf\u6027\u89c4\u5212\u89e3\u51b3\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5728Gazebo\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u81ea\u4e3b\u4e2a\u4eba\u79fb\u52a8\u8f66\u8f86\u5f00\u53d1\u6709\u6548\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\uff08\u5982\u907f\u969c\uff09\u7684\u540c\u65f6\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(NMPC)\u548c\u5355\u5c04\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u89c4\u5212\u89e3\u51b3\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528\u91cc\u7a0b\u8ba1\u6570\u636e\u8fdb\u884c\u5b9a\u4f4d\u53cd\u9988\uff0c\u5728ROS\u6846\u67b6\u4e2d\u5b9e\u73b0\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eNMPC\u7684\u65b9\u6cd5\u6210\u529f\u63a7\u5236\u8f66\u8f86\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\uff0c\u540c\u65f6\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\uff0c\u5728\u65e0\u969c\u788d\u548c\u9759\u6001\u969c\u788d\u73af\u5883\u4e2d\u5747\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86NMPC\u7ed3\u5408\u5355\u5c04\u51fb\u65b9\u6cd5\u5728\u81ea\u4e3b\u8f66\u8f86\u63a7\u5236\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u8bc4\u4f30\u7684\u573a\u666f\u3002"}}
{"id": "2509.22695", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22695", "abs": "https://arxiv.org/abs/2509.22695", "authors": ["Zhitao Wang", "Yanke Wang", "Jiangtao Wen", "Roberto Horowitz", "Yuxing Han"], "title": "ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows", "comment": "This work was submitted to 2026 IEEE International Conference on\n  Robotics & Automation", "summary": "Robotic manipulation in unstructured environments requires the generation of\nrobust and long-horizon trajectory-level policy with conditions of perceptual\nobservations and benefits from the advantages of SE(3)-equivariant diffusion\nmodels that are data-efficient. However, these models suffer from the inference\ntime costs. Inspired by the inference efficiency of rectified flows, we\nintroduce the rectification to the SE(3)-diffusion models and propose the\nReSeFlow, i.e., Rectifying SE(3)-Equivariant Policy Learning Flows, providing\nfast, geodesic-consistent, least-computational policy generation. Crucially,\nboth components employ SE(3)-equivariant networks to preserve rotational and\ntranslational symmetry, enabling robust generalization under rigid-body\nmotions. With the verification on the simulated benchmarks, we find that the\nproposed ReSeFlow with only one inference step can achieve better performance\nwith lower geodesic distance than the baseline methods, achieving up to a 48.5%\nerror reduction on the painting task and a 21.9% reduction on the rotating\ntriangle task compared to the baseline's 100-step inference. This method takes\nadvantages of both SE(3) equivariance and rectified flow and puts it forward\nfor the real-world application of generative policy learning models with the\ndata and inference efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReSeFlow\u65b9\u6cd5\uff0c\u5c06\u6574\u6d41\u6d41\u5f15\u5165SE(3)\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u6d4b\u5730\u7ebf\u4e00\u81f4\u4e14\u8ba1\u7b97\u91cf\u6700\u5c0f\u7684\u7b56\u7565\u751f\u6210\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u751f\u6210\u9c81\u68d2\u7684\u957f\u65f6\u7a0b\u8f68\u8ff9\u7ea7\u7b56\u7565\uff0cSE(3)\u7b49\u53d8\u6269\u6563\u6a21\u578b\u5177\u6709\u6570\u636e\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u65f6\u95f4\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5c06\u6574\u6d41\u6d41\u6280\u672f\u5f15\u5165SE(3)\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51faReSeFlow\u65b9\u6cd5\uff0c\u4f7f\u7528SE(3)\u7b49\u53d8\u7f51\u7edc\u4fdd\u6301\u65cb\u8f6c\u548c\u5e73\u79fb\u5bf9\u79f0\u6027\uff0c\u5b9e\u73b0\u5feb\u901f\u7b56\u7565\u751f\u6210\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReSeFlow\u4ec5\u9700\u4e00\u6b65\u63a8\u7406\u5c31\u80fd\u8fbe\u5230\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5728\u7ed8\u753b\u4efb\u52a1\u4e2d\u8bef\u5dee\u51cf\u5c1148.5%\uff0c\u5728\u65cb\u8f6c\u4e09\u89d2\u5f62\u4efb\u52a1\u4e2d\u51cf\u5c1121.9%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86SE(3)\u7b49\u53d8\u6027\u548c\u6574\u6d41\u6d41\u7684\u4f18\u52bf\uff0c\u4e3a\u751f\u6210\u5f0f\u7b56\u7565\u5b66\u4e60\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6570\u636e\u6548\u7387\u548c\u63a8\u7406\u6548\u7387\u63d0\u4f9b\u4e86\u524d\u8fdb\u65b9\u5411\u3002"}}
{"id": "2509.22698", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22698", "abs": "https://arxiv.org/abs/2509.22698", "authors": ["Hailong Zhang", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments", "comment": "Main paper (15 pages). Accepted for publication by ICONIP(\n  International Conference on Neural Information Processing) 2025", "summary": "Intelligent agents often require collaborative strategies to achieve complex\ntasks beyond individual capabilities in real-world scenarios. While existing\naudio-visual navigation (AVN) research mainly focuses on single-agent systems,\ntheir limitations emerge in dynamic 3D environments where rapid multi-agent\ncoordination is critical, especially for time-sensitive applications like\nemergency response. This paper introduces MASTAVN (Multi-Agent Scalable\nTransformer Audio-Visual Navigation), a scalable framework enabling two agents\nto collaboratively localize and navigate toward an audio target in shared 3D\nenvironments. By integrating cross-agent communication protocols and joint\naudio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal\nsynchronization. Through rigorous evaluation in photorealistic 3D simulators\n(Replica and Matterport3D), MASTAVN achieves significant reductions in task\ncompletion time and notable improvements in navigation success rates compared\nto single-agent and non-collaborative baselines. This highlights the essential\nrole of spatiotemporal coordination in multi-agent systems. Our findings\nvalidate MASTAVN's effectiveness in time-sensitive emergency scenarios and\nestablish a paradigm for advancing scalable multi-agent embodied intelligence\nin complex 3D environments.", "AI": {"tldr": "MASTAVN\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u53ef\u6269\u5c55\u7684\u97f3\u9891-\u89c6\u89c9\u5bfc\u822a\u6846\u67b6\uff0c\u4f7f\u4e24\u4e2a\u4ee3\u7406\u80fd\u591f\u5728\u5171\u4eab\u76843D\u73af\u5883\u4e2d\u534f\u4f5c\u5b9a\u4f4d\u548c\u5bfc\u822a\u5230\u97f3\u9891\u76ee\u6807\uff0c\u901a\u8fc7\u8de8\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\u548c\u8054\u5408\u97f3\u9891-\u89c6\u89c9\u878d\u5408\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u548c\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u5bfc\u822a\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4ee3\u7406\u7cfb\u7edf\uff0c\u5728\u52a8\u60013D\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u7d27\u6025\u54cd\u5e94\u7b49\u65f6\u95f4\u654f\u611f\u5e94\u7528\u4e2d\u9700\u8981\u5feb\u901f\u591a\u4ee3\u7406\u534f\u8c03\u3002", "method": "MASTAVN\u96c6\u6210\u4e86\u8de8\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\u548c\u8054\u5408\u97f3\u9891-\u89c6\u89c9\u878d\u5408\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u7a7a\u95f4\u63a8\u7406\u548c\u65f6\u95f4\u540c\u6b65\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u611f3D\u6a21\u62df\u5668\uff08Replica\u548cMatterport3D\uff09\u4e2d\u7684\u4e25\u683c\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u5355\u4ee3\u7406\u548c\u975e\u534f\u4f5c\u57fa\u7ebf\u76f8\u6bd4\uff0cMASTAVN\u663e\u8457\u51cf\u5c11\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86MASTAVN\u5728\u65f6\u95f4\u654f\u611f\u7d27\u6025\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u63a8\u8fdb\u590d\u67423D\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u591a\u4ee3\u7406\u4f53\u73b0\u667a\u80fd\u5efa\u7acb\u4e86\u8303\u5f0f\u3002"}}
{"id": "2509.22716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22716", "abs": "https://arxiv.org/abs/2509.22716", "authors": ["Hung-Ying Chu", "Guan-Wei Chen", "Shao-Yu Wei", "Yu-Cheng Lin"], "title": "Large Language Models for 3D IC Space Planning", "comment": "Accepted at AICCC 2025", "summary": "Three-dimensional integrated circuits (3D ICs) have emerged as a promising\nsolution to the scaling limits of two-dimensional designs, offering higher\nintegration density, shorter interconnects, and improved performance. As design\ncomplexity increases, effective space planning becomes essential to reduce dead\nspace and ensure layout quality. This study investigates the use of large\nlanguage models (LLMs) for 3D IC space planning through a post-order slicing\ntree representation, which guarantees legal space plans while aiming to\nminimize dead space. Open-source LLMs were fine-tuned on large-scale synthetic\ndatasets and further evaluated on MCNC-derived 3D benchmarks. Experimental\nresults indicate that the proposed framework achieves a favorable balance\nbetween runtime efficiency, legality, and dead-space reduction, with\nzero-dead-space layouts obtained in a significant portion of test cases under\npractical runtime budgets. Beyond synthetic benchmarks, the method generalizes\nto MCNC cases such as ami33 and ami49, though larger and irregular instances\nremain challenging. The approach also shows potential for cross-domain\napplications, including logistics and 3D object placement, where spatial\nefficiency is critical. Overall, the results suggest that LLM-based space\nplanning can serve as a data-driven complement to traditional electronic design\nautomation (EDA) methods, providing new insights for scalable 3D layout\ngeneration.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c3D IC\u7a7a\u95f4\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u5e8f\u5207\u7247\u6811\u8868\u793a\u4fdd\u8bc1\u5408\u6cd5\u7a7a\u95f4\u5e03\u5c40\uff0c\u65e8\u5728\u51cf\u5c11\u6b7b\u7a7a\u95f4\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8fd0\u884c\u6548\u7387\u3001\u5408\u6cd5\u6027\u548c\u6b7b\u7a7a\u95f4\u51cf\u5c11\u65b9\u9762\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u968f\u77403D\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u590d\u6742\u6027\u589e\u52a0\uff0c\u6709\u6548\u7684\u7a7a\u95f4\u89c4\u5212\u5bf9\u4e8e\u51cf\u5c11\u6b7b\u7a7a\u95f4\u548c\u786e\u4fdd\u5e03\u5c40\u8d28\u91cf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfEDA\u65b9\u6cd5\u9762\u4e34\u6269\u5c55\u6027\u6311\u6218\uff0c\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u8865\u5145\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c3D IC\u7a7a\u95f4\u89c4\u5212\uff0c\u91c7\u7528\u540e\u5e8f\u5207\u7247\u6811\u8868\u793a\u6cd5\u4fdd\u8bc1\u5e03\u5c40\u5408\u6cd5\u6027\uff0c\u5728\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5f00\u6e90LLMs\uff0c\u5e76\u5728MCNC\u884d\u751f\u76843D\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8fd0\u884c\u6548\u7387\u3001\u5408\u6cd5\u6027\u548c\u6b7b\u7a7a\u95f4\u51cf\u5c11\u4e4b\u95f4\u8fbe\u5230\u826f\u597d\u5e73\u8861\uff0c\u5728\u5b9e\u7528\u8fd0\u884c\u9884\u7b97\u4e0b\uff0c\u76f8\u5f53\u4e00\u90e8\u5206\u6d4b\u8bd5\u6848\u4f8b\u5b9e\u73b0\u4e86\u96f6\u6b7b\u7a7a\u95f4\u5e03\u5c40\u3002\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230MCNC\u6848\u4f8b\u5982ami33\u548cami49\uff0c\u4f46\u8f83\u5927\u548c\u4e0d\u89c4\u5219\u5b9e\u4f8b\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u7a7a\u95f4\u89c4\u5212\u53ef\u4ee5\u4f5c\u4e3a\u4f20\u7edf\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u6570\u636e\u9a71\u52a8\u8865\u5145\uff0c\u4e3a\u53ef\u6269\u5c55\u76843D\u5e03\u5c40\u751f\u6210\u63d0\u4f9b\u65b0\u89c1\u89e3\uff0c\u5e76\u5177\u6709\u8de8\u9886\u57df\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.22754", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22754", "abs": "https://arxiv.org/abs/2509.22754", "authors": ["Merve Atasever", "Zhuochen Liu", "Qingpei Li", "Akshay Hitendra Shah", "Hans Walker", "Jyotirmoy V. Deshmukh", "Rahul Jain"], "title": "Self-driving cars: Are we there yet?", "comment": null, "summary": "Autonomous driving remains a highly active research domain that seeks to\nenable vehicles to perceive dynamic environments, predict the future\ntrajectories of traffic agents such as vehicles, pedestrians, and cyclists and\nplan safe and efficient future motions. To advance the field, several\ncompetitive platforms and benchmarks have been established to provide\nstandardized datasets and evaluation protocols. Among these, leaderboards by\nthe CARLA organization and nuPlan and the Waymo Open Dataset have become\nleading benchmarks for assessing motion planning algorithms. Each offers a\nunique dataset and challenging planning problems spanning a wide range of\ndriving scenarios and conditions. In this study, we present a comprehensive\ncomparative analysis of the motion planning methods featured on these three\nleaderboards. To ensure a fair and unified evaluation, we adopt CARLA\nleaderboard v2.0 as our common evaluation platform and modify the selected\nmodels for compatibility. By highlighting the strengths and weaknesses of\ncurrent approaches, we identify prevailing trends, common challenges, and\nsuggest potential directions for advancing motion planning research.", "AI": {"tldr": "\u672c\u6587\u5bf9CARLA\u3001nuPlan\u548cWaymo Open Dataset\u4e09\u4e2a\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7b97\u6cd5\u6392\u884c\u699c\u4e2d\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u4f7f\u7528CARLA leaderboard v2.0\u4f5c\u4e3a\u7edf\u4e00\u8bc4\u4f30\u5e73\u53f0\uff0c\u8bc6\u522b\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u9700\u8981\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5e73\u53f0\u6765\u6bd4\u8f83\u4e0d\u540c\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u4f46\u76ee\u524d\u591a\u4e2a\u6392\u884c\u699c\u5404\u6709\u7279\u70b9\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6bd4\u8f83\u6846\u67b6\u3002", "method": "\u91c7\u7528CARLA leaderboard v2.0\u4f5c\u4e3a\u7edf\u4e00\u8bc4\u4f30\u5e73\u53f0\uff0c\u5bf9\u4e09\u4e2a\u6392\u884c\u699c\u4e2d\u7684\u89c4\u5212\u65b9\u6cd5\u8fdb\u884c\u517c\u5bb9\u6027\u4fee\u6539\u540e\u8fdb\u884c\u7efc\u5408\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\uff0c\u8bc6\u522b\u4e86\u4e3b\u6d41\u8d8b\u52bf\u548c\u5171\u540c\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2509.22756", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22756", "abs": "https://arxiv.org/abs/2509.22756", "authors": ["Shiyi Liang", "Xinyuan Chang", "Changjie Wu", "Huiyuan Yan", "Yifan Bai", "Xinran Liu", "Hang Zhang", "Yujian Yuan", "Shuang Zeng", "Mu Xu", "Xing Wei"], "title": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving", "comment": null, "summary": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences.", "AI": {"tldr": "PAMR\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u5f0f\u8054\u5408\u6784\u5efa\u8f66\u9053\u5411\u91cf\u548c\u4ea4\u901a\u89c4\u5219\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5728\u957f\u65f6\u95f4\u9a7e\u9a76\u5e8f\u5217\u4e2d\u4fdd\u6301\u89c4\u5219\u6301\u7eed\u6709\u6548\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u5173\u6ce8\u51e0\u4f55\u5143\u7d20\uff0c\u8981\u4e48\u5c06\u4ea4\u901a\u89c4\u5219\u89c6\u4e3a\u4e34\u65f6\u5206\u7c7b\uff0c\u65e0\u6cd5\u5728\u4ea4\u901a\u6807\u5fd7\u4e0d\u53ef\u89c1\u65f6\u4fdd\u6301\u89c4\u5219\u7684\u6301\u7eed\u6709\u6548\u6027\uff0c\u8fd9\u5bf9\u5b89\u5168\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faPAMR\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1aMap-Rule Co-Construction\u7528\u4e8e\u5904\u7406\u65f6\u95f4\u7247\u6bb5\u4e2d\u7684\u9a7e\u9a76\u573a\u666f\uff0cMap-Rule Cache\u7528\u4e8e\u8de8\u7247\u6bb5\u4fdd\u6301\u89c4\u5219\u4e00\u81f4\u6027\u3002", "result": "\u5728\u8054\u5408\u5411\u91cf-\u89c4\u5219\u6620\u5c04\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u5728\u957f\u65f6\u95f4\u9a7e\u9a76\u5e8f\u5217\u4e2d\u4fdd\u6301\u89c4\u5219\u7684\u6301\u7eed\u6709\u6548\u6027\u3002", "conclusion": "PAMR\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4ea4\u901a\u89c4\u5219\u5728\u957f\u65f6\u95f4\u9a7e\u9a76\u5e8f\u5217\u4e2d\u7684\u6301\u7eed\u6709\u6548\u6027\u95ee\u9898\uff0c\u4e3a\u5b89\u5168\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2509.22801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22801", "abs": "https://arxiv.org/abs/2509.22801", "authors": ["Huajing Zhao", "Brian Flynn", "Adam Norton", "Holly Yanco"], "title": "Towards Developing Standards and Guidelines for Robot Grasping and Manipulation Pipelines in the COMPARE Ecosystem", "comment": "The 3rd AAAI Fall Symposium on Unifying Representations for Robot\n  Application Development (UR-RAD), Arlington, VA, USA, November 2025", "summary": "The COMPARE Ecosystem aims to improve the compatibility and benchmarking of\nopen-source products for robot manipulation through a series of activities. One\nsuch activity is the development of standards and guidelines to specify\nmodularization practices at the component-level for individual modules (e.g.,\nperception, grasp planning, motion planning) and integrations of components\nthat form robot manipulation capabilities at the pipeline-level. This paper\nbriefly reviews our work-in-progress to date to (1) build repositories of\nopen-source products to identify common characteristics of each component in\nthe pipeline, (2) investigate existing modular pipelines to glean best\npractices, and (3) develop new modular pipelines that advance prior work while\nabiding by the proposed standards and guidelines.", "AI": {"tldr": "COMPARE\u751f\u6001\u7cfb\u7edf\u901a\u8fc7\u5f00\u53d1\u7ec4\u4ef6\u7ea7\u548c\u6d41\u6c34\u7ebf\u7ea7\u6807\u51c6\u6307\u5357\uff0c\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u5f00\u6e90\u4ea7\u54c1\u7684\u517c\u5bb9\u6027\u548c\u57fa\u51c6\u6d4b\u8bd5\u80fd\u529b\u3002", "motivation": "\u6539\u5584\u673a\u5668\u4eba\u64cd\u4f5c\u5f00\u6e90\u4ea7\u54c1\u7684\u517c\u5bb9\u6027\u548c\u57fa\u51c6\u6d4b\u8bd5\u80fd\u529b\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6a21\u5757\u5316\u5b9e\u8df5\u6765\u4fc3\u8fdb\u4e0d\u540c\u7ec4\u4ef6\u548c\u6d41\u6c34\u7ebf\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "method": "1) \u6784\u5efa\u5f00\u6e90\u4ea7\u54c1\u5e93\u8bc6\u522b\u6d41\u6c34\u7ebf\u4e2d\u5404\u7ec4\u4ef6\u7684\u5171\u540c\u7279\u5f81\uff1b2) \u7814\u7a76\u73b0\u6709\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\u4ee5\u83b7\u53d6\u6700\u4f73\u5b9e\u8df5\uff1b3) \u5f00\u53d1\u7b26\u5408\u6807\u51c6\u7684\u65b0\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\u3002", "result": "\u6b63\u5728\u8fdb\u884c\u7684\u5de5\u4f5c\u5305\u62ec\u6784\u5efa\u4ea7\u54c1\u5e93\u3001\u5206\u6790\u73b0\u6709\u6d41\u6c34\u7ebf\u5b9e\u8df5\uff0c\u5e76\u5f00\u53d1\u7b26\u5408\u6807\u51c6\u7684\u65b0\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\u3002", "conclusion": "COMPARE\u751f\u6001\u7cfb\u7edf\u901a\u8fc7\u6807\u51c6\u5316\u548c\u6a21\u5757\u5316\u5b9e\u8df5\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u517c\u5bb9\u6027\u548c\u57fa\u51c6\u6d4b\u8bd5\u80fd\u529b\u3002"}}
{"id": "2509.22815", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.22815", "abs": "https://arxiv.org/abs/2509.22815", "authors": ["Ruturaj Sambhus", "Muneeb Ahmad", "Basit Muhammad Imran", "Sujith Vijayan", "Dylan P. Losey", "Kaveh Akbari Hamed"], "title": "Teleoperator-Aware and Safety-Critical Adaptive Nonlinear MPC for Shared Autonomy in Obstacle Avoidance of Legged Robots", "comment": null, "summary": "Ensuring safe and effective collaboration between humans and autonomous\nlegged robots is a fundamental challenge in shared autonomy, particularly for\nteleoperated systems navigating cluttered environments. Conventional\nshared-control approaches often rely on fixed blending strategies that fail to\ncapture the dynamics of legged locomotion and may compromise safety. This paper\npresents a teleoperator-aware, safety-critical, adaptive nonlinear model\npredictive control (ANMPC) framework for shared autonomy of quadrupedal robots\nin obstacle-avoidance tasks. The framework employs a fixed arbitration weight\nbetween human and robot actions but enhances this scheme by modeling the human\ninput with a noisily rational Boltzmann model, whose parameters are adapted\nonline using a projected gradient descent (PGD) law from observed joystick\ncommands. Safety is enforced through control barrier function (CBF) constraints\nintegrated into a computationally efficient NMPC, ensuring forward invariance\nof safe sets despite uncertainty in human behavior. The control architecture is\nhierarchical: a high-level CBF-based ANMPC (10 Hz) generates blended\nhuman-robot velocity references, a mid-level dynamics-aware NMPC (60 Hz)\nenforces reduced-order single rigid body (SRB) dynamics to track these\nreferences, and a low-level nonlinear whole-body controller (500 Hz) imposes\nthe full-order dynamics via quadratic programming to track the mid-level\ntrajectories. Extensive numerical and hardware experiments, together with a\nuser study, on a Unitree Go2 quadrupedal robot validate the framework,\ndemonstrating real-time obstacle avoidance, online learning of human intent\nparameters, and safe teleoperator collaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5171\u4eab\u81ea\u4e3b\u6743\u7684\u81ea\u9002\u5e94\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u4eba\u7c7b\u610f\u56fe\u53c2\u6570\u548c\u96c6\u6210\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7ea6\u675f\uff0c\u5b9e\u73b0\u5b89\u5168\u7684\u4eba\u673a\u534f\u4f5c\u907f\u969c\u3002", "motivation": "\u4f20\u7edf\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7684\u6df7\u5408\u7b56\u7565\uff0c\u65e0\u6cd5\u6355\u6349\u817f\u5f0f\u673a\u5668\u4eba\u7684\u52a8\u6001\u7279\u6027\uff0c\u53ef\u80fd\u5371\u53ca\u5b89\u5168\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u4eba\u7c7b\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u7684\u5b89\u5168\u5173\u952e\u63a7\u5236\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5206\u5c42\u63a7\u5236\u67b6\u6784\uff1a\u9ad8\u5c42CBF-ANMPC\uff0810Hz\uff09\u751f\u6210\u6df7\u5408\u901f\u5ea6\u53c2\u8003\uff0c\u4e2d\u5c42\u52a8\u6001\u611f\u77e5NMPC\uff0860Hz\uff09\u8ddf\u8e2a\u53c2\u8003\uff0c\u4f4e\u5c42\u975e\u7ebf\u6027\u5168\u8eab\u63a7\u5236\u5668\uff08500Hz\uff09\u6267\u884c\u5b8c\u6574\u52a8\u6001\u8ddf\u8e2a\u3002\u4f7f\u7528Boltzmann\u6a21\u578b\u5728\u7ebf\u5b66\u4e60\u4eba\u7c7b\u610f\u56fe\u53c2\u6570\u3002", "result": "\u5728Unitree Go2\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u7684\u6570\u503c\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u907f\u969c\u3001\u5728\u7ebf\u5b66\u4e60\u4eba\u7c7b\u610f\u56fe\u53c2\u6570\u548c\u5b89\u5168\u7684\u4eba\u673a\u534f\u4f5c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u5171\u4eab\u81ea\u4e3b\u6743\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u534f\u4f5c\u6548\u679c\u3002"}}
{"id": "2509.22825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22825", "abs": "https://arxiv.org/abs/2509.22825", "authors": ["Philip Sanderink", "Yingfan Zhou", "Shuzhen Luo", "Cheng Fang"], "title": "Parameter Identification of a Differentiable Human Arm Musculoskeletal Model without Deep Muscle EMG Reconstruction", "comment": "8 pages, 5 figures", "summary": "Accurate parameter identification of a subject-specific human musculoskeletal\nmodel is crucial to the development of safe and reliable physically\ncollaborative robotic systems, for instance, assistive exoskeletons.\nElectromyography (EMG)-based parameter identification methods have demonstrated\npromising performance for personalized musculoskeletal modeling, whereas their\napplicability is limited by the difficulty of measuring deep muscle EMGs\ninvasively. Although several strategies have been proposed to reconstruct deep\nmuscle EMGs or activations for parameter identification, their reliability and\nrobustness are limited by assumptions about the deep muscle behavior. In this\nwork, we proposed an approach to simultaneously identify the bone and\nsuperficial muscle parameters of a human arm musculoskeletal model without\nreconstructing the deep muscle EMGs. This is achieved by only using the\nleast-squares solution of the deep muscle forces to calculate a loss gradient\nwith respect to the model parameters for identifying them in a framework of\ndifferentiable optimization. The results of extensive comparative simulations\nmanifested that our proposed method can achieve comparable estimation accuracy\ncompared to a similar method, but with all the muscle EMGs available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u5efa\u6df1\u5c42\u808c\u8089EMG\u4fe1\u53f7\u5373\u53ef\u540c\u65f6\u8bc6\u522b\u4eba\u4f53\u624b\u81c2\u9aa8\u9abc\u548c\u6d45\u5c42\u808c\u8089\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u6846\u67b6\u5b9e\u73b0\u53c2\u6570\u8bc6\u522b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eEMG\u7684\u4e2a\u6027\u5316\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\u53d7\u9650\u4e8e\u6df1\u5c42\u808c\u8089EMG\u4fe1\u53f7\u96be\u4ee5\u65e0\u521b\u6d4b\u91cf\uff0c\u73b0\u6709\u91cd\u5efa\u65b9\u6cd5\u56e0\u5bf9\u6df1\u5c42\u808c\u8089\u884c\u4e3a\u7684\u5047\u8bbe\u800c\u53ef\u9760\u6027\u6709\u9650\u3002", "method": "\u4f7f\u7528\u6df1\u5c42\u808c\u8089\u529b\u7684\u6700\u5c0f\u4e8c\u4e58\u89e3\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u7684\u635f\u5931\u68af\u5ea6\uff0c\u5728\u53ef\u5fae\u5206\u4f18\u5316\u6846\u67b6\u4e2d\u8bc6\u522b\u53c2\u6570\uff0c\u65e0\u9700\u91cd\u5efa\u6df1\u5c42\u808c\u8089EMG\u4fe1\u53f7\u3002", "result": "\u5e7f\u6cdb\u7684\u5bf9\u6bd4\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u8fbe\u5230\u4e0e\u4f7f\u7528\u6240\u6709\u808c\u8089EMG\u4fe1\u53f7\u7684\u7c7b\u4f3c\u65b9\u6cd5\u76f8\u5f53\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u53c2\u6570\uff0c\u907f\u514d\u4e86\u6df1\u5c42\u808c\u8089EMG\u6d4b\u91cf\u7684\u56f0\u96be\uff0c\u4e3a\u7269\u7406\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2509.22828", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.22828", "abs": "https://arxiv.org/abs/2509.22828", "authors": ["Arman Barghi", "Hamed Hosseini", "Seraj Ghasemi", "Mehdi Tale Masouleh", "Ahmad Kalhor"], "title": "Dynamic Buffers: Cost-Efficient Planning for Tabletop Rearrangement with Stacking", "comment": null, "summary": "Rearranging objects in cluttered tabletop environments remains a\nlong-standing challenge in robotics. Classical planners often generate\ninefficient, high-cost plans by shuffling objects individually and using fixed\nbuffers--temporary spaces such as empty table regions or static stacks--to\nresolve conflicts. When only free table locations are used as buffers, dense\nscenes become inefficient, since placing an object can restrict others from\nreaching their goals and complicate planning. Allowing stacking provides extra\nbuffer capacity, but conventional stacking is static: once an object supports\nanother, the base cannot be moved, which limits efficiency. To overcome these\nissues, a novel planning primitive called the Dynamic Buffer is introduced.\nInspired by human grouping strategies, it enables robots to form temporary,\nmovable stacks that can be transported as a unit. This improves both\nfeasibility and efficiency in dense layouts, and it also reduces travel in\nlarge-scale settings where space is abundant. Compared with a state-of-the-art\nrearrangement planner, the approach reduces manipulator travel cost by 11.89%\nin dense scenarios with a stationary robot and by 5.69% in large, low-density\nsettings with a mobile manipulator. Practicality is validated through\nexperiments on a Delta parallel robot with a two-finger gripper. These findings\nestablish dynamic buffering as a key primitive for cost-efficient and robust\nrearrangement planning.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u7f13\u51b2\u5668\u4f5c\u4e3a\u65b0\u7684\u89c4\u5212\u539f\u8bed\uff0c\u5141\u8bb8\u673a\u5668\u4eba\u5f62\u6210\u4e34\u65f6\u53ef\u79fb\u52a8\u7684\u5806\u53e0\uff0c\u63d0\u9ad8\u5bc6\u96c6\u73af\u5883\u4e0b\u7684\u91cd\u6392\u6548\u7387\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u5668\u5728\u6742\u4e71\u684c\u9762\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u4f7f\u7528\u56fa\u5b9a\u7f13\u51b2\u533a\u5bfc\u81f4\u9ad8\u6210\u672c\u8ba1\u5212\u3002\u9759\u6001\u5806\u53e0\u9650\u5236\u4e86\u6548\u7387\uff0c\u56e0\u4e3a\u4e00\u65e6\u7269\u4f53\u652f\u6491\u53e6\u4e00\u4e2a\uff0c\u57fa\u5ea7\u5c31\u65e0\u6cd5\u79fb\u52a8\u3002", "method": "\u5f15\u5165\u52a8\u6001\u7f13\u51b2\u5668\u89c4\u5212\u539f\u8bed\uff0c\u5141\u8bb8\u5f62\u6210\u4e34\u65f6\u53ef\u79fb\u52a8\u7684\u5806\u53e0\uff0c\u8fd9\u4e9b\u5806\u53e0\u53ef\u4ee5\u4f5c\u4e3a\u6574\u4f53\u8fd0\u8f93\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u5206\u7ec4\u7b56\u7565\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u91cd\u6392\u89c4\u5212\u5668\u76f8\u6bd4\uff0c\u5728\u5bc6\u96c6\u573a\u666f\u4e2d\u51cf\u5c11\u673a\u68b0\u81c2\u79fb\u52a8\u6210\u672c11.89%\uff0c\u5728\u5927\u578b\u4f4e\u5bc6\u5ea6\u8bbe\u7f6e\u4e2d\u51cf\u5c115.69%\u3002\u5728Delta\u5e76\u8054\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u52a8\u6001\u7f13\u51b2\u662f\u6210\u672c\u9ad8\u6548\u548c\u7a33\u5065\u91cd\u6392\u89c4\u5212\u7684\u5173\u952e\u539f\u8bed\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bc6\u96c6\u5e03\u5c40\u7684\u53ef\u884c\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.22847", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22847", "abs": "https://arxiv.org/abs/2509.22847", "authors": ["Brandon Vu", "Shameek Ganguly", "Pushkar Joshi"], "title": "Empart: Interactive Convex Decomposition for Converting Meshes to Parts", "comment": null, "summary": "Simplifying complex 3D meshes is a crucial step in robotics applications to\nenable efficient motion planning and physics simulation. Common methods, such\nas approximate convex decomposition, represent a mesh as a collection of simple\nparts, which are computationally inexpensive to simulate. However, existing\napproaches apply a uniform error tolerance across the entire mesh, which can\nresult in a sub-optimal trade-off between accuracy and performance. For\ninstance, a robot grasping an object needs high-fidelity geometry in the\nvicinity of the contact surfaces but can tolerate a coarser simplification\nelsewhere. A uniform tolerance can lead to excessive detail in non-critical\nareas or insufficient detail where it's needed most.\n  To address this limitation, we introduce Empart, an interactive tool that\nallows users to specify different simplification tolerances for selected\nregions of a mesh. Our method leverages existing convex decomposition\nalgorithms as a sub-routine but uses a novel, parallelized framework to handle\nregion-specific constraints efficiently. Empart provides a user-friendly\ninterface with visual feedback on approximation error and simulation\nperformance, enabling designers to iteratively refine their decomposition. We\ndemonstrate that our approach significantly reduces the number of convex parts\ncompared to a state-of-the-art method (V-HACD) at a fixed error threshold,\nleading to substantial speedups in simulation performance. For a robotic\npick-and-place task, Empart-generated collision meshes reduced the overall\nsimulation time by 69% compared to a uniform decomposition, highlighting the\nvalue of interactive, region-specific simplification for performant robotics\napplications.", "AI": {"tldr": "Empart\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u5141\u8bb8\u7528\u6237\u4e3a\u7f51\u683c\u7684\u4e0d\u540c\u533a\u57df\u6307\u5b9a\u4e0d\u540c\u7684\u7b80\u5316\u5bb9\u5dee\uff0c\u901a\u8fc7\u533a\u57df\u7279\u5b9a\u7684\u7ea6\u675f\u4f18\u5316\u51f8\u5206\u89e3\uff0c\u663e\u8457\u51cf\u5c11\u51f8\u90e8\u5206\u6570\u91cf\u5e76\u63d0\u5347\u6a21\u62df\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6574\u4e2a\u7f51\u683c\u4e0a\u5e94\u7528\u7edf\u4e00\u7684\u8bef\u5dee\u5bb9\u5dee\uff0c\u5bfc\u81f4\u5728\u975e\u5173\u952e\u533a\u57df\u8fc7\u5ea6\u7ec6\u5316\u6216\u5728\u5173\u952e\u533a\u57df\u7ec6\u8282\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5728\u7cbe\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u6700\u4f18\u6743\u8861\u3002", "method": "\u5229\u7528\u73b0\u6709\u51f8\u5206\u89e3\u7b97\u6cd5\u4f5c\u4e3a\u5b50\u7a0b\u5e8f\uff0c\u91c7\u7528\u65b0\u9896\u7684\u5e76\u884c\u5316\u6846\u67b6\u9ad8\u6548\u5904\u7406\u533a\u57df\u7279\u5b9a\u7ea6\u675f\uff0c\u63d0\u4f9b\u5e26\u6709\u89c6\u89c9\u53cd\u9988\u7684\u7528\u6237\u53cb\u597d\u754c\u9762\u3002", "result": "\u5728\u56fa\u5b9a\u8bef\u5dee\u9608\u503c\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5(V-HACD)\u663e\u8457\u51cf\u5c11\u51f8\u90e8\u5206\u6570\u91cf\uff0c\u5728\u673a\u5668\u4eba\u62fe\u653e\u4efb\u52a1\u4e2d\u5c06\u6574\u4f53\u6a21\u62df\u65f6\u95f4\u51cf\u5c1169%\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u3001\u533a\u57df\u7279\u5b9a\u7684\u7b80\u5316\u5bf9\u4e8e\u9ad8\u6027\u80fd\u673a\u5668\u4eba\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u7cbe\u5ea6-\u6027\u80fd\u6743\u8861\u3002"}}
{"id": "2509.22883", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22883", "abs": "https://arxiv.org/abs/2509.22883", "authors": ["Kaleb Ben Naveed", "Haejoon Lee", "Dimitra Panagou"], "title": "Multi-Robot Allocation for Information Gathering in Non-Uniform Spatiotemporal Environments", "comment": "Submitted to American Control Conference (ACC) 2026", "summary": "Autonomous robots are increasingly deployed to estimate spatiotemporal fields\n(e.g., wind, temperature, gas concentration) that vary across space and time.\nWe consider environments divided into non-overlapping regions with distinct\nspatial and temporal dynamics, termed non-uniform spatiotemporal environments.\nGaussian Processes (GPs) can be used to estimate these fields. The GP model\ndepends on a kernel that encodes how the field co-varies in space and time,\nwith its spatial and temporal lengthscales defining the correlation. Hence,\nwhen these lengthscales are incorrect or do not correspond to the actual field,\nthe estimates of uncertainty can be highly inaccurate. Existing GP methods\noften assume one global lengthscale or update only periodically; some allow\nspatial variation but ignore temporal changes. To address these limitations, we\npropose a two-phase framework for multi-robot field estimation. Phase 1 uses a\nvariogram-driven planner to learn region-specific spatial lengthscales. Phase 2\nemploys an allocation strategy that reassigns robots based on the current\nuncertainty, and updates sampling as temporal lengthscales are refined. For\nencoding uncertainty, we utilize clarity, an information metric from our\nearlier work. We evaluate the proposed method across diverse environments and\nprovide convergence analysis for spatial lengthscale estimation, along with\ndynamic regret bounds quantifying the gap to the oracle's allocation sequence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u591a\u673a\u5668\u4eba\u573a\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5dee\u51fd\u6570\u9a71\u52a8\u7684\u89c4\u5212\u5668\u5b66\u4e60\u533a\u57df\u7279\u5b9a\u7684\u7a7a\u95f4\u957f\u5ea6\u5c3a\u5ea6\uff0c\u5e76\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u91cd\u65b0\u5206\u914d\u673a\u5668\u4eba\u6765\u66f4\u65b0\u65f6\u95f4\u957f\u5ea6\u5c3a\u5ea6\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5168\u5c40\u957f\u5ea6\u5c3a\u5ea6\u6216\u4ec5\u5b9a\u671f\u66f4\u65b0\uff0c\u6709\u4e9b\u5141\u8bb8\u7a7a\u95f4\u53d8\u5316\u4f46\u5ffd\u7565\u65f6\u95f4\u53d8\u5316\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u53d8\u5dee\u51fd\u6570\u9a71\u52a8\u7684\u89c4\u5212\u5668\u5b66\u4e60\u533a\u57df\u7279\u5b9a\u7684\u7a7a\u95f4\u957f\u5ea6\u5c3a\u5ea6\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u5206\u914d\u7b56\u7565\uff0c\u57fa\u4e8e\u5f53\u524d\u4e0d\u786e\u5b9a\u6027\u91cd\u65b0\u5206\u914d\u673a\u5668\u4eba\uff0c\u5e76\u5728\u7ec6\u5316\u65f6\u95f4\u957f\u5ea6\u5c3a\u5ea6\u65f6\u66f4\u65b0\u91c7\u6837\u3002", "result": "\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u6240\u63d0\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u7a7a\u95f4\u957f\u5ea6\u5c3a\u5ea6\u4f30\u8ba1\u7684\u6536\u655b\u6027\u5206\u6790\uff0c\u4ee5\u53ca\u91cf\u5316\u4e0eoracle\u5206\u914d\u5e8f\u5217\u5dee\u8ddd\u7684\u52a8\u6001\u9057\u61be\u754c\u9650\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u5747\u5300\u65f6\u7a7a\u73af\u5883\u4e2d\u7684\u573a\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u533a\u57df\u7279\u5b9a\u7684\u957f\u5ea6\u5c3a\u5ea6\u5b66\u4e60\u548c\u52a8\u6001\u673a\u5668\u4eba\u5206\u914d\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2509.22910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22910", "abs": "https://arxiv.org/abs/2509.22910", "authors": ["Yanwei Du", "Jing-Chen Peng", "Patricio A. Vela"], "title": "Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM", "comment": "8 pages, 9 figures, 1 table. Submitted to IEEE Conference", "summary": "Given that Visual SLAM relies on appearance cues for localization and scene\nunderstanding, texture-less or visually degraded environments (e.g., plain\nwalls or low lighting) lead to poor pose estimation and track loss. However,\nrobots are typically equipped with sensors that provide some form of dead\nreckoning odometry with reasonable short-time performance but unreliable\nlong-time performance. The Good Weights (GW) algorithm described here provides\na framework to adaptively integrate dead reckoning (DR) with passive visual\nSLAM for continuous and accurate frame-level pose estimation. Importantly, it\ndescribes how all modules in a comprehensive SLAM system must be modified to\nincorporate DR into its design. Adaptive weighting increases DR influence when\nvisual tracking is unreliable and reduces when visual feature information is\nstrong, maintaining pose track without overreliance on DR. Good Weights yields\na practical solution for mobile navigation that improves visual SLAM\nperformance and robustness. Experiments on collected datasets and in real-world\ndeployment demonstrate the benefits of Good Weights.", "AI": {"tldr": "Good Weights\u7b97\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u822a\u4f4d\u63a8\u7b97\u548c\u89c6\u89c9SLAM\uff0c\u5728\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u63d0\u4f9b\u8fde\u7eed\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1", "motivation": "\u89c6\u89c9SLAM\u5728\u7eb9\u7406\u7f3a\u5931\u6216\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u673a\u5668\u4eba\u901a\u5e38\u914d\u5907\u7684\u822a\u4f4d\u63a8\u7b97\u4f20\u611f\u5668\u77ed\u671f\u6027\u80fd\u826f\u597d\u4f46\u957f\u671f\u4e0d\u53ef\u9760", "method": "\u5f00\u53d1\u81ea\u9002\u5e94\u6743\u91cd\u6846\u67b6\uff0c\u6839\u636e\u89c6\u89c9\u8ddf\u8e2a\u53ef\u9760\u6027\u52a8\u6001\u8c03\u6574\u822a\u4f4d\u63a8\u7b97\u7684\u878d\u5408\u6743\u91cd\uff0c\u5e76\u4fee\u6539SLAM\u7cfb\u7edf\u6240\u6709\u6a21\u5757\u4ee5\u96c6\u6210\u822a\u4f4d\u63a8\u7b97", "result": "\u5728\u6536\u96c6\u7684\u6570\u636e\u96c6\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0cGood Weights\u63d0\u9ad8\u4e86\u89c6\u89c9SLAM\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027", "conclusion": "Good Weights\u4e3a\u79fb\u52a8\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u5728\u89c6\u89c9\u4e0d\u53ef\u9760\u65f6\u4fdd\u6301\u59ff\u6001\u8ddf\u8e2a\u800c\u4e0d\u8fc7\u5ea6\u4f9d\u8d56\u822a\u4f4d\u63a8\u7b97"}}
{"id": "2509.22914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22914", "abs": "https://arxiv.org/abs/2509.22914", "authors": ["Rohan Walia", "Yusheng Wang", "Ralf R\u00f6mer", "Masahiro Nishio", "Angela P. Schoellig", "Jun Ota"], "title": "ARMimic: Learning Robotic Manipulation from Passive Human Demonstrations in Augmented Reality", "comment": null, "summary": "Imitation learning is a powerful paradigm for robot skill acquisition, yet\nconventional demonstration methods--such as kinesthetic teaching and\nteleoperation--are cumbersome, hardware-heavy, and disruptive to workflows.\nRecently, passive observation using extended reality (XR) headsets has shown\npromise for egocentric demonstration collection, yet current approaches require\nadditional hardware, complex calibration, or constrained recording conditions\nthat limit scalability and usability. We present ARMimic, a novel framework\nthat overcomes these limitations with a lightweight and hardware-minimal setup\nfor scalable, robot-free data collection using only a consumer XR headset and a\nstationary workplace camera. ARMimic integrates egocentric hand tracking,\naugmented reality (AR) robot overlays, and real-time depth sensing to ensure\ncollision-aware, kinematically feasible demonstrations. A unified imitation\nlearning pipeline is at the core of our method, treating both human and virtual\nrobot trajectories as interchangeable, which enables policies that generalize\nacross different embodiments and environments. We validate ARMimic on two\nmanipulation tasks, including challenging long-horizon bowl stacking. In our\nexperiments, ARMimic reduces demonstration time by 50% compared to\nteleoperation and improves task success by 11% over ACT, a state-of-the-art\nbaseline trained on teleoperated data. Our results demonstrate that ARMimic\nenables safe, seamless, and in-the-wild data collection, offering great\npotential for scalable robot learning in diverse real-world settings.", "AI": {"tldr": "ARMimic\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u6d88\u8d39\u7ea7XR\u5934\u663e\u548c\u56fa\u5b9a\u5de5\u4f5c\u573a\u6240\u6444\u50cf\u5934\uff0c\u901a\u8fc7\u96c6\u6210\u624b\u90e8\u8ffd\u8e2a\u3001AR\u673a\u5668\u4eba\u53e0\u52a0\u548c\u5b9e\u65f6\u6df1\u5ea6\u611f\u77e5\uff0c\u5b9e\u73b0\u65e0\u673a\u5668\u4eba\u3001\u53ef\u6269\u5c55\u7684\u6f14\u793a\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u6280\u80fd\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u52a8\u89c9\u6559\u5b66\u548c\u9065\u64cd\u4f5c\uff09\u7b28\u91cd\u3001\u786c\u4ef6\u5bc6\u96c6\u4e14\u5e72\u6270\u5de5\u4f5c\u6d41\u7a0b\uff0c\u800c\u73b0\u6709XR\u65b9\u6cd5\u9700\u8981\u989d\u5916\u786c\u4ef6\u3001\u590d\u6742\u6821\u51c6\u6216\u53d7\u9650\u8bb0\u5f55\u6761\u4ef6\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u53ef\u7528\u6027\u3002", "method": "\u96c6\u6210XR\u5934\u663e\u7684\u81ea\u6211\u4e2d\u5fc3\u624b\u90e8\u8ffd\u8e2a\u3001AR\u673a\u5668\u4eba\u53e0\u52a0\u548c\u5b9e\u65f6\u6df1\u5ea6\u611f\u77e5\uff0c\u786e\u4fdd\u78b0\u649e\u611f\u77e5\u548c\u8fd0\u52a8\u5b66\u53ef\u884c\u7684\u6f14\u793a\u3002\u7edf\u4e00\u6a21\u4eff\u5b66\u4e60\u7ba1\u9053\u5c06\u4eba\u7c7b\u548c\u865a\u62df\u673a\u5668\u4eba\u8f68\u8ff9\u89c6\u4e3a\u53ef\u4e92\u6362\u7684\u3002", "result": "\u5728\u4e24\u4e2a\u64cd\u4f5c\u4efb\u52a1\uff08\u5305\u62ec\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u65f6\u57df\u7897\u5806\u53e0\uff09\u4e0a\u9a8c\u8bc1\uff0cARMimic\u76f8\u6bd4\u9065\u64cd\u4f5c\u51cf\u5c1150%\u6f14\u793a\u65f6\u95f4\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebfACT\u63d0\u9ad811%\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "ARMimic\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u65e0\u7f1d\u7684\u91ce\u5916\u6570\u636e\u6536\u96c6\uff0c\u4e3a\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.22937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22937", "abs": "https://arxiv.org/abs/2509.22937", "authors": ["Trent Weiss", "Amar Kulkarni", "Madhur Behl"], "title": "DBF-MA: A Differential Bayesian Filtering Planner for Multi-Agent Autonomous Racing Overtakes", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A significant challenge in autonomous racing is to generate overtaking\nmaneuvers. Racing agents must execute these maneuvers on complex racetracks\nwith little room for error. Optimization techniques and graph-based methods\nhave been proposed, but these methods often rely on oversimplified assumptions\nfor collision-avoidance and dynamic constraints. In this work, we present an\napproach to trajectory synthesis based on an extension of the Differential\nBayesian Filtering framework. Our approach for collision-free trajectory\nsynthesis frames the problem as one of Bayesian Inference over the space of\nComposite Bezier Curves. Our method is derivative-free, does not require a\nspherical approximation of the vehicle footprint, linearization of constraints,\nor simplifying upper bounds on collision avoidance. We conduct a closed-loop\nanalysis of DBF-MA and find it successfully overtakes an opponent in 87% of\ntested scenarios, outperforming existing methods in autonomous overtaking.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u5c55\u5dee\u5206\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6846\u67b6\u7684\u8f68\u8ff9\u5408\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u4e3b\u8d5b\u8f66\u8d85\u8f66\u573a\u666f\uff0c\u5c06\u95ee\u9898\u6784\u5efa\u4e3a\u5728\u590d\u5408\u8d1d\u9f50\u5c14\u66f2\u7ebf\u7a7a\u95f4\u4e0a\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u95ee\u9898", "motivation": "\u81ea\u4e3b\u8d5b\u8f66\u8d85\u8f66\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u4f18\u5316\u6280\u672f\u548c\u56fe\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u8fc7\u5ea6\u7b80\u5316\u7684\u78b0\u649e\u907f\u514d\u548c\u52a8\u6001\u7ea6\u675f\u5047\u8bbe", "method": "\u91c7\u7528\u5dee\u5206\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6846\u67b6\uff0c\u5c06\u65e0\u78b0\u649e\u8f68\u8ff9\u5408\u6210\u95ee\u9898\u6784\u5efa\u4e3a\u590d\u5408\u8d1d\u9f50\u5c14\u66f2\u7ebf\u7a7a\u95f4\u4e0a\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u95ee\u9898\uff0c\u65e0\u9700\u5bfc\u6570\u3001\u7403\u5f62\u8f66\u8f86\u8db3\u8ff9\u8fd1\u4f3c\u3001\u7ea6\u675f\u7ebf\u6027\u5316\u6216\u7b80\u5316\u78b0\u649e\u907f\u514d\u4e0a\u754c", "result": "\u95ed\u73af\u5206\u6790\u663e\u793aDBF-MA\u65b9\u6cd5\u572887%\u7684\u6d4b\u8bd5\u573a\u666f\u4e2d\u6210\u529f\u8d85\u8f66\uff0c\u5728\u81ea\u4e3b\u8d85\u8f66\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u8d5b\u9053\u4e0a\u7684\u81ea\u4e3b\u8d85\u8f66\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8f68\u8ff9\u5408\u6210\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.22955", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.22955", "abs": "https://arxiv.org/abs/2509.22955", "authors": ["Pietro Bruschi"], "title": "Hierarchical Control Design for Space Robots with Application to In-Orbit Servicing Missions", "comment": null, "summary": "In-Orbit Servicing and Active Debris Removal require advanced robotic\ncapabilities for capturing and detumbling uncooperative targets. This work\npresents a hierarchical control framework for autonomous robotic capture of\ntumbling objects in space. A simulation environment is developed, incorporating\nsloshing dynamics of the chaser, a rarely studied effect in space robotics. The\nproposed controller combines an inner Lyapunov-based robust control loop for\nmulti-body dynamics with an outer loop addressing an extended inverse\nkinematics problem. Simulation results show improved robustness and\nadaptability compared to existing control schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u6355\u83b7\u7a7a\u95f4\u7ffb\u6eda\u7269\u4f53\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u674e\u96c5\u666e\u8bfa\u592b\u9c81\u68d2\u63a7\u5236\u548c\u6269\u5c55\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u6c42\u89e3\u3002", "motivation": "\u5728\u8f68\u670d\u52a1\u548c\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u9700\u8981\u5148\u8fdb\u7684\u673a\u5668\u4eba\u80fd\u529b\u6765\u6355\u83b7\u548c\u7a33\u5b9a\u975e\u5408\u4f5c\u76ee\u6807\uff0c\u7279\u522b\u662f\u8003\u8651\u7a7a\u95f4\u673a\u5668\u4eba\u4e2d\u5f88\u5c11\u7814\u7a76\u7684\u71c3\u6599\u6643\u52a8\u52a8\u529b\u5b66\u6548\u5e94\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u71c3\u6599\u6643\u52a8\u52a8\u529b\u5b66\u7684\u4eff\u771f\u73af\u5883\uff0c\u63d0\u51fa\u4e86\u5206\u5c42\u63a7\u5236\u5668\uff1a\u5185\u73af\u4f7f\u7528\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u7684\u9c81\u68d2\u63a7\u5236\u5904\u7406\u591a\u4f53\u52a8\u529b\u5b66\uff0c\u5916\u73af\u89e3\u51b3\u6269\u5c55\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u63a7\u5236\u65b9\u6848\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u5206\u5c42\u63a7\u5236\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u7a7a\u95f4\u673a\u5668\u4eba\u6355\u83b7\u7ffb\u6eda\u7269\u4f53\u65f6\u7684\u590d\u6742\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u7279\u522b\u662f\u71c3\u6599\u6643\u52a8\u6548\u5e94\uff0c\u4e3a\u5728\u8f68\u670d\u52a1\u548c\u788e\u7247\u6e05\u9664\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22970", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22970", "abs": "https://arxiv.org/abs/2509.22970", "authors": ["Siheng Zhao", "Jiageng Mao", "Wei Chow", "Zeyu Shangguan", "Tianheng Shi", "Rong Xue", "Yuxi Zheng", "Yijia Weng", "Yang You", "Daniel Seita", "Leonidas Guibas", "Sergey Zakharov", "Vitor Guizilini", "Yue Wang"], "title": "Robot Learning from Any Images", "comment": "CoRL 2025 camera ready", "summary": "We introduce RoLA, a framework that transforms any in-the-wild image into an\ninteractive, physics-enabled robotic environment. Unlike previous methods, RoLA\noperates directly on a single image without requiring additional hardware or\ndigital assets. Our framework democratizes robotic data generation by producing\nmassive visuomotor robotic demonstrations within minutes from a wide range of\nimage sources, including camera captures, robotic datasets, and Internet\nimages. At its core, our approach combines a novel method for single-view\nphysical scene recovery with an efficient visual blending strategy for\nphotorealistic data collection. We demonstrate RoLA's versatility across\napplications like scalable robotic data generation and augmentation, robot\nlearning from Internet images, and single-image real-to-sim-to-real systems for\nmanipulators and humanoids. Video results are available at\nhttps://sihengz02.github.io/RoLA .", "AI": {"tldr": "RoLA\u6846\u67b6\u53ef\u5c06\u4efb\u610f\u56fe\u50cf\u8f6c\u6362\u4e3a\u652f\u6301\u7269\u7406\u4ea4\u4e92\u7684\u673a\u5668\u4eba\u73af\u5883\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u6216\u6570\u5b57\u8d44\u4ea7\uff0c\u5b9e\u73b0\u5feb\u901f\u7684\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u673a\u5668\u4eba\u6570\u636e\u751f\u6210\u65b9\u6cd5\u9700\u8981\u989d\u5916\u786c\u4ef6\u6216\u6570\u5b57\u8d44\u4ea7\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u5355\u4e00\u56fe\u50cf\u76f4\u63a5\u751f\u6210\u4ea4\u4e92\u5f0f\u7269\u7406\u73af\u5883\u7684\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u5355\u89c6\u89d2\u7269\u7406\u573a\u666f\u6062\u590d\u65b9\u6cd5\u548c\u9ad8\u6548\u89c6\u89c9\u878d\u5408\u7b56\u7565\uff0c\u4ece\u76f8\u673a\u62cd\u6444\u3001\u673a\u5668\u4eba\u6570\u636e\u96c6\u548c\u7f51\u7edc\u56fe\u50cf\u7b49\u6765\u6e90\u751f\u6210\u903c\u771f\u6570\u636e\u3002", "result": "\u5c55\u793a\u4e86\u5728\u53ef\u6269\u5c55\u673a\u5668\u4eba\u6570\u636e\u751f\u6210\u3001\u4ece\u7f51\u7edc\u56fe\u50cf\u5b66\u4e60\u673a\u5668\u4eba\u6280\u80fd\u4ee5\u53ca\u5355\u56fe\u50cf\u5b9e-\u4eff-\u5b9e\u7cfb\u7edf\u7b49\u591a\u4e2a\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "RoLA\u6846\u67b6\u80fd\u591f\u6c11\u4e3b\u5316\u673a\u5668\u4eba\u6570\u636e\u751f\u6210\uff0c\u5728\u51e0\u5206\u949f\u5185\u4ece\u5404\u79cd\u56fe\u50cf\u6e90\u4ea7\u751f\u5927\u91cf\u89c6\u89c9\u8fd0\u52a8\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u3002"}}
{"id": "2509.22976", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.22976", "abs": "https://arxiv.org/abs/2509.22976", "authors": ["Rounak Bhattacharya", "Vrithik R. Guthikonda", "Ashwin P. Dani"], "title": "Safe Task Space Synchronization with Time-Delayed Information", "comment": null, "summary": "In this paper, an adaptive controller is designed for the synchronization of\nthe trajectory of a robot with unknown kinematics and dynamics to that of the\ncurrent human trajectory in the task space using the delayed human trajectory\ninformation. The communication time delay may be a result of various factors\nthat arise in human-robot collaboration tasks, such as sensor processing or\nfusion to estimate trajectory/intent, network delays, or computational\nlimitations. The developed adaptive controller uses Barrier Lyapunov Function\n(BLF) to constrain the Cartesian coordinates of the robot to ensure safety, an\nICL-based adaptive law to account for the unknown kinematics, and a\ngradient-based adaptive law to estimate unknown dynamics. Barrier\nLyapunov-Krasovskii (LK) functionals are used for the stability analysis to\nshow that the synchronization and parameter estimation errors remain\nsemi-globally uniformly ultimately bounded (SGUUB). The simulation results\nbased on a human-robot synchronization scenario with time delay are provided to\ndemonstrate the effectiveness of the designed synchronization controller with\nsafety constraints.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u901a\u4fe1\u65f6\u5ef6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u673a\u5668\u4eba\u8f68\u8ff9\u4e0e\u4eba\u7c7b\u8f68\u8ff9\u7684\u540c\u6b65\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u7ea6\u675f\u3002", "motivation": "\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\u4efb\u52a1\u4e2d\uff0c\u901a\u4fe1\u65f6\u5ef6\uff08\u5982\u4f20\u611f\u5668\u5904\u7406\u3001\u7f51\u7edc\u5ef6\u8fdf\u7b49\uff09\u4f1a\u5f71\u54cd\u8f68\u8ff9\u540c\u6b65\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u672a\u77e5\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u4e14\u4fdd\u8bc1\u5b89\u5168\u7684\u81ea\u9002\u5e94\u63a7\u5236\u5668\u3002", "method": "\u4f7f\u7528\u969c\u788dLyapunov\u51fd\u6570(BLF)\u7ea6\u675f\u7b1b\u5361\u5c14\u5750\u6807\u786e\u4fdd\u5b89\u5168\uff0c\u91c7\u7528ICL\u81ea\u9002\u5e94\u5f8b\u5904\u7406\u672a\u77e5\u8fd0\u52a8\u5b66\uff0c\u68af\u5ea6\u81ea\u9002\u5e94\u5f8b\u4f30\u8ba1\u672a\u77e5\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u969c\u788dLyapunov-Krasovskii\u51fd\u6570\u8fdb\u884c\u7a33\u5b9a\u6027\u5206\u6790\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8bbe\u8ba1\u7684\u540c\u6b65\u63a7\u5236\u5668\u5728\u5b58\u5728\u65f6\u5ef6\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5b9e\u73b0\u4e86\u8f68\u8ff9\u540c\u6b65\uff0c\u540c\u65f6\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u63a7\u5236\u5668\u80fd\u591f\u5904\u7406\u901a\u4fe1\u65f6\u5ef6\u3001\u672a\u77e5\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\uff0c\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u540c\u6b65\u573a\u666f\u4e2d\u5b9e\u73b0\u5b89\u5168\u6709\u6548\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3002"}}
{"id": "2509.23021", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23021", "abs": "https://arxiv.org/abs/2509.23021", "authors": ["Xiao Hu", "Qi Yin", "Yangming Shi", "Yang Ye"], "title": "UniPrototype: Humn-Robot Skill Learning with Uniform Prototypes", "comment": null, "summary": "Data scarcity remains a fundamental challenge in robot learning. While human\ndemonstrations benefit from abundant motion capture data and vast internet\nresources, robotic manipulation suffers from limited training examples. To\nbridge this gap between human and robot manipulation capabilities, we propose\nUniPrototype, a novel framework that enables effective knowledge transfer from\nhuman to robot domains via shared motion primitives. ur approach makes three\nkey contributions: (1) We introduce a compositional prototype discovery\nmechanism with soft assignments, enabling multiple primitives to co-activate\nand thus capture blended and hierarchical skills; (2) We propose an adaptive\nprototype selection strategy that automatically adjusts the number of\nprototypes to match task complexity, ensuring scalable and efficient\nrepresentation; (3) We demonstrate the effectiveness of our method through\nextensive experiments in both simulation environments and real-world robotic\nsystems. Our results show that UniPrototype successfully transfers human\nmanipulation knowledge to robots, significantly improving learning efficiency\nand task performance compared to existing approaches.The code and dataset will\nbe released upon acceptance at an anonymous repository.", "AI": {"tldr": "UniPrototype\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u8fd0\u52a8\u57fa\u5143\u5b9e\u73b0\u4ece\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u9886\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u800c\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u4e30\u5bcc\u3002\u4e3a\u4e86\u5f25\u5408\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUniPrototype\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a1\uff09\u5177\u6709\u8f6f\u5206\u914d\u7684\u590d\u5408\u539f\u578b\u53d1\u73b0\u673a\u5236\uff0c\u5141\u8bb8\u591a\u4e2a\u57fa\u5143\u5171\u540c\u6fc0\u6d3b\u4ee5\u6355\u6349\u6df7\u5408\u548c\u5206\u5c42\u6280\u80fd\uff1b2\uff09\u81ea\u9002\u5e94\u539f\u578b\u9009\u62e9\u7b56\u7565\uff0c\u81ea\u52a8\u8c03\u6574\u539f\u578b\u6570\u91cf\u4ee5\u5339\u914d\u4efb\u52a1\u590d\u6742\u6027\uff1b3\uff09\u5728\u4eff\u771f\u73af\u5883\u548c\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "UniPrototype\u6210\u529f\u5c06\u4eba\u7c7b\u64cd\u4f5c\u77e5\u8bc6\u8fc1\u79fb\u5230\u673a\u5668\u4eba\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u901a\u8fc7\u4eba\u7c7b\u77e5\u8bc6\u8fc1\u79fb\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2509.23048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23048", "abs": "https://arxiv.org/abs/2509.23048", "authors": ["Chang Liu", "Badrinath Balasubramaniam", "Neal Yancey", "Michael Severson", "Adam Shine", "Philip Bove", "Beiwen Li", "Xiao Liang", "Minghui Zheng"], "title": "RAISE: A Robot-Assisted Selective Disassembly and Sorting System for End-of-Life Phones", "comment": null, "summary": "End-of-Life (EoL) phones significantly exacerbate global e-waste challenges\ndue to their high production volumes and short lifecycles. Disassembly is among\nthe most critical processes in EoL phone recycling. However, it relies heavily\non human labor due to product variability. Consequently, the manual process is\nboth labor-intensive and time-consuming. In this paper, we propose a low-cost,\neasily deployable automated and selective disassembly and sorting system for\nEoL phones, consisting of three subsystems: an adaptive cutting system, a\nvision-based robotic sorting system, and a battery removal system. The system\ncan process over 120 phones per hour with an average disassembly success rate\nof 98.9%, efficiently delivering selected high-value components to downstream\nprocessing. It provides a reliable and scalable automated solution to the\npressing challenge of EoL phone disassembly. Additionally, the automated system\ncan enhance disassembly economics, converting a previously unprofitable process\ninto one that yields a net profit per unit weight of EoL phones.", "AI": {"tldr": "\u63d0\u51fa\u4f4e\u6210\u672c\u3001\u6613\u90e8\u7f72\u7684\u81ea\u52a8\u5316\u9009\u62e9\u6027\u62c6\u89e3\u7cfb\u7edf\uff0c\u7528\u4e8e\u5e9f\u5f03\u624b\u673a\u56de\u6536\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5207\u5272\u3001\u89c6\u89c9\u673a\u5668\u4eba\u5206\u62e3\u548c\u7535\u6c60\u79fb\u9664\u4e09\u4e2a\u5b50\u7cfb\u7edf\uff0c\u6bcf\u5c0f\u65f6\u53ef\u5904\u7406120+\u90e8\u624b\u673a\uff0c\u6210\u529f\u738798.9%\u3002", "motivation": "\u5e9f\u5f03\u624b\u673a\u56e0\u4ea7\u91cf\u9ad8\u3001\u751f\u547d\u5468\u671f\u77ed\u800c\u52a0\u5267\u5168\u7403\u7535\u5b50\u5783\u573e\u95ee\u9898\uff0c\u4eba\u5de5\u62c6\u89e3\u52b3\u52a8\u5bc6\u96c6\u4e14\u8017\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u5305\u542b\u81ea\u9002\u5e94\u5207\u5272\u7cfb\u7edf\u3001\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u548c\u7535\u6c60\u79fb\u9664\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u62c6\u89e3\u7cfb\u7edf\u3002", "result": "\u7cfb\u7edf\u6bcf\u5c0f\u65f6\u5904\u7406\u8d85\u8fc7120\u90e8\u624b\u673a\uff0c\u5e73\u5747\u62c6\u89e3\u6210\u529f\u738798.9%\uff0c\u80fd\u9ad8\u6548\u5206\u79bb\u9ad8\u4ef7\u503c\u7ec4\u4ef6\u4f9b\u4e0b\u6e38\u5904\u7406\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5e9f\u5f03\u624b\u673a\u62c6\u89e3\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u5c06\u539f\u672c\u65e0\u5229\u53ef\u56fe\u7684\u62c6\u89e3\u8fc7\u7a0b\u8f6c\u53d8\u4e3a\u4ea7\u751f\u51c0\u5229\u6da6\u7684\u8fc7\u7a0b\u3002"}}
{"id": "2509.23075", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23075", "abs": "https://arxiv.org/abs/2509.23075", "authors": ["Soofiyan Atar", "Daniel Huang", "Florian Richter", "Michael Yip"], "title": "In-Hand Manipulation of Articulated Tools with Dexterous Robot Hands with Sim-to-Real Transfer", "comment": null, "summary": "Reinforcement learning (RL) and sim-to-real transfer have advanced robotic\nmanipulation of rigid objects. Yet, policies remain brittle when applied to\narticulated mechanisms due to contact-rich dynamics and under-modeled joint\nphenomena such as friction, stiction, backlash, and clearances. We address this\nchallenge through dexterous in-hand manipulation of articulated tools using a\nrobotic hand with reduced articulation and kinematic redundancy relative to the\nhuman hand. Our controller augments a simulation-trained base policy with a\nsensor-driven refinement learned from hardware demonstrations, conditioning on\nproprioception and target articulation states while fusing whole-hand tactile\nand force feedback with the policy's internal action intent via\ncross-attention-based integration. This design enables online adaptation to\ninstance-specific articulation properties, stabilizes contact interactions,\nregulates internal forces, and coordinates coupled-link motion under\nperturbations. We validate our approach across a diversity of real-world\nexamples, including scissors, pliers, minimally invasive surgical tools, and\nstaplers. We achieve robust transfer from simulation to hardware, improved\ndisturbance resilience, and generalization to previously unseen articulated\ntools, thereby reducing reliance on precise physical modeling in contact-rich\nsettings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eff\u771f\u8bad\u7ec3\u548c\u786c\u4ef6\u6f14\u793a\u7684\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u624b\u5bf9\u94f0\u63a5\u5de5\u5177\u7684\u7075\u5de7\u64cd\u4f5c\uff0c\u901a\u8fc7\u89e6\u89c9\u548c\u529b\u53cd\u9988\u7684\u8de8\u6ce8\u610f\u529b\u96c6\u6210\u5b9e\u73b0\u5728\u7ebf\u9002\u5e94\u548c\u7a33\u5b9a\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u94f0\u63a5\u673a\u5236\u64cd\u4f5c\u4e2d\u7531\u4e8e\u63a5\u89e6\u52a8\u529b\u5b66\u590d\u6742\u548c\u5173\u8282\u73b0\u8c61\uff08\u5982\u6469\u64e6\u3001\u7c98\u6ede\u3001\u56de\u9699\u7b49\uff09\u5efa\u6a21\u4e0d\u8db3\u5bfc\u81f4\u7684\u7b56\u7565\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "\u5728\u4eff\u771f\u8bad\u7ec3\u7684\u57fa\u7840\u7b56\u7565\u4e0a\uff0c\u901a\u8fc7\u786c\u4ef6\u6f14\u793a\u5b66\u4e60\u4f20\u611f\u5668\u9a71\u52a8\u7684\u7ec6\u5316\u7b56\u7565\uff0c\u878d\u5408\u672c\u4f53\u611f\u77e5\u3001\u76ee\u6807\u5173\u8282\u72b6\u6001\u4ee5\u53ca\u5168\u624b\u89e6\u89c9\u548c\u529b\u53cd\u9988\uff0c\u4f7f\u7528\u8de8\u6ce8\u610f\u529b\u673a\u5236\u96c6\u6210\u7b56\u7565\u5185\u90e8\u52a8\u4f5c\u610f\u56fe\u3002", "result": "\u5728\u526a\u5200\u3001\u94b3\u5b50\u3001\u5fae\u521b\u624b\u672f\u5de5\u5177\u548c\u8ba2\u4e66\u673a\u7b49\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u786c\u4ef6\u7684\u9c81\u68d2\u8fc1\u79fb\u3001\u6539\u8fdb\u7684\u6297\u5e72\u6270\u80fd\u529b\u548c\u5bf9\u672a\u89c1\u94f0\u63a5\u5de5\u5177\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5728\u63a5\u89e6\u4e30\u5bcc\u573a\u666f\u4e2d\u5bf9\u7cbe\u786e\u7269\u7406\u5efa\u6a21\u7684\u4f9d\u8d56\uff0c\u80fd\u591f\u9002\u5e94\u7279\u5b9a\u5b9e\u4f8b\u7684\u5173\u8282\u7279\u6027\uff0c\u7a33\u5b9a\u63a5\u89e6\u4ea4\u4e92\uff0c\u8c03\u8282\u5185\u529b\uff0c\u5e76\u5728\u6270\u52a8\u4e0b\u534f\u8c03\u8026\u5408\u8fde\u6746\u8fd0\u52a8\u3002"}}
{"id": "2509.23107", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23107", "abs": "https://arxiv.org/abs/2509.23107", "authors": ["Yi Wang", "Zeyu Xue", "Mujie Liu", "Tongqin Zhang", "Yan Hu", "Zhou Zhao", "Chenguang Yang", "Zhenyu Lu"], "title": "Open-Vocabulary Spatio-Temporal Scene Graph for Robot Perception and Teleoperation Planning", "comment": null, "summary": "Teleoperation via natural-language reduces operator workload and enhances\nsafety in high-risk or remote settings. However, in dynamic remote scenes,\ntransmission latency during bidirectional communication creates gaps between\nremote perceived states and operator intent, leading to command\nmisunderstanding and incorrect execution. To mitigate this, we introduce the\nSpatio-Temporal Open-Vocabulary Scene Graph (ST-OVSG), a representation that\nenriches open-vocabulary perception with temporal dynamics and lightweight\nlatency annotations. ST-OVSG leverages LVLMs to construct open-vocabulary 3D\nobject representations, and extends them into the temporal domain via Hungarian\nassignment with our temporal matching cost, yielding a unified spatio-temporal\nscene graph. A latency tag is embedded to enable LVLM planners to\nretrospectively query past scene states, thereby resolving local-remote state\nmismatches caused by transmission delays. To further reduce redundancy and\nhighlight task-relevant cues, we propose a task-oriented subgraph filtering\nstrategy that produces compact inputs for the planner. ST-OVSG generalizes to\nnovel categories and enhances planning robustness against transmission latency\nwithout requiring fine-tuning. Experiments show that our method achieves 74\npercent node accuracy on the Replica benchmark, outperforming ConceptGraph.\nNotably, in the latency-robustness experiment, the LVLM planner assisted by\nST-OVSG achieved a planning success rate of 70.5 percent.", "AI": {"tldr": "\u63d0\u51fa\u4e86ST-OVSG\uff08\u65f6\u7a7a\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u56fe\uff09\u6765\u89e3\u51b3\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u4f20\u8f93\u5ef6\u8fdf\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u573a\u666f\u56fe\u4e2d\u5d4c\u5165\u65f6\u95f4\u52a8\u6001\u548c\u5ef6\u8fdf\u6807\u6ce8\uff0c\u5e2e\u52a9\u8bed\u8a00\u89c6\u89c9\u6a21\u578b\u89c4\u5212\u5668\u66f4\u51c6\u786e\u5730\u7406\u89e3\u8fdc\u7a0b\u573a\u666f\u72b6\u6001\u3002", "motivation": "\u5728\u52a8\u6001\u8fdc\u7a0b\u573a\u666f\u4e2d\uff0c\u53cc\u5411\u901a\u4fe1\u7684\u4f20\u8f93\u5ef6\u8fdf\u4f1a\u5bfc\u81f4\u8fdc\u7a0b\u611f\u77e5\u72b6\u6001\u4e0e\u64cd\u4f5c\u8005\u610f\u56fe\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9020\u6210\u547d\u4ee4\u8bef\u89e3\u548c\u6267\u884c\u9519\u8bef\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7f13\u89e3\u5ef6\u8fdf\u5f71\u54cd\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8bed\u8a00\u89c6\u89c9\u6a21\u578b\u6784\u5efa\u5f00\u653e\u8bcd\u6c473D\u7269\u4f53\u8868\u793a\uff0c\u901a\u8fc7\u5308\u7259\u5229\u5206\u914d\u548c\u65f6\u5e8f\u5339\u914d\u6210\u672c\u6269\u5c55\u5230\u65f6\u95f4\u57df\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u65f6\u7a7a\u573a\u666f\u56fe\u3002\u5d4c\u5165\u5ef6\u8fdf\u6807\u7b7e\u4f7f\u89c4\u5212\u5668\u80fd\u591f\u56de\u6eaf\u67e5\u8be2\u8fc7\u53bb\u573a\u666f\u72b6\u6001\uff0c\u5e76\u91c7\u7528\u4efb\u52a1\u5bfc\u5411\u5b50\u56fe\u8fc7\u6ee4\u7b56\u7565\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5728Replica\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523074%\u7684\u8282\u70b9\u51c6\u786e\u7387\uff0c\u4f18\u4e8eConceptGraph\u3002\u5728\u5ef6\u8fdf\u9c81\u68d2\u6027\u5b9e\u9a8c\u4e2d\uff0cST-OVSG\u8f85\u52a9\u7684\u8bed\u8a00\u89c6\u89c9\u6a21\u578b\u89c4\u5212\u5668\u5b9e\u73b0\u4e8670.5%\u7684\u89c4\u5212\u6210\u529f\u7387\u3002", "conclusion": "ST-OVSG\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u7c7b\u522b\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u589e\u5f3a\u89c4\u5212\u5bf9\u4f20\u8f93\u5ef6\u8fdf\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u72b6\u6001\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2509.23111", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23111", "abs": "https://arxiv.org/abs/2509.23111", "authors": ["Chen Yizhe", "Wang Qi", "Hu Dongxiao", "Jingzhe Fang", "Liu Sichao", "Zixin An", "Hongliang Niu", "Haoran Liu", "Li Dong", "Chuanfen Feng", "Lan Dapeng", "Liu Yu", "Zhibo Pang"], "title": "Liaohe-CobotMagic-PnP: an Imitation Learning Dataset of Intelligent Robot for Industrial Applications", "comment": "Accepted to IAI 2025 (International Conference on Industrial\n  Artificial Intelligence), Shenyang, China, Aug 21 - 24, 2025. Preprint\n  (before IEEE copyright transfer)", "summary": "In Industry 4.0 applications, dynamic environmental interference induces\nhighly nonlinear and strongly coupled interactions between the environmental\nstate and robotic behavior. Effectively representing dynamic environmental\nstates through multimodal sensor data fusion remains a critical challenge in\ncurrent robotic datasets. To address this, an industrial-grade multimodal\ninterference dataset is presented, designed for robotic perception and control\nunder complex conditions. The dataset integrates multi-dimensional interference\nfeatures including size, color, and lighting variations, and employs\nhigh-precision sensors to synchronously collect visual, torque, and joint-state\nmeasurements. Scenarios with geometric similarity exceeding 85\\% and\nstandardized lighting gradients are included to ensure real-world\nrepresentativeness. Microsecond-level time-synchronization and\nvibration-resistant data acquisition protocols, implemented via the Robot\nOperating System (ROS), guarantee temporal and operational fidelity.\nExperimental results demonstrate that the dataset enhances model validation\nrobustness and improves robotic operational stability in dynamic,\ninterference-rich environments. The dataset is publicly available\nat:https://modelscope.cn/datasets/Liaoh_LAB/Liaohe-CobotMagic-PnP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5de5\u4e1a\u7ea7\u591a\u6a21\u6001\u5e72\u6270\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u548c\u63a7\u5236\uff0c\u5305\u542b\u89c6\u89c9\u3001\u626d\u77e9\u548c\u5173\u8282\u72b6\u6001\u7684\u591a\u7ef4\u5e72\u6270\u7279\u5f81\u540c\u6b65\u91c7\u96c6\u3002", "motivation": "\u5de5\u4e1a4.0\u5e94\u7528\u4e2d\uff0c\u52a8\u6001\u73af\u5883\u5e72\u6270\u5bfc\u81f4\u73af\u5883\u72b6\u6001\u4e0e\u673a\u5668\u4eba\u884c\u4e3a\u4e4b\u95f4\u4ea7\u751f\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u5f3a\u8026\u5408\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5f53\u524d\u673a\u5668\u4eba\u6570\u636e\u96c6\u96be\u4ee5\u6709\u6548\u8868\u793a\u52a8\u6001\u73af\u5883\u72b6\u6001\u3002", "method": "\u96c6\u6210\u5c3a\u5bf8\u3001\u989c\u8272\u548c\u5149\u7167\u53d8\u5316\u7b49\u591a\u7ef4\u5e72\u6270\u7279\u5f81\uff0c\u91c7\u7528\u9ad8\u7cbe\u5ea6\u4f20\u611f\u5668\u540c\u6b65\u91c7\u96c6\u89c6\u89c9\u3001\u626d\u77e9\u548c\u5173\u8282\u72b6\u6001\u6d4b\u91cf\u6570\u636e\uff0c\u901a\u8fc7ROS\u5b9e\u73b0\u5fae\u79d2\u7ea7\u65f6\u95f4\u540c\u6b65\u548c\u6297\u632f\u52a8\u6570\u636e\u91c7\u96c6\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6570\u636e\u96c6\u589e\u5f3a\u4e86\u6a21\u578b\u9a8c\u8bc1\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u3001\u5e72\u6270\u4e30\u5bcc\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u611f\u77e5\u548c\u63a7\u5236\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2509.23112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23112", "abs": "https://arxiv.org/abs/2509.23112", "authors": ["Ryo Watanabe", "Maxime Alvarez", "Pablo Ferreiro", "Pavel Savkin", "Genki Sano"], "title": "FTACT: Force Torque aware Action Chunking Transformer for Pick-and-Reorient Bottle Task", "comment": null, "summary": "Manipulator robots are increasingly being deployed in retail environments,\nyet contact rich edge cases still trigger costly human teleoperation. A\nprominent example is upright lying beverage bottles, where purely visual cues\nare often insufficient to resolve subtle contact events required for precise\nmanipulation. We present a multimodal Imitation Learning policy that augments\nthe Action Chunking Transformer with force and torque sensing, enabling\nend-to-end learning over images, joint states, and forces and torques. Deployed\non Ghost, single-arm platform by Telexistence Inc, our approach improves\nPick-and-Reorient bottle task by detecting and exploiting contact transitions\nduring pressing and placement. Hardware experiments demonstrate greater task\nsuccess compared to baseline matching the observation space of ACT as an\nablation and experiments indicate that force and torque signals are beneficial\nin the press and place phases where visual observability is limited, supporting\nthe use of interaction forces as a complementary modality for contact rich\nskills. The results suggest a practical path to scaling retail manipulation by\ncombining modern imitation learning architectures with lightweight force and\ntorque sensing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u529b/\u626d\u77e9\u4f20\u611f\u4e0eAction Chunking Transformer\u7ed3\u5408\uff0c\u5728\u96f6\u552e\u73af\u5883\u4e2d\u6539\u8fdb\u673a\u5668\u4eba\u5bf9\u76f4\u7acb\u996e\u6599\u74f6\u7684\u62fe\u53d6\u548c\u91cd\u5b9a\u5411\u4efb\u52a1\u3002", "motivation": "\u96f6\u552e\u73af\u5883\u4e2d\u7684\u673a\u68b0\u81c2\u673a\u5668\u4eba\u7ecf\u5e38\u9700\u8981\u4eba\u5de5\u8fdc\u7a0b\u64cd\u4f5c\u6765\u5904\u7406\u63a5\u89e6\u4e30\u5bcc\u7684\u8fb9\u7f18\u60c5\u51b5\uff0c\u7279\u522b\u662f\u76f4\u7acb\u996e\u6599\u74f6\u64cd\u4f5c\u4e2d\uff0c\u4ec5\u9760\u89c6\u89c9\u7ebf\u7d22\u5f80\u5f80\u4e0d\u8db3\u4ee5\u89e3\u51b3\u7cbe\u786e\u64cd\u4f5c\u6240\u9700\u7684\u5fae\u5999\u63a5\u89e6\u4e8b\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u5c06\u529b/\u626d\u77e9\u4f20\u611f\u96c6\u6210\u5230Action Chunking Transformer\u4e2d\uff0c\u5b9e\u73b0\u56fe\u50cf\u3001\u5173\u8282\u72b6\u6001\u3001\u529b\u548c\u626d\u77e9\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4ec5\u5339\u914dACT\u89c2\u6d4b\u7a7a\u95f4\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u62fe\u53d6\u548c\u91cd\u5b9a\u5411\u74f6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u529b/\u626d\u77e9\u4fe1\u53f7\u5728\u89c6\u89c9\u53ef\u89c2\u6d4b\u6027\u6709\u9650\u7684\u6309\u538b\u548c\u653e\u7f6e\u9636\u6bb5\u7279\u522b\u6709\u76ca\u3002", "conclusion": "\u901a\u8fc7\u5c06\u73b0\u4ee3\u6a21\u4eff\u5b66\u4e60\u67b6\u6784\u4e0e\u8f7b\u91cf\u7ea7\u529b/\u626d\u77e9\u4f20\u611f\u76f8\u7ed3\u5408\uff0c\u4e3a\u6269\u5c55\u96f6\u552e\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u652f\u6301\u5c06\u4ea4\u4e92\u529b\u4f5c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u6280\u80fd\u7684\u8865\u5145\u6a21\u6001\u3002"}}
{"id": "2509.23118", "categories": ["cs.RO", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23118", "abs": "https://arxiv.org/abs/2509.23118", "authors": ["Zeyi Li", "Zhe Tang", "Kyeong Soo Kim", "Sihao Li", "Jeremy S. Smith"], "title": "EKF-Based Fusion of Wi-Fi/LiDAR/IMU for Indoor Localization and Navigation", "comment": "8 pages, 7 figures, 3 tables, and submitted for presentation at a\n  conference", "summary": "Conventional Wi-Fi received signal strength indicator (RSSI) fingerprinting\ncannot meet the growing demand for accurate indoor localization and navigation\ndue to its lower accuracy, while solutions based on light detection and ranging\n(LiDAR) can provide better localization performance but is limited by their\nhigher deployment cost and complexity. To address these issues, we propose a\nnovel indoor localization and navigation framework integrating Wi-Fi RSSI\nfingerprinting, LiDAR-based simultaneous localization and mapping (SLAM), and\ninertial measurement unit (IMU) navigation based on an extended Kalman filter\n(EKF). Specifically, coarse localization by deep neural network (DNN)-based\nWi-Fi RSSI fingerprinting is refined by IMU-based dynamic positioning using a\nGmapping-based SLAM to generate an occupancy grid map and output high-frequency\nattitude estimates, which is followed by EKF prediction-update integrating\nsensor information while effectively suppressing Wi-Fi-induced noise and IMU\ndrift errors. Multi-group real-world experiments conducted on the IR building\nat Xi'an Jiaotong-Liverpool University demonstrates that the proposed\nmulti-sensor fusion framework suppresses the instability caused by individual\napproaches and thereby provides stable accuracy across all path configurations\nwith mean two-dimensional (2D) errors ranging from 0.2449 m to 0.3781 m. In\ncontrast, the mean 2D errors of Wi-Fi RSSI fingerprinting reach up to 1.3404 m\nin areas with severe signal interference, and those of LiDAR/IMU localization\nare between 0.6233 m and 2.8803 m due to cumulative drift.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408Wi-Fi RSSI\u6307\u7eb9\u8bc6\u522b\u3001LiDAR SLAM\u548cIMU\u7684\u591a\u4f20\u611f\u5668\u5ba4\u5185\u5b9a\u4f4d\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7EKF\u878d\u5408\u6291\u5236Wi-Fi\u566a\u58f0\u548cIMU\u6f02\u79fb\uff0c\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "motivation": "\u4f20\u7edfWi-Fi RSSI\u6307\u7eb9\u5b9a\u4f4d\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800cLiDAR\u65b9\u6848\u6210\u672c\u9ad8\u4e14\u590d\u6742\uff0c\u9700\u8981\u4e00\u79cd\u878d\u5408\u591a\u4f20\u611f\u5668\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e73\u8861\u7cbe\u5ea6\u548c\u6210\u672c\u3002", "method": "\u4f7f\u7528DNN\u8fdb\u884cWi-Fi RSSI\u7c97\u5b9a\u4f4d\uff0c\u7ed3\u5408IMU\u52a8\u6001\u5b9a\u4f4d\u548cGmapping SLAM\u751f\u6210\u5730\u56fe\uff0c\u901a\u8fc7EKF\u9884\u6d4b-\u66f4\u65b0\u878d\u5408\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u6291\u5236Wi-Fi\u566a\u58f0\u548cIMU\u6f02\u79fb\u8bef\u5dee\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u76842D\u5e73\u5747\u8bef\u5dee\u4e3a0.2449-0.3781\u7c73\uff0c\u800cWi-Fi RSSI\u5728\u4fe1\u53f7\u5e72\u6270\u533a\u57df\u8bef\u5dee\u8fbe1.3404\u7c73\uff0cLiDAR/IMU\u56e0\u7d2f\u79ef\u6f02\u79fb\u8bef\u5dee\u4e3a0.6233-2.8803\u7c73\u3002", "conclusion": "\u591a\u4f20\u611f\u5668\u878d\u5408\u6846\u67b6\u80fd\u6709\u6548\u6291\u5236\u5355\u4e00\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5728\u6240\u6709\u8def\u5f84\u914d\u7f6e\u4e0b\u63d0\u4f9b\u7a33\u5b9a\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u5355\u4e00\u4f20\u611f\u5668\u65b9\u6848\u3002"}}
{"id": "2509.23155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23155", "abs": "https://arxiv.org/abs/2509.23155", "authors": ["Abdul Monaf Chowdhury", "Akm Moshiur Rahman Mazumder", "Rabeya Akter", "Safaeid Hossain Arib"], "title": "LAGEA: Language Guided Embodied Agents for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation benefits from foundation models that describe goals, but\ntoday's agents still lack a principled way to learn from their own mistakes. We\nask whether natural language can serve as feedback, an error reasoning signal\nthat helps embodied agents diagnose what went wrong and correct course. We\nintroduce LAGEA (Language Guided Embodied Agents), a framework that turns\nepisodic, schema-constrained reflections from a vision language model (VLM)\ninto temporally grounded guidance for reinforcement learning. LAGEA summarizes\neach attempt in concise language, localizes the decisive moments in the\ntrajectory, aligns feedback with visual state in a shared representation, and\nconverts goal progress and feedback agreement into bounded, step-wise shaping\nrewardswhose influence is modulated by an adaptive, failure-aware coefficient.\nThis design yields dense signals early when exploration needs direction and\ngracefully recedes as competence grows. On the Meta-World MT10 embodied\nmanipulation benchmark, LAGEA improves average success over the\nstate-of-the-art (SOTA) methods by 9.0% on random goals and 5.3% on fixed\ngoals, while converging faster. These results support our hypothesis: language,\nwhen structured and grounded in time, is an effective mechanism for teaching\nrobots to self-reflect on mistakes and make better choices. Code will be\nreleased soon.", "AI": {"tldr": "LAGEA\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u8a00\u53cd\u9988\uff0c\u5c06\u9519\u8bef\u53cd\u601d\u8f6c\u5316\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u65f6\u5e8f\u5f15\u5bfc\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5956\u52b1\u7cfb\u6570\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u81ea\u6211\u53cd\u601d\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u4ee3\u7406\u7f3a\u4e4f\u4ece\u81ea\u8eab\u9519\u8bef\u4e2d\u5b66\u4e60\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7814\u7a76\u63a2\u7d22\u81ea\u7136\u8bed\u8a00\u662f\u5426\u80fd\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u5e2e\u52a9\u5177\u8eab\u667a\u80fd\u4f53\u8bca\u65ad\u9519\u8bef\u5e76\u7ea0\u6b63\u884c\u4e3a\u3002", "method": "LAGEA\u6846\u67b6\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u601d\u603b\u7ed3\u4e3a\u7b80\u6d01\u8bed\u8a00\uff0c\u5b9a\u4f4d\u8f68\u8ff9\u4e2d\u7684\u5173\u952e\u65f6\u523b\uff0c\u5728\u5171\u4eab\u8868\u793a\u4e2d\u5bf9\u9f50\u53cd\u9988\u4e0e\u89c6\u89c9\u72b6\u6001\uff0c\u5e76\u5c06\u76ee\u6807\u8fdb\u5c55\u548c\u53cd\u9988\u4e00\u81f4\u6027\u8f6c\u5316\u4e3a\u6709\u754c\u7684\u9010\u6b65\u5851\u5f62\u5956\u52b1\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5931\u8d25\u611f\u77e5\u7cfb\u6570\u8c03\u8282\u5f71\u54cd\u3002", "result": "\u5728Meta-World MT10\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAGEA\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u968f\u673a\u76ee\u6807\u4e0a\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad89.0%\uff0c\u5728\u56fa\u5b9a\u76ee\u6807\u4e0a\u63d0\u9ad85.3%\uff0c\u4e14\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\uff1a\u5f53\u8bed\u8a00\u88ab\u7ed3\u6784\u5316\u5e76\u5728\u65f6\u95f4\u4e0a\u63a5\u5730\u65f6\uff0c\u662f\u6559\u5bfc\u673a\u5668\u4eba\u81ea\u6211\u53cd\u601d\u9519\u8bef\u5e76\u505a\u51fa\u66f4\u597d\u9009\u62e9\u7684\u6709\u6548\u673a\u5236\u3002"}}
{"id": "2509.23185", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23185", "abs": "https://arxiv.org/abs/2509.23185", "authors": ["Ziyi Zhou", "Qian Meng", "Hadas Kress-Gazit", "Ye Zhao"], "title": "Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion", "comment": null, "summary": "We present an integrated planning framework for quadrupedal locomotion over\ndynamically changing, unforeseen terrains. Existing methods often depend on\nheuristics for real-time foothold selection-limiting robustness and\nadaptability-or rely on computationally intensive trajectory optimization\nacross complex terrains and long horizons. In contrast, our approach combines\nreactive synthesis for generating correct-by-construction symbolic-level\ncontrollers with mixed-integer convex programming (MICP) for dynamic and\nphysically feasible footstep planning during each symbolic transition. To\nreduce the reliance on costly MICP solves and accommodate specifications that\nmay be violated due to physical infeasibility, we adopt a symbolic repair\nmechanism that selectively generates only the required symbolic transitions.\nDuring execution, real-time MICP replanning based on actual terrain data,\ncombined with runtime symbolic repair and delay-aware coordination, enables\nseamless bridging between offline synthesis and online operation. Through\nextensive simulation and hardware experiments, we validate the framework's\nability to identify missing locomotion skills and respond effectively in\nsafety-critical environments, including scattered stepping stones and rebar\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u56db\u8db3\u673a\u5668\u4eba\u52a8\u6001\u5730\u5f62\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u53cd\u5e94\u5f0f\u5408\u6210\u548c\u6df7\u5408\u6574\u6570\u51f8\u89c4\u5212\uff0c\u901a\u8fc7\u7b26\u53f7\u4fee\u590d\u673a\u5236\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5b9e\u73b0\u79bb\u7ebf\u5408\u6210\u4e0e\u5728\u7ebf\u64cd\u4f5c\u7684\u5e73\u6ed1\u8854\u63a5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u5b9e\u65f6\u7acb\u8db3\u70b9\u9009\u62e9\uff08\u9650\u5236\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff09\u6216\u8ba1\u7b97\u5bc6\u96c6\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u52a8\u6001\u5730\u5f62\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u53cd\u5e94\u5f0f\u5408\u6210\u751f\u6210\u6784\u9020\u6b63\u786e\u7684\u7b26\u53f7\u7ea7\u63a7\u5236\u5668\uff0c\u4f7f\u7528\u6df7\u5408\u6574\u6570\u51f8\u89c4\u5212\u8fdb\u884c\u52a8\u6001\u7269\u7406\u53ef\u884c\u6b65\u6001\u89c4\u5212\uff0c\u91c7\u7528\u7b26\u53f7\u4fee\u590d\u673a\u5236\u51cf\u5c11MICP\u6c42\u89e3\u6b21\u6570\uff0c\u5b9e\u65f6MICP\u91cd\u89c4\u5212\u7ed3\u5408\u8fd0\u884c\u65f6\u7b26\u53f7\u4fee\u590d\u548c\u5ef6\u8fdf\u611f\u77e5\u534f\u8c03\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u7f3a\u5931\u7684\u8fd0\u52a8\u6280\u80fd\uff0c\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u6709\u6548\u54cd\u5e94\uff0c\u5305\u62ec\u5206\u6563\u8e0f\u811a\u77f3\u548c\u94a2\u7b4b\u573a\u666f\u3002", "conclusion": "\u8be5\u96c6\u6210\u89c4\u5212\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u53d8\u5316\u3001\u672a\u77e5\u5730\u5f62\u4e0a\u7684\u56db\u8db3\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u79bb\u7ebf\u5408\u6210\u4e0e\u5728\u7ebf\u6267\u884c\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.23203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23203", "abs": "https://arxiv.org/abs/2509.23203", "authors": ["Kai Yang", "Tianlin Zhang", "Zhengbo Wang", "Zedong Chu", "Xiaolong Wu", "Yang Cai", "Mu Xu"], "title": "CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation", "comment": null, "summary": "Generalizing local navigation policies across diverse robot morphologies is a\ncritical challenge. Progress is often hindered by the need for costly and\nembodiment-specific data, the tight coupling of planning and control, and the\n\"disastrous averaging\" problem where deterministic models fail to capture\nmulti-modal decisions (e.g., turning left or right). We introduce CE-Nav, a\nnovel two-stage (IL-then-RL) framework that systematically decouples universal\ngeometric reasoning from embodiment-specific dynamic adaptation. First, we\ntrain an embodiment-agnostic General Expert offline using imitation learning.\nThis expert, a conditional normalizing flow model named VelFlow, learns the\nfull distribution of kinematically-sound actions from a large-scale dataset\ngenerated by a classical planner, completely avoiding real robot data and\nresolving the multi-modality issue. Second, for a new robot, we freeze the\nexpert and use it as a guiding prior to train a lightweight, Dynamics-Aware\nRefiner via online reinforcement learning. This refiner rapidly learns to\ncompensate for the target robot's specific dynamics and controller\nimperfections with minimal environmental interaction. Extensive experiments on\nquadrupeds, bipeds, and quadrotors show that CE-Nav achieves state-of-the-art\nperformance while drastically reducing adaptation cost. Successful real-world\ndeployments further validate our approach as an efficient and scalable solution\nfor building generalizable navigation systems.", "AI": {"tldr": "CE-Nav\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u63a8\u7406\u548c\u52a8\u6001\u9002\u5e94\uff0c\u5b9e\u73b0\u8de8\u673a\u5668\u4eba\u5f62\u6001\u7684\u901a\u7528\u5bfc\u822a\u7b56\u7565\uff0c\u5927\u5e45\u964d\u4f4e\u9002\u5e94\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u8de8\u673a\u5668\u4eba\u5f62\u6001\u5bfc\u822a\u7684\u6311\u6218\uff1a\u9700\u8981\u6602\u8d35\u7684\u7279\u5b9a\u5f62\u6001\u6570\u636e\u3001\u89c4\u5212\u4e0e\u63a7\u5236\u7d27\u5bc6\u8026\u5408\u3001\u4ee5\u53ca\u786e\u5b9a\u6027\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u591a\u6a21\u6001\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u79bb\u7ebf\u8bad\u7ec3\u901a\u7528\u4e13\u5bb6\uff08VelFlow\u6a21\u578b\uff09\u5b66\u4e60\u8fd0\u52a8\u5b66\u53ef\u884c\u52a8\u4f5c\u5206\u5e03\uff1b2) \u5728\u7ebf\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u52a8\u6001\u611f\u77e5\u7cbe\u70bc\u5668\uff0c\u9488\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u52a8\u6001\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u56db\u8db3\u3001\u53cc\u8db3\u548c\u56db\u65cb\u7ffc\u673a\u5668\u4eba\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cCE-Nav\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9002\u5e94\u6210\u672c\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "CE-Nav\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u7684\u901a\u7528\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u591a\u6a21\u6001\u51b3\u7b56\u548c\u8de8\u5f62\u6001\u9002\u5e94\u95ee\u9898\u3002"}}
{"id": "2509.23214", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23214", "abs": "https://arxiv.org/abs/2509.23214", "authors": ["Benjamin Wong", "Aaron Weber", "Mohamed M. Safwat", "Santosh Devasia", "Ashis G. Banerjee"], "title": "Simulated Annealing for Multi-Robot Ergodic Information Acquisition Using Graph-Based Discretization", "comment": null, "summary": "One of the goals of active information acquisition using multi-robot teams is\nto keep the relative uncertainty in each region at the same level to maintain\nidentical acquisition quality (e.g., consistent target detection) in all the\nregions. To achieve this goal, ergodic coverage can be used to assign the\nnumber of samples according to the quality of observation, i.e., sampling noise\nlevels. However, the noise levels are unknown to the robots. Although this\nnoise can be estimated from samples, the estimates are unreliable at first and\ncan generate fluctuating values. The main contribution of this paper is to use\nsimulated annealing to generate the target sampling distribution, starting from\nuniform and gradually shifting to an estimated optimal distribution, by varying\nthe coldness parameter of a Boltzmann distribution with the estimated sampling\nentropy as energy. Simulation results show a substantial improvement of both\ntransient and asymptotic entropy compared to both uniform and direct-ergodic\nsearches. Finally, a demonstration is performed with a TurtleBot swarm system\nto validate the physical applicability of the algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6a21\u62df\u9000\u706b\u65b9\u6cd5\u4e3a\u591a\u673a\u5668\u4eba\u56e2\u961f\u751f\u6210\u76ee\u6807\u91c7\u6837\u5206\u5e03\uff0c\u4ece\u5747\u5300\u5206\u5e03\u5f00\u59cb\uff0c\u9010\u6e10\u8f6c\u5411\u4f30\u8ba1\u7684\u6700\u4f18\u5206\u5e03\uff0c\u4ee5\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5b9e\u73b0\u4e00\u81f4\u7684\u91c7\u96c6\u8d28\u91cf\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u4e3b\u52a8\u4fe1\u606f\u91c7\u96c6\u7684\u76ee\u6807\u662f\u4fdd\u6301\u5404\u533a\u57df\u76f8\u5bf9\u4e0d\u786e\u5b9a\u6027\u5728\u76f8\u540c\u6c34\u5e73\uff0c\u4ee5\u786e\u4fdd\u6240\u6709\u533a\u57df\u5177\u6709\u4e00\u81f4\u7684\u91c7\u96c6\u8d28\u91cf\u3002\u7136\u800c\uff0c\u89c2\u6d4b\u566a\u58f0\u6c34\u5e73\u672a\u77e5\u4e14\u521d\u59cb\u4f30\u8ba1\u4e0d\u53ef\u9760\uff0c\u4f1a\u5bfc\u81f4\u6ce2\u52a8\u503c\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u9000\u706b\u751f\u6210\u76ee\u6807\u91c7\u6837\u5206\u5e03\uff0c\u4ece\u5747\u5300\u5206\u5e03\u5f00\u59cb\uff0c\u901a\u8fc7\u53d8\u5316\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u7684\u51b7\u5ea6\u53c2\u6570\uff0c\u4ee5\u4f30\u8ba1\u7684\u91c7\u6837\u71b5\u4f5c\u4e3a\u80fd\u91cf\uff0c\u9010\u6e10\u8f6c\u5411\u4f30\u8ba1\u7684\u6700\u4f18\u5206\u5e03\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u5747\u5300\u641c\u7d22\u548c\u76f4\u63a5\u904d\u5386\u641c\u7d22\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u77ac\u65f6\u71b5\u548c\u6e10\u8fd1\u71b5\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u5584\u3002\u5728TurtleBot\u7fa4\u7cfb\u7edf\u4e0a\u7684\u6f14\u793a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u7269\u7406\u9002\u7528\u6027\u3002", "conclusion": "\u6a21\u62df\u9000\u706b\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u672a\u77e5\u89c2\u6d4b\u566a\u58f0\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u4fe1\u606f\u91c7\u96c6\u8d28\u91cf\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2509.23220", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23220", "abs": "https://arxiv.org/abs/2509.23220", "authors": ["Ye Chen", "Zichen Zhou", "Jianyu Dou", "Te Cui", "Yi Yang", "Yufeng Yue"], "title": "GLUE: Global-Local Unified Encoding for Imitation Learning via Key-Patch Tracking", "comment": "8 pages, 5 figures", "summary": "In recent years, visual representation learning has gained widespread\nattention in robotic imitation learning. However, in complex\nOut-of-Distribution(OOD) settings characterized by clutter and occlusion, the\nattention of global visual representations can be diluted or interfered,\nleading to degraded policy performance. The invariance of local representations\nfor task-relevant objects offers a solution. By efficiently utilizing these\nlocal representations, training and testing data can be mapped to a more\nsimilar feature space, thereby mitigating the covariate shift problem.\nAccordingly, we propose GLUE, a global-local unified encoding framework for\nimitation learning based on key-patch tracking. GLUE selects and tracks\nkey-patches as critical local representations by employing a text-guided\nmechanism. It features a novel fusion framework where global patch features\nquery local patches to distill essential information, yielding fine-grained\nlocal features with low heterogeneity relative to the global context. This\nfused representation steers the robot's visual attention toward task-relevant\nobjects and preserves precise global context, which together align the training\nand testing distributions into a similar and task-informative feature space,\nultimately enhancing the robustness of the imitation learning policy.\nExperiments demonstrate that GLUE achieves strong performance across diverse\ntasks in both simulation and real-world settings, outperforming the strongest\nbaseline by 17.6% in simulation, 36.3% in real-world environments, and 58.3% on\nreal-world generalization settings. The project website of GLUE is available at\nhttps://GLUE666.github.io/.", "AI": {"tldr": "GLUE\u662f\u4e00\u4e2a\u57fa\u4e8e\u5173\u952e\u8865\u4e01\u8ddf\u8e2a\u7684\u5168\u5c40-\u5c40\u90e8\u7edf\u4e00\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u673a\u5236\u9009\u62e9\u5e76\u8ddf\u8e2a\u5173\u952e\u8865\u4e01\u4f5c\u4e3a\u5c40\u90e8\u8868\u793a\uff0c\u89e3\u51b3\u590d\u6742OOD\u73af\u5883\u4e0b\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u7a00\u91ca\u95ee\u9898\u3002", "motivation": "\u5728\u590d\u6742OOD\u8bbe\u7f6e\u4e2d\uff0c\u5168\u5c40\u89c6\u89c9\u8868\u793a\u7684\u6ce8\u610f\u529b\u53ef\u80fd\u88ab\u7a00\u91ca\u6216\u5e72\u6270\uff0c\u5bfc\u81f4\u7b56\u7565\u6027\u80fd\u4e0b\u964d\u3002\u5c40\u90e8\u8868\u793a\u5bf9\u4efb\u52a1\u76f8\u5173\u5bf9\u8c61\u7684\u4e0d\u53d8\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "method": "GLUE\u91c7\u7528\u6587\u672c\u5f15\u5bfc\u673a\u5236\u9009\u62e9\u5173\u952e\u8865\u4e01\uff0c\u901a\u8fc7\u5168\u5c40\u8865\u4e01\u7279\u5f81\u67e5\u8be2\u5c40\u90e8\u8865\u4e01\u6765\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff0c\u751f\u6210\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u5f02\u8d28\u6027\u4f4e\u7684\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGLUE\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5728\u4eff\u771f\u4e2d\u63d0\u534717.6%\uff0c\u771f\u5b9e\u73af\u5883\u4e2d\u63d0\u534736.3%\uff0c\u771f\u5b9e\u4e16\u754c\u6cdb\u5316\u8bbe\u7f6e\u4e2d\u63d0\u534758.3%\u3002", "conclusion": "GLUE\u901a\u8fc7\u878d\u5408\u8868\u793a\u5f15\u5bfc\u673a\u5668\u4eba\u89c6\u89c9\u6ce8\u610f\u529b\u5230\u4efb\u52a1\u76f8\u5173\u5bf9\u8c61\uff0c\u540c\u65f6\u4fdd\u7559\u7cbe\u786e\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5c06\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u5bf9\u9f50\u5230\u76f8\u4f3c\u4e14\u4efb\u52a1\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u589e\u5f3a\u4e86\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.23223", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23223", "abs": "https://arxiv.org/abs/2509.23223", "authors": ["Aoqian Zhang", "Zixuan Zhuang", "Chunzheng Wang", "Shuzhi Sam Ge", "Fan Shi", "Cheng Xiang"], "title": "SAC-Loco: Safe and Adjustable Compliant Quadrupedal Locomotion", "comment": null, "summary": "Quadruped robots are designed to achieve agile locomotion by mimicking legged\nanimals. However, existing control methods for quadrupeds often lack one of the\nkey capabilities observed in animals: adaptive and adjustable compliance in\nresponse to external disturbances. Most locomotion controllers do not provide\ntunable compliance and tend to fail under large perturbations. In this work, we\npropose a switched policy framework for compliant and safe quadruped\nlocomotion. First, we train a force compliant policy with adjustable compliance\nlevels using a teacher student reinforcement learning framework, eliminating\nthe need for explicit force sensing. Next, we develop a safe policy based on\nthe capture point concept to stabilize the robot when the compliant policy\nfails. Finally, we introduce a recoverability network that predicts the\nlikelihood of failure and switches between the compliant and safe policies.\nTogether, this framework enables quadruped robots to achieve both force\ncompliance and robust safety when subjected to severe external disturbances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5207\u6362\u7b56\u7565\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u7684\u67d4\u987a\u548c\u5b89\u5168\u8fd0\u52a8\u63a7\u5236\uff0c\u7ed3\u5408\u4e86\u53ef\u8c03\u67d4\u987a\u5ea6\u7684\u529b\u67d4\u987a\u7b56\u7565\u548c\u57fa\u4e8e\u6355\u83b7\u70b9\u7684\u5b89\u5168\u7b56\u7565\uff0c\u901a\u8fc7\u53ef\u6062\u590d\u6027\u7f51\u7edc\u8fdb\u884c\u7b56\u7565\u5207\u6362\u3002", "motivation": "\u73b0\u6709\u56db\u8db3\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u7269\u6240\u5177\u5907\u7684\u81ea\u9002\u5e94\u548c\u53ef\u8c03\u8282\u67d4\u987a\u6027\uff0c\u5927\u591a\u6570\u8fd0\u52a8\u63a7\u5236\u5668\u65e0\u6cd5\u63d0\u4f9b\u53ef\u8c03\u67d4\u987a\u5ea6\uff0c\u5728\u5927\u6270\u52a8\u4e0b\u5bb9\u6613\u5931\u6548\u3002", "method": "\u4f7f\u7528\u5e08\u751f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u5177\u6709\u53ef\u8c03\u67d4\u987a\u5ea6\u7684\u529b\u67d4\u987a\u7b56\u7565\uff08\u65e0\u9700\u663e\u5f0f\u529b\u611f\u77e5\uff09\uff0c\u5f00\u53d1\u57fa\u4e8e\u6355\u83b7\u70b9\u6982\u5ff5\u7684\u5b89\u5168\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u53ef\u6062\u590d\u6027\u7f51\u7edc\u9884\u6d4b\u5931\u8d25\u53ef\u80fd\u6027\u6765\u5207\u6362\u7b56\u7565\u3002", "result": "\u8be5\u6846\u67b6\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u5728\u906d\u53d7\u4e25\u91cd\u5916\u90e8\u6270\u52a8\u65f6\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u529b\u67d4\u987a\u548c\u9c81\u68d2\u5b89\u5168\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5207\u6362\u7b56\u7565\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u5916\u90e8\u6270\u52a8\u4e0b\u7684\u67d4\u987a\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u52a8\u7269\u7684\u81ea\u9002\u5e94\u67d4\u987a\u80fd\u529b\u3002"}}
{"id": "2509.23224", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23224", "abs": "https://arxiv.org/abs/2509.23224", "authors": ["Kohei Sendai", "Maxime Alvarez", "Tatsuya Matsushima", "Yutaka Matsuo", "Yusuke Iwasawa"], "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "comment": null, "summary": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA)\nmodels often predict action chunks; however, this action chunking harms\nreactivity under inference delay and long horizons. We introduce Asynchronous\nAction Chunk Correction (A2C2), which is a lightweight real-time chunk\ncorrection head that runs every control step and adds a time-aware correction\nto any off-the-shelf VLA's action chunk. The module combines the latest\nobservation, the predicted action from VLA (base action), a positional feature\nthat encodes the index of the base action within the chunk, and some features\nfrom the base policy, then outputs a per-step correction. This preserves the\nbase model's competence while restoring closed-loop responsiveness. The\napproach requires no retraining of the base policy and is orthogonal to\nasynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic\nKinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent\nsuccess rate improvements across increasing delays and execution horizons (+23%\npoint and +7% point respectively, compared to RTC), and also improves\nrobustness for long horizons even with zero injected delay. Since the\ncorrection head is small and fast, there is minimal overhead compared to the\ninference of large VLA models. These results indicate that A2C2 is an\neffective, plug-in mechanism for deploying high-capacity chunking policies in\nreal-time control.", "AI": {"tldr": "\u63d0\u51faA2C2\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u52a8\u4f5c\u5757\u6821\u6b63\u5934\u89e3\u51b3VLA\u6a21\u578b\u52a8\u4f5c\u5206\u5757\u5bfc\u81f4\u7684\u53cd\u5e94\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u7b56\u7565\u5373\u53ef\u63d0\u5347\u5b9e\u65f6\u63a7\u5236\u6027\u80fd\u3002", "motivation": "VLA\u6a21\u578b\u4f7f\u7528\u52a8\u4f5c\u5206\u5757\u63d0\u9ad8\u6548\u7387\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u4f46\u8fd9\u5728\u63a8\u7406\u5ef6\u8fdf\u548c\u957f\u65f6\u7a0b\u4e0b\u4f1a\u635f\u5bb3\u53cd\u5e94\u6027\uff0c\u9700\u8981\u4e00\u79cd\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u80fd\u529b\u540c\u65f6\u6062\u590d\u95ed\u73af\u54cd\u5e94\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u5f02\u6b65\u52a8\u4f5c\u5757\u6821\u6b63\u5934\uff0c\u7ed3\u5408\u6700\u65b0\u89c2\u6d4b\u3001VLA\u9884\u6d4b\u52a8\u4f5c\u3001\u52a8\u4f5c\u5728\u5757\u4e2d\u7684\u4f4d\u7f6e\u7279\u5f81\u548c\u57fa\u7840\u7b56\u7565\u7279\u5f81\uff0c\u8f93\u51fa\u6bcf\u6b65\u6821\u6b63\u91cf\uff0c\u4e0e\u5f02\u6b65\u6267\u884c\u65b9\u6848\u6b63\u4ea4\u3002", "result": "\u5728\u52a8\u6001Kinetix\u4efb\u52a1\u5957\u4ef6\u548cLIBERO Spatial\u4e0a\uff0c\u76f8\u6bd4RTC\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u5ef6\u8fdf\u548c\u6267\u884c\u65f6\u7a0b\u4e0b\u5747\u83b7\u5f97\u4e00\u81f4\u7684\u6210\u529f\u7387\u63d0\u5347\uff08\u5206\u522b+23%\u548c+7%\uff09\uff0c\u4e14\u5728\u96f6\u5ef6\u8fdf\u957f\u65f6\u7a0b\u4e0b\u4e5f\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "A2C2\u662f\u4e00\u79cd\u6709\u6548\u3001\u5373\u63d2\u5373\u7528\u7684\u673a\u5236\uff0c\u53ef\u5728\u5b9e\u65f6\u63a7\u5236\u4e2d\u90e8\u7f72\u9ad8\u5bb9\u91cf\u5206\u5757\u7b56\u7565\uff0c\u6821\u6b63\u5934\u8f7b\u91cf\u5feb\u901f\uff0c\u5bf9\u5927\u578bVLA\u6a21\u578b\u63a8\u7406\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2509.23244", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23244", "abs": "https://arxiv.org/abs/2509.23244", "authors": ["Shamir Matan", "Elhadad Osher", "Nageris Ben", "Mirsky Reuth"], "title": "Online Dynamic Goal Recognition in Gym Environments", "comment": null, "summary": "Goal Recognition (GR) is the task of inferring an agent's intended goal from\npartial observations of its behavior, typically in an online and one-shot\nsetting. Despite recent advances in model-free GR, particularly in applications\nsuch as human-robot interaction, surveillance, and assistive systems, the field\nremains fragmented due to inconsistencies in benchmarks, domains, and\nevaluation protocols.\n  To address this, we introduce gr-libs\n(https://github.com/MatanShamir1/gr_libs) and gr-envs\n(https://github.com/MatanShamir1/gr_envs), two complementary open-source\nframeworks that support the development, evaluation, and comparison of GR\nalgorithms in Gym-compatible environments. gr-libs includes modular\nimplementations of MDP-based GR baselines, diagnostic tools, and evaluation\nutilities. gr-envs provides a curated suite of environments adapted for dynamic\nand goal-directed behavior, along with wrappers that ensure compatibility with\nstandard reinforcement learning toolkits. Together, these libraries offer a\nstandardized, extensible, and reproducible platform for advancing GR research.\nBoth packages are open-source and available on GitHub and PyPI.", "AI": {"tldr": "\u63d0\u51fa\u4e86gr-libs\u548cgr-envs\u4e24\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u6807\u51c6\u5316\u76ee\u6807\u8bc6\u522b\u7b97\u6cd5\u7684\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u6bd4\u8f83\uff0c\u89e3\u51b3\u8be5\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u76ee\u6807\u8bc6\u522b\u9886\u57df\u7531\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3001\u9886\u57df\u548c\u8bc4\u4f30\u534f\u8bae\u7684\u4e0d\u4e00\u81f4\u6027\u800c\u788e\u7247\u5316\uff0c\u9700\u8981\u6807\u51c6\u5316\u5e73\u53f0\u6765\u4fc3\u8fdb\u7814\u7a76\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u4e2a\u4e92\u8865\u7684\u5f00\u6e90\u6846\u67b6\uff1agr-libs\u5305\u542b\u6a21\u5757\u5316\u7684MDP\u57fa\u7840\u76ee\u6807\u8bc6\u522b\u7b97\u6cd5\u5b9e\u73b0\u3001\u8bca\u65ad\u5de5\u5177\u548c\u8bc4\u4f30\u5de5\u5177\uff1bgr-envs\u63d0\u4f9b\u7ecf\u8fc7\u8c03\u6574\u7684\u73af\u5883\u5957\u4ef6\uff0c\u652f\u6301\u52a8\u6001\u548c\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\uff0c\u5e76\u786e\u4fdd\u4e0e\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u5de5\u5177\u5305\u7684\u517c\u5bb9\u6027\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u5e73\u53f0\uff0c\u652f\u6301\u76ee\u6807\u8bc6\u522b\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u6bd4\u8f83\uff0c\u4e24\u4e2a\u5305\u5df2\u5728GitHub\u548cPyPI\u4e0a\u5f00\u6e90\u63d0\u4f9b\u3002", "conclusion": "gr-libs\u548cgr-envs\u6846\u67b6\u4e3a\u76ee\u6807\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u51c6\u548c\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u548c\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2509.23281", "categories": ["cs.RO", "I.2.6; I.2.9"], "pdf": "https://arxiv.org/pdf/2509.23281", "abs": "https://arxiv.org/abs/2509.23281", "authors": ["Francesco Marchiori", "Rohan Sinha", "Christopher Agia", "Alexander Robey", "George J. Pappas", "Mauro Conti", "Marco Pavone"], "title": "Preventing Robotic Jailbreaking via Multimodal Domain Adaptation", "comment": "Project page: https://j-dapt.github.io/. 9 pages, 6 figures", "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) are\nincreasingly deployed in robotic environments but remain vulnerable to\njailbreaking attacks that bypass safety mechanisms and drive unsafe or\nphysically harmful behaviors in the real world. Data-driven defenses such as\njailbreak classifiers show promise, yet they struggle to generalize in domains\nwhere specialized datasets are scarce, limiting their effectiveness in robotics\nand other safety-critical contexts. To address this gap, we introduce J-DAPT, a\nlightweight framework for multimodal jailbreak detection through\nattention-based fusion and domain adaptation. J-DAPT integrates textual and\nvisual embeddings to capture both semantic intent and environmental grounding,\nwhile aligning general-purpose jailbreak datasets with domain-specific\nreference data. Evaluations across autonomous driving, maritime robotics, and\nquadruped navigation show that J-DAPT boosts detection accuracy to nearly 100%\nwith minimal overhead. These results demonstrate that J-DAPT provides a\npractical defense for securing VLMs in robotic applications. Additional\nmaterials are made available at: https://j-dapt.github.io.", "AI": {"tldr": "J-DAPT\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u8d8a\u72f1\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u548c\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\uff0c\u5728\u673a\u5668\u4eba\u73af\u5883\u4e2d\u4fdd\u62a4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u514d\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u63a5\u8fd1100%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u5bfc\u81f4\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\u73b0\u6709\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u9632\u5fa1\u65b9\u6cd5\u5728\u4e13\u4e1a\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "J-DAPT\u6846\u67b6\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u6574\u5408\u6587\u672c\u548c\u89c6\u89c9\u5d4c\u5165\uff0c\u6355\u6349\u8bed\u4e49\u610f\u56fe\u548c\u73af\u5883\u80cc\u666f\uff0c\u540c\u65f6\u5c06\u901a\u7528\u8d8a\u72f1\u6570\u636e\u96c6\u4e0e\u9886\u57df\u7279\u5b9a\u7684\u53c2\u8003\u6570\u636e\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u6d77\u6d0b\u673a\u5668\u4eba\u548c\u56db\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u7b49\u573a\u666f\u7684\u8bc4\u4f30\u4e2d\uff0cJ-DAPT\u5c06\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u81f3\u63a5\u8fd1100%\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "J-DAPT\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5b89\u5168\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2509.23288", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23288", "abs": "https://arxiv.org/abs/2509.23288", "authors": ["Yafes Enes \u015eahiner", "Esat Yusuf G\u00fcndo\u011fdu", "Volkan Sezer"], "title": "A Novel Narrow Region Detector for Sampling-Based Planners' Efficiency: Match Based Passage Identifier", "comment": null, "summary": "Autonomous technology, which has become widespread today, appears in many\ndifferent configurations such as mobile robots, manipulators, and drones. One\nof the most important tasks of these vehicles during autonomous operations is\npath planning. In the literature, path planners are generally divided into two\ncategories: probabilistic and deterministic methods. In the analysis of\nprobabilistic methods, the common problem of almost all methods is observed in\nnarrow passage environments. In this paper, a novel sampler is proposed that\ndeterministically identifies narrow passage environments using occupancy grid\nmaps and accordingly increases the amount of sampling in these regions. The\ncodes of the algorithm is provided as open source. To evaluate the performance\nof the algorithm, benchmark studies are conducted in three distinct categories:\nspecific and random simulation environments, and a real-world environment. As a\nresult, it is observed that our algorithm provides higher performance in\nplanning time and number of milestones compared to the baseline samplers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u91c7\u6837\u5668\uff0c\u4f7f\u7528\u5360\u636e\u6805\u683c\u5730\u56fe\u786e\u5b9a\u6027\u8bc6\u522b\u7a84\u901a\u9053\u73af\u5883\uff0c\u5e76\u5728\u8fd9\u4e9b\u533a\u57df\u589e\u52a0\u91c7\u6837\u91cf\uff0c\u5728\u89c4\u5212\u65f6\u95f4\u548c\u91cc\u7a0b\u7891\u6570\u91cf\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u91c7\u6837\u5668\u3002", "motivation": "\u6982\u7387\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u7a84\u901a\u9053\u73af\u5883\u4e2d\u666e\u904d\u5b58\u5728\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u5728\u8fd9\u4e9b\u533a\u57df\u7684\u91c7\u6837\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u5360\u636e\u6805\u683c\u5730\u56fe\u786e\u5b9a\u6027\u8bc6\u522b\u7a84\u901a\u9053\u73af\u5883\uff0c\u5e76\u5728\u8fd9\u4e9b\u533a\u57df\u589e\u52a0\u91c7\u6837\u91cf\u7684\u65b0\u578b\u91c7\u6837\u5668\u7b97\u6cd5\u3002", "result": "\u5728\u7279\u5b9a\u6a21\u62df\u73af\u5883\u3001\u968f\u673a\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7b97\u6cd5\u5728\u89c4\u5212\u65f6\u95f4\u548c\u91cc\u7a0b\u7891\u6570\u91cf\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u91c7\u6837\u5668\u3002", "conclusion": "\u63d0\u51fa\u7684\u91c7\u6837\u5668\u80fd\u6709\u6548\u89e3\u51b3\u7a84\u901a\u9053\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0\uff0c\u5728\u591a\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.23308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23308", "abs": "https://arxiv.org/abs/2509.23308", "authors": ["Jun Chen", "Jiaqing Ma", "Philip Dames"], "title": "Distributed Multi-Robot Multi-Target Simultaneous Search and Tracking in an Unknown Non-convex Environment", "comment": null, "summary": "In unknown non-convex environments, such as indoor and underground spaces,\ndeploying a fleet of robots to explore the surroundings while simultaneously\nsearching for and tracking targets of interest to maintain high-precision data\ncollection represents a fundamental challenge that urgently requires resolution\nin applications such as environmental monitoring and rescue operations. Current\nresearch has made significant progress in addressing environmental exploration,\ninformation search, and target tracking problems, but has yet to establish a\nframework for simultaneously optimizing these tasks in complex environments. In\nthis paper, we propose a novel motion planning algorithm framework that\nintegrates three control strategies: a frontier-based exploration strategy, a\nguaranteed coverage strategy based on Lloyd's algorithm, and a sensor-based\nmulti-target tracking strategy. By incorporating these three strategies, the\nproposed algorithm balances coverage search and high-precision active tracking\nduring exploration. Our approach is validated through a series of MATLAB\nsimulations, demonstrating validity and superiority over standard approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u524d\u6cbf\u63a2\u7d22\u3001\u4fdd\u8bc1\u8986\u76d6\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u7b56\u7565\u7684\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u975e\u51f8\u73af\u5883\u4e2d\u540c\u65f6\u4f18\u5316\u73af\u5883\u63a2\u7d22\u3001\u4fe1\u606f\u641c\u7d22\u548c\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u3002", "motivation": "\u5728\u5ba4\u5185\u3001\u5730\u4e0b\u7b49\u672a\u77e5\u975e\u51f8\u73af\u5883\u4e2d\uff0c\u90e8\u7f72\u673a\u5668\u4eba\u8230\u961f\u540c\u65f6\u8fdb\u884c\u73af\u5883\u63a2\u7d22\u3001\u76ee\u6807\u641c\u7d22\u548c\u8ddf\u8e2a\u4ee5\u7ef4\u6301\u9ad8\u7cbe\u5ea6\u6570\u636e\u91c7\u96c6\u662f\u73af\u5883\u76d1\u6d4b\u548c\u6551\u63f4\u7b49\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u5efa\u7acb\u540c\u65f6\u4f18\u5316\u8fd9\u4e9b\u4efb\u52a1\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u4e09\u79cd\u63a7\u5236\u7b56\u7565\u7684\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff1a\u57fa\u4e8e\u524d\u6cbf\u7684\u63a2\u7d22\u7b56\u7565\u3001\u57fa\u4e8eLloyd\u7b97\u6cd5\u7684\u4fdd\u8bc1\u8986\u76d6\u7b56\u7565\u3001\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u7b56\u7565\uff0c\u5e73\u8861\u8986\u76d6\u641c\u7d22\u548c\u9ad8\u7cbe\u5ea6\u4e3b\u52a8\u8ddf\u8e2a\u3002", "result": "\u901a\u8fc7MATLAB\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u76f8\u6bd4\u6807\u51c6\u65b9\u6cd5\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u540c\u65f6\u4f18\u5316\u73af\u5883\u63a2\u7d22\u3001\u4fe1\u606f\u641c\u7d22\u548c\u76ee\u6807\u8ddf\u8e2a\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23312", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23312", "abs": "https://arxiv.org/abs/2509.23312", "authors": ["Johannes A. Gaus", "Junheon Yoon", "Woo-Jeong Baek", "Seungwon Choi", "Suhan Park", "Jaeheung Park"], "title": "GUARD: Toward a Compromise between Traditional Control and Learning for Safe Robot Systems", "comment": "Submitted as workshop paper to IEEE IROS 2025", "summary": "This paper presents the framework \\textbf{GUARD} (\\textbf{G}uided robot\ncontrol via \\textbf{U}ncertainty attribution and prob\\textbf{A}bilistic kernel\noptimization for \\textbf{R}isk-aware \\textbf{D}ecision making) that combines\ntraditional control with an uncertainty-aware perception technique using active\nlearning with real-time capability for safe robot collision avoidance. By doing\nso, this manuscript addresses the central challenge in robotics of finding a\nreasonable compromise between traditional methods and learning algorithms to\nfoster the development of safe, yet efficient and flexible applications. By\nunifying a reactive model predictive countouring control (RMPCC) with an\nIterative Closest Point (ICP) algorithm that enables the attribution of\nuncertainty sources online using active learning with real-time capability via\na probabilistic kernel optimization technique, \\emph{GUARD} inherently handles\nthe existing ambiguity of the term \\textit{safety} that exists in robotics\nliterature. Experimental studies indicate the high performance of \\emph{GUARD},\nthereby highlighting the relevance and need to broaden its applicability in\nfuture.", "AI": {"tldr": "GUARD\u6846\u67b6\u7ed3\u5408\u4f20\u7edf\u63a7\u5236\u4e0e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u611f\u77e5\u6280\u672f\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u548c\u5b9e\u65f6\u80fd\u529b\u5b9e\u73b0\u5b89\u5168\u7684\u673a\u5668\u4eba\u78b0\u649e\u907f\u514d\uff0c\u5728\u4f20\u7edf\u65b9\u6cd5\u4e0e\u5b66\u4e60\u7b97\u6cd5\u4e4b\u95f4\u627e\u5230\u5408\u7406\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e2d\u4f20\u7edf\u65b9\u6cd5\u4e0e\u5b66\u4e60\u7b97\u6cd5\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5f00\u53d1\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u5e94\u7528\uff0c\u5904\u7406\u673a\u5668\u4eba\u6587\u732e\u4e2d\"\u5b89\u5168\"\u6982\u5ff5\u7684\u6a21\u7cca\u6027\u3002", "method": "\u5c06\u53cd\u5e94\u5f0f\u6a21\u578b\u9884\u6d4b\u8f6e\u5ed3\u63a7\u5236(RMPCC)\u4e0e\u8fed\u4ee3\u6700\u8fd1\u70b9(ICP)\u7b97\u6cd5\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6982\u7387\u6838\u4f18\u5316\u6280\u672f\u5728\u7ebf\u5f52\u56e0\u4e0d\u786e\u5b9a\u6027\u6e90\uff0c\u5177\u5907\u5b9e\u65f6\u4e3b\u52a8\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7814\u7a76\u8868\u660eGUARD\u5177\u6709\u9ad8\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u5176\u76f8\u5173\u6027\u548c\u672a\u6765\u6269\u5c55\u5e94\u7528\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "GUARD\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u4f20\u7edf\u63a7\u5236\u4e0e\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u5b89\u5168\u673a\u5668\u4eba\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u9700\u8981\u6269\u5927\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.23328", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23328", "abs": "https://arxiv.org/abs/2509.23328", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Space Robotics Bench: Robot Learning Beyond Earth", "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "The growing ambition for space exploration demands robust autonomous systems\nthat can operate in unstructured environments under extreme extraterrestrial\nconditions. The adoption of robot learning in this domain is severely hindered\nby the prohibitive cost of technology demonstrations and the limited\navailability of data. To bridge this gap, we introduce the Space Robotics\nBench, an open-source simulation framework for robot learning in space. It\noffers a modular architecture that integrates on-demand procedural generation\nwith massively parallel simulation environments to support the creation of vast\nand diverse training distributions for learning-based agents. To ground\nresearch and enable direct comparison, the framework includes a comprehensive\nsuite of benchmark tasks that span a wide range of mission-relevant scenarios.\nWe establish performance baselines using standard reinforcement learning\nalgorithms and present a series of experimental case studies that investigate\nkey challenges in generalization, end-to-end learning, adaptive control, and\nsim-to-real transfer. Our results reveal insights into the limitations of\ncurrent methods and demonstrate the utility of the framework in producing\npolicies capable of real-world operation. These contributions establish the\nSpace Robotics Bench as a valuable resource for developing, benchmarking, and\ndeploying the robust autonomous systems required for the final frontier.", "AI": {"tldr": "\u63d0\u51fa\u4e86Space Robotics Bench\u5f00\u6e90\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u592a\u7a7a\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u5305\u542b\u6a21\u5757\u5316\u67b6\u6784\u3001\u7a0b\u5e8f\u5316\u751f\u6210\u548c\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\uff0c\u652f\u6301\u521b\u5efa\u591a\u6837\u5316\u8bad\u7ec3\u5206\u5e03\u548c\u57fa\u51c6\u4efb\u52a1\u3002", "motivation": "\u592a\u7a7a\u63a2\u7d22\u9700\u8981\u80fd\u5728\u6781\u7aef\u5916\u661f\u73af\u5883\u4e0b\u8fd0\u884c\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u4f46\u673a\u5668\u4eba\u5b66\u4e60\u5728\u8be5\u9886\u57df\u5e94\u7528\u53d7\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u6280\u672f\u6f14\u793a\u6210\u672c\u9ad8\u6602\u548c\u6570\u636e\u53ef\u7528\u6027\u6709\u9650\u3002", "method": "\u5f00\u53d1\u5f00\u6e90\u4eff\u771f\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\u96c6\u6210\u6309\u9700\u7a0b\u5e8f\u5316\u751f\u6210\u548c\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u73af\u5883\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u57fa\u51c6\u4efb\u52a1\u5957\u4ef6\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5efa\u7acb\u6027\u80fd\u57fa\u7ebf\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u6cdb\u5316\u3001\u7aef\u5230\u7aef\u5b66\u4e60\u3001\u81ea\u9002\u5e94\u63a7\u5236\u548c\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u80fd\u4ea7\u751f\u5177\u5907\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u80fd\u529b\u7684\u7b56\u7565\u3002", "conclusion": "Space Robotics Bench\u4e3a\u5f00\u53d1\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u90e8\u7f72\u592a\u7a7a\u63a2\u7d22\u6240\u9700\u7684\u9c81\u68d2\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2509.23456", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23456", "abs": "https://arxiv.org/abs/2509.23456", "authors": ["Arjun Sadananda", "Ravi Banavar", "Kavi Arya"], "title": "Robust Orientation Estimation with TRIAD-aided Manifold EKF", "comment": null, "summary": "The manifold extended Kalman filter (Manifold EKF) has found extensive\napplication for attitude determination. Magnetometers employed as sensors for\nsuch attitude determination are easily prone to disturbances by their\nsensitivity to calibration and external magnetic fields. The TRIAD (Tri-Axial\nAttitude Determination) algorithm is well known as a sub-optimal attitude\nestimator. In this article, we incorporate this sub-optimal feature of the\nTRIAD in mitigating the influence of the magnetometer reading in the pitch and\nroll axis determination in the Manifold EKF algorithm. We substantiate our\nresults with experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408TRIAD\u7b97\u6cd5\u548c\u6d41\u5f62\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u8f7b\u78c1\u529b\u8ba1\u8bfb\u6570\u5bf9\u4fef\u4ef0\u548c\u6eda\u8f6c\u8f74\u59ff\u6001\u786e\u5b9a\u7684\u5f71\u54cd\u3002", "motivation": "\u78c1\u529b\u8ba1\u4f5c\u4e3a\u59ff\u6001\u786e\u5b9a\u4f20\u611f\u5668\u5bb9\u6613\u53d7\u5230\u6821\u51c6\u548c\u5916\u90e8\u78c1\u573a\u5e72\u6270\uff0c\u800cTRIAD\u7b97\u6cd5\u4f5c\u4e3a\u6b21\u4f18\u59ff\u6001\u4f30\u8ba1\u5668\u5177\u6709\u7279\u5b9a\u4f18\u52bf\uff0c\u9700\u8981\u5c06\u5176\u4e0e\u6d41\u5f62EKF\u7ed3\u5408\u4ee5\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5c06TRIAD\u7b97\u6cd5\u7684\u6b21\u4f18\u7279\u6027\u6574\u5408\u5230\u6d41\u5f62\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2d\uff0c\u7279\u522b\u9488\u5bf9\u4fef\u4ef0\u548c\u6eda\u8f6c\u8f74\u59ff\u6001\u786e\u5b9a\u4e2d\u7684\u78c1\u529b\u8ba1\u8bfb\u6570\u5f71\u54cd\u8fdb\u884c\u7f13\u89e3\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u8f7b\u78c1\u529b\u8ba1\u5e72\u6270\u5bf9\u59ff\u6001\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u5408TRIAD\u7b97\u6cd5\u7684\u6d41\u5f62EKF\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u59ff\u6001\u786e\u5b9a\u7cfb\u7edf\u5728\u78c1\u529b\u8ba1\u53d7\u5e72\u6270\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2509.23468", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23468", "abs": "https://arxiv.org/abs/2509.23468", "authors": ["Haonan Chen", "Jiaming Xu", "Hongyu Chen", "Kaiwen Hong", "Binghao Huang", "Chaoqi Liu", "Jiayuan Mao", "Yunzhu Li", "Yilun Du", "Katherine Driggs-Campbell"], "title": "Multi-Modal Manipulation via Multi-Modal Policy Consensus", "comment": "9 pages, 7 figures", "summary": "Effectively integrating diverse sensory modalities is crucial for robotic\nmanipulation. However, the typical approach of feature concatenation is often\nsuboptimal: dominant modalities such as vision can overwhelm sparse but\ncritical signals like touch in contact-rich tasks, and monolithic architectures\ncannot flexibly incorporate new or missing modalities without retraining. Our\nmethod factorizes the policy into a set of diffusion models, each specialized\nfor a single representation (e.g., vision or touch), and employs a router\nnetwork that learns consensus weights to adaptively combine their\ncontributions, enabling incremental of new representations. We evaluate our\napproach on simulated manipulation tasks in {RLBench}, as well as real-world\ntasks such as occluded object picking, in-hand spoon reorientation, and puzzle\ninsertion, where it significantly outperforms feature-concatenation baselines\non scenarios requiring multimodal reasoning. Our policy further demonstrates\nrobustness to physical perturbations and sensor corruption. We further conduct\nperturbation-based importance analysis, which reveals adaptive shifts between\nmodalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u7b56\u7565\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u7b56\u7565\u5206\u89e3\u4e3a\u591a\u4e2a\u4e13\u95e8\u5904\u7406\u5355\u4e00\u6a21\u6001\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8def\u7531\u5668\u7f51\u7edc\u81ea\u9002\u5e94\u7ec4\u5408\u5404\u6a21\u6001\u8d21\u732e\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u7279\u5f81\u62fc\u63a5\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u62fc\u63a5\u65b9\u6cd5\u5b58\u5728\u4e3b\u5bfc\u6a21\u6001\uff08\u5982\u89c6\u89c9\uff09\u6df9\u6ca1\u7a00\u758f\u4f46\u5173\u952e\u4fe1\u53f7\uff08\u5982\u89e6\u89c9\uff09\u7684\u95ee\u9898\uff0c\u4e14\u5355\u4e00\u67b6\u6784\u65e0\u6cd5\u7075\u6d3b\u5904\u7406\u65b0\u6a21\u6001\u6216\u7f3a\u5931\u6a21\u6001\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u5c06\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u7ec4\u6269\u6563\u6a21\u578b\uff0c\u6bcf\u4e2a\u4e13\u95e8\u5904\u7406\u5355\u4e00\u8868\u5f81\uff08\u5982\u89c6\u89c9\u6216\u89e6\u89c9\uff09\uff0c\u4f7f\u7528\u8def\u7531\u5668\u7f51\u7edc\u5b66\u4e60\u5171\u8bc6\u6743\u91cd\u6765\u81ea\u9002\u5e94\u7ec4\u5408\u5404\u6a21\u6001\u8d21\u732e\uff0c\u652f\u6301\u65b0\u8868\u5f81\u7684\u589e\u91cf\u96c6\u6210\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u906e\u6321\u7269\u4f53\u6293\u53d6\u3001\u624b\u4e2d\u52fa\u5b50\u91cd\u5b9a\u5411\u3001\u62fc\u56fe\u63d2\u5165\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u7279\u5f81\u62fc\u63a5\u57fa\u7ebf\uff0c\u5bf9\u7269\u7406\u6270\u52a8\u548c\u4f20\u611f\u5668\u635f\u574f\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u7684\u81ea\u9002\u5e94\u878d\u5408\uff0c\u5728\u9700\u8981\u591a\u6a21\u6001\u63a8\u7406\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6270\u52a8\u91cd\u8981\u6027\u5206\u6790\u63ed\u793a\u4e86\u6a21\u6001\u95f4\u7684\u81ea\u9002\u5e94\u5207\u6362\u3002"}}
{"id": "2509.23506", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23506", "abs": "https://arxiv.org/abs/2509.23506", "authors": ["Dan BW Choe", "Sundhar Vinodh Sangeetha", "Steven Emanuel", "Chih-Yuan Chiu", "Samuel Coogan", "Shreyas Kousik"], "title": "Ask, Reason, Assist: Decentralized Robot Collaboration via Language and Logic", "comment": null, "summary": "Increased robot deployment, such as in warehousing, has revealed a need for\nseamless collaboration among heterogeneous robot teams to resolve unforeseen\nconflicts. To address this challenge, we propose a novel decentralized\nframework that enables robots to request and provide help. The process begins\nwhen a robot detects a conflict and uses a Large Language Model (LLM) to decide\nwhether external assistance is required. If so, it crafts and broadcasts a\nnatural language (NL) help request. Potential helper robots reason over the\nrequest and respond with offers of assistance, including information about the\neffect on their ongoing tasks. Helper reasoning is implemented via an LLM\ngrounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar,\nensuring syntactically valid NL-to-STL translations, which are then solved as a\nMixed Integer Linear Program (MILP). Finally, the requester robot selects a\nhelper by reasoning over the expected increase in system-level total task\ncompletion time. We evaluated our framework through experiments comparing\ndifferent helper-selection strategies and found that considering multiple\noffers allows the requester to minimize added makespan. Our approach\nsignificantly outperforms heuristics such as selecting the nearest available\ncandidate helper robot, and achieves performance comparable to a centralized\n\"Oracle\" baseline but without heavy information demands.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u8ba9\u673a\u5668\u4eba\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u548c\u63d0\u4f9b\u5e2e\u52a9\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u51b3\u7b56\u548c\u63a8\u7406\uff0c\u7ed3\u5408STL\u548cMILP\u4f18\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u4ed3\u50a8\u7b49\u573a\u666f\u4e2d\u5e94\u5bf9\u610f\u5916\u51b2\u7a81\u65f6\u65e0\u7f1d\u534f\u4f5c\u7684\u9700\u6c42\u3002", "method": "\u673a\u5668\u4eba\u68c0\u6d4b\u51b2\u7a81\u540e\u4f7f\u7528LLM\u51b3\u5b9a\u662f\u5426\u9700\u8981\u5916\u90e8\u5e2e\u52a9\uff0c\u53d1\u9001\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\uff1b\u5e2e\u52a9\u673a\u5668\u4eba\u4f7f\u7528\u57fa\u4e8eSTL\u7684LLM\u63a8\u7406\uff0c\u901a\u8fc7BNF\u8bed\u6cd5\u786e\u4fddNL\u5230STL\u7684\u6709\u6548\u8f6c\u6362\uff0c\u5e76\u7528MILP\u6c42\u89e3\uff1b\u8bf7\u6c42\u8005\u57fa\u4e8e\u7cfb\u7edf\u7ea7\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u9009\u62e9\u5e2e\u52a9\u8005\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8003\u8651\u591a\u4e2a\u5e2e\u52a9\u63d0\u8bae\u80fd\u8ba9\u8bf7\u6c42\u8005\u6700\u5c0f\u5316\u589e\u52a0\u7684makespan\uff0c\u663e\u8457\u4f18\u4e8e\u9009\u62e9\u6700\u8fd1\u53ef\u7528\u673a\u5668\u4eba\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u57fa\u51c6\u4f46\u4fe1\u606f\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u65e0\u7f1d\u534f\u4f5c\uff0c\u5728\u51cf\u5c11\u4fe1\u606f\u9700\u6c42\u7684\u540c\u65f6\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2509.23556", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23556", "abs": "https://arxiv.org/abs/2509.23556", "authors": ["Curtis C. Johnson", "Carlo Alessi", "Egidio Falotico", "Marc D. Killpack"], "title": "Zero-shot Whole-Body Manipulation with a Large-Scale Soft Robotic Torso via Guided Reinforcement Learning", "comment": "Submitted to IEEE Transactions on Robotics for review", "summary": "Whole-body manipulation is a powerful yet underexplored approach that enables\nrobots to interact with large, heavy, or awkward objects using more than just\ntheir end-effectors. Soft robots, with their inherent passive compliance, are\nparticularly well-suited for such contact-rich manipulation tasks, but their\nuncertainties in kinematics and dynamics pose significant challenges for\nsimulation and control. In this work, we address this challenge with a\nsimulation that can run up to 350x real time on a single thread in MuJoCo and\nprovide a detailed analysis of the critical tradeoffs between speed and\naccuracy for this simulation. Using this framework, we demonstrate a successful\nzero-shot sim-to-real transfer of a learned whole-body manipulation policy,\nachieving an 88% success rate on the Baloo hardware platform. We show that\nguiding RL with a simple motion primitive is critical to this success where\nstandard reward shaping methods struggled to produce a stable and successful\npolicy for whole-body manipulation. Furthermore, our analysis reveals that the\nlearned policy does not simply mimic the motion primitive. It exhibits\nbeneficial reactive behavior, such as re-grasping and perturbation recovery. We\nanalyze and contrast this learned policy against an open-loop baseline to show\nthat the policy can also exhibit aggressive over-corrections under\nperturbation. To our knowledge, this is the first demonstration of forceful,\nsix-DoF whole-body manipulation using two continuum soft arms on a large-scale\nplatform (10 kg payloads), with zero-shot policy transfer.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u9ad8\u901f\u8f6f\u4f53\u673a\u5668\u4eba\u5168\u8eab\u64cd\u4f5c\u4eff\u771f\u6846\u67b6\uff0c\u5b9e\u73b0350\u500d\u5b9e\u65f6\u901f\u5ea6\uff0c\u5e76\u6210\u529f\u5c06\u5b66\u4e60\u7b56\u7565\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\uff0c\u572810kg\u8d1f\u8f7d\u4e0b\u8fbe\u523088%\u6210\u529f\u7387\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u56e0\u5176\u88ab\u52a8\u67d4\u987a\u6027\u9002\u5408\u63a5\u89e6\u4e30\u5bcc\u7684\u5168\u8eab\u64cd\u4f5c\u4efb\u52a1\uff0c\u4f46\u5176\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u4e0d\u786e\u5b9a\u6027\u7ed9\u4eff\u771f\u548c\u63a7\u5236\u5e26\u6765\u6311\u6218\u3002", "method": "\u4f7f\u7528MuJoCo\u6784\u5efa\u9ad8\u901f\u4eff\u771f\u6846\u67b6\uff0c\u7ed3\u5408\u7b80\u5355\u8fd0\u52a8\u57fa\u5143\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u5168\u8eab\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u4eff\u771f\u901f\u5ea6\u8fbe350\u500d\u5b9e\u65f6\uff0c\u96f6\u6837\u672c\u8fc1\u79fb\u6210\u529f\u738788%\uff0c\u5b66\u4e60\u7b56\u7565\u8868\u73b0\u51fa\u91cd\u65b0\u6293\u53d6\u548c\u6270\u52a8\u6062\u590d\u7b49\u53cd\u5e94\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5b9e\u73b0\u4e24\u4e2a\u8fde\u7eed\u8f6f\u4f53\u81c2\u5728\u5927\u578b\u5e73\u53f0\u4e0a\u8fdb\u884c\u5f3a\u529b\u516d\u81ea\u7531\u5ea6\u5168\u8eab\u64cd\u4f5c\uff0c\u5e76\u6210\u529f\u96f6\u6837\u672c\u7b56\u7565\u8fc1\u79fb\u3002"}}
{"id": "2509.23561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23561", "abs": "https://arxiv.org/abs/2509.23561", "authors": ["Jianren Wang", "Jie Han", "Abhinav Gupta", "Deepak Pathak", "Yang Zhang"], "title": "High Torque Density PCB Axial Flux Permanent Magnet Motor for Micro Robots", "comment": null, "summary": "Quasi-direct-drive (QDD) actuation is transforming legged and manipulator\nrobots by eliminating high-ratio gearboxes, yet it demands motors that deliver\nvery high torque at low speed within a thin, disc-shaped joint envelope.\nAxial-flux permanent-magnet (AFPM) machines meet these geometric and torque\nrequirements, but scaling them below a 20mm outer diameter is hampered by poor\ncopper fill in conventional wound stators, inflating resistance and throttling\ncontinuous torque. This paper introduces a micro-scale AFPM motor that\novercomes these limitations through printed-circuit-board (PCB) windings\nfabricated with advanced IC-substrate high-density interconnect (HDI)\ntechnology. The resulting 48-layer stator-formed by stacking four 12-layer HDI\nmodules-achieves a record 45\\% copper fill in a package only 5mm thick and 19mm\nin diameter. We perform comprehensive electromagnetic and thermal analyses to\ninform the motor design, then fabricate a prototype whose performance\ncharacteristics are experimentally verified.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u91c7\u7528PCB\u7ed5\u7ec4\u548cHDI\u6280\u672f\u7684\u5fae\u578b\u8f74\u5411\u78c1\u901a\u6c38\u78c1\u7535\u673a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ed5\u7ebf\u5b9a\u5b50\u5728\u5c0f\u578b\u5316\u65f6\u94dc\u586b\u5145\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8645%\u7684\u9ad8\u94dc\u586b\u5145\u7387\u548c\u7d27\u51d1\u76845mm\u539a\u5ea6\u8bbe\u8ba1\u3002", "motivation": "\u51c6\u76f4\u9a71\u9a71\u52a8\u9700\u8981\u5728\u5c0f\u5c3a\u5bf8\u5173\u8282\u5305\u7edc\u5185\u63d0\u4f9b\u9ad8\u626d\u77e9\uff0c\u4f46\u4f20\u7edfAFPM\u7535\u673a\u5728\u76f4\u5f84\u5c0f\u4e8e20mm\u65f6\u56e0\u94dc\u586b\u5145\u7387\u4f4e\u5bfc\u81f4\u7535\u963b\u589e\u52a0\u548c\u8fde\u7eed\u626d\u77e9\u53d7\u9650\u3002", "method": "\u4f7f\u7528\u5370\u5237\u7535\u8def\u677f\u7ed5\u7ec4\u548c\u5148\u8fdbIC\u57fa\u677f\u9ad8\u5bc6\u5ea6\u4e92\u8fde\u6280\u672f\uff0c\u901a\u8fc7\u5806\u53e0\u56db\u4e2a12\u5c42HDI\u6a21\u5757\u5f62\u621048\u5c42\u5b9a\u5b50\u7ed3\u6784\u3002", "result": "\u5b9e\u73b0\u4e8645%\u7684\u94dc\u586b\u5145\u7387\uff0c\u7535\u673a\u539a\u5ea6\u4ec55mm\uff0c\u76f4\u5f8419mm\uff0c\u5e76\u901a\u8fc7\u7535\u78c1\u548c\u70ed\u5206\u6790\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\uff0c\u5236\u9020\u4e86\u539f\u578b\u673a\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "conclusion": "PCB\u7ed5\u7ec4\u7ed3\u5408HDI\u6280\u672f\u6210\u529f\u89e3\u51b3\u4e86\u5fae\u578bAFPM\u7535\u673a\u7684\u94dc\u586b\u5145\u7387\u9650\u5236\uff0c\u4e3a\u5c0f\u578b\u5316\u9ad8\u626d\u77e9\u7535\u673a\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.23563", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23563", "abs": "https://arxiv.org/abs/2509.23563", "authors": ["Seungchan Kim", "Omar Alama", "Dmytro Kurdydyk", "John Keller", "Nikhil Keetha", "Wenshan Wang", "Yonatan Bisk", "Sebastian Scherer"], "title": "RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation", "comment": null, "summary": "Aerial outdoor semantic navigation requires robots to explore large,\nunstructured environments to locate target objects. Recent advances in semantic\nnavigation have demonstrated open-set object-goal navigation in indoor\nsettings, but these methods remain limited by constrained spatial ranges and\nstructured layouts, making them unsuitable for long-range outdoor search. While\noutdoor semantic navigation approaches exist, they either rely on reactive\npolicies based on current observations, which tend to produce short-sighted\nbehaviors, or precompute scene graphs offline for navigation, limiting\nadaptability to online deployment. We present RAVEN, a 3D memory-based,\nbehavior tree framework for aerial semantic navigation in unstructured outdoor\nenvironments. It (1) uses a spatially consistent semantic voxel-ray map as\npersistent memory, enabling long-horizon planning and avoiding purely reactive\nbehaviors, (2) combines short-range voxel search and long-range ray search to\nscale to large environments, (3) leverages a large vision-language model to\nsuggest auxiliary cues, mitigating sparsity of outdoor targets. These\ncomponents are coordinated by a behavior tree, which adaptively switches\nbehaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor\nsimulation environments over 100 semantic tasks, encompassing single-object\nsearch, multi-class, multi-instance navigation and sequential task changes.\nResults show RAVEN outperforms baselines by 85.25% in simulation and\ndemonstrate its real-world applicability through deployment on an aerial robot\nin outdoor field tests.", "AI": {"tldr": "RAVEN\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u8bb0\u5fc6\u548c\u884c\u4e3a\u6811\u7684\u7a7a\u4e2d\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\uff0c\u901a\u8fc7\u8bed\u4e49\u4f53\u7d20\u5c04\u7ebf\u5730\u56fe\u5b9e\u73b0\u957f\u7a0b\u89c4\u5212\uff0c\u7ed3\u5408\u77ed\u7a0b\u4f53\u7d20\u641c\u7d22\u548c\u957f\u7a0b\u5c04\u7ebf\u641c\u7d22\uff0c\u5229\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u8f85\u52a9\u7ebf\u7d22\uff0c\u5728\u6237\u5916\u8bed\u4e49\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6237\u5916\u7a7a\u4e2d\u8bed\u4e49\u5bfc\u822a\u7684\u6311\u6218\uff1a\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u5ba4\u5185\u7ed3\u6784\u5316\u73af\u5883\uff0c\u8981\u4e48\u5728\u6237\u5916\u73af\u5883\u4e2d\u4f9d\u8d56\u53cd\u5e94\u5f0f\u7b56\u7565\u5bfc\u81f4\u77ed\u89c6\u884c\u4e3a\uff0c\u6216\u9700\u8981\u79bb\u7ebf\u9884\u8ba1\u7b97\u573a\u666f\u56fe\u800c\u7f3a\u4e4f\u5728\u7ebf\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u7a7a\u95f4\u4e00\u81f4\u7684\u8bed\u4e49\u4f53\u7d20\u5c04\u7ebf\u5730\u56fe\u4f5c\u4e3a\u6301\u4e45\u8bb0\u5fc6\uff0c\u7ed3\u5408\u77ed\u7a0b\u4f53\u7d20\u641c\u7d22\u548c\u957f\u7a0b\u5c04\u7ebf\u641c\u7d22\uff0c\u5229\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u8f85\u52a9\u7ebf\u7d22\uff0c\u901a\u8fc7\u884c\u4e3a\u6811\u534f\u8c03\u81ea\u9002\u5e94\u5207\u6362\u884c\u4e3a\u3002", "result": "\u572810\u4e2a\u903c\u771f\u6237\u5916\u4eff\u771f\u73af\u5883\u4e2d\u6d4b\u8bd5100\u4e2a\u8bed\u4e49\u4efb\u52a1\uff0cRAVEN\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u534785.25%\uff0c\u5e76\u5728\u771f\u5b9e\u6237\u5916\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\u9a8c\u8bc1\u3002", "conclusion": "RAVEN\u6846\u67b6\u901a\u8fc73D\u8bb0\u5fc6\u3001\u591a\u5c3a\u5ea6\u641c\u7d22\u7b56\u7565\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u7ed3\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6237\u5916\u7a7a\u4e2d\u8bed\u4e49\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2509.23567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23567", "abs": "https://arxiv.org/abs/2509.23567", "authors": ["Fangting Xu", "Jilin Zhu", "Xiaoming Gu", "Jianzhong Tang"], "title": "GES-UniGrasp: A Two-Stage Dexterous Grasping Strategy With Geometry-Based Expert Selection", "comment": null, "summary": "Robust and human-like dexterous grasping of general objects is a critical\ncapability for advancing intelligent robotic manipulation in real-world\nscenarios. However, existing reinforcement learning methods guided by grasp\npriors often result in unnatural behaviors. In this work, we present\n\\textit{ContactGrasp}, a robotic dexterous pre-grasp and grasp dataset that\nexplicitly accounts for task-relevant wrist orientation and thumb-index\npinching coordination. The dataset covers 773 objects in 82 categories,\nproviding a rich foundation for training human-like grasp strategies. Building\nupon this dataset, we perform geometry-based clustering to group objects by\nshape, enabling a two-stage Geometry-based Expert Selection (GES) framework\nthat selects among specialized experts for grasping diverse object geometries,\nthereby enhancing adaptability to diverse shapes and generalization across\ncategories. Our approach demonstrates natural grasp postures and achieves high\nsuccess rates of 99.4\\% and 96.3\\% on the train and test sets, respectively,\nshowcasing strong generalization and high-quality grasp execution.", "AI": {"tldr": "\u63d0\u51fa\u4e86ContactGrasp\u6570\u636e\u96c6\u548cGeometry-based Expert Selection\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u7c7b\u4eba\u7075\u5de7\u6293\u63e1\uff0c\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.4%\u548c96.3%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6293\u63e1\u5148\u9a8c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u4ea7\u751f\u4e0d\u81ea\u7136\u7684\u884c\u4e3a\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u81ea\u7136\u7684\u4eba\u7c7b\u5316\u7075\u5de7\u6293\u63e1\u65b9\u6cd5\u3002", "method": "\u6784\u5efaContactGrasp\u6570\u636e\u96c6\uff0c\u8003\u8651\u4efb\u52a1\u76f8\u5173\u624b\u8155\u65b9\u5411\u548c\u62c7\u6307-\u98df\u6307\u634f\u5408\u534f\u8c03\uff1b\u901a\u8fc7\u51e0\u4f55\u805a\u7c7b\u5c06\u7269\u4f53\u6309\u5f62\u72b6\u5206\u7ec4\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u51e0\u4f55\u4e13\u5bb6\u9009\u62e9\u6846\u67b6\u9009\u62e9\u4e13\u95e8\u4e13\u5bb6\u3002", "result": "\u5c55\u793a\u4e86\u81ea\u7136\u7684\u6293\u63e1\u59ff\u52bf\uff0c\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.4%\u548c96.3%\u7684\u9ad8\u6210\u529f\u7387\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u8d28\u91cf\u7684\u6293\u63e1\u6267\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7c7b\u4eba\u7684\u7075\u5de7\u6293\u63e1\uff0c\u5bf9\u591a\u6837\u5316\u7269\u4f53\u5f62\u72b6\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\uff0c\u5e76\u80fd\u8de8\u7c7b\u522b\u6cdb\u5316\u3002"}}
{"id": "2509.23575", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23575", "abs": "https://arxiv.org/abs/2509.23575", "authors": ["Jianshu Hu", "Lidi Wang", "Shujia Li", "Yunpeng Jiang", "Xiao Li", "Paul Weng", "Yutong Ban"], "title": "Generalizable Coarse-to-Fine Robot Manipulation via Language-Aligned 3D Keypoints", "comment": null, "summary": "Hierarchical coarse-to-fine policy, where a coarse branch predicts a region\nof interest to guide a fine-grained action predictor, has demonstrated\nsignificant potential in robotic 3D manipulation tasks by especially enhancing\nsample efficiency and enabling more precise manipulation. However, even\naugmented with pre-trained models, these hierarchical policies still suffer\nfrom generalization issues. To enhance generalization to novel instructions and\nenvironment variations, we propose Coarse-to-fine Language-Aligned manipulation\nPolicy (CLAP), a framework that integrates three key components: 1) task\ndecomposition, 2) VLM fine-tuning for 3D keypoint prediction, and 3) 3D-aware\nrepresentation. Through comprehensive experiments in simulation and on a real\nrobot, we demonstrate its superior generalization capability. Specifically, on\nGemBench, a benchmark designed for evaluating generalization, our approach\nachieves a 12\\% higher average success rate than the SOTA method while using\nonly 1/5 of the training trajectories. In real-world experiments, our policy,\ntrained on only 10 demonstrations, successfully generalizes to novel\ninstructions and environments.", "AI": {"tldr": "\u63d0\u51faCLAP\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u3001VLM\u5fae\u8c03\u8fdb\u884c3D\u5173\u952e\u70b9\u9884\u6d4b\u548c3D\u611f\u77e5\u8868\u793a\uff0c\u589e\u5f3a\u5206\u5c42\u7b56\u7565\u5728\u673a\u5668\u4eba3D\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5206\u5c42\u7c97\u5230\u7ec6\u7b56\u7565\u5728\u673a\u5668\u4eba3D\u64cd\u4f5c\u4efb\u52a1\u4e2d\u867d\u7136\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u64cd\u4f5c\u7cbe\u5ea6\uff0c\u4f46\u5728\u9762\u5bf9\u65b0\u6307\u4ee4\u548c\u73af\u5883\u53d8\u5316\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "CLAP\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u4efb\u52a1\u5206\u89e3\u3001VLM\u5fae\u8c03\u8fdb\u884c3D\u5173\u952e\u70b9\u9884\u6d4b\u30013D\u611f\u77e5\u8868\u793a\uff0c\u901a\u8fc7\u8bed\u8a00\u5bf9\u9f50\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728GemBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6210\u529f\u7387\u6bd4SOTA\u65b9\u6cd5\u63d0\u9ad812%\uff0c\u4ec5\u4f7f\u75281/5\u7684\u8bad\u7ec3\u8f68\u8ff9\u3002\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u4ec5\u752810\u4e2a\u6f14\u793a\u5c31\u80fd\u6cdb\u5316\u5230\u65b0\u6307\u4ee4\u548c\u73af\u5883\u3002", "conclusion": "CLAP\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5206\u5c42\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.23623", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23623", "abs": "https://arxiv.org/abs/2509.23623", "authors": ["Nicholas Pagliocca", "Behrad Koohbor", "Mitja Trkov"], "title": "Encoding Material Safety using Control Barrier Functions for Soft Actuator Control", "comment": "8 pages, 5 figures", "summary": "Until recently, the concept of soft robot safety was an informal notion,\noften attributed solely to the fact that soft robots are less likely to damage\ntheir operating environment than rigid robots. As the field moves toward\nfeedback control for practical applications, it becomes increasingly important\nto define what safety means and to characterize how soft robots can become\nunsafe. The unifying theme of soft robotics is to achieve useful functionality\nthrough deformation. Consequently, limitations in constitutive model accuracy\nand risks of material failure are inherent to all soft robots and pose a key\nchallenge in designing provably safe controllers. This work introduces a formal\ndefinition of material safety based on strain energy functions and provides a\ncontroller that enforces it. We characterize safe and unsafe sets of an\nincompressible hyperelastic material and demonstrate that safety can be\nenforced using a high-order control barrier function (HOCBF) with quadratic\nprogram-based feedback control. As a case study, we consider a pressurized\nhyperelastic tube with inertial effects, first-order viscous effects, and\nfull-state feedback. Simulation results verify that the proposed methodology\ncan enforce the material safety specification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u5e94\u53d8\u80fd\u51fd\u6570\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u6750\u6599\u5b89\u5168\u6027\u6b63\u5f0f\u5b9a\u4e49\uff0c\u5e76\u5f00\u53d1\u4e86\u4f7f\u7528\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u548c\u4e8c\u6b21\u89c4\u5212\u53cd\u9988\u63a7\u5236\u6765\u5f3a\u5236\u6267\u884c\u6750\u6599\u5b89\u5168\u7684\u63a7\u5236\u5668\u3002", "motivation": "\u968f\u7740\u8f6f\u4f53\u673a\u5668\u4eba\u5411\u5b9e\u9645\u5e94\u7528\u53d1\u5c55\uff0c\u9700\u8981\u660e\u786e\u5b9a\u4e49\u5b89\u5168\u6027\u6982\u5ff5\u3002\u8f6f\u4f53\u673a\u5668\u4eba\u901a\u8fc7\u53d8\u5f62\u5b9e\u73b0\u529f\u80fd\uff0c\u4f46\u6750\u6599\u672c\u6784\u6a21\u578b\u7cbe\u5ea6\u9650\u5236\u548c\u5931\u6548\u98ce\u9669\u662f\u6240\u6709\u8f6f\u4f53\u673a\u5668\u4eba\u56fa\u6709\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u5e94\u53d8\u80fd\u51fd\u6570\u5b9a\u4e49\u6750\u6599\u5b89\u5168\u6027\uff0c\u4f7f\u7528\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u548c\u4e8c\u6b21\u89c4\u5212\u53cd\u9988\u63a7\u5236\u6765\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\uff0c\u4ee5\u4e0d\u53ef\u538b\u7f29\u8d85\u5f39\u6027\u7ba1\u4e3a\u6848\u4f8b\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5f3a\u5236\u6267\u884c\u6750\u6599\u5b89\u5168\u89c4\u8303\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u7684\u5b89\u5168\u5b9a\u4e49\u548c\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6750\u6599\u5931\u6548\u98ce\u9669\u8fd9\u4e00\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.23650", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23650", "abs": "https://arxiv.org/abs/2509.23650", "authors": ["Peizhuo Li", "Hongyi Li", "Yuxuan Ma", "Linnan Chang", "Xinrong Yang", "Ruiqi Yu", "Yifeng Zhang", "Yuhong Cao", "Qiuguo Zhu", "Guillaume Sartoretti"], "title": "KiVi: Kinesthetic-Visuospatial Integration for Dynamic and Safe Egocentric Legged Locomotion", "comment": null, "summary": "Vision-based locomotion has shown great promise in enabling legged robots to\nperceive and adapt to complex environments. However, visual information is\ninherently fragile, being vulnerable to occlusions, reflections, and lighting\nchanges, which often cause instability in locomotion. Inspired by animal\nsensorimotor integration, we propose KiVi, a Kinesthetic-Visuospatial\nintegration framework, where kinesthetics encodes proprioceptive sensing of\nbody motion and visuospatial reasoning captures visual perception of\nsurrounding terrain. Specifically, KiVi separates these pathways, leveraging\nproprioception as a stable backbone while selectively incorporating vision for\nterrain awareness and obstacle avoidance. This modality-balanced, yet\nintegrative design, combined with memory-enhanced attention, allows the robot\nto robustly interpret visual cues while maintaining fallback stability through\nproprioception. Extensive experiments show that our method enables quadruped\nrobots to stably traverse diverse terrains and operate reliably in unstructured\noutdoor environments, remaining robust to out-of-distribution (OOD) visual\nnoise and occlusion unseen during training, thereby highlighting its\neffectiveness and applicability to real-world legged locomotion.", "AI": {"tldr": "\u63d0\u51faKiVi\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u672c\u4f53\u611f\u89c9\u548c\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u8def\u5f84\uff0c\u8ba9\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7a33\u5b9a\u8fd0\u52a8\uff0c\u89c6\u89c9\u611f\u77e5\u7528\u4e8e\u5730\u5f62\u611f\u77e5\u548c\u907f\u969c\uff0c\u672c\u4f53\u611f\u89c9\u4f5c\u4e3a\u7a33\u5b9a\u652f\u6491\u3002", "motivation": "\u89c6\u89c9\u4fe1\u606f\u6613\u53d7\u906e\u6321\u3001\u53cd\u5c04\u548c\u5149\u7167\u53d8\u5316\u5f71\u54cd\uff0c\u5bfc\u81f4\u8fd0\u52a8\u4e0d\u7a33\u5b9a\u3002\u53d7\u52a8\u7269\u611f\u89c9\u8fd0\u52a8\u6574\u5408\u542f\u53d1\uff0c\u9700\u8981\u5e73\u8861\u89c6\u89c9\u548c\u672c\u4f53\u611f\u89c9\u7684\u6574\u5408\u3002", "method": "\u5206\u79bb\u672c\u4f53\u611f\u89c9\u548c\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u8def\u5f84\uff0c\u672c\u4f53\u611f\u89c9\u4f5c\u4e3a\u7a33\u5b9a\u4e3b\u5e72\uff0c\u9009\u62e9\u6027\u6574\u5408\u89c6\u89c9\u7528\u4e8e\u5730\u5f62\u611f\u77e5\uff0c\u7ed3\u5408\u8bb0\u5fc6\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u56db\u8db3\u673a\u5668\u4eba\u80fd\u7a33\u5b9a\u7a7f\u8d8a\u591a\u6837\u5316\u5730\u5f62\uff0c\u5728\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u53ef\u9760\u8fd0\u884c\uff0c\u5bf9\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u89c6\u89c9\u566a\u58f0\u548c\u906e\u6321\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "KiVi\u6846\u67b6\u901a\u8fc7\u6a21\u6001\u5e73\u8861\u7684\u6574\u5408\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u817f\u5f0f\u8fd0\u52a8\u7684\u6709\u6548\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2509.23651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23651", "abs": "https://arxiv.org/abs/2509.23651", "authors": ["Xinrong Yang", "Peizhuo Li", "Hongyi Li", "Junkai Lu", "Linnan Chang", "Yuhong Cao", "Yifeng Zhang", "Ge Sun", "Guillaume Sartoretti"], "title": "HeLoM: Hierarchical Learning for Whole-Body Loco-Manipulation in Hexapod Robot", "comment": null, "summary": "Robots in real-world environments are often required to move/manipulate\nobjects comparable in weight to their own bodies. Compared to grasping and\ncarrying, pushing provides a more straightforward and efficient non-prehensile\nmanipulation strategy, avoiding complex grasp design while leveraging direct\ncontact to regulate an object's pose. Achieving effective pushing, however,\ndemands both sufficient manipulation forces and the ability to maintain\nstability, which is particularly challenging when dealing with heavy or\nirregular objects. To address these challenges, we propose HeLoM, a\nlearning-based hierarchical whole-body manipulation framework for a hexapod\nrobot that exploits coordinated multi-limb control. Inspired by the cooperative\nstrategies of multi-legged insects, our framework leverages redundant contact\npoints and high degrees of freedom to enable dynamic redistribution of contact\nforces. HeLoM's high-level planner plans pushing behaviors and target object\nposes, while its low-level controller maintains locomotion stability and\ngenerates dynamically consistent joint actions. Our policies trained in\nsimulation are directly deployed on real robots without additional fine-tuning.\nThis design allows the robot to maintain balance while exerting continuous and\ncontrollable pushing forces through coordinated foreleg interaction and\nsupportive hind-leg propulsion. We validate the effectiveness of HeLoM through\nboth simulation and real-world experiments. Results show that our framework can\nstably push boxes of varying sizes and unknown physical properties to\ndesignated goal poses in the real world.", "AI": {"tldr": "\u63d0\u51faHeLoM\u6846\u67b6\uff0c\u8ba9\u516d\u8db3\u673a\u5668\u4eba\u901a\u8fc7\u534f\u8c03\u591a\u80a2\u4f53\u63a7\u5236\u5b9e\u73b0\u7a33\u5b9a\u63a8\u91cd\u7269\u64cd\u4f5c", "motivation": "\u673a\u5668\u4eba\u9700\u8981\u63a8\u4e3e\u4e0e\u81ea\u8eab\u91cd\u91cf\u76f8\u5f53\u7684\u7269\u4f53\uff0c\u63a8\u64cd\u4f5c\u6bd4\u6293\u53d6\u66f4\u76f4\u63a5\u9ad8\u6548\uff0c\u4f46\u9700\u8981\u8db3\u591f\u7684\u63a8\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5bf9\u91cd\u578b\u6216\u4e0d\u89c4\u5219\u7269\u4f53", "method": "\u5206\u5c42\u5b66\u4e60\u6846\u67b6\uff1a\u9ad8\u5c42\u89c4\u5212\u63a8\u884c\u4e3a\u548c\u76ee\u6807\u59ff\u6001\uff0c\u4f4e\u5c42\u63a7\u5236\u5668\u7ef4\u6301\u8fd0\u52a8\u7a33\u5b9a\u6027\u5e76\u751f\u6210\u52a8\u6001\u4e00\u81f4\u7684\u5173\u8282\u52a8\u4f5c\uff0c\u5229\u7528\u5197\u4f59\u63a5\u89e6\u70b9\u534f\u8c03\u591a\u80a2\u4f53\u63a7\u5236", "result": "\u4eff\u771f\u8bad\u7ec3\u7684\u7b56\u7565\u53ef\u76f4\u63a5\u90e8\u7f72\u5230\u771f\u5b9e\u673a\u5668\u4eba\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u80fd\u591f\u7a33\u5b9a\u63a8\u52a8\u4e0d\u540c\u5c3a\u5bf8\u548c\u672a\u77e5\u7269\u7406\u5c5e\u6027\u7684\u7bb1\u5b50\u5230\u6307\u5b9a\u76ee\u6807\u4f4d\u7f6e", "conclusion": "HeLoM\u6846\u67b6\u901a\u8fc7\u534f\u8c03\u591a\u80a2\u4f53\u63a7\u5236\uff0c\u4f7f\u516d\u8db3\u673a\u5668\u4eba\u80fd\u591f\u7a33\u5b9a\u63a8\u52a8\u91cd\u7269\uff0c\u9a8c\u8bc1\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2509.23655", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23655", "abs": "https://arxiv.org/abs/2509.23655", "authors": ["Rokas Bendikas", "Daniel Dijkman", "Markus Peschl", "Sanjay Haresh", "Pietro Mazzaglia"], "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models", "comment": "Presented at 9th Conference on Robot Learning (CoRL 2025), Seoul,\n  Korea", "summary": "Vision-Language-Action (VLA) models offer a pivotal approach to learning\nrobotic manipulation at scale by repurposing large pre-trained\nVision-Language-Models (VLM) to output robotic actions. However, adapting VLMs\nfor robotic domains comes with an unnecessarily high computational cost, which\nwe attribute to the tokenization scheme of visual inputs. In this work, we aim\nto enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric\nTokenization for VLAs. Building on the insights of object-centric\nrepresentation learning, our method introduces an inductive bias towards scene\nobjects and the agent's own visual information. As a result, we find that\nOat-VLA can drastically reduce the number of visual tokens to just a few tokens\nwithout sacrificing performance. We reveal that Oat-VLA converges at least\ntwice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in\ndiverse real-world pick and place tasks.", "AI": {"tldr": "Oat-VLA\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5bf9\u8c61\u548c\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u6765\u663e\u8457\u63d0\u9ad8Vision-Language-Action\u6a21\u578b\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u81f3\u5c11\u4e24\u500d\u7684\u6536\u655b\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u73b0\u6709Vision-Language-Action\u6a21\u578b\u5728\u9002\u5e94\u673a\u5668\u4eba\u9886\u57df\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u89c6\u89c9\u8f93\u5165\u7684\u6807\u8bb0\u5316\u65b9\u6848\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faOat-VLA\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u5b66\u4e60\u7684\u601d\u60f3\uff0c\u5f15\u5165\u5bf9\u573a\u666f\u5bf9\u8c61\u548c\u667a\u80fd\u4f53\u81ea\u8eab\u89c6\u89c9\u4fe1\u606f\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5c06\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u5927\u5e45\u51cf\u5c11\u5230\u4ec5\u51e0\u4e2a\u6807\u8bb0\u3002", "result": "\u5728LIBERO\u5957\u4ef6\u4e0a\u6536\u655b\u901f\u5ea6\u81f3\u5c11\u662fOpenVLA\u7684\u4e24\u500d\uff0c\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8eOpenVLA\u3002", "conclusion": "Oat-VLA\u901a\u8fc7\u9ad8\u6548\u7684\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6848\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23656", "abs": "https://arxiv.org/abs/2509.23656", "authors": ["Liangting Wu", "Roberto Tron"], "title": "Certifiably Optimal State Estimation and Robot Calibration Using Trace-Constrained SDP", "comment": null, "summary": "Many nonconvex problems in robotics can be relaxed into convex formulations\nvia semidefinite programming (SDP), which offers the advantage of global\noptimality. The practical quality of these solutions, however, critically\ndepends on achieving rank-1 matrices, a condition that typically requires\nadditional tightening. In this work, we focus on trace-constrained SDPs, where\nthe decision variables are positive semidefinite (PSD) matrices with fixed\ntrace values. These additional constraints not only capture important\nstructural properties but also facilitate first-order methods for recovering\nrank-1 solutions. We introduce customized fixed-trace variables and constraints\nto represent common robotic quantities such as rotations and translations,\nwhich can be exactly recovered when the corresponding variables are rank-1. To\nfurther improve practical performance, we develop a gradient-based refinement\nprocedure that projects relaxed SDP solutions toward rank-1, low-cost\ncandidates, which can then be certified for global optimality via the dual\nproblem. We demonstrate that many robotics tasks can be expressed within this\ntrace-constrained SDP framework, and showcase its effectiveness through\nsimulations in perspective-n-point (PnP) estimation, hand-eye calibration, and\ndual-robot system calibration. To support broader use, we also introduce a\nmodular ``virtual robot'' abstraction that simplifies modeling across different\nproblem settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ff9\u7ea6\u675f\u534a\u5b9a\u89c4\u5212\u7684\u673a\u5668\u4eba\u95ee\u9898\u6c42\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u56fa\u5b9a\u8ff9\u7ea6\u675f\u548c\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u6062\u590d\u79e91\u89e3\uff0c\u5e76\u5728\u591a\u4e2a\u673a\u5668\u4eba\u6807\u5b9a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u975e\u51f8\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u534a\u5b9a\u89c4\u5212\u677e\u5f1b\u4e3a\u51f8\u95ee\u9898\uff0c\u4f46\u5b9e\u9645\u89e3\u7684\u8d28\u91cf\u53d6\u51b3\u4e8e\u80fd\u5426\u83b7\u5f97\u79e91\u77e9\u9635\u3002\u8ff9\u7ea6\u675f\u4e0d\u4ec5\u6355\u6349\u7ed3\u6784\u7279\u6027\uff0c\u8fd8\u4fbf\u4e8e\u4f7f\u7528\u4e00\u9636\u65b9\u6cd5\u6062\u590d\u79e91\u89e3\u3002", "method": "\u5f15\u5165\u5b9a\u5236\u5316\u7684\u56fa\u5b9a\u8ff9\u53d8\u91cf\u548c\u7ea6\u675f\u6765\u8868\u793a\u65cb\u8f6c\u548c\u5e73\u79fb\u7b49\u673a\u5668\u4eba\u91cf\uff0c\u5f00\u53d1\u57fa\u4e8e\u68af\u5ea6\u7684\u7cbe\u70bc\u8fc7\u7a0b\u5c06\u677e\u5f1b\u89e3\u6295\u5f71\u5230\u79e91\u5019\u9009\u89e3\uff0c\u5e76\u901a\u8fc7\u5bf9\u5076\u95ee\u9898\u9a8c\u8bc1\u5168\u5c40\u6700\u4f18\u6027\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5e94\u7528\u4e8ePnP\u4f30\u8ba1\u3001\u624b\u773c\u6807\u5b9a\u548c\u53cc\u673a\u5668\u4eba\u7cfb\u7edf\u6807\u5b9a\u7b49\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8ff9\u7ea6\u675fSDP\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7684\"\u865a\u62df\u673a\u5668\u4eba\"\u62bd\u8c61\u7b80\u5316\u4e86\u4e0d\u540c\u95ee\u9898\u573a\u666f\u7684\u5efa\u6a21\u3002"}}
{"id": "2509.23705", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23705", "abs": "https://arxiv.org/abs/2509.23705", "authors": ["Jun Chen", "Mingjia Chen", "Shinkyu Park"], "title": "MDCPP: Multi-robot Dynamic Coverage Path Planning for Workload Adaptation", "comment": null, "summary": "Multi-robot Coverage Path Planning (MCPP) addresses the problem of computing\npaths for multiple robots to effectively cover a large area of interest.\nConventional approaches to MCPP typically assume that robots move at fixed\nvelocities, which is often unrealistic in real-world applications where robots\nmust adapt their speeds based on the specific coverage tasks assigned to\nthem.Consequently, conventional approaches often lead to imbalanced workload\ndistribution among robots and increased completion time for coverage tasks. To\naddress this, we introduce a novel Multi-robot Dynamic Coverage Path Planning\n(MDCPP) algorithm for complete coverage in two-dimensional environments. MDCPP\ndynamically estimates each robot's remaining workload by approximating the\ntarget distribution with Gaussian mixture models, and assigns coverage regions\nusing a capacity-constrained Voronoi diagram. We further develop a distributed\nimplementation of MDCPP for range-constrained robotic networks. Simulation\nresults validate the efficacy of MDCPP, showing qualitative improvements and\nsuperior performance compared to an existing sweeping algorithm, and a\nquantifiable impact of communication range on coverage efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u52a8\u6001\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4f30\u8ba1\u673a\u5668\u4eba\u5de5\u4f5c\u8d1f\u8f7d\u548c\u4f7f\u7528\u5bb9\u91cf\u7ea6\u675fVoronoi\u56fe\u6765\u5206\u914d\u8986\u76d6\u533a\u57df\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56fa\u5b9a\u901f\u5ea6\u5047\u8bbe\u5bfc\u81f4\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u591a\u673a\u5668\u4eba\u8986\u76d6\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5047\u8bbe\u673a\u5668\u4eba\u4ee5\u56fa\u5b9a\u901f\u5ea6\u79fb\u52a8\uff0c\u8fd9\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u4e0d\u5207\u5b9e\u9645\uff0c\u5bfc\u81f4\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u4e0d\u5747\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u589e\u52a0\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8fd1\u4f3c\u76ee\u6807\u5206\u5e03\u6765\u52a8\u6001\u4f30\u8ba1\u6bcf\u4e2a\u673a\u5668\u4eba\u7684\u5269\u4f59\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u91c7\u7528\u5bb9\u91cf\u7ea6\u675fVoronoi\u56fe\u5206\u914d\u8986\u76d6\u533a\u57df\uff0c\u8fd8\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u5b9e\u73b0\u7528\u4e8e\u901a\u4fe1\u8303\u56f4\u53d7\u9650\u7684\u673a\u5668\u4eba\u7f51\u7edc\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660eMDCPP\u7b97\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u626b\u63cf\u7b97\u6cd5\uff0c\u5e76\u91cf\u5316\u4e86\u901a\u4fe1\u8303\u56f4\u5bf9\u8986\u76d6\u6548\u7387\u7684\u5f71\u54cd\u3002", "conclusion": "MDCPP\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u8986\u76d6\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u52a8\u6001\u8d1f\u8f7d\u5206\u914d\u95ee\u9898\uff0c\u63d0\u9ad8\u8986\u76d6\u6548\u7387\u3002"}}
{"id": "2509.23721", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23721", "abs": "https://arxiv.org/abs/2509.23721", "authors": ["Chi Chu", "Huazhe Xu"], "title": "DA-MMP: Learning Coordinated and Accurate Throwing with Dynamics-Aware Motion Manifold Primitives", "comment": null, "summary": "Dynamic manipulation is a key capability for advancing robot performance,\nenabling skills such as tossing. While recent learning-based approaches have\npushed the field forward, most methods still rely on manually designed action\nparameterizations, limiting their ability to produce the highly coordinated\nmotions required in complex tasks. Motion planning can generate feasible\ntrajectories, but the dynamics gap-stemming from control inaccuracies, contact\nuncertainties, and aerodynamic effects-often causes large deviations between\nplanned and executed trajectories. In this work, we propose Dynamics-Aware\nMotion Manifold Primitives (DA-MMP), a motion generation framework for\ngoal-conditioned dynamic manipulation, and instantiate it on a challenging\nreal-world ring-tossing task. Our approach extends motion manifold primitives\nto variable-length trajectories through a compact parametrization and learns a\nhigh-quality manifold from a large-scale dataset of planned motions. Building\non this manifold, a conditional flow matching model is trained in the latent\nspace with a small set of real-world trials, enabling the generation of\nthrowing trajectories that account for execution dynamics. Experiments show\nthat our method can generate coordinated and smooth motion trajectories for the\nring-tossing task. In real-world evaluations, it achieves high success rates\nand even surpasses the performance of trained human experts. Moreover, it\ngeneralizes to novel targets beyond the training range, indicating that it\nsuccessfully learns the underlying trajectory-dynamics mapping.", "AI": {"tldr": "\u63d0\u51fa\u4e86Dynamics-Aware Motion Manifold Primitives (DA-MMP)\u6846\u67b6\uff0c\u7528\u4e8e\u76ee\u6807\u6761\u4ef6\u52a8\u6001\u64cd\u4f5c\uff0c\u5728\u73af\u629b\u4efb\u52a1\u4e2d\u901a\u8fc7\u7d27\u51d1\u53c2\u6570\u5316\u6269\u5c55\u8fd0\u52a8\u6d41\u5f62\u539f\u8bed\uff0c\u7ed3\u5408\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\u751f\u6210\u8003\u8651\u6267\u884c\u52a8\u529b\u5b66\u7684\u6295\u63b7\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u52a8\u4f5c\u53c2\u6570\u5316\uff0c\u96be\u4ee5\u4ea7\u751f\u590d\u6742\u4efb\u52a1\u6240\u9700\u7684\u9ad8\u5ea6\u534f\u8c03\u8fd0\u52a8\uff1b\u8fd0\u52a8\u89c4\u5212\u5b58\u5728\u52a8\u529b\u5b66\u5dee\u8ddd\u95ee\u9898\uff0c\u5bfc\u81f4\u89c4\u5212\u8f68\u8ff9\u4e0e\u5b9e\u9645\u6267\u884c\u8f68\u8ff9\u504f\u5dee\u8f83\u5927\u3002", "method": "\u6269\u5c55\u8fd0\u52a8\u6d41\u5f62\u539f\u8bed\u652f\u6301\u53d8\u957f\u8f68\u8ff9\uff0c\u4ece\u5927\u89c4\u6a21\u89c4\u5212\u8fd0\u52a8\u6570\u636e\u5b66\u4e60\u9ad8\u8d28\u91cf\u6d41\u5f62\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u8bad\u7ec3\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\uff0c\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u6570\u636e\u751f\u6210\u8003\u8651\u6267\u884c\u52a8\u529b\u5b66\u7684\u6295\u63b7\u8f68\u8ff9\u3002", "result": "\u5728\u73af\u629b\u4efb\u52a1\u4e2d\u751f\u6210\u534f\u8c03\u5e73\u6ed1\u7684\u8fd0\u52a8\u8f68\u8ff9\uff0c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u8fbe\u5230\u9ad8\u6210\u529f\u7387\uff0c\u751a\u81f3\u8d85\u8d8a\u8bad\u7ec3\u6709\u7d20\u7684\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u8bad\u7ec3\u8303\u56f4\u4e4b\u5916\u7684\u65b0\u76ee\u6807\u3002", "conclusion": "DA-MMP\u6210\u529f\u5b66\u4e60\u4e86\u8f68\u8ff9-\u52a8\u529b\u5b66\u6620\u5c04\uff0c\u4e3a\u52a8\u6001\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u5728\u590d\u6742\u52a8\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.23745", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23745", "abs": "https://arxiv.org/abs/2509.23745", "authors": ["Min Liu", "Deepak Pathak", "Ananye Agarwal"], "title": "LocoFormer: Generalist Locomotion via Long-context Adaptation", "comment": "Accepted to CoRL 2025", "summary": "Modern locomotion controllers are manually tuned for specific embodiments. We\npresent LocoFormer, a generalist omni-bodied locomotion model that can control\npreviously unseen legged and wheeled robots, even without precise knowledge of\ntheir kinematics. LocoFormer is able to adapt to changes in morphology and\ndynamics at test time. We find that two key choices enable adaptation. First,\nwe train massive scale RL on procedurally generated robots with aggressive\ndomain randomization. Second, in contrast to previous policies that are myopic\nwith short context lengths, we extend context by orders of magnitude to span\nepisode boundaries. We deploy the same LocoFormer to varied robots and show\nrobust control even with large disturbances such as weight change and motor\nfailures. In extreme scenarios, we see emergent adaptation across episodes,\nLocoFormer learns from falls in early episodes to improve control strategies in\nlater ones. We believe that this simple, yet general recipe can be used to\ntrain foundation models for other robotic skills in the future. Videos at\ngeneralist-locomotion.github.io.", "AI": {"tldr": "LocoFormer\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\u6a21\u578b\uff0c\u80fd\u591f\u63a7\u5236\u672a\u89c1\u8fc7\u7684\u817f\u5f0f\u548c\u8f6e\u5f0f\u673a\u5668\u4eba\uff0c\u65e0\u9700\u7cbe\u786e\u7684\u8fd0\u52a8\u5b66\u77e5\u8bc6\uff0c\u5e76\u80fd\u9002\u5e94\u5f62\u6001\u548c\u52a8\u6001\u53d8\u5316\u3002", "motivation": "\u73b0\u4ee3\u8fd0\u52a8\u63a7\u5236\u5668\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u624b\u52a8\u8c03\u4f18\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u63a7\u5236\u5404\u79cd\u4e0d\u540c\u5f62\u6001\u673a\u5668\u4eba\u7684\u901a\u7528\u8fd0\u52a8\u6a21\u578b\u3002", "method": "1. \u5728\u7a0b\u5e8f\u751f\u6210\u7684\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff1b2. \u4f7f\u7528\u6fc0\u8fdb\u7684\u9886\u57df\u968f\u673a\u5316\uff1b3. \u5927\u5e45\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u8de8\u8d8aepisode\u8fb9\u754c\u3002", "result": "LocoFormer\u80fd\u591f\u90e8\u7f72\u5230\u5404\u79cd\u673a\u5668\u4eba\u4e0a\uff0c\u5373\u4f7f\u5728\u91cd\u91cf\u53d8\u5316\u548c\u7535\u673a\u6545\u969c\u7b49\u5927\u6270\u52a8\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9c81\u68d2\u63a7\u5236\u3002\u5728\u6781\u7aef\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u80fd\u591f\u8de8episode\u5b66\u4e60\uff0c\u4ece\u65e9\u671f\u8dcc\u5012\u4e2d\u5b66\u4e60\u6539\u8fdb\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "\u8fd9\u79cd\u7b80\u5355\u800c\u901a\u7528\u7684\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3\u5176\u4ed6\u673a\u5668\u4eba\u6280\u80fd\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23778", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.23778", "abs": "https://arxiv.org/abs/2509.23778", "authors": ["Zeyuan Zhang", "Chaoran Li", "Shao Zhang", "Ying Wen"], "title": "Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse", "comment": "Preprint Under Review", "summary": "Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of\nMulti-Agent Path Finding (MAPF), where agents are required to sequentially\ncomplete tasks with fixed-location pickup and delivery demands. Although\nlearning-based methods have made progress in MAPD, they often perform poorly in\nwarehouse-like environments with narrow pathways and long corridors when\nrelying only on local observations for distributed decision-making.\nCommunication learning can alleviate the lack of global information but\nintroduce high computational complexity due to point-to-point communication. To\naddress this challenge, we formulate MAPF as a sequence modeling problem and\nprove that path-finding policies under sequence modeling possess\norder-invariant optimality, ensuring its effectiveness in MAPD. Building on\nthis, we propose the Sequential Pathfinder (SePar), which leverages the\nTransformer paradigm to achieve implicit information exchange, reducing\ndecision-making complexity from exponential to linear while maintaining\nefficiency and global awareness. Experiments demonstrate that SePar\nconsistently outperforms existing learning-based methods across various MAPF\ntasks and their variants, and generalizes well to unseen environments.\nFurthermore, we highlight the necessity of integrating imitation learning in\ncomplex maps like warehouses.", "AI": {"tldr": "\u63d0\u51fa\u4e86Sequential Pathfinder (SePar)\u65b9\u6cd5\uff0c\u4f7f\u7528Transformer\u8303\u5f0f\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u62fe\u53d6\u914d\u9001\u95ee\u9898\uff0c\u901a\u8fc7\u5e8f\u5217\u5efa\u6a21\u5b9e\u73b0\u9690\u5f0f\u4fe1\u606f\u4ea4\u6362\uff0c\u5c06\u51b3\u7b56\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u7ebf\u6027\u7ea7\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u4ed3\u5e93\u7b49\u72ed\u7a84\u73af\u5883\u4e2d\u4ec5\u4f9d\u8d56\u5c40\u90e8\u89c2\u6d4b\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u901a\u4fe1\u5b66\u4e60\u867d\u7136\u80fd\u7f13\u89e3\u5168\u5c40\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff0c\u4f46\u70b9\u5bf9\u70b9\u901a\u4fe1\u5e26\u6765\u4e86\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u5c06MAPF\u5efa\u6a21\u4e3a\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\uff0c\u5229\u7528Transformer\u67b6\u6784\u5b9e\u73b0\u9690\u5f0f\u4fe1\u606f\u4ea4\u6362\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u5904\u7406\u590d\u6742\u5730\u56fe\u73af\u5883\u3002", "result": "SePar\u5728\u5404\u79cdMAPF\u4efb\u52a1\u53ca\u5176\u53d8\u4f53\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u4e3aMAPD\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u8def\u5f84\u89c4\u5212\u7b56\u7565\u5728\u5e8f\u5217\u5efa\u6a21\u4e0b\u5177\u6709\u987a\u5e8f\u4e0d\u53d8\u7684\u6700\u4f18\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u5730\u56fe\u4e2d\u96c6\u6210\u6a21\u4eff\u5b66\u4e60\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.23801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23801", "abs": "https://arxiv.org/abs/2509.23801", "authors": ["Shuning Zhang", "Renjing Xu", "Zhanchen Zhu", "Xiangyu Chen", "Yunheng Wang", "Xu Jiang", "Peibo Duan"], "title": "High-Precision Climbing Robot Localization Using Planar Array UWB/GPS/IMU/Barometer Integration", "comment": null, "summary": "To address the need for high-precision localization of climbing robots in\ncomplex high-altitude environments, this paper proposes a multi-sensor fusion\nsystem that overcomes the limitations of single-sensor approaches. Firstly, the\nlocalization scenarios and the problem model are analyzed. An integrated\narchitecture of Attention Mechanism-based Fusion Algorithm (AMFA) incorporating\nplanar array Ultra-Wideband (UWB), GPS, Inertial Measurement Unit (IMU), and\nbarometer is designed to handle challenges such as GPS occlusion and UWB\nNon-Line-of-Sight (NLOS) problem. Then, End-to-end neural network inference\nmodels for UWB and barometer are developed, along with a multimodal attention\nmechanism for adaptive data fusion. An Unscented Kalman Filter (UKF) is applied\nto refine the trajectory, improving accuracy and robustness. Finally,\nreal-world experiments show that the method achieves 0.48 m localization\naccuracy and lower MAX error of 1.50 m, outperforming baseline algorithms such\nas GPS/INS-EKF and demonstrating stronger robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\uff0c\u7528\u4e8e\u590d\u6742\u9ad8\u7a7a\u73af\u5883\u4e2d\u722c\u58c1\u673a\u5668\u4eba\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\uff0c\u901a\u8fc7\u878d\u5408UWB\u3001GPS\u3001IMU\u548c\u6c14\u538b\u8ba1\u6570\u636e\uff0c\u89e3\u51b3\u4e86GPS\u906e\u6321\u548cUWB\u975e\u89c6\u8ddd\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u9ad8\u7a7a\u73af\u5883\u4e0b\u722c\u58c1\u673a\u5668\u4eba\u5b9a\u4f4d\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u514b\u670d\u5355\u4f20\u611f\u5668\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728GPS\u4fe1\u53f7\u906e\u6321\u548cUWB\u975e\u89c6\u8ddd\u4f20\u64ad\u7b49\u6311\u6218\u6027\u573a\u666f\u4e0b\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u878d\u5408\u7b97\u6cd5(AMFA)\uff0c\u7ed3\u5408\u5e73\u9762\u9635\u5217UWB\u3001GPS\u3001IMU\u548c\u6c14\u538b\u8ba1\uff1b\u5f00\u53d1\u4e86UWB\u548c\u6c14\u538b\u8ba1\u7684\u7aef\u5230\u7aef\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u6a21\u578b\uff1b\u91c7\u7528\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u81ea\u9002\u5e94\u6570\u636e\u878d\u5408\uff1b\u5e94\u7528\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(UKF)\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e860.48\u7c73\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u6700\u5927\u8bef\u5dee\u4e3a1.50\u7c73\uff0c\u4f18\u4e8eGPS/INS-EKF\u7b49\u57fa\u7ebf\u7b97\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u5728\u590d\u6742\u9ad8\u7a7a\u73af\u5883\u4e2d\u6709\u6548\u63d0\u9ad8\u4e86\u722c\u58c1\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7c7b\u4f3c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23821", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23821", "abs": "https://arxiv.org/abs/2509.23821", "authors": ["Federico Pablo-Marti", "Carlos Mir Fernandez"], "title": "Fostering Robots: A Governance-First Conceptual Framework for Domestic, Curriculum-Based Trajectory Collection", "comment": "7 pages, 2 figures", "summary": "We propose a conceptual, empirically testable framework for Robot Fostering,\n-a curriculum-driven, governance-first approach to domestic robot deployments,\nemphasizing long-term, curated interaction trajectories. We formalize\ntrajectory quality with quantifiable metrics and evaluation protocols aligned\nwith EU-grade governance standards, delineating a low-resource empirical\nroadmap to enable rigorous validation through future pilot studies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u57f9\u80b2\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u4ee5\u8bfe\u7a0b\u9a71\u52a8\u3001\u6cbb\u7406\u4f18\u5148\u7684\u5bb6\u5ead\u673a\u5668\u4eba\u90e8\u7f72\u65b9\u6cd5\uff0c\u5f3a\u8c03\u957f\u671f\u7684\u3001\u7cbe\u5fc3\u7b56\u5212\u7684\u4ea4\u4e92\u8f68\u8ff9\u3002", "motivation": "\u4e3a\u5bb6\u5ead\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e00\u4e2a\u6982\u5ff5\u6027\u548c\u7ecf\u9a8c\u53ef\u6d4b\u8bd5\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u957f\u671f\u4ea4\u4e92\u8f68\u8ff9\u7684\u8d28\u91cf\u548c\u6cbb\u7406\u6807\u51c6\u3002", "method": "\u5f62\u5f0f\u5316\u8f68\u8ff9\u8d28\u91cf\uff0c\u4f7f\u7528\u53ef\u91cf\u5316\u7684\u6307\u6807\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u7b26\u5408\u6b27\u76df\u7ea7\u6cbb\u7406\u6807\u51c6\uff0c\u5e76\u5236\u5b9a\u4f4e\u8d44\u6e90\u7ecf\u9a8c\u8def\u7ebf\u56fe\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u672a\u6765\u7684\u8bd5\u70b9\u7814\u7a76\u8fdb\u884c\u4e25\u683c\u9a8c\u8bc1\u3002", "conclusion": "\u673a\u5668\u4eba\u57f9\u80b2\u6846\u67b6\u4e3a\u5bb6\u5ead\u673a\u5668\u4eba\u7684\u957f\u671f\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u8def\u5f84\uff0c\u5f3a\u8c03\u6cbb\u7406\u548c\u4ea4\u4e92\u8d28\u91cf\u3002"}}
{"id": "2509.23823", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23823", "abs": "https://arxiv.org/abs/2509.23823", "authors": ["Tian Nian", "Weijie Ke", "Yao Mu", "Tianxing Chen", "Shaolong Zhu", "Bingshan Hu"], "title": "Control Your Robot: A Unified System for Robot Control and Policy Deployment", "comment": "Code: https://github.com/Tian-Nian/control_your_robot", "summary": "Cross-platform robot control remains difficult because hardware interfaces,\ndata formats, and control paradigms vary widely, which fragments toolchains and\nslows deployment. To address this, we present Control Your Robot, a modular,\ngeneral-purpose framework that unifies data collection and policy deployment\nacross diverse platforms. The system reduces fragmentation through a\nstandardized workflow with modular design, unified APIs, and a closed-loop\narchitecture. It supports flexible robot registration, dual-mode control with\nteleoperation and trajectory playback, and seamless integration from multimodal\ndata acquisition to inference. Experiments on single-arm and dual-arm systems\nshow efficient, low-latency data collection and effective support for policy\nlearning with imitation learning and vision-language-action models. Policies\ntrained on data gathered by Control Your Robot match expert demonstrations\nclosely, indicating that the framework enables scalable and reproducible robot\nlearning across platforms.", "AI": {"tldr": "Control Your Robot\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u5de5\u4f5c\u6d41\u3001\u7edf\u4e00API\u548c\u95ed\u73af\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u8de8\u5e73\u53f0\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u786c\u4ef6\u63a5\u53e3\u3001\u6570\u636e\u683c\u5f0f\u548c\u63a7\u5236\u8303\u5f0f\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u8de8\u5e73\u53f0\u673a\u5668\u4eba\u63a7\u5236\u9762\u4e34\u786c\u4ef6\u63a5\u53e3\u3001\u6570\u636e\u683c\u5f0f\u548c\u63a7\u5236\u8303\u5f0f\u5dee\u5f02\u5927\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5de5\u5177\u94fe\u788e\u7247\u5316\uff0c\u90e8\u7f72\u901f\u5ea6\u6162\u3002\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u7b80\u5316\u6570\u636e\u6536\u96c6\u548c\u7b56\u7565\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u7edf\u4e00API\u548c\u95ed\u73af\u67b6\u6784\uff0c\u652f\u6301\u7075\u6d3b\u7684\u673a\u5668\u4eba\u6ce8\u518c\u3001\u53cc\u6a21\u5f0f\u63a7\u5236\uff08\u9065\u64cd\u4f5c\u548c\u8f68\u8ff9\u56de\u653e\uff09\uff0c\u4ee5\u53ca\u4ece\u591a\u6a21\u6001\u6570\u636e\u91c7\u96c6\u5230\u63a8\u7406\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u5355\u81c2\u548c\u53cc\u81c2\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u5730\u6536\u96c6\u6570\u636e\uff0c\u5e76\u6709\u6548\u652f\u6301\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "\u4f7f\u7528Control Your Robot\u6536\u96c6\u7684\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\u4e0e\u4e13\u5bb6\u6f14\u793a\u9ad8\u5ea6\u5339\u914d\uff0c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u8de8\u5e73\u53f0\u7684\u53ef\u6269\u5c55\u548c\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u5b66\u4e60\u3002"}}
{"id": "2509.23829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23829", "abs": "https://arxiv.org/abs/2509.23829", "authors": ["Kefei Zhu", "Fengshuo Bai", "YuanHao Xiang", "Yishuai Cai", "Xinglin Chen", "Ruochong Li", "Xingtao Wang", "Hao Dong", "Yaodong Yang", "Xiaopeng Fan", "Yuanpei Chen"], "title": "DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation", "comment": "NeurIPS 2025, Spotlight", "summary": "Dexterous manipulation is critical for advancing robot capabilities in\nreal-world applications, yet diverse and high-quality datasets remain scarce.\nExisting data collection methods either rely on human teleoperation or require\nsignificant human engineering, or generate data with limited diversity, which\nrestricts their scalability and generalization. In this paper, we introduce\nDexFlyWheel, a scalable data generation framework that employs a self-improving\ncycle to continuously enrich data diversity. Starting from efficient seed\ndemonstrations warmup, DexFlyWheel expands the dataset through iterative\ncycles. Each cycle follows a closed-loop pipeline that integrates Imitation\nLearning (IL), residual Reinforcement Learning (RL), rollout trajectory\ncollection, and data augmentation. Specifically, IL extracts human-like\nbehaviors from demonstrations, and residual RL enhances policy generalization.\nThe learned policy is then used to generate trajectories in simulation, which\nare further augmented across diverse environments and spatial configurations\nbefore being fed back into the next cycle. Over successive iterations, a\nself-improving data flywheel effect emerges, producing datasets that cover\ndiverse scenarios and thereby scaling policy performance. Experimental results\ndemonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across\nfour challenging tasks. Policies trained on our dataset achieve an average\nsuccess rate of 81.9\\% on the challenge test sets and successfully transfer to\nthe real world through digital twin, achieving a 78.3\\% success rate on\ndual-arm lift tasks.", "AI": {"tldr": "DexFlyWheel\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7075\u5de7\u64cd\u4f5c\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6539\u8fdb\u5faa\u73af\u4e0d\u65ad\u4e30\u5bcc\u6570\u636e\u591a\u6837\u6027\uff0c\u5728\u56db\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u751f\u62102000\u591a\u4e2a\u591a\u6837\u5316\u6f14\u793a\uff0c\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u8fbe\u523078.3%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u7075\u5de7\u64cd\u4f5c\u5bf9\u63d0\u5347\u673a\u5668\u4eba\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u9065\u64cd\u4f5c\u6216\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5de5\u7a0b\uff0c\u751f\u6210\u7684\u6570\u636e\u591a\u6837\u6027\u6709\u9650\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u81ea\u6539\u8fdb\u5faa\u73af\u6846\u67b6\uff1a\u4ece\u79cd\u5b50\u6f14\u793a\u5f00\u59cb\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u63d0\u53d6\u4eba\u7c7b\u884c\u4e3a\uff0c\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7b56\u7565\u6cdb\u5316\uff0c\u5728\u4eff\u771f\u4e2d\u751f\u6210\u8f68\u8ff9\uff0c\u8fdb\u884c\u73af\u5883\u548c\u7a7a\u95f4\u914d\u7f6e\u7684\u6570\u636e\u589e\u5f3a\uff0c\u5f62\u6210\u6570\u636e\u98de\u8f6e\u6548\u5e94\u3002", "result": "\u751f\u6210\u8d85\u8fc72000\u4e2a\u591a\u6837\u5316\u6f14\u793a\uff0c\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u6311\u6218\u6d4b\u8bd5\u96c6\u4e0a\u5e73\u5747\u6210\u529f\u738781.9%\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\uff0c\u5728\u53cc\u81c2\u63d0\u5347\u4efb\u52a1\u4e2d\u8fbe\u523078.3%\u6210\u529f\u7387\u3002", "conclusion": "DexFlyWheel\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u591a\u6837\u5316\u7075\u5de7\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u7b56\u7565\u6027\u80fd\u5e76\u6210\u529f\u5b9e\u73b0\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u8fc1\u79fb\u3002"}}
{"id": "2509.23960", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23960", "abs": "https://arxiv.org/abs/2509.23960", "authors": ["Manan Tayal", "Aditya Singh", "Shishir Kolathaya", "Somil Bansal"], "title": "MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control", "comment": "9 Pages, 4 Figures, 3 Tables. First two authors have contributed\n  equally", "summary": "Co-optimizing safety and performance in large-scale multi-agent systems\nremains a fundamental challenge. Existing approaches based on multi-agent\nreinforcement learning (MARL), safety filtering, or Model Predictive Control\n(MPC) either lack strict safety guarantees, suffer from conservatism, or fail\nto scale effectively. We propose MAD-PINN, a decentralized physics-informed\nmachine learning framework for solving the multi-agent state-constrained\noptimal control problem (MASC-OCP). Our method leverages an epigraph-based\nreformulation of SC-OCP to simultaneously capture performance and safety, and\napproximates its solution via a physics-informed neural network. Scalability is\nachieved by training the SC-OCP value function on reduced-agent systems and\ndeploying them in a decentralized fashion, where each agent relies only on\nlocal observations of its neighbours for decision-making. To further enhance\nsafety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based\nneighbour selection strategy to prioritize safety-critical interactions, and a\nreceding-horizon policy execution scheme that adapts to dynamic interactions\nwhile reducing computational burden. Experiments on multi-agent navigation\ntasks demonstrate that MAD-PINN achieves superior safety-performance\ntrade-offs, maintains scalability as the number of agents grows, and\nconsistently outperforms state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51faMAD-PINN\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u72b6\u6001\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u5b89\u5168\u4e0e\u6027\u80fd\u7684\u534f\u540c\u4f18\u5316\u3002", "motivation": "\u73b0\u6709MARL\u3001\u5b89\u5168\u8fc7\u6ee4\u6216MPC\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u4e25\u683c\u5b89\u5168\u4fdd\u8bc1\uff0c\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\u6216\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5916\u5ef6\u7684\u91cd\u6784\u65b9\u6cd5\uff0c\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3cSC-OCP\u503c\u51fd\u6570\uff0c\u7ed3\u5408HJ\u53ef\u8fbe\u6027\u90bb\u5c45\u9009\u62e9\u7b56\u7565\u548c\u6eda\u52a8\u65f6\u57df\u7b56\u7565\u6267\u884c\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cMAD-PINN\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5b89\u5168-\u6027\u80fd\u6743\u8861\uff0c\u4fdd\u6301\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "MAD-PINN\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u5b89\u5168\u4e0e\u6027\u80fd\u534f\u540c\u4f18\u5316\u7684\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\u3002"}}
{"id": "2509.24094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24094", "abs": "https://arxiv.org/abs/2509.24094", "authors": ["Vignesh Ramanathan", "Michael Milford", "Tobias Fischer"], "title": "Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras", "comment": null, "summary": "Visual Place Recognition (VPR) enables systems to identify previously visited\nlocations within a map, a fundamental task for autonomous navigation. Prior\nworks have developed VPR solutions using event cameras, which asynchronously\nmeasure per-pixel brightness changes with microsecond temporal resolution.\nHowever, these approaches rely on dense representations of the inherently\nsparse camera output and require tens to hundreds of milliseconds of event data\nto predict a place. Here, we break this paradigm with Flash, a lightweight VPR\nsystem that predicts places using sub-millisecond slices of event data. Our\nmethod is based on the observation that active pixel locations provide strong\ndiscriminative features for VPR. Flash encodes these active pixel locations\nusing efficient binary frames and computes similarities via fast bitwise\noperations, which are then normalized based on the relative event activity in\nthe query and reference frames. Flash improves Recall@1 for sub-millisecond VPR\nover existing baselines by 11.33x on the indoor QCR-Event-Dataset and 5.92x on\nthe 8 km Brisbane-Event-VPR dataset. Moreover, our approach reduces the\nduration for which the robot must operate without awareness of its position, as\nevidenced by a localization latency metric we term Time to Correct Match (TCM).\nTo the best of our knowledge, this is the first work to demonstrate\nsub-millisecond VPR using event cameras.", "AI": {"tldr": "Flash\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e9a\u6beb\u79d2\u7ea7\u4e8b\u4ef6\u6570\u636e\u7247\u6bb5\u8fdb\u884c\u4f4d\u7f6e\u9884\u6d4b\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u5e27\u7f16\u7801\u6d3b\u8dc3\u50cf\u7d20\u4f4d\u7f6e\u5e76\u5229\u7528\u5feb\u901f\u4f4d\u8fd0\u7b97\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684VPR\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u8868\u793a\u4e14\u9700\u8981\u6570\u5341\u5230\u6570\u767e\u6beb\u79d2\u7684\u4e8b\u4ef6\u6570\u636e\uff0cFlash\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e9a\u6beb\u79d2\u7ea7\u7684\u4f4d\u7f6e\u8bc6\u522b\u3002", "method": "\u57fa\u4e8e\u6d3b\u8dc3\u50cf\u7d20\u4f4d\u7f6e\u63d0\u4f9b\u5f3a\u5224\u522b\u7279\u5f81\u7684\u89c2\u5bdf\uff0cFlash\u4f7f\u7528\u9ad8\u6548\u7684\u4e8c\u8fdb\u5236\u5e27\u7f16\u7801\u6d3b\u8dc3\u50cf\u7d20\u4f4d\u7f6e\uff0c\u901a\u8fc7\u5feb\u901f\u4f4d\u8fd0\u7b97\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u5e76\u6839\u636e\u67e5\u8be2\u5e27\u548c\u53c2\u8003\u5e27\u7684\u76f8\u5bf9\u4e8b\u4ef6\u6d3b\u52a8\u8fdb\u884c\u5f52\u4e00\u5316\u3002", "result": "\u5728\u5ba4\u5185QCR-Event-Dataset\u4e0aRecall@1\u63d0\u534711.33\u500d\uff0c\u57288\u516c\u91ccBrisbane-Event-VPR\u6570\u636e\u96c6\u4e0a\u63d0\u53475.92\u500d\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u65e0\u4f4d\u7f6e\u611f\u77e5\u7684\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "Flash\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u4e9a\u6beb\u79d2\u7ea7\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u901f\u5ea6\u548c\u7cbe\u5ea6\uff0c\u964d\u4f4e\u4e86\u5b9a\u4f4d\u5ef6\u8fdf\u3002"}}
{"id": "2509.24124", "categories": ["cs.RO", "cs.AI", "cs.LG", "F.2.2; G.3; I.5.3; F.2.2; I.2.9; G.3; I.5.3"], "pdf": "https://arxiv.org/pdf/2509.24124", "abs": "https://arxiv.org/abs/2509.24124", "authors": ["Ilari Vallivaara", "Bingnan Duan", "Yinhuan Dong", "Tughrul Arslan"], "title": "Ancestry Tree Clustering for Particle Filter Diversity Maintenance", "comment": "15th International Conference on Indoor Positioning and Indoor\n  Navigation, 15-18 September 2025, Tampere, Finland Originally 8 pages. The\n  online version with appendices is 14 pages", "summary": "We propose a method for linear-time diversity maintenance in particle\nfiltering. It clusters particles based on ancestry tree topology: closely\nrelated particles in sufficiently large subtrees are grouped together. The main\nidea is that the tree structure implicitly encodes similarity without the need\nfor spatial or other domain-specific metrics. This approach, when combined with\nintra-cluster fitness sharing and the protection of particles not included in a\ncluster, effectively prevents premature convergence in multimodal environments\nwhile maintaining estimate compactness. We validate our approach in a\nmultimodal robotics simulation and a real-world multimodal indoor environment.\nWe compare the performance to several diversity maintenance algorithms from the\nliterature, including Deterministic Resampling and Particle Gaussian Mixtures.\nOur algorithm achieves high success rates with little to no negative effect on\ncompactness, showing particular robustness to different domains and challenging\ninitial conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u7956\u5148\u6811\u62d3\u6251\u7ed3\u6784\u7684\u7ebf\u6027\u65f6\u95f4\u591a\u6837\u6027\u7ef4\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u5bc6\u5207\u76f8\u5173\u7684\u7c92\u5b50\u6765\u9632\u6b62\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u8fc7\u65e9\u6536\u655b\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\uff0c\u7c92\u5b50\u6ee4\u6ce2\u5668\u5bb9\u6613\u8fc7\u65e9\u6536\u655b\u5230\u5355\u4e00\u6a21\u5f0f\uff0c\u9700\u8981\u6709\u6548\u7684\u591a\u6837\u6027\u7ef4\u62a4\u673a\u5236\u6765\u4fdd\u6301\u5bf9\u591a\u4e2a\u53ef\u80fd\u72b6\u6001\u7684\u8ddf\u8e2a\u3002", "method": "\u57fa\u4e8e\u7c92\u5b50\u7956\u5148\u6811\u7684\u62d3\u6251\u7ed3\u6784\u8fdb\u884c\u805a\u7c7b\uff0c\u5c06\u5bc6\u5207\u76f8\u5173\u7684\u7c92\u5b50\u5206\u7ec4\uff0c\u7ed3\u5408\u7c07\u5185\u9002\u5e94\u5ea6\u5171\u4eab\u548c\u672a\u805a\u7c7b\u7c92\u5b50\u7684\u4fdd\u62a4\u673a\u5236\u3002", "result": "\u5728\u591a\u6a21\u6001\u673a\u5668\u4eba\u4eff\u771f\u548c\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u786e\u5b9a\u6027\u91cd\u91c7\u6837\u548c\u7c92\u5b50\u9ad8\u65af\u6df7\u5408\u7b49\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u4e14\u5bf9\u7d27\u51d1\u6027\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u548c\u6311\u6218\u6027\u521d\u59cb\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u80fd\u6709\u6548\u9632\u6b62\u8fc7\u65e9\u6536\u655b\u540c\u65f6\u4fdd\u6301\u4f30\u8ba1\u7d27\u51d1\u6027\u3002"}}
{"id": "2509.24126", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24126", "abs": "https://arxiv.org/abs/2509.24126", "authors": ["Athanasios Bacharis", "Konstantinos D. Polyzos", "Georgios B. Giannakis", "Nikolaos Papanikolopoulos"], "title": "BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes", "comment": null, "summary": "Active vision (AV) has been in the spotlight of robotics research due to its\nemergence in numerous applications including agricultural tasks such as\nprecision crop monitoring and autonomous harvesting to list a few. A major AV\nproblem that gained popularity is the 3D reconstruction of targeted\nenvironments using 2D images from diverse viewpoints. While collecting and\nprocessing a large number of arbitrarily captured 2D images can be arduous in\nmany practical scenarios, a more efficient solution involves optimizing the\nplacement of available cameras in 3D space to capture fewer, yet more\ninformative, images that provide sufficient visual information for effective\nreconstruction of the environment of interest. This process termed as view\nplanning (VP), can be markedly challenged (i) by noise emerging in the location\nof the cameras and/or in the extracted images, and (ii) by the need to\ngeneralize well in other unknown similar agricultural environments without need\nfor re-optimizing or re-training. To cope with these challenges, the present\nwork presents a novel VP framework that considers a reconstruction\nquality-based optimization formulation that relies on the notion of\n`structure-from-motion' to reconstruct the 3D structure of the sought\nenvironment from the selected 2D images. With no analytic expression of the\noptimization function and with costly function evaluations, a Bayesian\noptimization approach is proposed to efficiently carry out the VP process using\nonly a few function evaluations, while accounting for different noise cases.\nNumerical tests on both simulated and real agricultural settings signify the\nbenefits of the advocated VP approach in efficiently estimating the optimal\ncamera placement to accurately reconstruct 3D environments of interest, and\ngeneralize well on similar unknown environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u89c6\u70b9\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u519c\u4e1a\u73af\u5883\u4e2d\u9ad8\u6548\u9009\u62e9\u6700\u4f18\u76f8\u673a\u4f4d\u7f6e\uff0c\u4ee5\u6700\u5c11\u4f46\u4fe1\u606f\u4e30\u5bcc\u76842D\u56fe\u50cf\u5b9e\u73b0\u51c6\u786e\u76843D\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u4e3b\u52a8\u89c6\u89c9\u4e2d\u76f8\u673a\u4f4d\u7f6e\u566a\u58f0\u548c\u56fe\u50cf\u566a\u58f0\u7684\u6311\u6218\uff0c\u540c\u65f6\u5b9e\u73b0\u5728\u672a\u77e5\u76f8\u4f3c\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u907f\u514d\u91cd\u65b0\u4f18\u5316\u6216\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u91cd\u5efa\u8d28\u91cf\u7684\u4f18\u5316\u516c\u5f0f\uff0c\u5229\u7528'\u8fd0\u52a8\u6062\u590d\u7ed3\u6784'\u6982\u5ff5\u4ece\u9009\u5b9a2D\u56fe\u50cf\u91cd\u5efa3D\u73af\u5883\u3002\u7531\u4e8e\u4f18\u5316\u51fd\u6570\u65e0\u89e3\u6790\u8868\u8fbe\u5f0f\u4e14\u8bc4\u4f30\u6210\u672c\u9ad8\uff0c\u63d0\u51fa\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u51fd\u6570\u8bc4\u4f30\u5373\u53ef\u5b8c\u6210\u89c6\u70b9\u89c4\u5212\u8fc7\u7a0b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u6570\u503c\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4f30\u8ba1\u6700\u4f18\u76f8\u673a\u4f4d\u7f6e\uff0c\u51c6\u786e\u91cd\u5efa3D\u73af\u5883\uff0c\u5e76\u5728\u76f8\u4f3c\u672a\u77e5\u73af\u5883\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u89c6\u70b9\u89c4\u5212\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5904\u7406\u566a\u58f0\u6311\u6218\uff0c\u901a\u8fc7\u5c11\u91cf\u76f8\u673a\u4f4d\u7f6e\u5b9e\u73b0\u51c6\u786e\u76843D\u91cd\u5efa\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u73af\u5883\u76d1\u6d4b\u7b49\u5e94\u7528\u3002"}}
{"id": "2509.24129", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24129", "abs": "https://arxiv.org/abs/2509.24129", "authors": ["Priyanka Mandikal", "Jiaheng Hu", "Shivin Dass", "Sagnik Majumder", "Roberto Mart\u00edn-Mart\u00edn", "Kristen Grauman"], "title": "Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress", "comment": null, "summary": "Most robot manipulation focuses on changing the kinematic state of objects:\npicking, placing, opening, or rotating them. However, a wide range of\nreal-world manipulation tasks involve a different class of object state\nchange--such as mashing, spreading, or slicing--where the object's physical and\nvisual state evolve progressively without necessarily changing its position. We\npresent SPARTA, the first unified framework for the family of object state\nchange manipulation tasks. Our key insight is that these tasks share a common\nstructural pattern: they involve spatially-progressing, object-centric changes\nthat can be represented as regions transitioning from an actionable to a\ntransformed state. Building on this insight, SPARTA integrates spatially\nprogressing object change segmentation maps, a visual skill to perceive\nactionable vs. transformed regions for specific object state change tasks, to\ngenerate a) structured policy observations that strip away appearance\nvariability, and b) dense rewards that capture incremental progress over time.\nThese are leveraged in two SPARTA policy variants: reinforcement learning for\nfine-grained control without demonstrations or simulation; and greedy control\nfor fast, lightweight deployment. We validate SPARTA on a real robot for three\nchallenging tasks across 10 diverse real-world objects, achieving significant\nimprovements in training time and accuracy over sparse rewards and visual\ngoal-conditioned baselines. Our results highlight progress-aware visual\nrepresentations as a versatile foundation for the broader family of object\nstate manipulation tasks. Project website:\nhttps://vision.cs.utexas.edu/projects/sparta-robot", "AI": {"tldr": "SPARTA\u662f\u9996\u4e2a\u7edf\u4e00\u5904\u7406\u7269\u4f53\u72b6\u6001\u53d8\u5316\u64cd\u4f5c\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u6e10\u8fdb\u5f0f\u7269\u4f53\u53d8\u5316\u5206\u5272\u56fe\u6765\u611f\u77e5\u53ef\u64cd\u4f5c\u533a\u57df\u4e0e\u5df2\u53d8\u6362\u533a\u57df\uff0c\u751f\u6210\u7ed3\u6784\u5316\u7b56\u7565\u89c2\u5bdf\u548c\u5bc6\u96c6\u5956\u52b1\uff0c\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u548c\u8d2a\u5fc3\u63a7\u5236\u4e24\u79cd\u7b56\u7565\u53d8\u4f53\u3002", "motivation": "\u5927\u591a\u6570\u673a\u5668\u4eba\u64cd\u4f5c\u5173\u6ce8\u7269\u4f53\u7684\u8fd0\u52a8\u72b6\u6001\u53d8\u5316\uff08\u5982\u6293\u53d6\u3001\u653e\u7f6e\uff09\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u8bb8\u591a\u4efb\u52a1\u6d89\u53ca\u7269\u4f53\u7269\u7406\u548c\u89c6\u89c9\u72b6\u6001\u7684\u6e10\u8fdb\u5f0f\u53d8\u5316\uff08\u5982\u6363\u788e\u3001\u6d82\u62b9\u3001\u5207\u5272\uff09\uff0c\u8fd9\u4e9b\u4efb\u52a1\u7f3a\u4e4f\u7edf\u4e00\u7684\u5904\u7406\u6846\u67b6\u3002", "method": "SPARTA\u57fa\u4e8e\u7a7a\u95f4\u6e10\u8fdb\u5f0f\u7269\u4f53\u53d8\u5316\u5206\u5272\u56fe\uff0c\u5c06\u7269\u4f53\u72b6\u6001\u53d8\u5316\u8868\u793a\u4e3a\u4ece\u53ef\u64cd\u4f5c\u533a\u57df\u5411\u53d8\u6362\u533a\u57df\u7684\u8fc7\u6e21\uff0c\u751f\u6210\u7ed3\u6784\u5316\u7b56\u7565\u89c2\u5bdf\u548c\u5bc6\u96c6\u5956\u52b1\uff0c\u63d0\u4f9b\u5f3a\u5316\u5b66\u4e60\u548c\u8d2a\u5fc3\u63a7\u5236\u4e24\u79cd\u7b56\u7565\u53d8\u4f53\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e863\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u548c10\u79cd\u4e0d\u540c\u771f\u5b9e\u7269\u4f53\uff0c\u76f8\u6bd4\u7a00\u758f\u5956\u52b1\u548c\u89c6\u89c9\u76ee\u6807\u6761\u4ef6\u57fa\u7ebf\uff0c\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8e\u8fdb\u5ea6\u611f\u77e5\u7684\u89c6\u89c9\u8868\u793a\u662f\u5904\u7406\u66f4\u5e7f\u6cdb\u7269\u4f53\u72b6\u6001\u64cd\u4f5c\u4efb\u52a1\u7684\u591a\u529f\u80fd\u57fa\u7840\uff0cSPARTA\u6846\u67b6\u4e3a\u8fd9\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24143", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24143", "abs": "https://arxiv.org/abs/2509.24143", "authors": ["Deepak Prakash Kumar", "Swaroop Darbha", "Satyanarayana Gupta Manyam", "David Casbeer"], "title": "A Novel Model for 3D Motion Planning for a Generalized Dubins Vehicle with Pitch and Yaw Rate Constraints", "comment": "The code for this paper is available at\n  https://github.com/DeepakPrakashKumar/3D-Motion-Planning-for-Generalized-Dubins-with-Pitch-Yaw-constraints", "summary": "In this paper, we propose a new modeling approach and a fast algorithm for 3D\nmotion planning, applicable for fixed-wing unmanned aerial vehicles. The goal\nis to construct the shortest path connecting given initial and final\nconfigurations subject to motion constraints. Our work differs from existing\nliterature in two ways. First, we consider full vehicle orientation using a\nbody-attached frame, which includes roll, pitch, and yaw angles. However,\nexisting work uses only pitch and/or heading angle, which is insufficient to\nuniquely determine orientation. Second, we use two control inputs to represent\nbounded pitch and yaw rates, reflecting control by two separate actuators. In\ncontrast, most previous methods rely on a single input, such as path curvature,\nwhich is insufficient for accurately modeling the vehicle's kinematics in 3D.\nWe use a rotation minimizing frame to describe the vehicle's configuration and\nits evolution, and construct paths by concatenating optimal Dubins paths on\nspherical, cylindrical, or planar surfaces. Numerical simulations show our\napproach generates feasible paths within 10 seconds on average and yields\nshorter paths than existing methods in most cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a3D\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u548c\u5feb\u901f\u7b97\u6cd5\uff0c\u8003\u8651\u5b8c\u6574\u8f66\u8f86\u59ff\u6001\uff08\u6eda\u8f6c\u3001\u4fef\u4ef0\u3001\u504f\u822a\u89d2\uff09\u548c\u53cc\u63a7\u5236\u8f93\u5165\uff0c\u751f\u6210\u6ee1\u8db3\u8fd0\u52a8\u7ea6\u675f\u7684\u6700\u77ed\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u8003\u8651\u4fef\u4ef0\u548c/\u6216\u822a\u5411\u89d2\uff0c\u65e0\u6cd5\u552f\u4e00\u786e\u5b9a\u59ff\u6001\uff1b\u4e14\u5927\u591a\u4f9d\u8d56\u5355\u4e00\u8f93\u5165\uff08\u5982\u8def\u5f84\u66f2\u7387\uff09\uff0c\u65e0\u6cd5\u51c6\u786e\u5efa\u6a213D\u8fd0\u52a8\u5b66\u3002\u9700\u8981\u66f4\u5b8c\u6574\u7684\u59ff\u6001\u8868\u793a\u548c\u66f4\u7cbe\u786e\u7684\u63a7\u5236\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u65cb\u8f6c\u6700\u5c0f\u5316\u6846\u67b6\u63cf\u8ff0\u8f66\u8f86\u914d\u7f6e\u53ca\u5176\u6f14\u5316\uff0c\u901a\u8fc7\u8fde\u63a5\u7403\u9762\u3001\u5706\u67f1\u9762\u6216\u5e73\u9762\u4e0a\u7684\u6700\u4f18Dubins\u8def\u5f84\u6765\u6784\u5efa\u8def\u5f84\uff0c\u91c7\u7528\u4e24\u4e2a\u63a7\u5236\u8f93\u5165\u8868\u793a\u6709\u754c\u4fef\u4ef0\u548c\u504f\u822a\u901f\u7387\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\u8be5\u65b9\u6cd5\u5e73\u5747\u572810\u79d2\u5185\u751f\u6210\u53ef\u884c\u8def\u5f84\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u66f4\u77ed\u7684\u8def\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7684\u5b8c\u65743D\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8fd0\u52a8\u5b66\u5efa\u6a21\u548c\u66f4\u4f18\u7684\u8def\u5f84\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2509.24160", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24160", "abs": "https://arxiv.org/abs/2509.24160", "authors": ["Tomoyuki Kagaya", "Subramanian Lakshmi", "Yuxuan Lou", "Thong Jing Yuan", "Jayashree Karlekar", "Sugiri Pranata", "Natsuki Murakami", "Akira Kinose", "Yang You"], "title": "Memory Transfer Planning: LLM-driven Context-Aware Code Adaptation for Robot Manipulation", "comment": null, "summary": "Large language models (LLMs) are increasingly explored in robot manipulation,\nbut many existing methods struggle to adapt to new environments. Many systems\nrequire either environment-specific policy training or depend on fixed prompts\nand single-shot code generation, leading to limited transferability and manual\nre-tuning. We introduce Memory Transfer Planning (MTP), a framework that\nleverages successful control-code examples from different environments as\nprocedural knowledge, using them as in-context guidance for LLM-driven\nplanning. Specifically, MTP (i) generates an initial plan and code using LLMs,\n(ii) retrieves relevant successful examples from a code memory, and (iii)\ncontextually adapts the retrieved code to the target setting for re-planning\nwithout updating model parameters. We evaluate MTP on RLBench, CALVIN, and a\nphysical robot, demonstrating effectiveness beyond simulation. Across these\nsettings, MTP consistently improved success rate and adaptability compared with\nfixed-prompt code generation, naive retrieval, and memory-free re-planning.\nFurthermore, in hardware experiments, leveraging a memory constructed in\nsimulation proved effective. MTP provides a practical approach that exploits\nprocedural knowledge to realize robust LLM-based planning across diverse\nrobotic manipulation scenarios, enhancing adaptability to novel environments\nand bridging simulation and real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86Memory Transfer Planning (MTP)\u6846\u67b6\uff0c\u5229\u7528\u6765\u81ea\u4e0d\u540c\u73af\u5883\u7684\u6210\u529f\u63a7\u5236\u4ee3\u7801\u4f5c\u4e3a\u7a0b\u5e8f\u77e5\u8bc6\uff0c\u4e3aLLM\u9a71\u52a8\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c4\u5212\u63d0\u4f9b\u4e0a\u4e0b\u6587\u6307\u5bfc\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u5373\u53ef\u9002\u5e94\u65b0\u73af\u5883\u3002", "motivation": "\u73b0\u6709LLM\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u65b0\u73af\u5883\uff0c\u9700\u8981\u73af\u5883\u7279\u5b9a\u7684\u7b56\u7565\u8bad\u7ec3\u6216\u4f9d\u8d56\u56fa\u5b9a\u63d0\u793a\u548c\u5355\u6b21\u4ee3\u7801\u751f\u6210\uff0c\u5bfc\u81f4\u53ef\u8fc1\u79fb\u6027\u6709\u9650\u4e14\u9700\u8981\u624b\u52a8\u91cd\u65b0\u8c03\u6574\u3002", "method": "MTP\u6846\u67b6\uff1a(i) \u4f7f\u7528LLM\u751f\u6210\u521d\u59cb\u8ba1\u5212\u548c\u4ee3\u7801\uff1b(ii) \u4ece\u4ee3\u7801\u8bb0\u5fc6\u4e2d\u68c0\u7d22\u76f8\u5173\u6210\u529f\u793a\u4f8b\uff1b(iii) \u5c06\u68c0\u7d22\u5230\u7684\u4ee3\u7801\u4e0a\u4e0b\u6587\u9002\u5e94\u5230\u76ee\u6807\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u5728RLBench\u3001CALVIN\u548c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMTP\u76f8\u6bd4\u56fa\u5b9a\u63d0\u793a\u4ee3\u7801\u751f\u6210\u3001\u7b80\u5355\u68c0\u7d22\u548c\u65e0\u8bb0\u5fc6\u91cd\u65b0\u89c4\u5212\uff0c\u6301\u7eed\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u9002\u5e94\u6027\u3002\u5229\u7528\u4eff\u771f\u6784\u5efa\u7684\u8bb0\u5fc6\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u4e5f\u88ab\u8bc1\u660e\u6709\u6548\u3002", "conclusion": "MTP\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u5229\u7528\u7a0b\u5e8f\u77e5\u8bc6\u5b9e\u73b0\u8de8\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\u7684\u9c81\u68d2LLM\u89c4\u5212\uff0c\u589e\u5f3a\u4e86\u5bf9\u65b0\u73af\u5883\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5f25\u5408\u4e86\u4eff\u771f\u548c\u73b0\u5b9e\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.24163", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24163", "abs": "https://arxiv.org/abs/2509.24163", "authors": ["Wanming Yu", "Adrian R\u00f6fer", "Abhinav Valada", "Sethu Vijayakumar"], "title": "Preference-Based Long-Horizon Robotic Stacking with Multimodal Large Language Models", "comment": null, "summary": "Pretrained large language models (LLMs) can work as high-level robotic\nplanners by reasoning over abstract task descriptions and natural language\ninstructions, etc. However, they have shown a lack of knowledge and\neffectiveness in planning long-horizon robotic manipulation tasks where the\nphysical properties of the objects are essential. An example is the stacking of\ncontainers with hidden objects inside, which involves reasoning over hidden\nphysics properties such as weight and stability. To this end, this paper\nproposes to use multimodal LLMs as high-level planners for such long-horizon\nrobotic stacking tasks. The LLM takes multimodal inputs for each object to\nstack and infers the current best stacking sequence by reasoning over stacking\npreferences. Furthermore, in order to enable the LLM to reason over multiple\npreferences at the same time without giving explicit instructions, we propose\nto create a custom dataset considering stacking preferences including weight,\nstability, size, and footprint, to fine-tune the LLM. Compared to the\npretrained LLM with prompt tuning, we demonstrate the improved stacking\ncompletion of the LLM fine-tuned with our custom dataset via large-scale\nsimulation evaluation. Furthermore, we showcase the effectiveness of the\nproposed framework for the long-horizon stacking task on a real humanoid robot\nin an online manner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u673a\u5668\u4eba\u5806\u53e0\u4efb\u52a1\u7684\u9ad8\u7ea7\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u4f7f\u5176\u80fd\u591f\u63a8\u7406\u7269\u7406\u5c5e\u6027\u5982\u91cd\u91cf\u3001\u7a33\u5b9a\u6027\u7b49\uff0c\u5728\u957f\u65f6\u57df\u5806\u53e0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u7f3a\u4e4f\u5bf9\u7269\u4f53\u7269\u7406\u5c5e\u6027\u7684\u77e5\u8bc6\u548c\u957f\u65f6\u57df\u64cd\u4f5c\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u9690\u85cf\u7269\u7406\u5c5e\u6027\uff08\u5982\u91cd\u91cf\u3001\u7a33\u5b9a\u6027\uff09\u7684\u5806\u53e0\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u4f5c\u4e3a\u9ad8\u7ea7\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u521b\u5efa\u8003\u8651\u5806\u53e0\u504f\u597d\uff08\u91cd\u91cf\u3001\u7a33\u5b9a\u6027\u3001\u5c3a\u5bf8\u3001\u5360\u5730\u9762\u79ef\uff09\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u6765\u5fae\u8c03\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u540c\u65f6\u63a8\u7406\u591a\u4e2a\u504f\u597d\u800c\u65e0\u9700\u663e\u5f0f\u6307\u4ee4\u3002", "result": "\u4e0e\u4ec5\u4f7f\u7528\u63d0\u793a\u8c03\u4f18\u7684\u9884\u8bad\u7ec3LLM\u76f8\u6bd4\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u5fae\u8c03\u7684LLM\u5728\u5927\u89c4\u6a21\u6a21\u62df\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u6539\u8fdb\u7684\u5806\u53e0\u5b8c\u6210\u5ea6\uff0c\u5e76\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5728\u7ebf\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u591a\u6a21\u6001LLM\u548c\u5b9a\u5236\u6570\u636e\u96c6\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u957f\u65f6\u57df\u5806\u53e0\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u63a8\u7406\u7269\u7406\u5c5e\u6027\u7684\u590d\u6742\u573a\u666f\u4e2d\u3002"}}
{"id": "2509.24175", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24175", "abs": "https://arxiv.org/abs/2509.24175", "authors": ["Rafael Kourdis", "Maciej St\u0119pie\u0144", "J\u00e9r\u00f4me Manhes", "Nicolas Mansard", "Steve Tonneau", "Philippe Sou\u00e8res", "Thomas Flayols"], "title": "Very High Frequency Interpolation for Direct Torque Control", "comment": null, "summary": "Torque control enables agile and robust robot motion, but deployment is often\nhindered by instability and hardware limits. Here, we present a novel solution\nto execute whole-body linear feedback at up to 40 kHz on open-source hardware.\nWe use this to interpolate non-linear schemes during real-world execution, such\nas inverse dynamics and learned torque policies. Our results show that by\nstabilizing torque controllers, high-frequency linear feedback could be an\neffective route towards unlocking the potential of torque-controlled robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f00\u6e90\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe40kHz\u5168\u8eab\u7ebf\u6027\u53cd\u9988\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a33\u5b9a\u626d\u77e9\u63a7\u5236\u5668\u5e76\u6267\u884c\u975e\u7ebf\u6027\u65b9\u6848", "motivation": "\u626d\u77e9\u63a7\u5236\u867d\u7136\u80fd\u5b9e\u73b0\u7075\u6d3b\u7a33\u5065\u7684\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u5e38\u56e0\u4e0d\u7a33\u5b9a\u6027\u548c\u786c\u4ef6\u9650\u5236\u800c\u53d7\u963b", "method": "\u5728\u5f00\u6e90\u786c\u4ef6\u4e0a\u6267\u884c\u9ad8\u8fbe40kHz\u7684\u5168\u8eab\u7ebf\u6027\u53cd\u9988\uff0c\u5e76\u5728\u5b9e\u9645\u6267\u884c\u8fc7\u7a0b\u4e2d\u63d2\u503c\u975e\u7ebf\u6027\u65b9\u6848\uff08\u5982\u9006\u52a8\u529b\u5b66\u548c\u5b66\u4e60\u626d\u77e9\u7b56\u7565\uff09", "result": "\u901a\u8fc7\u7a33\u5b9a\u626d\u77e9\u63a7\u5236\u5668\uff0c\u9ad8\u9891\u7ebf\u6027\u53cd\u9988\u6210\u4e3a\u91ca\u653e\u626d\u77e9\u63a7\u5236\u673a\u5668\u4eba\u6f5c\u529b\u7684\u6709\u6548\u9014\u5f84", "conclusion": "\u9ad8\u9891\u7ebf\u6027\u53cd\u9988\u662f\u89e3\u9501\u626d\u77e9\u63a7\u5236\u673a\u5668\u4eba\u6f5c\u529b\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2509.24219", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24219", "abs": "https://arxiv.org/abs/2509.24219", "authors": ["Tomoyuki Kagaya", "Subramanian Lakshmi", "Anbang Ye", "Thong Jing Yuan", "Jayashree Karlekar", "Sugiri Pranata", "Natsuki Murakami", "Akira Kinose", "Yang You"], "title": "ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning", "comment": null, "summary": "Robots trained via Reinforcement Learning (RL) or Imitation Learning (IL)\noften adapt slowly to new tasks, whereas recent Large Language Models (LLMs)\nand Vision-Language Models (VLMs) promise knowledge-rich planning from minimal\ndata. Deploying LLMs/VLMs for motion planning, however, faces two key\nobstacles: (i) symbolic plans are rarely grounded in scene geometry and object\nphysics, and (ii) model outputs can vary for identical prompts, undermining\nexecution reliability. We propose ViReSkill, a framework that pairs\nvision-grounded replanning with a skill memory for accumulation and reuse. When\na failure occurs, the replanner generates a new action sequence conditioned on\nthe current scene, tailored to the observed state. On success, the executed\nplan is stored as a reusable skill and replayed in future encounters without\nadditional calls to LLMs/VLMs. This feedback loop enables autonomous continual\nlearning: each attempt immediately expands the skill set and stabilizes\nsubsequent executions. We evaluate ViReSkill on simulators such as LIBERO and\nRLBench as well as on a physical robot. Across all settings, it consistently\noutperforms conventional baselines in task success rate, demonstrating robust\nsim-to-real generalization.", "AI": {"tldr": "ViReSkill\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u91cd\u89c4\u5212\u548c\u6280\u80fd\u8bb0\u5fc6\u7684\u6846\u67b6\uff0c\u901a\u8fc7LLMs/VLMs\u751f\u6210\u52a8\u4f5c\u5e8f\u5217\u5e76\u5728\u5931\u8d25\u65f6\u91cd\u65b0\u89c4\u5212\uff0c\u6210\u529f\u65f6\u5b58\u50a8\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u5b9e\u73b0\u81ea\u4e3b\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edfRL/IL\u65b9\u6cd5\u9002\u5e94\u65b0\u4efb\u52a1\u6162\uff0c\u800cLLMs/VLMs\u867d\u7136\u80fd\u4ece\u5c11\u91cf\u6570\u636e\u4e2d\u89c4\u5212\uff0c\u4f46\u5b58\u5728\u7b26\u53f7\u8ba1\u5212\u7f3a\u4e4f\u573a\u666f\u51e0\u4f55\u57fa\u7840\u548c\u6267\u884c\u53ef\u9760\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faViReSkill\u6846\u67b6\uff0c\u5305\u542b\u89c6\u89c9\u57fa\u7840\u7684\u91cd\u89c4\u5212\u5668\u548c\u6280\u80fd\u8bb0\u5fc6\u5e93\u3002\u5931\u8d25\u65f6\u57fa\u4e8e\u5f53\u524d\u573a\u666f\u91cd\u65b0\u89c4\u5212\uff0c\u6210\u529f\u65f6\u5b58\u50a8\u6267\u884c\u8ba1\u5212\u4f5c\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\u3002", "result": "\u5728LIBERO\u3001RLBench\u6a21\u62df\u5668\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8bc4\u4f30\uff0cViReSkill\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u4eff\u771f\u5230\u771f\u5b9e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ViReSkill\u901a\u8fc7\u7ed3\u5408LLMs/VLMs\u7684\u89c4\u5212\u80fd\u529b\u548c\u6280\u80fd\u8bb0\u5fc6\u7684\u7a33\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u4e3b\u6301\u7eed\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u7684\u6210\u529f\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.24235", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.24235", "abs": "https://arxiv.org/abs/2509.24235", "authors": ["Xuan Lin", "Jiming Ren", "Yandong Luo", "Weijun Xie", "Ye Zhao"], "title": "Towards Tighter Convex Relaxation of Mixed-integer Programs: Leveraging Logic Network Flow for Task and Motion Planning", "comment": "35 pages, 17 figures, 7 tables", "summary": "This paper proposes an optimization-based task and motion planning framework,\nnamed \"Logic Network Flow\", that integrates temporal logic specifications into\nmixed-integer programs for efficient robot planning. Inspired by the\nGraph-of-Convex-Sets formulation, temporal predicates are encoded as polyhedron\nconstraints on each edge of a network flow model, instead of as constraints\nbetween nodes in traditional Logic Tree formulations. We further propose a\nnetwork-flow-based Fourier-Motzkin elimination procedure that removes\ncontinuous flow variables while preserving convex relaxation tightness, leading\nto provably tighter convex relaxations and fewer constraints than Logic Tree\nformulations. For temporal logic motion planning with piecewise-affine dynamic\nsystems, comprehensive experiments across vehicle routing, multi-robot\ncoordination, and temporal logic control on dynamical systems using point mass\nand linear inverted pendulum models demonstrate computational speedups of up to\nseveral orders of magnitude. Hardware demonstrations with quadrupedal robots\nvalidate real-time replanning capabilities under dynamically changing\nenvironmental conditions. The project website is at\nhttps://logicnetworkflow.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u903b\u8f91\u7f51\u7edc\u6d41\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u5c06\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u96c6\u6210\u5230\u6df7\u5408\u6574\u6570\u89c4\u5212\u4e2d\uff0c\u901a\u8fc7\u6d41\u7f51\u7edc\u6a21\u578b\u7f16\u7801\u65f6\u5e8f\u8c13\u8bcd\uff0c\u5b9e\u73b0\u6bd4\u4f20\u7edf\u903b\u8f91\u6811\u65b9\u6cd5\u66f4\u7d27\u7684\u51f8\u677e\u5f1b\u548c\u66f4\u5c11\u7684\u7ea6\u675f\u3002", "motivation": "\u4f20\u7edf\u903b\u8f91\u6811\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u5e8f\u903b\u8f91\u89c4\u5212\u65f6\u5b58\u5728\u51f8\u677e\u5f1b\u4e0d\u591f\u7d27\u3001\u7ea6\u675f\u8fc7\u591a\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u6846\u67b6\u6765\u63d0\u5347\u673a\u5668\u4eba\u89c4\u5212\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7f51\u7edc\u6d41\u6a21\u578b\uff0c\u5c06\u65f6\u5e8f\u8c13\u8bcd\u7f16\u7801\u4e3a\u8fb9\u4e0a\u7684\u591a\u9762\u4f53\u7ea6\u675f\uff0c\u63d0\u51fa\u57fa\u4e8e\u7f51\u7edc\u6d41\u7684Fourier-Motzkin\u6d88\u5143\u6cd5\u53bb\u9664\u8fde\u7eed\u6d41\u53d8\u91cf\uff0c\u4fdd\u6301\u51f8\u677e\u5f1b\u7d27\u5ea6\u3002", "result": "\u5728\u8f66\u8f86\u8def\u5f84\u89c4\u5212\u3001\u591a\u673a\u5668\u4eba\u534f\u8c03\u548c\u65f6\u5e8f\u903b\u8f91\u63a7\u5236\u7b49\u4efb\u52a1\u4e2d\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u56db\u8db3\u673a\u5668\u4eba\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u91cd\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "\u903b\u8f91\u7f51\u7edc\u6d41\u6846\u67b6\u5728\u65f6\u5e8f\u903b\u8f91\u8fd0\u52a8\u89c4\u5212\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u6027\u80fd\u548c\u5b9e\u65f6\u91cd\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2509.24236", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24236", "abs": "https://arxiv.org/abs/2509.24236", "authors": ["Siyan Dong", "Zijun Wang", "Lulu Cai", "Yi Ma", "Yanchao Yang"], "title": "PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization", "comment": null, "summary": "Real-time dense scene reconstruction during unstable camera motions is\ncrucial for robotics, yet current RGB-D SLAM systems fail when cameras\nexperience large viewpoint changes, fast motions, or sudden shaking. Classical\noptimization-based methods deliver high accuracy but fail with poor\ninitialization during large motions, while learning-based approaches provide\nrobustness but lack sufficient accuracy for dense reconstruction. We address\nthis challenge through a combination of learning-based initialization with\noptimization-based refinement. Our method employs a camera pose regression\nnetwork to predict metric-aware relative poses from consecutive RGB-D frames,\nwhich serve as reliable starting points for a randomized optimization algorithm\nthat further aligns depth images with the scene geometry. Extensive experiments\ndemonstrate promising results: our approach outperforms the best competitor on\nchallenging benchmarks, while maintaining comparable accuracy on stable motion\nsequences. The system operates in real-time, showcasing that combining simple\nand principled techniques can achieve both robustness for unstable motions and\naccuracy for dense reconstruction. Project page:\nhttps://github.com/siyandong/PROFusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u5f0f\u521d\u59cb\u5316\u548c\u4f18\u5316\u5f0f\u7cbe\u5316\u7684RGB-D SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u673a\u4f4d\u59ff\u56de\u5f52\u7f51\u7edc\u9884\u6d4b\u76f8\u5bf9\u4f4d\u59ff\u4f5c\u4e3a\u4f18\u5316\u8d77\u70b9\uff0c\u89e3\u51b3\u4e86\u5927\u89c6\u89d2\u53d8\u5316\u548c\u5feb\u901f\u8fd0\u52a8\u4e0b\u7684\u91cd\u5efa\u5931\u8d25\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RGB-D SLAM\u7cfb\u7edf\u5728\u76f8\u673a\u7ecf\u5386\u5927\u89c6\u89d2\u53d8\u5316\u3001\u5feb\u901f\u8fd0\u52a8\u6216\u7a81\u7136\u6296\u52a8\u65f6\u4f1a\u5931\u8d25\u3002\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u4f46\u5bf9\u521d\u59cb\u5316\u654f\u611f\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u9c81\u68d2\u6027\u597d\u4f46\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u76f8\u673a\u4f4d\u59ff\u56de\u5f52\u7f51\u7edc\u4ece\u8fde\u7eedRGB-D\u5e27\u9884\u6d4b\u5ea6\u91cf\u611f\u77e5\u7684\u76f8\u5bf9\u4f4d\u59ff\uff0c\u4f5c\u4e3a\u968f\u673a\u4f18\u5316\u7b97\u6cd5\u7684\u53ef\u9760\u8d77\u70b9\uff0c\u8fdb\u4e00\u6b65\u5c06\u6df1\u5ea6\u56fe\u50cf\u4e0e\u573a\u666f\u51e0\u4f55\u5bf9\u9f50\u3002", "result": "\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u4f73\u7ade\u4e89\u5bf9\u624b\uff0c\u5728\u7a33\u5b9a\u8fd0\u52a8\u5e8f\u5217\u4e0a\u4fdd\u6301\u76f8\u5f53\u7cbe\u5ea6\uff0c\u7cfb\u7edf\u53ef\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u7ed3\u5408\u7b80\u5355\u800c\u539f\u5219\u6027\u7684\u6280\u672f\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u4e0d\u7a33\u5b9a\u8fd0\u52a8\u7684\u9c81\u68d2\u6027\u548c\u5bc6\u96c6\u91cd\u5efa\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2509.24243", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24243", "abs": "https://arxiv.org/abs/2509.24243", "authors": ["Jeongyong Yang", "Seunghwan Jang", "Soojean Han"], "title": "SafeFlowMatcher: Safe and Fast Planning using Flow Matching with Control Barrier Functions", "comment": "10 pages, 7 figures, 4 tables", "summary": "Generative planners based on flow matching (FM) can produce high-quality\npaths in one or a few ODE steps, but their sampling dynamics offer no formal\nsafety guarantees and can yield incomplete paths near constraints. We present\nSafeFlowMatcher, a planning framework that couples FM with control barrier\nfunctions (CBFs) to achieve both real-time efficiency and certified safety.\nSafeFlowMatcher uses a two-phase prediction-correction (PC) integrator: (i) a\nprediction phase integrates the learned FM once (or a few steps) to obtain a\ncandidate path without intervention; (ii) a correction phase refines this path\nwith a vanishing time-scaled vector field and a CBF-based quadratic program\nthat minimally perturbs the vector field. We prove a barrier certificate for\nthe resulting flow system, establishing forward invariance of a robust safe set\nand finite-time convergence to the safe set. By enforcing safety only on the\nexecuted path (rather than on all intermediate latent paths), SafeFlowMatcher\navoids distributional drift and mitigates local trap problems. Across maze\nnavigation and locomotion benchmarks, SafeFlowMatcher attains faster, smoother,\nand safer paths than diffusion- and FM-based baselines. Extensive ablations\ncorroborate the contributions of the PC integrator and the barrier certificate.", "AI": {"tldr": "SafeFlowMatcher\u7ed3\u5408\u6d41\u5339\u914d\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u901a\u8fc7\u9884\u6d4b-\u6821\u6b63\u79ef\u5206\u5668\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u6548\u4e14\u5b89\u5168\u8ba4\u8bc1\u7684\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u89c4\u5212\u5668\u867d\u7136\u80fd\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u8def\u5f84\uff0c\u4f46\u7f3a\u4e4f\u6b63\u5f0f\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u5728\u7ea6\u675f\u9644\u8fd1\u53ef\u80fd\u4ea7\u751f\u4e0d\u5b8c\u6574\u8def\u5f84\u3002", "method": "\u4f7f\u7528\u4e24\u9636\u6bb5\u9884\u6d4b-\u6821\u6b63\u79ef\u5206\u5668\uff1a\u9884\u6d4b\u9636\u6bb5\u901a\u8fc7\u6d41\u5339\u914d\u751f\u6210\u5019\u9009\u8def\u5f84\uff1b\u6821\u6b63\u9636\u6bb5\u7528\u65f6\u95f4\u7f29\u653e\u5411\u91cf\u573a\u548cCBF\u4e8c\u6b21\u89c4\u5212\u8fdb\u884c\u6700\u5c0f\u6270\u52a8\u4fee\u6b63\u3002", "result": "\u5728\u8ff7\u5bab\u5bfc\u822a\u548c\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u57fa\u4e8e\u6269\u6563\u548c\u6d41\u5339\u914d\u7684\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u5feb\u3001\u66f4\u5e73\u6ed1\u3001\u66f4\u5b89\u5168\u7684\u8def\u5f84\u3002", "conclusion": "SafeFlowMatcher\u901a\u8fc7\u4ec5\u5bf9\u6267\u884c\u8def\u5f84\u65bd\u52a0\u5b89\u5168\u7ea6\u675f\uff0c\u907f\u514d\u4e86\u5206\u5e03\u6f02\u79fb\u548c\u5c40\u90e8\u9677\u9631\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b89\u5168\u89c4\u5212\u3002"}}
{"id": "2509.24281", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24281", "abs": "https://arxiv.org/abs/2509.24281", "authors": ["Kasra Torshizi", "Chak Lam Shek", "Khuzema Habib", "Guangyao Shi", "Pratap Tokekar", "Troi Williams"], "title": "Contextual Neural Moving Horizon Estimation for Robust Quadrotor Control in Varying Conditions", "comment": "9 pages, 7 Figures, Kasra Torshizi and Chak Lam Shek contributed\n  equally", "summary": "Adaptive controllers on quadrotors typically rely on estimation of\ndisturbances to ensure robust trajectory tracking. Estimating disturbances\nacross diverse environmental contexts is challenging due to the inherent\nvariability and uncertainty in the real world. Such estimators require\nextensive fine-tuning for a specific scenario, which makes them inflexible and\nbrittle to changing conditions. Machine-learning approaches, such as training a\nneural network to tune the estimator's parameters, are promising. However,\ncollecting data across all possible environmental contexts is impossible. It is\nalso inefficient as the same estimator parameters could work for \"nearby\"\ncontexts. In this paper, we present a sequential decision making strategy that\ndecides which environmental contexts, using Bayesian Optimization with a\nGaussian Process, to collect data from in order to ensure robust performance\nacross a wide range of contexts. Our method, Contextual NeuroMHE, eliminates\nthe need for exhaustive training across all environments while maintaining\nrobust performance under different conditions. By enabling the neural network\nto adapt its parameters dynamically, our method improves both efficiency and\ngeneralization. Experimental results in various real-world settings demonstrate\nthat our approach outperforms the prior work by 20.3\\% in terms of maximum\nabsolute position error and can capture the variations in the environment with\na few carefully chosen contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u9ad8\u65af\u8fc7\u7a0b\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u56db\u65cb\u7ffc\u98de\u884c\u5668\u4e2d\u81ea\u9002\u5e94\u8c03\u6574\u795e\u7ecf\u79fb\u52a8\u5730\u5e73\u7ebf\u4f30\u8ba1\u5668\u7684\u53c2\u6570\uff0c\u907f\u514d\u5728\u6240\u6709\u73af\u5883\u4e2d\u8fdb\u884c\u7a77\u4e3e\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u81ea\u9002\u5e94\u63a7\u5236\u5668\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u8fdb\u884c\u5927\u91cf\u8c03\u53c2\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e14\u96be\u4ee5\u9002\u5e94\u53d8\u5316\u7684\u73af\u5883\u6761\u4ef6\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5728\u6240\u6709\u53ef\u80fd\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\u4e0d\u73b0\u5b9e\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u9ad8\u65af\u8fc7\u7a0b\u6765\u9009\u62e9\u9700\u8981\u6536\u96c6\u6570\u636e\u7684\u73af\u5883\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u795e\u7ecf\u79fb\u52a8\u5730\u5e73\u7ebf\u4f30\u8ba1\u5668\u52a8\u6001\u8c03\u6574\u53c2\u6570\uff0c\u5b9e\u73b0\u8de8\u73af\u5883\u7684\u9c81\u68d2\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6700\u5927\u7edd\u5bf9\u4f4d\u7f6e\u8bef\u5dee\u65b9\u9762\u6bd4\u5148\u524d\u5de5\u4f5c\u63d0\u5347\u4e8620.3%\uff0c\u4ec5\u9700\u5c11\u91cf\u7cbe\u5fc3\u9009\u62e9\u7684\u4e0a\u4e0b\u6587\u5c31\u80fd\u6355\u6349\u73af\u5883\u53d8\u5316\u3002", "conclusion": "Contextual NeuroMHE\u65b9\u6cd5\u6d88\u9664\u4e86\u5728\u6240\u6709\u73af\u5883\u4e2d\u8fdb\u884c\u7a77\u4e3e\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.24313", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24313", "abs": "https://arxiv.org/abs/2509.24313", "authors": ["Korbinian Moller", "Roland Stroop", "Mattia Piccinini", "Alexander Langmann", "Johannes Betz"], "title": "Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning", "comment": "8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria", "summary": "Sampling-based motion planning is a well-established approach in autonomous\ndriving, valued for its modularity and analytical tractability. In complex\nurban scenarios, however, uniform or heuristic sampling often produces many\ninfeasible or irrelevant trajectories. We address this limitation with a hybrid\nframework that learns where to sample while keeping trajectory generation and\nevaluation fully analytical and verifiable. A reinforcement learning (RL) agent\nguides the sampling process toward regions of the action space likely to yield\nfeasible trajectories, while evaluation and final selection remains governed by\ndeterministic feasibility checks and cost functions. We couple the RL sampler\nwith a world model (WM) based on a decodable deep set encoder, enabling both\nvariable numbers of traffic participants and reconstructable latent\nrepresentations. The approach is evaluated in the CommonRoad simulation\nenvironment, showing up to 99% fewer required samples and a runtime reduction\nof up to 84% while maintaining planning quality in terms of success and\ncollision-free rates. These improvements lead to faster, more reliable\ndecision-making for autonomous vehicles in urban environments, achieving safer\nand more responsive navigation under real-world constraints. Code and trained\nartifacts are publicly available at:\nhttps://github.com/TUM-AVS/Learning-to-Sample", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u8f68\u8ff9\u751f\u6210\u548c\u8bc4\u4f30\u53ef\u5206\u6790\u9a8c\u8bc1\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u91c7\u6837\u9700\u6c42\u3002", "motivation": "\u5728\u590d\u6742\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u7684\u5747\u5300\u6216\u542f\u53d1\u5f0f\u91c7\u6837\u65b9\u6cd5\u4f1a\u4ea7\u751f\u5927\u91cf\u4e0d\u53ef\u884c\u6216\u4e0d\u76f8\u5173\u7684\u8f68\u8ff9\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\uff0c\u7ed3\u5408\u57fa\u4e8e\u53ef\u89e3\u7801\u6df1\u5ea6\u96c6\u5408\u7f16\u7801\u5668\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4fdd\u6301\u8f68\u8ff9\u751f\u6210\u548c\u8bc4\u4f30\u7684\u786e\u5b9a\u6027\u9a8c\u8bc1\u80fd\u529b\u3002", "result": "\u5728CommonRoad\u4eff\u771f\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u6240\u9700\u6837\u672c\u51cf\u5c11\u8fbe99%\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u8fbe84%\uff0c\u540c\u65f6\u4fdd\u6301\u89c4\u5212\u8d28\u91cf\u548c\u65e0\u78b0\u649e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u901f\u3001\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7ea6\u675f\u4e0b\u5b9e\u73b0\u66f4\u5b89\u5168\u548c\u54cd\u5e94\u6027\u66f4\u5f3a\u7684\u5bfc\u822a\u3002"}}
{"id": "2509.24321", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24321", "abs": "https://arxiv.org/abs/2509.24321", "authors": ["Yao Wang", "Zhirui Sun", "Wenzheng Chi", "Baozhi Jia", "Wenjun Xu", "Jiankun Wang"], "title": "SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm", "comment": null, "summary": "Understanding human instructions and accomplishing Vision-Language Navigation\ntasks in unknown environments is essential for robots. However, existing\nmodular approaches heavily rely on the quality of training data and often\nexhibit poor generalization. Vision-Language Model based methods, while\ndemonstrating strong generalization capabilities, tend to perform\nunsatisfactorily when semantic cues are weak. To address these issues, this\npaper proposes SONAR, an aggregated reasoning approach through a cross modal\nparadigm. The proposed method integrates a semantic map based target prediction\nmodule with a Vision-Language Model based value map module, enabling more\nrobust navigation in unknown environments with varying levels of semantic cues,\nand effectively balancing generalization ability with scene adaptability. In\nterms of target localization, we propose a strategy that integrates multi-scale\nsemantic maps with confidence maps, aiming to mitigate false detections of\ntarget objects. We conducted an evaluation of the SONAR within the Gazebo\nsimulator, leveraging the most challenging Matterport 3D (MP3D) dataset as the\nexperimental benchmark. Experimental results demonstrate that SONAR achieves a\nsuccess rate of 38.4% and an SPL of 17.7%.", "AI": {"tldr": "\u63d0\u51fa\u4e86SONAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u805a\u5408\u63a8\u7406\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u573a\u666f\u9002\u5e94\u6027\u95ee\u9898\uff0c\u5728MP3D\u6570\u636e\u96c6\u4e0a\u8fbe\u523038.4%\u7684\u6210\u529f\u7387\u548c17.7%\u7684SPL\u3002", "motivation": "\u73b0\u6709\u6a21\u5757\u5316\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4e14\u6cdb\u5316\u6027\u5dee\uff0c\u800c\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u8bed\u4e49\u7ebf\u7d22\u5f31\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5e73\u8861\u6cdb\u5316\u80fd\u529b\u548c\u573a\u666f\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faSONAR\u65b9\u6cd5\uff0c\u6574\u5408\u57fa\u4e8e\u8bed\u4e49\u5730\u56fe\u7684\u76ee\u6807\u9884\u6d4b\u6a21\u5757\u548c\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4ef7\u503c\u5730\u56fe\u6a21\u5757\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u8bed\u4e49\u5730\u56fe\u4e0e\u7f6e\u4fe1\u5ea6\u5730\u56fe\u878d\u5408\u7b56\u7565\u6765\u51cf\u5c11\u76ee\u6807\u8bef\u68c0\u3002", "result": "\u5728Gazebo\u6a21\u62df\u5668\u4e2d\u4f7f\u7528MP3D\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5\uff0cSONAR\u5b9e\u73b0\u4e8638.4%\u7684\u6210\u529f\u7387\u548c17.7%\u7684SPL\u3002", "conclusion": "SONAR\u901a\u8fc7\u8de8\u6a21\u6001\u805a\u5408\u63a8\u7406\u6709\u6548\u63d0\u5347\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u9c81\u68d2\u6027\uff0c\u5e73\u8861\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u573a\u666f\u9002\u5e94\u6027\u3002"}}
{"id": "2509.24387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24387", "abs": "https://arxiv.org/abs/2509.24387", "authors": ["Xin Ding", "Jianyu Wei", "Yifan Yang", "Shiqi Jiang", "Qianxi Zhang", "Hao Wu", "Fucheng Jia", "Liang Mi", "Yuxuan Yan", "Weijun Wang", "Yunxin Liu", "Zhibo Chen", "Ting Cao"], "title": "AdaNav: Adaptive Reasoning with Uncertainty for Vision-Language Navigation", "comment": null, "summary": "Vision Language Navigation (VLN) requires agents to follow natural language\ninstructions by grounding them in sequential visual observations over long\nhorizons. Explicit reasoning could enhance temporal consistency and perception\naction alignment, but reasoning at fixed steps often leads to suboptimal\nperformance and unnecessary computation. To address this, we propose AdaNav, an\nuncertainty-based adaptive reasoning framework for VLN. At its core is the\nUncertainty Adaptive Reasoning Block (UAR), a lightweight plugin that\ndynamically triggers reasoning. We introduce Action Entropy as a policy prior\nfor UAR and progressively refine it through a Heuristics to RL training method,\nenabling agents to learn difficulty aware reasoning policies under the strict\ndata limitations of embodied tasks. Results show that with only 6K training\nsamples, AdaNav achieves substantial gains over closed source models trained on\nmillion scale data, improving success rate by 20% on R2R val-unseen, 11.7% on\nRxR-CE, and 11.4% in real world scenes. The code is available at\nhttps://github.com/xinding-sys/AdaNav.", "AI": {"tldr": "AdaNav\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89e6\u53d1\u63a8\u7406\u6765\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6027\u80fd\uff0c\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u9700\u8981\u4ee3\u7406\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5728\u957f\u5e8f\u5217\u89c6\u89c9\u89c2\u5bdf\u4e2d\u8fdb\u884c\u5bfc\u822a\u3002\u56fa\u5b9a\u6b65\u957f\u7684\u63a8\u7406\u5f80\u5f80\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u548c\u8ba1\u7b97\u5197\u4f59\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\u6765\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u611f\u77e5-\u52a8\u4f5c\u5bf9\u9f50\u3002", "method": "\u63d0\u51faAdaNav\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u4e0d\u786e\u5b9a\u6027\u81ea\u9002\u5e94\u63a8\u7406\u5757(UAR)\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63d2\u4ef6\uff0c\u901a\u8fc7\u52a8\u4f5c\u71b5\u4f5c\u4e3a\u7b56\u7565\u5148\u9a8c\u52a8\u6001\u89e6\u53d1\u63a8\u7406\u3002\u91c7\u7528\u542f\u53d1\u5f0f\u5230\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\u9010\u6b65\u4f18\u5316\u63a8\u7406\u7b56\u7565\u3002", "result": "\u4ec5\u75286K\u8bad\u7ec3\u6837\u672c\uff0cAdaNav\u5728R2R val-unseen\u4e0a\u6210\u529f\u7387\u63d0\u534720%\uff0cRxR-CE\u4e0a\u63d0\u534711.7%\uff0c\u771f\u5b9e\u573a\u666f\u4e2d\u63d0\u534711.4%\uff0c\u663e\u8457\u4f18\u4e8e\u767e\u4e07\u7ea7\u6570\u636e\u8bad\u7ec3\u7684\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "AdaNav\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u63a8\u7406\u5728\u6570\u636e\u53d7\u9650\u7684\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u52a8\u6001\u63a8\u7406\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2509.24413", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.24413", "abs": "https://arxiv.org/abs/2509.24413", "authors": ["Tianqiang Yan", "Ziqiao Lin", "Sicheng Wang", "Tianwei Zhang", "Zhenglong Sun"], "title": "DynaMIC: Dynamic Multimodal In-Context Learning Enabled Embodied Robot Counterfactual Resistance Ability", "comment": null, "summary": "The emergence of large pre-trained models based on natural language has\nbreathed new life into robotics development. Extensive research has integrated\nlarge models with robots, utilizing the powerful semantic understanding and\ngeneration capabilities of large models to facilitate robot control through\nnatural language instructions gradually. However, we found that robots that\nstrictly adhere to human instructions, especially those containing misleading\ninformation, may encounter errors during task execution, potentially leading to\nsafety hazards. This resembles the concept of counterfactuals in natural\nlanguage processing (NLP), which has not yet attracted much attention in\nrobotic research. In an effort to highlight this issue for future studies, this\npaper introduced directive counterfactuals (DCFs) arising from misleading human\ndirectives. We present DynaMIC, a framework for generating robot task flows to\nidentify DCFs and relay feedback to humans proactively. This capability can\nhelp robots be sensitive to potential DCFs within a task, thus enhancing the\nreliability of the execution process. We conducted semantic-level experiments\nand ablation studies, showcasing the effectiveness of this framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DynaMIC\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u8bef\u5bfc\u6027\u6307\u4ee4\uff08DCFs\uff09\u5e76\u4e3b\u52a8\u5411\u4eba\u7c7b\u63d0\u4f9b\u53cd\u9988\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u6267\u884c\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u4e25\u683c\u9075\u5faa\u4eba\u7c7b\u6307\u4ee4\uff0c\u4f46\u5305\u542b\u8bef\u5bfc\u4fe1\u606f\u7684\u6307\u4ee4\u53ef\u80fd\u5bfc\u81f4\u6267\u884c\u9519\u8bef\u548c\u5b89\u5168\u98ce\u9669\uff0c\u8fd9\u4e00\u95ee\u9898\u5728\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u5c1a\u672a\u5f97\u5230\u8db3\u591f\u91cd\u89c6\u3002", "method": "\u63d0\u51faDynaMIC\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u673a\u5668\u4eba\u4efb\u52a1\u6d41\u7a0b\u6765\u8bc6\u522b\u6307\u4ee4\u53cd\u4e8b\u5b9e\uff08DCFs\uff09\uff0c\u5e76\u4e3b\u52a8\u5411\u4eba\u7c7b\u63d0\u4f9b\u53cd\u9988\u3002", "result": "\u8fdb\u884c\u4e86\u8bed\u4e49\u7ea7\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5e2e\u52a9\u673a\u5668\u4eba\u5bf9\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728DCFs\u4fdd\u6301\u654f\u611f\uff0c\u4ece\u800c\u589e\u5f3a\u6267\u884c\u8fc7\u7a0b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.24524", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.24524", "abs": "https://arxiv.org/abs/2509.24524", "authors": ["Zhihao Wang", "Jianxiong Li", "Jinliang Zheng", "Wencong Zhang", "Dongxiu Liu", "Yinan Zheng", "Haoyi Niu", "Junzhi Yu", "Xianyuan Zhan"], "title": "PhysiAgent: An Embodied Agent Framework in Physical World", "comment": null, "summary": "Vision-Language-Action (VLA) models have achieved notable success but often\nstruggle with limited generalizations. To address this, integrating generalized\nVision-Language Models (VLMs) as assistants to VLAs has emerged as a popular\nsolution. However, current approaches often combine these models in rigid,\nsequential structures: using VLMs primarily for high-level scene understanding\nand task planning, and VLAs merely as executors of lower-level actions, leading\nto ineffective collaboration and poor grounding challenges. In this paper, we\npropose an embodied agent framework, PhysiAgent, tailored to operate\neffectively in physical environments. By incorporating monitor, memory,\nself-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent\noffers an autonomous scaffolding framework to prompt VLMs to organize different\ncomponents based on real-time proficiency feedback from VLAs to maximally\nexploit VLAs' capabilities. Experimental results demonstrate significant\nimprovements in task-solving performance on complex real-world robotic tasks,\nshowcasing effective self-regulation of VLMs, coherent tool collaboration, and\nadaptive evolution of the framework during execution. PhysiAgent makes\npractical and pioneering efforts to integrate VLMs and VLAs, effectively\ngrounding embodied agent frameworks in real-world settings.", "AI": {"tldr": "\u63d0\u51faPhysiAgent\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u63a7\u3001\u8bb0\u5fc6\u3001\u81ea\u53cd\u601d\u673a\u5236\u548c\u8f7b\u91cf\u7ea7\u5de5\u5177\u7bb1\uff0c\u5b9e\u73b0VLM\u548cVLA\u7684\u6709\u6548\u534f\u4f5c\uff0c\u5728\u7269\u7406\u73af\u5883\u4e2d\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800cVLM\u4f5c\u4e3a\u8f85\u52a9\u7684\u96c6\u6210\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u50f5\u5316\u7684\u987a\u5e8f\u7ed3\u6784\uff0c\u5bfc\u81f4\u534f\u4f5c\u6548\u7387\u4f4e\u4e0b\u548c\u63a5\u5730\u6311\u6218\u3002", "method": "\u5f15\u5165\u76d1\u63a7\u3001\u8bb0\u5fc6\u3001\u81ea\u53cd\u601d\u673a\u5236\u548c\u8f7b\u91cf\u7ea7\u5de5\u5177\u7bb1\uff0c\u6784\u5efa\u81ea\u4e3b\u652f\u67b6\u6846\u67b6\uff0c\u6839\u636eVLA\u7684\u5b9e\u65f6\u80fd\u529b\u53cd\u9988\u6765\u7ec4\u7ec7\u4e0d\u540c\u7ec4\u4ef6\u3002", "result": "\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u6027\u80fd\uff0c\u5c55\u793a\u4e86VLM\u7684\u6709\u6548\u81ea\u6211\u8c03\u8282\u3001\u5de5\u5177\u534f\u4f5c\u8fde\u8d2f\u6027\u548c\u6846\u67b6\u7684\u81ea\u9002\u5e94\u6f14\u5316\u3002", "conclusion": "PhysiAgent\u4e3aVLM\u548cVLA\u7684\u96c6\u6210\u505a\u51fa\u4e86\u5b9e\u7528\u4e14\u5f00\u521b\u6027\u7684\u52aa\u529b\uff0c\u6709\u6548\u5c06\u5177\u8eab\u667a\u80fd\u4f53\u6846\u67b6\u63a5\u5730\u5230\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.24530", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24530", "abs": "https://arxiv.org/abs/2509.24530", "authors": ["Giulia Pusceddu", "Sara Mongile", "Francesco Rea", "Alessandra Sciutti"], "title": "Game Theory to Study Cooperation in Human-Robot Mixed Groups: Exploring the Potential of the Public Good Game", "comment": "Work presented at the workshop BAILAR in conjunction with IEEE RO-MAN\n  2023. Peer reviewed", "summary": "In this study, we explore the potential of Game Theory as a means to\ninvestigate cooperation and trust in human-robot mixed groups. Particularly, we\nintroduce the Public Good Game (PGG), a model highlighting the tension between\nindividual self-interest and collective well-being. In this work, we present a\nmodified version of the PGG, where three human participants engage in the game\nwith the humanoid robot iCub to assess whether various robot game strategies\n(e.g., always cooperate, always free ride, and tit-for-tat) can influence the\nparticipants' inclination to cooperate. We test our setup during a pilot study\nwith nineteen participants. A preliminary analysis indicates that participants\nprefer not to invest their money in the common pool, despite they perceive the\nrobot as generous. By conducting this research, we seek to gain valuable\ninsights into the role that robots can play in promoting trust and cohesion\nduring human-robot interactions within group contexts. The results of this\nstudy may hold considerable potential for developing social robots capable of\nfostering trust and cooperation within mixed human-robot groups.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6539\u8fdb\u7684\u516c\u5171\u7269\u54c1\u6e38\u620f\uff0c\u63a2\u8ba8\u4eba\u5f62\u673a\u5668\u4ebaiCub\u7684\u4e0d\u540c\u6e38\u620f\u7b56\u7565\u5bf9\u4eba\u7c7b\u53c2\u4e0e\u8005\u5408\u4f5c\u503e\u5411\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u535a\u5f08\u8bba\u4f5c\u4e3a\u7814\u7a76\u4eba\u673a\u6df7\u5408\u7fa4\u4f53\u4e2d\u5408\u4f5c\u4e0e\u4fe1\u4efb\u7684\u6f5c\u5728\u5de5\u5177\uff0c\u4e86\u89e3\u673a\u5668\u4eba\u5728\u4fc3\u8fdb\u4eba\u673a\u4ea4\u4e92\u4e2d\u4fe1\u4efb\u548c\u51dd\u805a\u529b\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u516c\u5171\u7269\u54c1\u6e38\u620f\u6a21\u578b\uff0c\u8ba9\u4e09\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u4e0e\u4eba\u5f62\u673a\u5668\u4ebaiCub\u8fdb\u884c\u6e38\u620f\uff0c\u6d4b\u8bd5\u4e0d\u540c\u673a\u5668\u4eba\u7b56\u7565\uff08\u603b\u662f\u5408\u4f5c\u3001\u603b\u662f\u642d\u4fbf\u8f66\u3001\u4ee5\u7259\u8fd8\u7259\uff09\u5bf9\u4eba\u7c7b\u5408\u4f5c\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u521d\u6b65\u5206\u6790\u663e\u793a\uff0c\u5c3d\u7ba1\u53c2\u4e0e\u8005\u8ba4\u4e3a\u673a\u5668\u4eba\u5f88\u6177\u6168\uff0c\u4f46\u4ed6\u4eec\u4ecd\u503e\u5411\u4e8e\u4e0d\u5411\u516c\u5171\u6c60\u6295\u8d44\u8d44\u91d1\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u5f00\u53d1\u80fd\u591f\u5728\u4eba\u673a\u6df7\u5408\u7fa4\u4f53\u4e2d\u4fc3\u8fdb\u4fe1\u4efb\u4e0e\u5408\u4f5c\u7684\u793e\u4f1a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u6f5c\u529b\u3002"}}
{"id": "2509.24539", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24539", "abs": "https://arxiv.org/abs/2509.24539", "authors": ["Nayari Marie Lessa", "Melya Boukheddimi", "Frank Kirchner"], "title": "Unlocking the Potential of Soft Actor-Critic for Imitation Learning", "comment": null, "summary": "Learning-based methods have enabled robots to acquire bio-inspired movements\nwith increasing levels of naturalness and adaptability. Among these, Imitation\nLearning (IL) has proven effective in transferring complex motion patterns from\nanimals to robotic systems. However, current state-of-the-art frameworks\npredominantly rely on Proximal Policy Optimization (PPO), an on-policy\nalgorithm that prioritizes stability over sample efficiency and policy\ngeneralization. This paper proposes a novel IL framework that combines\nAdversarial Motion Priors (AMP) with the off-policy Soft Actor-Critic (SAC)\nalgorithm to overcome these limitations. This integration leverages\nreplay-driven learning and entropy-regularized exploration, enabling\nnaturalistic behavior and task execution, improving data efficiency and\nrobustness. We evaluate the proposed approach (AMP+SAC) on quadruped gaits\ninvolving multiple reference motions and diverse terrains. Experimental results\ndemonstrate that the proposed framework not only maintains stable task\nexecution but also achieves higher imitation rewards compared to the widely\nused AMP+PPO method. These findings highlight the potential of an off-policy IL\nformulation for advancing motion generation in robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c(AMP)\u548c\u79bb\u7b56\u7565\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6(SAC)\u7684\u65b0\u578b\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u6b65\u6001\u5b66\u4e60\uff0c\u76f8\u6bd4\u4e3b\u6d41\u7684AMP+PPO\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8ePPO\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u7a33\u5b9a\u4f46\u6837\u672c\u6548\u7387\u4f4e\u4e14\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u79bb\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u5347\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u81ea\u7136\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u5c06\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c(AMP)\u4e0e\u79bb\u7b56\u7565\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6(SAC)\u7b97\u6cd5\u7ed3\u5408\uff0c\u5229\u7528\u91cd\u653e\u9a71\u52a8\u5b66\u4e60\u548c\u71b5\u6b63\u5219\u5316\u63a2\u7d22\uff0c\u5728\u591a\u79cd\u53c2\u8003\u8fd0\u52a8\u548c\u4e0d\u540c\u5730\u5f62\u4e0a\u8bc4\u4f30\u56db\u8db3\u673a\u5668\u4eba\u6b65\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAMP+SAC\u6846\u67b6\u4e0d\u4ec5\u4fdd\u6301\u7a33\u5b9a\u7684\u4efb\u52a1\u6267\u884c\uff0c\u8fd8\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684AMP+PPO\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u6a21\u4eff\u5956\u52b1\uff0c\u8bc1\u660e\u4e86\u79bb\u7b56\u7565IL\u5728\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u79bb\u7b56\u7565\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u884c\u4e3a\u548c\u4efb\u52a1\u6267\u884c\uff0c\u540c\u65f6\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.24575", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.24575", "abs": "https://arxiv.org/abs/2509.24575", "authors": ["Nicolas Pfitzer", "Eduardo Sebasti\u00e1n", "Ajay Shankar", "Amanda Prorok"], "title": "Prompting Robot Teams with Natural Language", "comment": null, "summary": "This paper presents a framework towards prompting multi-robot teams with\nhigh-level tasks using natural language expressions. Our objective is to use\nthe reasoning capabilities demonstrated by recent language models in\nunderstanding and decomposing human expressions of intent, and repurpose these\nfor multi-robot collaboration and decision-making. The key challenge is that an\nindividual's behavior in a collective can be hard to specify and interpret, and\nmust continuously adapt to actions from others. This necessitates a framework\nthat possesses the representational capacity required by the logic and\nsemantics of a task, and yet supports decentralized and interactive real-time\noperation. We solve this dilemma by recognizing that a task can be represented\nas a deterministic finite automaton (DFA), and that recurrent neural networks\n(RNNs) can encode numerous automata. This allows us to distill the logic and\nsequential decompositions of sub-tasks obtained from a language model into an\nRNN, and align its internal states with the semantics of a given task. By\ntraining a graph neural network (GNN) control policy that is conditioned on the\nhidden states of the RNN and the language embeddings, our method enables robots\nto execute task-relevant actions in a decentralized manner. We present\nevaluations of this single light-weight interpretable model on various\nsimulated and real-world multi-robot tasks that require sequential and\ncollaborative behavior by the team -- sites.google.com/view/prompting-teams.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u591a\u673a\u5668\u4eba\u56e2\u961f\u6267\u884c\u9ad8\u7ea7\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u51b3\u7b56\u76f8\u7ed3\u5408", "motivation": "\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u8868\u8fbe\u7684\u80fd\u529b\uff0c\u5c06\u5176\u91cd\u65b0\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u548c\u51b3\u7b56\u5236\u5b9a\uff0c\u89e3\u51b3\u96c6\u4f53\u4e2d\u4e2a\u4f53\u884c\u4e3a\u96be\u4ee5\u6307\u5b9a\u548c\u89e3\u91ca\u7684\u6311\u6218", "method": "\u5c06\u4efb\u52a1\u8868\u793a\u4e3a\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a(DFA)\uff0c\u4f7f\u7528RNN\u7f16\u7801\u591a\u4e2a\u81ea\u52a8\u673a\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u7684\u903b\u8f91\u548c\u5b50\u4efb\u52a1\u5e8f\u5217\u5206\u89e3\u63d0\u70bc\u5230RNN\u4e2d\uff0c\u5e76\u8bad\u7ec3\u57fa\u4e8eRNN\u9690\u85cf\u72b6\u6001\u548c\u8bed\u8a00\u5d4c\u5165\u7684GNN\u63a7\u5236\u7b56\u7565", "result": "\u5728\u9700\u8981\u987a\u5e8f\u548c\u534f\u4f5c\u884c\u4e3a\u7684\u5404\u79cd\u6a21\u62df\u548c\u5b9e\u9645\u591a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86\u8fd9\u79cd\u8f7b\u91cf\u7ea7\u53ef\u89e3\u91ca\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ee5\u5206\u6563\u65b9\u5f0f\u6267\u884c\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u52a8\u4f5c\uff0c\u652f\u6301\u5b9e\u65f6\u4ea4\u4e92\u64cd\u4f5c"}}
{"id": "2509.24579", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24579", "abs": "https://arxiv.org/abs/2509.24579", "authors": ["Linzhi Wu", "Aoran Mei", "Xiyue Wang", "Guo-Niu Zhu", "Zhongxue Gan"], "title": "U-DiT Policy: U-shaped Diffusion Transformers for Robotic Manipulation", "comment": null, "summary": "Diffusion-based methods have been acknowledged as a powerful paradigm for\nend-to-end visuomotor control in robotics. Most existing approaches adopt a\nDiffusion Policy in U-Net architecture (DP-U), which, while effective, suffers\nfrom limited global context modeling and over-smoothing artifacts. To address\nthese issues, we propose U-DiT Policy, a novel U-shaped Diffusion Transformer\nframework. U-DiT preserves the multi-scale feature fusion advantages of U-Net\nwhile integrating the global context modeling capability of Transformers,\nthereby enhancing representational power and policy expressiveness. We evaluate\nU-DiT extensively across both simulation and real-world robotic manipulation\ntasks. In simulation, U-DiT achieves an average performance gain of 10\\% over\nbaseline methods and surpasses Transformer-based diffusion policies (DP-T) that\nuse AdaLN blocks by 6\\% under comparable parameter budgets. On real-world\nrobotic tasks, U-DiT demonstrates superior generalization and robustness,\nachieving an average improvement of 22.5\\% over DP-U. In addition, robustness\nand generalization experiments under distractor and lighting variations further\nhighlight the advantages of U-DiT. These results highlight the effectiveness\nand practical potential of U-DiT Policy as a new foundation for diffusion-based\nrobotic manipulation.", "AI": {"tldr": "\u63d0\u51faU-DiT Policy\uff0c\u4e00\u79cd\u7ed3\u5408U-Net\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548cTransformer\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u7684\u65b0\u578b\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528U-Net\u67b6\u6784\uff08DP-U\uff09\uff0c\u5b58\u5728\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u6709\u9650\u548c\u8fc7\u5ea6\u5e73\u6ed1\u4f2a\u5f71\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1U\u5f62\u6269\u6563Transformer\u6846\u67b6\uff0c\u4fdd\u7559U-Net\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u4f18\u52bf\uff0c\u540c\u65f6\u96c6\u6210Transformer\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u4eff\u771f\u4efb\u52a1\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u534710%\uff0c\u8d85\u8d8a\u57fa\u4e8eTransformer\u7684\u6269\u6563\u7b56\u75656%\uff1b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u534722.5%\uff0c\u5728\u5e72\u6270\u548c\u5149\u7167\u53d8\u5316\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "U-DiT Policy\u4f5c\u4e3a\u57fa\u4e8e\u6269\u6563\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u65b0\u57fa\u7840\uff0c\u5c55\u73b0\u51fa\u6709\u6548\u6027\u548c\u5b9e\u9645\u6f5c\u529b\u3002"}}
{"id": "2509.24591", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24591", "abs": "https://arxiv.org/abs/2509.24591", "authors": ["Haozhuo Zhang", "Michele Caprio", "Jing Shao", "Qiang Zhang", "Jian Tang", "Shanghang Zhang", "Wei Pan"], "title": "PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control", "comment": null, "summary": "We present PoseDiff, a conditional diffusion model that unifies robot state\nestimation and control within a single framework. At its core, PoseDiff maps\nraw visual observations into structured robot states-such as 3D keypoints or\njoint angles-from a single RGB image, eliminating the need for multi-stage\npipelines or auxiliary modalities. Building upon this foundation, PoseDiff\nextends naturally to video-to-action inverse dynamics: by conditioning on\nsparse video keyframes generated by world models, it produces smooth and\ncontinuous long-horizon action sequences through an overlap-averaging strategy.\nThis unified design enables scalable and efficient integration of perception\nand control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy\nand real-time performance for pose estimation. On Libero-Object manipulation\ntasks, it substantially improves success rates over existing inverse dynamics\nmodules, even under strict offline settings. Together, these results show that\nPoseDiff provides a scalable, accurate, and efficient bridge between\nperception, planning, and control in embodied AI. The video visualization\nresults can be found on the project page:\nhttps://haozhuo-zhang.github.io/PoseDiff-project-page/.", "AI": {"tldr": "PoseDiff\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u548c\u63a7\u5236\u6574\u5408\u5728\u4e00\u8d77\uff0c\u80fd\u591f\u4ece\u5355\u5f20RGB\u56fe\u50cf\u4f30\u8ba1\u673a\u5668\u4eba\u72b6\u6001\uff0c\u5e76\u751f\u6210\u5e73\u6ed1\u7684\u957f\u65f6\u7a0b\u52a8\u4f5c\u5e8f\u5217\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u591a\u9636\u6bb5\u6d41\u7a0b\u548c\u8f85\u52a9\u6a21\u6001\u6765\u8fde\u63a5\u611f\u77e5\u4e0e\u63a7\u5236\uff0cPoseDiff\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u9ad8\u6548\u6574\u5408\u611f\u77e5\u4e0e\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u4ece\u539f\u59cb\u89c6\u89c9\u89c2\u5bdf\u6620\u5c04\u5230\u7ed3\u6784\u5316\u673a\u5668\u4eba\u72b6\u6001\uff0c\u5e76\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u751f\u6210\u7684\u7a00\u758f\u89c6\u9891\u5173\u952e\u5e27\u8fdb\u884c\u89c6\u9891\u5230\u52a8\u4f5c\u7684\u9006\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u91c7\u7528\u91cd\u53e0\u5e73\u5747\u7b56\u7565\u751f\u6210\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728DREAM\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\uff1b\u5728Libero-Object\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5373\u4f7f\u5728\u4e25\u683c\u7684\u79bb\u7ebf\u8bbe\u7f6e\u4e0b\u3002", "conclusion": "PoseDiff\u4e3a\u5177\u8eabAI\u4e2d\u7684\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u6865\u6881\u3002"}}
{"id": "2509.24661", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24661", "abs": "https://arxiv.org/abs/2509.24661", "authors": ["Zhiyuan Wu", "Rolandos Alexandros Potamias", "Xuyang Zhang", "Zhongqun Zhang", "Jiankang Deng", "Shan Luo"], "title": "CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations", "comment": null, "summary": "Cross-embodiment dexterous grasp synthesis refers to adaptively generating\nand optimizing grasps for various robotic hands with different morphologies.\nThis capability is crucial for achieving versatile robotic manipulation in\ndiverse environments and requires substantial amounts of reliable and diverse\ngrasp data for effective model training and robust generalization. However,\nexisting approaches either rely on physics-based optimization that lacks\nhuman-like kinematic understanding or require extensive manual data collection\nprocesses that are limited to anthropomorphic structures. In this paper, we\npropose CEDex, a novel cross-embodiment dexterous grasp synthesis method at\nscale that bridges human grasping kinematics and robot kinematics by aligning\nrobot kinematic models with generated human-like contact representations. Given\nan object's point cloud and an arbitrary robotic hand model, CEDex first\ngenerates human-like contact representations using a Conditional Variational\nAuto-encoder pretrained on human contact data. It then performs kinematic human\ncontact alignment through topological merging to consolidate multiple human\nhand parts into unified robot components, followed by a signed distance\nfield-based grasp optimization with physics-aware constraints. Using CEDex, we\nconstruct the largest cross-embodiment grasp dataset to date, comprising 500K\nobjects across four gripper types with 20M total grasps. Extensive experiments\nshow that CEDex outperforms state-of-the-art approaches and our dataset\nbenefits cross-embodiment grasp learning with high-quality diverse grasps.", "AI": {"tldr": "\u63d0\u51faCEDex\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u6a21\u578b\u4e0e\u751f\u6210\u7684\u4eba\u7c7b\u63a5\u89e6\u8868\u793a\u5bf9\u9f50\uff0c\u5b9e\u73b0\u8de8\u5f62\u6001\u7075\u5de7\u6293\u53d6\u5408\u6210\uff0c\u5e76\u6784\u5efa\u4e86\u6700\u5927\u7684\u8de8\u5f62\u6001\u6293\u53d6\u6570\u636e\u96c6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u7f3a\u4e4f\u4eba\u7c7b\u8fd0\u52a8\u5b66\u7406\u89e3\u7684\u7269\u7406\u4f18\u5316\uff0c\u8981\u4e48\u9700\u8981\u5c40\u9650\u4e8e\u62df\u4eba\u7ed3\u6784\u7684\u624b\u52a8\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\uff0c\u9700\u8981\u53ef\u9760\u591a\u6837\u7684\u6293\u53d6\u6570\u636e\u6765\u5b9e\u73b0\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c", "method": "\u4f7f\u7528\u5728\u4eba\u7c7b\u63a5\u89e6\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u4eba\u7c7b\u63a5\u89e6\u8868\u793a\uff0c\u901a\u8fc7\u62d3\u6251\u5408\u5e76\u8fdb\u884c\u8fd0\u52a8\u5b66\u5bf9\u9f50\uff0c\u7136\u540e\u8fdb\u884c\u57fa\u4e8e\u7b26\u53f7\u8ddd\u79bb\u573a\u7684\u6293\u53d6\u4f18\u5316", "result": "\u6784\u5efa\u4e86\u5305\u542b500K\u4e2a\u7269\u4f53\u3001\u56db\u79cd\u5939\u722a\u7c7b\u578b\u3001\u603b\u8ba12000\u4e07\u6293\u53d6\u7684\u6700\u5927\u8de8\u5f62\u6001\u6293\u53d6\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f", "conclusion": "CEDex\u901a\u8fc7\u6865\u63a5\u4eba\u7c7b\u6293\u53d6\u8fd0\u52a8\u5b66\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8de8\u5f62\u6001\u7075\u5de7\u6293\u53d6\u5408\u6210\uff0c\u6570\u636e\u96c6\u5bf9\u8de8\u5f62\u6001\u6293\u53d6\u5b66\u4e60\u6709\u76ca"}}
{"id": "2509.24697", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.24697", "abs": "https://arxiv.org/abs/2509.24697", "authors": ["Evelyn D'Elia", "Paolo Maria Viceconte", "Lorenzo Rapetti", "Diego Ferigo", "Giulio Romualdi", "Giuseppe L'Erario", "Raffaello Camoriano", "Daniele Pucci"], "title": "Stabilizing Humanoid Robot Trajectory Generation via Physics-Informed Learning and Control-Informed Steering", "comment": "This paper has been accepted for publication at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS), Hangzhou,\n  China, 2025", "summary": "Recent trends in humanoid robot control have successfully employed imitation\nlearning to enable the learned generation of smooth, human-like trajectories\nfrom human data. While these approaches make more realistic motions possible,\nthey are limited by the amount of available motion data, and do not incorporate\nprior knowledge about the physical laws governing the system and its\ninteractions with the environment. Thus they may violate such laws, leading to\ndivergent trajectories and sliding contacts which limit real-world stability.\nWe address such limitations via a two-pronged learning strategy which leverages\nthe known physics of the system and fundamental control principles. First, we\nencode physics priors during supervised imitation learning to promote\ntrajectory feasibility. Second, we minimize drift at inference time by applying\na proportional-integral controller directly to the generated output state. We\nvalidate our method on various locomotion behaviors for the ergoCub humanoid\nrobot, where a physics-informed loss encourages zero contact foot velocity. Our\nexperiments demonstrate that the proposed approach is compatible with multiple\ncontrollers on a real robot and significantly improves the accuracy and\nphysical constraint conformity of generated trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u548c\u6bd4\u4f8b\u79ef\u5206\u63a7\u5236\u5668\u7684\u53cc\u7ba1\u9f50\u4e0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u8f68\u8ff9\u751f\u6210\u7684\u7269\u7406\u53ef\u884c\u6027\u548c\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u91cf\u4e14\u7f3a\u4e4f\u7269\u7406\u77e5\u8bc6\uff0c\u53ef\u80fd\u5bfc\u81f4\u8f68\u8ff9\u53d1\u6563\u548c\u6ed1\u52a8\u63a5\u89e6\u95ee\u9898\uff0c\u5f71\u54cd\u771f\u5b9e\u4e16\u754c\u7a33\u5b9a\u6027", "method": "1) \u5728\u76d1\u7763\u6a21\u4eff\u5b66\u4e60\u4e2d\u7f16\u7801\u7269\u7406\u5148\u9a8c\u4ee5\u4fc3\u8fdb\u8f68\u8ff9\u53ef\u884c\u6027\uff1b2) \u5728\u63a8\u7406\u65f6\u5e94\u7528\u6bd4\u4f8b\u79ef\u5206\u63a7\u5236\u5668\u6700\u5c0f\u5316\u6f02\u79fb", "result": "\u5728ergoCub\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u7269\u7406\u4fe1\u606f\u635f\u5931\u9f13\u52b1\u96f6\u63a5\u89e6\u8db3\u90e8\u901f\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u8f68\u8ff9\u7cbe\u5ea6\u548c\u7269\u7406\u7ea6\u675f\u7b26\u5408\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u4e0e\u591a\u79cd\u63a7\u5236\u5668\u517c\u5bb9\uff0c\u80fd\u6709\u6548\u63d0\u5347\u751f\u6210\u8f68\u8ff9\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u7ea6\u675f\u4e00\u81f4\u6027"}}
{"id": "2509.24706", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24706", "abs": "https://arxiv.org/abs/2509.24706", "authors": ["Andreea Tulbure", "Rene Zurbruegg", "Timm Grigat", "Marco Hutter"], "title": "LLM-Handover:Exploiting LLMs for Task-Oriented Robot-Human Handovers", "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Effective human-robot collaboration depends on task-oriented handovers, where\nrobots present objects in ways that support the partners intended use. However,\nmany existing approaches neglect the humans post-handover action, relying on\nassumptions that limit generalizability. To address this gap, we propose\nLLM-Handover, a novel framework that integrates large language model\n(LLM)-based reasoning with part segmentation to enable context-aware grasp\nselection and execution. Given an RGB-D image and a task description, our\nsystem infers relevant object parts and selects grasps that optimize\npost-handover usability. To support evaluation, we introduce a new dataset of\n60 household objects spanning 12 categories, each annotated with detailed part\nlabels. We first demonstrate that our approach improves the performance of the\nused state-of-the-art part segmentation method, in the context of robot-human\nhandovers. Next, we show that LLM-Handover achieves higher grasp success rates\nand adapts better to post-handover task constraints. During hardware\nexperiments, we achieve a success rate of 83% in a zero-shot setting over\nconventional and unconventional post-handover tasks. Finally, our user study\nunderlines that our method enables more intuitive, context-aware handovers,\nwith participants preferring it in 86% of cases.", "AI": {"tldr": "LLM-Handover\u6846\u67b6\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u90e8\u4ef6\u5206\u5272\uff0c\u5b9e\u73b0\u57fa\u4e8e\u4efb\u52a1\u4e0a\u4e0b\u6587\u7684\u6293\u53d6\u9009\u62e9\uff0c\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u4e2d\u7269\u4f53\u4ea4\u63a5\u540e\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4eba\u7c7b\u4ea4\u63a5\u540e\u7684\u52a8\u4f5c\uff0c\u4f9d\u8d56\u9650\u5236\u6cdb\u5316\u6027\u7684\u5047\u8bbe\uff0c\u9700\u8981\u89e3\u51b3\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6293\u53d6\u9009\u62e9\u95ee\u9898\u3002", "method": "\u4f7f\u7528RGB-D\u56fe\u50cf\u548c\u4efb\u52a1\u63cf\u8ff0\uff0c\u901a\u8fc7LLM\u63a8\u7406\u76f8\u5173\u7269\u4f53\u90e8\u4ef6\uff0c\u9009\u62e9\u4f18\u5316\u4ea4\u63a5\u540e\u53ef\u7528\u6027\u7684\u6293\u53d6\u65b9\u5f0f\uff0c\u5e76\u5f15\u5165\u5305\u542b60\u4e2a\u5bb6\u5ead\u7269\u4f53\u7684\u65b0\u6570\u636e\u96c6\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u523083%\u7684\u6210\u529f\u7387\uff0c\u7528\u6237\u7814\u7a76\u4e2d86%\u7684\u53c2\u4e0e\u8005\u504f\u597d\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u66f4\u597d\u7684\u6293\u53d6\u6210\u529f\u7387\u548c\u4efb\u52a1\u9002\u5e94\u6027\u3002", "conclusion": "LLM-Handover\u80fd\u591f\u5b9e\u73b0\u66f4\u76f4\u89c2\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7269\u4f53\u4ea4\u63a5\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u679c\u3002"}}
{"id": "2509.24733", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24733", "abs": "https://arxiv.org/abs/2509.24733", "authors": ["Zihao Xu", "Kuankuan Sima", "Junhao Deng", "Zixuan Zhuang", "Chunzheng Wang", "Ce Hao", "Jin Song Dong"], "title": "APREBot: Active Perception System for Reflexive Evasion Robot", "comment": null, "summary": "Reliable onboard perception is critical for quadruped robots navigating\ndynamic environments, where obstacles can emerge from any direction under\nstrict reaction-time constraints. Single-sensor systems face inherent\nlimitations: LiDAR provides omnidirectional coverage but lacks rich texture\ninformation, while cameras capture high-resolution detail but suffer from\nrestricted field of view. We introduce APREBot (Active Perception System for\nReflexive Evasion Robot), a novel framework that integrates reflexive evasion\nwith active hierarchical perception. APREBot strategically combines LiDAR-based\nomnidirectional scanning with camera-based active focusing, achieving\ncomprehensive environmental awareness essential for agile obstacle avoidance in\nquadruped robots. We validate APREBot through extensive sim-to-real experiments\non a quadruped platform, evaluating diverse obstacle types, trajectories, and\napproach directions. Our results demonstrate substantial improvements over\nstate-of-the-art baselines in both safety metrics and operational efficiency,\nhighlighting APREBot's potential for dependable autonomy in safety-critical\nscenarios. Videos are available at https://sites.google.com/view/aprebot/", "AI": {"tldr": "APREBot\u662f\u4e00\u4e2a\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u4e3b\u52a8\u611f\u77e5\u7cfb\u7edf\uff0c\u7ed3\u5408LiDAR\u5168\u5411\u626b\u63cf\u548c\u76f8\u673a\u4e3b\u52a8\u805a\u7126\uff0c\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e2d\u7684\u654f\u6377\u907f\u969c\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u9760\u611f\u77e5\u7684\u6311\u6218\uff0c\u5355\u4f20\u611f\u5668\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u6027\uff1aLiDAR\u7f3a\u4e4f\u7eb9\u7406\u4fe1\u606f\uff0c\u76f8\u673a\u89c6\u91ce\u53d7\u9650\u3002", "method": "\u96c6\u6210\u53cd\u5c04\u5f0f\u89c4\u907f\u4e0e\u4e3b\u52a8\u5206\u5c42\u611f\u77e5\uff0c\u7b56\u7565\u6027\u7ed3\u5408LiDAR\u5168\u5411\u626b\u63cf\u548c\u76f8\u673a\u4e3b\u52a8\u805a\u7126\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5230\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u5b89\u5168\u6307\u6807\u548c\u64cd\u4f5c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "APREBot\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5177\u6709\u53ef\u9760\u81ea\u4e3b\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.24763", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24763", "abs": "https://arxiv.org/abs/2509.24763", "authors": ["Xiangyi Meng", "Delun Li", "Zihao Mao", "Yi Yang", "Wenjie Song"], "title": "SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations within a Hierarchical Exploration Framework", "comment": null, "summary": "Zero-shot object navigation in unknown environments presents significant\nchallenges, mainly due to two key limitations: insufficient semantic guidance\nleads to inefficient exploration, while limited spatial memory resulting from\nenvironmental structure causes entrapment in local regions. To address these\nissues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object\nnavigation method based on the TARE hierarchical exploration framework,\nintegrating a viewpoint generation strategy balancing spatial coverage and\nsemantic density with an LLM-based global guidance mechanism. The performance\nimprovement of the proposed method is due to two key innovations. First, the\nviewpoint generation strategy prioritizes areas of high semantic density within\ntraversable sub-regions to maximize spatial coverage and minimize invalid\nexploration. Second, coupled with an LLM-based global guidance mechanism, it\nassesses semantic associations to direct navigation toward high-value spaces,\npreventing local entrapment and ensuring efficient exploration. Deployed on\nhybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves\nreal-time operation and superior performance. On Matterport3D and\nHabitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\\% and\n11.2\\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140,\nrespectively, over state-of-the-art methods.", "AI": {"tldr": "SSR-ZSON\u662f\u4e00\u79cd\u57fa\u4e8eTARE\u5206\u5c42\u63a2\u7d22\u6846\u67b6\u7684\u7a7a\u95f4\u8bed\u4e49\u76f8\u5bf9\u96f6\u6837\u672c\u5bf9\u8c61\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u7a7a\u95f4\u8986\u76d6\u548c\u8bed\u4e49\u5bc6\u5ea6\u7684\u89c6\u70b9\u751f\u6210\u7b56\u7565\u4e0e\u57fa\u4e8eLLM\u7684\u5168\u5c40\u5f15\u5bfc\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u5bf9\u8c61\u5bfc\u822a\u4e2d\u7684\u8bed\u4e49\u5f15\u5bfc\u4e0d\u8db3\u548c\u7a7a\u95f4\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u96f6\u6837\u672c\u5bf9\u8c61\u5bfc\u822a\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8bed\u4e49\u5f15\u5bfc\u4e0d\u8db3\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\uff0c\u4ee5\u53ca\u73af\u5883\u7ed3\u6784\u9020\u6210\u7684\u7a7a\u95f4\u8bb0\u5fc6\u9650\u5236\u5bfc\u81f4\u5c40\u90e8\u533a\u57df\u56f0\u9677\u3002", "method": "\u63d0\u51faSSR-ZSON\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u89c6\u70b9\u751f\u6210\u7b56\u7565\u4f18\u5148\u8003\u8651\u53ef\u904d\u5386\u5b50\u533a\u57df\u5185\u7684\u9ad8\u8bed\u4e49\u5bc6\u5ea6\u533a\u57df\uff1b2\uff09\u57fa\u4e8eLLM\u7684\u5168\u5c40\u5f15\u5bfc\u673a\u5236\u8bc4\u4f30\u8bed\u4e49\u5173\u8054\uff0c\u5f15\u5bfc\u5bfc\u822a\u671d\u5411\u9ad8\u4ef7\u503c\u7a7a\u95f4\u3002", "result": "\u5728Matterport3D\u548cHabitat-Matterport3D\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u548cSPL\u5206\u522b\u63d0\u9ad8\u4e8618.5%/11.2%\u548c0.181/0.140\u3002", "conclusion": "SSR-ZSON\u5b9e\u73b0\u4e86\u5b9e\u65f6\u64cd\u4f5c\u548c\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u5bf9\u8c61\u5bfc\u822a\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u5c40\u90e8\u56f0\u9677\u95ee\u9898\u3002"}}
{"id": "2509.24768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24768", "abs": "https://arxiv.org/abs/2509.24768", "authors": ["Eric Hannus", "Miika Malin", "Tran Nguyen Le", "Ville Kyrki"], "title": "IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks", "comment": "Under review for ICRA 2026", "summary": "Vision-language-action models (VLAs) have become an increasingly popular\napproach for addressing robot manipulation problems in recent years. However,\nsuch models need to output actions at a rate suitable for robot control, which\nlimits the size of the language model they can be based on, and consequently,\ntheir language understanding capabilities. Manipulation tasks may require\ncomplex language instructions, such as identifying target objects by their\nrelative positions, to specify human intention. Therefore, we introduce IA-VLA,\na framework that utilizes the extensive language understanding of a large\nvision language model as a pre-processing stage to generate improved context to\naugment the input of a VLA. We evaluate the framework on a set of semantically\ncomplex tasks which have been underexplored in VLA literature, namely tasks\ninvolving visual duplicates, i.e., visually indistinguishable objects. A\ndataset of three types of scenes with duplicate objects is used to compare a\nbaseline VLA against two augmented variants. The experiments show that the VLA\nbenefits from the augmentation scheme, especially when faced with language\ninstructions that require the VLA to extrapolate from concepts it has seen in\nthe demonstrations. For the code, dataset, and videos, see\nhttps://sites.google.com/view/ia-vla.", "AI": {"tldr": "IA-VLA\u6846\u67b6\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4f5c\u4e3a\u9884\u5904\u7406\u9636\u6bb5\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b(VLA)\u751f\u6210\u6539\u8fdb\u7684\u4e0a\u4e0b\u6587\u8f93\u5165\uff0c\u4ee5\u5904\u7406\u6d89\u53ca\u89c6\u89c9\u91cd\u590d\u5bf9\u8c61\u7684\u590d\u6742\u8bed\u4e49\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b(VLA)\u7531\u4e8e\u9700\u8981\u8f93\u51fa\u9002\u5408\u673a\u5668\u4eba\u63a7\u5236\u9891\u7387\u7684\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u5176\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u6a21\uff0c\u4ece\u800c\u5f71\u54cd\u4e86\u7406\u89e3\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u7684\u80fd\u529b\u3002\u7279\u522b\u662f\u5728\u5904\u7406\u9700\u8981\u6839\u636e\u76f8\u5bf9\u4f4d\u7f6e\u8bc6\u522b\u76ee\u6807\u5bf9\u8c61\u7b49\u590d\u6742\u8bed\u4e49\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faIA-VLA\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9884\u5904\u7406\u9636\u6bb5\uff0c\u751f\u6210\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u6539\u8fdbVLA\u7684\u8f93\u5165\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u6d89\u53ca\u89c6\u89c9\u91cd\u590d\u5bf9\u8c61\uff08\u89c6\u89c9\u4e0a\u65e0\u6cd5\u533a\u5206\u7684\u5bf9\u8c61\uff09\u7684\u590d\u6742\u8bed\u4e49\u4efb\u52a1\u8fdb\u884c\u8bbe\u8ba1\u3002", "result": "\u5728\u5305\u542b\u91cd\u590d\u5bf9\u8c61\u7684\u4e09\u7c7b\u573a\u666f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5f3a\u540e\u7684VLA\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9700\u8981\u4ece\u6f14\u793a\u4e2d\u63a8\u65ad\u6982\u5ff5\u7684\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u65f6\u6548\u679c\u660e\u663e\u3002", "conclusion": "IA-VLA\u6846\u67b6\u901a\u8fc7\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u589e\u5f3aVLA\u7684\u8f93\u5165\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5904\u7406\u590d\u6742\u8bed\u4e49\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u9700\u8981\u63a8\u65ad\u7684\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.24797", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24797", "abs": "https://arxiv.org/abs/2509.24797", "authors": ["Zizhao Tong", "Di Chen", "Sicheng Hu", "Hongwei Fan", "Liliang Chen", "Guanghui Ren", "Hao Tang", "Hao Dong", "Ling Shao"], "title": "Fidelity-Aware Data Composition for Robust Robot Generalization", "comment": "33 pages", "summary": "Generalist robot policies trained on large-scale, visually homogeneous\ndatasets can be susceptible to shortcut learning, which impairs their\nout-of-distribution (OOD) generalization. While generative data augmentation is\na common approach to introduce diversity, it presents a subtle challenge: data\ncomposition. Naively mixing real and synthetic data can corrupt the learning\nsignal, as this process often prioritizes visual diversity at the expense of\ninformation fidelity. This paper suggests that robust generalization depends on\nprincipled, fidelity-aware data composition. We introduce Coherent Information\nFidelity Tuning (CIFT), a framework that treats data composition as an\noptimization problem. CIFT uses a practical proxy for Information Fidelity\nbased on the feature-space geometry of a dataset. This enables the\nidentification of a phase transition, termed the Decoherence Point, where\ntraining stability degrades. The framework includes a generative engine,\nMulti-View Video Augmentation (MVAug), to synthesize a causally disentangled\ndata spectrum for this tuning process. Applying CIFT to policy architectures\nsuch as $\\pi_0$ and Diffusion Policy improves OOD success rates by over 54\\%.\nThese results indicate that fidelity-aware composition, beyond data synthesis\nalone, is an important component for developing robust, general-purpose robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86CIFT\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u6570\u636e\u7ec4\u5408\u4f18\u5316\u6765\u89e3\u51b3\u673a\u5668\u4eba\u7b56\u7565\u5728\u5206\u5e03\u5916\u6cdb\u5316\u4e2d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u6210\u529f\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u901a\u624d\u673a\u5668\u4eba\u7b56\u7565\u5bb9\u6613\u53d7\u5230\u6377\u5f84\u5b66\u4e60\u5f71\u54cd\uff0c\u5bfc\u81f4\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u4f20\u7edf\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u591a\u6837\u6027\uff0c\u4f46\u53ef\u80fd\u635f\u5bb3\u4fe1\u606f\u4fdd\u771f\u5ea6\u3002", "method": "\u5f15\u5165CIFT\u6846\u67b6\uff0c\u5c06\u6570\u636e\u7ec4\u5408\u89c6\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u7279\u5f81\u7a7a\u95f4\u51e0\u4f55\u4f5c\u4e3a\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u4ee3\u7406\u6307\u6807\uff0c\u8bc6\u522b\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0b\u964d\u7684\u9000\u76f8\u5e72\u70b9\uff0c\u5e76\u5f00\u53d1MVAug\u751f\u6210\u5f15\u64ce\u5408\u6210\u56e0\u679c\u89e3\u7f20\u7684\u6570\u636e\u8c31\u3002", "result": "\u5728\u03c0\u2080\u548cDiffusion Policy\u7b49\u7b56\u7565\u67b6\u6784\u4e0a\u5e94\u7528CIFT\uff0c\u5c06OOD\u6210\u529f\u7387\u63d0\u9ad8\u4e8654%\u4ee5\u4e0a\u3002", "conclusion": "\u4fe1\u606f\u4fdd\u771f\u5ea6\u611f\u77e5\u7684\u6570\u636e\u7ec4\u5408\uff08\u800c\u4e0d\u4ec5\u4ec5\u662f\u6570\u636e\u5408\u6210\uff09\u662f\u5f00\u53d1\u9c81\u68d2\u901a\u7528\u673a\u5668\u4eba\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2509.24864", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24864", "abs": "https://arxiv.org/abs/2509.24864", "authors": ["Mingxi Zhou", "Farhang Naderi", "Yuewei Fu", "Tony Jacob", "Lin Zhao", "Manavi Panjnani", "Chengzhi Yuan", "William McConnell", "Emir Cem Gezer"], "title": "Towards Modular and Accessible AUV Systems", "comment": "6 pages, accepted by 2024 IEEE/OES Autonomous Underwater Vehicles\n  Symposium (AUV)", "summary": "This paper reports the development of a new open-access modular framework,\ncalled Marine Vehicle Packages (MVP), for Autonomous Underwater Vehicles. The\nframework consists of both software and hardware designs allowing easy\nconstruction of AUV for research with increased customizability and sufficient\npayload capacity. This paper will present the scalable hardware system design\nand the modular software design architecture. New features, such as articulated\nthruster integration and high-level Graphic User Interface will be discussed.\nBoth simulation and field experiments results are shown to highlight the\nperformance and compatibility of the MVP.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u540d\u4e3aMarine Vehicle Packages (MVP)\u7684\u65b0\u578b\u5f00\u6e90\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\uff0c\u5305\u542b\u8f6f\u786c\u4ef6\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u53ef\u5b9a\u5236\u6027\u548c\u6709\u6548\u8f7d\u8377\u80fd\u529b\u3002", "motivation": "\u4e3a\u7814\u7a76\u76ee\u7684\u63d0\u4f9b\u6613\u4e8e\u6784\u5efaAUV\u7684\u6846\u67b6\uff0c\u589e\u52a0\u53ef\u5b9a\u5236\u6027\u548c\u8db3\u591f\u7684\u6709\u6548\u8f7d\u8377\u5bb9\u91cf\u3002", "method": "\u91c7\u7528\u53ef\u6269\u5c55\u7684\u786c\u4ef6\u7cfb\u7edf\u8bbe\u8ba1\u548c\u6a21\u5757\u5316\u8f6f\u4ef6\u67b6\u6784\uff0c\u96c6\u6210\u94f0\u63a5\u63a8\u8fdb\u5668\u548c\u9ad8\u5c42\u56fe\u5f62\u7528\u6237\u754c\u9762\u7b49\u65b0\u529f\u80fd\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u73b0\u573a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MVP\u7684\u6027\u80fd\u548c\u517c\u5bb9\u6027\u3002", "conclusion": "MVP\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86AUV\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u5f00\u53d1\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u529f\u80fd\u5b8c\u5584\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24867", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24867", "abs": "https://arxiv.org/abs/2509.24867", "authors": ["Mariadas Capsran Roshan", "Edgar M Hidalgo", "Mats Isaksson", "Michelle Dunn", "Jagannatha Charjee Pyaraka"], "title": "Finding an Initial Probe Pose in Teleoperated Robotic Echocardiography via 2D LiDAR-Based 3D Reconstruction", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Echocardiography is a key imaging modality for cardiac assessment but remains\nhighly operator-dependent, and access to trained sonographers is limited in\nunderserved settings. Teleoperated robotic echocardiography has been proposed\nas a solution; however, clinical studies report longer examination times than\nmanual procedures, increasing diagnostic delays and operator workload.\nAutomating non-expert tasks, such as automatically moving the probe to an ideal\nstarting pose, offers a pathway to reduce this burden. Prior vision- and\ndepth-based approaches to estimate an initial probe pose are sensitive to\nlighting, texture, and anatomical variability. We propose a robot-mounted 2D\nLiDAR-based approach that reconstructs the chest surface in 3D and estimates\nthe initial probe pose automatically. To the best of our knowledge, this is the\nfirst demonstration of robot-mounted 2D LiDAR used for 3D reconstruction of a\nhuman body surface. Through plane-based extrinsic calibration, the\ntransformation between the LiDAR and robot base frames was estimated with an\noverall root mean square (RMS) residual of 1.8 mm and rotational uncertainty\nbelow 0.2{\\deg}. The chest front surface, reconstructed from two linear LiDAR\nsweeps, was aligned with non-rigid templates to identify an initial probe pose.\nA mannequin-based study assessing reconstruction accuracy showed mean surface\nerrors of 2.78 +/- 0.21 mm. Human trials (N=5) evaluating the proposed approach\nfound probe initial points typically 20-30 mm from the clinically defined\ninitial point, while the variation across repeated trials on the same subject\nwas less than 4 mm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u4eba\u5b89\u88c52D LiDAR\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u4f30\u8ba1\u8d85\u58f0\u5fc3\u52a8\u56fe\u68c0\u67e5\u7684\u521d\u59cb\u63a2\u5934\u4f4d\u7f6e\uff0c\u901a\u8fc73D\u91cd\u5efa\u80f8\u5ed3\u8868\u9762\u6765\u51cf\u5c11\u673a\u5668\u4eba\u8d85\u58f0\u68c0\u67e5\u7684\u65f6\u95f4\u8d1f\u62c5\u3002", "motivation": "\u89e3\u51b3\u8fdc\u7a0b\u673a\u5668\u4eba\u8d85\u58f0\u5fc3\u52a8\u56fe\u68c0\u67e5\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5149\u7167\u3001\u7eb9\u7406\u548c\u89e3\u5256\u53d8\u5f02\u654f\u611f\uff0c\u9700\u8981\u81ea\u52a8\u5316\u975e\u4e13\u5bb6\u4efb\u52a1\u6765\u51cf\u8f7b\u64cd\u4f5c\u8d1f\u62c5\u3002", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u5b89\u88c5\u76842D LiDAR\u8fdb\u884c\u80f8\u5ed3\u8868\u97623D\u91cd\u5efa\uff0c\u901a\u8fc7\u5e73\u9762\u6807\u5b9a\u4f30\u8ba1LiDAR\u4e0e\u673a\u5668\u4eba\u57fa\u5ea7\u4e4b\u95f4\u7684\u53d8\u6362\uff0c\u7136\u540e\u4f7f\u7528\u975e\u521a\u6027\u6a21\u677f\u5bf9\u9f50\u6765\u8bc6\u522b\u521d\u59cb\u63a2\u5934\u4f4d\u7f6e\u3002", "result": "\u6807\u5b9a\u7cbe\u5ea6RMS\u6b8b\u5dee1.8mm\uff0c\u65cb\u8f6c\u4e0d\u786e\u5b9a\u6027\u4f4e\u4e8e0.2\u5ea6\uff1b\u4eba\u4f53\u6a21\u578b\u91cd\u5efa\u8bef\u5dee2.78\u00b10.21mm\uff1b\u4eba\u4f53\u8bd5\u9a8c\u4e2d\u521d\u59cb\u70b9\u8ddd\u79bb\u4e34\u5e8a\u5b9a\u4e49\u70b920-30mm\uff0c\u540c\u4e00\u53d7\u8bd5\u8005\u91cd\u590d\u8bd5\u9a8c\u53d8\u5f02\u5c0f\u4e8e4mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5c55\u793a\u4e86\u673a\u5668\u4eba\u5b89\u88c52D LiDAR\u7528\u4e8e\u4eba\u4f53\u8868\u97623D\u91cd\u5efa\uff0c\u80fd\u591f\u53ef\u9760\u5730\u81ea\u52a8\u4f30\u8ba1\u8d85\u58f0\u63a2\u5934\u521d\u59cb\u4f4d\u7f6e\uff0c\u4e3a\u673a\u5668\u4eba\u8d85\u58f0\u68c0\u67e5\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24892", "abs": "https://arxiv.org/abs/2509.24892", "authors": ["Shilong Ji", "Yinuo Chen", "Chuqi Wang", "Jiayu Chen", "Ruize Zhang", "Feng Gao", "Wenhao Tang", "Shu'ang Yu", "Sirui Xiang", "Xinlei Chen", "Chao Yu", "Yu Wang"], "title": "JuggleRL: Mastering Ball Juggling with a Quadrotor via Deep Reinforcement Learning", "comment": null, "summary": "Aerial robots interacting with objects must perform precise, contact-rich\nmaneuvers under uncertainty. In this paper, we study the problem of aerial ball\njuggling using a quadrotor equipped with a racket, a task that demands accurate\ntiming, stable control, and continuous adaptation. We propose JuggleRL, the\nfirst reinforcement learning-based system for aerial juggling. It learns\nclosed-loop policies in large-scale simulation using systematic calibration of\nquadrotor and ball dynamics to reduce the sim-to-real gap. The training\nincorporates reward shaping to encourage racket-centered hits and sustained\njuggling, as well as domain randomization over ball position and coefficient of\nrestitution to enhance robustness and transferability. The learned policy\noutputs mid-level commands executed by a low-level controller and is deployed\nzero-shot on real hardware, where an enhanced perception module with a\nlightweight communication protocol reduces delays in high-frequency state\nestimation and ensures real-time control. Experiments show that JuggleRL\nachieves an average of $311$ hits over $10$ consecutive trials in the real\nworld, with a maximum of $462$ hits observed, far exceeding a model-based\nbaseline that reaches at most $14$ hits with an average of $3.1$. Moreover, the\npolicy generalizes to unseen conditions, successfully juggling a lighter $5$ g\nball with an average of $145.9$ hits. This work demonstrates that reinforcement\nlearning can empower aerial robots with robust and stable control in dynamic\ninteraction tasks.", "AI": {"tldr": "JuggleRL\u662f\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7a7a\u4e2d\u6742\u800d\u7cfb\u7edf\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u4eff\u771f\u5b66\u4e60\u95ed\u73af\u7b56\u7565\uff0c\u5728\u771f\u5b9e\u56db\u65cb\u7ffc\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u5e73\u5747\u80fd\u8fde\u7eed\u51fb\u7403311\u6b21\uff0c\u8fdc\u8d85\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "motivation": "\u7a7a\u4e2d\u673a\u5668\u4eba\u9700\u8981\u4e0e\u7269\u4f53\u8fdb\u884c\u7cbe\u786e\u7684\u63a5\u89e6\u5f0f\u4ea4\u4e92\uff0c\u4f46\u9762\u4e34\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002\u7a7a\u4e2d\u7403\u7c7b\u6742\u800d\u4efb\u52a1\u8981\u6c42\u51c6\u786e\u7684\u65f6\u95f4\u63a7\u5236\u3001\u7a33\u5b9a\u63a7\u5236\u548c\u6301\u7eed\u9002\u5e94\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u7cfb\u7edf\u6821\u51c6\u7684\u56db\u65cb\u7ffc\u548c\u7403\u4f53\u52a8\u529b\u5b66\u6765\u51cf\u5c0f\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u5956\u52b1\u5851\u9020\u9f13\u52b1\u7403\u62cd\u4e2d\u5fc3\u51fb\u7403\u548c\u6301\u7eed\u6742\u800d\uff0c\u91c7\u7528\u9886\u57df\u968f\u673a\u5316\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u8f93\u51fa\u4e2d\u5c42\u7ea7\u547d\u4ee4\u5e76\u7531\u5e95\u5c42\u63a7\u5236\u5668\u6267\u884c\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5e73\u5747\u8fde\u7eed\u51fb\u7403311\u6b21\uff0c\u6700\u9ad8\u8fbe462\u6b21\uff0c\u8fdc\u8d85\u57fa\u4e8e\u6a21\u578b\u57fa\u7ebf\uff08\u6700\u591a14\u6b21\uff0c\u5e73\u57473.1\u6b21\uff09\uff1b\u8fd8\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u76845\u514b\u8f7b\u7403\uff0c\u5e73\u5747\u51fb\u7403145.9\u6b21\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u4e3a\u7a7a\u4e2d\u673a\u5668\u4eba\u5728\u52a8\u6001\u4ea4\u4e92\u4efb\u52a1\u4e2d\u63d0\u4f9b\u9c81\u68d2\u7a33\u5b9a\u7684\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2509.24903", "categories": ["cs.RO", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.24903", "abs": "https://arxiv.org/abs/2509.24903", "authors": ["Lantao Li", "Kang Yang", "Rui Song", "Chen Sun"], "title": "DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving Beyond Limits", "comment": null, "summary": "Cooperative perception enabled by Vehicle-to-Everything communication has\nshown great promise in enhancing situational awareness for autonomous vehicles\nand other mobile robotic platforms. Despite recent advances in perception\nbackbones and multi-agent fusion, real-world deployments remain challenged by\nhard detection cases, exemplified by partial detections and noise accumulation\nwhich limit downstream detection accuracy. This work presents Diffusion on\nReinforced Cooperative Perception (DRCP), a real-time deployable framework\ndesigned to address aforementioned issues in dynamic driving environments. DRCP\nintegrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent,\na cross-modal cooperative perception module that leverages\ncamera-intrinsic-aware angular partitioning for attention-based fusion and\nadaptive convolution to better exploit external features; and (2)\nMask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement\nmodule that encourages robustness against feature perturbations and aligns\nbird's-eye-view features closer to the task-optimal manifold. The proposed\nsystem achieves real-time performance on mobile platforms while significantly\nimproving robustness under challenging conditions. Code will be released in\nlate 2025.", "AI": {"tldr": "DRCP\u662f\u4e00\u4e2a\u5b9e\u65f6\u53ef\u90e8\u7f72\u7684\u534f\u540c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u6a21\u6001\u534f\u540c\u611f\u77e5\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u6269\u6563\u7ec6\u5316\u6a21\u5757\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u52a8\u6001\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u9762\u4e34\u7684\u786c\u68c0\u6d4b\u95ee\u9898\uff0c\u5982\u90e8\u5206\u68c0\u6d4b\u548c\u566a\u58f0\u7d2f\u79ef\uff0c\u8fd9\u4e9b\u9650\u5236\u4e86\u4e0b\u6e38\u68c0\u6d4b\u7cbe\u5ea6\u3002", "method": "\u96c6\u6210\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aPrecise-Pyramid-Cross-Modality-Cross-Agent\uff08\u5229\u7528\u76f8\u673a\u5185\u53c2\u611f\u77e5\u7684\u89d2\u5ea6\u5206\u533a\u8fdb\u884c\u6ce8\u610f\u529b\u878d\u5408\u548c\u81ea\u9002\u5e94\u5377\u79ef\uff09\u548cMask-Diffusion-Mask-Aggregation\uff08\u8f7b\u91cf\u7ea7\u6269\u6563\u7ec6\u5316\u6a21\u5757\uff09\u3002", "result": "\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "DRCP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u534f\u540c\u611f\u77e5\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.24907", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24907", "abs": "https://arxiv.org/abs/2509.24907", "authors": ["Thanh Long Nguyen", "Duc Phu Nguyen", "Thanh Thao Ton Nu", "Quan Le", "Thuan Hoang Tran", "Manh Duong Phung"], "title": "Real-time Recognition of Human Interactions from a Single RGB-D Camera for Socially-Aware Robot Navigation", "comment": null, "summary": "{Recognizing human interactions is essential for social robots as it enables\nthem to navigate safely and naturally in shared environments. Conventional\nrobotic systems however often focus on obstacle avoidance, neglecting social\ncues necessary for seamless human-robot interaction. To address this gap, we\npropose a framework to recognize human group interactions for socially aware\nnavigation. Our method utilizes color and depth frames from a monocular RGB-D\ncamera to estimate 3D human keypoints and positions. Principal component\nanalysis (PCA) is then used to determine dominant interaction directions. The\nshoelace formula is finally applied to compute interest points and engagement\nareas. Extensive experiments have been conducted to evaluate the validity of\nthe proposed method. The results show that our method is capable of recognizing\ngroup interactions across different scenarios with varying numbers of\nindividuals. It also achieves high-speed performance, processing each frame in\napproximately 4 ms on a single-board computer used in robotic systems. The\nmethod is implemented as a ROS 2 package making it simple to integrate into\nexisting navigation systems. Source code is available at\nhttps://github.com/thanhlong103/social-interaction-detector", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u793e\u4ea4\u611f\u77e5\u5bfc\u822a\u7684\u4eba\u7c7b\u7fa4\u4f53\u4ea4\u4e92\u8bc6\u522b\u6846\u67b6\uff0c\u4f7f\u7528RGB-D\u76f8\u673a\u4f30\u8ba13D\u4eba\u4f53\u5173\u952e\u70b9\u548c\u4f4d\u7f6e\uff0c\u901a\u8fc7PCA\u786e\u5b9a\u4e3b\u8981\u4ea4\u4e92\u65b9\u5411\uff0c\u5229\u7528\u978b\u5e26\u516c\u5f0f\u8ba1\u7b97\u5174\u8da3\u70b9\u548c\u53c2\u4e0e\u533a\u57df\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u969c\u788d\u7269\u907f\u8ba9\uff0c\u5ffd\u89c6\u4e86\u793e\u4ea4\u7ebf\u7d22\uff0c\u65e0\u6cd5\u5b9e\u73b0\u65e0\u7f1d\u7684\u4eba\u673a\u4ea4\u4e92\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc6\u522b\u4eba\u7c7b\u7fa4\u4f53\u4ea4\u4e92\u7684\u793e\u4ea4\u611f\u77e5\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5355\u76eeRGB-D\u76f8\u673a\u7684\u5f69\u8272\u548c\u6df1\u5ea6\u5e27\u4f30\u8ba13D\u4eba\u4f53\u5173\u952e\u70b9\u548c\u4f4d\u7f6e\uff0c\u5e94\u7528\u4e3b\u6210\u5206\u5206\u6790\u786e\u5b9a\u4e3b\u5bfc\u4ea4\u4e92\u65b9\u5411\uff0c\u4f7f\u7528\u978b\u5e26\u516c\u5f0f\u8ba1\u7b97\u5174\u8da3\u70b9\u548c\u53c2\u4e0e\u533a\u57df\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u573a\u666f\u548c\u4eba\u6570\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u8bc6\u522b\u7fa4\u4f53\u4ea4\u4e92\uff0c\u5728\u5355\u677f\u8ba1\u7b97\u673a\u4e0a\u6bcf\u5e27\u5904\u7406\u65f6\u95f4\u7ea6\u4e3a4\u6beb\u79d2\uff0c\u5b9e\u73b0\u4e86\u9ad8\u901f\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u4eba\u7c7b\u7fa4\u4f53\u4ea4\u4e92\uff0c\u4f5c\u4e3aROS 2\u5305\u5b9e\u73b0\uff0c\u4fbf\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u66f4\u597d\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.24917", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24917", "abs": "https://arxiv.org/abs/2509.24917", "authors": ["Markus Peschl", "Pietro Mazzaglia", "Daniel Dijkman"], "title": "From Code to Action: Hierarchical Learning of Diffusion-VLM Policies", "comment": "19 pages including references, 6 figures. Accepted to CoRL LEAP 2025", "summary": "Imitation learning for robotic manipulation often suffers from limited\ngeneralization and data scarcity, especially in complex, long-horizon tasks. In\nthis work, we introduce a hierarchical framework that leverages code-generating\nvision-language models (VLMs) in combination with low-level diffusion policies\nto effectively imitate and generalize robotic behavior. Our key insight is to\ntreat open-source robotic APIs not only as execution interfaces but also as\nsources of structured supervision: the associated subtask functions - when\nexposed - can serve as modular, semantically meaningful labels. We train a VLM\nto decompose task descriptions into executable subroutines, which are then\ngrounded through a diffusion policy trained to imitate the corresponding robot\nbehavior. To handle the non-Markovian nature of both code execution and certain\nreal-world tasks, such as object swapping, our architecture incorporates a\nmemory mechanism that maintains subtask context across time. We find that this\ndesign enables interpretable policy decomposition, improves generalization when\ncompared to flat policies and enables separate evaluation of high-level\nplanning and low-level control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u4ee3\u7801\u751f\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4f4e\u5c42\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u5f00\u6e90\u673a\u5668\u4ebaAPI\u4f5c\u4e3a\u7ed3\u6784\u5316\u76d1\u7763\u6e90\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u884c\u4e3a\u7684\u6a21\u4eff\u548c\u6cdb\u5316\u3002", "motivation": "\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u5728\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5206\u89e3\u4efb\u52a1\u5e76\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u4efb\u52a1\u63cf\u8ff0\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u5b50\u7a0b\u5e8f\uff0c\u901a\u8fc7\u6269\u6563\u7b56\u7565\u6a21\u4eff\u5bf9\u5e94\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u5e76\u5f15\u5165\u8bb0\u5fc6\u673a\u5236\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u3002", "result": "\u8be5\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u5206\u89e3\uff0c\u76f8\u6bd4\u6241\u5e73\u7b56\u7565\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u5206\u522b\u8bc4\u4f30\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u63a7\u5236\u3002", "conclusion": "\u5c06\u673a\u5668\u4ebaAPI\u4f5c\u4e3a\u7ed3\u6784\u5316\u76d1\u7763\u6e90\u7684\u5206\u5c42\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u5206\u89e3\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.24921", "categories": ["cs.RO", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.24921", "abs": "https://arxiv.org/abs/2509.24921", "authors": ["Pablo Pueyo", "Fernando Caballero", "Ana Cristina Murillo", "Eduardo Montijano"], "title": "CineWild: Balancing Art and Robotics for Ethical Wildlife Documentary Filmmaking", "comment": null, "summary": "Drones, or unmanned aerial vehicles (UAVs), have become powerful tools across\ndomains-from industry to the arts. In documentary filmmaking, they offer\ndynamic, otherwise unreachable perspectives, transforming how stories are told.\nWildlife documentaries especially benefit, yet drones also raise ethical\nconcerns: the risk of disturbing the animals they aim to capture. This paper\nintroduces CineWild, an autonomous UAV framework that combines robotics,\ncinematography, and ethics. Built on model predictive control, CineWild\ndynamically adjusts flight paths and camera settings to balance cinematic\nquality with animal welfare. Key features include adaptive zoom for filming\nfrom acoustic and visual safe distances, path-planning that avoids an animal's\nfield of view, and smooth, low-noise maneuvers. CineWild exemplifies\ninterdisciplinary innovation-bridging engineering, visual storytelling, and\nenvironmental ethics. We validate the system through simulation studies and\nwill release the code upon acceptance.", "AI": {"tldr": "CineWild\u662f\u4e00\u4e2a\u7ed3\u5408\u673a\u5668\u4eba\u6280\u672f\u3001\u7535\u5f71\u6444\u5f71\u548c\u4f26\u7406\u5b66\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u52a8\u6001\u8c03\u6574\u98de\u884c\u8def\u5f84\u548c\u76f8\u673a\u8bbe\u7f6e\uff0c\u5728\u4fdd\u8bc1\u7535\u5f71\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u62a4\u52a8\u7269\u798f\u5229\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u91ce\u751f\u52a8\u7269\u7eaa\u5f55\u7247\u5236\u4f5c\u4e2d\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u89c6\u89d2\uff0c\u4f46\u4e5f\u5b58\u5728\u5e72\u6270\u52a8\u7269\u7684\u4f26\u7406\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u62cd\u6444\u8fc7\u7a0b\u4e2d\u5e73\u8861\u7535\u5f71\u8d28\u91cf\u548c\u52a8\u7269\u798f\u5229\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6784\u5efaCineWild\u6846\u67b6\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u53d8\u7126\uff08\u4ece\u58f0\u5b66\u548c\u89c6\u89c9\u5b89\u5168\u8ddd\u79bb\u62cd\u6444\uff09\u3001\u907f\u5f00\u52a8\u7269\u89c6\u91ce\u7684\u8def\u5f84\u89c4\u5212\u3001\u5e73\u6ed1\u4f4e\u566a\u97f3\u673a\u52a8\u7b49\u5173\u952e\u529f\u80fd\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u53d1\u5e03\u3002", "conclusion": "CineWild\u5c55\u793a\u4e86\u8de8\u5b66\u79d1\u521b\u65b0\u7684\u4ef7\u503c\uff0c\u5c06\u5de5\u7a0b\u5b66\u3001\u89c6\u89c9\u53d9\u4e8b\u548c\u73af\u5883\u4f26\u7406\u5b66\u76f8\u7ed3\u5408\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u91ce\u751f\u52a8\u7269\u7eaa\u5f55\u7247\u4e2d\u7684\u4f26\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24928", "abs": "https://arxiv.org/abs/2509.24928", "authors": ["Shunan Yin", "Zehui Lu", "Shaoshuai Mou"], "title": "Trajectory Prediction via Bayesian Intention Inference under Unknown Goals and Kinematics", "comment": null, "summary": "This work introduces an adaptive Bayesian algorithm for real-time trajectory\nprediction via intention inference, where a target's intentions and motion\ncharacteristics are unknown and subject to change. The method concurrently\nestimates two critical variables: the target's current intention, modeled as a\nMarkovian latent state, and an intention parameter that describes the target's\nadherence to a shortest-path policy. By integrating this joint update\ntechnique, the algorithm maintains robustness against abrupt intention shifts\nand unknown motion dynamics. A sampling-based trajectory prediction mechanism\nthen exploits these adaptive estimates to generate probabilistic forecasts with\nquantified uncertainty. We validate the framework through numerical\nexperiments: Ablation studies of two cases, and a 500-trial Monte Carlo\nanalysis; Hardware demonstrations on quadrotor and quadrupedal platforms.\nExperimental results demonstrate that the proposed approach significantly\noutperforms non-adaptive and partially adaptive methods. The method operates in\nreal time around 270 Hz without requiring training or detailed prior knowledge\nof target behavior, showcasing its applicability in various robotic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u8d1d\u53f6\u65af\u7b97\u6cd5\uff0c\u901a\u8fc7\u610f\u56fe\u63a8\u7406\u5b9e\u73b0\u5b9e\u65f6\u8f68\u8ff9\u9884\u6d4b\uff0c\u80fd\u591f\u5904\u7406\u76ee\u6807\u610f\u56fe\u548c\u8fd0\u52a8\u7279\u6027\u7684\u672a\u77e5\u53d8\u5316\u3002", "motivation": "\u76ee\u6807\u5728\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u7684\u610f\u56fe\u548c\u8fd0\u52a8\u7279\u6027\u901a\u5e38\u662f\u672a\u77e5\u4e14\u53ef\u80fd\u7a81\u7136\u53d8\u5316\u7684\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u8fd9\u4e9b\u53d8\u5316\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u540c\u65f6\u4f30\u8ba1\u4e24\u4e2a\u5173\u952e\u53d8\u91cf\uff1a\u76ee\u6807\u7684\u5f53\u524d\u610f\u56fe\uff08\u9a6c\u5c14\u53ef\u592b\u6f5c\u5728\u72b6\u6001\uff09\u548c\u610f\u56fe\u53c2\u6570\uff08\u63cf\u8ff0\u76ee\u6807\u9075\u5faa\u6700\u77ed\u8def\u5f84\u7b56\u7565\u7684\u7a0b\u5ea6\uff09\uff0c\u91c7\u7528\u8054\u5408\u66f4\u65b0\u6280\u672f\u4fdd\u6301\u5bf9\u610f\u56fe\u7a81\u53d8\u548c\u672a\u77e5\u8fd0\u52a8\u52a8\u6001\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6570\u503c\u5b9e\u9a8c\u548c\u786c\u4ef6\u6f14\u793a\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u975e\u81ea\u9002\u5e94\u548c\u90e8\u5206\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5b9e\u65f6\u8fd0\u884c\u9891\u7387\u8fbe270Hz\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u5bf9\u76ee\u6807\u884c\u4e3a\u7684\u8be6\u7ec6\u5148\u9a8c\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u8d1d\u53f6\u65af\u7b97\u6cd5\u5728\u5404\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u610f\u56fe\u53d8\u5316\u548c\u672a\u77e5\u8fd0\u52a8\u52a8\u6001\u7684\u5b9e\u65f6\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\u3002"}}
{"id": "2509.24948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24948", "abs": "https://arxiv.org/abs/2509.24948", "authors": ["Junjin Xiao", "Yandan Yang", "Xinyuan Chang", "Ronghan Chen", "Feng Xiong", "Mu Xu", "Wei-Shi Zheng", "Qing Zhang"], "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training", "comment": null, "summary": "Vision-Language-Action (VLA) models trained via imitation learning suffer\nfrom significant performance degradation in data-scarce scenarios due to their\nreliance on large-scale demonstration datasets. Although reinforcement learning\n(RL)-based post-training has proven effective in addressing data scarcity, its\napplication to VLA models is hindered by the non-resettable nature of\nreal-world environments. This limitation is particularly critical in high-risk\ndomains such as industrial automation, where interactions often induce state\nchanges that are costly or infeasible to revert. Furthermore, existing VLA\napproaches lack a reliable mechanism for detecting task completion, leading to\nredundant actions that reduce overall task success rates. To address these\nchallenges, we propose World-Env, an RL-based post-training framework that\nreplaces physical interaction with a low-cost, world model-based virtual\nsimulator. World-Env consists of two key components: (1) a video-based world\nsimulator that generates temporally consistent future visual observations, and\n(2) a vision-language model (VLM)-guided instant reflector that provides\ncontinuous reward signals and predicts action termination. This simulated\nenvironment enables VLA models to safely explore and generalize beyond their\ninitial imitation learning distribution. Our method achieves notable\nperformance gains with as few as five expert demonstrations per task.\nExperiments on complex robotic manipulation tasks demonstrate that World-Env\neffectively overcomes the data inefficiency, safety constraints, and\ninefficient execution of conventional VLA models that rely on real-world\ninteraction, offering a practical and scalable solution for post-training in\nresource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86World-Env\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u865a\u62df\u6a21\u62df\u5668\u66ff\u4ee3\u7269\u7406\u4ea4\u4e92\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u89e3\u51b3VLA\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "motivation": "VLA\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e0d\u53ef\u91cd\u7f6e\u7684\u7279\u6027\u9650\u5236\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u5de5\u4e1a\u81ea\u52a8\u5316\u9886\u57df\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4efb\u52a1\u5b8c\u6210\u68c0\u6d4b\u673a\u5236\uff0c\u5bfc\u81f4\u5197\u4f59\u52a8\u4f5c\u964d\u4f4e\u4efb\u52a1\u6210\u529f\u7387\u3002", "method": "World-Env\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u89c6\u9891\u7684\u4e16\u754c\u6a21\u62df\u5668\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u672a\u6765\u89c6\u89c9\u89c2\u5bdf\uff0c\u4ee5\u53caVLM\u5f15\u5bfc\u7684\u5373\u65f6\u53cd\u5c04\u5668\u63d0\u4f9b\u8fde\u7eed\u5956\u52b1\u4fe1\u53f7\u5e76\u9884\u6d4b\u52a8\u4f5c\u7ec8\u6b62\u3002", "result": "\u5728\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u6bcf\u4e2a\u4efb\u52a15\u4e2a\u4e13\u5bb6\u6f14\u793a\u5c31\u80fd\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfVLA\u6a21\u578b\u7684\u6570\u636e\u4f4e\u6548\u3001\u5b89\u5168\u7ea6\u675f\u548c\u6267\u884c\u6548\u7387\u95ee\u9898\u3002", "conclusion": "World-Env\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7fVLA\u6a21\u578b\u80fd\u591f\u5b89\u5168\u63a2\u7d22\u5e76\u8d85\u8d8a\u521d\u59cb\u6a21\u4eff\u5b66\u4e60\u5206\u5e03\u3002"}}
{"id": "2509.24956", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24956", "abs": "https://arxiv.org/abs/2509.24956", "authors": ["Jan Ole von Hartz", "Lukas Schweizer", "Joschka Boedecker", "Abhinav Valada"], "title": "MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation", "comment": null, "summary": "Generative robot policies such as Flow Matching offer flexible, multi-modal\npolicy learning but are sample-inefficient. Although object-centric policies\nimprove sample efficiency, it does not resolve this limitation. In this work,\nwe propose Multi-Stream Generative Policy (MSG), an inference-time composition\nframework that trains multiple object-centric policies and combines them at\ninference to improve generalization and sample efficiency. MSG is\nmodel-agnostic and inference-only, hence widely applicable to various\ngenerative policies and training paradigms. We perform extensive experiments\nboth in simulation and on a real robot, demonstrating that our approach learns\nhigh-quality generative policies from as few as five demonstrations, resulting\nin a 95% reduction in demonstrations, and improves policy performance by 89\npercent compared to single-stream approaches. Furthermore, we present\ncomprehensive ablation studies on various composition strategies and provide\npractical recommendations for deployment. Finally, MSG enables zero-shot object\ninstance transfer. We make our code publicly available at\nhttps://msg.cs.uni-freiburg.de.", "AI": {"tldr": "\u63d0\u51fa\u4e86MSG\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2a\u7269\u4f53\u4e2d\u5fc3\u7b56\u7565\u5e76\u5728\u63a8\u7406\u65f6\u7ec4\u5408\u5b83\u4eec\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u5f0f\u673a\u5668\u4eba\u7b56\u7565\u7684\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u751f\u6210\u5f0f\u673a\u5668\u4eba\u7b56\u7565\u5982Flow Matching\u867d\u7136\u7075\u6d3b\u591a\u6a21\u6001\uff0c\u4f46\u6837\u672c\u6548\u7387\u4f4e\u3002\u73b0\u6709\u7684\u7269\u4f53\u4e2d\u5fc3\u7b56\u7565\u867d\u7136\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u672a\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "MSG\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u7ec4\u5408\u6846\u67b6\uff0c\u8bad\u7ec3\u591a\u4e2a\u7269\u4f53\u4e2d\u5fc3\u7b56\u7565\u5e76\u5728\u63a8\u7406\u65f6\u7ec4\u5408\u5b83\u4eec\u3002\u8be5\u65b9\u6cd5\u662f\u6a21\u578b\u65e0\u5173\u4e14\u4ec5\u9700\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u751f\u6210\u5f0f\u7b56\u7565\u548c\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u4ece\u4ec55\u4e2a\u6f14\u793a\u4e2d\u5b66\u4e60\u9ad8\u8d28\u91cf\u751f\u6210\u5f0f\u7b56\u7565\uff0c\u6f14\u793a\u51cf\u5c1195%\uff0c\u6027\u80fd\u6bd4\u5355\u6d41\u65b9\u6cd5\u63d0\u9ad889%\u3002\u652f\u6301\u96f6\u6837\u672c\u7269\u4f53\u5b9e\u4f8b\u8fc1\u79fb\u3002", "conclusion": "MSG\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u5f0f\u673a\u5668\u4eba\u7b56\u7565\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u5e76\u4e3a\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2509.24972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24972", "abs": "https://arxiv.org/abs/2509.24972", "authors": ["Vijja Wichitwechkarn", "Emlyn Williams", "Charles Fox", "Ruchi Choudhary"], "title": "Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks", "comment": null, "summary": "Recent advances in one-shot imitation learning have enabled robots to acquire\nnew manipulation skills from a single human demonstration. While existing\nmethods achieve strong performance on single-step tasks, they remain limited in\ntheir ability to handle long-horizon, multi-step tasks without additional model\ntraining or manual annotation. We propose a method that can be applied to this\nsetting provided a single demonstration without additional model training or\nmanual annotation. We evaluated our method on multi-step and single-step\nmanipulation tasks where our method achieves an average success rate of 82.5%\nand 90%, respectively. Our method matches and exceeds the performance of the\nbaselines in both these cases. We also compare the performance and\ncomputational efficiency of alternative pre-trained feature extractors within\nour framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6807\u6ce8\u5373\u53ef\u5904\u7406\u591a\u6b65\u9aa4\u64cd\u4f5c\u4efb\u52a1\uff0c\u5728\u5355\u6b65\u9aa4\u548c\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523090%\u548c82.5%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u5355\u6b65\u9aa4\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u91ce\u3001\u591a\u6b65\u9aa4\u4efb\u52a1\u65f6\u9700\u8981\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u6216\u4eba\u5de5\u6807\u6ce8\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u6216\u4eba\u5de5\u6807\u6ce8\u7684\u65b9\u6cd5\uff0c\u4ec5\u9700\u5355\u6b21\u6f14\u793a\u5373\u53ef\u5e94\u7528\u4e8e\u591a\u6b65\u9aa4\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u4e3a82.5%\uff0c\u5355\u6b65\u9aa4\u4efb\u52a1\u4e2d\u4e3a90%\uff0c\u5747\u8fbe\u5230\u6216\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u6269\u5c55\u4e86\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6b65\u9aa4\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u826f\u597d\u3002"}}
{"id": "2509.24995", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.24995", "abs": "https://arxiv.org/abs/2509.24995", "authors": ["Da Saem Lee", "Akash Karthikeyan", "Yash Vardhan Pant", "Sebastian Fischmeister"], "title": "Path Diffuser: Diffusion Model for Data-Driven Traffic Simulator", "comment": null, "summary": "Simulating diverse and realistic traffic scenarios is critical for developing\nand testing autonomous planning. Traditional rule-based planners lack diversity\nand realism, while learning-based simulators often replay, forecast, or edit\nscenarios using historical agent trajectories. However, they struggle to\ngenerate new scenarios, limiting scalability and diversity due to their\nreliance on fully annotated logs and historical data. Thus, a key challenge for\na learning-based simulator's performance is that it requires agents' past\ntrajectories and pose information in addition to map data, which might not be\navailable for all agents on the road.Without which, generated scenarios often\nproduce unrealistic trajectories that deviate from drivable areas, particularly\nunder out-of-distribution (OOD) map scenes (e.g., curved roads). To address\nthis, we propose Path Diffuser (PD): a two-stage, diffusion model for\ngenerating agent pose initializations and their corresponding trajectories\nconditioned on the map, free of any historical context of agents' trajectories.\nFurthermore, PD incorporates a motion primitive-based prior, leveraging Frenet\nframe candidate trajectories to enhance diversity while ensuring road-compliant\ntrajectory generation. We also explore various design choices for modeling\ncomplex multi-agent interactions. We demonstrate the effectiveness of our\nmethod through extensive experiments on the Argoverse2 Dataset and additionally\nevaluate the generalizability of the approach on OOD map variants. Notably,\nPath Diffuser outperforms the baseline methods by 1.92x on distribution\nmetrics, 1.14x on common-sense metrics, and 1.62x on road compliance from\nadversarial benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Path Diffuser\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u65e0\u9700\u5386\u53f2\u8f68\u8ff9\u4fe1\u606f\u5373\u53ef\u751f\u6210\u5730\u56fe\u6761\u4ef6\u4e0b\u7684\u667a\u80fd\u4f53\u59ff\u6001\u521d\u59cb\u5316\u548c\u8f68\u8ff9\uff0c\u901a\u8fc7Frenet\u5e27\u5019\u9009\u8f68\u8ff9\u63d0\u5347\u591a\u6837\u6027\u5e76\u786e\u4fdd\u9053\u8def\u5408\u89c4\u6027\u3002", "motivation": "\u4f20\u7edf\u89c4\u5219\u89c4\u5212\u5668\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u62df\u5668\u4f9d\u8d56\u5386\u53f2\u8f68\u8ff9\u6570\u636e\uff0c\u96be\u4ee5\u751f\u6210\u65b0\u573a\u666f\u4e14\u5728\u5904\u7406\u5206\u5e03\u5916\u5730\u56fe\u573a\u666f\u65f6\u4ea7\u751f\u4e0d\u73b0\u5b9e\u8f68\u8ff9\u3002", "method": "\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff0c\u5305\u542b\u59ff\u6001\u521d\u59cb\u5316\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u91c7\u7528Frenet\u5e27\u8fd0\u52a8\u57fa\u5143\u5148\u9a8c\u6765\u589e\u5f3a\u591a\u6837\u6027\u5e76\u786e\u4fdd\u9053\u8def\u5408\u89c4\u6027\uff0c\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u5efa\u6a21\u3002", "result": "\u5728Argoverse2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5206\u5e03\u5916\u5730\u56fe\u53d8\u4f53\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5206\u5e03\u6307\u6807\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53471.92\u500d\uff0c\u5e38\u8bc6\u6307\u6807\u63d0\u53471.14\u500d\uff0c\u9053\u8def\u5408\u89c4\u6027\u63d0\u53471.62\u500d\u3002", "conclusion": "Path Diffuser\u80fd\u591f\u6709\u6548\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u4ea4\u901a\u573a\u666f\uff0c\u65e0\u9700\u5386\u53f2\u8f68\u8ff9\u4fe1\u606f\uff0c\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2509.25032", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25032", "abs": "https://arxiv.org/abs/2509.25032", "authors": ["Ryosuke Takanami", "Petr Khrapchenkov", "Shu Morikuni", "Jumpei Arima", "Yuta Takaba", "Shunsuke Maeda", "Takuya Okubo", "Genki Sano", "Satoshi Sekioka", "Aoi Kadoya", "Motonari Kambara", "Naoya Nishiura", "Haruto Suzuki", "Takanori Yoshimoto", "Koya Sakamoto", "Shinnosuke Ono", "Hu Yang", "Daichi Yashima", "Aoi Horo", "Tomohiro Motoda", "Kensuke Chiyoma", "Hiroshi Ito", "Koki Fukuda", "Akihito Goto", "Kazumi Morinaga", "Yuya Ikeda", "Riko Kawada", "Masaki Yoshikawa", "Norio Kosuge", "Yuki Noguchi", "Kei Ota", "Tatsuya Matsushima", "Yusuke Iwasawa", "Yutaka Matsuo", "Tetsuya Ogata"], "title": "AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation", "comment": null, "summary": "As robots transition from controlled settings to unstructured human\nenvironments, building generalist agents that can reliably follow natural\nlanguage instructions remains a central challenge. Progress in robust mobile\nmanipulation requires large-scale multimodal datasets that capture contact-rich\nand long-horizon tasks, yet existing resources lack synchronized force-torque\nsensing, hierarchical annotations, and explicit failure cases. We address this\ngap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset\nfor mobile manipulation. It includes synchronized RGB images, joint states,\nsix-axis wrist force-torque signals, and internal robot states, together with a\nnovel two-layer annotation schema of sub-goals and primitive actions for\nhierarchical learning and error analysis. The initial dataset comprises 25,469\nepisodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is\nfully standardized in the LeRobot v2.1 format. By uniquely integrating mobile\nmanipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa\nprovides a critical benchmark for advancing the next generation of\nVision-Language-Action models. The first version of our dataset is now\navailable at https://huggingface.co/datasets/airoa-org/airoa-moma .", "AI": {"tldr": "\u63d0\u51fa\u4e86AIRoA MoMa\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u64cd\u4f5c\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u540c\u6b65\u7684RGB\u56fe\u50cf\u3001\u5173\u8282\u72b6\u6001\u3001\u516d\u8f74\u8155\u90e8\u529b-\u626d\u77e9\u4fe1\u53f7\u548c\u673a\u5668\u4eba\u5185\u90e8\u72b6\u6001\uff0c\u4ee5\u53ca\u7528\u4e8e\u5206\u5c42\u5b66\u4e60\u548c\u9519\u8bef\u5206\u6790\u7684\u4e24\u5c42\u6ce8\u91ca\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u4ece\u53d7\u63a7\u73af\u5883\u8f6c\u5411\u975e\u7ed3\u6784\u5316\u7684\u4eba\u7c7b\u73af\u5883\uff0c\u6784\u5efa\u80fd\u591f\u53ef\u9760\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u901a\u7528\u667a\u80fd\u4f53\u4ecd\u7136\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u7f3a\u4e4f\u540c\u6b65\u7684\u529b-\u626d\u77e9\u611f\u77e5\u3001\u5206\u5c42\u6ce8\u91ca\u548c\u660e\u786e\u7684\u5931\u8d25\u6848\u4f8b\u3002", "method": "\u6536\u96c6\u4e8625,469\u4e2a\u7247\u6bb5\uff08\u7ea694\u5c0f\u65f6\uff09\u7684\u6570\u636e\uff0c\u4f7f\u7528Human Support Robot (HSR)\u673a\u5668\u4eba\uff0c\u5305\u542b\u540c\u6b65\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u91c7\u7528\u65b0\u9896\u7684\u4e24\u5c42\u6ce8\u91ca\u6a21\u5f0f\uff08\u5b50\u76ee\u6807\u548c\u57fa\u672c\u52a8\u4f5c\uff09\u3002", "result": "\u6570\u636e\u96c6\u5df2\u6807\u51c6\u5316\u4e3aLeRobot v2.1\u683c\u5f0f\uff0c\u5e76\u5728HuggingFace\u4e0a\u516c\u5f00\u53d1\u5e03\uff0c\u4e3a\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u51c6\u3002", "conclusion": "AIRoA MoMa\u901a\u8fc7\u6574\u5408\u79fb\u52a8\u64cd\u4f5c\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u548c\u957f\u65f6\u7a0b\u7ed3\u6784\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8d44\u6e90\u7684\u7a7a\u767d\uff0c\u4e3a\u673a\u5668\u4eba\u79fb\u52a8\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2509.25056", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25056", "abs": "https://arxiv.org/abs/2509.25056", "authors": ["Kenny Truong", "Yongkyu Lee", "Jason Irie", "Shivam Kumar Panda", "Shahab Ahmad", "Md. Mukhlesur Rahman", "M. Khalid Jawed"], "title": "AgriCruiser: An Open Source Agriculture Robot for Over-the-row Navigation", "comment": "GitHub: https://github.com/structuresComp/agri-cruiser", "summary": "We present the AgriCruiser, an open-source over-the-row agricultural robot\ndeveloped for low-cost deployment and rapid adaptation across diverse crops and\nrow layouts. The chassis provides an adjustable track width of 1.42 m to 1.57\nm, along with a ground clearance of 0.94 m. The AgriCruiser achieves compact\npivot turns with radii of 0.71 m to 0.79 m, enabling efficient headland\nmaneuvers. The platform is designed for the integration of the other\nsubsystems, and in this study, a precision spraying system was implemented to\nassess its effectiveness in weed management. In twelve flax plots, a single\nrobotic spray pass reduced total weed populations (pigweed and Venice mallow)\nby 24- to 42-fold compared to manual weeding in four flax plots, while also\ncausing less crop damage. Mobility experiments conducted on concrete, asphalt,\ngravel, grass, and both wet and dry soil confirmed reliable traversal\nconsistent with torque sizing. The complete chassis can be constructed from\ncommodity T-slot extrusion with minimal machining, resulting in a bill of\nmaterials costing approximately $5,000 - $6,000, which enables replication and\ncustomization. The mentioned results demonstrate that low-cost, reconfigurable\nover-the-row robots can achieve effective weed management with reduced crop\ndamage and labor requirements, while providing a versatile foundation for\nphenotyping, sensing, and other agriculture applications. Design files and\nimplementation details are released to accelerate research and adoption of\nmodular agricultural robotics.", "AI": {"tldr": "AgriCruiser\u662f\u4e00\u6b3e\u5f00\u6e90\u519c\u4e1a\u673a\u5668\u4eba\uff0c\u5177\u6709\u53ef\u8c03\u8282\u8f68\u9053\u5bbd\u5ea6\u548c\u7d27\u51d1\u8f6c\u5f2f\u534a\u5f84\uff0c\u901a\u8fc7\u7cbe\u51c6\u55b7\u6d12\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u6742\u8349\u7ba1\u7406\uff0c\u6210\u672c\u7ea6\u4e3a5000-6000\u7f8e\u5143\u3002", "motivation": "\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u53ef\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u4f5c\u7269\u548c\u884c\u8ddd\u5e03\u5c40\u7684\u5f00\u6e90\u519c\u4e1a\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u4ee5\u964d\u4f4e\u52b3\u52a8\u529b\u9700\u6c42\u5e76\u51cf\u5c11\u4f5c\u7269\u635f\u4f24\u3002", "method": "\u4f7f\u7528\u5546\u54c1T\u578b\u69fd\u578b\u6750\u6784\u5efa\u53ef\u8c03\u8282\u5e95\u76d8\uff08\u8f68\u9053\u5bbd\u5ea61.42-1.57\u7c73\uff0c\u79bb\u5730\u95f4\u96990.94\u7c73\uff09\uff0c\u96c6\u6210\u7cbe\u51c6\u55b7\u6d12\u7cfb\u7edf\uff0c\u5728\u4e9a\u9ebb\u7530\u8fdb\u884c\u6742\u8349\u7ba1\u7406\u6d4b\u8bd5\u3002", "result": "\u5355\u6b21\u673a\u5668\u4eba\u55b7\u6d12\u4f7f\u6742\u8349\u6570\u91cf\u6bd4\u4eba\u5de5\u9664\u8349\u51cf\u5c1124-42\u500d\uff0c\u4f5c\u7269\u635f\u4f24\u66f4\u5c0f\uff1b\u5728\u591a\u79cd\u5730\u9762\u6761\u4ef6\u4e0b\u5747\u80fd\u53ef\u9760\u8fd0\u884c\u3002", "conclusion": "\u4f4e\u6210\u672c\u3001\u53ef\u91cd\u6784\u7684\u884c\u95f4\u673a\u5668\u4eba\u80fd\u6709\u6548\u7ba1\u7406\u6742\u8349\uff0c\u51cf\u5c11\u4f5c\u7269\u635f\u4f24\u548c\u52b3\u52a8\u529b\u9700\u6c42\uff0c\u4e3a\u8868\u578b\u5206\u6790\u548c\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u591a\u529f\u80fd\u5e73\u53f0\u3002"}}
{"id": "2509.25091", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25091", "abs": "https://arxiv.org/abs/2509.25091", "authors": ["Lakshan Lavan", "Lanojithan Thiyagarasa", "Udara Muthugala", "Rajitha de Silva"], "title": "Crop Spirals: Re-thinking the field layout for future robotic agriculture", "comment": "Submitted to Computers and Electronics in Agriculture", "summary": "Conventional linear crop layouts, optimised for tractors, hinder robotic\nnavigation with tight turns, long travel distances, and perceptual aliasing. We\npropose a robot-centric square spiral layout with a central tramline, enabling\nsimpler motion and more efficient coverage. To exploit this geometry, we\ndevelop a navigation stack combining DH-ResNet18 waypoint regression,\npixel-to-odometry mapping, A* planning, and model predictive control (MPC). In\nsimulations, the spiral layout yields up to 28% shorter paths and about 25%\nfaster execution for waypoint-based tasks across 500 waypoints than linear\nlayouts, while full-field coverage performance is comparable to an optimised\nlinear U-turn strategy. Multi-robot studies demonstrate efficient coordination\non the spirals rule-constrained graph, with a greedy allocator achieving 33-37%\nlower batch completion times than a Hungarian assignment under our setup. These\nresults highlight the potential of redesigning field geometry to better suit\nautonomous agriculture.", "AI": {"tldr": "\u63d0\u51fa\u673a\u5668\u4eba\u4e2d\u5fc3\u7684\u65b9\u5f62\u87ba\u65cb\u5e03\u5c40\u66ff\u4ee3\u4f20\u7edf\u7ebf\u6027\u5e03\u5c40\uff0c\u901a\u8fc7DH-ResNet18\u8def\u5f84\u70b9\u56de\u5f52\u3001\u50cf\u7d20\u5230\u91cc\u7a0b\u8ba1\u6620\u5c04\u3001A*\u89c4\u5212\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\uff0c\u5728\u4eff\u771f\u4e2d\u8def\u5f84\u7f29\u77ed28%\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c1125%\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u4f5c\u7269\u5e03\u5c40\u4e3a\u62d6\u62c9\u673a\u4f18\u5316\uff0c\u4f46\u963b\u788d\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u5b58\u5728\u6025\u8f6c\u5f2f\u3001\u957f\u8ddd\u79bb\u884c\u9a76\u548c\u611f\u77e5\u6df7\u6dc6\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u7ed3\u5408DH-ResNet18\u8def\u5f84\u70b9\u56de\u5f52\u3001\u50cf\u7d20\u5230\u91cc\u7a0b\u8ba1\u6620\u5c04\u3001A*\u89c4\u5212\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u91c7\u7528\u65b9\u5f62\u87ba\u65cb\u5e03\u5c40\u548c\u4e2d\u592e\u8f68\u9053\u7ebf\u3002", "result": "\u87ba\u65cb\u5e03\u5c40\u6bd4\u7ebf\u6027\u5e03\u5c40\u8def\u5f84\u7f29\u77ed28%\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u7ea625%\uff1b\u591a\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u8d2a\u5a6a\u5206\u914d\u5668\u6bd4\u5308\u7259\u5229\u5206\u914d\u964d\u4f4e33-37%\u6279\u91cf\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "\u91cd\u65b0\u8bbe\u8ba1\u7530\u95f4\u51e0\u4f55\u5f62\u72b6\u80fd\u66f4\u597d\u5730\u9002\u5e94\u81ea\u4e3b\u519c\u4e1a\u9700\u6c42\uff0c\u87ba\u65cb\u5e03\u5c40\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.25097", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25097", "abs": "https://arxiv.org/abs/2509.25097", "authors": ["Jes\u00fas Roche", "Eduardo Sebasti\u00e1n", "Eduardo Montijano"], "title": "Curriculum Imitation Learning of Distributed Multi-Robot Policies", "comment": "Accepted and presented at the Eight Iberian Robotics Conference, 2025", "summary": "Learning control policies for multi-robot systems (MRS) remains a major\nchallenge due to long-term coordination and the difficulty of obtaining\nrealistic training data. In this work, we address both limitations within an\nimitation learning framework. First, we shift the typical role of Curriculum\nLearning in MRS, from scalability with the number of robots, to focus on\nimproving long-term coordination. We propose a curriculum strategy that\ngradually increases the length of expert trajectories during training,\nstabilizing learning and enhancing the accuracy of long-term behaviors. Second,\nwe introduce a method to approximate the egocentric perception of each robot\nusing only third-person global state demonstrations. Our approach transforms\nidealized trajectories into locally available observations by filtering\nneighbors, converting reference frames, and simulating onboard sensor\nvariability. Both contributions are integrated into a physics-informed\ntechnique to produce scalable, distributed policies from observations. We\nconduct experiments across two tasks with varying team sizes and noise levels.\nResults show that our curriculum improves long-term accuracy, while our\nperceptual estimation method yields policies that are robust to realistic\nuncertainty. Together, these strategies enable the learning of robust,\ndistributed controllers from global demonstrations, even in the absence of\nexpert actions or onboard measurements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u7b2c\u4e09\u4eba\u79f0\u5168\u5c40\u6f14\u793a\u4e2d\u5b66\u4e60\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5206\u5e03\u5f0f\u63a7\u5236\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u6539\u8fdb\u957f\u671f\u534f\u8c03\uff0c\u5e76\u901a\u8fc7\u611f\u77e5\u4f30\u8ba1\u65b9\u6cd5\u5904\u7406\u5c40\u90e8\u89c2\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u957f\u671f\u534f\u8c03\u7684\u6311\u6218\u4ee5\u53ca\u83b7\u53d6\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7684\u56f0\u96be\uff0c\u65e8\u5728\u4ece\u5168\u5c40\u6f14\u793a\u4e2d\u5b66\u4e60\u5206\u5e03\u5f0f\u63a7\u5236\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u9010\u6e10\u589e\u52a0\u4e13\u5bb6\u8f68\u8ff9\u957f\u5ea6\u4ee5\u7a33\u5b9a\u8bad\u7ec3\uff0c\u63d0\u51fa\u611f\u77e5\u4f30\u8ba1\u65b9\u6cd5\u5c06\u5168\u5c40\u72b6\u6001\u8f6c\u6362\u4e3a\u5c40\u90e8\u89c2\u6d4b\uff0c\u5305\u62ec\u90bb\u5c45\u8fc7\u6ee4\u3001\u53c2\u8003\u7cfb\u8f6c\u6362\u548c\u4f20\u611f\u5668\u566a\u58f0\u6a21\u62df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8bfe\u7a0b\u5b66\u4e60\u63d0\u9ad8\u4e86\u957f\u671f\u884c\u4e3a\u51c6\u786e\u6027\uff0c\u611f\u77e5\u4f30\u8ba1\u65b9\u6cd5\u4f7f\u7b56\u7565\u5bf9\u73b0\u5b9e\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u7a33\u5065\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5168\u5c40\u6f14\u793a\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u7b56\u7565\uff0c\u65e0\u9700\u4e13\u5bb6\u52a8\u4f5c\u6216\u673a\u8f7d\u6d4b\u91cf\uff0c\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2509.25124", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25124", "abs": "https://arxiv.org/abs/2509.25124", "authors": ["David Smith Sundarsingh", "Yifei Li", "Tianji Tang", "George J. Pappas", "Nikolay Atanasov", "Yiannis Kantaros"], "title": "Safe Planning in Unknown Environments using Conformalized Semantic Maps", "comment": "8 pages, 5 figures, 2 algorithms, 1 table", "summary": "This paper addresses semantic planning problems in unknown environments under\nperceptual uncertainty. The environment contains multiple unknown semantically\nlabeled regions or objects, and the robot must reach desired locations while\nmaintaining class-dependent distances from them. We aim to compute robot paths\nthat complete such semantic reach-avoid tasks with user-defined probability\ndespite uncertain perception. Existing planning algorithms either ignore\nperceptual uncertainty - thus lacking correctness guarantees - or assume known\nsensor models and noise characteristics. In contrast, we present the first\nplanner for semantic reach-avoid tasks that achieves user-specified mission\ncompletion rates without requiring any knowledge of sensor models or noise.\nThis is enabled by quantifying uncertainty in semantic maps - constructed\non-the-fly from perceptual measurements - using conformal prediction in a\nmodel- and distribution-free manner. We validate our approach and the\ntheoretical mission completion rates through extensive experiments, showing\nthat it consistently outperforms baselines in mission success rates.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u65e0\u9700\u4f20\u611f\u5668\u6a21\u578b\u77e5\u8bc6\u7684\u8bed\u4e49\u89c4\u5212\u7b97\u6cd5\uff0c\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b8c\u6210\u8bed\u4e49\u5230\u8fbe-\u907f\u969c\u4efb\u52a1\uff0c\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u91cf\u5316\u8bed\u4e49\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\uff0c\u786e\u4fdd\u7528\u6237\u6307\u5b9a\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u89e3\u51b3\u672a\u77e5\u73af\u5883\u4e0b\u8bed\u4e49\u89c4\u5212\u4e2d\u7684\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7f3a\u4e4f\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u8981\u4e48\u9700\u8981\u5df2\u77e5\u4f20\u611f\u5668\u6a21\u578b\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u4f20\u611f\u5668\u6a21\u578b\u5f80\u5f80\u672a\u77e5\u3002", "method": "\u4f7f\u7528\u4fdd\u5f62\u9884\u6d4b\u5728\u6a21\u578b\u65e0\u5173\u548c\u5206\u5e03\u65e0\u5173\u7684\u65b9\u5f0f\u4e0b\u91cf\u5316\u8bed\u4e49\u5730\u56fe\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u7ebf\u6784\u5efa\u8bed\u4e49\u5730\u56fe\uff0c\u786e\u4fdd\u4efb\u52a1\u5b8c\u6210\u6982\u7387\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u8981\u6c42\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u7406\u8bba\u4efb\u52a1\u5b8c\u6210\u7387\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c4\u5212\u5668\u662f\u9996\u4e2a\u80fd\u591f\u5728\u65e0\u9700\u4f20\u611f\u5668\u6a21\u578b\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u8bed\u4e49\u5230\u8fbe-\u907f\u969c\u4efb\u52a1\u63d0\u4f9b\u7528\u6237\u6307\u5b9a\u4efb\u52a1\u5b8c\u6210\u7387\u4fdd\u8bc1\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8bed\u4e49\u89c4\u5212\u95ee\u9898\u3002"}}
