<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 43]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [DIPP: Discriminative Impact Point Predictor for Catching Diverse In-Flight Objects](https://arxiv.org/abs/2509.15254)
*Ngoc Huy Nguyen,Kazuki Shibata,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本文提出了一种用于四足机器人空中抓取物体的方法，通过构建包含8000条轨迹的真实数据集，并开发了判别性撞击点预测器（DIPP），解决了非稳态空气动力学下物体轨迹预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决空中物体抓取中的两个关键挑战：缺乏捕捉非稳态空气动力学下多样化物体的公共数据集，以及在轨迹相似时难以进行早期撞击点预测。

Method: 构建了包含20种物体8000条轨迹的真实数据集，提出了DIPP方法，包括判别性特征嵌入（DFE）模块和撞击点预测器（IPP）模块。IPP有两种变体：基于神经加速度估计器（NAE）和基于直接点估计器（DPE）。

Result: 实验表明，所构建的数据集比现有数据集更多样化和复杂，方法在15个已知物体和5个未知物体上都优于基线方法。早期预测能力的提升提高了模拟中的抓取成功率，并通过真实实验验证了有效性。

Conclusion: 该方法通过构建高质量数据集和创新的预测架构，成功解决了复杂空气动力学下的空中物体抓取问题，为机器人动态交互任务提供了有效解决方案。

Abstract: In this study, we address the problem of in-flight object catching using a
quadruped robot with a basket. Our objective is to accurately predict the
impact point, defined as the object's landing position. This task poses two key
challenges: the absence of public datasets capturing diverse objects under
unsteady aerodynamics, which are essential for training reliable predictors;
and the difficulty of accurate early-stage impact point prediction when
trajectories appear similar across objects. To overcome these issues, we
construct a real-world dataset of 8,000 trajectories from 20 objects, providing
a foundation for advancing in-flight object catching under complex
aerodynamics. We then propose the Discriminative Impact Point Predictor (DIPP),
consisting of two modules: (i) a Discriminative Feature Embedding (DFE) that
separates trajectories by dynamics to enable early-stage discrimination and
generalization, and (ii) an Impact Point Predictor (IPP) that estimates the
impact point from these features. Two IPP variants are implemented: an Neural
Acceleration Estimator (NAE)-based method that predicts trajectories and
derives the impact point, and a Direct Point Estimator (DPE)-based method that
directly outputs it. Experimental results show that our dataset is more diverse
and complex than existing dataset, and that our method outperforms baselines on
both 15 seen and 5 unseen objects. Furthermore, we show that improved
early-stage prediction enhances catching success in simulation and demonstrate
the effectiveness of our approach through real-world experiments. The
demonstration is available at
https://sites.google.com/view/robot-catching-2025.

</details>


### [2] [GiAnt: A Bio-Inspired Hexapod for Adaptive Terrain Navigation and Object Detection](https://arxiv.org/abs/2509.15264)
*Aasfee Mosharraf Bhuiyan,Md Luban Mehda,Md. Thawhid Hasan Puspo,Jubayer Amin Pritom*

Main category: cs.RO

TL;DR: GiAnt是一款受蚂蚁启发的经济型六足机器人，具有轻量化3D打印结构、单自由度腿部设计，能够在复杂地形上灵活移动，并配备机器学习功能进行物体识别


<details>
  <summary>Details</summary>
Motivation: 设计灵感来源于蚂蚁在各种地形上的自然适应能力，旨在开发具有地形灵活性和高效能源利用的户外应用机器人

Method: 采用3D打印和激光切割制造轻量化结构（1.75kg），使用连杆曲柄机构实现单自由度腿部设计，基于Arduino构建控制系统，通过步态分析实现有效控制

Result: 机器人能够轻松跨越8cm高度障碍，在草地、岩石和陡坡等复杂地形上表现出色，并能识别81种不同物体

Conclusion: GiAnt代表了在可访问性六足机器人研究方面的重要进展，为研究、探索和勘测应用提供了独特的适应性和控制简单性优势

Abstract: This paper presents the design, development and testing of GiAnt, an
affordable hexapod which is inspired by the efficient motions of ants. The
decision to model GiAnt after ants rather than other insects is rooted in ants'
natural adaptability to a variety of terrains. This bio-inspired approach gives
it a significant advantage in outdoor applications, offering terrain
flexibility along with efficient energy use. It features a lightweight
3D-printed and laser cut structure weighing 1.75 kg with dimensions of 310 mm x
200 mm x 120 mm. Its legs have been designed with a simple Single Degree of
Freedom (DOF) using a link and crank mechanism. It is great for conquering
challenging terrains such as grass, rocks, and steep surfaces. Unlike
traditional robots using four wheels for motion, its legged design gives
superior adaptability to uneven and rough surfaces. GiAnt's control system is
built on Arduino, allowing manual operation. An effective way of controlling
the legs of GiAnt was achieved by gait analysis. It can move up to 8 cm of
height easily with its advanced leg positioning system. Furthermore, equipped
with machine learning and image processing technology, it can identify 81
different objects in a live monitoring system. It represents a significant step
towards creating accessible hexapod robots for research, exploration, and
surveying, offering unique advantages in adaptability and control simplicity.

</details>


### [3] [Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI](https://arxiv.org/abs/2509.15273)
*Fei Ni,Min Zhang,Pengyi Li,Yifu Yuan,Lingfeng Zhang,Yuecheng Liu,Peilong Han,Longxin Kou,Shaojin Ma,Jinbin Qiao,David Gamaliel Arcos Bravo,Yuening Wang,Xiao Hu,Zhanguang Zhang,Xianze Yao,Yutong Li,Zhao Zhang,Ying Wen,Ying-Cong Chen,Xiaodan Liang,Liang Lin,Bin He,Haitham Bou-Ammar,He Wang,Huazhe Xu,Jiankang Deng,Shan Luo,Shuqiang Jiang,Wei Pan,Yang Gao,Stefanos Zafeiriou,Jan Peters,Yuzheng Zhuang,Yingxue Zhang,Yan Zheng,Hongyao Tang,Jianye Hao*

Main category: cs.RO

TL;DR: Embodied Arena是一个全面的、统一的、不断发展的Embodied AI评估平台，旨在解决Embodied AI发展的三大挑战：缺乏系统能力理解、缺乏统一评估系统、缺乏可扩展的数据获取方法。


<details>
  <summary>Details</summary>
Motivation: Embodied AI发展显著落后于大型基础模型，主要面临三个关键挑战：(1)缺乏对Embodied AI核心能力的系统理解；(2)缺乏统一标准化的评估系统；(3)缺乏自动化和可扩展的体现数据获取方法。

Method: 建立系统化的体现能力分类法（3个层次、7个核心能力、25个细粒度维度）；构建标准化评估系统，支持22个不同基准的灵活集成；开发基于LLM的自动生成管道，确保可扩展的体现评估数据。

Result: 发布了三个实时排行榜（体现问答、导航、任务规划），提供双视角评估（基准视角和能力视角）；从评估结果中总结了九个重要发现。

Conclusion: Embodied Arena通过建立清晰的评估框架和研究脉络，帮助识别关键研究问题，推动Embodied AI领域的进步。

Abstract: Embodied AI development significantly lags behind large foundation models due
to three critical challenges: (1) lack of systematic understanding of core
capabilities needed for Embodied AI, making research lack clear objectives; (2)
absence of unified and standardized evaluation systems, rendering
cross-benchmark evaluation infeasible; and (3) underdeveloped automated and
scalable acquisition methods for embodied data, creating critical bottlenecks
for model scaling. To address these obstacles, we present Embodied Arena, a
comprehensive, unified, and evolving evaluation platform for Embodied AI. Our
platform establishes a systematic embodied capability taxonomy spanning three
levels (perception, reasoning, task execution), seven core capabilities, and 25
fine-grained dimensions, enabling unified evaluation with systematic research
objectives. We introduce a standardized evaluation system built upon unified
infrastructure supporting flexible integration of 22 diverse benchmarks across
three domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced
models from 20+ worldwide institutes. Additionally, we develop a novel
LLM-driven automated generation pipeline ensuring scalable embodied evaluation
data with continuous evolution for diversity and comprehensiveness. Embodied
Arena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task
Planning) with dual perspectives (benchmark view and capability view),
providing comprehensive overviews of advanced model capabilities. Especially,
we present nine findings summarized from the evaluation results on the
leaderboards of Embodied Arena. This helps to establish clear research veins
and pinpoint critical research problems, thereby driving forward progress in
the field of Embodied AI.

</details>


### [4] [Measurement and Potential Field-Based Patient Modeling for Model-Mediated Tele-ultrasound](https://arxiv.org/abs/2509.15325)
*Ryan S. Yeung,David G. Black,Septimiu E. Salcudean*

Main category: cs.RO

TL;DR: 提出了一种改进的模型介导远程超声方法，通过整合测量的力数据来更新内部势场模型，提高力反馈的准确性。


<details>
  <summary>Details</summary>
Motivation: 远程超声需要准确的力反馈来优化探头接触力，但通信延迟使得直接力反馈不可行。现有方法使用点云和势场模型估计力，但精度有待提高。

Method: 首先生成患者表面点云模型，转换为包含势场值的体素化体积。通过结合拉普拉斯算子和测量力的凸二次规划求解势场，基于体素与探头模型的交叠渲染力/扭矩。

Result: 在3名志愿者上的测试显示，加入测量力后相比仅使用拉普拉斯方程，力大小误差平均减少7.23N，力向量角度误差平均减少9.37°。

Conclusion: 该方法通过整合实际测量力数据显著提高了模型介导远程超声的力反馈透明度，为远程医疗提供了更准确的触觉反馈。

Abstract: Teleoperated ultrasound can improve diagnostic medical imaging access for
remote communities. Having accurate force feedback is important for enabling
sonographers to apply the appropriate probe contact force to optimize
ultrasound image quality. However, large time delays in communication make
direct force feedback impractical. Prior work investigated using point
cloud-based model-mediated teleoperation and internal potential field models to
estimate contact forces and torques. We expand on this by introducing a method
to update the internal potential field model of the patient with measured
positions and forces for more transparent model-mediated tele-ultrasound. We
first generate a point cloud model of the patient's surface and transmit this
to the sonographer in a compact data structure. This is converted to a static
voxelized volume where each voxel contains a potential field value. These
values determine the forces and torques, which are rendered based on overlap
between the voxelized volume and a point shell model of the ultrasound
transducer. We solve for the potential field using a convex quadratic that
combines the spatial Laplace operator with measured forces. This was evaluated
on volunteer patients ($n=3$) by computing the accuracy of rendered forces.
Results showed the addition of measured forces to the model reduced the force
magnitude error by an average of 7.23 N and force vector angle error by an
average of 9.37$^{\circ}$ compared to using only Laplace's equation.

</details>


### [5] [Trust-Aware Embodied Bayesian Persuasion for Mixed-Autonomy](https://arxiv.org/abs/2509.15404)
*Shaoting Peng,Katherine Driggs-Campbell,Roy Dong*

Main category: cs.RO

TL;DR: 本文提出了信任感知的具身贝叶斯说服框架（TA-EBP），用于解决自动驾驶车辆与人类驾驶车辆交互中的信任和安全问题，通过透明沟通而非传统博弈论方法实现更安全的交通交互。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论模型在自动驾驶车辆与人类驾驶车辆交互中存在长期影响力衰减和被感知为操纵性的问题，这会削弱人类信任并导致更危险的人类驾驶行为。

Method: 提出TA-EBP框架：1）应用贝叶斯说服建模交通路口通信；2）引入信任参数并推导最小信任水平定理；3）将抽象信号映射到连续物理动作空间，推导最优信号幅度定理（表现为AV的前进推动）。

Result: 在混合自主交通模拟中验证，TA-EBP成功说服人类驾驶车辆更谨慎驾驶，相比忽略信任或缺乏沟通的基线方法，消除了碰撞并改善了交通流。

Conclusion: 该工作为人机交互中的影响力提供了透明且非战略性的框架，同时提升了安全性和效率。

Abstract: Safe and efficient interaction between autonomous vehicles (AVs) and
human-driven vehicles (HVs) is a critical challenge for future transportation
systems. While game-theoretic models capture how AVs influence HVs, they often
suffer from a long-term decay of influence and can be perceived as
manipulative, eroding the human's trust. This can paradoxically lead to riskier
human driving behavior over repeated interactions. In this paper, we address
this challenge by proposing the Trust-Aware Embodied Bayesian Persuasion
(TA-EBP) framework. Our work makes three key contributions: First, we apply
Bayesian persuasion to model communication at traffic intersections, offering a
transparent alternative to traditional game-theoretic models. Second, we
introduce a trust parameter to the persuasion framework, deriving a theorem for
the minimum trust level required for influence. Finally, we ground the abstract
signals of Bayesian persuasion theory into a continuous, physically meaningful
action space, deriving a second theorem for the optimal signal magnitude,
realized as an AV's forward nudge. Additionally, we validate our framework in a
mixed-autonomy traffic simulation, demonstrating that TA-EBP successfully
persuades HVs to drive more cautiously, eliminating collisions and improving
traffic flow compared to baselines that either ignore trust or lack
communication. Our work provides a transparent and non-strategic framework for
influence in human-robot interaction, enhancing both safety and efficiency.

</details>


### [6] [Sym2Real: Symbolic Dynamics with Residual Learning for Data-Efficient Adaptive Control](https://arxiv.org/abs/2509.15412)
*Easop Lee,Samuel A. Moore,Boyuan Chen*

Main category: cs.RO

TL;DR: Sym2Real是一个数据驱动框架，通过符号回归和残差学习实现高效的低层自适应控制器训练，仅需约10条轨迹即可在真实世界中实现四旋翼和赛车的鲁棒控制。


<details>
  <summary>Details</summary>
Motivation: 解决符号回归在真实机器人应用中面临的关键挑战，包括噪声敏感性和模型退化导致的不安全控制问题。关键观察是系统的基础物理特性在不同条件下通常是共享的。

Method: 策略性地结合低保真仿真数据和有针对性的真实世界残差学习，通过符号回归方法实现数据高效的自适应控制。

Result: 在四旋翼和赛车平台上验证，在六个分布外仿真场景中实现一致的数据高效适应，并在五个真实世界条件下成功实现仿真到真实的迁移。

Conclusion: Sym2Real框架为真实世界机器人控制提供了一种原则性的数据高效训练方法，无需专家知识或仿真调优即可实现鲁棒控制。

Abstract: We present Sym2Real, a fully data-driven framework that provides a principled
way to train low-level adaptive controllers in a highly data-efficient manner.
Using only about 10 trajectories, we achieve robust control of both a quadrotor
and a racecar in the real world, without expert knowledge or simulation tuning.
Our approach achieves this data efficiency by bringing symbolic regression to
real-world robotics while addressing key challenges that prevent its direct
application, including noise sensitivity and model degradation that lead to
unsafe control. Our key observation is that the underlying physics is often
shared for a system regardless of internal or external changes. Hence, we
strategically combine low-fidelity simulation data with targeted real-world
residual learning. Through experimental validation on quadrotor and racecar
platforms, we demonstrate consistent data-efficient adaptation across six
out-of-distribution sim2sim scenarios and successful sim2real transfer across
five real-world conditions. More information and videos can be found at at
http://generalroboticslab.com/Sym2Real

</details>


### [7] [Online Slip Detection and Friction Coefficient Estimation for Autonomous Racing](https://arxiv.org/abs/2509.15423)
*Christopher Oeltjen,Carson Sobolewski,Saleh Faghfoorian,Lorant Domokos,Giancarlo Vidal,Ivan Ruchkin*

Main category: cs.RO

TL;DR: 提出了一种轻量级的在线滑移检测和轮胎-路面摩擦系数估计方法，仅使用IMU、LiDAR测量和控制动作，无需复杂的车辆模型或训练数据


<details>
  <summary>Details</summary>
Motivation: 轮胎-路面摩擦系数的准确估计对车辆安全至关重要，特别是在自动驾驶赛车中，但现有方法依赖不确定参数的模型或需要大量训练数据

Method: 通过比较指令运动和实测运动实时检测滑移事件，在无滑移条件下直接从观测加速度估计摩擦系数，无需特殊动力学模型或参数识别

Result: 在1:10比例自动驾驶赛车上进行不同摩擦水平实验，结果显示该方法能实现准确一致的滑移检测和摩擦系数估计，结果与地面真实测量高度匹配

Conclusion: 该方法简单、可部署且计算效率高，在自动驾驶实时滑移监测和摩擦系数估计方面具有潜力

Abstract: Accurate knowledge of the tire-road friction coefficient (TRFC) is essential
for vehicle safety, stability, and performance, especially in autonomous
racing, where vehicles often operate at the friction limit. However, TRFC
cannot be directly measured with standard sensors, and existing estimation
methods either depend on vehicle or tire models with uncertain parameters or
require large training datasets. In this paper, we present a lightweight
approach for online slip detection and TRFC estimation. Our approach relies
solely on IMU and LiDAR measurements and the control actions, without special
dynamical or tire models, parameter identification, or training data. Slip
events are detected in real time by comparing commanded and measured motions,
and the TRFC is then estimated directly from observed accelerations under
no-slip conditions. Experiments with a 1:10-scale autonomous racing car across
different friction levels demonstrate that the proposed approach achieves
accurate and consistent slip detections and friction coefficients, with results
closely matching ground-truth measurements. These findings highlight the
potential of our simple, deployable, and computationally efficient approach for
real-time slip monitoring and friction coefficient estimation in autonomous
driving.

</details>


### [8] [Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning](https://arxiv.org/abs/2509.15443)
*Xingyu Chen,Hanyu Wu,Sikai Wu,Mingliang Zhou,Diyun Xiang,Haodong Zhang*

Main category: cs.RO

TL;DR: 提出了一种名为IKMR的高效可扩展运动重定向框架，能够将大规模人体运动直接转换为机器人可执行的运动轨迹，同时考虑运动学和动力学约束。


<details>
  <summary>Details</summary>
Motivation: 当前的运动重定向方法主要基于逐帧处理，缺乏可扩展性，无法高效处理大规模人体运动数据。

Method: IKMR框架包含运动学预训练和动力学优化两部分：运动学部分通过双编码器-解码器架构学习运动域映射；动力学部分结合模仿学习来优化物理可行性。

Result: 在仿真和真实全尺寸人形机器人上的实验验证了该框架的有效性，能够实现实时的大规模物理可行运动重定向。

Conclusion: IKMR框架为从人体运动到人形机器人的高效运动重定向提供了一种可行的解决方案，可直接用于训练全身控制器。

Abstract: Human-to-humanoid imitation learning aims to learn a humanoid whole-body
controller from human motion. Motion retargeting is a crucial step in enabling
robots to acquire reference trajectories when exploring locomotion skills.
However, current methods focus on motion retargeting frame by frame, which
lacks scalability. Could we directly convert large-scale human motion into
robot-executable motion through a more efficient approach? To address this
issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel
efficient and scalable retargeting framework that considers both kinematics and
dynamics. In kinematics, IKMR pretrains motion topology feature representation
and a dual encoder-decoder architecture to learn a motion domain mapping. In
dynamics, IKMR integrates imitation learning with the motion retargeting
network to refine motion into physically feasible trajectories. After
fine-tuning using the tracking results, IKMR can achieve large-scale physically
feasible motion retargeting in real time, and a whole-body controller could be
directly trained and deployed for tracking its retargeted trajectories. We
conduct our experiments both in the simulator and the real robot on a full-size
humanoid robot. Extensive experiments and evaluation results verify the
effectiveness of our proposed framework.

</details>


### [9] [Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems](https://arxiv.org/abs/2509.15491)
*Reza Pirayeshshirazinezhad,Nima Fathi*

Main category: cs.RO

TL;DR: 提出了一种可解释AI增强的多智能体机器人监督控制框架，结合时序自动机监督器、鲁棒连续控制和可解释预测器，在航天器编队飞行和自主水下航行器两个领域验证了方法的有效性和可移植性。


<details>
  <summary>Details</summary>
Motivation: 为安全关键、资源受限的多智能体机器人系统开发一个既安全可靠又具有可解释性的控制框架，解决传统控制方法在复杂环境下性能不足和缺乏透明度的问题。

Method: 采用时序自动机监督器进行安全模式切换，结合李雅普诺夫控制器（大角度机动）和滑模控制器（精确控制和扰动抑制），通过蒙特卡洛优化生成训练数据，实现实时透明权衡。

Result: 在航天器任务中，SMC控制器相比PD基准实现了亚毫米级对齐，跟踪误差降低21.7%，能耗降低81.4%；在AUV测试中，SMC在随机水流下保持有界稳态误差。

Conclusion: 该方法具有高度的可移植性和可解释性，适用于具有不确定6-DOF刚体动力学、相对运动和严格跟踪需求的一般机器人系统，为安全关键应用提供了有效解决方案。

Abstract: We present an explainable AI-enhanced supervisory control framework for
multi-agent robotics that combines (i) a timed-automata supervisor for safe,
auditable mode switching, (ii) robust continuous control (Lyapunov-based
controller for large-angle maneuver; sliding-mode controller (SMC) with
boundary layers for precision and disturbance rejection), and (iii) an
explainable predictor that maps mission context to gains and expected
performance (energy, error). Monte Carlo-driven optimization provides the
training data, enabling transparent real-time trade-offs.
  We validated the approach in two contrasting domains, spacecraft formation
flying and autonomous underwater vehicles (AUVs). Despite different
environments (gravity/actuator bias vs. hydrodynamic drag/currents), both share
uncertain six degrees of freedom (6-DOF) rigid-body dynamics, relative motion,
and tight tracking needs, making them representative of general robotic
systems. In the space mission, the supervisory logic selects parameters that
meet mission criteria. In AUV leader-follower tests, the same SMC structure
maintains a fixed offset under stochastic currents with bounded steady error.
In spacecraft validation, the SMC controller achieved submillimeter alignment
with 21.7% lower tracking error and 81.4% lower energy consumption compared to
Proportional-Derivative PD controller baselines. At the same time, in AUV
tests, SMC maintained bounded errors under stochastic currents. These results
highlight both the portability and the interpretability of the approach for
safety-critical, resource-constrained multi-agent robotics.

</details>


### [10] [STARC: See-Through-Wall Augmented Reality Framework for Human-Robot Collaboration in Emergency Response](https://arxiv.org/abs/2509.15507)
*Shenghai Yuan,Weixiang Guo,Tianxin Hu,Yu Yang,Jinyu Chen,Rui Qian,Zhongyuan Liu,Lihua Xie*

Main category: cs.RO

TL;DR: STARC是一个用于紧急救援任务的AR框架，通过融合移动机器人和救援人员的LiDAR传感，实现遮挡环境下的人员和危险物实时可视化。


<details>
  <summary>Details</summary>
Motivation: 在紧急救援任务中，救援人员需要在遮挡严重的室内环境中导航，视线受阻会隐藏生命危险和需要救援的受害者。

Method: 使用地面机器人进行大范围探索和3D人体检测，同时通过相对姿态估计将救援人员头盔或手持LiDAR注册到机器人的全局地图中，实现跨LiDAR对齐和低延迟AR投影。

Result: 在仿真、实验室设置和战术现场试验中验证了稳健的姿态对齐、可靠的检测和稳定的叠加效果。

Conclusion: STARC系统通过实时可视化隐藏的乘员和危险物，增强了态势感知并降低了操作风险，在消防、救灾等安全关键操作中具有应用潜力。

Abstract: In emergency response missions, first responders must navigate cluttered
indoor environments where occlusions block direct line-of-sight, concealing
both life-threatening hazards and victims in need of rescue. We present STARC,
a see-through AR framework for human-robot collaboration that fuses
mobile-robot mapping with responder-mounted LiDAR sensing. A ground robot
running LiDAR-inertial odometry performs large-area exploration and 3D human
detection, while helmet- or handheld-mounted LiDAR on the responder is
registered to the robot's global map via relative pose estimation. This
cross-LiDAR alignment enables consistent first-person projection of detected
humans and their point clouds - rendered in AR with low latency - into the
responder's view. By providing real-time visualization of hidden occupants and
hazards, STARC enhances situational awareness and reduces operator risk.
Experiments in simulation, lab setups, and tactical field trials confirm robust
pose alignment, reliable detections, and stable overlays, underscoring the
potential of our system for fire-fighting, disaster relief, and other
safety-critical operations. Code and design will be open-sourced upon
acceptance.

</details>


### [11] [Distribution Estimation for Global Data Association via Approximate Bayesian Inference](https://arxiv.org/abs/2509.15565)
*Yixuan Jia,Mason B. Peterson,Qingyuan Li,Yulun Tian,Jonathan P. How*

Main category: cs.RO

TL;DR: 提出了一种基于近似贝叶斯推理的数据关联框架，能够捕获数据关联问题的多模态解分布，避免在模糊场景下过早确定单一解。


<details>
  <summary>Details</summary>
Motivation: 现有数据关联方法通常依赖最大似然估计或最大一致性来产生单一关联集，但在模糊场景下，全局数据关联问题的解分布通常是高度多模态的，单解方法容易失败。

Method: 使用粒子表示假设解，根据确定性或随机更新规则演化以覆盖底层解分布的模态，能够结合数据关联公式的优化约束并直接受益于GPU并行优化。

Result: 在高度模糊数据的模拟和真实世界实验中，该方法在配准点云或对象地图时能够正确估计变换分布。

Conclusion: 该方法通过捕获多模态解分布有效解决了模糊场景下的数据关联问题，优于传统的单解方法。

Abstract: Global data association is an essential prerequisite for robot operation in
environments seen at different times or by different robots. Repetitive or
symmetric data creates significant challenges for existing methods, which
typically rely on maximum likelihood estimation or maximum consensus to produce
a single set of associations. However, in ambiguous scenarios, the distribution
of solutions to global data association problems is often highly multimodal,
and such single-solution approaches frequently fail. In this work, we introduce
a data association framework that leverages approximate Bayesian inference to
capture multiple solution modes to the data association problem, thereby
avoiding premature commitment to a single solution under ambiguity. Our
approach represents hypothetical solutions as particles that evolve according
to a deterministic or randomized update rule to cover the modes of the
underlying solution distribution. Furthermore, we show that our method can
incorporate optimization constraints imposed by the data association
formulation and directly benefit from GPU-parallelized optimization. Extensive
simulated and real-world experiments with highly ambiguous data show that our
method correctly estimates the distribution over transformations when
registering point clouds or object maps.

</details>


### [12] [Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework with Residual-enhanced DRL for Visually Impaired Scenarios](https://arxiv.org/abs/2509.15582)
*Yuting Zeng,Zhiwen Zheng,You Zhou,JiaLing Xiao,Yongbin Yu,Manping Fan,Bo Gong,Liyong Ren*

Main category: cs.RO

TL;DR: 提出了一种动量约束混合启发式轨迹优化框架（MHHTOF），用于视觉障碍场景下的辅助导航，结合轨迹采样生成、优化和残差增强深度强化学习（DRL）。


<details>
  <summary>Details</summary>
Motivation: 针对视觉障碍辅助导航场景，需要开发能够确保平滑性、可行性和安全性的轨迹优化方法，以增强复杂辅助规划任务中的鲁棒性、安全性和实时可行性。

Method: 采用两阶段方法：第一阶段在Frenet坐标系中使用三阶插值和五阶多项式生成启发式轨迹采样簇（HTSC），并应用动量约束轨迹优化（MTO）；第二阶段在笛卡尔坐标系中使用基于LSTM的残差增强actor-critic网络自适应优化轨迹选择，并通过双阶段成本建模机制（DCMM）实现语义优先级对齐。

Result: 提出的LSTM-ResB-PPO方法相比PPO基线收敛速度显著加快，训练迭代次数减少约一半，同时提高了奖励结果和训练稳定性。相比基线方法，平均成本和成本方差分别降低30.3%和53.3%，自我和障碍风险降低超过77%。

Conclusion: 该框架在复杂辅助规划任务中有效增强了鲁棒性、安全性和实时可行性，验证了其在视觉障碍辅助导航场景中的有效性。

Abstract: This paper proposes a momentum-constrained hybrid heuristic trajectory
optimization framework (MHHTOF) tailored for assistive navigation in visually
impaired scenarios, integrating trajectory sampling generation, optimization
and evaluation with residual-enhanced deep reinforcement learning (DRL). In the
first stage, heuristic trajectory sampling cluster (HTSC) is generated in the
Frenet coordinate system using third-order interpolation with fifth-order
polynomials and momentum-constrained trajectory optimization (MTO) constraints
to ensure smoothness and feasibility. After first stage cost evaluation, the
second stage leverages a residual-enhanced actor-critic network with LSTM-based
temporal feature modeling to adaptively refine trajectory selection in the
Cartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with
weight transfer aligns semantic priorities across stages, supporting
human-centered optimization. Experimental results demonstrate that the proposed
LSTM-ResB-PPO achieves significantly faster convergence, attaining stable
policy performance in approximately half the training iterations required by
the PPO baseline, while simultaneously enhancing both reward outcomes and
training stability. Compared to baseline method, the selected model reduces
average cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle
risks by over 77%. These findings validate the framework's effectiveness in
enhancing robustness, safety, and real-time feasibility in complex assistive
planning tasks.

</details>


### [13] [Bench-RNR: Dataset for Benchmarking Repetitive and Non-repetitive Scanning LiDAR for Infrastructure-based Vehicle Localization](https://arxiv.org/abs/2509.15583)
*Runxin Zhao,Chunxiang Wang,Hanyang Zhuang,Ming Yang*

Main category: cs.RO

TL;DR: 本文提出了一个基于路边LiDAR的车辆定位数据集，包含重复扫描和非重复扫描两种LiDAR模式的数据，用于比较不同扫描模式在车辆定位中的性能。


<details>
  <summary>Details</summary>
Motivation: 目前大多数研究依赖重复扫描LiDAR，但非重复扫描LiDAR具有消除盲区和成本效益更高的优势，然而其在路边感知和定位中的应用仍然有限。

Method: 收集了来自重复和非重复扫描LiDAR的5,445帧点云数据，涵盖8种车辆轨迹序列，建立了基于基础设施的车辆定位基准方法。

Result: 实验建立了基于基础设施的车辆定位基准，并比较了使用非重复和重复扫描LiDAR的方法性能。

Conclusion: 这项工作为选择最适合基础设施车辆定位的LiDAR扫描模式提供了有价值的见解，数据集对科学社区具有重要意义。

Abstract: Vehicle localization using roadside LiDARs can provide centimeter-level
accuracy for cloud-controlled vehicles while simultaneously serving multiple
vehicles, enhanc-ing safety and efficiency. While most existing studies rely on
repetitive scanning LiDARs, non-repetitive scanning LiDAR offers advantages
such as eliminating blind zones and being more cost-effective. However, its
application in roadside perception and localization remains limited. To address
this, we present a dataset for infrastructure-based vehicle localization, with
data collected from both repetitive and non-repetitive scanning LiDARs, in
order to benchmark the performance of different LiDAR scanning patterns. The
dataset contains 5,445 frames of point clouds across eight vehicle trajectory
sequences, with diverse trajectory types. Our experiments establish base-lines
for infrastructure-based vehicle localization and compare the performance of
these methods using both non-repetitive and repetitive scanning LiDARs. This
work offers valuable insights for selecting the most suitable LiDAR scanning
pattern for infrastruc-ture-based vehicle localization. Our dataset is a
signifi-cant contribution to the scientific community, supporting advancements
in infrastructure-based perception and vehicle localization. The dataset and
source code are publicly available at:
https://github.com/sjtu-cyberc3/BenchRNR.

</details>


### [14] [Distributed Nash Equilibrium Seeking Algorithm in Aggregative Games for Heterogeneous Multi-Robot Systems](https://arxiv.org/abs/2509.15597)
*Yi Dong,Zhongguo Li,Sarvapali D. Ramchurn,Xiaowei Huang*

Main category: cs.RO

TL;DR: 本文开发了一种用于异构多机器人系统的分布式纳什均衡寻求算法，利用分布式优化和输出控制实现纳什均衡


<details>
  <summary>Details</summary>
Motivation: 解决异构多机器人系统中纳什均衡的分布式计算问题，通过邻域机器人间的信息共享实现高效协同

Method: 提出分布式优化算法计算纳什均衡作为每个机器人的定制参考，并设计输出控制律使异构多机器人系统在聚合博弈中跟踪该参考

Result: 算法被证明能够保证收敛并产生高效结果，通过数值仿真和物理机器人实证测试验证了方法的有效性

Conclusion: 所提出的分布式纳什均衡寻求算法在异构多机器人系统中具有实际应用价值，能够实现有效的协同控制

Abstract: This paper develops a distributed Nash Equilibrium seeking algorithm for
heterogeneous multi-robot systems. The algorithm utilises distributed
optimisation and output control to achieve the Nash equilibrium by leveraging
information shared among neighbouring robots. Specifically, we propose a
distributed optimisation algorithm that calculates the Nash equilibrium as a
tailored reference for each robot and designs output control laws for
heterogeneous multi-robot systems to track it in an aggregative game. We prove
that our algorithm is guaranteed to converge and result in efficient outcomes.
The effectiveness of our approach is demonstrated through numerical simulations
and empirical testing with physical robots.

</details>


### [15] [ORB: Operating Room Bot, Automating Operating Room Logistics through Mobile Manipulation](https://arxiv.org/abs/2509.15600)
*Jinkai Qiu,Yungjun Kim,Gaurav Sethia,Tanmay Agarwal,Siddharth Ghodasara,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: 提出ORB机器人框架，通过行为树架构自动化医院手术室物流，实现80%的物资检索成功率和96%的补货成功率


<details>
  <summary>Details</summary>
Motivation: 手术室物品高效配送关乎生死，现有机器人只能处理房间间批量运输，无法满足手术室物品级物流的感知、效率和灭菌要求

Method: 采用分层行为树架构，集成目标识别、场景理解和GPU加速运动规划；使用YOLOv7+SAM2+Grounded DINO的实时识别管道；适配cuRobo并行轨迹优化框架

Result: 在手术室物资检索任务中达到80%成功率，补货操作达到96%成功率

Conclusion: ORB系统被证明是可靠且适应性强的自主手术室物流解决方案

Abstract: Efficiently delivering items to an ongoing surgery in a hospital operating
room can be a matter of life or death. In modern hospital settings, delivery
robots have successfully transported bulk items between rooms and floors.
However, automating item-level operating room logistics presents unique
challenges in perception, efficiency, and maintaining sterility. We propose the
Operating Room Bot (ORB), a robot framework to automate logistics tasks in
hospital operating rooms (OR). ORB leverages a robust, hierarchical behavior
tree (BT) architecture to integrate diverse functionalities of object
recognition, scene interpretation, and GPU-accelerated motion planning. The
contributions of this paper include: (1) a modular software architecture
facilitating robust mobile manipulation through behavior trees; (2) a novel
real-time object recognition pipeline integrating YOLOv7, Segment Anything
Model 2 (SAM2), and Grounded DINO; (3) the adaptation of the cuRobo
parallelized trajectory optimization framework to real-time, collision-free
mobile manipulation; and (4) empirical validation demonstrating an 80% success
rate in OR supply retrieval and a 96% success rate in restocking operations.
These contributions establish ORB as a reliable and adaptable system for
autonomous OR logistics.

</details>


### [16] [PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models](https://arxiv.org/abs/2509.15607)
*Ruiqi Wang,Dezhong Zhao,Ziqin Yuan,Tianyu Shao,Guohua Chen,Dominic Kao,Sungeun Hong,Byung-Cheol Min*

Main category: cs.RO

TL;DR: PRIMT是一个基于偏好的强化学习框架，利用基础模型进行多模态合成反馈和轨迹合成，解决了传统PbRL对大量人工输入的依赖以及查询模糊性和信用分配困难的问题。


<details>
  <summary>Details</summary>
Motivation: 传统偏好强化学习（PbRL）存在两个关键挑战：对大量人工输入的依赖，以及在奖励学习过程中解决查询模糊性和信用分配的固有困难。

Method: PRIMT采用分层神经符号融合策略，整合大型语言模型和视觉语言模型的互补优势来评估机器人行为，提供更可靠和全面的反馈。同时包含前瞻轨迹生成和后见轨迹增强技术，前者通过引导样本预热轨迹缓冲区减少早期查询模糊性，后者通过因果辅助损失实现反事实推理以改进信用分配。

Result: 在2个运动任务和6个操作任务的各种基准测试中，PRIMT表现出优于基于基础模型和脚本基线的性能。

Conclusion: PRIMT通过利用基础模型的多模态能力，有效解决了PbRL中的关键挑战，为机器人复杂行为学习提供了一种更高效的方法。

Abstract: Preference-based reinforcement learning (PbRL) has emerged as a promising
paradigm for teaching robots complex behaviors without reward engineering.
However, its effectiveness is often limited by two critical challenges: the
reliance on extensive human input and the inherent difficulties in resolving
query ambiguity and credit assignment during reward learning. In this paper, we
introduce PRIMT, a PbRL framework designed to overcome these challenges by
leveraging foundation models (FMs) for multimodal synthetic feedback and
trajectory synthesis. Unlike prior approaches that rely on single-modality FM
evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy,
integrating the complementary strengths of large language models and
vision-language models in evaluating robot behaviors for more reliable and
comprehensive feedback. PRIMT also incorporates foresight trajectory
generation, which reduces early-stage query ambiguity by warm-starting the
trajectory buffer with bootstrapped samples, and hindsight trajectory
augmentation, which enables counterfactual reasoning with a causal auxiliary
loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6
manipulation tasks on various benchmarks, demonstrating superior performance
over FM-based and scripted baselines.

</details>


### [17] [Miniature soft robot with magnetically reprogrammable surgical functions](https://arxiv.org/abs/2509.15610)
*Chelsea Shan Xian Ng,Yu Xuan Yeoh,Nicholas Yong Wei Foo,Keerthana Radhakrishnan,Guo Zhan Lum*

Main category: cs.RO

TL;DR: 本文提出了一种毫米级软体机器人，其磁化特性可重编程，具备六自由度运动能力和五种手术功能，解决了现有磁性微型机器人功能受限和操作距离短的问题。


<details>
  <summary>Details</summary>
Motivation: 现有磁性微型机器人要么最多只有两种功能，要么只有有限的五自由度运动能力，且需要在强外部磁场近距离（<4厘米）下操作，限制了其在手术中的实际应用。

Method: 开发了一种毫米级软体机器人，其磁化特性可根据指令重编程，能够在相对均匀且较弱的磁场（最多65 mT和1.5 T/m）下实现六自由度运动。

Result: 该机器人能够执行五种手术功能：药物释放、切割生物组织、抓取、存储样本和远程加热，并能在具有挑战性的非结构化环境中滚动和双锚点爬行。

Conclusion: 这项工作标志着软体致动器发展的一个重要里程碑，有望通过具有前所未有功能的无线微型机器人彻底改变微创治疗。

Abstract: Miniature robots are untethered actuators, which have significant potential
to make existing minimally invasive surgery considerably safer and painless,
and enable unprecedented treatments because they are much smaller and dexterous
than existing surgical robots. Of the miniature robots, the magnetically
actuated ones are the most functional and dexterous. However, existing magnetic
miniature robots are currently impractical for surgery because they are either
restricted to possessing at most two on-board functionalities or having limited
five degrees-of-freedom (DOF) locomotion. Some of these actuators are also only
operational under specialized environments where actuation from strong external
magnets must be at very close proximity (< 4 cm away). Here we present a
millimeter-scale soft robot where its magnetization profile can be reprogrammed
upon command to perform five surgical functionalities: drug-dispensing, cutting
through biological tissues (simulated with gelatin), gripping, storing
(biological) samples and remote heating. By possessing full six-DOF motions,
including the sixth-DOF rotation about its net magnetic moment, our soft robot
can also roll and two-anchor crawl across challenging unstructured
environments, which are impassable by its five-DOF counterparts. Because our
actuating magnetic fields are relatively uniform and weak (at most 65 mT and
1.5 T/m), such fields can theoretically penetrate through biological tissues
harmlessly and allow our soft robot to remain controllable within the depths of
the human body. We envision that this work marks a major milestone for the
advancement of soft actuators, and towards revolutionizing minimally invasive
treatments with untethered miniature robots that have unprecedented
functionalities.

</details>


### [18] [Indoor Positioning Based on Active Radar Sensing and Passive Reflectors: Reflector Placement Optimization](https://arxiv.org/abs/2509.15613)
*Sven Hinderer,Pascal Schlachter,Zhibin Yu,Xiaofeng Wu,Bin Yang*

Main category: cs.RO

TL;DR: 提出一种基于雷达感知和被动反射器的室内定位系统，通过多目标粒子群优化算法优化反射器布局


<details>
  <summary>Details</summary>
Motivation: 为自主移动机器人开发低成本高精度的室内定位解决方案

Method: 使用单通道FMCW雷达结合被动雷达反射器，采用多目标粒子群优化算法优化反射器在复杂房间环境中的2D布局

Result: 实现了高定位精度和低系统成本

Conclusion: 该方法为自主移动机器人提供了有效的室内定位解决方案

Abstract: We extend our work on a novel indoor positioning system (IPS) for autonomous
mobile robots (AMRs) based on radar sensing of local, passive radar reflectors.
Through the combination of simple reflectors and a single-channel frequency
modulated continuous wave (FMCW) radar, high positioning accuracy at low system
cost can be achieved. Further, a multi-objective (MO) particle swarm
optimization (PSO) algorithm is presented that optimizes the 2D placement of
radar reflectors in complex room settings.

</details>


### [19] [Omni-LIVO: Robust RGB-Colored Multi-Camera Visual-Inertial-LiDAR Odometry via Photometric Migration and ESIKF Fusion](https://arxiv.org/abs/2509.15673)
*Yinong Cao,Xin He,Yuwei Chen,Chenyang Zhang,Chengyu Pu,Bingtao Wang,Kaile Wu,Shouzheng Zhu,Fei Han,Shijie Liu,Chunlai Li,Jianyu Wang*

Main category: cs.RO

TL;DR: Omni-LIVO是首个紧密耦合的多相机LiDAR-惯性-视觉里程计系统，通过跨视图直接跟踪策略和扩展的误差状态迭代卡尔曼滤波器，解决了宽视场LiDAR与传统相机之间的视场不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LiDAR-惯性-视觉里程计系统大多依赖单个相机，导致空间覆盖有限和鲁棒性降低。宽视场LiDAR传感器虽然能提供大范围环境的密集几何信息，但与常规相机的视场不匹配限制了系统性能。

Method: 提出跨视图直接跟踪策略，在非重叠视图间保持光度一致性；扩展误差状态迭代卡尔曼滤波器，增加多视图更新和自适应协方差加权功能。

Result: 在公共基准测试和自定义数据集上的评估表明，该系统在精度和鲁棒性方面优于当前最先进的LIVO、LIO和视觉-惯性基线方法。

Conclusion: Omni-LIVO通过多相机融合有效解决了视场不匹配问题，显著提升了LiDAR-惯性-视觉里程计系统的性能，代码和数据集将在发表后公开。

Abstract: Wide field-of-view (FoV) LiDAR sensors provide dense geometry across large
environments, but most existing LiDAR-inertial-visual odometry (LIVO) systems
rely on a single camera, leading to limited spatial coverage and degraded
robustness. We present Omni-LIVO, the first tightly coupled multi-camera LIVO
system that bridges the FoV mismatch between wide-angle LiDAR and conventional
cameras. Omni-LIVO introduces a Cross-View direct tracking strategy that
maintains photometric consistency across non-overlapping views, and extends the
Error-State Iterated Kalman Filter (ESIKF) with multi-view updates and adaptive
covariance weighting. The system is evaluated on public benchmarks and our
custom dataset, showing improved accuracy and robustness over state-of-the-art
LIVO, LIO, and visual-inertial baselines. Code and dataset will be released
upon publication.

</details>


### [20] [Imagination at Inference: Synthesizing In-Hand Views for Robust Visuomotor Policy Inference](https://arxiv.org/abs/2509.15717)
*Haoran Ding,Anqing Duan,Zezhou Sun,Dezhen Song,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: 该论文提出了一种通过新颖视角合成技术让机器人能够"想象"手部视角观察的方法，以解决硬件限制下无法配备手部摄像头的问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，手部视角观察对于精确控制至关重要，但实际应用中由于硬件限制、系统复杂性和成本问题，为机器人配备专用手部摄像头存在挑战。

Method: 使用基于LoRA的微调方法，将预训练的新颖视角合成模型（ZeroNVS）适配到机器人操作领域，通过扩散模型根据智能体视角和手部视角相机之间的相对位姿来合成手部视角观察。

Result: 在仿真基准（RoboMimic和MimicGen）和真实世界实验（Unitree Z1机械臂草莓采摘任务）中，合成的手部视角观察显著提升了策略推理性能，有效恢复了因缺少真实手部摄像头导致的性能下降。

Conclusion: 该方法为部署鲁棒的视觉运动策略提供了一种可扩展且硬件要求低的解决方案，展示了具身智能体中想象视觉推理的潜力。

Abstract: Visual observations from different viewpoints can significantly influence the
performance of visuomotor policies in robotic manipulation. Among these,
egocentric (in-hand) views often provide crucial information for precise
control. However, in some applications, equipping robots with dedicated in-hand
cameras may pose challenges due to hardware constraints, system complexity, and
cost. In this work, we propose to endow robots with imaginative perception -
enabling them to 'imagine' in-hand observations from agent views at inference
time. We achieve this via novel view synthesis (NVS), leveraging a fine-tuned
diffusion model conditioned on the relative pose between the agent and in-hand
views cameras. Specifically, we apply LoRA-based fine-tuning to adapt a
pretrained NVS model (ZeroNVS) to the robotic manipulation domain. We evaluate
our approach on both simulation benchmarks (RoboMimic and MimicGen) and
real-world experiments using a Unitree Z1 robotic arm for a strawberry picking
task. Results show that synthesized in-hand views significantly enhance policy
inference, effectively recovering the performance drop caused by the absence of
real in-hand cameras. Our method offers a scalable and hardware-light solution
for deploying robust visuomotor policies, highlighting the potential of
imaginative visual reasoning in embodied agents.

</details>


### [21] [GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation](https://arxiv.org/abs/2509.15733)
*Quanhao Qian,Guoyang Zhao,Gongjie Zhang,Jiuniu Wang,Ran Xu,Junlong Gao,Deli Zhao*

Main category: cs.RO

TL;DR: GP3是一种3D几何感知的机器人操作策略，利用多视角输入生成紧凑的3D场景表示，无需深度传感器即可实现高效操作


<details>
  <summary>Details</summary>
Motivation: 精确的3D场景几何理解对机器人操作至关重要，多视角观测是获取几何信息的最直接方式

Method: 使用空间编码器从RGB观测中推断密集空间特征，估计深度和相机参数，构建紧凑的3D场景表示，结合语言指令通过轻量级策略头生成连续动作

Result: 在模拟基准测试中持续优于最先进方法，无需深度传感器即可有效迁移到真实机器人，仅需少量微调

Conclusion: GP3为几何感知机器人操作提供了一个实用、传感器无关的解决方案

Abstract: Effective robotic manipulation relies on a precise understanding of 3D scene
geometry, and one of the most straightforward ways to acquire such geometry is
through multi-view observations. Motivated by this, we present GP3 -- a 3D
geometry-aware robotic manipulation policy that leverages multi-view input. GP3
employs a spatial encoder to infer dense spatial features from RGB
observations, which enable the estimation of depth and camera parameters,
leading to a compact yet expressive 3D scene representation tailored for
manipulation. This representation is fused with language instructions and
translated into continuous actions via a lightweight policy head. Comprehensive
experiments demonstrate that GP3 consistently outperforms state-of-the-art
methods on simulated benchmarks. Furthermore, GP3 transfers effectively to
real-world robots without depth sensors or pre-mapped environments, requiring
only minimal fine-tuning. These results highlight GP3 as a practical,
sensor-agnostic solution for geometry-aware robotic manipulation.

</details>


### [22] [SMART: Scalable Multi-Agent Reasoning and Trajectory Planning in Dense Environments](https://arxiv.org/abs/2509.15737)
*Heye Huang,Yibin Yang,Wang Chen,Tiantian Chen,Xiaopeng Li,Sikai Chen*

Main category: cs.RO

TL;DR: SMART是一个分层多车辆轨迹规划框架，结合优先级搜索和分布式优化，在密集环境中实现高效可行的多车辆协调规划。


<details>
  <summary>Details</summary>
Motivation: 多车辆轨迹规划是一个非凸问题，在密集环境中由于碰撞约束的快速增长而变得困难。需要高效探索可行行为并解决紧密交互以实现实时大规模协调。

Method: 上层使用基于强化学习的优先级估计和大步长混合A*搜索探索多样交互模式，下层通过可并行化的凸优化细化解决方案。通过空间分区和构建鲁棒可行走廊，将联合非凸问题分解为可并行高效求解的凸子问题。

Result: 在50m×50m地图上，SMART在1秒内维持超过90%成功率至25辆车，而基线方法常低于50%。在100m×100m地图上，SMART达到95%以上成功率至50辆车，在90辆车时仍可行，运行时间比纯优化方法快一个数量级以上。

Conclusion: SMART通过车联网通信整合车路协同，提高了可扩展性和安全性。真实世界实验验证了该设计，规划时间低至0.014秒，同时保持协作行为。

Abstract: Multi-vehicle trajectory planning is a non-convex problem that becomes
increasingly difficult in dense environments due to the rapid growth of
collision constraints. Efficient exploration of feasible behaviors and
resolution of tight interactions are essential for real-time, large-scale
coordination. This paper introduces SMART, Scalable Multi-Agent Reasoning and
Trajectory Planning, a hierarchical framework that combines priority-based
search with distributed optimization to achieve efficient and feasible
multi-vehicle planning. The upper layer explores diverse interaction modes
using reinforcement learning-based priority estimation and large-step hybrid A*
search, while the lower layer refines solutions via parallelizable convex
optimization. By partitioning space among neighboring vehicles and constructing
robust feasible corridors, the method decouples the joint non-convex problem
into convex subproblems solved efficiently in parallel. This design alleviates
the step-size trade-off while ensuring kinematic feasibility and collision
avoidance. Experiments show that SMART consistently outperforms baselines. On
50 m x 50 m maps, it sustains over 90% success within 1 s up to 25 vehicles,
while baselines often drop below 50%. On 100 m x 100 m maps, SMART achieves
above 95% success up to 50 vehicles and remains feasible up to 90 vehicles,
with runtimes more than an order of magnitude faster than optimization-only
approaches. Built on vehicle-to-everything communication, SMART incorporates
vehicle-infrastructure cooperation through roadside sensing and agent
coordination, improving scalability and safety. Real-world experiments further
validate this design, achieving planning times as low as 0.014 s while
preserving cooperative behaviors.

</details>


### [23] [FlyKites: Human-centric Interactive Exploration and Assistance under Limited Communication](https://arxiv.org/abs/2509.15807)
*Yuyang Zhang,Zhuoli Tian,Jinsheng Wei,Meng Guo*

Main category: cs.RO

TL;DR: FlyKites是一个面向多机器人系统在有限通信条件下的人机交互探索与协助框架，包含分布式探索、中继拓扑优化和人机在线执行三个核心组件。


<details>
  <summary>Details</summary>
Motivation: 在极端环境（如洞穴、地下隧道）中，机器人之间的通信严重受限，仅能通过ad-hoc网络进行近距离交换，这给需要人类持续协助的探索任务带来了挑战。

Method: 框架包含三个交织组件：(I)分布式探索和间歇通信（扩展模式），机器人协作探索环境并与操作员交换数据；(II)同时优化中继拓扑、操作员路径和机器人角色分配（中继模式），以最小延迟提供协助；(III)人在回路在线执行，机器人自适应切换角色并与操作员交互。

Result: 在多个挑战性场景中进行了广泛的人机回路仿真和硬件实验验证。

Conclusion: FlyKites框架有效解决了有限通信条件下多机器人系统的探索和人类协助问题，通过分布式协作和自适应角色切换实现了高效的人机交互。

Abstract: Fleets of autonomous robots have been deployed for exploration of unknown
scenes for features of interest, e.g., subterranean exploration,
reconnaissance, search and rescue missions. During exploration, the robots may
encounter un-identified targets, blocked passages, interactive objects,
temporary failure, or other unexpected events, all of which require consistent
human assistance with reliable communication for a time period. This however
can be particularly challenging if the communication among the robots is
severely restricted to only close-range exchange via ad-hoc networks,
especially in extreme environments like caves and underground tunnels. This
paper presents a novel human-centric interactive exploration and assistance
framework called FlyKites, for multi-robot systems under limited communication.
It consists of three interleaved components: (I) the distributed exploration
and intermittent communication (called the "spread mode"), where the robots
collaboratively explore the environment and exchange local data among the fleet
and with the operator; (II) the simultaneous optimization of the relay
topology, the operator path, and the assignment of robots to relay roles
(called the "relay mode"), such that all requested assistance can be provided
with minimum delay; (III) the human-in-the-loop online execution, where the
robots switch between different roles and interact with the operator
adaptively. Extensive human-in-the-loop simulations and hardware experiments
are performed over numerous challenging scenes.

</details>


### [24] [Coordinated Multi-Drone Last-mile Delivery: Learning Strategies for Energy-aware and Timely Operations](https://arxiv.org/abs/2509.15830)
*Chuhao Qin,Arun Narayanan,Evangelos Pournaras*

Main category: cs.RO

TL;DR: 本文提出了一种基于多智能体深度强化学习的能量感知无人机群多包裹配送优化方法，通过分解问题为三个子问题并集成解决方案，实现了经济高效和快速操作的物流配送。


<details>
  <summary>Details</summary>
Motivation: 无人机在最后一公里配送中具有速度更快、更安全、成本更低的优势，特别是在疫情期间的紧急医疗配送中表现突出。本文旨在解决能量感知无人机群在多包裹配送中的新挑战，考虑时间敏感的客户需求。

Method: 将问题分解为三个子问题：使用K-means聚类优化仓库位置和服务区域；通过强化学习确定无人机最佳飞行范围；通过优化的计划选择方法规划和选择多包裹配送路线。提出了一种基于actor-critic的多智能体深度强化学习算法来集成这些解决方案。

Result: 使用真实配送数据集进行的大量实验表明，所提算法表现出卓越性能，在最小化能耗、减少配送延迟和总执行时间方面取得显著效果。

Conclusion: 该方法为实际物流应用提供了经济效率、快速操作和仓库部署战略指导的新见解，证明了多智能体深度强化学习在无人机配送优化中的有效性。

Abstract: Drones have recently emerged as a faster, safer, and cost-efficient way for
last-mile deliveries of parcels, particularly for urgent medical deliveries
highlighted during the pandemic. This paper addresses a new challenge of
multi-parcel delivery with a swarm of energy-aware drones, accounting for
time-sensitive customer requirements. Each drone plans an optimal multi-parcel
route within its battery-restricted flight range to minimize delivery delays
and reduce energy consumption. The problem is tackled by decomposing it into
three sub-problems: (1) optimizing depot locations and service areas using
K-means clustering; (2) determining the optimal flight range for drones through
reinforcement learning; and (3) planning and selecting multi-parcel delivery
routes via a new optimized plan selection approach. To integrate these
solutions and enhance long-term efficiency, we propose a novel algorithm
leveraging actor-critic-based multi-agent deep reinforcement learning.
Extensive experimentation using realistic delivery datasets demonstrate an
exceptional performance of the proposed algorithm. We provide new insights into
economic efficiency (minimize energy consumption), rapid operations (reduce
delivery delays and overall execution time), and strategic guidance on depot
deployment for practical logistics applications.

</details>


### [25] [High-Bandwidth Tactile-Reactive Control for Grasp Adjustment](https://arxiv.org/abs/2509.15876)
*Yonghyeon Lee,Tzu-Yuan Lin,Alexander Alexiev,Sangbae Kim*

Main category: cs.RO

TL;DR: 提出了一种仅使用触觉反馈的抓取调整算法，该控制器不需要物体几何信息或精确抓取姿态，能够从粗糙的初始配置中改进抓取稳定性。


<details>
  <summary>Details</summary>
Motivation: 视觉抓取系统受限于标定误差、传感器噪声和抓取姿态预测不准确，导致最终抓取阶段存在不可避免的接触不确定性。高带宽触觉反馈与触觉反应控制器可以显著提高在感知误差存在时的鲁棒性。

Method: 设计了一种纯触觉反馈的抓取调整算法，该算法无需物体几何先验知识或精确抓取姿态，能够在粗糙、不精确的初始配置和不确定接触点情况下优化抓取。

Result: 通过在15自由度臂手系统（配备8自由度手和200Hz指尖触觉传感器）上的仿真和真实实验，验证了触觉反应抓取框架有效提高了抓取稳定性。

Conclusion: 触觉反应抓取控制器能够有效补偿视觉系统的局限性，提高抓取操作的鲁棒性和成功率。

Abstract: Vision-only grasping systems are fundamentally constrained by calibration
errors, sensor noise, and grasp pose prediction inaccuracies, leading to
unavoidable contact uncertainty in the final stage of grasping. High-bandwidth
tactile feedback, when paired with a well-designed tactile-reactive controller,
can significantly improve robustness in the presence of perception errors. This
paper contributes to controller design by proposing a purely tactile-feedback
grasp-adjustment algorithm. The proposed controller requires neither prior
knowledge of the object's geometry nor an accurate grasp pose, and is capable
of refining a grasp even when starting from a crude, imprecise initial
configuration and uncertain contact points. Through simulation studies and
real-world experiments on a 15-DoF arm-hand system (featuring an 8-DoF hand)
equipped with fingertip tactile sensors operating at 200 Hz, we demonstrate
that our tactile-reactive grasping framework effectively improves grasp
stability.

</details>


### [26] [Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder](https://arxiv.org/abs/2509.15880)
*An Dinh Vuong,Minh Nhat Vu,Ian Reid*

Main category: cs.RO

TL;DR: 该论文提出了一种高效的几何感知视觉编码器eVGGT，通过从VGGT中蒸馏得到，在保持强3D推理能力的同时，比VGGT快9倍、小5倍，显著提升了机器人模仿学习任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB模仿学习方法使用传统视觉编码器（如ResNet、ViT）缺乏显式3D推理能力，而几何感知模型虽然能提供更好的空间理解，但计算成本高，限制了在实际机器人系统中的部署。

Method: 提出eVGGT，一种从VGGT蒸馏得到的高效几何感知编码器，将其集成到模仿学习框架（ACT和DP）中，用于机器人操作任务。

Result: 在仿真和真实世界的单臂和双臂操作任务中，几何感知视觉编码器比标准视觉编码器的成功率提高了6.5%，eVGGT在保持性能的同时实现了9倍速度和5倍尺寸的优化。

Conclusion: 几何感知视觉表示能显著提升机器人模仿学习性能，eVGGT通过高效的蒸馏方法解决了计算成本问题，为几何感知机器人研究提供了实用解决方案。

Abstract: Existing RGB-based imitation learning approaches typically employ traditional
vision encoders such as ResNet or ViT, which lack explicit 3D reasoning
capabilities. Recent geometry-grounded vision models, such as
VGGT~\cite{wang2025vggt}, provide robust spatial understanding and are
promising candidates to address this limitation. This work investigates the
integration of geometry-aware visual representations into robotic manipulation.
Our results suggest that incorporating the geometry-aware vision encoder into
imitation learning frameworks, including ACT and DP, yields up to 6.5%
improvement over standard vision encoders in success rate across single- and
bi-manual manipulation tasks in both simulation and real-world settings.
Despite these benefits, most geometry-grounded models require high
computational cost, limiting their deployment in practical robotic systems. To
address this challenge, we propose eVGGT, an efficient geometry-aware encoder
distilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller than
VGGT, while preserving strong 3D reasoning capabilities. Code and pretrained
models will be released to facilitate further research in geometry-aware
robotics.

</details>


### [27] [An MPC framework for efficient navigation of mobile robots in cluttered environments](https://arxiv.org/abs/2509.15917)
*Johannes Köhler,Daniel Zhang,Raffaele Soloperto,Andrea Carron,Melanie Zeilinger*

Main category: cs.RO

TL;DR: 提出了一种用于移动机器人在杂乱环境中高效导航的模型预测控制框架，该框架将有限段最短路径规划器集成到MPC的有限时域轨迹优化中。


<details>
  <summary>Details</summary>
Motivation: 为了解决移动机器人在复杂、杂乱环境中的导航问题，特别是在需要动态选择目标和保证碰撞避免的情况下。

Method: 将有限段最短路径规划器集成到模型预测控制的有限时域轨迹优化中，确保收敛到动态选择的目标并保证碰撞避免。

Result: 通过小型地面机器人的硬件实验验证，机器人在复杂环境中成功导航，并在2-3秒内到达新目标。

Conclusion: 该方法能够有效处理非线性动力学和杂乱环境下的移动机器人导航问题，具有实际应用价值。

Abstract: We present a model predictive control (MPC) framework for efficient
navigation of mobile robots in cluttered environments. The proposed approach
integrates a finite-segment shortest path planner into the finite-horizon
trajectory optimization of the MPC. This formulation ensures convergence to
dynamically selected targets and guarantees collision avoidance, even under
general nonlinear dynamics and cluttered environments. The approach is
validated through hardware experiments on a small ground robot, where a human
operator dynamically assigns target locations. The robot successfully navigated
through complex environments and reached new targets within 2-3 seconds.

</details>


### [28] [A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning](https://arxiv.org/abs/2509.15937)
*Shaopeng Zhai,Qi Zhang,Tianyi Zhang,Fuxian Huang,Haoran Zhang,Ming Zhou,Shengzhe Zhang,Litao Liu,Sixu Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: VLAC是一个基于InternVL构建的通用过程奖励模型，通过大规模异构数据集训练，能够为机器人视觉语言动作模型提供密集的进度奖励信号，消除任务特定的奖励工程需求，并支持单次上下文迁移到未见任务和环境。


<details>
  <summary>Details</summary>
Motivation: 机器人真实世界强化学习面临稀疏的手工奖励和低效探索的瓶颈问题，需要开发能够提供密集奖励信号并支持高效探索的通用奖励模型。

Method: VLAC模型在视觉语言数据集上训练以增强感知、对话和推理能力，同时结合机器人和人类轨迹数据来接地动作生成和进度估计。通过构建大量负面和语义不匹配样本增强模型拒绝无关提示和检测退步或停滞的能力。部署在异步真实世界强化学习循环中，结合分级的人类参与协议。

Result: 在四个不同的真实世界操作任务中，VLAC将成功率从约30%提升到约90%（200次真实世界交互回合内）；结合人类参与干预进一步提高了50%的样本效率，并实现高达100%的最终成功率。

Conclusion: VLAC通过统一的奖励和动作生成模型，结合分级人类参与协议，显著提升了机器人真实世界强化学习的效率和性能，为解决稀疏奖励和低效探索问题提供了有效解决方案。

Abstract: Robotic real-world reinforcement learning (RL) with vision-language-action
(VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient
exploration. We introduce VLAC, a general process reward model built upon
InternVL and trained on large scale heterogeneous datasets. Given pairwise
observations and a language goal, it outputs dense progress delta and done
signal, eliminating task-specific reward engineering, and supports one-shot
in-context transfer to unseen tasks and environments. VLAC is trained on
vision-language datasets to strengthen perception, dialogic and reasoning
capabilities, together with robot and human trajectories data that ground
action generation and progress estimation, and additionally strengthened to
reject irrelevant prompts as well as detect regression or stagnation by
constructing large numbers of negative and semantically mismatched samples.
With prompt control, a single VLAC model alternately generating reward and
action tokens, unifying critic and policy. Deployed inside an asynchronous
real-world RL loop, we layer a graded human-in-the-loop protocol (offline
demonstration replay, return and explore, human guided explore) that
accelerates exploration and stabilizes early learning. Across four distinct
real-world manipulation tasks, VLAC lifts success rates from about 30\% to
about 90\% within 200 real-world interaction episodes; incorporating
human-in-the-loop interventions yields a further 50% improvement in sample
efficiency and achieves up to 100% final success.

</details>


### [29] [Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal](https://arxiv.org/abs/2509.15953)
*Chang Yu,Siyu Ma,Wenxin Du,Zeshun Zong,Han Xue,Wendi Chen,Cewu Lu,Yin Yang,Xuchen Han,Joseph Masterjohn,Alejandro Castro,Chenfanfu Jiang*

Main category: cs.RO

TL;DR: 提出Right-Side-Out框架，通过任务分解和高效模拟实现衣物翻面操作的零样本sim-to-real迁移


<details>
  <summary>Details</summary>
Motivation: 衣物翻面是极具挑战的机器人操作任务，具有高度动态性、快速接触变化和严重视觉遮挡问题

Method: 将任务分解为Drag/Fling和Insert&Pull两个步骤，使用深度推断的关键点参数化双手机器人基元；构建基于MPM的高保真GPU并行模拟器进行高效数据生成

Result: 仅使用单个深度相机，在真实硬件上实现零样本部署，成功率高达81.3%

Conclusion: 通过任务分解和高保真模拟，该框架能够处理高度动态、严重遮挡的任务，无需繁琐的人工演示

Abstract: Turning garments right-side out is a challenging manipulation task: it is
highly dynamic, entails rapid contact changes, and is subject to severe visual
occlusion. We introduce Right-Side-Out, a zero-shot sim-to-real framework that
effectively solves this challenge by exploiting task structures. We decompose
the task into Drag/Fling to create and stabilize an access opening, followed by
Insert&Pull to invert the garment. Each step uses a depth-inferred,
keypoint-parameterized bimanual primitive that sharply reduces the action space
while preserving robustness. Efficient data generation is enabled by our
custom-built, high-fidelity, GPU-parallel Material Point Method (MPM) simulator
that models thin-shell deformation and provides robust and efficient contact
handling for batched rollouts. Built on the simulator, our fully automated
pipeline scales data generation by randomizing garment geometry, material
parameters, and viewpoints, producing depth, masks, and per-primitive keypoint
labels without any human annotations. With a single depth camera, policies
trained entirely in simulation deploy zero-shot on real hardware, achieving up
to 81.3% success rate. By employing task decomposition and high fidelity
simulation, our framework enables tackling highly dynamic, severely occluded
tasks without laborious human demonstrations.

</details>


### [30] [Swarm Oracle: Trustless Blockchain Agreements through Robot Swarms](https://arxiv.org/abs/2509.15956)
*Alexandre Pacheco,Hanqing Zhao,Volker Strobel,Tarik Roukny,Gregory Dudek,Andreagiovanni Reina,Marco Dorigo*

Main category: cs.RO

TL;DR: Swarm Oracle是一个去中心化的机器人网络，利用机器人集群的传感器和点对点通信来验证现实世界数据并提供给区块链智能合约，解决了区块链与现实世界数据交互的信任问题。


<details>
  <summary>Details</summary>
Motivation: 区块链共识机制限制了与现实世界数据的交互，现有预言机解决方案可能降低自治性、透明度或重新引入信任需求。需要一种能够保持去中心化特性的可靠数据验证方案。

Method: 采用拜占庭容错协议，让来自不同利益相关方的机器人协同工作，通过共识机制达成比单个机器人更高质量的社会协议。结合基于区块链代币的声誉系统实现自主恢复。

Result: 通过真实和模拟机器人的广泛实验证明，即使在大量机器人发起攻击的情况下，仍能就不确定的环境信息达成共识，系统能够从故障和攻击中自主恢复。

Conclusion: Swarm Oracle展示了机器人集群在提供可信现实世界数据给区块链方面的潜力，通过去中心化架构和容错机制确保了系统的长期可靠运行。

Abstract: Blockchain consensus, rooted in the principle ``don't trust, verify'', limits
access to real-world data, which may be ambiguous or inaccessible to some
participants. Oracles address this limitation by supplying data to blockchains,
but existing solutions may reduce autonomy, transparency, or reintroduce the
need for trust. We propose Swarm Oracle: a decentralized network of autonomous
robots -- that is, a robot swarm -- that use onboard sensors and peer-to-peer
communication to collectively verify real-world data and provide it to smart
contracts on public blockchains. Swarm Oracle leverages the built-in
decentralization, fault tolerance and mobility of robot swarms, which can
flexibly adapt to meet information requests on-demand, even in remote
locations. Unlike typical cooperative robot swarms, Swarm Oracle integrates
robots from multiple stakeholders, protecting the system from single-party
biases but also introducing potential adversarial behavior. To ensure the
secure, trustless and global consensus required by blockchains, we employ a
Byzantine fault-tolerant protocol that enables robots from different
stakeholders to operate together, reaching social agreements of higher quality
than the estimates of individual robots. Through extensive experiments using
both real and simulated robots, we showcase how consensus on uncertain
environmental information can be achieved, despite several types of attacks
orchestrated by large proportions of the robots, and how a reputation system
based on blockchain tokens lets Swarm Oracle autonomously recover from faults
and attacks, a requirement for long-term operation.

</details>


### [31] [CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine](https://arxiv.org/abs/2509.15968)
*Shiyu Fang,Yiming Cui,Haoyang Liang,Chen Lv,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: CoReVLA是一个持续学习的端到端自动驾驶框架，通过双阶段数据收集和行为精炼过程，提升自动驾驶系统在长尾安全关键场景中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在长尾安全关键场景中表现有限，这些罕见情况导致了不成比例的事故数量。视觉语言动作模型具有强大的推理能力，但受限于高质量数据的缺乏和低效学习。

Method: 采用双阶段过程：1）在开源驾驶QA数据集上进行联合微调，获得基础驾驶场景理解；2）在CAVE仿真平台部署，收集驾驶员接管数据；3）通过直接偏好优化进行模型精炼，避免人工设计奖励导致的奖励黑客问题。

Result: 在Bench2Drive基准测试中，CoReVLA实现了72.18的驾驶分数和50%的成功率，在长尾安全关键场景下分别比最先进方法高出7.96分和15%。案例研究显示模型能够利用过去的接管经验持续改进性能。

Conclusion: CoReVLA框架通过持续学习和人类偏好优化，有效提升了自动驾驶系统在长尾安全关键场景中的表现，为自动驾驶的安全性和可靠性提供了有前景的解决方案。

Abstract: Autonomous Driving (AD) systems have made notable progress, but their
performance in long-tail, safety-critical scenarios remains limited. These rare
cases contribute a disproportionate number of accidents. Vision-Language Action
(VLA) models have strong reasoning abilities and offer a potential solution,
but their effectiveness is limited by the lack of high-quality data and
inefficient learning in such conditions. To address these challenges, we
propose CoReVLA, a continual learning end-to-end autonomous driving framework
that improves the performance in long-tail scenarios through a dual-stage
process of data Collection and behavior Refinement. First, the model is jointly
fine-tuned on a mixture of open-source driving QA datasets, allowing it to
acquire a foundational understanding of driving scenarios. Next, CoReVLA is
deployed within the Cave Automatic Virtual Environment (CAVE) simulation
platform, where driver takeover data is collected from real-time interactions.
Each takeover indicates a long-tail scenario that CoReVLA fails to handle
reliably. Finally, the model is refined via Direct Preference Optimization
(DPO), allowing it to learn directly from human preferences and thereby avoid
reward hacking caused by manually designed rewards. Extensive open-loop and
closed-loop experiments demonstrate that the proposed CoReVLA model can
accurately perceive driving scenarios and make appropriate decisions. On the
Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a
Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and
15% SR under long-tail, safety-critical scenarios. Furthermore, case studies
demonstrate the model's ability to continually improve its performance in
similar failure-prone scenarios by leveraging past takeover experiences. All
codea and preprocessed datasets are available at:
https://github.com/FanGShiYuu/CoReVLA

</details>


### [32] [Defining and Monitoring Complex Robot Activities via LLMs and Symbolic Reasoning](https://arxiv.org/abs/2509.16006)
*Francesco Argenziano,Elena Umili,Francesco Leotta,Daniele Nardi*

Main category: cs.RO

TL;DR: 本文提出了一种将大型语言模型与自动规划相结合的一般架构，使人类能够使用自然语言指定高级活动，并通过查询机器人来监控其执行。


<details>
  <summary>Details</summary>
Motivation: 在动态和不可预测的环境中自动化劳动密集型复杂活动时，人类监控高级活动的进展（包括过去、现在和未来的行动）对于确保安全关键流程的正确执行仍然至关重要。

Method: 引入一种集成大型语言模型与自动规划的一般架构，使用自然语言指定高级活动，并通过查询机器人来监控执行。采用最先进的组件实现该架构。

Result: 在真实世界的精准农业场景中对该方法进行了定量评估。

Conclusion: 该架构能够有效支持人类使用自然语言与机器人交互，监控复杂活动的执行过程。

Abstract: Recent years have witnessed a growing interest in automating labor-intensive
and complex activities, i.e., those consisting of multiple atomic tasks, by
deploying robots in dynamic and unpredictable environments such as industrial
and agricultural settings. A key characteristic of these contexts is that
activities are not predefined: while they involve a limited set of possible
tasks, their combinations may vary depending on the situation. Moreover,
despite recent advances in robotics, the ability for humans to monitor the
progress of high-level activities - in terms of past, present, and future
actions - remains fundamental to ensure the correct execution of
safety-critical processes. In this paper, we introduce a general architecture
that integrates Large Language Models (LLMs) with automated planning, enabling
humans to specify high-level activities (also referred to as processes) using
natural language, and to monitor their execution by querying a robot. We also
present an implementation of this architecture using state-of-the-art
components and quantitatively evaluate the approach in a real-world precision
agriculture scenario.

</details>


### [33] [A Matter of Height: The Impact of a Robotic Object on Human Compliance](https://arxiv.org/abs/2509.16032)
*Michael Faber,Andrey Grishko,Julian Waksberg,David Pardo,Tomer Leivy,Yuval Hazan,Emanuel Talmansky,Benny Megidish,Hadas Erel*

Main category: cs.RO

TL;DR: 本文研究了机器人高度对人类服从行为的影响，发现与人类身高影响模式相反，较矮的机器人比更高的机器人更能获得人类的服从。


<details>
  <summary>Details</summary>
Motivation: 研究机器人高度是否像人类身高一样影响社会互动，特别是在高度非人形机器人中验证身高对说服力的影响。

Method: 设计可调节高度的模块化机器人，通过两种高度条件（95cm和132cm）测试参与者对机器人请求完成300题问卷的服从程度。

Result: 较矮的机器人（95cm）比更高的机器人（132cm）获得了更高的服从率，这与人类身高影响模式相反。

Conclusion: 机器人高度对人类-机器人互动有重要社会影响，但这种影响遵循独特模式，不能简单套用人类社交动力学到机器人设计中。

Abstract: Robots come in various forms and have different characteristics that may
shape the interaction with them. In human-human interactions, height is a
characteristic that shapes human dynamics, with taller people typically
perceived as more persuasive. In this work, we aspired to evaluate if the same
impact replicates in a human-robot interaction and specifically with a highly
non-humanoid robotic object. The robot was designed with modules that could be
easily added or removed, allowing us to change its height without altering
other design features. To test the impact of the robot's height, we evaluated
participants' compliance with its request to volunteer to perform a tedious
task. In the experiment, participants performed a cognitive task on a computer,
which was framed as the main experiment. When done, they were informed that the
experiment was completed. While waiting to receive their credits, the robotic
object, designed as a mobile robotic service table, entered the room, carrying
a tablet that invited participants to complete a 300-question questionnaire
voluntarily. We compared participants' compliance in two conditions: A Short
robot composed of two modules and 95cm in height and a Tall robot consisting of
three modules and 132cm in height. Our findings revealed higher compliance with
the Short robot's request, demonstrating an opposite pattern to human dynamics.
We conclude that while height has a substantial social impact on human-robot
interactions, it follows a unique pattern of influence. Our findings suggest
that designers cannot simply adopt and implement elements from human social
dynamics to robots without testing them first.

</details>


### [34] [Learning Safety for Obstacle Avoidance via Control Barrier Functions](https://arxiv.org/abs/2509.16037)
*Shuo Liu,Zhe Huang,Calin A. Belta*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经网络的局部安全球方法，用于处理任意几何形状机器人在复杂环境中的实时避障问题，解决了传统CBF方法在复杂几何体上的计算困难。


<details>
  <summary>Details</summary>
Motivation: 现有CBF方法依赖解析计算或多面体近似，对于复杂几何形状的机器人难以实现快速有效的避障控制，特别是在未知配置下计算变得不可行。

Method: 使用残差神经网络在大规模机器人-障碍物配置数据集上进行训练，实现快速间隙预测；基于预测间隙定义局部安全球，并将其编码为离散时间高阶CBF约束，结合非线性优化框架和松弛技术确保可行性。

Result: 实验表明该方法能处理任意（包括非凸）几何形状，在杂乱环境中生成无碰撞、动态可行的轨迹，求解时间达到毫秒级，预测精度高。

Conclusion: 所提方法有效弥合了离散时间控制与连续时间安全性之间的差距，在安全性和效率方面超越了现有CBF方法。

Abstract: Obstacle avoidance is central to safe navigation, especially for robots with
arbitrary and nonconvex geometries operating in cluttered environments.
Existing Control Barrier Function (CBF) approaches often rely on analytic
clearance computations, which are infeasible for complex geometries, or on
polytopic approximations, which become intractable when robot configurations
are unknown. To address these limitations, this paper trains a residual neural
network on a large dataset of robot-obstacle configurations to enable fast and
tractable clearance prediction, even at unseen configurations. The predicted
clearance defines the radius of a Local Safety Ball (LSB), which ensures
continuous-time collision-free navigation. The LSB boundary is encoded as a
Discrete-Time High-Order CBF (DHOCBF), whose constraints are incorporated into
a nonlinear optimization framework. To improve feasibility, a novel relaxation
technique is applied. The resulting framework ensure that the robot's
rigid-body motion between consecutive time steps remains collision-free,
effectively bridging discrete-time control and continuous-time safety. We show
that the proposed method handles arbitrary, including nonconvex, robot
geometries and generates collision-free, dynamically feasible trajectories in
cluttered environments. Experiments demonstrate millisecond-level solve times
and high prediction accuracy, highlighting both safety and efficiency beyond
existing CBF-based methods.

</details>


### [35] [Compose by Focus: Scene Graph-based Atomic Skills](https://arxiv.org/abs/2509.16053)
*Han Qi,Changhe Chen,Heng Yang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于场景图的技能学习框架，通过图神经网络和扩散模仿学习来提升机器人在复杂长时程任务中的组合泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决通用机器人在组合泛化方面的关键需求，即如何将原子技能组合来解决复杂的长时程任务。现有方法主要关注规划器的合成，但单个技能的鲁棒执行仍然面临场景组合引起的分布偏移挑战。

Method: 引入基于场景图的表示方法，关注任务相关的物体和关系，减少对无关变化的敏感性。开发了场景图技能学习框架，结合图神经网络和扩散模仿学习，并与基于视觉语言模型的任务规划器集成。

Result: 在仿真和真实世界操作任务中的实验表明，该方法相比最先进的基线方法取得了显著更高的成功率。

Conclusion: 该方法在长时程任务中表现出更好的鲁棒性和组合泛化能力，为通用机器人的技能学习和任务执行提供了有效解决方案。

Abstract: A key requirement for generalist robots is compositional generalization - the
ability to combine atomic skills to solve complex, long-horizon tasks. While
prior work has primarily focused on synthesizing a planner that sequences
pre-learned skills, robust execution of the individual skills themselves
remains challenging, as visuomotor policies often fail under distribution
shifts induced by scene composition. To address this, we introduce a scene
graph-based representation that focuses on task-relevant objects and relations,
thereby mitigating sensitivity to irrelevant variation. Building on this idea,
we develop a scene-graph skill learning framework that integrates graph neural
networks with diffusion-based imitation learning, and further combine "focused"
scene-graph skills with a vision-language model (VLM) based task planner.
Experiments in both simulation and real-world manipulation tasks demonstrate
substantially higher success rates than state-of-the-art baselines,
highlighting improved robustness and compositional generalization in
long-horizon tasks.

</details>


### [36] [Latent Conditioned Loco-Manipulation Using Motion Priors](https://arxiv.org/abs/2509.16061)
*Maciej Stępień,Rafael Kourdis,Constant Roux,Olivier Stasse*

Main category: cs.RO

TL;DR: 提出一种基于模仿学习的多用途运动策略方法，用于人形和四足机器人的移动操作任务，通过潜在空间控制技能执行，并扩展原始方法以处理约束和使用扩散判别器提高模仿质量。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习方法主要关注单一技能，对于需要考虑高层目标、物理机器人限制和期望运动风格的复杂任务效率低下。

Method: 首先训练一个多用途运动策略，通过模仿获取低层技能，同时提供对技能执行的潜在空间控制；然后使用该策略高效解决下游任务。扩展原始方法以处理约束并确保部署安全性，使用扩散判别器提高模仿质量。

Result: 在H1人形机器人和Solo12四足机器人的仿真中验证了移动操作能力，并将策略部署到Solo12硬件上。

Conclusion: 该方法成功应用于人形和四足机器人的移动操作控制，证明了其在复杂任务中的有效性。

Abstract: Although humanoid and quadruped robots provide a wide range of capabilities,
current control methods, such as Deep Reinforcement Learning, focus mainly on
single skills. This approach is inefficient for solving more complicated tasks
where high-level goals, physical robot limitations and desired motion style
might all need to be taken into account. A more effective approach is to first
train a multipurpose motion policy that acquires low-level skills through
imitation, while providing latent space control over skill execution. Then,
this policy can be used to efficiently solve downstream tasks. This method has
already been successful for controlling characters in computer graphics. In
this work, we apply the approach to humanoid and quadrupedal loco-manipulation
by imitating either simple synthetic motions or kinematically retargeted dog
motions. We extend the original formulation to handle constraints, ensuring
deployment safety, and use a diffusion discriminator for better imitation
quality. We verify our methods by performing loco-manipulation in simulation
for the H1 humanoid and Solo12 quadruped, as well as deploying policies on
Solo12 hardware. Videos and code are available at
https://gepetto.github.io/LaCoLoco/

</details>


### [37] [DSPv2: Improved Dense Policy for Effective and Generalizable Whole-body Mobile Manipulation](https://arxiv.org/abs/2509.16063)
*Yue Su,Chubin Zhang,Sijin Chen,Liufan Tan,Yansong Tang,Jianan Wang,Xihui Liu*

Main category: cs.RO

TL;DR: DSPv2是一种用于全身移动操作的新策略架构，通过3D空间特征与多视角2D语义特征的对齐编码方案，实现了在保持精细感知的同时具备广泛泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决全身移动操作中的复杂观测处理、鲁棒泛化和连贯动作生成等关键挑战，推动机器人技能在多样化环境和复杂任务中的泛化应用。

Method: 提出有效的编码方案，将3D空间特征与多视角2D语义特征对齐融合；将密集策略范式扩展到全身移动操作领域。

Result: 大量实验表明，该方法在任务性能和泛化能力方面显著优于现有方法。

Conclusion: DSPv2架构能够为全身机器人平台生成连贯精确的动作，在移动操作任务中表现出色。

Abstract: Learning whole-body mobile manipulation via imitation is essential for
generalizing robotic skills to diverse environments and complex tasks. However,
this goal is hindered by significant challenges, particularly in effectively
processing complex observation, achieving robust generalization, and generating
coherent actions. To address these issues, we propose DSPv2, a novel policy
architecture. DSPv2 introduces an effective encoding scheme that aligns 3D
spatial features with multi-view 2D semantic features. This fusion enables the
policy to achieve broad generalization while retaining the fine-grained
perception necessary for precise control. Furthermore, we extend the Dense
Policy paradigm to the whole-body mobile manipulation domain, demonstrating its
effectiveness in generating coherent and precise actions for the whole-body
robotic platform. Extensive experiments show that our method significantly
outperforms existing approaches in both task performance and generalization
ability. Project page is available at: https://selen-suyue.github.io/DSPv2Net/.

</details>


### [38] [I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models](https://arxiv.org/abs/2509.16072)
*Clemence Grislain,Hamed Rahimi,Olivier Sigaud,Mohamed Chetouani*

Main category: cs.RO

TL;DR: 该论文提出了I-FailSense框架，专门用于检测语言条件机器人操作中的语义错位错误，通过后训练基础VLM和轻量级分类头的集成机制，在语义错位检测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在机器人操作中虽然具备良好的空间推理和任务规划能力，但缺乏识别自身失败的能力，特别是语义错位错误（机器人执行了语义上有意义但与指令不一致的任务）。

Method: 从现有语言条件操作数据集构建语义错位失败检测数据集，后训练基础VLM，并在VLM不同内部层附加轻量级分类头（FS块），通过集成机制聚合预测结果。

Result: I-FailSense在语义错位错误检测上优于同类规模和更大规模的SOTA VLMs，仅训练语义错位检测就能泛化到更广泛的机器人失败类别，并能零样本或最小后训练迁移到其他仿真环境和现实世界。

Conclusion: 该方法有效解决了机器人操作中的语义错位错误检测问题，具有很好的泛化能力和迁移性，相关数据集和模型已在HuggingFace上开源。

Abstract: Language-conditioned robotic manipulation in open-world settings requires not
only accurate task execution but also the ability to detect failures for robust
deployment in real-world environments. Although recent advances in
vision-language models (VLMs) have significantly improved the spatial reasoning
and task-planning capabilities of robots, they remain limited in their ability
to recognize their own failures. In particular, a critical yet underexplored
challenge lies in detecting semantic misalignment errors, where the robot
executes a task that is semantically meaningful but inconsistent with the given
instruction. To address this, we propose a method for building datasets
targeting Semantic Misalignment Failures detection, from existing
language-conditioned manipulation datasets. We also present I-FailSense, an
open-source VLM framework with grounded arbitration designed specifically for
failure detection. Our approach relies on post-training a base VLM, followed by
training lightweight classification heads, called FS blocks, attached to
different internal layers of the VLM and whose predictions are aggregated using
an ensembling mechanism. Experiments show that I-FailSense outperforms
state-of-the-art VLMs, both comparable in size and larger, in detecting
semantic misalignment errors. Notably, despite being trained only on semantic
misalignment detection, I-FailSense generalizes to broader robotic failure
categories and effectively transfers to other simulation environments and
real-world with zero-shot or minimal post-training. The datasets and models are
publicly released on HuggingFace (Webpage:
https://clemgris.github.io/I-FailSense/).

</details>


### [39] [Real-Time Planning and Control with a Vortex Particle Model for Fixed-Wing UAVs in Unsteady Flows](https://arxiv.org/abs/2509.16079)
*Ashwin Gupta,Kevin Wolfe,Gino Perrotta,Joseph Moore*

Main category: cs.RO

TL;DR: 提出了一种能够考虑非定常空气动力学的实时规划与控制方法，该方法基于轻量级涡粒子模型和基于采样的策略优化，通过GPU加速实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 非定常空气动力学对飞行器性能有重要影响，特别是在敏捷机动和复杂空气动力学环境中。现有方法通常无法实时处理这些效应。

Method: 使用轻量级涡粒子模型进行并行化处理以实现GPU加速，结合基于采样的策略优化方法进行预测推理。

Result: 通过仿真和硬件实验验证，在存在非定常环境流动扰动的情况下，该方法能够改善激进失速后机动性能。

Conclusion: 该方法能够有效处理非定常空气动力学效应，提升飞行器在复杂环境中的机动性能。

Abstract: Unsteady aerodynamic effects can have a profound impact on aerial vehicle
flight performance, especially during agile maneuvers and in complex
aerodynamic environments. In this paper, we present a real-time planning and
control approach capable of reasoning about unsteady aerodynamics. Our approach
relies on a lightweight vortex particle model, parallelized to allow GPU
acceleration, and a sampling-based policy optimization strategy capable of
leveraging the vortex particle model for predictive reasoning. We demonstrate,
through both simulation and hardware experiments, that by replanning with our
unsteady aerodynamics model, we can improve the performance of aggressive
post-stall maneuvers in the presence of unsteady environmental flow
disturbances.

</details>


### [40] [Efficient Detection of Objects Near a Robot Manipulator via Miniature Time-of-Flight Sensors](https://arxiv.org/abs/2509.16122)
*Carter Sifferman,Mohit Gupta,Michael Gleicher*

Main category: cs.RO

TL;DR: 提出了一种使用臂载微型飞行时间传感器检测和定位机器人手臂附近物体的方法，通过建立机器人自身传感器测量模型来区分机器人本体和外部物体。


<details>
  <summary>Details</summary>
Motivation: 解决臂载传感器在测量中难以区分机器人自身和外部物体的关键挑战，提高机器人安全交互能力。

Method: 利用现成的低分辨率飞行时间传感器的原始数据，建立机器人单独存在时的传感器测量经验模型，运行时使用该模型检测机器人附近的物体。

Result: 方法能够检测手臂附近的小物体，并沿机器人连杆长度方向以合理精度定位物体位置，评估了物体类型、位置和环境光照水平对性能的影响。

Conclusion: 该方法在避免机器人自检测的同时提供了传感器放置的灵活性，在碰撞避免和安全人机交互方面具有应用潜力。

Abstract: We provide a method for detecting and localizing objects near a robot arm
using arm-mounted miniature time-of-flight sensors. A key challenge when using
arm-mounted sensors is differentiating between the robot itself and external
objects in sensor measurements. To address this challenge, we propose a
computationally lightweight method which utilizes the raw time-of-flight
information captured by many off-the-shelf, low-resolution time-of-flight
sensor. We build an empirical model of expected sensor measurements in the
presence of the robot alone, and use this model at runtime to detect objects in
proximity to the robot. In addition to avoiding robot self-detections in common
sensor configurations, the proposed method enables extra flexibility in sensor
placement, unlocking configurations which achieve more efficient coverage of a
radius around the robot arm. Our method can detect small objects near the arm
and localize the position of objects along the length of a robot link to
reasonable precision. We evaluate the performance of the method with respect to
object type, location, and ambient light level, and identify limiting factors
on performance inherent in the measurement principle. The proposed method has
potential applications in collision avoidance and in facilitating safe
human-robot interaction.

</details>


### [41] [Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning](https://arxiv.org/abs/2509.16136)
*Changwei Yao,Xinzi Liu,Chen Li,Marios Savvides*

Main category: cs.RO

TL;DR: RE-GoT是一个新颖的双层框架，利用图思维增强LLMs的结构化推理能力，结合VLMs进行自动评估，实现无需人工干预的奖励函数自主进化。


<details>
  <summary>Details</summary>
Motivation: 设计有效的奖励函数是强化学习中的主要挑战，现有基于LLM的方法存在幻觉、依赖人工反馈以及处理复杂多步任务困难等问题。

Method: RE-GoT首先将任务分解为文本属性图进行分析和奖励函数生成，然后利用VLMs的视觉反馈迭代优化奖励函数，整个过程无需人工干预。

Result: 在10个RoboGen和4个ManiSkill2任务上的实验表明，RE-GoT显著优于现有LLM基线方法，在RoboGen上平均任务成功率提升32.25%，在ManiSkill2上达到93.73%的平均成功率。

Conclusion: 将LLMs和VLMs与图思维推理相结合，为强化学习中的自主奖励进化提供了可扩展且有效的解决方案。

Abstract: Designing effective reward functions remains a major challenge in
reinforcement learning (RL), often requiring considerable human expertise and
iterative refinement. Recent advances leverage Large Language Models (LLMs) for
automated reward design, but these approaches are limited by hallucinations,
reliance on human feedback, and challenges with handling complex, multi-step
tasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts
(RE-GoT), a novel bi-level framework that enhances LLMs with structured
graph-based reasoning and integrates Visual Language Models (VLMs) for
automated rollout evaluation. RE-GoT first decomposes tasks into
text-attributed graphs, enabling comprehensive analysis and reward function
generation, and then iteratively refines rewards using visual feedback from
VLMs without human intervention. Extensive experiments on 10 RoboGen and 4
ManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing
LLM-based baselines. On RoboGen, our method improves average task success rates
by 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2,
RE-GoT achieves an average success rate of 93.73% across four diverse
manipulation tasks, significantly surpassing prior LLM-based approaches and
even exceeding expert-designed rewards. Our results indicate that combining
LLMs and VLMs with graph-of-thoughts reasoning provides a scalable and
effective solution for autonomous reward evolution in RL.

</details>


### [42] [Modeling Elastic-Body Dynamics of Fish Swimming Using a Variational Framework](https://arxiv.org/abs/2509.16145)
*Zhiheng Chen,Wei Wang*

Main category: cs.RO

TL;DR: 本文提出了一个基于哈密顿原理推导的鱼类游动全身体动力学模型，该模型能够捕捉柔性鱼体的大变形和流固耦合效应，实现自推进运动。


<details>
  <summary>Details</summary>
Motivation: 鱼类仿生水下机器人因其高游动速度和高效推进而受到关注，但需要准确、可解释且计算可行的游泳动力学模型来支持系统优化设计和控制。

Method: 采用哈密顿原理严格推导了鱼类游动的全身体动力学模型，模型包含连续分布的弹性体大变形和流固耦合效应，无需预设运动学参数即可实现自推进。

Result: 参数研究表明，游动速度和能量效率与尾拍频率呈相反趋势，身体刚度和体长都存在最优值。模拟结果显示特定参数组合下可获得最佳性能。

Conclusion: 该研究为理解生物游泳机制提供了见解，并为高性能软体机器人游泳器的设计提供了指导。

Abstract: Fish-inspired aquatic robots are gaining increasing attention in research
communities due to their high swimming speeds and efficient propulsion enabled
by flexible bodies that generate undulatory motions. To support the design
optimizations and control of such systems, accurate, interpretable, and
computationally tractable modeling of the underlying swimming dynamics is
indispensable. In this letter, we present a full-body dynamics model for fish
swimming, rigorously derived from Hamilton's principle. The model captures the
continuously distributed elasticity of a deformable fish body undergoing large
deformations and incorporates fluid-structure coupling effects, enabling
self-propelled motion without prescribing kinematics. A preliminary parameter
study explores the influence of actuation frequency and body stiffness on
swimming speed and cost of transport (COT). Simulation results indicate that
swimming speed and energy efficiency exhibit opposing trends with tail-beat
frequency and that both body stiffness and body length have distinct optimal
values. These findings provide insights into biological swimming mechanisms and
inform the design of high-performance soft robotic swimmers.

</details>


### [43] [Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories](https://arxiv.org/abs/2509.16176)
*Yifan Lin,Sophie Ziyu Liu,Ran Qi,George Z. Xue,Xinping Song,Chao Qin,Hugh H. -T. Liu*

Main category: cs.RO

TL;DR: ACDC是一个基于自然语言对话的自主无人机摄影系统，利用大语言模型和视觉基础模型将自然语言提示直接转换为可执行的室内无人机视频拍摄轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决传统无人机摄影工作流需要手动选择航点和视角的问题，该方法劳动强度大且性能不一致。

Method: 采用视觉语言检索管道进行初始航点选择，基于偏好的贝叶斯优化框架使用美学反馈优化姿态，以及生成安全四旋翼轨迹的运动规划器。

Result: 通过仿真和硬件在环实验验证，ACDC能够在多样室内场景中稳健地生成专业质量的视频片段，无需机器人或摄影专业知识。

Conclusion: 结果表明具身AI代理有潜力实现从开放词汇对话到真实世界自主空中摄影的闭环。

Abstract: We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic
Trajectories (ACDC), an autonomous drone cinematography system driven by
natural language communication between human directors and drones. The main
limitation of previous drone cinematography workflows is that they require
manual selection of waypoints and view angles based on predefined human intent,
which is labor-intensive and yields inconsistent performance. In this paper, we
propose employing large language models (LLMs) and vision foundation models
(VFMs) to convert free-form natural language prompts directly into executable
indoor UAV video tours. Specifically, our method comprises a vision-language
retrieval pipeline for initial waypoint selection, a preference-based Bayesian
optimization framework that refines poses using aesthetic feedback, and a
motion planner that generates safe quadrotor trajectories. We validate ACDC
through both simulation and hardware-in-the-loop experiments, demonstrating
that it robustly produces professional-quality footage across diverse indoor
scenes without requiring expertise in robotics or cinematography. These results
highlight the potential of embodied AI agents to close the loop from
open-vocabulary dialogue to real-world autonomous aerial cinematography.

</details>
