{"id": "2510.05213", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05213", "abs": "https://arxiv.org/abs/2510.05213", "authors": ["Yixiao Wang", "Mingxiao Huo", "Zhixuan Liang", "Yushi Du", "Lingfeng Sun", "Haotian Lin", "Jinghuan Shang", "Chensheng Peng", "Mohit Bansal", "Mingyu Ding", "Masayoshi Tomizuka"], "title": "VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing", "comment": null, "summary": "Pretrained vision foundation models (VFMs) advance robotic learning via rich\nvisual representations, yet individual VFMs typically excel only in specific\ndomains, limiting generality across tasks. Distilling multiple VFMs into a\nunified representation for policy can mitigate this limitation but often yields\ninflexible task-specific feature selection and requires costly full re-training\nto incorporate robot-domain knowledge. We propose VER, a Vision Expert\ntransformer for Robot learning. During pretraining, VER distills multiple VFMs\ninto a vision expert library. It then fine-tunes only a lightweight routing\nnetwork (fewer than 0.4% of parameters) to dynamically select task-relevant\nexperts from the pretrained library for downstream robot tasks. We further\nintroduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve\nboth flexibility and precision of dynamic expert selection. Moreover, VER\nsupports parameter-efficient finetuning for scalable expert utilization and\nadaptive robot-domain knowledge integration. Across 17 diverse robotic tasks\nand multiple policy heads, VER achieves state-of-the-art performance. We find\nthat VER reduces large-norm outliers in task-irrelevant regions (e.g.,\nbackground) and concentrates on task-critical regions. Visualizations and codes\ncan be found in https://yixiaowang7.github.io/ver_page/."}
{"id": "2510.05330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05330", "abs": "https://arxiv.org/abs/2510.05330", "authors": ["Lu Yuanjie", "Mao Mingyang", "Xu Tong", "Wang Linji", "Lin Xiaomin", "Xiao Xuesu"], "title": "Adaptive Dynamics Planning for Robot Navigation", "comment": "8 pages, 4 figures", "summary": "Autonomous robot navigation systems often rely on hierarchical planning,\nwhere global planners compute collision-free paths without considering\ndynamics, and local planners enforce dynamics constraints to produce executable\ncommands. This discontinuity in dynamics often leads to trajectory tracking\nfailure in highly constrained environments. Recent approaches integrate\ndynamics within the entire planning process by gradually decreasing its\nfidelity, e.g., increasing integration steps and reducing collision checking\nresolution, for real-time planning efficiency. However, they assume that the\nfidelity of the dynamics should decrease according to a manually designed\nscheme. Such static settings fail to adapt to environmental complexity\nvariations, resulting in computational overhead in simple environments or\ninsufficient dynamics consideration in obstacle-rich scenarios. To overcome\nthis limitation, we propose Adaptive Dynamics Planning (ADP), a\nlearning-augmented paradigm that uses reinforcement learning to dynamically\nadjust robot dynamics properties, enabling planners to adapt across diverse\nenvironments. We integrate ADP into three different planners and further design\na standalone ADP-based navigation system, benchmarking them against other\nbaselines. Experiments in both simulation and real-world tests show that ADP\nconsistently improves navigation success, safety, and efficiency."}
{"id": "2510.05382", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05382", "abs": "https://arxiv.org/abs/2510.05382", "authors": ["Zhuowei Xu", "Zilin Si", "Kevin Zhang", "Oliver Kroemer", "Zeynep Temel"], "title": "A multi-modal tactile fingertip design for robotic hands to enhance dexterous manipulation", "comment": null, "summary": "Tactile sensing holds great promise for enhancing manipulation precision and\nversatility, but its adoption in robotic hands remains limited due to high\nsensor costs, manufacturing and integration challenges, and difficulties in\nextracting expressive and reliable information from signals. In this work, we\npresent a low-cost, easy-to-make, adaptable, and compact fingertip design for\nrobotic hands that integrates multi-modal tactile sensors. We use strain gauge\nsensors to capture static forces and a contact microphone sensor to measure\nhigh-frequency vibrations during contact. These tactile sensors are integrated\ninto a compact design with a minimal sensor footprint, and all sensors are\ninternal to the fingertip and therefore not susceptible to direct wear and tear\nfrom interactions. From sensor characterization, we show that strain gauge\nsensors provide repeatable 2D planar force measurements in the 0-5 N range and\nthe contact microphone sensor has the capability to distinguish contact\nmaterial properties. We apply our design to three dexterous manipulation tasks\nthat range from zero to full visual occlusion. Given the expressiveness and\nreliability of tactile sensor readings, we show that different tactile sensing\nmodalities can be used flexibly in different stages of manipulation, solely or\ntogether with visual observations to achieve improved task performance. For\ninstance, we can precisely count and unstack a desired number of paper cups\nfrom a stack with 100\\% success rate which is hard to achieve with vision only."}
{"id": "2510.05425", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05425", "abs": "https://arxiv.org/abs/2510.05425", "authors": ["Marta Lagomarsino", "Francesco Tassi"], "title": "Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios", "comment": null, "summary": "Work environments are often inadequate and lack inclusivity for individuals\nwith upper-body disabilities. This paper presents a novel online framework for\nadaptive human-robot interaction (HRI) that accommodates users' arm mobility\nimpairments, ultimately aiming to promote active work participation. Unlike\ntraditional human-robot collaboration approaches that assume able-bodied users,\nour method integrates a mobility model for specific joint limitations into a\nhierarchical optimal controller. This allows the robot to generate reactive,\nmobility-aware behaviour online and guides the user's impaired limb to exploit\nresidual functional mobility. The framework was tested in handover tasks\ninvolving different upper-limb mobility impairments (i.e., emulated elbow and\nshoulder arthritis, and wrist blockage), under both standing and seated\nconfigurations with task constraints using a mobile manipulator, and\ncomplemented by quantitative and qualitative comparisons with state-of-the-art\nergonomic HRI approaches. Preliminary results indicated that the framework can\npersonalise the interaction to fit within the user's impaired range of motion\nand encourage joint usage based on the severity of their functional\nlimitations."}
{"id": "2510.05430", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05430", "abs": "https://arxiv.org/abs/2510.05430", "authors": ["Huayi Tang", "Pratik Chaudhari"], "title": "Active Semantic Perception", "comment": null, "summary": "We develop an approach for active semantic perception which refers to using\nthe semantics of the scene for tasks such as exploration. We build a compact,\nhierarchical multi-layer scene graph that can represent large, complex indoor\nenvironments at various levels of abstraction, e.g., nodes corresponding to\nrooms, objects, walls, windows etc. as well as fine-grained details of their\ngeometry. We develop a procedure based on large language models (LLMs) to\nsample plausible scene graphs of unobserved regions that are consistent with\npartial observations of the scene. These samples are used to compute an\ninformation gain of a potential waypoint for sophisticated spatial reasoning,\ne.g., the two doors in the living room can lead to either a kitchen or a\nbedroom. We evaluate this approach in complex, realistic 3D indoor environments\nin simulation. We show using qualitative and quantitative experiments that our\napproach can pin down the semantics of the environment quicker and more\naccurately than baseline approaches."}
{"id": "2510.05443", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05443", "abs": "https://arxiv.org/abs/2510.05443", "authors": ["Shao-Yi Yu", "Jen-Wei Wang", "Maya Horii", "Vikas Garg", "Tarek Zohdi"], "title": "AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control", "comment": null, "summary": "Mobile robots, such as ground vehicles and quadrotors, are becoming\nincreasingly important in various fields, from logistics to agriculture, where\nthey automate processes in environments that are difficult to access for\nhumans. However, to perform effectively in uncertain environments using\nmodel-based controllers, these systems require dynamics models capable of\nresponding to environmental variations, especially when direct access to\nenvironmental information is limited. To enable such adaptivity and facilitate\nintegration with model predictive control, we propose an adaptive dynamics\nmodel which bypasses the need for direct environmental knowledge by inferring\noperational environments from state-action history. The dynamics model is based\non neural ordinary equations, and a two-phase training procedure is used to\nlearn latent environment representations. We demonstrate the effectiveness of\nour approach through goal-reaching and path-tracking tasks on three robotic\nplatforms of increasing complexity: a 2D differential wheeled robot with\nchanging wheel contact conditions, a 3D quadrotor in variational wind fields,\nand the Sphero BOLT robot under two contact conditions for real-world\ndeployment. Empirical results corroborate that our method can handle temporally\nand spatially varying environmental changes in both simulation and real-world\nsystems."}
{"id": "2510.05536", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05536", "abs": "https://arxiv.org/abs/2510.05536", "authors": ["Mahboubeh Zarei", "Robin Chhabra", "Farrokh Janabi-Sharifi"], "title": "Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation", "comment": null, "summary": "Accurate pose and velocity estimation is essential for effective spatial task\nplanning in robotic manipulators. While centralized sensor fusion has\ntraditionally been used to improve pose estimation accuracy, this paper\npresents a novel decentralized fusion approach to estimate both pose and\nvelocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand\nvision sensor configuration mounted on a manipulator to track a target object\nwhose motion is modeled as random walk (stochastic acceleration model). The\nrobot runs two independent adaptive extended Kalman filters formulated on a\nmatrix Lie group, developed as part of this work. These filters predict poses\nand velocities on the manifold $\\mathbb{SE}(3) \\times \\mathbb{R}^3 \\times\n\\mathbb{R}^3$ and update the state on the manifold $\\mathbb{SE}(3)$. The final\nfused state comprising the fused pose and velocities of the target is obtained\nusing a correlation-aware fusion rule on Lie groups. The proposed method is\nevaluated on a UFactory xArm 850 equipped with Intel RealSense cameras,\ntracking a moving target. Experimental results validate the effectiveness and\nrobustness of the proposed decentralized dual-view estimation framework,\nshowing consistent improvements over state-of-the-art methods."}
{"id": "2510.05547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05547", "abs": "https://arxiv.org/abs/2510.05547", "authors": ["Eugene Vorobiov", "Ammar Jaleel Mahmood", "Salim Rezvani", "Robin Chhabra"], "title": "ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation", "comment": null, "summary": "We present ARRC (Advanced Reasoning Robot Control), a practical system that\nconnects natural-language instructions to safe local robotic control by\ncombining Retrieval-Augmented Generation (RAG) with RGB-D perception and\nguarded execution on an affordable robot arm. The system indexes curated robot\nknowledge (movement patterns, task templates, and safety heuristics) in a\nvector database, retrieves task-relevant context for each instruction, and\nconditions a large language model (LLM) to produce JSON-structured action\nplans. Plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven\nparallel gripper and an Intel RealSense D435 camera. Perception uses AprilTag\ndetections fused with depth to produce object-centric metric poses. Execution\nis enforced via software safety gates: workspace bounds, speed and force caps,\ntimeouts, and bounded retries. We describe the architecture, knowledge design,\nintegration choices, and a reproducible evaluation protocol for tabletop scan,\napproach, and pick-place tasks. Experimental results demonstrate the efficacy\nof the proposed approach. Our design shows that RAG-based planning can\nsubstantially improve plan validity and adaptability while keeping perception\nand low-level control local to the robot."}
{"id": "2510.05553", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05553", "abs": "https://arxiv.org/abs/2510.05553", "authors": ["Yan Rui Tan", "Wenqi Liu", "Wai Lun Leong", "John Guan Zhong Tan", "Wayne Wen Huei Yong", "Fan Shi", "Rodney Swee Huat Teo"], "title": "GO-Flock: Goal-Oriented Flocking in 3D Unknown Environments with Depth Maps", "comment": null, "summary": "Artificial Potential Field (APF) methods are widely used for reactive\nflocking control, but they often suffer from challenges such as deadlocks and\nlocal minima, especially in the presence of obstacles. Existing solutions to\naddress these issues are typically passive, leading to slow and inefficient\ncollective navigation. As a result, many APF approaches have only been\nvalidated in obstacle-free environments or simplified, pseudo 3D simulations.\nThis paper presents GO-Flock, a hybrid flocking framework that integrates\nplanning with reactive APF-based control. GO-Flock consists of an upstream\nPerception Module, which processes depth maps to extract waypoints and virtual\nagents for obstacle avoidance, and a downstream Collective Navigation Module,\nwhich applies a novel APF strategy to achieve effective flocking behavior in\ncluttered environments. We evaluate GO-Flock against passive APF-based\napproaches to demonstrate their respective merits, such as their flocking\nbehavior and the ability to overcome local minima. Finally, we validate\nGO-Flock through obstacle-filled environment and also hardware-in-the-loop\nexperiments where we successfully flocked a team of nine drones, six physical\nand three virtual, in a forest environment."}
{"id": "2510.05662", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05662", "abs": "https://arxiv.org/abs/2510.05662", "authors": ["Taeyeop Lee", "Gyuree Kang", "Bowen Wen", "Youngho Kim", "Seunghyeok Back", "In So Kweon", "David Hyunchul Shim", "Kuk-Jin Yoon"], "title": "DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation", "comment": "Project page: https://sites.google.com/view/DeLTa25/", "summary": "Despite the prevalence of transparent object interactions in human everyday\nlife, transparent robotic manipulation research remains limited to\nshort-horizon tasks and basic grasping capabilities.Although some methods have\npartially addressed these issues, most of them have limitations in\ngeneralizability to novel objects and are insufficient for precise long-horizon\nrobot manipulation. To address this limitation, we propose DeLTa (Demonstration\nand Language-Guided Novel Transparent Object Manipulation), a novel framework\nthat integrates depth estimation, 6D pose estimation, and vision-language\nplanning for precise long-horizon manipulation of transparent objects guided by\nnatural task instructions. A key advantage of our method is its\nsingle-demonstration approach, which generalizes 6D trajectories to novel\ntransparent objects without requiring category-level priors or additional\ntraining. Additionally, we present a task planner that refines the\nVLM-generated plan to account for the constraints of a single-arm, eye-in-hand\nrobot for long-horizon object manipulation tasks. Through comprehensive\nevaluation, we demonstrate that our method significantly outperforms existing\ntransparent object manipulation approaches, particularly in long-horizon\nscenarios requiring precise manipulation capabilities. Project page:\nhttps://sites.google.com/view/DeLTa25/"}
{"id": "2510.05681", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05681", "abs": "https://arxiv.org/abs/2510.05681", "authors": ["Suhyeok Jang", "Dongyoung Kim", "Changyeon Kim", "Youngsuk Kim", "Jinwoo Shin"], "title": "Verifier-free Test-Time Sampling for Vision Language Action Models", "comment": "14 pages; 3 figures", "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\nin robot control. However, they remain fundamentally limited in tasks that\nrequire high precision due to their single-inference paradigm. While test-time\nscaling approaches using external verifiers have shown promise, they require\nadditional training and fail to generalize to unseen conditions. We propose\nMasking Distribution Guided Selection (MG-Select), a novel test-time scaling\nframework for VLAs that leverages the model's internal properties without\nrequiring additional training or external modules. Our approach utilizes KL\ndivergence from a reference action token distribution as a confidence metric\nfor selecting the optimal action from multiple candidates. We introduce a\nreference distribution generated by the same VLA but with randomly masked\nstates and language conditions as inputs, ensuring maximum uncertainty while\nremaining aligned with the target task distribution. Additionally, we propose a\njoint training strategy that enables the model to learn both conditional and\nunconditional distributions by applying dropout to state and language\nconditions, thereby further improving the quality of the reference\ndistribution. Our experiments demonstrate that MG-Select achieves significant\nperformance improvements, including a 28%/35% improvement in real-world\nin-distribution/out-of-distribution tasks, along with a 168% relative gain on\nRoboCasa pick-and-place tasks trained with 30 demonstrations."}
{"id": "2510.05692", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05692", "abs": "https://arxiv.org/abs/2510.05692", "authors": ["Yuhang Zhang", "Jiaping Xiao", "Chao Yan", "Mir Feroskhan"], "title": "Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies", "comment": null, "summary": "A prevailing approach for learning visuomotor policies is to employ\nreinforcement learning to map high-dimensional visual observations directly to\naction commands. However, the combination of high-dimensional visual inputs and\nagile maneuver outputs leads to long-standing challenges, including low sample\nefficiency and significant sim-to-real gaps. To address these issues, we\npropose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a\nnovel framework designed to improve the sample efficiency and asymptotic\nperformance of visuomotor policy learning. OMC-RL explicitly decouples the\nlearning process into two stages: an upstream representation learning stage and\na downstream policy learning stage. In the upstream stage, a masked Transformer\nmodule is trained with temporal modeling and contrastive learning to extract\ntemporally-aware and task-relevant representations from sequential visual\ninputs. After training, the learned encoder is frozen and used to extract\nvisual representations from consecutive frames, while the Transformer module is\ndiscarded. In the downstream stage, an oracle teacher policy with privileged\naccess to global state information supervises the agent during early training\nto provide informative guidance and accelerate early policy learning. This\nguidance is gradually reduced to allow independent exploration as training\nprogresses. Extensive experiments in simulated and real-world environments\ndemonstrate that OMC-RL achieves superior sample efficiency and asymptotic\npolicy performance, while also improving generalization across diverse and\nperceptually complex scenarios."}
{"id": "2510.05707", "categories": ["cs.RO", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.05707", "abs": "https://arxiv.org/abs/2510.05707", "authors": ["David Boetius", "Abdelrahman Abdelnaby", "Ashok Kumar", "Stefan Leue", "Abdalla Swikir", "Fares J. Abu-Dakka"], "title": "Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs", "comment": "12 pages, 6 figures", "summary": "Learning stable dynamical systems from data is crucial for safe and reliable\nrobot motion planning and control. However, extending stability guarantees to\ntrajectories defined on Riemannian manifolds poses significant challenges due\nto the manifold's geometric constraints. To address this, we propose a general\nframework for learning stable dynamical systems on Riemannian manifolds using\nneural ordinary differential equations. Our method guarantees stability by\nprojecting the neural vector field evolving on the manifold so that it strictly\nsatisfies the Lyapunov stability criterion, ensuring stability at every system\nstate. By leveraging a flexible neural parameterisation for both the base\nvector field and the Lyapunov function, our framework can accurately represent\ncomplex trajectories while respecting manifold constraints by evolving\nsolutions directly on the manifold. We provide an efficient training strategy\nfor applying our framework and demonstrate its utility by solving Riemannian\nLASA datasets on the unit quaternion (S^3) and symmetric positive-definite\nmatrix manifolds, as well as robotic motions evolving on \\mathbb{R}^3 \\times\nS^3. We demonstrate the performance, scalability, and practical applicability\nof our approach through extensive simulations and by learning robot motions in\na real-world experiment."}
{"id": "2510.05713", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05713", "abs": "https://arxiv.org/abs/2510.05713", "authors": ["Wanli Ni", "Hui Tian", "Shuai Wang", "Chengyang Li", "Lei Sun", "Zhaohui Yang"], "title": "Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions", "comment": "9 pages, 5 figures, submitted to the IEEE magazine", "summary": "Federated split learning (FedSL) has emerged as a promising paradigm for\nenabling collaborative intelligence in industrial Internet of Things (IoT)\nsystems, particularly in smart factories where data privacy, communication\nefficiency, and device heterogeneity are critical concerns. In this article, we\npresent a comprehensive study of FedSL frameworks tailored for\nresource-constrained robots in industrial scenarios. We compare synchronous,\nasynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of\nworkflow, scalability, adaptability, and limitations under dynamic industrial\nconditions. Furthermore, we systematically categorize token fusion strategies\ninto three paradigms: input-level (pre-fusion), intermediate-level\n(intra-fusion), and output-level (post-fusion), and summarize their respective\nstrengths in industrial applications. We also provide adaptive optimization\ntechniques to enhance the efficiency and feasibility of FedSL implementation,\nincluding model compression, split layer selection, computing frequency\nallocation, and wireless resource management. Simulation results validate the\nperformance of these frameworks under industrial detection scenarios. Finally,\nwe outline open issues and research directions of FedSL in future smart\nmanufacturing systems."}
{"id": "2510.05729", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05729", "abs": "https://arxiv.org/abs/2510.05729", "authors": ["Marc Kaufeld", "Johannes Betz"], "title": "Precise and Efficient Collision Prediction under Uncertainty in Autonomous Driving", "comment": "8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria", "summary": "This research introduces two efficient methods to estimate the collision risk\nof planned trajectories in autonomous driving under uncertain driving\nconditions. Deterministic collision checks of planned trajectories are often\ninaccurate or overly conservative, as noisy perception, localization errors,\nand uncertain predictions of other traffic participants introduce significant\nuncertainty into the planning process. This paper presents two semi-analytic\nmethods to compute the collision probability of planned trajectories with\narbitrary convex obstacles. The first approach evaluates the probability of\nspatial overlap between an autonomous vehicle and surrounding obstacles, while\nthe second estimates the collision probability based on stochastic boundary\ncrossings. Both formulations incorporate full state uncertainties, including\nposition, orientation, and velocity, and achieve high accuracy at computational\ncosts suitable for real-time planning. Simulation studies verify that the\nproposed methods closely match Monte Carlo results while providing significant\nruntime advantages, enabling their use in risk-aware trajectory planning. The\ncollision estimation methods are available as open-source software:\nhttps://github.com/TUM-AVS/Collision-Probability-Estimation"}
{"id": "2510.05780", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05780", "abs": "https://arxiv.org/abs/2510.05780", "authors": ["Andreas Christou", "Andreas Sochopoulos", "Elliot Lister", "Sethu Vijayakumar"], "title": "Human-in-the-loop Optimisation in Robot-assisted Gait Training", "comment": null, "summary": "Wearable robots offer a promising solution for quantitatively monitoring gait\nand providing systematic, adaptive assistance to promote patient independence\nand improve gait. However, due to significant interpersonal and intrapersonal\nvariability in walking patterns, it is important to design robot controllers\nthat can adapt to the unique characteristics of each individual. This paper\ninvestigates the potential of human-in-the-loop optimisation (HILO) to deliver\npersonalised assistance in gait training. The Covariance Matrix Adaptation\nEvolution Strategy (CMA-ES) was employed to continuously optimise an\nassist-as-needed controller of a lower-limb exoskeleton. Six healthy\nindividuals participated over a two-day experiment. Our results suggest that\nwhile the CMA-ES appears to converge to a unique set of stiffnesses for each\nindividual, no measurable impact on the subjects' performance was observed\nduring the validation trials. These findings highlight the impact of\nhuman-robot co-adaptation and human behaviour variability, whose effect may be\ngreater than potential benefits of personalising rule-based assistive\ncontrollers. Our work contributes to understanding the limitations of current\npersonalisation approaches in exoskeleton-assisted gait rehabilitation and\nidentifies key challenges for effective implementation of human-in-the-loop\noptimisation in this domain."}
{"id": "2510.05827", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05827", "abs": "https://arxiv.org/abs/2510.05827", "authors": ["Haoran Zhang", "Shuanghao Bai", "Wanqi Zhou", "Yuedi Zhang", "Qi Zhang", "Pengxiang Ding", "Cheng Chi", "Donglin Wang", "Badong Chen"], "title": "VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation", "comment": null, "summary": "Robotic grasping is one of the most fundamental tasks in robotic\nmanipulation, and grasp detection/generation has long been the subject of\nextensive research. Recently, language-driven grasp generation has emerged as a\npromising direction due to its practical interaction capabilities. However,\nmost existing approaches either lack sufficient reasoning and generalization\ncapabilities or depend on complex modular pipelines. Moreover, current grasp\nfoundation models tend to overemphasize dialog and object semantics, resulting\nin inferior performance and restriction to single-object grasping. To maintain\nstrong reasoning ability and generalization in cluttered environments, we\npropose VCoT-Grasp, an end-to-end grasp foundation model that incorporates\nvisual chain-of-thought reasoning to enhance visual understanding for grasp\ngeneration. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically\nfocuses on visual inputs while providing interpretable reasoning traces. For\ntraining, we refine and introduce a large-scale dataset, VCoT-GraspSet,\ncomprising 167K synthetic images with over 1.36M grasps, as well as 400+\nreal-world images with more than 1.2K grasps, annotated with intermediate\nbounding boxes. Extensive experiments on both VCoT-GraspSet and real robot\ndemonstrate that our method significantly improves grasp success rates and\ngeneralizes effectively to unseen objects, backgrounds, and distractors. More\ndetails can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io."}
{"id": "2510.05923", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05923", "abs": "https://arxiv.org/abs/2510.05923", "authors": ["Aman Singh", "Aastha Mishra", "Deepak Kapa", "Suryank Joshi", "Shishir Kolathaya"], "title": "A Co-Design Framework for Energy-Aware Monoped Jumping with Detailed Actuator Modeling", "comment": "7 pages, 8 figures, 1 table, Accepted at IEEE-RAS 24th International\n  Conference on Humanoid Robots (Humanoids) 2025, Aman Singh, Aastha Mishra -\n  Authors contributed equally", "summary": "A monoped's jump height and energy consumption depend on both, its mechanical\ndesign and control strategy. Existing co-design frameworks typically optimize\nfor either maximum height or minimum energy, neglecting their trade-off. They\nalso often omit gearbox parameter optimization and use oversimplified actuator\nmass models, producing designs difficult to replicate in practice. In this\nwork, we introduce a novel three-stage co-design optimization framework that\njointly maximizes jump height while minimizing mechanical energy consumption of\na monoped. The proposed method explicitly incorporates realistic actuator mass\nmodels and optimizes mechanical design (including gearbox) and control\nparameters within a unified framework. The resulting design outputs are then\nused to automatically generate a parameterized CAD model suitable for direct\nfabrication, significantly reducing manual design iterations. Our experimental\nevaluations demonstrate a 50 percent reduction in mechanical energy consumption\ncompared to the baseline design, while achieving a jump height of 0.8m. Video\npresentation is available at http://y2u.be/XW8IFRCcPgM"}
{"id": "2510.05957", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05957", "abs": "https://arxiv.org/abs/2510.05957", "authors": ["Vaughn Gzenda", "Robin Chhabra"], "title": "Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion", "comment": null, "summary": "Soft robotic crawlers are mobile robots that utilize soft body deformability\nand compliance to achieve locomotion through surface contact. Designing control\nstrategies for such systems is challenging due to model inaccuracies, sensor\nnoise, and the need to discover locomotor gaits. In this work, we present a\nmodel-based reinforcement learning (MB-RL) framework in which latent dynamics\ninferred from onboard sensors serve as a predictive model that guides an\nactor-critic algorithm to optimize locomotor policies. We evaluate the\nframework on a minimal crawler model in simulation using inertial measurement\nunits and time-of-flight sensors as observations. The learned latent dynamics\nenable short-horizon motion prediction while the actor-critic discovers\neffective locomotor policies. This approach highlights the potential of\nlatent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion\nbased solely on noisy sensor feedback."}
{"id": "2510.05981", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05981", "abs": "https://arxiv.org/abs/2510.05981", "authors": ["Cristina Luna", "Alba Guerra", "Almudena Moreno", "Manuel Esquer", "Willy Roa", "Mateusz Krawczak", "Robert Popela", "Piotr Osica", "Davide Nicolis"], "title": "The DISTANT Design for Remote Transmission and Steering Systems for Planetary Robotics", "comment": "Paper for 18th Symposium on Advanced Space Technologies in Robotics\n  and Automation (ASTRA), presented on October 7th at Leiden, Netherlands", "summary": "Planetary exploration missions require robust locomotion systems capable of\noperating in extreme environments over extended periods. This paper presents\nthe DISTANT (Distant Transmission and Steering Systems) design, a novel\napproach for relocating rover traction and steering actuators from\nwheel-mounted positions to a thermally protected warm box within the rover\nbody. The design addresses critical challenges in long-distance traversal\nmissions by protecting sensitive components from thermal cycling, dust\ncontamination, and mechanical wear. A double wishbone suspension configuration\nwith cardan joints and capstan drive steering has been selected as the optimal\narchitecture following comprehensive trade-off analysis. The system enables\nindependent wheel traction, steering control, and suspension management whilst\nmaintaining all motorisation within the protected environment. The design meets\na 50 km traverse requirement without performance degradation, with integrated\ndust protection mechanisms and thermal management solutions. Testing and\nvalidation activities are planned for Q1 2026 following breadboard\nmanufacturing at 1:3 scale."}
{"id": "2510.05985", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05985", "abs": "https://arxiv.org/abs/2510.05985", "authors": ["Cristina Luna", "Robert Field", "Steven Kay"], "title": "AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations", "comment": "Paper for 18th Symposium on Advanced Space Technologies in Robotics\n  and Automation (ASTRA), presented on October 7th at Leiden, Netherlands", "summary": "Current planetary rovers operate at traverse speeds of approximately 10 cm/s,\nfundamentally limiting exploration efficiency. This work presents integrated AI\nsystems which significantly improve autonomy through three components: (i) the\nFASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s\nspeeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot\ncoordination framework enabling human-robot collaboration for in-situ resource\nutilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain\nclassification studies. Field validation in Mars analogue environments\ndemonstrated these systems at Technology Readiness Level 4, providing\nmeasurable improvements in traverse speed, classification accuracy, and\noperational safety for next-generation planetary missions."}
{"id": "2510.05992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05992", "abs": "https://arxiv.org/abs/2510.05992", "authors": ["Tien-Dat Nguyen", "Thien-Minh Nguyen", "Vinh-Hao Nguyen"], "title": "Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations", "comment": null, "summary": "Onboard simultaneous localization and mapping (SLAM) methods are commonly\nused to provide accurate localization information for autonomous robots.\nHowever, the coordinate origin of SLAM estimate often resets for each run. On\nthe other hand, UWB-based localization with fixed anchors can ensure a\nconsistent coordinate reference across sessions; however, it requires an\naccurate assignment of the anchor nodes' coordinates. To this end, we propose a\ntwo-stage approach that calibrates and fuses UWB data and SLAM data to achieve\ncoordinate-wise consistent and accurate localization in the same environment.\nIn the first stage, we solve a continuous-time batch optimization problem by\nusing the range and odometry data from one full run, incorporating height\npriors and anchor-to-anchor distance factors to recover the anchors' 3D\npositions. For the subsequent runs in the second stage, a sliding-window\noptimization scheme fuses the UWB and SLAM data, which facilitates accurate\nlocalization in the same coordinate system. Experiments are carried out on the\nNTU VIRAL dataset with six scenarios of UAV flight, and we show that\ncalibration using data in one run is sufficient to enable accurate localization\nin the remaining runs. We release our source code to benefit the community at\nhttps://github.com/ntdathp/slam-uwb-calibration."}
{"id": "2510.06068", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06068", "abs": "https://arxiv.org/abs/2510.06068", "authors": ["Heng Zhang", "Kevin Yuchen Ma", "Mike Zheng Shou", "Weisi Lin", "Yan Wu"], "title": "Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning", "comment": null, "summary": "Dexterous grasping with multi-fingered hands remains challenging due to\nhigh-dimensional articulations and the cost of optimization-based pipelines.\nExisting end-to-end methods require training on large-scale datasets for\nspecific hands, limiting their ability to generalize across different\nembodiments. We propose an eigengrasp-based, end-to-end framework for\ncross-embodiment grasp generation. From a hand's morphology description, we\nderive a morphology embedding and an eigengrasp set. Conditioned on these,\ntogether with the object point cloud and wrist pose, an amplitude predictor\nregresses articulation coefficients in a low-dimensional space, which are\ndecoded into full joint articulations. Articulation learning is supervised with\na Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant\nmotions and injects morphology-specific structure. In simulation on unseen\nobjects across three dexterous hands, our model attains a 91.9% average grasp\nsuccess rate with less than 0.4 seconds inference per grasp. With few-shot\nadaptation to an unseen hand, it achieves 85.6% success on unseen objects in\nsimulation, and real-world experiments on this few-shot generalized hand\nachieve an 87% success rate. The code and additional materials will be made\navailable upon publication on our project website\nhttps://connor-zh.github.io/cross_embodiment_dexterous_grasping."}
{"id": "2510.06085", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06085", "abs": "https://arxiv.org/abs/2510.06085", "authors": ["Roman Ibrahimov", "Jannik Matthias Heinen"], "title": "Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor", "comment": null, "summary": "This project proposes a bioinspired multi-robot system using Distributed\nOptimization for efficient exploration and mapping of unknown environments.\nEach robot explores its environment and creates a map, which is afterwards put\ntogether to form a global 2D map of the environment. Inspired by wall-following\nbehaviors, each robot autonomously explores its neighborhood based on a tactile\nsensor, similar to the antenna of a cockroach, mounted on the surface of the\nrobot. Instead of avoiding obstacles, robots log collision points when they\ntouch obstacles. This decentralized control strategy ensures effective task\nallocation and efficient exploration of unknown terrains, with applications in\nsearch and rescue, industrial inspection, and environmental monitoring. The\napproach was validated through experiments using e-puck robots in a simulated\n1.5 x 1.5 m environment with three obstacles. The results demonstrated the\nsystem's effectiveness in achieving high coverage, minimizing collisions, and\nconstructing accurate 2D maps."}
{"id": "2510.06127", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06127", "abs": "https://arxiv.org/abs/2510.06127", "authors": ["Xiao Liang", "Lu Shen", "Peihan Zhang", "Soofiyan Atar", "Florian Richter", "Michael Yip"], "title": "Towards Autonomous Tape Handling for Robotic Wound Redressing", "comment": null, "summary": "Chronic wounds, such as diabetic, pressure, and venous ulcers, affect over\n6.5 million patients in the United States alone and generate an annual cost\nexceeding \\$25 billion. Despite this burden, chronic wound care remains a\nroutine yet manual process performed exclusively by trained clinicians due to\nits critical safety demands. We envision a future in which robotics and\nautomation support wound care to lower costs and enhance patient outcomes. This\npaper introduces an autonomous framework for one of the most fundamental yet\nchallenging subtasks in wound redressing: adhesive tape manipulation.\nSpecifically, we address two critical capabilities: tape initial detachment\n(TID) and secure tape placement. To handle the complex adhesive dynamics of\ndetachment, we propose a force-feedback imitation learning approach trained\nfrom human teleoperation demonstrations. For tape placement, we develop a\nnumerical trajectory optimization method based to ensure smooth adhesion and\nwrinkle-free application across diverse anatomical surfaces. We validate these\nmethods through extensive experiments, demonstrating reliable performance in\nboth quantitative evaluations and integrated wound redressing pipelines. Our\nresults establish tape manipulation as an essential step toward practical\nrobotic wound care automation."}
{"id": "2510.06146", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06146", "abs": "https://arxiv.org/abs/2510.06146", "authors": ["Jaehwan Jeong", "Tuan-Anh Vu", "Radha Lahoti", "Jiawen Wang", "Vivek Alumootil", "Sangpil Kim", "M. Khalid Jawed"], "title": "Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments", "comment": null, "summary": "Robotic pollination offers a promising alternative to manual labor and\nbumblebee-assisted methods in controlled agriculture, where wind-driven\npollination is absent and regulatory restrictions limit the use of commercial\npollinators. In this work, we present and validate a vision-guided robotic\nframework that uses data from an end-effector mounted RGB-D sensor and combines\n3D plant reconstruction, targeted grasp planning, and physics-based vibration\nmodeling to enable precise pollination. First, the plant is reconstructed in 3D\nand registered to the robot coordinate frame to identify obstacle-free grasp\nposes along the main stem. Second, a discrete elastic rod model predicts the\nrelationship between actuation parameters and flower dynamics, guiding the\nselection of optimal pollination strategies. Finally, a manipulator with soft\ngrippers grasps the stem and applies controlled vibrations to induce pollen\nrelease. End-to-end experiments demonstrate a 92.5\\% main-stem grasping success\nrate, and simulation-guided optimization of vibration parameters further\nvalidates the feasibility of our approach, ensuring that the robot can safely\nand effectively perform pollination without damaging the flower. To our\nknowledge, this is the first robotic system to jointly integrate vision-based\ngrasping and vibration modeling for automated precision pollination."}
{"id": "2510.06160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06160", "abs": "https://arxiv.org/abs/2510.06160", "authors": ["Blake Romrell", "Abigail Austin", "Braden Meyers", "Ryan Anderson", "Carter Noh", "Joshua G. Mangelson"], "title": "A Preview of HoloOcean 2.0", "comment": "5 pages, 9 figures, submitted to the ICRA 2025 aq2uasim workshop", "summary": "Marine robotics simulators play a fundamental role in the development of\nmarine robotic systems. With increased focus on the marine robotics field in\nrecent years, there has been significant interest in developing higher\nfidelitysimulation of marine sensors, physics, and visual rendering\ncapabilities to support autonomous marine robot development and validation.\nHoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art\nfeatures under a general marine simulator capable of supporting a variety of\ntasks. New features in HoloOcean 2.0 include migration to Unreal Engine (UE)\n5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2\nusing a custom bridge. Additional features are currently in development,\nincluding significantly more efficient ray tracing-based sidescan,\nforward-looking, and bathymetric sonar implementations; semantic sensors;\nenvironment generation tools; volumetric environmental effects; and realistic\nwaves."}
{"id": "2510.06199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06199", "abs": "https://arxiv.org/abs/2510.06199", "authors": ["Chengyang Zhao", "Uksang Yoo", "Arkadeep Narayan Chaudhury", "Giljoo Nam", "Jonathan Francis", "Jeffrey Ichnowski", "Jean Oh"], "title": "DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation", "comment": "Project page: https://chengyzhao.github.io/DYMOHair-web/", "summary": "Hair care is an essential daily activity, yet it remains inaccessible to\nindividuals with limited mobility and challenging for autonomous robot systems\ndue to the fine-grained physical structure and complex dynamics of hair. In\nthis work, we present DYMO-Hair, a model-based robot hair care system. We\nintroduce a novel dynamics learning paradigm that is suited for volumetric\nquantities such as hair, relying on an action-conditioned latent state editing\nmechanism, coupled with a compact 3D latent space of diverse hairstyles to\nimprove generalizability. This latent space is pre-trained at scale using a\nnovel hair physics simulator, enabling generalization across previously unseen\nhairstyles. Using the dynamics model with a Model Predictive Path Integral\n(MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair\nstyling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model\noutperforms baselines on capturing local deformation for diverse, unseen\nhairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling\ntasks on unseen hairstyles, with an average of 22% lower final geometric error\nand 42% higher success rate than the state-of-the-art system. Real-world\nexperiments exhibit zero-shot transferability of our system to wigs, achieving\nconsistent success on challenging unseen hairstyles where the state-of-the-art\nsystem fails. Together, these results introduce a foundation for model-based\nrobot hair care, advancing toward more generalizable, flexible, and accessible\nrobot hair styling in unconstrained physical environments. More details are\navailable on our project page: https://chengyzhao.github.io/DYMOHair-web/."}
{"id": "2510.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06207", "abs": "https://arxiv.org/abs/2510.06207", "authors": ["Zefu Lin", "Rongxu Cui", "Chen Hanning", "Xiangyu Wang", "Junjia Xu", "Xiaojuan Jin", "Chen Wenbo", "Hui Zhou", "Lue Fan", "Wenling Li", "Zhaoxiang Zhang"], "title": "EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model", "comment": "Demo Page: https://anonymous.4open.science/w/Embodied-Coder/", "summary": "Recent advances in control robot methods, from end-to-end\nvision-language-action frameworks to modular systems with predefined\nprimitives, have advanced robots' ability to follow natural language\ninstructions. Nonetheless, many approaches still struggle to scale to diverse\nenvironments, as they often rely on large annotated datasets and offer limited\ninterpretability.In this work, we introduce EmbodiedCoder, a training-free\nframework for open-world mobile robot manipulation that leverages coding models\nto directly generate executable robot trajectories. By grounding high-level\ninstructions in code, EmbodiedCoder enables flexible object geometry\nparameterization and manipulation trajectory synthesis without additional data\ncollection or fine-tuning.This coding-based paradigm provides a transparent and\ngeneralizable way to connect perception with manipulation. Experiments on real\nmobile robots show that EmbodiedCoder achieves robust performance across\ndiverse long-term tasks and generalizes effectively to novel objects and\nenvironments.Our results demonstrate an interpretable approach for bridging\nhigh-level reasoning and low-level control, moving beyond fixed primitives\ntoward versatile robot intelligence. See the project page at:\nhttps://anonymous.4open.science/w/Embodied-Coder/"}
