<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 74]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [RaFD: Flow-Guided Radar Detection for Robust Autonomous Driving](https://arxiv.org/abs/2509.16261)
*Shuocheng Yang,Zikun Xu,Jiahao Wang,Shahid Nawaz,Jianqiang Wang,Shaobing Xu*

Main category: cs.RO

TL;DR: RaFD是一个基于雷达的目标检测框架，通过估计帧间鸟瞰图流并利用几何线索来提高检测精度，在RADIATE数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 原始雷达图像经常受到噪声和"鬼影"伪影的影响，仅基于语义特征的目标检测非常具有挑战性，需要引入几何信息来有效解释语义上固有的模糊雷达信号。

Method: 设计了一个监督流估计辅助任务，与检测网络联合训练。估计的流用于指导从前一帧到当前帧的特征传播，利用几何线索增强检测准确性。

Result: 在RADIATE数据集上实现了最先进的性能，证明了将几何信息纳入雷达信号解释的重要性。

Conclusion: 通过结合几何信息的流引导雷达检测器能够有效解决雷达信号语义模糊的问题，显著提升目标检测性能。

Abstract: Radar has shown strong potential for robust perception in autonomous driving;
however, raw radar images are frequently degraded by noise and "ghost"
artifacts, making object detection based solely on semantic features highly
challenging. To address this limitation, we introduce RaFD, a radar-based
object detection framework that estimates inter-frame bird's-eye-view (BEV)
flow and leverages the resulting geometric cues to enhance detection accuracy.
Specifically, we design a supervised flow estimation auxiliary task that is
jointly trained with the detection network. The estimated flow is further
utilized to guide feature propagation from the previous frame to the current
one. Our flow-guided, radar-only detector achieves achieves state-of-the-art
performance on the RADIATE dataset, underscoring the importance of
incorporating geometric information to effectively interpret radar signals,
which are inherently ambiguous in semantics.

</details>


### [2] [Tactile-Based Human Intent Recognition for Robot Assistive Navigation](https://arxiv.org/abs/2509.16353)
*Shaoting Peng,Dakarai Crowder,Wenzhen Yuan,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 本文介绍了Tac-Nav机器人辅助导航系统，使用圆柱形触觉皮肤和CK-SVM算法实现更自然的人机交互界面。


<details>
  <summary>Details</summary>
Motivation: 现有机器人辅助导航系统缺乏人类看护者与用户之间直观的物理沟通方式，限制了系统有效性。

Method: 开发了Tac-Nav系统，采用圆柱形触觉皮肤和专门设计的圆柱核支持向量机(CK-SVM)算法来识别用户导航意图。

Result: CK-SVM在模拟数据集上达到97.1%的分类准确率，在真实数据集上达到90.8%，优于四个基线模型。用户研究显示Tac-Nav比传统操纵杆和语音控制更受欢迎。

Conclusion: Tac-Nav系统通过触觉界面和几何感知算法显著提升了机器人辅助导航的自然性和效率。

Abstract: Robot assistive navigation (RAN) is critical for enhancing the mobility and
independence of the growing population of mobility-impaired individuals.
However, existing systems often rely on interfaces that fail to replicate the
intuitive and efficient physical communication observed between a person and a
human caregiver, limiting their effectiveness. In this paper, we introduce
Tac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a
Stretch 3 mobile manipulator to provide a more natural and efficient interface
for human navigational intent recognition. To robustly classify the tactile
data, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an
algorithm that explicitly models the sensor's cylindrical geometry and is
consequently robust to the natural rotational shifts present in a user's grasp.
Comprehensive experiments were conducted to demonstrate the effectiveness of
our classification algorithm and the overall system. Results show that CK-SVM
achieved superior classification accuracy on both simulated (97.1%) and
real-world (90.8%) datasets compared to four baseline models. Furthermore, a
pilot study confirmed that users more preferred the Tac-Nav tactile interface
over conventional joystick and voice-based controls.

</details>


### [3] [Dynamic Objects Relocalization in Changing Environments with Flow Matching](https://arxiv.org/abs/2509.16398)
*Francesco Argenziano,Miguel Saavedra-Ruiz,Sacha Morin,Daniele Nardi,Liam Paull*

Main category: cs.RO

TL;DR: FlowMaps是一个基于流匹配的模型，用于在动态环境中推断多模态物体位置，解决机器人任务和运动规划中因物体移动导致的重新定位问题。


<details>
  <summary>Details</summary>
Motivation: 在动态环境（如家庭或仓库）中，人类活动导致物体位置频繁变化，增加了机器人任务失败的风险。现有方法往往忽视人类-物体交互的规律性模式。

Method: 提出FlowMaps模型，利用流匹配技术来推断物体在时空上的多模态位置分布，能够捕捉人类活动的重复性模式。

Result: 实验结果提供了统计证据支持该方法的有效性，表明可以利用人类交互模式来预测物体位置。

Conclusion: 该方法为解决动态环境中的物体重新定位问题提供了新思路，为更复杂的应用奠定了基础。代码已开源。

Abstract: Task and motion planning are long-standing challenges in robotics, especially
when robots have to deal with dynamic environments exhibiting long-term
dynamics, such as households or warehouses. In these environments, long-term
dynamics mostly stem from human activities, since previously detected objects
can be moved or removed from the scene. This adds the necessity to find such
objects again before completing the designed task, increasing the risk of
failure due to missed relocalizations. However, in these settings, the nature
of such human-object interactions is often overlooked, despite being governed
by common habits and repetitive patterns. Our conjecture is that these cues can
be exploited to recover the most likely objects' positions in the scene,
helping to address the problem of unknown relocalization in changing
environments. To this end we propose FlowMaps, a model based on Flow Matching
that is able to infer multimodal object locations over space and time. Our
results present statistical evidence to support our hypotheses, opening the way
to more complex applications of our approach. The code is publically available
at https://github.com/Fra-Tsuna/flowmaps

</details>


### [4] [Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation](https://arxiv.org/abs/2509.16412)
*Zihao Deng,Peng Gao,Williard Joshua Jose,Maggie Wigness,John Rogers,Brian Reily,Christopher Reardon,Hao Zhang*

Main category: cs.RO

TL;DR: 提出了一种用于多机器人协调导航的子团队自适应编队方法（STAF），能够在复杂场景中动态分割团队并自适应控制编队


<details>
  <summary>Details</summary>
Motivation: 多机器人导航需要保持特定编队，但在狭窄走廊等复杂场景中，刚性保持预定义编队变得不可行，需要动态分割团队并自适应控制

Method: 基于统一分层学习框架：高层深度图切割用于团队分割，中层图学习促进子团队间协调导航，底层策略学习控制单个移动机器人到达目标位置并避免碰撞

Result: 在室内外环境中进行的实验表明，STAF实现了子团队和自适应编队控制的新能力，在挑战性场景中取得了有前景的协调多机器人导航性能

Conclusion: STAF方法为多机器人系统在复杂环境中的协调导航提供了有效的解决方案，能够动态适应不同场景需求

Abstract: Coordinated multi-robot navigation is essential for robots to operate as a
team in diverse environments. During navigation, robot teams usually need to
maintain specific formations, such as circular formations to protect human
teammates at the center. However, in complex scenarios such as narrow
corridors, rigidly preserving predefined formations can become infeasible.
Therefore, robot teams must be capable of dynamically splitting into smaller
subteams and adaptively controlling the subteams to navigate through such
scenarios while preserving formations. To enable this capability, we introduce
a novel method for SubTeaming and Adaptive Formation (STAF), which is built
upon a unified hierarchical learning framework: (1) high-level deep graph cut
for team splitting, (2) intermediate-level graph learning for facilitating
coordinated navigation among subteams, and (3) low-level policy learning for
controlling individual mobile robots to reach their goal positions while
avoiding collisions. To evaluate STAF, we conducted extensive experiments in
both indoor and outdoor environments using robotics simulations and physical
robot teams. Experimental results show that STAF enables the novel capability
for subteaming and adaptive formation control, and achieves promising
performance in coordinated multi-robot navigation through challenging
scenarios. More details are available on the project website:
https://hcrlab.gitlab.io/project/STAF.

</details>


### [5] [End-to-end RL Improves Dexterous Grasping Policies](https://arxiv.org/abs/2509.16434)
*Ritvik Singh,Karl Van Wyk,Pieter Abbeel,Jitendra Malik,Nathan Ratliff,Ankur Handa*

Main category: cs.RO

TL;DR: 本文提出了一种新的分布式训练方法，通过将模拟器和RL训练分离到不同的GPU上，解决了基于视觉的端到端强化学习在灵巧抓取任务中的内存效率问题，显著提升了训练效率和现实世界性能。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的端到端RL虽然能够产生主动视觉行为，但由于内存效率低导致批量大小受限，无法充分发挥PPO等算法的性能。现有模拟器的数据并行方法存在瓶颈。

Method: 提出解耦模拟器和RL训练的方法：在四GPU节点上，三个GPU运行模拟器，一个GPU运行PPO训练和体验缓冲区。同时采用深度蒸馏技术将深度和状态策略蒸馏到立体RGB网络中。

Result: 相比标准数据并行基线，相同GPU数量下环境数量翻倍；深度蒸馏比状态蒸馏效果更好；解耦模拟方法带来的批量增加提升了现实世界性能；在真实世界中超越了之前的视觉方法SOTA。

Conclusion: 解耦模拟器与RL训练的方法有效解决了视觉RL的内存瓶颈，深度蒸馏策略优于状态蒸馏，该方法显著提升了端到端视觉RL在灵巧抓取任务中的训练效率和现实性能。

Abstract: This work explores techniques to scale up image-based end-to-end learning for
dexterous grasping with an arm + hand system. Unlike state-based RL,
vision-based RL is much more memory inefficient, resulting in relatively low
batch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is
still an attractive method as unlike the more commonly used techniques which
distill state-based policies into vision networks, end-to-end RL can allow for
emergent active vision behaviors. We identify a key bottleneck in training
these policies is the way most existing simulators scale to multiple GPUs using
traditional data parallelism techniques. We propose a new method where we
disaggregate the simulator and RL (both training and experience buffers) onto
separate GPUs. On a node with four GPUs, we have the simulator running on three
of them, and PPO running on the fourth. We are able to show that with the same
number of GPUs, we can double the number of existing environments compared to
the previous baseline of standard data parallelism. This allows us to train
vision-based environments, end-to-end with depth, which were previously
performing far worse with the baseline. We train and distill both depth and
state-based policies into stereo RGB networks and show that depth distillation
leads to better results, both in simulation and reality. This improvement is
likely due to the observability gap between state and vision policies which
does not exist when distilling depth policies into stereo RGB. We further show
that the increased batch size brought about by disaggregated simulation also
improves real world performance. When deploying in the real world, we improve
upon the previous state-of-the-art vision-based results using our end-to-end
policies.

</details>


### [6] [FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning](https://arxiv.org/abs/2509.16445)
*Naoki Yokoyama,Sehoon Ha*

Main category: cs.RO

TL;DR: FiLM-Nav通过直接微调预训练视觉语言模型作为导航策略，在复杂环境中实现基于自由语言描述的物体导航，在HM3D ObjectNav和OVON基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型（特别是视觉语言模型）具有强大的语义理解能力，但如何有效将其网络规模知识适应到具身决策中仍是一个关键挑战。需要开发能够直接利用VLM进行导航决策的方法。

Method: FiLM-Nav直接微调预训练的VLM作为导航策略，基于原始视觉轨迹历史和导航目标选择最佳探索边界。通过结合ObjectNav、OVON、ImageNav和空间推理任务的多样化数据混合进行微调。

Result: 在HM3D ObjectNav中，FiLM-Nav在开放词汇方法中实现了SPL和成功率的最新最优性能，在具有挑战性的HM3D-OVON基准测试中设置了最新的SPL记录，展示了对未见物体类别的强大泛化能力。

Conclusion: 直接在多样化模拟具身数据上微调VLM是实现可泛化和高效语义导航能力的有效途径，验证了该方法在具身人工智能中的潜力。

Abstract: Enabling robotic assistants to navigate complex environments and locate
objects described in free-form language is a critical capability for real-world
deployment. While foundation models, particularly Vision-Language Models
(VLMs), offer powerful semantic understanding, effectively adapting their
web-scale knowledge for embodied decision-making remains a key challenge. We
present FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that
directly fine-tunes pre-trained VLM as the navigation policy. In contrast to
methods that use foundation models primarily in a zero-shot manner or for map
annotation, FiLM-Nav learns to select the next best exploration frontier by
conditioning directly on raw visual trajectory history and the navigation goal.
Leveraging targeted simulated embodied experience allows the VLM to ground its
powerful pre-trained representations in the specific dynamics and visual
patterns relevant to goal-driven navigation. Critically, fine-tuning on a
diverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary
spatial reasoning task proves essential for achieving robustness and broad
generalization. FiLM-Nav sets a new state-of-the-art in both SPL and success
rate on HM3D ObjectNav among open-vocabulary methods, and sets a
state-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating
strong generalization to unseen object categories. Our work validates that
directly fine-tuning VLMs on diverse simulated embodied data is a highly
effective pathway towards generalizable and efficient semantic navigation
capabilities.

</details>


### [7] [A Framework for Optimal Ankle Design of Humanoid Robots](https://arxiv.org/abs/2509.16469)
*Guglielmo Cervettini,Roberto Mauceri,Alex Coppola,Fabio Bergonti,Luca Fiorio,Marco Maggiali,Daniele Pucci*

Main category: cs.RO

TL;DR: 提出了一种用于设计和评估并联踝关节机构的统一方法，通过多目标优化合成机构几何形状，并使用标量成本函数评估关键性能指标，比较了SPU和RSU两种架构。


<details>
  <summary>Details</summary>
Motivation: 人形机器人踝关节设计对安全高效的地面交互至关重要，机械柔顺性和电机质量分布等关键因素推动了并联机构架构的采用，但最优配置选择取决于执行器可用性和任务要求。

Method: 采用多目标优化方法合成机构几何形状，使用标量成本函数聚合关键性能指标进行跨架构比较，重点研究了SPU和RSU两种代表性架构，解决了运动学问题并为RSU引入了确保工作空间可行性的参数化方法。

Result: 通过重新设计现有人形机器人的踝关节验证了该方法，优化的RSU在成本函数上分别比原始串行设计和传统设计的RSU降低了41%和14%。

Conclusion: 所提出的方法能够有效优化并联踝关节机构设计，RSU架构在性能上表现出明显优势，为机器人踝关节设计提供了系统化的解决方案。

Abstract: The design of the humanoid ankle is critical for safe and efficient ground
interaction. Key factors such as mechanical compliance and motor mass
distribution have driven the adoption of parallel mechanism architectures.
However, selecting the optimal configuration depends on both actuator
availability and task requirements. We propose a unified methodology for the
design and evaluation of parallel ankle mechanisms. A multi-objective
optimization synthesizes the mechanism geometry, the resulting solutions are
evaluated using a scalar cost function that aggregates key performance metrics
for cross-architecture comparison. We focus on two representative
architectures: the Spherical-Prismatic-Universal (SPU) and the
Revolute-Spherical-Universal (RSU). For both, we resolve the kinematics, and
for the RSU, introduce a parameterization that ensures workspace feasibility
and accelerates optimization. We validate our approach by redesigning the ankle
of an existing humanoid robot. The optimized RSU consistently outperforms both
the original serial design and a conventionally engineered RSU, reducing the
cost function by up to 41% and 14%, respectively.

</details>


### [8] [Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems](https://arxiv.org/abs/2509.16482)
*Pranav Tiwari,Soumyodipta Nath*

Main category: cs.RO

TL;DR: 本文提出Robot Conga算法，一种基于空间位移而非时间参数的领导者-跟随者控制策略，用于解决多智能体系统中的顺序路径跟随问题，实现智能体在共同轨迹上保持固定空间间隔。


<details>
  <summary>Details</summary>
Motivation: 传统编队控制技术依赖时间参数化轨迹和路径积分，容易导致同步问题和刚性行为。本文旨在解决顺序路径跟随问题，使智能体在中央控制下沿共同轨迹保持固定空间分离。

Method: 采用领导者-跟随者控制策略，基于领导者的空间位移而非时间更新每个智能体的期望状态，假设存在全局位置参考（适用于室内环境中的运动捕捉、视觉跟踪或UWB定位系统）。

Result: 在TurtleBot3和四足机器人（Laikago）仿真中验证，结果显示准确的轨迹跟踪、稳定的智能体间距和快速收敛，四足机器人情况下所有智能体在250个时间步（约0.25秒）内对齐，TurtleBot3实现几乎瞬时对齐。

Conclusion: Robot Conga算法有效解决了多智能体顺序路径跟随问题，通过空间位移控制实现了更好的同步性和灵活性，适用于自动化物流、监控和协作探索等应用场景。

Abstract: Coordinated path following in multi-agent systems is a key challenge in
robotics, with applications in automated logistics, surveillance, and
collaborative exploration. Traditional formation control techniques often rely
on time-parameterized trajectories and path integrals, which can result in
synchronization issues and rigid behavior. In this work, we address the problem
of sequential path following, where agents maintain fixed spatial separation
along a common trajectory, guided by a leader under centralized control. We
introduce Robot Conga, a leader-follower control strategy that updates each
agent's desired state based on the leader's spatial displacement rather than
time, assuming access to a global position reference, an assumption valid in
indoor environments equipped with motion capture, vision-based tracking, or UWB
localization systems. The algorithm was validated in simulation using both
TurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate
trajectory tracking, stable inter-agent spacing, and fast convergence, with all
agents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped
case, and almost instantaneously in the TurtleBot3 implementation.

</details>


### [9] [Substrate-Timing-Independence for Meta-State Stability of Distributed Robotic Swarms](https://arxiv.org/abs/2509.16492)
*Tinapat Limsila,Mehul Sharma,Paulo Garcia*

Main category: cs.RO

TL;DR: 本文提出了一种基于并发进程演算的形式化方法，用于分析机器人群体系统中的错误元状态问题，并通过自动识别和修正设计来确保系统在时序变化下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的涌现特性由于时序不可预测性可能导致系统进入错误元状态，而机器人群体系统由于实现基质的变异性使得经验验证变得极其困难。

Method: 利用通信顺序进程（CSP）等并发进程演算，开发了一种基质时序无关的形式化推理方法，能够自动识别错误元状态的原因并修正设计。

Result: 在具有明确故障的机器人群体系统上进行评估，结果显示修正前系统会到达非法元状态，而修正后系统行为始终保持正确。

Conclusion: 该方法可跨不同设计方法学转移，为机器人学家提供了形式化方法的工具箱，有助于确保机器人群体系统的正确性。

Abstract: Emergent properties in distributed systems arise due to timing
unpredictability; asynchronous state evolution within each sub-system may lead
the macro-system to faulty meta-states. Empirical validation of correctness is
often prohibitively expensive, as the size of the state-space is too large to
be tractable. In robotic swarms this problem is exacerbated, when compared to
software systems, by the variability of the implementation substrate across the
design, or even the deployment, process. We present an approach for formally
reasoning about the correctness of robotic swarm design in a
substrate-timing-independent way. By leveraging concurrent process calculi
(namely, Communicating Sequential Processes), we introduce a methodology that
can automatically identify possible causes of faulty meta-states and correct
such designs such that meta-states are consistently stable, even in the
presence of timing variability due to substrate changes. We evaluate this
approach on a robotic swarm with a clearly identified fault, realized in both
simulation and reality. Results support the research hypothesis, showing that
the swarm reaches an illegal meta-state before the correction is applied, but
behaves consistently correctly after the correction. Our techniques are
transferable across different design methodologies, contributing to the toolbox
of formal methods for roboticists.

</details>


### [10] [No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning](https://arxiv.org/abs/2509.16532)
*Run Yu,Yangdi Liu,Wen-Da Wei,Chen Li*

Main category: cs.RO

TL;DR: 提出NoReal3D框架，通过3DStructureFormer将单目图像转换为几何有意义的伪点云特征，避免实际3D点云采集的高成本，同时达到与3D点云方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云方法数据采集成本高的问题，同时保持3D信息的优势。2D图像方法性能较差，而3D点云方法虽然性能好但成本高昂，限制了实际部署。

Method: 使用3DStructureFormer模块将单目图像转换为保留几何和拓扑结构的伪点云特征，与2D编码器输出特征融合，并设计伪点云编码器来保持这些属性。

Result: 在多个任务上的广泛实验验证了该框架可以达到与3D点云方法相当的性能，且无需实际点云数据。

Conclusion: NoReal3D框架在消除3D点云采集成本的同时，有效提升了机器人对3D空间结构的理解能力，为视觉机器人操作提供了更实用的解决方案。

Abstract: Recently,vision-based robotic manipulation has garnered significant attention
and witnessed substantial advancements. 2D image-based and 3D point cloud-based
policy learning represent two predominant paradigms in the field, with recent
studies showing that the latter consistently outperforms the former in terms of
both policy performance and generalization, thereby underscoring the value and
significance of 3D information. However, 3D point cloud-based approaches face
the significant challenge of high data acquisition costs, limiting their
scalability and real-world deployment. To address this issue, we propose a
novel framework NoReal3D: which introduces the 3DStructureFormer, a learnable
3D perception module capable of transforming monocular images into
geometrically meaningful pseudo-point cloud features, effectively fused with
the 2D encoder output features. Specially, the generated pseudo-point clouds
retain geometric and topological structures so we design a pseudo-point cloud
encoder to preserve these properties, making it well-suited for our framework.
We also investigate the effectiveness of different feature fusion
strategies.Our framework enhances the robot's understanding of 3D spatial
structures while completely eliminating the substantial costs associated with
3D point cloud acquisition.Extensive experiments across various tasks validate
that our framework can achieve performance comparable to 3D point cloud-based
methods, without the actual point cloud data.

</details>


### [11] [TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation](https://arxiv.org/abs/2509.16550)
*Yinghao Wu,Shuhong Hou,Haowen Zheng,Yichen Li,Weiyi Lu,Xun Zhou,Yitian Shao*

Main category: cs.RO

TL;DR: TranTac是一个低成本、数据高效的多模态触觉传感与控制框架，通过集成6轴IMU传感器在机器人夹爪尖端，结合Transformer编码器和扩散策略，实现了基于瞬态触觉线索的精细插入任务控制。


<details>
  <summary>Details</summary>
Motivation: 在视觉感知不足以检测微小错位的情况下（如钥匙插入锁孔、USB插入端口），触觉传感对于机器人监控任务状态和进行精确调整至关重要。现有触觉传感方案要么对细微变化不敏感，要么需要过多传感器数据。

Method: 在机器人夹爪的弹性尖端内集成单个接触敏感的6轴IMU传感器，检测微米级的动态平移和扭转变形；使用基于Transformer的编码器和扩散策略来模仿人类插入行为；结合视觉和触觉信息进行6-DoF姿态控制。

Result: 结合视觉时平均成功率79%，优于纯视觉策略和末端执行器6D力/力矩传感增强策略；仅触觉的错位插入任务平均成功率88%；在未见过的USB插头和金属钥匙上测试，平均成功率接近70%。

Conclusion: TranTac框架为精细操作任务提供了新的机器人触觉传感系统思路，展示了数据高效、低成本的多模态传感与控制的有效性。

Abstract: Robotic manipulation tasks such as inserting a key into a lock or plugging a
USB device into a port can fail when visual perception is insufficient to
detect misalignment. In these situations, touch sensing is crucial for the
robot to monitor the task's states and make precise, timely adjustments.
Current touch sensing solutions are either insensitive to detect subtle changes
or demand excessive sensor data. Here, we introduce TranTac, a data-efficient
and low-cost tactile sensing and control framework that integrates a single
contact-sensitive 6-axis inertial measurement unit within the elastomeric tips
of a robotic gripper for completing fine insertion tasks. Our customized
sensing system can detect dynamic translational and torsional deformations at
the micrometer scale, enabling the tracking of visually imperceptible pose
changes of the grasped object. By leveraging transformer-based encoders and
diffusion policy, TranTac can imitate human insertion behaviors using transient
tactile cues detected at the gripper's tip during insertion processes. These
cues enable the robot to dynamically control and correct the 6-DoF pose of the
grasped object. When combined with vision, TranTac achieves an average success
rate of 79% on object grasping and insertion tasks, outperforming both
vision-only policy and the one augmented with end-effector 6D force/torque
sensing. Contact localization performance is also validated through
tactile-only misaligned insertion tasks, achieving an average success rate of
88%. We assess the generalizability by training TranTac on a single prism-slot
pair and testing it on unseen data, including a USB plug and a metal key, and
find that the insertion tasks can still be completed with an average success
rate of nearly 70%. The proposed framework may inspire new robotic tactile
sensing systems for delicate manipulation tasks.

</details>


### [12] [Video-to-BT: Generating Reactive Behavior Trees from Human Demonstration Videos for Robotic Assembly](https://arxiv.org/abs/2509.16611)
*Xiwei Zhao,Yiwei Wang,Yansong Wu,Fan Wu,Teng Sun,Zhonghua Miao,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: 提出Video-to-BT框架，利用视觉语言模型将人类演示视频分解为子任务并生成行为树，实现机器人装配任务的高层认知规划与低层反应控制的集成


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配系统依赖专家编程，缺乏对产品变化的灵活性和处理变异的鲁棒性，需要更灵活可靠的方法

Method: 使用Vision-Language Model分解演示视频生成行为树，结合实时场景解释实现反应式控制，失败时触发VLM驱动的重新规划

Result: 在真实装配任务实验中表现出高规划可靠性、长时程任务的鲁棒性能，以及在多样化和扰动条件下的强泛化能力

Conclusion: 提出的闭环架构确保了系统的稳定性和自适应性，为柔性制造提供了有效解决方案

Abstract: Modern manufacturing demands robotic assembly systems with enhanced
flexibility and reliability. However, traditional approaches often rely on
programming tailored to each product by experts for fixed settings, which are
inherently inflexible to product changes and lack the robustness to handle
variations. As Behavior Trees (BTs) are increasingly used in robotics for their
modularity and reactivity, we propose a novel hierarchical framework,
Video-to-BT, that seamlessly integrates high-level cognitive planning with
low-level reactive control, with BTs serving both as the structured output of
planning and as the governing structure for execution. Our approach leverages a
Vision-Language Model (VLM) to decompose human demonstration videos into
subtasks, from which Behavior Trees are generated. During the execution, the
planned BTs combined with real-time scene interpretation enable the system to
operate reactively in the dynamic environment, while VLM-driven replanning is
triggered upon execution failure. This closed-loop architecture ensures
stability and adaptivity. We validate our framework on real-world assembly
tasks through a series of experiments, demonstrating high planning reliability,
robust performance in long-horizon assembly tasks, and strong generalization
across diverse and perturbed conditions. Project website:
https://video2bt.github.io/video2bt_page/

</details>


### [13] [ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks](https://arxiv.org/abs/2509.16614)
*Bojan Derajić,Sebastian Bernhard,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 提出基于Hamilton-Jacobi可达性分析的观测条件神经控制屏障函数方法，用于安全关键控制系统设计，能够近似恢复最大安全集并确保安全保证。


<details>
  <summary>Details</summary>
Motivation: 传统控制屏障函数虽然部署简单但设计困难，现有学习方法存在安全集次优、部分可观测环境适用性差、缺乏严格安全保证等问题。

Method: 利用HJ值函数的数学特性，确保预测安全集不与观测故障集相交，采用超网络架构设计观测条件安全滤波器。

Result: 在仿真和硬件实验中验证了地面机器人和四旋翼飞行器的性能，相比基线方法提高了成功率和对域外环境的泛化能力。

Conclusion: 该方法有效解决了现有学习方法的局限性，为安全关键控制系统提供了更可靠的安全保证。

Abstract: Control barrier functions (CBFs) have been demonstrated as an effective
method for safety-critical control of autonomous systems. Although CBFs are
simple to deploy, their design remains challenging, motivating the development
of learning-based approaches. Yet, issues such as suboptimal safe sets,
applicability in partially observable environments, and lack of rigorous safety
guarantees persist. In this work, we propose observation-conditioned neural
CBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately
recover the maximal safe sets. We exploit certain mathematical properties of
the HJ value function, ensuring that the predicted safe set never intersects
with the observed failure set. Moreover, we leverage a hypernetwork-based
architecture that is particularly suitable for the design of
observation-conditioned safety filters. The proposed method is examined both in
simulation and hardware experiments for a ground robot and a quadcopter. The
results show improved success rates and generalization to out-of-domain
environments compared to the baselines.

</details>


### [14] [LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning](https://arxiv.org/abs/2509.16615)
*Jelle Luijkx,Runyu Ma,Zlatan Ajanović,Jens Kober*

Main category: cs.RO

TL;DR: LLM-TALE是一个利用大语言模型规划能力直接指导强化学习探索的框架，通过任务级和可供性级规划提高学习效率，在机器人操作任务中表现出更好的样本效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人操作中样本效率低且需要大量探索，现有方法利用LLM的常识知识指导探索但会产生物理不可行的计划，导致不可靠行为。

Method: LLM-TALE框架集成任务级和可供性级规划，在线纠正次优性，无需人工监督即可探索多模态可供性级计划。

Result: 在标准RL基准测试中，pick-and-place任务的样本效率和成功率均有提升，真实机器人实验显示出有前景的零样本仿真到现实迁移能力。

Conclusion: LLM-TALE通过LLM规划直接指导RL探索，有效解决了物理不可行计划问题，提高了机器人操作任务的性能。

Abstract: Reinforcement learning (RL) is a promising approach for robotic manipulation,
but it can suffer from low sample efficiency and requires extensive exploration
of large state-action spaces. Recent methods leverage the commonsense knowledge
and reasoning abilities of large language models (LLMs) to guide exploration
toward more meaningful states. However, LLMs can produce plans that are
semantically plausible yet physically infeasible, yielding unreliable behavior.
We introduce LLM-TALE, a framework that uses LLMs' planning to directly steer
RL exploration. LLM-TALE integrates planning at both the task level and the
affordance level, improving learning efficiency by directing agents toward
semantically meaningful actions. Unlike prior approaches that assume optimal
LLM-generated plans or rewards, LLM-TALE corrects suboptimality online and
explores multimodal affordance-level plans without human supervision. We
evaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing
improvements in both sample efficiency and success rates over strong baselines.
Real-robot experiments indicate promising zero-shot sim-to-real transfer. Code
and supplementary material are available at https://llm-tale.github.io.

</details>


### [15] [KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control](https://arxiv.org/abs/2509.16638)
*Jinrui Han,Weiji Xie,Jiakun Zheng,Jiyuan Shi,Weinan Zhang,Ting Xiao,Chenjia Bai*

Main category: cs.RO

TL;DR: VMS是一个统一的人形机器人全身控制器，能够通过单一策略学习多样化的动态行为，结合混合跟踪目标和正交专家混合架构，实现稳定且通用的运动技能模仿。


<details>
  <summary>Details</summary>
Motivation: 学习跟踪各种人类运动的全身技能是迈向通用人形机器人的关键步骤，但现有方法难以在单一策略中同时掌握广泛运动技能并确保长期稳定性。

Method: 提出VMS框架，包含：1）平衡局部运动保真度和全局轨迹一致性的混合跟踪目标；2）鼓励技能专业化同时增强运动泛化能力的正交专家混合架构；3）放松严格逐步匹配的分段级跟踪奖励机制。

Result: 在仿真和真实实验中验证了VMS的有效性，展示了动态技能的准确模仿、分钟级序列的稳定性能以及对未见运动的强泛化能力。

Conclusion: VMS作为可扩展的基础框架，为人形机器人通用全身控制提供了有前景的解决方案。

Abstract: Learning versatile whole-body skills by tracking various human motions is a
fundamental step toward general-purpose humanoid robots. This task is
particularly challenging because a single policy must master a broad repertoire
of motion skills while ensuring stability over long-horizon sequences. To this
end, we present VMS, a unified whole-body controller that enables humanoid
robots to learn diverse and dynamic behaviors within a single policy. Our
framework integrates a hybrid tracking objective that balances local motion
fidelity with global trajectory consistency, and an Orthogonal
Mixture-of-Experts (OMoE) architecture that encourages skill specialization
while enhancing generalization across motions. A segment-level tracking reward
is further introduced to relax rigid step-wise matching, enhancing robustness
when handling global displacements and transient inaccuracies. We validate VMS
extensively in both simulation and real-world experiments, demonstrating
accurate imitation of dynamic skills, stable performance over minute-long
sequences, and strong generalization to unseen motions. These results highlight
the potential of VMS as a scalable foundation for versatile humanoid whole-body
control. The project page is available at
https://kungfubot2-humanoid.github.io.

</details>


### [16] [HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos](https://arxiv.org/abs/2509.16757)
*Haoyang Weng,Yitang Li,Nikhil Sobanbabu,Zihan Wang,Zhengyi Luo,Tairan He,Deva Ramanan,Guanya Shi*

Main category: cs.RO

TL;DR: HDMI是一个从单目RGB视频中学习全身人形机器人-物体交互技能的简单通用框架，通过提取人类运动轨迹、训练强化学习策略，并在真实人形机器人上实现零样本部署


<details>
  <summary>Details</summary>
Motivation: 解决全身人形机器人-物体交互的挑战，包括运动数据稀缺和接触密集性问题，旨在直接从人类视频中学习交互技能

Method: 三步流程：1）从无约束视频中提取并重定向人类和物体轨迹构建结构化数据集；2）训练强化学习策略，采用统一物体表示、残差动作空间和通用交互奖励；3）在真实人形机器人上零样本部署策略

Result: 在Unitree G1人形机器人上的仿真到真实实验显示：实现67次连续门穿越，在真实世界中成功执行6个不同的移动操作任务，在仿真中执行14个任务

Conclusion: HDMI是一个从人类视频中获取交互式人形机器人技能的简单通用框架，具有鲁棒性和通用性

Abstract: Enabling robust whole-body humanoid-object interaction (HOI) remains
challenging due to motion data scarcity and the contact-rich nature. We present
HDMI (HumanoiD iMitation for Interaction), a simple and general framework that
learns whole-body humanoid-object interaction skills directly from monocular
RGB videos. Our pipeline (i) extracts and retargets human and object
trajectories from unconstrained videos to build structured motion datasets,
(ii) trains a reinforcement learning (RL) policy to co-track robot and object
states with three key designs: a unified object representation, a residual
action space, and a general interaction reward, and (iii) zero-shot deploys the
RL policies on real humanoid robots. Extensive sim-to-real experiments on a
Unitree G1 humanoid demonstrate the robustness and generality of our approach:
HDMI achieves 67 consecutive door traversals and successfully performs 6
distinct loco-manipulation tasks in the real world and 14 tasks in simulation.
Our results establish HDMI as a simple and general framework for acquiring
interactive humanoid skills from human videos.

</details>


### [17] [Improve bounding box in Carla Simulator](https://arxiv.org/abs/2509.16773)
*Mohamad Mofeed Chaar,Jamal Raiyn,Galia Weidl*

Main category: cs.RO

TL;DR: 本文介绍了CARLA模拟器中用于自动驾驶数据生成的边界框方法，并提出了改进方案以过滤误检框，提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: CARLA模拟器中原有的边界框生成方法存在误检问题，如检测到被遮挡物体的虚假边界框（ghost boxes），这影响了自动驾驶数据集的准确性。

Method: 在原有方法基础上，通过改进算法来过滤掉不想要的边界框，具体包括优化对象检测和边界框标注流程，减少因遮挡导致的误检。

Result: 性能分析表明，改进后的方法实现了高准确率，有效减少了误检边界框的数量。

Conclusion: 改进的边界框生成方法显著提升了CARLA模拟器中数据生成的准确性，为自动驾驶算法的测试和数据集构建提供了更可靠的基础。

Abstract: The CARLA simulator (Car Learning to Act) serves as a robust platform for
testing algorithms and generating datasets in the field of Autonomous Driving
(AD). It provides control over various environmental parameters, enabling
thorough evaluation. Development bounding boxes are commonly utilized tools in
deep learning and play a crucial role in AD applications. The predominant
method for data generation in the CARLA Simulator involves identifying and
delineating objects of interest, such as vehicles, using bounding boxes. The
operation in CARLA entails capturing the coordinates of all objects on the map,
which are subsequently aligned with the sensor's coordinate system at the ego
vehicle and then enclosed within bounding boxes relative to the ego vehicle's
perspective. However, this primary approach encounters challenges associated
with object detection and bounding box annotation, such as ghost boxes.
Although these procedures are generally effective at detecting vehicles and
other objects within their direct line of sight, they may also produce false
positives by identifying objects that are obscured by obstructions. We have
enhanced the primary approach with the objective of filtering out unwanted
boxes. Performance analysis indicates that the improved approach has achieved
high accuracy.

</details>


### [18] [SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree](https://arxiv.org/abs/2509.16812)
*Priyanshu Agrawal,Shalabh Gupta,Zongyuan Shen*

Main category: cs.RO

TL;DR: SMART-3D是SMART算法在3D环境中的扩展，一种基于树的自适应重规划算法，用于处理快速移动障碍物的动态环境。


<details>
  <summary>Details</summary>
Motivation: 将SMART算法扩展到3D环境，解决动态环境中快速移动障碍物的路径规划问题，提高计算效率和可扩展性。

Method: 通过用热节点概念替代热点概念，移除网格分解要求，使算法能够实时变形底层树结构以寻找新路径。热节点允许高效重新连接来变形现有树。

Result: 在2D和3D环境中进行广泛模拟测试，结果显示SMART-3D实现了高成功率和低重规划时间。

Conclusion: SMART-3D适用于实时机载应用，具有高计算效率和可扩展性。

Abstract: This paper presents SMART-3D, an extension of the SMART algorithm to 3D
environments. SMART-3D is a tree-based adaptive replanning algorithm for
dynamic environments with fast moving obstacles. SMART-3D morphs the underlying
tree to find a new path in real-time whenever the current path is blocked by
obstacles. SMART-3D removed the grid decomposition requirement of the SMART
algorithm by replacing the concept of hot-spots with that of hot-nodes, thus
making it computationally efficient and scalable to 3D environments. The
hot-nodes are nodes which allow for efficient reconnections to morph the
existing tree to find a new safe and reliable path. The performance of SMART-3D
is evaluated by extensive simulations in 2D and 3D environments populated with
randomly moving dynamic obstacles. The results show that SMART-3D achieves high
success rates and low replanning times, thus highlighting its suitability for
real-time onboard applications.

</details>


### [19] [Factorizing Diffusion Policies for Observation Modality Prioritization](https://arxiv.org/abs/2509.16830)
*Omkar Patil,Prabin Rath,Kartikay Pangaonkar,Eric Rosen,Nakul Gopalan*

Main category: cs.RO

TL;DR: 提出了Factorized Diffusion Policies (FDP)，一种新的策略公式，通过设计使观测模态在动作扩散过程中具有不同的影响力，从而学习到更优先考虑某些观测模态的策略。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型策略在条件化多个观测模态（如本体感觉、视觉和触觉）时，无法捕捉不同模态对不同任务的不同影响力水平。

Method: FDP通过因子化扩散过程的观测条件化来实现模态优先级，使某些观测模态可以优先于其他模态。

Result: 在低数据情况下，FDP相比标准扩散策略在多个模拟基准上实现了15%的绝对成功率提升；在分布偏移下（如视觉干扰或相机遮挡），FDP的绝对成功率比现有扩散策略高40%。

Conclusion: FDP为真实世界部署提供了比标准扩散策略更安全、更稳健的替代方案。

Abstract: Diffusion models have been extensively leveraged for learning robot skills
from demonstrations. These policies are conditioned on several observational
modalities such as proprioception, vision and tactile. However, observational
modalities have varying levels of influence for different tasks that diffusion
polices fail to capture. In this work, we propose 'Factorized Diffusion
Policies' abbreviated as FDP, a novel policy formulation that enables
observational modalities to have differing influence on the action diffusion
process by design. This results in learning policies where certain observations
modalities can be prioritized over the others such as $\texttt{vision>tactile}$
or $\texttt{proprioception>vision}$. FDP achieves modality prioritization by
factorizing the observational conditioning for diffusion process, resulting in
more performant and robust policies. Our factored approach shows strong
performance improvements in low-data regimes with $15\%$ absolute improvement
in success rate on several simulated benchmarks when compared to a standard
diffusion policy that jointly conditions on all input modalities. Moreover, our
benchmark and real-world experiments show that factored policies are naturally
more robust with $40\%$ higher absolute success rate across several visuomotor
tasks under distribution shifts such as visual distractors or camera
occlusions, where existing diffusion policies fail catastrophically. FDP thus
offers a safer and more robust alternative to standard diffusion policies for
real-world deployment. Videos are available at
https://fdp-policy.github.io/fdp-policy/ .

</details>


### [20] [Robot Learning with Sparsity and Scarcity](https://arxiv.org/abs/2509.16834)
*Jingxi Xu*

Main category: cs.RO

TL;DR: 该论文讨论了机器人学习中的数据稀缺问题，重点分析了触觉感知（数据稀疏性）和康复机器人（数据稀缺性）两个领域，提出了相应的机器学习解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器人学习面临的主要挑战是缺乏大规模数据资源，具体表现为数据表示上的稀疏性和数据数量上的稀缺性。触觉感知数据稀疏，康复机器人数据极度稀缺，这限制了机器人学习的发展。

Method: 对于触觉感知，采用无模型强化学习来学习仅依赖触觉的探索和操作策略；对于康复机器人，开发了半监督学习、元学习和生成式AI等机器学习算法，在少量数据下实现意图推断。

Result: 开发了能够高效利用稀疏触觉信息的策略，以及能够在极少量数据下准确推断中风患者意图的机器学习方法。

Conclusion: 通过针对性的机器学习方法，可以有效解决机器人学习中的数据稀疏和稀缺问题，为触觉感知和康复机器人等领域的实际应用提供了可行的技术路径。

Abstract: Unlike in language or vision, one of the fundamental challenges in robot
learning is the lack of access to vast data resources. We can further break
down the problem into (1) data sparsity from the angle of data representation
and (2) data scarcity from the angle of data quantity. In this thesis, I will
discuss selected works on two domains: (1) tactile sensing and (2)
rehabilitation robots, which are exemplars of data sparsity and scarcity,
respectively. Tactile sensing is an essential modality for robotics, but
tactile data are often sparse, and for each interaction with the physical
world, tactile sensors can only obtain information about the local area of
contact. I will discuss my work on learning vision-free tactile-only
exploration and manipulation policies through model-free reinforcement learning
to make efficient use of sparse tactile information. On the other hand,
rehabilitation robots are an example of data scarcity to the extreme due to the
significant challenge of collecting biosignals from disabled-bodied subjects at
scale for training. I will discuss my work in collaboration with the medical
school and clinicians on intent inferral for stroke survivors, where a hand
orthosis developed in our lab collects a set of biosignals from the patient and
uses them to infer the activity that the patient intends to perform, so the
orthosis can provide the right type of physical assistance at the right moment.
My work develops machine learning algorithms that enable intent inferral with
minimal data, including semi-supervised, meta-learning, and generative AI
methods.

</details>


### [21] [Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics](https://arxiv.org/abs/2509.16858)
*Soon Jynn Chu,Raju Gottumukkala,Alan Barhorst*

Main category: cs.RO

TL;DR: 该论文研究使用离线强化学习作为在线强化学习的实用替代方案，使社交机器人能够基于预收集数据进行情感自适应响应，在数据稀疏情况下BCQ和CQL算法表现更优。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在社交机器人情感响应开发中成本高昂且存在安全风险，需要寻找更实用的替代方案。

Method: 提出集成多模态感知识别、决策制定和自适应响应的系统架构，在人类-机器人游戏场景的有限数据集上比较离线强化学习算法。

Result: BCQ和CQL算法对数据稀疏性更具鲁棒性，相比NFQ、DQN和DDQN获得更高的状态-动作值。

Conclusion: 该工作为情感自适应机器人中的离线强化学习基准测试奠定了基础，并为未来在真实人机交互场景中的部署提供了指导。

Abstract: The ability of social robots to respond to human emotions is crucial for
building trust and acceptance in human-robot collaborative environments.
However, developing such capabilities through online reinforcement learning is
sometimes impractical due to the prohibitive cost of data collection and the
risk of generating unsafe behaviors. In this paper, we study the use of offline
reinforcement learning as a practical and efficient alternative. This technique
uses pre-collected data to enable emotion-adaptive social robots. We present a
system architecture that integrates multimodal sensing and recognition,
decision-making, and adaptive responses. Using a limited dataset from a
human-robot game-playing scenario, we establish a benchmark for comparing
offline reinforcement learning algorithms that do not require an online
environment. Our results show that BCQ and CQL are more robust to data
sparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN.
This work establishes a foundation for benchmarking offline RL in
emotion-adaptive robotics and informs future deployment in real-world HRI. Our
findings provide empirical insight into the performance of offline
reinforcement learning algorithms in data-constrained HRI. This work
establishes a foundation for benchmarking offline RL in emotion-adaptive
robotics and informs its future deployment in real-world HRI, such as in
conversational agents, educational partners, and personal assistants, require
reliable emotional responsiveness.

</details>


### [22] [HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness](https://arxiv.org/abs/2509.16871)
*Yitian Shi,Zicheng Guo,Rosa Wolf,Edgar Welte,Rania Rayyes*

Main category: cs.RO

TL;DR: HOGraspFlow是一种基于RGB图像的手-物体交互(HOI)重定向方法，无需物体几何先验即可生成多模态平行夹爪抓取姿势


<details>
  <summary>Details</summary>
Motivation: 解决从单张RGB图像中提取可执行抓取姿势的挑战，避免对目标物体显式几何先验的依赖

Method: 基于基础模型进行手部重建和视觉分析，使用去噪流匹配(FM)在SE(3)空间中合成抓取姿势，结合RGB基础特征、HOI接触重建和抓取类型分类感知先验三个互补线索

Result: 在真实世界实验中平均成功率超过83%，相比基于扩散的变体(HOGraspDiff)在SE(3)空间中具有更高的分布保真度和更稳定的优化

Conclusion: HOGraspFlow能够从人类演示中实现可靠、物体无关的抓取合成，展示了高保真度的抓取合成能力

Abstract: We propose Hand-Object\emph{(HO)GraspFlow}, an affordance-centric approach
that retargets a single RGB with hand-object interaction (HOI) into multi-modal
executable parallel jaw grasps without explicit geometric priors on target
objects. Building on foundation models for hand reconstruction and vision, we
synthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned
on the following three complementary cues: RGB foundation features as visual
semantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.
Our approach demonstrates high fidelity in grasp synthesis without explicit HOI
contact input or object geometry, while maintaining strong contact and taxonomy
recognition. Another controlled comparison shows that \emph{HOGraspFlow}
consistently outperforms diffusion-based variants (\emph{HOGraspDiff}),
achieving high distributional fidelity and more stable optimization in $SE(3)$.
We demonstrate a reliable, object-agnostic grasp synthesis from human
demonstrations in real-world experiments, where an average success rate of over
$83\%$ is achieved.

</details>


### [23] [End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth Racing](https://arxiv.org/abs/2509.16894)
*Zhijie Qiao,Haowei Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TL;DR: End2Race是一种端到端模仿学习算法，专门用于F1Tenth自动驾驶赛车平台的头对头比赛，通过GRU架构和LiDAR数据归一化实现了高效推理和优越性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车需要与传统自动驾驶不同的算法，高速操作和动态环境限制了模型容量，因此需要开发专门的高效算法。

Method: 采用Gated Recurrent Unit(GRU)架构捕捉时序依赖关系，使用基于sigmoid的归一化函数将LiDAR扫描转换为空间压力标记，实现端到端模仿学习。

Result: 在F1Tenth模拟器中，End2Race在2400个超车场景中达到94.2%的安全率，59.2%的超车成功率，推理时间小于0.5毫秒。

Conclusion: End2Race超越了现有方法，成为F1Tenth赛车测试平台的领先解决方案，证明了其在高速自动驾驶赛车中的有效性。

Abstract: F1Tenth is a widely adopted reduced-scale platform for developing and testing
autonomous racing algorithms, hosting annual competitions worldwide. With high
operating speeds, dynamic environments, and head-to-head interactions,
autonomous racing requires algorithms that diverge from those in classical
autonomous driving. Training such algorithms is particularly challenging: the
need for rapid decision-making at high speeds severely limits model capacity.
To address this, we propose End2Race, a novel end-to-end imitation learning
algorithm designed for head-to-head autonomous racing. End2Race leverages a
Gated Recurrent Unit (GRU) architecture to capture continuous temporal
dependencies, enabling both short-term responsiveness and long-term strategic
planning. We also adopt a sigmoid-based normalization function that transforms
raw LiDAR scans into spatial pressure tokens, facilitating effective model
training and convergence. The algorithm is extremely efficient, achieving an
inference time of less than 0.5 milliseconds on a consumer-class GPU.
Experiments in the F1Tenth simulator demonstrate that End2Race achieves a 94.2%
safety rate across 2,400 overtaking scenarios, each with an 8-second time
limit, and successfully completes overtakes in 59.2% of cases. This surpasses
previous methods and establishes ours as a leading solution for the F1Tenth
racing testbed. Code is available at
https://github.com/michigan-traffic-lab/End2Race.

</details>


### [24] [SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for Robotic Swarms](https://arxiv.org/abs/2509.16920)
*Ettilla Mohiuddin Eumi,Hussein Abbass,Nadine Marcus*

Main category: cs.RO

TL;DR: SwarmChat是一个基于大语言模型的多模态人机群交互系统，通过自然语言命令实现机器人集群的直观控制


<details>
  <summary>Details</summary>
Motivation: 解决传统人机群交互方法缺乏直观实时自适应界面的问题，传统方法导致决策速度慢、认知负荷增加且命令灵活性有限

Method: 系统集成四个LLM模块（上下文生成器、意图识别、任务规划器和模态选择器），采用三层架构支持文本、语音或遥操作等多种交互方式

Result: 初步评估显示SwarmChat的LLM模块能够提供准确的上下文解释、相关意图识别和有效命令传递，获得高用户满意度

Conclusion: SwarmChat通过LLM驱动的多模态交互系统，为人机群交互提供了动态、自适应的解决方案，显著提升了控制灵活性和用户体验

Abstract: Traditional Human-Swarm Interaction (HSI) methods often lack intuitive
real-time adaptive interfaces, making decision making slower and increasing
cognitive load while limiting command flexibility. To solve this, we present
SwarmChat, a context-aware, multimodal interaction system powered by Large
Language Models (LLMs). SwarmChat enables users to issue natural language
commands to robotic swarms using multiple modalities, such as text, voice, or
teleoperation. The system integrates four LLM-based modules: Context Generator,
Intent Recognition, Task Planner, and Modality Selector. These modules
collaboratively generate context from keywords, detect user intent, adapt
commands based on real-time robot state, and suggest optimal communication
modalities. Its three-layer architecture offers a dynamic interface with both
fixed and customizable command options, supporting flexible control while
optimizing cognitive effort. The preliminary evaluation also shows that the
SwarmChat's LLM modules provide accurate context interpretation, relevant
intent recognition, and effective command delivery, achieving high user
satisfaction.

</details>


### [25] [A Reliable Robot Motion Planner in Complex Real-world Environments via Action Imagination](https://arxiv.org/abs/2509.16963)
*Chengjin Wang,Yanmin Zhou,Zhipeng Wang,Zheng Yan,Feng Luan,Shuo Jiang,Runjie Shen,Hongrui Sang,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一种受动物智能启发的想象力驱动运动规划器（I-MP）框架，通过想象可能的空间状态来增强机器人在未知非结构化环境中的动作可靠性。


<details>
  <summary>Details</summary>
Motivation: 受人类和动物能够通过想象动作结果来实时调整运动以防止意外运动失败的启发，研究旨在开发一种能够处理物理交互引起的不确定性的机器人运动规划方法。

Method: I-MP框架通过拓扑化工作空间，构建感知-动作循环使机器人自主建立接触模型。利用不动点理论和Hausdorff距离计算在交互特性和任务约束下的收敛空间状态，并通过功来同质化表示多维环境特征，实时计算能量梯度来接近想象的空间状态。

Result: 实验结果表明，I-MP在复杂杂乱环境中具有实用性和鲁棒性。

Conclusion: 该研究提出的想象力驱动运动规划器框架有效提升了机器人在未知非结构化环境中的动作可靠性，为复杂系统中的物理交互不确定性处理提供了新思路。

Abstract: Humans and animals can make real-time adjustments to movements by imagining
their action outcomes to prevent unanticipated or even catastrophic motion
failures in unknown unstructured environments. Action imagination, as a refined
sensorimotor strategy, leverages perception-action loops to handle physical
interaction-induced uncertainties in perception and system modeling within
complex systems. Inspired by the action-awareness capability of animal
intelligence, this study proposes an imagination-inspired motion planner (I-MP)
framework that specifically enhances robots' action reliability by imagining
plausible spatial states for approaching. After topologizing the workspace,
I-MP build perception-action loop enabling robots autonomously build contact
models. Leveraging fixed-point theory and Hausdorff distance, the planner
computes convergent spatial states under interaction characteristics and
mission constraints. By homogenously representing multi-dimensional
environmental characteristics through work, the robot can approach the imagined
spatial states via real-time computation of energy gradients. Consequently,
experimental results demonstrate the practicality and robustness of I-MP in
complex cluttered environments.

</details>


### [26] [Geometric Interpolation of Rigid Body Motions](https://arxiv.org/abs/2509.16966)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了刚体运动插值问题的两种变体：k阶初值轨迹插值问题（k-IV-TIP）和k阶边值轨迹插值问题（k-BV-TIP），并给出了k=1到4的解决方案，特别提出了一种新颖的三次插值方法。


<details>
  <summary>Details</summary>
Motivation: 解决刚体运动在给定初始和终端位姿之间的空间轨迹插值问题，满足对刚体扭转及其高阶导数在边界处的约束条件。

Method: 提出了k-IV-TIP和k-BV-TIP两种插值问题框架，分别处理初值条件和边值条件。对于k=1到4的情况给出了具体解法，特别针对1-IV-TBP问题提出了三次插值方法。

Result: 成功推导出高阶插值解决方案，数值实验验证了方法的有效性。当扭转设置为零时，三次插值自动退化为最小加速度曲线。

Conclusion: 本文建立了刚体运动插值的系统理论框架，提供了可扩展到更高阶插值的通用方法，为解决刚体轨迹规划问题提供了有效的数学工具。

Abstract: The problem of interpolating a rigid body motion is to find a spatial
trajectory between a prescribed initial and terminal pose. Two variants of this
interpolation problem are addressed. The first is to find a solution that
satisfies initial conditions on the k-1 derivatives of the rigid body twist.
This is called the kth-order initial value trajectory interpolation problem
(k-IV-TIP). The second is to find a solution that satisfies conditions on the
rigid body twist and its k-1 derivatives at the initial and terminal pose. This
is called the kth-order boundary value trajectory interpolation problem
(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and
up to the 4th time derivative are prescribed. Further, a solution to the
1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The
latter is a novel cubic interpolation between two spatial configurations with
given initial and terminal twist. This interpolation is automatically identical
to the minimum acceleration curve when the twists are set to zero. The general
approach to derive higher-order solutions is presented. Numerical results are
shown for two examples.

</details>


### [27] [IDfRA: Self-Verification for Iterative Design in Robotic Assembly](https://arxiv.org/abs/2509.16998)
*Nishka Khendry,Christos Margadji,Sebastian W. Pattinson*

Main category: cs.RO

TL;DR: 提出了IDfRA框架，通过迭代的规划-执行-验证-重规划循环，利用机器人自身进行设计验证，替代传统的物理仿真方法


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配设计依赖人工规划，耗时昂贵且复杂对象不实用；现有LLM方法依赖启发式策略和硬编码物理仿真，难以适应真实装配场景

Method: IDfRA框架采用迭代循环：规划、执行、验证和重规划，每个环节都基于自我评估，在固定但初始未完全指定的环境中逐步提升设计质量

Result: IDfRA在语义可识别性上达到73.3%的top-1准确率，优于基线；装配计划整体构建成功率达到86.9%，设计质量随迭代提升

Conclusion: 通过自我验证和上下文感知适应的结合，该框架在非结构化制造场景中展现出强大的部署潜力

Abstract: As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA),
which is designing products for efficient automated assembly, is increasingly
important. Traditional approaches to DfRA rely on manual planning, which is
time-consuming, expensive and potentially impractical for complex objects.
Large language models (LLM) have exhibited proficiency in semantic
interpretation and robotic task planning, stimulating interest in their
application to the automation of DfRA. But existing methodologies typically
rely on heuristic strategies and rigid, hard-coded physics simulators that may
not translate into real-world assembly contexts. In this work, we present
Iterative Design for Robotic Assembly (IDfRA), a framework using iterative
cycles of planning, execution, verification, and re-planning, each informed by
self-assessment, to progressively enhance design quality within a fixed yet
initially under-specified environment, thereby eliminating the physics
simulation with the real world itself. The framework accepts as input a target
structure together with a partial environmental representation. Through
successive refinement, it converges toward solutions that reconcile semantic
fidelity with physical feasibility. Empirical evaluation demonstrates that
IDfRA attains 73.3\% top-1 accuracy in semantic recognisability, surpassing the
baseline on this metric. Moreover, the resulting assembly plans exhibit robust
physical feasibility, achieving an overall 86.9\% construction success rate,
with design quality improving across iterations, albeit not always
monotonically. Pairwise human evaluation further corroborates the advantages of
IDfRA relative to alternative approaches. By integrating self-verification with
context-aware adaptation, the framework evidences strong potential for
deployment in unstructured manufacturing scenarios.

</details>


### [28] [Generalized Momenta-Based Koopman Formalism for Robust Control of Euler-Lagrangian Systems](https://arxiv.org/abs/2509.17010)
*Rajpal Singh,Aditya Singh,Chidre Shravista Kashyap,Jishnu Keshavan*

Main category: cs.RO

TL;DR: 本文提出了一种基于广义动量的隐式Koopman算子方法，用于欧拉-拉格朗日动力学建模，通过解耦线性驱动通道和状态依赖动力学，显著降低模型复杂度并提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统显式Koopman模型将输入与状态依赖项非线性耦合，需要学习完整系统，导致参数多、计算成本高。本文旨在开发更高效的线性Koopman建模方法。

Method: 采用隐式广义动量状态空间表示，分离线性驱动通道；提出两种神经网络架构构建Koopman嵌入；集成线性广义扩展状态观测器(GESO)进行实时扰动估计和补偿。

Result: 在机器人操纵器上的轨迹跟踪仿真和实验表明，该方法相比传统双线性模型具有更高的预测精度、鲁棒性和学习效率。

Conclusion: 所提出的动量基Koopman与GESO框架实现了高效的线性建模，在保持计算效率的同时显著提升了性能，为复杂动力学系统控制提供了有效解决方案。

Abstract: This paper presents a novel Koopman operator formulation for Euler Lagrangian
dynamics that employs an implicit generalized momentum-based state space
representation, which decouples a known linear actuation channel from state
dependent dynamics and makes the system more amenable to linear Koopman
modeling. By leveraging this structural separation, the proposed formulation
only requires to learn the unactuated dynamics rather than the complete
actuation dependent system, thereby significantly reducing the number of
learnable parameters, improving data efficiency, and lowering overall model
complexity. In contrast, conventional explicit formulations inherently couple
inputs with the state dependent terms in a nonlinear manner, making them more
suitable for bilinear Koopman models, which are more computationally expensive
to train and deploy. Notably, the proposed scheme enables the formulation of
linear models that achieve superior prediction performance compared to
conventional bilinear models while remaining substantially more efficient. To
realize this framework, we present two neural network architectures that
construct Koopman embeddings from actuated or unactuated data, enabling
flexible and efficient modeling across different tasks. Robustness is ensured
through the integration of a linear Generalized Extended State Observer (GESO),
which explicitly estimates disturbances and compensates for them in real time.
The combined momentum-based Koopman and GESO framework is validated through
comprehensive trajectory tracking simulations and experiments on robotic
manipulators, demonstrating superior accuracy, robustness, and learning
efficiency relative to state of the art alternatives.

</details>


### [29] [Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning](https://arxiv.org/abs/2509.17042)
*Zengqi Peng,Yusen Xie,Yubin Wang,Rui Yang,Qifeng Chen,Jun Ma*

Main category: cs.RO

TL;DR: OGR是一个基于视觉语言模型的多智能体协作框架，用于自动化驾驶策略学习，通过分层智能体系统自动生成奖励函数和训练课程，避免了传统方法中需要手动设计的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶策略学习中手动设计奖励函数和训练课程的高成本、耗时问题，利用基础模型的优势实现自动化策略学习。

Method: 提出OGR框架，包含编排器、生成模块、反思模块和记忆模块。采用分层多智能体协作，利用VLM的推理和多模态理解能力，通过分析-生成两步法自动生成奖励-课程对，并支持在线迭代优化。

Result: 在CARLA模拟器中验证了优越性能，表现出强大的跨场景泛化能力，并与多种RL算法兼容。真实世界实验证明了框架的实用性和有效性。

Conclusion: OGR框架通过多智能体协作和VLM能力，实现了自动驾驶策略的高效在线学习，为自动化驾驶策略学习提供了可行的解决方案。

Abstract: The advancement of foundation models fosters new initiatives for policy
learning in achieving safe and efficient autonomous driving. However, a
critical bottleneck lies in the manual engineering of reward functions and
training curricula for complex and dynamic driving tasks, which is a
labor-intensive and time-consuming process. To address this problem, we propose
OGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning
framework that leverages vision-language model (VLM)-based multi-agent
collaboration. Our framework capitalizes on advanced reasoning and multimodal
understanding capabilities of VLMs to construct a hierarchical agent system.
Specifically, a centralized orchestrator plans high-level training objectives,
while a generation module employs a two-step analyze-then-generate process for
efficient generation of reward-curriculum pairs. A reflection module then
facilitates iterative optimization based on the online evaluation. Furthermore,
a dedicated memory module endows the VLM agents with the capabilities of
long-term memory. To enhance robustness and diversity of the generation
process, we introduce a parallel generation scheme and a human-in-the-loop
technique for augmentation of the reward observation space. Through efficient
multi-agent cooperation and leveraging rich multimodal information, OGR enables
the online evolution of reinforcement learning policies to acquire
interaction-aware driving skills. Extensive experiments in the CARLA simulator
demonstrate the superior performance, robust generalizability across distinct
urban scenarios, and strong compatibility with various RL algorithms. Further
real-world experiments highlight the practical viability and effectiveness of
our framework. The source code will be available upon acceptance of the paper.

</details>


### [30] [FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.17053)
*Haizhou Ge,Yufei Jia,Zheng Li,Yue Li,Zhixing Chen,Ruqi Huang,Guyue Zhou*

Main category: cs.RO

TL;DR: FILIC是一个力引导的模仿学习框架，通过阻抗扭矩控制和双环结构实现接触丰富的机器人操作，无需昂贵的力/扭矩传感器


<details>
  <summary>Details</summary>
Motivation: 解决当前模仿学习策略缺乏力感知能力的问题，以及为协作机器人臂添加力/扭矩传感器成本高昂的挑战

Method: 结合基于Transformer的模仿学习策略和阻抗控制器，使用关节扭矩测量通过雅可比逆矩阵估计末端执行器力，并设计手持触觉和VR可视化力反馈框架

Result: FILIC显著优于仅基于视觉和关节扭矩的方法，实现更安全、更柔顺和适应性更强的接触丰富操作

Conclusion: FILIC框架为接触丰富的机器人操作提供了一种成本效益高且有效的解决方案，无需额外硬件传感器

Abstract: Contact-rich manipulation is crucial for robots to perform tasks requiring
precise force control, such as insertion, assembly, and in-hand manipulation.
However, most imitation learning (IL) policies remain position-centric and lack
explicit force awareness, and adding force/torque sensors to collaborative
robot arms is often costly and requires additional hardware design. To overcome
these issues, we propose FILIC, a Force-guided Imitation Learning framework
with impedance torque control. FILIC integrates a Transformer-based IL policy
with an impedance controller in a dual-loop structure, enabling compliant
force-informed, force-executed manipulation. For robots without force/torque
sensors, we introduce a cost-effective end-effector force estimator using joint
torque measurements through analytical Jacobian-based inversion while
compensating with model-predicted torques from a digital twin. We also design
complementary force feedback frameworks via handheld haptics and VR
visualization to improve demonstration quality. Experiments show that FILIC
significantly outperforms vision-only and joint-torque-based methods, achieving
safer, more compliant, and adaptable contact-rich manipulation. Our code can be
found in https://github.com/TATP-233/FILIC.

</details>


### [31] [RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments](https://arxiv.org/abs/2509.17057)
*Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Hanbit Oh,Koshi Makihara,Keisuke Shirai,Yukiyasu Domae*

Main category: cs.RO

TL;DR: RoboManipBaselines是一个用于机器人模仿学习的开源框架，统一了仿真和真实机器人的数据收集、训练和评估流程。


<details>
  <summary>Details</summary>
Motivation: 提供一个系统化的平台，用于对多样化任务、机器人和多模态策略进行基准测试，强调集成性、通用性、可扩展性和可复现性。

Method: 开发了一个开源框架，统一了仿真和真实机器人环境中的数据收集、训练和评估流程。

Result: 创建了一个能够支持多样化任务、机器人和多模态策略基准测试的平台。

Conclusion: RoboManipBaselines作为一个集成平台，能够促进机器人模仿学习领域的系统化研究和比较。

Abstract: RoboManipBaselines is an open framework for robot imitation learning that
unifies data collection, training, and evaluation across simulation and real
robots. We introduce it as a platform enabling systematic benchmarking of
diverse tasks, robots, and multimodal policies with emphasis on integration,
generality, extensibility, and reproducibility.

</details>


### [32] [CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving](https://arxiv.org/abs/2509.17080)
*Ruiguo Zhong,Ruoyu Yao,Pei Liu,Xiaolong Chen,Rui Yang,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了一种应急感知扩散规划器（CoPlanner），通过联合建模多智能体交互轨迹生成和应急感知运动规划，解决了自动驾驶中预测与规划模块解耦导致的过度自信决策和缺乏应急策略的问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在复杂交互环境中存在两个主要问题：1）生成-评估框架通常只采用最可能的结果，导致在关键场景中缺乏应急策略；2）预测与规划模块解耦可能导致社会不一致或不现实的联合轨迹。

Method: CoPlanner采用枢轴条件扩散机制，在验证的共享短期段上锚定轨迹采样以保持时间一致性，同时随机生成多样化的长期分支以捕捉多模态运动演化。并行设计了应急感知多场景评分策略，评估候选自车轨迹在多个可能长期演化场景中的表现。

Result: 在nuPlan基准测试上的广泛闭环实验表明，CoPlanner在Val14和Test14数据集上持续超越最先进方法，在反应性和非反应性设置下均实现了安全和舒适性的显著提升。

Conclusion: CoPlanner的统一框架通过保持可行的应急选项和增强不确定性下的鲁棒性，实现了更现实的交互感知规划，为自动驾驶系统在复杂交互环境中的安全导航提供了有效解决方案。

Abstract: Accurate trajectory prediction and motion planning are crucial for autonomous
driving systems to navigate safely in complex, interactive environments
characterized by multimodal uncertainties. However, current
generation-then-evaluation frameworks typically construct multiple plausible
trajectory hypotheses but ultimately adopt a single most likely outcome,
leading to overconfident decisions and a lack of fallback strategies that are
vital for safety in rare but critical scenarios. Moreover, the usual decoupling
of prediction and planning modules could result in socially inconsistent or
unrealistic joint trajectories, especially in highly interactive traffic. To
address these challenges, we propose a contingency-aware diffusion planner
(CoPlanner), a unified framework that jointly models multi-agent interactive
trajectory generation and contingency-aware motion planning. Specifically, the
pivot-conditioned diffusion mechanism anchors trajectory sampling on a
validated, shared short-term segment to preserve temporal consistency, while
stochastically generating diverse long-horizon branches that capture multimodal
motion evolutions. In parallel, we design a contingency-aware multi-scenario
scoring strategy that evaluates candidate ego trajectories across multiple
plausible long-horizon evolution scenarios, balancing safety, progress, and
comfort. This integrated design preserves feasible fallback options and
enhances robustness under uncertainty, leading to more realistic
interaction-aware planning. Extensive closed-loop experiments on the nuPlan
benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art
methods on both Val14 and Test14 datasets, achieving significant improvements
in safety and comfort under both reactive and non-reactive settings. Code and
model will be made publicly available upon acceptance.

</details>


### [33] [Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation](https://arxiv.org/abs/2509.17125)
*Liang Heng,Jiadong Xu,Yiwen Wang,Xiaoqi Li,Muhe Cai,Yan Shen,Juan Zhu,Guanghui Ren,Hao Dong*

Main category: cs.RO

TL;DR: Imagine2Act是一个3D模仿学习框架，通过生成想象的目标图像和点云来整合语义和几何约束，解决关系物体重排任务中的高精度操作问题


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖预收集的演示数据难以捕捉复杂几何约束，要么生成目标状态观察但未能显式耦合物体变换与动作预测，导致生成噪声引起的错误

Method: 首先生成基于语言指令的想象目标图像并重建对应3D点云，将这些点云作为策略模型的额外输入，同时使用软姿态监督的对象-动作一致性策略来对齐预测的末端执行器运动与生成的物体变换

Result: 在仿真和真实世界实验中都表明Imagine2Act优于之前的最先进策略

Conclusion: Imagine2Act能够推理物体间的语义和几何关系，并在多样化任务中预测准确动作

Abstract: Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)
require a robot to manipulate objects with precise semantic and geometric
reasoning. Existing approaches either rely on pre-collected demonstrations that
struggle to capture complex geometric constraints or generate goal-state
observations to capture semantic and geometric knowledge, but fail to
explicitly couple object transformation with action prediction, resulting in
errors due to generative noise. To address these limitations, we propose
Imagine2Act, a 3D imitation-learning framework that incorporates semantic and
geometric constraints of objects into policy learning to tackle high-precision
manipulation tasks. We first generate imagined goal images conditioned on
language instructions and reconstruct corresponding 3D point clouds to provide
robust semantic and geometric priors. These imagined goal point clouds serve as
additional inputs to the policy model, while an object-action consistency
strategy with soft pose supervision explicitly aligns predicted end-effector
motion with generated object transformation. This design enables Imagine2Act to
reason about semantic and geometric relationships between objects and predict
accurate actions across diverse tasks. Experiments in both simulation and the
real world demonstrate that Imagine2Act outperforms previous state-of-the-art
policies. More visualizations can be found at
https://sites.google.com/view/imagine2act.

</details>


### [34] [History-Aware Visuomotor Policy Learning via Point Tracking](https://arxiv.org/abs/2509.17141)
*Jingjing Chen,Hongjie Fang,Chenxi Wang,Shiquan Wang,Cewu Lu*

Main category: cs.RO

TL;DR: 本文提出了一种基于点跟踪的物体中心历史表示方法，用于解决视觉运动策略中的记忆需求问题，通过将过去观察抽象为紧凑的结构化形式，提高任务性能和决策准确性。


<details>
  <summary>Details</summary>
Motivation: 许多操作任务需要超越当前观察的记忆能力，但大多数视觉运动策略依赖马尔可夫假设，难以处理重复状态或长时依赖关系。现有方法尝试扩展观察视野但仍不足以满足多样化的记忆需求。

Method: 基于点跟踪的物体中心历史表示方法，将跟踪点编码并在物体级别聚合，生成紧凑的历史表示，可以无缝集成到各种视觉运动策略中。

Result: 在多样化操作任务上的广泛评估表明，该方法能够处理任务阶段识别、空间记忆、动作计数等多种记忆需求，以及连续和预加载记忆等长期需求，性能优于马尔可夫基线和现有历史方法。

Conclusion: 提出的物体中心历史表示方法提供了完整的历史感知能力，具有高计算效率，能够显著提升整体任务性能和决策准确性。

Abstract: Many manipulation tasks require memory beyond the current observation, yet
most visuomotor policies rely on the Markov assumption and thus struggle with
repeated states or long-horizon dependencies. Existing methods attempt to
extend observation horizons but remain insufficient for diverse memory
requirements. To this end, we propose an object-centric history representation
based on point tracking, which abstracts past observations into a compact and
structured form that retains only essential task-relevant information. Tracked
points are encoded and aggregated at the object level, yielding a compact
history representation that can be seamlessly integrated into various
visuomotor policies. Our design provides full history-awareness with high
computational efficiency, leading to improved overall task performance and
decision accuracy. Through extensive evaluations on diverse manipulation tasks,
we show that our method addresses multiple facets of memory requirements - such
as task stage identification, spatial memorization, and action counting, as
well as longer-term demands like continuous and pre-loaded memory - and
consistently outperforms both Markovian baselines and prior history-based
approaches. Project website: http://tonyfang.net/history

</details>


### [35] [MAST: Multi-Agent Spatial Transformer for Learning to Collaborate](https://arxiv.org/abs/2509.17195)
*Damian Owerko,Frederic Vatnsdal,Saurav Agarwal,Vijay Kumar,Alejandro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多智能体空间变换器（MAST），用于在大规模去中心化协作多机器人系统（DC-MRS）中学习通信策略。MAST通过改进的位置编码策略和注意力机制，解决了局部观测、有限通信范围和独立执行动作等挑战。


<details>
  <summary>Details</summary>
Motivation: DC-MRS面临三大挑战：局部可观测状态、有限通信范围（无中央服务器）以及独立执行动作。机器人需要在受限环境下通过通信策略优化共同任务目标，实现期望的协作行为。

Method: MAST是一种去中心化变换器架构，通过新的位置编码策略和采用窗口化限制感受野的注意力操作，学习通信策略来计算要与其他智能体共享的抽象信息，并将接收到的信息与机器人自身观测相结合处理。

Result: 在去中心化分配导航（DAN）和去中心化覆盖控制任务上的实验表明，MAST策略对通信延迟具有鲁棒性，可扩展到大型团队，性能优于基线方法和其他基于学习的方法。

Conclusion: MAST通过局部计算、平移等变性和置换等变性特性，为DC-MRS提供了一种有前景的方法，能够有效处理大规模去中心化协作任务。

Abstract: This article presents a novel multi-agent spatial transformer (MAST) for
learning communication policies in large-scale decentralized and collaborative
multi-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from:
(i) partial observable states as robots make only localized perception, (ii)
limited communication range with no central server, and (iii) independent
execution of actions. The robots need to optimize a common task-specific
objective, which, under the restricted setting, must be done using a
communication policy that exhibits the desired collaborative behavior. The
proposed MAST is a decentralized transformer architecture that learns
communication policies to compute abstract information to be shared with other
agents and processes the received information with the robot's own
observations. The MAST extends the standard transformer with new positional
encoding strategies and attention operations that employ windowing to limit the
receptive field for MRS. These are designed for local computation,
shift-equivariance, and permutation equivariance, making it a promising
approach for DC-MRS. We demonstrate the efficacy of MAST on decentralized
assignment and navigation (DAN) and decentralized coverage control. Efficiently
trained using imitation learning in a centralized setting, the decentralized
MAST policy is robust to communication delays, scales to large teams, and
performs better than the baselines and other learning-based approaches.

</details>


### [36] [Certifiably Optimal Doppler Positioning using Opportunistic LEO Satellites](https://arxiv.org/abs/2509.17198)
*Baoshan Song,Weisong Wen,Qi Zhang,Bing Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 提出了一种基于凸优化的可证明最优LEO多普勒定位方法，通过GWA算法和SDP松弛实现，无需初始估计即可获得全局最优解。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中无法获得精确初始估计时，传统的局部搜索方法可能陷入局部最优，需要一种无需初始化的全局优化方法。

Method: 采用毕业权重近似(GWA)算法和半定规划(SDP)松弛技术，推导了理想无噪情况下的最优性必要条件和有噪情况下的充分噪声边界条件。

Result: 使用Iridium-NEXT卫星的真实测试显示，该方法在无需初始估计的情况下获得可证明最优解，3D定位误差为140米，而传统方法在初始点距离真实位置1000公里时会陷入局部最优。

Conclusion: 该方法提供了可靠的全局最优定位解，也可作为局部搜索方法的初始化，将3D定位误差进一步降低到130米。

Abstract: To provide backup and augmentation to global navigation satellite system
(GNSS), Doppler shift from Low Earth Orbit (LEO) satellites can be employed as
signals of opportunity (SOP) for position, navigation and timing (PNT). Since
the Doppler positioning problem is non-convex, local searching methods may
produce two types of estimates: a global optimum without notice or a local
optimum given an inexact initial estimate. As exact initialization is
unavailable in some unknown environments, a guaranteed global optimization
method in no need of initialization becomes necessary. To achieve this goal, we
propose a certifiably optimal LEO Doppler positioning method by utilizing
convex optimization. In this paper, the certifiable positioning method is
implemented through a graduated weight approximation (GWA) algorithm and
semidefinite programming (SDP) relaxation. To guarantee the optimality, we
derive the necessary conditions for optimality in ideal noiseless cases and
sufficient noise bounds conditions in noisy cases. Simulation and real tests
are conducted to evaluate the effectiveness and robustness of the proposed
method. Specially, the real test using Iridium-NEXT satellites shows that the
proposed method estimates an certifiably optimal solution with an 3D
positioning error of 140 m without initial estimates while Gauss-Newton and
Dog-Leg are trapped in local optima when the initial point is equal or larger
than 1000 km away from the ground truth. Moreover, the certifiable estimation
can also be used as initialization in local searching methods to lower down the
3D positioning error to 130 m.

</details>


### [37] [Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation](https://arxiv.org/abs/2509.17204)
*James R. Han,Mithun Vanniasinghe,Hshmat Sahak,Nicholas Rhinehart,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: Ratatouille是一个用于社交机器人导航的离线模仿学习管道和模型架构，通过精心设计的架构和训练选择，相比简单的行为克隆方法，碰撞率降低6倍，成功率提高3倍。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中进行社交机器人导航的强化学习既数据密集又不安全，因为策略必须通过直接交互学习，不可避免地会遇到碰撞。离线模仿学习可以安全地收集专家演示，完全离线训练，并零样本部署策略。

Method: 提出了Ratatouille管道和模型架构，通过精心设计的架构和训练选择来改进行为克隆方法，而不需要改变数据。在密集大学校园收集了超过11小时的数据进行验证。

Result: 在仿真和真实世界验证中，相比简单的行为克隆方法，碰撞率降低6倍，成功率提高3倍。还在公共美食广场展示了定性结果。

Conclusion: 研究表明，深思熟虑的模仿学习设计（而非增加数据）可以显著提高现实世界社交导航的安全性和可靠性。

Abstract: Scaling Reinforcement Learning to in-the-wild social robot navigation is both
data-intensive and unsafe, since policies must learn through direct interaction
and inevitably encounter collisions. Offline Imitation learning (IL) avoids
these risks by collecting expert demonstrations safely, training entirely
offline, and deploying policies zero-shot. However, we find that naively
applying Behaviour Cloning (BC) to social navigation is insufficient; achieving
strong performance requires careful architectural and training choices. We
present Ratatouille, a pipeline and model architecture that, without changing
the data, reduces collisions per meter by 6 times and improves success rate by
3 times compared to naive BC. We validate our approach in both simulation and
the real world, where we collected over 11 hours of data on a dense university
campus. We further demonstrate qualitative results in a public food court. Our
findings highlight that thoughtful IL design, rather than additional data, can
substantially improve safety and reliability in real-world social navigation.
Video: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.

</details>


### [38] [Combining Performance and Passivity in Linear Control of Series Elastic Actuators](https://arxiv.org/abs/2509.17210)
*Shaunak A. Mehta,Dylan P. Losey*

Main category: cs.RO

TL;DR: 本文探讨了串联弹性执行器（SEAs）在安全性和性能之间的权衡，提出了通过低物理刚度和高控制器增益的设计方案，实现精确性能同时确保碰撞安全。


<details>
  <summary>Details</summary>
Motivation: 人类与机器人物理交互时，需要机器人在保证安全的同时具备高性能。串联弹性执行器通过引入柔性驱动提高安全性，但弹簧会引入振荡并降低运动精度，因此需要在物理安全性和性能之间找到平衡。

Method: 枚举了串联弹性执行器的不同线性控制和机械配置，探索每种选择对柔顺性、被动性和跟踪性能的影响。研究发现执行器侧控制具有显著优势，简单的PD控制器配合弹性传动中的阻尼器可实现高性能。

Result: 仿真和真实世界实验表明，通过设计低物理刚度和高控制器增益的系统，该解决方案能够在确保碰撞时用户安全的同时实现精确性能。

Conclusion: 执行器侧控制结合弹性传动阻尼器的设计方案，能够在保持安全性的前提下显著提升串联弹性执行器的性能表现。

Abstract: When humans physically interact with robots, we need the robots to be both
safe and performant. Series elastic actuators (SEAs) fundamentally advance
safety by introducing compliant actuation. On the one hand, adding a spring
mitigates the impact of accidental collisions between human and robot; but on
the other hand, this spring introduces oscillations and fundamentally decreases
the robot's ability to perform precise, accurate motions. So how should we
trade off between physical safety and performance? In this paper, we enumerate
the different linear control and mechanical configurations for series elastic
actuators, and explore how each choice affects the rendered compliance,
passivity, and tracking performance. While prior works focus on load side
control, we find that actuator side control has significant benefits. Indeed,
simple PD controllers on the actuator side allow for a much wider range of
control gains that maintain safety, and combining these with a damper in the
elastic transmission yields high performance. Our simulations and real world
experiments suggest that, by designing a system with low physical stiffness and
high controller gains, this solution enables accurate performance while also
ensuring user safety during collisions.

</details>


### [39] [Neural Network and ANFIS based auto-adaptive MPC for path tracking in autonomous vehicles](https://arxiv.org/abs/2509.17213)
*Yassine Kebbati,Naima Ait-Oufroukh,Vincent Vigneron,Dalil Ichala*

Main category: cs.RO

TL;DR: 本文设计了一种用于自动驾驶汽车路径跟踪的自适应MPC控制器，采用改进的粒子群优化算法进行调参，并通过神经网络和ANFIS实现在线参数自适应。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车在动态环境中运行，面临各种不确定性和干扰，这使得传统控制器在横向控制方面效果不佳。

Method: 设计自适应MPC控制器，使用改进的粒子群优化算法进行参数调优，结合神经网络和ANFIS实现在线参数自适应。

Result: 在三次换道和轨迹跟踪场景中，所设计的控制器相比标准MPC表现出更好的性能。

Conclusion: 自适应MPC控制器在自动驾驶路径跟踪任务中具有良好应用前景。

Abstract: Self-driving cars operate in constantly changing environments and are exposed
to a variety of uncertainties and disturbances. These factors render classical
controllers ineffective, especially for lateral control. Therefore, an adaptive
MPC controller is designed in this paper for the path tracking task, tuned by
an improved particle swarm optimization algorithm. Online parameter adaptation
is performed using Neural Networks and ANFIS. The designed controller showed
promising results compared to standard MPC in triple lane change and trajectory
tracking scenarios. Code can be found here:
https://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC

</details>


### [40] [Scalable Multi Agent Diffusion Policies for Coverage Control](https://arxiv.org/abs/2509.17244)
*Frederic Vatnsdal,Romina Garcia Camargo,Saurav Agarwal,Alejandro Ribeiro*

Main category: cs.RO

TL;DR: MADP是一种基于扩散模型的去中心化机器人群体协作方法，通过扩散模型生成复杂高维动作分布样本，每个机器人基于自身观察和同伴感知嵌入的融合表示来采样策略。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化机器人群体在复杂任务中的协作问题，特别是覆盖控制这一典型多智能体导航问题，需要能够捕捉智能体间动作依赖关系的方法。

Method: 使用扩散模型生成动作分布，通过空间变换器架构参数化扩散过程实现去中心化推理，通过模仿学习从覆盖控制问题的全知专家进行策略训练。

Result: 实验表明MADP继承了扩散模型的优良特性，能够跨智能体密度和环境泛化，在各种重要性密度函数的数量、位置和方差条件下都优于最先进的基线方法。

Conclusion: MADP为去中心化机器人群体协作提供了一种有效的扩散模型方法，在覆盖控制任务中表现出强大的泛化能力和鲁棒性。

Abstract: We propose MADP, a novel diffusion-model-based approach for collaboration in
decentralized robot swarms. MADP leverages diffusion models to generate samples
from complex and high-dimensional action distributions that capture the
interdependencies between agents' actions. Each robot conditions policy
sampling on a fused representation of its own observations and perceptual
embeddings received from peers. To evaluate this approach, we task a team of
holonomic robots piloted by MADP to address coverage control-a canonical multi
agent navigation problem. The policy is trained via imitation learning from a
clairvoyant expert on the coverage control problem, with the diffusion process
parameterized by a spatial transformer architecture to enable decentralized
inference. We evaluate the system under varying numbers, locations, and
variances of importance density functions, capturing the robustness demands of
real-world coverage tasks. Experiments demonstrate that our model inherits
valuable properties from diffusion models, generalizing across agent densities
and environments, and consistently outperforming state-of-the-art baselines.

</details>


### [41] [Learning and Optimization with 3D Orientations](https://arxiv.org/abs/2509.17274)
*Alexandros Ntagkas,Constantinos Tsakonas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 本文系统梳理了3D方向表示方法，并在多种机器人场景下进行了基准测试，为不同任务提供了基于实证的推荐指南。


<details>
  <summary>Details</summary>
Motivation: 现有的3D方向表示方法众多且各有优劣，但缺乏统一的梳理和实证比较，导致在实际应用中难以选择合适的表示方法。

Method: 1）统一清晰地介绍所有可用的3D方向表示方法及相关数学技巧；2）在四种典型机器人场景（直接优化、模仿学习、强化学习、轨迹优化）中进行基准测试。

Result: 通过实验比较了不同表示方法在各种场景下的性能表现。

Conclusion: 根据不同应用场景提供了具体的选择指南，并开源了所有方向数学的参考实现。

Abstract: There exist numerous ways of representing 3D orientations. Each
representation has both limitations and unique features. Choosing the best
representation for one task is often a difficult chore, and there exist
conflicting opinions on which representation is better suited for a set of
family of tasks. Even worse, when dealing with scenarios where we need to learn
or optimize functions with orientations as inputs and/or outputs, the set of
possibilities (representations, loss functions, etc.) is even larger and it is
not easy to decide what is best for each scenario. In this paper, we attempt to
a) present clearly, concisely and with unified notation all available
representations, and "tricks" related to 3D orientations (including Lie Group
algebra), and b) benchmark them in representative scenarios. The first part
feels like it is missing from the robotics literature as one has to read many
different textbooks and papers in order have a concise and clear understanding
of all possibilities, while the benchmark is necessary in order to come up with
recommendations based on empirical evidence. More precisely, we experiment with
the following settings that attempt to cover most widely used scenarios in
robotics: 1) direct optimization, 2) imitation/supervised learning with a
neural network controller, 3) reinforcement learning, and 4) trajectory
optimization using differential dynamic programming. We finally provide
guidelines depending on the scenario, and make available a reference
implementation of all the orientation math described.

</details>


### [42] [Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation](https://arxiv.org/abs/2509.17287)
*Gokul B. Nair,Alejandro Fontan,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 本文提出了首个基于事件相机的视觉示教-重复导航系统，通过频域互相关框架实现超过300Hz的处理速率，比传统帧式方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机（30-60Hz）的固定帧率限制了系统响应性，在环境变化和控制响应之间存在固有延迟。事件相机的高频特性可以解决这一问题。

Method: 开发了频域互相关框架，将事件流匹配问题转化为计算效率高的傅里叶空间乘法；利用事件帧的二进制特性并应用图像压缩技术提升计算速度。

Result: 在AgileX Scout Mini机器人上使用Prophesee EVK4 HD事件相机进行了4000+米的室内外轨迹实验，ATE低于24cm，保持高频控制更新。

Conclusion: 该方法相比传统帧式系统实现了显著更高的更新速率，证明了事件感知在实时机器人导航中的实际可行性。

Abstract: Visual teach-and-repeat navigation enables robots to autonomously traverse
previously demonstrated paths by comparing current sensory input with recorded
trajectories. However, conventional frame-based cameras fundamentally limit
system responsiveness: their fixed frame rates (typically 30-60 Hz) create
inherent latency between environmental changes and control responses. Here we
present the first event-camera-based visual teach-and-repeat system. To achieve
this, we develop a frequency-domain cross-correlation framework that transforms
the event stream matching problem into computationally efficient Fourier space
multiplications, capable of exceeding 300Hz processing rates, an order of
magnitude faster than frame-based approaches. By exploiting the binary nature
of event frames and applying image compression techniques, we further enhance
the computational speed of the cross-correlation process without sacrificing
localization accuracy. Extensive experiments using a Prophesee EVK4 HD event
camera mounted on an AgileX Scout Mini robot demonstrate successful autonomous
navigation across 4000+ meters of indoor and outdoor trajectories. Our system
achieves ATEs below 24 cm while maintaining consistent high-frequency control
updates. Our evaluations show that our approach achieves substantially higher
update rates compared to conventional frame-based systems, underscoring the
practical viability of event-based perception for real-time robotic navigation.

</details>


### [43] [Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)](https://arxiv.org/abs/2509.17299)
*Dorian Tsai,Christopher A. Brunner,Riki Lamont,F. Mikaela Nordborg,Andrea Severati,Java Terry,Karen Jackel,Matthew Dunbabin,Tobias Fischer,Scarlett Raine*

Main category: cs.RO

TL;DR: 提出CSLICS系统，使用低成本模块化摄像头和对象检测器，通过人机协作标注方法实现珊瑚产卵自动计数，显著节省人工时间并提高珊瑚养殖效率。


<details>
  <summary>Details</summary>
Motivation: 当前珊瑚养殖中的产卵计数方法劳动密集且效率低下，成为珊瑚生产流程的关键瓶颈，需要自动化解决方案来提升珊瑚礁恢复工作的规模。

Method: 开发Coral Spawn and Larvae Imaging Camera System (CSLICS)，采用低成本模块化摄像头，结合人机协作标注训练的对象检测器，实现珊瑚产卵的自动检测、分类和计数。

Result: 实验结果显示：表面产卵检测F1分数82.4%，水下产卵检测F1分数65.3%，相比人工方法每个产卵事件节省5,720小时劳动时间。在Great Barrier Reef的大规模产卵事件中验证了系统的准确性。

Conclusion: CSLICS系统显著提升了珊瑚养殖过程的效率，能够支持珊瑚礁恢复工作的规模化，以应对气候变化对生态系统的威胁。

Abstract: Coral aquaculture for reef restoration requires accurate and continuous spawn
counting for resource distribution and larval health monitoring, but current
methods are labor-intensive and represent a critical bottleneck in the coral
production pipeline. We propose the Coral Spawn and Larvae Imaging Camera
System (CSLICS), which uses low cost modular cameras and object detectors
trained using human-in-the-loop labeling approaches for automated spawn
counting in larval rearing tanks. This paper details the system engineering,
dataset collection, and computer vision techniques to detect, classify and
count coral spawn. Experimental results from mass spawning events demonstrate
an F1 score of 82.4\% for surface spawn detection at different embryogenesis
stages, 65.3\% F1 score for sub-surface spawn detection, and a saving of 5,720
hours of labor per spawning event compared to manual sampling methods at the
same frequency. Comparison of manual counts with CSLICS monitoring during a
mass coral spawning event on the Great Barrier Reef demonstrates CSLICS'
accurate measurement of fertilization success and sub-surface spawn counts.
These findings enhance the coral aquaculture process and enable upscaling of
coral reef restoration efforts to address climate change threats facing
ecosystems like the Great Barrier Reef.

</details>


### [44] [Pose Estimation of a Cable-Driven Serpentine Manipulator Utilizing Intrinsic Dynamics via Physical Reservoir Computing](https://arxiv.org/abs/2509.17308)
*Kazutoshi Tanaka,Tomoya Takahashi,Masashi Hamaya*

Main category: cs.RO

TL;DR: 提出了一种基于物理储层计算的位姿估计方法，用于解决轻量化缆驱蛇形机械臂的柔性变形问题，相比传统方法显著提高了精度。


<details>
  <summary>Details</summary>
Motivation: 缆驱蛇形机械臂在非结构化环境中具有避障、多向施力和轻量化等优势，但轻量化设计引入了电缆松弛、伸长和连杆变形等柔性变化，导致解析预测与实际位姿存在差异，位姿估计变得困难。

Method: 开发了9自由度缆驱蛇形机械臂（臂长545mm，总质量308g），并提出基于物理储层计算的位姿估计方法，利用机械臂固有的非线性动力学作为高维储层。

Result: 实验结果显示，该方法平均位姿误差为4.3mm，优于LSTM网络的4.4mm和解析方法的39.5mm。

Conclusion: 这项工作为利用轻量化缆驱蛇形机械臂固有动力学的控制和感知策略提供了新方向。

Abstract: Cable-driven serpentine manipulators hold great potential in unstructured
environments, offering obstacle avoidance, multi-directional force application,
and a lightweight design. By placing all motors and sensors at the base and
employing plastic links, we can further reduce the arm's weight. To demonstrate
this concept, we developed a 9-degree-of-freedom cable-driven serpentine
manipulator with an arm length of 545 mm and a total mass of only 308 g.
However, this design introduces flexibility-induced variations, such as cable
slack, elongation, and link deformation. These variations result in
discrepancies between analytical predictions and actual link positions, making
pose estimation more challenging. To address this challenge, we propose a
physical reservoir computing based pose estimation method that exploits the
manipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir.
Experimental results show a mean pose error of 4.3 mm using our method,
compared to 4.4 mm with a baseline long short-term memory network and 39.5 mm
with an analytical approach. This work provides a new direction for control and
perception strategies in lightweight cable-driven serpentine manipulators
leveraging their intrinsic dynamics.

</details>


### [45] [OpenGVL - Benchmarking Visual Temporal Progress for Data Curation](https://arxiv.org/abs/2509.17321)
*Paweł Budzianowski,Emilia Wiśnios,Gracjan Góral,Igor Kulakov,Viktor Petrenko,Krzysztof Walas*

Main category: cs.RO

TL;DR: OpenGVL是一个用于评估任务进度预测的基准测试，专注于机器人操作任务，旨在解决机器人领域数据稀缺问题，通过利用开源基础模型进行大规模数据自动标注和筛选。


<details>
  <summary>Details</summary>
Motivation: 机器人领域面临数据稀缺的挑战，而野外可用的机器人数据正在指数级增长。可靠的时间任务完成预测可以帮助自动标注和整理这些大规模数据。

Method: 基于生成价值学习(GVL)方法，利用视觉语言模型(VLMs)的知识从视觉观察中预测任务进度。OpenGVL提供了一个全面的基准测试，评估开源基础模型在多样化挑战性操作任务中的表现。

Result: 开源模型家族在时间进度预测任务上的表现显著低于闭源模型，仅达到闭源模型约70%的性能。OpenGVL被证明可以作为自动化数据整理和筛选的实用工具。

Conclusion: OpenGVL基准测试的发布为大规模机器人数据集的质量评估提供了有效工具，同时揭示了开源模型在任务进度预测方面与闭源模型存在的性能差距。

Abstract: Data scarcity remains one of the most limiting factors in driving progress in
robotics. However, the amount of available robotics data in the wild is growing
exponentially, creating new opportunities for large-scale data utilization.
Reliable temporal task completion prediction could help automatically annotate
and curate this data at scale. The Generative Value Learning (GVL) approach was
recently proposed, leveraging the knowledge embedded in vision-language models
(VLMs) to predict task progress from visual observations. Building upon GVL, we
propose OpenGVL, a comprehensive benchmark for estimating task progress across
diverse challenging manipulation tasks involving both robotic and human
embodiments. We evaluate the capabilities of publicly available open-source
foundation models, showing that open-source model families significantly
underperform closed-source counterparts, achieving only approximately $70\%$ of
their performance on temporal progress prediction tasks. Furthermore, we
demonstrate how OpenGVL can serve as a practical tool for automated data
curation and filtering, enabling efficient quality assessment of large-scale
robotics datasets. We release the benchmark along with the complete codebase at
\href{github.com/budzianowski/opengvl}{OpenGVL}.

</details>


### [46] [AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation](https://arxiv.org/abs/2509.17340)
*Xin Chen,Rui Huang,Longbin Tang,Lin Zhao*

Main category: cs.RO

TL;DR: AERO-MPPI是一个完全GPU加速的无人机导航框架，通过锚点引导的MPPI优化器集合统一感知和规划，在复杂3D环境中实现实时敏捷无地图导航。


<details>
  <summary>Details</summary>
Motivation: 传统的地图-规划-控制流水线计算成本高且会传播估计误差，在复杂3D环境中的无地图导航面临重大挑战。

Method: 设计多分辨率LiDAR点云表示快速提取空间分布的锚点作为前瞻中间端点，构建多项式轨迹引导探索不同同伦路径类，在每个规划步骤并行运行多个MPPI实例并使用两阶段多目标成本进行评估。

Result: 在森林、垂直和倾斜环境中的广泛仿真显示持续可靠飞行速度超过7m/s，成功率超过80%，轨迹比最先进基线更平滑。真实世界实验证实AERO-MPPI在板载实时运行，在复杂杂乱环境中实现安全、敏捷和鲁棒的飞行。

Conclusion: AERO-MPPI框架通过GPU加速和并行MPPI优化器有效解决了单MPPI方法的局部极小值问题，实现了复杂3D环境中的实时可靠无人机导航。

Abstract: Agile mapless navigation in cluttered 3D environments poses significant
challenges for autonomous drones. Conventional mapping-planning-control
pipelines incur high computational cost and propagate estimation errors. We
present AERO-MPPI, a fully GPU-accelerated framework that unifies perception
and planning through an anchor-guided ensemble of Model Predictive Path
Integral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR
point-cloud representation that rapidly extracts spatially distributed
"anchors" as look-ahead intermediate endpoints, from which we construct
polynomial trajectory guides to explore distinct homotopy path classes. At each
planning step, we run multiple MPPI instances in parallel and evaluate them
with a two-stage multi-objective cost that balances collision avoidance and
goal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI
achieves real-time onboard operation and mitigates the local-minima failures of
single-MPPI approaches. Extensive simulations in forests, verticals, and
inclines demonstrate sustained reliable flight above 7 m/s, with success rates
above 80% and smoother trajectories compared to state-of-the-art baselines.
Real-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX
16G confirm that AERO-MPPI runs in real time onboard and consistently achieves
safe, agile, and robust flight in complex cluttered environments. The code will
be open-sourced upon acceptance of the paper.

</details>


### [47] [DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception](https://arxiv.org/abs/2509.17350)
*Haoran Zhou,Yangwei You,Shuaijun Wang*

Main category: cs.RO

TL;DR: DyDexHandover是一个基于多智能体强化学习的RGB视觉框架，用于实现双手机器人的空中物体抛接任务，无需依赖动力学模型或深度感知，能够生成类人化的自然运动。


<details>
  <summary>Details</summary>
Motivation: 解决双手机器人空中交接物体的基本挑战，克服现有方法依赖动力学模型、强先验或深度感知的限制，提高泛化能力和运动自然性。

Method: 采用多智能体强化学习训练端到端的RGB视觉策略，通过人类策略正则化方案引导抛掷策略，鼓励流畅自然的运动，并在Isaac Sim中构建双臂仿真环境进行实验评估。

Result: 在训练物体上达到近99%的成功率，在未见物体上达到75%的成功率，同时生成类人化的抛接行为，是首个仅使用原始RGB感知实现双手机器人空中交接的方法。

Conclusion: DyDexHandover框架成功实现了基于RGB视觉的双手机器人空中物体抛接，具有高成功率和类人化运动特性，为机器人动态操作任务提供了新的解决方案。

Abstract: Dynamic in air handover is a fundamental challenge for dual-arm robots,
requiring accurate perception, precise coordination, and natural motion. Prior
methods often rely on dynamics models, strong priors, or depth sensing,
limiting generalization and naturalness. We present DyDexHandover, a novel
framework that employs multi-agent reinforcement learning to train an end to
end RGB based policy for bimanual object throwing and catching. To achieve more
human-like behavior, the throwing policy is guided by a human policy
regularization scheme, encouraging fluid and natural motion, and enhancing the
generalization capability of the policy. A dual arm simulation environment was
built in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly
99 percent success on training objects and 75 percent on unseen objects, while
generating human-like throwing and catching behaviors. To our knowledge, it is
the first method to realize dual-arm in-air handover using only raw RGB
perception.

</details>


### [48] [Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators](https://arxiv.org/abs/2509.17381)
*Yongliang Wang,Hamidreza Kasaei*

Main category: cs.RO

TL;DR: 提出了一种结合视觉任务空间路径规划和强化学习关节空间避障的快速轨迹规划系统，通过改进PPO算法提高机器人在复杂环境中的避障效率和目标到达精度。


<details>
  <summary>Details</summary>
Motivation: 在非结构化和杂乱环境中为机械臂生成无碰撞轨迹仍然是一个重大挑战，现有方法需要额外计算来求解运动学或动力学方程。本文强调无模型强化学习方法在关节空间避障轨迹规划中的潜力。

Method: 框架分为两个关键组件：1）基于视觉的任务空间轨迹规划器，结合大规模快速分割模型和B样条优化的运动学路径搜索；2）改进的PPO算法，集成动作集成和策略反馈，提高关节空间中的目标到达精度和避障稳定性。

Result: 实验结果表明PPO增强方法的有效性，以及仿真到仿真和仿真到现实的迁移能力，提高了模型在复杂场景中的鲁棒性和规划器效率，使机器人能够在有障碍环境中进行实时避障和轨迹规划。

Conclusion: 提出的系统成功实现了机械臂在复杂环境中的快速无碰撞轨迹规划，通过视觉路径规划和强化学习避障的结合，显著提高了规划效率和机器人执行能力。

Abstract: Generating obstacle-free trajectories for robotic manipulators in
unstructured and cluttered environments remains a significant challenge.
Existing motion planning methods often require additional computational effort
to generate the final trajectory by solving kinematic or dynamic equations.
This paper highlights the strong potential of model-free reinforcement learning
methods over model-based approaches for obstacle-free trajectory planning in
joint space. We propose a fast trajectory planning system for manipulators that
combines vision-based path planning in task space with reinforcement
learning-based obstacle avoidance in joint space. We divide the framework into
two key components. The first introduces an innovative vision-based trajectory
planner in task space, leveraging the large-scale fast segment anything (FSA)
model in conjunction with basis spline (B-spline)-optimized kinodynamic path
searching. The second component enhances the proximal policy optimization (PPO)
algorithm by integrating action ensembles (AE) and policy feedback (PF), which
greatly improve precision and stability in goal-reaching and obstacle avoidance
within the joint space. These PPO enhancements increase the algorithm's
adaptability across diverse robotic tasks, ensuring consistent execution of
commands from the first component by the manipulator, while also enhancing both
obstacle avoidance efficiency and reaching accuracy. The experimental results
demonstrate the effectiveness of PPO enhancements, as well as
simulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real)
transfer, in improving model robustness and planner efficiency in complex
scenarios. These enhancements allow the robot to perform obstacle avoidance and
real-time trajectory planning in obstructed environments. Project page
available at: https://sites.google.com/view/ftp4rm/home

</details>


### [49] [High-Precision and High-Efficiency Trajectory Tracking for Excavators Based on Closed-Loop Dynamics](https://arxiv.org/abs/2509.17387)
*Ziqing Zou,Cong Wang,Yue Hu,Xiao Liu,Bowen Xu,Rong Xiong,Changjie Fan,Yingfeng Chen,Yue Wang*

Main category: cs.RO

TL;DR: EfficientTrack是一种轨迹跟踪方法，结合基于模型的学习来处理液压挖掘机的非线性动力学问题，利用闭环动力学提高学习效率，最小化跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 液压挖掘机的复杂非线性动力学（如时间延迟和控制耦合）给高精度轨迹跟踪带来挑战，传统控制方法难以有效处理这些非线性特性，而常用学习型方法需要大量环境交互导致效率低下。

Method: 提出EfficientTrack方法，集成基于模型的学习来管理非线性动力学，并利用闭环动力学提高学习效率。

Result: 仿真实验表明该方法优于现有学习型方法，以最少的交互次数实现最高跟踪精度和平滑度；真实世界实验显示该方法在负载条件下仍有效，并具备持续学习能力。

Conclusion: EfficientTrack方法在液压挖掘机轨迹跟踪中具有实际应用价值，能够有效处理非线性动力学问题并提高学习效率。

Abstract: The complex nonlinear dynamics of hydraulic excavators, such as time delays
and control coupling, pose significant challenges to achieving high-precision
trajectory tracking. Traditional control methods often fall short in such
applications due to their inability to effectively handle these nonlinearities,
while commonly used learning-based methods require extensive interactions with
the environment, leading to inefficiency. To address these issues, we introduce
EfficientTrack, a trajectory tracking method that integrates model-based
learning to manage nonlinear dynamics and leverages closed-loop dynamics to
improve learning efficiency, ultimately minimizing tracking errors. We validate
our method through comprehensive experiments both in simulation and on a
real-world excavator. Comparative experiments in simulation demonstrate that
our method outperforms existing learning-based approaches, achieving the
highest tracking precision and smoothness with the fewest interactions.
Real-world experiments further show that our method remains effective under
load conditions and possesses the ability for continual learning, highlighting
its practical applicability. For implementation details and source code, please
refer to https://github.com/ZiqingZou/EfficientTrack.

</details>


### [50] [3D Printable Soft Liquid Metal Sensors for Delicate Manipulation Tasks](https://arxiv.org/abs/2509.17389)
*Lois Liow,Jonty Milford,Emre Uygun,Andre Farinha,Vinoth Viswanathan,Josh Pinskier,David Howard*

Main category: cs.RO

TL;DR: 本文提出了一种打印高灵敏度软体'物理孪生体'的新方法，用于生态保护中的精细操作任务，特别是珊瑚处理。该方法通过3D扫描创建可定制的软体传感结构，替代活体珊瑚实验。


<details>
  <summary>Details</summary>
Motivation: 生态保护工作需要与脆弱样本交互而不损坏它们，同时需要安全获取数据来训练操作策略。传统方法难以满足对精细操作的传感需求。

Method: 开发了自动化设计工作流程，从3D扫描或模型创建复杂的可定制3D软体传感结构，使用软体液态金属传感器精确再现自然几何形状。

Result: 实验显示传感珊瑚能够检测低于0.5N的抓取力，有效捕捉珊瑚处理所需的精细交互和轻微接触力。在自动珊瑚标记和机器人珊瑚养殖两个演示中表现出色。

Conclusion: 这种物理孪生体比传统传感器提供更丰富的抓取反馈，为处理脆弱物品提供了伦理和可扩展的途径，可在部署前进行实验验证。

Abstract: Robotics and automation are key enablers to increase throughput in ongoing
conservation efforts across various threatened ecosystems. Cataloguing,
digitisation, husbandry, and similar activities require the ability to interact
with delicate, fragile samples without damaging them. Additionally,
learning-based solutions to these tasks require the ability to safely acquire
data to train manipulation policies through, e.g., reinforcement learning. To
address these twin needs, we introduce a novel method to print free-form,
highly sensorised soft 'physical twins'. We present an automated design
workflow to create complex and customisable 3D soft sensing structures on
demand from 3D scans or models. Compared to the state of the art, our soft
liquid metal sensors faithfully recreate complex natural geometries and display
excellent sensing properties suitable for validating performance in delicate
manipulation tasks. We demonstrate the application of our physical twins as
'sensing corals': high-fidelity, 3D printed replicas of scanned corals that
eliminate the need for live coral experimentation, whilst increasing data
quality, offering an ethical and scalable pathway for advancing autonomous
coral handling and soft manipulation broadly. Through extensive bench-top
manipulation and underwater grasping experiments, we show that our sensing
coral is able to detect grasps under 0.5 N, effectively capturing the delicate
interactions and light contact forces required for coral handling. Finally, we
showcase the value of our physical twins across two demonstrations: (i)
automated coral labelling for lab identification and (ii) robotic coral
aquaculture. Sensing physical twins such as ours can provide richer grasping
feedback than conventional sensors providing experimental validation of prior
to deployment in handling fragile and delicate items.

</details>


### [51] [FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR](https://arxiv.org/abs/2509.17390)
*Junzhe Wu,Yufei Jia,Yiyi Yan,Zhixing Chen,Tiao Tan,Zifan Wang,Guangyu Wang*

Main category: cs.RO

TL;DR: FGGS-LiDAR是一个将预训练的3D高斯泼溅模型转换为高保真水密网格的框架，无需激光雷达特定监督即可实现高性能激光雷达模拟


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然革新了真实感渲染，但其资产生态系统与高性能激光雷达模拟不兼容，而激光雷达模拟是机器人和自动驾驶的关键工具

Method: 通过体积离散化和截断符号距离场提取的通用流程转换3DGS模型，并配合高度优化的GPU加速光线投射模块

Result: 在室内外场景验证中展示了卓越的几何保真度，激光雷达模拟速度超过500FPS

Conclusion: 该框架使3DGS资产能够直接用于几何精确的深度感知，将其应用范围扩展到可视化之外，为可扩展的多模态模拟解锁新能力

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic
rendering, its vast ecosystem of assets remains incompatible with
high-performance LiDAR simulation, a critical tool for robotics and autonomous
driving. We present \textbf{FGGS-LiDAR}, a framework that bridges this gap with
a truly plug-and-play approach. Our method converts \textit{any} pretrained
3DGS model into a high-fidelity, watertight mesh without requiring
LiDAR-specific supervision or architectural alterations. This conversion is
achieved through a general pipeline of volumetric discretization and Truncated
Signed Distance Field (TSDF) extraction. We pair this with a highly optimized,
GPU-accelerated ray-casting module that simulates LiDAR returns at over 500
FPS. We validate our approach on indoor and outdoor scenes, demonstrating
exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for
geometrically accurate depth sensing, our framework extends their utility
beyond visualization and unlocks new capabilities for scalable, multimodal
simulation. Our open-source implementation is available at
https://github.com/TATP-233/FGGS-LiDAR.

</details>


### [52] [GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a Low-Cost RGB Camera](https://arxiv.org/abs/2509.17435)
*Xiaoyu Wang,Yan Rui Tan,William Leong,Sunan Huang,Rodney Teo,Cheng Xiang*

Main category: cs.RO

TL;DR: 提出了一种基于图像的视觉伺服框架，仅使用RGB相机实现无人机导航和避障，无需显式路径规划，通过AI单目深度估计实现避障，完全在Jetson平台上运行。


<details>
  <summary>Details</summary>
Motivation: 虽然无人机导航已被广泛研究，但在涉及多个视觉目标和避障的任务中应用基于图像的视觉伺服仍然具有挑战性。现有方法通常依赖立体相机或外部工作站，缺乏自包含的可部署系统。

Method: 使用基于图像的视觉伺服框架，通过AI单目深度估计从RGB图像中获取深度信息实现避障，无需显式路径规划，整个系统完全在Jetson平台上运行。

Result: 实验验证表明，无人机能够在GPS拒止环境中有效导航通过多个AprilTag并避开障碍物。

Conclusion: 该框架提供了一种自包含、可部署的解决方案，仅使用RGB相机即可实现无人机导航和避障，在GPS拒止环境中表现良好。

Abstract: This paper proposes an image-based visual servoing (IBVS) framework for UAV
navigation and collision avoidance using only an RGB camera. While UAV
navigation has been extensively studied, it remains challenging to apply IBVS
in missions involving multiple visual targets and collision avoidance. The
proposed method achieves navigation without explicit path planning, and
collision avoidance is realized through AI-based monocular depth estimation
from RGB images. Unlike approaches that rely on stereo cameras or external
workstations, our framework runs fully onboard a Jetson platform, ensuring a
self-contained and deployable system. Experimental results validate that the
UAV can navigate across multiple AprilTags and avoid obstacles effectively in
GPS-denied environments.

</details>


### [53] [Learning Dexterous Manipulation with Quantized Hand State](https://arxiv.org/abs/2509.17450)
*Ying Feng,Hongjie Fang,Yinong He,Jingjing Chen,Chenxi Wang,Zihao He,Ruonan Liu,Cewu Lu*

Main category: cs.RO

TL;DR: DQ-RISE提出了一种新的灵巧操作策略，通过量化手部状态来简化动作空间，同时保持臂-手协调学习能力


<details>
  <summary>Details</summary>
Motivation: 现有的视觉运动策略将手臂和手部动作表示在单一组合空间中，导致高维手部动作主导耦合动作空间，影响手臂控制精度

Method: 量化手部状态以简化手部运动预测，同时应用连续松弛方法使手臂动作能够与这些紧凑的手部状态联合扩散

Result: 实验表明DQ-RISE实现了更平衡和高效的学习，为结构化和可泛化的灵巧操作铺平了道路

Conclusion: DQ-RISE通过分离和量化手部状态的方法，有效解决了灵巧操作中臂-手协调控制的挑战

Abstract: Dexterous robotic hands enable robots to perform complex manipulations that
require fine-grained control and adaptability. Achieving such manipulation is
challenging because the high degrees of freedom tightly couple hand and arm
motions, making learning and control difficult. Successful dexterous
manipulation relies not only on precise hand motions, but also on accurate
spatial positioning of the arm and coordinated arm-hand dynamics. However, most
existing visuomotor policies represent arm and hand actions in a single
combined space, which often causes high-dimensional hand actions to dominate
the coupled action space and compromise arm control. To address this, we
propose DQ-RISE, which quantizes hand states to simplify hand motion prediction
while preserving essential patterns, and applies a continuous relaxation that
allows arm actions to diffuse jointly with these compact hand states. This
design enables the policy to learn arm-hand coordination from data while
preventing hand actions from overwhelming the action space. Experiments show
that DQ-RISE achieves more balanced and efficient learning, paving the way
toward structured and generalizable dexterous manipulation. Project website:
http://rise-policy.github.io/DQ-RISE/

</details>


### [54] [Morphologies of a sagging elastica with intrinsic sensing and actuation](https://arxiv.org/abs/2509.17572)
*Vishnu Deo Mishra,S Ganga Prasath*

Main category: cs.RO

TL;DR: 该论文研究软体机器人的形态控制，通过简单的比例反馈策略（驱动力矩与感知曲率成正比）来补偿自重引起的下垂，分析传感器和致动器数量限制对形态控制的影响。


<details>
  <summary>Details</summary>
Motivation: 软体机器人通过传感器感知形状并通过致动器施加力矩来改变形态，但由于几何非线性、建模误差以及传感/致动能力限制，计算所需的驱动力矩非常困难。

Method: 将软体机器人建模为弹性杆，使用比例反馈策略，考虑有限传感器和致动器的影响（通过指定宽度的滤波器模拟），研究在补偿自重下垂任务中的形态不稳定性。

Result: 发现形态不稳定性在相空间（重力弯曲数、无量纲传感/反馈增益、缩放滤波器宽度）中呈现层次结构。在复杂形态任务中，存在长波和短波特征捕获之间的权衡，当选择适当的驱动增益时，形态误差最小。

Conclusion: 该模型为研究和设计具有有限传感和致动能力的细长软体设备提供了定量分析框架，特别适用于复杂机动应用。

Abstract: The morphology of a slender soft-robot can be modified by sensing its shape
via sensors and exerting moments via actuators embedded along its body. The
actuating moments required to morph these soft-robots to a desired shape are
often difficult to compute due to the geometric non-linearity associated with
the structure, the errors in modeling the experimental system, and the
limitations in sensing and feedback/actuation capabilities. In this article, we
explore the effect of a simple feedback strategy (actuation being proportional
to the sensed curvature) on the shape of a soft-robot, modeled as an elastica.
The finite number of sensors and actuators, often seen in experiments, is
captured in the model via filters of specified widths. Using proportional
feedback, we study the simple task of straightening the device by compensating
for the sagging introduced by its self-weight. The device undergoes a hierarchy
of morphological instabilities defined in the phase-space given by the
gravito-bending number, non-dimensional sensing/feedback gain, and the scaled
width of the filter. For complex shape-morphing tasks, given a perfect model of
the device with limited sensing and actuating capabilities, we find that a
trade-off arises (set by the sensor spacing & actuator size) between capturing
the long and short wavelength features. We show that the error in
shape-morphing is minimal for a fixed filter width when we choose an
appropriate actuating gain (whose magnitude goes as a square of the filter
width). Our model provides a quantitative lens to study and design slender soft
devices with limited sensing and actuating capabilities for complex maneuvering
applications.

</details>


### [55] [GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation Skills on Legged Robots](https://arxiv.org/abs/2509.17582)
*Vassil Atanassov,Wanming Yu,Siddhant Gangapurwala,James Wilson,Ioannis Havoutis*

Main category: cs.RO

TL;DR: GeCCo是一个基于深度强化学习的通用接触条件策略，能够跟踪四足机器人上的任意接触点，避免了为每个新任务重新训练控制器的需求。


<details>
  <summary>Details</summary>
Motivation: 传统端到端深度强化学习方法需要为每个新问题重新定义和调整奖励函数，耗时且难以扩展，因此需要一种通用的低层控制器来支持多种高层任务。

Method: 使用深度强化学习训练一个通用接触条件策略（GeCCo），该策略能够跟踪任意接触点，通过模块化设计将高层接触规划器与预训练的低层策略结合。

Result: 在多种步态、复杂地形（楼梯、斜坡、狭窄横梁）和物体交互任务中表现出良好的可扩展性和鲁棒性，能够高效获取新行为。

Conclusion: GeCCo提供了一个通用且模块化的低层控制器框架，显著提高了四足机器人学习新任务的效率，无需重新训练控制器。

Abstract: Most modern approaches to quadruped locomotion focus on using Deep
Reinforcement Learning (DRL) to learn policies from scratch, in an end-to-end
manner. Such methods often fail to scale, as every new problem or application
requires time-consuming and iterative reward definition and tuning. We present
Generalist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained
with Deep Reinforcement Learning that is capable of tracking arbitrary contact
points on a quadruped robot. The strength of our approach is that it provides a
general and modular low-level controller that can be reused for a wider range
of high-level tasks, without the need to re-train new controllers from scratch.
We demonstrate the scalability and robustness of our method by evaluating on a
wide range of locomotion and manipulation tasks in a common framework and under
a single generalist policy. These include a variety of gaits, traversing
complex terrains (eg. stairs and slopes) as well as previously unseen
stepping-stones and narrow beams, and interacting with objects (eg. pushing
buttons, tracking trajectories). Our framework acquires new behaviors more
efficiently, simply by combining a task-specific high-level contact planner and
the pre-trained generalist policy. A supplementary video can be found at
https://youtu.be/o8Dd44MkG2E.

</details>


### [56] [Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery](https://arxiv.org/abs/2509.17666)
*Mimo Shirasaka,Cristian C. Beltran-Hernandez,Masashi Hamaya,Yoshitaka Ushiku*

Main category: cs.RO

TL;DR: 提出一种基于被动柔顺软腕的鲁棒物体插入方法，通过柔顺性实现安全接触吸收和自动故障恢复，无需高频控制或力传感。


<details>
  <summary>Details</summary>
Motivation: 传统物体插入任务在姿态不确定性和环境变化下容易失败，需要手动调优或控制器重训练，需要一种更鲁棒和弹性的解决方案。

Method: 使用被动柔顺软腕实现安全接触吸收，将插入过程构建为柔顺性支持的接触形成序列，集成自动故障恢复策略，并利用预训练视觉语言模型评估技能执行、识别故障模式并提出恢复动作。

Result: 在仿真中达到83%的成功率，能够从随机化条件下的故障中恢复，包括5度抓取偏差、20mm孔位误差、5倍摩擦力增加以及未见过的方形/矩形插头。

Conclusion: 柔顺性支持的故障恢复方法显著提高了物体插入任务的鲁棒性和弹性，在仿真和真实机器人上都验证了有效性。

Abstract: Object insertion tasks are prone to failures under pose uncertainties and
environmental variations, traditionally requiring manual finetuning or
controller retraining. We present a novel approach for robust and resilient
object insertion using a passively compliant soft wrist that enables safe
contact absorption through large deformations, without high-frequency control
or force sensing. Our method structures insertion as compliance-enabled contact
formations, sequential contact states that progressively constrain degrees of
freedom, and integrates automated failure recovery strategies. Our key insight
is that wrist compliance permits safe, repeated recovery attempts; hence, we
refer to it as compliance-enabled failure recovery. We employ a pre-trained
vision-language model (VLM) that assesses each skill execution from terminal
poses and images, identifies failure modes, and proposes recovery actions by
selecting skills and updating goals. In simulation, our method achieved an 83%
success rate, recovering from failures induced by randomized
conditions--including grasp misalignments up to 5 degrees, hole-pose errors up
to 20mm, fivefold increases in friction, and previously unseen
square/rectangular pegs--and we further validate the approach on a real robot.

</details>


### [57] [Towards Learning Boulder Excavation with Hydraulic Excavators](https://arxiv.org/abs/2509.17683)
*Jonas Gruetter,Lorenzo Terenzi,Pascal Egli,Marco Hutter*

Main category: cs.RO

TL;DR: 该论文提出了一种使用强化学习让标准挖掘机铲斗自主提取大型不规则岩石的方法，能够在恶劣的室外环境中处理稀疏感知数据，并适应不同的土壤条件。


<details>
  <summary>Details</summary>
Motivation: 建筑工地需要移除大型岩石才能进行挖掘或平整作业，但现有自主挖掘方法要么只能处理连续介质（土壤、砾石），要么需要使用专门的抓取器进行详细的几何规划，这些方法要么无法处理大型不规则岩石，要么需要不切实际的工具更换，中断工作流程。

Method: 在模拟环境中使用刚体动力学和解析土壤模型训练强化学习策略，该策略处理来自视觉分割的稀疏LiDAR点（每个岩石仅20个点）和本体感受反馈，以控制标准挖掘机铲斗。

Result: 在12吨挖掘机上的现场测试显示，在各种岩石（0.4-0.7米）和土壤类型中取得了70%的成功率，而人类操作员的成功率为83%。

Conclusion: 研究表明，标准建筑设备可以在稀疏感知和具有挑战性的室外条件下学习复杂的操作技能。

Abstract: Construction sites frequently require removing large rocks before excavation
or grading can proceed. Human operators typically extract these boulders using
only standard digging buckets, avoiding time-consuming tool changes to
specialized grippers. This task demands manipulating irregular objects with
unknown geometries in harsh outdoor environments where dust, variable lighting,
and occlusions hinder perception. The excavator must adapt to varying soil
resistance--dragging along hard-packed surfaces or penetrating soft
ground--while coordinating multiple hydraulic joints to secure rocks using a
shovel. Current autonomous excavation focuses on continuous media (soil,
gravel) or uses specialized grippers with detailed geometric planning for
discrete objects. These approaches either cannot handle large irregular rocks
or require impractical tool changes that interrupt workflow. We train a
reinforcement learning policy in simulation using rigid-body dynamics and
analytical soil models. The policy processes sparse LiDAR points (just 20 per
rock) from vision-based segmentation and proprioceptive feedback to control
standard excavator buckets. The learned agent discovers different strategies
based on soil resistance: dragging along the surface in hard soil and
penetrating directly in soft conditions. Field tests on a 12-ton excavator
achieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to
83% for human operators. This demonstrates that standard construction equipment
can learn complex manipulation despite sparse perception and challenging
outdoor conditions.

</details>


### [58] [EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering](https://arxiv.org/abs/2509.17750)
*Inkyu Jang,Jonghae Park,Chams E. Mballo,Sihyun Cho,Claire J. Tomlin,H. Jin Kim*

Main category: cs.RO

TL;DR: EigenSafe是一个基于算子理论的框架，用于随机系统的学习型安全关键控制，通过主导特征对提供安全概率信息并构建安全过滤器。


<details>
  <summary>Details</summary>
Motivation: 在机器人系统中，由于感知噪声和环境干扰等因素，动力学通常建模为随机系统。传统方法如Hamilton-Jacobi可达性和控制屏障函数难以提供全面的安全度量。

Method: 推导了安全概率动态规划原理的线性算子，发现其主导特征对能提供状态和闭环系统的安全信息。EigenSafe框架离线学习主导特征对和安全备份策略，使用学习到的特征函数构建安全过滤器。

Result: 在三个模拟的随机安全关键控制任务中验证了该框架的有效性。

Conclusion: EigenSafe为随机系统提供了一种有效的学习型安全控制方法，能够检测潜在不安全情况并回退到备份策略。

Abstract: We present EigenSafe, an operator-theoretic framework for learning-enabled
safety-critical control for stochastic systems. In many robotic systems where
dynamics are best modeled as stochastic systems due to factors such as sensing
noise and environmental disturbances, it is challenging for conventional
methods such as Hamilton-Jacobi reachability and control barrier functions to
provide a holistic measure of safety. We derive a linear operator governing the
dynamic programming principle for safety probability, and find that its
dominant eigenpair provides information about safety for both individual states
and the overall closed-loop system. The proposed learning framework, called
EigenSafe, jointly learns this dominant eigenpair and a safe backup policy in
an offline manner. The learned eigenfunction is then used to construct a safety
filter that detects potentially unsafe situations and falls back to the backup
policy. The framework is validated in three simulated stochastic
safety-critical control tasks.

</details>


### [59] [MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies](https://arxiv.org/abs/2509.17759)
*Chengbo Yuan,Rui Zhou,Mengzhen Liu,Yingdong Hu,Shengjie Wang,Li Yi,Chuan Wen,Shanghang Zhang,Yang Gao*

Main category: cs.RO

TL;DR: MotionTrans是一个通过人类-机器人协同训练框架，直接从人类数据中学习运动知识并迁移到机器人策略的系统，实现了13个任务的零样本运动迁移，其中9个任务取得了显著成功率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模仿学习中真实数据稀缺的问题，探索如何利用人类丰富的操作行为数据来直接学习新的运动技能，而不仅仅是提高鲁棒性和训练效率。

Method: 提出MotionTrans框架，包括数据收集系统、人类数据转换管道和加权协同训练策略，通过同时训练30个人类-机器人任务来实现运动知识的迁移。

Result: 成功将13个任务的运动从人类数据直接迁移到可部署的端到端机器人策略，9个任务实现零样本非平凡成功率，预训练-微调性能提升40%。

Conclusion: 通过消融研究确定了成功运动学习的关键因素：与机器人数据协同训练和广泛的任务相关运动覆盖，揭示了从人类数据中进行运动级学习的潜力。

Abstract: Scaling real robot data is a key bottleneck in imitation learning, leading to
the use of auxiliary data for policy training. While other aspects of robotic
manipulation such as image or language understanding may be learned from
internet-based datasets, acquiring motion knowledge remains challenging. Human
data, with its rich diversity of manipulation behaviors, offers a valuable
resource for this purpose. While previous works show that using human data can
bring benefits, such as improving robustness and training efficiency, it
remains unclear whether it can realize its greatest advantage: enabling robot
policies to directly learn new motions for task completion. In this paper, we
systematically explore this potential through multi-task human-robot
cotraining. We introduce MotionTrans, a framework that includes a data
collection system, a human data transformation pipeline, and a weighted
cotraining strategy. By cotraining 30 human-robot tasks simultaneously, we
direcly transfer motions of 13 tasks from human data to deployable end-to-end
robot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot
manner. MotionTrans also significantly enhances pretraining-finetuning
performance (+40% success rate). Through ablation study, we also identify key
factors for successful motion learning: cotraining with robot data and broad
task-related motion coverage. These findings unlock the potential of
motion-level learning from human data, offering insights into its effective use
for training robotic manipulation policies. All data, code, and model weights
are open-sourced https://motiontrans.github.io/.

</details>


### [60] [Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research](https://arxiv.org/abs/2509.17760)
*Austin Wilson,Sahar Kapasi,Zane Greene,Alexis E. Block*

Main category: cs.RO

TL;DR: 提出了增强版NAO机器人，通过升级麦克风、RGB-D和热成像相机以及计算资源，为过时的机器人平台提供现代化感知和交互能力，显著提升了对话质量和用户偏好。


<details>
  <summary>Details</summary>
Motivation: 许多研究团队面临传统机器人平台因制造商停止支持而无法适应现代感知、语音和交互能力的挑战，需要一种方法来延长这些机器人的使用寿命和研究价值。

Method: 开发增强版NAO系统，集成云和本地模型进行感知和对话，保留NAO的表达性身体和行为，同时升级硬件包括波束成形麦克风、RGB-D相机、热成像相机和额外计算资源。

Result: 在验证研究中，增强版NAO相比NAO AI版显著提高了对话质量和用户偏好，同时没有增加响应延迟，改善了多参与者分离并减少了自听等伪影。

Conclusion: 该框架为延长传统机器人的寿命和研究效用提供了平台无关的策略，确保它们继续成为人机交互的有价值工具。

Abstract: Many research groups face challenges when legacy (unsupported) robotic
platforms lose manufacturer support and cannot accommodate modern sensing,
speech, and interaction capabilities. We present the Enhanced NAO, a
revitalized version of Aldebaran's NAO robot that uses upgraded microphones,
RGB-D and thermal cameras, and additional compute resources in a fully
self-contained package. This system combines cloud and local models for
perception and dialogue, while preserving the NAO's expressive body and
behaviors. In a pilot validation study, the Enhanced NAO delivered
significantly higher conversational quality and stronger user preference
compared to the NAO AI Edition, without increasing response latency. Key
upgrades, such as beamforming microphones and low-latency audio processing,
reduced artifacts like self-hearing and improved multi-party separation.
Expanded visual and thermal sensing established a foundation for future
interaction capabilities. Beyond the NAO, our framework provides a
platform-agnostic strategy for extending the lifespan and research utility of
legacy robots, ensuring they remain valuable tools for human-robot interaction.

</details>


### [61] [RoboSeek: You Need to Interact with Your Objects](https://arxiv.org/abs/2509.17783)
*Yibo Peng,Jiahao Yang,Shenhao Yan,Ziyu Huang,Shuang Li,Shuguang Cui,Yiming Zhao,Yatong Han*

Main category: cs.RO

TL;DR: RoboSeek是一个基于具身认知理论的机器人操作框架，通过仿真训练和real2sim2real迁移实现长时程任务的鲁棒执行，在8个复杂操作任务中平均成功率79%，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于交互的机器人学习方法在实际应用中仍探索不足，特别是对于需要顺序决策、物理约束和感知不确定性的长时程任务存在重大挑战。受具身认知理论启发，作者希望通过交互经验来优化动作执行。

Method: 首先通过3D重建在仿真中复现真实环境，然后利用视觉先验通过强化学习和交叉熵方法训练策略，最后通过real2sim2real迁移管道将学习到的策略部署到真实机器人平台执行。

Result: 在涉及顺序交互、工具使用和物体处理的8个长时程操作任务中，RoboSeek平均成功率达到79%，显著优于成功率低于50%的基线方法，验证了其跨任务和平台的泛化性和鲁棒性。

Conclusion: 实验结果表明该训练框架在复杂动态真实环境中的有效性，证明了所提real2sim2real迁移机制的稳定性，为更具泛化性的具身机器人学习铺平了道路。

Abstract: Optimizing and refining action execution through
  exploration and interaction is a promising way for robotic
  manipulation. However, practical approaches to interaction driven robotic
learning are still underexplored, particularly for
  long-horizon tasks where sequential decision-making, physical
  constraints, and perceptual uncertainties pose significant chal lenges.
Motivated by embodied cognition theory, we propose
  RoboSeek, a framework for embodied action execution that
  leverages interactive experience to accomplish manipulation
  tasks. RoboSeek optimizes prior knowledge from high-level
  perception models through closed-loop training in simulation
  and achieves robust real-world execution via a real2sim2real
  transfer pipeline. Specifically, we first replicate real-world
  environments in simulation using 3D reconstruction to provide
  visually and physically consistent environments., then we train
  policies in simulation using reinforcement learning and the
  cross-entropy method leveraging visual priors. The learned
  policies are subsequently deployed on real robotic platforms
  for execution. RoboSeek is hardware-agnostic and is evaluated
  on multiple robotic platforms across eight long-horizon ma nipulation tasks
involving sequential interactions, tool use, and
  object handling. Our approach achieves an average success rate
  of 79%, significantly outperforming baselines whose success
  rates remain below 50%, highlighting its generalization and
  robustness across tasks and platforms. Experimental results
  validate the effectiveness of our training framework in complex,
  dynamic real-world settings and demonstrate the stability of the
  proposed real2sim2real transfer mechanism, paving the way for
  more generalizable embodied robotic learning. Project Page:
  https://russderrick.github.io/Roboseek/

</details>


### [62] [Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation](https://arxiv.org/abs/2509.17812)
*Yitaek Kim,Casper Hewson Rask,Christoffer Sloth*

Main category: cs.RO

TL;DR: Tac2Motion是一个基于接触感知的强化学习框架，用于学习接触丰富的灵巧手操作任务，如打开盖子。通过触觉感知的奖励塑造和嵌入观察空间，提高了数据效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决接触丰富的灵巧手操作任务（如打开盖子）的学习挑战，传统方法在数据效率和鲁棒性方面存在不足。

Method: 提出触觉感知的奖励塑造方法，将触觉感知嵌入观察空间，设计奖励函数鼓励牢固抓握和流畅的手指步态。

Result: 在开盖场景中验证了框架的有效性，展示了策略对不同物体类型和动力学（如扭转摩擦）的泛化能力，并在Shadow Robot多指机器人上成功实现真实世界迁移。

Conclusion: Tac2Motion框架能够高效学习接触丰富的操作任务，并具有良好的泛化能力和真实世界适用性。

Abstract: This paper proposes Tac2Motion, a contact-aware reinforcement learning
framework to facilitate the learning of contact-rich in-hand manipulation
tasks, such as removing a lid. To this end, we propose tactile sensing-based
reward shaping and incorporate the sensing into the observation space through
embedding. The designed rewards encourage an agent to ensure firm grasping and
smooth finger gaiting at the same time, leading to higher data efficiency and
robust performance compared to the baseline. We verify the proposed framework
on the opening a lid scenario, showing generalization of the trained policy
into a couple of object types and various dynamics such as torsional friction.
Lastly, the learned policy is demonstrated on the multi-fingered robot, Shadow
Robot, showing that the control policy can be transferred to the real world.
The video is available: https://youtu.be/poeJBPR7urQ.

</details>


### [63] [SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model](https://arxiv.org/abs/2509.17850)
*Xiao Zhou,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: 提出SocialTraj框架，通过社会价值取向（SVO）整合社会心理学原理，改进自动驾驶中周围车辆轨迹预测的准确性和社会合规性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在高度动态和复杂的交通场景中难以有效捕捉驾驶员的多模态行为，导致预测轨迹与实际未来运动存在偏差。

Method: 使用贝叶斯逆强化学习估计周围车辆的SVO，将估计的SVO嵌入条件去噪扩散模型，并显式结合自车规划轨迹来增强交互建模。

Result: 在NGSIM和HighD数据集上的实验表明，SocialTraj能够适应高度动态和交互场景，生成社会合规且行为一致的轨迹预测，优于现有基线方法。

Conclusion: 动态SVO估计和显式自车规划组件显著提高了预测准确性并大幅减少了推理时间，证明了社会心理学原理在轨迹预测中的有效性。

Abstract: Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for
autonomous driving systems to avoid misguided decisions and potential
accidents. However, achieving reliable predictions in highly dynamic and
complex traffic scenarios remains a significant challenge. One of the key
impediments lies in the limited effectiveness of current approaches to capture
the multi-modal behaviors of drivers, which leads to predicted trajectories
that deviate from actual future motions. To address this issue, we propose
SocialTraj, a novel trajectory prediction framework integrating social
psychology principles through social value orientation (SVO). By utilizing
Bayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we
obtain the critical social context to infer the future interaction trend. To
ensure modal consistency in predicted behaviors, the estimated SVOs of SVs are
embedded into a conditional denoising diffusion model that aligns generated
trajectories with historical driving styles. Additionally, the planned future
trajectory of the ego vehicle (EV) is explicitly incorporated to enhance
interaction modeling. Extensive experiments on NGSIM and HighD datasets
demonstrate that SocialTraj is capable of adapting to highly dynamic and
interactive scenarios while generating socially compliant and behaviorally
consistent trajectory predictions, outperforming existing baselines. Ablation
studies demonstrate that dynamic SVO estimation and explicit ego-planning
components notably improve prediction accuracy and substantially reduce
inference time.

</details>


### [64] [Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection](https://arxiv.org/abs/2509.17877)
*Richard Kuhlmann,Jakob Wolfram,Boyang Sun,Jiaxu Xing,Davide Scaramuzza,Marc Pollefeys,Cesar Cadena*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的感知感知检查框架，将目标可见性作为主要目标，使机器人能够找到保证与目标视觉接触的最短轨迹，而无需依赖地图。


<details>
  <summary>Details</summary>
Motivation: 传统检查方法简化为导航任务，只关注到达预定位置，但实际检查中目标可能在到达精确坐标前就可见，进一步移动既冗余又低效。真正的检查关键是让机器人定位到能够观察目标的位置。

Method: 提出端到端强化学习框架，明确将目标可见性作为主要目标，利用感知和本体感知信息，在仿真中训练策略后部署到真实机器人。还开发了计算真实最短检查路径的算法用于评估。

Result: 通过大量实验证明，该方法在模拟和真实环境中都优于现有的经典和基于学习的导航方法，产生更高效的检查轨迹。

Conclusion: 从感知感知角度重新审视检查问题，提出的方法能够有效解决真实世界检查任务中的效率问题，为自主检查提供了新的解决方案。

Abstract: Autonomous inspection is a central problem in robotics, with applications
ranging from industrial monitoring to search-and-rescue. Traditionally,
inspection has often been reduced to navigation tasks, where the objective is
to reach a predefined location while avoiding obstacles. However, this
formulation captures only part of the real inspection problem. In real-world
environments, the inspection targets may become visible well before their exact
coordinates are reached, making further movement both redundant and
inefficient. What matters more for inspection is not simply arriving at the
target's position, but positioning the robot at a viewpoint from which the
target becomes observable. In this work, we revisit inspection from a
perception-aware perspective. We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map. The learned policy
leverages both perceptual and proprioceptive sensing and is trained entirely in
simulation, before being deployed to a real-world robot. We further develop an
algorithm to compute ground-truth shortest inspection paths, which provides a
reference for evaluation. Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings. The project is avialable at
https://sight-over-site.github.io/

</details>


### [65] [The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control](https://arxiv.org/abs/2509.17884)
*Arun L. Bishop,Juan Alvarez-Padilla,Sam Schoedel,Ibrahima Sory Sow,Juee Chandrachud,Sheitej Sharma,Will Kraus,Beomyeong Park,Robert J. Griffin,John M. Dolan,Zachary Manchester*

Main category: cs.RO

TL;DR: 本文展示了一种使用线性时不变近似的全身模型预测控制器，能够在复杂腿式机器人上执行基本运动任务，无需在线非线性动力学评估或矩阵求逆。


<details>
  <summary>Details</summary>
Motivation: 研究在何种情况下运动控制器需要考虑非线性问题，探索简化控制方法在复杂机器人上的可行性。

Method: 采用线性时不变近似来建模全身动力学，开发了无需在线非线性动力学评估或矩阵求逆的模型预测控制器。

Result: 在四足机器人上实现了行走、扰动抑制和无需单独步态规划器的目标位置导航，并在液压人形机器人上展示了动态行走能力。

Conclusion: 即使使用简化的线性模型，也能在具有显著肢体惯性、复杂执行器动力学和大仿真-现实差距的复杂机器人上实现有效的运动控制。

Abstract: When do locomotion controllers require reasoning about nonlinearities? In
this work, we show that a whole-body model-predictive controller using a simple
linear time-invariant approximation of the whole-body dynamics is able to
execute basic locomotion tasks on complex legged robots. The formulation
requires no online nonlinear dynamics evaluations or matrix inversions. We
demonstrate walking, disturbance rejection, and even navigation to a goal
position without a separate footstep planner on a quadrupedal robot. In
addition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with
significant limb inertia, complex actuator dynamics, and large sim-to-real gap.

</details>


### [66] [DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving](https://arxiv.org/abs/2509.17940)
*Shuyao Shang,Yuntao Chen,Yuqi Wang,Yingyan Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: DriveDPO是一个安全直接偏好优化策略学习框架，通过结合人类模仿相似性和基于规则的安全评分来优化自动驾驶策略，解决了现有方法在安全性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶方法虽然通过模仿学习取得了进展，但存在关键的安全限制，无法区分看似人类驾驶但潜在不安全的轨迹。一些最近的方法尝试通过回归多个规则驱动评分来解决这个问题，但监督与策略优化脱节，导致性能不佳。

Method: 首先从人类模仿相似性和基于规则的安全评分中提炼统一的策略分布用于直接策略优化。然后引入迭代的直接偏好优化阶段，将其制定为轨迹级别的偏好对齐。

Result: 在NAVSIM基准测试上的广泛实验表明，DriveDPO实现了90.0的PDMS新最先进水平。

Conclusion: DriveDPO能够产生更安全和更可靠的驾驶行为，在多样化的挑战性场景中表现出色。

Abstract: End-to-end autonomous driving has substantially progressed by directly
predicting future trajectories from raw perception inputs, which bypasses
traditional modular pipelines. However, mainstream methods trained via
imitation learning suffer from critical safety limitations, as they fail to
distinguish between trajectories that appear human-like but are potentially
unsafe. Some recent approaches attempt to address this by regressing multiple
rule-driven scores but decoupling supervision from policy optimization,
resulting in suboptimal performance. To tackle these challenges, we propose
DriveDPO, a Safety Direct Preference Optimization Policy Learning framework.
First, we distill a unified policy distribution from human imitation similarity
and rule-based safety scores for direct policy optimization. Further, we
introduce an iterative Direct Preference Optimization stage formulated as
trajectory-level preference alignment. Extensive experiments on the NAVSIM
benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of
90.0. Furthermore, qualitative results across diverse challenging scenarios
highlight DriveDPO's ability to produce safer and more reliable driving
behaviors.

</details>


### [67] [ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion](https://arxiv.org/abs/2509.17941)
*Zichao Hu,Chen Tang,Michael J. Munje,Yifeng Zhu,Alex Liu,Shuijing Liu,Garrett Warnell,Peter Stone,Joydeep Biswas*

Main category: cs.RO

TL;DR: ComposableNav是一个基于扩散模型的机器人导航系统，能够组合不同的运动基元来满足指令中的多个规范组合，即使这些组合在训练中未见过。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在动态环境中导航时遵循包含多个规范组合的指令的挑战，避免规范组合数量随技能扩展而指数增长的问题。

Method: 使用扩散模型分别学习每个运动基元，然后在部署时并行组合它们；采用两阶段训练：监督预训练学习基础扩散模型，强化学习微调将其塑造成不同的运动基元。

Result: 在仿真和真实世界实验中，ComposableNav能够生成满足多样且未见过的规范组合的轨迹，显著优于非组合的VLM策略和成本图组合基线。

Conclusion: ComposableNav通过组合学习的方法有效解决了指令规范组合的指数增长问题，为机器人导航提供了可扩展的解决方案。

Abstract: This paper considers the problem of enabling robots to navigate dynamic
environments while following instructions. The challenge lies in the
combinatorial nature of instruction specifications: each instruction can
include multiple specifications, and the number of possible specification
combinations grows exponentially as the robot's skill set expands. For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.
Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training. Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives. Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines. Videos and additional materials can
be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/

</details>


### [68] [Guided Multi-Fidelity Bayesian Optimization for Data-driven Controller Tuning with Digital Twins](https://arxiv.org/abs/2509.17952)
*Mahdi Nobar,Jürg Keller,Alessandro Forino,John Lygeros,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 提出了一种引导式多保真度贝叶斯优化框架，用于数据高效的控制器调谐，通过整合校正的数字孪生模拟和真实世界测量来应对有限保真度模拟系统。


<details>
  <summary>Details</summary>
Motivation: 针对具有有限保真度模拟或廉价近似的闭环系统，解决模型不匹配问题，提高控制器调谐的效率。

Method: 构建多保真度代理模型，包含学习校正模型来改进数字孪生估计；使用自适应成本感知采集函数平衡预期改进、保真度和采样成本；动态调整跨源相关性和采集函数。

Result: 在机器人驱动硬件和支持性数值研究上的实验表明，该方法相比标准贝叶斯优化和多保真度方法提高了调谐效率。

Conclusion: 该方法能够确保适应性，随着新测量数据的到来动态调整数字孪生的准确性，使准确数字孪生更频繁使用，不准确的适当降权。

Abstract: We propose a \textit{guided multi-fidelity Bayesian optimization} framework
for data-efficient controller tuning that integrates corrected digital twin
(DT) simulations with real-world measurements. The method targets closed-loop
systems with limited-fidelity simulations or inexpensive approximations. To
address model mismatch, we build a multi-fidelity surrogate with a learned
correction model that refines DT estimates from real data. An adaptive
cost-aware acquisition function balances expected improvement, fidelity, and
sampling cost. Our method ensures adaptability as new measurements arrive. The
accuracy of DTs is re-estimated, dynamically adapting both cross-source
correlations and the acquisition function. This ensures that accurate DTs are
used more frequently, while inaccurate DTs are appropriately downweighted.
Experiments on robotic drive hardware and supporting numerical studies
demonstrate that our method enhances tuning efficiency compared to standard
Bayesian optimization (BO) and multi-fidelity methods.

</details>


### [69] [M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer](https://arxiv.org/abs/2509.18005)
*Yanxin Zhang,Liang He,Zeyi Kang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 提出了M3ET轻量级多模态学习模型，通过Mamba模块和语义自适应注意力机制优化特征融合和对齐，在保持性能的同时显著提升推理速度和减少参数量


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法难以充分利用文本模态，依赖监督预训练模型，在无监督机器人环境中语义提取受限，且计算密集导致资源消耗高

Method: M3ET模型结合Mamba模块和语义自适应注意力机制，优化特征融合、对齐和模态重建

Result: M3ET提升跨任务性能，预训练推理速度提升2.3倍，VQA任务准确率保持0.74，参数量减少67%

Conclusion: 尽管EQA任务性能有限，但M3ET的轻量级设计使其特别适合部署在资源受限的机器人平台上

Abstract: In recent years, multimodal learning has become essential in robotic vision
and information fusion, especially for understanding human behavior in complex
environments. However, current methods struggle to fully leverage the textual
modality, relying on supervised pretrained models, which limits semantic
extraction in unsupervised robotic environments, particularly with significant
modality loss. These methods also tend to be computationally intensive, leading
to high resource consumption in real-world applications. To address these
challenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a
lightweight model designed for efficient multimodal learning, particularly on
mobile platforms. By incorporating the Mamba module and a semantic-based
adaptive attention mechanism, M3ET optimizes feature fusion, alignment, and
modality reconstruction. Our experiments show that M3ET improves cross-task
performance, with a 2.3 times increase in pretraining inference speed. In
particular, the core VQA task accuracy of M3ET remains at 0.74, while the
model's parameter count is reduced by 0.67. Although performance on the EQA
task is limited, M3ET's lightweight design makes it well suited for deployment
on resource-constrained robotic platforms.

</details>


### [70] [Prepare Before You Act: Learning From Humans to Rearrange Initial States](https://arxiv.org/abs/2509.18043)
*Yinlong Dai,Andre Keyser,Dylan P. Losey*

Main category: cs.RO

TL;DR: ReSET算法通过让机器人先调整环境布局使其更接近训练数据分布，然后再执行任务策略，从而解决模仿学习在面对分布外观察时的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在面对目标物体位置变化或被遮挡等分布外观察时表现不佳，需要大量演示数据才能达到鲁棒性。人类在这种情况下会先调整环境布局来简化任务执行，本文希望赋予机器人同样的能力。

Method: 提出ReSET算法，结合动作无关的人类视频和任务无关的遥操作数据，来决定何时修改场景、预测人类会采取的简化动作，并将这些预测映射到机器人动作基元中。

Result: 理论分析表明这种两步过程（先重新安排环境再执行策略）可以减少泛化差距。与扩散策略、VLAs等基线相比，使用ReSET准备环境可以在相同训练数据量下实现更鲁棒的任务执行。

Conclusion: ReSET算法通过让机器人主动调整环境布局来改善模仿学习的泛化能力，为处理分布外观察提供了一种有效的方法。

Abstract: Imitation learning (IL) has proven effective across a wide range of
manipulation tasks. However, IL policies often struggle when faced with
out-of-distribution observations; for instance, when the target object is in a
previously unseen position or occluded by other objects. In these cases,
extensive demonstrations are needed for current IL methods to reach robust and
generalizable behaviors. But when humans are faced with these sorts of atypical
initial states, we often rearrange the environment for more favorable task
execution. For example, a person might rotate a coffee cup so that it is easier
to grasp the handle, or push a box out of the way so they can directly grasp
their target object. In this work we seek to equip robot learners with the same
capability: enabling robots to prepare the environment before executing their
given policy. We propose ReSET, an algorithm that takes initial states -- which
are outside the policy's distribution -- and autonomously modifies object poses
so that the restructured scene is similar to training data. Theoretically, we
show that this two step process (rearranging the environment before rolling out
the given policy) reduces the generalization gap. Practically, our ReSET
algorithm combines action-agnostic human videos with task-agnostic
teleoperation data to i) decide when to modify the scene, ii) predict what
simplifying actions a human would take, and iii) map those predictions into
robot action primitives. Comparisons with diffusion policies, VLAs, and other
baselines show that using ReSET to prepare the environment enables more robust
task execution with equal amounts of total training data. See videos at our
project website: https://reset2025paper.github.io/

</details>


### [71] [HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2509.18046)
*Yinuo Wang,Yuanyang Qi,Jinzhao Zhou,Gavin Tao*

Main category: cs.RO

TL;DR: HuMam是一个基于Mamba编码器的端到端强化学习框架，用于人形机器人运动控制，通过融合机器人状态、足部目标位姿和连续相位时钟，实现了高效稳定的运动学习。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端强化学习方法在人形机器人运动控制中存在训练不稳定、特征融合效率低和驱动成本高的问题，需要更高效稳定的解决方案。

Method: 采用单层Mamba编码器融合机器人中心状态、定向足部目标和连续相位时钟，使用PPO算法优化策略，输出关节位置目标并通过PD控制器跟踪，设计六项奖励函数平衡接触质量、摆动平滑度等要素。

Result: 在JVRC-1人形机器人上，HuMam相比前馈基线方法显著提高了学习效率、训练稳定性和任务性能，同时降低了功耗和扭矩峰值。

Conclusion: 这是首个采用Mamba作为融合骨干的端到端人形机器人强化学习控制器，在效率、稳定性和控制经济性方面取得了实质性进展。

Abstract: End-to-end reinforcement learning (RL) for humanoid locomotion is appealing
for its compact perception-action mapping, yet practical policies often suffer
from training instability, inefficient feature fusion, and high actuation cost.
We present HuMam, a state-centric end-to-end RL framework that employs a
single-layer Mamba encoder to fuse robot-centric states with oriented footstep
targets and a continuous phase clock. The policy outputs joint position targets
tracked by a low-level PD loop and is optimized with PPO. A concise six-term
reward balances contact quality, swing smoothness, foot placement, posture, and
body stability while implicitly promoting energy saving. On the JVRC-1 humanoid
in mc-mujoco, HuMam consistently improves learning efficiency, training
stability, and overall task performance over a strong feedforward baseline,
while reducing power consumption and torque peaks. To our knowledge, this is
the first end-to-end humanoid RL controller that adopts Mamba as the fusion
backbone, demonstrating tangible gains in efficiency, stability, and control
economy.

</details>


### [72] [V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts](https://arxiv.org/abs/2509.18053)
*Hsu-kuang Chiu,Ryo Hachiuma,Chien-Yi Wang,Yu-Chiang Frank Wang,Min-Hung Chen,Stephen F. Smith*

Main category: cs.RO

TL;DR: 提出了一种新颖的图思维框架，专门用于基于多模态大语言模型的协作自动驾驶，通过遮挡感知感知和规划感知预测来提高安全性。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的自动驾驶车辆在局部传感器被大型物体遮挡时可能面临安全关键情况，而现有的V2V协作自动驾驶研究尚未考虑将图思维推理应用于MLLM。

Method: 设计了专门的图思维框架，包含遮挡感知感知和规划感知预测等新思想，并开发了V2V-GoT-QA数据集和V2V-GoT模型进行训练和测试。

Result: 实验结果表明，该方法在协作感知、预测和规划任务中优于其他基线方法。

Conclusion: 提出的图思维框架有效提升了协作自动驾驶的性能，特别是在传感器遮挡情况下的安全性。

Abstract: Current state-of-the-art autonomous vehicles could face safety-critical
situations when their local sensors are occluded by large nearby objects on the
road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed
as a means of addressing this problem, and one recently introduced framework
for cooperative autonomous driving has further adopted an approach that
incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative
perception and planning processes. However, despite the potential benefit of
applying graph-of-thoughts reasoning to the MLLM, this idea has not been
considered by previous cooperative autonomous driving research. In this paper,
we propose a novel graph-of-thoughts framework specifically designed for
MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our
proposed novel ideas of occlusion-aware perception and planning-aware
prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for
training and testing the cooperative driving graph-of-thoughts. Our
experimental results show that our method outperforms other baselines in
cooperative perception, prediction, and planning tasks.

</details>


### [73] [RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds](https://arxiv.org/abs/2509.18068)
*Bin Zhao,Nakul Garg*

Main category: cs.RO

TL;DR: RadarSFD是一个条件潜在扩散框架，能够从单帧毫米波雷达数据重建密集的LiDAR式点云，无需运动或多帧聚合，适用于小型机器人系统。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在雾、烟、灰尘和低光条件下具有鲁棒性，适合尺寸、重量和功率受限的机器人平台。但现有雷达成像方法需要合成孔径或多帧聚合来提高分辨率，这对小型系统不实用。

Method: 采用条件潜在扩散框架，从预训练的单目深度估计器转移几何先验，通过通道级潜在连接将其锚定到雷达输入，并使用结合潜在空间和像素空间损失的双空间目标进行正则化。

Result: 在RadarHD基准测试中，RadarSFD达到35厘米Chamfer距离和28厘米改进Hausdorff距离，优于单帧基线（56厘米，45厘米），并与使用5-41帧的多帧方法竞争。定性结果显示能够恢复精细墙壁和狭窄间隙。

Conclusion: RadarSFD建立了首个实用的单帧、无SAR毫米波雷达管道，为紧凑机器人系统提供密集点云感知能力，具有强泛化性能。

Abstract: Millimeter-wave radar provides perception robust to fog, smoke, dust, and low
light, making it attractive for size, weight, and power constrained robotic
platforms. Current radar imaging methods, however, rely on synthetic aperture
or multi-frame aggregation to improve resolution, which is impractical for
small aerial, inspection, or wearable systems. We present RadarSFD, a
conditional latent diffusion framework that reconstructs dense LiDAR-like point
clouds from a single radar frame without motion or SAR. Our approach transfers
geometric priors from a pretrained monocular depth estimator into the diffusion
backbone, anchors them to radar inputs via channel-wise latent concatenation,
and regularizes outputs with a dual-space objective combining latent and
pixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer
Distance and 28 cm Modified Hausdorff Distance, improving over the single-frame
RadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame
methods using 5-41 frames. Qualitative results show recovery of fine walls and
narrow gaps, and experiments across new environments confirm strong
generalization. Ablation studies highlight the importance of pretrained
initialization, radar BEV conditioning, and the dual-space loss. Together,
these results establish the first practical single-frame, no-SAR mmWave radar
pipeline for dense point cloud perception in compact robotic systems.

</details>


### [74] [ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces](https://arxiv.org/abs/2509.18084)
*Jiawen Tian,Liqun Huang,Zhongren Cui,Jingchao Qiao,Jiafeng Xu,Xiao Ma,Zeyu Ren*

Main category: cs.RO

TL;DR: ByteWrist是一种新型高柔性仿人并联手腕，通过紧凑的三级并联驱动机制和弧形末端连杆设计，解决了现有串联和并联手腕在狭窄空间操作中的关键限制。


<details>
  <summary>Details</summary>
Motivation: 解决现有串联和并联手腕在狭窄空间操作中的局限性，为复杂非结构化环境（如家庭服务、医疗辅助和精密装配）提供更紧凑、高效的解决方案。

Method: 采用嵌套三级电机驱动连杆设计减小体积，弧形末端连杆优化力传递和扩展运动范围，中央支撑球作为球关节增强结构刚度。同时提供完整的运动学建模，包括正向/逆向运动学和数值雅可比解。

Result: ByteWrist在狭窄空间机动性和双臂协同操作任务中表现出色，性能优于Kinova系统，在紧凑性、效率和刚度方面相比传统设计有显著改进。

Conclusion: ByteWrist为受限环境中的下一代机器人操作提供了一个有前景的解决方案，在紧凑性、灵活性和性能方面具有显著优势。

Abstract: This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic
parallel wrist for robotic manipulation. ByteWrist addresses the critical
limitations of existing serial and parallel wrists in narrow-space operations
through a compact three-stage parallel drive mechanism integrated with
arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)
motion while maintaining exceptional compactness, making it particularly
suitable for complex unstructured environments such as home services, medical
assistance, and precision assembly. The key innovations include: (1) a nested
three-stage motor-driven linkages that minimize volume while enabling
independent multi-DOF control, (2) arc-shaped end linkages that optimize force
transmission and expand motion range, and (3) a central supporting ball
functioning as a spherical joint that enhances structural stiffness without
compromising flexibility. Meanwhile, we present comprehensive kinematic
modeling including forward / inverse kinematics and a numerical Jacobian
solution for precise control. Empirically, we observe ByteWrist demonstrates
strong performance in narrow-space maneuverability and dual-arm cooperative
manipulation tasks, outperforming Kinova-based systems. Results indicate
significant improvements in compactness, efficiency, and stiffness compared to
traditional designs, establishing ByteWrist as a promising solution for
next-generation robotic manipulation in constrained environments.

</details>
