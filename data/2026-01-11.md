<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 94]
- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedPI是一个评估LLMs在医患对话中表现的高维基准，包含105个维度，覆盖医疗流程、治疗安全、治疗效果和医患沟通，通过5层架构评估9个主流模型，发现所有模型在鉴别诊断等维度表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有医学评估基准多为单轮问答形式，缺乏对真实医患对话复杂性的评估。需要开发一个能全面评估LLMs在医疗对话中表现的多维度基准，以指导LLMs在诊断和治疗建议方面的应用。

Method: 构建5层MedPI框架：1)患者包（合成EHR数据）；2)具有记忆和情感的AI患者；3)任务矩阵（就诊原因×就诊目标）；4)基于ACGME能力的105维度评估框架；5)校准的委员会式AI评委。评估9个主流LLM模型在366个AI患者和7,097次对话中的表现。

Result: 所有评估的LLM模型在多个维度表现不佳，特别是在鉴别诊断方面。模型在医疗流程、治疗安全、治疗效果和医患沟通等105个维度上的整体表现较低。

Conclusion: MedPI基准揭示了当前LLMs在医患对话中的局限性，特别是在诊断能力方面。该工作为未来LLMs在医疗诊断和治疗建议方面的应用提供了指导方向。

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

TL;DR: RAGVUE：一个诊断性、可解释的RAG系统评估框架，能够分解评估检索质量、答案相关性、完整性、忠实度和校准度，提供结构化解释。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估指标通常将异构行为压缩为单一分数，无法区分错误来自检索、推理还是基础，缺乏透明度和诊断能力。

Method: RAGVUE框架将RAG行为分解为检索质量、答案相关性和完整性、严格声明级忠实度、评估者校准度等维度，每个指标都包含结构化解释。支持手动指标选择和全自动代理评估，提供Python API、CLI和本地Streamlit交互界面。

Result: 在对比实验中，RAGVUE能够发现现有工具（如RAGAS）经常忽略的细粒度失败案例，提供更精细的评估结果。

Conclusion: RAGVUE为RAG系统提供了透明、可解释的诊断评估框架，能够集成到研究管道和实际RAG开发中，源代码已在GitHub公开。

Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [3] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

TL;DR: 提出完全无监督的中文动词构式数据库构建方法，通过聚类算法从大规模语料生成动词构式，用于增强LLM的可解释性


<details>
  <summary>Details</summary>
Motivation: 为中文语言构建动词构式数据库，补充LLM在需要解释性和可解释性的应用场景中的不足，提供显式和可解释的规则

Method: 将动词构式定义为投射的、有根的、有序的有向无环图，采用聚类算法从大规模语料中检索的句子列表生成动词构式

Result: 生成的构式具有功能独立性和分级典型性的设计特征；在动词语法错误纠正任务中，基于构式最大匹配的纠错算法优于LLM

Conclusion: 无监督构建的动词构式数据库能有效增强LLM的可解释性，在需要解释性的应用场景中具有实用价值

Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [4] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

TL;DR: 提出使用大语言模型生成合成电商产品数据的系统方法，通过三种策略（属性保留修改、受控负例生成、系统属性移除）创建高质量训练数据，在MAVE数据集上达到与真实数据相当的性能（60.5% vs 60.8%），混合配置进一步提升至68.8%准确率。


<details>
  <summary>Details</summary>
Motivation: 电商产品信息提取对电商服务至关重要，但获取高质量标注数据集仍然具有挑战性。现有方法面临数据稀缺和质量问题，特别是在低资源场景下。

Method: 提出系统化的合成电商产品数据生成方法，采用三种策略：1) 属性保留修改，2) 受控负例生成，3) 系统属性移除。使用最先进的大语言模型配合属性感知提示，强制执行店铺约束同时保持产品连贯性。

Result: 人工评估2000个合成产品显示：99.6%被评为自然，96.5%包含有效属性值，超过90%显示一致的属性使用。在公开MAVE数据集上，合成数据达到60.5%准确率，与真实训练数据（60.8%）相当，显著优于13.4%的零样本基线。混合合成和真实数据的配置进一步提升至68.8%准确率。

Conclusion: 该框架为增强电商数据集提供了实用解决方案，特别是在低资源场景下具有重要价值，能够有效解决高质量标注数据稀缺的问题，同时保持与真实数据相当的性能。

Abstract: Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [5] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

TL;DR: 提出Collective Narrative Grounding协议，通过参与式方法将社区故事转化为结构化叙事单元，集成到AI系统中，解决LLM在社区特定查询上的知识盲点问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在社区特定查询上经常失败，形成"知识盲点"，边缘化本地声音并强化认知不公。需要开发能够理解和回答本地问题的AI系统。

Method: 1) 设计参与式协议，通过工作坊收集社区故事并转化为结构化叙事单元；2) 开发实体、时间、地点提取、验证和来源控制的模式；3) 审计县级基准数据集(14,782个本地信息问答对)；4) 在参与式问答集上评估最先进LLM的性能。

Result: 1) 基准审计显示76.7%的错误源于事实空白、文化误解、地理混淆和时间错位；2) 在没有额外上下文的情况下，最先进LLM在参与式问答集上正确率低于21%；3) 收集的叙事中包含了缺失的事实，可直接解决主要错误模式。

Conclusion: 提出了Collective Narrative Grounding协议和参与式评估框架，为构建社区基础AI提供了严格基础。强调了表示与权力、治理与控制、隐私与同意等关键设计张力，为检索优先、来源可见、本地治理的问答系统提供了具体需求。

Abstract: Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [6] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

TL;DR: TeleTables是一个评估LLM在电信标准表格知识理解与解释能力的基准，发现小模型在3GPP标准表格处理上表现不佳，大模型表现更好，需要领域专业化微调。


<details>
  <summary>Details</summary>
Motivation: LLM在电信行业应用日益增多，但研究表明它们在3GPP标准上表现不佳，主要原因之一是标准中大量使用表格呈现关键信息，而LLM对这些表格的知识和解释能力尚未得到充分研究。

Method: 通过多阶段数据生成流程构建TeleTables基准：从3GPP标准中提取表格，使用多模态和推理导向的LLM生成和验证问题，最终得到500个人工验证的问答对，每个都关联多种格式的对应表格。

Result: 小模型（10B参数以下）在3GPP知识回忆和表格解释方面都表现不佳，表明预训练中接触电信标准有限且缺乏处理复杂技术材料的归纳偏置。大模型在表格解释方面表现出更强的推理能力。

Conclusion: TeleTables揭示了LLM在电信标准表格处理上的局限性，强调需要领域专业化微调来可靠地解释和推理电信标准。

Abstract: Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [7] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: FronTalk是一个前端代码生成基准测试，专注于多模态反馈的对话式代码生成，包含100个多轮对话，提出基于智能体的评估框架，发现模型存在遗忘问题和视觉反馈理解挑战，并提出AceCoder基线方法显著减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 在前端开发中，草图、线框图和标注截图等视觉工件对于传达设计意图至关重要，但它们在多轮代码生成中的作用尚未得到充分探索。现有研究缺乏对多模态反馈在对话式代码生成中作用的系统性研究。

Method: 1. 构建FronTalk基准：从真实网站收集100个多轮对话，每轮包含文本指令和等效的视觉指令；2. 提出基于智能体的评估框架：使用网页智能体模拟用户探索网站，评估功能正确性和用户体验；3. 提出AceCoder基线方法：利用自主网页智能体批判性评估每个过去指令的实现，减少遗忘问题。

Result: 评估20个模型发现两个关键挑战：1) 显著的遗忘问题，模型会覆盖先前实现的功能；2) 视觉反馈解释的持续挑战，特别是开源视觉语言模型。AceCoder方法将遗忘减少到接近零，性能提升高达9.3%（从56.0%到65.3%）。

Conclusion: FronTalk为前端开发和多轮多模态代码生成的一般交互动态提供了坚实基础，揭示了现有模型的局限性，并提出了有效的解决方案，为未来研究提供了有价值的基准和方向。

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [8] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

TL;DR: 提出一种基于时变方差和空间偏差的动态重掩码策略，显著提升扩散语言模型的推理效率


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型的重掩码策略使用单一全局置信度阈值，忽略了token的时空动态特性，导致冗余迭代和并行性受限

Method: 提出动态检测每个token的时变方差和空间偏差的方法，这些信号反映token的收敛状态和token间相关性，据此为每个token在每个步骤自适应调整置信度阈值

Result: 在主流数据集上显著提升扩散语言模型的运行效率，最高实现8.9倍加速，同时保持生成质量

Conclusion: 动态重掩码策略通过考虑token的时空动态特性，有效解决了固定阈值重掩码的效率瓶颈问题

Abstract: Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [9] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

TL;DR: 本文提出了一种结合微调语言模型与RAG的AI系统，用于提升大学招生办公室处理咨询的效率与质量，通过领域特定微调优化复杂招生场景下的响应准确性。


<details>
  <summary>Details</summary>
Motivation: 大学招生办公室面临高咨询量管理挑战，需要在保证响应质量的同时提高效率，这对潜在学生的感知至关重要。传统RAG在复杂、狭窄的招生领域可能表现有限，容易产生上下文不充分的回答。

Method: 提出混合AI系统：1) 使用检索增强生成(RAG)从大型数据集中获取相关信息；2) 在特定招生流程数据集上微调语言模型，增强其对RAG提供数据的解释能力；3) 探索响应生成逻辑的优化策略，平衡质量与速度。

Result: 通过领域特定微调，系统能够更准确地解释RAG检索的信息，生成与招生领域相关的输出。混合方法结合了RAG获取最新信息的能力和微调嵌入的领域理解能力。

Conclusion: 该AI系统能有效提升大学招生咨询的响应效率和质量，通过微调优化了RAG在复杂狭窄领域的表现，为招生沟通提供了高质量、领域相关的解决方案。

Abstract: University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [10] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

TL;DR: 提出一种轻量级线性探针方法，通过分析LLM内部特征来量化政治意识形态错位，并通过调整输出层概率来对齐用户观点，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型内部的政治意识形态组织与人类意识形态空间存在系统性错位，这种错位是模型特定的且可测量的，需要一种低成本的方法来对齐模型与特定用户观点。

Method: 开发轻量级线性探针，通过计算模型内部特征的偏置分数，直接调整最终输出概率，实现模型与用户观点的对齐，同时保持模型原有的推理能力。

Result: 该方法能够量化模型与人类意识形态的错位程度，并通过最小化修正输出层来实现对齐，是一种实用且低成本的解决方案。

Conclusion: LLMs内部意识形态组织与人类存在可测量的系统性错位，提出的线性探针方法能够有效量化并修正这种错位，为模型对齐提供了一种简单高效的途径。

Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


### [11] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: LEXMA是一个基于强化学习的LLM微调框架，用于生成面向多受众的叙事式AI决策解释，在抵押贷款审批场景中显著提升了预测性能和解释质量。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在高风险消费者交互中应用广泛，但决策逻辑不透明。现有可解释AI技术依赖事后数值特征归因，无法提供连贯的决策叙事。LLM虽能生成自然语言解释，但面临三大挑战：解释需要既决策正确又忠实于预测驱动因素；需要服务多受众而不改变底层决策规则；需要标签高效训练，不依赖大量人工标注解释。

Method: LEXMA是一个基于强化学习的微调框架，结合反思增强的监督微调和两阶段组相对策略优化（GRPO）。它微调两个独立的参数集：一个提升决策正确性，另一个满足不同受众的风格要求，使用不依赖人工标注解释的奖励信号。框架在抵押贷款审批决策场景中实例化。

Result: LEXMA在预测性能上显著优于其他LLM基线。人工评估显示，面向专家的解释更关注风险，面向消费者的解释更清晰、更具可操作性且更礼貌。

Conclusion: LEXMA提供了一个成本高效、系统化的LLM微调方法，用于提升商业决策的解释质量，为透明AI系统的可扩展部署提供了强大潜力。

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [12] [Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments](https://arxiv.org/abs/2601.04209)
*Seokhwan Ko,Donghyeon Lee,Jaewoo Chun,Hyungsoo Han,Junghwan Cho*

Main category: cs.CL

TL;DR: 开发了一个基于PubMed的本地检索增强生成系统，用于推荐医学机构内的研究合作者，结合PubMedBERT和LLaMA3模型实现隐私合规的本地部署


<details>
  <summary>Details</summary>
Motivation: 医疗环境中LLMs应用需求增长，但医院严格的隐私和网络安全规定要求敏感数据必须在完全本地基础设施中处理，需要开发符合隐私要求的本地化解决方案

Method: 开发检索增强生成系统，使用PubMedBERT进行领域特定嵌入生成，结合本地部署的LLaMA3模型进行生成合成，基于机构成员的PubMed出版物推荐研究合作者

Result: 展示了将领域专门化编码器与轻量级LLMs集成在本地部署约束下支持生物医学知识发现的可行性和实用性

Conclusion: 该系统证明了在严格隐私要求下，通过结合领域特定编码器和轻量级LLMs，可以在本地环境中有效支持生物医学知识发现和研究协作

Abstract: Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

</details>


### [13] [Complexity Agnostic Recursive Decomposition of Thoughts](https://arxiv.org/abs/2601.04210)
*Kaleem Ullah Qasim,Jiashu Zhang,Hafiz Saif Ur Rehman*

Main category: cs.CL

TL;DR: CARD框架通过预测问题复杂度并自适应分解，在保持高准确率的同时显著降低推理成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多步推理中常因固定的推理策略而失败，这些策略忽略了问题的具体难度差异

Method: CARD框架包含MRCE复杂度估计器（0.6B Qwen模型预测30个细粒度特征）和两阶段递归求解器：基于任务配置的层次分解和递归MRCE配置的每步思考预算分配

Result: 在三个推理模型上，GSM8K准确率达81.4%-89.2%，token成本降低1.88-2.40倍；MATH-500准确率达75.1-86.8%，token使用减少1.71-5.74倍

Conclusion: 预先的复杂度估计能够同时实现更高的准确率和显著的计算效率提升

Abstract: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.

</details>


### [14] [Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays](https://arxiv.org/abs/2601.04211)
*Nikita Zmanovskii*

Main category: cs.CL

TL;DR: Qwerty AI是一个端到端系统，用于根据俄罗斯联邦法律436-FZ对俄语剧本进行自动年龄分级和内容安全评估，能在2分钟内处理长达700页的剧本，准确率达80%。


<details>
  <summary>Details</summary>
Motivation: 解决俄罗斯媒体行业根据联邦法律436-FZ对剧本内容进行年龄分级和内容安全评估的实际编辑挑战，特别是在Wink黑客松中发现的现实需求。

Method: 使用经过微调的Phi-3-mini模型（4位量化），将剧本分割为叙事单元，检测五个内容违规类别（暴力、性内容、脏话、物质、恐怖元素），并分配年龄分级（0+、6+、12+、16+、18+）。

Result: 系统在80GB VRAM限制下，处理平均剧本时间<5分钟，达到80%的分级准确率和80-95%的分割精度（格式依赖），已部署在Yandex Cloud上并支持CUDA加速。

Conclusion: Qwerty AI展示了在生产工作流程中的实际应用性，成功解决了俄罗斯媒体行业的现实编辑需求，特别是在严格的计算约束下实现了高效的内容安全评估。

Abstract: We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.

</details>


### [15] [TrueBrief: Faithful Summarization through Small Language Models](https://arxiv.org/abs/2601.04212)
*Kumud Lakara,Ruibo Shi,Fran Silavong*

Main category: cs.CL

TL;DR: TrueBrief是一个通过偏好优化增强小型语言模型文本摘要忠实度的端到端框架，通过可控幻觉注入生成合成偏好数据。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成高质量文本方面表现出色，但在安全关键领域部署时，其产生幻觉的倾向构成重大挑战。需要提高小型语言模型在文本摘要任务中的忠实度。

Method: 提出TrueBrief端到端框架，采用偏好优化范式，核心是数据生成模块，通过可控幻觉注入生成合成偏好数据来训练小型语言模型。

Result: 该工作提供了关于数据质量和模型大小对基于偏好的优化影响的见解，强调了这些方法最有效的条件。

Conclusion: TrueBrief框架能有效增强小型语言模型在文本摘要任务中的忠实度，为安全关键领域的部署提供了可行方案。

Abstract: Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.

</details>


### [16] [AnimatedLLM: Explaining LLMs with Interactive Visualizations](https://arxiv.org/abs/2601.04213)
*Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: AnimatedLLM是一个交互式Web应用，通过浏览器提供Transformer语言模型的逐步可视化，用于NLP教学


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理教育中日益重要，但展示其内部工作机制的教学材料稀缺

Method: 开发基于浏览器的交互式Web应用，使用预计算的开源LLM轨迹和手动整理的输入数据

Result: 创建了可在https://animatedllm.github.io访问的教学工具，支持逐步可视化Transformer模型内部工作

Conclusion: AnimatedLLM为NLP教育提供了实用的可视化教学工具，既可作为教学辅助也可用于自学

Abstract: Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.

</details>


### [17] [From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning](https://arxiv.org/abs/2601.04278)
*Xiaoyu Xu,Minxin Du,Zitong Li,Zi Liang,Zhibiao Guo,Shiyu Zhang,Peizhao Hu,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: BiForget：一种利用目标模型自身合成高质量遗忘集的自动化框架，通过种子引导和对抗提示来匹配模型内部知识分布，改进了LLM遗忘评估的严谨性。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘基准往往无法准确反映模型实际学习的"遗忘范围"，需要更忠实表示模型内部知识分布的评估方法。

Method: 提出BiForget框架，形式化两种遗忘粒度（领域级和实例级），利用目标模型自身通过种子引导和对抗提示合成高质量遗忘集，匹配模型内部知识分布。

Result: 在多个基准测试中，BiForget在相关性、多样性和效率方面达到优越平衡。在哈利波特领域中，相关性提高约20，多样性提高约0.05，同时数据总量减半。

Conclusion: BiForget为评估LLM遗忘提供了更严谨的基础，促进了更稳健的遗忘和更好的效用保留。

Abstract: Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.

</details>


### [18] [RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation](https://arxiv.org/abs/2601.04350)
*Joseph James,Chenghao Xiao,Yucheng Li,Nafise Sadat Moosavi,Chenghua Lin*

Main category: cs.CL

TL;DR: RIGOURATE是一个两阶段多模态框架，用于从论文正文中检索支持证据并为每个主张分配夸大陈述分数，旨在提高科学严谨性和透明度。


<details>
  <summary>Details</summary>
Motivation: 科学严谨性往往被边缘化，作者倾向于做出超出结果支持的夸大陈述，这阻碍了清晰透明的科学交流。

Method: 构建包含10K+主张-证据对的数据集（来自ICLR和NeurIPS论文），使用8个LLM标注，通过同行评审评论校准夸大分数。采用微调的重新排序器进行证据检索，微调模型预测夸大分数并提供理由。

Result: 相比强基线，RIGOURATE在证据检索和夸大陈述检测方面表现更优，能够有效识别论文中的夸大主张。

Conclusion: 该工作实现了证据比例性的操作化，支持更清晰、更透明的科学交流，有助于减少科学论文中的夸大陈述问题。

Abstract: Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.

</details>


### [19] [Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties](https://arxiv.org/abs/2601.04373)
*Akriti Dhasmana,Aarohi Srivastava,David Chiang*

Main category: cs.CL

TL;DR: 该研究通过实证分析发现，虽然语音识别性能通常随语言间亲缘距离减小而提升，但这一因素不能完全解释方言环境下的性能表现。少量方言数据微调常能达到与大量亲缘语言数据微调相当的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索跨语言迁移在自然、嘈杂、代码混合的印度方言和语言变体中的表现，特别关注方言和非标准化语音对ASR系统的挑战，以及预训练语言偏向问题。

Method: 采用实证研究方法，在广泛的印度方言和语言变体上测试跨语言迁移效果，包括对Garhwali（低资源Pahari语言变体）的案例研究，评估多种当代ASR模型，并分析转录错误以检测预训练语言偏向。

Result: 结果显示：1) ASR性能通常随语言亲缘距离减小而改善；2) 但亲缘距离不能完全解释方言环境性能；3) 少量方言数据微调常能达到与大量亲缘语言数据微调相当的效果；4) 转录错误分析揭示了ASR系统对预训练语言的偏向。

Conclusion: 研究表明，对于方言和非标准化语音，少量方言特定数据微调可能比依赖大量亲缘语言数据更有效，同时需要关注ASR系统对预训练语言的偏向问题，这对低资源方言ASR开发具有重要启示。

Abstract: We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.

</details>


### [20] [Disco-RAG: Discourse-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04377)
*Dongqi Liu,Hang Ding,Qiming Feng,Jian Li,Xurong Xie,Zhucun Xue,Chengjie Wang,Jiangning Zhang,Yabiao Wang*

Main category: cs.CL

TL;DR: Disco-RAG：一种基于篇章结构的检索增强生成框架，通过构建篇章树和修辞图来增强LLM在知识密集型任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法通常将检索到的段落视为扁平无结构的信息，这限制了模型捕捉结构线索和整合分散知识的能力，需要更有效的篇章结构感知方法

Method: 提出Disco-RAG框架：1）构建块内篇章树捕捉局部层次结构；2）构建块间修辞图建模跨段落连贯性；3）将这些结构整合为规划蓝图来指导生成过程

Result: 在问答和长文档摘要基准测试中，Disco-RAG无需微调即达到最先进的结果，证明了篇章结构对RAG系统的重要作用

Conclusion: 篇章结构在提升RAG系统性能中扮演关键角色，Disco-RAG通过显式注入篇章信号有效增强了LLM在知识密集型任务中的表现

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.

</details>


### [21] [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](https://arxiv.org/abs/2601.04389)
*Iago Alves Brito,Walcy Santos Rezende Rios,Julia Soares Dollis,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 论文提出MiJaBench双语对抗基准，揭示LLM安全对齐存在选择性保护问题，不同少数群体防御率差异显著，模型缩放反而加剧这种不平等。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全评估存在"普遍性幻觉"，将"身份仇恨"等指标聚合成标量分数，掩盖了对特定人群的系统性脆弱性。需要揭示这种选择性安全保护问题。

Method: 引入MiJaBench双语（英语和葡萄牙语）对抗基准，包含16个少数群体的44,000个提示。从12个SOTA LLM生成528,000个提示-响应对，构建MiJaBench-Align数据集进行分析。

Result: 安全对齐不是普遍语义能力而是人口统计学层级：同一模型内仅基于目标群体，防御率波动高达33%。模型缩放加剧了这些差异，表明当前对齐技术没有建立非歧视原则，只强化了对特定群体的记忆拒绝边界。

Conclusion: 当前安全对齐技术存在选择性保护问题，挑战了现有的安全缩放定律。需要研究细粒度的人口统计学对齐方法，所有数据集和代码已开源。

Abstract: Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.

</details>


### [22] [ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://arxiv.org/abs/2601.04394)
*Sharanya Dasgupta,Arkaprabha Basu,Sujoy Nath,Swagatam Das*

Main category: cs.CL

TL;DR: ARREST是一个统一框架，通过外部网络识别和修正LLM潜在激活空间中的表征错位，调节虚假为真实、不安全输出为安全输出，无需微调模型参数。


<details>
  <summary>Details</summary>
Motivation: 人类认知能在想象与现实之间自我修正，而现有LLM缺乏平衡事实性与安全性的认知能力。作者认为LLM的事实性和安全性失败源于潜在激活空间的表征错位，而非完全独立的对齐问题。

Method: 提出ARREST框架：训练外部网络理解激活空间的波动，选择性干预模型，通过软硬拒绝和事实修正来调节错位特征。采用对抗训练增强鲁棒性。

Result: 实验结果表明ARREST不仅能有效调节错位，相比RLHF对齐模型，由于对抗训练还能生成更灵活的软拒绝响应，展现出更强的通用性。

Conclusion: 通过外部网络调节LLM潜在激活空间的表征错位是解决事实性和安全性问题的有效统一方法，ARREST框架为此提供了可行的技术方案。

Abstract: Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.

</details>


### [23] [Interpreting Transformers Through Attention Head Intervention](https://arxiv.org/abs/2601.04398)
*Mason Kadem,Rong Zheng*

Main category: cs.CL

TL;DR: 论文探讨神经网络机制解释的重要性，强调理解AI决策过程对问责、认知研究和知识发现的关键作用


<details>
  <summary>Details</summary>
Motivation: 神经网络能力不断增强但内部机制不透明，需要理解其决策过程以实现：1）高风险领域的问责与控制；2）数字大脑研究和认知涌现；3）AI超越人类时的知识发现

Method: 论文聚焦于机制解释（mechanistic interpretability）这一方法论，通过分析神经网络内部工作机制来理解其决策过程

Result: 提出机制解释作为理解神经网络决策的关键框架，强调其在三个重要领域的应用价值

Conclusion: 机制解释对于确保AI系统安全可靠、研究数字认知以及从先进AI系统中获取新知识至关重要

Abstract: Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.

</details>


### [24] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: 本文提出了Gavel-Ref评估框架和Gavel-Agent代理框架，用于评估LLM在多文档法律案例摘要任务上的表现，发现即使最强模型在复杂长上下文任务上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM现在支持高达1M token的上下文，但它们在复杂长上下文任务上的实际效果仍不清楚。本文针对多文档法律案例摘要任务进行研究，因为单个案例通常包含100K-500K token的多个文档，需要系统评估LLM在此类任务上的表现。

Method: 1. 提出Gavel-Ref评估框架：基于参考的评估方法，包含26个项目的多值清单评估、剩余事实评估和写作风格评估。2. 使用Gavel-Ref系统评估12个前沿LLM在100个法律案例上的表现。3. 开发Gavel-Agent代理框架：为LLM配备六个工具，使其能够直接从案例文档中导航和提取清单信息。

Result: 1. 即使最强的Gemini 2.5 Pro模型在Gavel-Ref评分上仅达到约50分，表明任务难度很高。2. 模型在简单清单项目上表现良好，但在多值或罕见项目上表现不佳。3. Gavel-Agent使用Qwen3时，相比GPT-4.1的端到端提取，token使用量减少36%，而清单评分仅下降7%。

Conclusion: 当前LLM在多文档法律案例摘要等复杂长上下文任务上仍有显著局限性。随着LLM继续改进并可能超越人工编写的摘要，基于人工参考的评估将变得不可靠，因此需要像Gavel-Agent这样的自主代理框架来直接处理原始文档。该研究为评估和改进LLM在长上下文任务上的能力提供了重要框架。

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [25] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: LLMs经常无法挑战用户的有害信念，这可以理解为它们默认迎合用户假设且缺乏认知警惕。研究发现社会语言因素影响LLMs的迎合行为，简单的语用干预能显著改善性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗建议、社会推理等领域经常无法挑战用户的有害信念，这种失败源于LLMs默认迎合用户假设并表现出不足的认知警惕性，需要从语用学角度理解和解决这一问题。

Method: 研究分析了影响人类迎合行为的社会和语言因素（话题相关性、语言编码、来源可靠性）如何影响LLMs的迎合行为，测试了三个安全基准（Cancer-Myth、SAGE-Eval、ELEPHANT），并提出了简单的语用干预方法，如添加"wait a minute"短语。

Result: 研究发现社会语言因素同样影响LLMs的迎合行为，能够解释在不同安全基准上的性能差异。简单的语用干预能显著提高在这些基准上的性能，同时保持较低的错误阳性率。

Conclusion: 考虑语用学对于评估LLM行为和改善LLM安全至关重要。通过理解LLMs如何迎合用户假设并实施简单的语用干预，可以有效提高LLMs挑战有害信念的能力。

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [26] [Learning to Simulate Human Dialogue](https://arxiv.org/abs/2601.04436)
*Kanishk Gandhi,Agam Bhatia,Noah D. Goodman*

Main category: cs.CL

TL;DR: 研究对话预测中思考机制与学习目标的影响：发现直接最大化人类对话对数概率优于基于LLM评判的奖励优化，且思考机制在分布匹配目标下效果最佳


<details>
  <summary>Details</summary>
Motivation: 通过预测人们会说什么来建模他们的思维过程，研究在对话预测中思考机制和学习方法如何影响模型对人类行为的理解

Method: 比较两种学习维度：(1)是否允许模型在回答前思考；(2)学习奖励方式：使用LLM评判器评分语义相似性和信息完整性，或直接最大化真实人类对话的对数概率。将思维链作为隐变量，推导对数概率的下界

Result: 优化评判器奖励虽提高评判分数，但降低真实人类响应的似然性，在人类评判测试中胜率下降。直接最大化人类响应对数概率在所有评估中表现最佳，思考机制在分布匹配目标下效果最好

Conclusion: 思考主要在基于真实人类对话的分布匹配目标下才有帮助，将此方法扩展到更广泛的对话数据可能产生对人类行为有更细致理解的模型

Abstract: To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.

</details>


### [27] [Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models](https://arxiv.org/abs/2601.04448)
*San Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出MB-Defense框架，通过防御性投毒和权重恢复两阶段训练，保护指令调优大语言模型免受各种后门攻击


<details>
  <summary>Details</summary>
Motivation: 指令调优的大语言模型依赖大规模数据集，容易受到后门攻击，但现有防御措施不足，需要开发有效的保护方法

Method: MB-Defense包含两个阶段：1) 防御性投毒阶段，将攻击者和防御者的触发器合并为统一的后门表示；2) 权重恢复阶段，通过额外训练打破这种表示，恢复干净行为

Result: 在多个大语言模型上的实验表明，MB-Defense显著降低了攻击成功率，同时保持了指令跟随能力，对未见过的后门攻击也有效

Conclusion: 该方法提供了一种通用且数据高效的防御策略，提高了指令调优大语言模型对后门攻击的鲁棒性

Abstract: Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.

</details>


### [28] [Users Mispredict Their Own Preferences for AI Writing Assistance](https://arxiv.org/abs/2601.04461)
*Vivian Lai,Zana Buçinca,Nil-Jana Akpinar,Mo Houtti,Hyeonsu B. Kang,Kevin Chian,Namjoon Suh,Alex C. Williams*

Main category: cs.CL

TL;DR: 用户对AI写作助手的需求偏好存在认知-行为差距：自述偏好与行为数据完全相反，基于行为数据的系统设计效果更好


<details>
  <summary>Details</summary>
Motivation: 主动式AI写作助手需要预测用户何时需要起草帮助，但缺乏对用户偏好驱动因素的经验理解

Method: 通过因子情境研究，50名参与者进行750对比较，分析用户对AI写作助手帮助偏好的驱动因素

Result: 组合努力是主要决策驱动因素（ρ=0.597），紧迫性无预测力（ρ≈0）。用户存在显著的认知-行为差距：自述中紧迫性最重要，但行为上却是最弱驱动因素。基于自述偏好的系统准确率仅57.7%，低于基线；基于行为模式的系统达到61.3%（p<0.05）

Conclusion: 依赖用户自省进行系统设计会误导优化，主动式自然语言生成系统应基于行为数据而非自述偏好

Abstract: Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.

</details>


### [29] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 提出ProMem方法，通过主动自我提问的循环反馈机制改进LLM代理的记忆提取，解决传统摘要方法的前瞻性盲点和一次性提取问题


<details>
  <summary>Details</summary>
Motivation: 现有基于摘要的记忆管理方法存在两大问题：1）摘要提取是"前瞻性"的，作为盲目的前馈过程，由于不知道未来任务而遗漏重要细节；2）提取通常是"一次性"的，缺乏验证事实的反馈循环，导致信息损失累积

Method: 提出主动记忆提取（ProMem），将提取视为迭代认知过程，引入循环反馈机制，让代理通过自我提问主动探索对话历史，从而恢复缺失信息和纠正错误

Result: ProMem显著提高了提取记忆的完整性和问答准确性，在提取质量和token成本之间实现了更好的权衡

Conclusion: 主动记忆提取方法通过循环反馈和自我提问机制，有效解决了传统摘要方法在LLM代理记忆管理中的局限性，为长期交互和个性化提供了更好的记忆提取方案

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [30] [Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions](https://arxiv.org/abs/2601.04465)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: 提出Concept Tokens方法：为预训练LLM添加特殊token，仅从其概念定义学习嵌入，冻结模型其他参数，通过语言建模目标优化嵌入，用于控制模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要微调整个模型或依赖上下文定义来控制LLM行为，缺乏轻量级、高效的概念控制方法。需要一种能够从概念定义学习、保持模型冻结、提供紧凑控制信号的方法。

Method: 1. 为预训练LLM添加新的特殊token；2. 用多个自然语言定义训练该token嵌入，其中概念出现处替换为新token；3. 保持LLM冻结，仅优化token嵌入；4. 使用标准语言建模目标进行优化。

Result: 1. 在HotpotQA闭卷问答中：否定幻觉token减少幻觉答案（主要通过增加弃答），肯定幻觉token增加幻觉并降低精度；2. 在二语教学重述诱导中：观察到相同方向效应，且比上下文提供完整定义语料更好地保持其他指令遵从性；3. 定性研究显示学习嵌入能捕获概念信息，但存在局限性。

Conclusion: Concept Tokens提供了一种从定义学习的紧凑控制信号，能够在冻结LLM中引导行为，具有方向性效应，比上下文定义方法更好地保持指令遵从性，但存在局限性。

Abstract: We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.

</details>


### [31] [SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers](https://arxiv.org/abs/2601.04469)
*Iaroslav Chelombitko,Ekaterina Chelombitko,Aleksey Komissarov*

Main category: cs.CL

TL;DR: SampoNLP工具包通过自参考原子性评分创建形态学词典，用于评估BPE分词器在芬兰语、匈牙利语和爱沙尼亚语等乌拉尔语系语言中的性能，提出统一指标IPS并确定最佳词汇量。


<details>
  <summary>Details</summary>
Motivation: 乌拉尔语系等形态丰富语言缺乏干净的语素词典，阻碍了分词器评估。需要一种适合低资源环境的语素词典创建方法，以系统评估BPE分词器在这些语言中的性能。

Method: 提出SampoNLP工具包，使用MDL启发的自参考原子性评分从内部结构线索过滤复合形式，无需语料库即可创建高纯度形态学词典。利用这些词典对芬兰语、匈牙利语和爱沙尼亚语的BPE分词器进行系统评估，词汇量范围8k-256k。提出统一指标IPS来平衡语素覆盖和过度切分。

Result: 通过IPS曲线分析确定了"拐点"（收益递减点），首次为这些语言提供了基于实证的最佳词汇量建议：芬兰语64k，匈牙利语32k，爱沙尼亚语16k。定量展示了标准BPE对高度黏着语言的局限性。

Conclusion: SampoNLP为低资源形态丰富语言提供了实用的语素词典创建工具，通过系统评估为BPE分词器提供了实证指导，揭示了标准BPE在黏着语言中的局限性。所有资源和工具已开源。

Abstract: The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.
  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.
  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP

</details>


### [32] [WESR: Scaling and Evaluating Word-level Event-Speech Recognition](https://arxiv.org/abs/2601.04508)
*Chenchen Yang,Kexin Huang,Liwei Fan,Qian Tu,Botian Jiang,Dong Zhang,Linqi Yin,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 论文提出了WESR-Bench，一个用于非语言声音事件检测的评估基准，包含21种声音事件的精细分类和900+专家标注语料，解决了现有方法类别覆盖不足、时间粒度模糊和缺乏标准化评估的问题。


<details>
  <summary>Details</summary>
Motivation: 语音不仅传递语言信息，还包含丰富的非语言声音事件（如笑声、哭声）。虽然语义转录研究充分，但非语言事件的精确定位仍是一个关键但未被充分探索的挑战。现有方法存在任务定义不足（类别覆盖有限、时间粒度模糊）和缺乏标准化评估框架的问题，阻碍了下游应用的发展。

Method: 1. 开发了包含21种声音事件的精细分类法，分为离散型（独立）和连续型（与语音混合）两类；2. 引入WESR-Bench专家标注评估集（900+语料），采用新颖的位置感知协议，将ASR错误与事件检测分离，实现对离散和连续事件的精确定位测量；3. 构建1700+小时语料库，训练专用模型。

Result: 构建的专用模型超越了开源音频语言模型和商业API，同时保持了ASR质量。WESR-Bench为建模丰富的真实世界听觉场景提供了基础资源。

Conclusion: WESR填补了非语言声音事件检测领域的空白，提供了精细的分类法、标准化评估基准和强大的基线模型，有望成为未来研究丰富真实世界听觉场景建模的基础资源。

Abstract: Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.

</details>


### [33] [LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation](https://arxiv.org/abs/2601.04516)
*Yuxiao Ye,Yiming Zhang,Yiran Ma,Huiyuan Xie,Huining Zhu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出LinguaGame框架，通过博弈论建模多智能体对话，提升语言交流效率


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统主要关注架构设计（如角色分配、工作流编排），而忽略了交互过程本身。本文旨在改进智能体的通信效率，帮助它们通过语言更有效地传达意图。

Method: 提出LinguaGame框架，将对话建模为基于通信意图和策略的信号博弈，采用无需训练的均衡近似算法进行推理时决策调整。该框架基于语言学推理，最小化任务特定耦合，将对话视为有意图和策略的通信过程。

Result: 在模拟法庭程序和辩论场景中评估框架，人类专家评估显示在通信效率方面取得显著提升。

Conclusion: LinguaGame为多智能体对话生成提供了一个基于语言学的博弈论范式，能够有效提升智能体间的通信效率，且具有较好的通用性。

Abstract: Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.

</details>


### [34] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: GRACE是一个强化学习框架，通过证据充分性评估、关键证据提取和选择性弃权，统一解决RAG系统中的证据缺失和幻觉问题，仅需10%的标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在两个关键缺陷：1) 在没有明确证据的情况下提供正确答案；2) 在检索上下文不足时产生幻觉回答。先前研究独立解决这些问题，缺乏统一的证据基础化和可靠弃权框架。

Method: 提出GRACE强化学习框架：1) 使用异构检索器生成多样化训练样本，无需人工标注；2) 采用多阶段门控奖励函数，训练模型评估证据充分性、提取关键支持证据，并选择回答或明确弃权。

Result: 在两个基准测试中，GRACE实现了最先进的整体准确率，在准确回答和拒绝之间取得了良好平衡，同时仅需先前方法10%的标注成本。

Conclusion: GRACE提供了一个统一的框架，同时解决RAG系统中的证据基础化和可靠弃权问题，显著降低了标注成本，在准确性和可靠性之间取得了良好平衡。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [35] [BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534)
*Amit Bin Tariqul,A N M Zahid Hossain Milkan,Sahab-Al-Chowdhury,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文首次系统评估了主流文本水印方法在孟加拉语LLM生成中的表现，发现跨语言往返翻译攻击会严重破坏检测准确率，并提出分层水印策略显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在文本生成中的广泛应用，水印技术对于作者归属、知识产权保护和滥用检测变得至关重要。现有水印方法在高资源语言中表现良好，但在低资源语言中的鲁棒性尚未得到充分探索，特别是孟加拉语等语言。

Method: 系统评估了KGW、指数采样(EXP)和Waterfall三种最先进的文本水印方法在孟加拉语LLM文本生成中的表现。针对跨语言往返翻译攻击导致检测准确率崩溃的问题，提出了分层水印策略，结合嵌入时水印和生成后水印。

Result: 在良性条件下，KGW和EXP达到高检测准确率(>88%)且困惑度和ROUGE指标退化可忽略。但往返翻译攻击使检测准确率崩溃至9-13%。分层水印将攻击后检测准确率提升25-35%，达到40-50%，相比单层方法有3-4倍相对改进，代价是可控的语义退化。

Conclusion: 研究量化了多语言水印中的鲁棒性-质量权衡，确立了分层水印作为低资源语言（如孟加拉语）的实用、无需训练的解决方案。代码和数据将公开。

Abstract: As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.

</details>


### [36] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: NeuronLLM：基于功能拮抗原理的LLM神经元识别框架，通过对比学习好/坏神经元来全面理解模型功能组织


<details>
  <summary>Details</summary>
Motivation: 现有方法只能识别特定能力的神经元，无法处理需要多种能力协调的任务场景；且只关注支持性神经元，忽略了抑制性神经元和LLM偶然正确行为导致的神经元归因误导

Method: 提出NeuronLLM框架，采用生物学功能拮抗原理，通过对比学习同时识别促进任务完成的"好神经元"和抑制任务完成的"坏神经元"，并利用增强问题集减轻LLM的偶然行为

Result: 在不同规模和家族的LLM上的综合实验表明，NeuronLLM在四个NLP任务上优于现有方法，为LLM功能组织提供了新见解

Conclusion: NeuronLLM通过功能拮抗原理实现了对LLM神经元的全面建模，解决了现有方法在任务级理解和神经元角色识别方面的局限性

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [37] [FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback](https://arxiv.org/abs/2601.04574)
*Seongyeub Chu,Jongwoo Kim,Munyong Yi*

Main category: cs.CL

TL;DR: FeedEval是一个基于LLM的框架，用于评估LLM生成的论文反馈质量，通过三个教学维度（具体性、帮助性、有效性）筛选高质量反馈，提升论文评分模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用LLM生成的反馈训练论文评分模型，但缺乏明确的质量验证，导致噪声传播到下游应用。需要一种系统方法来评估和筛选高质量的LLM生成反馈。

Method: 提出FeedEval框架，使用基于三个教学维度（具体性、帮助性、有效性）训练的专门化LLM评估器，评估多个反馈候选并选择高质量反馈用于下游任务。

Result: 在ASAP++基准测试中，FeedEval与人类专家判断高度一致；使用FeedEval筛选的高质量反馈训练的论文评分模型获得更优的评分性能；小型LLM使用这些反馈进行论文修订也取得更好效果。

Conclusion: FeedEval能有效评估和筛选LLM生成的论文反馈质量，提升自动化论文评分系统的性能，并为学生提供更有用的修订指导。

Abstract: Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.

</details>


### [38] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: RL-Text2Vis：首个基于强化学习的文本到可视化生成框架，通过多目标奖励函数联合优化文本准确性、代码有效性和可视化质量，显著提升图表质量和代码执行成功率


<details>
  <summary>Details</summary>
Motivation: 现有文本到可视化系统存在两大问题：闭源LLM生成的图表语义对齐和清晰度不足，开源模型则经常产生不可执行或视觉质量差的输出。传统的监督微调方法无法捕捉执行后反馈，难以提升整体可视化质量。

Method: 提出RL-Text2Vis框架，基于Group Relative Policy Optimization (GRPO)，设计新颖的多目标奖励函数，联合优化文本准确性、代码有效性和可视化质量，利用执行后反馈进行强化学习训练。

Result: 在Text2Vis基准测试中，RL-Text2Vis相比GPT-4o实现了22%的相对图表质量提升，代码执行成功率从零样本基线的78%提升到97%。在VIS-Eval和NVBench等域外数据集上也表现出强大的泛化能力。

Conclusion: GRPO是可视化生成中结构化多模态推理的有效策略，RL-Text2Vis框架显著提升了文本到可视化系统的性能，为后续研究提供了新方向。

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [39] [THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report](https://arxiv.org/abs/2601.04597)
*KBTG Labs,:,Anuruth Lertpiya,Danupat Khamnuansin,Kantapong Sucharitpongpan,Pornchanan Balee,Tawunrat Chalothorn,Thadpong Pongthawornkamol,Monchai Lertsutthiwong*

Main category: cs.CL

TL;DR: 该研究探索模型合并作为资源高效的方法，用于开发高性能、多能力的大语言模型，特别是在泰语和金融领域，通过合并Qwen-8B、ThaiLLM-8B和THaLLE-CFA-8B模型来提升综合能力。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、安全和监管考虑，组织通常偏好本地部署LLMs，但面临部署多个专门模型与训练单一多能力模型成本过高之间的权衡。ThaiLLM计划旨在增强开源LLMs的泰语能力，但需要解决多领域能力整合的资源效率问题。

Method: 采用模型合并作为资源高效替代方案，进行了两个关键实验：1) 合并Qwen-8B与ThaiLLM-8B以增强泰语通用能力；2) 合并Qwen-8B、ThaiLLM-8B和THaLLE-CFA-8B以同时提升通用和金融领域性能。

Result: 实验表明：1) ThaiLLM-8B显著提升了Qwen-8B在M3和M6 O-NET考试中的泰语通用能力；2) 三模型合并进一步提升了M3和M6 O-NET、Flare-CFA和Thai-IC基准测试的性能，在通用和金融领域都表现出改进。

Conclusion: 模型合并是高效创建多能力LLMs的可行方法，能够在不进行昂贵重新训练的情况下整合不同领域的专业能力，为组织提供资源高效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.

</details>


### [40] [On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions](https://arxiv.org/abs/2601.04600)
*Zhiyuan He,Binghan Chen,Tianxiang Xiong,Ziyang Sun,Mozhao Zhu,Xi Chen*

Main category: cs.CL

TL;DR: ROME知识编辑方法在单跳事实更新上高效，但在多跳推理任务中面临三大失败模式，作者提出冗余编辑策略显著提升多跳推理准确率


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法（特别是ROME）在单跳事实更新上表现出色，但在需要知识链的多跳推理任务中存在显著挑战，需要解决这些失败模式

Method: 通过分析ROME在不同层深度编辑知识的效果，识别出三大失败模式，并提出冗余编辑策略来缓解"跳得太晚"和泛化能力下降问题

Result: 冗余编辑策略可将2跳问题的准确率提高至少15.5个百分点，比之前的单次编辑策略提升96%，但在特异性和语言自然度上有所权衡

Conclusion: 冗余编辑是提升多跳推理知识编辑效果的有效策略，显著改善了ROME方法在多跳任务中的性能，为知识编辑在多跳推理中的应用提供了新思路

Abstract: Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.

</details>


### [41] [When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation](https://arxiv.org/abs/2601.04609)
*Rhea Kapur,Robert Hawkins,Elisa Kreiss*

Main category: cs.CL

TL;DR: 该研究指出当前视觉语言模型中描述特异性常与长度混淆，提出应区分这两个概念，并通过构建数据集验证了人们更偏好特异性描述而非冗长描述。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型生成图像描述时，常将描述的特异性（信息密度）与描述长度混为一谈。实际上，描述可以是简洁但信息密集的，也可以是冗长但空洞的。需要将这两个概念解耦，并建立更科学的评估方法。

Method: 1. 定义特异性：相对于对比集，描述越能准确区分目标图像与其他可能图像，特异性越高。2. 构建数据集：控制描述长度，但变化信息内容。3. 进行人类偏好实验：验证人们是否更偏好特异性描述（无论长度）。4. 分析长度预算分配对特异性的影响。

Result: 1. 人类确实更偏好特异性描述，与长度无关。2. 仅控制长度不能解释特异性差异：长度预算的分配方式很重要。3. 支持直接优先考虑特异性而非冗长性的评估方法。

Conclusion: 视觉语言模型的描述质量评估应关注特异性而非长度。需要开发能直接衡量描述特异性的评估方法，避免将冗长误认为信息丰富。这对改进VLM生成描述的质量和实用性有重要意义。

Abstract: Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.

</details>


### [42] [Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR](https://arxiv.org/abs/2601.04611)
*Yihong Tang,Kehai Chen,Xuefeng Bai,Benyou Wang,Zeming Liu,Haifeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: Character-R1框架通过三种核心设计提供可验证的奖励信号，解决角色扮演智能体在复杂情境中缺乏内部认知一致性的问题，显著提升角色扮演性能。


<details>
  <summary>Details</summary>
Motivation: 当前角色扮演智能体通常模仿表面行为，缺乏内部认知一致性，导致在复杂情境中出现角色不一致的错误。现有研究缺少有效的角色感知推理奖励信号。

Method: 提出Character-R1框架，包含三个核心设计：1) 认知聚焦奖励：基于10个角色元素（如世界观）进行显式标签分析以结构化内部认知；2) 参考引导奖励：使用与参考回答的重叠度指标作为优化锚点，增强探索和性能；3) 角色条件奖励归一化：根据角色类别调整奖励分布，确保跨异质角色的鲁棒优化。

Result: 大量实验表明，Character-R1在知识、记忆等多个方面显著优于现有方法。

Conclusion: Character-R1通过提供全面的可验证奖励信号，有效解决了角色扮演智能体的内部认知一致性问题，为角色感知推理提供了有效的优化框架。

Abstract: Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.

</details>


### [43] [From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset](https://arxiv.org/abs/2601.04632)
*Haneul Yoo,Won Ik Cho,Geunhye Kim,Jiyoon Han*

Main category: cs.CL

TL;DR: 提出CuCu框架，利用国家社会研究课程自动生成文化特定问答对，构建韩国文化问答数据集KCaQA，以解决LLM文化对齐问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在跨语言和跨文化任务中表现不均，主要反映英语中心训练数据的价值观，需要实现实用的文化对齐

Method: 提出CuCu框架，这是一个自动化的多智能体LLM框架，将国家教科书课程转化为开放式的文化特定问答对，应用于韩国社会研究课程构建KCaQA数据集

Result: 构建了包含34.1k个开放式问答对的KCaQA数据集，定量和定性分析表明该数据集覆盖文化特定主题，并产生基于当地社会文化背景的响应

Conclusion: 利用国家社会研究课程作为文化感知监督的基础，提供了一种可扩展的文化对齐方法，有助于解决LLM的文化偏差问题

Abstract: Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.

</details>


### [44] [MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark](https://arxiv.org/abs/2601.04633)
*Anyang Song,Ying Cheng,Yiqian Xu,Rui Feng*

Main category: cs.CL

TL;DR: 提出MAGA方法，通过增强机器生成文本的对齐性来改善检测器的泛化能力，同时测试现有检测器的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型对齐技术的发展，机器生成文本越来越难以与人类撰写文本区分，加剧了虚假新闻和网络欺诈等滥用问题。现有微调检测器的泛化能力高度依赖数据集质量，仅扩展机器生成文本来源不足，需要进一步增强生成过程。

Method: 提出MAGA方法，通过从提示构建到推理过程的全面对齐流程，其中关键组件是系统提出的RLDF（基于检测器反馈的强化学习）。

Result: 在RoBERTa检测器上，使用MAGA训练集微调后，泛化检测AUC平均提升4.60%；MAGA数据集导致所选检测器AUC平均下降8.13%，表明对检测器鲁棒性的攻击效果。

Conclusion: MAGA方法通过增强生成文本的对齐性，既能攻击现有检测器测试其鲁棒性，又能提升检测器的泛化能力，为未来检测器泛化检测能力研究提供重要参考。

Abstract: Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.

</details>


### [45] [SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation](https://arxiv.org/abs/2601.04638)
*Sirry Chen,Jieyi Wang,Wei Chen,Zhongyu Wei*

Main category: cs.CL

TL;DR: SpeechMedAssist：一种基于语音语言模型的医疗咨询系统，通过两阶段训练范式减少对医疗语音数据的需求，仅需1万合成样本即可实现语音驱动的多轮医疗对话。


<details>
  <summary>Details</summary>
Motivation: 医疗咨询本质上是语音中心的，但现有研究多基于长文本交互，不够自然且对患者不友好。虽然语音语言模型（SpeechLMs）能实现更自然的语音交互，但医疗语音数据稀缺以及直接微调效率低下阻碍了SpeechLMs在医疗咨询中的应用。

Method: 提出SpeechMedAssist，利用SpeechLMs的架构特性，将传统单阶段训练解耦为两阶段范式：1）通过文本注入知识与能力；2）使用有限语音数据进行模态重对齐。该方法仅需1万合成语音样本，大幅降低对医疗语音数据的需求。

Result: 实验结果表明，该模型在大多数评估设置中，在单轮问答和多轮模拟交互方面均优于所有基线模型，展现出更好的有效性和鲁棒性。

Conclusion: SpeechMedAssist通过创新的两阶段训练范式，成功解决了医疗语音数据稀缺问题，实现了高效的语音驱动医疗咨询，为SpeechLMs在医疗领域的应用提供了可行方案。

Abstract: Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.

</details>


### [46] [CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2601.04664)
*Yifan Le,Yunliang Li*

Main category: cs.CL

TL;DR: CRANE：基于相关性的神经元分析框架，通过神经元级干预重新定义语言特异性，发现语言选择但非排他性的神经元专门化模式


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在不同语言上表现强劲，但神经元层面的语言能力组织机制尚不清楚。现有基于激活的启发式方法混淆了语言偏好与功能重要性，需要更精确的分析方法。

Method: 提出CRANE框架，通过神经元级干预（如掩码）来识别语言特异性神经元，基于功能必要性而非激活幅度来定义语言特异性，使用相关性指标评估神经元对语言条件预测的贡献。

Result: 神经元干预揭示了一致的非对称模式：掩码目标语言相关神经元会选择性降低该语言性能，同时很大程度上保留其他语言性能。在英语、中文和越南语的多个基准测试中，CRANE比基于激活的方法更精确地分离语言特定组件。

Conclusion: CRANE框架提供了更精确的语言特异性神经元识别方法，揭示了语言选择但非排他性的神经元专门化模式，有助于理解多语言LLM中语言能力的组织机制。

Abstract: Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.

</details>


### [47] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: ToolGate：一个为LLM工具调用提供逻辑安全保证和可验证状态演进的框架，通过Hoare式契约和符号状态空间确保工具执行的安全性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具调用框架依赖自然语言推理来确定工具调用时机和结果提交，缺乏逻辑安全性和可验证性的形式化保证，可能导致无效或幻觉结果污染世界表示。

Method: 提出ToolGate框架，维护显式符号状态空间作为类型化键值映射，将每个工具形式化为Hoare式契约（前置条件和后置条件），通过前置条件门控工具调用，后置条件通过运行时验证决定结果是否提交更新状态。

Result: 实验验证表明ToolGate显著提高了工具增强LLM系统的可靠性和可验证性，同时在复杂多步推理任务上保持竞争力。

Conclusion: ToolGate为构建更可信、可调试的AI系统奠定了基础，这些系统将语言模型与外部工具集成，提供逻辑安全保证和可验证状态演进。

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [48] [See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation](https://arxiv.org/abs/2601.04692)
*Naquee Rizwan,Subhankar Swain,Paramananda Bhaskar,Gagan Aryan,Shehryaar Shah Khan,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 提出基于生成式AI的多模态框架，在有限数据条件下实现仇恨表情包检测、解释和干预的一体化处理


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将仇恨表情包的检测、解释和干预分开研究，不符合实际应用场景；同时构建大规模标注数据集成本过高，需要能在有限数据条件下工作的通用解决方案

Method: 提出新颖框架，利用任务特定的生成式多模态智能体和大型多模态模型的少样本适应能力，针对不同类型的表情包进行处理

Result: 这是首个专注于有限数据条件下通用仇恨表情包审核的工作，具有在实际生产场景中部署的强大潜力

Conclusion: 该框架通过生成式AI模型实现了仇恨表情包检测、解释和干预的一体化处理，解决了现有研究分离处理的问题，并在有限数据条件下展现出实际应用价值

Abstract: In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.

</details>


### [49] [Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding](https://arxiv.org/abs/2601.04693)
*Sungmok Jung,Yeonkyoung So,Joonhak Lee,Sangho Kim,Yelim Ahn,Jaejin Lee*

Main category: cs.CL

TL;DR: 论文分析了韩语否定对LLMs的挑战，创建了Thunder-KoNUBench基准测试，评估了47个LLMs，发现微调能提升韩语否定理解和上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 虽然已知否定对大型语言模型构成挑战，但评估否定理解的基准测试（特别是针对韩语的）很稀缺。需要创建反映韩语否定现象经验分布的基准测试来评估LLMs的性能。

Method: 1. 基于语料库分析韩语否定现象；2. 创建Thunder-KoNUBench句子级基准测试，反映韩语否定的经验分布；3. 评估47个LLMs；4. 分析模型大小和指令微调的影响；5. 在Thunder-KoNUBench上进行微调实验。

Result: 1. LLMs在否定条件下的性能下降；2. 模型大小和指令微调对否定理解有影响；3. 在Thunder-KoNUBench上微调能改善韩语否定理解和更广泛的上下文理解能力。

Conclusion: 该研究填补了韩语否定理解基准测试的空白，证明了微调能有效提升LLMs对韩语否定的理解能力，为韩语NLP任务提供了重要工具和见解。

Abstract: Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

</details>


### [50] [PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards](https://arxiv.org/abs/2601.04700)
*Mukesh Ghimire,Aosong Feng,Liwen You,Youzhi Luo,Fang Liu,Xuan Zhu*

Main category: cs.CL

TL;DR: PRISM：一种结合过程奖励模型和模型内部置信度的无监督训练框架，用于提升大语言模型在数学推理和代码生成等任务上的性能，无需人工标注或外部验证器。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型解决问题能力的提升，进一步改进需要高质量解决方案，但这些方案可能难以获得人工标注。现有方法依赖模型的内部一致性（如多数投票或置信度），但这些信号在大规模和长期训练中不可靠。

Method: 提出PRISM统一训练框架，使用过程奖励模型（PRM）结合模型内部置信度来指导学习，无需真实标签。PRM评估生成过程的质量，与模型自我确定性相结合，确保训练稳定性和性能提升。

Result: 有效结合PRM和自确定性可以实现稳定的训练和更好的测试性能，同时保持模型内部置信度的可控性。

Conclusion: PRISM框架通过结合过程奖励模型和模型内部置信度，解决了无监督学习中内部一致性信号不可靠的问题，为大语言模型的持续改进提供了有效的无监督训练方法。

Abstract: Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.

</details>


### [51] [Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.04710)
*Feihu Jin,Shipeng Cen,Ying Tan*

Main category: cs.CL

TL;DR: 提出一种改进零阶优化的方法，通过先验信息引导扰动方向，减少梯度估计方差，加速收敛，在多种LLM上优于传统零阶优化和部分梯度方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调需要大量内存，零阶优化可避免反向传播但存在梯度估计方差高、收敛慢的问题，需要改进梯度估计质量。

Method: 提出即插即用方法，使用先验信息引导扰动：动态计算引导向量指导扰动方向，并探索贪婪扰动策略，理论证明梯度估计器与真实梯度方向更一致。

Result: 在多种规模和架构的LLM上实验，方法能无缝集成现有优化方法，实现更快收敛和更好性能。在OPT-13B模型上，优于传统零阶优化（11个任务全部），并超越梯度基线（11个任务中的9个）。

Conclusion: 提出的先验信息引导扰动方法能有效改进零阶优化，在效率和准确性间取得良好平衡，为大模型高效微调提供新方案。

Abstract: Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.

</details>


### [52] [DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs](https://arxiv.org/abs/2601.04711)
*Anh Thi-Hoang Nguyen,Khanh Quoc Tran,Tin Van Huynh,Phuoc Tan-Hoang Nguyen,Cam Tan Nguyen,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: DSC2025 ViHallu Challenge是首个针对越南语大语言模型幻觉检测的大规模共享任务，建立了包含10,000个标注样本的ViHallu数据集，最佳系统达到84.80%的宏F1分数，显示指令调优LLM显著优于通用架构，但内在幻觉检测仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 越南语等低至中等资源语言在幻觉检测标准化评估框架方面覆盖不足，而LLM在生产环境中的可靠性受到幻觉问题的严重限制，需要建立针对越南语的系统化评估基准。

Method: 创建ViHallu数据集，包含10,000个标注的（上下文、提示、响应）三元组，分为无幻觉、内在幻觉和外在幻觉三类；采用事实性、噪声性和对抗性三种提示类型；组织111个团队参与共享任务，比较不同检测方法。

Result: 最佳系统达到84.80%的宏F1分数，相比仅编码器基线（32.83%）有显著提升；指令调优LLM结合结构化提示和集成策略表现最佳；内在幻觉（基于矛盾的幻觉）检测最为困难。

Conclusion: 该工作为越南语AI系统的可信度和可靠性研究建立了严格基准，表明幻觉检测仍是具有挑战性的问题，特别是内在幻觉检测，需要进一步研究改进。

Abstract: The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations -- fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types -- factual, noisy, and adversarial -- to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.

</details>


### [53] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: 该论文提出了"角色身份"的概念，将角色分解为参数化身份和属性化身份两个层次，并发现名人角色优势会随时间消失，而负面社会属性是角色扮演代理保真度的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的角色扮演代理（RPAs）对角色身份的结构维度缺乏形式化定义，通常将角色视为任意的文本输入，这限制了角色构建和评估的系统性。

Method: 提出了角色身份的多维构建概念，包含参数化身份（预训练编码的角色特定知识）和属性化身份（人格特质和道德价值观等行为属性）。构建了统一的角色档案模式，在相同结构约束下生成了名人和合成角色，并在单轮和多轮交互中进行评估。

Result: 发现了两个关键现象：1)"名声消退"：名人角色在初始回合因参数化知识而具有优势，但随着对话上下文积累，这种优势迅速消失；2)"本性难移"：模型能稳健地描绘一般人格特质，但角色扮演代理性能对道德和人际关系的价值取向高度敏感，负面社会本性是保真度的主要瓶颈。

Conclusion: 角色身份的多维框架为角色构建和评估提供了系统指导，负面社会属性是未来角色扮演代理改进的关键方向，研究结果有助于指导未来的角色构建和评估方法。

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [54] [Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking](https://arxiv.org/abs/2601.04720)
*Mingxin Li,Yanzhao Zhang,Dingkun Long,Keqin Chen,Sibo Song,Shuai Bai,Zhibo Yang,Pengjun Xie,An Yang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-VL-Embedding和Qwen3-VL-Reranker是基于Qwen3-VL的多模态嵌入和重排序模型系列，支持文本、图像、文档图像和视频的统一表示，在MMEB-V2等基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个端到端的高精度多模态搜索管道，将不同模态（文本、图像、文档图像、视频）映射到统一的表示空间，支持多语言和灵活部署需求。

Method: 采用多阶段训练范式：大规模对比预训练→重排序模型蒸馏；支持Matryoshka表示学习实现灵活嵌入维度；使用交叉编码器架构进行细粒度相关性估计；提供2B和8B参数版本。

Result: Qwen3-VL-Embedding-8B在MMEB-V2基准上获得77.8分，排名第一（截至2025年1月8日）；在多模态嵌入评估基准上达到最先进水平；支持超过30种语言。

Conclusion: 该系列模型为多模态检索任务提供了有效的端到端解决方案，在图像-文本检索、视觉问答、视频-文本匹配等任务上表现出色，满足多样化部署需求。

Abstract: In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.

</details>


### [55] [Automatic Classifiers Underdetect Emotions Expressed by Men](https://arxiv.org/abs/2601.04730)
*Ivan Smirnov,Segun T. Aroyehun,Paul Plener,David Garcia*

Main category: cs.CL

TL;DR: 研究发现情感分析模型在男性和女性作者文本上存在系统性偏见，男性作者文本的错误率普遍更高，表明当前机器学习工具在性别平衡未知或变化时应谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 自动情感和情绪分类器被广泛使用，但其可靠性通常基于第三方标注者而非情绪体验者本人进行评估，这可能掩盖系统性偏见。需要研究这些工具在不同人群中的表现差异。

Method: 使用超过100万条自我标注的帖子数据集，采用预先注册的研究设计，评估414种模型和情绪类别的组合，分析性别偏见在情感检测中的表现。

Result: 研究发现，在不同类型的自动分类器和各种基础情绪中，男性作者文本的错误率始终高于女性作者文本。这种偏见可能影响下游应用的结果。

Conclusion: 情感分析尚未完全解决，特别是在确保跨人口统计群体的公平模型行为方面。当样本的性别构成未知或变化时，包括大语言模型在内的当前机器学习工具应谨慎使用。

Abstract: The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.

</details>


### [56] [AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs](https://arxiv.org/abs/2601.04736)
*Han Zhu,Jiale Chen,Chengkun Cai,Shengjie Sun,Haoran Li,Yujin Zhou,Chi-Min Chan,Pengcheng Wen,Lei Li,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 提出了InterSafe-V多模态对话数据集和AM³Safety框架，用于提升多轮多模态对话中LLM的安全性，通过冷启动拒绝阶段和GRPO微调，显著降低攻击成功率并保持模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在交互应用中部署增多，但在多轮多模态场景下安全漏洞显著：有害意图可在多轮对话中逐渐重构，安全协议随对话进展被遗忘。现有RLHF对齐方法主要针对单轮VQA任务，需要昂贵的人工偏好标注，在对话场景中效果和可扩展性有限。

Method: 1) 构建InterSafe-V开源多模态对话数据集（11,270个对话和500个专门设计的拒绝VQA样本），通过模型间交互构建以更准确反映真实场景；2) 提出AM³Safety框架：结合冷启动拒绝阶段和使用回合感知双目标奖励的Group Relative Policy Optimization（GRPO）微调，在整个对话中进行优化。

Result: 在Qwen2.5-VL-7B-Instruct和LLaVA-NeXT-7B模型上实验显示：攻击成功率降低超过10%，无害维度提升至少8%，有帮助维度提升超过13%，在多模态多轮安全基准上表现优异，同时保持模型的通用能力。

Conclusion: InterSafe-V数据集和AM³Safety框架有效解决了多轮多模态对话中的安全问题，通过数据驱动的方法和创新的微调策略，显著提升了MLLM的安全性和实用性，为多模态对话安全对齐提供了可行的解决方案。

Abstract: Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.

</details>


### [57] [RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation](https://arxiv.org/abs/2601.04740)
*Huawei Zheng,Xinqi Jiang,Sen Yang,Shouling Ji,Yingcai Wu,Dazhen Deng*

Main category: cs.CL

TL;DR: 提出一个端到端框架，通过知识图谱引导的有害提示生成和双路径模糊重写，自动创建领域相关且隐晦的有害提示数据集，以提升LLM安全研究。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在金融、医疗等专业领域的应用增多，但领域特定的有害提示数据集稀缺且主要依赖人工构建。公开数据集主要关注显式有害提示，而现代LLM防御系统通常能检测并拒绝这些提示。相比之下，通过间接领域知识表达的隐式有害提示更难检测，更能反映现实威胁。

Method: 提出端到端框架：1) 知识图谱引导的有害提示生成：系统性地生成领域相关的提示；2) 双路径模糊重写：通过直接重写和上下文增强重写，将显式有害提示转换为隐式变体。

Result: 该框架能够生成高质量的数据集，结合了强大的领域相关性和隐晦性，支持更现实的红队测试，并推进LLM安全研究。代码和数据集已在GitHub上发布。

Conclusion: 提出的框架解决了将领域知识转化为可操作约束和增加生成有害提示隐晦性的挑战，为专业领域的LLM安全研究提供了更有效的工具和数据集。

Abstract: Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.

</details>


### [58] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: Tool-MAD：一个多智能体辩论框架，通过为每个智能体分配不同的外部工具（如搜索API或RAG模块）来增强事实核查能力，引入自适应查询机制和定量评估指标，显著提升事实验证准确性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论（MAD）系统主要依赖内部知识或静态文档，容易产生幻觉。虽然MADKE引入了外部证据，但其一次性检索机制无法适应辩论过程中的新论点或新兴信息。需要一种能够动态获取和验证外部证据的框架。

Method: 1）多智能体辩论框架，每个智能体使用异构外部工具（搜索API、RAG模块等）；2）自适应查询机制，根据辩论流程迭代优化证据检索；3）将忠实度和答案相关性分数整合到最终决策中，让法官智能体定量评估每个响应的连贯性和问题对齐度。

Result: 在四个事实验证基准测试中，Tool-MAD始终优于最先进的MAD框架，准确率提升高达5.5%。在医学专业领域，Tool-MAD在各种工具配置和领域条件下表现出强大的鲁棒性和适应性。

Conclusion: Tool-MAD通过整合异构外部工具和自适应证据检索，有效减少了LLM的幻觉问题，在事实核查任务中表现出色，具有广泛的实际应用潜力。

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [59] [PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks](https://arxiv.org/abs/2601.04758)
*Yehoon Jang,Chaewon Lee,Hyun-seok Min,Sungchul Choi*

Main category: cs.CL

TL;DR: PILOT-Bench是首个针对PTAB专利上诉的基准测试，包含三个IRAC对齐的分类任务，用于评估LLM在专利领域结构化法律推理的能力。


<details>
  <summary>Details</summary>
Motivation: PTAB每年处理大量专利上诉案件，需要结合技术理解和法律推理。虽然LLM在专利和法律实践中应用日益增多，但主要限于轻量级任务，缺乏系统评估其在专利领域结构化法律推理能力的方法。

Method: 创建PILOT-Bench基准，将PTAB决定与USPTO专利数据在案件级别对齐，并形式化三个IRAC对齐的分类任务：问题类型、委员会权威依据和子决定。评估了多种闭源和开源LLM，从多个角度进行分析。

Result: 在问题类型任务上，闭源模型Micro-F1分数持续超过0.75，而最强的开源模型(Qwen-8B)性能约为0.56，显示出推理能力上的显著差距。

Conclusion: PILOT-Bench为系统评估专利领域法律推理建立了基础，并指出了通过数据集设计和模型对齐改进LLM的未来方向。

Abstract: The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.

</details>


### [60] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: 研究发现大语言模型DeepSeek-V3的隐藏层表示中，句法和语义信息以线性方式编码，可以通过计算句法/语义"质心"向量来提取这些信息，且两种信息的编码模式不同。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型（特别是DeepSeek-V3）的内部表示如何编码句法和语义信息，探索这些语言信息在模型中的表示机制。

Method: 通过平均具有相同句法结构或语义的句子的隐藏表示向量，得到句法和语义"质心"向量，然后从句子向量中减去这些质心来分析它们对相似性的影响。

Result: 句法和语义信息在表示中至少部分线性编码，减去质心会显著影响与句法/语义匹配句子的相似性；句法和语义的跨层编码模式不同，两种信号可以在一定程度上解耦。

Conclusion: 大语言模型的内部表示以不同方式编码句法和语义信息，这些信息至少部分线性编码，且两种语言信息的编码机制存在差异。

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [61] [Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence](https://arxiv.org/abs/2601.04766)
*Shengyin Sun,Yiming Li,Renxi Liu,Weizhe Lin,Hui-Ling Zhen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: Judge Decoding加速LLM推理，但依赖昂贵监督。本文发现监督学习的"关键性"分数本质编码在草稿-目标分布差异中，理论证明线性judge与KL散度结构对应，提出基于KL散度的无训练验证机制，性能媲美复杂训练judge且更鲁棒。


<details>
  <summary>Details</summary>
Motivation: Judge Decoding通过放松Speculative Decoding的严格验证来加速LLM推理，但通常依赖昂贵且噪声的监督。现有方法需要训练复杂的judge模型，存在监督瓶颈和领域适应性差的问题。

Method: 从第一性原理出发，理论分析发现监督学习的"关键性"分数本质编码在草稿-目标模型的分布差异中。证明线性judge与KL散度在结构上对应，依赖相同的logit原语。基于此提出基于KL散度的无训练验证机制，完全消除监督需求。

Result: 在推理和代码生成基准测试上的广泛实验表明，该方法匹配或优于复杂训练judge（如AutoJudge），在领域转移时表现出更好的鲁棒性，完全消除了监督瓶颈。

Conclusion: 揭示了Judge Decoding中监督学习的本质，证明KL散度是有效的无监督验证机制，为加速LLM推理提供了更简单、鲁棒且无需监督的解决方案。

Abstract: Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.

</details>


### [62] [LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal](https://arxiv.org/abs/2601.04768)
*Dongjun Kim,Jeongho Yoon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 提出LANGSAE EDITING方法，通过稀疏自编码器在向量空间中可控地移除语言身份信号，提升多语言检索性能


<details>
  <summary>Details</summary>
Motivation: 多语言密集检索中，嵌入编码同时包含语义和语言身份信息，这会导致同语言对的相似度被夸大，并挤占其他语言相关证据的空间

Method: 使用后处理的稀疏自编码器，在池化嵌入上训练，通过跨语言激活统计识别语言相关潜在单元，在推理时抑制这些单元，并在原始维度重建嵌入

Result: 在多种语言上实验显示，排名质量和跨语言覆盖度均有持续改进，特别是在文字差异大的语言上获得显著提升

Conclusion: 该方法无需重新训练基础编码器或重新编码原始文本，即可与现有向量数据库兼容，有效提升多语言检索性能

Abstract: Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.

</details>


### [63] [NC2C: Automated Convexification of Generic Non-Convex Optimization Problems](https://arxiv.org/abs/2601.04789)
*Xinyue Peng,Yanming Liu,Yihan Cang,Yuwei Zhang,Xinyi Wang,Songhang Deng,Jiannan Cao*

Main category: cs.CL

TL;DR: NC2C是一个基于LLM的端到端自动化框架，能够将通用非凸优化问题转化为可解的凸形式，显著减少对专家知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 非凸优化问题在数学规划、工程设计和科学计算中普遍存在，传统求解器因其复杂的目标函数和约束条件而面临困难。手动凸化效率低下且过度依赖专家知识，需要自动化解决方案。

Method: NC2C利用LLM的数学推理能力，自主检测非凸组件，选择最优凸化策略，并生成严格的凸等价形式。框架集成了符号推理、自适应变换技术和迭代验证，配备错误纠正循环和可行性域校正机制。

Result: 在100个通用非凸问题的多样化数据集上，NC2C实现了89.3%的执行率和76%的成功率，能够产生可行、高质量的凸变换，显著优于基线方法。

Conclusion: NC2C展示了利用LLM进行自动非凸到凸变换的能力，减少了对专家知识的依赖，使凸求解器能够高效处理先前难以解决的优化任务。

Abstract: Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

</details>


### [64] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: 首次系统分析多智能体评估中的角色权威偏见，发现专家型和参照型权威角色比合法型权威更具影响力，权威偏见源于权威角色坚持立场而非普通智能体主动顺从。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体系统常分配权威角色以提升性能，但权威偏见对智能体交互的影响尚未得到充分探索。本研究旨在填补这一空白，系统分析角色权威偏见在多智能体评估中的作用机制。

Method: 基于French和Raven的权力理论，将权威角色分为合法型、参照型和专家型三类。使用ChatEval平台进行自由形式的多智能体评估，分析12轮对话中权威角色的影响。实验采用GPT-4o和DeepSeek R1模型。

Result: 专家型和参照型权威角色比合法型权威更具影响力。权威偏见并非源于普通智能体的主动顺从，而是权威角色坚持立场而普通智能体表现出灵活性。权威影响力需要明确的立场陈述，中性回应无法产生偏见。

Conclusion: 研究揭示了多智能体系统中权威偏见的形成机制，为设计具有非对称交互模式的多智能体框架提供了关键见解，强调了权威角色类型和立场明确性的重要性。

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [65] [When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection](https://arxiv.org/abs/2601.04833)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 论文发现AI生成文本存在"晚期波动衰减"现象，并提出基于后期统计的简单特征实现零样本检测，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本AI文本检测方法通常在整个序列上聚合token级统计信息，忽略了自回归生成过程中的时间动态特性。作者通过分析大量文本样本，旨在揭示AI生成文本与人类写作在生成过程中的差异模式。

Method: 论文分析了超过12万文本样本，发现了"晚期波动衰减"现象：AI生成文本在生成过程中对数概率波动迅速稳定，而人类写作在整个过程中保持较高的变异性。基于这一发现，提出了两个简单特征：导数离散度和局部波动度，这两个特征仅从后期统计信息计算得出。

Result: 研究发现AI生成文本在序列后半部分表现出24-32%更低的波动性。提出的方法无需扰动采样或额外模型访问，在EvoBench和MAGE基准测试中达到了最先进的性能，并且与现有全局方法表现出很强的互补性。

Conclusion: AI生成文本与人类写作在生成过程中的时间动态存在系统性差异，特别是晚期波动衰减现象。利用这一现象设计的简单后期统计特征能够有效检测AI生成文本，为AI文本检测提供了新的视角和高效方法。

Abstract: Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

</details>


### [66] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: RAAR是一个检索增强的代理推理框架，用于跨领域虚假信息检测，通过多视角证据检索和多智能体协作推理，显著提升了跨领域检测能力。


<details>
  <summary>Details</summary>
Motivation: 跨领域虚假信息检测面临挑战：现有方法依赖单一视角线索，难以泛化到具有挑战性或代表性不足的领域；大型语言模型虽然能处理复杂任务，但仅限于同分布数据。需要解决这些限制以实现有效的跨领域检测。

Method: RAAR框架包含三个核心组件：1）多视角源领域证据检索，根据目标样本的语义、情感和写作风格检索对齐的证据；2）多智能体协作构建可验证的多步推理路径，包括特定视角智能体进行互补分析和总结智能体在验证器指导下整合；3）通过监督微调和强化学习训练单一多任务验证器，增强验证和推理能力。

Result: 基于RAAR训练了RAAR-8b和RAAR-14b模型。在三个跨领域虚假信息检测任务上的评估显示，RAAR显著提升了基础模型的能力，并优于其他跨领域方法、先进LLM和基于LLM的适应方法。

Conclusion: RAAR是首个检索增强的代理推理框架，通过多视角证据检索和多智能体协作推理，有效解决了跨领域虚假信息检测的挑战，在多个任务上取得了优越性能。

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [67] [Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics](https://arxiv.org/abs/2601.04854)
*Oshri Naparstek*

Main category: cs.CL

TL;DR: 提出连续自回归语言生成框架，将token表示为连续向量，通过确定性动态过程演化成熟后再离散化，无需token级采样即可生成稳定、多样的文本。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型在离散token序列上操作，每个生成步骤都需要确定具体token，这种早期离散化迫使不确定性通过token级采样来解决，导致生成不稳定、重复和对解码启发式敏感等问题。

Method: 引入连续自回归语言生成框架：1) token表示为连续向量；2) 通过确定性动态过程演化多个更新步骤；3) 当表示充分收敛后才离散化；4) 通过硬解码恢复离散文本，不确定性在连续空间中保持和解决。

Result: 仅通过成熟过程就足以使用确定性解码（argmax）生成连贯多样的文本，无需依赖token级采样、扩散式去噪或辅助稳定机制。这是第一个通过演化连续token表示到收敛再离散化的自回归语言模型。

Conclusion: 连续自回归语言生成框架通过在连续空间中演化token表示，延迟离散化决策，实现了稳定生成而无需token级采样，为语言生成提供了新的范式。

Abstract: Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

</details>


### [68] [MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News](https://arxiv.org/abs/2601.04857)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Baojie Qu,Runteng Guo,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MisSpans是首个多领域、人工标注的span级虚假信息检测与分析基准，包含三个互补任务：定位句子中的虚假片段、分类虚假片段类型、提供基于片段的解释，支持细粒度定位和可操作分析。


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测方法通常在整句或段落级别使用粗糙的二元标签进行评估，这掩盖了真实和虚假细节常在同一句子中共存的情况，且全局解释无法识别具体误导片段或区分虚假细节的类型（如扭曲vs捏造）。

Method: 构建MisSpans基准，包含配对的真实和虚假新闻故事，定义三个任务：MisSpansIdentity（定位虚假片段）、MisSpansType（分类虚假片段类型）、MisSpansExplanation（提供基于片段的解释）。专家标注者遵循标准化指南和一致性检查，获得高标注者间一致性。评估15个代表性LLM，包括推理增强和非推理变体，在零样本和单样本设置下进行。

Result: 细粒度虚假信息识别与分析具有挑战性，性能受多种因素影响，包括模型大小、推理能力以及领域特定的文本特征。需要更深入理解这些交互因素如何影响性能。

Conclusion: MisSpans填补了细粒度虚假信息检测的空白，支持局部定位、超越真/假的细致表征和可操作解释，为更精确的虚假信息分析提供了基准和工具。

Abstract: Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.

</details>


### [69] [A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs](https://arxiv.org/abs/2601.04859)
*Maxime Delmas,Lei Xu,André Freitas*

Main category: cs.CL

TL;DR: ToPG是一个新颖的RAG框架，通过构建命题-实体-段落异构图，结合迭代的建议-选择循环，在简单事实检索和复杂多跳查询中都表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法存在局限性：基于分块的RAG擅长简单事实检索但无法处理复杂多跳查询；检索与推理交错的方法缺乏全局语料库意识；基于知识图谱的RAG在复杂任务上表现好但在简单事实查询上不佳。需要一种能兼顾两者的解决方案。

Method: 提出ToPG框架：1）将知识库建模为命题、实体和段落的异构图，结合命题的事实密度和图连接性；2）采用迭代的建议-选择循环：建议阶段实现查询感知的图遍历，选择阶段提供LLM反馈来修剪无关命题并为下一轮迭代提供种子。

Result: 在三种不同的QA任务（简单、复杂和抽象QA）上评估，ToPG在准确性和质量指标上都表现出色。证明查询感知的图遍历结合事实粒度是高效结构化RAG系统的关键组件。

Conclusion: ToPG通过异构图建模和迭代建议-选择循环，成功弥合了简单事实检索和复杂多跳查询之间的差距，为结构化RAG系统提供了有效的解决方案。

Abstract: Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.

</details>


### [70] [EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis](https://arxiv.org/abs/2601.04875)
*Xuanguang Pan,Chongyang Tao,Jiayuan Bai,Jianling Gao,Zhengwei Tao,Xiansheng Zhou,Gavin Cheung,Shuai Ma*

Main category: cs.CL

TL;DR: EvolSQL是一个结构感知的数据合成框架，通过从种子数据演化SQL查询来生成更丰富、语义更多样的文本到SQL数据集，解决了现有方法在结构多样性和复杂性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL模型训练面临高质量、多样化、结构复杂数据集稀缺的问题。现有方法要么依赖有限的人工标注语料，要么简单地通过提示LLM合成数据而缺乏对SQL结构的显式控制，导致结构多样性和复杂性有限。

Method: EvolSQL采用结构感知的数据合成框架：1) 探索性查询-SQL扩展以增加问题多样性和模式覆盖率；2) 基于SQL抽象语法树的六个原子转换算子的自适应定向演化策略，逐步增加关系、谓词、聚合和嵌套维度的查询复杂度；3) 执行基础的SQL精炼模块和模式感知去重确保高质量、结构多样化的映射对。

Result: 实验结果表明，在EvolSQL数据上微调的7B模型，仅使用SynSQL数据集1/18的数据量，就能超越在更大SynSQL数据集上训练的模型。

Conclusion: EvolSQL通过结构感知的SQL查询演化，能够生成高质量、结构多样化的文本到SQL数据集，显著提升模型性能，为解决文本到SQL数据稀缺问题提供了有效方案。

Abstract: Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

</details>


### [71] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: Mind2Report是一个模仿商业分析师认知过程的深度研究代理，通过动态记忆增强LLM，在商业报告合成任务上优于现有基线


<details>
  <summary>Details</summary>
Motivation: 从海量嘈杂网络源合成高质量商业报告对商业决策至关重要，现有深度研究代理在质量、可靠性和覆盖范围方面仍有局限

Method: 设计无训练代理工作流，模拟商业分析师认知过程：先探测细粒度意图，然后搜索网络源并实时记录提炼信息，最后迭代合成报告，使用动态记忆增强通用LLM

Result: 在包含200个真实商业任务的QRC-Eval基准上，Mind2Report在报告质量、可靠性和覆盖范围方面优于OpenAI和Gemini等领先基线

Conclusion: 虽然这是初步研究，但Mind2Report为未来商业深度研究代理的设计提供了基础，其代码和数据已开源

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [72] [CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters](https://arxiv.org/abs/2601.04885)
*Ao Sun,Xiaoyu Wang,Zhe Tan,Yu Li,Jiachen Zhu,Shu Su,Yuheng Jia*

Main category: cs.CL

TL;DR: CuMA框架通过条件容量分离解决LLM对齐中的文化多样性问题，使用文化混合适配器防止均值崩溃，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型服务全球用户，对齐需要从强制普遍共识转向尊重文化多元主义。现有密集模型在拟合冲突价值分布时会出现"均值崩溃"，收敛到无法代表多样群体的通用平均值。

Method: 提出CuMA（文化混合适配器）框架，将对齐视为条件容量分离问题。通过人口统计感知路由，内部化潜在文化拓扑结构，将冲突梯度显式解耦到专门的专家子空间。

Result: 在WorldValuesBench、Community Alignment和PRISM等基准测试中，CuMA实现了最先进的性能，显著优于密集基线和仅语义的MoE方法。分析证实CuMA有效缓解均值崩溃，保持文化多样性。

Conclusion: CuMA通过条件容量分离框架成功解决了LLM对齐中的文化多样性挑战，为服务全球用户的大语言模型提供了尊重文化多元主义的有效解决方案。

Abstract: As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.

</details>


### [73] [Faithful Summarisation under Disagreement via Belief-Level Aggregation](https://arxiv.org/abs/2601.04889)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 提出一种分歧感知的摘要生成方法，将信念聚合与语言生成分离，使用基于距离的信念合并操作符处理冲突，再用LLM生成自然语言摘要。


<details>
  <summary>Details</summary>
Motivation: 现有LLM摘要系统在处理观点冲突时倾向于平滑分歧、过度代表多数意见，限制了摘要的忠实性，特别是在观点密集的场景中。

Method: 采用两阶段流程：1) 将文档表示为结构化信念集，使用基于距离的信念合并操作符进行显式冲突建模；2) 仅用大语言模型将聚合后的信念生成为自然语言摘要。

Result: 实验表明，虽然足够大的模型在生成时进行聚合可以匹配信念级聚合的效果，但这种行为在不同架构或能力下不稳定。信念级聚合结合简单提示能在不同模型中保持稳定且强大的分歧感知性能。

Conclusion: 信念级聚合与语言生成分离的方法能产生更忠实、分歧感知的摘要，在不同模型间表现稳定，同时保持流畅和基于事实的摘要质量。

Abstract: Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.

</details>


### [74] [V-FAT: Benchmarking Visual Fidelity Against Text-bias](https://arxiv.org/abs/2601.04897)
*Ziteng Wang,Yujie He,Guanliang Li,Siqi Yang,Jiaqi Xiong,Songxiang Liu*

Main category: cs.CL

TL;DR: 该论文研究了多模态大语言模型中的文本偏见问题，提出了V-FAT基准来评估模型对视觉证据的真实依赖程度，发现现有模型在语言主导情况下会出现视觉崩溃。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在标准视觉推理基准上表现出色，但存在过度依赖语言捷径而非真正视觉基础的问题，即"文本偏见"。研究者关注视觉感知与语言先验之间的根本张力，需要量化这种偏见的影响。

Method: 将文本偏见解耦为两个维度：内部语料偏见（来自预训练中的统计相关性）和外部指令偏见（来自对齐导致的迎合倾向）。提出V-FAT诊断基准，包含4,026个VQA实例，涵盖六个语义领域。采用三级评估框架：L1（非典型图像内部偏见）、L2（误导性指令外部偏见）、L3（两者协同偏见）。引入视觉鲁棒性评分来惩罚"幸运"的语言猜测，奖励真实的视觉保真度。

Result: 评估12个前沿MLLM发现，虽然模型在现有基准上表现出色，但在高语言主导情况下会出现显著的视觉崩溃。模型更倾向于遵循文本线索而非视觉证据。

Conclusion: 多模态大语言模型存在严重的文本偏见问题，当前评估基准未能充分捕捉模型对视觉信息的真实依赖。需要更严格的评估方法来确保模型真正基于视觉证据进行推理，而不仅仅是利用语言捷径。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.

</details>


### [75] [Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences](https://arxiv.org/abs/2601.04925)
*Arkadiusz Modzelewski,Paweł Golik,Anna Kołos,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: LLM生成的劝说性文本检测研究：虽然明显的LLM劝说文本比人类写的更容易检测，但微妙的LLM劝说文本会显著降低自动检测性能，作者创建了多语言基准Persuaficial并进行全面分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能生成高度说服力的文本，引发了对其被滥用于宣传、操纵等有害目的的担忧。核心问题是：LLM生成的劝说性文本是否比人类写的更难自动检测？

Method: 1. 对LLM生成劝说性内容的可控生成方法进行分类；2. 创建Persuaficial高质量多语言基准，涵盖英语、德语、波兰语、意大利语、法语和俄语六种语言；3. 进行广泛的实证评估，比较人类撰写和LLM生成的劝说性文本；4. 提供首次全面的语言学分析，对比人类和LLM生成的劝说性文本。

Result: 研究发现：虽然明显的LLM生成的劝说性文本比人类写的更容易检测，但微妙的LLM生成的劝说性文本会持续降低自动检测性能。研究还提供了人类和LLM生成劝说性文本的全面语言学对比分析。

Conclusion: LLM生成的劝说性文本检测存在复杂性：明显的LLM劝说相对容易检测，但微妙的劝说会显著降低检测性能。研究结果为开发更可解释和鲁棒的检测工具提供了指导，强调了需要针对不同劝说风格开发专门的检测方法。

Abstract: Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.

</details>


### [76] [GenProve: Learning to Generate Text with Fine-Grained Provenance](https://arxiv.org/abs/2601.04932)
*Jingxuan Wei,Xingyue Wang,Yanghaoyu Liao,Jie Dong,Yuchen Liu,Caijun Jia,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: 论文提出GenProve框架，通过细粒度溯源三元组（引用、压缩、推理）增强LLM回答的可验证性，解决现有引用方法无法区分直接引用与复杂推理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM引用机制过于粗粒度，无法区分直接引用和复杂推理，导致用户难以验证生成声明的支持证据。需要细粒度溯源来增强问责性和可验证性。

Method: 提出Generation-time Fine-grained Provenance任务，创建ReFInE数据集（包含专家验证的引用、压缩、推理标注），开发GenProve框架（结合SFT和GRPO），优化回答保真度和溯源正确性的复合奖励。

Result: GenProve在联合评估中显著优于14个强LLM。分析发现模型在表面引用表现良好，但在推理溯源方面存在显著差距，表明可验证推理仍是独立于表面引用的前沿挑战。

Conclusion: 细粒度溯源是增强LLM问责性的关键，但模型在推理溯源方面的困难表明可验证推理能力需要专门改进，而非仅靠表面引用机制。

Abstract: Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.

</details>


### [77] [A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction](https://arxiv.org/abs/2601.04960)
*Qing Wang,Zehan Li,Yaodong Song,Hongjie Chen,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: 提出统一口语情感智能模型，采用IEAT数据构造策略将用户情感状态及原因融入模型推理过程，通过两阶段训练实现情感感知推理，在HumDial基准上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有口语对话系统缺乏深度情感理解能力，无法将用户情感状态及其原因融入内部推理过程。需要开发能够内部化情感感知推理的模型，而非仅依赖显式监督。

Method: 1. 提出IEAT数据构造策略，将用户情感状态及其原因注入模型推理过程；2. 采用两阶段渐进训练：第一阶段通过自蒸馏进行语音-文本对齐和情感属性建模，第二阶段进行端到端跨模态联合优化确保文本与口语情感表达一致性。

Result: 在HumDial情感智能基准测试中，该方法在情感轨迹建模、情感推理和共情回应生成任务上均取得排名第一的性能，无论是基于LLM还是人工评估都表现优异。

Conclusion: IEAT策略能有效将情感感知推理内部化，两阶段训练方法确保了跨模态情感表达的一致性，为构建具有情感智能的统一口语语言模型提供了有效方案。

Abstract: This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

</details>


### [78] [Text as a Universal Interface for Transferable Personalization](https://arxiv.org/abs/2601.04963)
*Yuting Liu,Jian Guan,Jia-Nan Li,Wei Wu,Jiang-Ming Yang,Jianzhe Zhao,Guibing Guo*

Main category: cs.CL

TL;DR: 提出使用自然语言作为通用偏好表示接口，开发AlignXplore+模型生成文本偏好摘要，在9个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化方法使用隐式、模型特定的向量或参数表示用户偏好，导致难以解释的"黑盒"配置文件，且难以在不同模型和任务间迁移。需要一种通用、可解释且可重用的偏好表示方法。

Method: 采用自然语言作为模型和任务无关的偏好表示接口。提出两阶段训练框架：1) 在高质量合成数据上进行监督微调；2) 使用强化学习优化长期效用和跨任务迁移性。基于此框架开发AlignXplore+通用偏好推理模型。

Result: 在9个基准测试中，8B参数的AlignXplore+模型达到最先进性能，显著优于更大的开源模型。展现出强大的跨任务、跨模型家族和跨交互格式的迁移能力。

Conclusion: 自然语言作为偏好表示接口具有显著优势：可解释、可重用、支持持续演化。提出的训练框架和AlignXplore+模型在个性化LLM方面取得了突破性进展。

Abstract: We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.

</details>


### [79] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: 该论文提出在监督微调中同时使用正负推理轨迹，并设计GLOW损失加权方法，显著提升语言模型的OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统SFT只使用正确最终答案的正向推理轨迹，忽略了大量负向轨迹，这浪费了监督信号并加剧过拟合，限制了模型的OOD泛化能力。

Method: 1) 在SFT中同时使用正负推理轨迹；2) 提出GLOW（基于增益的损失加权）方法，根据样本在训练过程中的进步动态调整损失权重。

Result: 负向轨迹训练使OOD泛化显著提升，推理时策略熵增加35.67%；GLOW方法在Qwen2.5-7B上比仅用正向轨迹提升5.51% OOD性能，MMLU从72.82%提升到76.47%。

Conclusion: 负向推理轨迹包含有价值的中间推理信息，应被纳入SFT训练；GLOW方法能有效利用未过滤轨迹，提升模型泛化能力和推理探索能力。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [80] [Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei](https://arxiv.org/abs/2601.05004)
*Peng Wang,Xilin Tao,Siyi Yao,Jiageng Wu,Yuntao Zou,Zhuotao Tian,Libo Qin,Dagang Li*

Main category: cs.CL

TL;DR: 提出SAS多智能体框架，通过自动检索和亚文化对齐解决LLM在亚文化群体自毁行为检测中的知识滞后和语义错配问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 亚文化群体的自毁行为因其独特表达方式而难以识别，现有LLM方法面临知识滞后（亚文化俚语更新快）和语义错配（难以理解亚文化特有表达）两大挑战。

Method: 提出Subcultural Alignment Solver (SAS)多智能体框架，包含自动检索和亚文化对齐机制，增强LLM在亚文化自毁行为检测中的表现。

Result: SAS性能优于当前先进的多智能体框架OWL，与微调LLM表现相当，显著提升了亚文化自毁行为检测效果。

Conclusion: SAS框架有效解决了LLM在亚文化自毁行为检测中的关键挑战，为未来研究提供了宝贵资源，有望推动该领域发展。

Abstract: Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.

</details>


### [81] [Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models](https://arxiv.org/abs/2601.05019)
*Yueqing Hu,Xinyang Peng,Shuting Peng,Hanqi Wang,Tianhong Wang*

Main category: cs.CL

TL;DR: 推理蒸馏训练学生模型模仿教师推理轨迹，但会导致功能对齐崩溃：学生仅表面模仿语言形式，未能内化教师的动态资源分配策略，使计算成本与认知需求脱钩。


<details>
  <summary>Details</summary>
Motivation: 当前通过强化学习训练的大型推理模型与人类认知成本存在"自然"对齐，但主流推理蒸馏范式（通过监督微调让学生模仿教师推理轨迹）是否能传递这种认知结构尚不清楚。

Method: 提出"邯郸学步"（表面模仿）假说，在14个模型上进行测试，分析推理蒸馏是否导致功能对齐崩溃。通过对比教师模型和学生模型与人类难度缩放的相关性来评估认知对齐程度。

Result: 教师模型与人类难度缩放高度相关（平均r=0.64），而蒸馏后的学生模型对齐显著退化（平均r=0.34），甚至低于蒸馏前基线（出现"负迁移"）。监督微调诱导了"货物崇拜"效应，学生仅仪式性地复制推理的语言形式（冗长性），未能内化教师的动态资源分配策略。

Conclusion: 推理蒸馏使计算成本与认知需求脱钩，揭示人类般认知是主动强化的涌现属性，而非被动模仿的结果。人类认知对齐是强化学习的涌现特性，无法通过监督微调有效传递。

Abstract: Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.

</details>


### [82] [ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG](https://arxiv.org/abs/2601.05038)
*Jianbo Li,Yi Jiang,Sendong Zhao,Bairui Hu,Haochun Wang,Bing Qin*

Main category: cs.CL

TL;DR: ArcAligner是一个轻量级模块，通过自适应门控系统帮助LLM更好地利用高度压缩的上下文表示，在保持系统快速的同时提高压缩文档的理解能力。


<details>
  <summary>Details</summary>
Motivation: RAG虽然能提高LLM准确性，但长文档输入会导致模型变慢且昂贵。现有的上下文压缩方法（如摘要和嵌入压缩）存在一个矛盾：压缩越多，LLM理解能力越差。

Method: 提出ArcAligner（自适应递归上下文对齐器），一个集成到语言模型层的轻量级模块。它使用自适应门控系统，仅在信息复杂时增加额外处理能力，保持系统快速。

Result: 在知识密集型QA基准测试中，ArcAligner在可比压缩率下持续优于压缩基线，特别是在多跳和长尾设置中表现突出。

Conclusion: ArcAligner通过自适应门控机制有效解决了压缩与理解之间的权衡问题，在保持效率的同时显著提升了LLM对压缩上下文的理解能力。

Abstract: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.

</details>


### [83] [Compositional Steering of Large Language Models with Steering Tokens](https://arxiv.org/abs/2601.05062)
*Gorjan Radevski,Kiril Gashteovski,Giwon Hong,Carolin Lawrence,Goran Glavaš*

Main category: cs.CL

TL;DR: 提出组合控制令牌方法，通过自蒸馏将自然语言指令嵌入专用令牌，实现LLM的多行为组合控制，在未见过的行为和组合数量上具有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM控制方法主要针对单一行为，而实际应用需要同时满足多个需求，组合控制（同时引导LLM朝向多个行为）仍是一个未充分探索的问题。

Method: 1. 通过自蒸馏将自然语言指令表达的个体行为嵌入专用令牌；2. 在输入令牌空间而非激活空间操作；3. 训练专门的组合令牌来捕捉组合概念，支持未见过的行为和组合数量。

Result: 在不同LLM架构上的实验表明，控制令牌在多行为控制方面优于竞争方法（指令、激活控制和LoRA合并）。控制令牌与自然语言指令互补，结合使用时效果更佳。

Conclusion: 提出的组合控制令牌方法有效解决了LLM多行为组合控制问题，在输入令牌空间操作使其具有更好的零样本组合能力，并能泛化到未见过的行为和组合数量。

Abstract: Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.

</details>


### [84] [SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment](https://arxiv.org/abs/2601.05075)
*Ziyang Chen,Zhenxuan Huang,Yile Wang,Weiqin Wang,Lu Yin,Hui Huang*

Main category: cs.CL

TL;DR: SemPA是一种通过语义偏好对齐增强LLM句子表示能力的方法，在保持生成能力的同时提升语义表示质量


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式大语言模型的句子嵌入方法存在两个问题：1）使用固定提示模板缺乏模型优化，性能有限；2）修改模型架构会破坏其内部计算机制，损害生成能力。需要一种既能提升句子表示质量又不牺牲生成能力的方法。

Method: 提出SemPA方法，利用句子级直接偏好优化（DPO）在释义生成任务上高效优化LLM。模型学习区分语义等价的句子，同时保留固有的生成能力。在理论层面，建立了DPO与Plackett-Luce模型框架下对比学习的形式化联系。

Result: 在语义文本相似性任务和各种LLM基准测试中，SemPA实现了更好的语义表示，同时没有牺牲LLM固有的生成能力。

Conclusion: SemPA通过语义偏好对齐成功提升了LLM的句子表示能力，同时保持了其生成能力，为LLM的多任务优化提供了有效解决方案。

Abstract: Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

</details>


### [85] [Code-Mix Sentiment Analysis on Hinglish Tweets](https://arxiv.org/abs/2601.05091)
*Aashi Garg,Aneshya Das,Arshi Arya,Anushka Goyal,Aditi*

Main category: cs.CL

TL;DR: 该研究针对印度社交媒体中广泛使用的Hinglish（印地语-英语混合语）提出了一种高性能情感分类框架，通过微调mBERT模型和子词分词技术，解决了传统NLP模型在代码混合语言处理中的不足。


<details>
  <summary>Details</summary>
Motivation: 印度品牌监测面临Hinglish混合语言的挑战，传统NLP模型无法有效处理这种代码混合语言的句法和语义复杂性，导致情感分析不准确和市场洞察误导。

Method: 提出专门针对Hinglish推文的情感分类框架，通过微调多语言BERT（mBERT）模型，利用其多语言能力理解印度社交媒体的语言多样性，并采用子词分词技术处理拼写变体、俚语和词汇外术语。

Result: 开发了一个可用于生产环境的品牌情感追踪AI解决方案，为低资源代码混合环境下的多语言NLP建立了强有力的基准。

Conclusion: 该研究成功解决了Hinglish混合语言的情感分析难题，为印度品牌监测提供了有效的技术方案，并为多语言NLP在代码混合环境中的应用奠定了基础。

Abstract: The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.

</details>


### [86] [How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness](https://arxiv.org/abs/2601.05104)
*Florence Bernays,Marco Henriques Pereira,Jochen Menges*

Main category: cs.CL

TL;DR: 研究发现人类在与ChatGPT互动时的情感语气会影响AI的回答质量，并会延续到后续的人际交流中。赞扬能最大程度提升ChatGPT回答质量，愤怒次之，责备则无效。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索人类在与AI互动时的情感语气如何影响ChatGPT的行为表现，以及这种影响是否会延续到后续的人际交流中。

Method: 采用被试间实验设计，参与者被要求在与ChatGPT（GPT-4.0）完成两项任务时表达特定情感：撰写公开回应和处理伦理困境。比较了中性语气、赞扬、愤怒和责备四种条件下的效果。

Result: 1. 赞扬ChatGPT的回答能最大程度提升其回答质量；2. 愤怒语气也能提升回答质量但效果较小；3. 责备语气无法改善回答质量；4. 在处理伦理困境时，愤怒语气使ChatGPT减少对企业利益的关注，责备语气则增加对公共利益的保护；5. 责备ChatGPT后，人们在后续人际交流中使用更多负面、敌对和失望的表达。

Conclusion: 人类在与AI互动时的情感语气不仅会影响ChatGPT的输出质量，还会延续到后续的人际交流中，形成情感传递效应。

Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.

</details>


### [87] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 该论文是第一篇全面调查从LLM-as-a-Judge向Agent-as-a-Judge范式转变的系统性综述，建立了发展分类法并提供了该领域的路线图。


<details>
  <summary>Details</summary>
Motivation: 随着被评估对象变得越来越复杂、专业化和多步骤，LLM-as-a-Judge方法受到固有偏见、浅层单次推理和无法验证评估结果等限制，这促使了向Agent-as-a-Judge的范式转变。然而，该领域缺乏统一框架来导航这一快速发展的格局。

Method: 作者通过识别表征这一范式转变的关键维度，建立发展分类法，组织核心方法论，并调查通用和专业领域的应用，提供了该领域的系统性综述。

Result: 提出了第一个全面追踪从LLM-as-a-Judge到Agent-as-a-Judge演变的调查，建立了统一的框架和发展分类法，分析了前沿挑战并确定了有前景的研究方向。

Conclusion: 该论文为下一代智能体评估提供了清晰的路线图，通过系统性综述填补了该领域缺乏统一框架的空白，有助于推动更稳健、可验证和细致的评估方法发展。

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [88] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: DocDancer是一个端到端训练的开源文档问答智能体，通过工具驱动框架解决文档探索与理解问题，使用探索-合成数据生成方法解决训练数据稀缺问题，在多个长文档理解基准测试中表现有效。


<details>
  <summary>Details</summary>
Motivation: 现有文档问答智能体缺乏有效的工具利用能力，且主要依赖闭源模型，需要开发开源、工具驱动的端到端文档问答解决方案。

Method: 将文档问答建模为信息检索问题，提出工具驱动的智能体框架，明确建模文档探索和理解过程；设计探索-合成数据合成管道，生成高质量训练数据以支持端到端训练。

Result: 在MMLongBench-Doc和DocBench两个长上下文文档理解基准测试上训练模型，展示了方法的有效性；进一步分析为智能体工具设计和合成数据提供了有价值的见解。

Conclusion: DocDancer是一个有效的开源文档问答智能体，通过工具驱动框架和数据合成方法解决了现有方法的局限性，为文档理解和智能体设计提供了新的方向。

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [89] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: RelayLLM：一种通过令牌级协作解码实现高效推理的新框架，让SLM作为主动控制器动态调用LLM处理关键令牌，大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有协作方法（如级联或路由）在粗粒度上将整个查询卸载给LLM，当SLM能够处理大部分推理步骤时会造成显著的计算浪费。需要更细粒度的协作机制来平衡性能和成本。

Method: 提出RelayLLM框架，采用令牌级协作解码：SLM作为主动控制器，通过特殊命令动态调用LLM处理关键令牌。使用两阶段训练框架：预热阶段和组相对策略优化（GRPO），教导模型在独立推理和策略性寻求帮助之间取得平衡。

Result: 在六个基准测试中，RelayLLM平均准确率达到49.52%，有效弥合了两种模型之间的性能差距。仅调用LLM处理总生成令牌的1.07%，相比性能匹配的随机路由器实现了98.2%的成本降低。

Conclusion: RelayLLM通过细粒度的令牌级协作，在保持高性能的同时显著降低了计算成本，为高效推理提供了一种有前景的解决方案。

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [90] [Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference](https://arxiv.org/abs/2601.05170)
*Rasmus Blanck,Bill Noble,Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 本文分析了自然语言推理（NLI）任务中标签集的三种可能解读，通过研究SNLI数据集来评估模型在元推理一致性方面的表现，并探究数据集编码了哪种逻辑关系解读。


<details>
  <summary>Details</summary>
Motivation: 自然语言推理（NLI）是评估语言模型自然语言理解能力的重要任务，但其逻辑特性理解不足且经常被错误描述。理解NLI捕捉的推理概念对于解释模型在该任务上的表现至关重要。

Method: 1. 形式化了NLI标签集的三种可能解读
2. 全面分析了这些解读所蕴含的元推理特性
3. 聚焦SNLI数据集，利用两种数据：
   a) 具有共享前提的NLI项目
   b) 由LLM生成的项目
4. 评估在SNLI上训练的模型的元推理一致性

Result: 通过分析得出了关于哪种逻辑关系解读被数据集编码的见解，并评估了模型在元推理一致性方面的表现。

Conclusion: 理解NLI标签集的不同逻辑解读对于正确解释模型性能至关重要，通过元推理一致性分析可以揭示数据集编码的推理逻辑特性。

Abstract: Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.

</details>


### [91] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: Inside Out框架使用PersonaTree进行长期用户画像，通过轻量级MemListener动态管理记忆，在保持一致性的同时压缩记忆，提升个性化对话质量。


<details>
  <summary>Details</summary>
Motivation: 现有长期个性化对话系统面临无限交互流与有限上下文约束的矛盾，存在记忆噪声累积、推理退化、人设不一致等问题，需要更有效的长期用户画像管理方案。

Method: 提出Inside Out框架：1) 使用全局维护的PersonaTree作为长期用户画像载体，通过初始模式约束主干并更新枝叶实现可控增长；2) 通过强化学习训练轻量级MemListener，产生结构化、可执行、可解释的{ADD, UPDATE, DELETE, NO_OP}操作；3) 响应生成时直接利用PersonaTree增强输出，或触发代理模式按需引入细节。

Result: PersonaTree在抑制上下文噪声和保持人设一致性方面优于全文拼接和各种个性化记忆系统。轻量级MemListener在记忆操作决策性能上达到甚至超过DeepSeek-R1-0528和Gemini-3-Pro等强大推理模型。

Conclusion: Inside Out框架通过PersonaTree和MemListener的组合，有效解决了长期个性化对话中的记忆管理问题，实现了记忆压缩与一致性的平衡，为实际应用提供了高效解决方案。

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [92] [LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation](https://arxiv.org/abs/2601.05192)
*Samy Haffoudhi,Fabian M. Suchanek,Nils Holzenberger*

Main category: cs.CL

TL;DR: LELA是一种无需微调的模块化粗到细实体链接方法，利用大语言模型能力，在不同领域、知识库和LLM上表现优异


<details>
  <summary>Details</summary>
Motivation: 实体链接是知识图谱构建、问答和信息提取等任务的基础步骤，现有方法通常需要针对特定领域或知识库进行微调，缺乏通用性

Method: LELA采用模块化粗到细方法，利用大语言模型能力，无需微调阶段，可适应不同目标领域、知识库和大语言模型

Result: 在各种实体链接设置下的实验表明，LELA与微调方法具有高度竞争力，并显著优于非微调方法

Conclusion: LELA提供了一种通用且有效的实体链接解决方案，无需微调即可在不同场景下实现高性能，展示了LLM在实体链接任务中的强大潜力

Abstract: Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

</details>


### [93] [Measuring and Fostering Peace through Machine Learning and Artificial Intelligence](https://arxiv.org/abs/2601.05232)
*P. Gilda,P. Dungarwal,A. Thongkham,E. T. Ajayi,S. Choudhary,T. M. Terol,C. Lam,J. P. Araujo,M. McFadyen-Mungalln,L. S. Liebovitch,P. T. Coleman,H. West,K. Sieck,S. Carter*

Main category: cs.CL

TL;DR: 使用机器学习从新闻和社交媒体测量和平水平，并开发Chrome扩展MirrorMirror为YouTube观众提供实时反馈，促进和平媒体消费


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体和新闻媒体存在情绪化偏见，特别是71%的20-40岁年轻人通过短视频获取新闻，内容创作者倾向于使用情感激活（如愤怒）来增加点击率，这不利于和平氛围的营造

Method: 1) 使用神经网络从在线新闻文本嵌入中测量和平水平；2) 为YouTube等社交媒体开发基于词级（GoEmotions）和上下文级（大语言模型）的方法来测量和平相关社会维度；3) 开发并测试Chrome扩展MirrorMirror，为用户提供媒体和平度的实时反馈

Result: 新闻媒体模型在一个数据集上训练后，在另一个新闻数据集上也表现出高准确性。开发了MirrorMirror工具，长期目标是将其发展为开源工具，帮助内容创作者、记者、研究人员、平台和用户更好地理解媒体内容的基调及其对观众的影响

Conclusion: 通过机器学习测量媒体和平度并开发实时反馈工具，可以超越简单的参与度指标，鼓励更尊重、细致和富有信息的沟通，从而促进和平

Abstract: We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.

</details>


### [94] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: GDPO提出一种新的策略优化方法，通过解耦多奖励归一化来解决GRPO在多奖励设置中的崩溃问题，在工具调用、数学推理和代码推理任务中表现优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能力增强，用户期望它们不仅提供准确响应，还要在不同场景中符合多样化的人类偏好。虽然强化学习管道开始整合多个奖励来引导模型行为，但现有方法GRPO在多奖励设置中存在问题，导致训练信号分辨率降低和收敛不理想。

Method: 提出Group reward-Decoupled Normalization Policy Optimization (GDPO)，通过解耦单个奖励的归一化来更准确地保留奖励间的相对差异，实现更精确的多奖励优化并显著提高训练稳定性。

Result: 在工具调用、数学推理和代码推理三个任务中，GDPO在正确性指标（准确率、错误率）和约束遵循指标（格式、长度）上都一致优于GRPO，证明了其有效性和泛化能力。

Conclusion: GDPO解决了GRPO在多奖励强化学习中的归一化崩溃问题，通过解耦奖励归一化更好地保留了奖励差异，为多奖励优化提供了更稳定有效的解决方案。

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [95] [Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization](https://arxiv.org/abs/2601.04334)
*Amit Jain,Richard Linares*

Main category: cs.RO

TL;DR: 该论文提出了一种结合大型语言模型（LLM）与组相对策略优化（GRPO）的学习型制导控制方法，通过两阶段训练（监督微调+GRPO）在多种动态系统中实现稳定控制，并提供可解释的决策过程。


<details>
  <summary>Details</summary>
Motivation: 传统控制方法在处理复杂非线性系统时面临挑战，而LLM具有推理能力但缺乏控制专业知识。本研究旨在结合LLM的推理能力和GRPO的优化技术，开发能够处理从线性到非线性复杂系统的可解释控制策略。

Method: 采用两阶段训练方法：1）监督微调（SFT）学习格式化和控制原语；2）组相对策略优化（GRPO）通过交互驱动策略改进。在四个控制问题上验证，涵盖从线性系统到非线性振荡动力学，再到三维航天器姿态控制（含陀螺耦合和推力约束）。

Result: 结果显示，通过GRPO优化的具有显式推理能力的LLM能够在一致的训练设置下，为线性和非线性系统合成可行的稳定策略。两阶段训练方法使模型能够生成控制序列，同时提供人类可读的决策过程解释。

Conclusion: 这项工作为将基于GRPO的推理应用于自主控制系统奠定了基础，在航空航天和其他安全关键领域具有潜在应用价值。证明了LLM与GRPO结合能够处理复杂动态系统的控制问题，并提供可解释的决策过程。

Abstract: This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.

</details>


### [96] [UNIC: Learning Unified Multimodal Extrinsic Contact Estimation](https://arxiv.org/abs/2601.04356)
*Zhengtong Xu,Yuki Shirai*

Main category: cs.RO

TL;DR: UNIC是一个无需先验知识或相机标定的统一多模态框架，用于估计抓取物体与环境之间的外部接触，通过场景可及性图和随机掩码多模态融合实现鲁棒学习。


<details>
  <summary>Details</summary>
Motivation: 接触丰富的操作需要可靠的外部接触估计，但现有方法依赖预定义接触类型、固定抓取配置或相机标定等限制性假设，阻碍了在新物体和非结构化环境中的泛化能力。

Method: UNIC直接在相机坐标系中编码视觉观察，与本体感觉和触觉模态以完全数据驱动方式集成；采用基于场景可及性图的统一接触表示，捕捉多样接触形态；使用带随机掩码的多模态融合机制进行鲁棒表示学习。

Result: 在未见过的接触位置实现9.6毫米平均Chamfer距离误差；在未见物体上表现良好；对缺失模态保持鲁棒性；能适应动态相机视角。

Conclusion: UNIC将外部接触估计确立为接触丰富操作中实用且通用的能力，无需先验知识或相机标定即可实现可靠估计。

Abstract: Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.

</details>


### [97] [Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces](https://arxiv.org/abs/2601.04401)
*Arsyi Aziz,Peng Wei*

Main category: cs.RO

TL;DR: 该论文提出了一种基于多智能体强化学习的分散式飞机间隔保障方法，通过相对极坐标状态空间和Transformer编码器架构，在结构化和非结构化空域中实现更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的计量方法依赖预先计算的严格调度，限制了高级空中交通管理所需的灵活性。现有MARL方法容易过拟合特定空域结构，难以适应新配置。

Method: 将MARL问题重新构建在相对极坐标状态空间中，使用Transformer编码器模型在不同交通模式和交叉角度下训练，为飞机提供速度建议以解决冲突并保持巡航速度。

Result: 单层编码器配置优于深层变体，实现了接近零的空中接近率，分离违规时间更短，且优于纯注意力基线模型。

Conclusion: 新的状态表示、神经网络架构设计和训练策略为结构化和非结构化空域提供了可适应、可扩展的分散式飞机间隔保障解决方案。

Abstract: Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.

</details>


### [98] [Fast Continuum Robot Shape and External Load State Estimation on SE(3)](https://arxiv.org/abs/2601.04493)
*James M. Ferguson,Alan Kuntz,Tucker Hermans*

Main category: cs.RO

TL;DR: 提出一个通用的连续体机器人时空状态估计框架，结合驱动输入、外力、测量不确定性和时间先验，通过因子图优化实现实时状态估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于流形的方法通常采用简化的Cosserat杆模型，无法直接考虑驱动输入或外部载荷。需要一种能统一处理驱动不确定性、外力、边界条件和各种测量的通用框架。

Method: 提出一个通用框架，包含驱动不确定性、外力矩、过程噪声、边界条件和任意背部测量模型。通过添加时间先验，在空间（弧长）和时间域进行联合估计。将弧长域离散化为因子图表示，利用类似SLAM的稀疏非线性优化进行快速批量计算。

Result: 在仿真中展示了肌腱驱动机器人的实时运动学（含不确定性）、从位置反馈的末端力感知、以及从背部应变的分布式载荷估计。在实验中验证了手术同心管机器人的精确运动学和末端力估计，展示了手术触诊的潜力。

Conclusion: 该框架为连续体机器人提供了一个通用的时空状态估计方法，能够处理各种不确定性源，在仿真和实验中均表现出良好性能，具有广泛的应用前景。

Abstract: Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads. We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements. By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \textit{spacetime} state estimation. Discretizing the arclength domain yields a factor graph representation of the continuum robot model, which can be exploited for fast batch sparse nonlinear optimization in the style of SLAM. The framework is general and applies to a broad class of continuum robots; as illustrative cases, we show (i) tendon-driven robots in simulation, where we demonstrate real-time kinematics with uncertainty, tip force sensing from position feedback, and distributed load estimation from backbone strain, and (ii) a surgical concentric tube robot in experiment, where we validate accurate kinematics and tip force estimation, highlighting potential for surgical palpation.

</details>


### [99] [Multiagent Reinforcement Learning with Neighbor Action Estimation](https://arxiv.org/abs/2601.04511)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu*

Main category: cs.RO

TL;DR: 提出一种增强型多智能体强化学习框架，通过动作估计神经网络推断邻居行为，无需显式动作交换，适用于通信受限的真实工程环境。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法依赖智能体间的显式动作交换，但在真实工程环境中常因通信限制、延迟、能耗和可靠性要求而不切实际，需要一种无需显式信息共享的协作决策方法。

Method: 提出增强型多智能体强化学习框架，集成轻量级动作估计模块，每个智能体仅使用局部可观测信息推断邻居行为，与标准TD3算法完全兼容并可扩展到更大系统。

Result: 在双臂机器人操作任务（协作举升物体）中验证，该方法显著增强了真实机器人系统的鲁棒性和部署可行性，同时减少了对信息基础设施的依赖。

Conclusion: 该研究推动了去中心化多智能体人工智能系统的发展，使AI能在动态、信息受限的真实环境中有效运行，为工程应用提供了实用解决方案。

Abstract: Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.

</details>


### [100] [Design and Development of Modular Limbs for Reconfigurable Robots on the Moon](https://arxiv.org/abs/2601.04541)
*Gustavo H. Diaz,A. Sejal Jain,Matteo Brugnera,Elian Neppel,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发名为Moonbots的4自由度机器人肢体模块，可与轮式模块连接组成不同配置，用于月球探索和建设的模块化机器人系统


<details>
  <summary>Details</summary>
Motivation: 为月球探索和建设任务开发模块化机器人系统，在资源受限的空间任务中提供灵活性和多功能性

Method: 采用统一的高扭矩-速度比驱动器设计，开发4自由度机器人肢体模块和轮式模块，实现硬件实现、机械设计和软件架构

Result: 展示了9种功能配置（4DOF肢体、8DOF肢体、车辆、龙形、最小型、四足、货物、最小货物、自行车），验证了系统的适应性和控制性能

Conclusion: 模块化机器人系统为可重构机器人研究提供了实用基础，统一驱动器设计简化了开发维护，系统展示了在不同环境和任务中的适应性

Abstract: In this paper, we present the development of 4-DOF robot limbs, which we call Moonbots, designed to connect in various configurations with each other and wheel modules, enabling adaptation to different environments and tasks. These modular components are intended primarily for robotic systems in space exploration and construction on the Moon in our Moonshot project. Such modular robots add flexibility and versatility for space missions where resources are constrained. Each module is driven by a common actuator characterized by a high torque-to-speed ratio, supporting both precise control and dynamic motion when required. This unified actuator design simplifies development and maintenance across the different module types. The paper describes the hardware implementation, the mechanical design of the modules, and the overall software architecture used to control and coordinate them. Additionally, we evaluate the control performance of the actuator under various load conditions to characterize its suitability for modular robot applications. To demonstrate the adaptability of the system, we introduce nine functional configurations assembled from the same set of modules: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. These configurations reflect different locomotion strategies and task-specific behaviors, offering a practical foundation for further research in reconfigurable robotic systems.

</details>


### [101] [Data-Driven Terramechanics Approach Towards a Realistic Real-Time Simulator for Lunar Rovers](https://arxiv.org/abs/2601.04547)
*Jakob M. Kern,James M. Hurrell,Shreya Santra,Keisuke Takehana,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发了一种结合高视觉保真度和真实地形交互的月球表面模拟器，采用数据驱动的回归模型来模拟轮-土相互作用，支持实时应用。


<details>
  <summary>Details</summary>
Motivation: 当前月球表面模拟器要么注重视觉真实感，要么注重物理准确性，无法全面复现月球条件，限制了其在漫游车操作和任务规划测试中的能力。

Method: 采用数据驱动方法，使用从全车和单轮实验及模拟收集的数据构建回归模型，预测滑移和沉陷行为；同时改进地形变形和车轮轨迹可视化的真实感。

Result: 回归式地形力学模型能准确复现稳态和动态滑移以及沉陷行为，在平坦地形和20度斜坡上经过现场测试验证；实现了物理合理的地形响应和高视觉保真度的结合。

Conclusion: 该方法成功填补了视觉真实感和物理准确性之间的差距，为需要物理合理地形响应和高视觉保真度的实时应用提供了支持。

Abstract: High-fidelity simulators for the lunar surface provide a digital environment for extensive testing of rover operations and mission planning. However, current simulators focus on either visual realism or physical accuracy, which limits their capability to replicate lunar conditions comprehensively. This work addresses that gap by combining high visual fidelity with realistic terrain interaction for a realistic representation of rovers on the lunar surface. Because direct simulation of wheel-soil interactions is computationally expensive, a data-driven approach was adopted, using regression models for slip and sinkage from data collected in both full-rover and single-wheel experiments and simulations. The resulting regression-based terramechanics model accurately reproduced steady-state and dynamic slip, as well as sinkage behavior, on flat terrain and slopes up to 20 degrees, with validation against field test results. Additionally, improvements were made to enhance the realism of terrain deformation and wheel trace visualization. This method supports real-time applications that require physically plausible terrain response alongside high visual fidelity.

</details>


### [102] [Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain](https://arxiv.org/abs/2601.04551)
*Riku Suzuki,Ayumi Umemura,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出一种基于离散傅里叶变换的点云地图压缩方法，通过转换数字高程模型到频域并去除高频分量，适用于行星、沙漠等渐变地形，在保持精度的同时实现有效压缩。


<details>
  <summary>Details</summary>
Motivation: SLAM技术在无人机器人探索任务中至关重要，但机载计算能力和通信带宽有限，而点云数据量大，需要有效的压缩方法来解决这一问题。

Method: 使用离散傅里叶变换将数字高程模型转换为频域2D图像，去除高频分量，专注于渐变地形（如行星、沙漠）的表示，因为这些地形的高频成分对表示贡献较小。

Result: 在两个不同高程剖面的地形相机序列上评估了压缩率和精度，证明了该方法在压缩数据大小的同时不会显著降低点云质量。

Conclusion: 提出的基于DFT的点云压缩方法对于渐变地形是有效的，能够在保持精度的同时实现显著的数据压缩，适用于计算和通信资源受限的机器人探索任务。

Abstract: Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions. While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods. To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT). The proposed technique converts the Digital Elevation Model (DEM) to the frequency-domain 2D image and omits its high-frequency components, focusing on the exploration of gradual terrains such as planets and deserts. Unlike terrains with detailed structures such as artificial environments, high-frequency components contribute little to the representation of gradual terrains. Thus, this method is effective in compressing data size without significant degradation of the point cloud. We evaluated the method in terms of compression rate and accuracy using camera sequences of two terrains with different elevation profiles.

</details>


### [103] [UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation](https://arxiv.org/abs/2601.04629)
*Zhongxuan Li,Zeliang Guo,Jun Hu,David Navarro-Alarcon,Jia Pan,Hongmin Wu,Peng Zhou*

Main category: cs.RO

TL;DR: UniBiDex是一个统一的双手机器人灵巧操作遥操作框架，支持VR和领导者-跟随者两种输入模式，通过零空间控制优化双手配置，在厨房整理任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人双手机器人灵巧操作需要高质量的遥操作框架来收集大规模人类演示数据，但现有系统往往只支持单一输入模式，缺乏统一的控制架构。

Method: UniBiDex将异构输入设备集成到共享控制堆栈中，采用一致的动力学处理和安全性保证，使用零空间控制优化双手配置，确保平滑、无碰撞、能感知奇异点的运动。

Result: 在包含五个连续操作子任务的厨房整理任务中，UniBiDex相比强基线方法表现出更高的任务成功率、更平滑的轨迹和更好的鲁棒性。

Conclusion: UniBiDex通过开源所有硬件和软件组件，旨在降低收集大规模高质量人类演示数据的门槛，加速机器人学习进展。

Abstract: We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees The framework employs nullspace control to optimize bimanual configurations ensuring smooth collisionfree and singularityaware motion across tasks We validate UniBiDex on a longhorizon kitchentidying task involving five sequential manipulation subtasks demonstrating higher task success rates smoother trajectories and improved robustness compared to strong baselines By releasing all hardware and software components as opensource we aim to lower the barrier to collecting largescale highquality human demonstration datasets and accelerate progress in robot learning.

</details>


### [104] [Model of Spatial Human-Agent Interaction with Consideration for Others](https://arxiv.org/abs/2601.04657)
*Takafumi Sakamoto,Yugo Takeuchi*

Main category: cs.RO

TL;DR: 构建了一个考虑他人通信意愿的计算空间交互模型，通过VR实验验证模型能有效调节机器人行为以避免打扰行人


<details>
  <summary>Details</summary>
Motivation: 公共空间中的通信机器人需要主动发起对话，但又不能打扰行人，因此需要根据他人行为估计其通信意愿并相应调整自身行为

Method: 构建了一个计算空间交互模型，将"考虑他人"量化为参数：根据估计的他人内部状态调整自身内部状态的程度。在VR环境中进行人机交互实验验证模型

Result: 实验显示：当参与者向目标移动时，低考虑值的虚拟机器人会抑制参与者移动，而高考虑值的机器人不会抑制；当参与者接近机器人时，无论考虑值高低，机器人都会表现出接近行为，从而减少参与者移动

Conclusion: 提出的模型能够阐明考虑他人的交互行为，验证了模型在调节机器人通信行为以避免打扰行人方面的有效性

Abstract: Communication robots often need to initiate conversations with people in public spaces. At the same time, such robots must not disturb pedestrians. To handle these two requirements, an agent needs to estimate the communication desires of others based on their behavior and then adjust its own communication activities accordingly. In this study, we construct a computational spatial interaction model that considers others. Consideration is expressed as a quantitative parameter: the amount of adjustment of one's internal state to the estimated internal state of the other. To validate the model, we experimented with a human and a virtual robot interacting in a VR environment. The results show that when the participant moves to the target, a virtual robot with a low consideration value inhibits the participant's movement, while a robot with a higher consideration value did not inhibit the participant's movement. When the participant approached the robot, the robot also exhibited approaching behavior, regardless of the consideration value, thus decreasing the participant's movement. These results appear to verify the proposed model's ability to clarify interactions with consideration for others.

</details>


### [105] [Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture](https://arxiv.org/abs/2601.04668)
*Laukik Patade,Rohan Rane,Sandeep Pillai*

Main category: cs.RO

TL;DR: 该研究使用深度强化学习在连续动作空间中优化农业无人地面车辆的路径规划，从传统网格方法转向DRL方法，最终在三维动态环境中验证了TD3算法的有效性，达到95%的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统网格路径规划方法（如A*和Dijkstra算法）在动态农业环境中存在局限性，需要更自适应、灵活的学习策略来应对复杂多变的农业场景。

Method: 研究采用渐进式方法：1）回顾传统网格方法；2）探索离散动作空间的DRL方法（DQN及其变体）；3）转向连续动作空间模型（DDPG和TD3）；4）在ROS和Gazebo构建的三维动态环境中进行实验验证。

Result: 连续DRL算法在动态农业场景中表现优异，特别是预训练的TD3智能体在动态环境中达到95%的成功率，能够有效处理移动障碍物，同时确保作物和机器人的安全。

Conclusion: 连续动作空间的深度强化学习（特别是TD3算法）为无人地面车辆在动态农业环境中的路径规划提供了有效的解决方案，相比传统方法具有更好的适应性和鲁棒性。

Abstract: This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra's algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.

</details>


### [106] [SeqWalker: Sequential-Horizon Vision-and-Language Navigation with Hierarchical Planning](https://arxiv.org/abs/2601.04699)
*Zebin Han,Xudong Wang,Baichen Liu,Qi Lyu,Zhenduo Shang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.RO

TL;DR: SeqWalker：用于顺序-视野视觉语言导航的分层规划框架，通过高层指令选择和低层探索-验证策略解决多任务导航中的信息过载问题


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言导航模型在处理复杂、长视野的多任务指令时性能显著下降，信息过载导致代理难以关注观察相关的细节

Method: SeqWalker采用分层规划框架：1）高层规划器根据当前视觉观察动态选择全局指令中的相关子指令，降低认知负荷；2）低层规划器采用探索-验证策略，利用指令的内在逻辑结构进行轨迹错误校正

Result: 论文扩展了IVLN数据集并建立了新的基准测试，大量实验证明了SeqWalker的优越性

Conclusion: SeqWalker通过分层规划有效解决了顺序-视野视觉语言导航中的信息过载问题，提高了多任务导航性能

Abstract: Sequential-Horizon Vision-and-Language Navigation (SH-VLN) presents a challenging scenario where agents should sequentially execute multi-task navigation guided by complex, long-horizon language instructions. Current vision-and-language navigation models exhibit significant performance degradation with such multi-task instructions, as information overload impairs the agent's ability to attend to observationally relevant details. To address this problem, we propose SeqWalker, a navigation model built on a hierarchical planning framework. Our SeqWalker features: i) A High-Level Planner that dynamically selects global instructions into contextually relevant sub-instructions based on the agent's current visual observations, thus reducing cognitive load; ii) A Low-Level Planner incorporating an Exploration-Verification strategy that leverages the inherent logical structure of instructions for trajectory error correction. To evaluate SH-VLN performance, we also extend the IVLN dataset and establish a new benchmark. Extensive experiments are performed to demonstrate the superiority of the proposed SeqWalker.

</details>


### [107] [Zero Wrench Control via Wrench Disturbance Observer for Learning-free Peg-in-hole Assembly](https://arxiv.org/abs/2601.04881)
*Kiyoung Choi,Juwon Jeong,Sehoon Oh*

Main category: cs.RO

TL;DR: 提出动态力矩扰动观测器(DW-DOB)，通过将任务空间惯性嵌入观测器名义模型，实现接触丰富操作中的高灵敏度零力矩控制，能清晰分离内在动态反应与真实外部力矩。


<details>
  <summary>Details</summary>
Motivation: 传统观测器无法补偿惯性效应，导致在接触丰富操作中无法实现高灵敏度的零力矩控制。需要一种能分离内在动态反应和外部力矩的方法，以实现精确的力控操作。

Method: 设计动态力矩扰动观测器(DW-DOB)，将任务空间惯性嵌入观测器名义模型中，从而清晰分离机器人内在动态反应与真实外部接触力矩。采用基于无源性的稳定性分析确保动态条件下的稳定交互。

Result: 在工业公差(H7/h6)的轴孔装配实验中，DW-DOB实现了更深、更柔顺的插入，残余力矩最小，性能优于传统力矩扰动观测器和PD基线控制器。

Conclusion: DW-DOB是一种实用的无需学习的解决方案，适用于接触丰富任务中的高精度零力矩控制，能有效解决传统观测器无法补偿惯性效应的缺陷。

Abstract: This paper proposes a Dynamic Wrench Disturbance Observer (DW-DOB) designed to achieve highly sensitive zero-wrench control in contact-rich manipulation. By embedding task-space inertia into the observer nominal model, DW-DOB cleanly separates intrinsic dynamic reactions from true external wrenches. This preserves sensitivity to small forces and moments while ensuring robust regulation of contact wrenches. A passivity-based analysis further demonstrates that DW-DOB guarantees stable interactions under dynamic conditions, addressing the shortcomings of conventional observers that fail to compensate for inertial effects. Peg-in-hole experiments at industrial tolerances (H7/h6) validate the approach, yielding deeper and more compliant insertions with minimal residual wrenches and outperforming a conventional wrench disturbance observer and a PD baseline. These results highlight DW-DOB as a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks.

</details>


### [108] [SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles](https://arxiv.org/abs/2601.04948)
*Junchi Gu,Feiyang Yuan,Weize Shi,Tianchen Huang,Haopeng Zhang,Xiaohu Zhang,Yu Wang,Wei Gao,Shiwu Zhang*

Main category: cs.RO

TL;DR: 提出一种配备被动轮的人形机器人，通过深度强化学习控制实现轮滑滑行，相比传统行走显著降低冲击强度和运输成本


<details>
  <summary>Details</summary>
Motivation: 传统人形机器人行走和跑步时频繁的足部撞击会产生高瞬时冲击力，导致关节磨损加剧和能量利用率差。轮滑作为一种具有重要生物力学价值的运动，可以通过合理利用身体惯性实现快速连续滑动，动能损失最小

Method: 设计每只脚配备四个被动轮的新型人形机器人，开发基于深度强化学习的控制框架，针对轮滑特有的swizzle步态设计奖励函数

Result: 在仿真中分析学习策略后部署到物理机器人上，证明swizzle步态相比传统双足行走在冲击强度和运输成本方面更平滑高效。这两个指标分别降低了75.86%和63.34%

Conclusion: 轮滑作为一种优越的运动模式，能够显著提高能量效率和延长关节寿命，为人形机器人提供了一种更高效、更平滑的运动方式

Abstract: Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\%$ and $63.34\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.

</details>


### [109] [When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics](https://arxiv.org/abs/2601.04982)
*Johannes A. Gaus,Winfried Ilg,Daniel Haeufle*

Main category: cs.RO

TL;DR: 提出基于校准概率的安全关键触发框架，用于日常活动中多模态下一动作预测，通过后处理校准使预测置信度与经验可靠性对齐，并设计ACT/HOLD规则确保仅在可靠性高时提供辅助


<details>
  <summary>Details</summary>
Motivation: 辅助设备在提供支持前需要准确判断用户的意图及其预测的可靠性。原始模型置信度往往无法反映真实正确性，存在安全风险，需要建立安全关键的触发机制

Method: 1) 对多模态下一动作预测模型进行后处理校准，使预测置信度与经验可靠性对齐；2) 基于校准后的置信度设计简单的ACT/HOLD规则：仅在可靠性高时采取行动，否则暂缓辅助；3) 将置信度阈值转化为辅助动作的量化安全参数

Result: 后处理校准将误校准降低约一个数量级，同时不影响预测准确性。校准后的置信度驱动ACT/HOLD规则，使置信度阈值成为辅助动作的可验证安全参数，实现了辅助控制回路中的可验证行为

Conclusion: 基于校准概率的安全关键触发框架能够显著提高辅助设备的安全性，通过将置信度阈值转化为量化安全参数，实现了可验证的辅助行为控制，为日常活动中的多模态动作预测提供了可靠的安全保障机制

Abstract: Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.

</details>


### [110] [The RoboSense Challenge: Sense Anything, Navigate Anywhere, Adapt Across Platforms](https://arxiv.org/abs/2601.05014)
*Lingdong Kong,Shaoyuan Xie,Zeying Gong,Ye Li,Meng Chu,Ao Liang,Yuhao Dong,Tianshuai Hu,Ronghe Qiu,Rong Li,Hanjiang Hu,Dongyue Lu,Wei Yin,Wenhao Ding,Linfeng Li,Hang Song,Wenwei Zhang,Yuexin Ma,Junwei Liang,Zhedong Zheng,Lai Xing Ng,Benoit R. Cottereau,Wei Tsang Ooi,Ziwei Liu,Zhanpeng Zhang,Weichao Qiu,Wei Zhang,Ji Ao,Jiangpeng Zheng,Siyu Wang,Guang Yang,Zihao Zhang,Yu Zhong,Enzhu Gao,Xinhan Zheng,Xueting Wang,Shouming Li,Yunkai Gao,Siming Lan,Mingfei Han,Xing Hu,Dusan Malic,Christian Fruhwirth-Reisinger,Alexander Prutsch,Wei Lin,Samuel Schulter,Horst Possegger,Linfeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li,Youngseok Kim,Sihwan Hwang,Hyeonjun Jeong,Aodi Wu,Xubo Luo,Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao,Shuangzhi Li,Junlong Shen,Xingyu Li,Hao Ruan,Jinliang Lin,Zhiming Luo,Yu Zang,Cheng Wang,Hanshi Wang,Xijie Gong,Yixiang Yang,Qianli Ma,Zhipeng Zhang,Wenxiang Shi,Jingmeng Zhou,Weijun Zeng,Kexin Xu,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Xiyan Feng,Wenbo Zhang,Lu Zhang,Yunzhi Zhuge,Huchuan Lu,You He,Seungjun Yu,Junsung Park,Youngsun Lim,Hyunjung Shim,Faduo Liang,Zihang Wang,Yiming Peng,Guanyu Zong,Xu Li,Binghao Wang,Hao Wei,Yongxin Ma,Yunke Shi,Shuaipeng Liu,Dong Kong,Yongchun Lin,Huitong Yang,Liang Lei,Haoang Li,Xinliang Zhang,Zhiyong Wang,Xiaofeng Wang,Yuxia Fu,Yadan Luo,Djamahl Etchegaray,Yang Li,Congfei Li,Yuxiang Sun,Wenkai Zhu,Wang Xu,Linru Li,Longjie Liao,Jun Yan,Benwu Wang,Xueliang Ren,Xiaoyu Yue,Jixian Zheng,Jinfeng Wu,Shurui Qin,Wei Cong,Yao He*

Main category: cs.RO

TL;DR: RoboSense 2025挑战赛旨在推进机器人感知在多样化传感场景中的鲁棒性和适应性，包含五个互补研究赛道，吸引了全球广泛参与，通过分析获胜方案揭示了方法趋势和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 自主系统在开放动态环境中部署时，感知模型需要在传感器噪声、环境变化和平台差异下保持可靠。现有方法在未见条件下性能下降，需要更鲁棒和可泛化的机器人感知能力。

Method: 挑战赛设计了五个互补研究赛道：语言基础决策、社会合规导航、传感器配置泛化、跨视图跨模态对应、跨平台3D感知。提供标准化数据集、基线模型和统一评估协议。

Result: 吸引了来自16个国家85个机构的143支团队参与，反映了广泛的社区参与度。通过分析23个获胜解决方案，揭示了新兴方法趋势和共享设计原则。

Conclusion: RoboSense 2025挑战赛为评估真实世界感知可靠性提供了全面基准，通过整合各赛道见解，朝着构建能在真实环境中可靠感知、鲁棒行动和跨平台适应的机器人迈进了一步。

Abstract: Autonomous systems are increasingly deployed in open and dynamic environments -- from city streets to aerial and indoor spaces -- where perception models must remain reliable under sensor noise, environmental variation, and platform shifts. However, even state-of-the-art methods often degrade under unseen conditions, highlighting the need for robust and generalizable robot sensing. The RoboSense 2025 Challenge is designed to advance robustness and adaptability in robot perception across diverse sensing scenarios. It unifies five complementary research tracks spanning language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view and cross-modal correspondence, and cross-platform 3D perception. Together, these tasks form a comprehensive benchmark for evaluating real-world sensing reliability under domain shifts, sensor failures, and platform discrepancies. RoboSense 2025 provides standardized datasets, baseline models, and unified evaluation protocols, enabling large-scale and reproducible comparison of robust perception methods. The challenge attracted 143 teams from 85 institutions across 16 countries, reflecting broad community engagement. By consolidating insights from 23 winning solutions, this report highlights emerging methodological trends, shared design principles, and open challenges across all tracks, marking a step toward building robots that can sense reliably, act robustly, and adapt across platforms in real-world environments.

</details>


### [111] [Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses](https://arxiv.org/abs/2601.05074)
*Julian Kulozik,Nathanaël Jarrassé*

Main category: cs.RO

TL;DR: 提出一种基于躯干运动的补偿效应放大控制(CEAC)方法，利用躯干屈伸控制假肢肘部速度，实现连续精确的协调控制。


<details>
  <summary>Details</summary>
Motivation: 当前上肢假肢在控制中间关节（如手腕和肘部）方面仍面临挑战，特别是实现连续和速度调节的运动。需要一种更直观的控制方法来改善假肢的协调性和功能性。

Method: 提出补偿效应放大控制(CEAC)范式，利用躯干屈伸作为输入控制假肢肘部速度。该方法放大躯干与假肢之间的自然耦合，同时引入受控延迟，让用户能够调节假肢关节的位置和速度。

Result: 在12名健全参与者使用带主动肘部的超数假肢进行的绘图任务中，CEAC表现出与自然手臂运动相当的任务性能，即使手势速度或绘图尺寸变化时也能保持符合人体工学的躯干姿势。分析显示CEAC有效恢复了关节协调动作，在躯干和肘部之间分配运动努力。

Conclusion: CEAC为上肢假肢中间关节提供了一种有前景的控制策略，特别适用于需要连续精确协调的任务，能够实现直观的轨迹控制而无需极端的补偿性运动。

Abstract: Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.

</details>


### [112] [Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration](https://arxiv.org/abs/2601.05243)
*Xingyi He,Adhitya Polavaram,Yunhao Cao,Om Deshmukh,Tianrui Wang,Xiaowei Zhou,Kuan Fang*

Main category: cs.RO

TL;DR: CorDex框架通过基于对应关系的数据引擎从单个人类演示生成合成数据，学习灵巧机器人手的功能性抓取，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 灵巧机器人手的功能性抓取是实现工具使用和复杂操作的关键能力，但受到两个瓶颈制约：大规模数据集的稀缺性，以及学习模型中缺乏语义和几何推理的集成。

Method: 1. 基于对应关系的数据引擎：从单个人类演示生成多样化的合成训练数据，通过对应估计将专家抓取转移到生成的对象上，并通过优化调整抓取；2. 多模态预测网络：集成视觉和几何信息，设计局部-全局融合模块和重要性感知采样机制。

Result: CorDex在不同物体类别上表现出色，能够很好地泛化到未见过的物体实例，并显著优于最先进的基线方法。

Conclusion: CorDex框架通过从有限演示生成高质量合成数据，并结合多模态推理，有效解决了灵巧功能性抓取的数据稀缺和推理集成问题，为机器人工具使用和复杂操作提供了有力支持。

Abstract: Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.

</details>


### [113] [LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model](https://arxiv.org/abs/2601.05248)
*Zhuoyang Liu,Jiaming Liu,Hao Chen,Ziyu Guo,Chengkai Hou,Chenyang Gu,Jiale Yu,Xiangju Mi,Renrui Zhang,Zhengping Che,Jian Tang,Pheng-Ann Heng,Shanghang Zhang*

Main category: cs.RO

TL;DR: LaST₀提出了一种通过潜在时空思维链实现高效推理的视觉语言动作模型，解决了现有方法推理延迟高和语言表示瓶颈的问题，在机器人操作任务中取得了更好的性能和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法通过显式生成语言推理轨迹或未来视觉观察来提高动作准确性，但存在两个主要问题：1）显式推理带来不可忽视的推理延迟，限制了机器人操作所需的时间分辨率；2）推理局限于语言空间，形成了表示瓶颈，难以准确捕捉难以言表的物理属性。

Method: 提出LaST₀框架，通过潜在时空思维链实现高效推理。核心创新包括：1）引入高效的潜在CoT空间，建模未来视觉动态、3D结构信息和机器人本体感知状态，并跨时间扩展这些表示；2）采用混合Transformer的双系统架构，推理专家进行低频潜在推理，动作专家基于机器人导向的潜在表示生成高频动作；3）通过异构操作频率训练，实现推理和动作推理速率的自适应切换。

Result: 在10个模拟任务和6个真实世界操作任务中，LaST₀相比之前的VLA方法分别提高了8%和13%的平均成功率，同时实现了显著更快的推理速度。

Conclusion: LaST₀通过潜在时空思维链有效解决了现有VLA方法的推理延迟和表示瓶颈问题，在机器人操作任务中实现了更好的性能和效率，为机器人控制中的高效推理提供了新思路。

Abstract: Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0

</details>
