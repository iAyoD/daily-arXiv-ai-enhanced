<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: FLEET是一个混合分散式框架，将自然语言指令转化为优化的多机器人调度，结合LLM前端和形式化后端来协调异构机器人团队。


<details>
  <summary>Details</summary>
Motivation: 解决从自由形式自然语言指令协调异构机器人团队的挑战，语言规划器难以处理长期协调和幻觉问题，而纯形式化方法需要封闭世界模型。

Method: 使用LLM前端生成任务图和机器人-任务适应度矩阵，形式化后端解决最小化完工时间问题，机器人执行具有自主闭环控制的子任务。

Result: 在多个自由形式语言引导的自主协调基准测试中，FLEET在异构任务的两智能体团队上比最先进的生成规划器提高了成功率。

Conclusion: 混合整数线性规划主要改善时间结构，LLM导出的适应度对能力耦合任务至关重要，两者结合提供最高整体性能，并在具有不连续能力的四足机器人硬件试验中得到验证。

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [2] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: 提出基于门控循环单元的轻量级编码器-解码器模型，用于在信息稀缺条件下预测高性能车辆的未来状态，仅使用车载测量数据和驾驶员控制动作，在极端动态条件下最大平均相对误差低于2.6%。


<details>
  <summary>Details</summary>
Motivation: 高性能车辆的动态建模需要大量结构信息，但这些信息对于非设计者通常不可得，这在自动驾驶应用中尤为常见，因此需要在信息稀缺条件下开发车辆模型。

Method: 使用基于门控循环单元(GRU)的轻量级编码器-解码器模型，通过车辆过去状态的车载测量数据和驾驶员控制动作来关联预测未来状态。

Result: 模型在极端动态条件下最大平均相对误差低于2.6%，对感兴趣频率范围内的噪声输入数据具有良好的鲁棒性，输出信号（纵向/横向加速度、横摆率、纵向速度）具有物理一致性。

Conclusion: 该数据驱动模型无需物理约束，在信息稀缺条件下能够准确预测车辆动态状态，为自动驾驶应用提供了有效的车辆建模解决方案。

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [3] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: HJCD-IK是一种基于GPU加速的混合逆运动学求解器，结合了方向感知的贪婪坐标下降初始化方案和基于雅可比矩阵的优化程序，在精度和速度方面都优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统解析求解器受限于低自由度和特定拓扑结构，而数值优化方法计算成本高且容易陷入局部极小值。需要结合采样和优化的GPU加速方法来提高逆运动学求解的准确性和速度。

Method: 提出HJCD-IK混合求解器，使用方向感知的贪婪坐标下降进行初始化，然后采用基于雅可比矩阵的优化程序进行精炼。

Result: 相比现有技术，该方法在收敛速度和整体精度方面都有提升，在精度-延迟帕累托边界上持续找到解决方案，通常实现数量级的增益，并产生高质量样本的广泛分布。

Conclusion: HJCD-IK是一种高效的GPU加速逆运动学求解器，在精度和速度方面都优于现有方法，代码已开源供社区使用。

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [4] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了摊销价值优化(AVO)方法，通过引入学习价值函数来预测未来任务性能，指导轨迹优化器选择有利于后续子任务的状态，从而解决灵巧操作中接触模式切换的优化问题。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作任务需要在不同接触模式间切换，传统方法将任务分解为独立优化的子任务，这限制了性能且计算成本高昂。

Method: 引入学习价值函数预测总未来任务性能，将其纳入轨迹优化的成本函数中，通过价值函数梯度指导优化器选择有利于后续子任务的状态。

Result: 在螺丝刀抓取和转动任务中验证，相比无价值函数的轨迹优化，在减少50%计算预算的情况下仍能提高性能。

Conclusion: AVO方法能有效桥接独立优化的子任务，加速优化过程并提升性能。

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [5] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: 提出了IPIM原语计算，使基于采样的检测规划器能够完全使用神经SDF表示，在保持检测轨迹质量的同时显著减少内存使用（高达70倍）。


<details>
  <summary>Details</summary>
Motivation: 基础设施老化和复杂性增加使得高效检测规划更为关键。现有基于采样的检测规划器虽然快速但内存需求大，特别是对于大型复杂结构。虽然隐式模型（如神经SDF）能高效表示这些结构，但现有规划器原语计算主要针对显式环境模型设计。

Method: 提出了一组名为IPIM的原语计算，使基于采样的检测规划器能够在整个规划过程中完全使用神经SDF表示，无需在隐式和显式环境模型之间频繁转换。

Result: 在三个场景（包括具有超过9200万个三角网格面的复杂真实世界结构）的评估表明，即使使用基本的基于采样规划器配合IPIM，也能生成与最先进规划器质量相当的检测轨迹，同时内存使用减少高达70倍。

Conclusion: IPIM方法成功解决了基于采样检测规划器在大型复杂结构中的内存效率问题，通过完全使用隐式模型表示实现了显著的内存节省，同时保持检测质量。

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [6] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: GATO是一个开源GPU加速的批量轨迹优化求解器，专门为中等批量规模（数十到数百个求解）的实时MPC应用设计，在算法、软件和计算硬件层面协同优化，实现超高性能。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速方法要么并行化单个求解以满足实时截止时间，要么以慢于实时速率扩展到非常大的批量，要么通过限制模型通用性来获得速度。这为需要实时批量求解的现代MPC应用留下了性能差距。

Method: 采用块级、warp级和线程级并行化策略，在单个求解内部和多个求解之间实现并行计算，通过算法、软件和计算硬件的协同设计来优化性能。

Result: 模拟基准测试显示，随着批量大小增加，相比CPU基线加速18-21倍，相比GPU基线加速1.4-16倍；案例研究显示改进了扰动抑制和收敛行为；在工业机械臂上的硬件验证也取得成功。

Conclusion: GATO填补了中等批量规模实时MPC应用的性能空白，通过协同设计方法实现了显著的性能提升，并开源以支持可重复性和采用。

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [7] [Differentiable Particle Optimization for Fast Sequential Manipulation](https://arxiv.org/abs/2510.07674)
*Lucas Chen,Shrutheesh Raman Iyer,Zachary Kingston*

Main category: cs.RO

TL;DR: SPaSM是一个完全GPU并行化的框架，通过两阶段粒子优化策略解决顺序机器人操作任务中的轨迹优化问题，实现了毫秒级求解速度。


<details>
  <summary>Details</summary>
Motivation: 顺序机器人操作任务需要在可能的高维配置空间中寻找满足多个物体交互几何约束的无碰撞轨迹。现有方法由于CPU-GPU数据传输开销和复杂逻辑导致硬件利用率不足，无法实现实时大规模求解。

Method: SPaSM将约束评估、采样和基于梯度的优化编译为优化的CUDA内核，实现无需CPU协调的端到端轨迹优化。采用两阶段粒子优化：首先通过大规模并行采样解决放置约束，然后在关节空间中提升为完整轨迹优化。

Result: 在具有挑战性的基准测试中，SPaSM实现了毫秒级的求解时间，成功率达到100%，相比现有方法实现了4000倍的加速。

Conclusion: SPaSM通过完全GPU并行化和联合优化物体放置与机器人轨迹，有效解决了顺序操作任务的实时求解问题，显著提升了计算效率。

Abstract: Sequential robot manipulation tasks require finding collision-free
trajectories that satisfy geometric constraints across multiple object
interactions in potentially high-dimensional configuration spaces. Solving
these problems in real-time and at large scales has remained out of reach due
to computational requirements. Recently, GPU-based acceleration has shown
promising results, but prior methods achieve limited performance due to CPU-GPU
data transfer overhead and complex logic that prevents full hardware
utilization. To this end, we present SPaSM (Sampling Particle optimization for
Sequential Manipulation), a fully GPU-parallelized framework that compiles
constraint evaluation, sampling, and gradient-based optimization into optimized
CUDA kernels for end-to-end trajectory optimization without CPU coordination.
The method consists of a two-stage particle optimization strategy: first
solving placement constraints through massively parallel sampling, then lifting
solutions to full trajectory optimization in joint space. Unlike hierarchical
approaches, SPaSM jointly optimizes object placements and robot trajectories to
handle scenarios where motion feasibility constrains placement options.
Experimental evaluation on challenging benchmarks demonstrates solution times
in the realm of $\textbf{milliseconds}$ with a 100% success rate; a
$4000\times$ speedup compared to existing approaches.

</details>


### [8] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: 提出基于新兴屏障函数的模型扩散约束方法，解决传统约束方法导致的性能下降问题，在2D避障和3D水下机械臂系统中显著提升解质量并大幅减少计算时间


<details>
  <summary>Details</summary>
Motivation: 传统模型扩散中的约束会导致灾难性性能下降，即使在简单2D系统中也会因蒙特卡洛近似得分函数的样本效率低下而出现问题

Method: 引入新兴屏障模型扩散（EB-MBD），使用逐步引入的屏障约束避免性能问题，无需计算昂贵的投影操作，通过分析每轮迭代的采样活跃度来指导屏障参数调度选择

Result: 在2D碰撞避免和3D水下机械臂系统中，该方法相比模型扩散获得更低成本解，相比基于投影的方法计算时间减少数个数量级

Conclusion: 新兴屏障函数约束能有效提升模型扩散的性能，避免约束导致的样本效率问题，提供高质量解的同时大幅降低计算成本

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [9] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一种用于双足机器人在粗糙地形上安全导航的概率安全规划与控制框架，结合高斯过程地形估计和保形预测，确保动态可行性和质心鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在不确定地形环境中安全导航的挑战，需要同时考虑地形不确定性和动态稳定性。

Method: 采用高斯过程回归估计地形高程，利用保形预测构建置信区间，提出基于收缩的可达管和飞轮扭矩控制律，结合模型预测控制框架。

Result: 在MuJoCo中对Digit双足机器人进行物理仿真验证，证明了该框架在给定置信水平下能够确保概率安全和目标可达性。

Conclusion: 所提出的规划控制框架能够有效保证双足机器人在不确定地形环境中的安全导航，提供概率安全保证和动态稳定性。

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [10] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: 本文提出了一种可配置、组件无关的幻觉注入框架，用于研究自动驾驶车辆感知故障对安全性的影响，通过模拟六种幻觉类型来量化其对碰撞和险情的影响。


<details>
  <summary>Details</summary>
Motivation: 现有故障注入研究通常针对单一传感器或感知模块，导致难以推广或集成到统一仿真环境中的孤岛框架。本文旨在解决这一限制，将感知故障重新定义为幻觉，从而独立于特定传感器或算法进行分析。

Method: 构建了一个可配置、组件无关的幻觉注入框架，在开源模拟器中诱导六种合理的幻觉类型，执行了超过18,350次模拟，在自动驾驶车辆穿越无信号横向街道的场景中注入幻觉。

Result: 统计验证了该框架的有效性，并量化了每种幻觉类型对碰撞和险情的影响。某些幻觉（如感知延迟和漂移）在测试场景中显著增加了碰撞风险。

Conclusion: 该框架提供了一个可扩展、统计验证、组件无关且完全互操作的工具集，简化并加速了自动驾驶安全验证，即使对于具有新型感知架构和组件的系统也适用，有望缩短自动驾驶上市时间并为未来容错和弹性设计研究奠定基础。

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [11] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: TrajSkill是一个从人类演示视频直接学习机器人操作技能的框架，通过稀疏光流轨迹作为跨形态运动线索，实现人类到机器人的技能迁移。


<details>
  <summary>Details</summary>
Motivation: 解决人类演示视频与机器人操作器之间的形态差异问题，现有方法依赖配对数据集或手工奖励，限制了可扩展性和泛化能力。

Method: 将人类动作表示为稀疏光流轨迹，作为形态无关的运动线索，结合视觉和文本输入，联合合成时间一致的机器人操作视频并转换为可执行动作。

Result: 在MetaWorld模拟实验中，FVD降低39.6%，KVD降低36.6%，跨形态成功率提升16.7%；真实机器人厨房操作任务验证了方法的有效性。

Conclusion: TrajSkill实现了从人类演示视频到机器人的跨形态技能迁移，在模拟和真实环境中都表现出色。

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [12] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: IntentionVLA是一个视觉-语言-动作模型框架，通过课程训练范式赋予模型推理能力，在间接指令下实现快速推理和动作生成，显著提升了人机交互性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在预训练阶段缺乏与具身场景相关的推理密集型任务，无法进行隐式人类意图推理，限制了在复杂真实世界交互中的表现。

Method: 采用课程训练范式：首先使用精心设计的推理数据（包含意图推断、空间定位和紧凑具身推理）进行预训练，然后在微调阶段使用紧凑推理输出作为动作生成的上下文指导。

Result: IntentionVLA在直接指令下比π0模型成功率提高18%，在意图指令下比ECoT提高28%。在分布外意图任务中，成功率是基线模型的两倍以上，零样本人机交互成功率可达40%。

Conclusion: IntentionVLA为下一代人机交互系统提供了一个有前景的范式，通过赋予模型推理能力显著提升了复杂交互场景下的性能。

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [13] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: 提出了GM3模型，一个基于轮胎刷表示的通用微移动车辆动力学模型，支持任意车轮配置，解决了现有模型忽略轮胎滑移、载荷转移和骑手/车辆倾斜的问题。


<details>
  <summary>Details</summary>
Motivation: 主流工具依赖运动学自行车模型或特定模式的物理模型，无法捕捉轮胎滑移、载荷转移和骑手/车辆倾斜等关键动力学特性，缺乏统一的物理基础模型来覆盖各种微移动车辆和车轮布局。

Method: 提出GM3模型，基于轮胎刷表示，支持单/双轨和多轮平台等任意车轮配置；开发交互式模型无关仿真框架，使用固定步长RK4积分、人在回路和脚本控制，实时轨迹跟踪和分析日志。

Result: 在斯坦福无人机数据集的死亡圆环场景中，对自行车、滑板车和手推车类别进行了GM3模型的实证验证。

Conclusion: GM3模型为微移动车辆动力学建模提供了统一的物理基础方法，能够更准确地捕捉真实世界中的车辆行为。

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [14] [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DM1通过引入分散正则化到MeanFlow中，解决了基于流的策略中的表示崩溃问题，在保持一步推理效率的同时显著提升了机器人操作的精确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的策略存在表示崩溃问题，无法区分相似的视觉表示，导致在精确操作任务中失败。需要一种既能保持一步推理效率又能避免表示崩溃的方法。

Method: 在MeanFlow框架中集成分散正则化，在多个中间嵌入层应用不同变体的分散正则化，鼓励训练批次间的表示多样性，无需额外网络模块或专门训练过程。

Result: 在RoboMimic基准测试中，DM1实现了20-40倍更快的推理速度（0.07s vs 2-3.5s），成功率提升10-20个百分点，Lift任务达到99%成功率（基线为85%）。真实机器人部署验证了从仿真到物理世界的有效迁移。

Conclusion: 这是首个利用表示正则化使基于流的策略在机器人操作中实现强大性能的工作，为高效鲁棒的操作提供了一种简单而强大的方法。

Abstract: The ability to learn multi-modal action distributions is indispensable for
robotic manipulation policies to perform precise and robust control. Flow-based
generative models have recently emerged as a promising solution to learning
distributions of actions, offering one-step action generation and thus
achieving much higher sampling efficiency compared to diffusion-based methods.
However, existing flow-based policies suffer from representation collapse, the
inability to distinguish similar visual representations, leading to failures in
precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive
Regularization for One-Step Robotic Manipulation), a novel flow matching
framework that integrates dispersive regularization into MeanFlow to prevent
collapse while maintaining one-step efficiency. DM1 employs multiple dispersive
regularization variants across different intermediate embedding layers,
encouraging diverse representations across training batches without introducing
additional network modules or specialized training procedures. Experiments on
RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s
vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the
Lift task reaching 99% success over 85% of the baseline. Real-robot deployment
on a Franka Panda further validates that DM1 transfers effectively from
simulation to the physical world. To the best of our knowledge, this is the
first work to leverage representation regularization to enable flow-based
policies to achieve strong performance in robotic manipulation, establishing a
simple yet powerful approach for efficient and robust manipulation.

</details>


### [15] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: 提出了USIM水下机器人仿真数据集和U0视觉-语言-动作模型，解决了水下机器人多任务自主操作的数据稀缺问题，在多种任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂，现有数据驱动方法面临大规模高质量数据集稀缺的挑战，限制了水下机器人多任务自主能力的发展。

Method: 构建了包含56.1万帧数据的USIM仿真数据集，并提出U0 VLA模型，通过多模态融合和卷积注意力感知增强模块(CAP)提升空间理解和移动操作能力。

Result: 在检查、避障、扫描和动态跟踪等任务中达到80%成功率，在移动操作任务中比基线方法减少21.2%的目标距离。

Conclusion: VLA模型可有效应用于水下机器人，为可扩展数据集构建、任务自主性提升和智能通用水下机器人的实际实现奠定了基础。

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [16] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文提出了一种基于Falcon模型的主动风险感知模块，用于提升机器人在动态人群环境中的社交导航性能，在IROS 2025 RoboSense挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 开发能够在动态人类环境中安全、高效且符合社交规范的自主导航系统，解决仅使用机载传感器（RGB-D和里程计）且无全局地图的挑战。

Method: 在Falcon模型基础上引入主动风险感知模块，通过学习预测周围人类的基于距离的碰撞风险评分，增强空间感知和主动避碰能力。

Result: 在Social-HM3D基准测试中，该方法提高了机器人在拥挤室内场景中保持个人空间合规性的能力，在16支参赛队伍中获得第二名。

Conclusion: 主动风险感知模块有效提升了社交导航性能，证明了基于风险预测的方法在动态人类环境导航中的价值。

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [17] [Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots](https://arxiv.org/abs/2510.07882)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 提出了DualTHOR双臂人形机器人仿真器和Proprio-MLLM模型，解决了MLLMs在长时程双臂人形机器人任务中的局限性，将规划性能平均提升了19.75%。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在涉及双臂人形机器人的长时程任务中效果有限，主要由于缺乏系统支持人形机器人任务评估和数据收集的仿真平台，以及MLLMs缺乏具身意识，难以推理双臂选择逻辑和身体位置。

Method: 开发了DualTHOR双臂人形仿真器，具有连续过渡和应急机制；提出了Proprio-MLLM模型，通过运动基位置嵌入和跨空间编码器整合本体感觉信息来增强具身意识。

Result: 实验显示，现有MLLMs在该环境中表现不佳，而Proprio-MLLM在规划性能上平均提升了19.75%。

Conclusion: 该工作为人形机器人具身智能提供了必要的仿真平台和有效模型，推动了该领域的发展。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
the ability to serve as high-level planners, enabling robots to follow complex
human instructions. However, their effectiveness, especially in long-horizon
tasks involving dual-arm humanoid robots, remains limited. This limitation
arises from two main challenges: (i) the absence of simulation platforms that
systematically support task evaluation and data collection for humanoid robots,
and (ii) the insufficient embodiment awareness of current MLLMs, which hinders
reasoning about dual-arm selection logic and body positions during planning. To
address these issues, we present DualTHOR, a new dual-arm humanoid simulator,
with continuous transition and a contingency mechanism. Building on this
platform, we propose Proprio-MLLM, a model that enhances embodiment awareness
by incorporating proprioceptive information with motion-based position
embedding and a cross-spatial encoder. Experiments show that, while existing
MLLMs struggle in this environment, Proprio-MLLM achieves an average
improvement of 19.75% in planning performance. Our work provides both an
essential simulation platform and an effective model to advance embodied
intelligence in humanoid robotics. The code is available at
https://anonymous.4open.science/r/DualTHOR-5F3B.

</details>


### [18] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: GRACE框架通过可执行分析概念（EAC）弥合视觉语言模型的语义理解与机器人物理执行之间的差距，实现精确和通用的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在语义推理和任务规划方面的能力与机器人实际物理执行之间的"语义到物理"差距，使机器人能够在非结构化环境中进行精确和通用的操作。

Method: 提出GRACE框架，通过可执行分析概念（EAC）将自然语言指令和视觉信息转化为实例化的数学蓝图，编码物体功能、几何约束和操作语义，从而推导抓取姿态、力方向和物理可行的运动轨迹。

Result: 在模拟和真实环境中的大量实验表明，GRACE在各种关节物体上实现了强大的零样本泛化能力，无需任务特定训练。

Conclusion: GRACE提供了一个统一且可解释的接口，连接高层指令理解和低层机器人控制，通过语义-物理基础有效实现了精确和可泛化的操作。

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [19] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: 提出基于角度-轴空间的方向表示方法，通过加权平均机制在SO(3)流形上融合多条轨迹，解决多个局部约束同时整合的问题


<details>
  <summary>Details</summary>
Motivation: 旋转群SO(3)是黎曼流形，其非欧几何性质导致局部约束整合困难，特别是多个局部约束的同时整合

Method: 使用角度-轴表示方法，在不同基点考虑不同局部约束生成多条轨迹，然后通过提出的加权平均机制融合这些轨迹

Result: 仿真和实验验证表明，该方法能适应任意期望路径点的方向、处理角加速度约束，并能同时整合多个局部约束以获得额外收益

Conclusion: 所提方法能解决SO(3)流形上的扭曲问题，使现成的欧几里得学习算法在非欧空间中重新适用，实现多个局部约束的同时整合

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [20] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: FastUMI-100K是一个大规模UMI风格的多模态演示数据集，包含10万+条演示轨迹，涵盖54个任务和数百种物体类型，用于解决数据驱动机器人操作学习中的可扩展性和适应性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类遥操作机器人收集的数据集在可扩展性、轨迹平滑性和不同机器人实体间的适用性方面存在局限，无法满足现实世界操作任务日益增长的复杂性需求。

Method: 使用FastUMI机器人系统收集数据，该系统采用模块化、硬件解耦的机械设计和集成轻量级跟踪系统，在代表性家庭环境中收集多模态数据流，包括末端执行器状态、多视角腕戴鱼眼图像和文本注释。

Result: 实验结果表明，FastUMI-100K能够在各种基线算法中实现高策略成功率，验证了其鲁棒性、适应性和解决复杂动态操作挑战的现实世界适用性。

Conclusion: FastUMI-100K提供了一个更可扩展、灵活和适应性强的解决方案，能够满足现实世界机器人演示数据的多样化需求，为复杂操作任务的学习提供了高质量数据集。

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [21] [Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation](https://arxiv.org/abs/2510.08044)
*Shiyuan Yin,Chenjia Bai,Zihao Zhang,Junwei Jin,Xinxin Zhang,Chi Zhang,Xuelong Li*

Main category: cs.RO

TL;DR: 提出了CURE方法，通过分解认知不确定性和内在不确定性来改进LLM在机器人规划中的可靠性，在厨房操作和桌面重排实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在机器人规划中存在幻觉问题，导致过度自信但可能不准确或不安全的计划。现有研究未能充分区分认知和内在不确定性，限制了不确定性估计的效果。

Method: CURE方法将不确定性分解为认知不确定性和内在不确定性，其中认知不确定性进一步细分为任务清晰度和任务熟悉度。使用随机网络蒸馏和基于LLM特征的多层感知机回归头来获取总体不确定性评估。

Result: 在厨房操作和桌面重排两个实验环境中验证，相比现有方法，CURE产生的不确定性估计与实际执行结果更加一致。

Conclusion: CURE方法通过分解不同类型的不确定性，显著提高了LLM在机器人规划中的可靠性，不确定性估计与实际执行结果更加匹配。

Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities,
enabling robots to understand natural language instructions and generate
high-level plans with appropriate grounding. However, LLM hallucinations
present a significant challenge, often leading to overconfident yet potentially
misaligned or unsafe plans. While researchers have explored uncertainty
estimation to improve the reliability of LLM-based planning, existing studies
have not sufficiently differentiated between epistemic and intrinsic
uncertainty, limiting the effectiveness of uncertainty estimation. In this
paper, we present Combined Uncertainty estimation for Reliable Embodied
planning (CURE), which decomposes the uncertainty into epistemic and intrinsic
uncertainty, each estimated separately. Furthermore, epistemic uncertainty is
subdivided into task clarity and task familiarity for more accurate evaluation.
The overall uncertainty assessments are obtained using random network
distillation and multi-layer perceptron regression heads driven by LLM
features. We validated our approach in two distinct experimental settings:
kitchen manipulation and tabletop rearrangement experiments. The results show
that, compared to existing methods, our approach yields uncertainty estimates
that are more closely aligned with the actual execution outcomes.

</details>


### [22] [Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography](https://arxiv.org/abs/2510.08106)
*Zihan Li,Yixiao Xu,Lei Zhang,Taiyu Han,Xinshan Yang,Yingni Wang,Mingxuan Liu,Shenghai Xin,Linxun Liu,Hongen Liao,Guochen Ning*

Main category: cs.RO

TL;DR: 开发了一个轻量级自主超声机器人系统，能够在资源有限地区自动获取专家级肝脏超声平面并检测病理，适用于运动个体和野外环境。


<details>
  <summary>Details</summary>
Motivation: 解决资源有限地区超声专家严重短缺的问题，肝脏超声需要定位多个非连续平面，对专业知识要求高。

Method: 集成多模态感知与记忆注意力的AI代理用于定位不可见目标结构，配合588克6自由度线驱动机器人，通过腹部安装增强运动鲁棒性。

Result: 系统能够自主获取专家级标准肝脏超声平面，在患者中检测病理，包括来自高海拔城市西宁的病例，在快速运动个体和野外环境中表现有效。

Conclusion: 这是首个在多种挑战性场景下实现自主超声检查的系统，有望改变资源不足地区获得专家级诊断的途径。

Abstract: Liver disease is a major global health burden. While ultrasound is the
first-line diagnostic tool, liver sonography requires locating multiple
non-continuous planes from positions where target structures are often not
visible, for biometric assessment and lesion detection, requiring significant
expertise. However, expert sonographers are severely scarce in resource-limited
regions. Here, we develop an autonomous lightweight ultrasound robot comprising
an AI agent that integrates multi-modal perception with memory attention for
localization of unseen target structures, and a 588-gram 6-degrees-of-freedom
cable-driven robot. By mounting on the abdomen, the system enhances robustness
against motion. Our robot can autonomously acquire expert-level standard liver
ultrasound planes and detect pathology in patients, including two from Xining,
a 2261-meter-altitude city with limited medical resources. Our system performs
effectively on rapid-motion individuals and in wilderness environments. This
work represents the first demonstration of autonomous sonography across
multiple challenging scenarios, potentially transforming access to expert-level
diagnostics in underserved regions.

</details>


### [23] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: 提出一种基于聚类的技术，从UI日志中提取常规日志以支持机器人流程自动化，特别针对存在执行不一致性（噪声）的场景，相比现有技术能获得更准确的常规日志。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多不直接关注模型发现，仅提取常规动作集合，且未在存在执行不一致性（噪声）的场景下评估，这反映了人类执行的自然变异性和偶然错误。

Method: 采用基于聚类的技术从UI日志中提取常规日志，在九个具有不同噪声水平的UI日志上进行实验，并与现有技术进行比较。

Result: 通过标准先进指标评估，该技术能够比现有技术提取更准确的常规日志，特别是在存在噪声的情况下表现更优。

Conclusion: 提出的聚类技术能有效处理UI日志中的噪声，提取更准确的常规日志，为机器人流程自动化提供更好的支持。

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [24] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: NavSpace是一个新的导航基准，包含6个任务类别和1,228个轨迹-指令对，专门用于评估导航代理的空间感知和推理能力。作者评估了22个导航代理，并提出了新的空间智能导航模型SNav，在基准测试和真实机器人测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的导航基准主要关注语义理解，但忽视了系统评估导航代理的空间感知和推理能力。需要一个新的基准来专门评估导航代理的空间智能。

Method: 提出了NavSpace基准，包含6个任务类别和1,228个轨迹-指令对。评估了22个导航代理，包括最先进的导航模型和多模态大语言模型。提出了新的空间智能导航模型SNav。

Result: 评估结果揭示了具身导航中的空间智能现状。SNav在NavSpace基准和真实机器人测试中均优于现有导航代理，为未来工作建立了强基线。

Conclusion: NavSpace基准填补了导航评估中空间智能评估的空白，SNav模型在空间智能导航方面表现出色，为未来研究提供了重要基准和模型基础。

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


### [25] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: 比较经典PID控制器与现代强化学习算法在电缆驱动并联机器人控制中的性能，发现TRPO方法表现最佳，在多种轨迹下获得最低RMS误差，并对大时间间隔控制更新具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估经典和现代控制方法在真实世界电缆驱动并联机器人中的性能，特别关注欠约束系统和有限时间离散化的情况。

Method: 对经典PID控制器与现代强化学习算法（包括DDPG、PPO和TRPO）进行对比分析。

Result: TRPO在所有方法中表现最优，在各种轨迹下获得最低的均方根误差，并且对大时间间隔控制更新具有鲁棒性。TRPO能够平衡探索和利用，在噪声环境中实现稳定控制。

Conclusion: TRPO作为复杂机器人控制任务的鲁棒解决方案具有潜力，对动态环境和未来在传感器融合或混合控制策略中的应用具有重要意义。

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [26] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: Airy是一个艺术装置，通过两个强化学习训练的机械臂竞争抖床单，将复杂的多智能体AI决策转化为直观可理解的视觉体验。


<details>
  <summary>Details</summary>
Motivation: 随着工业机器人进入人类共享空间，其不透明的决策过程威胁安全、信任和公共监督。该作品旨在探索如何让复杂的多智能体AI变得直观可理解。

Method: 基于三个设计原则：竞争作为清晰指标（谁抖得更高）、具身熟悉性（观众熟悉抖床单动作）、传感器到感知映射（通过森林和天气投影显示机器人合作或竞争），创建了一个机械臂抖床单的竞赛装置。

Result: 在五个国际展览中的观察表明，观众能够实时解读机器人的策略、冲突和合作，情感反应与系统内部状态一致。

Conclusion: 该项目展示了感官隐喻如何将黑盒系统转变为公共界面，使复杂的AI决策变得直观可理解。

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [27] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: 本文提出了一种基于单层重构的逆最优控制方法，相比传统的双层方法，计算速度提升15倍，同时对噪声具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有逆最优控制方法要么基于缓慢的双层过程，要么基于快速但对噪声敏感的优化条件违反最小化方法，需要一种既快速又鲁棒的新方法。

Method: 采用单层重构方法替代传统的双层逆最优控制方法，假设人类运动模型为等式约束的最优控制模型。

Result: 在模拟实验中，该方法对噪声表现出很强的鲁棒性，计算时间相比经典双层实现减少了15倍。

Conclusion: 单层重构方法在保持结果等效性的同时，显著提高了逆最优控制的计算效率和鲁棒性。

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [28] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出了一种使用CAD软件API验证6-6 Stewart-Gough平台机械臂最大无碰撞球体尺寸的方法，通过自动化更新移动平台位置并检查腿部碰撞来验证CFS安全性。


<details>
  <summary>Details</summary>
Motivation: 需要验证6-6 Stewart-Gough平台机械臂在给定移动平台方向下的最大无碰撞球体尺寸，确保机械臂在工作空间内的安全操作。

Method: 利用CAD软件API自动化更新移动平台位置，在CFS表面的包围壳内采样，检查每个姿态下各对腿部之间的相互碰撞情况。

Result: 该方法能够验证预计算CFS的安全性，并且可以估计任何空间并联机械臂的无碰撞球体。

Conclusion: 所提出的基于CAD API的验证方法为并联机械臂的无碰撞工作空间分析提供了一种有效的自动化解决方案。

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


### [29] [Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/abs/2510.08464)
*Jason Jabbour,Dong-Ki Kim,Max Smith,Jay Patrikar,Radhika Ghosal,Youhui Wang,Ali Agha,Vijay Janapa Reddi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: GLUESTICK是一种后剪枝恢复方法，通过在权重空间对密集模型和剪枝模型进行一次性插值，计算校正项，在推理时恢复剪枝VLA模型的功能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作(VLA)模型在资源受限硬件上部署困难，现有剪枝方法会导致VLA模型性能急剧下降和安全违规增加。

Method: 在权重空间对密集和剪枝模型进行一次性插值计算校正项，推理时每个剪枝层使用该校正恢复功能，仅引入一个控制效率与精度权衡的超参数。

Result: 在多种VLA架构和操作、导航任务中，GLUESTICK实现了竞争性的内存效率，同时显著恢复成功率并减少安全违规。

Conclusion: GLUESTICK提供了一种无需训练、与剪枝算法无关的有效方法，可在保持稀疏性优势的同时恢复剪枝VLA模型的功能。

Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but
remain challenging to deploy on resource-limited hardware. Pruning has enabled
efficient compression of large language models (LLMs), yet it is largely
understudied in robotics. Surprisingly, we observe that pruning VLA models
leads to drastic degradation and increased safety violations. We introduce
GLUESTICK, a post-pruning recovery method that restores much of the original
model's functionality while retaining sparsity benefits. Our method performs a
one-time interpolation between the dense and pruned models in weight-space to
compute a corrective term. This correction is used during inference by each
pruned layer to recover lost capabilities with minimal overhead. GLUESTICK
requires no additional training, is agnostic to the pruning algorithm, and
introduces a single hyperparameter that controls the tradeoff between
efficiency and accuracy. Across diverse VLA architectures and tasks in
manipulation and navigation, GLUESTICK achieves competitive memory efficiency
while substantially recovering success rates and reducing safety violations.
Additional material can be found at: https://gluestick-vla.github.io/.

</details>


### [30] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan是一个自动化框架，可将人类视觉演示转换为类人机器人的双手灵巧操作技能，无需相机标定、深度传感器或3D对象资产。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法仅考虑简化浮动手部、需要昂贵运动捕捉和手动数据收集的问题，实现从野外视频直接学习灵巧操作技能。

Method: 直接在第三人称人类操作视频上操作，利用基于接触的奖励改进从噪声手-物体姿态估计的策略学习，控制完整类人机器人。

Result: 在TACO基准测试中物体姿态估计达到SOTA性能（ADD-S和VSD分别提升0.08和0.12），强化学习策略在OakInk-v2上成功率比先前方法高19%。

Conclusion: DexMan能够从真实和合成视频生成技能，无需手动数据收集和昂贵运动捕捉，为训练通用灵巧操作创建大规模多样化数据集。

Abstract: We present DexMan, an automated framework that converts human visual
demonstrations into bimanual dexterous manipulation skills for humanoid robots
in simulation. Operating directly on third-person videos of humans manipulating
rigid objects, DexMan eliminates the need for camera calibration, depth
sensors, scanned 3D object assets, or ground-truth hand and object motion
annotations. Unlike prior approaches that consider only simplified floating
hands, it directly controls a humanoid robot and leverages novel contact-based
rewards to improve policy learning from noisy hand-object poses estimated from
in-the-wild videos.
  DexMan achieves state-of-the-art performance in object pose estimation on the
TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.
Meanwhile, its reinforcement learning policy surpasses previous methods by 19%
in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both
real and synthetic videos, without the need for manual data collection and
costly motion capture, and enabling the creation of large-scale, diverse
datasets for training generalist dexterous manipulation.

</details>


### [31] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出R2RGen框架，通过真实到真实的3D数据生成增强点云观测-动作对，无需模拟器和渲染器，实现高效即插即用的空间泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中空间泛化问题，现有方法存在显著的模拟到真实差距，且受限于固定基座场景和预定义相机视角。

Method: 基于单源演示数据，通过细粒度场景和轨迹解析、组级增强策略处理多对象组合和任务约束，以及相机感知处理对齐真实3D传感器分布。

Result: R2RGen在大量实验中显著提高了数据效率，并展现出在移动操作中扩展和应用的强大潜力。

Conclusion: R2RGen框架有效解决了空间泛化问题，为机器人操作提供了高效的数据增强解决方案。

Abstract: Towards the aim of generalized robotic manipulation, spatial generalization
is the most fundamental capability that requires the policy to work robustly
under different spatial distribution of objects, environment and agent itself.
To achieve this, substantial human demonstrations need to be collected to cover
different spatial configurations for training a generalized visuomotor policy
via imitation learning. Prior works explore a promising direction that
leverages data generation to acquire abundant spatially diverse data from
minimal source demonstrations. However, most approaches face significant
sim-to-real gap and are often limited to constrained settings, such as
fixed-base scenarios and predefined camera viewpoints. In this paper, we
propose a real-to-real 3D data generation framework (R2RGen) that directly
augments the pointcloud observation-action pairs to generate real-world data.
R2RGen is simulator- and rendering-free, thus being efficient and
plug-and-play. Specifically, given a single source demonstration, we introduce
an annotation mechanism for fine-grained parsing of scene and trajectory. A
group-wise augmentation strategy is proposed to handle complex multi-object
compositions and diverse task constraints. We further present camera-aware
processing to align the distribution of generated data with real-world 3D
sensor. Empirically, R2RGen substantially enhances data efficiency on extensive
experiments and demonstrates strong potential for scaling and application on
mobile manipulation.

</details>


### [32] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 提出了一种解决机器人手内物体旋转sim-to-real挑战的新框架，通过关节级动力学模型有效拟合真实世界数据并调整模拟策略动作，实现了单一策略在真实世界中广泛物体和条件下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人手内物体旋转从模拟到真实世界的泛化难题，克服复杂接触动力学造成的"现实鸿沟"，使单一策略能够适应各种物体形状、尺寸和手腕姿态。

Method: 使用关节级动力学模型，通过因子化关节间动力学、压缩系统影响为低维变量，并学习每个关节的动态特性来桥接现实鸿沟；配合自主数据收集策略获取多样化真实交互数据。

Result: 单一策略成功旋转复杂形状物体（如动物模型）、高宽比物体（达5.33）和小尺寸物体，同时处理多样化手腕方向和旋转轴，在真实世界评估和遥操作应用中验证了有效性。

Conclusion: 该方法在机器人手内物体旋转任务中实现了前所未有的泛化能力，有效解决了sim-to-real挑战，为复杂灵巧操作任务提供了实用解决方案。

Abstract: Achieving generalized in-hand object rotation remains a significant challenge
in robotics, largely due to the difficulty of transferring policies from
simulation to the real world. The complex, contact-rich dynamics of dexterous
manipulation create a "reality gap" that has limited prior work to constrained
scenarios involving simple geometries, limited object sizes and aspect ratios,
constrained wrist poses, or customized hands. We address this sim-to-real
challenge with a novel framework that enables a single policy, trained in
simulation, to generalize to a wide variety of objects and conditions in the
real world. The core of our method is a joint-wise dynamics model that learns
to bridge the reality gap by effectively fitting limited amount of real-world
collected data and then adapting the sim policy's actions accordingly. The
model is highly data-efficient and generalizable across different whole-hand
interaction distributions by factorizing dynamics across joints, compressing
system-wide influences into low-dimensional variables, and learning each
joint's evolution from its own dynamic profile, implicitly capturing these net
effects. We pair this with a fully autonomous data collection strategy that
gathers diverse, real-world interaction data with minimal human intervention.
Our complete pipeline demonstrates unprecedented generality: a single policy
successfully rotates challenging objects with complex shapes (e.g., animals),
high aspect ratios (up to 5.33), and small sizes, all while handling diverse
wrist orientations and rotation axes. Comprehensive real-world evaluations and
a teleoperation application for complex tasks validate the effectiveness and
robustness of our approach. Website: https://meowuu7.github.io/DexNDM/

</details>


### [33] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本自主操作框架，能将任务描述转换为可执行计划，无需演示或特定平台训练，支持刚性、铰接和可变形物体的操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设任务分布内或需要特定平台数据微调，限制了跨平台迁移能力。目标是实现机器人零样本执行新操作任务。

Method: 使用视频生成模型合成任务视频，通过感知模块提取3D对象流，对刚性物体计算相对位姿并转化为机器人动作，对可变形物体使用基于粒子的动力学模型进行规划。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、铰接和可变形物体操作任务的有效零样本执行。

Conclusion: 通过将任务理解与底层控制解耦，NovaFlow实现了跨平台的自然迁移，无需演示或特定平台训练。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.

</details>


### [34] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 本文研究发现自动驾驶系统离线评估与在线性能相关性较差，提出基于认知不确定性的离线评估指标，将相关性提升超过13%。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的离线评估与在线实际性能之间存在较大差距，现有评估方法难以准确预测系统在真实环境中的表现，这影响了评估的有效性。

Method: 通过大量仿真实验分析离线与在线评估的相关性，提出基于认知不确定性的离线评估指标来捕捉可能导致闭环错误的事件。

Result: 发现离线与在线评估的相关性比先前研究报道的更差，新提出的评估指标在相关性上提升了超过13%，在真实环境中表现更好。

Conclusion: 当前自动驾驶策略评估实践存在局限性，基于认知不确定性的离线评估方法能更好地预测在线性能，为改进评估方法提供了新思路。

Abstract: Real-World evaluation of perception-based planning models for robotic
systems, such as autonomous vehicles, can be safely and inexpensively conducted
offline, i.e., by computing model prediction error over a pre-collected
validation dataset with ground-truth annotations. However, extrapolating from
offline model performance to online settings remains a challenge. In these
settings, seemingly minor errors can compound and result in test-time
infractions or collisions. This relationship is understudied, particularly
across diverse closed-loop metrics and complex urban maneuvers. In this work,
we revisit this undervalued question in policy evaluation through an extensive
set of experiments across diverse conditions and metrics. Based on analysis in
simulation, we find an even worse correlation between offline and online
settings than reported by prior studies, casting doubts on the validity of
current evaluation practices and metrics for driving policies. Next, we bridge
the gap between offline and online evaluation. We investigate an offline metric
based on epistemic uncertainty, which aims to capture events that are likely to
cause errors in closed-loop settings. The resulting metric achieves over 13%
improvement in correlation compared to previous offline metrics. We further
validate the generalization of our findings beyond the simulation environment
in real-world settings, where even greater gains are observed.

</details>


### [35] [BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation](https://arxiv.org/abs/2510.08572)
*Rocktim Jyoti Das,Harsh Singh,Diana Turmakhan,Muhammad Abdullah Sohail,Mingfei Han,Preslav Nakov,Fabio Pizzati,Ivan Laptev*

Main category: cs.RO

TL;DR: BLAZER是一个从自动生成数据中学习机器人操作策略的框架，利用LLM规划器的零样本能力在模拟环境中自动生成多样化操作任务的演示，成功示例用于微调LLM以提升规划能力，无需人工监督。


<details>
  <summary>Details</summary>
Motivation: 机器人领域缺乏互联网规模的多样化任务演示数据，现有数据集受限于人工收集和整理，限制了数据规模和模型泛化能力的发展。

Method: 基于LLM规划器的零样本能力，在模拟环境中自动生成多样化操作任务的演示，使用成功示例微调LLM以改进其规划能力，无需人工监督。

Result: BLAZER显著提升了模拟和真实环境中的零样本操作能力，在训练集外的任务上也有改进，并支持LLM模型的下游缩放。

Conclusion: BLAZER框架通过自动生成训练数据有效解决了机器人领域数据稀缺问题，实现了从模拟到真实环境的技能迁移，提升了零样本操作性能。

Abstract: Scaling data and models has played a pivotal role in the remarkable progress
of computer vision and language. Inspired by these domains, recent efforts in
robotics have similarly focused on scaling both data and model size to develop
more generalizable and robust policies. However, unlike vision and language,
robotics lacks access to internet-scale demonstrations across diverse robotic
tasks and environments. As a result, the scale of existing datasets typically
suffers from the need for manual data collection and curation. To address this
problem, here we propose BLAZER, a framework that learns manipulation policies
from automatically generated training data. We build on the zero-shot
capabilities of LLM planners and automatically generate demonstrations for
diverse manipulation tasks in simulation. Successful examples are then used to
finetune an LLM and to improve its planning capabilities without human
supervision. Notably, while BLAZER training requires access to the simulator's
state, we demonstrate direct transfer of acquired skills to sensor-based
manipulation. Through extensive experiments, we show BLAZER to significantly
improve zero-shot manipulation in both simulated and real environments.
Moreover, BLAZER improves on tasks outside of its training pool and enables
downscaling of LLM models. Our code and data will be made publicly available on
the project page.

</details>
