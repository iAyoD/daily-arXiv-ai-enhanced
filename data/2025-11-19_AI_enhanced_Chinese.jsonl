{"id": "2511.13722", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13722", "abs": "https://arxiv.org/abs/2511.13722", "authors": ["William Guo", "Adaku Uchendu", "Ana Smith"], "title": "Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models", "comment": null, "summary": "To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\\to$ another language $\\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u79cd\u6c34\u5370\u6280\u672f\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u53ca\u5176\u5bf9\u6587\u672c\u8d28\u91cf\u548c\u5199\u4f5c\u98ce\u683c\u7684\u4fdd\u6301\u80fd\u529b\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6280\u672f\u80fd\u4fdd\u6301\u8bed\u4e49\u4f46\u504f\u79bb\u539f\u59cb\u5199\u4f5c\u98ce\u683c\uff0c\u4e14\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u7279\u522b\u662f\u56de\u8bd1\u653b\u51fb\u7684\u5f71\u54cd\u3002", "motivation": "\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u6f5c\u5728\u5371\u5bb3\uff0c\u901a\u8fc7\u6c34\u5370\u6280\u672f\u5d4c\u5165\u53ef\u68c0\u6d4b\u4fe1\u53f7\u3002\u7136\u800c\uff0c\u73b0\u6709\u6280\u672f\u5e38\u964d\u4f4e\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e14\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u5bfc\u81f4\u4e1a\u754c\u5bf9\u6c34\u5370\u6280\u672f\u5e7f\u6cdb\u5e94\u7528\u7684\u62b5\u5236\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u91ca\u4e49\u548c\u56de\u8bd1\uff08\u82f1\u8bed\u2192\u5176\u4ed6\u8bed\u8a00\u2192\u82f1\u8bed\uff09\u653b\u51fb\u6765\u8bc4\u4f30\u591a\u79cd\u6c34\u5370\u6280\u672f\u7684\u9c81\u68d2\u6027\uff1b\u4f7f\u7528\u8bed\u8a00\u6307\u6807\u6765\u8bc4\u4f30\u6c34\u5370\u6280\u672f\u4fdd\u6301\u6587\u672c\u8d28\u91cf\u548c\u5199\u4f5c\u98ce\u683c\u7684\u80fd\u529b\u3002", "result": "\u6c34\u5370\u6280\u672f\u80fd\u4fdd\u6301\u8bed\u4e49\uff0c\u4f46\u504f\u79bb\u672a\u52a0\u6c34\u5370\u6587\u672c\u7684\u5199\u4f5c\u98ce\u683c\uff0c\u4e14\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u7279\u522b\u662f\u56de\u8bd1\u653b\u51fb\u3002", "conclusion": "\u4e3a\u9f13\u52b1\u6c34\u5370\u6280\u672f\u7684\u91c7\u7528\uff0c\u9700\u8981\u63d0\u9ad8\u5176\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u66f4\u597d\u5730\u4fdd\u6301\u539f\u59cb\u6587\u672c\u7684\u5199\u4f5c\u98ce\u683c\u3002"}}
{"id": "2511.13726", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13726", "abs": "https://arxiv.org/abs/2511.13726", "authors": ["Guangzhi Wang", "Kai Li", "Yinghao Jiao", "Zhi Liu"], "title": "Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning", "comment": null, "summary": "We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.", "AI": {"tldr": "RT\u662f\u4e00\u79cd\u901a\u8fc7\u591a\u6b21\u524d\u5411\u4f20\u9012\u63d0\u5347\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5728\u8bed\u4e49\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u8bed\u4e49\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u8fdb\u4e00\u6b65\u6fc0\u6d3b\u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u591a\u6b21\u524d\u5411\u4f20\u9012\u8fd0\u884c\u6587\u672c\u5d4c\u5165\u6a21\u578b\u6765\u83b7\u5f97\u6700\u7ec8\u7684\u8bed\u4e49\u8868\u793a\uff0c\u662f\u4e00\u79cd\u6d4b\u8bd5\u65f6\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u5728BRIGHT\u548cPJBenchmark1\u8bed\u4e49\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5728C-MTEB\u7b49\u901a\u7528\u8bed\u4e49\u7406\u89e3\u4efb\u52a1\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "RT\u65b9\u6cd5\u6709\u6548\u6fc0\u6d3b\u4e86\u89e3\u7801\u5668\u4e13\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.13884", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13884", "abs": "https://arxiv.org/abs/2511.13884", "authors": ["Govardhan Padmanabhan"], "title": "Can QE-informed (Re)Translation lead to Error Correction?", "comment": "10 pages, 3 figures, WMT25 Shared Task in EMNLP 2025 Conference", "summary": "The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u8bad\u7ec3\u7684QE-informed\u9519\u8bef\u6821\u6b63\u65b9\u6cd5\uff1a\u4e00\u79cd\u662f\u9009\u62e9\u4e0d\u540cLLM\u751f\u6210\u7684\u6700\u4f73\u7ffb\u8bd1\u5019\u9009\uff0c\u53e6\u4e00\u79cd\u662f\u57fa\u4e8eQE\u89e3\u91ca\u66ff\u6362\u9519\u8bef\u5b50\u4e32\u3002\u7b2c\u4e00\u79cd\u65b9\u6cd5\u5728WMT 2025\u4efb\u52a1\u4e2d\u83b7\u80dc\u3002", "motivation": "\u867d\u7136\u8054\u5408\u8bad\u7ec3QE\u548cAPE\u7cfb\u7edf\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46APE\u7cfb\u7edf\u5b58\u5728\u8fc7\u5ea6\u6821\u6b63\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u65e0\u9700\u8bad\u7ec3\u7684\u7b80\u5355\u65b9\u6cd5\u6765\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "1. QE-informed\u91cd\u7ffb\u8bd1\uff1a\u4ece\u4e0d\u540cLLM\u751f\u6210\u7684\u591a\u4e2a\u5019\u9009\u7ffb\u8bd1\u4e2d\u9009\u62e9\u8d28\u91cf\u6700\u9ad8\u7684\u4e00\u4e2a\n2. \u57fa\u4e8eQE\u89e3\u91ca\u7684\u9519\u8bef\u66ff\u6362\uff1a\u6839\u636e\u63d0\u4f9b\u7684QE\u89e3\u91ca\uff0c\u6307\u5bfcLLM\u66ff\u6362\u9519\u8bef\u5b50\u4e32\uff0c\u5e76\u4f7f\u7528\u6761\u4ef6\u542f\u53d1\u5f0f\u65b9\u6cd5\u6700\u5c0f\u5316\u7f16\u8f91\u6b21\u6570", "result": "\u4e24\u79cd\u65b9\u6cd5\u5206\u522b\u83b7\u5f97\u4e860.0201\u548c-0.0108\u7684Delta COMET\u5206\u6570\uff0c\u7b2c\u4e00\u79cd\u65b9\u6cd5\u5728\u5b50\u4efb\u52a1\u6392\u884c\u699c\u4e0a\u83b7\u80dc", "conclusion": "\u65e0\u9700\u8bad\u7ec3\u7684QE-informed\u91cd\u7ffb\u8bd1\u65b9\u6cd5\u5728\u673a\u5668\u7ffb\u8bd1\u9519\u8bef\u6821\u6b63\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u907f\u514d\u4e86APE\u7cfb\u7edf\u7684\u8fc7\u5ea6\u6821\u6b63\u95ee\u9898\uff0c\u4e3a\u8d28\u91cf\u8bc4\u4f30\u6307\u5bfc\u7684\u9519\u8bef\u6821\u6b63\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2511.13900", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13900", "abs": "https://arxiv.org/abs/2511.13900", "authors": ["Mihir Gupte", "Eshan Dixit", "Muhammad Tayyab", "Arun Adiththan"], "title": "What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations", "comment": "To be submitted for publication", "summary": "The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the \"lost-in-the-middle\" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.", "AI": {"tldr": "\u63d0\u51fa\u4e86GM-Extract\u57fa\u51c6\u6570\u636e\u96c6\u6765\u8bc4\u4f30LLM\u5728\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6570\u636e\u8868\u793a\u65b9\u5f0f\u663e\u8457\u5f71\u54cd\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u5404\u79cd\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e2d\u51fa\u73b0\u7684\"\u8ff7\u5931\u5728\u4e2d\u95f4\"\u73b0\u8c61\uff0c\u8fd9\u5728\u57fa\u4e8e\u68c0\u7d22\u7684LLM\u5e94\u7528\u4e2d\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86GM-Extract\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\uff08\u7a7a\u95f4\u68c0\u7d22\u80fd\u529b\u548c\u8bed\u4e49\u68c0\u7d22\u80fd\u529b\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e867-8B\u53c2\u6570\u6a21\u578b\u5728\u591a\u6587\u6863\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u5404\u79cd\u7f13\u89e3\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u901a\u8fc7\u6539\u53d8\u6570\u636e\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u7684\u8868\u793a\u65b9\u5f0f\u53ef\u4ee5\u663e\u8457\u5f71\u54cd\u68c0\u7d22\u6027\u80fd\uff0c\u867d\u7136\u672a\u59cb\u7ec8\u89c2\u5bdf\u5230\u660e\u663e\u7684U\u578b\u66f2\u7ebf\uff0c\u4f46\u53d1\u73b0\u4e86\u6e05\u6670\u7684\u6027\u80fd\u6a21\u5f0f\u3002\u7f13\u89e3\u65b9\u6cd5\u7684\u6548\u679c\u5177\u6709\u9ad8\u5ea6\u590d\u6742\u6027\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u4f1a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u6027\u80fd\u53d7\u6570\u636e\u8868\u793a\u65b9\u5f0f\u5f71\u54cd\u663e\u8457\uff0c\u73b0\u6709\u7684\u7f13\u89e3\u65b9\u6cd5\u6548\u679c\u590d\u6742\u591a\u53d8\uff0c\u9700\u8981\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u8c28\u614e\u9009\u62e9\u548c\u4f7f\u7528\u3002"}}
{"id": "2511.13961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13961", "abs": "https://arxiv.org/abs/2511.13961", "authors": ["Jiarui Li", "Alessandro Zanardi", "Runyu Zhang", "Gioele Zardini"], "title": "FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding", "comment": null, "summary": "Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7ea7\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u5c06\u89c4\u5212\u4e0e\u6267\u884c\u96c6\u6210\uff0c\u901a\u8fc7\u95ed\u73af\u63a7\u5236\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u652f\u6301\u6570\u5343\u667a\u80fd\u4f53\u7684\u5b9e\u65f6\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5c06\u89c4\u5212\u4e0e\u6267\u884c\u5206\u79bb\uff0c\u4ee5\u4e34\u65f6\u65b9\u5f0f\u5904\u7406\u95ee\u9898\u53d8\u4f53\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u5f15\u5165MAPF\u7cfb\u7edf\u4f5c\u4e3a\u5f62\u5f0f\u6a21\u578b\uff0c\u5c06MAPF\u89c6\u4e3a\u63a7\u5236\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u56e0\u5b50\u5206\u89e3\u7684\u6709\u9650\u65f6\u57df\u95ed\u73af\u7b97\u6cd5FICO\uff0c\u5229\u7528\u7ec4\u5408\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6548\u95ed\u73af\u64cd\u4f5c\u3002", "result": "FICO\u7b97\u6cd5\u5b9e\u73b0\u6beb\u79d2\u7ea7\u54cd\u5e94\uff0c\u53ef\u6269\u5c55\u5230\u6570\u5343\u667a\u80fd\u4f53\uff0c\u5728\u968f\u673a\u5ef6\u8fdf\u548c\u667a\u80fd\u4f53\u5230\u8fbe\u60c5\u51b5\u4e0b\u6bd4\u5f00\u73af\u57fa\u7ebf\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u541e\u5410\u91cf\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u7ea7\u5efa\u6a21\u3001\u56e0\u5b50\u5206\u89e3\u548c\u95ed\u73af\u8bbe\u8ba1\uff0c\u4e3a\u5206\u6790\u548c\u63a8\u8fdbMAPF\u5efa\u7acb\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2511.13994", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13994", "abs": "https://arxiv.org/abs/2511.13994", "authors": ["Yilun Zhu", "Nikhita Vedula", "Shervin Malmasi"], "title": "Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition", "comment": "AACL 2025", "summary": "Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.", "AI": {"tldr": "LLM\u6846\u67b6\u89e3\u6790\u7535\u5546\u641c\u7d22\u4e2d\u6700\u9ad8\u7ea7\u67e5\u8be2\u7684\u6f5c\u5728\u610f\u56fe\uff0c\u901a\u8fc7\u5c5e\u6027-\u503c\u63d0\u793a\u5206\u89e3\u67e5\u8be2\uff0c\u63d0\u5347\u641c\u7d22\u6027\u80fd10.9 MAP\u548c5.9 MRR\uff0c\u5e76\u5f00\u53d1\u9ad8\u6548\u65b9\u6cd5\u5c06\u89e3\u91ca\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u4ee5\u89e3\u51b3\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u7535\u5546\u641c\u7d22\u4e2d\u7684\u6700\u9ad8\u7ea7\u67e5\u8be2\uff08\u5982\"\u6700\u597d\"\u3001\"\u6700\u53d7\u6b22\u8fce\"\uff09\u9700\u8981\u8de8\u591a\u4e2a\u7ef4\u5ea6\u6bd4\u8f83\u5019\u9009\u5546\u54c1\uff0c\u8fd9\u9700\u8981\u8bed\u8a00\u7406\u89e3\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u7c7b\u590d\u6742\u8bed\u4e49\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7LLM\u63d0\u53d6\u7ed3\u6784\u5316\u89e3\u91ca\u6216\u63d0\u793a\uff0c\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u5c5e\u6027-\u503c\u63d0\u793a\uff0c\u5e76\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u5e76\u884c\u751f\u6210\uff0c\u7136\u540e\u96c6\u6210\u5230\u6392\u5e8f\u7ba1\u9053\u4e2d\u3002", "result": "\u65b9\u6cd5\u5728MAP\u4e0a\u63d0\u534710.9\u70b9\uff0c\u5728MRR\u4e0a\u63d0\u53475.9\u70b9\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u7531\u4e8e\u76f4\u63a5\u4f7f\u7528LLM\u91cd\u65b0\u6392\u5e8f\u9762\u4e34\u8fc7\u9ad8\u5ef6\u8fdf\uff0c\u5f00\u53d1\u4e86\u5c06\u6700\u9ad8\u7ea7\u89e3\u91ca\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6700\u9ad8\u7ea7\u8bed\u4e49\u5982\u4f55\u5728\u6a21\u578b\u95f4\u8868\u793a\u548c\u8f6c\u79fb\uff0c\u63a8\u8fdb\u4e86\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u8bed\u8a00\u89e3\u91ca\u80fd\u529b\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u7ea6\u675f\u95ee\u9898\u3002"}}
{"id": "2511.13985", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13985", "abs": "https://arxiv.org/abs/2511.13985", "authors": ["Jan Quenzel", "Sven Behnke"], "title": "LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry", "comment": "submitted to T-RO, 19 pages", "summary": "Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLiDAR-IMU\u7684\u91cc\u7a0b\u8ba1\u7cfb\u7edfLIO-MARS\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u9762\u5143\u5730\u56fe\u4e0e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u8054\u5408\u914d\u51c6\uff0c\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4B\u6837\u6761\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u5b9a\u4f4d\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u9700\u8981\u9c81\u68d2\u7684\u5b9e\u65f6\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u641c\u6551\u7b49\u5e94\u7528\u4e2d\u3002LiDAR\u63d0\u4f9b\u7cbe\u786e\u8ddd\u79bb\u6d4b\u91cf\uff0cIMU\u7ea6\u675f\u52a0\u901f\u5ea6\u548c\u65cb\u8f6c\uff0c\u4e24\u8005\u4e92\u8865\u53ef\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u65f6\u95f4B\u6837\u6761\u8f68\u8ff9\uff0c\u901a\u8fc7\u975e\u5747\u5300\u65f6\u95f4\u8282\u70b9\u653e\u7f6e\u786e\u4fdd\u8f68\u8ff9\u8fde\u7eed\u6027\uff1b\u91c7\u7528Kronecker\u548c\u4e0e\u79ef\u52a0\u901f\u534f\u65b9\u5dee\u548cGMM\u8ba1\u7b97\uff1b\u4f7f\u7528\u65e0\u8ff9\u53d8\u6362\u53bb\u504f\u659c\u9762\u5143\uff1b\u5f15\u5165\u76f8\u5bf9\u4f4d\u59ff\u548c\u9884\u79ef\u5206IMU\u4f2a\u6d4b\u91cf\u7684\u8f6f\u7ea6\u675f\u3002", "result": "\u5728\u624b\u6301\u3001\u5730\u9762\u548c\u7a7a\u4e2d\u8f66\u8f86\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cLIO-MARS\u76f8\u6bd4\u73b0\u6709LIO\u7cfb\u7edf\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u4e863.3\u500d\u3002", "conclusion": "LIO-MARS\u901a\u8fc7\u521b\u65b0\u7684\u591a\u5206\u8fa8\u7387\u9762\u5143\u5730\u56fe\u914d\u51c6\u3001\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\u8868\u793a\u548c\u8ba1\u7b97\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u7684LiDAR-IMU\u91cc\u7a0b\u8ba1\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14010", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14010", "abs": "https://arxiv.org/abs/2511.14010", "authors": ["Chenchen Kuai", "Zihao Li", "Braden Rosen", "Stephanie Paan", "Navid Jafari", "Jean-Louis Briaud", "Yunlong Zhang", "Youssef M. A. Hashash", "Yang Zhou"], "title": "Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports", "comment": "17 pages, 5 figures", "summary": "Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.", "AI": {"tldr": "\u63d0\u51faMoRA-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u68c0\u7d22\u548c\u4ee3\u7406\u5206\u5757\u673a\u5236\uff0c\u5c06\u707e\u540e\u52d8\u5bdf\u62a5\u544a\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff0c\u7528\u4e8e\u591a\u707e\u5bb3\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u707e\u540e\u52d8\u5bdf\u62a5\u544a\u5305\u542b\u7406\u89e3\u591a\u707e\u5bb3\u76f8\u4e92\u4f5c\u7528\u7684\u5173\u952e\u8bc1\u636e\uff0c\u4f46\u5176\u975e\u7ed3\u6784\u5316\u53d9\u8ff0\u4f7f\u5f97\u7cfb\u7edf\u77e5\u8bc6\u4f20\u9012\u56f0\u96be\u3002\u73b0\u6709LLM\u5728\u7f3a\u4e4f\u9886\u57df\u57fa\u7840\u65f6\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9760\u6216\u5e7b\u89c9\u8f93\u51fa\u3002", "method": "\u5f00\u53d1MoRA-RAG\u6846\u67b6\uff0c\u5305\u542b\u6df7\u5408\u68c0\u7d22\u673a\u5236\u52a8\u6001\u8def\u7531\u8de8\u707e\u5bb3\u7279\u5b9a\u6570\u636e\u5e93\u67e5\u8be2\uff0c\u4f7f\u7528\u4ee3\u7406\u5206\u5757\u4fdd\u6301\u68c0\u7d22\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\uff0c\u5e76\u96c6\u6210\u9a8c\u8bc1\u5faa\u73af\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\u3001\u4f18\u5316\u67e5\u8be2\u548c\u542f\u52a8\u9488\u5bf9\u6027\u641c\u7d22\u3002", "result": "\u5728HazardRecQA\u6570\u636e\u96c6\u4e0a\uff0cMoRA-RAG\u8fbe\u523094.5%\u51c6\u786e\u7387\uff0c\u6bd4\u96f6\u6837\u672cLLM\u63d0\u534730%\uff0c\u6bd4\u6700\u5148\u8fdbRAG\u7cfb\u7edf\u63d0\u534710%\uff0c\u540c\u65f6\u5728\u4e0d\u540cLLM\u67b6\u6784\u4e0a\u51cf\u5c11\u5e7b\u89c9\u3002", "conclusion": "MoRA-RAG\u4e3a\u5c06\u707e\u540e\u6587\u6863\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u3001\u53ef\u4fe1\u8d56\u7684\u707e\u5bb3\u97e7\u6027\u60c5\u62a5\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u4f7f\u5f00\u6e90LLM\u8fbe\u5230\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2511.14004", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14004", "abs": "https://arxiv.org/abs/2511.14004", "authors": ["Taijing Chen", "Sateesh Kumar", "Junhong Xu", "George Pavlakos", "J oydeep Biswas", "Roberto Mart\u00edn-Mart\u00edn"], "title": "Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval", "comment": "This paper is under review at ICRA", "summary": "Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes (\"the red mug\"), spatial context (\"the mug on the table\"), or past states (\"the mug that was here yesterday\"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem.", "AI": {"tldr": "STAR\u6846\u67b6\u7edf\u4e00\u65f6\u7a7a\u67e5\u8be2\u548c\u5177\u8eab\u4ea4\u4e92\uff0c\u901a\u8fc7\u957f\u671f\u8bb0\u5fc6\u548c\u5de5\u4f5c\u8bb0\u5fc6\u652f\u6301\u9ad8\u6548\u68c0\u7d22\uff0c\u5728\u52a8\u6001\u5f00\u653e\u4e16\u754c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u89e3\u51b3\u90e8\u5206\u95ee\u9898\uff1a\u573a\u666f\u56fe\u5ffd\u7565\u65f6\u95f4\u7ef4\u5ea6\uff0c\u65f6\u5e8f\u63a8\u7406\u4e0d\u652f\u6301\u5177\u8eab\u4ea4\u4e92\uff0c\u52a8\u6001\u573a\u666f\u56fe\u4ecd\u662f\u5c01\u95ed\u4e16\u754c\u3002\u9700\u8981\u7edf\u4e00\u65f6\u7a7a\u68c0\u7d22\u548c\u5177\u8eab\u52a8\u4f5c\u7684\u6846\u67b6\u3002", "method": "STAR\u6846\u67b6\u7ed3\u5408\u975e\u53c2\u6570\u957f\u671f\u8bb0\u5fc6\u548c\u5de5\u4f5c\u8bb0\u5fc6\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6bcf\u4e00\u6b65\u9009\u62e9\u65f6\u7a7a\u52a8\u4f5c\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u51b3\u7b56\u5faa\u73af\u3002", "result": "\u5728STARBench\u57fa\u51c6\u6d4b\u8bd5\u548cTiago\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cSTAR\u6301\u7eed\u4f18\u4e8e\u573a\u666f\u56fe\u548c\u7eaf\u8bb0\u5fc6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u65f6\u95f4\u641c\u7d22\u548c\u7a7a\u95f4\u641c\u7d22\u4f5c\u4e3a\u7edf\u4e00\u95ee\u9898\u5904\u7406\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cSTAR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u5bf9\u8c61\u68c0\u7d22\u95ee\u9898\u3002"}}
{"id": "2511.14027", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14027", "abs": "https://arxiv.org/abs/2511.14027", "authors": ["Junjie Wu", "Yumeng Fu", "Nan Yu", "Guohong Fu"], "title": "HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection", "comment": null, "summary": "Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86HiEAG\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bc1\u636e\u589e\u5f3a\u751f\u6210\u6765\u6539\u8fdb\u591a\u6a21\u6001\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u5916\u90e8\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u7ed3\u5408\u68c0\u7d22\u3001\u91cd\u6392\u5e8f\u548c\u91cd\u5199\u6a21\u5757\uff0c\u5229\u7528MLLMs\u7684\u5e7f\u6cdb\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u7684OOC\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u8fc7\u4e8e\u5f3a\u8c03\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0e\u5916\u90e8\u8bc1\u636e\u4e4b\u95f4\u7684\u5916\u90e8\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002", "method": "HiEAG\u6846\u67b6\u5c06\u5916\u90e8\u4e00\u81f4\u6027\u68c0\u67e5\u5206\u89e3\u4e3a\u68c0\u7d22\u3001\u91cd\u6392\u5e8f\u548c\u91cd\u5199\u7684\u7efc\u5408\u5f15\u64ce\u7ba1\u9053\uff0c\u4f7f\u7528AESP\u8fdb\u884c\u8bc1\u636e\u91cd\u6392\u5e8f\uff0cAEGP\u8fdb\u884c\u8bc1\u636e\u91cd\u5199\uff0c\u5e76\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u5b9e\u73b0\u5224\u65ad\u89e3\u91ca\u3002", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHiEAG\u5728\u6240\u6709\u6837\u672c\u7684\u51c6\u786e\u7387\u4e0a\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HiEAG\u901a\u8fc7\u5206\u5c42\u8bc1\u636e\u589e\u5f3a\u751f\u6210\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5916\u90e8\u4e00\u81f4\u6027\u68c0\u67e5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.14024", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14024", "abs": "https://arxiv.org/abs/2511.14024", "authors": ["Jaskirat Singh", "Rohan Chandra"], "title": "FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities", "comment": null, "summary": "Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins.", "AI": {"tldr": "FACA\u662f\u4e00\u79cd\u516c\u5e73\u654f\u6377\u7684\u78b0\u649e\u907f\u514d\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u534f\u8c03\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5728\u53d7\u9650\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6551\u63f4\u3001\u914d\u9001\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u9700\u8981\u5728\u53d7\u9650\u7a7a\u95f4\u9ad8\u901f\u5bfc\u822a\uff0c\u4f46\u62e5\u6324\u73af\u5883\u4e2d\u7684\u89d2\u8272\u4f18\u5148\u7ea7\u52a8\u6001\u53d8\u5316\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u534f\u8c03\u4efb\u52a1\uff0c\u7ed3\u5408\u65b0\u578b\u4eba\u5de5\u52bf\u573a\u7b97\u6cd5\u5728\u51b2\u7a81\u65f6\u81ea\u52a8\u521b\u5efa\"\u73af\u5c9b\"\u6548\u5e94\uff0c\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u654f\u6377\u6027", "result": "FACA\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6548\u7387\u63d0\u53473.5\u500d\u4ee5\u4e0a\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1170%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u7684\u5b89\u5168\u8fb9\u754c", "conclusion": "FACA\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u534f\u8c03\u548c\u667a\u80fd\u52bf\u573a\u8bbe\u8ba1\uff0c\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u5b89\u5168\u4e0e\u654f\u6377\u6027\u7684\u826f\u597d\u5e73\u8861"}}
{"id": "2511.14073", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14073", "abs": "https://arxiv.org/abs/2511.14073", "authors": ["Zijin Su", "Huanzhu Lv", "Yuren Niu", "Yiming Liu"], "title": "Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement", "comment": "12 pages, 8 figures, 5 tables. Dataset and code available at https://doi.org/10.5281/zenodo.16890154 and https://doi.org/10.5281/zenodo.15837871", "summary": "Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.", "AI": {"tldr": "\u6784\u5efa\u5e73\u8861\u7684\u591a\u6807\u7b7e\u60c5\u611f\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u589e\u5f3a\u5206\u7c7b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u6027\u80fd", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5982GoEmotions\u5b58\u5728\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u60c5\u611f\u7c7b\u522b", "method": "\u6574\u5408GoEmotions\u539f\u59cb\u6570\u636e\u3001\u4f7f\u7528RoBERTa-base-GoEmotions\u6a21\u578b\u6807\u6ce8\u7684Sentiment140\u6837\u672c\uff0c\u4ee5\u53caGPT-4 mini\u751f\u6210\u7684\u624b\u52a8\u6807\u6ce8\u6587\u672c\uff0c\u6784\u5efa\u5e73\u8861\u6570\u636e\u96c6\uff1b\u5f00\u53d1\u7ed3\u5408FastText\u5d4c\u5165\u3001\u5377\u79ef\u5c42\u3001\u53cc\u5411LSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6a21\u578b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u65b9\u9762\u76f8\u6bd4\u4e0d\u5e73\u8861\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5e73\u8861\u6570\u636e\u96c6\u548c\u589e\u5f3a\u6a21\u578b\u67b6\u6784\u7684\u6709\u6548\u6027"}}
{"id": "2511.14037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14037", "abs": "https://arxiv.org/abs/2511.14037", "authors": ["Hesam Mojtahedi", "Reza Akhavian"], "title": "BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation", "comment": null, "summary": "This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eBIM\u5dee\u5f02\u9a71\u52a8\u7684\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u548c\u5730\u9762\u673a\u5668\u4eba\u5728\u52a8\u6001\u5efa\u7b51\u73af\u5883\u4e2d\u7684\u534f\u540c\u5bfc\u822a\uff0c\u901a\u8fc7\u98ce\u9669\u89e6\u53d1\u91cd\u65b0\u626b\u63cf\u663e\u8457\u964d\u4f4e\u5bfc\u822a\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001BIM\u5148\u9a8c\u6216\u6709\u9650\u7684\u673a\u8f7d\u611f\u77e5\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u5efa\u7b51\u73af\u5883\u7684\u53d8\u5316\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u878d\u5408\u5b9e\u65f6\u611f\u77e5\u6570\u636e\u4e0eBIM\u5148\u9a8c\u7684\u4e3b\u52a8\u611f\u77e5\u65b9\u6cd5\u3002", "method": "\u878d\u5408\u7a7a\u4e2d\u548c\u5730\u9762\u673a\u5668\u4eba\u7684\u5b9e\u65f6LiDAR\u6570\u636e\u4e0eBIM\u5148\u9a8c\uff0c\u7ef4\u62a4\u52a8\u6001\u6f14\u5316\u76842D\u5360\u636e\u5730\u56fe\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8d70\u5eca\u98ce\u9669\u6307\u6807\u91cf\u5316\u5bfc\u822a\u5b89\u5168\u6027\uff0c\u5f53\u98ce\u9669\u8d85\u8fc7\u9608\u503c\u65f6\u65e0\u4eba\u673a\u81ea\u4e3b\u91cd\u65b0\u626b\u63cf\u53d7\u5f71\u54cd\u533a\u57df\u3002", "result": "\u5728PX4-Gazebo\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u98ce\u9669\u89e6\u53d1\u91cd\u65b0\u626b\u63cf\u4f7f\u5e73\u5747\u8d70\u5eca\u98ce\u9669\u964d\u4f4e58%\uff0c\u5730\u56fe\u71b5\u964d\u4f4e43%\uff0c\u540c\u65f6\u4fdd\u63010.4\u7c73\u4ee5\u4e0a\u7684\u5b89\u5168\u95f4\u8ddd\uff0c\u76f8\u6bd4\u524d\u6cbf\u63a2\u7d22\u65b9\u6cd5\u5728\u76f8\u540c\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\u4efb\u52a1\u65f6\u95f4\u51cf\u534a\u3002", "conclusion": "\u5c06BIM\u5148\u9a8c\u4e0e\u98ce\u9669\u81ea\u9002\u5e94\u7a7a\u4e2d\u611f\u77e5\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4e3a\u5efa\u7b51\u673a\u5668\u4eba\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u4e3b\u6027\u3002"}}
{"id": "2511.14106", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14106", "abs": "https://arxiv.org/abs/2511.14106", "authors": ["Le Yu", "Zhengyue Zhao", "Yawen Zheng", "Yunhao Liu"], "title": "Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT", "comment": "10 pages, 7 figures", "summary": "Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \\textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \\textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \\textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \\textcolor{red}{\\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStealth Fine-Tuning\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u5206\u6bb5\u7ea7\u5e72\u6270\uff0c\u5229\u7528\u81ea\u751f\u6210\u8f93\u51fa\u4f5c\u4e3a\u76d1\u7763\u5fae\u8c03\u6570\u636e\uff0c\u80fd\u591f\u4ee5\u4f4e\u6210\u672c\u6709\u6548\u7ed5\u8fc7RVLMs\u7684\u5b89\u5168\u5bf9\u9f50\u9632\u5fa1\u3002", "motivation": "\u5c3d\u7ba1RVLMs\u4f9d\u8d56\u5b89\u5168\u5bf9\u9f50\u6765\u9632\u6b62\u6709\u5bb3\u884c\u4e3a\uff0c\u4f46\u5176\u66b4\u9732\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u5b89\u5168\u5bf9\u9f50\u5bb9\u6613\u88ab\u7a81\u7834\u3002", "method": "\u901a\u8fc7\u5206\u6bb5\u7ea7\u5e72\u6270\u5f15\u53d1\u6709\u5bb3\u63a8\u7406\u8f68\u8ff9\uff0c\u5c06\u81ea\u751f\u6210\u8f93\u51fa\u4f5c\u4e3a\u76d1\u7763\u5fae\u8c03\u6570\u636e\uff0c\u91c7\u7528\u57fa\u4e8e\u8f6e\u6b21\u7684\u52a0\u6743\u635f\u5931\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u3001\u5206\u5e03\u4e00\u81f4\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u4ec5\u7528499\u4e2a\u6837\u672c\u548c\u5355\u5f20A100\u663e\u53613\u5c0f\u65f6\u5185\uff0c\u653b\u51fb\u6210\u529f\u7387\u6bd4IDEATOR\u9ad8\u51fa38.52%\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u6a21\u578b\u8868\u793a\u5206\u5e03\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "Stealth Fine-Tuning\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7ed5\u8fc7\u5bf9\u9f50\u9632\u5fa1\u7684\u65b9\u6cd5\uff0c\u5728AdvBench\u548c\u591a\u4e2a\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.14139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14139", "abs": "https://arxiv.org/abs/2511.14139", "authors": ["Junhao Gong", "Shoujie Li", "Kit-Wa Sou", "Changqing Guo", "Hourong Huang", "Tong Wu", "Yifan Xie", "Chenxin Liang", "Chuqiao Lyu", "Xiaojun Liang", "Wenbo Ding"], "title": "FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing", "comment": null, "summary": "Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at https://anonymous.4open.science/api/repo/FlexiCup-DA7D/file/index.html?v=8f531b44.", "AI": {"tldr": "FlexiCup\u662f\u4e00\u79cd\u5b8c\u5168\u65e0\u7ebf\u591a\u6a21\u6001\u5438\u76d8\uff0c\u96c6\u6210\u4e86\u53cc\u533a\u57df\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\uff0c\u652f\u6301\u771f\u7a7a\u548c\u4f2f\u52aa\u5229\u4e24\u79cd\u5438\u9644\u6a21\u5f0f\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u673a\u68b0\u914d\u7f6e\u5b9e\u73b0\u65e0\u7ebf\u81ea\u4e3b\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u5438\u76d8\u7f3a\u4e4f\u4f20\u611f\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u63a5\u89e6\u611f\u77e5\u64cd\u4f5c\u3002\u9700\u8981\u5f00\u53d1\u5177\u6709\u4f20\u611f\u529f\u80fd\u7684\u5438\u76d8\u6765\u6539\u5584\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6293\u53d6\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u53cc\u533a\u57df\u4f20\u611f\u7cfb\u7edf\uff1a\u4e2d\u5fc3\u533a\u57df\u901a\u8fc7\u7167\u660e\u63a7\u5236\u5728\u89c6\u89c9\u548c\u89e6\u89c9\u6a21\u6001\u95f4\u52a8\u6001\u5207\u6362\u7528\u4e8e\u63a5\u89e6\u68c0\u6d4b\uff0c\u5916\u56f4\u533a\u57df\u63d0\u4f9b\u8fde\u7eed\u7a7a\u95f4\u611f\u77e5\u7528\u4e8e\u63a5\u8fd1\u89c4\u5212\u3002\u91c7\u7528\u6a21\u5757\u5316\u673a\u68b0\u914d\u7f6e\u652f\u6301\u4e24\u79cd\u5438\u9644\u6a21\u5f0f\uff0c\u5e76\u5b9e\u73b0\u5b8c\u5168\u65e0\u7ebf\u81ea\u4e3b\u64cd\u4f5c\u3002", "result": "\u5728\u7ed3\u6784\u5316\u8868\u9762\u4e0a\u7684\u6a21\u5757\u5316\u611f\u77e5\u9a71\u52a8\u6293\u53d6\u4e2d\uff0c\u771f\u7a7a\u6a21\u5f0f\u5e73\u5747\u6210\u529f\u738790.0%\uff0c\u4f2f\u52aa\u5229\u6a21\u5f0f86.7%\u3002\u57fa\u4e8e\u6269\u6563\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u5728\u503e\u659c\u8fd0\u8f93\u4efb\u52a1\u4e2d\u6210\u529f\u738773.3%\uff0c\u6a59\u5b50\u63d0\u53d6\u4efb\u52a1\u4e2d66.7%\u3002\u53cc\u533a\u57df\u89c2\u6d4b\u534f\u8c03\u63d0\u4f9b13%\u7684\u63a5\u89e6\u611f\u77e5\u64cd\u4f5c\u6539\u8fdb\u3002", "conclusion": "FlexiCup\u901a\u8fc7\u96c6\u6210\u591a\u6a21\u6001\u4f20\u611f\u548c\u53cc\u5438\u9644\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5438\u76d8\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u65e0\u7ebf\u591a\u6a21\u6001\u5438\u76d8\u5728\u590d\u6742\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.14112", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14112", "abs": "https://arxiv.org/abs/2511.14112", "authors": ["Truong Vo", "Weiyi Wu", "Kaize Ding"], "title": "Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding", "comment": "4 page-short paper", "summary": "Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u51fa\u9662\u6458\u8981\u6765\u89e3\u51b3ICD\u7f16\u7801\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u63d0\u5347\u7f55\u89c1\u548c\u96f6\u6837\u672c\u4ee3\u7801\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u6587\u672c\u81ea\u52a8ICD\u7f16\u7801\u4efb\u52a1\u53d7\u5230\u8bca\u65ad\u4ee3\u7801\u6781\u7aef\u957f\u5c3e\u5206\u5e03\u7684\u963b\u788d\uff0c\u6570\u5343\u4e2a\u7f55\u89c1\u548c\u96f6\u6837\u672cICD\u4ee3\u7801\u5728\u6570\u636e\u96c6\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5b8f\u89c2F1\u5206\u6570\u8f83\u4f4e\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u7f55\u89c1\u4ee3\u7801\u7684\u73b0\u5b9e\u591a\u6807\u7b7e\u4ee3\u7801\u96c6\uff0c\u5229\u7528\u771f\u5b9e\u4e16\u754c\u5171\u73b0\u6a21\u5f0f\u3001ICD\u63cf\u8ff0\u3001\u540c\u4e49\u8bcd\u3001\u5206\u7c7b\u6cd5\u548c\u76f8\u4f3c\u4e34\u5e8a\u7b14\u8bb0\u751f\u6210\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u751f\u621090,000\u4e2a\u5408\u6210\u7b14\u8bb0\u8986\u76d67,902\u4e2aICD\u4ee3\u7801\u3002", "result": "\u5728\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u4e8etransformer\u7684\u6a21\u578b\u4e0a\u5fae\u8c03\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u9002\u5ea6\u63d0\u9ad8\u4e86\u5b8f\u89c2F1\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u5fae\u89c2F1\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u867d\u7136\u76f8\u5bf9\u4e8e\u8ba1\u7b97\u6210\u672c\u589e\u76ca\u53ef\u80fd\u663e\u5f97\u6709\u9650\uff0c\u4f46\u7ed3\u679c\u8868\u660e\u7cbe\u5fc3\u5236\u4f5c\u7684\u5408\u6210\u6570\u636e\u53ef\u4ee5\u589e\u5f3a\u957f\u5c3eICD\u4ee3\u7801\u9884\u6d4b\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2511.14148", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14148", "abs": "https://arxiv.org/abs/2511.14148", "authors": ["Yuhua Jiang", "Shuang Cheng", "Yan Ding", "Feifei Gao", "Biqing Qi"], "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models", "comment": null, "summary": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.", "AI": {"tldr": "\u63d0\u51faAsyncVLA\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u6d41\u5339\u914d\u548c\u81ea\u6821\u6b63\u673a\u5236\u6539\u8fdbVLA\u6a21\u578b\uff0c\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u4f20\u7edfVLA\u6a21\u578b\u4f7f\u7528\u540c\u6b65\u6d41\u5339\u914d\uff0c\u4f9d\u8d56\u521a\u6027\u7684\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\uff0c\u7f3a\u4e4f\u52a8\u4f5c\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u5f02\u6b65\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5bb9\u6613\u56e0\u5355\u4e2a\u52a8\u4f5c\u9519\u8bef\u5bfc\u81f4\u7ea7\u8054\u5931\u8d25\u3002", "method": "\u5f15\u5165\u5f02\u6b65\u6d41\u5339\u914d\uff0c\u5728\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\u4e2d\u751f\u6210\u52a8\u4f5ctoken\uff1b\u8bbe\u8ba1\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u5668\u63d0\u53d6\u521d\u59cb\u751f\u6210\u52a8\u4f5c\u7684\u7f6e\u4fe1\u5ea6\uff0c\u9009\u62e9\u6027\u7cbe\u70bc\u4e0d\u51c6\u786e\u7684\u52a8\u4f5ctoken\uff1b\u63d0\u51faSFM\u548cAFM\u7684\u7edf\u4e00\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAsyncVLA\u8868\u73b0\u51fa\u6570\u636e\u6548\u7387\u548c\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u5728\u901a\u7528\u5177\u8eab\u8bc4\u4f30\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "AsyncVLA\u901a\u8fc7\u5f02\u6b65\u751f\u6210\u548c\u81ea\u6821\u6b63\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2511.14142", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14142", "abs": "https://arxiv.org/abs/2511.14142", "authors": ["Omkar Mahesh Kashyap", "Padegal Amit", "Madhav Kashyap", "Ashwini M Joshi", "Shylaja SS"], "title": "From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling", "comment": null, "summary": "Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.", "AI": {"tldr": "HyperABSA\u662f\u4e00\u4e2a\u52a8\u6001\u8d85\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u6837\u672c\u7279\u5b9a\u7684\u5c42\u6b21\u805a\u7c7b\u6784\u5efa\u65b9\u9762-\u89c2\u70b9\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u65b9\u6cd5\u5728\u77ed\u6587\u672c\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5197\u4f59\u548c\u53c2\u6570\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u53ea\u80fd\u5efa\u6a21\u6210\u5bf9\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u6784\u5efa\u591a\u4e2a\u56fe\u6765\u6355\u6349\u4e0d\u540c\u5173\u7cfb\u89c6\u56fe\uff0c\u8fd9\u5f15\u5165\u4e86\u5197\u4f59\u3001\u53c2\u6570\u5f00\u9500\u548c\u878d\u5408\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\uff0c\u5728\u77ed\u6587\u672c\u3001\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u9650\u5236\u4e86\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u8d85\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u6837\u672c\u7279\u5b9a\u7684\u5c42\u6b21\u805a\u7c7b\u8bf1\u5bfc\u65b9\u9762-\u89c2\u70b9\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u52a0\u901f-\u56de\u9000\u622a\u6b62\u70b9\u7684\u65b0\u65b9\u6cd5\u6765\u81ea\u9002\u5e94\u786e\u5b9a\u805a\u7c7b\u7684\u7c92\u5ea6\u7ea7\u522b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08Lap14\u3001Rest14\u3001MAMS\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u5f3a\u5927\u7684\u56fe\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u7279\u522b\u662f\u4e0eRoBERTa\u9aa8\u5e72\u7f51\u7edc\u7ed3\u5408\u65f6\u83b7\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u52a8\u6001\u8d85\u56fe\u6784\u5efa\u662fABSA\u4efb\u52a1\u7684\u9ad8\u6548\u5f3a\u5927\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u6709\u6f5c\u529b\u6269\u5c55\u5230\u5176\u4ed6\u77ed\u6587\u672cNLP\u4efb\u52a1\u3002"}}
{"id": "2511.14161", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14161", "abs": "https://arxiv.org/abs/2511.14161", "authors": ["Xiaoquan Sun", "Ruijian Zhang", "Kang Pang", "Bingchen Miao", "Yuxiang Tan", "Zhen Yang", "Ming Li", "Jiayu Chen"], "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action", "comment": null, "summary": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.", "AI": {"tldr": "RoboTidy\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bed\u8a00\u5f15\u5bfc\u5bb6\u5ead\u6574\u7406\u57fa\u51c6\uff0c\u652f\u6301\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u548c\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u5177\u8eabAI\u4e2d\u73b0\u5b9e\u5bb6\u5ead\u6574\u7406\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7684\u5bb6\u5ead\u6574\u7406\u57fa\u51c6\u65e2\u4e0d\u80fd\u6a21\u62df\u7528\u6237\u504f\u597d\uff0c\u4e5f\u4e0d\u652f\u6301\u79fb\u52a8\u6027\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u8bed\u8a00\u5230\u52a8\u4f5c\u7684\u96c6\u6210\u80fd\u529b\u3002", "method": "\u63d0\u4f9b500\u4e2a\u903c\u771f\u76843D\u9ad8\u65af\u6e85\u5c04\u5bb6\u5ead\u573a\u666f\uff0c\u6db5\u76d6500\u4e2a\u7269\u4f53\u548c\u5bb9\u5668\uff0c\u5c06\u6574\u7406\u8868\u8ff0\u4e3a\"\u52a8\u4f5c(\u7269\u4f53,\u5bb9\u5668)\"\u5217\u8868\uff0c\u5e76\u63d0\u4f9b6.4k\u9ad8\u8d28\u91cf\u64cd\u4f5c\u6f14\u793a\u8f68\u8ff9\u548c1.5k\u5bfc\u822a\u8f68\u8ff9\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u652f\u6301\u5c0f\u6837\u672c\u548c\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u7528\u4e8e\u7269\u4f53\u6574\u7406\uff0c\u5efa\u7acb\u4e86\u7aef\u5230\u7aef\u7684\u5bb6\u5ead\u6574\u7406\u57fa\u51c6\u3002", "conclusion": "RoboTidy\u901a\u8fc7\u5b9e\u73b0\u8bed\u8a00\u5f15\u5bfc\u673a\u5668\u4eba\u7684\u6574\u4f53\u548c\u73b0\u5b9e\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u5177\u8eabAI\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2511.14144", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.14144", "abs": "https://arxiv.org/abs/2511.14144", "authors": ["Naoki Shimoda", "Akihiro Yamamoto"], "title": "Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions", "comment": "Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575)", "summary": "In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the \"fill-in-the-blank\" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u5173\u7cfb\u62bd\u53d6\u548c\u77e5\u8bc6\u56fe\u8c31\u5339\u914d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u56de\u7b54\u586b\u7a7a\u9898\u5f62\u5f0f\u7684\u591a\u9009\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8fc7\u7a0b\u7684\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u5229\u7528Transformer\u5173\u7cfb\u62bd\u53d6\u65b9\u6cd5\u52a8\u6001\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u6765\u8868\u793a\u8f93\u5165\u53e5\u5b50\u7684\u542b\u4e49\uff0c\u89e3\u51b3\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6210\u672c\u9ad8\u4e14\u9759\u6001\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5173\u7cfb\u62bd\u53d6\u5c06\u95ee\u9898\u53e5\u5b50\u8f6c\u6362\u4e3a\u5173\u7cfb\u56fe\uff0c\u7136\u540e\u5728\u5c01\u95ed\u4e16\u754c\u5047\u8bbe\u4e0b\u4e0e\u4e8b\u5b9e\u6b63\u786e\u7684\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4ee5\u8861\u91cf\u53e5\u5b50\u7684\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u6b63\u786e\u56de\u7b54\u7ea670%\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u4f9b\u8fc7\u7a0b\u53ef\u8ffd\u6eaf\u6027\uff0c\u4e14\u95ee\u9898\u7c7b\u522b\u5bf9\u51c6\u786e\u7387\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5173\u7cfb\u62bd\u53d6\u548c\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\u7684\u591a\u9009\u9898\u56de\u7b54\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.14178", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14178", "abs": "https://arxiv.org/abs/2511.14178", "authors": ["Zhuo Li", "Junjia Liu", "Zhipeng Dong", "Tao Teng", "Quentin Rouxel", "Darwin Caldwell", "Fei Chen"], "title": "Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion", "comment": "9 pages, 8 figures, submitted to IEEE RA-L", "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.", "AI": {"tldr": "VLA-Pilot\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u7b56\u7565\u5f15\u5bfc\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u6216\u6570\u636e\u6536\u96c6\u5373\u53ef\u5b9e\u73b0\u9884\u8bad\u7ec3VLA\u6a21\u578b\u7684\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u4e0b\u6e38\u90e8\u7f72\u65f6\u5b58\u5728\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u800c\u5fae\u8c03\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6f14\u793a\u6570\u636e\u6536\u96c6\u548c\u5bc6\u96c6\u8ba1\u7b97\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5b9e\u7528\u3002", "method": "\u63d0\u51faVLA-Pilot\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u63a8\u7406\u65f6\u7b56\u7565\u5f15\u5bfc\u6280\u672f\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u989d\u5916\u6570\u636e\u6536\u96c6\uff0c\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u65b9\u5f0f\u63d0\u5347\u9884\u8bad\u7ec3VLA\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u4e0b\u6e38\u64cd\u4f5c\u4efb\u52a1\u548c\u4e24\u79cd\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6db5\u76d6\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aVLA-Pilot\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6210\u9884\u8bad\u7ec3VLA\u7b56\u7565\u7684\u6210\u529f\u7387\u3002", "conclusion": "VLA-Pilot\u80fd\u591f\u5b9e\u73b0\u9884\u8bad\u7ec3VLA\u6a21\u578b\u7684\u9c81\u68d2\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u548c\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14166", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14166", "abs": "https://arxiv.org/abs/2511.14166", "authors": ["Hao Lang", "Fei Huang", "Yongbin Li"], "title": "Selective Weak-to-Strong Generalization", "comment": "AAAI2025 Special Track on AI Alignment", "summary": "Future superhuman models will surpass the ability of humans and humans will only be able to \\textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.", "AI": {"tldr": "\u63d0\u51fa\u9009\u62e9\u6027\u5f31\u5230\u5f3a\u6cdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e8c\u5143\u5206\u7c7b\u5668\u8bc6\u522b\u5f3a\u6a21\u578b\u80fd\u56de\u7b54\u7684\u95ee\u9898\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5f31\u76d1\u7763\uff0c\u5e76\u4f7f\u7528\u56fe\u5e73\u6ed1\u65b9\u6cd5\u4f18\u5316\u5f31\u6807\u7b7e\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8d85\u4eba\u7c7b\u6a21\u578b\u5bf9\u9f50\u65f6\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5f31\u5230\u5f3a\u6cdb\u5316\u65b9\u6cd5\u4f7f\u7528\u5f31\u76d1\u7763\u5b58\u5728\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u90e8\u5206\u5f31\u6807\u7b7e\u5bf9\u6a21\u578b\u6709\u5bb3\u3002", "method": "\u8bad\u7ec3P(IK)\u4e8c\u5143\u5206\u7c7b\u5668\u8bc6\u522b\u5f3a\u6a21\u578b\u80fd\u56de\u7b54\u7684\u95ee\u9898\uff0c\u4f7f\u7528\u81ea\u751f\u6210\u6807\u7b7e\u8fdb\u884c\u5bf9\u9f50\uff0c\u901a\u8fc7\u56fe\u5e73\u6ed1\u65b9\u6cd5\u4f18\u5316\u5f31\u6807\u7b7e\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0cP(IK)\u80fd\u8de8\u4efb\u52a1\u548c\u96be\u5ea6\u6cdb\u5316\u3002", "conclusion": "\u9009\u62e9\u6027\u5f31\u5230\u5f3a\u6cdb\u5316\u6709\u52a9\u4e8e\u8d85\u5bf9\u9f50\u95ee\u9898\u7684\u89e3\u51b3\u3002"}}
{"id": "2511.14327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14327", "abs": "https://arxiv.org/abs/2511.14327", "authors": ["Felipe Ballen-Moreno", "Pasquale Ferrentino", "Milan Amighi", "Bram Vanderborght", "Tom Verstraten"], "title": "Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics", "comment": "36 pages, 10 figures, submitted and under-review in Journal of the Mechanical Behavior of Biomedical Materials", "summary": "Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u53d8\u91cf\u8868\u5f81\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u6cd5\u5411\u548c\u5207\u5411\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u53d8\u91cf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u4e0e\u4eba\u4f53\u4e4b\u95f4\u7684\u7269\u7406\u4ea4\u4e92\u590d\u6742\uff0c\u6d89\u53ca\u591a\u81ea\u7531\u5ea6\u8fd0\u52a8\u548c\u8f6f\u7ec4\u7ec7\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u73b0\u6709\u5355\u53d8\u91cf\u8868\u5f81\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u63cf\u8ff0\u8fd9\u79cd\u590d\u6742\u4ea4\u4e92\u3002", "method": "\u5f15\u5165\u53cc\u53d8\u91cf\u8868\u5f81\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u6cd5\u5411\u529b\u548c\u5207\u5411\u529b\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u573a\u666f\u548c\u6750\u6599\u6a21\u578b\u4e0b\u7684\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u6765\u8bc4\u4f30\u5355\u53d8\u91cf\u62df\u5408\u5bf9\u529b\u548c\u626d\u77e9\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u8868\u5f81\u8fc7\u7a0b\u4e2d\u7eb3\u5165\u4e24\u4e2a\u53d8\u91cf\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6a21\u62df\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u8896\u5e26\u4e0e\u4eba\u4f53\u80a2\u4f53\u4e4b\u95f4\u7684\u7269\u7406\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u57fa\u7840\u3002", "conclusion": "\u53cc\u53d8\u91cf\u8868\u5f81\u65b9\u6cd5\u80fd\u591f\u66f4\u53ef\u9760\u5730\u8bc6\u522b\u6750\u6599\u53c2\u6570\uff0c\u63d0\u9ad8\u5bf9\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7269\u7406\u4ea4\u4e92\u7684\u7406\u89e3\uff0c\u4e3a\u6539\u5584\u8896\u5e26\u8bbe\u8ba1\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.14172", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14172", "abs": "https://arxiv.org/abs/2511.14172", "authors": ["Naveen Lamba", "Sanju Tiwari", "Manas Gaur"], "title": "SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA", "comment": null, "summary": "LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u7b26\u53f7\u8bed\u8a00\u77e5\u8bc6\u7684\u5e7b\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u53d1\u73b0LLMs\u7684\u5e7b\u89c9\u672c\u8d28\u4e0a\u662f\u7b26\u53f7\u8bed\u8a00\u5904\u7406\u5931\u8d25\u800c\u975e\u4e00\u822c\u751f\u6210\u95ee\u9898\uff0c\u7b26\u53f7\u8bed\u4e49\u77e5\u8bc6\u662f\u7406\u89e3\u5e7b\u89c9\u673a\u5236\u7684\u5173\u952e\u3002", "motivation": "LLMs\u5728\u9047\u5230\u4fee\u9970\u8bed\u3001\u5426\u5b9a\u3001\u6570\u5b57\u3001\u4f8b\u5916\u548c\u547d\u540d\u5b9e\u4f53\u7b49\u7b26\u53f7\u89e6\u53d1\u5668\u65f6\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u7b26\u53f7\u5e7b\u89c9\u6765\u6e90\u7684\u6e05\u6670\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5904\u7406\u8fd9\u4e9b\u89e6\u53d1\u5668\u5e76\u5728\u6a21\u578b\u5185\u90e8\u5b9a\u4f4d\u5e7b\u89c9\u7684\u51fa\u73b0\u4f4d\u7f6e\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7b26\u53f7\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528\u7b26\u53f7\u8bed\u8a00\u548c\u8bed\u4e49\u77e5\u8bc6\u6765\u6709\u610f\u4e49\u5730\u8ffd\u8e2a\u6240\u6709\u6a21\u578b\u5c42\u4e2d\u5e7b\u89c9\u7684\u53d1\u5c55\u3002\u901a\u8fc7\u5173\u6ce8\u6a21\u578b\u5982\u4f55\u5904\u7406\u7b26\u53f7\u89e6\u53d1\u5668\uff0c\u4f7f\u7528HaluEval\u548cTruthfulQA\u5206\u6790\u4e86\u4e94\u4e2a\u6a21\u578b\u3002", "result": "\u7b26\u53f7\u77e5\u8bc6\u65b9\u6cd5\u663e\u793a\u8fd9\u4e9b\u8bed\u8a00\u5143\u7d20\u7684\u6ce8\u610f\u529b\u65b9\u5dee\u5728\u65e9\u671f\u5c42\uff082-4\uff09\u7206\u70b8\u5230\u4e34\u754c\u4e0d\u7a33\u5b9a\u6027\uff0c\u5426\u5b9a\u89e6\u53d1\u4e86\u707e\u96be\u6027\u7684\u65b9\u5dee\u6c34\u5e73\uff0c\u8868\u660e\u7b26\u53f7\u8bed\u4e49\u5904\u7406\u4ece\u4e00\u5f00\u59cb\u5c31\u5d29\u6e83\u3002\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u66f4\u5927\uff0c\u5e7b\u89c9\u7387\u4ecd\u4fdd\u6301\u9ad8\u4f4d\uff08Gemma\u53d8\u4f53\u4e3a78.3%-83.7%\uff09\uff0c\u5728\u66f4\u6df1\u5c42\u4e2d\u7b26\u53f7\u8bed\u4e49\u89e6\u53d1\u5668\u7684\u6ce8\u610f\u529b\u6025\u5267\u4e0b\u964d\u3002", "conclusion": "\u5e7b\u89c9\u672c\u8d28\u4e0a\u662f\u7b26\u53f7\u8bed\u8a00\u5904\u7406\u5931\u8d25\uff0c\u800c\u975e\u4e00\u822c\u751f\u6210\u95ee\u9898\uff0c\u7b26\u53f7\u8bed\u4e49\u77e5\u8bc6\u4e3a\u7406\u89e3\u548c\u5b9a\u4f4dLLMs\u4e2d\u7684\u5e7b\u89c9\u673a\u5236\u63d0\u4f9b\u4e86\u5173\u952e\u3002"}}
{"id": "2511.14330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14330", "abs": "https://arxiv.org/abs/2511.14330", "authors": ["Yizhen Yin", "Yuhua Qi", "Dapeng Feng", "Hongbo Chen", "Hongjun Ma", "Jin Wu", "Yi Jiang"], "title": "MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning", "comment": null, "summary": "Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faMA-SLAM\u7cfb\u7edf\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4e3b\u52a8SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5730\u56fe\u8868\u793a\u548c\u5168\u5c40\u89c4\u5212\u5668\u4f18\u5316\u63a2\u7d22\u8def\u5f84\uff0c\u663e\u8457\u51cf\u5c11\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u65f6\u95f4\u548c\u8ddd\u79bb\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8SLAM\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u53d7\u63a7\u73af\u5883\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u591a\u6837\u5316\u73af\u5883\u4e2d\u9762\u4e34\u63a2\u7d22\u65f6\u95f4\u957f\u3001\u8def\u5f84\u4f18\u5316\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u5730\u56fe\u8868\u793a\uff08\u79bb\u6563\u5316\u7a7a\u95f4\u6570\u636e\u3001\u6574\u5408\u8fb9\u754c\u70b9\u548c\u5386\u53f2\u8f68\u8ff9\uff09\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u51b3\u7b56\u6a21\u5757\u548c\u5168\u5c40\u89c4\u5212\u5668\u6765\u4f18\u5316\u957f\u8ddd\u79bb\u63a2\u7d22\u8def\u5f84\u3002", "result": "\u5728\u4e09\u4e2a\u4eff\u771f\u73af\u5883\u548c\u771f\u5b9e\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a2\u7d22\u65f6\u95f4\u548c\u8ddd\u79bb\u3002", "conclusion": "MA-SLAM\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u63a2\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5730\u56fe\u548c\u5168\u5c40\u89c4\u5212\u7b56\u7565\u63d0\u5347\u4e3b\u52a8SLAM\u6027\u80fd\u3002"}}
{"id": "2511.14181", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14181", "abs": "https://arxiv.org/abs/2511.14181", "authors": ["Jiajun Hou", "Chenyu Zhang", "Rui Meng"], "title": "Harnessing Deep LLM Participation for Robust Entity Linking", "comment": null, "summary": "Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.\n  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.\n  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.", "AI": {"tldr": "DeepEL\u662f\u4e00\u4e2a\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u6df1\u5ea6\u96c6\u6210\u5230\u5b9e\u4f53\u94fe\u63a5\u6240\u6709\u9636\u6bb5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9a8c\u8bc1\u673a\u5236\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u7ea0\u6b63\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u5728\u5b9e\u4f53\u94fe\u63a5\u7684\u5b64\u7acb\u9636\u6bb5\u4f7f\u7528LLMs\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u5728\u6574\u4e2a\u6d41\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u4e14\u5b64\u7acb\u6d88\u6b67\u65e0\u6cd5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "method": "\u63d0\u51faDeepEL\u6846\u67b6\uff0c\u5c06LLMs\u96c6\u6210\u5230\u5b9e\u4f53\u94fe\u63a5\u7684\u6bcf\u4e2a\u9636\u6bb5\uff0c\u5e76\u5f15\u5165\u81ea\u9a8c\u8bc1\u673a\u5236\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u7ea0\u6b63\u9884\u6d4b\u548c\u8bc6\u522b\u5b9e\u4f53\u95f4\u7684\u8fde\u8d2f\u5173\u7cfb\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDeepEL\u5728\u6574\u4f53F1\u5206\u6570\u4e0a\u5e73\u5747\u63d0\u53472.6%\uff0c\u5728\u57df\u5916\u6570\u636e\u96c6\u4e0a\u63d0\u53474%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6df1\u5ea6\u96c6\u6210LLMs\u80fd\u6709\u6548\u63a8\u8fdb\u5b9e\u4f53\u94fe\u63a5\u6280\u672f\u53d1\u5c55\uff0c\u81ea\u9a8c\u8bc1\u673a\u5236\u901a\u8fc7\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2511.14335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14335", "abs": "https://arxiv.org/abs/2511.14335", "authors": ["Jeryes Danial", "Yosi Ben Asher", "Itzik Klein"], "title": "Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors", "comment": null, "summary": "Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a00\u758f\u5173\u952e\u70b9\u59ff\u6001\u4f30\u8ba1\u4e0e\u7a20\u5bc6\u8fb9\u7f18\u91cd\u5efa\u7684\u8fb9\u7f18\u611f\u77e5\u8f7b\u91cf\u7ea7\u5355\u76eeSLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6df1\u5ea6\u9884\u6d4b\u548c\u8fb9\u7f18\u68c0\u6d4b\uff0c\u878d\u5408\u60ef\u6027\u6570\u636e\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u5efa\u56fe\u548c\u5bfc\u822a\u3002", "motivation": "\u5f53\u524d\u5355\u76eeSLAM\u7b97\u6cd5\u5b58\u5728\u7a00\u758f\u65b9\u6cd5\u7f3a\u4e4f\u8be6\u7ec6\u51e0\u4f55\u4fe1\u606f\u3001\u5b66\u4e60\u9a71\u52a8\u65b9\u6cd5\u8ba1\u7b97\u5bc6\u96c6\u3001\u4ee5\u53ca\u5c3a\u5ea6\u6a21\u7cca\u5f71\u54cd\u7cbe\u5ea6\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u80fd\u63d0\u4f9b\u4e30\u5bcc\u51e0\u4f55\u4fe1\u606f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u5173\u952e\u70b9\u59ff\u6001\u4f30\u8ba1\u4e0e\u7a20\u5bc6\u8fb9\u7f18\u91cd\u5efa\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6df1\u5ea6\u9884\u6d4b\u548c\u8fb9\u7f18\u68c0\u6d4b\uff0c\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u7cbe\u5316\u5173\u952e\u70b9\u548c\u8fb9\u7f18\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u878d\u5408\u60ef\u6027\u6570\u636e\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u3002", "result": "\u7cfb\u7edf\u5728\u4f4e\u529f\u8017\u5e73\u53f0\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u5728DJI Tello\u65e0\u4eba\u673a\u4e0a\u9a8c\u8bc1\uff0c\u5e76\u5728\u5ba4\u5185\u8d70\u5eca\u548cTUM RGBD\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u9c81\u68d2\u7684\u81ea\u4e3b\u5bfc\u822a\u548c\u907f\u969c\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u5efa\u56fe\u548c\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14230", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14230", "abs": "https://arxiv.org/abs/2511.14230", "authors": ["Ahlam Alrehili", "Areej Alhothali"], "title": "ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC", "comment": "26 pages", "summary": "Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.", "AI": {"tldr": "\u63d0\u51fa\u4e86ArbESC+\u7cfb\u7edf\uff0c\u8fd9\u662f\u9996\u4e2a\u963f\u62c9\u4f2f\u8bed\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u7684\u591a\u7cfb\u7edf\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u591a\u4e2a\u6a21\u578b\u548c\u7279\u5f81\u5206\u7c7b\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea0\u9519\u6027\u80fd\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u5177\u6709\u590d\u6742\u7684\u5f62\u6001\u548c\u53e5\u6cd5\u7ed3\u6784\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355\u4e00\u6a21\u578b\uff0c\u672a\u5145\u5206\u5229\u7528\u591a\u7cfb\u7edf\u96c6\u6210\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528AraT5\u3001ByT5\u3001mT5\u3001AraBART\u7b49\u591a\u4e2a\u6a21\u578b\u751f\u6210\u4fee\u6b63\u5efa\u8bae\uff0c\u901a\u8fc7\u6570\u503c\u7279\u5f81\u8868\u793a\u548c\u5206\u7c7b\u5668\u9009\u62e9\u6700\u4f73\u4fee\u6b63\uff0c\u5e76\u91c7\u7528\u652f\u6301\u6280\u672f\u8fc7\u6ee4\u91cd\u53e0\u4fee\u6b63\u548c\u8bc4\u4f30\u51b3\u7b56\u53ef\u9760\u6027\u3002", "result": "\u5728QALB-14\u6d4b\u8bd5\u6570\u636e\u4e0aF0.5\u8fbe\u523082.63%\uff0cQALB-15 L1\u6570\u636e\u8fbe\u523084.64%\uff0cQALB-15 L2\u6570\u636e\u8fbe\u523065.55%\uff0c\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u96c6\u6210\u8bed\u8a00\u9519\u8bef\u4fee\u6b63\u7684\u963f\u62c9\u4f2f\u8bed\u7cfb\u7edf\uff0c\u4e3a\u5f00\u53d1\u5148\u8fdb\u7684\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u5904\u7406\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9e\u7528\u6b65\u9aa4\u3002"}}
{"id": "2511.14341", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14341", "abs": "https://arxiv.org/abs/2511.14341", "authors": ["Michael Milford", "Tobias Fischer"], "title": "Going Places: Place Recognition in Artificial and Natural Systems", "comment": null, "summary": "Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u6574\u5408\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u3001\u52a8\u7269\u7814\u7a76\u548c\u4eba\u7c7b\u7814\u7a76\uff0c\u63a2\u8ba8\u4e0d\u540c\u7cfb\u7edf\u5982\u4f55\u7f16\u7801\u548c\u56de\u5fc6\u5730\u70b9\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u6982\u5ff5\u6846\u67b6\u6765\u8003\u8651\u548c\u5f00\u53d1\u5730\u70b9\u8bc6\u522b\u673a\u5236\u3002", "motivation": "\u5730\u70b9\u8bc6\u522b\u5bf9\u4e8e\u751f\u7269\u5bfc\u822a\u548c\u81ea\u4e3b\u7cfb\u7edf\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4ece\u4e0d\u540c\u7cfb\u7edf\u7684\u89d2\u5ea6\u7406\u89e3\u5730\u70b9\u7f16\u7801\u548c\u56de\u5fc6\u7684\u673a\u5236\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u5de5\u5b9a\u4f4d\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u673a\u5668\u4eba\u7cfb\u7edf\u3001\u52a8\u7269\u7814\u7a76\u548c\u4eba\u7c7b\u7814\u7a76\u7684\u53d1\u73b0\uff0c\u5206\u6790\u8ba1\u7b97\u548c\u8868\u5f81\u7b56\u7565\uff0c\u5305\u62ec\u62d3\u6251\u6620\u5c04\u3001\u7ebf\u7d22\u6574\u5408\u548c\u8bb0\u5fc6\u7ba1\u7406\u7b49\u6536\u655b\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8bc6\u522b\u4e86\u52a8\u7269\u7cfb\u7edf\u7684\u591a\u6a21\u6001\u5bfc\u822a\u548c\u73af\u5883\u9002\u5e94\u673a\u5236\uff0c\u4eba\u7c7b\u7814\u7a76\u7684\u8bed\u4e49\u5730\u70b9\u6982\u5ff5\u3001\u6587\u5316\u5f71\u54cd\u548c\u5185\u7701\u80fd\u529b\uff0c\u4ee5\u53ca\u4eba\u5de5\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u67b6\u6784\u548c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u786e\u5b9a\u4e86\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u73af\u5883\u53d8\u5f02\u6027\u7b49\u5173\u952e\u6311\u6218\uff0c\u65e8\u5728\u901a\u8fc7\u8fde\u63a5\u52a8\u7269\u5bfc\u822a\u7814\u7a76\u548c\u4eba\u7c7b\u7a7a\u95f4\u8ba4\u77e5\u7814\u7a76\u7684\u89c1\u89e3\u6765\u4fc3\u8fdb\u4eba\u5de5\u5730\u70b9\u8bc6\u522b\u7cfb\u7edf\u7684\u521b\u65b0\u3002"}}
{"id": "2511.14245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14245", "abs": "https://arxiv.org/abs/2511.14245", "authors": ["Kai Tian", "Yirong Mao", "Wendong Bi", "Hanjie Wang", "Que Wenhui"], "title": "MuCPT: Music-related Natural Language Model Continued Pretraining", "comment": null, "summary": "Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u97f3\u4e50\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u5927\u89c4\u6a21\u97f3\u4e50\u76f8\u5173\u8bed\u6599\u5e93\uff0840B tokens\uff09\u3001\u5b9e\u65bd\u9886\u57df\u4f18\u5148\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3001\u5f15\u5165\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u7684\u8f6f\u8bc4\u5206\u8d28\u91cf\u63a7\u5236\u7cfb\u7edf\uff0c\u4ee5\u53ca\u521b\u5efaMusicSimpleQA\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u97f3\u4e50\u7b49\u4e13\u4e1a\u9886\u57df\u53d7\u5230\u9650\u5236\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u8bed\u6599\u5e93\u89c4\u6a21\u3001\u7eaf\u51c0\u5ea6\u4ee5\u53ca\u6570\u636e\u4e0e\u8bad\u7ec3\u76ee\u6807\u7684\u5339\u914d\u5ea6\u4e0d\u8db3\u3002", "method": "1. \u6784\u5efa\u5927\u89c4\u6a21\u97f3\u4e50\u76f8\u5173\u81ea\u7136\u8bed\u8a00\u8bed\u6599\u5e93\uff1b2. \u5b9e\u65bd\u9886\u57df\u4f18\u5148\u6570\u636e\u7ba1\u9053\uff0c\u5305\u62ec\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u8fc7\u6ee4\u3001\u591a\u9636\u6bb5\u6e05\u7406\u548c\u9690\u79c1\u4fdd\u62a4\u63a9\u7801\uff1b3. \u5f15\u5165\u57fa\u4e8e\u53c2\u8003\u6a21\u578b\u7684token\u7ea7\u8f6f\u8bc4\u5206\u8fdb\u884c\u8d28\u91cf\u63a7\u5236\uff1b4. \u8bbe\u8ba1MusicSimpleQA\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e8b\u5b9e\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u8bad\u7ec3\u6846\u67b6\u548c\u53ef\u91cd\u7528\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8fdb\u884c\u97f3\u4e50\u9886\u57df\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u6b63\u786e\u7684\u8bed\u6599\u5e93\u548c\u8bad\u7ec3\u76ee\u6807\uff0c\u4e3a\u6784\u5efa\u97f3\u4e50\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u4e13\u7528LLM\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.14393", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14393", "abs": "https://arxiv.org/abs/2511.14393", "authors": ["Svetlana Seliunina", "Daniel Schleich", "Sven Behnke"], "title": "Perception-aware Exploration for Consumer-grade UAVs", "comment": null, "summary": "In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs.", "AI": {"tldr": "\u5c06\u591a\u65e0\u4eba\u673a\u81ea\u4e3b\u63a2\u7d22\u6280\u672f\u6269\u5c55\u5230\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\uff0c\u63d0\u51fa\u5305\u542b\u6df1\u5ea6\u4f30\u8ba1\u89c6\u70b9\u9009\u62e9\u3001\u8f68\u8ff9\u89c4\u5212\u548c\u534a\u5206\u5e03\u5f0f\u901a\u4fe1\u7684\u5b8c\u6574\u6d41\u7a0b", "motivation": "\u5c06\u5148\u8fdb\u7684\u591a\u65e0\u4eba\u673a\u81ea\u4e3b\u63a2\u7d22\u6280\u672f\u5e94\u7528\u4e8e\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\uff0c\u514b\u670d\u786c\u4ef6\u9650\u5236\uff0c\u5b9e\u73b0\u5b89\u5168\u7684\u73af\u5883\u63a2\u7d22\u548c\u5730\u56fe\u91cd\u5efa", "method": "\u63d0\u51fa\u5b8c\u6574\u6d41\u7a0b\uff1a\u9009\u62e9\u53ef\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u7684\u89c6\u70b9\u5bf9\uff0c\u89c4\u5212\u6ee1\u8db3\u8fd0\u52a8\u7ea6\u675f\u7684\u8f68\u8ff9\uff0c\u91c7\u7528\u534a\u5206\u5e03\u5f0f\u901a\u4fe1\u65b9\u6848\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d", "result": "\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u4e0d\u540c\u6570\u91cf\u65e0\u4eba\u673a\u4e0b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u5373\u4f7f\u5728\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\u786c\u4ef6\u9650\u5236\u4e0b\u4e5f\u80fd\u5b89\u5168\u63a2\u7d22\u73af\u5883\u5e76\u91cd\u5efa\u5730\u56fe", "conclusion": "\u6210\u529f\u5c06\u591a\u65e0\u4eba\u673a\u81ea\u4e3b\u63a2\u7d22\u6280\u672f\u6269\u5c55\u5230\u6d88\u8d39\u7ea7\u5e73\u53f0\uff0c\u4e3a\u4f4e\u6210\u672c\u65e0\u4eba\u673a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.14249", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14249", "abs": "https://arxiv.org/abs/2511.14249", "authors": ["Rui Liu", "Yuan Zhao", "Zhenqi Jia"], "title": "Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning", "comment": "Accepted by AAAI 2026", "summary": "The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.", "AI": {"tldr": "\u63d0\u51fa\u4e86Authentic-Dubber\u6a21\u578b\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u5bfc\u6f14-\u6f14\u5458\u4ea4\u4e92\u5b66\u4e60\u65b9\u6848\uff0c\u6a21\u62df\u771f\u5b9e\u7535\u5f71\u914d\u97f3\u5de5\u4f5c\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u60c5\u611f\u8868\u8fbe\u80fd\u529b", "motivation": "\u73b0\u6709\u7535\u5f71\u914d\u97f3\u65b9\u6cd5\u6a21\u62df\u7b80\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5ffd\u7565\u4e86\u5bfc\u6f14\u4e0e\u6f14\u5458\u4e4b\u95f4\u7684\u5173\u952e\u4e92\u52a8\u73af\u8282\uff0c\u800c\u771f\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u6d89\u53ca\u5bfc\u6f14\u6307\u5bfc\u6f14\u5458\u5185\u5316\u60c5\u611f\u7ebf\u7d22\u7684\u52a8\u6001\u534f\u4f5c", "method": "\u6784\u5efa\u591a\u6a21\u6001\u53c2\u8003\u7247\u6bb5\u5e93\uff0c\u4f7f\u7528\u60c5\u611f\u76f8\u4f3c\u6027\u68c0\u7d22\u589e\u5f3a\u7b56\u7565\uff0c\u5f00\u53d1\u6e10\u8fdb\u5f0f\u56fe\u57fa\u8bed\u97f3\u751f\u6210\u65b9\u6cd5\uff0c\u6574\u5408LLM\u8fdb\u884c\u591a\u6a21\u6001\u60c5\u611f\u8868\u793a\u6df1\u5ea6\u7406\u89e3", "result": "\u5728V2C Animation\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u7684\u5168\u9762\u63d0\u5347", "conclusion": "Authentic-Dubber\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u914d\u97f3\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5728\u60c5\u611f\u8868\u8fbe\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u81ea\u52a8\u7535\u5f71\u914d\u97f3\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.14396", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14396", "abs": "https://arxiv.org/abs/2511.14396", "authors": ["Xiuxiu Qi", "Yu Yang", "Jiannong Cao", "Luyao Bai", "Chongshan Fan", "Chengtai Cao", "Hongpeng Wang"], "title": "Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning", "comment": "Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/", "summary": "Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.", "AI": {"tldr": "\u63d0\u51fa\u4e86CCoL\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7684\u8fde\u7eed\u534f\u540c\u5b66\u4e60\u548c\u8bed\u4e49-\u7269\u7406\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u884c\u4e3a\u514b\u9686\u4e2d\u7684\u590d\u5408\u8bef\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u51c6\u786e\u7684\u52a8\u4f5c\u6267\u884c\u3002", "motivation": "\u89e3\u51b3\u884c\u4e3a\u514b\u9686\u4e2d\u590d\u5408\u8bef\u5dee\u5bfc\u81f4\u7684\u7269\u7406\u4e0d\u8fde\u7eed\u6027\u548c\u8bed\u4e49-\u7269\u7406\u9519\u4f4d\u95ee\u9898\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u52a8\u4f5c\u6267\u884c\u7684\u51c6\u786e\u6027\u548c\u8fde\u7eed\u6027\u3002", "method": "\u91c7\u7528\u89c6\u89c9-\u8bed\u8a00-\u672c\u4f53\u611f\u89c9\u8f93\u5165\u7684\u8fde\u7eed\u534f\u540c\u5b66\u4e60\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u8bed\u8a00\u8bed\u4e49\u951a\u5b9a\u5230\u89c6\u89c9\u8fd0\u52a8\u8868\u793a\u4e2d\uff0c\u5b66\u4e60\u52a8\u4f5c\u751f\u6210\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u4eff\u771f\u5957\u4ef6\u4e2d\u5e73\u5747\u76f8\u5bf9\u63d0\u53478.0%\uff0c\u5728\u53cc\u624b\u63d2\u5165\u4efb\u52a1\u4e2d\u76f8\u5bf9\u589e\u76ca\u8fbe19.2%\uff0c\u57287\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5728\u672a\u89c1\u548c\u566a\u58f0\u7269\u4f53\u72b6\u6001\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CCoL\u6846\u67b6\u901a\u8fc7\u8fde\u7eed\u534f\u540c\u5b66\u4e60\u548c\u8bed\u4e49-\u7269\u7406\u5bf9\u9f50\uff0c\u6709\u6548\u514b\u670d\u4e86\u884c\u4e3a\u514b\u9686\u4e2d\u7684\u590d\u5408\u8bef\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u5e73\u6ed1\u7684\u52a8\u4f5c\u6267\u884c\u8f68\u8ff9\u3002"}}
{"id": "2511.14255", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14255", "abs": "https://arxiv.org/abs/2511.14255", "authors": ["Gabrial Zencha Ashungafac", "Mardhiyah Sanni", "Busayo Awobade", "Alex Gichamba", "Tobi Olatunji"], "title": "AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR", "comment": "Accepted As a Conference Paper IJCNLP-AACL 2025", "summary": "Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.", "AI": {"tldr": "AfriSpeech-MultiBench\u662f\u9996\u4e2a\u9488\u5bf9\u975e\u6d32\u82f1\u8bed\u53e3\u97f3\u7684\u9886\u57df\u7279\u5b9a\u8bc4\u4f30\u5957\u4ef6\uff0c\u6db5\u76d6100\u591a\u79cd\u53e3\u97f3\u300110\u591a\u4e2a\u56fd\u5bb6\u548c7\u4e2a\u5e94\u7528\u9886\u57df\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u975e\u6d32\u8bed\u5883\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u97f3AI\u6280\u672f\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u591a\u6837\u6027\u7684\u516c\u5f00\u5e94\u7528\u7279\u5b9a\u6a21\u578b\u8bc4\u4f30\uff0c\u9700\u8981\u4e3a\u975e\u6d32\u793e\u533a\u5f00\u53d1\u5305\u5bb9\u6027\u8bed\u97f3\u5e94\u7528\u63d0\u4f9b\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b10+\u56fd\u5bb6\u3001100\u591a\u79cd\u975e\u6d32\u82f1\u8bed\u53e3\u97f3\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u6db5\u76d67\u4e2a\u5e94\u7528\u9886\u57df\uff0c\u4f7f\u7528\u81ea\u53d1\u548c\u975e\u81ea\u53d1\u8bed\u97f3\u5bf9\u8bdd\u6570\u636e\uff0c\u5bf9\u5f00\u6e90\u3001\u95ed\u6e90\u3001\u5355\u6a21\u6001ASR\u548c\u591a\u6a21\u6001LLM\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5f00\u6e90ASR\u5728\u81ea\u53d1\u8bed\u97f3\u4e2d\u8868\u73b0\u826f\u597d\u4f46\u5728\u5608\u6742\u975e\u6bcd\u8bed\u5bf9\u8bdd\u4e2d\u9000\u5316\uff1b\u591a\u6a21\u6001LLM\u5bf9\u53e3\u97f3\u66f4\u9c81\u68d2\u4f46\u5728\u9886\u57df\u7279\u5b9a\u547d\u540d\u5b9e\u4f53\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u4e13\u6709\u6a21\u578b\u5728\u6e05\u6670\u8bed\u97f3\u4e2d\u51c6\u786e\u7387\u9ad8\u4f46\u4e0d\u540c\u56fd\u5bb6\u548c\u9886\u57df\u5dee\u5f02\u663e\u8457\uff1b\u975e\u6d32\u82f1\u8bed\u5fae\u8c03\u7684\u6a21\u578b\u5177\u6709\u7ade\u4e89\u6027\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u8fd9\u4e00\u5168\u9762\u57fa\u51c6\uff0c\u4f7f\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u9009\u62e9\u9002\u5408\u975e\u6d32\u7528\u4f8b\u7684\u8bed\u97f3\u6280\u672f\uff0c\u4fc3\u8fdb\u4e3a\u670d\u52a1\u4e0d\u8db3\u793e\u533a\u5f00\u53d1\u5305\u5bb9\u6027\u8bed\u97f3\u5e94\u7528\u3002"}}
{"id": "2511.14427", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14427", "abs": "https://arxiv.org/abs/2511.14427", "authors": ["Rickmer Krohn", "Vignesh Prasad", "Gabriele Tiboni", "Georgia Chalvatzaki"], "title": "Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning", "comment": "9 pages, 10 figures, preprint", "summary": "Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.", "AI": {"tldr": "MSDP\u662f\u4e00\u4e2a\u591a\u611f\u5b98\u52a8\u6001\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u81ea\u7f16\u7801\u5b66\u4e60\u591a\u611f\u5b98\u8868\u5f81\uff0c\u7ed3\u5408\u975e\u5bf9\u79f0\u67b6\u6784\u5b9e\u73b0\u5feb\u901f\u3001\u9c81\u68d2\u7684\u591a\u611f\u5b98\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u611f\u5b98\u8bbe\u7f6e\u4e2d\u5b66\u4e60\u56f0\u96be\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u611f\u5b98\u566a\u58f0\u548c\u52a8\u6001\u53d8\u5316\u7684\u60c5\u51b5\u4e0b", "method": "\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u8bad\u7ec3transformer\u7f16\u7801\u5668\u91cd\u5efa\u591a\u611f\u5b98\u89c2\u6d4b\uff0c\u4e0b\u6e38\u7b56\u7565\u5b66\u4e60\u91c7\u7528\u975e\u5bf9\u79f0\u67b6\u6784\uff0c\u8bc4\u8bba\u5bb6\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u63d0\u53d6\u52a8\u6001\u7279\u5f81\uff0c\u6f14\u5458\u63a5\u6536\u7a33\u5b9a\u7684\u6c60\u5316\u8868\u5f81", "result": "\u5728\u591a\u79cd\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u52a0\u901f\u5b66\u4e60\u548c\u9c81\u68d2\u6027\u80fd\uff0c\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u548c\u7269\u4f53\u52a8\u6001\u53d8\u5316\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u4ec5\u97006000\u6b21\u5728\u7ebf\u4ea4\u4e92\u5373\u53ef\u8fbe\u5230\u9ad8\u6210\u529f\u7387", "conclusion": "MSDP\u4e3a\u590d\u6742\u591a\u611f\u5b98\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.14258", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14258", "abs": "https://arxiv.org/abs/2511.14258", "authors": ["Hourun Zhu", "Yang Gao", "Wenlong Fei", "Jiawei Li", "Huashan Sun"], "title": "Entropy-Guided Reasoning Compression", "comment": "10pages, 4 figures", "summary": "Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u71b5\u5f15\u5bfc\u8bad\u7ec3\u6846\u67b6\u6765\u89e3\u51b3\u63a8\u7406\u6a21\u578b\u538b\u7f29\u4e2d\u7684\u71b5\u51b2\u7a81\u95ee\u9898\uff0c\u5c06\u63a8\u7406\u957f\u5ea6\u538b\u7f29\u81f3\u539f\u59cb\u768420%\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u7387", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u8f93\u51fa\u8fc7\u957f\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u71b5\u51b2\u7a81\u73b0\u8c61", "method": "\u91c7\u7528\u71b5\u5f15\u5bfc\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u71b5\u4e0b\u964d\u65f6\u9f13\u52b1\u7b80\u6d01\u601d\u7ef4\u6b65\u9aa4\uff0c\u5728\u71b5\u4e0a\u5347\u65f6\u5728\u7d27\u51d1\u63a8\u7406\u6a21\u5f0f\u4e0b\u52a0\u5f3a\u63a2\u7d22\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c06\u63a8\u7406\u957f\u5ea6\u538b\u7f29\u81f3\u539f\u59cb\u768420%\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u8d85\u8fc7\u57fa\u7ebf\u51c6\u786e\u7387", "conclusion": "\u71b5\u5f15\u5bfc\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406\u538b\u7f29\u4e2d\u7684\u71b5\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u63a8\u7406\u538b\u7f29"}}
{"id": "2511.14432", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14432", "abs": "https://arxiv.org/abs/2511.14432", "authors": ["Marcela Gon\u00e7alves dos Santos", "Sylvain Hall\u00e9", "F\u00e1bio Petrillo"], "title": "Mutation Testing for Industrial Robotic Systems", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9886\u57df\u7279\u5b9a\u53d8\u5f02\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u6355\u83b7\u673a\u5668\u4eba\u52a8\u4f5c\u548c\u4f20\u611f\u5668\u8bed\u4e49\u7684\u53d8\u5f02\u7b97\u5b50\uff0c\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u5957\u4ef6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u6545\u969c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u4e8b\u6545\u548c\u9ad8\u6602\u505c\u673a\u6210\u672c\uff0c\u4f20\u7edf\u53d8\u5f02\u6d4b\u8bd5\u7b97\u5b50\u4e0d\u9002\u7528\u4e8e\u6d89\u53ca\u6d88\u606f\u547d\u4ee4\u548c\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u673a\u5668\u4eba\u7a0b\u5e8f\u3002", "method": "\u63d0\u51fa\u5728\u9ad8\u5c42\u8bfb\u5199\u64cd\u4f5c\u7ea7\u522b\u751f\u6210\u6709\u610f\u4e49\u53d8\u5f02\u4f53\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u8fd0\u52a8\u3001\u5939\u722a\u52a8\u4f5c\u548c\u4f20\u611f\u5668\u566a\u58f0\u6ce8\u5165\u7b49\u673a\u5668\u4eba\u7279\u5b9a\u64cd\u4f5c\u3002", "result": "\u5728\u62fe\u53d6\u653e\u7f6e\u573a\u666f\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u66f4\u591a\u4fe1\u606f\u4e30\u5bcc\u7684\u53d8\u5f02\u4f53\uff0c\u76f8\u6bd4\u4f20\u7edf\u7b97\u5b50\u51cf\u5c11\u4e86\u65e0\u6548\u6216\u7b49\u4ef7\u60c5\u51b5\u7684\u6570\u91cf\u3002", "conclusion": "\u53d8\u5f02\u6d4b\u8bd5\u6709\u6f5c\u529b\u63d0\u9ad8\u6d4b\u8bd5\u5957\u4ef6\u8d28\u91cf\uff0c\u4e3a\u66f4\u5b89\u5168\u53ef\u9760\u7684\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2511.14275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14275", "abs": "https://arxiv.org/abs/2511.14275", "authors": ["Ante Wang", "Weizhi Ma", "Yang Liu"], "title": "Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space", "comment": null, "summary": "Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u9884\u6d4b\u8bed\u8a00\u5316\u6982\u7387\u5206\u5e03\u6765\u589e\u5f3aLLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9f13\u52b1\u6a21\u578b\u6df1\u5165\u601d\u8003\u6240\u6709\u5019\u9009\u7b54\u6848\u800c\u975e\u5355\u4e00\u731c\u6d4b\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "motivation": "\u4e86\u89e3\u6a21\u578b\u54cd\u5e94\u7684\u53ef\u9760\u6027\u5728\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5173\u6ce8\u751f\u6210\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\u5e76\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u63d0\u4f9b\u903b\u8f91\u900f\u660e\u7684\u4f30\u8ba1\uff0c\u4f46\u63a8\u7406\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u9884\u6d4b\u8bed\u8a00\u5316\u6982\u7387\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u8981\u6c42LLM\u8003\u8651\u7b54\u6848\u7a7a\u95f4\u4e2d\u7684\u6240\u6709\u5019\u9009\u7b54\u6848\u800c\u975e\u5355\u4e00\u731c\u6d4b\uff0c\u5e76\u4ed4\u7ec6\u5206\u914d\u7f6e\u4fe1\u5ea6\u5206\u6570\u4ee5\u6ee1\u8db3\u5206\u5e03\u8981\u6c42\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u548c\u5404\u79cd\u4efb\u52a1\u4e2d\u5747\u663e\u793a\u51fa\u4f18\u52bf\uff0c\u65e0\u8bba\u7b54\u6848\u7a7a\u95f4\u662f\u5426\u5df2\u77e5\u3002\u5373\u4f7f\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u5176\u4f18\u52bf\u4ecd\u5f97\u4ee5\u4fdd\u6301\uff0c\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\u5176\u63a8\u7406\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u671f\u671b\u4e00\u81f4\u3002", "conclusion": "\u9884\u6d4b\u8bed\u8a00\u5316\u6982\u7387\u5206\u5e03\u80fd\u6709\u6548\u9f13\u52b1\u6df1\u5ea6\u63a8\u7406\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6539\u8fdbLLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.14434", "categories": ["cs.RO", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.14434", "abs": "https://arxiv.org/abs/2511.14434", "authors": ["Marlow Fawn", "Matthias Scheutz"], "title": "Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u8c10\u6ce2\u63a7\u5236\u674e\u96c5\u666e\u8bfa\u592b-\u5c4f\u969c\u51fd\u6570\u4e0e\u673a\u5668\u4eba\u7b56\u7565\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5c06\u4e0d\u5b89\u5168\u7b56\u7565\u8f6c\u5316\u4e3a\u5177\u6709\u5f62\u5f0f\u5316\u4fdd\u8bc1\u7684\u5b89\u5168\u7b56\u7565", "motivation": "\u5c06\u57fa\u4e8e\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u7684\u5b89\u5168\u7ea6\u675f\u4e0e\u4efb\u610f\u673a\u5668\u4eba\u7b56\u7565\u7ed3\u5408\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u9a71\u52a8\u884c\u4e3a", "method": "\u901a\u8fc7HCLBF\u5bfc\u51fa\u7684\u5b89\u5168\u8bc1\u4e66\u5c06\u4e24\u4e2a\u7ec4\u4ef6\u7ed3\u5408\uff0c\u4ea7\u751f\u65e2\u4fdd\u6301\u5b89\u5168\u53c8\u4fdd\u7559\u4efb\u52a1\u9a71\u52a8\u884c\u4e3a\u7684\u6307\u4ee4", "result": "\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u9759\u6b62\u673a\u68b0\u81c2\u5728\u684c\u9762\u79fb\u52a8\u4efb\u52a1\u4e2d\u80fd\u591f\u907f\u514d\u4e0e\u969c\u788d\u7269\u78b0\u649e", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u66f4\u590d\u6742\u7684\u89c4\u8303\u8981\u6c42\u548c\u52a8\u6001\u4efb\u52a1\u573a\u666f"}}
{"id": "2511.14295", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14295", "abs": "https://arxiv.org/abs/2511.14295", "authors": ["Mohammad Zbib", "Hasan Abed Al Kader Hammoud", "Sina Mukalled", "Nadine Rizk", "Fatima Karnib", "Issam Lakkis", "Ammar Mohanna", "Bernard Ghanem"], "title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models", "comment": null, "summary": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.", "AI": {"tldr": "AraLingBench\u662f\u4e00\u4e2a\u5168\u9762\u4eba\u5de5\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u8a00\u5b66\u80fd\u529b\uff0c\u5305\u542b\u8bed\u6cd5\u3001\u5f62\u6001\u5b66\u3001\u62fc\u5199\u3001\u9605\u8bfb\u7406\u89e3\u548c\u53e5\u6cd5\u4e94\u4e2a\u6838\u5fc3\u7c7b\u522b\uff0c\u5171150\u4e2a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u591a\u9879\u9009\u62e9\u9898\u3002", "motivation": "\u5f53\u524d\u963f\u62c9\u4f2f\u8bed\u548c\u53cc\u8bedLLMs\u5728\u8868\u9762\u6c34\u5e73\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6df1\u5c42\u8bed\u6cd5\u548c\u53e5\u6cd5\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u533a\u5206\u771f\u5b9e\u8bed\u8a00\u638c\u63e1\u4e0e\u6a21\u5f0f\u8bc6\u522b\u3002", "method": "\u901a\u8fc7150\u4e2a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u591a\u9879\u9009\u62e9\u9898\uff0c\u8bc4\u4f3035\u4e2a\u963f\u62c9\u4f2f\u8bed\u548c\u53cc\u8bedLLMs\u5728\u4e94\u4e2a\u8bed\u8a00\u5b66\u6838\u5fc3\u7c7b\u522b\uff08\u8bed\u6cd5\u3001\u5f62\u6001\u5b66\u3001\u62fc\u5199\u3001\u9605\u8bfb\u7406\u89e3\u3001\u53e5\u6cd5\uff09\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u8868\u9762\u6c34\u5e73\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6df1\u5c42\u8bed\u6cd5\u548c\u53e5\u6cd5\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8bb8\u591a\u6a21\u578b\u901a\u8fc7\u8bb0\u5fc6\u6216\u6a21\u5f0f\u8bc6\u522b\u800c\u975e\u771f\u5b9e\u7406\u89e3\u83b7\u5f97\u9ad8\u5206\u3002", "conclusion": "AraLingBench\u63ed\u793a\u4e86\u57fa\u4e8e\u77e5\u8bc6\u7684\u57fa\u51c6\u6d4b\u8bd5\u9ad8\u5206\u4e0e\u771f\u5b9e\u8bed\u8a00\u638c\u63e1\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\uff0c\u4e3a\u5f00\u53d1\u963f\u62c9\u4f2f\u8bedLLMs\u63d0\u4f9b\u4e86\u8bca\u65ad\u6846\u67b6\uff0c\u8bc4\u4f30\u4ee3\u7801\u5df2\u5728GitHub\u516c\u5f00\u3002"}}
{"id": "2511.14458", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14458", "abs": "https://arxiv.org/abs/2511.14458", "authors": ["Michelle Mattille", "Alexandre Mesot", "Miriam Weisskopf", "Nicole Ochsenbein-K\u00f6lble", "Ueli Moehrlen", "Bradley J. Nelson", "Quentin Boehler"], "title": "Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy", "comment": null, "summary": "Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f00\u653e\u8154\u4f53\u5185\u7aa5\u955c\u624b\u672f\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u7ed3\u5408\u78c1\u9a71\u52a8\u67d4\u6027\u5185\u7aa5\u955c\u548c\u534a\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u901a\u8fc7\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u589e\u5f3a\u624b\u672f\u611f\u77e5\uff0c\u5e76\u5728\u7f8a\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u67d4\u6027\u673a\u5668\u4eba\u5728\u5fae\u521b\u624b\u672f\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5f00\u653e\u8154\u4f53\u4e2d\u7684\u5185\u7aa5\u955c\u5e72\u9884\u9762\u4e34\u63a7\u5236\u56f0\u96be\u548c\u89c6\u91ce\u53d7\u9650\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u5ea6\u4e13\u4e1a\u6280\u80fd\u7684\u80ce\u513f\u955c\u6fc0\u5149\u51dd\u56fa\u624b\u672f\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u78c1\u9a71\u52a8\u67d4\u6027\u5185\u7aa5\u955c\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5177\u6709\u9065\u64cd\u4f5c\u548c\u534a\u81ea\u4e3b\u5bfc\u822a\u529f\u80fd\uff0c\u80fd\u591f\u91cd\u5efa\u5185\u7aa5\u955c\u573a\u666f\u7684\u5b9e\u65f6\u62fc\u63a5\u56fe\u50cf\u4ee5\u63d0\u4f9b\u6269\u5c55\u7684\u89c6\u89c9\u73af\u5883\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u7f8a\u6a21\u578b\u4e2d\u8fdb\u884c\u4e86\u4f53\u5185\u9a8c\u8bc1\uff0c\u8bc1\u660e\u80fd\u591f\u514b\u670d\u5f00\u653e\u7a7a\u95f4\u5fae\u521b\u624b\u672f\u7684\u5173\u952e\u9650\u5236\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u5e73\u53f0\u6210\u529f\u89e3\u51b3\u4e86\u5f00\u653e\u8154\u4f53\u5185\u7aa5\u955c\u624b\u672f\u7684\u63a7\u5236\u548c\u89c6\u91ce\u9650\u5236\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u5fae\u521b\u624b\u672f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.14342", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14342", "abs": "https://arxiv.org/abs/2511.14342", "authors": ["Xingwei He", "Qianru Zhang", "Pengfei Chen", "Guanhua Chen", "Linlin Yu", "Yuan Yuan", "Siu-Ming Yiu"], "title": "ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions", "comment": "Accepted to AAAI 2026", "summary": "Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.", "AI": {"tldr": "ConInstruct\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLMs\u5728\u68c0\u6d4b\u548c\u89e3\u51b3\u7528\u6237\u6307\u4ee4\u4e2d\u51b2\u7a81\u7ea6\u675f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e13\u6709LLMs\u51b2\u7a81\u68c0\u6d4b\u80fd\u529b\u5f3a\u4f46\u5f88\u5c11\u660e\u786e\u901a\u77e5\u7528\u6237\u51b2\u7a81\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLMs\u9075\u5faa\u7528\u6237\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u4f46\u5ffd\u7565\u4e86\u6307\u4ee4\u5305\u542b\u51b2\u7a81\u7ea6\u675f\u7684\u5e38\u89c1\u573a\u666f\uff0cLLMs\u5728\u6b64\u7c7b\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165ConInstruct\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30LLMs\u68c0\u6d4b\u548c\u89e3\u51b3\u7528\u6237\u6307\u4ee4\u4e2d\u51b2\u7a81\u7684\u80fd\u529b\uff0c\u5e76\u5206\u6790\u5176\u51b2\u7a81\u89e3\u51b3\u884c\u4e3a\u3002", "result": "\u4e13\u6709LLMs\u51b2\u7a81\u68c0\u6d4b\u80fd\u529b\u5f3a\uff0cDeepSeek-R1\u548cClaude-4.5-Sonnet\u5206\u522b\u4ee591.5%\u548c87.3%\u7684F1\u5206\u6570\u6392\u540d\u524d\u4e8c\uff1b\u4f46LLMs\u5f88\u5c11\u660e\u786e\u901a\u77e5\u7528\u6237\u51b2\u7a81\u6216\u8bf7\u6c42\u6f84\u6e05\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u51b2\u7a81\u5904\u7406\u65b9\u9762\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff0c\u8fd9\u662f\u8bbe\u8ba1\u6307\u4ee4\u9075\u5faaLLMs\u65f6\u9700\u8981\u6539\u8fdb\u7684\u91cd\u8981\u9886\u57df\u3002"}}
{"id": "2511.14504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14504", "abs": "https://arxiv.org/abs/2511.14504", "authors": ["Jan Quenzel", "Valerij Sekin", "Daniel Schleich", "Alexander Miller", "Merlin Stampa", "Norbert Pahlke", "Christof R\u00f6hrig", "Sven Behnke"], "title": "Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations", "comment": "7 pages, Presented at IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR), Galway, Ireland, 2025", "summary": "Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.\n  We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u65e0\u4eba\u673a\u548c\u673a\u52a8\u6d88\u9632\u76d1\u63a7\u5668\u7684\u81ea\u52a8\u5316\u6d88\u9632\u8f85\u52a9\u7cfb\u7edf\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u81ea\u4e3b\u63a2\u6d4b\u70ed\u6e90\u5e76\u5b9a\u4f4d\uff0c\u7cfb\u7edf\u81ea\u52a8\u5f15\u5bfc\u6c34\u67aa\u7784\u51c6\u706b\u6e90\u3002", "motivation": "\u5de5\u4e1a\u8bbe\u65bd\u706b\u707e\u7531\u4e8e\u5efa\u7b51\u89c4\u6a21\u5927\u3001\u89c6\u89c9\u906e\u6321\u4e25\u91cd\uff0c\u5bfc\u81f4\u6d88\u9632\u5458\u96be\u4ee5\u51c6\u786e\u5b9a\u4f4d\u706b\u6e90\uff0c\u5f71\u54cd\u706d\u706b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5b89\u88c5\u5728\u8f6c\u53f0\u68af\u4e0a\u7684\u673a\u52a8\u6d88\u9632\u76d1\u63a7\u5668\uff0c\u914d\u5408\u65e0\u4eba\u673a\u7a7a\u4e2d\u652f\u6301\u3002\u65e0\u4eba\u673a\u5728\u65e0\u969c\u788d\u98de\u884c\u901a\u9053\u5185\u81ea\u4e3b\u98de\u884c\uff0c\u63a2\u6d4b\u548c\u5b9a\u4f4d\u70ed\u6e90\uff0c\u64cd\u4f5c\u5458\u901a\u8fc7\u624b\u6301\u63a7\u5236\u5668\u9009\u62e9\u76ee\u6807\u540e\uff0c\u7cfb\u7edf\u81ea\u52a8\u89c4\u5212\u4e09\u89d2\u6d4b\u91cf\u8def\u5f84\u5e76\u5f15\u5bfc\u6c34\u67aa\u7784\u51c6\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u6210\u529f\u5b9a\u4f4d\u4e86\u591a\u4e2a\u70ed\u6e90\uff0c\u5e76\u80fd\u5c06\u6c34\u67aa\u51c6\u786e\u5bfc\u5411\u706b\u6e90\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u8f85\u52a9\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5de5\u4e1a\u706b\u707e\u7684\u706d\u706b\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u51cf\u5c11\u635f\u5931\u548c\u6d88\u9632\u4f5c\u4e1a\u65f6\u95f4\u3002"}}
{"id": "2511.14365", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14365", "abs": "https://arxiv.org/abs/2511.14365", "authors": ["Prathamesh Kalamkar", "Ned Letcher", "Meissane Chami", "Sahger Lad", "Shayan Mohanty", "Prasanna Pendse"], "title": "The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models", "comment": null, "summary": "The application of large language models (LLMs) to chemistry is frequently hampered by a \"tokenization bottleneck\", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.14565", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14565", "abs": "https://arxiv.org/abs/2511.14565", "authors": ["Minyoung Hwang", "Alexandra Forsey-Smerek", "Nathaniel Dennler", "Andreea Bobu"], "title": "Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language", "comment": null, "summary": "Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL", "AI": {"tldr": "Masked IRL\u7ed3\u5408\u8bed\u8a00\u6307\u4ee4\u548c\u6f14\u793a\u6570\u636e\uff0c\u901a\u8fc7LLM\u63a8\u65ad\u72b6\u6001\u76f8\u5173\u6027\u63a9\u7801\uff0c\u5728\u6570\u636e\u6709\u9650\u65f6\u63d0\u9ad8\u5956\u52b1\u51fd\u6570\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u534715%\u4e14\u6570\u636e\u9700\u6c42\u51cf\u5c114.7\u500d\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6f14\u793a\u7684\u5956\u52b1\u5b66\u4e60\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u865a\u5047\u76f8\u5173\u6027\uff0c\u56e0\u4e3a\u6f14\u793a\u53ea\u5c55\u793a\u5982\u4f55\u6267\u884c\u4efb\u52a1\u800c\u4e0d\u6307\u660e\u4efb\u52a1\u91cd\u70b9\u3002\u8bed\u8a00\u6307\u4ee4\u80fd\u66f4\u76f4\u63a5\u6307\u5b9a\u673a\u5668\u4eba\u5e94\u5173\u6ce8\u7684\u5185\u5bb9\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u8bed\u8a00\u6d88\u9664\u6b67\u4e49\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faMasked IRL\u6846\u67b6\uff0c\u4f7f\u7528LLM\u4ece\u8bed\u8a00\u6307\u4ee4\u63a8\u65ad\u72b6\u6001\u76f8\u5173\u6027\u63a9\u7801\uff0c\u5f3a\u5236\u5956\u52b1\u51fd\u6570\u5bf9\u65e0\u5173\u72b6\u6001\u7ec4\u4ef6\u4fdd\u6301\u4e0d\u53d8\u6027\u3002\u5f53\u6307\u4ee4\u6a21\u7cca\u65f6\uff0c\u5229\u7528LLM\u5728\u6f14\u793a\u4e0a\u4e0b\u6587\u4e2d\u7684\u63a8\u7406\u6765\u6f84\u6e05\u6307\u4ee4\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cMasked IRL\u6bd4\u73b0\u6709\u8bed\u8a00\u6761\u4ef6IRL\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u8fbe15%\uff0c\u540c\u65f6\u6570\u636e\u4f7f\u7528\u91cf\u51cf\u5c11\u8fbe4.7\u500d\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u6a21\u7cca\u8bed\u8a00\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bed\u8a00\u6307\u4ee4\u548c\u6f14\u793a\u6570\u636e\u5305\u542b\u4e92\u8865\u4fe1\u606f\uff0cMasked IRL\u901a\u8fc7LLM\u6709\u6548\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u5956\u52b1\u5b66\u4e60\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u673a\u5668\u4eba\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2511.14366", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14366", "abs": "https://arxiv.org/abs/2511.14366", "authors": ["Hongwei Liu", "Junnan Liu", "Shudong Liu", "Haodong Duan", "Yuqiang Li", "Mao Su", "Xiaohong Liu", "Guangtao Zhai", "Xinyu Fang", "Qianhong Ma", "Taolin Zhang", "Zihan Ma", "Yufeng Zhao", "Peiheng Zhou", "Linchen Xiao", "Wenlong Zhang", "Shijie Zhou", "Xingjian Ma", "Siqi Sun", "Jiaye Ge", "Meng Li", "Yuhong Liu", "Jianxin Dong", "Jiaying Li", "Hui Wu", "Hanwen Liang", "Jintai Lin", "Yanting Wang", "Jie Dong", "Tong Zhu", "Tianfan Fu", "Conghui He", "Qi Zhang", "Songyang Zhang", "Lei Bai", "Kai Chen"], "title": "ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning", "comment": "39 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable \"ruler\" for progress toward Artificial General Intelligence.", "AI": {"tldr": "ATLAS\u662f\u4e00\u4e2a\u9762\u5411AGI\u7684\u5927\u89c4\u6a21\u3001\u9ad8\u96be\u5ea6\u8de8\u5b66\u79d1\u79d1\u5b66\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u542b\u7ea6800\u4e2a\u539f\u521b\u95ee\u9898\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u533a\u5206\u524d\u6cbf\u6a21\u578b\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6027\u80fd\u9971\u548c\u3001\u5b66\u79d1\u8303\u56f4\u72ed\u7a84\u3001\u7b54\u6848\u683c\u5f0f\u7b80\u5316\u3001\u6613\u53d7\u6570\u636e\u6c61\u67d3\u7b49\u95ee\u9898\uff0c\u4e0e\u771f\u5b9e\u79d1\u5b66\u63a2\u7a76\u5b58\u5728\u4fdd\u771f\u5ea6\u5dee\u8ddd\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u51c6\u786e\u8861\u91cf\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b7\u4e2a\u6838\u5fc3\u79d1\u5b66\u9886\u57df\u7684\u539f\u521b\u95ee\u9898\u96c6\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u4e13\u5bb6\u8bc4\u5ba1\u548c\u5bf9\u6297\u6d4b\u8bd5\u8fdb\u884c\u8d28\u91cf\u63a7\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7f\u7528LLM\u8bc4\u59d4\u5c0f\u7ec4\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30\u7684\u7a33\u5065\u8bc4\u4f30\u8303\u5f0f\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793aATLAS\u80fd\u6709\u6548\u533a\u5206\u9886\u5148\u6a21\u578b\u7684\u5148\u8fdb\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "ATLAS\u5c06\u53d1\u5c55\u6210\u4e3a\u4e00\u4e2a\u957f\u671f\u3001\u5f00\u653e\u3001\u793e\u533a\u9a71\u52a8\u7684\u5e73\u53f0\uff0c\u4e3a\u8fc8\u5411\u4eba\u5de5\u901a\u7528\u667a\u80fd\u7684\u8fdb\u5c55\u63d0\u4f9b\u53ef\u9760\u7684\u8861\u91cf\u6807\u51c6\u3002"}}
{"id": "2511.14592", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14592", "abs": "https://arxiv.org/abs/2511.14592", "authors": ["Xianhui Meng", "Yuchen Zhang", "Zhijian Huang", "Zheng Lu", "Ziling Ji", "Yaoyao Yin", "Hongyuan Zhang", "Guangfeng Jiang", "Yandan Lin", "Long Chen", "Hangjun Ye", "Li Zhang", "Jun Liu", "Xiaoshuai Hao"], "title": "Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks", "comment": null, "summary": "Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.", "AI": {"tldr": "DSBench\u662f\u9996\u4e2a\u7efc\u5408\u6027\u7684\u9a7e\u9a76\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7edf\u4e00\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5916\u90e8\u73af\u5883\u98ce\u9669\u548c\u8f66\u5185\u9a7e\u9a76\u884c\u4e3a\u5b89\u5168\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5305\u542b10\u5927\u7c7b28\u4e2a\u5b50\u7c7b\u573a\u666f\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u540c\u65f6\u8bc4\u4f30\u5916\u90e8\u73af\u5883\u98ce\u9669\u548c\u8f66\u5185\u9a7e\u9a76\u884c\u4e3a\u5b89\u5168\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efaDSBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u5916\u90e8\u73af\u5883\u98ce\u9669\u548c\u8f66\u5185\u9a7e\u9a76\u884c\u4e3a\u5b89\u5168\u4e24\u5927\u7c7b\u522b\uff0c\u7ec6\u5206\u4e3a10\u4e2a\u5173\u952e\u7c7b\u522b\u548c28\u4e2a\u5b50\u7c7b\u522b\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u5e76\u6784\u5efa98K\u4e2a\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\u7528\u4e8e\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5bf9\u5404\u79cd\u4e3b\u6d41\u5f00\u6e90\u548c\u95ed\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u590d\u6742\u5b89\u5168\u5173\u952e\u60c5\u51b5\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7a81\u663e\u4e86\u7d27\u8feb\u7684\u5b89\u5168\u95ee\u9898\u3002\u901a\u8fc7\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DSBench\u586b\u8865\u4e86\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u63a8\u8fdb\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\u53ef\u6709\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2511.14385", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14385", "abs": "https://arxiv.org/abs/2511.14385", "authors": ["Mario Sanz-Guerrero", "Katharina von der Wense"], "title": "Mitigating Label Length Bias in Large Language Models", "comment": "Accepted to AACL 2025 (Main)", "summary": "Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.", "AI": {"tldr": "\u63d0\u51fa\u6807\u51c6\u5316\u4e0a\u4e0b\u6587\u6821\u51c6(NCC)\u65b9\u6cd5\uff0c\u89e3\u51b3LLMs\u4e2d\u591a\u6807\u8bb0\u7c7b\u6807\u7b7e\u7684\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6700\u9ad8\u8fbe10% F1\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u5ffd\u89c6\u4e86\u591a\u6807\u8bb0\u7c7b\u6807\u7b7e\u5e26\u6765\u7684\u504f\u5dee\uff0c\u7279\u522b\u662f\u6807\u7b7e\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u5373\u4e0d\u540c\u957f\u5ea6\u7684\u6807\u7b7e\u5373\u4f7f\u7ecf\u8fc7\u6807\u51c6\u5316\u5904\u7406\u540e\u4ecd\u88ab\u4e0d\u4e00\u81f4\u5bf9\u5f85\u3002", "method": "\u63d0\u51fa\u6807\u51c6\u5316\u4e0a\u4e0b\u6587\u6821\u51c6(NCC)\uff0c\u5728\u5168\u6807\u7b7e\u7ea7\u522b\u5bf9\u9884\u6d4b\u8fdb\u884c\u6807\u51c6\u5316\u548c\u6821\u51c6\uff0c\u53ef\u6269\u5c55\u5230\u591a\u9879\u9009\u62e9\u9898\u7b49\u66f4\u5e7f\u6cdb\u4efb\u52a1\u3002", "result": "NCC\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u53d6\u5f97\u7edf\u8ba1\u663e\u8457\u6539\u8fdb\uff0c\u6700\u9ad8\u63d0\u534710% F1\u5206\u6570\uff1b\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u65f6\uff0c\u5bf9\u5c11\u6837\u672c\u793a\u4f8b\u9009\u62e9\u4e0d\u654f\u611f\uff0c\u9700\u8981\u66f4\u5c11\u793a\u4f8b\u8fbe\u5230\u7ade\u4e89\u6027\u80fd\uff0c\u4ea7\u751f\u66f4\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "conclusion": "\u7f13\u89e3\u5168\u6807\u7b7e\u504f\u5dee\u5bf9\u4e8e\u63d0\u9ad8\u57fa\u4e8eLLM\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u7c7b\u6807\u7b7e\u81ea\u7136\u5305\u542b\u591a\u4e2a\u6807\u8bb0\u7684\u73b0\u5b9e\u5e94\u7528\u4e2d\u3002"}}
{"id": "2511.14625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14625", "abs": "https://arxiv.org/abs/2511.14625", "authors": ["Qingwei Ben", "Botian Xu", "Kailin Li", "Feiyu Jia", "Wentao Zhang", "Jingping Wang", "Jingbo Wang", "Dahua Lin", "Jiangmiao Pang"], "title": "Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains", "comment": null, "summary": "Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.", "AI": {"tldr": "Gallant\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f53\u7d20\u7f51\u683c\u7684\u4eba\u5f62\u673a\u5668\u4eba3D\u5730\u5f62\u5bfc\u822a\u6846\u67b6\uff0c\u5229\u7528\u4f53\u7d20\u5316LiDAR\u6570\u636e\u4f5c\u4e3a\u611f\u77e5\u8868\u793a\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u5b9e\u73b0\u7a33\u5065\u76843D\u73af\u5883\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u611f\u77e5\u6a21\u5757\uff08\u6df1\u5ea6\u56fe\u50cf\u6216\u9ad8\u7a0b\u56fe\uff09\u53ea\u80fd\u63d0\u4f9b\u5c40\u90e8\u5e73\u5766\u7684\u73af\u5883\u89c6\u56fe\uff0c\u65e0\u6cd5\u6355\u6349\u5b8c\u6574\u76843D\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u67423D\u5730\u5f62\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4f53\u7d20\u5316LiDAR\u6570\u636e\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u7ed3\u6784\u5316\u611f\u77e5\u8868\u793a\uff0c\u91c7\u7528z\u5206\u7ec42D CNN\u5c06\u611f\u77e5\u6620\u5c04\u5230\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u5b8c\u5168\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u4fdd\u771fLiDAR\u4eff\u771f\u652f\u6301\u53ef\u6269\u5c55\u8bad\u7ec3\u3002", "result": "Gallant\u7684\u5e7f\u6cdb\u611f\u77e5\u8986\u76d6\u4f7f\u5355\u4e00\u7b56\u7565\u80fd\u591f\u5904\u7406\u5730\u9762\u969c\u788d\u7269\u3001\u4fa7\u5411\u6742\u7269\u3001\u9876\u90e8\u7ea6\u675f\u3001\u591a\u5c42\u7ed3\u6784\u548c\u72ed\u7a84\u901a\u9053\u7b49\u573a\u666f\uff0c\u5728\u697c\u68af\u6500\u722c\u548c\u5e73\u53f0\u767b\u9ad8\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u5b9e\u73b0\u63a5\u8fd1100%\u7684\u6210\u529f\u7387\u3002", "conclusion": "Gallant\u6846\u67b6\u901a\u8fc7\u4f53\u7d20\u5316LiDAR\u8868\u793a\u548c\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u57283D\u7ea6\u675f\u5730\u5f62\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.14423", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14423", "abs": "https://arxiv.org/abs/2511.14423", "authors": ["Xin Yi", "Yue Li", "Dongsheng Shi", "Linlin Wang", "Xiaoling Wang", "Liang He"], "title": "Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduHarm\u57fa\u51c6\u548cTSSF\u4e09\u9636\u6bb5\u9632\u62a4\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u9632\u5fa1\u6559\u80b2\u573a\u666f\u4e2dLLM\u7684\u8d8a\u72f1\u548c\u5fae\u8c03\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u826f\u6027\u67e5\u8be2\u6548\u7528\u7684\u540c\u65f6\u589e\u5f3a\u5b89\u5168\u6027\u3002", "motivation": "LLM\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u9762\u4e34\u8d8a\u72f1\u548c\u5fae\u8c03\u653b\u51fb\u98ce\u9669\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u5b89\u5168\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6559\u80b2\u573a\u666f\u7279\u6b8a\u5b89\u5168\u9700\u6c42\u7684\u4e13\u95e8\u7814\u7a76\u3002", "method": "\u6784\u5efaEduHarm\u57fa\u51c6\u5305\u542b\u4e94\u4e2a\u6559\u80b2\u573a\u666f\u7684\u5b89\u5168-\u4e0d\u5b89\u5168\u6307\u4ee4\u5bf9\uff1b\u63d0\u51fa\u4e09\u9636\u6bb5\u9632\u62a4\u6846\u67b6\uff1a\u5b89\u5168\u611f\u77e5\u6ce8\u610f\u529b\u91cd\u65b0\u5bf9\u9f50\u3001\u5206\u5c42\u5b89\u5168\u5224\u65ad\u3001\u9632\u5fa1\u9a71\u52a8\u53cc\u8def\u7531\u673a\u5236\u3002", "result": "\u5728\u516b\u79cd\u8d8a\u72f1\u653b\u51fb\u7b56\u7565\u4e0b\u6709\u6548\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u9632\u6b62\u826f\u6027\u67e5\u8be2\u8fc7\u5ea6\u62d2\u7edd\uff1b\u5728\u4e09\u4e2a\u5fae\u8c03\u653b\u51fb\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u5bf9\u6709\u5bb3\u67e5\u8be2\u7684\u9c81\u68d2\u9632\u5fa1\uff0c\u540c\u65f6\u4fdd\u7559\u826f\u6027\u5fae\u8c03\u7684\u6548\u7528\u589e\u76ca\u3002", "conclusion": "TSSF\u6846\u67b6\u80fd\u591f\u540c\u65f6\u7f13\u89e3\u8d8a\u72f1\u548c\u5fae\u8c03\u653b\u51fb\uff0c\u4e3a\u6559\u80b2LLM\u63d0\u4f9b\u6709\u6548\u7684\u5b89\u5168\u9632\u62a4\uff0c\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2511.14659", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14659", "abs": "https://arxiv.org/abs/2511.14659", "authors": ["Chia-Yu Hung", "Navonil Majumder", "Haoyuan Deng", "Liu Renhang", "Yankang Ang", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Ziwei Wang", "Soujanya Poria"], "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards", "comment": "https://declare-lab.github.io/nora-1.5", "summary": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.", "AI": {"tldr": "NORA-1.5\u662f\u57fa\u4e8e\u9884\u8bad\u7ec3NORA\u9aa8\u5e72\u6784\u5efa\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u6dfb\u52a0\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u52a8\u4f5c\u4e13\u5bb6\u548c\u5956\u52b1\u9a71\u52a8\u7684\u540e\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u5177\u8eab\u7cfb\u7edf\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u8868\u73b0\u4e0d\u4f73\u3002", "method": "1) \u5728\u9884\u8bad\u7ec3NORA\u9aa8\u5e72\u4e0a\u6dfb\u52a0\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u52a8\u4f5c\u4e13\u5bb6\uff1b2) \u5f00\u53d1\u7ed3\u5408\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\u548c\u504f\u79bb\u5730\u9762\u771f\u503c\u542f\u53d1\u5f0f\u7684\u5956\u52b1\u6a21\u578b\uff1b3) \u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "NORA-1.5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86NORA\u548c\u591a\u4e2a\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\uff0c\u5956\u52b1\u9a71\u52a8\u7684\u540e\u8bad\u7ec3\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "NORA-1.5\u548c\u5956\u52b1\u5f15\u5bfc\u7684\u540e\u8bad\u7ec3\u4e3a\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u9002\u5408\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u3002"}}
{"id": "2511.14439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14439", "abs": "https://arxiv.org/abs/2511.14439", "authors": ["Jinru Ding", "Lu Lu", "Chao Ding", "Mouxiao Bian", "Jiayuan Chen", "Renjie Lu", "Wenrao Pang", "Xiaoqin Wu", "Zhiqiang Liu", "Luyi Jiang", "Bing Han", "Yunqiu Wang", "Jie Xu"], "title": "MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents", "comment": null, "summary": "Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.", "AI": {"tldr": "MedBench v4\u662f\u4e00\u4e2a\u5168\u56fd\u6027\u7684\u533b\u7597AI\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b70\u591a\u4e07\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u4efb\u52a1\uff0c\u6db5\u76d624\u4e2a\u4e3b\u8981\u4e13\u79d1\u548c91\u4e2a\u6b21\u8981\u4e13\u79d1\uff0c\u4e13\u95e8\u8bc4\u4f30LLM\u3001\u591a\u6a21\u6001\u6a21\u578b\u548c\u667a\u80fd\u4f53\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u3001\u591a\u6a21\u6001\u6a21\u578b\u548c\u667a\u80fd\u4f53\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u80fd\u591f\u53cd\u6620\u771f\u5b9e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u548c\u5b89\u5168\u7ea6\u675f\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u5305\u542b70\u591a\u4e07\u4e2a\u4efb\u52a1\u7684\u4e91\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\u8bbe\u65bd\uff0c\u4efb\u52a1\u7ecf\u8fc7\u591a\u9636\u6bb5\u7ec6\u5316\u548c\u591a\u8f6e\u4e34\u5e8a\u533b\u751f\u8bc4\u5ba1\uff0c\u5f00\u653e\u5f0f\u56de\u7b54\u901a\u8fc7\u7ecf\u8fc7\u4eba\u7c7b\u8bc4\u5206\u6821\u51c6\u7684LLM-as-a-judge\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u57fa\u7840LLM\u5e73\u5747\u5f97\u520654.1/100\uff08\u6700\u4f73\uff1aClaude Sonnet 4.5\uff0c62.5/100\uff09\uff0c\u4f46\u5b89\u5168\u548c\u4f26\u7406\u5f97\u5206\u8f83\u4f4e\uff0818.4/100\uff09\u3002\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff08\u5e73\u574747.5/100\uff09\uff0c\u667a\u80fd\u4f53\u8868\u73b0\u6700\u4f73\uff08\u5e73\u574779.8/100\uff09\uff0c\u57fa\u4e8eClaude Sonnet 4.5\u7684\u667a\u80fd\u4f53\u5728\u5b89\u5168\u4efb\u52a1\u4e0a\u8fbe\u523088.9/100\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u5b89\u5168\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u5dee\u8ddd\uff0c\u4f46\u5177\u6709\u6cbb\u7406\u610f\u8bc6\u7684\u667a\u80fd\u4f53\u7f16\u6392\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4e34\u5e8a\u51c6\u5907\u5ea6\u800c\u4e0d\u727a\u7272\u80fd\u529b\uff0c\u8be5\u5e73\u53f0\u4e3a\u533b\u9662\u3001\u5f00\u53d1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u533b\u7597AI\u5ba1\u8ba1\u53c2\u8003\u3002"}}
{"id": "2511.14755", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.14755", "abs": "https://arxiv.org/abs/2511.14755", "authors": ["Albert Lin", "Alessandro Pinto", "Somil Bansal"], "title": "Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis", "comment": "Submitted to the 8th Annual Learning for Dynamics & Control Conference", "summary": "As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoVer-CoRe\u6846\u67b6\uff0c\u4f7f\u7528Hamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u6765\u9a8c\u8bc1\u611f\u77e5\u63a7\u5236\u7cfb\u7edf\u5728\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5c06\u63a7\u5236\u5668\u3001\u89c2\u6d4b\u51fd\u6570\u548c\u72b6\u6001\u4f30\u8ba1\u6a21\u5757\u4e32\u8054\u6784\u5efa\u7b49\u6548\u95ed\u73af\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u611f\u77e5\u7684\u81ea\u4e3b\u7cfb\u7edf\u63a7\u5236\u5668\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u5728\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0b\u6b63\u5f0f\u9a8c\u8bc1\u5176\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u8981\u4e48\u9650\u5236\u63a7\u5236\u5668\u548c\u7cfb\u7edf\u7c7b\u578b\uff0c\u8981\u4e48\u5206\u6790\u8fc7\u4e8e\u4fdd\u5b88\u3002", "method": "\u63d0\u51faRoVer-CoRe\u6846\u67b6\uff0c\u5c06\u7cfb\u7edf\u63a7\u5236\u5668\u3001\u89c2\u6d4b\u51fd\u6570\u548c\u72b6\u6001\u4f30\u8ba1\u6a21\u5757\u4e32\u8054\u6784\u5efa\u7b49\u6548\u95ed\u73af\u7cfb\u7edf\uff0c\u4f7f\u5176\u4e0e\u73b0\u6709\u53ef\u8fbe\u6027\u6846\u67b6\u517c\u5bb9\u3002\u5305\u542b\u6b63\u5f0f\u5b89\u5168\u9a8c\u8bc1\u548c\u9c81\u68d2\u63a7\u5236\u5668\u8bbe\u8ba1\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5728\u98de\u673a\u6ed1\u884c\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6f2b\u6e38\u8f66\u5bfc\u822a\u6848\u4f8b\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "RoVer-CoRe\u662f\u9996\u4e2a\u57fa\u4e8eHJ\u53ef\u8fbe\u6027\u5206\u6790\u7684\u611f\u77e5\u7cfb\u7edf\u9a8c\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5b89\u5168\u9a8c\u8bc1\u95ee\u9898\u3002"}}
{"id": "2511.14445", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14445", "abs": "https://arxiv.org/abs/2511.14445", "authors": ["Trishala Jayesh Ahalpara"], "title": "Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning", "comment": "8 pages, 2 figures, 1 Table. Submitted to the Computation and Language (cs.CL) category. Uses the ACL-style template. Code and demo will be released at: https://github.com/trystine/Tell_Me_Mental_Wellbeing_System", "summary": "We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.", "AI": {"tldr": "Tell Me\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7406\u5065\u5eb7\u7cfb\u7edf\uff0c\u5305\u542b\u4e2a\u6027\u5316\u5bf9\u8bdd\u52a9\u624b\u3001\u5408\u6210\u6cbb\u7597\u5bf9\u8bdd\u751f\u6210\u5668\u548cAI\u5065\u5eb7\u89c4\u5212\u5668\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u65e8\u5728\u63d0\u4f9b\u53ef\u8bbf\u95ee\u7684\u5fc3\u7406\u652f\u6301\u800c\u975e\u66ff\u4ee3\u4e13\u4e1a\u6cbb\u7597\u3002", "motivation": "\u89e3\u51b3\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7684\u53ef\u53ca\u6027\u95ee\u9898\uff0c\u964d\u4f4e\u5bfb\u6c42\u5e2e\u52a9\u7684\u95e8\u69db\uff0c\u540c\u65f6\u5e94\u5bf9\u6cbb\u7597\u5bf9\u8bdd\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u4fc3\u8fdbNLP\u7814\u7a76\u8005\u4e0e\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u7684\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u5b9e\u73b0\u4e2a\u6027\u5316\u5bf9\u8bdd\uff0c\u57fa\u4e8e\u7528\u6237\u6863\u6848\u751f\u6210\u5408\u6210\u6cbb\u7597\u5bf9\u8bdd\u7528\u4e8e\u7814\u7a76\u548c\u6570\u636e\u589e\u5f3a\uff0c\u4f7f\u7528CrewAI\u5b9e\u73b0\u52a8\u6001\u81ea\u6108\u8ba1\u5212\u751f\u6210\u548c\u51a5\u60f3\u97f3\u9891\u6307\u5bfc\u3002", "result": "\u7cfb\u7edf\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5065\u5eb7\u573a\u666f\u4e2d\u901a\u8fc7\u81ea\u52a8LLM\u8bc4\u4f30\u548c\u4eba\u5de5\u7528\u6237\u7814\u7a76\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u4f9b\u60c5\u5883\u611f\u77e5\u652f\u6301\u548c\u964d\u4f4e\u652f\u6301\u969c\u788d\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5bf9\u8bdd\u52a9\u624b\u80fd\u591f\u6709\u6548\u8865\u5145\u73b0\u6709\u62a4\u7406\uff0c\u6269\u5927\u5fc3\u7406\u5065\u5eb7\u8d44\u6e90\u7684\u53ef\u53ca\u6027\uff0c\u4e3a\u4eba\u7c7b-AI\u4e92\u52a8\u5728\u5065\u5eb7\u9886\u57df\u7684\u8d1f\u8d23\u4efb\u521b\u65b0\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002"}}
{"id": "2511.14756", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14756", "abs": "https://arxiv.org/abs/2511.14756", "authors": ["Lai Wei", "Xuanbin Peng", "Ri-Zhao Qiu", "Tianshu Huang", "Xuxin Cheng", "Xiaolong Wang"], "title": "HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation", "comment": null, "summary": "Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u5143\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ed3\u5408\u4f4d\u7f6e\u3001\u963b\u6297\u548c\u6df7\u5408\u529b-\u4f4d\u7f6e\u63a7\u5236\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u6f14\u793a\u5b66\u4e60\u9762\u4e34\u590d\u6742\u4ea4\u4e92\u52a8\u6001\u7684\u6311\u6218\uff0c\u7eaf\u4f4d\u7f6e\u63a7\u5236\u5668\u5728\u5904\u7406\u63a5\u89e6\u548c\u53d8\u8d1f\u8f7d\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u5f00\u53d1\u4e86HMC-Controller\u63a5\u53e3\u5728\u626d\u77e9\u7a7a\u95f4\u8fde\u7eed\u6df7\u5408\u4e0d\u540c\u63a7\u5236\u6a21\u5f0f\u7684\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e86HMC-Policy\u4f7f\u7528\u4e13\u5bb6\u6df7\u5408\u8def\u7531\u4ece\u5927\u89c4\u6a21\u4f4d\u7f6e\u6570\u636e\u548c\u7ec6\u7c92\u5ea6\u529b\u611f\u77e5\u6f14\u793a\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6311\u6218\u6027\u4efb\u52a1\u5982\u67d4\u6027\u684c\u9762\u64e6\u62ed\u548c\u62bd\u5c49\u5f00\u542f\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u8d85\u8fc750%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "HMC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5f02\u6784\u63a7\u5236\u7b56\u7565\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2511.14460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14460", "abs": "https://arxiv.org/abs/2511.14460", "authors": ["Mingyue Cheng", "Jie Ouyang", "Shuo Yu", "Ruiran Yan", "Yucong Luo", "Zirui Liu", "Daoyu Wang", "Qi Liu", "Enhong Chen"], "title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "comment": "This paper serves as the technical report of the Agent-R1 project", "summary": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5316\u5730\u6269\u5c55\u4e86MDP\u6846\u67b6\u6765\u5b9a\u4e49LLM\u667a\u80fd\u4f53\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86Agent-R1\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dRL\u5728LLM\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9LLM\u667a\u80fd\u4f53\u573a\u666f\u7684RL\u65b9\u6cd5\u63a2\u7d22\u548c\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "1. \u7cfb\u7edf\u5316\u6269\u5c55MDP\u6846\u67b6\u6765\u5b9a\u4e49LLM\u667a\u80fd\u4f53\u7684\u5173\u952e\u7ec4\u4ef6\uff1b2. \u63d0\u51faAgent-R1\u8bad\u7ec3\u6846\u67b6\uff0c\u5177\u6709\u6a21\u5757\u5316\u3001\u7075\u6d3b\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u7279\u70b9\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u521d\u6b65\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u548c\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aRL\u5728LLM\u667a\u80fd\u4f53\u8bad\u7ec3\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6846\u67b6\u652f\u6301\u3002"}}
{"id": "2511.14531", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.14531", "abs": "https://arxiv.org/abs/2511.14531", "authors": ["David Carmel", "Simone Filice", "Guy Horowitz", "Yoelle Maarek", "Alex Shtoff", "Oren Somekh", "Ran Tavory"], "title": "LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation", "comment": "14 pages, 4 figures, 5 tables", "summary": "With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.", "AI": {"tldr": "LiveRAG benchmark\u662f\u4e00\u4e2a\u5305\u542b895\u4e2a\u5408\u6210\u95ee\u7b54\u5bf9\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u57fa\u4e8eRAG\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u6e90\u81eaSIGIR'2025 LiveRAG\u6311\u6218\u8d5b\uff0c\u5305\u542b\u771f\u5b9e\u7b54\u6848\u3001\u652f\u6301\u8bc1\u636e\u4ee5\u53ca\u96be\u5ea6\u548c\u533a\u5206\u5ea6\u8bc4\u5206\u3002", "motivation": "\u968f\u7740RAG\u5728\u751f\u6210\u5f0fAI\u89e3\u51b3\u65b9\u6848\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b895\u4e2a\u5408\u6210\u95ee\u7b54\u5bf9\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u57fa\u4e8eSIGIR'2025 LiveRAG\u6311\u6218\u8d5b\uff0c\u6dfb\u52a0\u771f\u5b9e\u7b54\u6848\u3001\u652f\u6301\u8bc1\u636e\uff0c\u5e76\u5e94\u7528\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u6a21\u578b\u8ba1\u7b97\u96be\u5ea6\u548c\u533a\u5206\u5ea6\u8bc4\u5206\u3002", "result": "\u5206\u6790\u663e\u793a\u57fa\u51c6\u95ee\u9898\u5177\u6709\u591a\u6837\u6027\u3001\u96be\u5ea6\u8303\u56f4\u5e7f\uff0c\u80fd\u6709\u6548\u533a\u5206\u7cfb\u7edf\u80fd\u529b\u5dee\u5f02\u3002", "conclusion": "LiveRAG\u57fa\u51c6\u5c06\u5e2e\u52a9\u793e\u533a\u63a8\u8fdbRAG\u7814\u7a76\u3001\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u5e76\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u95ee\u7b54\u7cfb\u7edf\u3002"}}
{"id": "2511.14566", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14566", "abs": "https://arxiv.org/abs/2511.14566", "authors": ["Lucia Makaiov\u00e1", "Martin Faj\u010d\u00edk", "Anton\u00edn Jarol\u00edm"], "title": "Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak", "comment": null, "summary": "Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u6587\u6863\u7ea7\u4e3b\u5f20\u63d0\u53d6\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u7b97\u6cd5\u8ba1\u7b97\u4e24\u7ec4\u4e3b\u5f20\u7684\u76f8\u4f3c\u5ea6\uff0c\u4e3a\u4e8b\u5b9e\u6838\u67e5\u9886\u57df\u63d0\u4f9b\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u6587\u6863\u7ea7\u4e3b\u5f20\u63d0\u53d6\u5728\u4e8b\u5b9e\u6838\u67e5\u9886\u57df\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u5173\u6ce8\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6bd4\u8f83\u6a21\u578b\u63d0\u53d6\u4e3b\u5f20\u548c\u4eba\u5de5\u6807\u6ce8\u4e3b\u5f20\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u7814\u7a76\u4e86\u5bf9\u9f50\u4e24\u7ec4\u4e3b\u5f20\u7684\u6280\u672f\uff0c\u901a\u8fc7\u8ba1\u7b97\u5bf9\u9f50\u5f97\u5206\u6765\u8861\u91cf\u76f8\u4f3c\u5ea6\uff0c\u5e76\u5728\u6377\u514b\u548c\u65af\u6d1b\u4f10\u514b\u65b0\u95fb\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5728\u6587\u6863\u7ea7\u4e3b\u5f20\u63d0\u53d6\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u975e\u6b63\u5f0f\u8bed\u8a00\u3001\u5f3a\u672c\u5730\u8bed\u5883\u548c\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u65f6\u7684\u4e0d\u8db3\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u6b63\u786e\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5e76\u8bc4\u4f30\u5173\u952e\u4e3b\u5f20\u5c5e\u6027\uff0c\u5982\u539f\u5b50\u6027\u3001\u53ef\u6838\u67e5\u6027\u548c\u53bb\u8bed\u5883\u5316\u3002"}}
{"id": "2511.14598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14598", "abs": "https://arxiv.org/abs/2511.14598", "authors": ["Noam Dahan", "Omer Kidron", "Gabriel Stanovsky"], "title": "Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages", "comment": null, "summary": "High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u62a5\u7eb8\u5934\u7248\u9884\u544a\u6536\u96c6\u81ea\u7136\u6458\u8981\u7684\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u8a00\uff0c\u5e76\u521b\u5efa\u4e86\u5e0c\u4f2f\u6765\u8bed\u9996\u4e2a\u591a\u6587\u6863\u6458\u8981\u6570\u636e\u96c6", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9ad8\u8d28\u91cf\u6458\u8981\u6570\u636e\u7a00\u7f3a\uff0c\u800c\u6570\u5b57\u5316\u5386\u53f2\u62a5\u7eb8\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u81ea\u7136\u6807\u6ce8\u6570\u636e\u6e90", "method": "\u5f00\u53d1\u81ea\u52a8\u6d41\u7a0b\u4ece\u62a5\u7eb8\u5934\u7248\u9884\u544a\u4e2d\u6536\u96c6\u81ea\u7136\u6458\u8981\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8bed\u8a00\u8d44\u6e90\u6c34\u5e73", "result": "\u8be5\u65b9\u6cd5\u5728\u4e03\u79cd\u8bed\u8a00\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u5e76\u6210\u529f\u521b\u5efa\u4e86HEBTEASESUM\u5e0c\u4f2f\u6765\u8bed\u591a\u6587\u6863\u6458\u8981\u6570\u636e\u96c6", "conclusion": "\u62a5\u7eb8\u5934\u7248\u9884\u544a\u662f\u83b7\u53d6\u591a\u8bed\u8a00\u591a\u6587\u6863\u6458\u8981\u6570\u636e\u7684\u6709\u6548\u6765\u6e90\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00"}}
{"id": "2511.14603", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14603", "abs": "https://arxiv.org/abs/2511.14603", "authors": ["Yilu Fang", "Jordan G. Nestor", "Casey N. Ta", "Jerard Z. Kneifati-Hayek", "Chunhua Weng"], "title": "A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease", "comment": null, "summary": "Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.", "AI": {"tldr": "\u4f7f\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u52a8\u6001\u8ffd\u8e2aAKI\u60a3\u8005\u4e34\u5e8a\u6f14\u53d8\uff0c\u8bc6\u522bAKI\u5411CKD\u8fdb\u5c55\u7684\u6a21\u5f0f\u3002\u901a\u8fc7\u805a\u7c7b\u5206\u6790\u53d1\u73b015\u79cd\u4e0d\u540c\u7684AKI\u540e\u4e34\u5e8a\u72b6\u6001\uff0c\u6bcf\u79cd\u72b6\u6001\u5177\u6709\u4e0d\u540c\u7684CKD\u53d1\u5c55\u6982\u7387\u3002", "motivation": "\u6025\u6027\u80be\u635f\u4f24\uff08AKI\uff09\u60a3\u8005\u53d1\u5c55\u4e3a\u6162\u6027\u80be\u75c5\uff08CKD\uff09\u7684\u98ce\u9669\u5f88\u9ad8\uff0c\u4f46\u8bc6\u522b\u6700\u9ad8\u98ce\u9669\u60a3\u8005\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u901a\u8fc7\u7eb5\u5411\u533b\u7597\u4ee3\u7801\u548c\u808c\u9150\u6d4b\u91cf\u503c\u805a\u7c7b\u60a3\u8005\u5411\u91cf\u8bc6\u522bAKI\u540e\u4e34\u5e8a\u72b6\u6001\uff0c\u4f7f\u7528\u591a\u72b6\u6001\u6a21\u578b\u4f30\u8ba1\u72b6\u6001\u95f4\u8f6c\u6362\u6982\u7387\u548cCKD\u8fdb\u5c55\u6982\u7387\u3002", "result": "\u572820,699\u540dAKI\u60a3\u8005\u4e2d\uff0c3,491\u4eba\uff0817%\uff09\u53d1\u5c55\u4e3aCKD\u3002\u8bc6\u522b\u51fa15\u79cd\u4e0d\u540c\u7684AKI\u540e\u72b6\u6001\uff0c75%\u60a3\u8005\u5728\u7814\u7a76\u671f\u95f4\u4fdd\u6301\u5355\u4e00\u72b6\u6001\u6216\u4ec5\u8fdb\u884c\u4e00\u6b21\u8f6c\u6362\u3002\u53d1\u73b0\u4f20\u7edf\u548c\u65b0\u578bCKD\u98ce\u9669\u56e0\u7d20\u5728\u4e0d\u540c\u4e34\u5e8a\u72b6\u6001\u4e2d\u5f71\u54cd\u5404\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u8bc6\u522b\u9ad8\u98ce\u9669AKI\u60a3\u8005\uff0c\u652f\u6301\u5f00\u53d1\u7528\u4e8e\u65e9\u671fCKD\u68c0\u6d4b\u548c\u5e72\u9884\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2511.14606", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14606", "abs": "https://arxiv.org/abs/2511.14606", "authors": ["Shreya Adrita Banik", "Niaz Nafi Rahman", "Tahsina Moiukh", "Farig Sadeque"], "title": "Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models", "comment": null, "summary": "Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u6807\u6ce8\u4e0e\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u504f\u89c1\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0RoBERTa\u4e0e\u4eba\u7c7b\u6807\u6ce8\u6700\u4e00\u81f4\uff0cGPT\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0e\u6a21\u578b\u5728\u653f\u6cbb\u504f\u89c1\u611f\u77e5\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1NLP\u6280\u672f\u5df2\u80fd\u81ea\u52a8\u5206\u7c7b\u653f\u6cbb\u504f\u89c1\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u7a0b\u5ea6\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u548c\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4eba\u7c7b\u4e0e\u6a21\u578b\u5728\u504f\u89c1\u68c0\u6d4b\u4e0a\u7684\u5dee\u5f02\u3002", "method": "\u6784\u5efa\u624b\u52a8\u6807\u6ce8\u7684\u65b0\u95fb\u6587\u7ae0\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u6807\u6ce8\u4e00\u81f4\u6027\u3001\u504f\u89c1\u6781\u6027\u548c\u6a21\u578b\u95f4\u4e00\u81f4\u6027\uff0c\u91cf\u5316\u4eba\u7c7b\u4e0e\u6a21\u578b\u5728\u504f\u89c1\u611f\u77e5\u4e0a\u7684\u5dee\u5f02\uff0c\u6bd4\u8f83GPT\u3001BERT\u3001RoBERTa\u548cFLAN\u7b49\u6a21\u578b\u8868\u73b0\u3002", "result": "\u4f20\u7edf\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u4e2d\uff0cRoBERTa\u4e0e\u4eba\u7c7b\u6807\u7b7e\u5bf9\u9f50\u5ea6\u6700\u9ad8\uff1b\u751f\u6210\u6a21\u578b\u5982GPT\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u603b\u4f53\u4e00\u81f4\u6027\u6700\u5f3a\uff1b\u5fae\u8c03\u7684RoBERTa\u6a21\u578b\u5728\u6240\u6709\u57fa\u7ebf\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u51c6\u786e\u7387\u548c\u6700\u5f3a\u7684\u4eba\u7c7b\u6807\u6ce8\u5bf9\u9f50\u5ea6\u3002", "conclusion": "\u4eba\u7c7b\u4e0eLLMs\u5728\u653f\u6cbb\u503e\u5411\u611f\u77e5\u4e0a\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u9700\u8981\u7ed3\u5408\u4eba\u7c7b\u53ef\u89e3\u91ca\u6027\u548c\u6a21\u578b\u53ef\u6269\u5c55\u6027\u7684\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u6765\u81ea\u52a8\u5316\u5a92\u4f53\u504f\u89c1\u68c0\u6d4b\u3002"}}
{"id": "2511.14631", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.14631", "abs": "https://arxiv.org/abs/2511.14631", "authors": ["Kahaan Gandhi", "Boris Bolliet", "Inigo Zubeldia"], "title": "Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities", "comment": null, "summary": "We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent", "AI": {"tldr": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u8fc7\u5c06\u56fe\u8868\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u68c0\u67e5\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u80fd\u529b\uff0c\u5728\u5b87\u5b99\u5b66\u548c\u5929\u4f53\u5316\u5b66\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u9519\u8bef\u63a8\u7406\u8def\u5f84\u7684\u81ea\u6211\u4fee\u6b63\u548c\u65b0\u6570\u636e\u96c6\u7684\u9002\u5e94\u6027\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u7f3a\u4e4f\u5b9e\u65f6\u9519\u8bef\u4fee\u6b63\u548c\u9002\u5e94\u6027\u5b66\u4e60\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\u548c\u8c03\u6574\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u7684\u9886\u57df\u7279\u5b9a\u8bc4\u5206\u6807\u51c6\u8bc4\u4f30\u56fe\u8868\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5b9e\u65f6\u7ea0\u6b63\u9519\u8bef\u5e76\u5f15\u5bfc\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u3002", "result": "\u572810\u4e2a\u6570\u636e\u9a71\u52a8\u53d1\u73b0\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLM\u589e\u5f3a\u7cfb\u7edf\u8fbe\u52300.7-0.8\u7684pass@1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4ee3\u7801\uff080.2-0.3\uff09\u548c\u4ee3\u7801\u52a0\u6587\u672c\uff080.4-0.5\uff09\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u5347\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2511.14638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14638", "abs": "https://arxiv.org/abs/2511.14638", "authors": ["Tao Yang", "Dandan Huang", "Yunting Lin", "Pengfei Wu", "Zhikun Wu", "Gangyuan Ma", "Yulan Lu", "Xinran Dong", "Dingpeng Li", "Junshuang Ge", "Zhiyan Zhang", "Xuanzhao Huang", "Wenyan Nong", "Yao Zhou", "Hui Tang", "Hongxi Yang", "Shijie Zhang", "Juan Li", "Xiaojun Cao", "Lin Yang", "Xia Gao", "Kaishou Xu", "Xiaoqiong Gu", "Wen Zhang", "Huimin Xia", "Li Liu", "Wenhao Zhou", "Mulin Jun Li"], "title": "A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases", "comment": "50 pages, 5 figures", "summary": "Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.", "AI": {"tldr": "RareSeek R1\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u7f55\u89c1\u75c5\u8bca\u65ad\u7684AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u4e34\u5e8a\u8bed\u6599\u5e93\u548c\u5206\u9636\u6bb5\u6307\u4ee4\u8c03\u4f18\uff0c\u5728\u5608\u6742\u6216\u91cd\u53e0\u8868\u578b\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u7f55\u89c1\u75c5\u5f71\u54cd\u5168\u7403\u6570\u4ebf\u4eba\uff0c\u4f46\u8bca\u65ad\u901a\u5e38\u9700\u8981\u6570\u5e74\u65f6\u95f4\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u8bc1\u636e\u63d0\u53d6\u4e0e\u8bca\u65ad\u63a8\u7406\u5206\u79bb\uff0c\u800c\u901a\u7528/\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u771f\u5b9e\u4e16\u754c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7a00\u7f3a\u3001\u9886\u57df\u77e5\u8bc6\u9648\u65e7\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5927\u578b\u9886\u57df\u4e13\u7528\u4e34\u5e8a\u8bed\u6599\u5e93\u548c\u4e34\u5e8a\u533b\u751f\u9a8c\u8bc1\u7684\u63a8\u7406\u96c6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u6307\u4ee4\u8c03\u4f18\u3001\u601d\u7ef4\u94fe\u5b66\u4e60\u548c\u56fe\u57fa\u7840\u68c0\u7d22\u5f00\u53d1RareSeek R1\u3002", "result": "\u5728\u591a\u4e2d\u5fc3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u53d9\u8ff0\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRareSeek R1\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u6027\u3001\u7a33\u5065\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002\u589e\u5f3a\u68c0\u7d22\u5728\u53d9\u8ff0\u4e0e\u4f18\u5148\u53d8\u5f02\u914d\u5bf9\u65f6\u4ea7\u751f\u6700\u5927\u6536\u76ca\u3002\u4eba\u7c7b\u7814\u7a76\u663e\u793a\u5176\u6027\u80fd\u4e0e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u533b\u751f\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u4ee5\u53d9\u8ff0\u4e3a\u5148\u3001\u77e5\u8bc6\u6574\u5408\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u7f29\u77ed\u8bca\u65ad\u5386\u7a0b\uff0c\u5e76\u63d0\u4f9b\u53ef\u5ba1\u8ba1\u3001\u4e34\u5e8a\u53ef\u8f6c\u5316\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2511.14642", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14642", "abs": "https://arxiv.org/abs/2511.14642", "authors": ["Yuhan Zhang", "Erxiao Wang", "Cory Shain"], "title": "Graded strength of comparative illusions is explained by Bayesian inference", "comment": "49 pages, 7 figures", "summary": "Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9a\u91cf\u6a21\u578b\u8bc1\u660e\u6bd4\u8f83\u6027\u8bed\u8a00\u5e7b\u89c9\u53ef\u4ee5\u7528\u8d1d\u53f6\u65af\u566a\u58f0\u4fe1\u9053\u7406\u8bba\u89e3\u91ca\uff0c\u6a21\u578b\u6210\u529f\u9884\u6d4b\u4e86\u5e7b\u89c9\u5f3a\u5ea6\u7684\u7ec6\u5fae\u5dee\u5f02\u548c\u4ee3\u8bcd\u4e0e\u540d\u8bcd\u77ed\u8bed\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u6bd4\u8f83\u6027\u8bed\u8a00\u5e7b\u89c9\u73b0\u8c61\uff0c\u9a8c\u8bc1\u566a\u58f0\u4fe1\u9053\u7406\u8bba\u662f\u5426\u80fd\u89e3\u91ca\u8fd9\u79cd\u7cfb\u7edf\u6027\u8bed\u8a00\u8bef\u611f\u77e5\uff0c\u5e76\u8d85\u8d8a\u5148\u524d\u7814\u7a76\u7684\u5c40\u9650\u3002", "method": "\u7ed3\u5408\u7edf\u8ba1\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u884c\u4e3a\u6570\u636e\uff0c\u6784\u5efa\u540e\u9a8c\u6982\u7387\u5b9a\u91cf\u6a21\u578b\u6765\u9884\u6d4b\u5e7b\u89c9\u5f3a\u5ea6\uff0c\u5206\u6790\u4e0d\u540c\u89e3\u91ca\u7684\u8d1d\u53f6\u65af\u6982\u7387\u3002", "result": "\u6a21\u578b\u6210\u529f\u89e3\u91ca\u4e86\u6bd4\u8f83\u6027\u5e7b\u89c9\u7684\u5f3a\u5ea6\u68af\u5ea6\u4ee5\u53ca\u4ee3\u8bcd\u4e0e\u5168\u540d\u8bcd\u77ed\u8bedthan\u4ece\u53e5\u4e3b\u8bed\u7684\u5f71\u54cd\uff0c\u652f\u6301\u566a\u58f0\u4fe1\u9053\u7406\u8bba\u3002", "conclusion": "\u566a\u58f0\u4fe1\u9053\u63a8\u7406\u53ef\u4ee5\u4f5c\u4e3a\u7edf\u4e00\u7684\u8ba1\u7b97\u5c42\u9762\u7406\u8bba\u89e3\u91ca\u5404\u79cd\u8bed\u8a00\u5904\u7406\u73b0\u8c61\uff0c\u5305\u62ec\u5e7b\u89c9\u6027\u548c\u975e\u5e7b\u89c9\u6027\u60c5\u5883\u3002"}}
{"id": "2511.14662", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14662", "abs": "https://arxiv.org/abs/2511.14662", "authors": ["Xia Cui", "Ziyi Huang", "Naeemeh Adel"], "title": "Bias in, Bias out: Annotation Bias in Multilingual Large Language Models", "comment": null, "summary": "Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u89e3NLP\u6570\u636e\u96c6\u4e2d\u6807\u6ce8\u504f\u89c1\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u6307\u4ee4\u504f\u89c1\u3001\u6807\u6ce8\u8005\u504f\u89c1\u548c\u60c5\u5883\u6587\u5316\u504f\u89c1\uff0c\u5e76\u63d0\u4f9b\u4e86\u68c0\u6d4b\u65b9\u6cd5\u548c\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "NLP\u6570\u636e\u96c6\u4e2d\u7684\u6807\u6ce8\u504f\u89c1\u662f\u5f00\u53d1\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6587\u5316\u591a\u6837\u73af\u5883\u4e2d\u3002\u504f\u89c1\u4f1a\u626d\u66f2\u6a21\u578b\u8f93\u51fa\u5e76\u52a0\u5267\u793e\u4f1a\u5371\u5bb3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6cd5\u6765\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u6807\u6ce8\u504f\u89c1\uff0c\u7efc\u8ff0\u4e86\u68c0\u6d4b\u65b9\u6cd5\uff08\u5305\u62ec\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u3001\u6a21\u578b\u5206\u6b67\u548c\u5143\u6570\u636e\u5206\u6790\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\uff0c\u5305\u62ec\u591a\u6837\u5316\u6807\u6ce8\u8005\u62db\u52df\u3001\u8fed\u4ee3\u6307\u5357\u6539\u8fdb\u548c\u4e8b\u540e\u6a21\u578b\u8c03\u6574\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u504f\u89c1\u5206\u7c7b\u6cd5\u3001\u68c0\u6d4b\u6307\u6807\u7efc\u5408\u3001\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u73af\u5883\u7684\u96c6\u6210\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5bf9\u6807\u6ce8\u8fc7\u7a0b\u7684\u4f26\u7406\u5206\u6790\u3002", "conclusion": "\u8fd9\u4e9b\u89c1\u89e3\u65e8\u5728\u4e3aLLMs\u63d0\u4f9b\u66f4\u516c\u5e73\u548c\u6587\u5316\u57fa\u7840\u7684\u6807\u6ce8\u6d41\u7a0b\uff0c\u4fc3\u8fdb\u591a\u8bed\u8a00\u6a21\u578b\u7684\u516c\u6b63\u53d1\u5c55\u3002"}}
{"id": "2511.14671", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14671", "abs": "https://arxiv.org/abs/2511.14671", "authors": ["Kristi Topollai", "Tolga Dimlioglu", "Anna Choromanska", "Simon Odie", "Reginald Hui"], "title": "Streamlining Industrial Contract Management with Retrieval-Augmented LLMs", "comment": null, "summary": "Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5408\u540c\u7ba1\u7406\u6d41\u7a0b\uff0c\u80fd\u591f\u8bc6\u522b\u95ee\u9898\u4fee\u8ba2\u5e76\u751f\u6210\u6539\u8fdb\u65b9\u6848\uff0c\u5728\u771f\u5b9e\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u8fbe\u523080%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5408\u540c\u7ba1\u7406\u6d89\u53ca\u6761\u6b3e\u5ba1\u67e5\u548c\u8c08\u5224\uff0c\u4f46\u81ea\u52a8\u5316\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u975e\u7ed3\u6784\u5316\u5386\u53f2\u5408\u540c\u4e30\u5bcc\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u6574\u5408\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u8bed\u4e49\u6761\u6b3e\u68c0\u7d22\u3001\u53ef\u63a5\u53d7\u6027\u5206\u7c7b\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u5bf9\u9f50\u673a\u5236\uff0c\u6784\u5efaRAG\u6d41\u7a0b\u3002", "result": "\u4e0e\u884c\u4e1a\u5408\u4f5c\u4f19\u4f34\u5171\u540c\u5f00\u53d1\u8bc4\u4f30\uff0c\u7cfb\u7edf\u5728\u8bc6\u522b\u548c\u4f18\u5316\u95ee\u9898\u4fee\u8ba2\u65b9\u9762\u5747\u8fbe\u523080%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u771f\u5b9e\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u52a0\u901f\u5408\u540c\u4fee\u8ba2\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14683", "abs": "https://arxiv.org/abs/2511.14683", "authors": ["Oscar Fontanelli", "Wentian Li"], "title": "Quadratic Term Correction on Heaps' Law", "comment": "3 figures", "summary": "Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement\" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance\" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u8bcd\u6c47\u7c7b\u578b-\u8bcd\u6c47\u6807\u8bb0\u5173\u7cfb\u5728\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u4ecd\u5448\u8f7b\u5fae\u51f9\u5f62\uff0c\u4e0d\u7b26\u5408\u5e42\u5f8b\u5173\u7cfb\uff0c\u800c\u662f\u7528\u4e8c\u6b21\u51fd\u6570\u80fd\u5b8c\u7f8e\u62df\u5408\u6570\u636e\u3002\u901a\u8fc7\u968f\u673a\u62bd\u53d6\u6a21\u578b\u5206\u6790\uff0c\u53d1\u73b0\u8fd9\u79cd\u66f2\u7387\u4e0e\"\u4f2a\u65b9\u5dee\"\u76f8\u5173\u3002", "motivation": "\u4f20\u7edfHeap\u5b9a\u5f8b\u8ba4\u4e3a\u8bcd\u6c47\u7c7b\u578b\u4e0e\u6807\u8bb0\u5173\u7cfb\u7b26\u5408\u5e42\u5f8b\u51fd\u6570\uff0c\u5728\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u5e94\u4e3a\u76f4\u7ebf\u3002\u4f46\u5b9e\u9645\u89c2\u5bdf\u53d1\u73b0\u5373\u4f7f\u5728\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\uff0c\u7c7b\u578b-\u6807\u8bb0\u66f2\u7ebf\u4ecd\u5448\u8f7b\u5fae\u51f9\u5f62\uff0c\u8fd9\u6311\u6218\u4e86\u5e42\u5f8b\u5173\u7cfb\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u752820\u90e8\u82f1\u6587\u5c0f\u8bf4\u6216\u4f5c\u54c1\u8fdb\u884c\u56de\u5f52\u5206\u6790\uff0c\u5728\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u7528\u7ebf\u6027\u9879\u548c\u4e8c\u6b21\u9879\u62df\u5408\u7c7b\u578b-\u6807\u8bb0\u6570\u636e\u3002\u540c\u65f6\u91c7\u7528\"\u4ece\u888b\u4e2d\u968f\u673a\u62bd\u53d6\u5f69\u8272\u7403\u5e76\u653e\u56de\"\u7684\u6a21\u578b\u6765\u5206\u6790\u66f2\u7387\u7279\u6027\u3002", "result": "\u56de\u5f52\u5206\u6790\u663e\u793a\u7ebf\u6027\u7cfb\u6570\u7565\u5927\u4e8e1\uff0c\u4e8c\u6b21\u7cfb\u6570\u7ea6\u4e3a-0.02\u3002\u6a21\u578b\u5206\u6790\u8868\u660e\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u7684\u66f2\u7387\u7b49\u540c\u4e8e\u8d1f\u7684\"\u4f2a\u65b9\u5dee\"\uff0c\u867d\u7136\u5728\u5927\u6807\u8bb0\u6570\u65f6\u53ef\u80fd\u9047\u5230\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8bcd\u6c47\u7c7b\u578b-\u6807\u8bb0\u5173\u7cfb\u5728\u53cc\u5bf9\u6570\u5750\u6807\u4e0b\u786e\u5b9e\u5b58\u5728\u8f7b\u5fae\u51f9\u5f62\uff0c\u4e8c\u6b21\u51fd\u6570\u6bd4\u5e42\u5f8b\u51fd\u6570\u80fd\u66f4\u597d\u5730\u63cf\u8ff0\u8fd9\u79cd\u5173\u7cfb\uff0c\u8fd9\u4e3a\u7406\u89e3\u8bed\u8a00\u7edf\u8ba1\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2511.14684", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14684", "abs": "https://arxiv.org/abs/2511.14684", "authors": ["Biaojie Zeng", "Min Zhang", "Juan Zhou", "Fengrui Liu", "Ruiyang Huang", "Xin Lin"], "title": "SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction", "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \\textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \\textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \\texttt{SMRC} (\\textit{\\underline{S}tudent \\underline{M}athematical \\underline{R}easoning \\underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \\texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \\textbf{solution accuracy} and \\textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \\texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.", "AI": {"tldr": "\u63d0\u51faSMRC\u65b9\u6cd5\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u63a2\u7d22\u6700\u4f18\u4fee\u6b63\u8def\u5f84\uff0c\u901a\u8fc7\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u7b54\u6848\u8bc4\u4f30\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u5b9e\u73b0\u5bf9\u5b66\u751f\u6570\u5b66\u63a8\u7406\u8fc7\u7a0b\u7684\u81ea\u52a8\u4fee\u6b63\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u5185\u90e8\u7684\u81ea\u4fee\u6b63\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6559\u80b2\u573a\u666f\u4e2d\"\u6559\u5e08\u5f0f\"\u4fee\u6b63\u7684\u9700\u6c42\uff0c\u5373\u7cfb\u7edf\u6027\u5730\u6307\u5bfc\u548c\u4fee\u6b63\u5b66\u751f\u89e3\u9898\u8fc7\u7a0b\u3002", "method": "\u5c06\u5b66\u751f\u63a8\u7406\u5efa\u6a21\u4e3a\u591a\u6b65\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u5f15\u5165\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u63a2\u7d22\u4fee\u6b63\u8def\u5f84\uff0c\u5229\u7528\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u6700\u7ec8\u7b54\u6848\u8bc4\u4f30\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u76d1\u7763\u3002", "result": "\u5728ProcessBench\u3001MR-GSM8K\u548c\u81ea\u5efa\u7684MSEB\u6570\u636e\u96c6\u4e0a\uff0cSMRC\u5728\u6548\u679c\u548c\u6574\u4f53\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SMRC\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5b66\u751f\u63a8\u7406\uff0c\u5b9e\u73b0\u6559\u80b2\u573a\u666f\u4e2d\u6240\u9700\u7684\u7cfb\u7edf\u6027\u4fee\u6b63\uff0c\u5728\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.14685", "categories": ["cs.CL", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2511.14685", "abs": "https://arxiv.org/abs/2511.14685", "authors": ["Kiera McCormick", "Rafael Mart\u00ednez-Galarza"], "title": "Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries", "comment": "Accepted to the Machine Learning and the Physical Sciences Workshop at NeurIPS 2025, 11 pages, 4 figures", "summary": "Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22LLM\u5d4c\u5165\u662f\u5426\u80fd\u7f16\u7801\u4ece\u79d1\u5b66\u6d4b\u91cf\u83b7\u5f97\u7684\u5929\u4f53\u7269\u7406\u7edf\u8ba1\u91cf\uff0c\u5206\u6790\u63d0\u793a\u5de5\u7a0b\u548c\u8bed\u8a00\u8981\u7d20\u5bf9\u7269\u7406\u4fe1\u606f\u7f16\u7801\u7684\u5f71\u54cd\u3002", "motivation": "\u5229\u7528LLM\u5728\u8de8\u9886\u57df\u6cdb\u5316\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7814\u7a76\u5176\u662f\u5426\u80fd\u7f16\u7801\u901a\u5e38\u4ec5\u4ece\u79d1\u5b66\u6d4b\u91cf\u83b7\u5f97\u4e14\u677e\u6563\u7f16\u7801\u5728\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u7269\u7406\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u5929\u4f53\u7269\u7406\u5b66\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u5206\u6790\u63d0\u793a\u5de5\u7a0b\u548c\u8bed\u8a00\u8981\u7d20\u7684\u4f5c\u7528\u3002", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u7406\u89e3LLM\u5982\u4f55\u7f16\u7801\u7269\u7406\u4fe1\u606f\uff0c\u7279\u522b\u662f\u63d0\u793a\u5de5\u7a0b\u548c\u8bed\u8a00\u8981\u7d20\u5728\u7f16\u7801\u6d4b\u91cf\u6240\u4ee3\u8868\u7684\u7269\u7406\u7279\u6027\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.14688", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14688", "abs": "https://arxiv.org/abs/2511.14688", "authors": ["Clovis Gladstone", "Zhao Fang", "Spencer Dean Stewart"], "title": "Ground Truth Generation for Multilingual Historical NLP using LLMs", "comment": "13 pages, 5 tables, 1 figure", "summary": "Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u5386\u53f2\u6cd5\u8bed\u548c\u4e2d\u6587\u6587\u672c\u751f\u6210\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u5fae\u8c03spaCy\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u8bcd\u6027\u6807\u6ce8\u3001\u8bcd\u5f62\u8fd8\u539f\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u5728\u7279\u5b9a\u5386\u53f2\u65f6\u671f\u7684\u6027\u80fd", "motivation": "\u89e3\u51b3\u5386\u53f2\u6587\u672c\u548c\u4f4e\u8d44\u6e90NLP\u9762\u4e34\u7684\u6807\u6ce8\u6570\u636e\u6709\u9650\u4ee5\u53ca\u4e0e\u73b0\u4ee3\u7f51\u7edc\u8bed\u6599\u9886\u57df\u4e0d\u5339\u914d\u7684\u6311\u6218", "method": "\u5229\u7528LLM\u751f\u6210\u5386\u53f2\u6587\u672c\u7684\u6807\u6ce8\u6570\u636e\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u6570\u636e\u5fae\u8c03spaCy\u6a21\u578b", "result": "\u5728\u7279\u5b9a\u5386\u53f2\u65f6\u671f\u7684\u6d4b\u8bd5\u4e2d\uff0c\u8bcd\u6027\u6807\u6ce8\u3001\u8bcd\u5f62\u8fd8\u539f\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347", "conclusion": "\u9886\u57df\u7279\u5b9a\u6a21\u578b\u7684\u91cd\u8981\u6027\u5f97\u5230\u9a8c\u8bc1\uff0c\u5373\u4f7f\u76f8\u5bf9\u6709\u9650\u7684\u5408\u6210\u6570\u636e\u4e5f\u80fd\u6539\u5584\u8ba1\u7b97\u4eba\u6587\u5b66\u7814\u7a76\u4e2d\u4f4e\u8d44\u6e90\u8bed\u6599\u7684NLP\u5de5\u5177"}}
{"id": "2511.14693", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14693", "abs": "https://arxiv.org/abs/2511.14693", "authors": ["Rishu Kumar Singh", "Navneet Shreya", "Sarmistha Das", "Apoorva Singh", "Sriparna Saha"], "title": "Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances", "comment": "To be published in the Proceedings of the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026 Special Track on AI for Social Impact )", "summary": "Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR", "AI": {"tldr": "\u63d0\u51faVALOR\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u591a\u8f6e\u5ba2\u6237\u652f\u6301\u5bf9\u8bdd\u4e2d\u7684\u6295\u8bc9\u5206\u6790\uff0c\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u8bed\u4e49\u5bf9\u9f50\u548c\u4e13\u5bb6\u8def\u7531\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6295\u8bc9\u65b9\u9762\u548c\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u6295\u8bc9\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u6a21\u6001\u77ed\u6587\u672c\uff0c\u800c\u5b9e\u9645\u5ba2\u6237\u652f\u6301\u5bf9\u8bdd\u4e2d\u7528\u6237\u5e38\u540c\u65f6\u63d0\u4f9b\u6587\u672c\u6295\u8bc9\u548c\u89c6\u89c9\u8bc1\u636e\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u591a\u6a21\u6001\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u591a\u4e13\u5bb6\u63a8\u7406\u8bbe\u7f6e\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u8fdb\u884c\u51b3\u7b56\uff0c\u8ba1\u7b97\u6a21\u6001\u95f4\u8bed\u4e49\u5bf9\u9f50\u5206\u6570\u5e76\u901a\u8fc7\u5143\u878d\u5408\u7b56\u7565\u6574\u5408\u5230\u6700\u7ec8\u5206\u7c7b\u4e2d\u3002", "result": "\u5728\u6807\u6ce8\u7ec6\u7c92\u5ea6\u65b9\u9762\u548c\u4e25\u91cd\u7a0b\u5ea6\u7684\u591a\u6a21\u6001\u6295\u8bc9\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cVALOR\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4fe1\u606f\u5206\u5e03\u5728\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u7684\u590d\u6742\u6295\u8bc9\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u4e13\u5bb6\u9a8c\u8bc1\u5728\u5b9e\u9645\u6295\u8bc9\u7406\u89e3\u7cfb\u7edf\u4e2d\u7684\u4ef7\u503c\uff0c\u652f\u6301\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u68079\u548c12\uff0c\u4fc3\u8fdb\u66f4\u8d1f\u8d23\u4efb\u7684\u4ea7\u54c1\u8bbe\u8ba1\u548c\u6d88\u8d39\u8005\u670d\u52a1\u95ee\u8d23\u5236\u3002"}}
{"id": "2511.14696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14696", "abs": "https://arxiv.org/abs/2511.14696", "authors": ["Ali Salehi", "Cassandra L. Jacobs"], "title": "Subword Tokenization Strategies for Kurdish Word Embeddings", "comment": null, "summary": "We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\\% of test cases compared to 68.7\\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.", "AI": {"tldr": "\u6bd4\u8f83\u5e93\u5c14\u5fb7\u8bed\u8bcd\u5d4c\u5165\u7684\u4e09\u79cd\u5206\u8bcd\u7b56\u7565\uff1a\u8bcd\u7ea7\u3001\u57fa\u4e8e\u8bed\u7d20\u548cBPE\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u8bed\u7d20\u7684\u65b9\u6cd5\u5728\u5168\u9762\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cBPE\u7531\u4e8e\u8986\u76d6\u7387\u4f4e\u5bfc\u81f4\u8bc4\u4f30\u504f\u5dee\u3002", "motivation": "\u7814\u7a76\u5e93\u5c14\u5fb7\u8bed\u8fd9\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5206\u8bcd\u7b56\u7565\uff0c\u63a2\u7d22\u5982\u4f55\u66f4\u597d\u5730\u4fdd\u7559\u5f62\u6001\u76f8\u4f3c\u6027\u5e76\u6784\u5efa\u6709\u6548\u7684\u8bcd\u5d4c\u5165\u7a7a\u95f4\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eBiLSTM-CRF\u7684\u5f62\u6001\u5206\u5272\u5668\uff0c\u4f7f\u7528\u6700\u5c0f\u624b\u52a8\u6807\u6ce8\u8fdb\u884c\u81ea\u4e3e\u8bad\u7ec3\uff0c\u6bd4\u8f83\u4e09\u79cd\u5206\u8bcd\u65b9\u6cd5\u5728Word2Vec\u5d4c\u5165\u4e0a\u7684\u8868\u73b0\uff0c\u91c7\u7528\u5305\u62ec\u76f8\u4f3c\u6027\u4fdd\u6301\u3001\u805a\u7c7b\u8d28\u91cf\u548c\u8bed\u4e49\u7ec4\u7ec7\u5728\u5185\u7684\u7efc\u5408\u8bc4\u4f30\u6307\u6807\u3002", "result": "BPE\u5728\u5f62\u6001\u76f8\u4f3c\u6027\u4e0a\u8868\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ec5\u8bc4\u4f30\u4e8628.6%\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u800c\u8bed\u7d20\u6a21\u578b\u8bc4\u4f30\u4e8668.7%\u3002\u5168\u9762\u8bc4\u4f30\u663e\u793a\u57fa\u4e8e\u8bed\u7d20\u7684\u5206\u8bcd\u5728\u5d4c\u5165\u7a7a\u95f4\u7ec4\u7ec7\u3001\u8bed\u4e49\u90bb\u57df\u7ed3\u6784\u548c\u8de8\u5f62\u6001\u590d\u6742\u5ea6\u8986\u76d6\u65b9\u9762\u66f4\u4f18\u3002", "conclusion": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u4e2d\uff0c\u57fa\u4e8e\u8986\u76d6\u7387\u7684\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u57fa\u4e8e\u8bed\u7d20\u7684\u5206\u8bcd\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14709", "abs": "https://arxiv.org/abs/2511.14709", "authors": ["Raha Aghaei", "Ali A. Kiaei", "Mahnaz Boush", "Mahan Rofoosheh", "Mohammad Zavvar"], "title": "Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance", "comment": null, "summary": "This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.", "AI": {"tldr": "LLMs\u901a\u8fc7\u81ea\u52a8\u5316\u77e5\u8bc6\u53d1\u73b0\u3001\u4fc3\u8fdb\u5047\u8bbe\u751f\u6210\u3001\u6574\u5408\u8de8\u5b66\u79d1\u89c1\u89e3\u548c\u4fc3\u8fdb\u521b\u65b0\u751f\u6001\u7cfb\u7edf\u5408\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u7814\u53d1\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u5206\u6790LLMs\u5728\u8f6c\u53d8\u7814\u53d1\u8fc7\u7a0b\u4e2d\u7684\u591a\u91cd\u529f\u80fd\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u81ea\u52a8\u5316\u77e5\u8bc6\u53d1\u73b0\u3001\u5047\u8bbe\u751f\u6210\u548c\u8de8\u5b66\u79d1\u6574\u5408\u6765\u63d0\u5347\u7814\u53d1\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u5206\u6790\u79d1\u5b66\u6587\u732e\u3001\u4e13\u5229\u6570\u636e\u5e93\u548c\u5b9e\u9a8c\u6570\u636e\uff0c\u5229\u7528LLMs\u5b9e\u73b0\u66f4\u7075\u6d3b\u548c\u77e5\u60c5\u7684\u7814\u53d1\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "LLMs\u663e\u8457\u63d0\u9ad8\u4e86\u7814\u53d1\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u52a0\u901f\u4e86\u521b\u65b0\u5468\u671f\uff0c\u7f29\u77ed\u4e86\u7a81\u7834\u6027\u60f3\u6cd5\u7684\u4e0a\u5e02\u65f6\u95f4\u3002", "conclusion": "LLMs\u901a\u8fc7\u591a\u91cd\u529f\u80fd\u8f6c\u53d8\u7814\u53d1\u8fc7\u7a0b\uff0c\u4e3a\u521b\u65b0\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u5f3a\u5927\u7684\u652f\u6301\uff0c\u663e\u8457\u63d0\u5347\u7814\u53d1\u6548\u7387\u548c\u521b\u65b0\u901f\u5ea6\u3002"}}
