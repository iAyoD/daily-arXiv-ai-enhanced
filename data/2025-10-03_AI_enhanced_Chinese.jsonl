{"id": "2510.01348", "categories": ["cs.RO", "x", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.01348", "abs": "https://arxiv.org/abs/2510.01348", "authors": ["Michal Werner", "David \u010capek", "Tom\u00e1\u0161 Musil", "Ond\u0159ej Fran\u011bk", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge", "comment": "8 pages", "summary": "Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied\nenvironments is challenging: integrating odometry leads to drift, loop closures\nare unavailable in previously unseen areas and embedded platforms provide\nlimited computational power. We present a fully onboard UAV system developed\nfor the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km\nlong-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS\nor prior dense mapping. The system integrates perception, mapping, planning,\nand control with a lightweight drift-correction method that matches\nLiDAR-derived local heightmaps to a prior geo-data heightmap via\ngradient-template matching and fuses the evidence with odometry in a clustered\nparticle filter. Deployed during the competition, the system executed\nkilometer-scale flights across urban, forest, and open-field terrain and\nreduced drift substantially relative to raw odometry, while running in real\ntime on CPU-only hardware. We describe the system architecture, the\nlocalization pipeline, and the competition evaluation, and we report practical\ninsights from field deployment that inform the design of GNSS-denied UAV\nautonomy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728GNSS\u62d2\u6b62\u73af\u5883\u4e0b\u5b9e\u73b0\u65e0\u4eba\u673a\u957f\u8ddd\u79bb\u81ea\u4e3b\u98de\u884c\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7LiDAR\u9ad8\u5ea6\u56fe\u5339\u914d\u548c\u7c92\u5b50\u6ee4\u6ce2\u6765\u6821\u6b63\u91cc\u7a0b\u8ba1\u6f02\u79fb\uff0c\u5728CPU\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u6210\u529f\u5b8c\u6210\u5343\u7c73\u7ea7\u98de\u884c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3GNSS\u62d2\u6b62\u73af\u5883\u4e0b\u65e0\u4eba\u673a\u957f\u8ddd\u79bb\u98de\u884c\u7684\u6311\u6218\uff1a\u91cc\u7a0b\u8ba1\u96c6\u6210\u4f1a\u5bfc\u81f4\u6f02\u79fb\uff0c\u672a\u77e5\u533a\u57df\u65e0\u6cd5\u4f7f\u7528\u95ed\u73af\u68c0\u6d4b\uff0c\u5d4c\u5165\u5f0f\u5e73\u53f0\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u3002", "method": "\u96c6\u6210\u611f\u77e5\u3001\u5efa\u56fe\u3001\u89c4\u5212\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u6f02\u79fb\u6821\u6b63\u65b9\u6cd5\uff1a\u901a\u8fc7\u68af\u5ea6\u6a21\u677f\u5339\u914d\u5c06LiDAR\u5c40\u90e8\u9ad8\u5ea6\u56fe\u4e0e\u5148\u9a8c\u5730\u7406\u6570\u636e\u9ad8\u5ea6\u56fe\u5339\u914d\uff0c\u5e76\u5728\u805a\u7c7b\u7c92\u5b50\u6ee4\u6ce2\u4e2d\u5c06\u8bc1\u636e\u4e0e\u91cc\u7a0b\u8ba1\u878d\u5408\u3002", "result": "\u5728\u7ade\u8d5b\u4e2d\u6210\u529f\u6267\u884c\u4e86\u5343\u7c73\u7ea7\u98de\u884c\uff0c\u8986\u76d6\u57ce\u5e02\u3001\u68ee\u6797\u548c\u5f00\u9614\u5730\u5e26\u5730\u5f62\uff0c\u76f8\u5bf9\u4e8e\u539f\u59cb\u91cc\u7a0b\u8ba1\u663e\u8457\u51cf\u5c11\u4e86\u6f02\u79fb\uff0c\u5728\u4ec5\u4f7f\u7528CPU\u7684\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3aGNSS\u62d2\u6b62\u73af\u5883\u4e0b\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u53ef\u9760\u957f\u8ddd\u79bb\u98de\u884c\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.01357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01357", "abs": "https://arxiv.org/abs/2510.01357", "authors": ["Alejandro Gonzalez-Garcia", "Wei Xiao", "Wei Wang", "Alejandro Astudillo", "Wilm Decr\u00e9", "Jan Swevers", "Carlo Ratti", "Daniela Rus"], "title": "Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels", "comment": "IROS 2025", "summary": "Safe motion planning is essential for autonomous vessel operations,\nespecially in challenging spaces such as narrow inland waterways. However,\nconventional motion planning approaches are often computationally intensive or\noverly conservative. This paper proposes a safe motion planning strategy\ncombining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).\nWe introduce a time-varying inflated ellipse obstacle representation, where the\ninflation radius is adjusted depending on the relative position and attitude\nbetween the vessel and the obstacle. The proposed adaptive inflation reduces\nthe conservativeness of the controller compared to traditional fixed-ellipsoid\nobstacle formulations. The MPC solution provides an approximate motion plan,\nand high-order CBFs ensure the vessel's safety using the varying inflation\nradius. Simulation and real-world experiments demonstrate that the proposed\nstrategy enables the fully-actuated autonomous robot vessel to navigate through\nnarrow spaces in real time and resolve potential deadlocks, all while ensuring\nsafety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u7528\u4e8e\u81ea\u4e3b\u8239\u8236\u5728\u72ed\u7a84\u5185\u6cb3\u6c34\u9053\u7684\u5bfc\u822a\u3002\u901a\u8fc7\u65f6\u53d8\u81a8\u80c0\u692d\u5706\u969c\u788d\u7269\u8868\u793a\uff0c\u6839\u636e\u8239\u8236\u4e0e\u969c\u788d\u7269\u7684\u76f8\u5bf9\u4f4d\u7f6e\u548c\u59ff\u6001\u81ea\u9002\u5e94\u8c03\u6574\u81a8\u80c0\u534a\u5f84\uff0c\u51cf\u5c11\u63a7\u5236\u5668\u7684\u4fdd\u5b88\u6027\u3002", "motivation": "\u81ea\u4e3b\u8239\u8236\u5728\u72ed\u7a84\u5185\u6cb3\u6c34\u9053\u7b49\u6311\u6218\u6027\u7a7a\u95f4\u4e2d\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u91cf\u5927\u6216\u8fc7\u4e8e\u4fdd\u5b88\u3002", "method": "\u7ed3\u5408MPC\u548cCBFs\uff0c\u4f7f\u7528\u65f6\u95f4\u53d8\u5316\u7684\u81a8\u80c0\u692d\u5706\u969c\u788d\u7269\u8868\u793a\uff0c\u6839\u636e\u76f8\u5bf9\u4f4d\u7f6e\u548c\u59ff\u6001\u81ea\u9002\u5e94\u8c03\u6574\u81a8\u80c0\u534a\u5f84\u3002MPC\u63d0\u4f9b\u8fd1\u4f3c\u8fd0\u52a8\u89c4\u5212\uff0c\u9ad8\u9636CBFs\u4f7f\u7528\u53d8\u5316\u7684\u81a8\u80c0\u534a\u5f84\u786e\u4fdd\u8239\u8236\u5b89\u5168\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u4f7f\u5168\u9a71\u52a8\u81ea\u4e3b\u673a\u5668\u4eba\u8239\u8236\u80fd\u591f\u5b9e\u65f6\u5bfc\u822a\u72ed\u7a84\u7a7a\u95f4\uff0c\u89e3\u51b3\u6f5c\u5728\u6b7b\u9501\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u81a8\u80c0\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u56fa\u5b9a\u692d\u5706\u969c\u788d\u7269\u8868\u793a\u51cf\u5c11\u4e86\u4fdd\u5b88\u6027\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u81ea\u4e3b\u8239\u8236\u5bfc\u822a\u3002"}}
{"id": "2510.01381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01381", "abs": "https://arxiv.org/abs/2510.01381", "authors": ["Spencer Teetaert", "Sven Lilge", "Jessica Burgner-Kahrs", "Timothy D. Barfoot"], "title": "A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots", "comment": "17 pages, 11 figures. Submitted to IEEE Transactions on Robotics", "summary": "State estimation techniques for continuum robots (CRs) typically involve\nusing computationally complex dynamic models, simplistic shape approximations,\nor are limited to quasi-static methods. These limitations can be sensitive to\nunmodelled disturbances acting on the robot. Inspired by a factor-graph\noptimization paradigm, this work introduces a continuous-time stochastic state\nestimation framework for continuum robots. We introduce factors based on\ncontinuous-time kinematics that are corrupted by a white noise Gaussian process\n(GP). By using a simple robot model paired with high-rate sensing, we show\nadaptability to unmodelled external forces and data dropout. The result\ncontains an estimate of the mean and covariance for the robot's pose, velocity,\nand strain, each of which can be interpolated continuously in time or space.\nThis same interpolation scheme can be used during estimation, allowing for\ninclusion of measurements on states that are not explicitly estimated. Our\nmethod's inherent sparsity leads to a linear solve complexity with respect to\ntime and interpolation queries in constant time. We demonstrate our method on a\nCR with gyroscope and pose sensors, highlighting its versatility in real-world\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u4f18\u5316\u7684\u8fde\u7eed\u65f6\u95f4\u968f\u673a\u72b6\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u80fd\u591f\u9002\u5e94\u672a\u5efa\u6a21\u7684\u5916\u90e8\u529b\u548c\u6570\u636e\u4e22\u5931\u3002", "motivation": "\u73b0\u6709\u7684\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u8ba1\u7b97\u590d\u6742\u7684\u52a8\u6001\u6a21\u578b\u3001\u7b80\u5316\u7684\u5f62\u72b6\u8fd1\u4f3c\u6216\u4ec5\u9650\u4e8e\u51c6\u9759\u6001\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u672a\u5efa\u6a21\u7684\u5e72\u6270\u5f88\u654f\u611f\u3002", "method": "\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u8fd0\u52a8\u5b66\u7684\u56e0\u5b50\u56fe\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u9ad8\u65af\u767d\u566a\u58f0\u8fc7\u7a0b\uff0c\u7ed3\u5408\u7b80\u5355\u673a\u5668\u4eba\u6a21\u578b\u548c\u9ad8\u9891\u4f20\u611f\u3002", "result": "\u80fd\u591f\u4f30\u8ba1\u673a\u5668\u4eba\u4f4d\u59ff\u3001\u901f\u5ea6\u548c\u5e94\u53d8\u7684\u5747\u503c\u548c\u534f\u65b9\u5dee\uff0c\u53ef\u5728\u65f6\u95f4\u6216\u7a7a\u95f4\u4e0a\u8fde\u7eed\u63d2\u503c\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u65f6\u95f4\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u9640\u87ba\u4eea\u548c\u4f4d\u59ff\u4f20\u611f\u5668\u7684\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2510.01388", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01388", "abs": "https://arxiv.org/abs/2510.01388", "authors": ["Arthur Zhang", "Xiangyun Meng", "Luca Calliari", "Dong-Ki Kim", "Shayegan Omidshafiei", "Joydeep Biswas", "Ali Agha", "Amirreza Shaban"], "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation", "comment": "9 pages, 6 figures, 3 tables", "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io", "AI": {"tldr": "VENTURA\u662f\u4e00\u4e2a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u5fae\u8c03\u4e92\u8054\u7f51\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8def\u5f84\u89c4\u5212\uff0c\u751f\u6210\u89c6\u89c9\u8def\u5f84\u63a9\u7801\uff0c\u518d\u7531\u8f7b\u91cf\u7ea7\u7b56\u7565\u6267\u884c\u8f68\u8ff9\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u591a\u6837\u5316\u673a\u5668\u4eba\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u96be\u4ee5\u6709\u6548\u8fc1\u79fb\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u52a8\u4f5c\u7a7a\u95f4\u5dee\u5f02\u548c\u9884\u8bad\u7ec3\u76ee\u6807\u4e0d\u5339\u914d\u963b\u788d\u4e86\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u8def\u5f84\u63a9\u7801\u4f5c\u4e3a\u89c6\u89c9\u89c4\u5212\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8ddf\u8e2a\u6a21\u578b\u548cVLM\u589e\u5f3a\u7684\u6807\u6ce8\u8fdb\u884c\u8bad\u7ec3\uff0c\u907f\u514d\u4eba\u5de5\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u7531\u8f7b\u91cf\u7ea7\u884c\u4e3a\u514b\u9686\u7b56\u7565\u5c06\u89c6\u89c9\u89c4\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cVENTURA\u5728\u7269\u4f53\u5230\u8fbe\u3001\u969c\u788d\u7269\u907f\u8ba9\u548c\u5730\u5f62\u504f\u597d\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\uff0c\u6210\u529f\u7387\u63d0\u9ad833%\uff0c\u78b0\u649e\u51cf\u5c1154%\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u7ec4\u5408\u3002", "conclusion": "VENTURA\u5c55\u793a\u4e86\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u673a\u5668\u4eba\u5bfc\u822a\u80fd\u529b\uff0c\u5177\u6709\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01389", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01389", "abs": "https://arxiv.org/abs/2510.01389", "authors": ["Ulas Berk Karli", "Ziyao Shangguan", "Tesca FItzgerald"], "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models show strong generalization\ncapabilities, yet they lack introspective mechanisms for anticipating failures\nand requesting help from a human supervisor. We present \\textbf{INSIGHT}, a\nlearning framework for leveraging token-level uncertainty signals to predict\nwhen a VLA should request help. Using $\\pi_0$-FAST as the underlying model, we\nextract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based\nestimates of \\emph{aleatoric and epistemic uncertainty}, and train compact\ntransformer classifiers to map these sequences to help triggers. We explore\nsupervision regimes for strong or weak supervision, and extensively compare\nthem across in-distribution and out-of-distribution tasks. Our results show a\ntrade-off: strong labels enable models to capture fine-grained uncertainty\ndynamics for reliable help detection, while weak labels, though noisier, still\nsupport competitive introspection when training and evaluation are aligned,\noffering a scalable path when dense annotation is impractical. Crucially, we\nfind that modeling the temporal evolution of token-level uncertainty signals\nwith transformers provides far greater predictive power than static\nsequence-level scores. This study provides the first systematic evaluation of\nuncertainty-based introspection in VLAs, opening future avenues for active\nlearning and for real-time error mitigation through selective human\nintervention.", "AI": {"tldr": "INSIGHT\u6846\u67b6\u5229\u7528token\u7ea7\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u9884\u6d4bVLA\u6a21\u578b\u4f55\u65f6\u9700\u8981\u8bf7\u6c42\u4eba\u7c7b\u5e2e\u52a9\uff0c\u901a\u8fc7\u8bad\u7ec3\u7d27\u51d1\u7684transformer\u5206\u7c7b\u5668\u6765\u6620\u5c04\u4e0d\u786e\u5b9a\u6027\u5e8f\u5217\u5230\u5e2e\u52a9\u89e6\u53d1\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u7f3a\u4e4f\u5185\u7701\u673a\u5236\u6765\u9884\u6d4b\u5931\u8d25\u5e76\u8bf7\u6c42\u4eba\u7c7b\u76d1\u7763\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4e3b\u52a8\u8bc6\u522b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u03c0\u2080-FAST\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u53d6token\u7ea7\u71b5\u3001\u5bf9\u6570\u6982\u7387\u4ee5\u53caDirichlet\u4f30\u8ba1\u7684\u4efb\u610f\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u8bad\u7ec3transformer\u5206\u7c7b\u5668\u6765\u6620\u5c04\u8fd9\u4e9b\u5e8f\u5217\u5230\u5e2e\u52a9\u89e6\u53d1\u4fe1\u53f7\uff0c\u63a2\u7d22\u5f3a\u76d1\u7763\u548c\u5f31\u76d1\u7763\u4e24\u79cd\u76d1\u7763\u673a\u5236\u3002", "result": "\u5f3a\u76d1\u7763\u6807\u7b7e\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u5e2e\u52a9\u68c0\u6d4b\uff0c\u5f31\u76d1\u7763\u6807\u7b7e\u867d\u7136\u566a\u58f0\u8f83\u5927\u4f46\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u5bf9\u9f50\u65f6\u4ecd\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5efa\u6a21token\u7ea7\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u7684\u65f6\u95f4\u6f14\u5316\u6bd4\u9759\u6001\u5e8f\u5217\u7ea7\u5206\u6570\u5177\u6709\u66f4\u5f3a\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8fd9\u662f\u5bf9VLA\u4e2d\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u5185\u7701\u673a\u5236\u7684\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e3a\u4e3b\u52a8\u5b66\u4e60\u548c\u901a\u8fc7\u9009\u62e9\u6027\u4eba\u7c7b\u5e72\u9884\u5b9e\u73b0\u5b9e\u65f6\u9519\u8bef\u7f13\u89e3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.01402", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01402", "abs": "https://arxiv.org/abs/2510.01402", "authors": ["Hun Kuk Park", "Taekyung Kim", "Dimitra Panagou"], "title": "Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions", "comment": "The first two authors contributed equally to this work. Project page:\n  https://www.taekyung.me/dpcbf", "summary": "Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety\nof autonomous systems, yet applying them to nonholonomic robots in cluttered,\ndynamic environments remains an open challenge. State-of-the-art methods often\nrely on collision-cone or velocity-obstacle constraints which, by only\nconsidering the angle of the relative velocity, are inherently conservative and\ncan render the CBF-based quadratic program infeasible, particularly in dense\nscenarios. To address this issue, we propose a Dynamic Parabolic Control\nBarrier Function (DPCBF) that defines the safe set using a parabolic boundary.\nThe parabola's vertex and curvature dynamically adapt based on both the\ndistance to an obstacle and the magnitude of the relative velocity, creating a\nless restrictive safety constraint. We prove that the proposed DPCBF is valid\nfor a kinematic bicycle model subject to input constraints. Extensive\ncomparative simulations demonstrate that our DPCBF-based controller\nsignificantly enhances navigation success rates and QP feasibility compared to\nbaseline methods. Our approach successfully navigates through dense\nenvironments with up to 100 dynamic obstacles, scenarios where collision\ncone-based methods fail due to infeasibility.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u629b\u7269\u7ebf\u63a7\u5236\u5c4f\u969c\u51fd\u6570(DPCBF)\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u629b\u7269\u7ebf\u8fb9\u754c\u6765\u89e3\u51b3\u975e\u5b8c\u6574\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2dCBF\u65b9\u6cd5\u4fdd\u5b88\u6027\u548c\u4e0d\u53ef\u884c\u6027\u95ee\u9898", "motivation": "\u73b0\u6709CBF\u65b9\u6cd5\u5728\u975e\u5b8c\u6574\u673a\u5668\u4eba\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2d\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u57fa\u4e8e\u78b0\u649e\u9525\u6216\u901f\u5ea6\u969c\u788d\u7684\u7ea6\u675f\u53ea\u8003\u8651\u76f8\u5bf9\u901f\u5ea6\u89d2\u5ea6\uff0c\u5bfc\u81f4QP\u95ee\u9898\u4e0d\u53ef\u884c", "method": "\u4f7f\u7528\u52a8\u6001\u629b\u7269\u7ebf\u8fb9\u754c\u5b9a\u4e49\u5b89\u5168\u96c6\uff0c\u629b\u7269\u7ebf\u9876\u70b9\u548c\u66f2\u7387\u6839\u636e\u969c\u788d\u7269\u8ddd\u79bb\u548c\u76f8\u5bf9\u901f\u5ea6\u5927\u5c0f\u52a8\u6001\u8c03\u6574\uff0c\u521b\u5efa\u9650\u5236\u66f4\u5c11\u7684\u5b89\u5168\u7ea6\u675f", "result": "DPCBF\u63a7\u5236\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u548cQP\u53ef\u884c\u6027\uff0c\u5728100\u4e2a\u52a8\u6001\u969c\u788d\u7269\u7684\u5bc6\u96c6\u73af\u5883\u4e2d\u6210\u529f\u5bfc\u822a\uff0c\u800c\u57fa\u4e8e\u78b0\u649e\u9525\u7684\u65b9\u6cd5\u56e0\u4e0d\u53ef\u884c\u6027\u800c\u5931\u8d25", "conclusion": "DPCBF\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5b8c\u6574\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u53ef\u884c\u6027"}}
{"id": "2510.01404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01404", "abs": "https://arxiv.org/abs/2510.01404", "authors": ["Lexi Foland", "Thomas Cohn", "Adam Wei", "Nicholas Pfaff", "Boyuan Chen", "Russ Tedrake"], "title": "How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?", "comment": "Under review. 8 pages, 3 figures, 3 tables. Additional results\n  available at https://diffusion-learns-kinematic.github.io", "summary": "Diffusion policies have shown impressive results in robot imitation learning,\neven for tasks that require satisfaction of kinematic equality constraints.\nHowever, task performance alone is not a reliable indicator of the policy's\nability to precisely learn constraints in the training data. To investigate, we\nanalyze how well diffusion policies discover these manifolds with a case study\non a bimanual pick-and-place task that encourages fulfillment of a kinematic\nconstraint for success. We study how three factors affect trained policies:\ndataset size, dataset quality, and manifold curvature. Our experiments show\ndiffusion policies learn a coarse approximation of the constraint manifold with\nlearning affected negatively by decreases in both dataset size and quality. On\nthe other hand, the curvature of the constraint manifold showed inconclusive\ncorrelations with both constraint satisfaction and task success. A hardware\nevaluation verifies the applicability of our results in the real world. Project\nwebsite with additional results and visuals:\nhttps://diffusion-learns-kinematic.github.io", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u5b66\u4e60\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u53cc\u624b\u6293\u53d6\u653e\u7f6e\u4efb\u52a1\u5206\u6790\u6570\u636e\u96c6\u5927\u5c0f\u3001\u8d28\u91cf\u548c\u6d41\u5f62\u66f2\u7387\u5bf9\u7ea6\u675f\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u867d\u7136\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4efb\u52a1\u6027\u80fd\u4e0d\u80fd\u53ef\u9760\u53cd\u6620\u7b56\u7565\u7cbe\u786e\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u4e2d\u7ea6\u675f\u7684\u80fd\u529b\uff0c\u9700\u8981\u7814\u7a76\u6269\u6563\u7b56\u7565\u5982\u4f55\u53d1\u73b0\u8fd9\u4e9b\u7ea6\u675f\u6d41\u5f62\u3002", "method": "\u901a\u8fc7\u53cc\u624b\u6293\u53d6\u653e\u7f6e\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u4e09\u4e2a\u56e0\u7d20\u5bf9\u8bad\u7ec3\u7b56\u7565\u7684\u5f71\u54cd\uff1a\u6570\u636e\u96c6\u5927\u5c0f\u3001\u6570\u636e\u96c6\u8d28\u91cf\u548c\u6d41\u5f62\u66f2\u7387\u3002", "result": "\u6269\u6563\u7b56\u7565\u5b66\u4e60\u5230\u7ea6\u675f\u6d41\u5f62\u7684\u7c97\u7565\u8fd1\u4f3c\uff0c\u6570\u636e\u96c6\u5927\u5c0f\u548c\u8d28\u91cf\u7684\u964d\u4f4e\u5bf9\u5b66\u4e60\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u6d41\u5f62\u66f2\u7387\u4e0e\u7ea6\u675f\u6ee1\u8db3\u548c\u4efb\u52a1\u6210\u529f\u7684\u5173\u7cfb\u4e0d\u660e\u786e\u3002\u786c\u4ef6\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u7ed3\u679c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u6269\u6563\u7b56\u7565\u80fd\u591f\u5b66\u4e60\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u4f46\u5b66\u4e60\u6548\u679c\u53d7\u6570\u636e\u96c6\u7279\u5f81\u5f71\u54cd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6d41\u5f62\u66f2\u7387\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.01433", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01433", "abs": "https://arxiv.org/abs/2510.01433", "authors": ["Anukriti Singh", "Kasra Torshizi", "Khuzema Habib", "Kelin Yu", "Ruohan Gao", "Pratap Tokekar"], "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation", "comment": null, "summary": "Vision-based robot learning often relies on dense image or point-cloud\ninputs, which are computationally heavy and entangle irrelevant background\nfeatures. Existing keypoint-based approaches can focus on manipulation-centric\nfeatures and be lightweight, but either depend on manual heuristics or\ntask-coupled selection, limiting scalability and semantic understanding. To\naddress this, we propose AFFORD2ACT, an affordance-guided framework that\ndistills a minimal set of semantic 2D keypoints from a text prompt and a single\nimage. AFFORD2ACT follows a three-stage pipeline: affordance filtering,\ncategory-level keypoint construction, and transformer-based policy learning\nwith embedded gating to reason about the most relevant keypoints, yielding a\ncompact 38-dimensional state policy that can be trained in 15 minutes, which\nperforms well in real-time without proprioception or dense representations.\nAcross diverse real-world manipulation tasks, AFFORD2ACT consistently improves\ndata efficiency, achieving an 82% success rate on unseen objects, novel\ncategories, backgrounds, and distractors.", "AI": {"tldr": "AFFORD2ACT\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u6587\u672c\u63d0\u793a\u548c\u5355\u5f20\u56fe\u50cf\u4e2d\u63d0\u53d6\u8bed\u4e492D\u5173\u952e\u70b9\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u7b56\u7565\uff0c\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u5bc6\u96c6\u56fe\u50cf\u6216\u70b9\u4e91\u8f93\u5165\u8ba1\u7b97\u91cf\u5927\u3001\u5305\u542b\u65e0\u5173\u80cc\u666f\u7279\u5f81\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u542f\u53d1\u5f0f\u6216\u4efb\u52a1\u8026\u5408\u9009\u62e9\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u53ef\u64cd\u4f5c\u6027\u8fc7\u6ee4\uff1b2) \u7c7b\u522b\u7ea7\u5173\u952e\u70b9\u6784\u5efa\uff1b3) \u57fa\u4e8etransformer\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u5305\u542b\u5d4c\u5165\u5f0f\u95e8\u63a7\u673a\u5236\u6765\u63a8\u7406\u6700\u76f8\u5173\u7684\u5173\u952e\u70b9\uff0c\u751f\u6210\u7d27\u51d1\u768438\u7ef4\u72b6\u6001\u7b56\u7565\u3002", "result": "\u572815\u5206\u949f\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u65e0\u9700\u672c\u4f53\u611f\u77e5\u6216\u5bc6\u96c6\u8868\u793a\u5373\u53ef\u5b9e\u65f6\u826f\u597d\u8fd0\u884c\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u3001\u65b0\u7c7b\u522b\u3001\u80cc\u666f\u548c\u5e72\u6270\u7269\u4e0a\u8fbe\u523082%\u7684\u6210\u529f\u7387\u3002", "conclusion": "AFFORD2ACT\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u3001\u8bed\u4e49\u7406\u89e3\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01438", "abs": "https://arxiv.org/abs/2510.01438", "authors": ["Minglun Wei", "Xintong Yang", "Yu-Kun Lai", "Ze Ji"], "title": "Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation", "comment": "4 pages", "summary": "Robotic automation is accelerating scientific discovery by reducing manual\neffort in laboratory workflows. However, precise manipulation of powders\nremains challenging, particularly in tasks such as transport that demand\naccuracy and stability. We propose a trajectory optimisation framework for\npowder transport in laboratory settings, which integrates differentiable\nphysics simulation for accurate modelling of granular dynamics, low-dimensional\nskill-space parameterisation to reduce optimisation complexity, and a\ncurriculum-based strategy that progressively refines task competence over long\nhorizons. This formulation enables end-to-end optimisation of contact-rich\nrobot trajectories while maintaining stability and convergence efficiency.\nExperimental results demonstrate that the proposed method achieves superior\ntask success rates and stability compared to the reinforcement learning\nbaseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5b9e\u9a8c\u5ba4\u7c89\u672b\u8fd0\u8f93\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u7269\u7406\u6a21\u62df\u3001\u4f4e\u7ef4\u6280\u80fd\u7a7a\u95f4\u53c2\u6570\u5316\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u7aef\u5230\u7aef\u4f18\u5316\u3002", "motivation": "\u673a\u5668\u4eba\u81ea\u52a8\u5316\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\uff0c\u4f46\u7c89\u672b\u7cbe\u786e\u64cd\u4f5c\u4ecd\u5177\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u7684\u8fd0\u8f93\u4efb\u52a1\u4e2d\u3002", "method": "\u96c6\u6210\u53ef\u5fae\u5206\u7269\u7406\u6a21\u62df\u7cbe\u786e\u5efa\u6a21\u9897\u7c92\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u4f4e\u7ef4\u6280\u80fd\u7a7a\u95f4\u53c2\u6570\u5316\u964d\u4f4e\u4f18\u5316\u590d\u6742\u5ea6\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u9010\u6b65\u63d0\u5347\u957f\u65f6\u57df\u4efb\u52a1\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u63a5\u89e6\u4e30\u5bcc\u673a\u5668\u4eba\u8f68\u8ff9\u7684\u7aef\u5230\u7aef\u4f18\u5316\u3002"}}
{"id": "2510.01452", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01452", "abs": "https://arxiv.org/abs/2510.01452", "authors": ["Laura Connolly", "Tamas Ungi", "Adnan Munawar", "Anton Deguet", "Chris Yeung", "Russell H. Taylor", "Parvin Mousavi", "Gabor Fichtinger Keyvan Hashtrudi-Zaad"], "title": "Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery", "comment": null, "summary": "Purpose: Delineating tumor boundaries during breast-conserving surgery is\nchallenging as tumors are often highly mobile, non-palpable, and have\nirregularly shaped borders. To address these challenges, we introduce a\ncooperative robotic guidance system that applies haptic feedback for tumor\nlocalization. In this pilot study, we aim to assess if and how this system can\nbe successfully integrated into breast cancer care.\n  Methods: A small haptic robot is retrofitted with an electrocautery blade to\noperate as a cooperatively controlled surgical tool. Ultrasound and\nelectromagnetic navigation are used to identify the tumor boundaries and\nposition. A forbidden region virtual fixture is imposed when the surgical tool\ncollides with the tumor boundary. We conducted a study where users were asked\nto resect tumors from breast simulants both with and without the haptic\nguidance. We then assess the results of these simulated resections both\nqualitatively and quantitatively.\n  Results: Virtual fixture guidance is shown to improve resection margins. On\naverage, users find the task to be less mentally demanding, frustrating, and\neffort intensive when haptic feedback is available. We also discovered some\nunanticipated impacts on surgical workflow that will guide design adjustments\nand training protocol moving forward.\n  Conclusion: Our results suggest that virtual fixtures can help localize tumor\nboundaries in simulated breast-conserving surgery. Future work will include an\nextensive user study to further validate these results and fine-tune our\nguidance system.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u4e73\u817a\u764c\u4fdd\u4e73\u624b\u672f\u7684\u534f\u4f5c\u673a\u5668\u4eba\u5f15\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u5e2e\u52a9\u5916\u79d1\u533b\u751f\u5b9a\u4f4d\u80bf\u7624\u8fb9\u754c\uff0c\u5728\u6a21\u62df\u624b\u672f\u4e2d\u663e\u793a\u51fa\u6539\u5584\u5207\u9664\u8fb9\u7f18\u548c\u964d\u4f4e\u624b\u672f\u96be\u5ea6\u7684\u6548\u679c\u3002", "motivation": "\u5728\u4fdd\u4e73\u624b\u672f\u4e2d\uff0c\u7531\u4e8e\u80bf\u7624\u5177\u6709\u9ad8\u5ea6\u79fb\u52a8\u6027\u3001\u4e0d\u53ef\u89e6\u53ca\u6027\u548c\u4e0d\u89c4\u5219\u8fb9\u754c\uff0c\u51c6\u786e\u5b9a\u4f4d\u80bf\u7624\u8fb9\u754c\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5e2e\u52a9\u5916\u79d1\u533b\u751f\u7cbe\u786e\u8bc6\u522b\u548c\u5207\u9664\u80bf\u7624\u7684\u6280\u672f\u3002", "method": "\u5c06\u5c0f\u578b\u89e6\u89c9\u673a\u5668\u4eba\u6539\u88c5\u4e3a\u7535\u5207\u5200\uff0c\u7ed3\u5408\u8d85\u58f0\u548c\u7535\u78c1\u5bfc\u822a\u8bc6\u522b\u80bf\u7624\u8fb9\u754c\uff0c\u5f53\u624b\u672f\u5de5\u5177\u4e0e\u80bf\u7624\u8fb9\u754c\u78b0\u649e\u65f6\u65bd\u52a0\u7981\u6b62\u533a\u57df\u865a\u62df\u7ea6\u675f\u3002\u901a\u8fc7\u5bf9\u6bd4\u6709\u65e0\u89e6\u89c9\u5f15\u5bfc\u7684\u6a21\u62df\u5207\u9664\u5b9e\u9a8c\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u865a\u62df\u7ea6\u675f\u5f15\u5bfc\u6539\u5584\u4e86\u5207\u9664\u8fb9\u7f18\uff0c\u7528\u6237\u5728\u4f7f\u7528\u89e6\u89c9\u53cd\u9988\u65f6\u611f\u89c9\u4efb\u52a1\u7684\u5fc3\u7406\u9700\u6c42\u3001\u632b\u8d25\u611f\u548c\u52aa\u529b\u7a0b\u5ea6\u90fd\u8f83\u4f4e\u3002\u540c\u65f6\u53d1\u73b0\u4e86\u4e00\u4e9b\u5bf9\u624b\u672f\u6d41\u7a0b\u7684\u610f\u5916\u5f71\u54cd\u3002", "conclusion": "\u865a\u62df\u7ea6\u675f\u6280\u672f\u6709\u52a9\u4e8e\u5728\u6a21\u62df\u4fdd\u4e73\u624b\u672f\u4e2d\u5b9a\u4f4d\u80bf\u7624\u8fb9\u754c\u3002\u672a\u6765\u5c06\u8fdb\u884c\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u7814\u7a76\u6765\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u7ed3\u679c\u5e76\u4f18\u5316\u5f15\u5bfc\u7cfb\u7edf\u3002"}}
{"id": "2510.01483", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01483", "abs": "https://arxiv.org/abs/2510.01483", "authors": ["Mohamad Al Mdfaa", "Svetlana Lukina", "Timur Akhtyamov", "Arthur Nigmatzyanov", "Dmitrii Nalberskii", "Sergey Zagoruyko", "Gonzalo Ferrer"], "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Vision-language models (VLMs) have shown potential for robot navigation but\nencounter fundamental limitations: they lack persistent scene memory, offer\nlimited spatial reasoning, and do not scale effectively with video duration for\nreal-time application. We present VL-KnG, a Visual Scene Understanding system\nthat tackles these challenges using spatiotemporal knowledge graph construction\nand computationally efficient query processing for navigation goal\nidentification. Our approach processes video sequences in chunks utilizing\nmodern VLMs, creates persistent knowledge graphs that maintain object identity\nover time, and enables explainable spatial reasoning through queryable graph\nstructures. We also introduce WalkieKnowledge, a new benchmark with about 200\nmanually annotated questions across 8 diverse trajectories spanning\napproximately 100 minutes of video data, enabling fair comparison between\nstructured approaches and general-purpose VLMs. Real-world deployment on a\ndifferential drive robot demonstrates practical applicability, with our method\nachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5\nPro performance while providing explainable reasoning supported by the\nknowledge graph, computational efficiency for real-time deployment across\ndifferent tasks, such as localization, navigation and planning. Code and\ndataset will be released after acceptance.", "AI": {"tldr": "VL-KnG\u662f\u4e00\u4e2a\u89c6\u89c9\u573a\u666f\u7406\u89e3\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u65f6\u7a7a\u77e5\u8bc6\u56fe\u8c31\u6765\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u7f3a\u4e4f\u6301\u4e45\u573a\u666f\u8bb0\u5fc6\u3001\u6709\u9650\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u5b9e\u65f6\u5e94\u7528\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u5b58\u5728\u4e09\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u7f3a\u4e4f\u6301\u4e45\u573a\u666f\u8bb0\u5fc6\u3001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u3001\u65e0\u6cd5\u6709\u6548\u6269\u5c55\u5230\u5b9e\u65f6\u89c6\u9891\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6301\u7eed\u8ddf\u8e2a\u7269\u4f53\u8eab\u4efd\u3001\u652f\u6301\u53ef\u89e3\u91ca\u7a7a\u95f4\u63a8\u7406\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5757\u5904\u7406\u89c6\u9891\u5e8f\u5217\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u6301\u4e45\u77e5\u8bc6\u56fe\u8c31\uff0c\u4fdd\u6301\u7269\u4f53\u8eab\u4efd\u968f\u65f6\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u67e5\u8be2\u7684\u56fe\u7ed3\u6784\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u63a8\u7406\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5dee\u5206\u9a71\u52a8\u673a\u5668\u4eba\u4e0a\u7684\u90e8\u7f72\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u523077.27%\u7684\u6210\u529f\u7387\u548c76.92%\u7684\u7b54\u6848\u51c6\u786e\u7387\uff0c\u4e0eGemini 2.5 Pro\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u5e76\u5728\u5b9a\u4f4d\u3001\u5bfc\u822a\u548c\u89c4\u5212\u7b49\u4e0d\u540c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u8ba1\u7b97\u9ad8\u6548\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "VL-KnG\u901a\u8fc7\u65f6\u7a7a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u63a8\u7406\u3001\u8ba1\u7b97\u9ad8\u6548\u6027\u548c\u5b9e\u65f6\u90e8\u7f72\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01485", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01485", "abs": "https://arxiv.org/abs/2510.01485", "authors": ["Nicholas B. Andrews", "Yanhao Yang", "Sofya Akhetova", "Kristi A. Morgansen", "Ross L. Hatton"], "title": "Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot", "comment": "8 pages, 8 figures, 3 tables", "summary": "This work demonstrates pose (position and shape) estimation for a\nfree-floating, bioinspired multi-link robot with unactuated joints,\nlink-mounted thrusters for control, and a single gyroscope per link, resulting\nin an underactuated, minimally sensed platform. Through a proof-of-concept\nhardware experiment and offline Kalman filter analysis, we show that the\nrobot's pose can be reliably estimated. State estimation is performed using an\nunscented Kalman filter augmented with Gaussian process residual learning to\ncompensate for non-zero-mean, non-Gaussian noise. We further show that a filter\ntrained on a multi-gait dataset (forward, backward, left, right, and turning)\nperforms comparably to one trained on a larger forward-gait-only dataset when\nboth are evaluated on the same forward-gait test trajectory. These results\nreveal overlap in the gait input space, which can be exploited to reduce\ntraining data requirements while enhancing the filter's generalizability across\nmultiple gaits.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u6b8b\u5dee\u5b66\u4e60\uff0c\u5bf9\u5177\u6709\u975e\u9a71\u52a8\u5173\u8282\u3001\u5355\u9640\u87ba\u4eea\u611f\u77e5\u7684\u81ea\u7531\u6d6e\u52a8\u4eff\u751f\u591a\u8fde\u6746\u673a\u5668\u4eba\u8fdb\u884c\u4f4d\u59ff\u4f30\u8ba1\uff0c\u8bc1\u660e\u4e86\u5728\u591a\u6b65\u6001\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u4e0e\u4ec5\u5728\u524d\u8fdb\u6b65\u6001\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u81ea\u7531\u6d6e\u52a8\u4eff\u751f\u591a\u8fde\u6746\u673a\u5668\u4eba\u7684\u4f4d\u59ff\u4f30\u8ba1\u95ee\u9898\uff0c\u8be5\u5e73\u53f0\u5177\u6709\u975e\u9a71\u52a8\u5173\u8282\u3001\u6700\u5c0f\u5316\u611f\u77e5\u80fd\u529b\uff08\u6bcf\u8fde\u6746\u4ec5\u4e00\u4e2a\u9640\u87ba\u4eea\uff09\u548c\u6b20\u9a71\u52a8\u7279\u6027\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u6b8b\u5dee\u5b66\u4e60\u6765\u8865\u507f\u975e\u96f6\u5747\u503c\u3001\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u786c\u4ef6\u5b9e\u9a8c\u548c\u79bb\u7ebf\u5361\u5c14\u66fc\u6ee4\u6ce2\u5206\u6790\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u673a\u5668\u4eba\u7684\u4f4d\u59ff\u53ef\u4ee5\u53ef\u9760\u4f30\u8ba1\uff0c\u5728\u591a\u6b65\u6001\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u4e0e\u5728\u66f4\u5927\u524d\u8fdb\u6b65\u6001\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u5728\u76f8\u540c\u6d4b\u8bd5\u8f68\u8ff9\u4e0a\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6b65\u6001\u8f93\u5165\u7a7a\u95f4\u7684\u91cd\u53e0\u6027\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u4e00\u7279\u6027\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u6ee4\u6ce2\u5668\u5728\u591a\u79cd\u6b65\u6001\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.01519", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01519", "abs": "https://arxiv.org/abs/2510.01519", "authors": ["Wei Han Chen", "Yuchen Liu", "Alexiy Buynitsky", "Ahmed H. Qureshi"], "title": "Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments", "comment": null, "summary": "Robot navigation in large, complex, and unknown indoor environments is a\nchallenging problem. The existing approaches, such as traditional\nsampling-based methods, struggle with resolution control and scalability, while\nimitation learning-based methods require a large amount of demonstration data.\nActive Neural Time Fields (ANTFields) have recently emerged as a promising\nsolution by using local observations to learn cost-to-go functions without\nrelying on demonstrations. Despite their potential, these methods are hampered\nby challenges such as spectral bias and catastrophic forgetting, which diminish\ntheir effectiveness in complex scenarios. To address these issues, our approach\ndecomposes the planning problem into a hierarchical structure. At the high\nlevel, a sparse graph captures the environment's global connectivity, while at\nthe low level, a planner based on neural fields navigates local obstacles by\nsolving the Eikonal PDE. This physics-informed strategy overcomes common\npitfalls like spectral bias and neural field fitting difficulties, resulting in\na smooth and precise representation of the cost landscape. We validate our\nframework in large-scale environments, demonstrating its enhanced adaptability\nand precision compared to previous methods, and highlighting its potential for\nonline exploration, mapping, and real-world navigation.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u89c4\u5212\u65b9\u6cd5\u89e3\u51b3\u673a\u5668\u4eba\u5bfc\u822a\u95ee\u9898\uff0c\u9ad8\u5c42\u7528\u7a00\u758f\u56fe\u6355\u6349\u5168\u5c40\u8fde\u901a\u6027\uff0c\u4f4e\u5c42\u7528\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u89c4\u5212\u5668\u89e3\u51b3Eikonal PDE\uff0c\u514b\u670d\u8c31\u504f\u5dee\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u96be\u4ee5\u63a7\u5236\u5206\u8fa8\u7387\u548c\u6269\u5c55\u6027\uff0c\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6f14\u793a\u6570\u636e\uff0cANTFields\u65b9\u6cd5\u53d7\u8c31\u504f\u5dee\u548c\u707e\u96be\u6027\u9057\u5fd8\u5f71\u54cd\u3002", "method": "\u5206\u5c42\u89c4\u5212\u7ed3\u6784\uff1a\u9ad8\u5c42\u4f7f\u7528\u7a00\u758f\u56fe\u8868\u793a\u5168\u5c40\u73af\u5883\u8fde\u901a\u6027\uff0c\u4f4e\u5c42\u4f7f\u7528\u795e\u7ecf\u573a\u89c4\u5212\u5668\u901a\u8fc7\u6c42\u89e3Eikonal PDE\u6765\u5bfc\u822a\u5c40\u90e8\u969c\u788d\u7269\u3002", "result": "\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u5177\u6709\u5728\u7ebf\u63a2\u7d22\u3001\u5efa\u56fe\u548c\u5b9e\u9645\u5bfc\u822a\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.01592", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01592", "abs": "https://arxiv.org/abs/2510.01592", "authors": ["Shun Niijima", "Ryoichi Tsuzaki", "Noriaki Takasugi", "Masaya Kinoshita"], "title": "Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion", "comment": "8 pages, 12 figures, This work has been submitted to the IEEE for\n  possible publication. Copyright may be transfered without notice, after which\n  this version may no longer be accessible", "summary": "This paper proposes a real-time multi-plane segmentation method based on\nGPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.\nExisting online planar mapping approaches struggle to balance accuracy and\ncomputational efficiency: direct depth image segmentation from specific sensors\nsuffers from poor temporal integration, height map-based methods cannot\nrepresent complex 3D structures like overhangs, and voxel-based plane\nsegmentation remains unexplored for real-time applications. To address these\nlimitations, we develop a novel framework that integrates vertex-based\nconnected component labeling with random sample consensus based plane detection\nand convex hull, leveraging GPU parallel computing to rapidly extract planar\nregions from point clouds accumulated in high-resolution 3D voxel maps.\nExperimental results demonstrate that the proposed method achieves fast and\naccurate 3D multi-plane segmentation at over 30 Hz update rate even at a\nresolution of 0.01 m, enabling the detected planes to be utilized in real time\nfor locomotion tasks. Furthermore, we validate the effectiveness of our\napproach through experiments in both simulated environments and physical legged\nrobot platforms, confirming robust locomotion performance when considering 3D\nplanar structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u9ad8\u5206\u8fa8\u73873D\u4f53\u7d20\u6620\u5c04\u7684\u5b9e\u65f6\u591a\u5e73\u9762\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u80fd\u591f\u572830Hz\u66f4\u65b0\u7387\u4e0b\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u76843D\u591a\u5e73\u9762\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u5e73\u9762\u6620\u5c04\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff1a\u76f4\u63a5\u6df1\u5ea6\u56fe\u50cf\u5206\u5272\u7f3a\u4e4f\u65f6\u95f4\u6574\u5408\uff0c\u9ad8\u5ea6\u56fe\u65b9\u6cd5\u65e0\u6cd5\u8868\u793a\u590d\u67423D\u7ed3\u6784\uff0c\u4f53\u7d20\u5e73\u9762\u5206\u5272\u5c1a\u672a\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u9876\u70b9\u7684\u8fde\u901a\u5206\u91cf\u6807\u8bb0\u4e0eRANSAC\u5e73\u9762\u68c0\u6d4b\u548c\u51f8\u5305\u76f8\u7ed3\u5408\uff0c\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\u4ece\u9ad8\u5206\u8fa8\u73873D\u4f53\u7d20\u56fe\u4e2d\u5feb\u901f\u63d0\u53d6\u5e73\u9762\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u57280.01\u7c73\u5206\u8fa8\u7387\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e5f\u80fd\u4ee5\u8d85\u8fc730Hz\u7684\u66f4\u65b0\u7387\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u76843D\u591a\u5e73\u9762\u5206\u5272\uff0c\u4f7f\u68c0\u6d4b\u5230\u7684\u5e73\u9762\u80fd\u591f\u5b9e\u65f6\u7528\u4e8e\u8fd0\u52a8\u4efb\u52a1\u3002", "conclusion": "\u5728\u6a21\u62df\u73af\u5883\u548c\u7269\u7406\u817f\u5f0f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u786e\u8ba4\u4e86\u5728\u8003\u86513D\u5e73\u9762\u7ed3\u6784\u65f6\u7684\u9c81\u68d2\u8fd0\u52a8\u6027\u80fd\u3002"}}
{"id": "2510.01603", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01603", "abs": "https://arxiv.org/abs/2510.01603", "authors": ["Sharfin Islam", "Zewen Chen", "Zhanpeng He", "Swapneel Bhatt", "Andres Permuy", "Brock Taylor", "James Vickery", "Pedro Piacenza", "Cheng Zhang", "Matei Ciocarlie"], "title": "MiniBEE: A New Form Factor for Compact Bimanual Dexterity", "comment": null, "summary": "Bimanual robot manipulators can achieve impressive dexterity, but typically\nrely on two full six- or seven- degree-of-freedom arms so that paired grippers\ncan coordinate effectively. This traditional framework increases system\ncomplexity while only exploiting a fraction of the overall workspace for\ndexterous interaction. We introduce the MiniBEE (Miniature Bimanual\nEnd-effector), a compact system in which two reduced-mobility arms (3+ DOF\neach) are coupled into a kinematic chain that preserves full relative\npositioning between grippers. To guide our design, we formulate a kinematic\ndexterity metric that enlarges the dexterous workspace while keeping the\nmechanism lightweight and wearable. The resulting system supports two\ncomplementary modes: (i) wearable kinesthetic data collection with self-tracked\ngripper poses, and (ii) deployment on a standard robot arm, extending dexterity\nacross its entire workspace. We present kinematic analysis and design\noptimization methods for maximizing dexterous range, and demonstrate an\nend-to-end pipeline in which wearable demonstrations train imitation learning\npolicies that perform robust, real-world bimanual manipulation.", "AI": {"tldr": "\u63d0\u51faMiniBEE\u7cfb\u7edf\uff0c\u5c06\u4e24\u4e2a\u4f4e\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u8026\u5408\u4e3a\u7d27\u51d1\u7684\u53cc\u624b\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\uff0c\u652f\u6301\u7a7f\u6234\u5f0f\u6570\u636e\u91c7\u96c6\u548c\u6807\u51c6\u673a\u68b0\u81c2\u90e8\u7f72\uff0c\u5b9e\u73b0\u5168\u5de5\u4f5c\u7a7a\u95f4\u7684\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u53cc\u624b\u673a\u5668\u4eba\u7cfb\u7edf\u590d\u6742\u4e14\u4ec5\u5229\u7528\u90e8\u5206\u5de5\u4f5c\u7a7a\u95f4\u8fdb\u884c\u7075\u5de7\u64cd\u4f5c\uff0c\u9700\u8981\u66f4\u7d27\u51d1\u3001\u8f7b\u91cf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e24\u4e2a\u4f4e\u81ea\u7531\u5ea6\u673a\u68b0\u81c2(3+\u81ea\u7531\u5ea6)\u8026\u5408\u7684\u7d27\u51d1\u7cfb\u7edf\uff0c\u5236\u5b9a\u8fd0\u52a8\u5b66\u7075\u5de7\u5ea6\u6307\u6807\uff0c\u652f\u6301\u7a7f\u6234\u5f0f\u6570\u636e\u91c7\u96c6\u548c\u6807\u51c6\u673a\u68b0\u81c2\u90e8\u7f72\u4e24\u79cd\u6a21\u5f0f\u3002", "result": "\u5f00\u53d1\u51fa\u8f7b\u91cf\u5316\u53ef\u7a7f\u6234\u7cfb\u7edf\uff0c\u80fd\u591f\u8fdb\u884c\u7a7f\u6234\u5f0f\u6f14\u793a\u6570\u636e\u91c7\u96c6\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u7a33\u5065\u7684\u5b9e\u65f6\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "conclusion": "MiniBEE\u7cfb\u7edf\u901a\u8fc7\u7d27\u51d1\u8bbe\u8ba1\u6269\u5c55\u4e86\u53cc\u624b\u673a\u5668\u4eba\u7684\u7075\u5de7\u5de5\u4f5c\u7a7a\u95f4\uff0c\u4e3a\u7a7f\u6234\u5f0f\u6570\u636e\u91c7\u96c6\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01607", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01607", "abs": "https://arxiv.org/abs/2510.01607", "authors": ["Qiyuan Zeng", "Chengmeng Li", "Jude St. John", "Zhongyi Zhou", "Junjie Wen", "Guorui Feng", "Yichen Zhu", "Yi Xu"], "title": "ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations", "comment": "technique report. The website is available at\n  https://activeumi.github.io", "summary": "We present ActiveUMI, a framework for a data collection system that transfers\nin-the-wild human demonstrations to robots capable of complex bimanual\nmanipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized\ncontrollers that mirror the robot's end-effectors, bridging human-robot\nkinematics via precise pose alignment. To ensure mobility and data quality, we\nintroduce several key techniques, including immersive 3D model rendering, a\nself-contained wearable computer, and efficient calibration methods.\nActiveUMI's defining feature is its capture of active, egocentric perception.\nBy recording an operator's deliberate head movements via a head-mounted\ndisplay, our system learns the crucial link between visual attention and\nmanipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies\ntrained exclusively on ActiveUMI data achieve an average success rate of 70\\%\non in-distribution tasks and demonstrate strong generalization, retaining a\n56\\% success rate when tested on novel objects and in new environments. Our\nresults demonstrate that portable data collection systems, when coupled with\nlearned active perception, provide an effective and scalable pathway toward\ncreating generalizable and highly capable real-world robot policies.", "AI": {"tldr": "ActiveUMI\u662f\u4e00\u4e2a\u4fbf\u643a\u5f0fVR\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u6355\u6349\u64cd\u4f5c\u8005\u7684\u4e3b\u52a8\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u6765\u6536\u96c6\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\uff0c\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6536\u96c6\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u4fbf\u643a\u4e14\u80fd\u6355\u6349\u4eba\u7c7b\u4e3b\u52a8\u611f\u77e5\u7684\u6570\u636e\u6536\u96c6\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u4fbf\u643a\u5f0fVR\u9065\u64cd\u4f5c\u5957\u4ef6\u548c\u4f20\u611f\u5668\u5316\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7cbe\u786e\u59ff\u6001\u5bf9\u9f50\u6865\u63a5\u4eba\u673a\u8fd0\u52a8\u5b66\uff0c\u5f15\u5165\u6c89\u6d78\u5f0f3D\u6a21\u578b\u6e32\u67d3\u3001\u81ea\u5305\u542b\u53ef\u7a7f\u6234\u8ba1\u7b97\u673a\u548c\u9ad8\u6548\u6807\u5b9a\u65b9\u6cd5\uff0c\u5e76\u8bb0\u5f55\u64cd\u4f5c\u8005\u7684\u4e3b\u52a8\u5934\u90e8\u8fd0\u52a8\u6765\u5b66\u4e60\u89c6\u89c9\u6ce8\u610f\u529b\u4e0e\u64cd\u4f5c\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u5728\u516d\u4e2a\u6311\u6218\u6027\u53cc\u624b\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u4ec5\u4f7f\u7528ActiveUMI\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u5206\u5e03\u5185\u4efb\u52a1\u4e0a\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523070%\uff0c\u5728\u65b0\u7269\u4f53\u548c\u65b0\u73af\u5883\u4e2d\u4ecd\u4fdd\u630156%\u7684\u6210\u529f\u7387\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4fbf\u643a\u5f0f\u6570\u636e\u6536\u96c6\u7cfb\u7edf\u4e0e\u5b66\u4e60\u7684\u4e3b\u52a8\u611f\u77e5\u76f8\u7ed3\u5408\uff0c\u4e3a\u521b\u5efa\u53ef\u6cdb\u5316\u4e14\u9ad8\u80fd\u529b\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2510.01642", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01642", "abs": "https://arxiv.org/abs/2510.01642", "authors": ["Zijun Lin", "Jiafei Duan", "Haoquan Fang", "Dieter Fox", "Ranjay Krishna", "Cheston Tan", "Bihan Wen"], "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models", "comment": "Project Page: https://jimntu.github.io/FailSafe/", "summary": "Recent advances in robotic manipulation have integrated low-level robotic\ncontrol into Vision-Language Models (VLMs), extending them into\nVision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve\nstrong performance in downstream robotic applications, supported by large-scale\ncrowd-sourced robot training data, they still inevitably encounter failures\nduring execution. Enabling robots to reason about and recover from\nunpredictable and abrupt failures remains a critical challenge. Existing\nrobotic manipulation datasets, collected in either simulation or the real\nworld, primarily provide only ground-truth trajectories, leaving robots unable\nto recover once failures occur. Moreover, the few datasets that address failure\ndetection typically offer only textual explanations, which are difficult to\nutilize directly in VLA models. To address this gap, we introduce FailSafe, a\nnovel failure generation and recovery system that automatically produces\ndiverse failure cases paired with executable recovery actions. FailSafe can be\nseamlessly applied to any manipulation task in any simulator, enabling scalable\ncreation of failure-action data. To demonstrate its effectiveness, we fine-tune\nLLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results\nshow that FailSafe-VLM successfully helps robotic arm detect and recover from\npotential failures, improving the performance of three state-of-the-art VLA\nmodels pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several\ntasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different\nspatial configurations, camera viewpoints, and robotic embodiments. We plan to\nrelease the FailSafe code to the community.", "AI": {"tldr": "FailSafe\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u5931\u8d25\u751f\u6210\u548c\u6062\u590d\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u4ea7\u751f\u591a\u6837\u5316\u7684\u5931\u8d25\u6848\u4f8b\u548c\u53ef\u6267\u884c\u7684\u6062\u590d\u52a8\u4f5c\uff0c\u901a\u8fc7\u5fae\u8c03LLaVa-OneVision-7B\u6784\u5efaFailSafe-VLM\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u5931\u8d25\u68c0\u6d4b\u548c\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u96c6\u4e3b\u8981\u63d0\u4f9b\u5730\u9762\u5b9e\u51b5\u8f68\u8ff9\uff0c\u65e0\u6cd5\u5e2e\u52a9\u673a\u5668\u4eba\u4ece\u5931\u8d25\u4e2d\u6062\u590d\uff1b\u5c11\u6570\u5904\u7406\u5931\u8d25\u68c0\u6d4b\u7684\u6570\u636e\u96c6\u901a\u5e38\u53ea\u63d0\u4f9b\u6587\u672c\u89e3\u91ca\uff0c\u96be\u4ee5\u76f4\u63a5\u5728VLA\u6a21\u578b\u4e2d\u4f7f\u7528\u3002", "method": "\u63d0\u51faFailSafe\u7cfb\u7edf\uff0c\u53ef\u5728\u4efb\u4f55\u6a21\u62df\u5668\u4e2d\u65e0\u7f1d\u5e94\u7528\u4e8e\u4efb\u4f55\u64cd\u4f5c\u4efb\u52a1\uff0c\u5b9e\u73b0\u5931\u8d25-\u52a8\u4f5c\u6570\u636e\u7684\u53ef\u6269\u5c55\u521b\u5efa\uff0c\u5e76\u57fa\u4e8eLLaVa-OneVision-7B\u6784\u5efaFailSafe-VLM\u3002", "result": "FailSafe-VLM\u6210\u529f\u5e2e\u52a9\u673a\u68b0\u81c2\u68c0\u6d4b\u548c\u6062\u590d\u6f5c\u5728\u5931\u8d25\uff0c\u5728Maniskill\u591a\u4e2a\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u5347\u4e09\u4e2a\u6700\u5148\u8fdbVLA\u6a21\u578b\uff08pi0-FAST\u3001OpenVLA\u3001OpenVLA-OFT\uff09\u6027\u80fd\u8fbe22.6%\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u7a7a\u95f4\u914d\u7f6e\u3001\u76f8\u673a\u89c6\u89d2\u548c\u673a\u5668\u4eba\u5f62\u6001\u3002", "conclusion": "FailSafe\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5931\u8d25\u6062\u590d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.01648", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01648", "abs": "https://arxiv.org/abs/2510.01648", "authors": ["Seungwon Choi", "Donggyu Park", "Seo-Yeon Hwang", "Tae-Wan Kim"], "title": "Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation", "comment": null, "summary": "A fundamental challenge in robust visual-inertial odometry (VIO) is to\ndynamically assess the reliability of sensor measurements. This assessment is\ncrucial for properly weighting the contribution of each measurement to the\nstate estimate. Conventional methods often simplify this by assuming a static,\nuniform uncertainty for all measurements. This heuristic, however, may be\nlimited in its ability to capture the dynamic error characteristics inherent in\nreal-world data. To improve this limitation, we present a statistical framework\nthat learns measurement reliability assessment online, directly from sensor\ndata and optimization results. Our approach leverages multi-view geometric\nconsistency as a form of self-supervision. This enables the system to infer\nlandmark uncertainty and adaptively weight visual measurements during\noptimization. We evaluated our method on the public EuRoC dataset,\ndemonstrating improvements in tracking accuracy with average reductions of\napproximately 24\\% in translation error and 42\\% in rotation error compared to\nbaseline methods with fixed uncertainty parameters. The resulting framework\noperates in real time while showing enhanced accuracy and robustness. To\nfacilitate reproducibility and encourage further research, the source code will\nbe made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u5b66\u4e60\u6d4b\u91cf\u53ef\u9760\u6027\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u51e0\u4f55\u4e00\u81f4\u6027\u4f5c\u4e3a\u81ea\u76d1\u7763\uff0c\u52a8\u6001\u8bc4\u4f30\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e2d\u4f20\u611f\u5668\u6d4b\u91cf\u7684\u53ef\u9760\u6027\uff0c\u76f8\u6bd4\u56fa\u5b9a\u4e0d\u786e\u5b9a\u6027\u53c2\u6570\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u6d4b\u91cf\u5177\u6709\u9759\u6001\u3001\u5747\u5300\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u7684\u52a8\u6001\u8bef\u5dee\u7279\u6027\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528\u591a\u89c6\u56fe\u51e0\u4f55\u4e00\u81f4\u6027\u4f5c\u4e3a\u81ea\u76d1\u7763\uff0c\u4ece\u4f20\u611f\u5668\u6570\u636e\u548c\u4f18\u5316\u7ed3\u679c\u4e2d\u5728\u7ebf\u5b66\u4e60\u6d4b\u91cf\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u81ea\u9002\u5e94\u5730\u52a0\u6743\u89c6\u89c9\u6d4b\u91cf\u3002", "result": "\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u56fa\u5b9a\u4e0d\u786e\u5b9a\u6027\u53c2\u6570\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e73\u5747\u5e73\u79fb\u8bef\u5dee\u51cf\u5c11\u7ea624%\uff0c\u65cb\u8f6c\u8bef\u5dee\u51cf\u5c11\u7ea642%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u65f6\u8fd0\u884c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u6e90\u4ee3\u7801\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.01661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01661", "abs": "https://arxiv.org/abs/2510.01661", "authors": ["Yifei Simon Shao", "Yuchen Zheng", "Sunan Sun", "Pratik Chaudhari", "Vijay Kumar", "Nadia Figueroa"], "title": "Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation", "comment": "CoRL 2025 Learning Effective Abstractions for Planning (LEAP)\n  Workshop Best Paper Award (https://sites.google.com/view/symskill)", "summary": "Multi-step manipulation in dynamic environments remains challenging. Two\nmajor families of methods fail in distinct ways: (i) imitation learning (IL) is\nreactive but lacks compositional generalization, as monolithic policies do not\ndecide which skill to reuse when scenes change; (ii) classical task-and-motion\nplanning (TAMP) offers compositionality but has prohibitive planning latency,\npreventing real-time failure recovery. We introduce SymSkill, a unified\nlearning framework that combines the benefits of IL and TAMP, allowing\ncompositional generalization and failure recovery in real-time. Offline,\nSymSkill jointly learns predicates, operators, and skills directly from\nunlabeled and unsegmented demonstrations. At execution time, upon specifying a\nconjunction of one or more learned predicates, SymSkill uses a symbolic planner\nto compose and reorder learned skills to achieve the symbolic goals, while\nperforming recovery at both the motion and symbolic levels in real time.\nCoupled with a compliant controller, SymSkill enables safe and uninterrupted\nexecution under human and environmental disturbances. In RoboCasa simulation,\nSymSkill can execute 12 single-step tasks with 85% success rate. Without\nadditional data, it composes these skills into multi-step plans requiring up to\n6 skill recompositions, recovering robustly from execution failures. On a real\nFranka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented\nand unlabeled play data, is capable of performing multiple tasks simply by goal\nspecifications. The source code and additional analysis can be found on\nhttps://sites.google.com/view/symskill.", "AI": {"tldr": "SymSkill\u662f\u4e00\u4e2a\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u4efb\u52a1\u8fd0\u52a8\u89c4\u5212\u4f18\u52bf\u7684\u7edf\u4e00\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\u548c\u5b9e\u65f6\u6545\u969c\u6062\u590d\u3002", "motivation": "\u89e3\u51b3\u591a\u6b65\u64cd\u4f5c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6311\u6218\uff1a\u6a21\u4eff\u5b66\u4e60\u7f3a\u4e4f\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u4f20\u7edf\u4efb\u52a1\u8fd0\u52a8\u89c4\u5212\u5b58\u5728\u89c4\u5212\u5ef6\u8fdf\u95ee\u9898\uff0c\u65e0\u6cd5\u5b9e\u65f6\u6062\u590d\u6545\u969c\u3002", "method": "\u4ece\u65e0\u6807\u7b7e\u3001\u65e0\u5206\u5272\u7684\u6f14\u793a\u6570\u636e\u4e2d\u8054\u5408\u5b66\u4e60\u8c13\u8bcd\u3001\u64cd\u4f5c\u7b26\u548c\u6280\u80fd\uff1b\u6267\u884c\u65f6\u4f7f\u7528\u7b26\u53f7\u89c4\u5212\u5668\u7ec4\u5408\u548c\u91cd\u6392\u5e8f\u5b66\u4e60\u5230\u7684\u6280\u80fd\u6765\u5b9e\u73b0\u7b26\u53f7\u76ee\u6807\uff0c\u540c\u65f6\u5728\u8fd0\u52a8\u548c\u7b26\u53f7\u5c42\u9762\u8fdb\u884c\u5b9e\u65f6\u6062\u590d\u3002", "result": "\u5728RoboCasa\u6a21\u62df\u4e2d\uff0cSymSkill\u6267\u884c12\u4e2a\u5355\u6b65\u4efb\u52a1\u6210\u529f\u738785%\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u5373\u53ef\u7ec4\u5408\u6210\u6700\u591a\u9700\u89816\u6b21\u6280\u80fd\u91cd\u7ec4\u7684\u590d\u6742\u591a\u6b65\u8ba1\u5212\uff1b\u5728\u771f\u5b9eFranka\u673a\u5668\u4eba\u4e0a\uff0c\u4ec5\u75285\u5206\u949f\u65e0\u6807\u7b7e\u6f14\u793a\u6570\u636e\u5c31\u80fd\u901a\u8fc7\u76ee\u6807\u89c4\u8303\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u3002", "conclusion": "SymSkill\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u53cd\u5e94\u6027\u548c\u4efb\u52a1\u8fd0\u52a8\u89c4\u5212\u7684\u7ec4\u5408\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u3001\u4e0d\u95f4\u65ad\u6267\u884c\uff0c\u5e76\u80fd\u4ece\u6267\u884c\u6545\u969c\u4e2d\u7a33\u5065\u6062\u590d\u3002"}}
{"id": "2510.01675", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01675", "abs": "https://arxiv.org/abs/2510.01675", "authors": ["Jaewoo Lee", "Dongjae Lee", "Jinwoo Lee", "Hyungyu Lee", "Yeonjoon Kim", "H. Jin Kim"], "title": "Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances", "comment": null, "summary": "This work presents a geometric backstepping controller for a variable-tilt\nomnidirectional multirotor that explicitly accounts for both servo and rotor\ndynamics. Considering actuator dynamics is essential for more effective and\nreliable operation, particularly during aggressive flight maneuvers or recovery\nfrom sudden disturbances. While prior studies have investigated actuator-aware\ncontrol for conventional and fixed-tilt multirotors, these approaches rely on\nlinear relationships between actuator input and wrench, which cannot capture\nthe nonlinearities induced by variable tilt angles. In this work, we exploit\nthe cascade structure between the rigid-body dynamics of the multirotor and its\nnonlinear actuator dynamics to design the proposed backstepping controller and\nestablish exponential stability of the overall system. Furthermore, we reveal\nparametric uncertainty in the actuator model through experiments, and we\ndemonstrate that the proposed controller remains robust against such\nuncertainty. The controller was compared against a baseline that does not\naccount for actuator dynamics across three experimental scenarios: fast\ntranslational tracking, rapid rotational tracking, and recovery from sudden\ndisturbance. The proposed method consistently achieved better tracking\nperformance, and notably, while the baseline diverged and crashed during the\nfastest translational trajectory tracking and the recovery experiment, the\nproposed controller maintained stability and successfully completed the tasks,\nthereby demonstrating its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u53cd\u6b65\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u53ef\u53d8\u503e\u659c\u5168\u5411\u591a\u65cb\u7ffc\u98de\u884c\u5668\uff0c\u660e\u786e\u8003\u8651\u4e86\u4f3a\u670d\u548c\u8f6c\u5b50\u52a8\u529b\u5b66\uff0c\u5728\u5feb\u901f\u673a\u52a8\u548c\u6270\u52a8\u6062\u590d\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e0d\u8003\u8651\u6267\u884c\u5668\u52a8\u6001\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6267\u884c\u5668\u611f\u77e5\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6267\u884c\u5668\u8f93\u5165\u4e0e\u529b\u77e9\u4e4b\u95f4\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u65e0\u6cd5\u6355\u6349\u53ef\u53d8\u503e\u659c\u89d2\u5f15\u8d77\u7684\u975e\u7ebf\u6027\u6548\u5e94\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u6fc0\u8fdb\u98de\u884c\u6216\u6270\u52a8\u6062\u590d\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528\u591a\u65cb\u7ffc\u521a\u4f53\u52a8\u529b\u5b66\u4e0e\u975e\u7ebf\u6027\u6267\u884c\u5668\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u7ea7\u8054\u7ed3\u6784\uff0c\u8bbe\u8ba1\u51e0\u4f55\u53cd\u6b65\u63a7\u5236\u5668\uff0c\u5e76\u5efa\u7acb\u4e86\u6574\u4e2a\u7cfb\u7edf\u7684\u6307\u6570\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5feb\u901f\u5e73\u79fb\u8ddf\u8e2a\u3001\u5feb\u901f\u65cb\u8f6c\u8ddf\u8e2a\u548c\u7a81\u7136\u6270\u52a8\u6062\u590d\u4e09\u4e2a\u5b9e\u9a8c\u573a\u666f\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u59cb\u7ec8\u83b7\u5f97\u66f4\u597d\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u5728\u57fa\u51c6\u65b9\u6cd5\u53d1\u6563\u548c\u5760\u6bc1\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "\u8be5\u63a7\u5236\u5668\u80fd\u591f\u6709\u6548\u5904\u7406\u6267\u884c\u5668\u975e\u7ebf\u6027\u52a8\u6001\uff0c\u5bf9\u6a21\u578b\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u53d8\u503e\u659c\u5168\u5411\u591a\u65cb\u7ffc\u5728\u6fc0\u8fdb\u98de\u884c\u548c\u6270\u52a8\u6062\u590d\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.01708", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01708", "abs": "https://arxiv.org/abs/2510.01708", "authors": ["Zixing Lei", "Zibo Zhou", "Sheng Yin", "Yueru Chen", "Qingyao Xu", "Weixin Li", "Yunhong Wang", "Bowei Tang", "Wei Jing", "Siheng Chen"], "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization", "comment": "8 pages, 5 figures", "summary": "Humanoid whole-body control (WBC) policies trained in simulation often suffer\nfrom the sim-to-real gap, which fundamentally arises from simulator inductive\nbias, the inherent assumptions and limitations of any single simulator. These\nbiases lead to nontrivial discrepancies both across simulators and between\nsimulation and the real world. To mitigate the effect of simulator inductive\nbias, the key idea is to train policies jointly across multiple simulators,\nencouraging the learned controller to capture dynamics that generalize beyond\nany single simulator's assumptions. We thus introduce PolySim, a WBC training\nplatform that integrates multiple heterogeneous simulators. PolySim can launch\nparallel environments from different engines simultaneously within a single\ntraining run, thereby realizing dynamics-level domain randomization.\nTheoretically, we show that PolySim yields a tighter upper bound on simulator\ninductive bias than single-simulator training. In experiments, PolySim\nsubstantially reduces motion-tracking error in sim-to-sim evaluations; for\nexample, on MuJoCo, it improves execution success by 52.8 over an IsaacSim\nbaseline. PolySim further enables zero-shot deployment on a real Unitree G1\nwithout additional fine-tuning, showing effective transfer from simulation to\nthe real world. We will release the PolySim code upon acceptance of this work.", "AI": {"tldr": "PolySim\u662f\u4e00\u4e2a\u591a\u4eff\u771f\u5668\u96c6\u6210\u5e73\u53f0\uff0c\u901a\u8fc7\u540c\u65f6\u5728\u591a\u4e2a\u5f02\u6784\u4eff\u771f\u5668\u4e2d\u8bad\u7ec3\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u7b56\u7565\uff0c\u51cf\u5c11\u4eff\u771f\u5668\u5f52\u7eb3\u504f\u5dee\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u5dee\u8ddd\u95ee\u9898\uff0c\u8fd9\u79cd\u5dee\u8ddd\u6e90\u4e8e\u5355\u4e2a\u4eff\u771f\u5668\u7684\u5f52\u7eb3\u504f\u5dee\uff08\u5047\u8bbe\u548c\u9650\u5236\uff09\uff0c\u5bfc\u81f4\u8de8\u4eff\u771f\u5668\u548c\u4eff\u771f-\u771f\u5b9e\u4e16\u754c\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u5f00\u53d1PolySim\u5e73\u53f0\uff0c\u5728\u5355\u4e2a\u8bad\u7ec3\u8fd0\u884c\u4e2d\u540c\u65f6\u542f\u52a8\u6765\u81ea\u4e0d\u540c\u4eff\u771f\u5f15\u64ce\u7684\u5e76\u884c\u73af\u5883\uff0c\u5b9e\u73b0\u52a8\u6001\u5c42\u9762\u7684\u57df\u968f\u673a\u5316\uff0c\u8054\u5408\u8bad\u7ec3\u7b56\u7565\u4ee5\u6355\u6349\u8d85\u8d8a\u4efb\u4f55\u5355\u4e2a\u4eff\u771f\u5668\u5047\u8bbe\u7684\u901a\u7528\u52a8\u529b\u5b66\u3002", "result": "\u5728\u4eff\u771f\u95f4\u8bc4\u4f30\u4e2d\u663e\u8457\u51cf\u5c11\u8fd0\u52a8\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5728MuJoCo\u4e0a\u6bd4IsaacSim\u57fa\u7ebf\u63d0\u9ad8\u6267\u884c\u6210\u529f\u738752.8%\uff1b\u80fd\u591f\u96f6\u6837\u672c\u90e8\u7f72\u5230\u771f\u5b9eUnitree G1\u673a\u5668\u4eba\u4e0a\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "conclusion": "PolySim\u901a\u8fc7\u591a\u4eff\u771f\u5668\u8054\u5408\u8bad\u7ec3\u6709\u6548\u7f13\u89e3\u4e86\u4eff\u771f\u5668\u5f52\u7eb3\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6709\u6548\u8fc1\u79fb\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2510.01711", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01711", "abs": "https://arxiv.org/abs/2510.01711", "authors": ["Taeyoung Kim", "Jimin Lee", "Myungkyu Koo", "Dongyoung Kim", "Kyungmin Lee", "Changyeon Kim", "Younggyo Seo", "Jinwoo Shin"], "title": "Contrastive Representation Regularization for Vision-Language-Action Models", "comment": "20 pages, 12 figures", "summary": "Vision-Language-Action (VLA) models have shown its capabilities in robot\nmanipulation by leveraging rich representations from pre-trained\nVision-Language Models (VLMs). However, their representations arguably remain\nsuboptimal, lacking sensitivity to robotic signals such as control actions and\nproprioceptive states. To address the issue, we introduce Robot State-aware\nContrastive Loss (RS-CL), a simple and effective representation regularization\nfor VLA models, designed to bridge the gap between VLM representations and\nrobotic signals. In particular, RS-CL aligns the representations more closely\nwith the robot's proprioceptive states, by using relative distances between the\nstates as soft supervision. Complementing the original action prediction\nobjective, RS-CL effectively enhances control-relevant representation learning,\nwhile being lightweight and fully compatible with standard VLA training\npipeline. Our empirical results demonstrate that RS-CL substantially improves\nthe manipulation performance of state-of-the-art VLA models; it pushes the\nprior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,\nthrough more accurate positioning during grasping and placing, and boosts\nsuccess rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.", "AI": {"tldr": "\u63d0\u51faRobot State-aware Contrastive Loss (RS-CL)\uff0c\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8868\u793a\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u8868\u793a\u8d28\u91cf\uff0c\u4f7f\u5176\u5bf9\u673a\u5668\u4eba\u72b6\u6001\u4fe1\u53f7\u66f4\u654f\u611f\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u867d\u7136\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u4e30\u5bcc\u8868\u793a\uff0c\u4f46\u8fd9\u4e9b\u8868\u793a\u5bf9\u673a\u5668\u4eba\u4fe1\u53f7\uff08\u5982\u63a7\u5236\u52a8\u4f5c\u548c\u672c\u4f53\u611f\u89c9\u72b6\u6001\uff09\u4e0d\u591f\u654f\u611f\uff0c\u5bfc\u81f4\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u6b20\u4f73\u3002", "method": "\u5f15\u5165RS-CL\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u673a\u5668\u4eba\u672c\u4f53\u611f\u89c9\u72b6\u6001\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb\u4f5c\u4e3a\u8f6f\u76d1\u7763\uff0c\u5c06\u8868\u793a\u4e0e\u673a\u5668\u4eba\u72b6\u6001\u66f4\u7d27\u5bc6\u5730\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6VLA\u8bad\u7ec3\u6d41\u7a0b\u7684\u517c\u5bb9\u6027\u3002", "result": "RS-CL\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdbVLA\u6a21\u578b\u7684\u6027\u80fd\uff1a\u5728RoboCasa-Kitchen\u7684\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\u4ece30.8%\u63d0\u5347\u523041.5%\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4ece45.0%\u63d0\u5347\u523058.3%\u3002", "conclusion": "RS-CL\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6709\u6548\u7684\u8868\u793a\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u6293\u53d6\u548c\u653e\u7f6e\u65f6\u7684\u7cbe\u786e\u5b9a\u4f4d\u65b9\u9762\u3002"}}
{"id": "2510.01761", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01761", "abs": "https://arxiv.org/abs/2510.01761", "authors": ["Wendu Zhang", "Heng Wang", "Shuangyi Wang", "Yuanrui Huang"], "title": "Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery", "comment": "7 pages, 3 figures, under review of ICRA 2026", "summary": "Magnetic continuum robots (MCRs) enable minimally invasive navigation through\ntortuous anatomical channels, yet axially magnetized designs have largely been\nlimited to bending-only motion. To expand deformation capabilities, this paper\npresents a simple assembly that embeds permanent magnets radially within the\ncatheter wall, allowing a single externally steered permanent magnet to\nindependently induce either bending or torsion. A physics-based formulation\ntogether with finite-element analysis establishes the actuation principles, and\nbenchtop experiments validate decoupled mode control under practical fields.\nBuilding on this, a dual-layer blockage mechanism consisting of outer grooves\nand inner plates leverages torsional shear to achieve on-demand drug release.\nFinally, an in-phantom intervention experiment demonstrates end-to-end\noperation: lumen following by bending for target approach, followed by\ntwist-activated release at the site. The resulting compact, cable-free platform\ncombines versatile deformation with precise payload delivery, indicating strong\npotential for next-generation, site-specific therapies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u78c1\u6027\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5728\u5bfc\u7ba1\u58c1\u5185\u5f84\u5411\u5d4c\u5165\u6c38\u78c1\u4f53\uff0c\u5b9e\u73b0\u5355\u4e2a\u5916\u90e8\u6c38\u78c1\u4f53\u72ec\u7acb\u63a7\u5236\u5f2f\u66f2\u6216\u626d\u8f6c\u8fd0\u52a8\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u626d\u8f6c\u526a\u5207\u7684\u53cc\u5c42\u963b\u585e\u673a\u5236\u7528\u4e8e\u6309\u9700\u836f\u7269\u91ca\u653e\u3002", "motivation": "\u4f20\u7edf\u8f74\u5411\u78c1\u5316\u8bbe\u8ba1\u7684\u78c1\u6027\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u4e3b\u8981\u9650\u4e8e\u5f2f\u66f2\u8fd0\u52a8\uff0c\u9650\u5236\u4e86\u53d8\u5f62\u80fd\u529b\u3002\u4e3a\u4e86\u6269\u5c55\u53d8\u5f62\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u72ec\u7acb\u63a7\u5236\u5f2f\u66f2\u548c\u626d\u8f6c\u7684\u65b0\u578b\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u5f84\u5411\u5d4c\u5165\u6c38\u78c1\u4f53\u7684\u7b80\u5355\u7ec4\u88c5\u7ed3\u6784\uff0c\u7ed3\u5408\u57fa\u4e8e\u7269\u7406\u7684\u5efa\u6a21\u548c\u6709\u9650\u5143\u5206\u6790\u5efa\u7acb\u9a71\u52a8\u539f\u7406\uff0c\u5f00\u53d1\u4e86\u5229\u7528\u626d\u8f6c\u526a\u5207\u5b9e\u73b0\u6309\u9700\u836f\u7269\u91ca\u653e\u7684\u53cc\u5c42\u963b\u585e\u673a\u5236\uff08\u5916\u69fd\u548c\u5185\u677f\uff09\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u5b9e\u9645\u78c1\u573a\u4e0b\u7684\u89e3\u8026\u6a21\u5f0f\u63a7\u5236\uff0c\u5e76\u5728\u4f53\u6a21\u5e72\u9884\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u4ece\u8154\u9053\u8ddf\u968f\u3001\u5f2f\u66f2\u63a5\u8fd1\u76ee\u6807\u5230\u626d\u8f6c\u6fc0\u6d3b\u91ca\u653e\u7684\u7aef\u5230\u7aef\u64cd\u4f5c\u3002", "conclusion": "\u8fd9\u79cd\u7d27\u51d1\u3001\u65e0\u7f06\u7684\u5e73\u53f0\u7ed3\u5408\u4e86\u591a\u529f\u80fd\u53d8\u5f62\u4e0e\u7cbe\u786e\u6709\u6548\u8f7d\u8377\u8f93\u9001\uff0c\u663e\u793a\u51fa\u5728\u4e0b\u4e00\u4ee3\u5b9a\u70b9\u6cbb\u7597\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.01770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01770", "abs": "https://arxiv.org/abs/2510.01770", "authors": ["Christopher Leet", "Aidan Sciortino", "Sven Koenig"], "title": "An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory", "comment": null, "summary": "Modern automated factories increasingly run manufacturing procedures using a\nmatrix of programmable machines, such as 3D printers, interconnected by a\nprogrammable transport system, such as a fleet of tabletop robots. To embed a\nmanufacturing procedure into a smart factory, an operator must: (a) assign each\nof its processes to a machine and (b) specify how agents should transport parts\nbetween machines. The problem of embedding a manufacturing process into a smart\nfactory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art\nSFE solvers can only scale to factories containing a couple dozen machines.\nModern smart factories, however, may contain hundreds of machines. We fill this\nhole by introducing the first highly scalable solution to the SFE, TS-ACES, the\nTraffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is\ncomplete and can scale to SFE instances based on real industrial scenarios with\nmore than a hundred machines.", "AI": {"tldr": "TS-ACES\u662f\u9996\u4e2a\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u667a\u80fd\u5de5\u5382\u5d4c\u5165(SFE)\u95ee\u9898\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u5305\u542b\u6570\u767e\u53f0\u673a\u5668\u7684\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u3002", "motivation": "\u73b0\u4ee3\u667a\u80fd\u5de5\u5382\u53ef\u80fd\u5305\u542b\u6570\u767e\u53f0\u673a\u5668\uff0c\u4f46\u73b0\u6709SFE\u6c42\u89e3\u5668\u53ea\u80fd\u6269\u5c55\u5230\u51e0\u5341\u53f0\u673a\u5668\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u8ddd\u3002", "method": "TS-ACES\u662f\u57fa\u4e8e\u4ea4\u901a\u7cfb\u7edf\u7684\u968f\u65f6\u5faa\u73af\u5d4c\u5165\u6c42\u89e3\u5668\uff0c\u5229\u7528\u53ef\u7f16\u7a0b\u8fd0\u8f93\u7cfb\u7edf\u6765\u4f18\u5316\u5236\u9020\u8fc7\u7a0b\u7684\u5d4c\u5165\u3002", "result": "TS-ACES\u88ab\u8bc1\u660e\u662f\u5b8c\u5907\u7684\uff0c\u5e76\u4e14\u80fd\u591f\u6269\u5c55\u5230\u57fa\u4e8e\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u3001\u5305\u542b\u8d85\u8fc7\u4e00\u767e\u53f0\u673a\u5668\u7684SFE\u5b9e\u4f8b\u3002", "conclusion": "TS-ACES\u586b\u8865\u4e86\u667a\u80fd\u5de5\u5382\u5d4c\u5165\u95ee\u9898\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u6c42\u89e3\u7a7a\u767d\uff0c\u4e3a\u73b0\u4ee3\u81ea\u52a8\u5316\u5de5\u5382\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01795", "abs": "https://arxiv.org/abs/2510.01795", "authors": ["Haibo Hu", "Lianming Huang", "Xinyu Wang", "Yufei Cui", "Nan Guan", "Chun Jason Xue"], "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving\nfor unified perception and reasoning, but high inference latency hinders\nreal-time deployment. Early-exit reduces latency by terminating inference at\nintermediate layers, yet its task-dependent nature limits generalization across\ndiverse scenarios. We observe that this limitation aligns with autonomous\ndriving: navigation systems can anticipate upcoming contexts (e.g.,\nintersections, traffic lights), indicating which tasks will be required. We\npropose Nav-EE, a navigation-guided early-exit framework that precomputes\ntask-specific exit layers offline and dynamically applies them online based on\nnavigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE\nachieves accuracy comparable to full inference while reducing latency by up to\n63.9%. Real-vehicle integration with Autoware Universe further demonstrates\nreduced inference latency (600ms to 300ms), supporting faster decision-making\nin complex scenarios. These results suggest that coupling navigation foresight\nwith early-exit offers a viable path toward efficient deployment of large\nmodels in autonomous systems. Code and data are available at our anonymous\nrepository: https://anonymous.4open.science/r/Nav-EE-BBC4", "AI": {"tldr": "\u63d0\u51faNav-EE\u6846\u67b6\uff0c\u901a\u8fc7\u5bfc\u822a\u5148\u9a8c\u6307\u5bfc\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u65e9\u671f\u9000\u51fa\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9ad8\u63a8\u7406\u5ef6\u8fdf\u963b\u788d\u5b9e\u65f6\u90e8\u7f72\u3002\u4f20\u7edf\u65e9\u671f\u9000\u51fa\u65b9\u6cd5\u53d7\u9650\u4e8e\u4efb\u52a1\u4f9d\u8d56\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u573a\u666f", "method": "\u57fa\u4e8e\u5bfc\u822a\u7cfb\u7edf\u9884\u77e5\u4e0a\u4e0b\u6587\uff08\u5982\u8def\u53e3\u3001\u4ea4\u901a\u706f\uff09\uff0c\u79bb\u7ebf\u9884\u8ba1\u7b97\u4efb\u52a1\u7279\u5b9a\u7684\u9000\u51fa\u5c42\uff0c\u5728\u7ebf\u6839\u636e\u5bfc\u822a\u5148\u9a8c\u52a8\u6001\u5e94\u7528\u65e9\u671f\u9000\u51fa", "result": "\u5728CODA\u3001Waymo\u548cBOSCH\u6570\u636e\u96c6\u4e0a\uff0cNav-EE\u5728\u4fdd\u6301\u4e0e\u5b8c\u6574\u63a8\u7406\u76f8\u5f53\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe63.9%\u3002\u5b9e\u8f66\u96c6\u6210\u663e\u793a\u63a8\u7406\u5ef6\u8fdf\u4ece600ms\u964d\u81f3300ms", "conclusion": "\u5c06\u5bfc\u822a\u9884\u89c1\u6027\u4e0e\u65e9\u671f\u9000\u51fa\u76f8\u7ed3\u5408\uff0c\u4e3a\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u9ad8\u6548\u90e8\u7f72\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84"}}
{"id": "2510.01830", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01830", "abs": "https://arxiv.org/abs/2510.01830", "authors": ["Hongze Wang", "Boyang Sun", "Jiaxu Xing", "Fan Yang", "Marco Hutter", "Dhruv Shah", "Davide Scaramuzza", "Marc Pollefeys"], "title": "What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework", "comment": null, "summary": "Object-Goal Navigation (ObjectNav) is a critical component toward deploying\nmobile robots in everyday, uncontrolled environments such as homes, schools,\nand workplaces. In this context, a robot must locate target objects in\npreviously unseen environments using only its onboard perception. Success\nrequires the integration of semantic understanding, spatial reasoning, and\nlong-horizon planning, which is a combination that remains extremely\nchallenging. While reinforcement learning (RL) has become the dominant\nparadigm, progress has spanned a wide range of design choices, yet the field\nstill lacks a unifying analysis to determine which components truly drive\nperformance. In this work, we conduct a large-scale empirical study of modular\nRL-based ObjectNav systems, decomposing them into three key components:\nperception, policy, and test-time enhancement. Through extensive controlled\nexperiments, we isolate the contribution of each and uncover clear trends:\nperception quality and test-time strategies are decisive drivers of\nperformance, whereas policy improvements with current methods yield only\nmarginal gains. Building on these insights, we propose practical design\nguidelines and demonstrate an enhanced modular system that surpasses\nState-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We\nalso introduce a human baseline under identical conditions, where experts\nachieve an average 98% success, underscoring the gap between RL agents and\nhuman-level navigation. Our study not only sets the SotA performance but also\nprovides principled guidance for future ObjectNav development and evaluation.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7269\u4f53\u76ee\u6807\u5bfc\u822a\u7cfb\u7edf\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u611f\u77e5\u8d28\u91cf\u548c\u6d4b\u8bd5\u65f6\u7b56\u7565\u662f\u6027\u80fd\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u800c\u7b56\u7565\u6539\u8fdb\u5e26\u6765\u7684\u589e\u76ca\u6709\u9650\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u589e\u5f3a\u7684\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u5728SPL\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd56.6%\u3002", "motivation": "\u7269\u4f53\u76ee\u6807\u5bfc\u822a\u662f\u79fb\u52a8\u673a\u5668\u4eba\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u5bf9\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u5404\u4e2a\u7ec4\u4ef6\u8d21\u732e\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u65e0\u6cd5\u786e\u5b9a\u771f\u6b63\u9a71\u52a8\u6027\u80fd\u7684\u56e0\u7d20\u3002", "method": "\u5c06\u6a21\u5757\u5316RL\u7cfb\u7edf\u5206\u89e3\u4e3a\u611f\u77e5\u3001\u7b56\u7565\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5927\u91cf\u53d7\u63a7\u5b9e\u9a8c\u9694\u79bb\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u8d21\u732e\uff0c\u5e76\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u6784\u5efa\u589e\u5f3a\u7684\u6a21\u5757\u5316\u7cfb\u7edf\u3002", "result": "\u611f\u77e5\u8d28\u91cf\u548c\u6d4b\u8bd5\u65f6\u7b56\u7565\u662f\u6027\u80fd\u7684\u51b3\u5b9a\u6027\u9a71\u52a8\u56e0\u7d20\uff0c\u7b56\u7565\u6539\u8fdb\u4ec5\u5e26\u6765\u8fb9\u9645\u589e\u76ca\u3002\u63d0\u51fa\u7684\u589e\u5f3a\u7cfb\u7edf\u5728SPL\u6307\u6807\u4e0a\u8d85\u8d8aSotA\u65b9\u6cd56.6%\uff0c\u6210\u529f\u7387\u63d0\u53472.7%\u3002\u4eba\u7c7b\u4e13\u5bb6\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u8fbe\u523098%\u6210\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u8bbe\u5b9a\u4e86\u65b0\u7684SotA\u6027\u80fd\u6807\u51c6\uff0c\u8fd8\u4e3a\u672a\u6765ObjectNav\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\uff0c\u63ed\u793a\u4e86RL\u667a\u80fd\u4f53\u4e0e\u4eba\u7c7b\u6c34\u5e73\u5bfc\u822a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.01843", "categories": ["cs.RO", "I.2.9; I.2.8; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.01843", "abs": "https://arxiv.org/abs/2510.01843", "authors": ["Wanyue Li", "Ji Ma", "Minghao Lu", "Peng Lu"], "title": "Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots", "comment": "8 pages, 8 figures, conference paper", "summary": "Humanoid robot soccer presents several challenges, particularly in\nmaintaining system stability during aggressive kicking motions while achieving\nprecise ball trajectory control. Current solutions, whether traditional\nposition-based control methods or reinforcement learning (RL) approaches,\nexhibit significant limitations. Model predictive control (MPC) is a prevalent\napproach for ordinary quadruped and biped robots. While MPC has demonstrated\nadvantages in legged robots, existing studies often oversimplify the leg swing\nprogress, relying merely on simple trajectory interpolation methods. This\nseverely constrains the foot's environmental interaction capability, hindering\ntasks such as ball kicking. This study innovatively adapts the spatial-temporal\ntrajectory planning method, which has been successful in drone applications, to\nbipedal robotic systems. The proposed approach autonomously generates foot\ntrajectories that satisfy constraints on target kicking position, velocity, and\nacceleration while simultaneously optimizing swing phase duration. Experimental\nresults demonstrate that the optimized trajectories closely mimic human kicking\nbehavior, featuring a backswing motion. Simulation and hardware experiments\nconfirm the algorithm's efficiency, with trajectory planning times under 1 ms,\nand its reliability, achieving nearly 100 % task completion accuracy when the\nsoccer goal is within the range of -90{\\deg} to 90{\\deg}.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u65e0\u4eba\u673a\u4e2d\u6210\u529f\u7684\u65f6\u7a7a\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u521b\u65b0\u6027\u5730\u5e94\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u81ea\u4e3b\u751f\u6210\u6ee1\u8db3\u76ee\u6807\u8e22\u7403\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u8db3\u90e8\u8f68\u8ff9\uff0c\u540c\u65f6\u4f18\u5316\u6446\u52a8\u9636\u6bb5\u6301\u7eed\u65f6\u95f4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u673a\u5668\u4eba\u8e22\u7403\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u8db3\u7403\u4e2d\u5728\u6fc0\u70c8\u8e22\u7403\u52a8\u4f5c\u65f6\u4fdd\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u5e76\u5b9e\u73b0\u7cbe\u786e\u7403\u8f68\u8ff9\u63a7\u5236\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u4f4d\u7f6e\u63a7\u5236\u6216\u5f3a\u5316\u5b66\u4e60\uff09\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0cMPC\u65b9\u6cd5\u5728\u8db3\u5f0f\u673a\u5668\u4eba\u4e2d\u867d\u6d41\u884c\u4f46\u901a\u5e38\u8fc7\u5ea6\u7b80\u5316\u817f\u90e8\u6446\u52a8\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u8db3\u90e8\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u521b\u65b0\u6027\u5730\u5c06\u65e0\u4eba\u673a\u5e94\u7528\u4e2d\u6210\u529f\u7684\u65f6\u7a7a\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u9002\u914d\u5230\u53cc\u8db3\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u81ea\u4e3b\u751f\u6210\u6ee1\u8db3\u76ee\u6807\u8e22\u7403\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u8db3\u90e8\u8f68\u8ff9\uff0c\u540c\u65f6\u4f18\u5316\u6446\u52a8\u9636\u6bb5\u6301\u7eed\u65f6\u95f4\u3002", "result": "\u4f18\u5316\u7684\u8f68\u8ff9\u7d27\u5bc6\u6a21\u4eff\u4eba\u7c7b\u8e22\u7403\u884c\u4e3a\uff0c\u5177\u6709\u540e\u6446\u52a8\u4f5c\u7279\u5f81\u3002\u8f68\u8ff9\u89c4\u5212\u65f6\u95f4\u4f4e\u4e8e1\u6beb\u79d2\uff0c\u5728\u8db3\u7403\u95e8\u4f4d\u4e8e-90\u5ea6\u523090\u5ea6\u8303\u56f4\u5185\u65f6\uff0c\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u7387\u63a5\u8fd1100%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65f6\u7a7a\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5728\u4eba\u5f62\u673a\u5668\u4eba\u8e22\u7403\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u8e22\u7403\u884c\u4e3a\u7684\u8f68\u8ff9\uff0c\u4e3a\u673a\u5668\u4eba\u8db3\u7403\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01848", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01848", "abs": "https://arxiv.org/abs/2510.01848", "authors": ["Diram Tabaa", "Gianni Di Caro"], "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics", "comment": null, "summary": "Simulating greenhouse environments is critical for developing and evaluating\nrobotic systems for agriculture, yet existing approaches rely on simplistic or\nsynthetic assets that limit simulation-to-real transfer. Recent advances in\nradiance field methods, such as Gaussian splatting, enable photorealistic\nreconstruction but have so far been restricted to individual plants or\ncontrolled laboratory conditions. In this work, we introduce GreenhouseSplat, a\nframework and dataset for generating photorealistic greenhouse assets directly\nfrom inexpensive RGB images. The resulting assets are integrated into a\nROS-based simulation with support for camera and LiDAR rendering, enabling\ntasks such as localization with fiducial markers. We provide a dataset of 82\ncucumber plants across multiple row configurations and demonstrate its utility\nfor robotics evaluation. GreenhouseSplat represents the first step toward\ngreenhouse-scale radiance-field simulation and offers a foundation for future\nresearch in agricultural robotics.", "AI": {"tldr": "GreenhouseSplat\u662f\u4e00\u4e2a\u4eceRGB\u56fe\u50cf\u751f\u6210\u903c\u771f\u6e29\u5ba4\u8d44\u4ea7\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u652f\u6301ROS\u4eff\u771f\uff0c\u7528\u4e8e\u519c\u4e1a\u673a\u5668\u4eba\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u6e29\u5ba4\u4eff\u771f\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u6216\u5408\u6210\u8d44\u4ea7\uff0c\u9650\u5236\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002\u8f90\u5c04\u573a\u65b9\u6cd5\u867d\u80fd\u5b9e\u73b0\u903c\u771f\u91cd\u5efa\uff0c\u4f46\u4ec5\u9650\u4e8e\u5355\u4e2a\u690d\u7269\u6216\u5b9e\u9a8c\u5ba4\u73af\u5883\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u8f90\u5c04\u573a\u65b9\u6cd5\uff0c\u4ece\u5ec9\u4ef7RGB\u56fe\u50cf\u76f4\u63a5\u751f\u6210\u903c\u771f\u6e29\u5ba4\u8d44\u4ea7\uff0c\u96c6\u6210\u5230\u652f\u6301\u76f8\u673a\u548cLiDAR\u6e32\u67d3\u7684ROS\u4eff\u771f\u4e2d\u3002", "result": "\u63d0\u4f9b\u4e86\u5305\u542b82\u682a\u9ec4\u74dc\u690d\u7269\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u884c\u914d\u7f6e\uff0c\u5c55\u793a\u4e86\u5728\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8fd9\u662f\u8fc8\u5411\u6e29\u5ba4\u5c3a\u5ea6\u8f90\u5c04\u573a\u4eff\u771f\u7684\u7b2c\u4e00\u6b65\uff0c\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.01869", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01869", "abs": "https://arxiv.org/abs/2510.01869", "authors": ["Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "TACOS: Task Agnostic COordinator of a multi-drone System", "comment": "6 pages, 6 figures, accepted as poster at 2025 IEEE International\n  Symposium on Multi-Robot & Multi-Agent Systems", "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.", "AI": {"tldr": "TACOS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u5b9e\u73b0\u9ad8\u5c42\u4efb\u52a1\u63a7\u5236\uff0c\u96c6\u6210\u4e86\u4e00\u5bf9\u591a\u4ea4\u4e92\u754c\u9762\u3001\u667a\u80fd\u534f\u8c03\u5668\u548c\u81ea\u4e3b\u6267\u884c\u4ee3\u7406\u3002", "motivation": "\u89e3\u51b3\u5355\u98de\u884c\u5458\u7ba1\u7406\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u65f6\u9762\u4e34\u7684\u81ea\u4e3b\u6027\u9700\u6c42\u53d8\u5316\u95ee\u9898\uff0c\u4ece\u76f4\u63a5\u63a7\u5236\u5230\u7fa4\u4f53\u534f\u8c03\u518d\u5230\u5b8c\u5168\u81ea\u4e3b\u7684\u7fa4\u4f53\u884c\u4e3a\uff0c\u9700\u8981\u652f\u6301\u591a\u79cd\u5171\u4eab\u81ea\u4e3b\u6a21\u5f0f\u7684\u7075\u6d3b\u4ea4\u4e92\u6846\u67b6\u3002", "method": "TACOS\u6846\u67b6\u96c6\u6210\u4e09\u4e2a\u5173\u952e\u80fd\u529b\uff1a\u4e00\u5bf9\u591a\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u754c\u9762\u3001\u5c06\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u4efb\u52a1\u8ba1\u5212\u7684\u667a\u80fd\u534f\u8c03\u5668\u3001\u4e0e\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u6267\u884c\u8ba1\u5212\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7LLM\u4e0e\u53ef\u6267\u884cAPI\u5e93\u4ea4\u4e92\u3002", "result": "\u5728\u771f\u5b9e\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e86\u5404\u6a21\u5757\u7684\u8d21\u732e\u3002", "conclusion": "TACOS\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e86\u8bed\u4e49\u63a8\u7406\u4e0e\u5b9e\u65f6\u591a\u673a\u5668\u4eba\u534f\u8c03\u7684\u6865\u6881\uff0c\u4e3a\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u9ad8\u5c42\u4efb\u52a1\u59d4\u6258\u80fd\u529b\u3002"}}
{"id": "2510.01984", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01984", "abs": "https://arxiv.org/abs/2510.01984", "authors": ["Yue Wang"], "title": "SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot", "comment": null, "summary": "We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module\nthat combines revolute (pitch) and prismatic (axial) motion with programmable\ntask-space impedance for quadruped robots. The system integrates three\ntorque-controlled actuators, a custom 1 kHz control board, and a protected\npower unit in a 1.26 kg package, enabling closed-loop stiffness and damping\nshaping along x, z, and theta. We develop an RNEA-based computed-acceleration\ncontroller with smooth Stribeck friction compensation to render spring-damper\nbehavior without explicit inertia shaping. Bench experiments validate the\napproach. Quasi-static push-pull tests show linear force-displacement\ncharacteristics with commanded horizontal stiffness spanning 300-700 N/m and <=\n1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic\ndisplace-and-release trials confirm mass-spring-damper responses over multiple\ndamping settings, with small, interpretable phase deviations due to\nconfiguration-dependent inertia and low-speed friction effects. A task-space PD\ncontroller produces roughly linear stiffness but with greater variability and\ncoupling sensitivity. SPARC provides a portable platform for systematic studies\nof spine compliance in legged locomotion and will be released with complete\nhardware and firmware resources.", "AI": {"tldr": "SPARC\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u5f00\u6e903\u81ea\u7531\u5ea6\u810a\u67f1\u6a21\u5757\uff0c\u7ed3\u5408\u4e86\u65cb\u8f6c\u548c\u7ebf\u6027\u8fd0\u52a8\uff0c\u5177\u6709\u53ef\u7f16\u7a0b\u4efb\u52a1\u7a7a\u95f4\u963b\u6297\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u3002", "motivation": "\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u7cfb\u7edf\u7814\u7a76\u810a\u67f1\u67d4\u987a\u6027\u7684\u4fbf\u643a\u5e73\u53f0\uff0c\u652f\u6301\u817f\u90e8\u8fd0\u52a8\u4e2d\u7684\u810a\u67f1\u67d4\u987a\u6027\u7814\u7a76\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u626d\u77e9\u63a7\u5236\u6267\u884c\u5668\u3001\u5b9a\u52361kHz\u63a7\u5236\u677f\u548c\u53d7\u4fdd\u62a4\u7535\u6e90\u5355\u5143\uff0c\u91c7\u7528\u57fa\u4e8eRNEA\u7684\u8ba1\u7b97\u52a0\u901f\u5ea6\u63a7\u5236\u5668\u548cStribeck\u6469\u64e6\u8865\u507f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u7ebf\u6027\u529b-\u4f4d\u79fb\u7279\u6027\uff0c\u6c34\u5e73\u521a\u5ea6300-700N/m\uff0c\u76f8\u5bf9\u8bef\u5dee\u22641.5%\uff1b\u52a8\u6001\u6d4b\u8bd5\u786e\u8ba4\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u5668\u54cd\u5e94\u3002", "conclusion": "SPARC\u4e3a\u817f\u90e8\u8fd0\u52a8\u4e2d\u7684\u810a\u67f1\u67d4\u987a\u6027\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u4fbf\u643a\u5e73\u53f0\uff0c\u5e76\u5c06\u53d1\u5e03\u5b8c\u6574\u7684\u786c\u4ef6\u548c\u56fa\u4ef6\u8d44\u6e90\u3002"}}
{"id": "2510.01986", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY", "math.OC", "q-bio.NC", "I.6"], "pdf": "https://arxiv.org/pdf/2510.01986", "abs": "https://arxiv.org/abs/2510.01986", "authors": ["Varun Kotian", "Vishrut Jain", "Andrea Michelle Rios Lazcano", "Daan Marinus Pool", "Riender Happee", "Barys Shyrokau"], "title": "Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation", "comment": null, "summary": "Driving simulators are increasingly used in research and development.\nHowever, simulators often cause motion sickness due to downscaled motion and\nunscaled veridical visuals. In this paper, a motion cueing algorithm is\nproposed that reduces motion sickness as predicted by the subjective vertical\nconflict (SVC) model using model predictive control (MPC). Both sensory\nconflict and specific force errors are penalised in the cost function, allowing\nthe algorithm to jointly optimise fidelity and comfort.\n  Human-in-the-loop experiments were conducted to compare four simulator motion\nsettings: two variations of our MPC-based algorithm, one focused on pure\nspecific force tracking and the second compromising specific force tracking and\nmotion sickness minimisation, as well as reference adaptive washout and no\nmotion cases. The experiments were performed on a hexapod driving simulator\nwith participants exposed to passive driving.\n  Experimental motion sickness results closely matched the sickness model\npredictions. As predicted by the model, the no motion condition yielded the\nlowest sickness levels. However, it was rated lowest in terms of fidelity. The\ncompromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)\ncompared to adaptive washout and the algorithm focusing on specific force\ntracking, without any significant reduction in fidelity rating.\n  The proposed approach for developing MCA that takes into account both the\nsimulator dynamics and time evolution of motion sickness offers a significant\nadvancement in achieving an optimal control of motion sickness and specific\nforce recreation in driving simulators, supporting broader simulator use.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u8fd0\u52a8\u63d0\u793a\u7b97\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u611f\u5b98\u51b2\u7a81\u548c\u6bd4\u529b\u8bef\u5dee\u6765\u540c\u65f6\u4f18\u5316\u9a7e\u9a76\u6a21\u62df\u5668\u7684\u4fdd\u771f\u5ea6\u548c\u8212\u9002\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u6655\u52a8\u75c7", "motivation": "\u9a7e\u9a76\u6a21\u62df\u5668\u5728\u7814\u53d1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u7531\u4e8e\u8fd0\u52a8\u7f29\u653e\u548c\u89c6\u89c9\u4e0d\u5339\u914d\u5e38\u5bfc\u81f4\u6655\u52a8\u75c7\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u540c\u65f6\u4fdd\u8bc1\u4fdd\u771f\u5ea6\u548c\u8212\u9002\u5ea6\u7684\u8fd0\u52a8\u63d0\u793a\u7b97\u6cd5", "method": "\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5f00\u53d1\u8fd0\u52a8\u63d0\u793a\u7b97\u6cd5\uff0c\u5728\u6210\u672c\u51fd\u6570\u4e2d\u540c\u65f6\u60e9\u7f5a\u611f\u5b98\u51b2\u7a81\u548c\u6bd4\u529b\u8bef\u5dee\uff0c\u8fdb\u884c\u4eba\u673a\u56de\u8def\u5b9e\u9a8c\u6bd4\u8f83\u56db\u79cd\u8fd0\u52a8\u8bbe\u7f6e", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u59a5\u534f\u65b9\u6848\u5c06\u6655\u52a8\u75c7\u964d\u4f4e50%\u4ee5\u4e0a\uff08\u5e73\u5747MISC\u6c34\u5e73\u4ece3\u964d\u81f31.5\uff09\uff0c\u4e14\u4fdd\u771f\u5ea6\u8bc4\u5206\u65e0\u663e\u8457\u4e0b\u964d\uff0c\u4e0e\u6a21\u578b\u9884\u6d4b\u4e00\u81f4", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u8003\u8651\u4e86\u6a21\u62df\u5668\u52a8\u529b\u5b66\u548c\u6655\u52a8\u75c7\u65f6\u95f4\u6f14\u5316\uff0c\u5728\u5b9e\u73b0\u6655\u52a8\u75c7\u63a7\u5236\u548c\u6bd4\u529b\u91cd\u73b0\u65b9\u9762\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u652f\u6301\u6a21\u62df\u5668\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528"}}
{"id": "2510.02080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02080", "abs": "https://arxiv.org/abs/2510.02080", "authors": ["Lingxiang Hu", "Naima Ait Oufroukh", "Fabien Bonardi", "Raymond Ghandour"], "title": "EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction", "comment": null, "summary": "The application of monocular dense Simultaneous Localization and Mapping\n(SLAM) is often hindered by high latency, large GPU memory consumption, and\nreliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,\na novel calibration-free monocular dense SLAM framework that jointly achieves\nhigh localization and mapping accuracy, low latency, and low GPU memory\nconsumption. This enables the framework to achieve efficiency through the\ncoupling of a tracking module, which maintains a sparse map of feature points,\nand a mapping module based on a feed-forward 3D reconstruction model that\nsimultaneously estimates camera intrinsics. In addition, both local and global\nloop closures are incorporated to ensure mid-term and long-term data\nassociation, enforcing multi-view consistency and thereby enhancing the overall\naccuracy and robustness of the system. Experiments across multiple benchmarks\nshow that EC3R-SLAM achieves competitive performance compared to\nstate-of-the-art methods, while being faster and more memory-efficient.\nMoreover, it runs effectively even on resource-constrained platforms such as\nlaptops and Jetson Orin NX, highlighting its potential for real-world robotics\napplications.", "AI": {"tldr": "EC3R-SLAM\u662f\u4e00\u4e2a\u65e0\u9700\u76f8\u673a\u6807\u5b9a\u7684\u5355\u76ee\u7a20\u5bc6SLAM\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u8ddf\u8e2a\u6a21\u5757\u548c\u57fa\u4e8e\u524d\u99883D\u91cd\u5efa\u7684\u6620\u5c04\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u5efa\u56fe\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4eGPU\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76ee\u7a20\u5bc6SLAM\u65b9\u6cd5\u5b58\u5728\u9ad8\u5ef6\u8fdf\u3001\u5927GPU\u5185\u5b58\u6d88\u8017\u548c\u4f9d\u8d56\u76f8\u673a\u6807\u5b9a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u7279\u5f81\u70b9\u5730\u56fe\u7684\u8ddf\u8e2a\u6a21\u5757\u548c\u540c\u65f6\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\u7684\u524d\u99883D\u91cd\u5efa\u6620\u5c04\u6a21\u5757\uff0c\u5e76\u878d\u5165\u5c40\u90e8\u548c\u5168\u5c40\u95ed\u73af\u68c0\u6d4b\u4ee5\u786e\u4fdd\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901f\u5ea6\u66f4\u5feb\u3001\u5185\u5b58\u6548\u7387\u66f4\u9ad8\uff0c\u80fd\u5728\u7b14\u8bb0\u672c\u7535\u8111\u548cJetson Orin NX\u7b49\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u6709\u6548\u8fd0\u884c\u3002", "conclusion": "EC3R-SLAM\u6846\u67b6\u5177\u6709\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6d88\u9664\u76f8\u673a\u6807\u5b9a\u9700\u6c42\u5e76\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u7a20\u5bc6SLAM\u3002"}}
{"id": "2510.02104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02104", "abs": "https://arxiv.org/abs/2510.02104", "authors": ["Yunhan Lin", "Wenqi Wu", "Zhijie Zhang", "Huasong Min"], "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions", "comment": "8 pages, 6 figures", "summary": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp.", "AI": {"tldr": "LangGrasp\u662f\u4e00\u4e2a\u8bed\u8a00\u4ea4\u4e92\u5f0f\u673a\u5668\u4eba\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5305\u542b\u9690\u542b\u610f\u56fe\u7684\u6a21\u7cca\u6307\u4ee4\uff0c\u5b9e\u73b0\u4ece\u7c97\u7c92\u5ea6\u7269\u4f53\u7ea7\u5230\u7ec6\u7c92\u5ea6\u90e8\u4ef6\u7ea7\u7684\u7cbe\u786e\u6293\u53d6\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u9a71\u52a8\u6293\u53d6\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u5904\u7406\u5305\u542b\u9690\u542b\u610f\u56fe\u7684\u6a21\u7cca\u6307\u4ee4\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u8bed\u8a00\u6307\u4ee4\u4e2d\u672a\u660e\u786e\u8868\u8fbe\u4f46\u4efb\u52a1\u5b8c\u6210\u5fc5\u9700\u7684\u5173\u952e\u64cd\u4f5c\u548c\u76ee\u6807\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5229\u7528\u5176\u5e38\u8bc6\u7406\u89e3\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\u63a8\u65ad\u9690\u542b\u610f\u56fe\uff0c\u8bbe\u8ba1\u57fa\u4e8e2D\u90e8\u4ef6\u5206\u5272\u6307\u5bfc\u7684\u70b9\u4e91\u5b9a\u4f4d\u6a21\u5757\uff0c\u5b9e\u73b0\u573a\u666f\u4e2d\u90e8\u5206\u70b9\u4e91\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLangGrasp\u6846\u67b6\u80fd\u51c6\u786e\u89e3\u6790\u6a21\u7cca\u6307\u4ee4\u4e2d\u7684\u9690\u542b\u610f\u56fe\uff0c\u8bc6\u522b\u672a\u660e\u786e\u8868\u8fbe\u4f46\u4efb\u52a1\u5fc5\u9700\u7684\u5173\u952e\u64cd\u4f5c\u548c\u76ee\u6807\u4fe1\u606f\uff0c\u901a\u8fc7\u6574\u5408\u73af\u5883\u4fe1\u606f\u52a8\u6001\u9009\u62e9\u6700\u4f18\u6293\u53d6\u59ff\u6001\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4ece\u7269\u4f53\u7ea7\u5230\u90e8\u4ef6\u7ea7\u7684\u9ad8\u7cbe\u5ea6\u6293\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2510.02129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02129", "abs": "https://arxiv.org/abs/2510.02129", "authors": ["Philip Reichenberg", "Tim Laue"], "title": "Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control", "comment": null, "summary": "Stand-up motions are an indispensable part of humanoid robot soccer. A robot\nincapable of standing up by itself is removed from the game for some time. In\nthis paper, we present our stand-up motions for the NAO robot. Our approach\ndates back to 2019 and has been evaluated and slightly expanded over the past\nsix years. We claim that the main reason for failed stand-up attempts are large\nerrors in the executed joint positions. By addressing such problems by either\nexecuting special motions to free up stuck limbs such as the arms, or by\ncompensating large errors with other joints, we significantly increased the\noverall success rate of our stand-up routine. The motions presented in this\npaper are also used by several other teams in the Standard Platform League,\nwhich thereby achieve similar success rates, as shown in an analysis of videos\nfrom multiple tournaments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NAO\u673a\u5668\u4eba\u7684\u7ad9\u7acb\u52a8\u4f5c\u65b9\u6848\uff0c\u901a\u8fc7\u89e3\u51b3\u5173\u8282\u4f4d\u7f6e\u6267\u884c\u8bef\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ad9\u7acb\u6210\u529f\u7387\u3002\u8be5\u65b9\u6cd5\u5df2\u88ab\u591a\u4e2a\u6807\u51c6\u5e73\u53f0\u8054\u76df\u56e2\u961f\u91c7\u7528\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u8db3\u7403\u4e2d\uff0c\u7ad9\u7acb\u52a8\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u65e0\u6cd5\u81ea\u4e3b\u7ad9\u7acb\u7684\u673a\u5668\u4eba\u4f1a\u88ab\u6682\u65f6\u79fb\u51fa\u6bd4\u8d5b\u3002\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u7ad9\u7acb\u52a8\u4f5c\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6267\u884c\u7279\u6b8a\u52a8\u4f5c\u91ca\u653e\u5361\u4f4f\u7684\u80a2\u4f53\uff08\u5982\u624b\u81c2\uff09\uff0c\u6216\u4f7f\u7528\u5176\u4ed6\u5173\u8282\u8865\u507f\u5927\u8bef\u5dee\uff0c\u6765\u89e3\u51b3\u5173\u8282\u4f4d\u7f6e\u6267\u884c\u8bef\u5dee\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u81ea2019\u5e74\u8d77\u7ecf\u8fc76\u5e74\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u7ad9\u7acb\u52a8\u4f5c\u7684\u6574\u4f53\u6210\u529f\u7387\u3002\u5206\u6790\u591a\u4e2a\u6bd4\u8d5b\u89c6\u9891\u663e\u793a\uff0c\u91c7\u7528\u8be5\u65b9\u6cd5\u7684\u5176\u4ed6\u56e2\u961f\u4e5f\u53d6\u5f97\u4e86\u76f8\u4f3c\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u5173\u8282\u4f4d\u7f6e\u6267\u884c\u8bef\u5dee\u662f\u7ad9\u7acb\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u7ad9\u7acb\u52a8\u4f5c\u7684\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u548c\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2510.02164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02164", "abs": "https://arxiv.org/abs/2510.02164", "authors": ["Nathaniel Hanson", "Austin Allison", "Charles DiMarzio", "Ta\u015fk\u0131n Pad\u0131r", "Kristen L. Dorsey"], "title": "SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation", "comment": "Accepted to IEEE Robotics & Automation Letters Special Issue on\n  Interdisciplinarity and Widening Horizons in Soft Robotics", "summary": "We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,\nelectronics-free, fluidically actuated soft manipulator capable of assessing\nthe spectral properties of objects either in hand or through pre-touch caging.\nThis platform offers a wider spectral sensing capability than previous soft\nrobotic counterparts. We perform a material analysis to explore optimal soft\nsubstrates for spectral sensing, and evaluate both pre-touch and in-hand\nperformance. Experiments demonstrate explainable, statistical separation across\ndiverse object classes and sizes (metal, wood, plastic, organic, paper, foam),\nwith large spectral angle differences between items. Through linear\ndiscriminant analysis, we show that sensitivity in the near-infrared\nwavelengths is critical to distinguishing visually similar objects. These\ncapabilities advance the potential of optics as a multi-functional sensory\nmodality for soft robots. The complete parts list, assembly guidelines, and\nprocessing code for the SCANS gripper are accessible at:\nhttps://parses-lab.github.io/scans/.", "AI": {"tldr": "SCANS\u7cfb\u7edf\u662f\u4e00\u79cd\u65e0\u9700\u7535\u5b50\u5143\u4ef6\u3001\u6d41\u4f53\u9a71\u52a8\u7684\u8f6f\u4f53\u673a\u68b0\u624b\uff0c\u80fd\u591f\u901a\u8fc7\u9884\u63a5\u89e6\u5305\u56f4\u6216\u624b\u6301\u65b9\u5f0f\u8bc4\u4f30\u7269\u4f53\u7684\u5149\u8c31\u7279\u6027\uff0c\u5177\u6709\u6bd4\u4ee5\u5f80\u8f6f\u4f53\u673a\u5668\u4eba\u66f4\u5bbd\u7684\u5149\u8c31\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u591a\u529f\u80fd\u7684\u5149\u5b66\u4f20\u611f\u8f6f\u4f53\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6269\u5c55\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5149\u8c31\u611f\u77e5\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u533a\u5206\u4e0d\u540c\u6750\u8d28\u7684\u7269\u4f53\u3002", "method": "\u8fdb\u884c\u6750\u6599\u5206\u6790\u4ee5\u5bfb\u627e\u9002\u5408\u5149\u8c31\u4f20\u611f\u7684\u8f6f\u57fa\u5e95\u6750\u6599\uff0c\u8bc4\u4f30\u9884\u63a5\u89e6\u548c\u624b\u6301\u6027\u80fd\uff0c\u4f7f\u7528\u7ebf\u6027\u5224\u522b\u5206\u6790\u786e\u5b9a\u8fd1\u7ea2\u5916\u6ce2\u957f\u5bf9\u533a\u5206\u76f8\u4f3c\u7269\u4f53\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7cfb\u7edf\u80fd\u591f\u5bf9\u4e0d\u540c\u7c7b\u522b\u548c\u5c3a\u5bf8\u7684\u7269\u4f53\uff08\u91d1\u5c5e\u3001\u6728\u6750\u3001\u5851\u6599\u3001\u6709\u673a\u7269\u3001\u7eb8\u5f20\u3001\u6ce1\u6cab\uff09\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u7edf\u8ba1\u5206\u79bb\uff0c\u7269\u4f53\u95f4\u5149\u8c31\u89d2\u5ea6\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u8fd9\u4e9b\u80fd\u529b\u63a8\u8fdb\u4e86\u5149\u5b66\u4f5c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u591a\u529f\u80fd\u4f20\u611f\u6a21\u5f0f\u7684\u6f5c\u529b\uff0c\u6240\u6709\u90e8\u4ef6\u6e05\u5355\u3001\u7ec4\u88c5\u6307\u5357\u548c\u5904\u7406\u4ee3\u7801\u90fd\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.02167", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.02167", "abs": "https://arxiv.org/abs/2510.02167", "authors": ["Sara Strakosova", "Petr Novak", "Petr Kadera"], "title": "Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network", "comment": "This work has been submitted to the IEEE for possible publication. 6\n  pages, 4 figures", "summary": "In the context of the circular economy, products in their end-of-life phase\nshould be either remanufactured or recycled. Both of these processes are\ncrucial for sustainability and environmental conservation. However,\nmanufacturers often do not support these processes enough by not sharing\nrelevant data. This paper proposes use of a digital twin technology, which is\ncapable to help optimizing the disassembly processes to reduce ecological\nimpact and enhance sustainability. The proposed approach is demonstrated\nthrough a disassembly use-case of the product digital twin of an electric\nvehicle battery. By utilizing product digital twins, challenges associated with\nthe disassembly of electric vehicle batteries can be solved flexibly and\nefficiently for various battery types. As a backbone for the product digital\ntwin representation, the paper uses the paradigm of product-process-resource\nasset networks (PAN). Such networks enable to model relevant relationships\nacross products, production resources, manufacturing processes, and specific\nproduction operations that have to be done in the manufacturing phase of a\nproduct. This paper introduces a Bi-Flow Product-Process-Resource Asset Network\n(Bi-PAN) representation, which extends the PAN paradigm to cover not only the\nmanufacturing, but also the remanufacturing/recycling phase.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u7684\u62c6\u89e3\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6269\u5c55\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90\u8d44\u4ea7\u7f51\u7edc\uff08PAN\uff09\u4e3a\u53cc\u5411PAN\uff08Bi-PAN\uff09\uff0c\u540c\u65f6\u8986\u76d6\u5236\u9020\u548c\u518d\u5236\u9020/\u56de\u6536\u9636\u6bb5\u3002", "motivation": "\u5728\u5faa\u73af\u7ecf\u6d4e\u80cc\u666f\u4e0b\uff0c\u5236\u9020\u5546\u5f80\u5f80\u4e0d\u5171\u4eab\u76f8\u5173\u6570\u636e\uff0c\u5bfc\u81f4\u4ea7\u54c1\u751f\u547d\u5468\u671f\u7ed3\u675f\u9636\u6bb5\u7684\u518d\u5236\u9020\u548c\u56de\u6536\u8fc7\u7a0b\u652f\u6301\u4e0d\u8db3\uff0c\u5f71\u54cd\u53ef\u6301\u7eed\u6027\u548c\u73af\u5883\u4fdd\u62a4\u3002", "method": "\u91c7\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u57fa\u4e8e\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90\u8d44\u4ea7\u7f51\u7edc\uff08PAN\uff09\u8303\u5f0f\uff0c\u63d0\u51fa\u53cc\u5411PAN\uff08Bi-PAN\uff09\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u5efa\u6a21\u8303\u56f4\u4ece\u5236\u9020\u9636\u6bb5\u6269\u5c55\u5230\u518d\u5236\u9020/\u56de\u6536\u9636\u6bb5\u3002", "result": "\u901a\u8fc7\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u62c6\u89e3\u7528\u4f8b\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u7075\u6d3b\u9ad8\u6548\u5730\u89e3\u51b3\u4e0d\u540c\u7c7b\u578b\u7535\u6c60\u7684\u62c6\u89e3\u6311\u6218\uff0c\u51cf\u5c11\u751f\u6001\u5f71\u54cd\u5e76\u589e\u5f3a\u53ef\u6301\u7eed\u6027\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7ed3\u5408Bi-PAN\u8868\u793a\u80fd\u591f\u6709\u6548\u652f\u6301\u5faa\u73af\u7ecf\u6d4e\u4e2d\u7684\u4ea7\u54c1\u62c6\u89e3\u8fc7\u7a0b\uff0c\u4e3a\u53ef\u6301\u7eed\u5236\u9020\u548c\u56de\u6536\u63d0\u4f9b\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2510.02178", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02178", "abs": "https://arxiv.org/abs/2510.02178", "authors": ["Jialin Gao", "Donghao Zhou", "Mingjian Liang", "Lihao Liu", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis", "comment": null, "summary": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.", "AI": {"tldr": "DisCo-Layout\u662f\u4e00\u4e2a\u89e3\u8026\u7269\u7406\u548c\u8bed\u4e49\u7cbe\u70bc\u76843D\u5ba4\u5185\u5e03\u5c40\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7cbe\u70bc\u5de5\u5177\u548c\u7269\u7406\u7cbe\u70bc\u5de5\u5177\u5206\u522b\u5904\u7406\u62bd\u8c61\u5173\u7cfb\u548c\u5177\u4f53\u7a7a\u95f4\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u667a\u80fd\u5e03\u5c40\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u7531\u4e8e\u56fa\u5b9a\u6570\u636e\u96c6\u800c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u73b0\u6709LLM\u548cVLM\u65b9\u6cd5\u867d\u7136\u8bed\u4e49\u4e30\u5bcc\u4f46\u7f3a\u4e4f\u9c81\u68d2\u7075\u6d3b\u7684\u7cbe\u70bc\u673a\u5236\uff0c\u5bfc\u81f4\u5e03\u5c40\u8d28\u91cf\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u534f\u8c03\u6846\u67b6\uff1a\u8bed\u4e49\u7cbe\u70bc\u5de5\u5177\u4fee\u6b63\u62bd\u8c61\u5bf9\u8c61\u5173\u7cfb\uff0c\u7269\u7406\u7cbe\u70bc\u5de5\u5177\u901a\u8fc7\u7f51\u683c\u5339\u914d\u7b97\u6cd5\u89e3\u51b3\u5177\u4f53\u7a7a\u95f4\u95ee\u9898\uff1b\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5305\u62ec\u89c4\u5212\u5668\u3001\u8bbe\u8ba1\u5668\u548c\u8bc4\u4f30\u5668\u8fdb\u884c\u534f\u4f5c\u7cbe\u70bc\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDisCo-Layout\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u80fd\u751f\u6210\u771f\u5b9e\u3001\u8fde\u8d2f\u4e14\u53ef\u6cdb\u5316\u76843D\u5ba4\u5185\u5e03\u5c40\u3002", "conclusion": "DisCo-Layout\u901a\u8fc7\u89e3\u8026\u7269\u7406\u548c\u8bed\u4e49\u7cbe\u70bc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u5ba4\u5185\u5e03\u5c40\u5408\u6210\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2510.02248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02248", "abs": "https://arxiv.org/abs/2510.02248", "authors": ["Yan Miao", "Ege Yuceel", "Georgios Fainekos", "Bardh Hoxha", "Hideki Okamoto", "Sayan Mitra"], "title": "Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0", "comment": null, "summary": "Visual policy design is crucial for aerial navigation. However,\nstate-of-the-art visual policies often overfit to a single track and their\nperformance degrades when track geometry changes. We develop FalconGym 2.0, a\nphotorealistic simulation framework built on Gaussian Splatting (GSplat) with\nan Edit API that programmatically generates diverse static and dynamic tracks\nin milliseconds. Leveraging FalconGym 2.0's editability, we propose a\nPerformance-Guided Refinement (PGR) algorithm, which concentrates visual\npolicy's training on challenging tracks while iteratively improving its\nperformance. Across two case studies (fixed-wing UAVs and quadrotors) with\ndistinct dynamics and environments, we show that a single visual policy trained\nwith PGR in FalconGym 2.0 outperforms state-of-the-art baselines in\ngeneralization and robustness: it generalizes to three unseen tracks with 100%\nsuccess without per-track retraining and maintains higher success rates under\ngate-pose perturbations. Finally, we demonstrate that the visual policy trained\nwith PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a\nquadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30\ntrials spanning two three-gate tracks and a moving-gate track.", "AI": {"tldr": "FalconGym 2.0\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u903c\u771f\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u6027\u80fd\u5f15\u5bfc\u7cbe\u70bc\u7b97\u6cd5\u8bad\u7ec3\u89c6\u89c9\u7b56\u7565\uff0c\u5728\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u7b56\u7565\u5728\u822a\u8ff9\u51e0\u4f55\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fc7\u62df\u5408\u5355\u4e00\u822a\u8ff9\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1FalconGym 2.0\u6a21\u62df\u6846\u67b6\uff0c\u63d0\u51fa\u6027\u80fd\u5f15\u5bfc\u7cbe\u70bc\u7b97\u6cd5\uff0c\u5728\u6311\u6218\u6027\u822a\u8ff9\u4e0a\u96c6\u4e2d\u8bad\u7ec3\u89c6\u89c9\u7b56\u7565\u3002", "result": "\u8bad\u7ec3\u51fa\u7684\u5355\u4e00\u89c6\u89c9\u7b56\u7565\u5728\u4e09\u4e2a\u672a\u89c1\u822a\u8ff9\u4e0a\u5b9e\u73b0100%\u6210\u529f\u7387\uff0c\u5728\u95e8\u59ff\u6001\u6270\u52a8\u4e0b\u4fdd\u6301\u66f4\u9ad8\u6210\u529f\u7387\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u56db\u65cb\u7ffc\u786c\u4ef6\uff0c\u8fbe\u523098.6%\u6210\u529f\u7387\u3002", "conclusion": "FalconGym 2.0\u7ed3\u5408PGR\u7b97\u6cd5\u80fd\u591f\u8bad\u7ec3\u51fa\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u7684\u89c6\u89c9\u7b56\u7565\uff0c\u652f\u6301\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u3002"}}
{"id": "2510.02252", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02252", "abs": "https://arxiv.org/abs/2510.02252", "authors": ["Joao Pedro Araujo", "Yanjie Ze", "Pei Xu", "Jiajun Wu", "C. Karen Liu"], "title": "Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking", "comment": null, "summary": "Humanoid motion tracking policies are central to building teleoperation\npipelines and hierarchical controllers, yet they face a fundamental challenge:\nthe embodiment gap between humans and humanoid robots. Current approaches\naddress this gap by retargeting human motion data to humanoid embodiments and\nthen training reinforcement learning (RL) policies to imitate these reference\ntrajectories. However, artifacts introduced during retargeting, such as foot\nsliding, self-penetration, and physically infeasible motion are often left in\nthe reference trajectories for the RL policy to correct. While prior work has\ndemonstrated motion tracking abilities, they often require extensive reward\nengineering and domain randomization to succeed. In this paper, we\nsystematically evaluate how retargeting quality affects policy performance when\nexcessive reward tuning is suppressed. To address issues that we identify with\nexisting retargeting methods, we propose a new retargeting method, General\nMotion Retargeting (GMR). We evaluate GMR alongside two open-source\nretargeters, PHC and ProtoMotions, as well as with a high-quality closed-source\ndataset from Unitree. Using BeyondMimic for policy training, we isolate\nretargeting effects without reward tuning. Our experiments on a diverse subset\nof the LAFAN1 dataset reveal that while most motions can be tracked, artifacts\nin retargeted data significantly reduce policy robustness, particularly for\ndynamic or long sequences. GMR consistently outperforms existing open-source\nmethods in both tracking performance and faithfulness to the source motion,\nachieving perceptual fidelity and policy success rates close to the\nclosed-source baseline. Website:\nhttps://jaraujo98.github.io/retargeting_matters. Code:\nhttps://github.com/YanjieZe/GMR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5f62\u8fd0\u52a8\u91cd\u5b9a\u5411\u65b9\u6cd5GMR\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u91cd\u5b9a\u5411\u8d28\u91cf\u5bf9\u7b56\u7565\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u8db3\u90e8\u6ed1\u52a8\u3001\u81ea\u7a7f\u900f\u548c\u7269\u7406\u4e0d\u53ef\u884c\u8fd0\u52a8\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\u9762\u4e34\u4eba\u4f53\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u5177\u8eab\u5dee\u8ddd\u95ee\u9898\uff0c\u73b0\u6709\u91cd\u5b9a\u5411\u65b9\u6cd5\u5728\u53c2\u8003\u8f68\u8ff9\u4e2d\u7559\u4e0b\u7684\u4f2a\u5f71\u9700\u8981\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u4fee\u6b63\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u5956\u52b1\u5de5\u7a0b\u548c\u9886\u57df\u968f\u673a\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u901a\u7528\u8fd0\u52a8\u91cd\u5b9a\u5411(GMR)\u65b9\u6cd5\uff0c\u5e76\u4e0ePHC\u3001ProtoMotions\u7b49\u5f00\u6e90\u91cd\u5b9a\u5411\u5668\u4ee5\u53caUnitree\u7684\u9ad8\u8d28\u91cf\u95ed\u6e90\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f7f\u7528BeyondMimic\u8fdb\u884c\u7b56\u7565\u8bad\u7ec3\u4ee5\u9694\u79bb\u91cd\u5b9a\u5411\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u591a\u6570\u8fd0\u52a8\u53ef\u4ee5\u88ab\u8ddf\u8e2a\uff0c\u4f46\u91cd\u5b9a\u5411\u6570\u636e\u4e2d\u7684\u4f2a\u5f71\u663e\u8457\u964d\u4f4e\u4e86\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u6216\u957f\u5e8f\u5217\u4e2d\u3002GMR\u5728\u8ddf\u8e2a\u6027\u80fd\u548c\u6e90\u8fd0\u52a8\u4fdd\u771f\u5ea6\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u65b9\u6cd5\u3002", "conclusion": "GMR\u5b9e\u73b0\u4e86\u63a5\u8fd1\u95ed\u6e90\u57fa\u7ebf\u7684\u611f\u77e5\u4fdd\u771f\u5ea6\u548c\u7b56\u7565\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u9ad8\u8d28\u91cf\u91cd\u5b9a\u5411\u5bf9\u4e8e\u4eba\u5f62\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.02268", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02268", "abs": "https://arxiv.org/abs/2510.02268", "authors": ["Tianchong Jiang", "Jingtian Ji", "Xiangshan Tan", "Jiading Fang", "Anand Bhattad", "Vitor Guizilini", "Matthew R. Walter"], "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning", "comment": "Code and project materials are available at\n  ripl.github.io/know_your_camera", "summary": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .", "AI": {"tldr": "\u901a\u8fc7\u663e\u5f0f\u5730\u5c06\u7b56\u7565\u4e0e\u76f8\u673a\u5916\u53c2\u6761\u4ef6\u5316\uff0c\u7814\u7a76\u89c6\u89d2\u4e0d\u53d8\u6a21\u4eff\u5b66\u4e60\u3002\u4f7f\u7528Plucker\u5d4c\u5165\u7684\u50cf\u7d20\u5149\u7ebf\uff0c\u8bc1\u660e\u6761\u4ef6\u5316\u5916\u53c2\u663e\u8457\u63d0\u5347\u4e86\u6807\u51c6\u884c\u4e3a\u514b\u9686\u7b56\u7565\u5728\u8de8\u89c6\u89d2\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u907f\u514d\u7b56\u7565\u4f9d\u8d56\u9759\u6001\u80cc\u666f\u7b49\u89c6\u89c9\u7ebf\u7d22\u6765\u63a8\u65ad\u76f8\u673a\u59ff\u6001\uff0c\u4ece\u800c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u6cdb\u5316\u3002", "method": "\u4f7f\u7528Plucker\u5d4c\u5165\u8868\u793a\u50cf\u7d20\u5149\u7ebf\uff0c\u5c06\u76f8\u673a\u5916\u53c2\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u5230\u7b56\u7565\u4e2d\u3002\u5728RoboSuite\u548cManiSkill\u4e2d\u8bbe\u8ba1\u4e866\u4e2a\u64cd\u4f5c\u4efb\u52a1\uff0c\u5305\u542b\u56fa\u5b9a\u548c\u968f\u673a\u5316\u573a\u666f\u53d8\u4f53\uff0c\u5206\u79bb\u80cc\u666f\u7ebf\u7d22\u548c\u76f8\u673a\u59ff\u6001\u7684\u5f71\u54cd\u3002", "result": "\u65e0\u5916\u53c2\u6761\u4ef6\u7684\u7b56\u7565\u5728\u56fa\u5b9a\u573a\u666f\u4e2d\u4f1a\u5229\u7528\u9759\u6001\u80cc\u666f\u63a8\u65ad\u76f8\u673a\u59ff\u6001\uff0c\u4f46\u5728\u5de5\u4f5c\u7a7a\u95f4\u51e0\u4f55\u6216\u76f8\u673a\u4f4d\u7f6e\u53d8\u5316\u65f6\u6027\u80fd\u5d29\u6e83\u3002\u6761\u4ef6\u5316\u5916\u53c2\u6062\u590d\u4e86\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u4ec5\u4f7f\u7528RGB\u7684\u9c81\u68d2\u63a7\u5236\u3002", "conclusion": "\u663e\u5f0f\u6761\u4ef6\u5316\u76f8\u673a\u5916\u53c2\u662f\u5b9e\u73b0\u89c6\u89d2\u4e0d\u53d8\u6a21\u4eff\u5b66\u4e60\u7684\u5173\u952e\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7b56\u7565\u5728\u771f\u5b9e\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.02298", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02298", "abs": "https://arxiv.org/abs/2510.02298", "authors": ["Wenye Yu", "Jun Lv", "Zixi Ying", "Yang Jin", "Chuan Wen", "Cewu Lu"], "title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation", "comment": null, "summary": "Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.", "AI": {"tldr": "ARMADA\u662f\u4e00\u4e2a\u591a\u673a\u5668\u4eba\u90e8\u7f72\u548c\u9002\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7FLOAT\u81ea\u4e3b\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\u51cf\u5c11\u5bf9\u4eba\u7c7b\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u5e76\u884c\u7b56\u7565\u90e8\u7f72\u548c\u9ad8\u6548\u9886\u57df\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9884\u8bad\u7ec3\u7b56\u7565\u5728\u7f3a\u4e4f\u9886\u57df\u6570\u636e\u65f6\u6027\u80fd\u8f83\u5dee\uff0c\u4e14\u4eba\u5de5\u6536\u96c6\u6f14\u793a\u6570\u636e\u6210\u672c\u9ad8\u3001\u8d28\u91cf\u4e0d\u4e00\u3002\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u9700\u8981\u5168\u7a0b\u4eba\u5de5\u76d1\u7763\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1ARMADA\u7cfb\u7edf\uff0c\u91c7\u7528FLOAT\u81ea\u4e3b\u5728\u7ebf\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5e76\u884c\u7b56\u7565\u90e8\u7f72\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8bf7\u6c42\u4eba\u5de5\u5e72\u9884\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u76d1\u7763\u7684\u4f9d\u8d56\u3002", "result": "FLOAT\u6545\u969c\u68c0\u6d4b\u51c6\u786e\u7387\u5e73\u5747\u8fbe\u5230\u8fd195%\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u534720%\u4ee5\u4e0a\u3002ARMADA\u5728\u591a\u6b21\u7b56\u7565\u90e8\u7f72\u548c\u540e\u8bad\u7ec3\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u9ad84\u500d\u4ee5\u4e0a\uff0c\u4eba\u5de5\u5e72\u9884\u7387\u964d\u4f4e2\u500d\u4ee5\u4e0a\u3002", "conclusion": "ARMADA\u7cfb\u7edf\u901a\u8fc7FLOAT\u6545\u969c\u68c0\u6d4b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u673a\u5668\u4eba\u90e8\u7f72\u548c\u9002\u5e94\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u7c7b\u76d1\u7763\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u9886\u57df\u6570\u636e\u6536\u96c6\u6548\u7387\u3002"}}
