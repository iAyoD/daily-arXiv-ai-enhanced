<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 38]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: 提出了一种在通信受限环境下处理非对称障碍物和多智能体任务分配的分布式协调方法


<details>
  <summary>Details</summary>
Motivation: 解决在通信能力有限、环境部分可观测且存在主动非对称障碍物的多智能体系统中，任务频繁重新分配的问题

Method: 受市场机制启发的分布式协调算法，专门处理非对称障碍物，适用于低通信场景

Result: 在仿真和真实NAO机器人实验中验证，在有限通信设置下任务重叠显著减少52%

Conclusion: 该方法能有效应对通信条件差、环境部分可观测且存在主动非对称障碍物的复杂多智能体协调场景

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [2] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: 本文提出了一种基于3D纤维缠绕技术的轻量化无人机框架制造方法，使用连续单丝纤维构建面心立方晶格结构，实现了比传统金属和热塑性材料高4-8倍的比强度，重量减轻10%，飞行时间延长3分钟。


<details>
  <summary>Details</summary>
Motivation: 航空航天和机器人领域对轻量化高强度复合材料结构的需求日益增长，但传统复合材料制造方法难以实现复杂的3D架构，存在连接点薄弱和连续纤维增强困难的问题。

Method: 采用3D纤维缠绕（3DFiT）技术，使用连续单丝纤维构建面心立方（FCC）晶格结构的无人机框架，确保精确的纤维排列，消除传统复合材料组装的薄弱点。

Result: 制造的无人机框架重量仅260克，比商用DJI F450框架轻10%，比强度达到金属和热塑性材料的4-8倍，飞行测试显示其稳定性和耐久性良好，飞行时间延长3分钟。

Conclusion: 基于单丝晶格桁架的无人机框架具有巨大潜力，3DFiT技术作为一种可扩展的高效制造方法，为轻量化复合材料结构制造提供了新的解决方案。

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [3] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出基于流场的运动规划方法KoopMotion，使用Koopman算子参数化动力学系统，使机器人从任意初始状态收敛到期望轨迹终点，在LASA手写数据集和3D机械臂轨迹上验证有效


<details>
  <summary>Details</summary>
Motivation: 虽然Koopman算子理论在动力学系统建模中表现出色，但它本身不能保证收敛到期望轨迹或指定目标点，而这是从演示学习(LfD)的关键需求

Method: KoopMotion将运动流场表示为动力学系统，用Koopman算子参数化来模仿期望轨迹，利用学习流场的发散特性获得平滑运动场，使机器人偏离期望轨迹时能收敛回轨迹并跟踪至终点

Result: 在LASA手写数据集和3D机械臂末端轨迹数据集上评估显示优异性能，包括谱分析。在微型自主水面车辆的非静态流体环境中物理实验验证。仅需LASA数据集的3%即可生成密集运动规划，时空建模效果显著优于基线

Conclusion: KoopMotion方法具有高度样本效率，在空间和时间维度都表现出色，为从演示学习的运动规划提供了有效的解决方案，特别是在非静态环境中的实际机器人应用

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [4] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: 提出了一种欠驱动变形加载机械手(UMLM)，通过几何约束实现拓扑重构和灵活运动轨迹，无需额外驱动器，结合被动自适应夹具实现多样化物体抓取。


<details>
  <summary>Details</summary>
Motivation: 解决传统固定自由度加载机构存在驱动器过多、控制复杂、对动态任务适应性有限的问题，需要开发更高效、适应性强的加载解决方案。

Method: 集成变形臂和被动自适应夹具，利用几何约束实现拓扑重构，进行运动静力学分析，使用粒子群优化算法优化夹具尺寸参数。

Result: 仿真验证了UMLM控制策略易于实现、操作灵活，在动态环境中能有效抓取多样化物体。

Conclusion: 欠驱动变形机构在需要高效适应性加载的应用中具有实际潜力，所提出的建模优化框架可扩展到更广泛的机械手类别。

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [5] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: 提出基于线性倒立摆模型(LIPM)的新型奖励设计，通过调节质心高度和躯干方向实现双足机器人在非结构化户外环境中的稳定感知运动


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在复杂地形和外部干扰下的稳定感知运动挑战，需要确保机器人摄像头的稳定视角以实现地形感知

Method: 设计基于LIPM理论的奖励函数，促进平衡和动态稳定性；采用奖励融合模块(RFM)自适应权衡速度跟踪与稳定性；使用双评论家架构分别评估稳定性和运动目标

Result: 在仿真和真实户外环境中验证，展示了优异的地形适应性、干扰抑制能力，以及在各种速度和感知条件下的稳定性能

Conclusion: 基于LIPM的奖励设计和双评论家架构有效提升了双足机器人在非结构化环境中的感知运动稳定性和鲁棒性

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [6] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: AEOS是一个受猫头鹰主动感知启发的自适应LiDAR控制框架，结合MPC和RL，通过预测姿态可观测性和学习隐式代价图来优化无人机LiDAR-惯性里程计性能


<details>
  <summary>Details</summary>
Motivation: 解决无人机LiDAR感知和定位中因窄视场和载荷限制导致的性能下降问题，传统固定转速扫描系统缺乏场景感知和任务适应性

Method: 采用混合架构：模型预测控制(MPC)预测未来姿态可观测性，轻量级神经网络从全景深度表示学习隐式代价图指导探索，开发点云仿真环境支持训练

Result: 在仿真和真实环境实验中，AEOS相比固定速率、纯优化和全学习基线方法显著提高了里程计精度，同时保持实时性能

Conclusion: AEOS框架通过生物启发式主动感知方法有效解决了无人机LiDAR扫描的适应性挑战，实现了在复杂遮挡环境中的优越性能

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [7] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: 提出了一种自动驾驶代客泊车系统，通过预测停车位未来占用情况和动态代理运动，结合概率估计和信息增益策略规划，显著提升泊车效率和安全性能


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖瞬时观测或静态假设，无法在动态不确定环境中准确预测停车位可用性，需要一种能够区分初始空置/占用车位、预测动态代理运动并处理有限视野和不确定性的方法

Method: 开发概率停车位占用估计器处理有限视野中的部分噪声观测，考虑未观测区域的不确定性；设计策略规划器基于信息增益自适应平衡目标导向泊车和探索导航，智能整合等待行为

Result: 通过模拟大型停车场随机实验，证明该框架相比现有方法显著提高了泊车效率、安全裕度和轨迹平滑度

Conclusion: 该框架为动态不确定环境中的自动驾驶代客泊车提供了有效的未来停车位可用性预测和集成规划解决方案

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [8] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: 提出Redundant Estimator Network (RENet)框架，通过双估计器架构解决四足机器人在户外环境中视觉定位的挑战，特别是在视觉感知退化时保持稳定运动性能。


<details>
  <summary>Details</summary>
Motivation: 户外环境中四足机器人的视觉定位面临环境预测不准确和深度传感器噪声等挑战，严重限制了算法的实际应用。需要解决部署过程中的视觉故障问题。

Method: 采用双估计器架构的RENet框架，通过在线估计器自适应机制，在处理视觉感知不确定性时实现估计模块之间的无缝切换。

Result: 在真实机器人上的实验验证表明，该框架在复杂户外环境中有效，特别是在视觉感知退化场景中表现出明显优势。

Conclusion: 该框架为解决挑战性野外条件下可靠机器人部署提供了实用解决方案，展示了在实际应用中的潜力。

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [9] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: OmniEVA是一个多模态大语言模型，通过任务自适应3D接地机制和具身感知推理框架，解决了现有具身智能系统在几何适应性和物理约束方面的局限性，实现了先进的具身推理和任务规划。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的具身系统存在两个关键局限：几何适应性差距（2D输入或硬编码3D几何注入导致空间信息不足或泛化受限）和具身约束差距（忽视真实机器人的物理约束和能力，导致计划理论上有效但实际不可行）。

Method: 提出两个关键创新：1）任务自适应3D接地机制，使用门控路由器根据上下文需求进行选择性3D融合调节；2）具身感知推理框架，将任务目标和具身约束共同纳入推理循环。

Result: 实验结果表明OmniEVA不仅实现了最先进的通用具身推理性能，还在广泛的下游场景中表现出强大能力，在一系列具身基准测试中确认了其鲁棒和通用的规划能力。

Conclusion: OmniEVA通过创新的3D接地和具身感知推理方法，有效解决了现有具身系统的局限性，为多模态大语言模型在具身智能领域的应用提供了新的解决方案。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [10] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: AGILOped是一个开源人形机器人，旨在填补高性能与可访问性之间的差距，使用现成的驱动器和标准电子组件，高度110cm，重量14.5kg，单人即可操作。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人系统大多闭源或成本高昂，需要开发一个既高性能又易于获取的开源平台来促进研究。

Method: 采用现成的可反向驱动驱动器和高功率密度组件，使用标准电子元件，设计轻量化结构（14.5kg）。

Result: 实验展示了机器人在行走、跳跃、冲击缓解和起身等任务中的可行性能。

Conclusion: AGILOped成功证明了开源人形机器人可以在保持高性能的同时实现可访问性，为研究社区提供了实用平台。

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [11] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: VLA-Adapter是一种轻量级适配器方法，通过桥接注意力机制将视觉语言表示有效连接到动作空间，无需大规模预训练即可实现高性能VLA模型


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型依赖大规模视觉语言模型和机器人数据预训练，成本高昂。本文旨在降低这种依赖，探索如何有效桥接视觉语言表示与动作空间

Method: 提出VLA-Adapter范式，包含轻量级策略模块和桥接注意力机制，自动将最优条件注入动作空间，仅需0.5B参数主干网络且无需机器人数据预训练

Result: 在仿真和真实机器人基准测试中达到最先进性能，推理速度最快，单张消费级GPU仅需8小时即可训练出强大VLA模型

Conclusion: VLA-Adapter显著降低了VLA模型的部署门槛，提供了一种高效桥接感知与动作空间的轻量级解决方案

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [12] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: 提出了一种疲劳感知的连续体机器人设计，通过混合铰链-梁结构、被动限位器和实时疲劳感知方法，显著减少疲劳积累并实现安全可靠的长期操作。


<details>
  <summary>Details</summary>
Motivation: 电缆驱动的连续体机器人虽然灵活轻便，但长期使用会产生机械疲劳和材料退化，影响性能并可能导致结构失效。现有研究对连续体机器人的疲劳估计研究不足，限制了其长期操作能力。

Method: 1) 混合铰链-梁结构：TwistBeam和BendBeam解耦扭转和弯曲，被动旋转关节减轻应力集中；2) 被动限位器：通过机械约束安全限制运动，利用电机扭矩传感检测极限扭矩；3) 实时疲劳感知方法：在极限位姿下从电机扭矩估计刚度，实现在线疲劳估计。

Result: 实验表明，与传统设计相比，所提设计减少约49%的疲劳积累，被动机械限制与电机侧传感结合能够准确估计结构疲劳和损伤。

Conclusion: 该架构有效实现了连续体机器人的安全可靠长期操作，为解决机械疲劳问题提供了创新解决方案。

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [13] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于自适应感兴趣结构(SOI)操作策略的双机械臂自动装袋系统，能够根据实时视觉反馈动态调整动作，无需预先了解袋子属性。


<details>
  <summary>Details</summary>
Motivation: 工业场景中的装袋任务具有挑战性，因为可变形袋子的复杂性和不可预测性。需要开发能够适应不同袋子特性的自动化系统。

Method: 使用高斯混合模型(GMM)估计SOI状态，优化技术生成SOI，通过约束双向快速探索随机树(CBiRRT)进行运动规划，采用模型预测控制(MPC)实现双机械臂协调。

Result: 大量实验验证了系统在各种物体上执行精确和鲁棒装袋的能力，展示了其适应性。

Conclusion: 这项工作为机器人可变形物体操作(DOM)，特别是在自动化装袋任务中提供了新的解决方案。

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [14] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: SMapper是一个开源的硬件平台，专为SLAM研究设计，集成了同步的LiDAR、多摄像头和惯性传感，并提供了校准和同步管道。论文还发布了SMapper-light数据集，包含室内外序列，用于SLAM算法的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前SLAM和自主导航研究缺乏可靠且可复现的多模态数据集，现有数据集在传感模态、环境多样性和硬件设置可复现性方面存在局限。

Method: 开发了开源的SMapper硬件平台，集成了同步的LiDAR、多摄像头和惯性传感，并建立了校准和同步管道。发布了SMapper-light数据集，包含室内外序列和地面真实轨迹。

Result: SMapper平台实现了精确的时空对齐，SMapper-light数据集提供了亚厘米精度的地面真实轨迹和密集3D重建。论文还展示了在先进LiDAR和视觉SLAM框架上的基准测试结果。

Conclusion: 通过开源硬件设计、可复现的数据收集和全面的基准测试，SMapper为SLAM算法开发、评估和可复现性奠定了坚实基础。

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [15] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 基于NeuroTac传感器和脉冲卷积神经网络的神经形态触觉传感系统，用于早期滑移检测，在动态重力诱导滑移条件下能够提前360ms检测到初始滑移


<details>
  <summary>Details</summary>
Motivation: 在边缘平台上部署滑移检测系统面临能量约束挑战，需要开发低功耗的早期滑移检测方案来增强机器人操作安全性

Method: 使用NeuroTac传感器（具有突出乳头状皮肤）和脉冲卷积神经网络（SCNN）进行滑移状态分类，通过传感器运动诱导滑移条件，并对最终层脉冲计数进行时间平滑处理

Result: SCNN模型在三类滑移状态（无滑移、初始滑移、完全滑移）分类准确率达到94.33%，在动态重力诱导滑移验证条件下，系统在所有试验中都能在完全滑移发生前至少360ms检测到初始滑移

Conclusion: 该神经形态系统具有稳定且响应迅速的初始滑移检测能力，为解决边缘平台的能源约束问题提供了有效解决方案

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [16] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 提出了一种基于对象相对控制的视觉导航新范式，使用相对3D场景图作为拓扑地图表示，训练ObjectReact局部控制器，在跨传感器高度变化和反向导航等任务中优于图像相对方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基于图像的视觉导航方法受限于agent姿态和具体实现方式，而对象作为地图属性提供了与实现方式和轨迹无关的世界表示，能够实现更好的跨部署泛化能力。

Method: 提出相对3D场景图作为拓扑地图表示，用于获取更丰富的对象级全局路径规划成本。训练ObjectReact局部控制器，直接基于高级WayObject Costmap表示，无需显式RGB输入。

Result: 在传感器高度变化和多种导航任务中，对象相对控制方法优于图像相对方法，特别是在挑战空间理解能力的任务中（如反向导航）。仅使用模拟训练的策略能够很好地泛化到真实室内环境。

Conclusion: 对象相对控制范式具有多个优势：无需严格模仿先验经验即可遍历新路线、控制预测问题可与图像匹配问题解耦、在跨实现方式部署中实现高不变性，为视觉导航提供了更鲁棒的解决方案。

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [17] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: 开发了能够全身膨胀收缩的机器人MOFU，研究发现膨胀收缩运动能显著增强机器人感知的生命感，但双机器人配置并未进一步提升生命感


<details>
  <summary>Details</summary>
Motivation: 现有治疗和社交机器人主要模仿外观和关节运动，但忽视了生物体中观察到的全身膨胀收缩这种体积变化运动对生命感感知的影响

Method: 开发了MOFU机器人，采用Jitterbug几何变换机制实现单电机驱动的全身膨胀收缩，并通过在线视频调查使用Godspeed问卷评估不同运动模式对生命感感知的影响

Result: 膨胀收缩运动显著提高了感知的生命感；双机器人配置相比单机器人未产生显著差异；膨胀收缩与运动结合比单纯运动产生更高的生命感评分

Conclusion: 体积变化运动如膨胀收缩能增强机器人的感知生命感，应作为未来机器人设计中塑造人类印象的重要设计元素

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [18] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: Dexplore是一个统一的单循环优化框架，直接从动作捕捉数据中学习机器人控制策略，避免了传统三阶段方法中的误差累积问题


<details>
  <summary>Details</summary>
Motivation: 现有方法采用重定向、跟踪和残差校正的三阶段工作流程，导致演示数据利用不足且误差在阶段间累积，限制了人类动作捕捉数据在机器人灵巧操作中的有效应用

Method: 提出统一单循环优化，将重定向和跟踪联合执行，将演示作为软指导而非绝对真值，从原始轨迹推导自适应空间范围，使用强化学习训练策略保持在范围内同时最小化控制努力并完成任务

Result: 统一框架保留了演示意图，使机器人特定策略得以涌现，提高了对噪声的鲁棒性，并能扩展到大规模演示语料库

Conclusion: Dexplore作为一个原则性桥梁，将不完美的演示转化为灵巧操作的有效训练信号，并通过蒸馏生成支持跨对象泛化和真实世界部署的视觉基础技能条件生成控制器

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [19] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: SimpleVLA-RL是一个针对视觉-语言-动作模型的强化学习框架，通过引入VLA特定的轨迹采样、并行化、多环境渲染和优化损失计算，显著提升了机器人操作的长期动作规划能力，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型面临的两个核心挑战：(1)大规模人类操作机器人轨迹数据的稀缺性和高成本问题；(2)在分布偏移任务中的泛化能力有限。受大型推理模型通过强化学习提升推理能力的启发，探索RL是否能类似地改善VLA模型的长期动作规划。

Method: 基于veRL框架，开发了SimpleVLA-RL框架，包含VLA特定的轨迹采样、可扩展并行化、多环境渲染和优化损失计算。应用于OpenVLA-OFT模型，并引入了探索增强策略。

Result: 在LIBERO基准测试中达到最先进性能，在RoboTwin 1.0和2.0上甚至超越了π0基准。不仅减少了对大规模数据的依赖，实现了鲁棒泛化，而且在真实世界任务中显著超越了监督微调方法。发现了RL训练中的新现象"pushcut"。

Conclusion: SimpleVLA-RL为VLA模型提供了一个高效的RL框架，成功解决了数据稀缺和泛化问题，证明了强化学习在提升视觉-语言-动作模型长期动作规划能力方面的有效性，为机器人操作任务提供了新的解决方案。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>


### [20] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: 提出了一种针对通信受限环境下分布式多智能体系统的任务分配方法，特别处理非对称障碍物和部分可观测环境


<details>
  <summary>Details</summary>
Motivation: 解决在通信带宽和载荷受限、环境部分可观测且存在活跃非对称障碍物的情况下，多智能体系统的协调挑战

Method: 基于市场机制的任务分配方法，设计分布式协调算法，特别考虑非对称障碍物的处理

Result: 在仿真和真实NAO机器人实验中验证，在有限通信环境下任务重叠减少52%

Conclusion: 该方法能有效处理通信受限、非对称障碍物和部分可观测环境下的多智能体协调问题

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [21] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: 本研究提出了一种基于3D纤维系留技术的轻量化面心立方晶格结构无人机框架，使用连续单丝纤维实现精确纤维排列，消除了传统复合材料组装的弱点，使无人机框架重量减轻10%，比金属和热塑性材料具有4-8倍的比强度。


<details>
  <summary>Details</summary>
Motivation: 航空航天和机器人领域对轻量化高强度复合材料结构的需求日益增长，但传统复合材料制造方法难以实现复杂的3D架构，且组装部件会在连接处产生弱点，连续纤维增强也面临挑战。

Method: 采用3D纤维系留（3DFiT）技术，使用连续单丝纤维制造面心立方晶格结构无人机框架，确保精确的纤维排列，消除传统复合材料组装的弱点。

Result: 制造的无人机框架仅重260克，比商用DJI F450框架轻10%，比金属和热塑性材料具有4-8倍的比强度，飞行测试显示其稳定性和耐久性良好，飞行时间延长3分钟。

Conclusion: 基于单丝晶格桁架的无人机框架具有巨大潜力，3DFiT技术作为一种可扩展且高效的制造方法，为轻量化高强度复合材料结构制造提供了新途径。

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [22] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出基于Koopman算子的运动规划方法KoopMotion，通过流场建模使机器人从任意初始状态收敛到期望轨迹终点，在样本效率和时空动态建模方面显著优于基线方法


<details>
  <summary>Details</summary>
Motivation: 虽然Koopman算子理论在动力系统建模中表现出色，但它本身不能保证收敛到期望轨迹或指定目标，这在从演示学习中是一个重要需求

Method: KoopMotion将运动流场表示为动力系统，用Koopman算子参数化来模仿期望轨迹，利用学习流场的发散特性获得平滑运动场，使机器人偏离期望轨迹时仍能收敛并跟踪到终点

Result: 在LASA手写数据集和3D机械臂末端轨迹数据集上评估显示优异性能，物理机器人实验验证了在非静态流体环境中的有效性，仅需3%的LASA数据即可生成密集运动规划

Conclusion: KoopMotion方法在样本效率和时空动态建模方面显著改进，为机器人运动规划提供了高效可靠的解决方案

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [23] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: 提出了一种欠驱动变形加载机械臂(UMLM)，结合变形臂和被动自适应抓取器，通过几何约束实现拓扑重构和灵活运动轨迹，无需额外驱动器。


<details>
  <summary>Details</summary>
Motivation: 解决传统固定自由度加载机构存在驱动器过多、控制复杂、对动态任务适应性有限的问题，开发高效且适应性强的加载解决方案。

Method: 集成变形臂和被动自适应抓取器，利用几何约束实现拓扑重构，进行运动静力学分析，使用粒子群优化(PSO)优化抓取器尺寸参数。

Result: 仿真验证了UMLM易于实现的控制策略、操作多功能性以及在动态环境中抓取多样化物体的有效性。

Conclusion: 这项工作强调了欠驱动变形机构在需要高效和适应性加载解决方案应用中的实际潜力，其通用建模和优化框架可扩展到更广泛的机械臂类别。

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [24] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: 提出基于线性倒立摆模型(LIPM)的新型奖励设计，用于双足机器人在非结构化户外环境中的感知和稳定运动，通过双评论家架构和奖励融合模块实现地形适应性和抗干扰能力


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在复杂非结构化户外环境中由于地形几何复杂性和外部干扰导致的感知运动稳定性和鲁棒性挑战

Method: 基于LIPM理论设计奖励函数，调节质心高度和躯干方向以保持动态平衡；采用奖励融合模块(RFM)自适应权衡速度跟踪和稳定性；使用双评论家架构分别评估稳定性和运动目标

Result: 在仿真和真实户外环境中进行广泛实验，展示了优异的地形适应性、干扰抑制能力，以及在各种速度和感知条件下的一致性能

Conclusion: 所提出的LIPM启发式奖励设计和双评论家架构有效提升了双足机器人在野外环境中的感知运动稳定性和鲁棒性

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [25] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: AEOS是一个受猫头鹰主动感知启发的自适应LiDAR控制框架，结合模型预测控制和强化学习，在无人机LiDAR惯性里程计中显著提高了定位精度


<details>
  <summary>Details</summary>
Motivation: 解决无人机LiDAR感知中窄视场角和固定转速扫描系统缺乏场景感知能力的问题，传统方法在复杂遮挡环境中性能下降

Method: 采用混合架构：模型预测控制预测未来位姿可观测性，轻量级神经网络从全景深度表示学习隐式代价图指导探索，结合点云仿真环境实现sim-to-real迁移

Result: 在仿真和真实环境实验中，AEOS相比固定速率、纯优化和全学习基线方法显著提高了里程计精度，同时保持实时性能

Conclusion: AEOS框架成功解决了无人机LiDAR感知的视场限制和自适应扫描问题，为复杂环境中的精准定位提供了有效解决方案

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [26] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: 提出了一种自动驾驶代客泊车系统，通过预测停车位未来占用情况和动态智能体运动，结合概率估计和自适应规划策略，显著提升泊车效率和安全性能


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖瞬时观测或静态假设，无法准确预测动态不确定环境中的停车位可用性，需要一种能够处理部分观测、噪声和不确定性演变的综合方法

Method: 开发概率停车位占用估计器处理有限视野内的部分噪声观测，设计自适应策略规划器平衡目标导向泊车与探索导航，智能整合等待-前进行为

Result: 通过大规模停车场随机模拟验证，该框架相比现有方法显著提高了泊车效率、安全裕度和轨迹平滑度

Conclusion: 该方法通过显式区分初始空置和占用车位、利用动态智能体运动预测，为自动驾驶代客泊车提供了更准确可靠的未来停车位可用性推理和集成规划能力

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [27] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: 提出了RENet框架，通过双估计器架构解决四足机器人在户外环境中视觉定位的挑战，确保在视觉感知失效时的稳定部署。


<details>
  <summary>Details</summary>
Motivation: 户外环境中基于视觉的定位面临环境预测不准确和深度传感器噪声等问题，严重限制了四足机器人的户外应用。

Method: 采用冗余估计器网络(RENet)框架，包含双估计器架构和在线估计器自适应机制，能够在视觉感知不确定时实现估计模块间的无缝切换。

Result: 在真实机器人上的实验验证表明，该框架在复杂户外环境中有效，特别是在视觉感知退化场景中表现出优势。

Conclusion: 该框架为在挑战性野外条件下实现可靠的机器人部署提供了实用解决方案。

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [28] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: OmniEVA是一个多模态大语言模型驱动的具身智能规划器，通过任务自适应3D接地机制和具身感知推理框架，解决了现有系统在几何适应性和物理约束方面的局限性，实现了先进的具身推理和任务规划。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的具身系统面临两个关键限制：几何适应性差距（2D输入或硬编码3D几何注入导致空间信息不足或泛化受限）和具身约束差距（忽略真实机器人的物理约束和能力，导致计划理论上有效但实际不可行）。

Method: 提出了两个关键创新：(1)任务自适应3D接地机制，使用门控路由器根据上下文需求进行选择性3D融合调节；(2)具身感知推理框架，将任务目标和具身约束联合纳入推理循环，确保规划决策既目标导向又可执行。

Result: 广泛的实验结果表明，OmniEVA不仅实现了最先进的通用具身推理性能，还在广泛的下游场景中展现出强大能力。在包括原始和复合任务在内的具身基准测试中证实了其鲁棒和通用的规划能力。

Conclusion: OmniEVA通过创新的3D接地和具身感知推理方法，有效解决了现有MLLM具身系统的局限性，为具身智能提供了更加实用和适应性强的规划解决方案。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [29] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: AGILOped是一个开源人形机器人，旨在填补高性能与可访问性之间的差距，使用现成的驱动器和标准电子元件，重量轻、体积小，单人即可操作。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人系统多为闭源或成本高昂，限制了研究和应用的可及性。本文旨在开发一个既高性能又易于获取的开源平台。

Method: 采用现成的反向驱动驱动器和高功率密度执行器，使用标准电子元件构建，机器人高度110厘米，重量仅14.5公斤，无需龙门架即可单人操作。

Result: 通过行走、跳跃、冲击缓解和起身等实验验证了机器人的可行性，展示了其在研究中的实用价值。

Conclusion: AGILOped成功实现了高性能与可访问性的平衡，为研究社区提供了一个实用的开源人形机器人平台。

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [30] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: VLA-Adapter是一种新颖的轻量级方法，通过桥接注意力机制将视觉语言表示有效连接到动作空间，无需大规模预训练即可实现高性能VLA模型


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型依赖大规模视觉语言模型和机器人数据预训练，成本高昂。本文旨在降低VLA模型对大规模VL模型和预训练的依赖

Method: 提出轻量级策略模块和桥接注意力机制，系统分析各种VL条件的有效性，自主将最优条件注入动作空间

Result: 仅使用0.5B参数主干网络，无需机器人数据预训练，在仿真和真实机器人基准测试中达到SOTA性能，推理速度最快，单消费级GPU仅需8小时训练

Conclusion: VLA-Adapter显著降低了VLA模型的部署门槛，为高效视觉语言动作建模提供了新范式

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [31] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: 提出了一种疲劳感知的连续体机器人设计，通过混合铰链-梁结构、被动限位器和实时疲劳感知方法，显著减少疲劳积累并实现安全可靠的长期操作。


<details>
  <summary>Details</summary>
Motivation: 电缆驱动的连续体机器人在长期使用中会产生机械疲劳和材料退化，影响性能并可能导致结构失效，而现有技术对连续体机器人的疲劳估计研究不足。

Method: 采用三种创新设计：(1)混合铰链-梁结构分离扭转和弯曲；(2)被动限位器通过机械约束安全限制运动；(3)基于电机扭矩的实时疲劳感知方法进行在线疲劳估计。

Result: 实验表明，与传统设计相比，该设计减少约49%的疲劳积累，被动机械限制与电机侧传感结合能够准确估计结构疲劳和损伤。

Conclusion: 所提出的架构在实现安全可靠的长期操作方面具有有效性，为连续体机器人的疲劳管理提供了实用解决方案。

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [32] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: 提出基于自适应感兴趣结构(SOI)的双机械臂自动装袋系统，通过实时视觉反馈动态调整动作，无需预先了解袋子属性


<details>
  <summary>Details</summary>
Motivation: 工业装袋任务面临可变形袋子的复杂性和不可预测性挑战，需要自动化解决方案

Method: 使用高斯混合模型(GMM)估计SOI状态，优化技术生成SOI，通过约束双向快速探索随机树(CBiRRT)进行运动规划，采用模型预测控制(MPC)实现双臂协调

Result: 大量实验验证系统能够对各种物体进行精确鲁棒的装袋操作，展示其适应性

Conclusion: 这项工作为机器人可变形物体操作(DOM)提供了新的解决方案，特别是在自动化装袋任务中

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [33] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: SMapper是一个开源的硬件平台，专为SLAM研究设计，集成了激光雷达、多摄像头和惯性传感器，提供精确的时空同步和校准。同时发布了SMapper-light数据集，包含室内外场景的多模态数据和地面真实轨迹，用于SLAM算法的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前SLAM和自主导航研究缺乏可靠、可复现的多模态数据集，现有数据集在传感模态、环境多样性和硬件设置可复现性方面存在局限。

Method: 开发了开源的SMapper硬件平台，集成同步的LiDAR、多摄像头和惯性传感，配备强大的校准和同步管道。发布了SMapper-light数据集，包含精确同步的多模态数据和基于LiDAR SLAM的高精度地面真实轨迹。

Result: 构建了完整的开源硬件平台和数据集，提供了精确的时空对齐数据。在先进LiDAR和视觉SLAM框架上进行了基准测试，验证了平台和数据集的有效性。

Conclusion: SMapper通过开源硬件设计、可复现数据收集和全面基准测试，为SLAM算法开发、评估和可复现性建立了坚实基础，推动了SLAM研究的发展。

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [34] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 基于NeuroTac传感器和脉冲卷积神经网络的神经形态触觉传感系统，用于早期滑移检测，在能量受限的边缘平台上实现高效滑移状态分类


<details>
  <summary>Details</summary>
Motivation: 检测早期滑移可以防止物体滑落并提高机器人操作安全性，但在边缘平台上部署此类系统面临能量约束的挑战

Method: 使用NeuroTac传感器（具有突出乳头状皮肤）和脉冲卷积神经网络（SCNN）进行滑移状态分类，包括无滑移、早期滑移和明显滑移三类

Result: SCNN模型在传感器运动诱导的滑移条件下达到94.33%的分类准确率；在动态重力诱导滑移验证条件下，系统在所有试验中至少提前360ms检测到早期滑移

Conclusion: 该神经形态系统具有稳定且响应迅速的早期滑移检测能力，适用于能量受限的边缘平台部署

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [35] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 提出了一种基于对象相对控制的新视觉导航范式，使用相对3D场景图作为拓扑地图表示，训练ObjectReact局部控制器，在传感器高度变化和反向导航等任务中优于图像相对方法，并能从仿真泛化到真实室内环境。


<details>
  <summary>Details</summary>
Motivation: 传统基于单摄像头和拓扑地图的视觉导航方法采用"图像相对"控制，但图像表示受限于智能体姿态和具体实现。对象作为地图属性提供了与实现方式和轨迹无关的世界表示，能够实现更好的泛化能力。

Method: 提出相对3D场景图作为拓扑地图表示，用于获取对象级别的全局路径规划成本。训练ObjectReact局部控制器，直接基于高级"WayObject Costmap"表示进行条件控制，无需显式RGB输入。

Result: 在传感器高度变化和多种导航任务（如反向导航）中，对象相对控制方法优于图像相对方法。仅使用仿真训练的策略能够很好地泛化到真实室内环境。

Conclusion: 对象相对控制范式具有多个优势：无需严格模仿先验经验即可遍历新路线、控制预测问题可与图像匹配问题解耦、在跨实现部署中实现高度不变性，为视觉导航提供了更鲁棒的解决方案。

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [36] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: 开发了能够进行全身伸缩运动的机器人MOFU，研究发现伸缩运动能显著提升机器人感知的生命感，但多机器人展示并未增强效果，伸缩与移动结合效果最佳


<details>
  <summary>Details</summary>
Motivation: 现有社交机器人主要模仿外观和关节运动，但忽视了生物体中常见的全身伸缩体积变化运动对生命感感知的影响

Method: 开发MOFU机器人，采用Jitterbug结构实现单电机驱动的全身伸缩，通过在线视频调查使用Godspeed问卷评估不同运动模式对生命感感知的影响

Result: 伸缩运动显著增加感知生命感；双机器人展示无显著差异；伸缩与移动结合比单纯移动产生更高的生命感评分

Conclusion: 体积变化运动如伸缩能增强机器人感知的生命感，应作为未来机器人设计中塑造人类印象的重要元素

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [37] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: Dexplore是一个统一的单循环优化方法，直接从大规模运动捕捉数据中学习机器人控制策略，避免了传统三阶段流程的误差累积问题


<details>
  <summary>Details</summary>
Motivation: 现有的手-物体运动捕捉数据存在不准确性和人-机器人手之间的本体差异，传统三阶段方法（重定向、跟踪、残差校正）导致演示数据利用不足和误差累积

Method: 采用统一的单循环优化，联合执行重定向和跟踪，将演示作为软指导而非绝对真值；从原始轨迹推导自适应空间范围，使用强化学习训练策略保持在范围内同时最小化控制努力并完成任务

Result: 统一公式保留了演示意图，使机器人特定策略得以出现，提高了对噪声的鲁棒性，并能扩展到大规模演示语料库；最终将跟踪策略提炼为基于视觉的技能条件生成控制器

Conclusion: Dexplore提供了一个原则性的桥梁，将不完美的演示转化为灵巧操作的有效训练信号，支持跨对象泛化和现实世界部署

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [38] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: SimpleVLA-RL是一个针对视觉-语言-动作模型的强化学习框架，通过引入特定轨迹采样、并行化等技术，显著提升了机器人操作任务的性能，减少了对大规模监督数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型面临的两个核心挑战：大规模人类操作轨迹数据的稀缺性和高成本，以及在分布偏移任务中的有限泛化能力。受大型推理模型通过强化学习提升推理能力的启发，探索RL是否能类似地提升VLA模型的长期动作规划能力。

Method: 基于veRL框架，引入了VLA特定的轨迹采样、可扩展并行化、多环境渲染和优化损失计算等技术，构建了高效的RL框架SimpleVLA-RL。

Result: 在OpenVLA-OFT上应用SimpleVLA-RL，在LIBERO基准上达到最先进性能，在RoboTwin 1.0和2.0上甚至超越了π_0基准。减少了大规模数据依赖，实现了鲁棒泛化，在真实世界任务中显著超越了监督微调方法。

Conclusion: SimpleVLA-RL不仅证明了强化学习可以有效提升VLA模型的长期动作规划能力，还发现了RL训练过程中的新现象"pushcut"，即策略能够发现训练过程中未见的新模式，为VLA模型的进一步发展提供了新方向。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>
