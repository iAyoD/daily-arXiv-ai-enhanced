<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Toward an Agricultural Operational Design Domain: A Framework](https://arxiv.org/abs/2511.02937)
*Mirco Felske,Jannik Redenius,Georg Happich,Julius Schöning*

Main category: cs.RO

TL;DR: 本文提出了农业运营设计域（Ag-ODD）框架，用于描述和验证自主农业系统的运营边界，解决现有ODD概念在农业应用中面临的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 农业自动化系统在复杂多变的环境中运行，需要整合驾驶和工作过程，每个过程都有不同的运营约束。现有ODD概念无法满足农业应用的特殊需求，需要一种结构化、透明且可验证的环境描述方法。

Method: Ag-ODD框架包含三个核心要素：1）Ag-ODD描述概念，使用ASAM Open ODD和CityGML概念结构化定义环境和运营参数；2）7层模型，在PEGASUS 6层模型基础上扩展了过程层以捕捉动态农业操作；3）迭代验证过程，验证Ag-ODD与其逻辑场景的一致性和完整性。

Result: 该框架为创建明确且可验证的Ag-ODD提供了一致的方法。示范性用例表明Ag-ODD框架能够支持自主农业系统环境描述的标准化和可扩展性。

Conclusion: Ag-ODD框架成功解决了农业自动化系统环境描述的特殊需求，为农业自主系统的开发和验证提供了结构化、可验证的方法，有助于推动农业自动化的标准化发展。

Abstract: The agricultural sector increasingly relies on autonomous systems that
operate in complex and variable environments. Unlike on-road applications,
agricultural automation integrates driving and working processes, each of which
imposes distinct operational constraints. Handling this complexity and ensuring
consistency throughout the development and validation processes requires a
structured, transparent, and verified description of the environment. However,
existing Operational Design Domain (ODD) concepts do not yet address the unique
challenges of agricultural applications.
  Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework,
which can be used to describe and verify the operational boundaries of
autonomous agricultural systems. The Ag-ODD Framework consists of three core
elements. First, the Ag-ODD description concept, which provides a structured
method for unambiguously defining environmental and operational parameters
using concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model
derived from the PEGASUS 6-Layer Model, has been extended to include a process
layer to capture dynamic agricultural operations. Third, the iterative
verification process verifies the Ag-ODD against its corresponding logical
scenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness
and consistency.
  Together, these elements provide a consistent approach for creating
unambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD
Framework can support the standardization and scalability of environmental
descriptions for autonomous agricultural systems.

</details>


### [2] [Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data](https://arxiv.org/abs/2511.02994)
*Syed Mostaquim Ali,Taufiq Rahman,Ghazal Farhani,Mohamed H. Zaki,Benoit Anctil,Dominique Charlebois*

Main category: cs.RO

TL;DR: 本文探索了用于比较真实世界和模拟LiDAR扫描的评估指标，发现密度感知Chamfer距离(DCD)在所有情况下表现最佳，并在虚拟测试环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于成本和安全性考虑，传统物理测试不切实际，需要虚拟测试环境(VTE)作为替代方案。比较VTE生成的传感器输出与真实世界对应物可以表明VTE是否准确反映现实。

Method: 采用综合实验方法寻找适合比较真实和模拟LiDAR扫描的评估指标，测试了不同噪声、密度、失真、传感器方向和通道设置下的敏感性和准确性。使用真实LiDAR扫描数据生成虚拟测试环境，并在相同姿态下生成模拟LiDAR扫描进行比较。

Result: 密度感知Chamfer距离(DCD)在所有情况下表现最佳。实际和模拟LiDAR扫描在语义分割输出上相似(mIoU为21%)，平均DCD为0.63，表明几何特性略有差异但模型输出存在显著差异。

Conclusion: 密度感知Chamfer距离是与感知方法最相关的指标，可用于评估虚拟测试环境中LiDAR扫描的准确性。

Abstract: For developing safe Autonomous Driving Systems (ADS), rigorous testing is
required before they are deemed safe for road deployments. Since comprehensive
conventional physical testing is impractical due to cost and safety concerns,
Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing
VTE-generated sensor outputs against their real-world analogues can be a strong
indication that the VTE accurately represents reality. Correspondingly, this
work explores a comprehensive experimental approach to finding evaluation
metrics suitable for comparing real-world and simulated LiDAR scans. The
metrics were tested in terms of sensitivity and accuracy with different noise,
density, distortion, sensor orientation, and channel settings. From comparing
the metrics, we found that Density Aware Chamfer Distance (DCD) works best
across all cases. In the second step of the research, a Virtual Testing
Environment was generated using real LiDAR scan data. The data was collected in
a controlled environment with only static objects using an instrumented vehicle
equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from
the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans
were compared in terms of model perception and geometric similarity. Actual and
simulated LiDAR scans have a similar semantic segmentation output with a mIoU
of 21\% with corrected intensity and an average density aware chamfer distance
(DCD) of 0.63. This indicates a slight difference in the geometric properties
of simulated and real LiDAR scans and a significant difference between model
outputs. During the comparison, density-aware chamfer distance was found to be
the most correlated among the metrics with perception methods.

</details>


### [3] [A Collaborative Reasoning Framework for Anomaly Diagnostics in Underwater Robotics](https://arxiv.org/abs/2511.03075)
*Markus Buchholz,Ignacio Carlucho,Yvan R. Petillot*

Main category: cs.RO

TL;DR: AURA是一个用于机器人异常和故障诊断的协作框架，结合了大型语言模型、高保真数字孪生和人在回路交互，通过双代理架构实时检测和响应异常行为。


<details>
  <summary>Details</summary>
Motivation: 在安全关键环境中安全部署自主系统需要将人类专业知识与AI驱动分析相结合，特别是在遇到未预见异常时。

Method: 使用两个代理：低级状态异常特征化代理监控遥测数据并转换为结构化问题描述，高级诊断推理代理与操作员进行知识基础对话识别根本原因。人类验证的诊断转化为新训练样本来改进感知模型。

Result: 建立了一个反馈循环，逐步将专家知识提炼到AI中，使其从静态工具转变为自适应合作伙伴。

Conclusion: 该框架为可信赖、持续改进的人机团队建立了模式。

Abstract: The safe deployment of autonomous systems in safety-critical settings
requires a paradigm that combines human expertise with AI-driven analysis,
especially when anomalies are unforeseen. We introduce AURA (Autonomous
Resilience Agent), a collaborative framework for anomaly and fault diagnostics
in robotics. AURA integrates large language models (LLMs), a high-fidelity
digital twin (DT), and human-in-the-loop interaction to detect and respond to
anomalous behavior in real time. The architecture uses two agents with clear
roles: (i) a low-level State Anomaly Characterization Agent that monitors
telemetry and converts signals into a structured natural-language problem
description, and (ii) a high-level Diagnostic Reasoning Agent that conducts a
knowledge-grounded dialogue with an operator to identify root causes, drawing
on external sources. Human-validated diagnoses are then converted into new
training examples that refine the low-level perceptual model. This feedback
loop progressively distills expert knowledge into the AI, transforming it from
a static tool into an adaptive partner. We describe the framework's operating
principles and provide a concrete implementation, establishing a pattern for
trustworthy, continually improving human-robot teams.

</details>


### [4] [WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models](https://arxiv.org/abs/2511.03077)
*R. Khorrambakht,Joaquim Ortiz-Haro,Joseph Amigo,Omar Mostafa,Daniel Dugas,Franziska Meier,Ludovic Righetti*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的方法，使用少量非结构化游戏数据学习视觉世界模型、扩散动作采样器和奖励模型，通过MCTS规划器优化动作序列，在真实机器人任务中显著优于行为克隆基线。


<details>
  <summary>Details</summary>
Motivation: 行为克隆方法难以迁移到新任务且数据收集困难，需要精心演示和频繁环境重置。本文采用基于模型的方法，使用易于收集的非结构化数据来解决这些问题。

Method: 收集少量非结构化游戏数据，学习动作条件视觉世界模型、扩散动作采样器和可选奖励模型，结合MCTS规划器和零阶模型预测控制器进行动作序列优化。

Result: 在3个真实机器人任务中验证了方法的有效性，动作采样器有效缓解了世界模型在规划过程中的幻觉问题。

Conclusion: 规划方法相比行为克隆基线在标准操作测试环境中带来显著改进，证明了基于模型方法的优势。

Abstract: Robots must understand their environment from raw sensory inputs and reason
about the consequences of their actions in it to solve complex tasks. Behavior
Cloning (BC) leverages task-specific human demonstrations to learn this
knowledge as end-to-end policies. However, these policies are difficult to
transfer to new tasks, and generating training data is challenging because it
requires careful demonstrations and frequent environment resets. In contrast to
such policy-based view, in this paper we take a model-based approach where we
collect a few hours of unstructured easy-to-collect play data to learn an
action-conditioned visual world model, a diffusion-based action sampler, and
optionally a reward model. The world model -- in combination with the action
sampler and a reward model -- is then used to optimize long sequences of
actions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are
executed on the robot via a zeroth-order Model Predictive Controller (MPC). We
show that the action sampler mitigates hallucinations of the world model during
planning and validate our approach on 3 real-world robotic tasks with varying
levels of planning and modeling complexity. Our experiments support the
hypothesis that planning leads to a significant improvement over BC baselines
on a standard manipulation test environment.

</details>


### [5] [3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors](https://arxiv.org/abs/2511.03078)
*Rohan Kota,Kaival Shah,J. Edward Colgate,Gregory Reardon*

Main category: cs.RO

TL;DR: 开发了一个开源库，将低成本3D打印机改造成自动探测设备，用于生成大量标记训练数据来校准触觉传感器。


<details>
  <summary>Details</summary>
Motivation: 触觉传感器校准过程通常需要大量人工操作且缺乏标准化方法，限制了触觉传感在机器人操作中的应用。

Method: 使用3D打印机作为自动探测设备收集数据，结合定制卷积神经网络来重建高质量深度图。

Result: 成功校准了DIGIT和GelSight Mini两种商用触觉传感器，并通过数据消融研究确定了准确校准所需的数据量。

Conclusion: 该自动化方法能加速触觉传感研究，简化传感器部署，促进触觉传感在机器人平台中的实际集成。

Abstract: Tactile sensing plays a key role in enabling dexterous and reliable robotic
manipulation, but realizing this capability requires substantial calibration to
convert raw sensor readings into physically meaningful quantities. Despite its
near-universal necessity, the calibration process remains ad hoc and
labor-intensive. Here, we introduce \libname{}, an open-source library that
transforms a low-cost 3D printer into an automated probing device capable of
generating large volumes of labeled training data for tactile sensor
calibration. We demonstrate the utility of \libname{} by calibrating two
commercially available vision-based tactile sensors, DIGIT and GelSight Mini,
to reconstruct high-quality depth maps using the collected data and a custom
convolutional neural network. In addition, we perform a data ablation study to
determine how much data is needed for accurate calibration, providing practical
guidelines for researchers working with these specific sensors, and we
benchmark the trained models on previously unseen objects to evaluate
calibration accuracy and generalization performance. By automating tactile
sensor calibration, \libname{} can accelerate tactile sensing research,
simplify sensor deployment, and promote the practical integration of tactile
sensing in robotic platforms.

</details>


### [6] [SENT Map -- Semantically Enhanced Topological Maps with Foundation Models](https://arxiv.org/abs/2511.03165)
*Raj Surya Rajendran Kathirvel,Zach A Chavis,Stephen J. Guy,Karthik Desingh*

Main category: cs.RO

TL;DR: SENT-Map是一种语义增强的拓扑地图，利用基础模型支持室内环境中的自主导航和操作，通过JSON文本格式表示环境语义信息。


<details>
  <summary>Details</summary>
Motivation: 利用基础模型的最新进展，为室内环境提供语义增强的地图表示，以支持自主导航和操作任务。

Method: 采用两阶段方法：首先使用视觉基础模型与操作员一起映射环境，然后使用SENT-Map表示和自然语言查询在基础模型中进行规划。

Result: 实验结果表明，语义增强使得即使是小型本地可部署的基础模型也能成功在室内环境中进行规划。

Conclusion: SENT-Map通过语义增强和JSON文本表示，为机器人提供了可理解和可编辑的环境表示，有效支持自主导航和操作。

Abstract: We introduce SENT-Map, a semantically enhanced topological map for
representing indoor environments, designed to support autonomous navigation and
manipulation by leveraging advancements in foundational models (FMs). Through
representing the environment in a JSON text format, we enable semantic
information to be added and edited in a format that both humans and FMs
understand, while grounding the robot to existing nodes during planning to
avoid infeasible states during deployment. Our proposed framework employs a two
stage approach, first mapping the environment alongside an operator with a
Vision-FM, then using the SENT-Map representation alongside a natural-language
query within an FM for planning. Our experimental results show that
semantic-enhancement enables even small locally-deployable FMs to successfully
plan over indoor environments.

</details>


### [7] [Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.03167)
*Xin Liu,Jinze Wu,Yinghui Li,Chenkun Qi,Yufei Xue,Feng Gao*

Main category: cs.RO

TL;DR: 提出了一种基于运动先验的方法，成功将深度强化学习应用于真实六足机器人，使其在复杂地形中生成自然步态并表现出卓越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多足机器人在复杂地形中具有更好的稳定性，但如何在大动作探索空间中有效协调多条腿以生成自然且鲁棒的运动是一个关键问题。

Method: 生成优化的运动先验数据集，基于这些先验训练对抗判别器来指导六足机器人学习自然步态，并将学习到的策略成功转移到真实六足机器人上。

Result: 学习到的策略在真实六足机器人上成功实现了自然步态模式，在复杂地形中无需视觉信息即表现出卓越的鲁棒性。

Conclusion: 这是首次使用强化学习控制器在真实六足机器人上实现复杂地形行走。

Abstract: Multi-legged robots offer enhanced stability to navigate complex terrains
with their multiple legs interacting with the environment. However, how to
effectively coordinate the multiple legs in a larger action exploration space
to generate natural and robust movements is a key issue. In this paper, we
introduce a motion prior-based approach, successfully applying deep
reinforcement learning algorithms to a real hexapod robot. We generate a
dataset of optimized motion priors, and train an adversarial discriminator
based on the priors to guide the hexapod robot to learn natural gaits. The
learned policy is then successfully transferred to a real hexapod robot, and
demonstrate natural gait patterns and remarkable robustness without visual
information in complex terrains. This is the first time that a reinforcement
learning controller has been used to achieve complex terrain walking on a real
hexapod robot.

</details>


### [8] [Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control](https://arxiv.org/abs/2511.03181)
*Rewida Ali,Cristian C. Beltran-Hernandez,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一种基于学习的框架，结合LLM高层任务规划和混合模仿学习/强化学习的底层策略，用于解决礼品包装等长时程可变形物体操作任务，在真实世界包装任务中达到97%的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中可变形物体操作的挑战，特别是礼品包装这类需要精确折叠、控制折痕和安全固定的长时程操作任务。

Method: 集成LLM高层任务规划器和混合IL/RL底层策略，核心是Sub-task Aware Robotic Transformer (START)，通过子任务ID提供显式时间定位，学习统一策略。

Result: 在真实世界包装任务中达到97%的成功率，统一transformer策略减少了对专用模型的需求，支持可控的人类监督。

Conclusion: 该框架有效连接高层意图与细粒度力控制，为可变形物体操作提供了稳健的解决方案。

Abstract: Human-robot cooperation is essential in environments such as warehouses and
retail stores, where workers frequently handle deformable objects like paper,
bags, and fabrics. Coordinating robotic actions with human assistance remains
difficult due to the unpredictable dynamics of deformable materials and the
need for adaptive force control. To explore this challenge, we focus on the
task of gift wrapping, which exemplifies a long-horizon manipulation problem
involving precise folding, controlled creasing, and secure fixation of paper.
Success is achieved when the robot completes the sequence to produce a neatly
wrapped package with clean folds and no tears.
  We propose a learning-based framework that integrates a high-level task
planner powered by a large language model (LLM) with a low-level hybrid
imitation learning (IL) and reinforcement learning (RL) policy. At its core is
a Sub-task Aware Robotic Transformer (START) that learns a unified policy from
human demonstrations. The key novelty lies in capturing long-range temporal
dependencies across the full wrapping sequence within a single model. Unlike
vanilla Action Chunking with Transformer (ACT), typically applied to short
tasks, our method introduces sub-task IDs that provide explicit temporal
grounding. This enables robust performance across the entire wrapping process
and supports flexible execution, as the policy learns sub-goals rather than
merely replicating motion sequences.
  Our framework achieves a 97% success rate on real-world wrapping tasks. We
show that the unified transformer-based policy reduces the need for specialized
models, allows controlled human supervision, and effectively bridges high-level
intent with the fine-grained force control required for deformable object
manipulation.

</details>


### [9] [Collaborative Assembly Policy Learning of a Sightless Robot](https://arxiv.org/abs/2511.03189)
*Zeqing Zhang,Weifeng Lu,Lei Yang,Wei Jing,Bowei Tang,Jia Pan*

Main category: cs.RO

TL;DR: 提出了一种结合导纳控制和强化学习的新方法，用于盲机器人-人类协作的板插入任务，相比传统导纳控制提高了成功率和效率，同时减少了人力负担。


<details>
  <summary>Details</summary>
Motivation: 传统导纳控制在物理人机协作中难以准确测量人力/力矩来估计人类意图，限制了机器人的协助能力；而纯强化学习方法由于安全约束和稀疏奖励也不适用于板插入任务。

Method: 提出了一种新颖的强化学习方法，利用人类设计的导纳控制器来促进更主动的机器人行为并减少人力负担。

Result: 通过仿真和真实实验证明，该方法在成功率和任务完成时间上优于导纳控制，且测得的力/力矩显著减少。

Conclusion: 所提出的方法有效解决了盲机器人-人类协作板插入任务中的挑战，提高了协作效率和安全性。

Abstract: This paper explores a physical human-robot collaboration (pHRC) task
involving the joint insertion of a board into a frame by a sightless robot and
a human operator. While admittance control is commonly used in pHRC tasks, it
can be challenging to measure the force/torque applied by the human for
accurate human intent estimation, limiting the robot's ability to assist in the
collaborative task. Other methods that attempt to solve pHRC tasks using
reinforcement learning (RL) are also unsuitable for the board-insertion task
due to its safety constraints and sparse rewards. Therefore, we propose a novel
RL approach that utilizes a human-designed admittance controller to facilitate
more active robot behavior and reduce human effort. Through simulation and
real-world experiments, we demonstrate that our approach outperforms admittance
control in terms of success rate and task completion time. Additionally, we
observed a significant reduction in measured force/torque when using our
proposed approach compared to admittance control. The video of the experiments
is available at https://youtu.be/va07Gw6YIog.

</details>


### [10] [GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement](https://arxiv.org/abs/2511.03400)
*Minquan Gao,Xinyi Li,Qing Yan,Xiaojian Sun,Xiaopan Zhang,Chien-Ming Huang,Jiachen Li*

Main category: cs.RO

TL;DR: GUIDES是一个轻量级框架，通过将基础模型的语义指导注入预训练机器人策略的潜在空间，无需架构重新设计即可增强现有策略的语义感知能力。


<details>
  <summary>Details</summary>
Motivation: 预训练机器人策略缺乏基础模型的语义意识，但完全替换成本高昂且会丢失积累的知识。需要一种方法在保留现有策略的同时增强其语义能力。

Method: 使用微调的视觉语言模型生成上下文指令，通过辅助模块编码为指导嵌入，注入策略的潜在空间。通过大型语言模型反射器监控置信度，在低置信度时启动推理循环来优化后续动作。

Result: 在RoboCasa模拟环境中验证，各种策略架构的任务成功率均获得一致且显著提升。UR5机器人真实部署显示GUIDES提高了关键子任务（如抓取）的运动精度。

Conclusion: GUIDES提供了一种实用且资源高效的途径来升级而非替换经过验证的机器人策略。

Abstract: Pre-trained robot policies serve as the foundation of many validated robotic
systems, which encapsulate extensive embodied knowledge. However, they often
lack the semantic awareness characteristic of foundation models, and replacing
them entirely is impractical in many situations due to high costs and the loss
of accumulated knowledge. To address this gap, we introduce GUIDES, a
lightweight framework that augments pre-trained policies with semantic guidance
from foundation models without requiring architectural redesign. GUIDES employs
a fine-tuned vision-language model (Instructor) to generate contextual
instructions, which are encoded by an auxiliary module into guidance
embeddings. These embeddings are injected into the policy's latent space,
allowing the legacy model to adapt to this new semantic input through brief,
targeted fine-tuning. For inference-time robustness, a large language
model-based Reflector monitors the Instructor's confidence and, when confidence
is low, initiates a reasoning loop that analyzes execution history, retrieves
relevant examples, and augments the VLM's context to refine subsequent actions.
Extensive validation in the RoboCasa simulation environment across diverse
policy architectures shows consistent and substantial improvements in task
success rates. Real-world deployment on a UR5 robot further demonstrates that
GUIDES enhances motion precision for critical sub-tasks such as grasping.
Overall, GUIDES offers a practical and resource-efficient pathway to upgrade,
rather than replace, validated robot policies.

</details>


### [11] [Value Elicitation for a Socially Assistive Robot Addressing Social Anxiety: A Participatory Design Approach](https://arxiv.org/abs/2511.03444)
*Vesna Poprcova,Iulia Lefter,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: 通过参与式设计工作坊，研究心理健康学术研究人员对社交焦虑支持机器人的设计价值观，包括适应性、接受度和有效性等核心价值。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑是普遍的心理健康问题，但支持不足。社交机器人技术为补充传统心理健康治疗提供了新机会，需要理解什么是有意义、可接受和有效的设计价值。

Method: 采用参与式设计工作坊，通过创造性、反思性和展望性活动，让心理健康学术研究人员探索场景和设计可能性，系统获取价值观、期望、需求和偏好。

Result: 研究发现适应性、接受度和有效性等核心设计价值对社交焦虑支持至关重要，提供了丰富的设计相关见解。

Conclusion: 强调研究导向的价值获取方法的重要性，在开发社交辅助机器人时需注重以用户为中心和情境感知的设计考量。

Abstract: Social anxiety is a prevalent mental health condition that can significantly
impact overall well-being and quality of life. Despite its widespread effects,
adequate support or treatment for social anxiety is often insufficient.
Advances in technology, particularly in social robotics, offer promising
opportunities to complement traditional mental health. As an initial step
toward developing effective solutions, it is essential to understand the values
that shape what is considered meaningful, acceptable, and helpful. In this
study, a participatory design workshop was conducted with mental health
academic researchers to elicit the underlying values that should inform the
design of socially assistive robots for social anxiety support. Through
creative, reflective, and envisioning activities, participants explored
scenarios and design possibilities, allowing for systematic elicitation of
values, expectations, needs, and preferences related to robot-supported
interventions. The findings reveal rich insights into design-relevant
values-including adaptivity, acceptance, and efficacy-that are core to support
for individuals with social anxiety. This study highlights the significance of
a research-led approach to value elicitation, emphasising user-centred and
context-aware design considerations in the development of socially assistive
robots.

</details>


### [12] [Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control](https://arxiv.org/abs/2511.03481)
*Jianbo Yuan,Haohua Zhu,Jing Dai,Sheng Yi*

Main category: cs.RO

TL;DR: Dex-Hand 021是一款高性能、线驱动的五指机器人手，具有12个主动和7个被动自由度，重量仅1kg。采用基于本体感觉力感知的导纳控制方法，在抓取能力和精度方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 人类手在日常生活中和工业应用中起着重要作用，但复制其多功能能力（包括运动、感知和协调操作）对机器人系统仍然是一个巨大挑战。需要平衡类人敏捷性与工程约束（如复杂性、尺寸重量比、耐久性和力感知性能）。

Method: 开发了线驱动的五指机器人手，具有12个主动和7个被动自由度，共19个自由度。提出了基于本体感觉力感知的导纳控制方法来增强操作能力。

Result: 实验结果显示：单指负载能力超过10N，指尖重复性低于0.001m，力估计误差低于0.2N。与PID控制相比，多物体抓取中的关节扭矩减少了31.19%，显著提高了力感知能力并防止碰撞过载。成功执行了33种GRASP分类动作和复杂操作任务。

Conclusion: 这项工作推进了轻量级工业级灵巧手的设计，增强了本体感觉控制，为机器人操作和智能制造做出了贡献。

Abstract: The human hand plays a vital role in daily life and industrial applications,
yet replicating its multifunctional capabilities-including motion, sensing, and
coordinated manipulation-with robotic systems remains a formidable challenge.
Developing a dexterous robotic hand requires balancing human-like agility with
engineering constraints such as complexity, size-to-weight ratio, durability,
and force-sensing performance. This letter presents Dex-Hand 021, a
high-performance, cable-driven five-finger robotic hand with 12 active and 7
passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight
1 kg design. We propose a proprioceptive force-sensing-based admittance control
method to enhance manipulation. Experimental results demonstrate its superior
performance: a single-finger load capacity exceeding 10 N, fingertip
repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared
to PID control, joint torques in multi-object grasping are reduced by 31.19%,
significantly improves force-sensing capability while preventing overload
during collisions. The hand excels in both power and precision grasps,
successfully executing 33 GRASP taxonomy motions and complex manipulation
tasks. This work advances the design of lightweight, industrial-grade dexterous
hands and enhances proprioceptive control, contributing to robotic manipulation
and intelligent manufacturing.

</details>


### [13] [ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications](https://arxiv.org/abs/2511.03497)
*Lei Fu,Sahar Salimpour,Leonardo Militano,Harry Edelman,Jorge Peña Queralta,Giovanni Toffetti*

Main category: cs.RO

TL;DR: 本文提出了一个用于分析ROS和ROS 2数据包的MCP服务器，通过LLM和VLM实现机器人数据的自然语言分析、可视化和处理，并评估了不同LLM/VLM模型的工具调用能力。


<details>
  <summary>Details</summary>
Motivation: 当前在智能体AI系统和物理/具身AI系统的交叉领域研究稀缺，需要开发能够通过自然语言分析机器人数据的工具。

Method: 构建了具有机器人领域知识的MCP服务器工具，支持移动机器人轨迹、激光扫描数据、变换和时间序列数据的分析，并提供了轻量级UI用于基准测试。

Result: 实验评估了8种最先进的LLM/VLM模型，发现Kimi K2和Claude Sonnet 4在工具调用能力上表现最佳，工具描述模式、参数数量和可用工具数量都会影响成功率。

Conclusion: 该工作填补了智能体具身AI领域的空白，提供了开源工具并展示了不同LLM在机器人数据分析任务中的性能差异。

Abstract: Agentic AI systems and Physical or Embodied AI systems have been two key
research verticals at the forefront of Artificial Intelligence and Robotics,
with Model Context Protocol (MCP) increasingly becoming a key component and
enabler of agentic applications. However, the literature at the intersection of
these verticals, i.e., Agentic Embodied AI, remains scarce. This paper
introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for
analyzing, visualizing and processing robot data with natural language through
LLMs and VLMs. We describe specific tooling built with robotics domain
knowledge, with our initial release focused on mobile robotics and supporting
natively the analysis of trajectories, laser scan data, transforms, or time
series data. This is in addition to providing an interface to standard ROS 2
CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to
filter bags with a subset of topics or trimmed in time. Coupled with the MCP
server, we provide a lightweight UI that allows the benchmarking of the tooling
with different LLMs, both proprietary (Anthropic, OpenAI) and open-source
(through Groq). Our experimental results include the analysis of tool calling
capabilities of eight different state-of-the-art LLM/VLM models, both
proprietary and open-source, large and small. Our experiments indicate that
there is a large divide in tool calling capabilities, with Kimi K2 and Claude
Sonnet 4 demonstrating clearly superior performance. We also conclude that
there are multiple factors affecting the success rates, from the tool
description schema to the number of arguments, as well as the number of tools
available to the models. The code is available with a permissive license at
https://github.com/binabik-ai/mcp-rosbags.

</details>


### [14] [Indicating Robot Vision Capabilities with Augmented Reality](https://arxiv.org/abs/2511.03550)
*Hong Wang,Ridhima Phatak,James Ocampo,Zhao Han*

Main category: cs.RO

TL;DR: 该研究提出四种AR视野指示器来改善人类对机器人视野能力的认知模型，通过实验验证这些指示器在准确性、信心、任务效率和认知负荷方面的表现。


<details>
  <summary>Details</summary>
Motivation: 人类经常错误地认为机器人和人类有相同的视野范围，这种错误的心理模型会导致人机协作失败，特别是当机器人被要求完成关于视野外物体的不可能任务时。

Method: 提出四种AR视野指示器，从自我中心（机器人眼睛和头部空间）到异我中心（任务空间）的频谱，并通过用户实验（N=41）评估这些指示器的准确性、信心、任务效率和认知负荷。

Result: 结果显示，任务空间的异我中心方块指示器具有最高的准确性，但解读机器人视野时有延迟。自我中心的深眼窝指示器也能提高准确性。所有指示器中参与者信心都很高，认知负荷保持较低水平。

Conclusion: 贡献了六条实践指南，帮助从业者应用AR指示器或物理改造来使人类的心理模型与机器人的视觉能力保持一致。

Abstract: Research indicates that humans can mistakenly assume that robots and humans
have the same field of view (FoV), possessing an inaccurate mental model of
robots. This misperception may lead to failures during human-robot
collaboration tasks where robots might be asked to complete impossible tasks
about out-of-view objects. The issue is more severe when robots do not have a
chance to scan the scene to update their world model while focusing on assigned
tasks. To help align humans' mental models of robots' vision capabilities, we
propose four FoV indicators in augmented reality (AR) and conducted a user
human-subjects experiment (N=41) to evaluate them in terms of accuracy,
confidence, task efficiency, and workload. These indicators span a spectrum
from egocentric (robot's eye and head space) to allocentric (task space).
Results showed that the allocentric blocks at the task space had the highest
accuracy with a delay in interpreting the robot's FoV. The egocentric indicator
of deeper eye sockets, possible for physical alteration, also increased
accuracy. In all indicators, participants' confidence was high while cognitive
load remained low. Finally, we contribute six guidelines for practitioners to
apply our AR indicators or physical alterations to align humans' mental models
with robots' vision capabilities.

</details>


### [15] [OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera](https://arxiv.org/abs/2511.03571)
*Hao Shi,Ze Wang,Shangwei Guo,Mengfei Duan,Song Wang,Teng Chen,Kailun Yang,Lin Wang,Kaiwei Wang*

Main category: cs.RO

TL;DR: OneOcc是一个专为腿式/人形机器人设计的全景语义场景补全框架，通过双投影融合、双网格体素化、轻量级解码器和步态位移补偿等技术，在360度连续性和抗抖动方面表现出色，在两个新发布的全景占用基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语义场景补全系统主要针对轮式平台和前向传感器设计，无法满足腿式/人形机器人在步态引起的身体抖动和360度连续感知方面的需求。

Method: 结合双投影融合(DP-ER)利用环形全景及其等距柱面展开，保持360度连续性和网格对齐；双网格体素化(BGV)在笛卡尔和柱极坐标系中推理；轻量级解码器采用分层AMoE-3D进行动态多尺度融合；可插拔的步态位移补偿(GDC)学习特征级运动校正。

Result: 在QuadOcc基准上超越强视觉基线和流行LiDAR方法；在H3O基准上获得+3.83 mIoU(城内)和+8.08(跨城)提升；模块轻量级，适合腿式/人形机器人部署。

Conclusion: OneOcc为腿式/人形机器人提供了可部署的全环绕感知解决方案，在两个新发布的全景占用基准上建立了新的SOTA，填补了该领域的空白。

Abstract: Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most
semantic scene completion (SSC) systems target wheeled platforms with
forward-facing sensors. We present OneOcc, a vision-only panoramic SSC
framework designed for gait-introduced body jitter and 360{\deg} continuity.
OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular
panorama and its equirectangular unfolding, preserving 360{\deg} continuity and
grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and
cylindrical-polar spaces, reducing discretization bias and sharpening
free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D
for dynamic multi-scale fusion and better long-range/occlusion reasoning; and
(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level
motion correction without extra sensors. We also release two panoramic
occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and
Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic
occupancy; standardized within-/cross-city splits). OneOcc sets new
state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and
popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08
(cross-city). Modules are lightweight, enabling deployable full-surround
perception for legged/humanoid robots. Datasets and code will be publicly
available at https://github.com/MasterHow/OneOcc.

</details>


### [16] [Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution](https://arxiv.org/abs/2511.03576)
*Aniol Civit,Antonio Andriella,Carles Sierra,Guillem Alenyà*

Main category: cs.RO

TL;DR: 提出了MUP-QBAF框架，基于定量双极论证框架解决多用户偏好冲突，结合用户参数和机器人环境观察，实现动态适应和透明决策。


<details>
  <summary>Details</summary>
Motivation: 现有HRI个性化方法主要关注单用户适应，忽视了多用户场景中可能存在的偏好冲突问题，需要开发能够明确建模和解决多用户偏好冲突的框架。

Method: 基于定量双极论证框架(QBAFs)，将正负偏好表示为参数，通过迭代重新计算参数强度，结合用户输入和机器人环境观察，实现动态适应。

Result: 通过辅助机器人在衰弱评估任务中调解护理人员和被护理者冲突偏好的案例研究验证了框架有效性，包括参数基础分数的敏感性分析。

Conclusion: 该工作为多用户HRI提供了透明、结构化且上下文敏感的竞争用户偏好解决方法，为数据驱动方法提供了原则性替代方案，使机器人能够在真实环境中导航冲突。

Abstract: While personalisation in Human-Robot Interaction (HRI) has advanced
significantly, most existing approaches focus on single-user adaptation,
overlooking scenarios involving multiple stakeholders with potentially
conflicting preferences. To address this, we propose the Multi-User Preferences
Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user
personalisation framework based on Quantitative Bipolar Argumentation
Frameworks (QBAFs) that explicitly models and resolves multi-user preference
conflicts. Unlike prior work in Argumentation Frameworks, which typically
assumes static inputs, our approach is tailored to robotics: it incorporates
both users' arguments and the robot's dynamic observations of the environment,
allowing the system to adapt over time and respond to changing contexts.
Preferences, both positive and negative, are represented as arguments whose
strength is recalculated iteratively based on new information. The framework's
properties and capabilities are presented and validated through a realistic
case study, where an assistive robot mediates between the conflicting
preferences of a caregiver and a care recipient during a frailty assessment
task. This evaluation further includes a sensitivity analysis of argument base
scores, demonstrating how preference outcomes can be shaped by user input and
contextual observations. By offering a transparent, structured, and
context-sensitive approach to resolving competing user preferences, this work
advances the field of multi-user HRI. It provides a principled alternative to
data-driven methods, enabling robots to navigate conflicts in real-world
environments.

</details>


### [17] [Manifold-constrained Hamilton-Jacobi Reachability Learning for Decentralized Multi-Agent Motion Planning](https://arxiv.org/abs/2511.03591)
*Qingyi Chen,Ruiqi Ni,Jun Kim,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 提出了一种流形约束的Hamilton-Jacobi可达性学习框架，用于解决多智能体运动规划中的流形约束问题，使机器人能够生成既安全又满足任务可行性的运动规划。


<details>
  <summary>Details</summary>
Motivation: 现实场景中机器人需要在动态环境中导航，同时遵守任务施加的流形约束（如服务机器人端杯子需保持直立），但现有去中心化多智能体运动规划方法难以有效整合流形约束。

Method: 通过求解流形约束下的Hamilton-Jacobi可达性问题来捕获任务感知的安全条件，然后将其集成到去中心化轨迹优化规划器中。

Result: 该方法在多样化流形约束任务中表现出良好的泛化能力，能够有效扩展到高维多智能体操作问题，性能优于现有约束运动规划器，且运行速度适合实际应用。

Conclusion: 所提出的框架为多智能体运动规划中的流形约束问题提供了有效解决方案，无需对其他智能体策略做出假设，实现了安全且任务可行的运动规划。

Abstract: Safe multi-agent motion planning (MAMP) under task-induced constraints is a
critical challenge in robotics. Many real-world scenarios require robots to
navigate dynamic environments while adhering to manifold constraints imposed by
tasks. For example, service robots must carry cups upright while avoiding
collisions with humans or other robots. Despite recent advances in
decentralized MAMP for high-dimensional systems, incorporating manifold
constraints remains difficult. To address this, we propose a
manifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for
decentralized MAMP. Our method solves HJR problems under manifold constraints
to capture task-aware safety conditions, which are then integrated into a
decentralized trajectory optimization planner. This enables robots to generate
motion plans that are both safe and task-feasible without requiring assumptions
about other agents' policies. Our approach generalizes across diverse
manifold-constrained tasks and scales effectively to high-dimensional
multi-agent manipulation problems. Experiments show that our method outperforms
existing constrained motion planners and operates at speeds suitable for
real-world applications. Video demonstrations are available at
https://youtu.be/RYcEHMnPTH8 .

</details>


### [18] [Multi-robot searching with limited sensing range for static and mobile intruders](https://arxiv.org/abs/2511.03622)
*Swadhin Agrawal,Sujoy Bhore,Joseph S. B. Mitchell,P. B. Sujit,Aayush Gohil*

Main category: cs.RO

TL;DR: 在多机器人搜索几何领域中的入侵者问题研究中，针对静态和移动入侵者，发现该问题即使对于静止入侵者也是NP难的。为此开发了基于空间填充曲线、随机搜索和协作随机搜索的高效鲁棒算法，并评估了机器人数量与搜索时间之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 研究在多机器人系统中搜索几何领域内入侵者的问题，特别是针对具有有限感知能力的机器人在简单连通正交多边形环境中的搜索任务。

Method: 提出了三种主要算法：基于空间填充曲线的搜索方法、随机搜索算法以及协作随机搜索策略。

Result: 发现搜索入侵者问题是NP难的，即使对于静止入侵者也是如此。开发的算法能够有效处理这一复杂问题。

Conclusion: 通过分析机器人数量与搜索时间之间的权衡关系，为多机器人搜索系统在几何环境中的部署提供了实用的算法解决方案。

Abstract: We consider the problem of searching for an intruder in a geometric domain by
utilizing multiple search robots. The domain is a simply connected orthogonal
polygon with edges parallel to the cartesian coordinate axes. Each robot has a
limited sensing capability. We study the problem for both static and mobile
intruders. It turns out that the problem of finding an intruder is NP-hard,
even for a stationary intruder. Given this intractability, we turn our
attention towards developing efficient and robust algorithms, namely methods
based on space-filling curves, random search, and cooperative random search.
Moreover, for each proposed algorithm, we evaluate the trade-off between the
number of search robots and the time required for the robots to complete the
search process while considering the geometric properties of the connected
orthogonal search area.

</details>


### [19] [Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural](https://arxiv.org/abs/2511.03651)
*Andrei A. Korigodskii,Oleg D. Kalachev,Artem E. Vasiunik,Matvei V. Urvantsev,Georgii E. Bondar*

Main category: cs.RO

TL;DR: 开发了用于绘制世界最大无人机壁画的自主无人机系统，结合红外运动捕捉和LiDAR技术实现精确定位，采用独特控制架构实现精确轨迹跟踪和稳定线条绘制。


<details>
  <summary>Details</summary>
Motivation: 解决在恶劣户外条件（如风、阳光）下保持艺术精度和操作可靠性的双重挑战，扩展机器人在创意领域的应用。

Method: 使用红外运动捕捉相机和LiDAR的组合导航系统，采用切向和法向不同调节的控制架构，开发定制喷漆机制和轨迹规划算法。

Result: 实验结果表明系统在各种条件下具有鲁棒性和精确性，成功完成大规模壁画创作。

Conclusion: 该系统展示了自主大规模艺术创作的潜力，扩展了机器人在创意领域的应用范围。

Abstract: This paper presents the innovative design and successful deployment of a
pioneering autonomous unmanned aerial system developed for executing the
world's largest mural painted by a drone. Addressing the dual challenges of
maintaining artistic precision and operational reliability under adverse
outdoor conditions such as wind and direct sunlight, our work introduces a
robust system capable of navigating and painting outdoors with unprecedented
accuracy. Key to our approach is a novel navigation system that combines an
infrared (IR) motion capture camera and LiDAR technology, enabling precise
location tracking tailored specifically for largescale artistic applications.
We employ a unique control architecture that uses different regulation in
tangential and normal directions relative to the planned path, enabling precise
trajectory tracking and stable line rendering. We also present algorithms for
trajectory planning and path optimization, allowing for complex curve drawing
and area filling. The system includes a custom-designed paint spraying
mechanism, specifically engineered to function effectively amidst the turbulent
airflow generated by the drone's propellers, which also protects the drone's
critical components from paint-related damage, ensuring longevity and
consistent performance. Experimental results demonstrate the system's
robustness and precision in varied conditions, showcasing its potential for
autonomous large-scale art creation and expanding the functional applications
of robotics in creative fields.

</details>


### [20] [Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments](https://arxiv.org/abs/2511.03652)
*Azizollah Taheri,Derya Aksaray*

Main category: cs.RO

TL;DR: 提出了一种在不确定环境中处理时空逻辑任务的运动规划方法，使用特殊乘积自动机来捕捉语义标签的不确定性，并通过值迭代进行在线重规划。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定环境中实现时空逻辑任务的问题，其中环境语义标签具有概率性知识，而非精确位置信息。

Method: 构建特殊乘积自动机来捕获语义标签的不确定性，为每个边设计奖励函数，并利用值迭代进行在线重规划。

Result: 展示了理论结果和仿真实验，证明了所提方法的有效性。

Conclusion: 提出的自动机理论方法能够有效处理不确定环境中的时空逻辑任务规划问题。

Abstract: This paper addresses a motion planning problem to achieve
spatio-temporal-logical tasks, expressed by syntactically co-safe linear
temporal logic specifications (scLTL\next), in uncertain environments. Here,
the uncertainty is modeled as some probabilistic knowledge on the semantic
labels of the environment. For example, the task is "first go to region 1, then
go to region 2"; however, the exact locations of regions 1 and 2 are not known
a priori, instead a probabilistic belief is available. We propose a novel
automata-theoretic approach, where a special product automaton is constructed
to capture the uncertainty related to semantic labels, and a reward function is
designed for each edge of this product automaton. The proposed algorithm
utilizes value iteration for online replanning. We show some theoretical
results and present some simulations/experiments to demonstrate the efficacy of
the proposed approach.

</details>


### [21] [Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm Motion Design](https://arxiv.org/abs/2511.03676)
*Taito Tashiro,Tomoko Yonezawa,Hirotake Yamazoe*

Main category: cs.RO

TL;DR: 研究探索如何利用人类运动线索设计富有表现力的机器人手臂动作，通过Geister游戏分析人类自然游戏和有意表达的动作模式，并评估物理机器人与视频展示对观察者印象的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是利用人类运动线索来设计更富有表现力的机器人动作，特别是关注在非完全信息游戏情境下人类无意识倾向和有意表达的动作差异。

Method: 通过分析Geister游戏中人类棋子移动动作（自然游戏动作和指导性表达动作），创建基于运动速度和停止时长的阶段特定机器人动作，并在物理机器人和录制视频两种展示模式下评估观察者印象。

Result: 结果显示后期运动时机（特别是撤回阶段）在印象形成中起重要作用，物理实体化增强了运动线索的可解释性。

Conclusion: 研究结论是基于人类时机行为设计富有表现力的机器人动作提供了重要见解，强调了运动时机和物理实体化在机器人表达性设计中的重要性。

Abstract: This study investigates how human motion cues can be used to design
expressive robot-arm movements. Using the imperfect-information game Geister,
we analyzed two types of human piece-moving motions: natural gameplay
(unconscious tendencies) and instructed expressions (intentional cues). Based
on these findings, we created phase-specific robot motions by varying movement
speed and stop duration, and evaluated observer impressions under two
presentation modalities: a physical robot and a recorded video. Results
indicate that late-phase motion timing, particularly during withdrawal, plays
an important role in impression formation and that physical embodiment enhances
the interpretability of motion cues. These findings provide insights for
designing expressive robot motions based on human timing behavior.

</details>


### [22] [Source-Free Bistable Fluidic Gripper for Size-Selective and Stiffness-Adaptive Grasping](https://arxiv.org/abs/2511.03691)
*Zhihang Qin,Yueheng Zhang,Wan Su,Linxin Hou,Shenghao Zhou,Zhijun Chen,Yu Jun Tan,Cecilia Laschi*

Main category: cs.RO

TL;DR: 提出了一种自包含的软抓手，通过内部液体在三联双稳态腔室间重新分配实现抓取，无需外部能源，具有尺寸选择性和刚度自适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统流体驱动软抓手依赖外部能源，限制了便携性和长期自主性，需要开发自包含、无需持续能量输入的软抓手系统。

Method: 利用三个相互连接的双稳态突跳腔室，当顶部传感腔室接触物体变形时，液体位移触发抓取腔室的突跳膨胀，实现稳定抓取。

Result: 实现了无需外部能源的尺寸选择性抓取，能够被动适应物体刚度的抓取压力，在软体机器人中提供了轻量级、刚度自适应的流体驱动操作方案。

Conclusion: 这种无源紧凑设计为软体机器人开辟了新的可能性，特别适用于水下和野外环境中的目标尺寸特定采样和操作。

Abstract: Conventional fluid-driven soft grippers typically depend on external sources,
which limit portability and long-term autonomy. This work introduces a
self-contained soft gripper with fixed size that operates solely through
internal liquid redistribution among three interconnected bistable snap-through
chambers. When the top sensing chamber deforms upon contact, the displaced
liquid triggers snap-through expansion of the grasping chambers, enabling
stable and size-selective grasping without continuous energy input. The
internal hydraulic feedback further allows passive adaptation of gripping
pressure to object stiffness. This source-free and compact design opens new
possibilities for lightweight, stiffness-adaptive fluid-driven manipulation in
soft robotics, providing a feasible approach for targeted size-specific
sampling and operation in underwater and field environments.

</details>
