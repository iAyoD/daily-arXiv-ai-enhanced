<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Translating Milli/Microrobots with A Value-Centered Readiness Framework](https://arxiv.org/abs/2510.12090)
*Hakan Ceylan,Edoardo Sinibaldi,Sanjay Misra,Pankaj J. Pasricha,Dietmar W. Hutmacher*

Main category: cs.RO

TL;DR: 本文回顾了无绳移动毫米/微米机器人在介入医学中的转化挑战，提出了技术就绪度框架（mTRL）来加速从实验室到临床的转化过程。


<details>
  <summary>Details</summary>
Motivation: 尽管毫米/微米机器人在实验室中取得了显著进展，但大多数仍停留在概念验证阶段，缺乏实际临床应用可行性。需要弥合技术创新与现实应用之间的差距。

Method: 引入战略性的毫米/微米机器人技术就绪度框架（mTRL），通过明确定义的里程碑和逐步活动，将系统开发从概念化映射到临床采用。

Result: mTRL模型提供了技术成熟度的结构化衡量标准，为跨学科合作提供共同语言，并为加速转化发展提供可操作的指导。

Conclusion: 该领域的长期影响和可持续性取决于将开发与未满足的医疗需求对齐，确保应用可行性，并无缝集成到现有临床工作流程中。

Abstract: Untethered mobile milli/microrobots hold transformative potential for
interventional medicine by enabling more precise and entirely non-invasive
diagnosis and therapy. Realizing this promise requires bridging the gap between
groundbreaking laboratory demonstrations and successful clinical integration.
Despite remarkable technical progress over the past two decades, most
millirobots and microrobots remain confined to laboratory proof-of-concept
demonstrations, with limited real-world feasibility. In this Review, we
identify key factors that slow translation from bench to bedside, focusing on
the disconnect between technical innovation and real-world application. We
argue that the long-term impact and sustainability of the field depend on
aligning development with unmet medical needs, ensuring applied feasibility,
and integrating seamlessly into existing clinical workflows, which are
essential pillars for delivering meaningful patient outcomes. To support this
shift, we introduce a strategic milli/microrobot Technology Readiness Level
framework (mTRL), which maps system development from initial conceptualization
to clinical adoption through clearly defined milestones and their associated
stepwise activities. The mTRL model provides a structured gauge of
technological maturity, a common language for cross-disciplinary collaboration
and actionable guidance to accelerate translational development toward new,
safer and more efficient interventions.

</details>


### [2] [Gaussian Semantic Field for One-shot LiDAR Global Localization](https://arxiv.org/abs/2510.12101)
*Pengyu Yin,Shenghai Yuan,Haozhi Cao,Xingyu Ji,Ruofei Bai,Siyu Chen,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种基于轻量级三层场景图的单次LiDAR全局定位算法，通过高斯过程学习语义分布的连续函数来改善地标语义歧义问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于地标语义注册的方法在全局定位中表现优于纯几何方法，但地标可能重复且具有误导性，需要解决语义歧义问题。

Method: 使用高斯过程学习语义分布的连续函数，构建包含对象层、连续函数层和度量语义层的三层3D场景图，作为轻量级单次定位后端。

Result: 在公开数据集上的广泛实验验证了该方法相比当前最先进方法的优越性能。

Conclusion: 提出的Outram-GSF全局定位管道通过连续语义函数有效缓解语义歧义，实现了高性能的单次LiDAR全局定位。

Abstract: We present a one-shot LiDAR global localization algorithm featuring semantic
disambiguation ability based on a lightweight tri-layered scene graph. While
landmark semantic registration-based methods have shown promising performance
improvements in global localization compared with geometric-only methods,
landmarks can be repetitive and misleading for correspondence establishment. We
propose to mitigate this problem by modeling semantic distributions with
continuous functions learned from a population of Gaussian processes. Compared
with discrete semantic labels, the continuous functions capture finer-grained
geo-semantic information and also provide more detailed metric information for
correspondence establishment. We insert this continuous function as the middle
layer between the object layer and the metric-semantic layer, forming a
tri-layered 3D scene graph, serving as a light-weight yet performant backend
for one-shot localization. We term our global localization pipeline Outram-GSF
(Gaussian semantic field) and conduct a wide range of experiments on publicly
available data sets, validating the superior performance against the current
state-of-the-art.

</details>


### [3] [Hybrid Terrain-Aware Path Planning: Integrating VD--RRT\(^{*}\) Exploration and VD--D\(^{*}\) Lite Repair](https://arxiv.org/abs/2510.12169)
*Akshay Naik,William R. Norris,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.RO

TL;DR: 提出了一种结合土壤强度和坡度分析的连续状态-成本度量方法，用于越野自动驾驶车辆的实时路径规划，支持在可变形地形中的确定性、采样或学习驱动的规划。


<details>
  <summary>Details</summary>
Motivation: 越野自动驾驶车辆需要在实时规划中同时考虑曲率可行性和地形变化（如土壤强度和坡度），以确保在非结构化环境中的可靠导航。

Method: 使用Bekker压力-沉降模型与高程导出的坡度惩罚相结合，构建连续状态-成本度量；在格子上评估该度量，采用精确转向基元（Dubins、Reeds-Shepp运动）；全局探索使用Vehicle-Dynamics RRT*，局部修复使用Vehicle-Dynamics D* Lite。

Result: 硬件试验表明，该框架能够在软土和坡度变化的地形中实现实时导航，支持非结构化环境中的可靠自主性。

Conclusion: 通过将地形-车辆模型与规划器分离，该框架为可变形地形中的确定性、采样或学习驱动规划提供了可重用的基础，实现了毫秒级的重新规划而无需启发式平滑。

Abstract: Autonomous ground vehicles operating off-road must plan curvature-feasible
paths while accounting for spatially varying soil strength and slope hazards in
real time. We present a continuous state--cost metric that combines a Bekker
pressure--sinkage model with elevation-derived slope and attitude penalties.
The resulting terrain cost field is analytic, bounded, and monotonic in soil
modulus and slope, ensuring well-posed discretization and stable updates under
sensor noise. This metric is evaluated on a lattice with exact steering
primitives: Dubins and Reeds--Shepp motions for differential drive and
time-parameterized bicycle arcs for Ackermann steering. Global exploration is
performed using Vehicle-Dynamics RRT\(^{*}\), while local repair is managed by
Vehicle-Dynamics D\(^{*}\) Lite, enabling millisecond-scale replanning without
heuristic smoothing. By separating the terrain--vehicle model from the planner,
the framework provides a reusable basis for deterministic, sampling-based, or
learning-driven planning in deformable terrain. Hardware trials on an off-road
platform demonstrate real-time navigation across soft soil and slope
transitions, supporting reliable autonomy in unstructured environments.

</details>


### [4] [Controllable Collision Scenario Generation via Collision Pattern Prediction](https://arxiv.org/abs/2510.12206)
*Pin-Lun Chen,Chi-Hsi Kung,Che-Han Chang,Wei-Chen Chiu,Yi-Ting Chen*

Main category: cs.RO

TL;DR: 提出可控碰撞场景生成任务，开发COLLIDE数据集和框架，通过预测碰撞模式来生成指定碰撞类型和TTA的轨迹，提升自动驾驶车辆安全性测试。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆安全性评估需要多样化的安全关键场景，特别是碰撞场景在现实中难以收集且危险，因此需要在仿真中生成可控的碰撞场景。

Method: 提出可控碰撞场景生成任务，构建COLLIDE数据集，开发预测碰撞模式的框架，该模式捕捉碰撞时车辆空间配置，然后生成完整对抗轨迹。

Result: 方法在碰撞率和可控性方面优于基线，生成的场景能诱导更高的规划器失败率，并能通过微调提升规划器鲁棒性。

Conclusion: 该方法能有效生成可控碰撞场景，揭示现有规划器局限性，并通过微调提升自动驾驶车辆在不同碰撞场景中的安全性。

Abstract: Evaluating the safety of autonomous vehicles (AVs) requires diverse,
safety-critical scenarios, with collisions being especially important yet rare
and unsafe to collect in the real world. Therefore, the community has been
focusing on generating safety-critical scenarios in simulation. However,
controlling attributes such as collision type and time-to-accident (TTA)
remains challenging. We introduce a new task called controllable collision
scenario generation, where the goal is to produce trajectories that realize a
user-specified collision type and TTA, to investigate the feasibility of
automatically generating desired collision scenarios. To support this task, we
present COLLIDE, a large-scale collision scenario dataset constructed by
transforming real-world driving logs into diverse collisions, balanced across
five representative collision types and different TTA intervals. We propose a
framework that predicts Collision Pattern, a compact and interpretable
representation that captures the spatial configuration of the ego and the
adversarial vehicles at impact, before rolling out full adversarial
trajectories. Experiments show that our approach outperforms strong baselines
in both collision rate and controllability. Furthermore, generated scenarios
consistently induce higher planner failure rates, revealing limitations of
existing planners. We demonstrate that these scenarios fine-tune planners for
robustness improvements, contributing to safer AV deployment in different
collision scenarios.

</details>


### [5] [Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications](https://arxiv.org/abs/2510.12215)
*Chanwoo Kim,Jihwan Yoon,Hyeonseong Kim,Taemoon Jeong,Changwoo Yoo,Seungbeen Lee,Soohwan Byeon,Hoon Chung,Matthew Pan,Jean Oh,Kyungjae Lee,Sungjoon Choi*

Main category: cs.RO

TL;DR: 提出了一种结合数据驱动奖励和规则目标的移动机器人导航框架，通过密度奖励学习和采样前瞻控制器实现安全自适应导航，并蒸馏为紧凑的学生策略用于实时部署。


<details>
  <summary>Details</summary>
Motivation: 在动态人类环境中，机器人导航需要平衡适应性和安全性。假设将数据驱动奖励与规则目标结合能实现更好的平衡。

Method: 学习基于密度的奖励函数，结合障碍物避让和到达目标的规则目标，使用采样前瞻控制器生成监督动作，然后蒸馏为紧凑的学生策略。

Result: 在合成和电梯共乘仿真中，相比基线方法在成功率和时间效率上都有提升，真实世界演示验证了部署的实用性。

Conclusion: 该框架成功实现了安全自适应导航，在仿真和真实环境中都表现出色，证明了数据驱动奖励与规则目标结合的有效性。

Abstract: Mobile robot navigation in dynamic human environments requires policies that
balance adaptability to diverse behaviors with compliance to safety
constraints. We hypothesize that integrating data-driven rewards with
rule-based objectives enables navigation policies to achieve a more effective
balance of adaptability and safety. To this end, we develop a framework that
learns a density-based reward from positive and negative demonstrations and
augments it with rule-based objectives for obstacle avoidance and goal
reaching. A sampling-based lookahead controller produces supervisory actions
that are both safe and adaptive, which are subsequently distilled into a
compact student policy suitable for real-time operation with uncertainty
estimates. Experiments in synthetic and elevator co-boarding simulations show
consistent gains in success rate and time efficiency over baselines, and
real-world demonstrations with human participants confirm the practicality of
deployment. A video illustrating this work can be found on our project page
https://chanwookim971024.github.io/PioneeR/.

</details>


### [6] [Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model](https://arxiv.org/abs/2510.12276)
*Fuhao Li,Wenxuan Song,Han Zhao,Jingbo Wang,Pengxiang Ding,Donglin Wang,Long Zeng,Haoang Li*

Main category: cs.RO

TL;DR: 提出了Spatial Forcing (SF)方法，通过将VLA模型的中间视觉嵌入与预训练3D基础模型生成的几何表示对齐，隐式地增强模型的空间理解能力，无需依赖显式3D输入或深度估计器。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型主要基于2D数据预训练，缺乏准确的空间感知能力，限制了在3D物理世界中的操作能力。现有解决方案要么依赖有噪声的3D传感器输入，要么受限于深度估计器的性能。

Method: SF是一种对齐策略，将VLA模型的中间视觉嵌入与预训练3D基础模型生成的几何表示进行对齐，引导VLA模型编码更丰富的空间表示。

Result: 在仿真和真实环境中的广泛实验表明，SF实现了最先进的结果，超越了基于2D和3D的VLA模型，训练速度提升高达3.8倍，并在多样化的机器人任务中提高了数据效率。

Conclusion: SF是一种简单有效的策略，能够隐式地增强VLA模型的空间理解能力，无需依赖显式3D输入，在机器人任务中表现出优越的性能和效率。

Abstract: Vision-language-action (VLA) models have recently shown strong potential in
enabling robots to follow language instructions and execute precise actions.
However, most VLAs are built upon vision-language models pretrained solely on
2D data, which lack accurate spatial awareness and hinder their ability to
operate in the 3D physical world. Existing solutions attempt to incorporate
explicit 3D sensor inputs such as depth maps or point clouds, but these
approaches face challenges due to sensor noise, hardware heterogeneity, and
incomplete depth coverage in existing datasets. Alternative methods that
estimate 3D cues from 2D images also suffer from the limited performance of
depth estimators.We propose Spatial Forcing (SF), a simple yet effective
alignment strategy that implicitly forces VLA models to develop spatial
comprehension capabilities without relying on explicit 3D inputs or depth
estimators. SF aligns intermediate visual embeddings of VLAs with geometric
representations produced by pretrained 3D foundation models. By enforcing
alignment at intermediate layers, SF guides VLAs to encode richer spatial
representations that enhance action precision.Extensive experiments in
simulation and real-world environments demonstrate that SF achieves
state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further
accelerates training by up to 3.8x and improves data efficiency across diverse
robotic tasks. Project page is at https://spatial-forcing.github.io/

</details>


### [7] [Shape-Aware Whole-Body Control for Continuum Robots with Application in Endoluminal Surgical Robotics](https://arxiv.org/abs/2510.12332)
*Mohammadreza Kasaei,Mostafa Ghobadi,Mohsen Khadem*

Main category: cs.RO

TL;DR: 提出了一种用于肌腱驱动连续体机器人的形状感知全身控制框架，结合物理模型和增强神经ODE实现精确形状估计，通过MPPI控制器优化尖端跟踪、骨架顺应性和避障，在支气管镜手术中实现毫米级精度和安全性提升。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜手术中传统仅控制尖端方法导致的壁接触、组织创伤或无法到达远端目标的问题，需要在弯曲的患者特定解剖结构中实现精确安全导航。

Method: 结合物理信息骨架模型和增强神经ODE进行残差学习，实现精确形状估计和高效雅可比计算；采用基于采样的MPPI控制器联合优化尖端跟踪、骨架顺应性和避障；任务管理器允许实时调整目标。

Result: 仿真研究显示在不同场景下达到毫米级精度，包括轨迹跟踪、动态避障和形状约束到达；真实机器人实验在支气管镜模型中验证了框架，相比仅使用操纵杆和现有基线方法，提高了管腔跟随精度、减少壁接触并增强适应性。

Conclusion: 所提框架在微创内窥镜手术中具有提高安全性、可靠性和操作效率的潜力，并适用于其他受限和安全关键环境。

Abstract: This paper presents a shape-aware whole-body control framework for
tendon-driven continuum robots with direct application to endoluminal surgical
navigation. Endoluminal procedures, such as bronchoscopy, demand precise and
safe navigation through tortuous, patient-specific anatomy where conventional
tip-only control often leads to wall contact, tissue trauma, or failure to
reach distal targets. To address these challenges, our approach combines a
physics-informed backbone model with residual learning through an Augmented
Neural ODE, enabling accurate shape estimation and efficient Jacobian
computation. A sampling-based Model Predictive Path Integral (MPPI) controller
leverages this representation to jointly optimize tip tracking, backbone
conformance, and obstacle avoidance under actuation constraints. A task manager
further enhances adaptability by allowing real-time adjustment of objectives,
such as wall clearance or direct advancement, during tele-operation. Extensive
simulation studies demonstrate millimeter-level accuracy across diverse
scenarios, including trajectory tracking, dynamic obstacle avoidance, and
shape-constrained reaching. Real-robot experiments on a bronchoscopy phantom
validate the framework, showing improved lumen-following accuracy, reduced wall
contacts, and enhanced adaptability compared to joystick-only navigation and
existing baselines. These results highlight the potential of the proposed
framework to increase safety, reliability, and operator efficiency in minimally
invasive endoluminal surgery, with broader applicability to other confined and
safety-critical environments.

</details>


### [8] [Achieving Meaningful Collaboration: Worker-centered Design of a Physical Human-Robot Collaborative Blending Task](https://arxiv.org/abs/2510.12340)
*Nicky Mol,Luka Peternel,Alessandro Ianniello,Denis Zatyagov,Auke Nachenius,Stephan Balvert,J. Micah Prendergast,Sara Muscolo,Olger Siebinga,Eva Verhoef,Deborah Forster,David A. Abbink*

Main category: cs.RO

TL;DR: 本文提倡在工业机器人应用中采用跨学科方法，整合学术研究与实践经验，重点关注工人福祉和工作吸引力，并以飞机发动机维修为例进行验证。


<details>
  <summary>Details</summary>
Motivation: 应对劳动力短缺、人口老龄化和生产需求增长等社会挑战，通过跨学科方法提升工业机器人应用效果。

Method: 采用跨学科方法，整合学术研究、实践经验和体验知识，在飞机发动机维修场景中探索协作机器人的应用潜力。

Result: 展示了跨学科方法在工业机器人应用中的可行性和价值，特别是在飞机发动机维修维护操作中。

Conclusion: 跨学科方法是工业机器人应用的关键，能够更好地平衡技术效率与工人福祉、工作吸引力等价值目标。

Abstract: The use of robots in industrial settings continues to grow, driven by the
need to address complex societal challenges such as labor shortages, aging
populations, and ever-increasing production demands. In this abstract, we
advocate for (and demonstrate) a transdisciplinary approach when considering
robotics in the workplace. Transdisciplinarity emphasizes the integration of
academic research with pragmatic expertise and embodied experiential knowledge,
that prioritize values such as worker wellbeing and job attractiveness. In the
following, we describe an ongoing multi-pronged effort to explore the potential
of collaborative robots in the context of airplane engine repair and
maintenance operations.

</details>


### [9] [PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing](https://arxiv.org/abs/2510.12346)
*Bingquan Li,Ning Wang,Tianwei Zhang,Zhicheng He,Yucong Wu*

Main category: cs.RO

TL;DR: 提出了PolyMap框架，基于感知的人形机器人楼梯攀爬规划系统，通过多传感器融合实时构建多边形楼梯平面语义地图，并进行脚步规划。


<details>
  <summary>Details</summary>
Motivation: 为了让人形机器人能够在未知空间中准确踩踏位置，模拟人类行走，特别是在楼梯攀爬场景中需要精确的感知和规划。

Method: 使用多传感器融合（LiDAR、RGB-D相机和IMU）进行平面分割和视觉里程计，构建实时多边形楼梯平面语义地图，并基于这些平面段进行脚步规划。

Result: 在NVIDIA Orin上部署，实现20-30Hz全身运动规划输出，室内外真实场景实验表明方法高效且鲁棒。

Conclusion: PolyMap框架为人形机器人楼梯攀爬提供了有效的感知和规划解决方案，在真实环境中表现出良好的性能。

Abstract: Recently, biped robot walking technology has been significantly developed,
mainly in the context of a bland walking scheme. To emulate human walking,
robots need to step on the positions they see in unknown spaces accurately. In
this paper, we present PolyMap, a perception-based locomotion planning
framework for humanoid robots to climb stairs. Our core idea is to build a
real-time polygonal staircase plane semantic map, followed by a footstep planar
using these polygonal plane segments. These plane segmentation and visual
odometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The
proposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz
whole-body motion planning output. Both indoor and outdoor real-scene
experiments indicate that our method is efficient and robust for humanoid robot
stair climbing.

</details>


### [10] [Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control](https://arxiv.org/abs/2510.12363)
*Jiale Fan,Andrei Cramariuc,Tifanny Portela,Marco Hutter*

Main category: cs.RO

TL;DR: 提出了一种用于机器人运动控制的预训练方法，通过探索性数据收集训练逆动力学模型，然后将其权重用于初始化PPO算法的actor和critic网络，显著提升了样本效率和任务性能。


<details>
  <summary>Details</summary>
Motivation: 在机器人强化学习中，每个技能通常都是从零开始学习，但属于同一机器人体现的所有任务特定策略很可能共享一些可泛化的知识。

Method: 使用任务无关的探索性数据收集算法收集多样化动态转换数据，通过监督学习训练Proprioceptive Inverse Dynamics Model (PIDM)，然后将预训练权重加载到actor和critic网络中以热启动策略优化。

Result: 在7个不同的机器人运动控制任务上验证，相比随机初始化，平均样本效率提升40.1%，任务性能提升7.5%。

Conclusion: 该方法为机器人强化学习提供了一种有效的预训练范式，能够显著改善学习效率和性能。

Abstract: The pretraining-finetuning paradigm has facilitated numerous transformative
advancements in artificial intelligence research in recent years. However, in
the domain of reinforcement learning (RL) for robot motion control, individual
skills are often learned from scratch despite the high likelihood that some
generalizable knowledge is shared across all task-specific policies belonging
to a single robot embodiment. This work aims to define a paradigm for
pretraining neural network models that encapsulate such knowledge and can
subsequently serve as a basis for warm-starting the RL process in classic
actor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin
with a task-agnostic exploration-based data collection algorithm to gather
diverse, dynamic transition data, which is then used to train a Proprioceptive
Inverse Dynamics Model (PIDM) through supervised learning. The pretrained
weights are loaded into both the actor and critic networks to warm-start the
policy optimization of actual tasks. We systematically validated our proposed
method on seven distinct robot motion control tasks, showing significant
benefits to this initialization strategy. Our proposed approach on average
improves sample efficiency by 40.1% and task performance by 7.5%, compared to
random initialization. We further present key ablation studies and empirical
analyses that shed light on the mechanisms behind the effectiveness of our
method.

</details>


### [11] [Controlling Intent Expressiveness in Robot Motion with Diffusion Models](https://arxiv.org/abs/2510.12370)
*Wenli Shi,Clemence Grislain,Olivier Sigaud,Mohamed Chetouani*

Main category: cs.RO

TL;DR: 提出了一种新颖的运动生成框架，能够在从高度可读性到高度模糊性的全谱系中实现可控的可读性，通过基于信息势场的建模方法和两阶段扩散框架生成不同可读性水平的轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹生成方法通常优先考虑效率，但未能清晰传达机器人意图给人类；现有可读性运动方法通常只产生单一"最可读"轨迹，忽略了在不同情境下调节意图表达性的需求。

Method: 基于信息势场模型为轨迹分配连续可读性分数，采用两阶段扩散框架：首先生成指定可读性水平的路径，然后将其转换为可执行的机器人动作。

Result: 在2D和3D到达任务中的实验表明，该方法能够产生具有不同可读性程度的多样化可控运动，同时达到与最先进方法相当的性能。

Conclusion: 该框架成功实现了机器人运动可读性的连续控制，为人类-机器人交互中意图表达的灵活调节提供了有效解决方案。

Abstract: Legibility of robot motion is critical in human-robot interaction, as it
allows humans to quickly infer a robot's intended goal. Although traditional
trajectory generation methods typically prioritize efficiency, they often fail
to make the robot's intentions clear to humans. Meanwhile, existing approaches
to legible motion usually produce only a single "most legible" trajectory,
overlooking the need to modulate intent expressiveness in different contexts.
In this work, we propose a novel motion generation framework that enables
controllable legibility across the full spectrum, from highly legible to highly
ambiguous motions. We introduce a modeling approach based on an Information
Potential Field to assign continuous legibility scores to trajectories, and
build upon it with a two-stage diffusion framework that first generates paths
at specified legibility levels and then translates them into executable robot
actions. Experiments in both 2D and 3D reaching tasks demonstrate that our
approach produces diverse and controllable motions with varying degrees of
legibility, while achieving performance comparable to SOTA. Code and project
page: https://legibility-modulator.github.io.

</details>


### [12] [Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking](https://arxiv.org/abs/2510.12392)
*Junhyuk So,Chiwoong Lee,Shinyoung Lee,Jungseul Ok,Eunhyeok Park*

Main category: cs.RO

TL;DR: 提出两种新技术来增强扩散策略的一致性和反应性：自我引导和自适应分块，显著提高了生成行为克隆在机器人操作任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决生成行为克隆中扩散策略的随机性导致的错误动作采样和开环控制的延迟响应问题，这些限制会导致意外任务失败和在动态环境中性能下降。

Method: 1. 自我引导：利用过去观察并隐式促进未来感知行为来提高动作保真度；2. 自适应分块：当反应性的好处超过时间一致性需求时，选择性地更新动作序列。

Result: 在广泛的模拟和真实世界机器人操作任务中，该方法显著提高了生成行为克隆的性能。

Conclusion: 所提出的自我引导和自适应分块技术有效解决了扩散策略的随机性和开环控制的延迟问题，为生成行为克隆提供了更一致和反应性的解决方案。

Abstract: Generative Behavior Cloning (GBC) is a simple yet effective framework for
robot learning, particularly in multi-task settings. Recent GBC methods often
employ diffusion policies with open-loop (OL) control, where actions are
generated via a diffusion process and executed in multi-step chunks without
replanning. While this approach has demonstrated strong success rates and
generalization, its inherent stochasticity can result in erroneous action
sampling, occasionally leading to unexpected task failures. Moreover, OL
control suffers from delayed responses, which can degrade performance in noisy
or dynamic environments. To address these limitations, we propose two novel
techniques to enhance the consistency and reactivity of diffusion policies: (1)
self-guidance, which improves action fidelity by leveraging past observations
and implicitly promoting future-aware behavior; and (2) adaptive chunking,
which selectively updates action sequences when the benefits of reactivity
outweigh the need for temporal consistency. Extensive experiments show that our
approach substantially improves GBC performance across a wide range of
simulated and real-world robotic manipulation tasks. Our code is available at
https://github.com/junhyukso/SGAC

</details>


### [13] [Robot Learning: A Tutorial](https://arxiv.org/abs/2510.12403)
*Francesco Capuano,Caroline Pascal,Adil Zouitine,Thomas Wolf,Michel Aractingi*

Main category: cs.RO

TL;DR: 本教程概述了现代机器人学习的发展历程，从强化学习和行为克隆的基础原理到能够跨任务和机器人实体操作的通才语言条件模型。


<details>
  <summary>Details</summary>
Motivation: 机器人学习正处于转折点，机器学习快速发展和大规模机器人数据的可用性推动了从传统基于模型方法向数据驱动学习范式的转变。

Method: 本教程采用概念理解与实践工具相结合的方法，通过$	exttt{lerobot}$中的现成示例来指导研究人员和从业者。

Result: 该教程为读者提供了必要的概念理解和实用工具，使他们能够为机器人学习的发展做出贡献。

Conclusion: 本教程旨在作为研究人员和从业者的指南，帮助他们掌握现代机器人学习的关键技术和工具。

Abstract: Robot learning is at an inflection point, driven by rapid advancements in
machine learning and the growing availability of large-scale robotics data.
This shift from classical, model-based methods to data-driven, learning-based
paradigms is unlocking unprecedented capabilities in autonomous systems. This
tutorial navigates the landscape of modern robot learning, charting a course
from the foundational principles of Reinforcement Learning and Behavioral
Cloning to generalist, language-conditioned models capable of operating across
diverse tasks and even robot embodiments. This work is intended as a guide for
researchers and practitioners, and our goal is to equip the reader with the
conceptual understanding and practical tools necessary to contribute to
developments in robot learning, with ready-to-use examples implemented in
$\texttt{lerobot}$.

</details>


### [14] [M3D-skin: Multi-material 3D-printed Tactile Sensor with Hierarchical Infill Structures for Pressure Sensing](https://arxiv.org/abs/2510.12419)
*Shunnosuke Yoshimura,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 提出了一种基于多材料FDM 3D打印填充图案的触觉传感器M3D-skin，通过导电和非导电柔性材料的分层结构实现压力检测，并展示了其在足底运动测量、机器人手集成和触觉操作中的应用。


<details>
  <summary>Details</summary>
Motivation: 触觉传感器在机器人抓取和人体运动测量中有广泛应用，但制造和集成难度限制了其应用范围。本研究旨在开发一种易于制造且具有高通用性的触觉传感器。

Method: 利用多材料FDM 3D打印机的填充图案作为传感原理，使用导电和非导电柔性材料创建具有特定填充图案的分层结构。柔性分层结构在压力下变形，导致电阻变化，从而获取触觉信息。

Result: 测量了分层结构修改对传感器特性的影响，展示了多瓦片传感器的制造和使用。通过足底运动模式测量、机器人手集成和触觉机器人操作等应用验证了传感器的有效性。

Conclusion: 提出的M3D-skin触觉传感器易于制造且具有高通用性，通过实验验证了其在多种应用场景中的有效性。

Abstract: Tactile sensors have a wide range of applications, from utilization in
robotic grippers to human motion measurement. If tactile sensors could be
fabricated and integrated more easily, their applicability would further
expand. In this study, we propose a tactile sensor-M3D-skin-that can be easily
fabricated with high versatility by leveraging the infill patterns of a
multi-material fused deposition modeling (FDM) 3D printer as the sensing
principle. This method employs conductive and non-conductive flexible filaments
to create a hierarchical structure with a specific infill pattern. The flexible
hierarchical structure deforms under pressure, leading to a change in
electrical resistance, enabling the acquisition of tactile information. We
measure the changes in characteristics of the proposed tactile sensor caused by
modifications to the hierarchical structure. Additionally, we demonstrate the
fabrication and use of a multi-tile sensor. Furthermore, as applications, we
implement motion pattern measurement on the sole of a foot, integration with a
robotic hand, and tactile-based robotic operations. Through these experiments,
we validate the effectiveness of the proposed tactile sensor.

</details>


### [15] [A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation](https://arxiv.org/abs/2510.12477)
*Gaoyuan Liu,Joris de Winter,Kelly Merckaert,Denis Steckelmacher,Ann Nowe,Bram Vanderborght*

Main category: cs.RO

TL;DR: 提出了一种混合强化学习规划框架，结合交互式运动规划器和RL任务规划器，解决人机协作中安全与效率的平衡问题


<details>
  <summary>Details</summary>
Motivation: 在人机协作环境中，安全机制通常会阻碍任务效率，因为人类干预会导致机器人备份运动和目标失败。频繁的运动重规划会增加计算负载和失败机会

Method: 使用混合强化学习规划框架，包含交互式运动规划器和RL任务规划器。RL任务规划器基于运动规划器的反馈选择统计上安全高效的任务序列，运动规划器通过检测人类手臂运动来保持任务执行过程无碰撞

Result: 在仿真和真实世界中的协作机器人上验证，相比硬编码任务运动规划方法，该框架能够：1）在关节和任务级别响应不确定的人类运动；2）减少重复失败目标命令的次数；3）减少总的重规划请求次数

Conclusion: 提出的混合规划框架有效平衡了人机协作中的安全性和效率，通过RL学习避免危险任务，同时通过运动规划确保所选任务的安全执行

Abstract: In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the
two core properties to evaluate robot performance. However, safety mechanisms
usually hinder task efficiency since human intervention will cause backup
motions and goal failures of the robot. Frequent motion replanning will
increase the computational load and the chance of failure. In this paper, we
present a hybrid Reinforcement Learning (RL) planning framework which is
comprised of an interactive motion planner and a RL task planner. The RL task
planner attempts to choose statistically safe and efficient task sequences
based on the feedback from the motion planner, while the motion planner keeps
the task execution process collision-free by detecting human arm motions and
deploying new paths when the previous path is not valid anymore. Intuitively,
the RL agent will learn to avoid dangerous tasks, while the motion planner
ensures that the chosen tasks are safe. The proposed framework is validated on
the cobot in both simulation and the real world, we compare the planner with
hard-coded task motion planning methods. The results show that our planning
framework can 1) react to uncertain human motions at both joint and task
levels; 2) reduce the times of repeating failed goal commands; 3) reduce the
total number of replanning requests.

</details>


### [16] [Fast Visuomotor Policy for Robotic Manipulation](https://arxiv.org/abs/2510.12483)
*Jingkai Jia,Tong Yang,Xueyao Chen,Chenhuan Liu,Wenqiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种名为Energy Policy的高效机器人操作策略框架，能够在单次前向传播中预测多模态动作，实现高速高精度操作。


<details>
  <summary>Details</summary>
Motivation: 针对高频机器人任务和资源受限系统，需要一种既能处理多模态动作又计算高效的策略框架。

Method: 采用能量分数作为学习目标来促进多模态动作建模，并引入能量MLP来实现该目标，保持架构简单高效。

Result: 在模拟环境和真实机器人任务中，Energy Policy达到或超越了最先进方法的性能，同时显著降低了计算开销。在MimicGen基准测试中实现了更优性能且推理速度更快。

Conclusion: Energy Policy是一个高效的多模态动作预测框架，在保持高性能的同时大幅减少了计算需求，适用于资源受限的机器人系统。

Abstract: We present a fast and effective policy framework for robotic manipulation,
named Energy Policy, designed for high-frequency robotic tasks and
resource-constrained systems. Unlike existing robotic policies, Energy Policy
natively predicts multimodal actions in a single forward pass, enabling
high-precision manipulation at high speed. The framework is built upon two core
components. First, we adopt the energy score as the learning objective to
facilitate multimodal action modeling. Second, we introduce an energy MLP to
implement the proposed objective while keeping the architecture simple and
efficient. We conduct comprehensive experiments in both simulated environments
and real-world robotic tasks to evaluate the effectiveness of Energy Policy.
The results show that Energy Policy matches or surpasses the performance of
state-of-the-art manipulation methods while significantly reducing
computational overhead. Notably, on the MimicGen benchmark, Energy Policy
achieves superior performance with at a faster inference compared to existing
approaches.

</details>


### [17] [Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge](https://arxiv.org/abs/2510.12509)
*Gaoyuan Liu,Bas Boom,Naftali Slob,Yuri Durodié,Ann Nowé,Bram Vanderborght*

Main category: cs.RO

TL;DR: 本文提出了一种用于果园修剪的机器人行为规划方法，解决了在复杂碰撞环境中高维机械臂的多层次规划问题，通过整合感知、建模和整体规划来提升修剪性能。


<details>
  <summary>Details</summary>
Motivation: 果园修剪是重要的农业实践，机器人修剪可以替代季节性专业劳动力。虽然已有研究主要关注感知挑战，但操纵的复杂性（包括关节空间和笛卡尔空间的规划与控制）往往被忽视，特别是在复杂树枝环境中的路径规划问题。

Method: 提出了一个综合的修剪工作流程，包括：制定高维机械臂在修剪场景中的规划问题、分析系统内在冗余性、整合感知、建模和整体规划。在真实机器人上实现了该工作流程。

Result: 实验表明，更全面的规划方法能显著提升机器人操纵器的性能。在真实机器人上成功实现了所提出的工作流程。

Conclusion: 这项工作补充了先前关于机器人修剪的研究，并为修剪应用中的规划研究和发展提供了动力。

Abstract: Pruning is an essential agricultural practice for orchards. Proper pruning
can promote healthier growth and optimize fruit production throughout the
orchard's lifespan. Robot manipulators have been developed as an automated
solution for this repetitive task, which typically requires seasonal labor with
specialized skills. While previous research has primarily focused on the
challenges of perception, the complexities of manipulation are often
overlooked. These challenges involve planning and control in both joint and
Cartesian spaces to guide the end-effector through intricate, obstructive
branches. Our work addresses the behavior planning challenge for a robotic
pruning system, which entails a multi-level planning problem in environments
with complex collisions. In this paper, we formulate the planning problem for a
high-dimensional robotic arm in a pruning scenario, investigate the system's
intrinsic redundancies, and propose a comprehensive pruning workflow that
integrates perception, modeling, and holistic planning. In our experiments, we
demonstrate that more comprehensive planning methods can significantly enhance
the performance of the robotic manipulator. Finally, we implement the proposed
workflow on a real-world robot. As a result, this work complements previous
efforts on robotic pruning and motivates future research and development in
planning for pruning applications.

</details>


### [18] [Two-stream network-driven vision-based tactile sensor for object feature extraction and fusion perception](https://arxiv.org/abs/2510.12528)
*Muxing Huang,Zibin Chen,Weiliang Xu,Zilan Li,Yuanzhi Zhou,Guoyuan Zhou,Wenjing Chen,Xinming Li*

Main category: cs.RO

TL;DR: 提出基于视觉的触觉系统双流网络特征提取与融合感知策略，通过分布式方法提取物体内外特征，结合深度图和接触力数据，使用CNN提取特征后进行加权融合，显著提升物体识别准确率。


<details>
  <summary>Details</summary>
Motivation: 视觉触觉传感器在提取物体物理属性时产生大量冗余信息，且单维度提取缺乏有效融合，无法充分表征物体属性，限制了识别精度的提升。

Method: 采用双流网络特征提取和融合策略：一路通过三维重建获取深度图信息，另一路通过测量接触力数据获取硬度信息；使用CNN提取特征后进行加权融合。

Result: 力预测误差0.06N（12N范围内），硬度识别准确率98.0%，形状识别准确率93.75%；融合算法下实际抓取场景物体识别准确率超过98.5%。

Conclusion: 该方法专注于物体物理属性感知，增强了人工触觉系统从感知到认知的能力，使其能够在具身感知应用中使用。

Abstract: Tactile perception is crucial for embodied intelligent robots to recognize
objects. Vision-based tactile sensors extract object physical attributes
multidimensionally using high spatial resolution; however, this process
generates abundant redundant information. Furthermore, single-dimensional
extraction, lacking effective fusion, fails to fully characterize object
attributes. These challenges hinder the improvement of recognition accuracy. To
address this issue, this study introduces a two-stream network feature
extraction and fusion perception strategy for vision-based tactile systems.
This strategy employs a distributed approach to extract internal and external
object features. It obtains depth map information through three-dimensional
reconstruction while simultaneously acquiring hardness information by measuring
contact force data. After extracting features with a convolutional neural
network (CNN), weighted fusion is applied to create a more informative and
effective feature representation. In standard tests on objects of varying
shapes and hardness, the force prediction error is 0.06 N (within a 12 N
range). Hardness recognition accuracy reaches 98.0%, and shape recognition
accuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in
actual grasping scenarios exceeds 98.5%. Focused on object physical attributes
perception, this method enhances the artificial tactile system ability to
transition from perception to cognition, enabling its use in embodied
perception applications.

</details>


### [19] [Learning Robust Agile Flight Control with Stability Guarantees](https://arxiv.org/abs/2510.12611)
*Lukas Pries,Markus Ryll*

Main category: cs.RO

TL;DR: 提出了一种用于敏捷飞行控制的新型神经增强反馈控制器，结合了现有控制范式的优势，具有稳定性保证、高计算效率和强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在高速敏捷四旋翼飞行中，需要在平台操作极限下实现精确轨迹跟踪，控制器必须处理执行器约束、具备抗干扰鲁棒性，并保持计算效率以满足安全关键应用需求。

Method: 开发了一种神经增强反馈控制器，通过结合现有最先进控制范式的优势，解决各自的局限性。控制器具有非线性反馈结构，在仿真中实现快速稳定的学习过程。

Result: 控制器能够准确跟踪高度激进的轨迹，甚至超越执行器的可行性限制。在极度干扰环境中仍能保持鲁棒性和跟踪性能，无需训练增强或微调即可直接部署到真实平台。

Conclusion: 该控制器提供了通用稳定性保证，具有高计算效率，能够快速更新，并且其固有的鲁棒性使其能够直接从仿真部署到现实世界平台。

Abstract: In the evolving landscape of high-speed agile quadrotor flight, achieving
precise trajectory tracking at the platform's operational limits is paramount.
Controllers must handle actuator constraints, exhibit robustness to
disturbances, and remain computationally efficient for safety-critical
applications. In this work, we present a novel neural-augmented feedback
controller for agile flight control. The controller addresses individual
limitations of existing state-of-the-art control paradigms and unifies their
strengths. We demonstrate the controller's capabilities, including the accurate
tracking of highly aggressive trajectories that surpass the feasibility of the
actuators. Notably, the controller provides universal stability guarantees,
enhancing its robustness and tracking performance even in exceedingly
disturbance-prone settings. Its nonlinear feedback structure is highly
efficient enabling fast computation at high update rates. Moreover, the
learning process in simulation is both fast and stable, and the controller's
inherent robustness allows direct deployment to real-world platforms without
the need for training augmentations or fine-tuning.

</details>


### [20] [Designing Tools with Control Confidence](https://arxiv.org/abs/2510.12630)
*Ajith Anil Meera,Abian Torres,Pablo Lanillos*

Main category: cs.RO

TL;DR: 本文提出了一种考虑控制置信度的自主工具设计框架，通过引入神经启发的控制置信度项来优化机器人工具的鲁棒性，在环境不确定性下表现优于纯精度驱动的设计方法。


<details>
  <summary>Details</summary>
Motivation: 当前自主工具设计框架仅依赖性能优化，未考虑代理在重复使用工具时的置信度。受史前人类设计石器的启发，作者认为增加对工具的置信度有助于提高工具在环境不确定性下的鲁棒性。

Method: 提出了任务条件自主手工具设计优化框架，引入神经启发的控制置信度项到优化过程中，使用基于CMAES的进化优化策略进行工具设计。

Result: 仿真实验表明，以控制置信度为目标函数设计的工具在环境不确定性下比纯精度驱动设计的工具更鲁棒，且在控制扰动下能在鲁棒性和目标精度之间取得平衡。CMAES优化器在最少迭代次数内设计出最优工具。

Conclusion: 将控制置信度纳入工具设计目标函数可以显著提高工具的鲁棒性，为自主工具设计提供了新的优化方向。

Abstract: Prehistoric humans invented stone tools for specialized tasks by not just
maximizing the tool's immediate goal-completion accuracy, but also increasing
their confidence in the tool for later use under similar settings. This factor
contributed to the increased robustness of the tool, i.e., the least
performance deviations under environmental uncertainties. However, the current
autonomous tool design frameworks solely rely on performance optimization,
without considering the agent's confidence in tool use for repeated use. Here,
we take a step towards filling this gap by i) defining an optimization
framework for task-conditioned autonomous hand tool design for robots, where
ii) we introduce a neuro-inspired control confidence term into the optimization
routine that helps the agent to design tools with higher robustness. Through
rigorous simulations using a robotic arm, we show that tools designed with
control confidence as the objective function are more robust to environmental
uncertainties during tool use than a pure accuracy-driven objective. We further
show that adding control confidence to the objective function for tool design
provides a balance between the robustness and goal accuracy of the designed
tools under control perturbations. Finally, we show that our CMAES-based
evolutionary optimization strategy for autonomous tool design outperforms other
state-of-the-art optimizers by designing the optimal tool within the fewest
iterations. Code: https://github.com/ajitham123/Tool_design_control_confidence.

</details>


### [21] [Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop](https://arxiv.org/abs/2510.12662)
*Oz Gitelson,Satya Prakash Nayak,Ritam Raha,Anne-Kathrin Schmuck*

Main category: cs.RO

TL;DR: 提出了一个用于人机逻辑交互的新框架，使机器人能够可靠地满足时序逻辑任务，同时与追求独立未知任务的人类有效协作。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中机器人需要满足长期时序逻辑任务，而人类可能追求独立甚至冲突目标的问题，需要在保证机器人任务完成的同时最小化对人机干扰。

Method: 结合最大适应性和最小可调反馈两种能力：最大适应性让机器人能在线调整策略以利用人类行为进行合作；最小可调反馈仅在必要时请求人类合作以保证进展。

Result: 在真实世界的积木操作任务和Overcooked-AI基准测试中验证，该方法产生了超越现有方法的丰富涌现合作行为，同时保持强大的形式化保证。

Conclusion: 该框架在最小化人机干扰、保护人类自主性的同时，确保了机器人任务的持久满足，即使在人类目标冲突的情况下也能有效工作。

Abstract: We present a novel framework for human-robot \emph{logical} interaction that
enables robots to reliably satisfy (infinite horizon) temporal logic tasks
while effectively collaborating with humans who pursue independent and unknown
tasks. The framework combines two key capabilities: (i) \emph{maximal
adaptation} enables the robot to adjust its strategy \emph{online} to exploit
human behavior for cooperation whenever possible, and (ii) \emph{minimal
tunable feedback} enables the robot to request cooperation by the human online
only when necessary to guarantee progress. This balance minimizes human-robot
interference, preserves human autonomy, and ensures persistent robot task
satisfaction even under conflicting human goals. We validate the approach in a
real-world block-manipulation task with a Franka Emika Panda robotic arm and in
the Overcooked-AI benchmark, demonstrating that our method produces rich,
\emph{emergent} cooperative behaviors beyond the reach of existing approaches,
while maintaining strong formal guarantees.

</details>


### [22] [Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning](https://arxiv.org/abs/2510.12684)
*Alvaro Belmonte-Baeza,Miguel Cazorla,Gabriel J. García,Carlos J. Pérez-Del-Pulgar,Jorge Pomares*

Main category: cs.RO

TL;DR: 提出用于月球环境的四足移动机械臂约束强化学习框架，整合全身运动与操作能力，确保碰撞避免、动态稳定性和能效等安全约束，在月球重力条件下实现精确的6D末端执行器姿态跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统轮式机器人在非结构化和陡峭地形中的局限性促使采用腿式机器人，其具有更好的移动性和适应性，适合月球基地建设和探索任务。

Method: 采用约束强化学习框架，集成全身运动与操作能力，明确处理碰撞避免、动态稳定性和功率效率等关键安全约束，适应月球特定条件如低重力和不规则地形。

Result: 实验结果显示框架在6D任务空间末端执行器姿态跟踪中达到平均位置精度4厘米和方向精度8.1度，系统始终遵守软硬约束，在月球重力条件下表现出自适应行为。

Conclusion: 该工作有效桥接了自适应学习与关键任务安全需求，为未来月球任务的高级自主机器人探索者铺平了道路。

Abstract: Robotics plays a pivotal role in planetary science and exploration, where
autonomous and reliable systems are crucial due to the risks and challenges
inherent to space environments. The establishment of permanent lunar bases
demands robotic platforms capable of navigating and manipulating in the harsh
lunar terrain. While wheeled rovers have been the mainstay for planetary
exploration, their limitations in unstructured and steep terrains motivate the
adoption of legged robots, which offer superior mobility and adaptability. This
paper introduces a constrained reinforcement learning framework designed for
autonomous quadrupedal mobile manipulators operating in lunar environments. The
proposed framework integrates whole-body locomotion and manipulation
capabilities while explicitly addressing critical safety constraints, including
collision avoidance, dynamic stability, and power efficiency, in order to
ensure robust performance under lunar-specific conditions, such as reduced
gravity and irregular terrain. Experimental results demonstrate the framework's
effectiveness in achieving precise 6D task-space end-effector pose tracking,
achieving an average positional accuracy of 4 cm and orientation accuracy of
8.1 degrees. The system consistently respects both soft and hard constraints,
exhibiting adaptive behaviors optimized for lunar gravity conditions. This work
effectively bridges adaptive learning with essential mission-critical safety
requirements, paving the way for advanced autonomous robotic explorers for
future lunar missions.

</details>


### [23] [Reflection-Based Task Adaptation for Self-Improving VLA](https://arxiv.org/abs/2510.12710)
*Baicheng Li,Dong Wu,Zike Yan,Xinchen Liu,Zecui Zeng,Lusong Li,Hongbin Zha*

Main category: cs.RO

TL;DR: 提出了Reflective Self-Adaptation框架，通过双路径架构实现VLA模型的快速自主任务适应，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉-语言-动作模型在适应新任务时效率低下，强化学习过程缓慢，阻碍快速任务掌握。

Method: 采用双路径架构：1) 失败驱动的反思RL路径，利用VLM因果推理自动合成密集奖励函数；2) 成功驱动的质量引导SFT路径，选择性地模仿高质量成功轨迹。

Result: 在挑战性操作任务实验中，该框架相比基线方法实现了更快的收敛速度和更高的最终成功率。

Conclusion: 该工作为创建能够高效可靠适应新环境的自改进智能体提供了稳健解决方案。

Abstract: Pre-trained Vision-Language-Action (VLA) models represent a major leap
towards general-purpose robots, yet efficiently adapting them to novel,
specific tasks in-situ remains a significant hurdle. While reinforcement
learning (RL) is a promising avenue for such adaptation, the process often
suffers from low efficiency, hindering rapid task mastery. We introduce
Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation
without human intervention. Our framework establishes a self-improving loop
where the agent learns from its own experience to enhance both strategy and
execution.
  The core of our framework is a dual-pathway architecture that addresses the
full adaptation lifecycle. First, a Failure-Driven Reflective RL pathway
enables rapid learning by using the VLM's causal reasoning to automatically
synthesize a targeted, dense reward function from failure analysis. This
provides a focused learning signal that significantly accelerates policy
exploration. However, optimizing such proxy rewards introduces a potential risk
of "reward hacking," where the agent masters the reward function but fails the
actual task. To counteract this, our second pathway, Success-Driven
Quality-Guided SFT, grounds the policy in holistic success. It identifies and
selectively imitates high-quality successful trajectories, ensuring the agent
remains aligned with the ultimate task goal. This pathway is strengthened by a
conditional curriculum mechanism to aid initial exploration.
  We conduct experiments in challenging manipulation tasks. The results
demonstrate that our framework achieves faster convergence and higher final
success rates compared to representative baselines. Our work presents a robust
solution for creating self-improving agents that can efficiently and reliably
adapt to new environments.

</details>


### [24] [Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control](https://arxiv.org/abs/2510.12717)
*Se Hwan Jeon,Ho Jae Lee,Seungwoo Hong,Sangbae Kim*

Main category: cs.RO

TL;DR: 提出了一种GPU并行化的残差架构，将MPC和RL在扭矩控制层面紧密集成，结合了模型控制的解释性和约束处理能力与RL的适应性。


<details>
  <summary>Details</summary>
Motivation: MPC提供可解释、可调的控制但受限于模型失配和实时计算约束，RL能产生鲁棒行为但缺乏可解释性且需要大量奖励工程。两者结合可互补优势。

Method: 开发了基于动力学的全身MPC公式，在RL训练中以100Hz并行评估数千个智能体。残差策略学习对MPC输出进行针对性修正。

Result: 相比独立MPC或端到端RL，该方法具有更高的样本效率、更好的渐近奖励、更广的速度命令跟踪范围，并能零样本适应未见步态和不平地形。

Conclusion: 模型控制先验作为强偏置，用简单奖励集引导策略朝向理想行为，成功结合了模型控制和RL的优势。

Abstract: Model Predictive Control (MPC) provides interpretable, tunable locomotion
controllers grounded in physical models, but its robustness depends on frequent
replanning and is limited by model mismatch and real-time computational
constraints. Reinforcement Learning (RL), by contrast, can produce highly
robust behaviors through stochastic training but often lacks interpretability,
suffers from out-of-distribution failures, and requires intensive reward
engineering. This work presents a GPU-parallelized residual architecture that
tightly integrates MPC and RL by blending their outputs at the torque-control
level. We develop a kinodynamic whole-body MPC formulation evaluated across
thousands of agents in parallel at 100 Hz for RL training. The residual policy
learns to make targeted corrections to the MPC outputs, combining the
interpretability and constraint handling of model-based control with the
adaptability of RL. The model-based control prior acts as a strong bias,
initializing and guiding the policy towards desirable behavior with a simple
set of rewards. Compared to standalone MPC or end-to-end RL, our approach
achieves higher sample efficiency, converges to greater asymptotic rewards,
expands the range of trackable velocity commands, and enables zero-shot
adaptation to unseen gaits and uneven terrain.

</details>


### [25] [T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping](https://arxiv.org/abs/2510.12724)
*Xin Fei,Zhixuan Xu,Huaicong Fang,Tianrui Zhang,Lin Shao*

Main category: cs.RO

TL;DR: T(R,O) Grasp是一个基于扩散模型的灵巧抓取框架，通过统一的T(R,O)图表示和高效的逆运动学求解器，能够快速生成准确多样的多机械手抓取姿态。


<details>
  <summary>Details</summary>
Motivation: 灵巧抓取在机器人学中面临高维状态和动作空间的复杂性挑战，需要开发能够高效生成准确抓取姿态的新方法。

Method: 提出T(R,O)图统一表示法建模机械手与物体间的空间变换关系，结合图扩散模型和高效逆运动学求解器，支持无条件和条件抓取合成。

Result: 在多种灵巧手上测试，平均成功率94.83%，推理速度0.21秒，A100 GPU上吞吐量41抓取/秒，显著优于现有基线方法。

Conclusion: 该方法具有鲁棒性和泛化能力，大幅减少内存消耗，高推理速度支持闭环灵巧操作，有望成为灵巧抓取的基础模型。

Abstract: Dexterous grasping remains a central challenge in robotics due to the
complexity of its high-dimensional state and action space. We introduce T(R,O)
Grasp, a diffusion-based framework that efficiently generates accurate and
diverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,
a unified representation that models spatial transformations between robotic
hands and objects while encoding their geometric properties. A graph diffusion
model, coupled with an efficient inverse kinematics solver, supports both
unconditioned and conditioned grasp synthesis. Extensive experiments on a
diverse set of dexterous hands show that T(R,O) Grasp achieves average success
rate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per
second on an NVIDIA A100 40GB GPU, substantially outperforming existing
baselines. In addition, our approach is robust and generalizable across
embodiments while significantly reducing memory consumption. More importantly,
the high inference speed enables closed-loop dexterous manipulation,
underscoring the potential of T(R,O) Grasp to scale into a foundation model for
dexterous grasping.

</details>


### [26] [HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions](https://arxiv.org/abs/2510.12733)
*Hang Yu,Julian Jordan,Julian Schmidt,Silvan Lindner,Alessandro Canevaro,Wilhelm Stork*

Main category: cs.RO

TL;DR: HYPE是一种混合运动规划器，将学习到的多模态轨迹提案作为启发式先验集成到蒙特卡洛树搜索中，通过自我条件占用预测建模双向交互，简化了成本函数设计。


<details>
  <summary>Details</summary>
Motivation: 在复杂城市环境中进行安全可解释的运动规划需要处理双向多智能体交互，现有方法需要设计复杂的成本函数来编码期望的车辆行为，这在考虑广泛复杂场景时非常困难。

Method: 提出HYPE混合规划器，将学习到的提案模型生成的多模态轨迹作为启发式先验集成到MCTS细化中，引入自我条件占用预测模型来建模双向交互，使用基于网格的简化成本项。

Result: 在nuPlan和DeepUrban大规模真实世界基准测试中，HYPE实现了最先进的性能，特别是在安全性和适应性方面表现突出。

Conclusion: HYPE通过集成学习到的轨迹提案和简化成本函数设计，有效解决了复杂城市环境中的运动规划问题，在安全性和适应性方面达到先进水平。

Abstract: Safe and interpretable motion planning in complex urban environments needs to
reason about bidirectional multi-agent interactions. This reasoning requires to
estimate the costs of potential ego driving maneuvers. Many existing planners
generate initial trajectories with sampling-based methods and refine them by
optimizing on learned predictions of future environment states, which requires
a cost function that encodes the desired vehicle behavior. Designing such a
cost function can be very challenging, especially if a wide range of complex
urban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego
proposal-conditioned predictions, a planner that integrates multimodal
trajectory proposals from a learned proposal model as heuristic priors into a
Monte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,
we introduce an ego-conditioned occupancy prediction model, enabling
consistent, scene-aware reasoning. Our design significantly simplifies cost
function design in refinement by considering proposal-driven guidance,
requiring only minimalistic grid-based cost terms. Evaluations on large-scale
real-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves
state-of-the-art performance, especially in safety and adaptability.

</details>
