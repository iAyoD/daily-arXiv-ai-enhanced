{"id": "2511.00026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00026", "abs": "https://arxiv.org/abs/2511.00026", "authors": ["Chaitanya Shinde", "Divya Garikapati"], "title": "Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience", "comment": null, "summary": "Generative Artificial Intelligence is emerging as a transformative force in\nthe automotive industry, enabling novel applications across vehicle design,\nmanufacturing, autonomous driving, predictive maintenance, and in vehicle user\nexperience. This paper provides a comprehensive review of the current state of\nGenAI in automotive, highlighting enabling technologies such as Generative\nAdversarial Networks and Variational Autoencoders. Key opportunities include\naccelerating autonomous driving validation through synthetic data generation,\noptimizing component design, and enhancing human machine interaction via\npersonalized and adaptive interfaces. At the same time, the paper identifies\nsignificant technical, ethical, and safety challenges, including computational\ndemands, bias, intellectual property concerns, and adversarial robustness, that\nmust be addressed for responsible deployment. A case study on Mercedes Benzs\nMBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more\nnatural, proactive, and personalized in car interactions compared to legacy\nrule based assistants. Through this review and case study, the paper outlines\nboth the promise and limitations of GenAI integration in the automotive sector\nand presents directions for future research and development aimed at achieving\nsafer, more efficient, and user centric mobility. Unlike prior reviews that\nfocus solely on perception or manufacturing, this paper emphasizes generative\nAI in voice based HMI, bridging safety and user experience perspectives.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u5f0fAI\u5728\u6c7d\u8f66\u884c\u4e1a\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u91cd\u70b9\u5206\u6790\u4e86GAN\u548cVAE\u7b49\u4f7f\u80fd\u6280\u672f\uff0c\u63a2\u8ba8\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u9a8c\u8bc1\u3001\u96f6\u90e8\u4ef6\u8bbe\u8ba1\u4f18\u5316\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u65b9\u9762\u7684\u673a\u9047\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8ba1\u7b97\u9700\u6c42\u3001\u504f\u89c1\u3001\u77e5\u8bc6\u4ea7\u6743\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u5728\u6210\u4e3a\u6c7d\u8f66\u884c\u4e1a\u7684\u53d8\u9769\u529b\u91cf\uff0c\u4f46\u73b0\u6709\u7efc\u8ff0\u591a\u96c6\u4e2d\u4e8e\u611f\u77e5\u6216\u5236\u9020\u9886\u57df\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u751f\u6210\u5f0fAI\u5728\u8bed\u97f3\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u8fde\u63a5\u5b89\u5168\u6027\u548c\u7528\u6237\u4f53\u9a8c\u89c6\u89d2\u3002", "method": "\u901a\u8fc7\u5168\u9762\u6587\u732e\u56de\u987e\u548c\u6848\u4f8b\u5206\u6790\uff08\u6885\u8d5b\u5fb7\u65af-\u5954\u9a70MBUX\u865a\u62df\u52a9\u624b\uff09\uff0c\u7cfb\u7edf\u68b3\u7406\u751f\u6210\u5f0fAI\u5728\u6c7d\u8f66\u884c\u4e1a\u7684\u5e94\u7528\u73b0\u72b6\u3001\u6280\u672f\u57fa\u7840\u548c\u53d1\u5c55\u524d\u666f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u5f0fAI\u80fd\u591f\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u9a8c\u8bc1\u3001\u4f18\u5316\u96f6\u90e8\u4ef6\u8bbe\u8ba1\uff0c\u5e76\u5b9e\u73b0\u6bd4\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u52a9\u624b\u66f4\u81ea\u7136\u3001\u4e3b\u52a8\u548c\u4e2a\u6027\u5316\u7684\u8f66\u5185\u4ea4\u4e92\u4f53\u9a8c\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u6c7d\u8f66\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u6280\u672f\u3001\u4f26\u7406\u548c\u5b89\u5168\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u5e94\u81f4\u529b\u4e8e\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u79fb\u52a8\u51fa\u884c\u3002"}}
{"id": "2511.00033", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00033", "abs": "https://arxiv.org/abs/2511.00033", "authors": ["Diqi He", "Xuehao Gao", "Hao Li", "Junwei Han", "Dingwen Zhang"], "title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization", "comment": null, "summary": "The Zero-shot Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) task requires agents to navigate previously unseen 3D environments\nusing natural language instructions, without any scene-specific training. A\ncritical challenge in this setting lies in ensuring agents' actions align with\nboth spatial structure and task intent over long-horizon execution. Existing\nmethods often fail to achieve robust navigation due to a lack of structured\ndecision-making and insufficient integration of feedback from previous actions.\nTo address these challenges, we propose STRIDER (Instruction-Aligned Structural\nDecision Space Optimization), a novel framework that systematically optimizes\nthe agent's decision space by integrating spatial layout priors and dynamic\ntask feedback. Our approach introduces two key innovations: 1) a Structured\nWaypoint Generator that constrains the action space through spatial structure,\nand 2) a Task-Alignment Regulator that adjusts behavior based on task progress,\nensuring semantic alignment throughout navigation. Extensive experiments on the\nR2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms\nstrong SOTA across key metrics; in particular, it improves Success Rate (SR)\nfrom 29% to 35%, a relative gain of 20.7%. Such results highlight the\nimportance of spatially constrained decision-making and feedback-guided\nexecution in improving navigation fidelity for zero-shot VLN-CE.", "AI": {"tldr": "STRIDER\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u5e03\u5c40\u5148\u9a8c\u548c\u52a8\u6001\u4efb\u52a1\u53cd\u9988\uff0c\u7cfb\u7edf\u4f18\u5316\u667a\u80fd\u4f53\u5728\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u51b3\u7b56\u548c\u5148\u524d\u52a8\u4f5c\u53cd\u9988\u7684\u5145\u5206\u6574\u5408\uff0c\u5bfc\u81f4\u5bfc\u822a\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u89e3\u51b3\u52a8\u4f5c\u4e0e\u7a7a\u95f4\u7ed3\u6784\u548c\u4efb\u52a1\u610f\u56fe\u7684\u957f\u671f\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51faSTRIDER\u6846\u67b6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u8def\u5f84\u70b9\u751f\u6210\u5668\uff08\u901a\u8fc7\u7a7a\u95f4\u7ed3\u6784\u7ea6\u675f\u52a8\u4f5c\u7a7a\u95f4\uff09\u548c\u4efb\u52a1\u5bf9\u9f50\u8c03\u8282\u5668\uff08\u57fa\u4e8e\u4efb\u52a1\u8fdb\u5ea6\u8c03\u6574\u884c\u4e3a\uff09\uff0c\u786e\u4fdd\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728R2R-CE\u548cRxR-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u4ece29%\u63d0\u5347\u81f335%\uff0c\u76f8\u5bf9\u589e\u76ca\u8fbe20.7%\u3002", "conclusion": "\u7a7a\u95f4\u7ea6\u675f\u51b3\u7b56\u548c\u53cd\u9988\u5f15\u5bfc\u6267\u884c\u5bf9\u4e8e\u63d0\u5347\u96f6\u6837\u672cVLN-CE\u5bfc\u822a\u4fdd\u771f\u5ea6\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.00041", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00041", "abs": "https://arxiv.org/abs/2511.00041", "authors": ["Yingzhao Jian", "Zhongan Wang", "Yi Yang", "Hehe Fan"], "title": "Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World", "comment": null, "summary": "Humanoid agents often struggle to handle flexible and diverse interactions in\nopen environments. A common solution is to collect massive datasets to train a\nhighly capable model, but this approach can be prohibitively expensive. In this\npaper, we explore an alternative solution: empowering off-the-shelf\nVision-Language Models (VLMs, such as GPT-4) to control humanoid agents,\nthereby leveraging their strong open-world generalization to mitigate the need\nfor extensive data collection. To this end, we present \\textbf{BiBo}\n(\\textbf{B}uilding humano\\textbf{I}d agent \\textbf{B}y \\textbf{O}ff-the-shelf\nVLMs). It consists of two key components: (1) an \\textbf{embodied instruction\ncompiler}, which enables the VLM to perceive the environment and precisely\ntranslate high-level user instructions (e.g., {\\small\\itshape ``have a rest''})\ninto low-level primitive commands with control parameters (e.g.,\n{\\small\\itshape ``sit casually, location: (1, 2), facing: 90$^\\circ$''}); and\n(2) a diffusion-based \\textbf{motion executor}, which generates human-like\nmotions from these commands, while dynamically adapting to physical feedback\nfrom the environment. In this way, BiBo is capable of handling not only basic\ninteractions but also diverse and complex motions. Experiments demonstrate that\nBiBo achieves an interaction task success rate of 90.2\\% in open environments,\nand improves the precision of text-guided motion execution by 16.3\\% over prior\nmethods. The code will be made publicly available.", "AI": {"tldr": "BiBo\u7cfb\u7edf\u5229\u7528\u73b0\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u4eba\u5f62\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u6307\u4ee4\u7f16\u8bd1\u5668\u548c\u8fd0\u52a8\u6267\u884c\u5668\u5b9e\u73b0\u5f00\u653e\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u4ea4\u4e92\uff0c\u65e0\u9700\u5927\u91cf\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u667a\u80fd\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5904\u7406\u7075\u6d3b\u591a\u6837\u5316\u4ea4\u4e92\u7684\u56f0\u96be\uff0c\u907f\u514d\u6536\u96c6\u6d77\u91cf\u6570\u636e\u7684\u9ad8\u6602\u6210\u672c\uff0c\u5229\u7528\u73b0\u6210VLM\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5177\u8eab\u6307\u4ee4\u7f16\u8bd1\u5668 - \u8ba9VLM\u611f\u77e5\u73af\u5883\u5e76\u5c06\u9ad8\u7ea7\u6307\u4ee4\u8f6c\u6362\u4e3a\u5e26\u63a7\u5236\u53c2\u6570\u7684\u4f4e\u7ea7\u539f\u59cb\u547d\u4ee4\uff1b2) \u57fa\u4e8e\u6269\u6563\u7684\u8fd0\u52a8\u6267\u884c\u5668 - \u4ece\u547d\u4ee4\u751f\u6210\u7c7b\u4eba\u8fd0\u52a8\u5e76\u52a8\u6001\u9002\u5e94\u73af\u5883\u7269\u7406\u53cd\u9988\u3002", "result": "\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5b9e\u73b090.2%\u7684\u4ea4\u4e92\u4efb\u52a1\u6210\u529f\u7387\uff0c\u6587\u672c\u5f15\u5bfc\u8fd0\u52a8\u6267\u884c\u7cbe\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad816.3%\u3002", "conclusion": "BiBo\u7cfb\u7edf\u6210\u529f\u5c55\u793a\u4e86\u5229\u7528\u73b0\u6210VLM\u63a7\u5236\u4eba\u5f62\u667a\u80fd\u4f53\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u5904\u7406\u57fa\u7840\u4ea4\u4e92\u548c\u590d\u6742\u8fd0\u52a8\uff0c\u4e3a\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2511.00088", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00088", "abs": "https://arxiv.org/abs/2511.00088", "authors": ["NVIDIA", ":", "Yan Wang", "Wenjie Luo", "Junjie Bai", "Yulong Cao", "Tong Che", "Ke Chen", "Yuxiao Chen", "Jenna Diamond", "Yifan Ding", "Wenhao Ding", "Liang Feng", "Greg Heinrich", "Jack Huang", "Peter Karkus", "Boyi Li", "Pinyi Li", "Tsung-Yi Lin", "Dongran Liu", "Ming-Yu Liu", "Langechuan Liu", "Zhijian Liu", "Jason Lu", "Yunxiang Mao", "Pavlo Molchanov", "Lindsey Pavao", "Zhenghao Peng", "Mike Ranzinger", "Ed Schmerling", "Shida Shen", "Yunfei Shi", "Sarah Tariq", "Ran Tian", "Tilman Wekel", "Xinshuo Weng", "Tianjun Xiao", "Eric Yang", "Xiaodong Yang", "Yurong You", "Xiaohui Zeng", "Wenyuan Zhang", "Boris Ivanovic", "Marco Pavone"], "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "comment": null, "summary": "End-to-end architectures trained via imitation learning have advanced\nautonomous driving by scaling model size and data, yet performance remains\nbrittle in safety-critical long-tail scenarios where supervision is sparse and\ncausal understanding is limited. To address this, we introduce Alpamayo-R1\n(AR1), a vision-language-action model (VLA) that integrates Chain of Causation\nreasoning with trajectory planning to enhance decision-making in complex\ndriving scenarios. Our approach features three key innovations: (1) the Chain\nof Causation (CoC) dataset, built through a hybrid auto-labeling and\nhuman-in-the-loop pipeline producing decision-grounded, causally linked\nreasoning traces aligned with driving behaviors; (2) a modular VLA architecture\ncombining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI\napplications, with a diffusion-based trajectory decoder that generates\ndynamically feasible plans in real time; (3) a multi-stage training strategy\nusing supervised fine-tuning to elicit reasoning and reinforcement learning\n(RL) to optimize reasoning quality via large reasoning model feedback and\nenforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%\nimprovement in planning accuracy on challenging cases compared to a\ntrajectory-only baseline, with a 35% reduction in off-road rate and 25%\nreduction in close encounter rate in closed-loop simulation. RL post-training\nimproves reasoning quality by 45% as measured by a large reasoning model critic\nand reasoning-action consistency by 37%. Model scaling from 0.5B to 7B\nparameters shows consistent improvements. On-vehicle road tests confirm\nreal-time performance (99 ms latency) and successful urban deployment. By\nbridging interpretable reasoning with precise control, AR1 demonstrates a\npractical path towards Level 4 autonomous driving. We plan to release AR1\nmodels and a subset of the CoC in a future update.", "AI": {"tldr": "AR1\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u56e0\u679c\u94fe\u63a8\u7406\u4e0e\u8f68\u8ff9\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u63d0\u5347\u51b3\u7b56\u80fd\u529b\uff0c\u5728\u89c4\u5212\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u76f8\u6bd4\u7eaf\u8f68\u8ff9\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u7684\u957f\u5c3e\u573a\u666f\u4e2d\u6027\u80fd\u8106\u5f31\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u573a\u666f\u4e2d\u76d1\u7763\u7a00\u758f\u4e14\u56e0\u679c\u7406\u89e3\u6709\u9650\u3002", "method": "1) \u6784\u5efa\u56e0\u679c\u94fe\u6570\u636e\u96c6\uff1b2) \u6a21\u5757\u5316VLA\u67b6\u6784\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u8f68\u8ff9\u89e3\u7801\u5668\uff1b3) \u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u6311\u6218\u6027\u6848\u4f8b\u4e2d\u89c4\u5212\u51c6\u786e\u6027\u63d0\u534712%\uff0c\u8131\u8f68\u7387\u964d\u4f4e35%\uff0c\u8fd1\u8ddd\u79bb\u906d\u9047\u7387\u964d\u4f4e25%\uff1bRL\u540e\u8bad\u7ec3\u4f7f\u63a8\u7406\u8d28\u91cf\u63d0\u534745%\uff0c\u63a8\u7406-\u52a8\u4f5c\u4e00\u81f4\u6027\u63d0\u534737%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7cbe\u786e\u63a7\u5236\u76f8\u7ed3\u5408\uff0cAR1\u5c55\u793a\u4e86\u5b9e\u73b0L4\u7ea7\u81ea\u52a8\u9a7e\u9a76\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.00094", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00094", "abs": "https://arxiv.org/abs/2511.00094", "authors": ["Angelos Alexopoulos", "Agorakis Bompotas", "Nikitas Rigas Kalogeropoulos", "Panagiotis Kechagias", "Athanasios P. Kalogeras", "Christos Alexakos"], "title": "Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments", "comment": "Accepted for presentation to 11th IEEE International Smart Cities\n  Conference (ISC2 2025)", "summary": "Robotic systems have become integral to smart environments, enabling\napplications ranging from urban surveillance and automated agriculture to\nindustrial automation. However, their effective operation in dynamic settings -\nsuch as smart cities and precision farming - is challenged by continuously\nevolving topographies and environmental conditions. Traditional control systems\noften struggle to adapt quickly, leading to inefficiencies or operational\nfailures. To address this limitation, we propose a novel framework for\nautonomous and dynamic reconfiguration of robotic controllers using Digital\nTwin technology. Our approach leverages a virtual replica of the robot's\noperational environment to simulate and optimize movement trajectories in\nresponse to real-world changes. By recalculating paths and control parameters\nin the Digital Twin and deploying the updated code to the physical robot, our\nmethod ensures rapid and reliable adaptation without manual intervention. This\nwork advances the integration of Digital Twins in robotics, offering a scalable\nsolution for enhancing autonomy in smart, dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u673a\u5668\u4eba\u63a7\u5236\u5668\u81ea\u4e3b\u52a8\u6001\u91cd\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u73af\u5883\u6a21\u62df\u4f18\u5316\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5b9e\u73b0\u7269\u7406\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5feb\u901f\u81ea\u9002\u5e94\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\uff08\u5982\u667a\u6167\u57ce\u5e02\u3001\u7cbe\u51c6\u519c\u4e1a\uff09\u4e2d\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u5730\u5f62\u548c\u73af\u5883\u6761\u4ef6\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u64cd\u4f5c\u5931\u8d25\u3002", "method": "\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u521b\u5efa\u673a\u5668\u4eba\u64cd\u4f5c\u73af\u5883\u7684\u865a\u62df\u526f\u672c\uff0c\u6a21\u62df\u548c\u4f18\u5316\u8fd0\u52a8\u8f68\u8ff9\uff0c\u6839\u636e\u73b0\u5b9e\u4e16\u754c\u53d8\u5316\u91cd\u65b0\u8ba1\u7b97\u8def\u5f84\u548c\u63a7\u5236\u53c2\u6570\uff0c\u5e76\u5c06\u66f4\u65b0\u540e\u7684\u4ee3\u7801\u90e8\u7f72\u5230\u7269\u7406\u673a\u5668\u4eba\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u5feb\u901f\u53ef\u9760\u81ea\u9002\u5e94\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u8fdb\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u96c6\u6210\uff0c\u4e3a\u667a\u80fd\u52a8\u6001\u73af\u5883\u4e2d\u589e\u5f3a\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00112", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00112", "abs": "https://arxiv.org/abs/2511.00112", "authors": ["Yanbing Mao", "Yihao Cai", "Lui Sha"], "title": "Real-DRL: Teach and Learn in Reality", "comment": "37 pages", "summary": "This paper introduces the Real-DRL framework for safety-critical autonomous\nsystems, enabling runtime learning of a deep reinforcement learning (DRL) agent\nto develop safe and high-performance action policies in real plants (i.e., real\nphysical systems to be controlled), while prioritizing safety! The Real-DRL\nconsists of three interactive components: a DRL-Student, a PHY-Teacher, and a\nTrigger. The DRL-Student is a DRL agent that innovates in the dual\nself-learning and teaching-to-learn paradigm and the real-time safety-informed\nbatch sampling. On the other hand, PHY-Teacher is a physics-model-based design\nof action policies that focuses solely on safety-critical functions.\nPHY-Teacher is novel in its real-time patch for two key missions: i) fostering\nthe teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of\nreal plants. The Trigger manages the interaction between the DRL-Student and\nthe PHY-Teacher. Powered by the three interactive components, the Real-DRL can\neffectively address safety challenges that arise from the unknown unknowns and\nthe Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,\nii) automatic hierarchy learning (i.e., safety-first learning and then\nhigh-performance learning), and iii) safety-informed batch sampling to address\nthe learning experience imbalance caused by corner cases. Experiments with a\nreal quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole\nsystem, along with comparisons and ablation studies, demonstrate the Real-DRL's\neffectiveness and unique features.", "AI": {"tldr": "Real-DRL\u6846\u67b6\u901a\u8fc7DRL-Student\u3001PHY-Teacher\u548cTrigger\u4e09\u4e2a\u4ea4\u4e92\u7ec4\u4ef6\uff0c\u5728\u771f\u5b9e\u7269\u7406\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5b89\u5168\u4f18\u5148\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u672a\u77e5\u98ce\u9669\u548cSim2Real\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5b89\u5168\u5173\u952e\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u65f6\u5b89\u5168\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5e94\u5bf9\u672a\u77e5\u98ce\u9669\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u7ec4\u4ef6\u4ea4\u4e92\u6846\u67b6\uff1aDRL-Student\u8d1f\u8d23\u53cc\u91cd\u81ea\u5b66\u4e60\u4e0e\u6559\u5b66\u5b66\u4e60\u8303\u5f0f\uff0cPHY-Teacher\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u63d0\u4f9b\u5b89\u5168\u5173\u952e\u529f\u80fd\uff0cTrigger\u7ba1\u7406\u4e24\u8005\u4ea4\u4e92\u3002", "result": "\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u3001NVIDIA Isaac Gym\u56db\u8db3\u673a\u5668\u4eba\u548c\u5012\u7acb\u6446\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u4fdd\u8bc1\u548c\u81ea\u52a8\u5c42\u6b21\u5b66\u4e60\u3002", "conclusion": "Real-DRL\u6846\u67b6\u80fd\u591f\u6709\u6548\u786e\u4fdd\u5b89\u5168\u5173\u952e\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fd\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfDRL\u65b9\u6cd5\u7684\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2511.00139", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00139", "abs": "https://arxiv.org/abs/2511.00139", "authors": ["Yu Cui", "Yujian Zhang", "Lina Tao", "Yang Li", "Xinyu Yi", "Zhibin Li"], "title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection", "comment": null, "summary": "Achieving human-like dexterous manipulation remains a major challenge for\ngeneral-purpose robots. While Vision-Language-Action (VLA) models show\npotential in learning skills from demonstrations, their scalability is limited\nby scarce high-quality training data. Existing data collection methods face\ninherent constraints: manual teleoperation overloads human operators, while\nautomated planning often produces unnatural motions. We propose a Shared\nAutonomy framework that divides control between macro and micro motions. A\nhuman operator guides the robot's arm pose through intuitive VR teleoperation,\nwhile an autonomous DexGrasp-VLA policy handles fine-grained hand control using\nreal-time tactile and visual feedback. This division significantly reduces\ncognitive load and enables efficient collection of high-quality coordinated\narm-hand demonstrations. Using this data, we train an end-to-end VLA policy\nenhanced with our novel Arm-Hand Feature Enhancement module, which captures\nboth distinct and shared representations of macro and micro movements for more\nnatural coordination. Our Corrective Teleoperation system enables continuous\npolicy improvement through human-in-the-loop failure recovery. Experiments\ndemonstrate that our framework generates high-quality data with minimal\nmanpower and achieves a 90% success rate across diverse objects, including\nunseen instances. Comprehensive evaluations validate the system's effectiveness\nin developing dexterous manipulation capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5171\u4eab\u81ea\u4e3b\u6846\u67b6\uff0c\u901a\u8fc7VR\u9065\u64cd\u4f5c\u548c\u81ea\u4e3b\u624b\u90e8\u63a7\u5236\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u9ad8\u6548\u6536\u96c6\u9ad8\u8d28\u91cf\u7684\u624b\u81c2-\u624b\u90e8\u534f\u8c03\u6f14\u793a\u6570\u636e\uff0c\u5e76\u8bad\u7ec3\u7aef\u5230\u7aefVLA\u7b56\u7565\uff0c\u5728\u591a\u6837\u5316\u7269\u4f53\u4e0a\u8fbe\u523090%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u4e2d\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4eba\u5de5\u9065\u64cd\u4f5c\u8d1f\u62c5\u91cd\u3001\u81ea\u52a8\u5316\u89c4\u5212\u52a8\u4f5c\u4e0d\u81ea\u7136\u7b49\u9650\u5236\u3002", "method": "\u91c7\u7528\u5171\u4eab\u81ea\u4e3b\u6846\u67b6\uff0c\u4eba\u7c7b\u64cd\u4f5c\u5458\u901a\u8fc7VR\u63a7\u5236\u624b\u81c2\u5b8f\u89c2\u8fd0\u52a8\uff0c\u81ea\u4e3bDexGrasp-VLA\u7b56\u7565\u57fa\u4e8e\u89e6\u89c9\u548c\u89c6\u89c9\u53cd\u9988\u63a7\u5236\u624b\u90e8\u7cbe\u7ec6\u52a8\u4f5c\uff1b\u8bad\u7ec3\u7aef\u5230\u7aefVLA\u7b56\u7565\u5e76\u52a0\u5165\u624b\u81c2-\u624b\u90e8\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff1b\u901a\u8fc7\u7ea0\u6b63\u9065\u64cd\u4f5c\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002", "result": "\u4ee5\u6700\u5c11\u4eba\u529b\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5728\u591a\u6837\u5316\u7269\u4f53\uff08\u5305\u62ec\u672a\u89c1\u5b9e\u4f8b\uff09\u4e0a\u8fbe\u523090%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u6570\u636e\u6536\u96c6\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2511.00153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00153", "abs": "https://arxiv.org/abs/2511.00153", "authors": ["Justin Yu", "Yide Shentu", "Di Wu", "Pieter Abbeel", "Ken Goldberg", "Philipp Wu"], "title": "EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations", "comment": null, "summary": "Imitation learning from human demonstrations offers a promising approach for\nrobot skill acquisition, but egocentric human data introduces fundamental\nchallenges due to the embodiment gap. During manipulation, humans actively\ncoordinate head and hand movements, continuously reposition their viewpoint and\nuse pre-action visual fixation search strategies to locate relevant objects.\nThese behaviors create dynamic, task-driven head motions that static robot\nsensing systems cannot replicate, leading to a significant distribution shift\nthat degrades policy performance. We present EgoMI (Egocentric Manipulation\nInterface), a framework that captures synchronized end-effector and active head\ntrajectories during manipulation tasks, resulting in data that can be\nretargeted to compatible semi-humanoid robot embodiments. To handle rapid and\nwide-spanning head viewpoint changes, we introduce a memory-augmented policy\nthat selectively incorporates historical observations. We evaluate our approach\non a bimanual robot equipped with an actuated camera head and find that\npolicies with explicit head-motion modeling consistently outperform baseline\nmethods. Results suggest that coordinated hand-eye learning with EgoMI\neffectively bridges the human-robot embodiment gap for robust imitation\nlearning on semi-humanoid embodiments. Project page:\nhttps://egocentric-manipulation-interface.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86EgoMI\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u6349\u540c\u6b65\u7684\u672b\u7aef\u6267\u884c\u5668\u548c\u4e3b\u52a8\u5934\u90e8\u8f68\u8ff9\u6765\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u4eba\u673a\u4f53\u73b0\u5dee\u5f02\u95ee\u9898\uff0c\u4f7f\u7528\u8bb0\u5fc6\u589e\u5f3a\u7b56\u7565\u5904\u7406\u5feb\u901f\u89c6\u89d2\u53d8\u5316\u3002", "motivation": "\u4eba\u7c7b\u6f14\u793a\u7684\u6a21\u4eff\u5b66\u4e60\u9762\u4e34\u4f53\u73b0\u5dee\u5f02\u6311\u6218\uff0c\u4eba\u7c7b\u5728\u64cd\u4f5c\u65f6\u4e3b\u52a8\u534f\u8c03\u5934\u90e8\u548c\u624b\u90e8\u8fd0\u52a8\uff0c\u4ea7\u751f\u52a8\u6001\u7684\u5934\u90e8\u8fd0\u52a8\uff0c\u800c\u9759\u6001\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u65e0\u6cd5\u590d\u5236\u8fd9\u4e9b\u884c\u4e3a\uff0c\u5bfc\u81f4\u7b56\u7565\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f00\u53d1EgoMI\u6846\u67b6\u6355\u6349\u540c\u6b65\u7684\u672b\u7aef\u6267\u884c\u5668\u548c\u4e3b\u52a8\u5934\u90e8\u8f68\u8ff9\uff0c\u5f15\u5165\u8bb0\u5fc6\u589e\u5f3a\u7b56\u7565\u9009\u62e9\u6027\u6574\u5408\u5386\u53f2\u89c2\u5bdf\uff0c\u4ee5\u5904\u7406\u5feb\u901f\u548c\u5e7f\u6cdb\u7684\u5934\u90e8\u89c6\u89d2\u53d8\u5316\u3002", "result": "\u5728\u914d\u5907\u9a71\u52a8\u76f8\u673a\u5934\u7684\u53cc\u624b\u673a\u5668\u4eba\u4e0a\u8bc4\u4f30\uff0c\u663e\u793a\u5177\u6709\u663e\u5f0f\u5934\u90e8\u8fd0\u52a8\u5efa\u6a21\u7684\u7b56\u7565\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EgoMI\u901a\u8fc7\u534f\u8c03\u7684\u624b\u773c\u5b66\u4e60\u6709\u6548\u5f25\u5408\u4e86\u4eba\u673a\u4f53\u73b0\u5dee\u5f02\uff0c\u4e3a\u534a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u9c81\u68d2\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00193", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00193", "abs": "https://arxiv.org/abs/2511.00193", "authors": ["Faranak Akbarifar", "Nooshin Maghsoodi", "Sean P Dukelow", "Stephen Scott", "Parvin Mousavi"], "title": "Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach", "comment": null, "summary": "Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive\nkinematic biomarkers but requires 40-64 reaches, imposing time and fatigue\nburdens. We evaluate whether time-series foundation models can replace\nunrecorded trials from an early subset of reaches while preserving the\nreliability of standard Kinarm parameters.\n  Methods: We analyzed VGR speed signals from 461 stroke and 599 control\nparticipants across 4- and 8-target reaching protocols. We withheld all but the\nfirst 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,\nfine-tuned on 70 percent of subjects, to forecast synthetic trials. We\nrecomputed four kinematic features of reaching (reaction time, movement time,\nposture speed, maximum speed) on combined recorded plus forecasted trials and\ncompared them to full-length references using ICC(2,1).\n  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only\n8 recorded trials plus forecasts, matching the reliability of 24-28 recorded\nreaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA\nimprovements were minimal. Across cohorts and protocols, synthetic trials\nreplaced reaches without materially compromising feature reliability.\n  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR\nassessment time. For the most impaired stroke survivors, sessions drop from 4-5\nminutes to about 1 minute while preserving kinematic precision. This\nforecast-augmented paradigm promises efficient robotic evaluations for\nassessing motor impairments following stroke.", "AI": {"tldr": "\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08\u7279\u522b\u662fChronos\uff09\u9884\u6d4b\u672a\u8bb0\u5f55\u7684\u4f38\u624b\u8bd5\u9a8c\uff0c\u53ef\u5728\u4ec5\u4f7f\u75288\u6b21\u5b9e\u9645\u8bd5\u9a8c\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e24-28\u6b21\u5b8c\u6574\u8bd5\u9a8c\u76f8\u5f53\u7684\u53ef\u9760\u6027\uff0c\u663e\u8457\u7f29\u77edKinarm\u89c6\u89c9\u5f15\u5bfc\u4f38\u624b\u8bc4\u4f30\u65f6\u95f4\u3002", "motivation": "Kinarm\u673a\u5668\u4eba\u7684\u89c6\u89c9\u5f15\u5bfc\u4f38\u624b\u8bc4\u4f30\u9700\u898140-64\u6b21\u4f38\u624b\u8bd5\u9a8c\uff0c\u9020\u6210\u65f6\u95f4\u548c\u75b2\u52b3\u8d1f\u62c5\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u662f\u5426\u80fd\u7528\u65e9\u671f\u8bd5\u9a8c\u5b50\u96c6\u66ff\u4ee3\u672a\u8bb0\u5f55\u8bd5\u9a8c\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u53c2\u6570\u7684\u53ef\u9760\u6027\u3002", "method": "\u5206\u6790461\u540d\u4e2d\u98ce\u548c599\u540d\u5bf9\u7167\u53c2\u4e0e\u8005\u7684VGR\u901f\u5ea6\u4fe1\u53f7\uff0c\u4ec5\u4fdd\u7559\u524d8\u621616\u6b21\u8bd5\u9a8c\uff0c\u4f7f\u7528ARIMA\u3001MOMENT\u548cChronos\u6a21\u578b\u9884\u6d4b\u5408\u6210\u8bd5\u9a8c\uff0c\u91cd\u65b0\u8ba1\u7b97\u56db\u4e2a\u8fd0\u52a8\u5b66\u7279\u5f81\u5e76\u4e0e\u5b8c\u6574\u8bd5\u9a8c\u53c2\u8003\u6bd4\u8f83\u3002", "result": "Chronos\u9884\u6d4b\u4f7f\u6240\u6709\u53c2\u6570ICC\u22650.90\uff0c\u4ec5\u97008\u6b21\u5b9e\u9645\u8bd5\u9a8c\u52a0\u9884\u6d4b\u8bd5\u9a8c\u5373\u53ef\u8fbe\u523024-28\u6b21\u5b9e\u9645\u8bd5\u9a8c\u7684\u53ef\u9760\u6027\u3002MOMENT\u6709\u4e2d\u7b49\u6539\u5584\uff0cARIMA\u6539\u5584\u6700\u5c0f\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u53ef\u5927\u5e45\u7f29\u77edKinarm VGR\u8bc4\u4f30\u65f6\u95f4\uff0c\u5bf9\u6700\u4e25\u91cd\u4e2d\u98ce\u60a3\u8005\uff0c\u8bc4\u4f30\u65f6\u95f4\u4ece4-5\u5206\u949f\u964d\u81f3\u7ea61\u5206\u949f\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u5b66\u7cbe\u5ea6\uff0c\u4e3a\u4e2d\u98ce\u540e\u8fd0\u52a8\u969c\u788d\u8bc4\u4f30\u63d0\u4f9b\u9ad8\u6548\u673a\u5668\u4eba\u8bc4\u4f30\u8303\u5f0f\u3002"}}
{"id": "2511.00259", "categories": ["cs.RO", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00259", "abs": "https://arxiv.org/abs/2511.00259", "authors": ["Andria J. Farrens", "Luis Garcia-Fernandez", "Raymond Diaz Rojas", "Jillian Obeso Estrada", "Dylan Reinsdorf", "Vicky Chan", "Disha Gupta", "Joel Perry", "Eric Wolbrecht", "An Do", "Steven C. Cramer", "David J. Reinkensmeyer"], "title": "Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial", "comment": "Main manuscript: 38 pages (double spaced, with references), 6\n  figures, 2 tables and collated supplemental materials (17 pages, double\n  spaced)", "summary": "Precision rehabilitation aims to tailor movement training to improve\noutcomes. We tested whether proprioceptively-tailored robotic training improves\nhand function and neural processing in stroke survivors. Using a robotic finger\nexoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel\nTraining, which uses robot-facilitated, gamified movements to enhance\nproprioceptive processing, and Virtual Assistance Training, which reduces\nrobotic aid to increase reliance on self-generated feedback. In a randomized\ncontrolled trial, forty-six chronic stroke survivors completed nine 2-hour\nsessions of Standard, Propriopixel or Virtual training. Among participants with\nproprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)\nand Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand\nfunction (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with\nimprovements in hand function. Tailored training enhanced neural sensitivity to\nproprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive\nContingent Negative Variation. These findings support proprioceptively-tailored\ntraining as a pathway to precision neurorehabilitation.", "AI": {"tldr": "\u9488\u5bf9\u6709\u672c\u4f53\u611f\u89c9\u7f3a\u9677\u7684\u4e2d\u98ce\u60a3\u8005\uff0c\u4e24\u79cd\u672c\u4f53\u611f\u89c9\u5b9a\u5236\u8bad\u7ec3\u65b9\u6cd5\uff08Propriopixel\u8bad\u7ec3\u548c\u865a\u62df\u8f85\u52a9\u8bad\u7ec3\uff09\u6bd4\u6807\u51c6\u8bad\u7ec3\u66f4\u80fd\u6539\u5584\u624b\u90e8\u529f\u80fd\uff0c\u4e14\u672c\u4f53\u611f\u89c9\u6539\u5584\u4e0e\u624b\u529f\u80fd\u63d0\u5347\u76f8\u5173\uff0c\u795e\u7ecf\u654f\u611f\u6027\u4e5f\u5f97\u5230\u589e\u5f3a\u3002", "motivation": "\u6d4b\u8bd5\u672c\u4f53\u611f\u89c9\u5b9a\u5236\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u662f\u5426\u80fd\u6539\u5584\u4e2d\u98ce\u60a3\u8005\u7684\u624b\u90e8\u529f\u80fd\u548c\u795e\u7ecf\u5904\u7406\uff0c\u63a2\u7d22\u7cbe\u51c6\u5eb7\u590d\u7684\u9014\u5f84\u3002", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u624b\u6307\u5916\u9aa8\u9abc\uff0c\u6bd4\u8f83\u4e09\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff1a\u6807\u51c6\u8bad\u7ec3\u3001Propriopixel\u8bad\u7ec3\uff08\u901a\u8fc7\u6e38\u620f\u5316\u8fd0\u52a8\u589e\u5f3a\u672c\u4f53\u611f\u89c9\u5904\u7406\uff09\u548c\u865a\u62df\u8f85\u52a9\u8bad\u7ec3\uff08\u51cf\u5c11\u673a\u5668\u4eba\u8f85\u52a9\u4ee5\u589e\u52a0\u5bf9\u81ea\u6211\u751f\u6210\u53cd\u9988\u7684\u4f9d\u8d56\uff09\u300246\u540d\u6162\u6027\u4e2d\u98ce\u60a3\u8005\u968f\u673a\u5206\u7ec4\u5b8c\u62109\u6b212\u5c0f\u65f6\u8bad\u7ec3\u3002", "result": "\u5728\u6709\u672c\u4f53\u611f\u89c9\u7f3a\u9677\u7684\u53c2\u4e0e\u8005\u4e2d\uff0cPropriopixel\u8bad\u7ec3\uff08Box and Block Test: 7\u00b14.2, p=0.002\uff09\u548c\u865a\u62df\u8f85\u52a9\u8bad\u7ec3\uff084.5\u00b14.4, p=0.068\uff09\u6bd4\u6807\u51c6\u8bad\u7ec3\uff080.8\u00b12.3\u79ef\u6728\uff09\u5e26\u6765\u66f4\u5927\u7684\u624b\u529f\u80fd\u6539\u5584\u3002\u672c\u4f53\u611f\u89c9\u6539\u5584\u4e0e\u624b\u529f\u80fd\u63d0\u5347\u76f8\u5173\uff0c\u5b9a\u5236\u8bad\u7ec3\u589e\u5f3a\u4e86\u795e\u7ecf\u5bf9\u672c\u4f53\u611f\u89c9\u7ebf\u7d22\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u672c\u4f53\u611f\u89c9\u5b9a\u5236\u8bad\u7ec3\u662f\u7cbe\u51c6\u795e\u7ecf\u5eb7\u590d\u7684\u6709\u6548\u9014\u5f84\uff0c\u80fd\u6539\u5584\u624b\u90e8\u529f\u80fd\u5e76\u589e\u5f3a\u795e\u7ecf\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2511.00306", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00306", "abs": "https://arxiv.org/abs/2511.00306", "authors": ["Baoshan Song", "Ruijie Xu", "Li-Ta Hsu"], "title": "FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications", "comment": null, "summary": "Sliding window-factor graph optimization (SW-FGO) has gained more and more\nattention in navigation research due to its robust approximation to\nnon-Gaussian noises and nonlinearity of measuring models. There are lots of\nworks focusing on its application performance compared to extended Kalman\nfilter (EKF) but there is still a myth at the theoretical relationship between\nthe SW-FGO and EKF. In this paper, we find the necessarily fair condition to\nconnect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF\n(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the\nconditions, we propose a recursive FGO (Re-FGO) framework to represent KFV\nunder SW-FGO formulation. Under explicit conditions (Markov assumption,\nGaussian noise with L2 loss, and a one-state window), Re-FGO regenerates\nexactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in\nnonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after\nclarifying the connection between them, we highlight the unique advantages of\nSW-FGO in practical phases, especially on numerical estimation and deep\nlearning integration. The code and data used in this work is open sourced at\nhttps://github.com/Baoshan-Song/KFV-FGO-Comparison.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u6ed1\u52a8\u7a97\u53e3\u56e0\u5b50\u56fe\u4f18\u5316(SW-FGO)\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53d8\u4f53(KFV)\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u9012\u5f52FGO(Re-FGO)\u6846\u67b6\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0bRe-FGO\u53ef\u7cbe\u786e\u91cd\u73b0EKF/IEKF/REKF/RIEKF\uff0c\u540c\u65f6\u9610\u660e\u4e86SW-FGO\u5728\u975e\u7ebf\u6027\u3001\u975e\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u867d\u7136SW-FGO\u5728\u5bfc\u822a\u7814\u7a76\u4e2d\u56e0\u5bf9\u975e\u9ad8\u65af\u566a\u58f0\u548c\u975e\u7ebf\u6027\u6d4b\u91cf\u6a21\u578b\u7684\u9c81\u68d2\u6027\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u4e0e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(EKF)\u7b49\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53d8\u4f53\u4e4b\u95f4\u7684\u7406\u8bba\u5173\u7cfb\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5efa\u7acb\u4e24\u8005\u4e4b\u95f4\u7684\u7406\u8bba\u8fde\u63a5\u3002", "method": "\u63d0\u51fa\u4e86\u9012\u5f52FGO(Re-FGO)\u6846\u67b6\uff0c\u5728\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\u3001\u9ad8\u65af\u566a\u58f0\u548cL2\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u5355\u72b6\u6001\u7a97\u53e3\u7684\u660e\u786e\u6761\u4ef6\u4e0b\uff0c\u5c06KFV\u8868\u793a\u4e3aSW-FGO\u5f62\u5f0f\u3002", "result": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cRe-FGO\u80fd\u591f\u7cbe\u786e\u91cd\u73b0EKF/IEKF/REKF/RIEKF\uff0c\u540c\u65f6SW-FGO\u5728\u975e\u7ebf\u6027\u3001\u975e\u9ad8\u65af\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u53ef\u9884\u6d4b\u8ba1\u7b97\u6210\u672c\u4e0b\u7684\u53ef\u6d4b\u91cf\u4f18\u52bf\u3002", "conclusion": "\u6f84\u6e05\u4e86SW-FGO\u4e0eKFV\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7a81\u51fa\u4e86SW-FGO\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u6570\u503c\u4f30\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\u96c6\u6210\u65b9\u9762\u3002"}}
{"id": "2511.00392", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00392", "abs": "https://arxiv.org/abs/2511.00392", "authors": ["Lingpeng Chen", "Jiakun Tang", "Apple Pui-Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping", "comment": "8 pages, 9 figures, conference", "summary": "Accurate 3D reconstruction in visually-degraded underwater environments\nremains a formidable challenge. Single-modality approaches are insufficient:\nvision-based methods fail due to poor visibility and geometric constraints,\nwhile sonar is crippled by inherent elevation ambiguity and low resolution.\nConsequently, prior fusion technique relies on heuristics and flawed geometric\nassumptions, leading to significant artifacts and an inability to model complex\nscenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep\nlearning framework that overcomes these limitations by adapting the principled\nplane sweep algorithm for cross-modal fusion between sonar and visual data.\nExtensive experiments in both high-fidelity simulation and real-world\nenvironments demonstrate that SonarSweep consistently generates dense and\naccurate depth maps, significantly outperforming state-of-the-art methods\nacross challenging conditions, particularly in high turbidity. To foster\nfurther research, we will publicly release our code and a novel dataset\nfeaturing synchronized stereo-camera and sonar data, the first of its kind.", "AI": {"tldr": "SonarSweep\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5e73\u9762\u626b\u63cf\u7b97\u6cd5\u9002\u914d\u4e8e\u58f0\u7eb3\u548c\u89c6\u89c9\u6570\u636e\u7684\u8de8\u6a21\u6001\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u76843D\u91cd\u5efa\u96be\u9898\u3002", "motivation": "\u5728\u89c6\u89c9\u9000\u5316\u7684\u6c34\u4e0b\u73af\u5883\u4e2d\u8fdb\u884c\u51c6\u786e\u76843D\u91cd\u5efa\u5177\u6709\u6311\u6218\u6027\u3002\u5355\u6a21\u6001\u65b9\u6cd5\u4e0d\u8db3\uff1a\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\u56e0\u80fd\u89c1\u5ea6\u5dee\u548c\u51e0\u4f55\u7ea6\u675f\u800c\u5931\u8d25\uff0c\u800c\u58f0\u7eb3\u5219\u56e0\u56fa\u6709\u7684\u9ad8\u7a0b\u6a21\u7cca\u6027\u548c\u4f4e\u5206\u8fa8\u7387\u800c\u53d7\u9650\u3002\u73b0\u6709\u7684\u878d\u5408\u6280\u672f\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u6709\u7f3a\u9677\u7684\u51e0\u4f55\u5047\u8bbe\uff0c\u5bfc\u81f4\u663e\u8457\u4f2a\u5f71\u4e14\u65e0\u6cd5\u5efa\u6a21\u590d\u6742\u573a\u666f\u3002", "method": "SonarSweep\u901a\u8fc7\u5c06\u539f\u7406\u6027\u7684\u5e73\u9762\u626b\u63cf\u7b97\u6cd5\u9002\u914d\u4e8e\u58f0\u7eb3\u548c\u89c6\u89c9\u6570\u636e\u7684\u8de8\u6a21\u6001\u878d\u5408\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "SonarSweep\u80fd\u591f\u6301\u7eed\u751f\u6210\u5bc6\u96c6\u4e14\u51c6\u786e\u7684\u6df1\u5ea6\u56fe\uff0c\u5728\u5404\u79cd\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff08\u7279\u522b\u662f\u5728\u9ad8\u6d4a\u5ea6\u73af\u5883\u4e2d\uff09\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SonarSweep\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6c34\u4e0b3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4f5c\u8005\u5c06\u516c\u5f00\u4ee3\u7801\u548c\u9996\u4e2a\u5305\u542b\u540c\u6b65\u7acb\u4f53\u76f8\u673a\u548c\u58f0\u7eb3\u6570\u636e\u7684\u65b0\u578b\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.00412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00412", "abs": "https://arxiv.org/abs/2511.00412", "authors": ["John A. Christian", "Michael R. Walker II", "Wyatt Bridgman", "Michael J. Sparapany"], "title": "Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory", "comment": null, "summary": "The integration of gyroscope measurements is an essential task for most\nnavigation systems. Modern vehicles typically use strapdown systems, such that\ngyro integration requires coning compensation to account for the sensor's\nrotation during the integration. Many coning compensation algorithms have been\ndeveloped and a few are reviewed. This work introduces a new class of coning\ncorrection algorithm built directly from the classical Runge-Kutta integration\nroutines. A simple case is shown to collapse to one of the most popular coning\nalgorithms and a clear procedure for generating higher-order algorithms is\npresented.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ecf\u5178Runge-Kutta\u79ef\u5206\u65b9\u6cd5\u7684\u65b0\u578b\u5706\u9525\u8865\u507f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u7b80\u5316\u5230\u6700\u6d41\u884c\u7684\u5706\u9525\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u751f\u6210\u9ad8\u9636\u7b97\u6cd5\u7684\u6e05\u6670\u6d41\u7a0b\u3002", "motivation": "\u73b0\u4ee3\u8f66\u8f86\u901a\u5e38\u4f7f\u7528\u6377\u8054\u7cfb\u7edf\uff0c\u9640\u87ba\u4eea\u79ef\u5206\u9700\u8981\u8003\u8651\u4f20\u611f\u5668\u5728\u79ef\u5206\u671f\u95f4\u7684\u65cb\u8f6c\uff0c\u56e0\u6b64\u9700\u8981\u5706\u9525\u8865\u507f\u3002\u73b0\u6709\u591a\u79cd\u5706\u9525\u8865\u507f\u7b97\u6cd5\uff0c\u4f46\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u751f\u6210\u9ad8\u9636\u7b97\u6cd5\u3002", "method": "\u4ece\u7ecf\u5178\u7684Runge-Kutta\u79ef\u5206\u65b9\u6cd5\u76f4\u63a5\u6784\u5efa\u65b0\u578b\u5706\u9525\u6821\u6b63\u7b97\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u60c5\u51b5\u53ef\u9000\u5316\u4e3a\u6700\u6d41\u884c\u7684\u5706\u9525\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u751f\u6210\u9ad8\u9636\u7b97\u6cd5\u7684\u660e\u786e\u6b65\u9aa4\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u7c7b\u65b0\u7684\u5706\u9525\u8865\u507f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8eRunge-Kutta\u65b9\u6cd5\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u751f\u6210\u4e0d\u540c\u9636\u6570\u7684\u8865\u507f\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eRunge-Kutta\u7684\u65b0\u578b\u5706\u9525\u8865\u507f\u7b97\u6cd5\u4e3a\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u9ad8\u9636\u8865\u507f\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u7b97\u6cd5\u5f00\u53d1\u8fc7\u7a0b\u3002"}}
{"id": "2511.00492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00492", "abs": "https://arxiv.org/abs/2511.00492", "authors": ["Simon Giel", "James Hurrell", "Shreya Santra", "Ashutosh Mishra", "Kentaro Uno", "Kazuya Yoshida"], "title": "Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU", "comment": "6 pages, 4 figures. Accepted at IEEE iSpaRo 2025", "summary": "In-Situ Resource Utilization (ISRU) is one of the key technologies for\nenabling sustainable access to the Moon. The ability to excavate lunar regolith\nis the first step in making lunar resources accessible and usable. This work\npresents the development of a bucket drum for the modular robotic system\nMoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made\nof PLA was manufactured to evaluate its efficiency through a series of sandbox\ntests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is\ncapable of continuous excavation at a rate of 777.54 kg/h with a normalized\nenergy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is\n172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of\nexcavated material. The obtained results demonstrate the successful\nimplementation of the concept. A key advantage of the developed tool is its\ncompatibility with the modular MoonBot robotic platform, which enables flexible\nand efficient mission planning. Further improvements may include the\nintegration of sensors and an autonomous control system to enhance the\nexcavation process.", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8e\u6708\u7403\u673a\u5668\u4eba\u7cfb\u7edfMoonBot\u7684\u94f2\u6597\u6eda\u7b52\uff0c\u901a\u8fc73D\u6253\u5370\u539f\u578b\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u8fde\u7eed\u6316\u6398\u7387777.54 kg/h\u548c\u6279\u91cf\u6316\u6398\u7387172.02 kg/h\uff0c\u80fd\u8017\u5206\u522b\u4e3a0.022 Wh/kg\u548c0.86 Wh/kg\u3002", "motivation": "\u6708\u7403\u539f\u4f4d\u8d44\u6e90\u5229\u7528\u662f\u5b9e\u73b0\u53ef\u6301\u7eed\u6708\u7403\u63a2\u7d22\u7684\u5173\u952e\u6280\u672f\uff0c\u6316\u6398\u6708\u58e4\u662f\u83b7\u53d6\u6708\u7403\u8d44\u6e90\u7684\u7b2c\u4e00\u6b65\u3002", "method": "\u4e3a\u65e5\u672cMoonshot\u8ba1\u5212\u7684\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edfMoonBot\u5f00\u53d1\u94f2\u6597\u6eda\u7b52\uff0c\u4f7f\u7528PLA\u6750\u65993D\u6253\u5370\u539f\u578b\uff0c\u901a\u8fc7\u6c99\u76d2\u6d4b\u8bd5\u8bc4\u4f30\u6548\u7387\u3002", "result": "\u5de5\u5177\u91cd4.8kg\uff0c\u4f53\u79ef14.06L\uff0c\u8fde\u7eed\u6316\u6398\u7387777.54 kg/h\uff0c\u80fd\u80170.022 Wh/kg\uff1b\u6279\u91cf\u6316\u6398\u7387172.02 kg/h\uff0c\u80fd\u80170.86 Wh/kg\u3002", "conclusion": "\u6982\u5ff5\u6210\u529f\u5b9e\u73b0\uff0c\u4e3b\u8981\u4f18\u52bf\u662f\u4e0e\u6a21\u5757\u5316MoonBot\u5e73\u53f0\u7684\u517c\u5bb9\u6027\uff0c\u672a\u6765\u53ef\u96c6\u6210\u4f20\u611f\u5668\u548c\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u6539\u8fdb\u6316\u6398\u8fc7\u7a0b\u3002"}}
{"id": "2511.00512", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00512", "abs": "https://arxiv.org/abs/2511.00512", "authors": ["Suraj Kumar", "Andy Ruina"], "title": "Descriptive Model-based Learning and Control for Bipedal Locomotion", "comment": "8 pages, 15 figures", "summary": "Bipedal balance is challenging due to its multi-phase, hybrid nature and\nhigh-dimensional state space. Traditional balance control approaches for\nbipedal robots rely on low-dimensional models for locomotion planning and\nreactive control, constraining the full robot to behave like these simplified\nmodels. This involves tracking preset reference paths for the Center of Mass\nand upper body obtained through low-dimensional models, often resulting in\ninefficient walking patterns with bent knees. However, we observe that bipedal\nbalance is inherently low-dimensional and can be effectively described with\nsimple state and action descriptors in a low-dimensional state space. This\nallows the robot's motion to evolve freely in its high-dimensional state space,\nonly constraining its projection in the low-dimensional state space. In this\nwork, we propose a novel control approach that avoids prescribing a\nlow-dimensional model to the full model. Instead, our control framework uses a\ndescriptive model with the minimum degrees of freedom necessary to maintain\nbalance, allowing the remaining degrees of freedom to evolve freely in the\nhigh-dimensional space. This results in an efficient human-like walking gait\nand improved robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u8db3\u5e73\u8861\u63a7\u5236\u65b9\u6cd5\uff0c\u907f\u514d\u5c06\u4f4e\u7ef4\u6a21\u578b\u5f3a\u52a0\u4e8e\u5b8c\u6574\u6a21\u578b\uff0c\u800c\u662f\u4f7f\u7528\u63cf\u8ff0\u6027\u6a21\u578b\u6765\u7ef4\u6301\u5e73\u8861\uff0c\u8ba9\u5269\u4f59\u81ea\u7531\u5ea6\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u81ea\u7531\u6f14\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u4eba\u5f62\u6b65\u6001\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u8db3\u673a\u5668\u4eba\u5e73\u8861\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u7ef4\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\u548c\u53cd\u5e94\u63a7\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u5b8c\u6574\u673a\u5668\u4eba\u7684\u884c\u4e3a\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u7684\u884c\u8d70\u6a21\u5f0f\uff08\u5982\u5f2f\u66f2\u819d\u76d6\uff09\u3002\u4f46\u53cc\u8db3\u5e73\u8861\u672c\u8d28\u4e0a\u662f\u4f4e\u7ef4\u7684\uff0c\u53ef\u4ee5\u7528\u7b80\u5355\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u63cf\u8ff0\u7b26\u5728\u4f4e\u7ef4\u72b6\u6001\u7a7a\u95f4\u4e2d\u6709\u6548\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u63a7\u5236\u6846\u67b6\u4f7f\u7528\u5177\u6709\u7ef4\u6301\u5e73\u8861\u6240\u9700\u6700\u5c0f\u81ea\u7531\u5ea6\u7684\u63cf\u8ff0\u6027\u6a21\u578b\uff0c\u5141\u8bb8\u5269\u4f59\u81ea\u7531\u5ea6\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u81ea\u7531\u6f14\u5316\uff0c\u4e0d\u9884\u8bbe\u4f4e\u7ef4\u6a21\u578b\u5230\u5b8c\u6574\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u9ad8\u6548\u7684\u4eba\u5f62\u884c\u8d70\u6b65\u6001\uff0c\u5e76\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u4ec5\u7ea6\u675f\u4f4e\u7ef4\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u6295\u5f71\uff0c\u8ba9\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u5728\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u4e2d\u81ea\u7531\u6f14\u5316\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u81ea\u7136\u9ad8\u6548\u7684\u53cc\u8db3\u5e73\u8861\u63a7\u5236\u3002"}}
{"id": "2511.00516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00516", "abs": "https://arxiv.org/abs/2511.00516", "authors": ["Peiyi Wang", "Paul A. M. Lefeuvre", "Shangwei Zou", "Zhenwei Ni", "Daniela Rus", "Cecilia Laschi"], "title": "Adaptive and Multi-object Grasping via Deformable Origami Modules", "comment": null, "summary": "Soft robotics gripper have shown great promise in handling fragile and\ngeometrically complex objects. However, most existing solutions rely on bulky\nactuators, complex control strategies, or advanced tactile sensing to achieve\nstable and reliable grasping performance. In this work, we present a\nmulti-finger hybrid gripper featuring passively deformable origami modules that\ngenerate constant force and torque output. Each finger composed of parallel\norigami modules is driven by a 1-DoF actuator mechanism, enabling passive shape\nadaptability and stable grasping force without active sensing or feedback\ncontrol. More importantly, we demonstrate an interesting capability in\nsimultaneous multi-object grasping, which allows stacked objects of varied\nshape and size to be picked, transported and placed independently at different\nstates, significantly improving manipulation efficiency compared to\nsingle-object grasping. These results highlight the potential of origami-based\ncompliant structures as scalable modules for adaptive, stable and efficient\nmulti-object manipulation in domestic and industrial pick-and-place scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6298\u7eb8\u7ed3\u6784\u7684\u6df7\u5408\u5939\u722a\uff0c\u5177\u6709\u88ab\u52a8\u53d8\u5f62\u80fd\u529b\u548c\u6052\u5b9a\u529b\u8f93\u51fa\uff0c\u53ef\u5b9e\u73b0\u591a\u7269\u4f53\u540c\u65f6\u6293\u53d6\uff0c\u65e0\u9700\u4e3b\u52a8\u4f20\u611f\u6216\u53cd\u9988\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u8f6f\u4f53\u673a\u5668\u4eba\u5939\u722a\u901a\u5e38\u4f9d\u8d56\u7b28\u91cd\u7684\u6267\u884c\u5668\u3001\u590d\u6742\u63a7\u5236\u7b56\u7565\u6216\u5148\u8fdb\u89e6\u89c9\u4f20\u611f\u6765\u5b9e\u73b0\u7a33\u5b9a\u6293\u53d6\uff0c\u9700\u8981\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u6307\u6df7\u5408\u5939\u722a\u8bbe\u8ba1\uff0c\u6bcf\u4e2a\u624b\u6307\u7531\u5e76\u884c\u6298\u7eb8\u6a21\u5757\u7ec4\u6210\uff0c\u901a\u8fc7\u5355\u81ea\u7531\u5ea6\u6267\u884c\u5668\u9a71\u52a8\uff0c\u5b9e\u73b0\u88ab\u52a8\u5f62\u72b6\u9002\u5e94\u548c\u7a33\u5b9a\u6293\u53d6\u529b\u3002", "result": "\u5c55\u793a\u4e86\u540c\u65f6\u591a\u7269\u4f53\u6293\u53d6\u80fd\u529b\uff0c\u80fd\u591f\u6293\u53d6\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u5806\u53e0\u7269\u4f53\uff0c\u5e76\u5728\u4e0d\u540c\u72b6\u6001\u4e0b\u72ec\u7acb\u653e\u7f6e\uff0c\u663e\u8457\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u3002", "conclusion": "\u6298\u7eb8\u57fa\u67d4\u6027\u7ed3\u6784\u4f5c\u4e3a\u53ef\u6269\u5c55\u6a21\u5757\uff0c\u5728\u5bb6\u5ead\u548c\u5de5\u4e1a\u62fe\u53d6\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u7a33\u5b9a\u548c\u9ad8\u6548\u591a\u7269\u4f53\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.00555", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00555", "abs": "https://arxiv.org/abs/2511.00555", "authors": ["Dianye Huang", "Nassir Navab", "Zhongliang Jiang"], "title": "Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy", "comment": "Accepted by IEEE T-RO", "summary": "Integrating generative models with action chunking has shown significant\npromise in imitation learning for robotic manipulation. However, the existing\ndiffusion-based paradigm often struggles to capture strong temporal\ndependencies across multiple steps, particularly when incorporating\nproprioceptive input. This limitation can lead to task failures, where the\npolicy overfits to proprioceptive cues at the expense of capturing the visually\nderived features of the task. To overcome this challenge, we propose the Deep\nKoopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a\ndual-branch architecture to decouple the roles of different sensory modality\ncombinations. The visual branch encodes the visual observations to indicate\ntask progression, while the fused branch integrates both visual and\nproprioceptive inputs for precise manipulation. Within this architecture, when\nthe robot fails to accomplish intermediate goals, such as grasping a drawer\nhandle, the policy can dynamically switch to execute action chunks generated by\nthe visual branch, allowing recovery to previously observed states and\nfacilitating retrial of the task. To further enhance visual representation\nlearning, we incorporate a Deep Koopman Operator module that captures\nstructured temporal dynamics from visual inputs. During inference, we use the\ntest-time loss of the generative model as a confidence signal to guide the\naggregation of the temporally overlapping predicted action chunks, thereby\nenhancing the reliability of policy execution. In simulation experiments across\nsix RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion\npolicy by an average of 14.6\\%. On three real-world robotic manipulation tasks,\nit achieves a 15.0\\% improvement. Code: https://github.com/dianyeHuang/D3P.", "AI": {"tldr": "\u63d0\u51faD3P\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u89e3\u8026\u4e0d\u540c\u611f\u5b98\u6a21\u6001\uff0c\u7ed3\u5408\u6df1\u5ea6Koopman\u7b97\u5b50\u589e\u5f3a\u89c6\u89c9\u8868\u793a\u5b66\u4e60\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u96be\u4ee5\u6355\u6349\u8de8\u591a\u6b65\u7684\u5f3a\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u7279\u522b\u662f\u5f53\u7ed3\u5408\u672c\u4f53\u611f\u77e5\u8f93\u5165\u65f6\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff1a\u89c6\u89c9\u5206\u652f\u7f16\u7801\u89c6\u89c9\u89c2\u5bdf\u6307\u793a\u4efb\u52a1\u8fdb\u5c55\uff0c\u878d\u5408\u5206\u652f\u6574\u5408\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u8f93\u5165\u8fdb\u884c\u7cbe\u786e\u64cd\u4f5c\uff1b\u5f15\u5165\u6df1\u5ea6Koopman\u7b97\u5b50\u6a21\u5757\u6355\u6349\u89c6\u89c9\u8f93\u5165\u7684\u7ed3\u6784\u5316\u65f6\u95f4\u52a8\u6001\uff1b\u4f7f\u7528\u751f\u6210\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u635f\u5931\u4f5c\u4e3a\u7f6e\u4fe1\u4fe1\u53f7\u6307\u5bfc\u52a8\u4f5c\u5757\u805a\u5408\u3002", "result": "\u57286\u4e2aRLBench\u684c\u9762\u4efb\u52a1\u4eff\u771f\u5b9e\u9a8c\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u534714.6%\uff1b\u57283\u4e2a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b015.0%\u7684\u6539\u8fdb\u3002", "conclusion": "D3P\u7b97\u6cd5\u901a\u8fc7\u89e3\u8026\u611f\u5b98\u6a21\u6001\u548c\u589e\u5f3a\u89c6\u89c9\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u6267\u884c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.00635", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00635", "abs": "https://arxiv.org/abs/2511.00635", "authors": ["Hyungtae Lim", "Daebeom Kim", "Hyun Myung"], "title": "Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles", "comment": "13 pages, 12 figures", "summary": "As various 3D light detection and ranging (LiDAR) sensors have been\nintroduced to the market, research on multi-session simultaneous localization\nand mapping (MSS) using heterogeneous LiDAR sensors has been actively\nconducted. Existing MSS methods mostly rely on loop closure detection for\ninter-session alignment; however, the performance of loop closure detection can\nbe potentially degraded owing to the differences in the density and field of\nview (FoV) of the sensors used in different sessions. In this study, we\nchallenge the existing paradigm that relies heavily on loop detection modules\nand propose a novel MSS framework, called Multi-Mapcher, that employs\nlarge-scale map-to-map registration to perform inter-session initial alignment,\nwhich is commonly assumed to be infeasible, by leveraging outlier-robust 3D\npoint cloud registration. Next, after finding inter-session loops by radius\nsearch based on the assumption that the inter-session initial alignment is\nsufficiently precise, anchor node-based robust pose graph optimization is\nemployed to build a consistent global map. As demonstrated in our experiments,\nour approach shows substantially better MSS performance for various LiDAR\nsensors used to capture the sessions and is faster than state-of-the-art\napproaches. Our code is available at\nhttps://github.com/url-kaist/multi-mapcher.", "AI": {"tldr": "\u63d0\u51faMulti-Mapcher\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5730\u56fe\u5230\u5730\u56fe\u914d\u51c6\u5b9e\u73b0\u591a\u4f1a\u8bddSLAM\u7684\u521d\u59cb\u5bf9\u9f50\uff0c\u66ff\u4ee3\u4f20\u7edf\u4f9d\u8d56\u95ed\u73af\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5f02\u8d28LiDAR\u4f20\u611f\u5668\u7684\u5efa\u56fe\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MSS\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u95ed\u73af\u68c0\u6d4b\uff0c\u4f46\u5728\u5f02\u8d28LiDAR\u4f20\u611f\u5668\uff08\u4e0d\u540c\u5bc6\u5ea6\u548c\u89c6\u573a\u89d2\uff09\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u9700\u8981\u65b0\u7684\u521d\u59cb\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u5730\u56fe\u5230\u5730\u56fe\u914d\u51c6\u8fdb\u884c\u4f1a\u8bdd\u95f4\u521d\u59cb\u5bf9\u9f50\uff0c\u7136\u540e\u57fa\u4e8e\u534a\u5f84\u641c\u7d22\u627e\u5230\u95ed\u73af\uff0c\u6700\u540e\u91c7\u7528\u951a\u8282\u70b9\u4f18\u5316\u7684\u4f4d\u59ff\u56fe\u4f18\u5316\u6784\u5efa\u5168\u5c40\u4e00\u81f4\u5730\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cdLiDAR\u4f20\u611f\u5668\u4e0a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "Multi-Mapcher\u6846\u67b6\u901a\u8fc7\u5730\u56fe\u7ea7\u914d\u51c6\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u8d28LiDAR\u591a\u4f1a\u8bddSLAM\u7684\u521d\u59cb\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00783", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00783", "abs": "https://arxiv.org/abs/2511.00783", "authors": ["Jingzehua Xu", "Weihang Zhang", "Yangyang Li", "Hongmiaoyi Zhang", "Guanwen Xie", "Jiwei Tang", "Shuai Zhang", "Yi Li"], "title": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage", "comment": "This paper has been submitted to IEEE Transactions on Mobile\n  Computing", "summary": "Underwater multi-robot cooperative coverage remains challenging due to\npartial observability, limited communication, environmental uncertainty, and\nthe lack of access to global localization. To address these issues, this paper\npresents a semantics-guided fuzzy control framework that couples Large Language\nModels (LLMs) with interpretable control and lightweight coordination. Raw\nmultimodal observations are compressed by the LLM into compact,\nhuman-interpretable semantic tokens that summarize obstacles, unexplored\nregions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy\ninference system with pre-defined membership functions then maps these tokens\ninto smooth and stable steering and gait commands, enabling reliable navigation\nwithout relying on global positioning. Then, we further coordinate multiple\nrobots by introducing semantic communication that shares intent and local\ncontext in linguistic form, enabling agreement on who explores where while\navoiding redundant revisits. Extensive simulations in unknown reef-like\nenvironments show that, under limited sensing and communication, the proposed\nframework achieves robust OOI-oriented navigation and cooperative coverage with\nimproved efficiency and adaptability, narrowing the gap between semantic\ncognition and distributed underwater control in GPS-denied, map-free\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5f15\u5bfc\u7684\u6a21\u7cca\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u53ef\u89e3\u91ca\u63a7\u5236\u548c\u8f7b\u91cf\u7ea7\u534f\u8c03\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u534f\u540c\u8986\u76d6\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u534f\u540c\u8986\u76d6\u9762\u4e34\u7684\u5c40\u90e8\u53ef\u89c2\u6d4b\u6027\u3001\u6709\u9650\u901a\u4fe1\u3001\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u7f3a\u4e4f\u5168\u5c40\u5b9a\u4f4d\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528LLM\u5c06\u539f\u59cb\u591a\u6a21\u6001\u89c2\u6d4b\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u4eba\u7c7b\u53ef\u89e3\u91ca\u8bed\u4e49\u6807\u8bb0\uff0c\u901a\u8fc7\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u6620\u5c04\u4e3a\u5e73\u6ed1\u7a33\u5b9a\u7684\u8f6c\u5411\u548c\u6b65\u6001\u547d\u4ee4\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u901a\u4fe1\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u534f\u8c03\u3002", "result": "\u5728\u672a\u77e5\u73ca\u745a\u7901\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u6a21\u62df\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u6709\u9650\u611f\u77e5\u548c\u901a\u4fe1\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5065\u7684OOI\u5bfc\u5411\u5bfc\u822a\u548c\u534f\u540c\u8986\u76d6\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u7f29\u5c0f\u4e86\u8bed\u4e49\u8ba4\u77e5\u4e0e\u5206\u5e03\u5f0f\u6c34\u4e0b\u63a7\u5236\u5728GPS\u62d2\u7edd\u3001\u65e0\u5730\u56fe\u6761\u4ef6\u4e0b\u7684\u5dee\u8ddd\uff0c\u4e3a\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00814", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "93C41, 93E11, 37M10", "I.2.9; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.00814", "abs": "https://arxiv.org/abs/2511.00814", "authors": ["Stella Kombo", "Masih Haseli", "Skylar Wei", "Joel W. Burdick"], "title": "Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning", "comment": "10 pages, 6 figures, submitted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2025", "summary": "Autonomous systems often must predict the motions of nearby agents from\npartial and noisy data. This paper asks and answers the question: \"can we\nlearn, in real-time, a nonlinear predictive model of another agent's motions?\"\nOur online framework denoises and forecasts such dynamics using a modified\nsliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy\nmeasurements are embedded into a Hankel matrix, while an associated Page matrix\nenables singular-value hard thresholding (SVHT) to estimate the effective rank.\nA Cadzow projection enforces structured low-rank consistency, yielding a\ndenoised trajectory and local noise variance estimates. From this\nrepresentation, a time-varying Hankel-DMD lifted linear predictor is\nconstructed for multi-step forecasts. The residual analysis provides\nvariance-tracking signals that can support downstream estimators and risk-aware\nplanning. We validate the approach in simulation under Gaussian and\nheavy-tailed noise, and experimentally on a dynamic crane testbed. Results show\nthat the method achieves stable variance-aware denoising and short-horizon\nprediction suitable for integration into real-time control frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHankel\u52a8\u6001\u6a21\u6001\u5206\u89e3\u7684\u5728\u7ebf\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u5b66\u4e60\u548c\u9884\u6d4b\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u578b\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5904\u7406\u90e8\u5206\u566a\u58f0\u6570\u636e\uff0c\u5b9e\u73b0\u53bb\u566a\u548c\u77ed\u671f\u9884\u6d4b\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u9700\u8981\u4ece\u90e8\u5206\u566a\u58f0\u6570\u636e\u4e2d\u9884\u6d4b\u9644\u8fd1\u667a\u80fd\u4f53\u7684\u8fd0\u52a8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5b9e\u65f6\u6761\u4ef6\u4e0b\u5b66\u4e60\u975e\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u6ed1\u52a8\u7a97\u53e3Hankel-DMD\uff0c\u901a\u8fc7Hankel\u77e9\u9635\u5d4c\u5165\u90e8\u5206\u566a\u58f0\u6d4b\u91cf\uff0c\u5229\u7528Page\u77e9\u9635\u8fdb\u884c\u5947\u5f02\u503c\u786c\u9608\u503c\u4f30\u8ba1\u6709\u6548\u79e9\uff0cCadzow\u6295\u5f71\u786e\u4fdd\u7ed3\u6784\u5316\u4f4e\u79e9\u4e00\u81f4\u6027\uff0c\u6784\u5efa\u65f6\u53d8Hankel-DMD\u63d0\u5347\u7ebf\u6027\u9884\u6d4b\u5668\u8fdb\u884c\u591a\u6b65\u9884\u6d4b\u3002", "result": "\u5728\u4eff\u771f\u548c\u52a8\u6001\u8d77\u91cd\u673a\u5b9e\u9a8c\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u5728Gaussian\u548c\u91cd\u5c3e\u566a\u58f0\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u7684\u65b9\u5dee\u611f\u77e5\u53bb\u566a\u548c\u77ed\u671f\u9884\u6d4b\uff0c\u9002\u5408\u96c6\u6210\u5230\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u5b66\u4e60\u975e\u7ebf\u6027\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u4e3a\u4e0b\u6e38\u4f30\u8ba1\u5668\u548c\u98ce\u9669\u611f\u77e5\u89c4\u5212\u63d0\u4f9b\u65b9\u5dee\u8ddf\u8e2a\u4fe1\u53f7\uff0c\u9002\u5408\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\u9700\u6c42\u3002"}}
{"id": "2511.00840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00840", "abs": "https://arxiv.org/abs/2511.00840", "authors": ["William Suliman", "Ekaterina Chaikovskaia", "Egor Davydenko", "Roman Gorbachev"], "title": "Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches", "comment": null, "summary": "This work presents an extended framework for learning-based bipedal\nlocomotion that incorporates a heuristic step-planning strategy guided by\ndesired torso velocity tracking. The framework enables precise interaction\nbetween a humanoid robot and its environment, supporting tasks such as crossing\ngaps and accurately approaching target objects. Unlike approaches based on full\nor simplified dynamics, the proposed method avoids complex step planners and\nanalytical models. Step planning is primarily driven by heuristic commands,\nwhile a Raibert-type controller modulates the foot placement length based on\nthe error between desired and actual torso velocity. We compare our method with\na model-based step-planning approach -- the Linear Inverted Pendulum Model\n(LIPM) controller. Experimental results demonstrate that our approach attains\ncomparable or superior accuracy in maintaining target velocity (up to 80%),\nsignificantly greater robustness on uneven terrain (over 50% improvement), and\nimproved energy efficiency. These results suggest that incorporating complex\nanalytical, model-based components into the training architecture may be\nunnecessary for achieving stable and robust bipedal walking, even in\nunstructured environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u542f\u53d1\u5f0f\u6b65\u6001\u89c4\u5212\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u4e24\u8db3\u673a\u5668\u4eba\u884c\u8d70\u6846\u67b6\uff0c\u901a\u8fc7\u671f\u671b\u8eaf\u5e72\u901f\u5ea6\u8ddf\u8e2a\u5b9e\u73b0\u7cbe\u786e\u7684\u73af\u5883\u4ea4\u4e92\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5b8c\u6574\u6216\u7b80\u5316\u52a8\u529b\u5b66\u7684\u6b65\u6001\u89c4\u5212\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u6b65\u6001\u89c4\u5212\u5668\u548c\u5206\u6790\u6a21\u578b\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u91c7\u7528\u542f\u53d1\u5f0f\u6b65\u6001\u89c4\u5212\u7b56\u7565\uff0c\u7ed3\u5408Raibert\u578b\u63a7\u5236\u5668\u6839\u636e\u671f\u671b\u4e0e\u5b9e\u9645\u8eaf\u5e72\u901f\u5ea6\u8bef\u5dee\u8c03\u8282\u8db3\u90e8\u4f4d\u7f6e\uff0c\u907f\u514d\u4f7f\u7528\u590d\u6742\u7684\u5206\u6790\u6a21\u578b\u3002", "result": "\u4e0e\u57fa\u4e8e\u7ebf\u6027\u5012\u7acb\u6446\u6a21\u578b\u7684\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u76ee\u6807\u901f\u5ea6\u8ddf\u8e2a\u7cbe\u5ea6\u4e0a\u8fbe\u523080%\u7684\u51c6\u786e\u7387\uff0c\u5728\u5d0e\u5c96\u5730\u5f62\u4e0a\u7684\u9c81\u68d2\u6027\u63d0\u9ad8\u8d85\u8fc750%\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u80fd\u6548\u3002", "conclusion": "\u5728\u8bad\u7ec3\u67b6\u6784\u4e2d\u5f15\u5165\u590d\u6742\u7684\u5206\u6790\u6a21\u578b\u53ef\u80fd\u4e0d\u662f\u5b9e\u73b0\u7a33\u5b9a\u9c81\u68d2\u4e24\u8db3\u884c\u8d70\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5373\u4f7f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2511.00917", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00917", "abs": "https://arxiv.org/abs/2511.00917", "authors": ["Junyao Shi", "Rujia Yang", "Kaitian Chao", "Selina Bingqing Wan", "Yifei Shao", "Jiahui Lei", "Jianing Qian", "Long Le", "Pratik Chaudhari", "Kostas Daniilidis", "Chuan Wen", "Dinesh Jayaraman"], "title": "Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots", "comment": "Project website: https://maestro-robot.github.io", "summary": "Today's best-explored routes towards generalist robots center on collecting\never larger \"observations-in actions-out\" robotics datasets to train large\nend-to-end models, copying a recipe that has worked for vision-language models\n(VLMs). We pursue a road less traveled: building generalist policies directly\naround VLMs by augmenting their general capabilities with specific robot\ncapabilities encapsulated in a carefully curated set of perception, planning,\nand control modules. In Maestro, a VLM coding agent dynamically composes these\nmodules into a programmatic policy for the current task and scenario. Maestro's\narchitecture benefits from a streamlined closed-loop interface without many\nmanually imposed structural constraints, and a comprehensive and diverse tool\nrepertoire. As a result, it largely surpasses today's VLA models for zero-shot\nperformance on challenging manipulation skills. Further, Maestro is easily\nextensible to incorporate new modules, easily editable to suit new embodiments\nsuch as a quadruped-mounted arm, and even easily adapts from minimal real-world\nexperiences through local code edits.", "AI": {"tldr": "Maestro\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u7ec4\u5408\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u6a21\u5757\u6765\u6784\u5efa\u901a\u7528\u7b56\u7565\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709VLA\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u673a\u5668\u4eba\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u53e6\u4e00\u6761\u8def\u5f84\uff1a\u56f4\u7ed5VLM\u6784\u5efa\u901a\u7528\u7b56\u7565\uff0c\u5229\u7528\u5176\u901a\u7528\u80fd\u529b\u4e0e\u7279\u5b9a\u673a\u5668\u4eba\u6a21\u5757\u76f8\u7ed3\u5408\u3002", "method": "\u4f7f\u7528VLM\u7f16\u7801\u4ee3\u7406\u52a8\u6001\u7ec4\u5408\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u6a21\u5757\u4e3a\u7a0b\u5e8f\u5316\u7b56\u7565\uff0c\u5177\u6709\u7b80\u5316\u7684\u95ed\u73af\u63a5\u53e3\u548c\u591a\u6837\u5316\u5de5\u5177\u5e93\u3002", "result": "\u5728\u6311\u6218\u6027\u64cd\u4f5c\u6280\u80fd\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709VLA\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u6613\u4e8e\u6269\u5c55\u65b0\u6a21\u5757\uff0c\u9002\u5e94\u65b0\u5f62\u6001\uff0c\u5e76\u80fd\u901a\u8fc7\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u7ecf\u9a8c\u8fdb\u884c\u672c\u5730\u4ee3\u7801\u7f16\u8f91\u9002\u5e94\u3002", "conclusion": "Maestro\u5c55\u793a\u4e86\u57fa\u4e8eVLM\u6784\u5efa\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u53ef\u884c\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u7f16\u8f91\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.00933", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00933", "abs": "https://arxiv.org/abs/2511.00933", "authors": ["Xiangyu Shi", "Zerui Li", "Yanyuan Qiao", "Qi Wu"], "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation", "comment": null, "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve\nzero-shot navigation. However, existing methods often rely on panoramic\nobservations and two-stage pipelines involving waypoint predictors, which\nintroduce significant latency and limit real-world applicability. In this work,\nwe propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that\neliminates the need for panoramic views and waypoint predictors. Our approach\nuses only three frontal RGB-D images combined with natural language\ninstructions, enabling MLLMs to directly predict actions. To enhance decision\nrobustness, we introduce an Uncertainty-Aware Reasoning module that integrates\n(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past\nBidirectional Reasoning mechanism for globally coherent planning. Experiments\non both simulated and real-robot environments demonstrate that our method\nsignificantly reduces per-step latency while achieving competitive or superior\nperformance compared to panoramic-view baselines. These results demonstrate the\npracticality and effectiveness of Fast-SmartWay for real-world zero-shot\nembodied navigation.", "AI": {"tldr": "Fast-SmartWay\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u4e09\u4e2a\u524d\u89c6RGB-D\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u65e0\u9700\u5168\u666f\u89c6\u56fe\u548c\u8def\u5f84\u70b9\u9884\u6d4b\u5668\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u73b0\u5b9e\u4e16\u754c\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684VLN-CE\u65b9\u6cd5\u4f9d\u8d56\u5168\u666f\u89c2\u6d4b\u548c\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u5bfc\u81f4\u663e\u8457\u5ef6\u8fdf\u5e76\u9650\u5236\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u3002\u9700\u8981\u6d88\u9664\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u66f4\u5b9e\u7528\u7684\u96f6\u6837\u672c\u5bfc\u822a\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u524d\u89c6RGB-D\u56fe\u50cf\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u8ba9MLLM\u76f4\u63a5\u9884\u6d4b\u52a8\u4f5c\u3002\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a8\u7406\u6a21\u5757\uff0c\u5305\u62ec\u6d88\u6b67\u6a21\u5757\u548c\u672a\u6765-\u8fc7\u53bb\u53cc\u5411\u63a8\u7406\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u51b3\u7b56\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6bcf\u6b65\u5ef6\u8fdf\uff0c\u540c\u65f6\u8fbe\u5230\u6216\u4f18\u4e8e\u5168\u666f\u89c6\u56fe\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "conclusion": "Fast-SmartWay\u8bc1\u660e\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u96f6\u6837\u672c\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00940", "categories": ["cs.RO", "cs.AI", "I.2.6"], "pdf": "https://arxiv.org/pdf/2511.00940", "abs": "https://arxiv.org/abs/2511.00940", "authors": ["Zhe Li", "Xiang Bai", "Jieyu Zhang", "Zhuangzhe Wu", "Che Xu", "Ying Li", "Chengkai Hou", "Shanghang Zhang"], "title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model", "comment": "Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Constructing accurate digital twins of articulated objects is essential for\nrobotic simulation training and embodied AI world model building, yet\nhistorically requires painstaking manual modeling or multi-stage pipelines. In\nthis work, we propose \\textbf{URDF-Anything}, an end-to-end automatic\nreconstruction framework based on a 3D multimodal large language model (MLLM).\nURDF-Anything utilizes an autoregressive prediction framework based on\npoint-cloud and text multimodal input to jointly optimize geometric\nsegmentation and kinematic parameter prediction. It implements a specialized\n$[SEG]$ token mechanism that interacts directly with point cloud features,\nenabling fine-grained part-level segmentation while maintaining consistency\nwith the kinematic parameter predictions. Experiments on both simulated and\nreal-world datasets demonstrate that our method significantly outperforms\nexisting approaches regarding geometric segmentation (mIoU 17\\% improvement),\nkinematic parameter prediction (average error reduction of 29\\%), and physical\nexecutability (surpassing baselines by 50\\%). Notably, our method exhibits\nexcellent generalization ability, performing well even on objects outside the\ntraining set. This work provides an efficient solution for constructing digital\ntwins for robotic simulation, significantly enhancing the sim-to-real transfer\ncapability.", "AI": {"tldr": "URDF-Anything\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u91cd\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u5173\u8282\u7269\u4f53\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u51e0\u4f55\u5206\u5272\u548c\u8fd0\u52a8\u5b66\u53c2\u6570\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u3001\u53c2\u6570\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7269\u7406\u53ef\u6267\u884c\u6027\u3002", "motivation": "\u6784\u5efa\u51c6\u786e\u7684\u5173\u8282\u7269\u4f53\u6570\u5b57\u5b6a\u751f\u5bf9\u673a\u5668\u4eba\u4eff\u771f\u8bad\u7ec3\u548c\u5177\u8eabAI\u4e16\u754c\u6a21\u578b\u6784\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7e41\u7410\u7684\u624b\u52a8\u5efa\u6a21\u6216\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u70b9\u4e91\u548c\u6587\u672c\u591a\u6a21\u6001\u8f93\u5165\u7684\u81ea\u56de\u5f52\u9884\u6d4b\u6846\u67b6\uff0c\u5b9e\u73b0\u51e0\u4f55\u5206\u5272\u548c\u8fd0\u52a8\u5b66\u53c2\u6570\u9884\u6d4b\u7684\u8054\u5408\u4f18\u5316\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684[SEG]\u6807\u8bb0\u673a\u5236\u4e0e\u70b9\u4e91\u7279\u5f81\u76f4\u63a5\u4ea4\u4e92\uff0c\u786e\u4fdd\u7ec6\u7c92\u5ea6\u90e8\u4ef6\u7ea7\u5206\u5272\u4e0e\u8fd0\u52a8\u5b66\u53c2\u6570\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u5206\u5272\uff08mIoU\u63d0\u534717%\uff09\u3001\u8fd0\u52a8\u5b66\u53c2\u6570\u9884\u6d4b\uff08\u5e73\u5747\u8bef\u5dee\u51cf\u5c1129%\uff09\u548c\u7269\u7406\u53ef\u6267\u884c\u6027\uff08\u8d85\u8d8a\u57fa\u7ebf50%\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u8bad\u7ec3\u96c6\u5916\u7269\u4f53\u4e0a\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u4eba\u4eff\u771f\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2511.00983", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00983", "abs": "https://arxiv.org/abs/2511.00983", "authors": ["Yizhao Qian", "Yujie Zhu", "Jiayuan Luo", "Li Liu", "Yixuan Yuan", "Guochen Ning", "Hongen Liao"], "title": "Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing", "comment": null, "summary": "Real-time tracking of dynamic targets amidst large-scale, high-frequency\ndisturbances remains a critical unsolved challenge in Robotic Ultrasound\nSystems (RUSS), primarily due to the end-to-end latency of existing systems.\nThis paper argues that breaking this latency barrier requires a fundamental\nshift towards the synergistic co-design of perception and control. We realize\nit in a novel framework with two tightly-coupled contributions: (1) a Decoupled\nDual-Stream Perception Network that robustly estimates 3D translational state\nfrom 2D images at high frequency, and (2) a Single-Step Flow Policy that\ngenerates entire action sequences in one inference pass, bypassing the\niterative bottleneck of conventional policies. This synergy enables a\nclosed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system\nnot only tracks complex 3D trajectories with a mean error below 6.5mm but also\ndemonstrates robust re-acquisition from over 170mm displacement. Furthermore,\nit can track targets at speeds of 102mm/s, achieving a terminal error below\n1.7mm. Moreover, in-vivo experiments on a human volunteer validate the\nframework's effectiveness and robustness in a realistic clinical setting. Our\nwork presents a RUSS holistically architected to unify high-bandwidth tracking\nwith large-scale repositioning, a critical step towards robust autonomy in\ndynamic clinical environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\u7684\u534f\u540c\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u53cc\u6d41\u611f\u77e5\u7f51\u7edc\u548c\u5355\u6b65\u6d41\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc760Hz\u7684\u95ed\u73af\u63a7\u5236\u9891\u7387\uff0c\u5728\u52a8\u6001\u76ee\u6807\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\u4e2d\u5927\u89c4\u6a21\u9ad8\u9891\u5e72\u6270\u4e0b\u52a8\u6001\u76ee\u6807\u5b9e\u65f6\u8ddf\u8e2a\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u73b0\u6709\u7cfb\u7edf\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u611f\u77e5\u4e0e\u63a7\u5236\u7684\u534f\u540c\u8bbe\u8ba1\u3002", "method": "\u5305\u542b\u4e24\u4e2a\u7d27\u5bc6\u8026\u5408\u7684\u8d21\u732e\uff1a(1) \u89e3\u8026\u53cc\u6d41\u611f\u77e5\u7f51\u7edc\uff0c\u4ece2D\u56fe\u50cf\u9ad8\u9891\u4f30\u8ba13D\u5e73\u79fb\u72b6\u6001\uff1b(2) \u5355\u6b65\u6d41\u7b56\u7565\uff0c\u5728\u4e00\u6b21\u63a8\u7406\u4e2d\u751f\u6210\u5b8c\u6574\u52a8\u4f5c\u5e8f\u5217\uff0c\u7ed5\u8fc7\u4f20\u7edf\u7b56\u7565\u7684\u8fed\u4ee3\u74f6\u9888\u3002", "result": "\u5728\u52a8\u6001\u4f53\u6a21\u4e0a\uff0c\u7cfb\u7edf\u8ddf\u8e2a\u590d\u67423D\u8f68\u8ff9\u7684\u5e73\u5747\u8bef\u5dee\u4f4e\u4e8e6.5mm\uff0c\u80fd\u4ece\u8d85\u8fc7170mm\u4f4d\u79fb\u4e2d\u7a33\u5065\u91cd\u83b7\u76ee\u6807\uff0c\u80fd\u4ee5102mm/s\u901f\u5ea6\u8ddf\u8e2a\u76ee\u6807\uff0c\u7ec8\u7aef\u8bef\u5dee\u4f4e\u4e8e1.7mm\u3002\u4eba\u4f53\u5fd7\u613f\u8005\u4f53\u5185\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6574\u4f53\u67b6\u6784\u7684\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\uff0c\u7edf\u4e00\u4e86\u9ad8\u5e26\u5bbd\u8ddf\u8e2a\u4e0e\u5927\u89c4\u6a21\u91cd\u65b0\u5b9a\u4f4d\uff0c\u662f\u5b9e\u73b0\u52a8\u6001\u4e34\u5e8a\u73af\u5883\u4e2d\u7a33\u5065\u81ea\u4e3b\u6027\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2511.00998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00998", "abs": "https://arxiv.org/abs/2511.00998", "authors": ["Ziye Wang", "Li Kang", "Yiran Qin", "Jiahua Ma", "Zhanglin Peng", "Lei Bai", "Ruimao Zhang"], "title": "GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies", "comment": "Accepted by NeurIPS 2025. Project page:\n  https://ziyeeee.github.io/gaudp.io/", "summary": "Recently, effective coordination in embodied multi-agent systems has remained\na fundamental challenge, particularly in scenarios where agents must balance\nindividual perspectives with global environmental awareness. Existing\napproaches often struggle to balance fine-grained local control with\ncomprehensive scene understanding, resulting in limited scalability and\ncompromised collaboration quality. In this paper, we present GauDP, a novel\nGaussian-image synergistic representation that facilitates scalable,\nperception-aware imitation learning in multi-agent collaborative systems.\nSpecifically, GauDP constructs a globally consistent 3D Gaussian field from\ndecentralized RGB observations, then dynamically redistributes 3D Gaussian\nattributes to each agent's local perspective. This enables all agents to\nadaptively query task-critical features from the shared scene representation\nwhile maintaining their individual viewpoints. This design facilitates both\nfine-grained control and globally coherent behavior without requiring\nadditional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the\nRoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our\nmethod achieves superior performance over existing image-based methods and\napproaches the effectiveness of point-cloud-driven methods, while maintaining\nstrong scalability as the number of agents increases.", "AI": {"tldr": "\u63d0\u51faGauDP\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af-\u56fe\u50cf\u534f\u540c\u8868\u793a\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u611f\u77e5\u611f\u77e5\u6a21\u4eff\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u4e2a\u4f53\u89c6\u89d2\u7684\u540c\u65f6\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u7684\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u7ec6\u7c92\u5ea6\u5c40\u90e8\u63a7\u5236\u4e0e\u5168\u5c40\u573a\u666f\u7406\u89e3\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u6709\u9650\u548c\u534f\u4f5c\u8d28\u91cf\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4fdd\u6301\u4e2a\u4f53\u89c6\u89d2\u7684\u540c\u65f6\u5b9e\u73b0\u5168\u5c40\u534f\u8c03\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u5168\u5c40\u4e00\u81f4\u76843D\u9ad8\u65af\u573a\uff0c\u7136\u540e\u52a8\u6001\u5730\u5c063D\u9ad8\u65af\u5c5e\u6027\u91cd\u65b0\u5206\u914d\u5230\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u5c40\u90e8\u89c6\u89d2\uff0c\u4f7f\u6240\u6709\u667a\u80fd\u4f53\u80fd\u591f\u4ece\u5171\u4eab\u573a\u666f\u8868\u793a\u4e2d\u81ea\u9002\u5e94\u67e5\u8be2\u4efb\u52a1\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728RoboFactory\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u63a5\u8fd1\u57fa\u4e8e\u70b9\u4e91\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u5728\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u65f6\u4fdd\u6301\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "GauDP\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u4f20\u611f\u6a21\u6001\u5373\u53ef\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u5168\u5c40\u4e00\u81f4\u884c\u4e3a\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01031", "abs": "https://arxiv.org/abs/2511.01031", "authors": ["Mathieu Dubied", "Paolo Tiso", "Robert K. Katzschmann"], "title": "AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models", "comment": null, "summary": "The efficient optimization of actuated soft structures, particularly under\ncomplex nonlinear forces, remains a critical challenge in advancing robotics.\nSimulations of nonlinear structures, such as soft-bodied robots modeled using\nthe finite element method (FEM), often demand substantial computational\nresources, especially during optimization. To address this challenge, we\npropose a novel optimization algorithm based on a tensorial parametric reduced\norder model (PROM). Our algorithm leverages dimensionality reduction and\nsolution approximation techniques to facilitate efficient solving of nonlinear\nconstrained optimization problems. The well-structured tensorial approach\nenables the use of analytical gradients within a specifically chosen reduced\norder basis (ROB), significantly enhancing computational efficiency. To\nshowcase the performance of our method, we apply it to optimizing soft robotic\nswimmer shapes. These actuated soft robots experience hydrodynamic forces,\nsubjecting them to both internal and external nonlinear forces, which are\nincorporated into our optimization process using a data-free ROB for fast and\naccurate computations. This approach not only reduces computational complexity\nbut also unlocks new opportunities to optimize complex nonlinear systems in\nsoft robotics, paving the way for more efficient design and control.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u53c2\u6570\u964d\u9636\u6a21\u578b\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f18\u5316\u53d7\u590d\u6742\u975e\u7ebf\u6027\u529b\u4f5c\u7528\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u8f6f\u4f53\u7ed3\u6784\u5728\u590d\u6742\u975e\u7ebf\u6027\u529b\u4f5c\u7528\u4e0b\u7684\u9ad8\u6548\u4f18\u5316\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u6709\u9650\u5143\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f7f\u7528\u5f20\u91cf\u53c2\u6570\u964d\u9636\u6a21\u578b\uff0c\u7ed3\u5408\u7ef4\u5ea6\u7f29\u51cf\u548c\u8fd1\u4f3c\u6c42\u89e3\u6280\u672f\uff0c\u5728\u7279\u5b9a\u964d\u9636\u57fa\u4e0a\u4f7f\u7528\u89e3\u6790\u68af\u5ea6\u8fdb\u884c\u975e\u7ebf\u6027\u7ea6\u675f\u4f18\u5316\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u8f6f\u4f53\u6e38\u6cf3\u673a\u5668\u4eba\u5f62\u72b6\u4f18\u5316\uff0c\u80fd\u591f\u5904\u7406\u5185\u5916\u975e\u7ebf\u6027\u6d41\u4f53\u52a8\u529b\uff0c\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u8fd8\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4f18\u5316\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u63a8\u52a8\u66f4\u9ad8\u6548\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\u3002"}}
{"id": "2511.01083", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01083", "abs": "https://arxiv.org/abs/2511.01083", "authors": ["Zihan Wang", "Jianwen Li", "Li-Fan Wu", "Nina Mahmoudian"], "title": "Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment", "comment": "Submitted to ICRA 2026", "summary": "Rivers are critical corridors for environmental monitoring and disaster\nresponse, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven\npolicies can provide fast, low-cost coverage. However, deployment exposes\nsimulation-trained policies with distribution shift and safety risks and\nrequires efficient adaptation from limited human interventions. We study\nhuman-in-the-loop (HITL) learning with a conservative overseer who vetoes\nunsafe or inefficient actions and provides statewise preferences by comparing\nthe agent's proposal with a corrective override. We introduce Statewise Hybrid\nPreference Alignment for Robotics (SPAR-H), which fuses direct preference\noptimization on policy logits with a reward-based pathway that trains an\nimmediate-reward estimator from the same preferences and updates the policy\nusing a trust-region surrogate. With five HITL rollouts collected from a fixed\nnovice policy, SPAR-H achieves the highest final episodic reward and the lowest\nvariance across initial conditions among tested methods. The learned reward\nmodel aligns with human-preferred actions and elevates nearby non-intervened\nchoices, supporting stable propagation of improvements. We benchmark SPAR-H\nagainst imitation learning (IL), direct preference variants, and evaluative\nreinforcement learning (RL) in the HITL setting, and demonstrate real-world\nfeasibility of continual preference alignment for UAV river following. Overall,\ndual statewise preferences empirically provide a practical route to\ndata-efficient online adaptation in riverine navigation.", "AI": {"tldr": "SPAR-H\u662f\u4e00\u4e2a\u7528\u4e8e\u65e0\u4eba\u673a\u6cb3\u6d41\u8ddf\u8e2a\u7684\u4eba\u7c7b\u5728\u73af\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u76f4\u63a5\u504f\u597d\u4f18\u5316\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u8def\u5f84\uff0c\u5728\u6709\u9650\u7684\u4eba\u7c7b\u5e72\u9884\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u9002\u5e94\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u6cb3\u6d41\u73af\u5883\u76d1\u6d4b\u4e2d\u9762\u4e34\u6a21\u62df\u8bad\u7ec3\u7b56\u7565\u7684\u5206\u5e03\u504f\u79fb\u548c\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u4ece\u6709\u9650\u7684\u4eba\u7c7b\u5e72\u9884\u4e2d\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\u3002", "method": "\u63d0\u51faSPAR-H\u65b9\u6cd5\uff0c\u7ed3\u5408\u76f4\u63a5\u504f\u597d\u4f18\u5316\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u8def\u5f84\uff0c\u8bad\u7ec3\u5373\u65f6\u5956\u52b1\u4f30\u8ba1\u5668\uff0c\u5e76\u4f7f\u7528\u4fe1\u4efb\u57df\u4ee3\u7406\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u4f7f\u75285\u6b21\u4eba\u7c7b\u5728\u73af\u6d4b\u8bd5\uff0cSPAR-H\u5728\u6d4b\u8bd5\u65b9\u6cd5\u4e2d\u83b7\u5f97\u6700\u9ad8\u7684\u6700\u7ec8\u56de\u5408\u5956\u52b1\u548c\u6700\u4f4e\u7684\u521d\u59cb\u6761\u4ef6\u65b9\u5dee\uff0c\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002", "conclusion": "\u53cc\u91cd\u72b6\u6001\u504f\u597d\u4e3a\u6cb3\u6d41\u5bfc\u822a\u4e2d\u7684\u6570\u636e\u9ad8\u6548\u5728\u7ebf\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.01107", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01107", "abs": "https://arxiv.org/abs/2511.01107", "authors": ["Y. Isabel Liu", "Bowen Li", "Benjamin Eysenbach", "Tom Silver"], "title": "SLAP: Shortcut Learning for Abstract Planning", "comment": null, "summary": "Long-horizon decision-making with sparse rewards and continuous states and\nactions remains a fundamental challenge in AI and robotics. Task and motion\nplanning (TAMP) is a model-based framework that addresses this challenge by\nplanning hierarchically with abstract actions (options). These options are\nmanually defined, limiting the agent to behaviors that we as human engineers\nknow how to program (pick, place, move). In this work, we propose Shortcut\nLearning for Abstract Planning (SLAP), a method that leverages existing TAMP\noptions to automatically discover new ones. Our key idea is to use model-free\nreinforcement learning (RL) to learn shortcuts in the abstract planning graph\ninduced by the existing options in TAMP. Without any additional assumptions or\ninputs, shortcut learning leads to shorter solutions than pure planning, and\nhigher task success rates than flat and hierarchical RL. Qualitatively, SLAP\ndiscovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that\ndiffer significantly from the manually-defined ones. In experiments in four\nsimulated robotic environments, we show that SLAP solves and generalizes to a\nwide range of tasks, reducing overall plan lengths by over 50% and consistently\noutperforming planning and RL baselines.", "AI": {"tldr": "SLAP\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212(TAMP)\u548c\u6a21\u578b\u65e0\u5173\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u81ea\u52a8\u53d1\u73b0\u65b0\u7684\u62bd\u8c61\u52a8\u4f5c\u9009\u9879\uff0c\u4ece\u800c\u89e3\u51b3\u957f\u65f6\u7a0b\u51b3\u7b56\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfTAMP\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5b9a\u4e49\u7684\u62bd\u8c61\u52a8\u4f5c\u9009\u9879\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u53ea\u80fd\u6267\u884c\u4eba\u7c7b\u5de5\u7a0b\u5e08\u5df2\u77e5\u7684\u884c\u4e3a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u53d1\u73b0\u65b0\u9009\u9879\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u5229\u7528\u73b0\u6709TAMP\u9009\u9879\uff0c\u5728\u62bd\u8c61\u89c4\u5212\u56fe\u4e2d\u4f7f\u7528\u6a21\u578b\u65e0\u5173\u5f3a\u5316\u5b66\u4e60\u6765\u5b66\u4e60\u6377\u5f84\uff0c\u81ea\u52a8\u53d1\u73b0\u65b0\u7684\u62bd\u8c61\u52a8\u4f5c\u9009\u9879\u3002", "result": "\u5728\u56db\u4e2a\u6a21\u62df\u673a\u5668\u4eba\u73af\u5883\u4e2d\uff0cSLAP\u663e\u8457\u7f29\u77ed\u4e86\u89c4\u5212\u957f\u5ea6(\u51cf\u5c1150%\u4ee5\u4e0a)\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u53d1\u73b0\u4e86\u52a8\u6001\u7269\u7406\u5373\u5174\u52a8\u4f5c(\u5982\u62cd\u6253\u3001\u6446\u52a8\u3001\u64e6\u62ed)\u3002", "conclusion": "SLAP\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u7684\u65b0\u884c\u4e3a\u9009\u9879\uff0c\u5728\u957f\u65f6\u7a0b\u51b3\u7b56\u4efb\u52a1\u4e2d\u4f18\u4e8e\u7eaf\u89c4\u5212\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2511.01165", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01165", "abs": "https://arxiv.org/abs/2511.01165", "authors": ["Dong Heon Han", "Mayank Mehta", "Runze Zuo", "Zachary Wanger", "Daniel Bruder"], "title": "An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs", "comment": null, "summary": "This study presents an enhanced proprioceptive method for accurate shape\nestimation of soft robots using only off-the-shelf sensors, ensuring\ncost-effectiveness and easy applicability. By integrating inertial measurement\nunits (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling\nreliable long-term proprioception. A Kalman filter fuses segment tip\norientations from both sensors in a mutually compensatory manner, improving\nshape estimation over single-sensor methods. A piecewise constant curvature\nmodel estimates the tip location from the fused orientation data and\nreconstructs the robot's deformation. Experiments under no loading, external\nforces, and passive obstacle interactions during 45 minutes of continuous\noperation showed a root mean square error of 16.96 mm (2.91% of total length),\na 56% reduction compared to IMU-only benchmarks. These results demonstrate that\nour approach not only enables long-duration proprioception in soft robots but\nalso maintains high accuracy and robustness across these diverse conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u589e\u5f3a\u7684\u8f6f\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u73b0\u6210\u4f20\u611f\u5668\u5b9e\u73b0\u7cbe\u786e\u5f62\u72b6\u4f30\u8ba1\uff0c\u901a\u8fc7IMU\u548c\u5f2f\u66f2\u4f20\u611f\u5668\u878d\u5408\u6765\u51cf\u8f7bIMU\u6f02\u79fb\uff0c\u572845\u5206\u949f\u8fde\u7eed\u64cd\u4f5c\u4e2d\u5b9e\u73b016.96\u6beb\u7c73\u5747\u65b9\u6839\u8bef\u5dee\u3002", "motivation": "\u5f00\u53d1\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u6613\u4e8e\u5e94\u7528\u7684\u8f6f\u673a\u5668\u4eba\u5f62\u72b6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3IMU\u4f20\u611f\u5668\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u957f\u671f\u672c\u4f53\u611f\u77e5\u3002", "method": "\u96c6\u6210\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u548c\u4e92\u8865\u5f2f\u66f2\u4f20\u611f\u5668\uff0c\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u878d\u5408\u4e24\u79cd\u4f20\u611f\u5668\u7684\u6bb5\u7aef\u65b9\u5411\u6570\u636e\uff0c\u91c7\u7528\u5206\u6bb5\u6052\u5b9a\u66f2\u7387\u6a21\u578b\u4ece\u878d\u5408\u65b9\u5411\u6570\u636e\u4f30\u8ba1\u5c16\u7aef\u4f4d\u7f6e\u5e76\u91cd\u5efa\u673a\u5668\u4eba\u53d8\u5f62\u3002", "result": "\u5728\u65e0\u8d1f\u8f7d\u3001\u5916\u529b\u548c\u88ab\u52a8\u969c\u788d\u7269\u4ea4\u4e92\u6761\u4ef6\u4e0b\uff0c45\u5206\u949f\u8fde\u7eed\u64cd\u4f5c\u4e2d\u5747\u65b9\u6839\u8bef\u5dee\u4e3a16.96\u6beb\u7c73\uff08\u603b\u957f\u5ea6\u76842.91%\uff09\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528IMU\u7684\u65b9\u6cd5\u8bef\u5dee\u51cf\u5c1156%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u8f6f\u673a\u5668\u4eba\u7684\u957f\u671f\u672c\u4f53\u611f\u77e5\uff0c\u800c\u4e14\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.01177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01177", "abs": "https://arxiv.org/abs/2511.01177", "authors": ["Zihao He", "Bo Ai", "Tongzhou Mu", "Yulin Liu", "Weikang Wan", "Jiawei Fu", "Yilun Du", "Henrik I. Christensen", "Hao Su"], "title": "Scaling Cross-Embodiment World Models for Dexterous Manipulation", "comment": null, "summary": "Cross-embodiment learning seeks to build generalist robots that operate\nacross diverse morphologies, but differences in action spaces and kinematics\nhinder data sharing and policy transfer. This raises a central question: Is\nthere any invariance that allows actions to transfer across embodiments? We\nconjecture that environment dynamics are embodiment-invariant, and that world\nmodels capturing these dynamics can provide a unified interface across\nembodiments. To learn such a unified world model, the crucial step is to design\nstate and action representations that abstract away embodiment-specific details\nwhile preserving control relevance. To this end, we represent different\nembodiments (e.g., human hands and robot hands) as sets of 3D particles and\ndefine actions as particle displacements, creating a shared representation for\nheterogeneous data and control problems. A graph-based world model is then\ntrained on exploration data from diverse simulated robot hands and real human\nhands, and integrated with model-based planning for deployment on novel\nhardware. Experiments on rigid and deformable manipulation tasks reveal three\nfindings: (i) scaling to more training embodiments improves generalization to\nunseen ones, (ii) co-training on both simulated and real data outperforms\ntraining on either alone, and (iii) the learned models enable effective control\non robots with varied degrees of freedom. These results establish world models\nas a promising interface for cross-embodiment dexterous manipulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5177\u8eab\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e0d\u540c\u5f62\u6001\u7684\u673a\u5668\u4eba\u8868\u793a\u4e3a3D\u7c92\u5b50\u96c6\uff0c\u5e76\u5b9a\u4e49\u7c92\u5b50\u4f4d\u79fb\u4f5c\u4e3a\u52a8\u4f5c\uff0c\u6784\u5efa\u4e86\u57fa\u4e8e\u56fe\u7684\u4e16\u754c\u6a21\u578b\u6765\u6355\u6349\u73af\u5883\u52a8\u6001\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u95f4\u7684\u7b56\u7565\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u95f4\u7531\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u548c\u8fd0\u52a8\u5b66\u5dee\u5f02\u5bfc\u81f4\u7684\u6570\u636e\u5171\u4eab\u548c\u7b56\u7565\u8fc1\u79fb\u56f0\u96be\u95ee\u9898\uff0c\u63a2\u7d22\u662f\u5426\u5b58\u5728\u8de8\u5177\u8eab\u7684\u52a8\u4f5c\u8fc1\u79fb\u4e0d\u53d8\u6027\u3002", "method": "\u5c06\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u8868\u793a\u4e3a3D\u7c92\u5b50\u96c6\uff0c\u5b9a\u4e49\u7c92\u5b50\u4f4d\u79fb\u4f5c\u4e3a\u5171\u4eab\u52a8\u4f5c\u8868\u793a\uff0c\u8bad\u7ec3\u57fa\u4e8e\u56fe\u7684\u4e16\u754c\u6a21\u578b\u6765\u6355\u6349\u73af\u5883\u52a8\u6001\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a(1)\u589e\u52a0\u8bad\u7ec3\u5f62\u6001\u6570\u91cf\u80fd\u63d0\u9ad8\u5bf9\u672a\u89c1\u5f62\u6001\u7684\u6cdb\u5316\u80fd\u529b\uff1b(2)\u540c\u65f6\u4f7f\u7528\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\uff1b(3)\u5b66\u4e60\u5230\u7684\u6a21\u578b\u80fd\u5728\u4e0d\u540c\u81ea\u7531\u5ea6\u7684\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u6709\u6548\u63a7\u5236\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u4e3a\u8de8\u5177\u8eab\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7edf\u4e00\u63a5\u53e3\u3002"}}
{"id": "2511.01186", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01186", "abs": "https://arxiv.org/abs/2511.01186", "authors": ["Lijie Wang", "Lianjie Guo", "Ziyi Xu", "Qianhao Wang", "Fei Gao", "Xieyuanli Chen"], "title": "LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping", "comment": null, "summary": "Reconstructing large-scale colored point clouds is an important task in\nrobotics, supporting perception, navigation, and scene understanding. Despite\nadvances in LiDAR inertial visual odometry (LIVO), its performance remains\nhighly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation\nmodels, such as VGGT, suffer from limited scalability in large environments and\ninherently lack metric scale. To overcome these limitations, we propose\nLiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with\nthe state-of-the-art VGGT model through a two-stage coarse- to-fine fusion\npipeline: First, a pre-fusion module with robust initialization refinement\nefficiently estimates VGGT poses and point clouds with coarse metric scale\nwithin each session. Then, a post-fusion module enhances cross-modal 3D\nsimilarity transformation, using bounding-box-based regularization to reduce\nscale distortions caused by inconsistent FOVs between LiDAR and camera sensors.\nExtensive experiments across multiple datasets demonstrate that LiDAR-VGGT\nachieves dense, globally consistent colored point clouds and outperforms both\nVGGT-based methods and LIVO baselines. The implementation of our proposed novel\ncolor point cloud evaluation toolkit will be released as open source.", "AI": {"tldr": "\u63d0\u51faLiDAR-VGGT\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u878d\u5408\u5c06LiDAR\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e0eVGGT\u6a21\u578b\u7d27\u5bc6\u8026\u5408\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u5f69\u8272\u70b9\u4e91\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86VGGT\u5728\u5927\u73af\u5883\u4e2d\u5c3a\u5ea6\u7f3a\u5931\u548cLIVO\u5bf9\u5916\u53c2\u6807\u5b9a\u654f\u611f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LIVO\u65b9\u6cd5\u5bf9\u5916\u53c2\u6807\u5b9a\u9ad8\u5ea6\u654f\u611f\uff0c\u800c3D\u89c6\u89c9\u57fa\u7840\u6a21\u578bVGGT\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u6269\u5c55\u6027\u6709\u9650\u4e14\u7f3a\u4e4f\u5ea6\u91cf\u5c3a\u5ea6\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u6765\u5b9e\u73b0\u66f4\u597d\u7684\u5927\u89c4\u6a21\u5f69\u8272\u70b9\u4e91\u91cd\u5efa\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7c97\u5230\u7cbe\u878d\u5408\u7ba1\u9053\uff1a\u9884\u878d\u5408\u6a21\u5757\u901a\u8fc7\u9c81\u68d2\u521d\u59cb\u5316\u7ec6\u5316\u4f30\u8ba1VGGT\u4f4d\u59ff\u548c\u7c97\u5c3a\u5ea6\u70b9\u4e91\uff1b\u540e\u878d\u5408\u6a21\u5757\u589e\u5f3a\u8de8\u6a21\u60013D\u76f8\u4f3c\u53d8\u6362\uff0c\u4f7f\u7528\u57fa\u4e8e\u8fb9\u754c\u6846\u7684\u6b63\u5219\u5316\u51cf\u5c11LiDAR\u4e0e\u76f8\u673aFOV\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u5c3a\u5ea6\u5931\u771f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLiDAR-VGGT\u5b9e\u73b0\u4e86\u5bc6\u96c6\u3001\u5168\u5c40\u4e00\u81f4\u7684\u5f69\u8272\u70b9\u4e91\uff0c\u4f18\u4e8eVGGT\u65b9\u6cd5\u548cLIVO\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LiDAR-VGGT\u6210\u529f\u89e3\u51b3\u4e86VGGT\u7684\u5c3a\u5ea6\u7f3a\u5931\u548cLIVO\u7684\u6807\u5b9a\u654f\u611f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5927\u89c4\u6a21\u5f69\u8272\u70b9\u4e91\u91cd\u5efa\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90\u7684\u989c\u8272\u70b9\u4e91\u8bc4\u4f30\u5de5\u5177\u5305\u3002"}}
{"id": "2511.01199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01199", "abs": "https://arxiv.org/abs/2511.01199", "authors": ["Max McCandless", "Jonathan Hamid", "Sammy Elmariah", "Nathaniel Langer", "Pierre E. Dupont"], "title": "Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures", "comment": "8 pages, 11 figures", "summary": "To move away from open-heart surgery towards safer transcatheter procedures,\nthere is a growing need for improved imaging techniques and robotic solutions\nto enable simple, accurate tool navigation. Common imaging modalities, such as\nfluoroscopy and ultrasound, have limitations that can be overcome using\ncardioscopy, i.e., direct optical visualization inside the beating heart. We\npresent a cardioscope designed as a steerable balloon. As a balloon, it can be\ncollapsed to pass through the vasculature and subsequently inflated inside the\nheart for visualization and tool delivery through an integrated working\nchannel. Through careful design of balloon wall thickness, a single input,\nballoon inflation pressure, is used to independently control two outputs,\nballoon diameter (corresponding to field of view diameter) and balloon bending\nangle (enabling precise working channel positioning). This balloon technology\ncan be tuned to produce cardioscopes designed for a range of intracardiac\ntasks. To illustrate this approach, a balloon design is presented for the\nspecific task of aortic leaflet laceration. Image-based closed-loop control of\nbending angle is also demonstrated as a means of enabling stable orientation\ncontrol during tool insertion and removal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u64cd\u7eb5\u7403\u56ca\u5f0f\u5fc3\u810f\u955c\uff0c\u901a\u8fc7\u5355\u4e00\u8f93\u5165\uff08\u7403\u56ca\u5145\u6c14\u538b\u529b\uff09\u72ec\u7acb\u63a7\u5236\u7403\u56ca\u76f4\u5f84\u548c\u5f2f\u66f2\u89d2\u5ea6\uff0c\u5b9e\u73b0\u5fc3\u810f\u5185\u53ef\u89c6\u5316\u4e0e\u5de5\u5177\u8f93\u9001\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e3b\u52a8\u8109\u74e3\u53f6\u5207\u5f00\u7b49\u7ecf\u5bfc\u7ba1\u624b\u672f\u3002", "motivation": "\u4e3a\u4e86\u4ece\u5f00\u80f8\u624b\u672f\u8f6c\u5411\u66f4\u5b89\u5168\u7684\u7ecf\u5bfc\u7ba1\u624b\u672f\uff0c\u9700\u8981\u6539\u8fdb\u6210\u50cf\u6280\u672f\u548c\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u7b80\u5316\u3001\u7cbe\u786e\u5730\u5bfc\u822a\u5de5\u5177\u3002\u4f20\u7edf\u6210\u50cf\u65b9\u5f0f\uff08\u5982\u8367\u5149\u900f\u89c6\u548c\u8d85\u58f0\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u901a\u8fc7\u5fc3\u810f\u5185\u76f4\u63a5\u5149\u5b66\u53ef\u89c6\u5316\u6765\u514b\u670d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53ef\u64cd\u7eb5\u7403\u56ca\u5f0f\u5fc3\u810f\u955c\uff0c\u7403\u56ca\u53ef\u6536\u7f29\u901a\u8fc7\u8840\u7ba1\u7cfb\u7edf\uff0c\u5728\u5fc3\u810f\u5185\u5145\u6c14\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u5de5\u4f5c\u901a\u9053\u8f93\u9001\u5de5\u5177\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7403\u56ca\u58c1\u539a\u5ea6\uff0c\u4f7f\u7528\u5355\u4e00\u8f93\u5165\uff08\u5145\u6c14\u538b\u529b\uff09\u72ec\u7acb\u63a7\u5236\u7403\u56ca\u76f4\u5f84\uff08\u5bf9\u5e94\u89c6\u91ce\u76f4\u5f84\uff09\u548c\u5f2f\u66f2\u89d2\u5ea6\uff08\u5b9e\u73b0\u5de5\u4f5c\u901a\u9053\u7cbe\u786e\u5b9a\u4f4d\uff09\u3002", "result": "\u8be5\u7403\u56ca\u6280\u672f\u53ef\u8c03\u6574\u4ee5\u9002\u5e94\u5404\u79cd\u5fc3\u5185\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u7528\u4e8e\u4e3b\u52a8\u8109\u74e3\u53f6\u5207\u5f00\u7684\u7279\u5b9a\u7403\u56ca\u8bbe\u8ba1\uff0c\u5e76\u5b9e\u73b0\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u5f2f\u66f2\u89d2\u5ea6\u95ed\u73af\u63a7\u5236\uff0c\u5728\u5de5\u5177\u63d2\u5165\u548c\u53d6\u51fa\u671f\u95f4\u4fdd\u6301\u7a33\u5b9a\u65b9\u5411\u63a7\u5236\u3002", "conclusion": "\u53ef\u64cd\u7eb5\u7403\u56ca\u5f0f\u5fc3\u810f\u955c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u4e00\u8f93\u5165\u63a7\u5236\u591a\u4e2a\u8f93\u51fa\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u5fc3\u810f\u5185\u7cbe\u786e\u53ef\u89c6\u5316\u4e0e\u5de5\u5177\u64cd\u4f5c\uff0c\u4e3a\u7ecf\u5bfc\u7ba1\u624b\u672f\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.01219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01219", "abs": "https://arxiv.org/abs/2511.01219", "authors": ["Muhua Zhang", "Lei Ma", "Ying Wu", "Kai Shen", "Deqing Huang", "Henry Leung"], "title": "Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference", "comment": "10 pages, 8 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This paper addresses the Kidnapped Robot Problem (KRP), a core localization\nchallenge of relocalizing a robot in a known map without prior pose estimate\nwhen localization loss or at SLAM initialization. For this purpose, a passive\n2-D global relocalization framework is proposed. It estimates the global pose\nefficiently and reliably from a single LiDAR scan and an occupancy grid map\nwhile the robot remains stationary, thereby enhancing the long-term autonomy of\nmobile robots. The proposed framework casts global relocalization as a\nnon-convex problem and solves it via the multi-hypothesis scheme with batched\nmulti-stage inference and early termination, balancing completeness and\nefficiency. The Rapidly-exploring Random Tree (RRT), under traversability\nconstraints, asymptotically covers the reachable space to generate sparse,\nuniformly distributed feasible positional hypotheses, fundamentally reducing\nthe sampling space. The hypotheses are preliminarily ordered by the proposed\nScan Mean Absolute Difference (SMAD), a coarse beam-error level metric that\nfacilitates the early termination by prioritizing high-likelihood candidates.\nThe SMAD computation is optimized for non-panoramic scans. And the\nTranslation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for\nreliable orientation selection at hypothesized positions and accurate final\npose evaluation to mitigate degradation in conventional likelihood-field\nmetrics under translational uncertainty induced by sparse hypotheses, as well\nas non-panoramic LiDAR scan and environmental changes. Real-world experiments\non a resource-constrained mobile robot with non-panoramic LiDAR scan\ndemonstrate that the proposed framework outperforms existing methods in both\nglobal relocalization success rate and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u88ab\u52a82D\u5168\u5c40\u91cd\u5b9a\u4f4d\u6846\u67b6\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u7ed1\u67b6\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u6b21LiDAR\u626b\u63cf\u548c\u5360\u636e\u6805\u683c\u5730\u56fe\u9ad8\u6548\u53ef\u9760\u5730\u4f30\u8ba1\u5168\u5c40\u4f4d\u59ff\uff0c\u4f7f\u7528\u591a\u5047\u8bbe\u65b9\u6848\u5e73\u8861\u5b8c\u6574\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7ed1\u67b6\u95ee\u9898(KRP)\uff0c\u5373\u5728\u5df2\u77e5\u5730\u56fe\u4e2d\u91cd\u65b0\u5b9a\u4f4d\u673a\u5668\u4eba\u800c\u65e0\u9700\u5148\u9a8c\u4f4d\u59ff\u4f30\u8ba1\uff0c\u8fd9\u5bf9\u4e8eSLAM\u521d\u59cb\u5316\u548c\u5b9a\u4f4d\u4e22\u5931\u6062\u590d\u81f3\u5173\u91cd\u8981\uff0c\u65e8\u5728\u63d0\u5347\u79fb\u52a8\u673a\u5668\u4eba\u7684\u957f\u671f\u81ea\u4e3b\u6027\u3002", "method": "\u91c7\u7528\u591a\u5047\u8bbe\u65b9\u6848\uff0c\u4f7f\u7528RRT\u5728\u53ef\u901a\u884c\u7ea6\u675f\u4e0b\u751f\u6210\u7a00\u758f\u5747\u5300\u7684\u4f4d\u7f6e\u5047\u8bbe\uff0c\u901a\u8fc7SMAD\u6307\u6807\u521d\u6b65\u6392\u5e8f\u5047\u8bbe\u5e76\u5b9e\u73b0\u65e9\u671f\u7ec8\u6b62\uff0c\u63d0\u51faTAM\u6307\u6807\u8fdb\u884c\u53ef\u9760\u7684\u65b9\u5411\u9009\u62e9\u548c\u6700\u7ec8\u4f4d\u59ff\u8bc4\u4f30\u3002", "result": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5168\u5c40\u91cd\u5b9a\u4f4d\u6210\u529f\u7387\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u88ab\u52a82D\u5168\u5c40\u91cd\u5b9a\u4f4d\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u53ef\u9760\u5730\u89e3\u51b3\u673a\u5668\u4eba\u7ed1\u67b6\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5047\u8bbe\u65b9\u6848\u548c\u4f18\u5316\u7684\u6307\u6807\u8bbe\u8ba1\uff0c\u5728\u975e\u5168\u666fLiDAR\u626b\u63cf\u548c\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.01224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01224", "abs": "https://arxiv.org/abs/2511.01224", "authors": ["Chengmeng Li", "Yaxin Peng"], "title": "Embodiment Transfer Learning for Vision-Language-Action Models", "comment": null, "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nlearning, enabling training on large-scale, cross-embodiment data and\nfine-tuning for specific robots. However, state-of-the-art autoregressive VLAs\nstruggle with multi-robot collaboration. We introduce embodiment transfer\nlearning, denoted as ET-VLA, a novel framework for efficient and effective\ntransfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic\nContinued Pretraining (SCP), which uses synthetically generated data to warm up\nthe model for the new embodiment, bypassing the need for real human\ndemonstrations and reducing data collection costs. SCP enables the model to\nlearn correct actions and precise action token numbers. Following SCP, the\nmodel is fine-tuned on target embodiment data. To further enhance the model\nperformance on multi-embodiment, we present the Embodied Graph-of-Thought\ntechnique, a novel approach that formulates each sub-task as a node, that\nallows the VLA model to distinguish the functionalities and roles of each\nembodiment during task execution. Our work considers bimanual robots, a simple\nversion of multi-robot to verify our approaches. We validate the effectiveness\nof our method on both simulation benchmarks and real robots covering three\ndifferent bimanual embodiments. In particular, our proposed ET-VLA \\space can\noutperform OpenVLA on six real-world tasks over 53.2%. We will open-source all\ncodes to support the community in advancing VLA models for robot learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86ET-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5177\u8eab\u601d\u7ef4\u56fe\u6280\u672f\uff0c\u5b9e\u73b0\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5411\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6709\u6548\u8fc1\u79fb\uff0c\u5728\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\u5ec9\u7684\u8de8\u5177\u8eab\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "ET-VLA\u6846\u67b6\u5305\u542b\u5408\u6210\u6301\u7eed\u9884\u8bad\u7ec3\uff08\u4f7f\u7528\u5408\u6210\u6570\u636e\u9884\u70ed\u6a21\u578b\uff09\u548c\u5177\u8eab\u601d\u7ef4\u56fe\u6280\u672f\uff08\u5c06\u5b50\u4efb\u52a1\u5efa\u6a21\u4e3a\u8282\u70b9\u4ee5\u533a\u5206\u4e0d\u540c\u5177\u8eab\u7684\u529f\u80fd\u89d2\u8272\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u53cc\u624b\u673a\u5668\u4eba\u5177\u8eab\u4e0a\u9a8c\u8bc1\uff0c\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u6bd4OpenVLA\u6027\u80fd\u63d0\u5347\u8d85\u8fc753.2%\u3002", "conclusion": "ET-VLA\u4e3a\u591a\u673a\u5668\u4eba\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8fc1\u79fb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5177\u8eab\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.01232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01232", "abs": "https://arxiv.org/abs/2511.01232", "authors": ["Yu-Ting Lai", "Jacob Rosen", "Yasamin Foroutani", "Ji Ma", "Wen-Cheng Wu", "Jean-Pierre Hubschman", "Tsu-Chin Tsao"], "title": "High-Precision Surgical Robotic System for Intraocular Procedures", "comment": null, "summary": "Despite the extensive demonstration of robotic systems for both cataract and\nvitreoretinal procedures, existing technologies or mechanisms still possess\ninsufficient accuracy, precision, and degrees of freedom for instrument\nmanipulation or potentially automated tool exchange during surgical procedures.\nA new robotic system that focuses on improving tooltip accuracy, tracking\nperformance, and smooth instrument exchange mechanism is therefore designed and\nmanufactured. Its tooltip accuracy, precision, and mechanical capability of\nmaintaining small incision through remote center of motion were externally\nevaluated using an optical coherence tomography (OCT) system. Through robot\ncalibration and precise coordinate registration, the accuracy of tooltip\npositioning was measured to be 0.053$\\pm$0.031 mm, and the overall performance\nwas demonstrated on an OCT-guided automated cataract lens extraction procedure\nwith deep learning-based pre-operative anatomical modeling and real-time\nsupervision.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u9ad8\u773c\u79d1\u624b\u672f\u4e2d\u7684\u5de5\u5177\u5c16\u7aef\u7cbe\u5ea6\u3001\u8ddf\u8e2a\u6027\u80fd\u548c\u5de5\u5177\u4ea4\u6362\u673a\u5236\uff0c\u5728OCT\u5f15\u5bfc\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u767d\u5185\u969c\u6676\u72b6\u4f53\u63d0\u53d6\u624b\u672f\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u773c\u79d1\u624b\u672f\u4e2d\u7cbe\u5ea6\u3001\u81ea\u7531\u5ea6\u548c\u5de5\u5177\u4ea4\u6362\u673a\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u3001\u66f4\u7075\u6d3b\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u6765\u6539\u5584\u624b\u672f\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u5236\u9020\u4e86\u65b0\u578b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u6821\u51c6\u548c\u7cbe\u786e\u5750\u6807\u914d\u51c6\uff0c\u4f7f\u7528\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u7cfb\u7edf\u8bc4\u4f30\u5de5\u5177\u5c16\u7aef\u7cbe\u5ea6\u548c\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\u673a\u5236\u3002", "result": "\u5de5\u5177\u5c16\u7aef\u5b9a\u4f4d\u7cbe\u5ea6\u8fbe\u52300.053\u00b10.031\u6beb\u7c73\uff0c\u6210\u529f\u5728OCT\u5f15\u5bfc\u4e0b\u5b8c\u6210\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u672f\u524d\u89e3\u5256\u5efa\u6a21\u548c\u5b9e\u65f6\u76d1\u7763\u7684\u81ea\u52a8\u5316\u767d\u5185\u969c\u6676\u72b6\u4f53\u63d0\u53d6\u624b\u672f\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u773c\u79d1\u624b\u672f\u7684\u7cbe\u5ea6\u548c\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u4e3a\u590d\u6742\u773c\u79d1\u624b\u672f\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2511.01236", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01236", "abs": "https://arxiv.org/abs/2511.01236", "authors": ["Junwen Zhang", "Changyue Liu", "Pengqi Fu", "Xiang Guo", "Ye Shi", "Xudong Liang", "Zhijian Wang", "Hanzhi Ma"], "title": "Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments", "comment": "8 pages, 5 figures", "summary": "Endowed with inherent dynamical properties that grant them remarkable\nruggedness and adaptability, spherical tensegrity robots stand as prototypical\nexamples of hybrid softrigid designs and excellent mobile platforms. However,\npath planning for these robots in unknown environments presents a significant\nchallenge, requiring a delicate balance between efficient exploration and\nrobust planning. Traditional path planners, which treat the environment as a\ngeometric grid, often suffer from redundant searches and are prone to failure\nin complex scenarios due to their lack of semantic understanding. To overcome\nthese limitations, we reframe path planning in unknown environments as a\nsemantic reasoning task. We introduce a Semantic Agent for Tensegrity robots\n(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages\nhigh-level environmental comprehension to generate efficient and reliable\nplanning strategies.At the core of SATPlanner is an Adaptive Observation Window\nmechanism, inspired by the \"fast\" and \"slow\" thinking paradigms of LLMs. This\nmechanism dynamically adjusts the perceptual field of the agent: it narrows for\nrapid traversal of open spaces and expands to reason about complex obstacle\nconfigurations. This allows the agent to construct a semantic belief of the\nenvironment, enabling the search space to grow only linearly with the path\nlength (O(L)) while maintaining path quality. We extensively evaluate\nSATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,\noutperforming other real-time planning algorithms. Critically, SATPlanner\nreduces the search space by 37.2% compared to the A* algorithm while achieving\ncomparable, near-optimal path lengths. Finally, the practical feasibility of\nSATPlanner is validated on a physical spherical tensegrity robot prototype.", "AI": {"tldr": "SATPlanner\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7403\u9762\u5f20\u62c9\u6574\u4f53\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u8def\u5f84\u89c4\u5212\uff0c\u663e\u8457\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u5e76\u4fdd\u6301\u8def\u5f84\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u5668\u5c06\u73af\u5883\u89c6\u4e3a\u51e0\u4f55\u7f51\u683c\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5bb9\u6613\u5931\u8d25\u4e14\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u3002\u7403\u9762\u5f20\u62c9\u6574\u4f53\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u9700\u8981\u5e73\u8861\u63a2\u7d22\u6548\u7387\u548c\u89c4\u5212\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faSATPlanner\uff0c\u5229\u7528LLM\u8fdb\u884c\u9ad8\u5c42\u6b21\u73af\u5883\u7406\u89e3\u3002\u6838\u5fc3\u662f\u81ea\u9002\u5e94\u89c2\u6d4b\u7a97\u53e3\u673a\u5236\uff0c\u6839\u636e\u73af\u5883\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u611f\u77e5\u8303\u56f4\uff1a\u5728\u5f00\u9614\u7a7a\u95f4\u7f29\u5c0f\u7a97\u53e3\u5feb\u901f\u7a7f\u8d8a\uff0c\u5728\u590d\u6742\u969c\u788d\u914d\u7f6e\u65f6\u6269\u5927\u7a97\u53e3\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u57281000\u6b21\u4eff\u771f\u8bd5\u9a8c\u4e2d\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u641c\u7d22\u7a7a\u95f4\u6bd4A*\u7b97\u6cd5\u51cf\u5c1137.2%\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u5f84\u957f\u5ea6\u3002\u5728\u7269\u7406\u539f\u578b\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u5c06\u8def\u5f84\u89c4\u5212\u91cd\u6784\u4e3a\u8bed\u4e49\u63a8\u7406\u4efb\u52a1\uff0cSATPlanner\u901a\u8fc7\u8bed\u4e49\u73af\u5883\u7406\u89e3\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89c4\u5212\uff0c\u641c\u7d22\u7a7a\u95f4\u4ec5\u968f\u8def\u5f84\u957f\u5ea6\u7ebf\u6027\u589e\u957f(O(L))\uff0c\u4e3a\u6df7\u5408\u8f6f\u786c\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8def\u5f84\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01256", "abs": "https://arxiv.org/abs/2511.01256", "authors": ["Yasamin Foroutani", "Yasamin Mousavi-Motlagh", "Aya Barzelay", "Tsu-Chin Tsao"], "title": "Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control", "comment": "10 pages, 10 figures", "summary": "Achieving precise control of robotic tool paths is often challenged by\ninherent system misalignments, unmodeled dynamics, and actuation inaccuracies.\nThis work introduces an Iterative Learning Control (ILC) strategy to enable\nprecise rotational insertion of a tool during robotic surgery, improving\npenetration efficacy and safety compared to straight insertion tested in\nsubretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,\nwhere misalignment of the fourth joint complicates the simple application of\nneedle rotation, motivating an ILC approach that iteratively adjusts joint\ncommands based on positional feedback. The process begins with calibrating the\nforward kinematics for the chosen surgical tool to achieve higher accuracy,\nfollowed by successive ILC iterations guided by Optical Coherence Tomography\n(OCT) volume scans to measure the error and refine control inputs. Experimental\nresults, tested on subretinal injection tasks on ex vivo pig eyes, show that\nthe optimized trajectory resulted in higher success rates in tissue penetration\nand subretinal injection compared to straight insertion, demonstrating the\neffectiveness of ILC in overcoming misalignment challenges. This approach\noffers potential applications for other high precision robot tasks requiring\ncontrolled insertions as well.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u624b\u672f\u4e2d\u5de5\u5177\u7684\u7cbe\u786e\u65cb\u8f6c\u63d2\u5165\uff0c\u76f8\u6bd4\u76f4\u7ebf\u63d2\u5165\u63d0\u9ad8\u4e86\u7a7f\u900f\u6548\u679c\u548c\u5b89\u5168\u6027", "motivation": "\u673a\u5668\u4eba\u5de5\u5177\u8def\u5f84\u7684\u7cbe\u786e\u63a7\u5236\u9762\u4e34\u7cfb\u7edf\u4e0d\u5bf9\u51c6\u3001\u672a\u5efa\u6a21\u52a8\u6001\u548c\u9a71\u52a8\u8bef\u5dee\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u89c6\u7f51\u819c\u4e0b\u6ce8\u5c04\u7b49\u7cbe\u5bc6\u624b\u672f\u4e2d", "method": "\u4f7f\u75284\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u64cd\u7eb5\u5668\uff0c\u901a\u8fc7\u524d\u5411\u8fd0\u52a8\u5b66\u6821\u51c6\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u7136\u540e\u57fa\u4e8eOCT\u4f53\u79ef\u626b\u63cf\u7684\u8bef\u5dee\u6d4b\u91cf\u8fdb\u884c\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff0c\u9010\u6b65\u8c03\u6574\u5173\u8282\u6307\u4ee4", "result": "\u5728\u79bb\u4f53\u732a\u773c\u89c6\u7f51\u819c\u4e0b\u6ce8\u5c04\u5b9e\u9a8c\u4e2d\uff0c\u4f18\u5316\u8f68\u8ff9\u76f8\u6bd4\u76f4\u7ebf\u63d2\u5165\u5728\u7ec4\u7ec7\u7a7f\u900f\u548c\u89c6\u7f51\u819c\u4e0b\u6ce8\u5c04\u65b9\u9762\u83b7\u5f97\u66f4\u9ad8\u6210\u529f\u7387", "conclusion": "ILC\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u5bf9\u51c6\u6311\u6218\uff0c\u4e3a\u5176\u4ed6\u9700\u8981\u53d7\u63a7\u63d2\u5165\u7684\u9ad8\u7cbe\u5ea6\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u6f5c\u5728\u5e94\u7528"}}
{"id": "2511.01272", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01272", "abs": "https://arxiv.org/abs/2511.01272", "authors": ["Sehui Jeong", "Magaly C. Aviles", "Athena X. Naylor", "Cynthia Sung", "Allison M. Okamura"], "title": "Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics", "comment": null, "summary": "Soft robots employing compliant materials and deformable structures offer\ngreat potential for wearable devices that are comfortable and safe for human\ninteraction. However, achieving both structural integrity and compliance for\ncomfort remains a significant challenge. In this study, we present a novel\nfabrication and design method that combines the advantages of origami\nstructures with the material programmability and wearability of knitted\nfabrics. We introduce a general design method that translates origami patterns\ninto knit designs by programming both stitch and material patterns. The method\ncreates folds in preferred directions while suppressing unintended buckling and\nbending by selectively incorporating heat fusible yarn to create rigid panels\naround compliant creases. We experimentally quantify folding moments and show\nthat stitch patterning enhances folding directionality while the heat fusible\nyarn (1) keeps geometry consistent by reducing edge curl and (2) prevents\nout-of-plane deformations by stiffening panels. We demonstrate the framework\nthrough the successful reproduction of complex origami tessellations, including\nMiura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted\nKaleidocycle robot capable of locomotion. The combination of structural\nreconfigurability, material programmability, and potential for manufacturing\nscalability highlights knitted origami as a promising platform for\nnext-generation wearable robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6298\u7eb8\u7ed3\u6784\u4e0e\u9488\u7ec7\u9762\u6599\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7a0b\u9488\u6cd5\u548c\u6750\u6599\u56fe\u6848\u6765\u5236\u9020\u53ef\u7a7f\u6234\u8f6f\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u53ef\u91cd\u6784\u6027\u548c\u8212\u9002\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2d\u5177\u6709\u8212\u9002\u5b89\u5168\u7684\u4f18\u52bf\uff0c\u4f46\u5982\u4f55\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u987a\u5e94\u6027\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6298\u7eb8\u7ed3\u6784\u7684\u4f18\u52bf\u4e0e\u9488\u7ec7\u9762\u6599\u7684\u6750\u6599\u53ef\u7f16\u7a0b\u6027\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u52a0\u5165\u70ed\u7194\u7eb1\u7ebf\u5728\u67d4\u6027\u6298\u75d5\u5468\u56f4\u521b\u5efa\u521a\u6027\u9762\u677f\uff0c\u7f16\u7a0b\u9488\u6cd5\u548c\u6750\u6599\u56fe\u6848\u5c06\u6298\u7eb8\u56fe\u6848\u8f6c\u5316\u4e3a\u9488\u7ec7\u8bbe\u8ba1\u3002", "result": "\u6210\u529f\u590d\u5236\u4e86\u590d\u6742\u7684\u6298\u7eb8\u9576\u5d4c\u56fe\u6848\uff08Miura-ori\u3001Yoshimura\u3001Kresling\uff09\uff0c\u5e76\u5f00\u53d1\u51fa\u80fd\u591f\u8fd0\u52a8\u7684\u53ef\u7a7f\u6234\u9488\u7ec7\u4e07\u82b1\u7b52\u5faa\u73af\u673a\u5668\u4eba\uff0c\u5b9e\u9a8c\u91cf\u5316\u4e86\u6298\u53e0\u529b\u77e9\u3002", "conclusion": "\u9488\u7ec7\u6298\u7eb8\u7ed3\u5408\u4e86\u7ed3\u6784\u53ef\u91cd\u6784\u6027\u3001\u6750\u6599\u53ef\u7f16\u7a0b\u6027\u548c\u5236\u9020\u53ef\u6269\u5c55\u6027\uff0c\u662f\u4e0b\u4e00\u4ee3\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7684\u6709\u524d\u666f\u5e73\u53f0\u3002"}}
{"id": "2511.01276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01276", "abs": "https://arxiv.org/abs/2511.01276", "authors": ["Yiyao Ma", "Kai Chen", "Kexin Zheng", "Qi Dou"], "title": "Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation", "comment": null, "summary": "Dexterous grasp generation is a fundamental challenge in robotics, requiring\nboth grasp stability and adaptability across diverse objects and tasks.\nAnalytical methods ensure stable grasps but are inefficient and lack task\nadaptability, while generative approaches improve efficiency and task\nintegration but generalize poorly to unseen objects and tasks due to data\nlimitations. In this paper, we propose a transfer-based framework for dexterous\ngrasp generation, leveraging a conditional diffusion model to transfer\nhigh-quality grasps from shape templates to novel objects within the same\ncategory. Specifically, we reformulate the grasp transfer problem as the\ngeneration of an object contact map, incorporating object shape similarity and\ntask specifications into the diffusion process. To handle complex shape\nvariations, we introduce a dual mapping mechanism, capturing intricate\ngeometric relationship between shape templates and novel objects. Beyond the\ncontact map, we derive two additional object-centric maps, the part map and\ndirection map, to encode finer contact details for more stable grasps. We then\ndevelop a cascaded conditional diffusion model framework to jointly transfer\nthese three maps, ensuring their intra-consistency. Finally, we introduce a\nrobust grasp recovery mechanism, identifying reliable contact points and\noptimizing grasp configurations efficiently. Extensive experiments demonstrate\nthe superiority of our proposed method. Our approach effectively balances grasp\nquality, generation efficiency, and generalization performance across various\ntasks. Project homepage: https://cmtdiffusion.github.io/", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u7075\u5de7\u6293\u53d6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ad8\u8d28\u91cf\u6293\u53d6\u4ece\u5f62\u72b6\u6a21\u677f\u8f6c\u79fb\u5230\u540c\u7c7b\u65b0\u7269\u4f53\uff0c\u89e3\u51b3\u4e86\u6293\u53d6\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u9002\u5e94\u6027\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5206\u6790\u65b9\u6cd5\u80fd\u4fdd\u8bc1\u6293\u53d6\u7a33\u5b9a\u6027\u4f46\u6548\u7387\u4f4e\u4e14\u7f3a\u4e4f\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u751f\u6210\u65b9\u6cd5\u6548\u7387\u9ad8\u4f46\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u6293\u53d6\u8d28\u91cf\u3001\u751f\u6210\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5c06\u6293\u53d6\u8f6c\u79fb\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7269\u4f53\u63a5\u89e6\u56fe\u751f\u6210\uff0c\u5f15\u5165\u53cc\u6620\u5c04\u673a\u5236\u5904\u7406\u590d\u6742\u5f62\u72b6\u53d8\u5316\uff0c\u5e76\u8054\u5408\u8f6c\u79fb\u63a5\u89e6\u56fe\u3001\u90e8\u4ef6\u56fe\u548c\u65b9\u5411\u56fe\u4e09\u4e2a\u5730\u56fe\uff0c\u6700\u540e\u901a\u8fc7\u9c81\u68d2\u6293\u53d6\u6062\u590d\u673a\u5236\u4f18\u5316\u6293\u53d6\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u6293\u53d6\u8d28\u91cf\u3001\u751f\u6210\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8f6c\u79fb\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7075\u5de7\u6293\u53d6\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u673a\u5668\u4eba\u6293\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01288", "categories": ["cs.RO", "cs.SY", "eess.SY", "I.2.9"], "pdf": "https://arxiv.org/pdf/2511.01288", "abs": "https://arxiv.org/abs/2511.01288", "authors": ["Bixuan Zhang", "Fengqi Zhang", "Haojie Chen", "You Wang", "Jie Hao", "Zhiyuan Luo", "Guang Li"], "title": "A High-Speed Capable Spherical Robot", "comment": "5 pages", "summary": "This paper designs a new spherical robot structure capable of supporting\nhigh-speed motion at up to 10 m/s. Building upon a single-pendulum-driven\nspherical robot, the design incorporates a momentum wheel with an axis aligned\nwith the secondary pendulum, creating a novel spherical robot structure.\nPractical experiments with the physical prototype have demonstrated that this\nnew spherical robot can achieve stable high-speed motion through simple\ndecoupled control, which was unattainable with the original structure. The\nspherical robot designed for high-speed motion not only increases speed but\nalso significantly enhances obstacle-crossing performance and terrain\nrobustness.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u7ed3\u6784\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8fbe10\u7c73/\u79d2\u7684\u9ad8\u901f\u8fd0\u52a8\uff0c\u901a\u8fc7\u5f15\u5165\u4e0e\u6b21\u7ea7\u6446\u5bf9\u9f50\u7684\u52a8\u91cf\u8f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u6027\u80fd\u548c\u5730\u5f62\u9002\u5e94\u6027\u3002", "motivation": "\u57fa\u4e8e\u5355\u6446\u9a71\u52a8\u7403\u5f62\u673a\u5668\u4eba\u7684\u5c40\u9650\u6027\uff0c\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u901f\u8fd0\u52a8\u7684\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u7ed3\u6784\uff0c\u89e3\u51b3\u539f\u6709\u7ed3\u6784\u65e0\u6cd5\u8fbe\u5230\u9ad8\u901f\u7a33\u5b9a\u8fd0\u52a8\u7684\u95ee\u9898\u3002", "method": "\u5728\u5355\u6446\u9a71\u52a8\u7403\u5f62\u673a\u5668\u4eba\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e0e\u6b21\u7ea7\u6446\u8f74\u7ebf\u5bf9\u9f50\u7684\u52a8\u91cf\u8f6e\uff0c\u6784\u5efa\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u89e3\u8026\u63a7\u5236\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u901f\u8fd0\u52a8\u3002", "result": "\u7269\u7406\u6837\u673a\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u7b80\u5355\u7684\u89e3\u8026\u63a7\u5236\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u901f\u8fd0\u52a8\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8d8a\u969c\u6027\u80fd\u548c\u5730\u5f62\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u8bbe\u8ba1\u7684\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u7ed3\u6784\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u901f\u8fd0\u52a8\u80fd\u529b\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8fd0\u52a8\u901f\u5ea6\uff0c\u8fd8\u5927\u5e45\u6539\u5584\u4e86\u8d8a\u969c\u6027\u80fd\u548c\u5730\u5f62\u9002\u5e94\u6027\uff0c\u4e3a\u7403\u5f62\u673a\u5668\u4eba\u7684\u5e94\u7528\u62d3\u5c55\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.01294", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01294", "abs": "https://arxiv.org/abs/2511.01294", "authors": ["Jiawei Wang", "Dingyou Wang", "Jiaming Hu", "Qixuan Zhang", "Jingyi Yu", "Lan Xu"], "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects", "comment": null, "summary": "A deep understanding of kinematic structures and movable components is\nessential for enabling robots to manipulate objects and model their own\narticulated forms. Such understanding is captured through articulated objects,\nwhich are essential for tasks such as physical simulation, motion planning, and\npolicy learning. However, creating these models, particularly for complex\nsystems like robots or objects with high degrees of freedom (DoF), remains a\nsignificant challenge. Existing methods typically rely on motion sequences or\nstrong assumptions from hand-curated datasets, which hinders scalability. In\nthis paper, we introduce Kinematify, an automated framework that synthesizes\narticulated objects directly from arbitrary RGB images or text prompts. Our\nmethod addresses two core challenges: (i) inferring kinematic topologies for\nhigh-DoF objects and (ii) estimating joint parameters from static geometry. To\nachieve this, we combine MCTS search for structural inference with\ngeometry-driven optimization for joint reasoning, producing physically\nconsistent and functionally valid descriptions. We evaluate Kinematify on\ndiverse inputs from both synthetic and real-world environments, demonstrating\nimprovements in registration and kinematic topology accuracy over prior work.", "AI": {"tldr": "Kinematify\u662f\u4e00\u4e2a\u4eceRGB\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u81ea\u52a8\u5408\u6210\u5173\u8282\u7269\u4f53\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u9ad8\u81ea\u7531\u5ea6\u7269\u4f53\u8fd0\u52a8\u5b66\u62d3\u6251\u63a8\u65ad\u548c\u9759\u6001\u51e0\u4f55\u5173\u8282\u53c2\u6570\u4f30\u8ba1\u7684\u6311\u6218\u3002", "motivation": "\u5173\u8282\u7269\u4f53\u5efa\u6a21\u5bf9\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u81ea\u8eab\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fd0\u52a8\u5e8f\u5217\u6216\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u7ed3\u5408MCTS\u641c\u7d22\u8fdb\u884c\u7ed3\u6784\u63a8\u65ad\u548c\u51e0\u4f55\u9a71\u52a8\u4f18\u5316\u8fdb\u884c\u5173\u8282\u63a8\u7406\uff0c\u751f\u6210\u7269\u7406\u4e00\u81f4\u4e14\u529f\u80fd\u6709\u6548\u7684\u63cf\u8ff0\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u8f93\u5165\u4e0a\u8bc4\u4f30\uff0c\u5728\u914d\u51c6\u548c\u8fd0\u52a8\u5b66\u62d3\u6251\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "Kinematify\u80fd\u591f\u4ece\u4efb\u610fRGB\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u81ea\u52a8\u5408\u6210\u5173\u8282\u7269\u4f53\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01331", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01331", "abs": "https://arxiv.org/abs/2511.01331", "authors": ["Hongyin Zhang", "Shuo Zhang", "Junxi Jin", "Qixin Zeng", "Runze Li", "Donglin Wang"], "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently emerged as powerful\ngeneral-purpose policies for robotic manipulation, benefiting from large-scale\nmulti-modal pre-training. However, they often fail to generalize reliably in\nout-of-distribution deployments, where unavoidable disturbances such as\nobservation noise, sensor errors, or actuation perturbations become prevalent.\nWhile recent Reinforcement Learning (RL)-based post-training provides a\npractical means to adapt pre-trained VLA models, existing methods mainly\nemphasize reward maximization and overlook robustness to environmental\nuncertainty. In this work, we introduce RobustVLA, a lightweight online RL\npost-training method designed to explicitly enhance the resilience of VLA\nmodels. Through a systematic robustness analysis, we identify two key\nregularizations: Jacobian regularization, which mitigates sensitivity to\nobservation noise, and smoothness regularization, which stabilizes policies\nunder action perturbations. Extensive experiments across diverse robotic\nenvironments demonstrate that RobustVLA significantly outperforms prior\nstate-of-the-art methods in robustness and reliability. Our results highlight\nthe importance of principled robustness-aware RL post-training as a key step\ntoward improving the reliability and robustness of VLA models.", "AI": {"tldr": "RobustVLA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c5\u53ef\u6bd4\u6b63\u5219\u5316\u548c\u5e73\u6ed1\u6b63\u5219\u5316\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u73af\u5883\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u5206\u5e03\u5916\u90e8\u7f72\u65f6\u5bf9\u89c2\u6d4b\u566a\u58f0\u3001\u4f20\u611f\u5668\u8bef\u5dee\u548c\u6267\u884c\u6270\u52a8\u7b49\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u800c\u73b0\u6709\u7684RL\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5956\u52b1\u6700\u5927\u5316\uff0c\u5ffd\u89c6\u4e86\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faRobustVLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u9c81\u68d2\u6027\u5206\u6790\u8bc6\u522b\u51fa\u4e24\u4e2a\u5173\u952e\u6b63\u5219\u5316\uff1a\u96c5\u53ef\u6bd4\u6b63\u5219\u5316\uff08\u51cf\u8f7b\u89c2\u6d4b\u566a\u58f0\u654f\u611f\u6027\uff09\u548c\u5e73\u6ed1\u6b63\u5219\u5316\uff08\u5728\u52a8\u4f5c\u6270\u52a8\u4e0b\u7a33\u5b9a\u7b56\u7565\uff09\u3002", "result": "\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRobustVLA\u5728\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u539f\u5219\u6027\u7684\u9c81\u68d2\u6027\u611f\u77e5RL\u540e\u8bad\u7ec3\u662f\u63d0\u9ad8VLA\u6a21\u578b\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u7684\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2511.01334", "categories": ["cs.RO", "cs.AI", "cs.HC", "68T45"], "pdf": "https://arxiv.org/pdf/2511.01334", "abs": "https://arxiv.org/abs/2511.01334", "authors": ["Ling Niu", "Xiaoji Zheng", "Han Wang", "Chen Zheng", "Ziyuan Yang", "Bokui Chen", "Jiangtao Gong"], "title": "Embodied Cognition Augmented End2End Autonomous Driving", "comment": "24 pages,4 pages", "summary": "In recent years, vision-based end-to-end autonomous driving has emerged as a\nnew paradigm. However, popular end-to-end approaches typically rely on visual\nfeature extraction networks trained under label supervision. This limited\nsupervision framework restricts the generality and applicability of driving\nmodels. In this paper, we propose a novel paradigm termed $E^{3}AD$, which\nadvocates for comparative learning between visual feature extraction networks\nand the general EEG large model, in order to learn latent human driving\ncognition for enhancing end-to-end planning. In this work, we collected a\ncognitive dataset for the mentioned contrastive learning process. Subsequently,\nwe investigated the methods and potential mechanisms for enhancing end-to-end\nplanning with human driving cognition, using popular driving models as\nbaselines on publicly available autonomous driving datasets. Both open-loop and\nclosed-loop tests are conducted for a comprehensive evaluation of planning\nperformance. Experimental results demonstrate that the $E^{3}AD$ paradigm\nsignificantly enhances the end-to-end planning performance of baseline models.\nAblation studies further validate the contribution of driving cognition and the\neffectiveness of comparative learning process. To the best of our knowledge,\nthis is the first work to integrate human driving cognition for improving\nend-to-end autonomous driving planning. It represents an initial attempt to\nincorporate embodied cognitive data into end-to-end autonomous driving,\nproviding valuable insights for future brain-inspired autonomous driving\nsystems. Our code will be made available at Github", "AI": {"tldr": "\u63d0\u51faE\u00b3AD\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u4e0eEEG\u5927\u6a21\u578b\u7684\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u4eba\u7c7b\u9a7e\u9a76\u8ba4\u77e5\uff0c\u4ee5\u589e\u5f3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6027\u80fd", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4f9d\u8d56\u6807\u7b7e\u76d1\u7763\u8bad\u7ec3\u7684\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u8fd9\u79cd\u6709\u9650\u76d1\u7763\u6846\u67b6\u9650\u5236\u4e86\u9a7e\u9a76\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u9002\u7528\u6027", "method": "\u6536\u96c6\u8ba4\u77e5\u6570\u636e\u96c6\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7814\u7a76\u7528\u4eba\u7c7b\u9a7e\u9a76\u8ba4\u77e5\u589e\u5f3a\u7aef\u5230\u7aef\u89c4\u5212\u7684\u65b9\u6cd5\u548c\u673a\u5236\uff0c\u5728\u516c\u5f00\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u6d41\u884c\u9a7e\u9a76\u6a21\u578b\u4f5c\u4e3a\u57fa\u7ebf", "result": "E\u00b3AD\u8303\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u6a21\u578b\u7684\u7aef\u5230\u7aef\u89c4\u5212\u6027\u80fd\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u9a7e\u9a76\u8ba4\u77e5\u7684\u8d21\u732e\u548c\u5bf9\u6bd4\u5b66\u4e60\u8fc7\u7a0b\u7684\u6709\u6548\u6027", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c06\u4eba\u7c7b\u9a7e\u9a76\u8ba4\u77e5\u6574\u5408\u5230\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u5de5\u4f5c\uff0c\u4e3a\u672a\u6765\u8111\u542f\u53d1\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3"}}
{"id": "2511.01346", "categories": ["cs.RO", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2511.01346", "abs": "https://arxiv.org/abs/2511.01346", "authors": ["Shun Yoshida", "Qingchuan Song", "Bastian E. Rapp", "Thomas Speck", "Falk J. Tauber"], "title": "Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers", "comment": "Conference Proceedings Paper Living Machines 2025", "summary": "Despite their often perceived static and slow nature, some plants can move\nfaster than the blink of an eye. The rapid snap closure motion of the Venus\nflytrap (Dionaea muscipula) has long captivated the interest of researchers and\nengineers alike, serving as a model for plant-inspired soft machines and\nrobots. The translation of the fast snapping closure has inspired the\ndevelopment of various artificial Venus flytrap (AVF) systems. However,\ntranslating both the closing and reopening motion of D. muscipula into an\nautonomous plant inspired soft machine has yet to be achieved. In this study,\nwe present an AVF that autonomously closes and reopens, utilizing novel\nthermo-responsive UV-curable shape memory materials for soft robotic systems.\nThe life-sized thermo-responsive AVF exhibits closing and reopening motions\ntriggered in a naturally occurring temperature range. The doubly curved trap\nlobes, built from shape memory polymers, close at 38{\\deg}C, while reopening\ninitiates around 45{\\deg}C, employing shape memory elastomer strips as\nantagonistic actuators to facilitate lobe reopening. This work represents the\nfirst demonstration of thermo-responsive closing and reopening in an AVF with\nprogrammed sequential motion in response to increasing temperature. This\napproach marks the next step toward autonomously bidirectional moving soft\nmachines/robots.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u9996\u4e2a\u80fd\u591f\u81ea\u4e3b\u95ed\u5408\u548c\u91cd\u65b0\u5f20\u5f00\u7684\u4eba\u9020\u6355\u8747\u8349\u7cfb\u7edf\uff0c\u4f7f\u7528\u70ed\u54cd\u5e94\u5f62\u72b6\u8bb0\u5fc6\u6750\u6599\u5b9e\u73b0\u53cc\u5411\u8fd0\u52a8\u3002", "motivation": "\u867d\u7136\u6355\u8747\u8349\u7684\u5feb\u901f\u95ed\u5408\u8fd0\u52a8\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5c06\u5176\u95ed\u5408\u548c\u91cd\u65b0\u5f20\u5f00\u4e24\u79cd\u8fd0\u52a8\u90fd\u8f6c\u5316\u4e3a\u81ea\u4e3b\u7684\u690d\u7269\u542f\u53d1\u8f6f\u673a\u5668\u5c1a\u672a\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528\u65b0\u578b\u70ed\u54cd\u5e94UV\u56fa\u5316\u5f62\u72b6\u8bb0\u5fc6\u6750\u6599\u6784\u5efa\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f62\u72b6\u8bb0\u5fc6\u805a\u5408\u7269\u6784\u5efa\u53cc\u66f2\u9677\u9631\u53f6\u7247\uff0c\u572838\u00b0C\u65f6\u95ed\u5408\uff0c\u4f7f\u7528\u5f62\u72b6\u8bb0\u5fc6\u5f39\u6027\u4f53\u6761\u4f5c\u4e3a\u62ee\u6297\u9a71\u52a8\u5668\u572845\u00b0C\u65f6\u91cd\u65b0\u5f20\u5f00\u3002", "result": "\u6210\u529f\u5236\u9020\u51fa\u4e0e\u771f\u5b9e\u6355\u8747\u8349\u5927\u5c0f\u76f8\u5f53\u7684\u70ed\u54cd\u5e94\u4eba\u9020\u6355\u8747\u8349\uff0c\u80fd\u591f\u5728\u81ea\u7136\u6e29\u5ea6\u8303\u56f4\u5185\u89e6\u53d1\u95ed\u5408\u548c\u91cd\u65b0\u5f20\u5f00\u8fd0\u52a8\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c55\u793a\u70ed\u54cd\u5e94\u95ed\u5408\u548c\u91cd\u65b0\u5f20\u5f00\u529f\u80fd\u7684\u4eba\u9020\u6355\u8747\u8349\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6e29\u5ea6\u5347\u9ad8\u7684\u7a0b\u5e8f\u5316\u987a\u5e8f\u8fd0\u52a8\uff0c\u6807\u5fd7\u7740\u81ea\u4e3b\u53cc\u5411\u79fb\u52a8\u8f6f\u673a\u5668\u53d1\u5c55\u7684\u65b0\u8fdb\u5c55\u3002"}}
{"id": "2511.01347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01347", "abs": "https://arxiv.org/abs/2511.01347", "authors": ["Riddhi Das", "Joscha Teichmann", "Thomas Speck", "Falk J. Tauber"], "title": "Design and development of an electronics-free earthworm robot", "comment": "Conference Proceedings Paper Living Machines 2025", "summary": "Soft robotic systems have gained widespread attention due to their inherent\nflexibility, adaptability, and safety, making them well-suited for varied\napplications. Among bioinspired designs, earthworm locomotion has been\nextensively studied for its efficient peristaltic motion, enabling movement in\nconfined and unstructured environments. Existing earthworm-inspired robots\nprimarily utilize pneumatic actuation due to its high force-to-weight ratio and\nease of implementation. However, these systems often rely on bulky,\npower-intensive electronic control units, limiting their practicality. In this\nwork, we present an electronics-free, earthworm-inspired pneumatic robot\nutilizing a modified Pneumatic Logic Gate (PLG) design. By integrating\npreconfigured PLG units with bellow actuators, we achieved a plug-and-play\nstyle modular system capable of peristaltic locomotion without external\nelectronic components. The proposed design reduces system complexity while\nmaintaining efficient actuation. We characterize the bellow actuators under\ndifferent operating conditions and evaluate the robots locomotion performance.\nOur findings demonstrate that the modified PLG-based control system effectively\ngenerates peristaltic wave propagation, achieving autonomous motion with\nminimal deviation. This study serves as a proof of concept for the development\nof electronics-free, peristaltic soft robots. The proposed system has potential\nfor applications in hazardous environments, where untethered, adaptable\nlocomotion is critical. Future work will focus on further optimizing the robot\ndesign and exploring untethered operation using onboard compressed air sources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u7535\u5b50\u5143\u4ef6\u7684\u4eff\u86af\u8693\u6c14\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u6c14\u52a8\u903b\u8f91\u95e8\u8bbe\u8ba1\u5b9e\u73b0\u8815\u52a8\u8fd0\u52a8\uff0c\u65e0\u9700\u5916\u90e8\u7535\u5b50\u63a7\u5236\u5355\u5143\u3002", "motivation": "\u73b0\u6709\u4eff\u86af\u8693\u673a\u5668\u4eba\u4e3b\u8981\u4f9d\u8d56\u6c14\u52a8\u9a71\u52a8\uff0c\u4f46\u9700\u8981\u7b28\u91cd\u3001\u9ad8\u529f\u8017\u7684\u7535\u5b50\u63a7\u5236\u5355\u5143\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u65e0\u7535\u5b50\u63a7\u5236\u7684\u6c14\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u5c06\u9884\u914d\u7f6e\u7684\u6c14\u52a8\u903b\u8f91\u95e8\u5355\u5143\u4e0e\u6ce2\u7eb9\u7ba1\u6267\u884c\u5668\u96c6\u6210\uff0c\u6784\u5efa\u5373\u63d2\u5373\u7528\u5f0f\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdb\u7684PLG\u8bbe\u8ba1\u5b9e\u73b0\u8815\u52a8\u8fd0\u52a8\u63a7\u5236\u3002", "result": "\u6539\u8fdb\u7684PLG\u63a7\u5236\u7cfb\u7edf\u6709\u6548\u4ea7\u751f\u8815\u52a8\u6ce2\u4f20\u64ad\uff0c\u5b9e\u73b0\u81ea\u4e3b\u8fd0\u52a8\u4e14\u504f\u5dee\u6700\u5c0f\uff0c\u7cfb\u7edf\u590d\u6742\u5ea6\u964d\u4f4e\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u9a71\u52a8\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u65e0\u7535\u5b50\u5143\u4ef6\u7684\u8815\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5728\u5371\u9669\u73af\u5883\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u4f18\u5316\u8bbe\u8ba1\u5e76\u63a2\u7d22\u4f7f\u7528\u673a\u8f7d\u538b\u7f29\u7a7a\u6c14\u6e90\u7684\u65e0\u7ebf\u64cd\u4f5c\u3002"}}
{"id": "2511.01350", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01350", "abs": "https://arxiv.org/abs/2511.01350", "authors": ["Maartje H. M. Wermelink", "Renate Sachse", "Sebastian Kruppert", "Thomas Speck", "Falk J. Tauber"], "title": "Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator", "comment": "Conference Proceedings Paper Living machines 2025", "summary": "The Venus flytrap (Dionaea muscipula) does not only serve as the textbook\nmodel for a carnivorous plant, but also has long intrigued both botanists and\nengineers with its rapidly closing leaf trap. The trap closure is triggered by\ntwo consecutive touches of a potential prey, after which the lobes rapidly\nswitch from their concave open-state to their convex close-state and catch the\nprey within 100-500 ms after being triggered. This transformation from concave\nto convex is initiated by changes in turgor pressure and the release of stored\nelastic energy from prestresses in the concave state, which accelerate this\nmovement, leading to inversion of the lobes bi-axial curvature. Possessing two\nlow-energy states, the leaves can be characterized as bistable systems. With\nour research, we seek to deepen the understanding of Venus flytrap motion\nmechanics and apply its principles to the design of an artificial bistable lobe\nactuator. We identified geometrical characteristics, such as dimensional ratios\nand the thickness gradient in the lobe, and transferred these to two 3D-printed\nbistable actuator models. One actuator parallels the simulated geometry of a\nVenus flytrap leaf, the other is a lobe model designed with CAD. Both models\ndisplay concave-convex bi-stability and snap close. These demonstrators are the\nfirst step in the development of an artificial Venus flytrap that mimics the\nmechanical behavior of the biological model and can be used as a soft fast\ngripper.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u6355\u8747\u8349\u7684\u5feb\u901f\u95ed\u5408\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u5176\u51e0\u4f55\u7279\u5f81\u548c\u53cc\u7a33\u6001\u7279\u6027\u8bbe\u8ba1\u4e86\u4e24\u79cd3D\u6253\u5370\u7684\u53cc\u7a33\u6001\u6267\u884c\u5668\u6a21\u578b\u3002", "motivation": "\u6df1\u5165\u7406\u89e3\u6355\u8747\u8349\u8fd0\u52a8\u529b\u5b66\uff0c\u5e76\u5c06\u5176\u539f\u7406\u5e94\u7528\u4e8e\u4eba\u5de5\u53cc\u7a33\u6001\u53f6\u7247\u6267\u884c\u5668\u7684\u8bbe\u8ba1\uff0c\u5f00\u53d1\u53ef\u4f5c\u4e3a\u8f6f\u5feb\u901f\u6293\u53d6\u5668\u7684\u4eba\u5de5\u6355\u8747\u8349\u3002", "method": "\u8bc6\u522b\u6355\u8747\u8349\u53f6\u7247\u7684\u51e0\u4f55\u7279\u5f81\uff08\u5c3a\u5bf8\u6bd4\u4f8b\u3001\u539a\u5ea6\u68af\u5ea6\uff09\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7279\u5f81\u8f6c\u79fb\u5230\u4e24\u79cd3D\u6253\u5370\u7684\u53cc\u7a33\u6001\u6267\u884c\u5668\u6a21\u578b\u4e2d\uff1a\u4e00\u79cd\u6a21\u62df\u6355\u8747\u8349\u53f6\u7247\u51e0\u4f55\u5f62\u72b6\uff0c\u53e6\u4e00\u79cd\u4f7f\u7528CAD\u8bbe\u8ba1\u7684\u53f6\u7247\u6a21\u578b\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u51f9-\u51f8\u53cc\u7a33\u6001\u7279\u6027\u5e76\u80fd\u5feb\u901f\u95ed\u5408\uff0c\u8fd9\u662f\u5f00\u53d1\u4eba\u5de5\u6355\u8747\u8349\u7684\u7b2c\u4e00\u6b65\u3002", "conclusion": "\u6210\u529f\u5c06\u6355\u8747\u8349\u7684\u53cc\u7a33\u6001\u673a\u5236\u5e94\u7528\u4e8e\u4eba\u5de5\u6267\u884c\u5668\u8bbe\u8ba1\uff0c\u4e3a\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u751f\u7269\u6a21\u578b\u673a\u68b0\u884c\u4e3a\u7684\u4eba\u5de5\u6355\u8747\u8349\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.01369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01369", "abs": "https://arxiv.org/abs/2511.01369", "authors": ["Luis Diener", "Jens Kalkkuhl", "Markus Enzweiler"], "title": "Lateral Velocity Model for Vehicle Parking Applications", "comment": "This manuscript has been submitted to Vehicle System Dynamics for\n  possible publication", "summary": "Automated parking requires accurate localization for quick and precise\nmaneuvering in tight spaces. While the longitudinal velocity can be measured\nusing wheel encoders, the estimation of the lateral velocity remains a key\nchallenge due to the absence of dedicated sensors in consumer-grade vehicles.\nExisting approaches often rely on simplified vehicle models, such as the\nzero-slip model, which assumes no lateral velocity at the rear axle. It is well\nestablished that this assumption does not hold during low-speed driving and\nresearchers thus introduce additional heuristics to account for differences. In\nthis work, we analyze real-world data from parking scenarios and identify a\nsystematic deviation from the zero-slip assumption. We provide explanations for\nthe observed effects and then propose a lateral velocity model that better\ncaptures the lateral dynamics of the vehicle during parking. The model improves\nestimation accuracy, while relying on only two parameters, making it\nwell-suited for integration into consumer-grade applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4fa7\u5411\u901f\u5ea6\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u6cca\u8f66\u4e2d\u4fa7\u5411\u901f\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u8be5\u6a21\u578b\u4ec5\u9700\u4e24\u4e2a\u53c2\u6570\uff0c\u66f4\u9002\u5408\u6d88\u8d39\u7ea7\u8f66\u8f86\u5e94\u7528\u3002", "motivation": "\u81ea\u52a8\u6cca\u8f66\u9700\u8981\u7cbe\u786e\u7684\u5b9a\u4f4d\uff0c\u4f46\u6d88\u8d39\u7ea7\u8f66\u8f86\u7f3a\u4e4f\u4e13\u7528\u4f20\u611f\u5668\u6765\u6d4b\u91cf\u4fa7\u5411\u901f\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u96f6\u6ed1\u79fb\u5047\u8bbe\uff0c\u4f46\u5728\u4f4e\u901f\u9a7e\u9a76\u65f6\u8be5\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002", "method": "\u5206\u6790\u771f\u5b9e\u6cca\u8f66\u573a\u666f\u6570\u636e\uff0c\u8bc6\u522b\u96f6\u6ed1\u79fb\u5047\u8bbe\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u63d0\u51fa\u4e00\u4e2a\u4ec5\u9700\u4e24\u4e2a\u53c2\u6570\u7684\u6539\u8fdb\u4fa7\u5411\u901f\u5ea6\u6a21\u578b\u6765\u66f4\u597d\u5730\u6355\u6349\u8f66\u8f86\u5728\u6cca\u8f66\u65f6\u7684\u4fa7\u5411\u52a8\u529b\u5b66\u3002", "result": "\u65b0\u6a21\u578b\u63d0\u9ad8\u4e86\u4fa7\u5411\u901f\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b80\u5355\u6027\uff0c\u9002\u5408\u96c6\u6210\u5230\u6d88\u8d39\u7ea7\u5e94\u7528\u4e2d\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fa7\u5411\u901f\u5ea6\u6a21\u578b\u5728\u6cca\u8f66\u573a\u666f\u4e2d\u6bd4\u4f20\u7edf\u96f6\u6ed1\u79fb\u6a21\u578b\u66f4\u51c6\u786e\uff0c\u4e14\u53c2\u6570\u5c11\uff0c\u6613\u4e8e\u5728\u6d88\u8d39\u7ea7\u8f66\u8f86\u4e2d\u5b9e\u73b0\u3002"}}
{"id": "2511.01379", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01379", "abs": "https://arxiv.org/abs/2511.01379", "authors": ["Kun Hu", "Menggang Li", "Zhiwen Jin", "Chaoquan Tang", "Eryi Hu", "Gongbo Zhou"], "title": "CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels", "comment": "Accepted by IROS 2025", "summary": "Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and\nGPS-denied underground coal mine environments presents significant challenges.\nSensors must contend with abnormal operating conditions: GPS unavailability\nimpedes scene reconstruction and absolute geographic referencing, uneven or\nslippery terrain degrades wheel odometer accuracy, and long, feature-poor\ntunnels reduce LiDAR effectiveness. To address these issues, we propose\nCoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM\nframework based on the Iterated Error-State Kalman Filter (IESKF). First,\nLiDAR-inertial odometry is tightly fused with UWB absolute positioning\nconstraints to align the SLAM system with a global coordinate. Next, wheel\nodometer is integrated through tight coupling, enhanced by nonholonomic\nconstraints (NHC) and vehicle lever arm compensation, to address performance\ndegradation in areas beyond UWB measurement range. Finally, an adaptive motion\nmode switching mechanism dynamically adjusts the robot's motion mode based on\nUWB measurement range and environmental degradation levels. Experimental\nresults validate that our method achieves superior accuracy and robustness in\nreal-world underground coal mine scenarios, outperforming state-of-the-art\napproaches. We open source our code of this work on Github to benefit the\nrobotics community.", "AI": {"tldr": "\u63d0\u51faCM-LIUW-Odometry\u591a\u6a21\u6001SLAM\u6846\u67b6\uff0c\u878d\u5408LiDAR-IMU-UWB-\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\uff0c\u7528\u4e8eGPS\u7f3a\u5931\u7684\u590d\u6742\u5730\u4e0b\u7164\u77ff\u73af\u5883\u5b9a\u4f4d\u4e0e\u5efa\u56fe", "motivation": "\u89e3\u51b3\u5730\u4e0b\u7164\u77ff\u73af\u5883\u4e2dGPS\u4e0d\u53ef\u7528\u3001\u5730\u5f62\u4e0d\u5e73\u5bfc\u81f4\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\u4e0b\u964d\u3001\u957f\u96a7\u9053\u7279\u5f81\u7a00\u5c11\u964d\u4f4eLiDAR\u6548\u679c\u7b49\u6311\u6218", "method": "\u57fa\u4e8eIESKF\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff1aLiDAR-IMU\u91cc\u7a0b\u8ba1\u4e0eUWB\u7edd\u5bf9\u5b9a\u4f4d\u7d27\u8026\u5408\uff1b\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u7d27\u8026\u5408\u96c6\u6210\uff0c\u8f85\u4ee5\u975e\u5b8c\u6574\u7ea6\u675f\u548c\u8f66\u8f86\u6760\u6746\u81c2\u8865\u507f\uff1b\u81ea\u9002\u5e94\u8fd0\u52a8\u6a21\u5f0f\u5207\u6362\u673a\u5236", "result": "\u5728\u771f\u5b9e\u5730\u4e0b\u7164\u77ff\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001SLAM\u6846\u67b6\u5728\u5730\u4e0b\u7164\u77ff\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2511.01383", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01383", "abs": "https://arxiv.org/abs/2511.01383", "authors": ["Landson Guo", "Andres M. Diaz Aguilar", "William Talbot", "Turcan Tuna", "Marco Hutter", "Cesar Cadena"], "title": "CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation", "comment": null, "summary": "Accurate point-wise velocity estimation in 3D is crucial for robot\ninteraction with non-rigid, dynamic agents, such as humans, enabling robust\nperformance in path planning, collision avoidance, and object manipulation in\ndynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,\nand camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.\nThis pipeline leverages raw RADAR measurements to create a novel RADAR\nrepresentation, the velocity cube, which densely represents radial velocities\nwithin the RADAR's field-of-view. By combining the velocity cube for radial\nvelocity extraction, optical flow for tangential velocity estimation, and LiDAR\nfor point-wise range measurements through a closed-form solution, our approach\ncan produce 3D velocity estimates for a dense array of points. Developed as an\nopen-source ROS2 package, CaRLi-V has been field-tested against a custom\ndataset and proven to produce low velocity error metrics relative to ground\ntruth, enabling point-wise velocity estimation for robotic applications.", "AI": {"tldr": "\u63d0\u51fa\u540d\u4e3aCaRLi-V\u7684\u591a\u4f20\u611f\u5668\u878d\u5408\u7ba1\u9053\uff0c\u901a\u8fc7\u7ed3\u5408RADAR\u3001LiDAR\u548c\u76f8\u673a\u6570\u636e\uff0c\u5b9e\u73b0\u5bc6\u96c6\u70b9\u4e91\u76843D\u901f\u5ea6\u4f30\u8ba1\uff0c\u7279\u522b\u9002\u7528\u4e8e\u673a\u5668\u4eba\u4e0e\u975e\u521a\u6027\u52a8\u6001\u7269\u4f53\uff08\u5982\u4eba\u7c7b\uff09\u7684\u4ea4\u4e92\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u51c6\u786e\u76843D\u70b9\u901f\u5ea6\u4f30\u8ba1\u5bf9\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\u3001\u78b0\u649e\u907f\u514d\u548c\u7269\u4f53\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u4e0e\u975e\u521a\u6027\u52a8\u6001\u4ee3\u7406\uff08\u5982\u4eba\u7c7b\uff09\u4ea4\u4e92\u65f6\u3002", "method": "\u4f7f\u7528\u539f\u59cbRADAR\u6d4b\u91cf\u521b\u5efa\u901f\u5ea6\u7acb\u65b9\u4f53\u8868\u793a\u5f84\u5411\u901f\u5ea6\uff0c\u7ed3\u5408\u5149\u6d41\u4f30\u8ba1\u5207\u5411\u901f\u5ea6\uff0c\u901a\u8fc7LiDAR\u83b7\u53d6\u70b9\u4e91\u8ddd\u79bb\u6d4b\u91cf\uff0c\u91c7\u7528\u95ed\u5f0f\u89e3\u878d\u5408\u8fd9\u4e9b\u4fe1\u606f\u3002", "result": "CaRLi-V\u4f5c\u4e3a\u5f00\u6e90ROS2\u5305\uff0c\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u663e\u793a\u76f8\u5bf9\u4e8e\u771f\u5b9e\u503c\u5177\u6709\u8f83\u4f4e\u7684\u901f\u5ea6\u8bef\u5dee\u6307\u6807\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u5bc6\u96c6\u70b9\u4e91\u751f\u62103D\u901f\u5ea6\u4f30\u8ba1\uff0c\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u6709\u6548\u7684\u70b9\u901f\u5ea6\u4f30\u8ba1\u80fd\u529b\u3002"}}
{"id": "2511.01407", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01407", "abs": "https://arxiv.org/abs/2511.01407", "authors": ["Paolo Rabino", "Gabriele Tiboni", "Tatiana Tommasi"], "title": "FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths", "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Object-Centric Motion Generation (OCMG) is instrumental in advancing\nautomated manufacturing processes, particularly in domains requiring\nhigh-precision expert robotic motions, such as spray painting and welding. To\nrealize effective automation, robust algorithms are essential for generating\nextended, object-aware trajectories across intricate 3D geometries. However,\ncontemporary OCMG techniques are either based on ad-hoc heuristics or employ\nlearning-based pipelines that are still reliant on sensitive post-processing\nsteps to generate executable paths. We introduce FoldPath, a novel, end-to-end,\nneural field based method for OCMG. Unlike prior deep learning approaches that\npredict discrete sequences of end-effector waypoints, FoldPath learns the robot\nmotion as a continuous function, thus implicitly encoding smooth output paths.\nThis paradigm shift eliminates the need for brittle post-processing steps that\nconcatenate and order the predicted discrete waypoints. Particularly, our\napproach demonstrates superior predictive performance compared to recently\nproposed learning-based methods, and attains generalization capabilities even\nin real industrial settings, where only a limited amount of 70 expert samples\nare provided. We validate FoldPath through comprehensive experiments in a\nrealistic simulation environment and introduce new, rigorous metrics designed\nto comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG\ntask towards practical maturity.", "AI": {"tldr": "FoldPath\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u7aef\u5230\u7aef\u5bf9\u8c61\u4e2d\u5fc3\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u51fd\u6570\u5b66\u4e60\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u65e0\u9700\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u5728\u4ec5\u670970\u4e2a\u4e13\u5bb6\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5de5\u4e1a\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5bf9\u8c61\u4e2d\u5fc3\u8fd0\u52a8\u751f\u6210\u6280\u672f\u8981\u4e48\u57fa\u4e8e\u4e34\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8981\u4e48\u4f9d\u8d56\u654f\u611f\u540e\u5904\u7406\u6b65\u9aa4\u7684\u5b66\u4e60\u6d41\u7a0b\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u7b97\u6cd5\u6765\u751f\u6210\u590d\u67423D\u51e0\u4f55\u4e0a\u7684\u6269\u5c55\u3001\u5bf9\u8c61\u611f\u77e5\u8f68\u8ff9\u3002", "method": "\u63d0\u51faFoldPath\u65b9\u6cd5\uff0c\u4f7f\u7528\u795e\u7ecf\u573a\u5c06\u673a\u5668\u4eba\u8fd0\u52a8\u5efa\u6a21\u4e3a\u8fde\u7eed\u51fd\u6570\uff0c\u9690\u5f0f\u7f16\u7801\u5e73\u6ed1\u8f93\u51fa\u8def\u5f84\uff0c\u907f\u514d\u4e86\u79bb\u6563\u8def\u5f84\u70b9\u9884\u6d4b\u548c\u62fc\u63a5\u6392\u5e8f\u7684\u540e\u5904\u7406\u6b65\u9aa4\u3002", "result": "FoldPath\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u8fd1\u63d0\u51fa\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u4ec5\u752870\u4e2a\u4e13\u5bb6\u6837\u672c\u5c31\u5c55\u73b0\u51fa\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u4eff\u771f\u73af\u5883\u9a8c\u8bc1\u548c\u65b0\u7684\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u8def\u5f84\u8bc4\u4f30\u6307\u6807\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "FoldPath\u901a\u8fc7\u8fde\u7eed\u51fd\u6570\u5b66\u4e60\u8303\u5f0f\u6d88\u9664\u4e86\u8106\u5f31\u7684\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u63a8\u8fdb\u4e86\u5bf9\u8c61\u4e2d\u5fc3\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u5411\u5b9e\u9645\u5e94\u7528\u7684\u6210\u719f\u53d1\u5c55\u3002"}}
{"id": "2511.01437", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01437", "abs": "https://arxiv.org/abs/2511.01437", "authors": ["Elian Neppel", "Shamistan Karimov", "Ashutosh Mishra", "Gustavo Hernan Diaz Huenupan", "Hazal Gozbasi", "Kentaro Uno", "Shreya Santra", "Kazuya Yoshida"], "title": "Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots", "comment": "6 pages, 8 figures. Accepted at ISPARO 2025", "summary": "This paper presents the software architecture and deployment strategy behind\nthe MoonBot platform: a modular space robotic system composed of heterogeneous\ncomponents distributed across multiple computers, networks and ultimately\ncelestial bodies. We introduce a principled approach to distributed,\nheterogeneous modularity, extending modular robotics beyond physical\nreconfiguration to software, communication and orchestration. We detail the\narchitecture of our system that integrates component-based design, a\ndata-oriented communication model using ROS2 and Zenoh, and a deployment\norchestrator capable of managing complex multi-module assemblies. These\nabstractions enable dynamic reconfiguration, decentralized control, and\nseamless collaboration between numerous operators and modules. At the heart of\nthis system lies our open-source Motion Stack software, validated by months of\nfield deployment with self-assembling robots, inter-robot cooperation, and\nremote operation. Our architecture tackles the significant hurdles of modular\nrobotics by significantly reducing integration and maintenance overhead, while\nremaining scalable and robust. Although tested with space in mind, we propose\ngeneralizable patterns for designing robotic systems that must scale across\ntime, hardware, teams and operational environments.", "AI": {"tldr": "MoonBot\u5e73\u53f0\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7a7a\u95f4\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u5f02\u6784\u7ec4\u4ef6\u67b6\u6784\uff0c\u901a\u8fc7ROS2\u548cZenoh\u5b9e\u73b0\u6570\u636e\u5bfc\u5411\u901a\u4fe1\uff0c\u652f\u6301\u52a8\u6001\u91cd\u6784\u548c\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\uff0c\u5df2\u5728\u5b9e\u5730\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u96c6\u6210\u548c\u7ef4\u62a4\u65b9\u9762\u7684\u91cd\u5927\u6311\u6218\uff0c\u5c06\u6a21\u5757\u5316\u6982\u5ff5\u4ece\u7269\u7406\u91cd\u6784\u6269\u5c55\u5230\u8f6f\u4ef6\u3001\u901a\u4fe1\u548c\u7f16\u6392\u5c42\u9762\uff0c\u6784\u5efa\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "method": "\u91c7\u7528\u7ec4\u4ef6\u5316\u8bbe\u8ba1\u3001\u57fa\u4e8eROS2\u548cZenoh\u7684\u6570\u636e\u5bfc\u5411\u901a\u4fe1\u6a21\u578b\uff0c\u4ee5\u53ca\u80fd\u591f\u7ba1\u7406\u590d\u6742\u591a\u6a21\u5757\u7ec4\u4ef6\u7684\u90e8\u7f72\u7f16\u6392\u5668\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u5f02\u6784\u6a21\u5757\u5316\u3002", "result": "\u5f00\u53d1\u4e86\u5f00\u6e90Motion Stack\u8f6f\u4ef6\uff0c\u7ecf\u8fc7\u6570\u6708\u7684\u5b9e\u5730\u90e8\u7f72\u9a8c\u8bc1\uff0c\u652f\u6301\u673a\u5668\u4eba\u81ea\u7ec4\u88c5\u3001\u673a\u5668\u4eba\u95f4\u534f\u4f5c\u548c\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u96c6\u6210\u548c\u7ef4\u62a4\u5f00\u9500\u3002", "conclusion": "\u867d\u7136\u4ee5\u592a\u7a7a\u5e94\u7528\u4e3a\u76ee\u6807\uff0c\u4f46\u63d0\u51fa\u4e86\u53ef\u63a8\u5e7f\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u9700\u8981\u8de8\u65f6\u95f4\u3001\u786c\u4ef6\u3001\u56e2\u961f\u548c\u64cd\u4f5c\u73af\u5883\u6269\u5c55\u7684\u7cfb\u7edf\u3002"}}
{"id": "2511.01472", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01472", "abs": "https://arxiv.org/abs/2511.01472", "authors": ["Sarthak Mishra", "Rishabh Dev Yadav", "Avirup Das", "Saksham Gupta", "Wei Pan", "Spandan Roy"], "title": "AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models", "comment": null, "summary": "The rapid progress of vision--language models (VLMs) has sparked growing\ninterest in robotic control, where natural language can express the operation\ngoals while visual feedback links perception to action. However, directly\ndeploying VLM-driven policies on aerial manipulators remains unsafe and\nunreliable since the generated actions are often inconsistent,\nhallucination-prone, and dynamically infeasible for flight. In this work, we\npresent AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial\nmanipulation by separating high-level reasoning from low-level control, without\nany task-specific fine-tuning. Our framework encodes natural language\ninstructions, task context, and safety constraints into a structured prompt\nthat guides the model to generate a step-by-step reasoning trace in natural\nlanguage. This reasoning output is used to select from a predefined library of\ndiscrete, flight-safe skills, ensuring interpretable and temporally consistent\nexecution. By decoupling symbolic reasoning from physical action, AERMANI-VLM\nmitigates hallucinated commands and prevents unsafe behavior, enabling robust\ntask completion. We validate the framework in both simulation and hardware on\ndiverse multi-step pick-and-place tasks, demonstrating strong generalization to\npreviously unseen commands, objects, and environments.", "AI": {"tldr": "AERMANI-VLM\u662f\u4e00\u4e2a\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9002\u914d\u5230\u7a7a\u4e2d\u673a\u68b0\u81c2\u63a7\u5236\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u5c42\u63a8\u7406\u548c\u4f4e\u5c42\u63a7\u5236\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u64cd\u4f5c\u3002", "motivation": "\u76f4\u63a5\u90e8\u7f72\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u7b56\u7565\u5230\u7a7a\u4e2d\u673a\u68b0\u81c2\u5b58\u5728\u4e0d\u5b89\u5168\u3001\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u751f\u6210\u7684\u52a8\u4f5c\u5f80\u5f80\u4e0d\u4e00\u81f4\u3001\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u4e14\u98de\u884c\u52a8\u529b\u5b66\u4e0d\u53ef\u884c\u3002", "method": "\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u4efb\u52a1\u4e0a\u4e0b\u6587\u548c\u5b89\u5168\u7ea6\u675f\u7f16\u7801\u5230\u7ed3\u6784\u5316\u63d0\u793a\u4e2d\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u4ece\u9884\u5b9a\u4e49\u7684\u5b89\u5168\u6280\u80fd\u5e93\u4e2d\u9009\u62e9\u79bb\u6563\u52a8\u4f5c\uff0c\u5b9e\u73b0\u7b26\u53f7\u63a8\u7406\u4e0e\u7269\u7406\u52a8\u4f5c\u7684\u89e3\u8026\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u591a\u6837\u5316\u591a\u6b65\u9aa4\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5bf9\u672a\u89c1\u6307\u4ee4\u3001\u7269\u4f53\u548c\u73af\u5883\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5206\u79bb\u63a8\u7406\u548c\u63a7\u5236\uff0cAERMANI-VLM\u80fd\u591f\u51cf\u8f7b\u5e7b\u89c9\u547d\u4ee4\u5e76\u9632\u6b62\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u4efb\u52a1\u5b8c\u6210\u3002"}}
{"id": "2511.01476", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01476", "abs": "https://arxiv.org/abs/2511.01476", "authors": ["Cankut Bora Tuncer", "Marc Toussaint", "Ozgur S. Oguz"], "title": "MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments", "comment": "8 pages, 8 figures, website:https://sites.google.com/view/mo-segman/", "summary": "In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided\nManipulation planner for highly constrained rearrangement problems. MO-SeGMan\ngenerates object placement sequences that minimize both replanning per object\nand robot travel distance while preserving critical dependency structures with\na lazy evaluation method. To address highly cluttered, non-monotone scenarios,\nwe propose a Selective Guided Forward Search (SGFS) that efficiently relocates\nonly critical obstacles and to feasible relocation points. Furthermore, we\nadopt a refinement method for adaptive subgoal selection to eliminate\nunnecessary pick-and-place actions, thereby improving overall solution quality.\nExtensive evaluations on nine benchmark rearrangement tasks demonstrate that\nMO-SeGMan generates feasible motion plans in all cases, consistently achieving\nfaster solution times and superior solution quality compared to the baselines.\nThese results highlight the robustness and scalability of the proposed\nframework for complex rearrangement planning problems.", "AI": {"tldr": "MO-SeGMan\u662f\u4e00\u4e2a\u591a\u76ee\u6807\u987a\u5e8f\u5f15\u5bfc\u64cd\u4f5c\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u5ea6\u53d7\u9650\u7684\u91cd\u6392\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u91cd\u65b0\u89c4\u5212\u548c\u673a\u5668\u4eba\u79fb\u52a8\u8ddd\u79bb\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u751f\u6210\u53ef\u884c\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5ea6\u53d7\u9650\u3001\u975e\u5355\u8c03\u7684\u91cd\u6392\u89c4\u5212\u95ee\u9898\uff0c\u9700\u8981\u5728\u4fdd\u6301\u5173\u952e\u4f9d\u8d56\u7ed3\u6784\u7684\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76ee\u6807\u3002", "method": "\u91c7\u7528\u9009\u62e9\u6027\u5f15\u5bfc\u524d\u5411\u641c\u7d22(SGFS)\u548c\u81ea\u9002\u5e94\u5b50\u76ee\u6807\u9009\u62e9\u7684\u7cbe\u70bc\u65b9\u6cd5\uff0c\u901a\u8fc7\u60f0\u6027\u8bc4\u4f30\u751f\u6210\u5bf9\u8c61\u653e\u7f6e\u5e8f\u5217\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u91cd\u6392\u4efb\u52a1\u4e2d\uff0cMO-SeGMan\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u751f\u6210\u4e86\u53ef\u884c\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u5feb\u7684\u6c42\u89e3\u65f6\u95f4\u548c\u66f4\u4f18\u7684\u89e3\u8d28\u91cf\u3002", "conclusion": "MO-SeGMan\u6846\u67b6\u5728\u5904\u7406\u590d\u6742\u91cd\u6392\u89c4\u5212\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.01493", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01493", "abs": "https://arxiv.org/abs/2511.01493", "authors": ["Wei Huang", "Jiaxin Li", "Zang Wan", "Huijun Di", "Wei Liang", "Zhu Yang"], "title": "Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues", "comment": null, "summary": "Guiding an agent to a specific target in indoor environments based solely on\nRGB inputs and a floor plan is a promising yet challenging problem. Although\nexisting methods have made significant progress, two challenges remain\nunresolved. First, the modality gap between egocentric RGB observations and the\nfloor plan hinders the integration of visual and spatial information for both\nlocal obstacle avoidance and global planning. Second, accurate localization is\ncritical for navigation performance, but remains challenging at deployment in\nunseen environments due to the lack of explicit geometric alignment between RGB\ninputs and floor plans. We propose a novel diffusion-based policy, denoted as\nGlocDiff, which integrates global path planning from the floor plan with local\ndepth-aware features derived from RGB observations. The floor plan offers\nexplicit global guidance, while the depth features provide implicit geometric\ncues, collectively enabling precise prediction of optimal navigation directions\nand robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation\nduring training to enhance robustness against pose estimation errors, and we\nfind that combining this with a relatively stable VO module during inference\nresults in significantly improved navigation performance. Extensive experiments\non the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in\nachieving superior navigation performance, and the success of real-world\ndeployments also highlights its potential for widespread practical\napplications.", "AI": {"tldr": "\u63d0\u51faGlocDiff\u6269\u6563\u7b56\u7565\uff0c\u7ed3\u5408\u697c\u5c42\u5e73\u9762\u56fe\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548cRGB\u89c2\u6d4b\u7684\u5c40\u90e8\u6df1\u5ea6\u7279\u5f81\uff0c\u89e3\u51b3\u5ba4\u5185\u5bfc\u822a\u4e2d\u89c6\u89c9\u4e0e\u7a7a\u95f4\u4fe1\u606f\u878d\u5408\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u6270\u52a8\u589e\u5f3a\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u8bef\u5dee\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5185\u5bfc\u822a\u4e2d\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u7b2c\u4e00\u4eba\u79f0RGB\u89c2\u6d4b\u4e0e\u697c\u5c42\u5e73\u9762\u56fe\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u963b\u788d\u89c6\u89c9\u548c\u7a7a\u95f4\u4fe1\u606f\u7684\u878d\u5408\uff1b2\uff09\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7531\u4e8eRGB\u8f93\u5165\u4e0e\u697c\u5c42\u5e73\u9762\u56fe\u7f3a\u4e4f\u663e\u5f0f\u51e0\u4f55\u5bf9\u9f50\uff0c\u5bfc\u81f4\u7cbe\u786e\u5b9a\u4f4d\u56f0\u96be\u3002", "method": "\u63d0\u51faGlocDiff\u6269\u6563\u7b56\u7565\uff0c\u96c6\u6210\u697c\u5c42\u5e73\u9762\u56fe\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548cRGB\u89c2\u6d4b\u7684\u5c40\u90e8\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u3002\u697c\u5c42\u5e73\u9762\u56fe\u63d0\u4f9b\u663e\u5f0f\u5168\u5c40\u5f15\u5bfc\uff0c\u6df1\u5ea6\u7279\u5f81\u63d0\u4f9b\u9690\u5f0f\u51e0\u4f55\u7ebf\u7d22\u3002\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u566a\u58f0\u6270\u52a8\u589e\u5f3a\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u8bef\u5dee\u7684\u9c81\u68d2\u6027\uff0c\u5728\u63a8\u7406\u65f6\u7ed3\u5408\u76f8\u5bf9\u7a33\u5b9a\u7684VO\u6a21\u5757\u3002", "result": "\u5728FloNa\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660eGlocDiff\u5728\u5b9e\u73b0\u4f18\u8d8a\u5bfc\u822a\u6027\u80fd\u65b9\u9762\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u6210\u529f\u4e5f\u7a81\u663e\u4e86\u5176\u5e7f\u6cdb\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002", "conclusion": "GlocDiff\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548c\u5c40\u90e8\u6df1\u5ea6\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5185\u5bfc\u822a\u4e2d\u7684\u6a21\u6001\u5dee\u8ddd\u548c\u5b9a\u4f4d\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u6270\u52a8\u548cVO\u6a21\u5757\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.01520", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01520", "abs": "https://arxiv.org/abs/2511.01520", "authors": ["Shipeng Lyu", "Lijie Sheng", "Fangyuan Wang", "Wenyao Zhang", "Weiwei Lin", "Zhenzhong Jia", "David Navarro-Alarcon", "Guodong Guo"], "title": "Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals", "comment": "9 papges, 10 figures, 3 tables", "summary": "Humans naturally grasp objects with minimal level required force for\nstability, whereas robots often rely on rigid, over-squeezing control. To\nnarrow this gap, we propose a human-inspired physics-conditioned tactile method\n(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,\ntactile prediction, and force regulation. A physics-based pose selector first\nidentifies feasible contact regions with optimal force distribution based on\nsurface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)\npredicts the tactile imprint under FOSG target. Last, a latent-space LQR\ncontroller drives the gripper toward this tactile imprint with minimal\nactuation, preventing unnecessary compression. Trained on a physics-conditioned\ntactile dataset covering diverse objects and contact conditions, the proposed\nPhy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac\noutperforms fixed-force and GraspNet-based baselines in grasp stability and\nforce efficiency. Experiments on classical robotic platforms demonstrate\nforce-efficient and adaptive manipulation that bridges the gap between robotic\nand human grasping.", "AI": {"tldr": "\u63d0\u51faPhy-Tac\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u6761\u4ef6\u89e6\u89c9\u6280\u672f\u5b9e\u73b0\u529b\u6700\u4f18\u7a33\u5b9a\u6293\u53d6\uff0c\u7ed3\u5408\u59ff\u6001\u9009\u62e9\u3001\u89e6\u89c9\u9884\u6d4b\u548c\u529b\u8c03\u8282\uff0c\u4f7f\u673a\u5668\u4eba\u50cf\u4eba\u7c7b\u4e00\u6837\u7528\u6700\u5c0f\u5fc5\u8981\u529b\u6293\u53d6\u7269\u4f53\u3002", "motivation": "\u4eba\u7c7b\u80fd\u81ea\u7136\u4f7f\u7528\u6700\u5c0f\u5fc5\u8981\u529b\u7a33\u5b9a\u6293\u53d6\u7269\u4f53\uff0c\u800c\u673a\u5668\u4eba\u901a\u5e38\u4f9d\u8d56\u521a\u6027\u3001\u8fc7\u5ea6\u6324\u538b\u7684\u63a7\u5236\u65b9\u5f0f\uff0c\u9700\u8981\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "1) \u57fa\u4e8e\u7269\u7406\u7684\u59ff\u6001\u9009\u62e9\u5668\u8bc6\u522b\u6700\u4f18\u529b\u5206\u5e03\u7684\u53ef\u884c\u63a5\u89e6\u533a\u57df\uff1b2) \u7269\u7406\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u9884\u6d4b\u76ee\u6807\u89e6\u89c9\u5370\u8bb0\uff1b3) \u6f5c\u5728\u7a7a\u95f4LQR\u63a7\u5236\u5668\u9a71\u52a8\u5939\u722a\u4ee5\u6700\u5c0f\u9a71\u52a8\u529b\u8fbe\u5230\u76ee\u6807\u89e6\u89c9\u5370\u8bb0\u3002", "result": "Phy-LDM\u5728\u89e6\u89c9\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPhy-Tac\u5728\u6293\u53d6\u7a33\u5b9a\u6027\u548c\u529b\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u56fa\u5b9a\u529b\u548cGraspNet\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u529b\u9ad8\u6548\u548c\u81ea\u9002\u5e94\u64cd\u4f5c\uff0c\u7f29\u5c0f\u4e86\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u6293\u53d6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.01594", "categories": ["cs.RO", "cs.CV", "I.2.9; I.2.11; I.2.6; I.4.8"], "pdf": "https://arxiv.org/pdf/2511.01594", "abs": "https://arxiv.org/abs/2511.01594", "authors": ["Renjun Gao", "Peiyan Zhong"], "title": "MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence", "comment": "3 figures, 1 table; under review at Multimedia Systems (Springer)", "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities\nin cross-modal understanding and reasoning, offering new opportunities for\nintelligent assistive systems, yet existing systems still struggle with\nrisk-aware planning, user personalization, and grounding language plans into\nexecutable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic\nSystem powered by MLLMs for assistive intelligence and designed for smart home\nrobots supporting people with disabilities. The system integrates four agents:\na visual perception agent for extracting semantic and spatial features from\nenvironment images, a risk assessment agent for identifying and prioritizing\nhazards, a planning agent for generating executable action sequences, and an\nevaluation agent for iterative optimization. By combining multimodal perception\nwith hierarchical multi-agent decision-making, the framework enables adaptive,\nrisk-aware, and personalized assistance in dynamic indoor environments.\nExperiments on multiple datasets demonstrate the superior overall performance\nof the proposed system in risk-aware planning and coordinated multi-agent\nexecution compared with state-of-the-art multimodal models. The proposed\napproach also highlights the potential of collaborative AI for practical\nassistive scenarios and provides a generalizable methodology for deploying\nMLLM-enabled multi-agent systems in real-world environments.", "AI": {"tldr": "MARS\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4e13\u4e3a\u667a\u80fd\u5bb6\u5c45\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u7528\u4e8e\u4e3a\u6b8b\u969c\u4eba\u58eb\u63d0\u4f9b\u8f85\u52a9\u670d\u52a1\u3002\u7cfb\u7edf\u5305\u542b\u89c6\u89c9\u611f\u77e5\u3001\u98ce\u9669\u8bc4\u4f30\u3001\u89c4\u5212\u548c\u8bc4\u4f30\u56db\u4e2a\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u548c\u5206\u5c42\u51b3\u7b56\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u98ce\u9669\u611f\u77e5\u548c\u4e2a\u6027\u5316\u7684\u5ba4\u5185\u73af\u5883\u8f85\u52a9\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u98ce\u9669\u611f\u77e5\u89c4\u5212\u3001\u7528\u6237\u4e2a\u6027\u5316\u548c\u5c06\u8bed\u8a00\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u6280\u80fd\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6742\u4e71\u7684\u5bb6\u5ead\u73af\u5883\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u4f9b\u667a\u80fd\u8f85\u52a9\u7684\u7cfb\u7edf\u6765\u652f\u6301\u6b8b\u969c\u4eba\u58eb\u3002", "method": "\u96c6\u6210\u56db\u4e2a\u667a\u80fd\u4f53\uff1a\u89c6\u89c9\u611f\u77e5\u667a\u80fd\u4f53\u4ece\u73af\u5883\u56fe\u50cf\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u98ce\u9669\u8bc4\u4f30\u667a\u80fd\u4f53\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u5371\u9669\uff0c\u89c4\u5212\u667a\u80fd\u4f53\u751f\u6210\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\uff0c\u8bc4\u4f30\u667a\u80fd\u4f53\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u7ed3\u5408\u591a\u6a21\u6001\u611f\u77e5\u548c\u5206\u5c42\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u98ce\u9669\u611f\u77e5\u89c4\u5212\u548c\u534f\u8c03\u591a\u667a\u80fd\u4f53\u6267\u884c\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u534f\u4f5cAI\u5728\u5b9e\u9645\u8f85\u52a9\u573a\u666f\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u57fa\u4e8eMLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2511.01718", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01718", "abs": "https://arxiv.org/abs/2511.01718", "authors": ["Jiayi Chen", "Wenxuan Song", "Pengxiang Ding", "Ziyang Zhou", "Han Zhao", "Feilong Tang", "Donglin Wang", "Haoang Li"], "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process", "comment": null, "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u6269\u6563\u8fc7\u7a0b\u540c\u6b65\u4f18\u5316\u56fe\u50cf\u751f\u6210\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u5b9e\u73b0\u7406\u89e3\u3001\u751f\u6210\u548c\u884c\u52a8\u7684\u534f\u540c\u4f5c\u7528", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u8981\u4e48\u4f9d\u8d56\u5916\u90e8\u4e13\u5bb6\u8fdb\u884c\u6a21\u6001\u7edf\u4e00\uff0c\u8981\u4e48\u5c06\u56fe\u50cf\u751f\u6210\u548c\u52a8\u4f5c\u9884\u6d4b\u4f5c\u4e3a\u72ec\u7acb\u8fc7\u7a0b\u5904\u7406\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u95f4\u7684\u76f4\u63a5\u534f\u540c\u6548\u76ca", "method": "\u63d0\u51fa\u7edf\u4e00\u6269\u6563VLA\u548c\u8054\u5408\u79bb\u6563\u53bb\u566a\u6269\u6563\u8fc7\u7a0b(JD3P)\uff0c\u901a\u8fc7\u5355\u4e00\u53bb\u566a\u8f68\u8ff9\u6574\u5408\u591a\u6a21\u6001\uff0c\u6784\u5efa\u7edf\u4e00\u6807\u8bb0\u7a7a\u95f4\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u548c\u63a8\u7406\u4f18\u5316\u6280\u672f", "result": "\u5728CALVIN\u3001LIBERO\u548cSimplerEnv\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u81ea\u56de\u5f52\u65b9\u6cd5\u5feb4\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u540c\u6b65\u53bb\u566a\u8fc7\u7a0b\u5b9e\u73b0\u4e86\u7406\u89e3\u3001\u751f\u6210\u548c\u884c\u52a8\u7684\u5185\u5728\u534f\u540c\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272"}}
{"id": "2511.01770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01770", "abs": "https://arxiv.org/abs/2511.01770", "authors": ["Liudi Yang", "Yang Bai", "Yuhao Wang", "Ibrahim Alsarraj", "Gitta Kutyniok", "Zhanchi Wang", "Ke Wu"], "title": "Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping", "comment": null, "summary": "Robotic grasping under uncertainty remains a fundamental challenge due to its\nuncertain and contact-rich nature. Traditional rigid robotic hands, with\nlimited degrees of freedom and compliance, rely on complex model-based and\nheavy feedback controllers to manage such interactions. Soft robots, by\ncontrast, exhibit embodied mechanical intelligence: their underactuated\nstructures and passive flexibility of their whole body, naturally accommodate\nuncertain contacts and enable adaptive behaviors. To harness this capability,\nwe propose a lightweight actuation-space learning framework that infers\ndistributional control representations for whole-body soft robotic grasping,\ndirectly from deterministic demonstrations using a flow matching model\n(Rectified Flow),without requiring dense sensing or heavy control loops. Using\nonly 30 demonstrations (less than 8% of the reachable workspace), the learned\npolicy achieves a 97.5% grasp success rate across the whole workspace,\ngeneralizes to grasped-object size variations of +-33%, and maintains stable\nperformance when the robot's dynamic response is directly adjusted by scaling\nthe execution time from 20% to 200%. These results demonstrate that\nactuation-space learning, by leveraging its passive redundant DOFs and\nflexibility, converts the body's mechanics into functional control intelligence\nand substantially reduces the burden on central controllers for this\nuncertain-rich task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9a71\u52a8\u7a7a\u95f4\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u6d41\u5339\u914d\u6a21\u578b\u4ece\u786e\u5b9a\u6027\u6f14\u793a\u4e2d\u5b66\u4e60\u5206\u5e03\u63a7\u5236\u8868\u793a\uff0c\u5b9e\u73b0\u8f6f\u4f53\u673a\u5668\u4eba\u6293\u53d6\uff0c\u4ec5\u970030\u6b21\u6f14\u793a\u5373\u53ef\u5728\u5b8c\u6574\u5de5\u4f5c\u7a7a\u95f4\u8fbe\u523097.5%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u673a\u5668\u4eba\u624b\u4f9d\u8d56\u590d\u6742\u7684\u57fa\u4e8e\u6a21\u578b\u548c\u91cd\u53cd\u9988\u63a7\u5236\u5668\u6765\u5904\u7406\u4e0d\u786e\u5b9a\u63a5\u89e6\uff0c\u800c\u8f6f\u4f53\u673a\u5668\u4eba\u5177\u6709\u673a\u68b0\u667a\u80fd\u7279\u6027\uff0c\u5176\u6b20\u9a71\u52a8\u7ed3\u6784\u548c\u5168\u8eab\u88ab\u52a8\u67d4\u6027\u80fd\u81ea\u7136\u9002\u5e94\u4e0d\u786e\u5b9a\u63a5\u89e6\u3002", "method": "\u4f7f\u7528\u6d41\u5339\u914d\u6a21\u578b\u4ece\u786e\u5b9a\u6027\u6f14\u793a\u4e2d\u5b66\u4e60\u9a71\u52a8\u7a7a\u95f4\u7684\u5206\u5e03\u63a7\u5236\u8868\u793a\uff0c\u65e0\u9700\u5bc6\u96c6\u4f20\u611f\u6216\u91cd\u63a7\u5236\u5faa\u73af\uff0c\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u6570\u636e\u3002", "result": "\u4ec5\u752830\u6b21\u6f14\u793a\uff08\u4e0d\u5230\u53ef\u8fbe\u5de5\u4f5c\u7a7a\u95f4\u76848%\uff09\u5c31\u5b9e\u73b0\u4e8697.5%\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u80fd\u6cdb\u5316\u5230\u00b133%\u7684\u6293\u53d6\u7269\u4f53\u5c3a\u5bf8\u53d8\u5316\uff0c\u4e14\u572820%-200%\u7684\u6267\u884c\u65f6\u95f4\u7f29\u653e\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u9a71\u52a8\u7a7a\u95f4\u5b66\u4e60\u901a\u8fc7\u5229\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u88ab\u52a8\u5197\u4f59\u81ea\u7531\u5ea6\u548c\u67d4\u97e7\u6027\uff0c\u5c06\u8eab\u4f53\u529b\u5b66\u8f6c\u5316\u4e3a\u529f\u80fd\u63a7\u5236\u667a\u80fd\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u4e2d\u592e\u63a7\u5236\u5668\u5728\u8fd9\u79cd\u4e0d\u786e\u5b9a\u4efb\u52a1\u4e2d\u7684\u8d1f\u62c5\u3002"}}
{"id": "2511.01774", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.01774", "abs": "https://arxiv.org/abs/2511.01774", "authors": ["Alexander Schperberg", "Yusuke Tanaka", "Stefano Di Cairano", "Dennis Hong"], "title": "MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll", "comment": "23 pages, 20 figures. Collaborative work between the Robotics and\n  Mechanisms Laboratory (RoMeLa) and Mitsubishi Electric Research Laboratories\n  (MERL)", "summary": "This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot\n(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features\nfour limbs--two 6-DoF arms with two-finger grippers for manipulation and\nclimbing, and two 4-DoF legs for locomotion--enabling smooth transitions across\ndiverse terrains without reconfiguration. A hybrid control architecture\ncombines reinforcement learning-based locomotion with model-based predictive\nand admittance control enhanced for safety by a Reference Governor toward\ncompliant contact interactions. A high-level MIQCP planner autonomously selects\nlocomotion modes to balance stability and energy efficiency. Hardware\nexperiments demonstrate robust gait transitions, dynamic climbing, and\nfull-body load support via pinch grasp. Overall, MOBIUS demonstrates the\nimportance of tight integration between morphology, high-level planning, and\ncontrol to enable mobile loco-manipulation and grasping, substantially\nexpanding its interaction capabilities, workspace, and traversability.", "AI": {"tldr": "MOBIUS\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u53cc\u8db3\u57ce\u5e02\u4fa6\u5bdf\u673a\u5668\u4eba\uff0c\u80fd\u591f\u884c\u8d70\u3001\u722c\u884c\u3001\u6500\u722c\u548c\u6eda\u52a8\uff0c\u901a\u8fc7\u6df7\u5408\u63a7\u5236\u67b6\u6784\u548c\u9ad8\u7ea7\u89c4\u5212\u5b9e\u73b0\u4e0d\u540c\u8fd0\u52a8\u6a21\u5f0f\u95f4\u7684\u5e73\u6ed1\u8f6c\u6362", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u6267\u884c\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff08\u884c\u8d70\u3001\u722c\u884c\u3001\u6500\u722c\u3001\u6eda\u52a8\uff09\u7684\u673a\u5668\u4eba\uff0c\u6269\u5c55\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u80fd\u529b\u3001\u5de5\u4f5c\u7a7a\u95f4\u548c\u7a7f\u8d8a\u80fd\u529b", "method": "\u91c7\u7528\u56db\u80a2\u4f53\u8bbe\u8ba1\uff08\u4e24\u4e2a6\u81ea\u7531\u5ea6\u624b\u81c2\u548c\u4e24\u4e2a4\u81ea\u7531\u5ea6\u817f\u90e8\uff09\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fd0\u52a8\u63a7\u5236\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u9884\u6d4b\u548c\u5bfc\u7eb3\u63a7\u5236\uff0c\u4f7f\u7528MIQCP\u89c4\u5212\u5668\u81ea\u4e3b\u9009\u62e9\u8fd0\u52a8\u6a21\u5f0f", "result": "\u786c\u4ef6\u5b9e\u9a8c\u5c55\u793a\u4e86\u7a33\u5065\u7684\u6b65\u6001\u8f6c\u6362\u3001\u52a8\u6001\u6500\u722c\u548c\u901a\u8fc7\u5939\u6301\u6293\u63e1\u5b9e\u73b0\u7684\u5168\u8eab\u4f53\u91cd\u652f\u6491", "conclusion": "MOBIUS\u8bc1\u660e\u4e86\u5f62\u6001\u5b66\u3001\u9ad8\u7ea7\u89c4\u5212\u548c\u63a7\u5236\u7684\u7d27\u5bc6\u96c6\u6210\u5bf9\u4e8e\u5b9e\u73b0\u79fb\u52a8\u5b9a\u4f4d\u64cd\u4f5c\u548c\u6293\u53d6\u7684\u91cd\u8981\u6027\uff0c\u663e\u8457\u6269\u5c55\u4e86\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u80fd\u529b"}}
{"id": "2511.01791", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01791", "abs": "https://arxiv.org/abs/2511.01791", "authors": ["Feng Chen", "Zhuxiu Xu", "Tianzhe Chu", "Xunzhe Zhou", "Li Sun", "Zewen Wu", "Shenghua Gao", "Zhongyu Li", "Yanchao Yang", "Yi Ma"], "title": "GenDexHand: Generative Simulation for Dexterous Hands", "comment": null, "summary": "Data scarcity remains a fundamental bottleneck for embodied intelligence.\nExisting approaches use large language models (LLMs) to automate gripper-based\nsimulation generation, but they transfer poorly to dexterous manipulation,\nwhich demands more specialized environment design. Meanwhile, dexterous\nmanipulation tasks are inherently more difficult due to their higher degrees of\nfreedom. Massively generating feasible and trainable dexterous hand tasks\nremains an open challenge. To this end, we present GenDexHand, a generative\nsimulation pipeline that autonomously produces diverse robotic tasks and\nenvironments for dexterous manipulation. GenDexHand introduces a closed-loop\nrefinement process that adjusts object placements and scales based on\nvision-language model (VLM) feedback, substantially improving the average\nquality of generated environments. Each task is further decomposed into\nsub-tasks to enable sequential reinforcement learning, reducing training time\nand increasing success rates. Our work provides a viable path toward scalable\ntraining of diverse dexterous hand behaviors in embodied intelligence by\noffering a simulation-based solution to synthetic data generation. Our website:\nhttps://winniechen2002.github.io/GenDexHand/.", "AI": {"tldr": "GenDexHand\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u4eff\u771f\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u591a\u6837\u5316\u7684\u7075\u5de7\u624b\u64cd\u4f5c\u4efb\u52a1\u548c\u73af\u5883\uff0c\u901a\u8fc7VLM\u53cd\u9988\u7684\u95ed\u73af\u7cbe\u70bc\u8fc7\u7a0b\u63d0\u9ad8\u73af\u5883\u8d28\u91cf\uff0c\u5e76\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u4ee5\u652f\u6301\u987a\u5e8f\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u6839\u672c\u74f6\u9888\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\u8fc1\u79fb\u6548\u679c\u5dee\uff0c\u800c\u7075\u5de7\u64cd\u4f5c\u7531\u4e8e\u81ea\u7531\u5ea6\u66f4\u9ad8\u800c\u66f4\u56f0\u96be\uff0c\u5927\u89c4\u6a21\u751f\u6210\u53ef\u884c\u4e14\u53ef\u8bad\u7ec3\u7684\u7075\u5de7\u624b\u4efb\u52a1\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "\u5f15\u5165\u95ed\u73af\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u9988\u8c03\u6574\u7269\u4f53\u653e\u7f6e\u548c\u5c3a\u5ea6\uff1b\u5c06\u6bcf\u4e2a\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u4ee5\u652f\u6301\u987a\u5e8f\u5f3a\u5316\u5b66\u4e60\uff1b\u63d0\u4f9b\u751f\u6210\u5f0f\u4eff\u771f\u6d41\u6c34\u7ebf\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u73af\u5883\u7684\u5e73\u5747\u8d28\u91cf\uff1b\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u5e76\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff1b\u4e3a\u7075\u5de7\u624b\u884c\u4e3a\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8e\u4eff\u771f\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4e2d\u591a\u6837\u5316\u7075\u5de7\u624b\u884c\u4e3a\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.01797", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01797", "abs": "https://arxiv.org/abs/2511.01797", "authors": ["Javier Ballesteros-Jerez", "Jesus Mart\u00ednez-G\u00f3mez", "Ismael Garc\u00eda-Varea", "Luis Orozco-Barbosa", "Manuel Castillo-Cara"], "title": "Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator", "comment": "13 pages, 7 figures. Conference paper (ROBOVIS 2025)", "summary": "We present a hybrid neural network model for inferring the position of mobile\nrobots using Channel State Information (CSI) data from a Massive MIMO system.\nBy leveraging an existing CSI dataset, our approach integrates a Convolutional\nNeural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural\nNetwork (HyNN) that estimates 2D robot positions. CSI readings are converted\ninto synthetic images using the TINTO tool. The localisation solution is\nintegrated with a robotics simulator, and the Robot Operating System (ROS),\nwhich facilitates its evaluation through heterogeneous test cases, and the\nadoption of state estimators like Kalman filters. Our contributions illustrate\nthe potential of our HyNN model in achieving precise indoor localisation and\nnavigation for mobile robots in complex environments. The study follows, and\nproposes, a generalisable procedure applicable beyond the specific use case\nstudied, making it adaptable to different scenarios and datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u89c4\u6a21MIMO\u7cfb\u7edfCSI\u6570\u636e\u7684\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u4f4d\u7f6e\u63a8\u65ad\u3002\u8be5\u65b9\u6cd5\u5c06CNN\u4e0eMLP\u7ed3\u5408\u5f62\u6210HyNN\uff0c\u901a\u8fc7TINTO\u5de5\u5177\u5c06CSI\u6570\u636e\u8f6c\u6362\u4e3a\u5408\u6210\u56fe\u50cf\u6765\u4f30\u8ba12D\u673a\u5668\u4eba\u4f4d\u7f6e\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u548c\u5bfc\u822a\u95ee\u9898\uff0c\u5229\u7528\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684CSI\u6570\u636e\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4f4d\u7f6e\u4f30\u8ba1\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u795e\u7ecf\u7f51\u7edc(HyNN)\u6a21\u578b\uff0c\u7ed3\u5408CNN\u548cMLP\uff0c\u5c06CSI\u6570\u636e\u901a\u8fc7TINTO\u5de5\u5177\u8f6c\u6362\u4e3a\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u5e76\u4e0e\u673a\u5668\u4eba\u4eff\u771f\u5668\u548cROS\u7cfb\u7edf\u96c6\u6210\u3002", "result": "\u5b9e\u73b0\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u548c\u5bfc\u822a\uff0c\u5c55\u793a\u4e86HyNN\u6a21\u578b\u5728\u4f4d\u7f6e\u4f30\u8ba1\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a8\u5e7f\u7684\u901a\u7528\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u548c\u6570\u636e\u96c6\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u5ba4\u5185\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
