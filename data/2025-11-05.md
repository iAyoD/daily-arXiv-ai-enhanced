<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [TRACE: Textual Reasoning for Affordance Coordinate Extraction](https://arxiv.org/abs/2511.01999)
*Sangyun Park,Jin Kim,Yuchen Cui,Matthew S. Brown*

Main category: cs.RO

TL;DR: TRACE方法通过文本推理链来提升视觉语言模型在机器人操作中的空间推理能力，在Where2Place基准上达到48.1%的准确率，相对提升9.6%。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型难以将高级指令转换为机器人操作所需的精确空间关系，现有视觉思维链方法计算成本高。

Method: 提出TRACE方法，将文本推理链整合到空间关系预测过程中，通过自主流水线创建包含指令和显式文本推理的大规模数据集，并基于此微调视觉语言模型。

Result: 在主要Where2Place基准上达到48.1%准确率（相对提升9.6%），在更具挑战性的W2P(h)子集上达到55.0%，性能与推理数据量直接相关。

Conclusion: 训练视觉语言模型生成文本推理链是提升VLM机器人控制精度、可靠性和可解释性的有效策略。

Abstract: Vision-Language Models (VLMs) struggle to translate high-level instructions
into the precise spatial affordances required for robotic manipulation. While
visual Chain-of-Thought (CoT) methods exist, they are often computationally
intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance
Coordinate Extraction), a novel methodology that integrates a textual Chain of
Reasoning (CoR) into the affordance prediction process. We use this methodology
to create the TRACE dataset, a large-scale collection created via an autonomous
pipeline that pairs instructions with explicit textual rationales. By
fine-tuning a VLM on this data, our model learns to externalize its spatial
reasoning before acting. Our experiments show that our TRACE-tuned model
achieves state-of-the-art performance, reaching 48.1% accuracy on the primary
Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more
challenging W2P(h) subset. Crucially, an ablation study demonstrates that
performance scales directly with the amount of reasoning data used, confirming
the CoR's effectiveness. Furthermore, analysis of the model's attention maps
reveals an interpretable reasoning process where focus shifts dynamically
across reasoning steps. This work shows that training VLMs to generate a
textual CoR is an effective and robust strategy for enhancing the precision,
reliability, and interpretability of VLM-based robot control. Our dataset and
code are available at https://github.com/jink-ucla/TRACE

</details>


### [2] [Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control](https://arxiv.org/abs/2511.02015)
*Jace Aldrich,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: 提出了一种结合MPPI和SVGD的新方法SOPPI，通过SVGD优化MPPI的样本生成，动态更新噪声分布以改善轨迹优化性能，在多个系统中验证了优于标准MPPI的效果。


<details>
  <summary>Details</summary>
Motivation: 传统MPPI依赖于高斯分布的随机采样轨迹，可能导致样本匮乏和次优结果，需要改进样本生成方法。

Method: 在MPPI环境步骤之间引入SVGD更新，动态调整噪声分布，形成SOPPI算法，在不显著增加计算需求的情况下优化轨迹表示。

Result: 在Cart-Pole到二维双足行走任务等多个系统中验证，SOPPI在不同超参数下均优于标准MPPI，且在较少粒子数下仍可行。

Conclusion: SOPPI方法对高自由度系统具有适用性，并有望推动可微分模拟器的新发展。

Abstract: This paper presents a novel method for Model Predictive Path Integral (MPPI)
control that optimizes sample generation towards an optimal trajectory through
Stein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on
randomly sampled trajectories, often by a Gaussian distribution. The result can
lead to sample deprivation, under-representing the space of possible
trajectories, and yield suboptimal results. Through introducing SVGD updates in
between MPPI environment steps, we present Stein-Optimized Path-Integral
Inference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise
distributions at runtime to shape a more optimal representation without an
excessive increase in computational requirements. We demonstrate the efficacy
of our method systems ranging from a Cart-Pole to a two-dimensional bipedal
walking task, indicating improved performance above standard MPPI across a
range of hyper-parameters and demonstrate feasibility at lower particle counts.
We discuss the applicability of this MPPI/SVGD method to higher
degree-of-freedom systems, as well as its potential to new developments in
state-of-the-art differentiable simulators.

</details>


### [3] [TurboMap: GPU-Accelerated Local Mapping for Visual SLAM](https://arxiv.org/abs/2511.02036)
*Parsa Hosseininejad,Kimia Khabiri,Shishir Gopinath,Soudabeh Mohammadhashemi,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: TurboMap是一个GPU加速和CPU优化的视觉SLAM局部建图模块，通过在GPU上执行地图点三角化和融合，在CPU上加速冗余关键帧剔除，并集成GPU加速的局部束调整求解器，实现了在保持精度的同时显著提升局部建图速度。


<details>
  <summary>Details</summary>
Motivation: 识别视觉SLAM中局部建图过程的关键性能瓶颈，通过针对性的GPU和CPU优化来解决这些瓶颈，提升SLAM系统的实时性能。

Method: 在ORB-SLAM3基础上构建，使用CUDA进行GPU编程，将地图点三角化和融合卸载到GPU，在CPU上加速冗余关键帧剔除，集成GPU加速的局部束调整求解器。

Result: 在EuRoC数据集上平均加速1.3倍，在TUM-VI数据集上平均加速1.6倍，在桌面和嵌入式平台上均能保持原始系统的精度。

Conclusion: TurboMap通过GPU和CPU协同优化有效解决了视觉SLAM局部建图的性能瓶颈，实现了显著的加速效果，同时保持了系统的定位精度。

Abstract: This paper presents TurboMap, a GPU-accelerated and CPU-optimized local
mapping module for visual SLAM systems. We identify key performance bottlenecks
in the local mapping process for visual SLAM and address them through targeted
GPU and CPU optimizations. Specifically, we offload map point triangulation and
fusion to the GPU, accelerate redundant keyframe culling on the CPU, and
integrate a GPU-accelerated solver to speed up local bundle adjustment. Our
implementation is built on top of ORB-SLAM3 and leverages CUDA for GPU
programming. The experimental results show that TurboMap achieves an average
speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the
local mapping module, on both desktop and embedded platforms, while maintaining
the accuracy of the original system.

</details>


### [4] [TACO: Trajectory-Aware Controller Optimization for Quadrotors](https://arxiv.org/abs/2511.02060)
*Hersh Sanghvi,Spencer Folk,Vijay Kumar,Camillo Jose Taylor*

Main category: cs.RO

TL;DR: TACO是一个实时优化四旋翼控制器参数的框架，能够根据参考轨迹和当前状态在线调整参数，显著提升轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼控制器参数固定且手动调优，无法适应不同任务需求，牺牲了任务特定性能。

Method: 使用学习预测模型和轻量级优化方案，实时优化控制器增益，并可调整轨迹以提高动态可行性。

Result: 实验表明TACO在多种轨迹类型上优于传统静态参数调优，运行速度比黑盒优化基准快几个数量级，在物理四旋翼上实现实时部署。

Conclusion: TACO框架能够显著降低四旋翼的跟踪误差，通过轨迹适应进一步提升性能。

Abstract: Controller performance in quadrotor trajectory tracking depends heavily on
parameter tuning, yet standard approaches often rely on fixed, manually tuned
parameters that sacrifice task-specific performance. We present
Trajectory-Aware Controller Optimization (TACO), a framework that adapts
controller parameters online based on the upcoming reference trajectory and
current quadrotor state. TACO employs a learned predictive model and a
lightweight optimization scheme to optimize controller gains in real time with
respect to a broad class of trajectories, and can also be used to adapt
trajectories to improve dynamic feasibility while respecting smoothness
constraints. To enable large-scale training, we also introduce a parallelized
quadrotor simulator supporting fast data collection on diverse trajectories.
Experiments on a variety of trajectory types show that TACO outperforms
conventional, static parameter tuning while operating orders of magnitude
faster than black-box optimization baselines, enabling practical real-time
deployment on a physical quadrotor. Furthermore, we show that adapting
trajectories using TACO significantly reduces the tracking error obtained by
the quadrotor.

</details>


### [5] [A Step Toward World Models: A Survey on Robotic Manipulation](https://arxiv.org/abs/2511.02097)
*Peng-Fei Zhang,Ying Cheng,Xiaofan Sun,Shijie Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.RO

TL;DR: 本文综述了机器人操作中的世界模型方法，分析了它们在感知、预测和控制中的作用，旨在为开发通用实用的机器人世界模型制定路线图。


<details>
  <summary>Details</summary>
Motivation: 自主代理需要在复杂动态环境中执行任务，这要求它们理解世界的基本机制和动态，而不仅仅是反应性控制或简单复制观察状态，因此需要开发能够编码环境状态、捕捉动态并支持预测、规划和推理的世界模型。

Method: 通过回顾机器人操作中的方法，分析具有世界模型核心能力的方法，考察它们在感知、预测和控制中的角色，识别关键挑战和解决方案，提炼真实世界模型应具备的核心组件、能力和功能。

Result: 确定了世界模型的核心组件、能力和功能，为开发通用实用的机器人世界模型提供了基础分析框架。

Conclusion: 基于分析结果，旨在为开发通用实用的机器人世界模型制定路线图，推动自主代理在复杂动态环境中的能力发展。

Abstract: Autonomous agents are increasingly expected to operate in complex, dynamic,
and uncertain environments, performing tasks such as manipulation, navigation,
and decision-making. Achieving these capabilities requires agents to understand
the underlying mechanisms and dynamics of the world, moving beyond purely
reactive control or simple replication of observed states. This motivates the
development of world models as internal representations that encode
environmental states, capture dynamics, and enable prediction, planning, and
reasoning. Despite growing interest, the definition, scope, architectures, and
essential capabilities of world models remain ambiguous. In this survey, rather
than directly imposing a fixed definition and limiting our scope to methods
explicitly labeled as world models, we examine approaches that exhibit the core
capabilities of world models through a review of methods in robotic
manipulation. We analyze their roles across perception, prediction, and
control, identify key challenges and solutions, and distill the core
components, capabilities, and functions that a real world model should possess.
Building on this analysis, we aim to outline a roadmap for developing
generalizable and practical world models for robotics.

</details>


### [6] [Census-Based Population Autonomy For Distributed Robotic Teaming](https://arxiv.org/abs/2511.02147)
*Tyler M. Paine,Anastasia Bizyaeva,Michael R. Benjamin*

Main category: cs.RO

TL;DR: 本文提出了一种多机器人自主性的分层模型，结合了基于共识的集体决策和多目标行为优化的个体决策，能够实现分布式优化和新的集体行为。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在海洋环境中具有高效性和鲁棒性优势，但如何建模、分析和设计这些系统以实现协作的全部效益是一个挑战，因为多机器人自主性领域既包含集体行为也包含个体行为。

Method: 引入分层模型：使用非线性意见动态模型进行基于共识的集体决策，使用区间规划进行多目标行为优化的个体决策。还提出了分布式子群分配优化方法，机器人使用梯度下降算法最小化局部已知成本函数部分，同时受邻居意见状态影响以考虑未观测成本。

Result: 该模型可以简化为分布式优化和控制的基础算法，完整模型能够实现现实场景中有用的新型集体行为。通过三个不同类别的自主水面车辆实验验证了模型的有效性：自适应采样场景、高价值单元保护场景和夺旗竞争游戏。

Conclusion: 提出的分层模型能够有效处理多机器人系统中的集体和个体决策问题，实验验证了该模型在实际应用中的实用性，特别是在分布式优化和新型集体行为实现方面表现出色。

Abstract: Collaborating teams of robots show promise due in their ability to complete
missions more efficiently and with improved robustness, attributes that are
particularly useful for systems operating in marine environments. A key issue
is how to model, analyze, and design these multi-robot systems to realize the
full benefits of collaboration, a challenging task since the domain of
multi-robot autonomy encompasses both collective and individual behaviors. This
paper introduces a layered model of multi-robot autonomy that uses the
principle of census, or a weighted count of the inputs from neighbors, for
collective decision-making about teaming, coupled with multi-objective behavior
optimization for individual decision-making about actions. The census component
is expressed as a nonlinear opinion dynamics model and the multi-objective
behavior optimization is accomplished using interval programming. This model
can be reduced to recover foundational algorithms in distributed optimization
and control, while the full model enables new types of collective behaviors
that are useful in real-world scenarios. To illustrate these points, a new
method for distributed optimization of subgroup allocation is introduced where
robots use a gradient descent algorithm to minimize portions of the cost
functions that are locally known, while being influenced by the opinion states
from neighbors to account for the unobserved costs. With this method the group
can collectively use the information contained in the Hessian matrix of the
total global cost. The utility of this model is experimentally validated in
three categorically different experiments with fleets of autonomous surface
vehicles: an adaptive sampling scenario, a high value unit protection scenario,
and a competitive game of capture the flag.

</details>


### [7] [Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models](https://arxiv.org/abs/2511.02162)
*Alexander Htet Kyaw,Richa Gupta,Dhruv Shah,Anoop Sinha,Kory Mathewson,Stefanie Pender,Sachin Chitta,Yotto Koga,Faez Ahmed,Lawrence Sass,Randall Davis*

Main category: cs.RO

TL;DR: 提出了一种结合3D生成AI和视觉语言模型的管道，用于从自然语言实现多组件物体的机器人组装。


<details>
  <summary>Details</summary>
Motivation: 解决3D生成AI在创建多组件类型物体时面临的挑战，实现从文本提示到物理对象的完整流程。

Method: 利用视觉语言模型进行零样本多模态推理，将AI生成的网格分解为多组件3D模型，使用预定义的结构和面板组件。

Result: 评估显示用户90.6%的时间偏好VLM生成的组件分配，相比规则基础的59.4%和随机分配的2.5%。

Conclusion: 该系统通过对话反馈允许用户细化组件分配，为使用生成AI和机器人技术制造物理对象提供了更大的人类控制和自主权。

Abstract: Advances in 3D generative AI have enabled the creation of physical objects
from text prompts, but challenges remain in creating objects involving multiple
component types. We present a pipeline that integrates 3D generative AI with
vision-language models (VLMs) to enable the robotic assembly of multi-component
objects from natural language. Our method leverages VLMs for zero-shot,
multi-modal reasoning about geometry and functionality to decompose
AI-generated meshes into multi-component 3D models using predefined structural
and panel components. We demonstrate that a VLM is capable of determining which
mesh regions need panel components in addition to structural components, based
on object functionality. Evaluation across test objects shows that users
preferred the VLM-generated assignments 90.6% of the time, compared to 59.4%
for rule-based and 2.5% for random assignment. Lastly, the system allows users
to refine component assignments through conversational feedback, enabling
greater human control and agency in making physical objects with generative AI
and robotics.

</details>


### [8] [Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery](https://arxiv.org/abs/2511.02167)
*Tian Hao,Tong Lu,Che Chan*

Main category: cs.RO

TL;DR: 提出了一种用于腹腔镜手术的7自由度机器人手臂，通过运动学优化和人体工程学设计，显著提高了手术精度和操作舒适度。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术可以增强手术精度并减少外科医生疲劳，需要研究运动学和人体工程学设计原则。

Method: 设计具有远程运动中心(RCM)的7自由度机器人手臂系统，在通用机器人平台上实现，通过模拟手术任务评估性能。

Result: 实验结果显示目标精度提高50%以上，任务完成时间缩短，操作者肌肉劳损和不适感显著降低。

Conclusion: 运动学优化和人体工程学设计对提升机器人辅助手术性能至关重要，为下一代手术机器人的开发提供了指导。

Abstract: Robotic assistance in minimally invasive surgery can greatly enhance surgical
precision and reduce surgeon fatigue. This paper presents a focused
investigation on the kinematic and ergonomic design principles for a
laparoscopic surgical robotic arm aimed at high-precision tasks. We propose a
7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote
center of motion (RCM) at the instrument insertion point and ergonomic
considerations to improve surgeon interaction. The design is implemented on a
general-purpose robotic platform, and a series of simulated surgical tasks were
performed to evaluate targeting accuracy, task efficiency, and surgeon comfort
compared to conventional manual laparoscopy. Experimental results demonstrate
that the optimized robotic design achieves significantly improved targeting
accuracy (error reduced by over 50%) and shorter task completion times, while
substantially lowering operator muscle strain and discomfort. These findings
validate the importance of kinematic optimization (such as added articulations
and tremor filtering) and human-centered ergonomic design in enhancing the
performance of robot-assisted surgery. The insights from this work can guide
the development of next-generation surgical robots that improve surgical
outcomes and ergonomics for the operating team.

</details>


### [9] [A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms](https://arxiv.org/abs/2511.02192)
*Linxin Hou,Qirui Wu,Zhihang Qin,Neil Banerjee,Yongxin Guo,Cecilia Laschi*

Main category: cs.RO

TL;DR: 比较集中式和分布式多智能体强化学习在软体机器人臂控制中的性能，发现当控制段数n≤4时分布式策略无显著优势，n≤2时集中式策略更优，4<n≤12时分布式策略样本效率更高但训练时间更长。


<details>
  <summary>Details</summary>
Motivation: 研究集中式和分布式MARL架构在软体机器人控制中的性能差异，为软体机器人系统的强化学习控制提供设计指导。

Method: 使用PyElastica和OpenAI Gym接口，在相同预算下训练全局PPO控制器和MAPPO控制器，系统改变控制段数n，评估三种场景下的性能。

Result: 当n≤4时分布式策略无显著优势；n≤2时集中式策略更优；4<n≤12时分布式策略样本效率更高、成功率更高、更具弹性和鲁棒性，但集中式策略训练时间效率更高。

Conclusion: 集中式和分布式策略在软体机器人控制中存在权衡，为未来软体杆状机械臂的仿真到现实转移提供了可行的设计指导。

Abstract: This paper presents a quantitative comparison between centralised and
distributed multi-agent reinforcement learning (MARL) architectures for
controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
budgets. Both approaches are based on the arm having $n$ number of controlled
sections. The study systematically varies $n$ and evaluates the performance of
the arm to reach a fixed target in three scenarios: default baseline condition,
recovery from external disturbance, and adaptation to actuator failure.
Quantitative metrics used for the evaluation are mean action magnitude, mean
final distance, mean episode length, and success rate. The results show that
there are no significant benefits of the distributed policy when the number of
controlled sections $n\le4$. In very simple systems, when $n\le2$, the
centralised policy outperforms the distributed one. When $n$ increases to $4<
n\le 12$, the distributed policy shows a high sample efficiency. In these
systems, distributed policy promotes a stronger success rate, resilience, and
robustness under local observability and yields faster convergence given the
same sample size. However, centralised policies achieve much higher time
efficiency during training as it takes much less time to train the same size of
samples. These findings highlight the trade-offs between centralised and
distributed policy in reinforcement learning-based control for soft robotic
systems and provide actionable design guidance for future sim-to-real transfer
in soft rod-like manipulators.

</details>


### [10] [LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](https://arxiv.org/abs/2511.02239)
*Youngjin Hong,Houjian Yu,Mingen Li,Changhyun Choi*

Main category: cs.RO

TL;DR: LACY框架通过联合学习语言到动作(L2A)、动作到语言(A2L)和语言一致性验证(L2C)三个任务，实现双向语言-动作映射，在机器人操作任务中显著提升成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语言到动作的单向映射方法缺乏对行为的深度理解，限制了泛化能力和行为解释性。需要双向映射来建立更全面的语言-动作基础。

Method: 在单一视觉语言模型中联合训练L2A、A2L和L2C三个任务，通过主动增强策略针对低置信度案例自动生成和过滤训练数据，实现无人工标注的自改进循环。

Result: 在模拟和真实世界的拾取放置任务中，平均任务成功率提升56.46%，并产生更鲁棒的语言-动作基础。

Conclusion: 双向语言-动作映射框架LACY通过自监督学习显著提升了机器人操作任务的性能和可解释性，为构建更智能的机器人系统提供了新范式。

Abstract: Learning generalizable policies for robotic manipulation increasingly relies
on large-scale models that map language instructions to actions (L2A). However,
this one-way paradigm often produces policies that execute tasks without deeper
contextual understanding, limiting their ability to generalize or explain their
behavior. We argue that the complementary skill of mapping actions back to
language (A2L) is essential for developing more holistic grounding. An agent
capable of both acting and explaining its actions can form richer internal
representations and unlock new paradigms for self-supervised learning. We
introduce LACY (Language-Action Cycle), a unified framework that learns such
bidirectional mappings within a single vision-language model. LACY is jointly
trained on three synergistic tasks: generating parameterized actions from
language (L2A), explaining observed actions in language (A2L), and verifying
semantic consistency between two language descriptions (L2C). This enables a
self-improving cycle that autonomously generates and filters new training data
through an active augmentation strategy targeting low-confidence cases, thereby
improving the model without additional human labels. Experiments on
pick-and-place tasks in both simulation and the real world show that LACY
improves task success rates by 56.46% on average and yields more robust
language-action grounding for robotic manipulation. Project page:
https://vla2026.github.io/LACY/

</details>


### [11] [SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction](https://arxiv.org/abs/2511.02294)
*Ruiyong Yuan,Jieji Ren,Zhanxuan Peng,Feifei Chen,Guoying Gu*

Main category: cs.RO

TL;DR: 提出了一种名为SuckTac的智能吸盘，将基于摄像头的触觉传感器集成到优化结构中，实现高密度感知和鲁棒吸附，灵感来自头足类动物的自适应结构和感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有吸盘缺乏高保真感知和触觉传感能力，无法识别目标表面的精细几何特征和交互状态，限制了在复杂非结构化环境中的鲁棒性能。

Method: 通过结构设计和优化，采用多材料集成铸造技术，将摄像头和光源嵌入吸盘内部，实现原位高密度感知；同时优化机械设计，改进轮廓、增加柔性唇边和表面微结构。

Result: 在机器人布料操作和软体移动机器人检测等挑战性任务中，该系统表现出优越性能和广泛适用性。

Conclusion: SuckTac智能吸盘成功整合了高密度感知和鲁棒吸附能力，为复杂环境下的机器人操作提供了有效解决方案。

Abstract: Suckers are significant for robots in picking, transferring, manipulation and
locomotion on diverse surfaces. However, most of the existing suckers lack
high-fidelity perceptual and tactile sensing, which impedes them from resolving
the fine-grained geometric features and interaction status of the target
surface. This limits their robust performance with irregular objects and in
complex, unstructured environments. Inspired by the adaptive structure and
high-performance sensory capabilities of cephalopod suckers, in this paper, we
propose a novel, intelligent sucker, named SuckTac, that integrates a
camera-based tactile sensor directly within its optimized structure to provide
high-density perception and robust suction. Specifically, through joint
structure design and optimization and based on a multi-material integrated
casting technique, a camera and light source are embedded into the sucker,
which enables in-situ, high-density perception of fine details like surface
shape, texture and roughness. To further enhance robustness and adaptability,
the sucker's mechanical design is also optimized by refining its profile,
adding a compliant lip, and incorporating surface microstructure. Extensive
experiments, including challenging tasks such as robotic cloth manipulation and
soft mobile robot inspection, demonstrate the superior performance and broad
applicability of the proposed system.

</details>


### [12] [ZJUNlict Extended Team Description Paper 2025](https://arxiv.org/abs/2511.02315)
*Zifei Wu,Lijie Wang,Zhe Yang,Shijie Yang,Liang Wang,Haoran Fu,Yinliang Cai,Rong Xiong*

Main category: cs.RO

TL;DR: ZJUNlict团队在过去一年中在硬件和软件方面都取得了进展，硬件上为v2023机器人集成了IMU以提升姿态精度和角速度规划，软件上优化了策略和CUDA模块，提高了决策效率、球追踪预测和控球预测能力。


<details>
  <summary>Details</summary>
Motivation: 为了适应高节奏的比赛动态，需要提升机器人的姿态控制精度和软件决策效率。

Method: 硬件方面为机器人集成IMU传感器，软件方面优化策略模块和CUDA模块。

Result: 实现了更高的姿态精度、角速度规划能力，以及更高效的决策、球追踪预测和控球预测。

Conclusion: 通过硬件IMU集成和软件模块优化，ZJUNlict团队显著提升了机器人在高节奏比赛中的性能表现。

Abstract: This paper presents the ZJUNlict team's work over the past year, covering
both hardware and software advancements. In the hardware domain, the
integration of an IMU into the v2023 robot was completed to enhance posture
accuracy and angular velocity planning. On the software side, key modules were
optimized, including the strategy and CUDA modules, with significant
improvements in decision making efficiency, ball pursuit prediction, and ball
possession prediction to adapt to high-tempo game dynamics.

</details>


### [13] [Whole-body motion planning and safety-critical control for aerial manipulation](https://arxiv.org/abs/2511.02342)
*Lin Yang,Jinwoo Lee,Domenico Campolo,H. Jin Kim,Jeonghyun Byun*

Main category: cs.RO

TL;DR: 提出了基于超二次曲面的空中机械臂运动规划和安全控制框架，通过几何精确建模和最大间隙规划，在杂乱环境中生成平滑、安全的轨迹。


<details>
  <summary>Details</summary>
Motivation: 空中机械臂在复杂环境中规划安全、动态可行的轨迹仍然困难，主要面临全身碰撞避免和常见几何抽象（如包围盒或椭球）保守性的挑战。

Method: 使用超二次曲面加代理表示法对车辆和障碍物进行可微分的几何精确表面建模，结合Voronoi图和平衡流形公式的最大间隙规划器，以及基于高阶控制屏障函数的安全关键控制器。

Result: 在模拟中，该方法在杂乱环境中优于基于采样的规划器，产生更快、更安全、更平滑的轨迹，在几何保真度上超过基于椭球的基线方法。物理实验验证了可行性和鲁棒性。

Conclusion: 该框架在模拟和硬件设置中表现一致，为空中机械臂在复杂环境中的安全操作提供了有效的解决方案。

Abstract: Aerial manipulation combines the maneuverability of multirotors with the
dexterity of robotic arms to perform complex tasks in cluttered spaces. Yet
planning safe, dynamically feasible trajectories remains difficult due to
whole-body collision avoidance and the conservativeness of common geometric
abstractions such as bounding boxes or ellipsoids. We present a whole-body
motion planning and safety-critical control framework for aerial manipulators
built on superquadrics (SQs). Using an SQ-plus-proxy representation, we model
both the vehicle and obstacles with differentiable, geometry-accurate surfaces.
Leveraging this representation, we introduce a maximum-clearance planner that
fuses Voronoi diagrams with an equilibrium-manifold formulation to generate
smooth, collision-aware trajectories. We further design a safety-critical
controller that jointly enforces thrust limits and collision avoidance via
high-order control barrier functions. In simulation, our approach outperforms
sampling-based planners in cluttered environments, producing faster, safer, and
smoother trajectories and exceeding ellipsoid-based baselines in geometric
fidelity. Actual experiments on a physical aerial-manipulation platform confirm
feasibility and robustness, demonstrating consistent performance across
simulation and hardware settings. The video can be found at
https://youtu.be/hQYKwrWf1Ak.

</details>


### [14] [Dexterous Robotic Piano Playing at Scale](https://arxiv.org/abs/2511.02504)
*Le Chen,Yi Zhao,Jan Schneider,Quankai Gao,Simon Guist,Cheng Qian,Juho Kannala,Bernhard Schölkopf,Joni Pajarinen,Dieter Büchler*

Main category: cs.RO

TL;DR: OmniPianist是首个能够通过可扩展、无需人类演示的学习方式演奏近千首音乐作品的机器人代理，采用最优传输自动指法策略、大规模强化学习和流匹配变换器实现。


<details>
  <summary>Details</summary>
Motivation: 赋予机器人手人类水平的灵巧性是机器人学的长期目标，而双手机器人钢琴演奏是一个特别具有挑战性的高维度、接触丰富且需要快速精确控制的任务。

Method: 1. 基于最优传输的自动指法策略，让代理能够自主发现高效的钢琴演奏策略；2. 训练2000多个专门化代理的大规模强化学习，构建包含100万条轨迹的RP1M++数据集；3. 使用流匹配变换器进行大规模模仿学习。

Result: 开发出能够演奏广泛音乐作品的OmniPianist代理，实验证明该方法具有有效性和可扩展性。

Conclusion: 该方法推动了大规模灵巧机器人钢琴演奏的发展，展示了无需人类演示的可扩展学习方法在复杂机器人任务中的潜力。

Abstract: Endowing robot hands with human-level dexterity has been a long-standing goal
in robotics. Bimanual robotic piano playing represents a particularly
challenging task: it is high-dimensional, contact-rich, and requires fast,
precise control. We present OmniPianist, the first agent capable of performing
nearly one thousand music pieces via scalable, human-demonstration-free
learning. Our approach is built on three core components. First, we introduce
an automatic fingering strategy based on Optimal Transport (OT), allowing the
agent to autonomously discover efficient piano-playing strategies from scratch
without demonstrations. Second, we conduct large-scale Reinforcement Learning
(RL) by training more than 2,000 agents, each specialized in distinct music
pieces, and aggregate their experience into a dataset named RP1M++, consisting
of over one million trajectories for robotic piano playing. Finally, we employ
a Flow Matching Transformer to leverage RP1M++ through large-scale imitation
learning, resulting in the OmniPianist agent capable of performing a wide range
of musical pieces. Extensive experiments and ablation studies highlight the
effectiveness and scalability of our approach, advancing dexterous robotic
piano playing at scale.

</details>


### [15] [Non-Contact Manipulation of Induced Magnetic Dipoles](https://arxiv.org/abs/2511.02761)
*Seth Stewart,Joseph Pawelski,Steve Ward,Andrew J. Petruska*

Main category: cs.RO

TL;DR: 本文展示了在实验室测试中对半浮力铝球进行闭环位置控制，探索了不同力反演方法的有效性，这是实现感应磁偶极子3自由度位置控制更广泛应用的关键第一步。


<details>
  <summary>Details</summary>
Motivation: 将磁操纵扩展到导电非磁性物体，为以前仅限于硬磁或软磁材料的广泛应用打开了大门，特别是在利用振荡磁场回收空间碎片方面，这代表了在特别适合感应磁操纵产生的低力环境中的原材料缓存。

Method: 基于先前利用感应涡流产生的反向磁矩实现3D开环位置控制的工作，本研究展示了在实验室测试中对半浮力铝球的闭环位置控制，并探索了不同力反演方法的有效性。

Result: 成功实现了对半浮力铝球的闭环位置控制，并验证了不同力反演方法的有效性。

Conclusion: 闭环控制方法是实现感应磁偶极子3自由度位置控制更广泛应用的关键第一步。

Abstract: Extending the field of magnetic manipulation to conductive, non-magnetic
objects opens the door for a wide array of applications previously limited to
hard or soft magnetic materials. Of particular interest is the recycling of
space debris through the use of oscillating magnetic fields, which represent a
cache of raw materials in an environment particularly suited to the low forces
generated from inductive magnetic manipulation. Building upon previous work
that demonstrated 3D open-loop position control by leveraging the opposing
dipole moment created from induced eddy currents, this work demonstrates
closed-loop position control of a semi-buoyant aluminum sphere in lab tests,
and the efficacy of varying methods for force inversion is explored. The
closed-loop methods represent a critical first step towards wider applications
for 3-DOF position control of induced magnetic dipoles.

</details>


### [16] [XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](https://arxiv.org/abs/2511.02776)
*Shichao Fan,Kun Wu,Zhengping Che,Xinhua Wang,Di Wu,Fei Liao,Ning Liu,Yixue Zhang,Zhen Zhao,Zhiyuan Xu,Meng Li,Qingjie Liu,Shanghang Zhang,Min Wan,Jian Tang*

Main category: cs.RO

TL;DR: XR-1是一个新颖的视觉-语言-动作模型框架，通过统一视觉-运动编码来解决现有VLA模型在精确低层动作生成和跨域数据融合方面的挑战，在6种不同机器人平台上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型面临两个基本挑战：(i)从高维观测生成精确的低层动作，(ii)跨越异构数据源的领域鸿沟，包括不同的机器人具身和人类演示。现有方法未能充分利用大规模异构数据集中的互补多模态知识。

Method: 提出XR-1框架，引入统一视觉-运动编码(UVMC)，通过双分支VQ-VAE联合编码视觉动态和机器人运动。采用三阶段训练范式：自监督UVMC学习、UVMC引导的大规模跨具身数据集预训练、任务特定后训练。

Result: 在6种不同机器人具身上进行了超过14,000次真实世界实验，涵盖120多个多样化操作任务。XR-1持续优于最先进的基线模型，并展现出对新物体、背景变化、干扰物和光照变化的强泛化能力。

Conclusion: XR-1通过UVMC表示和三阶段训练范式，有效解决了VLA模型的关键挑战，为跨机器人、任务和环境的通用可扩展VLA学习提供了有效框架。

Abstract: Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.

</details>


### [17] [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](https://arxiv.org/abs/2511.02832)
*Yanjie Ze,Siheng Zhao,Weizhuo Wang,Angjoo Kanazawa,Rocky Duan,Pieter Abbeel,Guanya Shi,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: TWIST2是一个便携式、无需动作捕捉的人形机器人遥操作和数据收集系统，通过VR设备获取全身运动数据，结合低成本机器人颈部实现全身控制，能够高效收集演示数据并训练分层视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人缺乏有效的数据收集框架，现有遥操作系统要么使用解耦控制，要么依赖昂贵的动作捕捉设备，限制了可扩展性。

Method: 使用PICO4U VR实时获取全身人体运动，配合定制的2自由度机器人颈部（成本约250美元）实现自我中心视觉，构建整体人形机器人控制系统。

Result: 能够执行长时程灵巧和移动技能，15分钟内收集100个演示且成功率接近100%，训练的分层视觉运动策略成功完成全身灵巧操作和动态踢球任务。

Conclusion: TWIST2提供了一个完全可复现的开源系统，解决了人形机器人数据收集的瓶颈问题，为大规模人形机器人学习奠定了基础。

Abstract: Large-scale data has driven breakthroughs in robotics, from language models
to vision-language-action models in bimanual manipulation. However, humanoid
robotics lacks equally effective data collection frameworks. Existing humanoid
teleoperation systems either use decoupled control or depend on expensive
motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid
teleoperation and data collection system that preserves full whole-body control
while advancing scalability. Our system leverages PICO4U VR for obtaining
real-time whole-body human motions, with a custom 2-DoF robot neck (cost around
$250) for egocentric vision, enabling holistic human-to-humanoid control. We
demonstrate long-horizon dexterous and mobile humanoid skills and we can
collect 100 demonstrations in 15 minutes with an almost 100% success rate.
Building on this pipeline, we propose a hierarchical visuomotor policy
framework that autonomously controls the full humanoid body based on egocentric
vision. Our visuomotor policy successfully demonstrates whole-body dexterous
manipulation and dynamic kicking tasks. The entire system is fully reproducible
and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also
open-sourced at https://twist-data.github.io .

</details>
