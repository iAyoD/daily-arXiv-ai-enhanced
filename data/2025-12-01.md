<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 104]
- [cs.RO](#cs.RO) [Total: 43]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EvalCards: A Framework for Standardized Evaluation Reporting](https://arxiv.org/abs/2511.21695)
*Ruchira Dhar,Danae Sanchez Villegas,Antonia Karamolegkou,Alice Schiavone,Yifei Yuan,Xinyi Chen,Jiaang Li,Stella Frank,Laura De Grazia,Monorama Swain,Stephanie Brandl,Daniel Hershcovich,Anders Søgaard,Desmond Elliott*

Main category: cs.CL

TL;DR: 论文提出EvalCards作为解决当前NLP评估报告实践不足的新方法，旨在提高评估的透明度、可复现性和治理能力。


<details>
  <summary>Details</summary>
Motivation: 当前NLP评估报告存在三个主要问题：可复现性不足、可访问性差和治理机制缺失。随着开源模型的快速发展，透明报告实践变得比以往任何时候都更加重要。

Method: 引入Evaluation Disclosure Cards (EvalCards)作为解决方案。EvalCards旨在为研究人员和从业者提供增强的透明度，同时为满足新兴治理要求提供实用基础。

Result: 论文认为现有标准化努力仍然不足，EvalCards为改善评估报告实践提供了可行的路径。

Conclusion: EvalCards是推动NLP评估报告实践向前发展的重要工具，能够解决当前报告实践中的关键缺陷，促进更透明、可复现和可治理的评估过程。

Abstract: Evaluation has long been a central concern in NLP, and transparent reporting practices are more critical than ever in today's landscape of rapidly released open-access models. Drawing on a survey of recent work on evaluation and documentation, we identify three persistent shortcomings in current reporting practices: reproducibility, accessibility, and governance. We argue that existing standardization efforts remain insufficient and introduce Evaluation Disclosure Cards (EvalCards) as a path forward. EvalCards are designed to enhance transparency for both researchers and practitioners while providing a practical foundation to meet emerging governance requirements.

</details>


### [2] [Cacheback: Speculative Decoding With Nothing But Cache](https://arxiv.org/abs/2511.21699)
*Zhiyao Ma,In Gim,Lin Zhong*

Main category: cs.CL

TL;DR: Cacheback Decoding是一种无需训练、与模型无关的推测解码方法，利用语言中的局部性来加速大语言模型推理，仅使用LRU缓存表生成草稿序列。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理速度较慢，需要加速方法。现有推测解码方法通常需要额外训练或复杂设计，作者希望开发一种简单、无需训练且模型无关的加速方法。

Method: 利用语言中的局部性，仅使用最近最少使用（LRU）缓存表来存储token n-gram，基于这些缓存生成推测性草稿序列，然后由主模型进行验证。

Result: 在可比方法中达到最先进的性能，尽管设计极简但效果显著，且易于集成到现有系统中，还显示出快速适应新领域的潜力。

Conclusion: Cacheback Decoding是一种简单有效的推测解码方法，无需训练且模型无关，能够显著加速LLM推理，具有实际部署的便利性和适应性。

Abstract: We present Cacheback Decoding, a training-free and model-agnostic speculative decoding method that exploits the locality in language to accelerate Large Language Model (LLM) inference. Cacheback leverages only Least Recently Used (LRU) cache tables of token n-grams to generate draft sequences. Cacheback achieves state-of-the-art performance among comparable methods despite its minimalist design, and its simplicity allows easy integration into existing systems. Cacheback also shows potential for fast adaptation to new domains.

</details>


### [3] [JELV: A Judge of Edit-Level Validity for Evaluation and Automated Reference Expansion in Grammatical Error Correction](https://arxiv.org/abs/2511.21700)
*Yuhao Zhan,Yuqing Zhang,Jing Yuan,Qixiang Ma,Zhiqi Yang,Yu Gu,Zemin Liu,Fei Wu*

Main category: cs.CL

TL;DR: 提出JELV框架，通过编辑级有效性验证解决GEC系统参考多样性不足问题，提升评估准确性和模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有语法错误纠正系统因参考多样性有限，导致评估被低估且模型泛化能力受限

Method: 提出JELV框架，包含多轮LLM-as-Judges管道和蒸馏的DeBERTa分类器，用于验证语法性、忠实度和流畅性三个维度的编辑有效性

Result: JELV与人工标注者达成90%一致率，DeBERTa分类器对有效编辑达到85%精确度；应用JELV重新分类误判假阳性并整合假阳性解耦和流畅度评分，获得与人工判断最先进的相关性；扩展BEA19数据集后重训练GEC系统获得可测量的性能提升

Conclusion: JELV为增强参考多样性和加强评估与模型泛化提供了可扩展的解决方案

Abstract: Existing Grammatical Error Correction (GEC) systems suffer from limited reference diversity, leading to underestimated evaluation and restricted model generalization. To address this issue, we introduce the Judge of Edit-Level Validity (JELV), an automated framework to validate correction edits from grammaticality, faithfulness, and fluency. Using our proposed human-annotated Pair-wise Edit-level Validity Dataset (PEVData) as benchmark, JELV offers two implementations: a multi-turn LLM-as-Judges pipeline achieving 90% agreement with human annotators, and a distilled DeBERTa classifier with 85% precision on valid edits. We then apply JELV to reclassify misjudged false positives in evaluation and derive a comprehensive evaluation metric by integrating false positive decoupling and fluency scoring, resulting in state-of-the-art correlation with human judgments. We also apply JELV to filter LLM-generated correction candidates, expanding the BEA19's single-reference dataset containing 38,692 source sentences. Retraining top GEC systems on this expanded dataset yields measurable performance gains. JELV provides a scalable solution for enhancing reference diversity and strengthening both evaluation and model generalization.

</details>


### [4] [47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations](https://arxiv.org/abs/2511.21701)
*Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song,Ziqian Bi*

Main category: cs.CL

TL;DR: 该论文对27个最先进的大语言模型在中国医学考试题目上进行了全面基准评估，涵盖7个医学专业和2个专业级别，发现Mixtral-8x7B表现最佳，模型大小与性能无一致相关性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，其在医学领域的应用潜力引起了广泛关注。需要评估这些模型在专业医学知识上的表现，特别是在中国医学考试环境下的能力。

Method: 构建了一个包含2,800道精心筛选题目的数据集，涵盖心血管、消化、血液、感染、肾脏、神经和呼吸7个医学专业，分为主治医师和主任医师两个难度级别。评估了27个最先进的大语言模型，建立了稳健的评估框架。

Result: Mixtral-8x7B以74.25%的准确率表现最佳，DeepSeek-R1-671B以64.07%次之。模型大小与性能无一致相关性，小型专家混合架构表现优异。不同医学专业间存在显著性能差异，心血管和神经科表现较好，消化和肾脏科较差。顶级模型在两个难度级别间性能下降很小。

Conclusion: 该基准为LLMs在医学教育和临床决策支持系统中的部署提供了重要见解，展示了这些技术在专业医学环境中的潜力和当前局限性，强调了需要针对特定医学领域进行优化。

Abstract: The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.

</details>


### [5] [CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2511.21702)
*Dong Liu,Yanxuan Yu,Ben Lengerich*

Main category: cs.CL

TL;DR: CSV-Decode 是一种通过几何上界构建小子词汇表的方法，在保持正确性保证的同时显著加速大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时面临显著的计算瓶颈，主要原因是输出层需要在大型词汇表上进行昂贵的计算。

Method: 离线聚类词汇嵌入，使用质心加半径的几何上界为每个解码步骤构建小子词汇表，实现稀疏计算，同时提供精确top-k认证和ε认证的softmax近似保证。

Result: 实验结果显示相比完整词汇表解码有显著加速，同时保持分布保证和较低的回退率。

Conclusion: CSV-Decode 通过几何上界构建小子词汇表的方法，在保证正确性的同时有效解决了大语言模型推理时的计算瓶颈问题。

Abstract: Large language models face significant computational bottlenecks during inference due to the expensive output layer computation over large vocabularies. We present CSV-Decode, a novel approach that uses geometric upper bounds to construct small sub-vocabularies for each decoding step, enabling efficient sparse computation while maintaining dual correctness guarantees: exact top-$k$ certification and $\varepsilon$-certified softmax approximations. Our method clusters vocabulary embeddings offline and uses centroid-plus-radius bounds to identify which tokens can be safely omitted from computation. We provide a complete system implementation with sparse GEMV kernels, multi-GPU sharding, and CUDA Graph optimization. Experimental results demonstrate significant speedup over full vocabulary decoding while maintaining distributional guarantees and low fallback rates. Our code implementation available at \href{https://github.com/FastLM/CSV-Decode}{https://github.com/FastLM/CSV-Decode}.

</details>


### [6] [Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry](https://arxiv.org/abs/2511.21703)
*Siyaxolisa Kabane*

Main category: cs.CL

TL;DR: 研究比较了LLM与非LLM编码器在文本嵌入任务中的泛化能力，并探讨了SLERP模型合并如何缓解任务特定适应带来的过专业化问题。通过数值序列分类实验发现，LLM能更好捕捉高阶组合模式，但易受适配器主导；SLERP合并能恢复基础模型结构同时保留任务增益，实现更好的泛化平衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型语言模型（LLM）与非LLM编码器在文本嵌入任务中的泛化特性差异，并探索如何通过模型合并技术（特别是球形线性插值SLERP）来缓解任务特定适应（如LoRA）导致的过专业化问题，从而获得更好的泛化平衡。

Method: 设计了受控实验套件，让模型嵌入短数值序列并根据数论属性进行分类和聚类。比较了四类模型：1）从头训练或微调的非LLM编码器；2）使用参数高效方法（LoRA）适应的LLM编码器；3）LoRA适应后通过模型融合合并到基础权重的LLM；4）使用SLERP在检查点或阶段间合并的LoRA适应LLM。使用聚类指标（轮廓系数和Davies Bouldin指数）评估表示质量，并分析kmeans标签以检测嵌入中是否包含额外信息。

Result: 实证发现：LLM基础模型能更好地捕捉高阶、组合性数值模式，但容易产生适配器主导现象，损害平衡泛化能力；SLERP合并能一致地恢复基础模型结构，同时保留大部分任务增益，在聚类可分性、鲁棒性和泛化平衡方面优于模型融合或未合并的模型。

Conclusion: LLM在捕捉复杂模式方面优于非LLM编码器，但需要谨慎处理任务适应带来的过专业化问题；SLERP模型合并是一种有效策略，能在保留任务特定改进的同时维持基础模型的泛化能力，实现更好的性能平衡。

Abstract: We investigate the generalization properties of dense text embeddings when the embedding backbone is a large language model (LLM) versus when it is a non-LLM encoder, and we study the extent to which spherical linear interpolation (SLERP) model-merging mitigates over-specialization introduced by task-specific adaptation (e.g., LoRA). To make the comparison concrete and domain-agnostic, we design a controlled suite of experiments in which models embed short numerical sequences and are evaluated on their ability to cluster and classify those sequences according to well-defined number-theoretic properties. Our experimental protocol compares four families of models: (1) non-LLM encoders trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with parameter-efficient methods (LoRA), (3) LLM-based encoders with LoRA followed by model souping merging into the base weights, and (4) the same LoRA-adapted LLMs merged using SLERP across checkpoints or stages. We evaluate representational quality with clustering indices (Silhouette and Davies Bouldin). We additionally analyze the use of kmeans labels to see if the embeddings encode any other information besides the one we are testing for. Empirically, we find that LLM-based backbones produce embeddings that better capture higher-order, compositional numeric patterns, but are prone to adapter dominance that degrades balanced generalization; SLERP merging consistently recovers base-model structure while retaining most task gains, yielding superior tradeoffs in clustering separability, and robustness compared to model souping or models that were not merged.

</details>


### [7] [On the Cross-lingual Transferability of Pre-trained wav2vec2-based Models](https://arxiv.org/abs/2511.21704)
*Jonatas Grosman,Cassio Almeida,Guilherme Schardong,Hélio Lopes*

Main category: cs.CL

TL;DR: 研究调查了基于wav2vec 2.0架构的预训练模型在多语言语音识别任务中的跨语言迁移能力，发现数据多样性比数据量更重要，印欧语系表现优于非印欧语系，且语言相似性促进知识迁移。


<details>
  <summary>Details</summary>
Motivation: 虽然wav2vec 2.0等大型预训练模型在各种语音任务中取得了SOTA结果，但很少有研究深入探讨这些模型在不同语言间的跨语言迁移能力，特别是当目标语言与预训练语言不同时。了解这种迁移特性对于有效利用现有模型和指导新模型预训练至关重要。

Method: 使用15个大型预训练模型在18种语言的语音识别任务上进行微调实验，系统评估不同模型在不同语言上的表现，分析预训练数据规模、语言多样性以及语言相似性对迁移效果的影响。

Result: 1. 预训练数据的多样性比数据规模对最终性能更重要；2. 印欧语系语言的表现优于非印欧语系语言；3. 单语模型存在正向的跨语言知识迁移，且当预训练语言与下游任务语言更相似时，迁移效果更明显。

Conclusion: 该研究揭示了基于wav2vec 2.0的预训练模型在多语言环境下的迁移特性，为科学社区有效利用现有模型和指导新模型预训练提供了重要见解，强调了数据多样性和语言相似性在跨语言迁移中的关键作用。

Abstract: Using representations provided by a large pre-trained model has become the primary strategy for achieving state-of-the-art results in a wide range of tasks. A recently proposed large pre-trained model, wav2vec 2.0, was seminal for several other works on pre-training large models on speech data. Many models are being pre-trained using the same architecture as wav2vec 2.0 and are getting state-of-the-art in various speech-related tasks. Previous work has demonstrated that the data used during the pre-training of these wav2vec2-based models can impact the model's performance in downstream tasks, and this should be taken into consideration before utilizing these models. However, few works have proposed investigating further how the transfer knowledge of these pre-trained models behaves in different languages, even when the target language differs from the one used during the model's pre-training. Our work aims to investigate the cross-lingual transferability of these wav2vec2-based models. We performed several fine-tuning experiments on the speech recognition task in 18 languages using 15 large pre-trained models. The results of our experiments showed us that the size of data used during the pre-training of these models is not as important to the final performance as the diversity. We noticed that the performance of Indo-European languages is superior to non-Indo-European languages in the evaluated models. We have observed a positive cross-lingual transfer of knowledge using monolingual models, which was evident in all the languages we used, but more pronounced when the language used during pre-training was more similar to the downstream task language. With these findings, we aim to assist the scientific community in utilizing existing wav2vec2-based pre-trained models, as well as facilitate the pre-training of new ones.

</details>


### [8] [Insight-A: Attribution-aware for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.21705)
*Junjie Wu,Yumeng Fu,Chen Gong,Guohong Fu*

Main category: cs.CL

TL;DR: 提出Insight-A方法，利用多模态大语言模型（MLLMs）的洞察力，通过溯源伪造来源和分层推理来检测多模态虚假信息


<details>
  <summary>Details</summary>
Motivation: AIGC技术已成为社交媒体上创建多模态虚假信息的普遍手段，对社会安全构成威胁。现有方法使用MLLMs检测虚假信息但忽略了虚假信息的溯源问题

Method: 提出Insight-A方法：1) 使用跨溯源提示（CAP）将虚假信息溯源到伪造来源；2) 使用自动溯源去偏提示（ADP）减少人工标注的主观性；3) 设计图像描述（IC）增强跨模态一致性检查；4) 构建分层推理管道检测跨模态失真

Result: 大量实验证明了该方法的优越性，为AIGC时代的多模态虚假信息检测提供了新范式

Conclusion: Insight-A通过溯源伪造来源和分层推理，有效解决了多模态虚假信息检测中的溯源问题，为应对AIGC时代的虚假信息威胁提供了创新解决方案

Abstract: AI-generated content (AIGC) technology has emerged as a prevalent alternative to create multimodal misinformation on social media platforms, posing unprecedented threats to societal safety. However, standard prompting leverages multimodal large language models (MLLMs) to identify the emerging misinformation, which ignores the misinformation attribution. To this end, we present Insight-A, exploring attribution with MLLM insights for detecting multimodal misinformation. Insight-A makes two efforts: I) attribute misinformation to forgery sources, and II) an effective pipeline with hierarchical reasoning that detects distortions across modalities. Specifically, to attribute misinformation to forgery traces based on generation patterns, we devise cross-attribution prompting (CAP) to model the sophisticated correlations between perception and reasoning. Meanwhile, to reduce the subjectivity of human-annotated prompts, automatic attribution-debiased prompting (ADP) is used for task adaptation on MLLMs. Additionally, we design image captioning (IC) to achieve visual details for enhancing cross-modal consistency checking. Extensive experiments demonstrate the superiority of our proposal and provide a new paradigm for multimodal misinformation detection in the era of AIGC.

</details>


### [9] [A General Highly Accurate Online Planning Method Integrating Large Language Models into Nested Rollout Policy Adaptation for Dialogue Tasks](https://arxiv.org/abs/2511.21706)
*Hui Wang,Fafa Zhang,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.CL

TL;DR: NRPA-GD：一种基于大语言模型的免训练对话策略规划方法，通过嵌套蒙特卡洛模拟和策略自适应优化，在目标导向对话任务中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有目标导向对话方法要么依赖人工经验驱动的提示工程，要么需要训练难以适应新场景的特定策略模型，存在效率低、成本高的问题

Method: 提出NRPA-GD方法，利用LLM同时模拟用户和系统行为，构建对话轨迹完整评估机制，采用嵌套蒙特卡洛模拟和策略自适应优化框架动态调整对话策略

Result: 在四个典型目标导向对话数据集上，NRPA-GD超越了现有提示工程和预训练模型方法，仅用6亿参数的LLM就超越了ChatGPT和预训练策略模型

Conclusion: 该方法展示了在LLM上应用规划方法解决实际规划任务的优势和新颖性，为免训练对话策略优化提供了有效途径

Abstract: In goal-oriented dialogue tasks, the main challenge is to steer the interaction towards a given goal within a limited number of turns. Existing approaches either rely on elaborate prompt engineering, whose effectiveness is heavily dependent on human experience, or integrate policy networks and pre-trained policy models, which are usually difficult to adapt to new dialogue scenarios and costly to train. Therefore, in this paper, we present Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel dialogue policy planning method that completely avoids specific model training by utilizing a Large Language Model (LLM) to simulate behaviors of user and system at the same time. Specifically, NRPA-GD constructs a complete evaluation mechanism for dialogue trajectories and employs an optimization framework of nested Monte Carlo simulation and policy self-adaptation to dynamically adjust policies during the dialogue process. The experimental results on four typical goal-oriented dialogue datasets show that NRPA-GD outperforms both existing prompt engineering and specifically pre-trained model-based methods. Impressively, NRPA-GD surpasses ChatGPT and pre-trained policy models with only a 0.6-billion-parameter LLM. The proposed approach further demonstrates the advantages and novelty of employing planning methods on LLMs to solve practical planning tasks.

</details>


### [10] [Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?](https://arxiv.org/abs/2511.21708)
*Matteo Spreafico,Ludovica Tassini,Camilla Sancricca,Cinzia Cappiello*

Main category: cs.CL

TL;DR: 本文研究大型语言模型在数据准备任务中的能力，包括数据分析和清洗，并与传统工具对比


<details>
  <summary>Details</summary>
Motivation: 数据准备是数据驱动流程中关键但劳动密集的步骤，需要探索大型语言模型是否能有效支持用户选择和自动化数据准备任务

Method: 使用通用和微调的表格型大型语言模型，用低质量数据集进行提示，测量其执行数据分析和清洗任务的能力，并与传统数据准备工具对比

Result: 开发了经过用户研究验证的自定义质量模型，以了解从业者的期望，评估大型语言模型在数据准备任务中的能力

Conclusion: 大型语言模型在支持数据准备任务方面具有潜力，但需要进一步评估其与传统工具相比的实际效果

Abstract: Large language models have recently demonstrated their exceptional capabilities in supporting and automating various tasks. Among the tasks worth exploring for testing large language model capabilities, we considered data preparation, a critical yet often labor-intensive step in data-driven processes. This paper investigates whether large language models can effectively support users in selecting and automating data preparation tasks. To this aim, we considered both general-purpose and fine-tuned tabular large language models. We prompted these models with poor-quality datasets and measured their ability to perform tasks such as data profiling and cleaning. We also compare the support provided by large language models with that offered by traditional data preparation tools. To evaluate the capabilities of large language models, we developed a custom-designed quality model that has been validated through a user study to gain insights into practitioners' expectations.

</details>


### [11] [Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach](https://arxiv.org/abs/2511.21709)
*Blessed Guda,Lawrence Francis,Gabrial Zencha Ashungafac,Carlee Joe-Wong,Moise Busogi*

Main category: cs.CL

TL;DR: 提出PBM指标、BaQCKV高效多数投票和LoRA-1微调策略，用于无监督地量化和缓解LLM在选择题中的选择偏差。


<details>
  <summary>Details</summary>
Motivation: LLM在选择题任务中存在选择偏差（受答案位置或选项符号影响而非内容），这削弱了选择题作为评估框架的可靠性。现有偏差度量需要答案标签且无法充分捕捉模型在不同选项排列下预测的一致性，而现有缓解策略存在计算成本高或泛化能力差的问题。

Method: 1) 提出无监督无标签的排列偏差度量(PBM)，直接量化模型在不同答案排列下预测的不一致性；2) 提出批量问题上下文KV缓存(BaQCKV)的高效多数投票方法，显著降低计算成本；3) 基于PBM和BaQCKV提出无监督低秩适应(LoRA-1)微调策略。

Result: 在多个选择题基准测试上的实验表明，这些方法能够减少偏差，提高准确率的一致性，同时最小化计算成本。

Conclusion: 提出的PBM指标、BaQCKV高效投票和LoRA-1微调策略为LLM选择题评估中的选择偏差问题提供了有效的无监督量化和缓解方案，在保持模型泛化能力的同时显著降低了计算开销。

Abstract: Multiple Choice Question (MCQ) answering is a widely used method for evaluating the performance of Large Language Models (LLMs). However, LLMs often exhibit selection bias in MCQ tasks, where their choices are influenced by factors like answer position or option symbols rather than the content. This bias undermines the reliability of MCQ as an evaluation framework. Most existing selection bias metrics require answer labels and measure divergences between prediction and answer distributions, but do not fully capture the consistency of a model's predictions across different orderings of answer choices. Existing selection bias mitigation strategies have notable limitations: majority voting, though effective, is computationally prohibitive; calibration-based methods require validation sets and often fail to generalize across datasets. To address these gaps, we propose three key contributions: (1) a new unsupervised label-free Permutation Bias Metric (PBM) that directly quantifies inconsistencies in model predictions across answer permutations, providing a more precise measure of selection bias, (2) an efficient majority voting approach called Batch Question-Context KV caching (BaQCKV), to significantly reduce computational costs while preserving bias mitigation effectiveness, and (3) an unsupervised Low-Rank Adaptation (LoRA-1) fine-tuning strategy based on our proposed metric and the BaQCKV that mitigates selection bias, providing a computationally efficient alternative that maintains model generalizability. Experiments across multiple MCQ benchmarks demonstrate that our approaches reduce bias, increasing consistency in accuracy while minimizing computational costs.

</details>


### [12] [Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation](https://arxiv.org/abs/2511.21711)
*Fatima Kazi*

Main category: cs.CL

TL;DR: 研究评估大型语言模型中的偏见问题，使用StereoSet和CrowSPairs基准测试BERT、GPT-3.5等模型，发现微调模型在性别偏见方面表现不佳但在种族偏见识别上较好，通过增强学习策略可将隐式偏见检测性能提升达20%。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在自然语言处理领域的快速发展及其在各学科和日常生活中的广泛应用，这些模型从训练数据中继承了显性和隐性的偏见，包括社会、伦理、文化、宗教等方面的偏见和刻板印象。需要全面检查这些缺陷，识别偏见的存在和程度，了解其来源，并尝试减轻这些有偏见的输出，以确保公平输出，减少有害的刻板印象和错误信息。

Method: 采用三管齐下的方法进行彻底和包容性分析：1) 使用StereoSet和CrowSPairs等偏见特定基准测试评估BERT、GPT-3.5、ADA等多种生成模型中的各种偏见；2) 检测显性和隐性偏见；3) 应用增强学习策略，包括微调模型、使用不同的提示技术和偏见基准的数据增强。

Result: 结果显示：1) 微调模型在性别偏见方面表现不佳，但在识别和避免种族偏见方面表现优异；2) 尽管在某些情况下成功，但大型语言模型往往过度依赖提示中的关键词，无法真正理解其输出的准确性和真实性；3) 增强学习策略使微调模型在跨数据集测试中表现出有希望的适应性，在隐性偏见基准测试中性能显著提升，性能增益高达20%。

Conclusion: 研究强调了在生成式人工智能快速发展背景下解决大型语言模型中偏见问题的重要性。虽然微调模型在特定偏见检测方面表现出优势，但模型仍存在过度依赖关键词和无法真正理解输出的局限性。增强学习策略为改善模型性能提供了有效途径，特别是在隐性偏见检测方面取得了显著进展，表明通过适当的技术干预可以减轻语言模型中的偏见问题。

Abstract: Large Language models (LLMs), such as ChatGPT, have gained popularity in recent years with the advancement of Natural Language Processing (NLP), with use cases spanning many disciplines and daily lives as well. LLMs inherit explicit and implicit biases from the datasets they were trained on; these biases can include social, ethical, cultural, religious, and other prejudices and stereotypes. It is important to comprehensively examine such shortcomings by identifying the existence and extent of such biases, recognizing the origin, and attempting to mitigate such biased outputs to ensure fair outputs to reduce harmful stereotypes and misinformation. This study inspects and highlights the need to address biases in LLMs amid growing generative Artificial Intelligence (AI). We utilize bias-specific benchmarks such StereoSet and CrowSPairs to evaluate the existence of various biases in many different generative models such as BERT, GPT 3.5, and ADA. To detect both explicit and implicit biases, we adopt a three-pronged approach for thorough and inclusive analysis. Results indicate fine-tuned models struggle with gender biases but excel at identifying and avoiding racial biases. Our findings also illustrated that despite some cases of success, LLMs often over-rely on keywords in prompts and its outputs. This demonstrates the incapability of LLMs to attempt to truly understand the accuracy and authenticity of its outputs. Finally, in an attempt to bolster model performance, we applied an enhancement learning strategy involving fine-tuning, models using different prompting techniques, and data augmentation of the bias benchmarks. We found fine-tuned models to exhibit promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.

</details>


### [13] [EulerESG: Automating ESG Disclosure Analysis with LLMs](https://arxiv.org/abs/2511.21712)
*Yi Ding,Xushuo Tang,Zhengyi Yang,Wenqian Zhang,Simin Wu,Yuxin Huang,Lingjing Lan,Weiyuan Li,Yin Chen,Mingchen Ju,Wenke Yang,Thong Hoang,Mykhailo Klymenko,Xiwei Zu,Wenjie Zhang*

Main category: cs.CL

TL;DR: EulerESG是一个基于LLM的系统，用于自动化ESG披露分析，通过双通道检索和LLM驱动的方法，能够以高准确率填充标准对齐的指标表。


<details>
  <summary>Details</summary>
Motivation: ESG报告主要以冗长、异构的PDF文档形式发布，难以系统回答简单问题。现有工具要么依赖脆弱的基于规则的提取，要么将ESG报告视为通用文本，没有明确建模底层报告标准。

Method: 结合(i)双通道检索和LLM驱动的ESG报告披露分析，以及(ii)交互式仪表板和聊天机器人，用于探索、基准测试和解释。系统明确考虑ESG框架，使用LLM模型进行自动化分析。

Result: 在四个全球知名公司和十二个SASB子行业上测试，EulerESG能够以高保真度（最高0.95平均准确率）自动填充标准对齐的指标表，同时保持端到端运行时间的实用性，并比较了多个近期LLM模型的表现。

Conclusion: EulerESG是一个实用的LLM驱动系统，能够有效自动化ESG披露分析，填补了现有工具的不足，为ESG报告分析提供了高效、准确的解决方案。

Abstract: Environmental, Social, and Governance (ESG) reports have become central to how companies communicate climate risk, social impact, and governance practices, yet they are still published primarily as long, heterogeneous PDF documents. This makes it difficult to systematically answer seemingly simple questions. Existing tools either rely on brittle rule-based extraction or treat ESG reports as generic text, without explicitly modelling the underlying reporting standards. We present \textbf{EulerESG}, an LLM-powered system for automating ESG disclosure analysis with explicit awareness of ESG frameworks. EulerESG combines (i) dual-channel retrieval and LLM-driven disclosure analysis over ESG reports, and (ii) an interactive dashboard and chatbot for exploration, benchmarking, and explanation. Using four globally recognised companies and twelve SASB sub-industries, we show that EulerESG can automatically populate standard-aligned metric tables with high fidelity (up to 0.95 average accuracy) while remaining practical in end-to-end runtime, and we compare several recent LLM models in this setting. The full implementation, together with a demonstration video, is publicly available at https://github.com/UNSW-database/EulerESG.

</details>


### [14] [GPS: General Per-Sample Prompter](https://arxiv.org/abs/2511.21714)
*Pawel Batorski,Paul Swoboda*

Main category: cs.CL

TL;DR: GPS是一种通用的按样本提示方法，无需任务特定调优即可为每个输入生成定制提示，在多个任务上实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示方法存在三个关键限制：需要大量任务特定数据训练、依赖耗时的优化循环、只能生成单一任务级提示而无法适应个体输入。手动设计提示既困难又耗时。

Method: 提出GPS方法：1) 使用强化学习在训练任务套件上训练提示生成器；2) 引入新颖的正则化技术以有效适应按样本提示；3) 采用最小贝叶斯风险解码来稳定推理。

Result: 在文本简化任务中达到基线方法第二好结果，在摘要任务中第三好，在分类任务中表现相当，且未在这些任务上进行训练。在GSM8K任务上获得最先进性能。

Conclusion: GPS展示了一种新颖有效的自动提示范式：无需大量优化和任务特定训练集即可生成自适应、输入特定的提示，具有实际应用潜力。

Abstract: LLMs are sensitive to prompting, with task performance often hinging on subtle, sometimes imperceptible variations in phrasing. As a result, crafting effective prompts manually remains challenging and time-consuming. Recent automatic prompting methods mitigate this difficulty but face three key limitations: (i) for each new task, they require large datasets to train good prompts;(ii) they rely on costly optimization loops that may take hours; (iii)they typically produce a single task-level prompt that does not adapt to the individual input problem to be solved.
  We propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen input, improving performance across diverse tasks. The prompter is trained with reinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.
  Empirically, GPS demonstrates competitive performance: we attain second best results among baselines on text simplification, third best results on summarization and on-par results on classification, while not training on any of these tasks, in contrast to the baselines. For in-domain prompting, we obtain sota on GSM8K. Our work shows the potential of a novel and effective paradigm for automatic prompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Our code is available at https://github.com/Batorskq/GPS.

</details>


### [15] [An Optimized Machine Learning Classifier for Detecting Fake Reviews Using Extracted Features](https://arxiv.org/abs/2511.21716)
*Shabbir Anees,Anshuman,Ayush Chaurasia,Prathmesh Bogar*

Main category: cs.CL

TL;DR: 提出基于机器学习的方法检测AI生成的虚假评论，结合文本预处理、多模态特征提取、哈里斯鹰优化算法和堆叠集成分类器，在公开数据集上达到95.4%准确率


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的发展，计算机生成的虚假评论混入真实评论中，损害在线购物的可信度，需要有效方法识别这些AI生成的虚假内容

Method: 采用文本预处理、多模态特征提取、哈里斯鹰优化算法进行特征选择，使用堆叠集成分类器进行检测，并考虑云平台上的隐私保护技术

Result: 在40,432条真实和AI生成评论的数据集上，哈里斯鹰优化将特征从13,539个减少到1,368个（降维89.9%），最终模型达到95.40%准确率、92.81%精确率、95.01%召回率和93.90% F1分数

Conclusion: 集成学习与生物启发优化相结合是检测机器生成文本的有效方法，在云平台上应用时需结合差分隐私和安全外包等隐私保护技术

Abstract: It is well known that fraudulent reviews cast doubt on the legitimacy and dependability of online purchases. The most recent development that leads customers towards darkness is the appearance of human reviews in computer-generated (CG) ones. In this work, we present an advanced machine-learning-based system that analyses these reviews produced by AI with remarkable precision. Our method integrates advanced text preprocessing, multi-modal feature extraction, Harris Hawks Optimization (HHO) for feature selection, and a stacking ensemble classifier. We implemented this methodology on a public dataset of 40,432 Original (OR) and Computer-Generated (CG) reviews. From an initial set of 13,539 features, HHO selected the most applicable 1,368 features, achieving an 89.9% dimensionality reduction. Our final stacking model achieved 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-Score, which demonstrates that the combination of ensemble learning and bio-inspired optimisation is an effective method for machine-generated text recognition. Because large-scale review analytics commonly run on cloud platforms, privacy-preserving techniques such as differential approaches and secure outsourcing are essential to protect user data in these systems.

</details>


### [16] [CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution](https://arxiv.org/abs/2511.21717)
*Baoliang Tian,Yuxuan Si,Jilong Wang,Lingyao Li,Zhongyuan Bao,Zineng Zhou,Tao Wang,Sixu Li,Ziyao Xu,Mingze Wang,Zhouzhuo Zhang,Zhihao Wang,Yike Yun,Ke Tian,Ning Yang,Minghui Qiu*

Main category: cs.CL

TL;DR: CrossCheck-Bench是一个诊断性基准测试，用于评估多模态大语言模型在检测和解决图像-文本矛盾方面的能力，包含15k个问题-答案对，覆盖三个推理复杂度层次和七种核心能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要在对齐的图像-文本对上训练和评估，缺乏对现实世界中不一致性检测和解决能力的探索。在开放域应用中，视觉和文本线索经常冲突，需要模型进行超越表面对齐的结构化推理。

Method: 构建CrossCheck-Bench基准测试，采用分层任务框架覆盖三个推理复杂度层次，定义七种解决跨模态不一致性的核心能力。数据集包含15k个问题-答案对，来自真实世界素材并注入合成矛盾，通过多阶段标注流程（450+专家小时）确保语义有效性和难度校准。

Result: 评估13个最先进视觉语言模型发现：1) 随着任务从感知匹配转向逻辑矛盾检测，性能一致下降；2) 大多数模型在孤立实体识别上表现良好，但在需要综合多个线索进行冲突推理时失败；3) 能力分析显示技能获取不均衡，特别是在需要多步推理或基于规则验证的任务中；4) 传统提示策略（如思维链、标记集）仅带来边际改进，而将符号推理与基础视觉处理交织的方法获得更稳定提升。

Conclusion: 多模态推理存在持续瓶颈，需要新的方法来构建能够进行稳健跨模态验证的模型。将符号推理与基础视觉处理相结合的方法显示出潜力，为未来研究指明了方向。

Abstract: Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.

</details>


### [17] [When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers](https://arxiv.org/abs/2511.21718)
*Zhaoxin Zhang,Borui Chen,Yiming Hu,Youyang Qu,Tianqing Zhu,Longxiang Gao*

Main category: cs.CL

TL;DR: MICM是一种新型的LLM越狱方法，通过概念形态学理论编码微妙概念配置，操纵模型隐含的社会价值观，绕过安全机制生成不当内容，在多个先进LLM上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全研究主要关注直接绕过安全机制获取有害输出的方法，但忽视了攻击模型抽象泛化能力的漏洞。这种盲点使得攻击者可以通过微妙操纵模型输出中隐含的社会价值观来诱导不当内容，而现有对齐策略对此防御不足。

Method: MICM基于概念形态学理论，通过预定义短语集将微妙概念的具体配置编码到固定提示模板中。这些短语作为概念触发器，引导模型输出朝向特定价值立场，同时避免触发传统安全过滤器。该方法具有模型无关性。

Result: 在GPT-4o、Deepseek-R1、Qwen3-8B等五个先进LLM上的实验表明，MICM在保持低拒绝率的同时，始终优于最先进的越狱技术，取得了高成功率。

Conclusion: 研究发现商业LLM的安全机制对底层价值对齐的隐蔽操纵仍然脆弱，揭示了当前对齐策略的关键漏洞，需要开发更精细的价值检测和防御机制。

Abstract: Recent research on large language model (LLM) jailbreaks has primarily focused on techniques that bypass safety mechanisms to elicit overtly harmful outputs. However, such efforts often overlook attacks that exploit the model's capacity for abstract generalization, creating a critical blind spot in current alignment strategies. This gap enables adversaries to induce objectionable content by subtly manipulating the implicit social values embedded in model outputs. In this paper, we introduce MICM, a novel, model-agnostic jailbreak method that targets the aggregate value structure reflected in LLM responses. Drawing on conceptual morphology theory, MICM encodes specific configurations of nuanced concepts into a fixed prompt template through a predefined set of phrases. These phrases act as conceptual triggers, steering model outputs toward a specific value stance without triggering conventional safety filters. We evaluate MICM across five advanced LLMs, including GPT-4o, Deepseek-R1, and Qwen3-8B. Experimental results show that MICM consistently outperforms state-of-the-art jailbreak techniques, achieving high success rates with minimal rejection. Our findings reveal a critical vulnerability in commercial LLMs: their safety mechanisms remain susceptible to covert manipulation of underlying value alignment.

</details>


### [18] [PeerCoPilot: A Language Model-Powered Assistant for Behavioral Health Organizations](https://arxiv.org/abs/2511.21721)
*Gao Mo,Naveen Raman,Megan Chai,Cindy Peng,Shannon Pagdon,Nev Jones,Hong Shen,Peggy Swarbrick,Fei Fang*

Main category: cs.CL

TL;DR: PeerCoPilot是一个基于大语言模型的助手，帮助同伴提供者制定健康计划、设定目标并查找资源，已在真实行为健康组织中部署使用。


<details>
  <summary>Details</summary>
Motivation: 行为健康问题是美国最主要的疾病负担，同伴运营的行为健康组织(PROs)在帮助患者方面至关重要，但资金和人员有限，难以满足所有服务需求。

Method: 开发PeerCoPilot系统，采用检索增强生成(RAG)管道，基于1300多个经过审核的资源数据库，确保信息可靠性。系统帮助同伴提供者创建健康计划、构建分步目标并定位组织资源。

Result: 人类评估显示超过90%的用户支持使用PeerCoPilot。系统比基线LLM提供更可靠和具体的信息。目前已在CSPNJ组织中由5-10名同伴提供者使用，服务超过10,000名用户。

Conclusion: PeerCoPilot是一个有效的LLM助手，能够帮助同伴提供者更好地服务行为健康患者，系统已在实际环境中成功部署并计划进一步扩展。

Abstract: Behavioral health conditions, which include mental health and substance use disorders, are the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) critically assist individuals facing these conditions by combining mental health services with assistance for needs such as income, employment, and housing. However, limited funds and staffing make it difficult for PROs to address all service user needs. To assist peer providers at PROs with their day-to-day tasks, we introduce PeerCoPilot, a large language model (LLM)-powered assistant that helps peer providers create wellness plans, construct step-by-step goals, and locate organizational resources to support these goals. PeerCoPilot ensures information reliability through a retrieval-augmented generation pipeline backed by a large database of over 1,300 vetted resources. We conducted human evaluations with 15 peer providers and 6 service users and found that over 90% of users supported using PeerCoPilot. Moreover, we demonstrated that PeerCoPilot provides more reliable and specific information than a baseline LLM. PeerCoPilot is now used by a group of 5-10 peer providers at CSPNJ, a large behavioral health organization serving over 10,000 service users, and we are actively expanding PeerCoPilot's use.

</details>


### [19] [German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies](https://arxiv.org/abs/2511.21722)
*Jens Rupprecht,Leon Fröhling,Claudia Wagner,Markus Strohmaier*

Main category: cs.CL

TL;DR: GGP是一个基于德国综合社会调查构建的全面、有代表性的人格提示集合，用于引导LLM生成与德国人口对齐的响应，在数据稀缺时优于现有分类器。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM通过人格提示模拟人类视角的研究缺乏经过精心策划、基于实证的人格集合，限制了模拟的准确性和代表性。

Method: 基于德国综合社会调查(ALLBUS)构建德国通用人格(GGP)集合，设计易于集成到各种LLM和任务中的人格提示，评估其在模拟调查响应分布上的表现。

Result: GGP引导的LLM在模拟多样主题的调查响应分布上优于最先进的分类器，特别是在数据稀缺条件下。人格提示的代表性和属性选择显著影响与人口响应的对齐程度。

Conclusion: GGP为基于LLM的社会模拟研究提供了有价值的资源，能够促进NLP和社会科学研究中更系统化的人口对齐人格提示探索。

Abstract: The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.

</details>


### [20] [AD-CDO: A Lightweight Ontology for Representing Eligibility Criteria in Alzheimer's Disease Clinical Trials](https://arxiv.org/abs/2511.21724)
*Zenan Sun,Rashmie Abeysinghe,Xiaojin Li,Xinyue Hu,Licong Cui,Guo-Qiang Zhang,Jiang Bian,Cui Tao*

Main category: cs.CL

TL;DR: AD-CDO是一个轻量级语义本体，用于标准化阿尔茨海默病临床试验的资格标准概念，覆盖63%的试验概念，支持试验模拟和实体规范化等应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有生物医学本体过于宽泛、无法满足特定临床试验建模需求的问题，需要开发一个专门针对阿尔茨海默病临床试验资格标准的轻量级语义本体。

Method: 从1500多个AD临床试验中提取高频概念，分为7个语义类别，使用UMLS、OMOP等标准词汇进行标注，应用Jenks自然断点法优化概念集平衡覆盖率和可管理性。

Result: 优化后的AD-CDO覆盖了超过63%的提取试验概念，同时保持了可解释性和紧凑性。通过两个用例展示了实用性：试验模拟系统和实体规范化任务。

Conclusion: AD-CDO通过协调关键资格实体并与标准化词汇对齐，为基于本体的AD临床试验研究提供了多功能基础，支持表型算法开发、队列识别和结构化数据集成等应用。

Abstract: Objective
  This study introduces the Alzheimer's Disease Common Data Element Ontology for Clinical Trials (AD-CDO), a lightweight, semantically enriched ontology designed to represent and standardize key eligibility criteria concepts in Alzheimer's disease (AD) clinical trials.
  Materials and Methods
  We extracted high-frequency concepts from more than 1,500 AD clinical trials on ClinicalTrials.gov and organized them into seven semantic categories: Disease, Medication, Diagnostic Test, Procedure, Social Determinants of Health, Rating Criteria, and Fertility. Each concept was annotated with standard biomedical vocabularies, including the UMLS, OMOP Standardized Vocabularies, DrugBank, NDC, and NLM VSAC value sets. To balance coverage and manageability, we applied the Jenks Natural Breaks method to identify an optimal set of representative concepts.
  Results
  The optimized AD-CDO achieved over 63% coverage of extracted trial concepts while maintaining interpretability and compactness. The ontology effectively captured the most frequent and clinically meaningful entities used in AD eligibility criteria. We demonstrated AD-CDO's practical utility through two use cases: (a) an ontology-driven trial simulation system for formal modeling and virtual execution of clinical trials, and (b) an entity normalization task mapping raw clinical text to ontology-aligned terms, enabling consistency and integration with EHR data.
  Discussion
  AD-CDO bridges the gap between broad biomedical ontologies and task-specific trial modeling needs. It supports multiple downstream applications, including phenotyping algorithm development, cohort identification, and structured data integration.
  Conclusion
  By harmonizing essential eligibility entities and aligning them with standardized vocabularies, AD-CDO provides a versatile foundation for ontology-driven AD clinical trial research.

</details>


### [21] [PromptTailor: Multi-turn Intent-Aligned Prompt Synthesis for Lightweight LLMs](https://arxiv.org/abs/2511.21725)
*Yizhou Xu,Janet Davis*

Main category: cs.CL

TL;DR: PromptTailor是一个轻量级提示优化系统，使用量化Llama3-8B模型和LoRA适配器，通过从强LLM蒸馏学习的方法，将用户简单指令扩展为丰富、领域感知的提示，同时保持用户意图对齐。


<details>
  <summary>Details</summary>
Motivation: 轻量级语言模型在设备端和隐私敏感应用中很有吸引力，但它们的响应质量对提示质量高度敏感。非专业用户通常缺乏知识或时间来制作高质量提示，而现有提示优化工具难以确保优化后的提示与用户原始意图和偏好保持一致。

Method: 使用量化Llama3-8B模型，通过轻量级LoRA适配器在12,300个提示优化对话上进行微调，这些对话涵盖41个日常领域，从三个更强的LLM蒸馏而来。系统可以将最小用户指令扩展为丰富、领域感知的提示。

Result: 在人类和LLM评估中，PromptTailor在多个目标模型和优化基准上，比思维链提示获得更高的偏好率，匹配或超越最先进的提示优化方法，同时需要更少的模型调用（如3次 vs 9次）。

Conclusion: 紧凑的学生模型在强大教师模型的指导下，可以学习有效的提示生成策略，在保持与用户意图对齐的同时提高响应质量，适合边缘部署。

Abstract: Lightweight language models remain attractive for on-device and privacy-sensitive applications, but their responses are highly sensitive to prompt quality. For open-ended generation, non-expert users often lack the knowledge or time to consistently craft high-quality prompts, leading them to rely on prompt optimization tools. However, a key challenge is ensuring the optimized prompts genuinely align with users' original intents and preferences. We introduce PromptTailor, a system for controllable prompt generation for open-ended text that improves model output quality by intent-aligned prompt synthesis. PromptTailor expands minimal user instructions into rich, domain-aware prompts while preserving the user's stated preferences. The system is a quantized Llama3-8B model fine-tuned with a lightweight LoRA adapter on 12,300 prompt-refinement dialogues spanning 41 everyday domains, distilled from three stronger LLMs. The adapter attaches to any Llama3-8B base, enabling edge deployment. In human and LLM-judge evaluations across multiple target models and optimization baselines, PromptTailor yields higher preference rates than chain-of-thought prompting and matches or surpasses state-of-the-art prompt optimization methods while requiring fewer model calls (e.g., 3 vs. 9). These results show that a compact student, guided by powerful teachers, can learn effective prompt-generation strategies that enhance response quality while maintaining alignment with user intent.

</details>


### [22] [Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks](https://arxiv.org/abs/2511.21726)
*Yicong Zheng,Kevin L. McKee,Thomas Miconi,Zacharie Bugaud,Mick van Gelderen,Jed McCaleb*

Main category: cs.CL

TL;DR: SUMER提出了一种基于强化学习的搜索方法，直接在未压缩的原始数据中进行目标导向搜索，超越了现有基于压缩的记忆方法，在长上下文对话理解任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆框架和基准测试主要关注寻找最优的记忆压缩算法，但这引入了人为偏见，且压缩是信息有损的，预定义的压缩算法无法适应所有原始数据分布。需要一种更通用的解决方案。

Method: 提出SUMER（Search in Uncompressed Memory via Experience Replay），一个端到端的强化学习智能体，学习使用搜索工具收集信息并回答目标问题，采用可验证奖励的强化学习（RLVR）。

Result: 在LoCoMo长上下文对话理解数据集上，SUMER使用Qwen2.5-7B-Instruct模型学习使用搜索工具，超越了所有基于压缩的记忆方法和完整上下文基线，达到SOTA性能（比之前最佳提升43%）。

Conclusion: 简单的搜索方法应用于原始数据优于当前长上下文记忆任务中的目标无关和带有偏见的压缩算法，需要更动态和自主可扩展的新范式与基准测试。

Abstract: How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.

</details>


### [23] [Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue](https://arxiv.org/abs/2511.21728)
*Lin Yu,Xiaofei Han,Yifei Kang,Chiung-Yi Tseng,Danyang Zhang,Ziqian Bi,Zhimo Han*

Main category: cs.CL

TL;DR: AffectMind是一个多模态情感对话代理，通过主动推理和动态知识基础，在营销对话中实现情感对齐和说服性交互，显著优于现有LLM基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在情感丰富、目标导向的营销对话中表现被动，难以维持情感对齐和说服性交互，需要更主动的情感推理能力。

Method: 提出AffectMind系统，包含三个核心组件：主动知识基础网络（PKGN）从文本、视觉和韵律更新事实和情感上下文；情感-意图对齐模型（EIAM）联合建模用户情感和购买意图；强化话语循环（RDL）通过用户反馈优化情感连贯性和参与度。

Result: 在两个新构建的营销对话数据集上，AffectMind在情感一致性（+26%）、说服成功率（+19%）和长期用户参与度（+23%）方面显著优于强LLM基线。

Conclusion: 情感基础的主动性是多模态商业代理的关键能力，AffectMind通过主动推理和动态知识基础，在情感丰富的目标导向对话中实现了更有效的交互。

Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.

</details>


### [24] [Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems](https://arxiv.org/abs/2511.21729)
*Jithin Krishnan*

Main category: cs.CL

TL;DR: RAG系统性能提升的关键在于组件间的协同整合，而非单个组件的强度。通过50个查询的消融研究，发现混合检索、集成验证和自适应阈值等增强措施单独使用时几乎无益，但协同使用可将弃答率从40%降至2%且不增加幻觉。同时揭示了测量挑战：不同验证策略会产生不一致标签，导致幻觉率评估失真。


<details>
  <summary>Details</summary>
Motivation: 构建可靠的RAG系统需要理解组件间的相互作用，而不仅仅是添加强大组件。现有研究往往关注单个组件的改进，但缺乏对组件协同效应的系统分析。同时，不同验证策略产生的标签不一致问题也影响了性能评估的准确性。

Method: 使用消融研究分析50个查询（15个可回答、10个边缘案例、25个对抗性查询），评估混合检索、集成验证和自适应阈值等增强措施的效果。特别关注不同验证策略产生的标签不一致问题，以及组件间的协同作用。

Result: 增强措施单独使用时几乎无益，但协同整合可实现95%的弃答率降低（从40%到2%），且不增加幻觉。发现测量挑战：不同验证策略会产生不一致标签（如"弃答"vs"不支持"），导致幻觉率评估失真。需要自适应校准来防止检索质量高时的过度自信回答。

Conclusion: RAG系统的性能提升关键在于组件的协同整合而非单个组件强度。需要标准化指标和标签来正确解释性能，自适应校准对于防止过度自信回答至关重要。组件间的协同效应比任何单个组件的强度更重要。

Abstract: Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, "abstained" versus "unsupported"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.

</details>


### [25] [A Benchmark for Procedural Memory Retrieval in Language Agents](https://arxiv.org/abs/2511.21730)
*Ishant Kohar,Aswanth Krishnan*

Main category: cs.CL

TL;DR: 该论文提出了首个评估AI代理程序记忆检索能力的基准测试，发现现有嵌入方法在熟悉场景表现良好但在新场景急剧下降，而LLM生成的程序抽象能实现可靠的跨上下文迁移。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在熟悉环境中表现出色，但在面对具有未见词汇的新任务时表现急剧下降——这是程序记忆系统的核心限制。需要评估代理是否能识别跨越不同对象实例的功能等价程序。

Method: 使用ALFWorld构建专家和LLM生成轨迹的双重语料库，评估六种检索方法，使用系统分层查询，并进行受控消融实验。

Result: 发现明显的泛化悬崖：基于嵌入的方法在熟悉上下文中表现强劲，但在新场景中显著退化；LLM生成的程序抽象展示了可靠的跨上下文迁移能力。语料库规模比表示丰富度带来更大收益。

Conclusion: 该基准测试提供了首个诊断框架，能够区分真正的程序理解和表面记忆，并为开发具有可靠泛化能力的检索系统提供了工具。

Abstract: Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).

</details>


### [26] [Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition](https://arxiv.org/abs/2511.21731)
*Diederik Aerts,Jonito Aerts Arguëlles,Lester Beltran,Suzette Geriente,Roberto Leporini,Massimiliano Sassoli de Bianchi,Sandro Sozzo*

Main category: cs.CL

TL;DR: 该研究通过认知测试发现，大型语言模型（LLM）在概念组合中表现出量子纠缠特征，在文本词分布中呈现玻色-爱因斯坦统计而非麦克斯韦-玻尔兹曼统计，这与人类认知实验结果一致，表明概念-语言领域存在系统性的量子结构涌现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究人类认知与人工智能语言模型在概念组织和语言处理方面是否存在共同的底层结构。作者希望验证量子认知理论中提出的"概念-语言领域的量子结构"是否不仅存在于人类认知中，也存在于LLM中。

Method: 采用两种认知测试方法：1）使用ChatGPT和Gemini进行概念组合测试，测量贝尔不等式违反情况；2）使用相同LLM分析大型文本中的词分布统计特性。将结果与先前人类认知实验和语料库信息检索结果进行比较。

Result: 测试结果显示：1）LLM在概念组合中显著违反贝尔不等式，表明存在"量子纠缠"现象；2）文本词分布呈现玻色-爱因斯坦统计而非预期的麦克斯韦-玻尔兹曼统计。这些发现与人类认知实验结果高度一致。

Conclusion: 研究结论认为，概念-语言领域存在系统性的量子结构涌现，这种结构在人类认知和LLM中都存在。作者提出统一框架解释这种普遍存在的量子意义组织，并认为向量空间的分布式语义结构是知识组织的本质形式，导致了人类认知与LLM认知的进化趋同。

Abstract: We present the results of cognitive tests on conceptual combinations, performed using specific Large Language Models (LLMs) as test subjects. In the first test, performed with ChatGPT and Gemini, we show that Bell's inequalities are significantly violated, which indicates the presence of 'quantum entanglement' in the tested concepts. In the second test, also performed using ChatGPT and Gemini, we instead identify the presence of 'Bose-Einstein statistics', rather than the intuitively expected 'Maxwell-Boltzmann statistics', in the distribution of the words contained in large-size texts. Interestingly, these findings mirror the results previously obtained in both cognitive tests with human participants and information retrieval tests on large corpora. Taken together, they point to the 'systematic emergence of quantum structures in conceptual-linguistic domains', regardless of whether the cognitive agent is human or artificial. Although LLMs are classified as neural networks for historical reasons, we believe that a more essential form of knowledge organization takes place in the distributive semantic structure of vector spaces built on top of the neural network. It is this meaning-bearing structure that lends itself to a phenomenon of evolutionary convergence between human cognition and language, slowly established through biological evolution, and LLM cognition and language, emerging much more rapidly as a result of self-learning and training. We analyze various aspects and examples that contain evidence supporting the above hypothesis. We also advance a unifying framework that explains the pervasive quantum organization of meaning that we identify.

</details>


### [27] [HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation](https://arxiv.org/abs/2511.21732)
*Jiajun Zhang,Shijia Luo,Ruikang Zhang,Qi Su*

Main category: cs.CL

TL;DR: HUMORCHAIN：首个将幽默认知理论嵌入多模态幽默生成的框架，通过视觉语义解析、幽默心理学推理和幽默评估器，实现从视觉理解到幽默创作的结构化推理链。


<details>
  <summary>Details</summary>
Motivation: 幽默作为人类创造性活动和社会纽带机制，对AI生成构成重大挑战。现有数据驱动方法缺乏对幽默的显式建模或理论基础，生成的图像描述流畅但缺乏真正的幽默或认知深度。多模态幽默已成为在线交流的普遍形式，需要AI系统能够整合视觉理解与幽默语言生成。

Method: 提出HUMORCHAIN框架，整合视觉语义解析、基于幽默和心理学的推理，以及微调的幽默评估判别器，形成可解释和可控的认知推理链。这是首个将幽默理论中的认知结构显式嵌入多模态幽默生成的工作。

Result: 在Meme-Image-No-Text、Oogiri-GO和OxfordTVG-HIC数据集上的实验表明，HUMORCHAIN在人类幽默偏好、Elo/BT分数和语义多样性方面优于最先进的基线方法。

Conclusion: 理论驱动的结构化推理使大语言模型能够生成符合人类感知的幽默，证明了将认知理论嵌入多模态幽默生成的有效性。

Abstract: Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.

</details>


### [28] [RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models](https://arxiv.org/abs/2511.21733)
*Dayan Pan,Jingyuan Wang,Yilong Zhou,Jiawei Cheng,Pengyue Jia,Xiangyu Zhao*

Main category: cs.CL

TL;DR: RoSA是一种新型参数高效微调方法，通过RoPE感知的注意力增强和动态层选择，在可训练参数相近的情况下优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 当前PEFT方法通常忽略模型组件的不同角色和各层的异质性重要性，限制了适应效率。研究发现RoPE在注意力状态的低频维度中诱导关键激活，这启发了更针对性的参数分配策略。

Method: RoSA包含两个核心组件：1) RoPE感知注意力增强模块，选择性地增强RoPE影响的注意力状态的低频分量；2) 动态层选择策略，基于LayerNorm梯度范数自适应识别和更新最关键层。

Result: 在15个常识和算术基准测试上的广泛实验表明，RoSA在可训练参数相近的情况下优于现有主流PEFT方法。

Conclusion: 通过结合维度增强和层适应，RoSA实现了更针对性和高效的微调，为参数高效微调提供了新思路。

Abstract: Fine-tuning large language models is essential for task-specific adaptation, yet it remains computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution, but current approaches typically ignore the distinct roles of model components and the heterogeneous importance across layers, thereby limiting adaptation efficiency. Motivated by the observation that Rotary Position Embeddings (RoPE) induce critical activations in the low-frequency dimensions of attention states, we propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters in a more targeted and effective manner. RoSA comprises a RoPE-aware Attention Enhancement (RoAE) module, which selectively enhances the low-frequency components of RoPE-influenced attention states, and a Dynamic Layer Selection (DLS) strategy that adaptively identifies and updates the most critical layers based on LayerNorm gradient norms. By combining dimension-wise enhancement with layer-wise adaptation, RoSA achieves more targeted and efficient fine-tuning. Extensive experiments on fifteen commonsense and arithmetic benchmarks demonstrate that RoSA outperforms existing mainstream PEFT methods under comparable trainable parameters. The code is available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/RoSA.

</details>


### [29] [Asking LLMs to Verify First is Almost Free Lunch](https://arxiv.org/abs/2511.21734)
*Shiguang Wu,Quanming Yao*

Main category: cs.CL

TL;DR: 提出Verification-First策略，让LLMs先验证候选答案再生成解决方案，触发"逆向推理"过程，比标准CoT更有效且计算开销小


<details>
  <summary>Details</summary>
Motivation: 在不增加训练成本或大量测试时采样的前提下，提升大型语言模型的推理能力。现有方法如Chain-of-Thought需要大量计算资源，而验证优先策略能以更低的成本实现更好的推理效果。

Method: 提出Verification-First策略：先让模型验证一个候选答案（即使是随机或简单的答案），然后再生成解决方案。这种方法触发"逆向推理"过程，与标准前向CoT互补。进一步扩展为Iter-VF：一种顺序测试时扩展方法，通过迭代循环验证-生成过程，使用模型之前的答案。

Result: 在多种基准测试（从数学推理到编码和代理任务）和多种LLM（从开源1B到尖端商业模型）上的广泛实验证实：VF使用随机答案始终优于标准CoT，且计算开销最小；Iter-VF优于现有的测试时扩展策略。

Conclusion: Verification-First策略通过触发逆向推理过程，有效提升了LLMs的推理能力，且成本效益高。该方法为增强模型推理提供了一种简单而有效的新途径。

Abstract: To enhance the reasoning capabilities of Large Language Models (LLMs) without high costs of training, nor extensive test-time sampling, we introduce Verification-First (VF), a strategy that prompts models to verify a provided candidate answer, even a trivial or random one, before generating a solution. This approach triggers a "reverse reasoning" process that is cognitively easier and complementary to standard forward Chain-of-Thought (CoT), effectively invoking the model's critical thinking to reduce logical errors. We further generalize the VF strategy to Iter-VF, a sequential test-time scaling (TTS) method that iteratively cycles the verification-generation process using the model's previous answer. Extensive experiments across various benchmarks (from mathematical reasoning to coding and agentic tasks) and various LLMs (from open-source 1B to cutting-edge commercial ones) confirm that VF with random answer consistently outperforms standard CoT with minimal computational overhead, and Iter-VF outperforms existing TTS strategies.

</details>


### [30] [Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting](https://arxiv.org/abs/2511.21735)
*Harshita Sharma,Maxwell C. Reynolds,Valentina Salvatelli,Anne-Marie G. Sykes,Kelly K. Horst,Anton Schwaighofer,Maximilian Ilse,Olesya Melnichenko,Sam Bond-Taylor,Fernando Pérez-García,Vamshi K. Mugu,Alex Chan,Ceylan Colak,Shelby A. Swartz,Motassem B. Nashawaty,Austin J. Gonzalez,Heather A. Ouellette,Selnur B. Erdal,Beth A. Schueler,Maria T. Wetscherek,Noel Codella,Mohit Jain,Shruthi Bannur,Kenza Bouzid,Daniel C. Castro,Stephanie Hyland,Panos Korfiatis,Ashish Khandelwal,Javier Alvarez-Valle*

Main category: cs.CL

TL;DR: MAIRA-X是一个用于胸部X光报告生成的多模态AI模型，在临床评估中表现出色，特别是在导管和管路报告方面，错误率与原始报告相当。


<details>
  <summary>Details</summary>
Motivation: AI辅助报告生成可以减轻放射科医生的工作负担，特别是在导管和管路解释这种重复性任务上，同时应对筛查指南扩展、复杂病例和人员短缺的挑战。

Method: 使用梅奥诊所的大规模多中心纵向数据集（310万研究，600万图像，80.6万患者）开发MAIRA-X模型，并在三个保留数据集和MIMIC-CXR数据集上评估。开发了专门的导管和管路评估框架，并进行了首次回顾性用户评估研究。

Result: MAIRA-X在词汇质量、临床正确性和导管管路相关元素方面显著优于现有技术。用户研究发现关键错误率相当（原始3.0% vs AI 4.6%），可接受句子率相似（原始97.8% vs AI 97.4%），相比之前研究有显著改进。

Conclusion: MAIRA-X可以有效辅助放射科医生，特别是在高流量临床环境中，为AI辅助胸部X光报告生成提供了有前景的解决方案。

Abstract: AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&T-related elements. A novel L&T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.

</details>


### [31] [R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization](https://arxiv.org/abs/2511.21736)
*Jiayi Chen,Jieqi Shi,Jing Huo,Chen Wu*

Main category: cs.CL

TL;DR: R2Q是一种新颖的2位量化框架，通过将2位量化分解为两个顺序的1位子量化，形成自适应量化网格，显著提升了2位量化的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算和内存需求巨大，促使低比特量化成为必要。虽然8位和4位量化已普遍应用，但2位量化由于严重的精度下降而面临挑战。

Method: 提出残差精化量化(R2Q)框架，将2位量化过程分解为两个顺序的1位子量化，形成自适应量化网格。通过残差学习机制精化量化过程，采用模块化设计可与现有量化感知训练框架无缝集成。

Result: 在Llama、OPT和Qwen模型上的广泛评估表明，R2Q在细粒度和粗粒度设置下均优于现有2位量化方法，涵盖问答、常识推理和语言建模等多种基准测试。同时提高了训练稳定性并加速了收敛。

Conclusion: R2Q通过创新的残差精化量化方法成功解决了2位量化的精度下降问题，在极端压缩下实现了更好的性能、稳定性和收敛速度，其模块化设计确保了与现有框架的兼容性。

Abstract: The rapid progress of Large Language Models (LLMs) has brought substantial computational and memory demands, spurring the adoption of low-bit quantization. While 8-bit and 4-bit formats have become prevalent, extending quantization to 2 bits remains challenging due to severe accuracy degradation. To address this, we propose Residual Refinement Quantization (R2Q)-a novel 2-bit quantization framework that decomposes the process into two sequential 1-bit sub-quantizations, forming an adaptive quantization lattice. Extensive evaluations on Llama, OPT, and Qwen across diverse benchmarks-covering question answering, commonsense reasoning, and language modeling-demonstrate that R2Q consistently outperforms existing 2-bit quantization methods in both fine-grained and coarse-grained settings. By refining quantization through a residual learning mechanism, R2Q enhances performance, improves training stability, and accelerates convergence under extreme compression. Furthermore, its modular design enables seamless integration with existing quantization-aware training (QAT) frameworks.

</details>


### [32] [Polarity-Aware Probing for Quantifying Latent Alignment in Language Models](https://arxiv.org/abs/2511.21737)
*Sabrina Sadiekh,Elena Ericheva,Chirag Agarwal*

Main category: cs.CL

TL;DR: PA-CCS方法通过极性反转评估模型内部表征一致性，提出两个对齐指标，发现能识别有害知识的架构和层级差异，为无监督探测提供结构鲁棒性检查。


<details>
  <summary>Details</summary>
Motivation: 研究无监督探测方法（如CCS）是否能可靠评估模型对齐，特别是对有害与安全陈述的敏感性，需要评估模型内部表征在极性反转下的语义鲁棒性。

Method: 提出极性感知CCS（PA-CCS），引入极性一致性和矛盾指数两个对齐指标，构建三个数据集（两个主要数据集和一个控制集），包含匹配的有害-安全句子对，应用于16个语言模型。

Result: PA-CCS能识别有害知识编码的架构和层级差异；对齐良好的模型在否定标记替换为无意义标记时PA-CCS分数下降，而缺乏内部校准的模型则无此退化。

Conclusion: 无监督探测在模型对齐评估中具有潜力，但需将结构鲁棒性检查纳入可解释性基准，PA-CCS为评估模型内部表征一致性提供了有效方法。

Abstract: Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.

</details>


### [33] [Decoding inner speech with an end-to-end brain-to-text neural interface](https://arxiv.org/abs/2511.21740)
*Yizi Zhang,Linyang He,Chaofei Fan,Tingkai Liu,Han Yu,Trung Le,Jingyuan Li,Scott Linderman,Lea Duncker,Francis R Willett,Nima Mesgarani,Liam Paninski*

Main category: cs.CL

TL;DR: BIT框架通过端到端神经网络将神经活动直接翻译为连贯句子，使用跨任务跨物种预训练神经编码器，结合音频大语言模型，显著降低词错误率，实现尝试和想象语音的跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有语音脑机接口大多采用级联框架，先解码音素再用n-gram语言模型组句，无法同时优化所有阶段。需要端到端可微分框架来提升性能并支持跨任务泛化。

Method: 提出Brain-to-Text (BIT)端到端框架：1) 使用跨任务跨物种预训练神经编码器，可迁移到尝试和想象语音；2) 与音频大语言模型集成端到端训练；3) 采用对比学习进行跨模态对齐。

Result: 1) 在级联设置下，预训练编码器在Brain-to-Text '24和'25基准上达到新SOTA；2) 端到端集成将词错误率从24.69%降至10.22%；3) 发现小规模音频LLMs显著改善端到端解码；4) 实现尝试和想象语音嵌入对齐，支持跨任务泛化。

Conclusion: BIT框架通过端到端可微分优化，整合大规模多样化神经数据集，为支持无缝优化的端到端解码框架铺平道路，显著推进了语音脑机接口的发展。

Abstract: Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.

</details>


### [34] [A Multiscale Geometric Method for Capturing Relational Topic Alignment](https://arxiv.org/abs/2511.21741)
*Conrad D. Hougen,Karl T. Pazdernik,Alfred O. Hero*

Main category: cs.CL

TL;DR: 提出一种几何方法整合多模态文本和合著网络数据，使用Hellinger距离和Ward连接构建层次主题树状图，有效识别罕见主题并可视化平滑主题漂移。


<details>
  <summary>Details</summary>
Motivation: 在科学文献中，追踪合著社区研究兴趣的演变需要可解释的主题建模。当前基于密集Transformer嵌入的模型容易遗漏罕见主题，并且无法捕捉平滑的时间对齐。

Method: 提出几何方法整合多模态文本和合著网络数据，使用Hellinger距离和Ward连接构建层次主题树状图，支持语义和时间维度的多尺度学习。

Result: 该方法有效识别罕见主题结构，可视化平滑的主题漂移，实验显示可解释的词袋模型与几何对齐结合的优势。

Conclusion: 几何方法能够捕捉局部和全局结构，支持多尺度学习，在识别罕见主题和可视化主题演变方面优于基于Transformer嵌入的当代模型。

Abstract: Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.

</details>


### [35] [EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants](https://arxiv.org/abs/2511.21742)
*Meenakshi Mittal,Rishi Khare,Mihran Miroyan,Chancharik Mitra,Narges Norouzi*

Main category: cs.CL

TL;DR: EduMod-LLM是一个模块化的函数调用LLM管道，用于教育问答系统评估，重点关注函数调用策略、检索方法和生成模型三个关键组件。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的教育问答系统日益普及，需要对其各个管道组件进行细粒度评估，以了解系统性能、发现故障模式，并开发更透明、教学对齐的系统。

Method: 提出模块化的函数调用LLM管道EduMod-LLM，通过隔离和评估三个关键组件：函数调用策略、检索方法（包括新颖的结构感知检索）和生成语言模型，实现细粒度分析。

Result: 对LLM的函数调用性能进行了基准测试，比较了结构感知检索与基于向量和LLM评分的基线方法，评估了各种LLM的响应合成能力，揭示了具体的故障模式和性能模式。

Conclusion: 模块化的函数调用方法能提高教育问答系统的透明度和教学对齐性，支持开发可解释且有效的教育系统。

Abstract: With the growing use of Large Language Model (LLM)-based Question-Answering (QA) systems in education, it is critical to evaluate their performance across individual pipeline components. In this work, we introduce {\model}, a modular function-calling LLM pipeline, and present a comprehensive evaluation along three key axes: function calling strategies, retrieval methods, and generative language models. Our framework enables fine-grained analysis by isolating and assessing each component. We benchmark function-calling performance across LLMs, compare our novel structure-aware retrieval method to vector-based and LLM-scoring baselines, and evaluate various LLMs for response synthesis. This modular approach reveals specific failure modes and performance patterns, supporting the development of interpretable and effective educational QA systems. Our findings demonstrate the value of modular function calling in improving system transparency and pedagogical alignment. Website and Supplementary Material: https://chancharikmitra.github.io/EduMod-LLM-website/

</details>


### [36] [Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning](https://arxiv.org/abs/2511.21743)
*Mukul Singh,Ananya Singha,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.CL

TL;DR: 论文分析语言模型在任务微调中的推理过程，将推理标记（解决问题的中间步骤）与人类工作记忆类比，发现推理行为遵循从无意识到有意识再到自动化的四阶段发展模式，可作为训练诊断和优化的信号。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解语言模型在任务特定微调过程中的推理行为，将模型的推理标记与人类工作记忆机制进行类比，探索推理在模型学习过程中的作用及其动态变化规律。

Method: 方法基于认知科学的"四阶段能力模型"：从无意识无能、有意识无能、有意识有能到无意识有能。通过分析推理标记的长度和模式变化，追踪模型在训练过程中的能力发展阶段，并提出相应的度量指标来跟踪这一轨迹。

Result: 研究发现推理标记长度随性能提升而增加，在"有意识有能"阶段达到峰值，然后随着任务内化而减少。训练完成后，即使移除推理标记，模型仍能保持性能，表明推理在学习过程中起到脚手架作用但非最终必需。

Conclusion: 结论是推理行为动态可作为诊断训练阶段、识别收敛和指导早停的信号。推理行为对于理解和优化推理模型训练具有重要价值，提出的度量指标能有效跟踪模型能力发展轨迹。

Abstract: We analyze reasoning in language models during task-specific fine-tuning and draws parallel between reasoning tokens--intermediate steps generated while solving problem and the human working memory. Drawing from cognitive science, we align training dynamics with the Four Stages of Competence: models initially produce incorrect outputs without reasoning, then begin reasoning (but still fail), eventually reason effectively, and finally solve tasks without explicit reasoning. We find that reasoning token length expands as performance improves, peaks at the stage of conscious competence, then declines as the model internalizes the task. Notably, after training, models retain performance even when reasoning is removed--suggesting it scaffolded learning but is no longer needed. This progression offers actionable insights: reasoning token dynamics can serve as a signal for diagnosing training stage, identifying convergence, and guiding early stopping. We propose metrics to track this trajectory and argue that reasoning behavior is valuable for understanding and optimizing reasoning model training.

</details>


### [37] [A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features](https://arxiv.org/abs/2511.21744)
*Sergey K. Aityan,William Claster,Karthik Sai Emani,Sohni Rais,Thy Tran*

Main category: cs.CL

TL;DR: NEULIF是一种轻量级AI生成文本检测方法，使用风格计量和可读性特征结合CNN或随机森林分类器，在Kaggle数据集上达到97%准确率，模型大小仅25MB和10.6MB，远小于基于transformer的模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测方法主要依赖微调大型transformer模型或构建集成模型，计算成本高且跨领域泛化能力有限。轻量级替代方案在大数据集上准确率显著较低。

Method: 首先将文本分解为风格计量和可读性特征，然后使用紧凑的卷积神经网络(CNN)或随机森林(RF)进行分类。模型设计轻量，可在标准CPU设备上高效运行。

Result: 在Kaggle AI vs. Human语料库上，CNN模型达到97%准确率(~0.95 F1)，随机森林达到95%准确率(~0.94 F1)。CNN模型约25MB，随机森林约10.6MB，比基于transformer的集成模型小几个数量级。

Conclusion: NEULIF证明了轻量级方法在AI生成文本检测中的有效性，表明在结构洞察指导下，简单性可以与复杂性相媲美。该方法具有跨语言、领域和流式上下文应用的潜力。

Abstract: A growing number of AI-generated texts raise serious concerns. Most existing approaches to AI-generated text detection rely on fine-tuning large transformer models or building ensembles, which are computationally expensive and often provide limited generalization across domains. Existing lightweight alternatives achieved significantly lower accuracy on large datasets. We introduce NEULIF, a lightweight approach that achieves best performance in the lightweight detector class, that does not require extensive computational power and provides high detection accuracy. In our approach, a text is first decomposed into stylometric and readability features which are then used for classification by a compact Convolutional Neural Network (CNN) or Random Forest (RF). Evaluated and tested on the Kaggle AI vs. Human corpus, our models achieve 97% accuracy (~ 0.95 F1) for CNN and 95% accuracy (~ 0.94 F1) for the Random Forest, demonstrating high precision and recall, with ROC-AUC scores of 99.5% and 95%, respectively. The CNN (~ 25 MB) and Random Forest (~ 10.6 MB) models are orders of magnitude smaller than transformer-based ensembles and can be run efficiently on standard CPU devices, without sacrificing accuracy.This study also highlights the potential of such models for broader applications across languages, domains, and streaming contexts, showing that simplicity, when guided by structural insights, can rival complexity in AI-generated content detection.

</details>


### [38] [DELTA: Language Diffusion-based EEG-to-Text Architecture](https://arxiv.org/abs/2511.21746)
*Mingyu Jeon,Hyobin Kim*

Main category: cs.CL

TL;DR: DELTA：使用残差向量量化（RVQ）EEG分词器和掩码语言扩散模型（LLaDA），通过非顺序去噪从EEG信号生成文本，显著提升语义对齐效果。


<details>
  <summary>Details</summary>
Motivation: EEG到文本转换面临高维噪声、受试者变异性以及自回归解码中的误差累积等挑战，需要更可靠的生成方法。

Method: 1. 使用残差向量量化（RVQ）将连续EEG信号离散化为多层token以减少噪声和个体差异；2. 采用掩码语言扩散模型（LLaDA）通过非顺序去噪重建句子。

Result: 在ZuCo数据集上，DELTA比自回归基线提升语义对齐达5.37分，在词级条件下达到BLEU-1 21.9和ROUGE-1 F 17.2，能够从小规模EEG-文本数据集中可靠生成文本。

Conclusion: DELTA方法实现了从EEG信号到文本的可靠生成，为可扩展的多模态EEG-语言模型指明了方向。

Abstract: Electroencephalogram (EEG)-to-text remains challenging due to high-dimensional noise, subject variability, and error accumulation in autoregressive decoding. We introduce DELTA, which pairs a Residual Vector Quantization (RVQ) EEG tokenizer with a masked language diffusion model (LLaDA). RVQ discretizes continuous EEG into multi-layer tokens to reduce noise and individual differences, while LLaDA reconstructs sentences via non-sequential denoising. On ZuCo, DELTA improves semantic alignment by up to 5.37 points over autoregressive baselines, achieving BLEU-1 21.9 and ROUGE-1 F 17.2 under word-level conditions. These results enable reliable text generation from small EEG-text datasets and point toward scalable multimodal EEG-language models.

</details>


### [39] [Building Domain-Specific Small Language Models via Guided Data Generation](https://arxiv.org/abs/2511.21748)
*Aman Kumar,Ekant Muljibhai Amin,Xian Yeow Lee,Lasitha Vidyaratne,Ahmed K. Farahat,Dipanjan D. Ghosh,Yuta Koreeda,Chetan Gupta*

Main category: cs.CL

TL;DR: 提出一个成本效益高、可扩展的训练流程，通过合成数据生成和领域数据整理，结合DAPT、DSFT和DPO训练小型领域专用LLM，并在工业故障诊断领域验证效果。


<details>
  <summary>Details</summary>
Motivation: 在专业领域中部署LLM面临数据隐私、计算资源需求大、高质量领域数据缺乏等挑战，需要开发小型、领域专用的LLM解决方案。

Method: 提出成本效益高、可扩展的训练流程：1) 从小规模种子语料库进行引导式合成数据生成；2) 自下而上的领域数据整理；3) 结合领域自适应预训练(DAPT)、领域特定监督微调(DSFT)和直接偏好优化(DPO)。

Result: 开发了DiagnosticSLM（3B参数），在工业故障诊断领域表现优异：在多项选择题任务上比开源模型（2B-9B）准确率提升高达25%，在其他任务上也表现相当或更好。

Conclusion: 该训练流程能够有效开发小型领域专用LLM，解决数据隐私、计算资源和领域数据缺乏的问题，在专业领域应用中具有实用价值。

Abstract: Large Language Models (LLMs) have shown remarkable success in supporting a wide range of knowledge-intensive tasks. In specialized domains, there is growing interest in leveraging LLMs to assist subject matter experts with domain-specific challenges. However, deploying LLMs as SaaS solutions raises data privacy concerns, while many open-source models demand significant computational resources for effective domain adaptation and deployment. A promising alternative is to develop smaller, domain-specialized LLMs, though this approach is often constrained by the lack of high-quality domain-specific training data. In this work, we address these limitations by presenting a cost-efficient and scalable training pipeline that combines guided synthetic data generation from a small seed corpus with bottom-up domain data curation. Our pipeline integrates Domain-Adaptive Pretraining (DAPT), Domain-specific Supervised Fine-tuning (DSFT), and Direct Preference Optimization (DPO) to train effective small-scale models for specialized use cases. We demonstrate this approach through DiagnosticSLM, a 3B-parameter domain-specific model tailored for fault diagnosis, root cause analysis, and repair recommendation in industrial settings. To evaluate model performance, we introduce four domain-specific benchmarks: multiple-choice questions (DiagnosticMCQ), question answering (DiagnosticQA), sentence completion (DiagnosticComp), and summarization (DiagnosticSum). DiagnosticSLM achieves up to 25% accuracy improvement over open-source models of comparable or larger size (2B-9B) on the MCQ task, while also outperforming or matching them in other tasks, demonstrating effective domain-specific reasoning and generalization capabilities.

</details>


### [40] [Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness](https://arxiv.org/abs/2511.21749)
*Svitlana Volkova,Will Dupree,Hsien-Te Kao,Peter Bautista,Gabe Ganberg,Jeff Beaubien,Laura Cassani*

Main category: cs.CL

TL;DR: BRIES是一个复合AI架构，用于检测和评估信息环境中的说服攻击效果，包含生成攻击内容、检测攻击类型、创建防御内容和评估防御效果的专门代理，在不同语言模型上展示了显著的性能差异。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI面临说服攻击的安全风险，需要量化不同语言模型对说服攻击的脆弱性，并开发增强人类认知韧性的框架，以提升生成AI安全性和认知安全性。

Method: 提出BRIES复合AI架构，包含四个专门代理：Twister（生成对抗性内容）、Detector（识别攻击类型）、Defender（创建防御内容）、Assessor（评估防御效果）。使用SemEval 2023 Task 3分类法在合成说服数据集上进行实验，比较不同语言模型（GPT-4、Llama3、Mistral、Gemma）的性能，分析提示工程和温度设置的影响，并进行因果分析。

Result: GPT-4在复杂说服技术检测上表现最优，开源模型如Llama3和Mistral在识别微妙修辞方面存在明显弱点。提示工程显著影响检测效果，不同模型对温度设置有特定偏好。因果分析揭示了不同攻击类型针对特定认知维度的特征。

Conclusion: BRIES架构通过量化LLM对说服攻击的特定脆弱性，推进了生成AI安全和认知安全研究，为在接触有害内容前通过结构化干预增强人类认知韧性提供了框架。

Abstract: This paper introduces BRIES, a novel compound AI architecture designed to detect and measure the effectiveness of persuasion attacks across information environments. We present a system with specialized agents: a Twister that generates adversarial content employing targeted persuasion tactics, a Detector that identifies attack types with configurable parameters, a Defender that creates resilient content through content inoculation, and an Assessor that employs causal inference to evaluate inoculation effectiveness. Experimenting with the SemEval 2023 Task 3 taxonomy across the synthetic persuasion dataset, we demonstrate significant variations in detection performance across language agents. Our comparative analysis reveals significant performance disparities with GPT-4 achieving superior detection accuracy on complex persuasion techniques, while open-source models like Llama3 and Mistral demonstrated notable weaknesses in identifying subtle rhetorical, suggesting that different architectures encode and process persuasive language patterns in fundamentally different ways. We show that prompt engineering dramatically affects detection efficacy, with temperature settings and confidence scoring producing model-specific variations; Gemma and GPT-4 perform optimally at lower temperatures while Llama3 and Mistral show improved capabilities at higher temperatures. Our causal analysis provides novel insights into socio-emotional-cognitive signatures of persuasion attacks, revealing that different attack types target specific cognitive dimensions. This research advances generative AI safety and cognitive security by quantifying LLM-specific vulnerabilities to persuasion attacks and delivers a framework for enhancing human cognitive resilience through structured interventions before exposure to harmful content.

</details>


### [41] [Lips-Jaw and Tongue-Jaw Articulatory Tradeoff in DYNARTmo](https://arxiv.org/abs/2511.22155)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: DYNARTmo模型研究发音器官间的协调与权衡，特别是唇-颌和舌-颌协调，通过简化的一阶任务空间规范模拟发音协同效应。


<details>
  <summary>Details</summary>
Motivation: 研究动态发音模型DYNARTmo如何解释主要和次要发音器官之间的协调关系，特别是唇-颌和舌-颌协调，探索简化模型能否捕捉真实发音模式。

Method: 采用一阶任务空间手势规范（类似发音音系学），集成简化机制在多发音器官间分配发音努力，通过CV音节模拟分析颌位移随发音部位和元音环境的变化。

Result: 模型成功复现了经验证实的发音协同模式：颌支持的舌尖闭合、双唇塞音的下唇抬高、舌-颌协同运动、唇部收缩的饱和效应，展示了简化假设下仍能生成现实的时空运动模式。

Conclusion: 即使采用计算简化的假设，DYNARTmo模型仍能生成捕捉关键发音权衡和协同方面的现实时空运动模式，为理解发音协调机制提供了有效工具。

Abstract: This paper investigates how the dynamic articulatory model DYNARTmo accounts for articulatory tradeoffs between primary and secondary articulators, with a focus on lips-jaw and tongue-jaw coordination. While DYNARTmo does not implement full task-dynamic second-order biomechanics, it adopts first-order task-space gesture specifications comparable to those used in articulatory phonology and integrates a simplified mechanism for distributing articulatory effort across multiple articulators. We first outline the conceptual relationship between task dynamics and DYNARTmo, emphasizing the distinction between high-level task-space trajectories and their low-level articulatory execution. We then present simulation results for a set of CV syllables that illustrate how jaw displacement varies as a function of both place of articulation (labial, apical, dorsal) and vowel context (/a/, /i/, /u/). The model reproduces empirically attested patterns of articulatory synergy, including jaw-supported apical closures, lower-lip elevation in bilabial stops, tongue-jaw co-movement, and saturation effects in labial constrictions. These results demonstrate that even with computationally simplified assumptions, DYNARTmo can generate realistic spatio-temporal movement patterns that capture key aspects of articulatory tradeoff and synergy across a range of consonant-vowel combinations.

</details>


### [42] [Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification](https://arxiv.org/abs/2511.21752)
*Yanxi Li,Ruocheng Shan*

Main category: cs.CL

TL;DR: 本文提出Label Disguise Defense (LDD)，一种轻量级、模型无关的防御策略，通过将真实标签替换为语义转换或无关的别名标签来抵御提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本分类任务中广泛使用，但其依赖自然语言提示的特性使其容易受到提示注入攻击。现有防御方法要么需要模型重新训练，要么容易受到混淆攻击的规避。

Method: LDD通过隐藏真实标签，使用语义转换或无关的别名标签（如"蓝色vs黄色"代替"正面vs负面"），模型通过少量示例隐式学习这些新标签映射，防止注入指令与决策输出之间的直接对应关系。

Result: 在9个最先进模型上的评估显示，LDD能够恢复因对抗攻击而损失的部分性能。对于绝大多数模型，可以找到多个别名对，其准确率高于仅依赖少量学习而无防御机制的基准。语义对齐的别名标签比未对齐的符号具有更强的鲁棒性。

Conclusion: 标签语义可以作为有效的防御层，将意义本身转化为抵御提示注入的盾牌。LDD展示了通过语义转换保护模型免受提示注入攻击的可行性。

Abstract: Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive vs. negative) to override its intended behavior through adversarial instructions. Existing defenses, such as detection-based filters, instruction hierarchies, and signed prompts, either require model retraining or remain vulnerable to obfuscation. This paper introduces Label Disguise Defense (LDD), a lightweight and model-agnostic strategy that conceals true labels by replacing them with semantically transformed or unrelated alias labels(e.g., blue vs. yellow). The model learns these new label mappings implicitly through few-shot demonstrations, preventing direct correspondence between injected directives and decision outputs. We evaluate LDD across nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under varying few-shot and an adversarial setting. Our results show that the ability of LDD to recover performance lost to the adversarial attack varies across models and alias choices. For every model evaluated, LDD is able to restore a portion of the accuracy degradation caused by the attack. Moreover, for the vast majority of models, we can identify more than one alias pair that achieves higher accuracy than the under-attack baseline, in which the model relies solely on few-shot learning without any defensive mechanism. A linguistic analysis further reveals that semantically aligned alias labels(e.g., good vs. bad) yield stronger robustness than unaligned symbols(e.g., blue vs. yellow). Overall, this study demonstrates that label semantics can serve as an effective defense layer, transforming meaning itself into a shield against prompt injection.

</details>


### [43] [Extracting Disaster Impacts and Impact Related Locations in Social Media Posts Using Large Language Models](https://arxiv.org/abs/2511.21753)
*Sameeah Noreen Hameed,Surangika Ranathunga,Raj Prasanna,Kristin Stock,Christopher B. Jones*

Main category: cs.CL

TL;DR: 利用微调的大型语言模型从灾害相关社交媒体帖子中提取受影响位置和影响信息，以填补地理时空信息空白，支持应急响应决策


<details>
  <summary>Details</summary>
Motivation: 大规模灾害往往造成严重后果，但传统权威数据（现场传感器、遥感影像等）存在大气不透明、卫星重访周期长、时间限制等问题，导致地理时空信息空白。社交媒体帖子可作为"地理传感器"提供实时影响信息，但并非所有提及的位置都实际受灾，需要区分受影响位置和非受影响位置

Method: 使用大型语言模型（LLMs）识别灾害相关社交媒体帖子中的所有位置、影响和受影响位置。通过微调LLMs专门识别影响和受影响位置（区别于非受影响位置），包括非正式表达、缩写和简写形式中提及的位置

Result: 微调模型在影响提取上达到F1分数0.69，受影响位置提取达到0.74，显著优于预训练基线。模型能有效处理非正式表达、缩写和简写形式的位置信息

Conclusion: 微调语言模型为资源分配、态势感知和灾后恢复规划提供了可扩展的及时决策支持方案，证实了其在灾害响应中的实用价值

Abstract: Large-scale disasters can often result in catastrophic consequences on people and infrastructure. Situation awareness about such disaster impacts generated by authoritative data from in-situ sensors, remote sensing imagery, and/or geographic data is often limited due to atmospheric opacity, satellite revisits, and time limitations. This often results in geo-temporal information gaps. In contrast, impact-related social media posts can act as "geo-sensors" during a disaster, where people describe specific impacts and locations. However, not all locations mentioned in disaster-related social media posts relate to an impact. Only the impacted locations are critical for directing resources effectively. e.g., "The death toll from a fire which ripped through the Greek coastal town of #Mati stood at 80, with dozens of people unaccounted for as forensic experts tried to identify victims who were burned alive #Greecefires #AthensFires #Athens #Greece." contains impacted location "Mati" and non-impacted locations "Greece" and "Athens". This research uses Large Language Models (LLMs) to identify all locations, impacts and impacted locations mentioned in disaster-related social media posts. In the process, LLMs are fine-tuned to identify only impacts and impacted locations (as distinct from other, non-impacted locations), including locations mentioned in informal expressions, abbreviations, and short forms. Our fine-tuned model demonstrates efficacy, achieving an F1-score of 0.69 for impact and 0.74 for impacted location extraction, substantially outperforming the pre-trained baseline. These robust results confirm the potential of fine-tuned language models to offer a scalable solution for timely decision-making in resource allocation, situational awareness, and post-disaster recovery planning for responders.

</details>


### [44] [Dissecting the Ledger: Locating and Suppressing "Liar Circuits" in Financial Large Language Models](https://arxiv.org/abs/2511.21756)
*Soham Mirajkar*

Main category: cs.CL

TL;DR: 该论文提出了一种机制性方法来检测LLM在金融算术推理中的内在幻觉，通过因果追踪在GPT-2 XL上发现了算术推理的双阶段机制，并展示了如何利用这一发现进行高精度幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融等高风险领域部署时，存在特定且可复现的算术运算幻觉问题。当前缓解策略通常将模型视为黑盒，需要更深入理解幻觉的内部机制。

Method: 采用机制性方法进行内在幻觉检测，对GPT-2 XL架构在ConvFinQA基准上应用因果追踪技术，识别算术推理的双阶段机制：中间层（L12-L30）的分布式计算草稿和晚期层（特别是第46层）的决策聚合电路。

Result: 研究发现抑制第46层可将模型对幻觉输出的置信度降低81.8%。在该层训练的线性探针能够以98%的准确率泛化到未见过的金融主题，表明算术欺骗存在通用几何结构。

Conclusion: 通过机制性分析揭示了LLM算术幻觉的内部工作机制，为开发更有效的幻觉检测和缓解方法提供了理论基础，特别是在高风险金融应用场景中。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes financial domains, yet they suffer from specific, reproducible hallucinations when performing arithmetic operations. Current mitigation strategies often treat the model as a black box. In this work, we propose a mechanistic approach to intrinsic hallucination detection. By applying Causal Tracing to the GPT-2 XL architecture on the ConvFinQA benchmark, we identify a dual-stage mechanism for arithmetic reasoning: a distributed computational scratchpad in middle layers (L12-L30) and a decisive aggregation circuit in late layers (specifically Layer 46). We verify this mechanism via an ablation study, demonstrating that suppressing Layer 46 reduces the model's confidence in hallucinatory outputs by 81.8%. Furthermore, we demonstrate that a linear probe trained on this layer generalizes to unseen financial topics with 98% accuracy, suggesting a universal geometry of arithmetic deception.

</details>


### [45] [Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models](https://arxiv.org/abs/2511.21759)
*Linye Wei,Wenjue Chen,Pingzhi Tang,Xiaotian Guo,Le Ye,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: ODB-dLLM通过自适应长度预测和跳转共享推测解码，加速扩散大语言模型推理，相比基线实现46-162倍加速


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型(dLLM)的双向注意力机制需要周期性缓存刷新，导致预填充和解码阶段交错，造成大量推理成本并限制加速潜力。预填充阶段固定响应长度引入冗余计算，解码阶段迭代次数多影响效率。

Method: 提出ODB-dLLM框架，采用双边界协调策略：1) 预填充阶段引入自适应长度预测机制，渐进减少预填充开销和不必要计算；2) 解码阶段提出dLLM特定的跳转共享推测解码方法，减少解码迭代次数。

Result: 实验结果显示，ODB-dLLM相比基线dLLM实现46-162倍加速，相比Fast-dLLM实现2.63-6.30倍加速，同时缓解了现有加速框架的准确率下降问题。

Conclusion: ODB-dLLM通过协调预填充和解码阶段的双边界策略，有效加速dLLM推理，在保持准确率的同时显著提升效率，为扩散大语言模型的实际部署提供了高效解决方案。

Abstract: Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.

</details>


### [46] [fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding](https://arxiv.org/abs/2511.21760)
*Yuxiang Wei,Yanteng Zhang,Xi Xiao,Chengxuan Qian,Tianyang Wang,Vince D. Calhoun*

Main category: cs.CL

TL;DR: fMRI-LM：通过三阶段框架将功能磁共振成像（fMRI）与语言对齐的基础模型，实现神经活动与语义认知的桥梁


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型已在图像、音频、视频等领域实现统一推理，但脑成像领域尚未探索。需要弥合这一差距以连接神经活动与语义认知，并开发跨模态脑表征。

Method: 三阶段框架：1）学习神经分词器，将fMRI映射到语言一致空间中的离散标记；2）适配预训练LLM，联合建模fMRI标记和文本，将脑活动视为可预测和描述的序列；3）多任务、多范式指令微调，赋予模型高级语义理解能力。

Result: fMRI-LM在各种基准测试中实现了强大的零样本和少样本性能，并通过参数高效调优（LoRA）高效适应，为fMRI的结构和语义理解建立了可扩展的语言对齐通用模型路径。

Conclusion: fMRI-LM成功构建了fMRI与语言之间的桥梁，为神经活动与语义认知的连接提供了可扩展的解决方案，支持多样化的下游应用。

Abstract: Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.

</details>


### [47] [LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A Case Study on Sylheti](https://arxiv.org/abs/2511.21761)
*Tabia Tanzin Prama,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: 本文提出Sylheti-CAP框架，通过上下文感知提示改进LLM在低资源方言翻译中的表现，显著提升Sylheti-Bangla翻译质量。


<details>
  <summary>Details</summary>
Motivation: LLM在标准语言翻译中表现良好，但在方言和低资源语境下的翻译能力尚未充分探索。Sylheti作为孟加拉语的低资源方言，缺乏有效的机器翻译解决方案。

Method: 提出Sylheti-CAP三步骤框架：1)嵌入语言规则手册；2)集成包含2260个核心词汇和习语的词典；3)真实性检查。在五个先进LLM上评估，对比不同提示策略。

Result: Sylheti-CAP在所有模型和提示策略中一致提升翻译质量。自动指标和人工评估均证实其有效性，显著减少幻觉、歧义和生硬表达。

Conclusion: Sylheti-CAP为方言和低资源机器翻译提供了可扩展的解决方案，通过上下文感知提示有效解决了LLM在方言特定词汇方面的不足。

Abstract: Large Language Models (LLMs) have demonstrated strong translation abilities through prompting, even without task-specific training. However, their effectiveness in dialectal and low-resource contexts remains underexplored. This study presents the first systematic investigation of LLM-based machine translation (MT) for Sylheti, a dialect of Bangla that is itself low-resource. We evaluate five advanced LLMs (GPT-4.1, GPT-4.1, LLaMA 4, Grok 3, and DeepSeek V3.2) across both translation directions (Bangla $\Leftrightarrow$ Sylheti), and find that these models struggle with dialect-specific vocabulary. To address this, we introduce Sylheti-CAP (Context-Aware Prompting), a three-step framework that embeds a linguistic rulebook, a dictionary (2{,}260 core vocabulary items and idioms), and an authenticity check directly into prompts. Extensive experiments show that Sylheti-CAP consistently improves translation quality across models and prompting strategies. Both automatic metrics and human evaluations confirm its effectiveness, while qualitative analysis reveals notable reductions in hallucinations, ambiguities, and awkward phrasing, establishing Sylheti-CAP as a scalable solution for dialectal and low-resource MT. Dataset link: \href{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}

</details>


### [48] [Factors That Support Grounded Responses in LLM Conversations: A Rapid Review](https://arxiv.org/abs/2511.21762)
*Gabriele Cesar Iwashima,Claudia Susie Rodrigues,Claudio Dipolitto,Geraldo Xexéo*

Main category: cs.CL

TL;DR: 这篇综述论文系统回顾了提升大语言模型对话质量的技术，重点解决输出与用户意图不一致、缺乏上下文基础、幻觉和话题漂移等问题，通过PRISMA框架和PICO策略识别并分类了不同阶段的校准方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在对话中可能产生与用户意图不一致、缺乏上下文基础或出现幻觉的输出，这些问题会损害基于LLM应用的可靠性，因此需要系统性地识别和分析提升对话质量的校准技术。

Method: 采用PRISMA框架和PICO策略指导的快速综述方法，系统性地进行文献检索、筛选和选择，将识别出的校准策略按照LLM生命周期阶段分类：推理时方法、后训练方法和基于强化学习的方法。

Result: 研究发现推理时方法特别高效，能够在无需重新训练的情况下校准输出，同时支持用户意图对齐、上下文基础和幻觉缓解，为提升LLM响应质量和可靠性提供了结构化机制。

Conclusion: 综述的技术为改善LLM响应质量提供了系统框架，特别是推理时方法在平衡效率和效果方面表现出色，为实现LLM与对话目标对齐、确保上下文基础和减少幻觉提供了有效途径。

Abstract: Large language models (LLMs) may generate outputs that are misaligned with user intent, lack contextual grounding, or exhibit hallucinations during conversation, which compromises the reliability of LLM-based applications. This review aimed to identify and analyze techniques that align LLM responses with conversational goals, ensure grounding, and reduce hallucination and topic drift. We conducted a Rapid Review guided by the PRISMA framework and the PICO strategy to structure the search, filtering, and selection processes. The alignment strategies identified were categorized according to the LLM lifecycle phase in which they operate: inference-time, post-training, and reinforcement learning-based methods. Among these, inference-time approaches emerged as particularly efficient, aligning outputs without retraining while supporting user intent, contextual grounding, and hallucination mitigation. The reviewed techniques provided structured mechanisms for improving the quality and reliability of LLM responses across key alignment objectives.

</details>


### [49] [FLAWS: A Benchmark for Error Identification and Localization in Scientific Papers](https://arxiv.org/abs/2511.21843)
*Sarina Xi,Vishisht Rao,Justin Payan,Nihar B. Shah*

Main category: cs.CL

TL;DR: FLAWS是一个用于评估LLM在科研论文中检测和定位错误能力的自动化基准测试，包含713个论文-错误对，GPT-5在测试中表现最佳（39.1%准确率）。


<details>
  <summary>Details</summary>
Motivation: 随着科学产出的指数级增长，人工审稿越来越难以可靠地检测错误，而LLM在错误检测方面的能力尚未得到充分探索，需要系统性的评估基准。

Method: 通过LLM系统性地在同行评审论文中插入破坏性错误，构建包含713个论文-错误对的FLAWS基准，并设计自动化评估指标来测量模型识别和定位错误的能力。

Result: 在评估的五个前沿LLM（Claude Sonnet 4.5、DeepSeek Reasoner v3.1、Gemini 2.5 Pro、GPT 5、Grok 4）中，GPT 5表现最佳，在k=10时达到39.1%的识别准确率。

Conclusion: FLAWS基准为评估LLM在科研错误检测方面的能力提供了系统方法，GPT-5在当前模型中表现最优，但整体准确率仍有提升空间，表明该领域需要进一步研究。

Abstract: The identification and localization of errors is a core task in peer review, yet the exponential growth of scientific output has made it increasingly difficult for human reviewers to reliably detect errors given the limited pool of experts. Recent advances in Large Language Models (LLMs) have sparked interest in their potential to support such evaluation tasks, from academic peer review to automated scientific assessment. However, despite the growing use of LLMs in review systems, their capabilities to pinpoint errors remain underexplored. In this work, we introduce Fault Localization Across Writing in Science (FLAWS), an automated benchmark consisting of 713 paper-error pairs designed to evaluate how effectively LLMs detect errors that undermine key claims in research papers. We construct the benchmark by systematically inserting claim-invalidating errors into peer-reviewed papers using LLMs, paired with an automated evaluation metric that measures whether models can identify and localize these errors. Developing such a benchmark presents unique challenges that we overcome: ensuring that the inserted errors are well-defined, challenging, and relevant to the content of the paper, avoiding artifacts that would make identification trivial, and designing a scalable, automated evaluation metric. On the resulting benchmark, we evaluate five frontier LLMs: Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4. Among these, GPT 5 is the top-performing model, achieving 39.1% identification accuracy when k=10, where k is the number of top-ranked error text candidates generated by the LLM.

</details>


### [50] [Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices](https://arxiv.org/abs/2511.21860)
*Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Claudio Pinhanez,Yago Primerano*

Main category: cs.CL

TL;DR: 提出CoRA指标，通过分析LLM在合成问题上的回答一致性来调整MCQA分数，提高评估可靠性


<details>
  <summary>Details</summary>
Motivation: 现有MCQA基准测试中，LLM可能获得高分但回答一致性低，需要更可靠的评估指标来反映模型真实能力

Method: 使用合成生成的问题（改变答案选项）评估LLM回答一致性，通过BMCA和CI两个中间分数计算CoRA，调整原始MCQA分数

Result: 实验显示LLM在MCQA中可能高分但一致性低，CoRA能成功降低不一致模型的分数，提供更可靠的评估

Conclusion: CoRA指标通过结合一致性评估改进了LLM在MC基准测试中的评分可靠性，有助于更准确地评估模型能力

Abstract: In this work we present the Consistency-Rebalanced Accuracy (CoRA) metric, improving the reliability of Large Language Model (LLM) scores computed on multiple choice (MC) benchmarks. Our metric explores the response consistency of the LLMs, taking advantage of synthetically-generated questions with altered answer choices. With two intermediate scores, i.e. Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI), CoRA is computed by adjusting the multiple-choice question answering (MCQA) scores to better reflect the level of consistency of the LLM. We present evaluations in different benchmarks using diverse LLMs, and not only demonstrate that LLMs can present low response consistency even when they present high MCQA scores, but also that CoRA can successfully scale down the scores of inconsistent models.

</details>


### [51] [A Customer Journey in the Land of Oz: Leveraging the Wizard of Oz Technique to Model Emotions in Customer Service Interactions](https://arxiv.org/abs/2511.21909)
*Sofie Labat,Thomas Demeester,Véronique Hoste*

Main category: cs.CL

TL;DR: EmoWOZ-CS：一个用于情感感知客服的双语对话语料库，通过Wizard of Oz实验收集，包含2,148个对话，用于评估情感轨迹设计、标注差异和实时情感推理


<details>
  <summary>Details</summary>
Motivation: 现有情感识别资源往往领域不匹配、标注有限且侧重于事后检测，而情感感知客服需要领域内对话数据、丰富标注和预测能力

Method: 采用受控的Wizard of Oz实验，引导具有目标情感轨迹的交互，收集了2,148个双语（荷兰语-英语）书面对话，涵盖航空、电商、在线旅游和电信四个场景

Result: 中性情绪占主导；愿望和感激是最常见的非中性情绪；多标签情感和效价标注一致性中等，唤醒度和支配度较低；自我报告与第三方标注存在显著差异；客观策略常引发中性或感激，次优策略增加愤怒等负面情绪

Conclusion: WOZ实验能成功引导对话级情感轨迹，特别是负面目标；前瞻性情感推理具有挑战性，突显了主动情感感知支持的复杂性

Abstract: Emotion-aware customer service needs in-domain conversational data, rich annotations, and predictive capabilities, but existing resources for emotion recognition are often out-of-domain, narrowly labeled, and focused on post-hoc detection. To address this, we conducted a controlled Wizard of Oz (WOZ) experiment to elicit interactions with targeted affective trajectories. The resulting corpus, EmoWOZ-CS, contains 2,148 bilingual (Dutch-English) written dialogues from 179 participants across commercial aviation, e-commerce, online travel agencies, and telecommunication scenarios. Our contributions are threefold: (1) Evaluate WOZ-based operator-steered valence trajectories as a design for emotion research; (2) Quantify human annotation performance and variation, including divergences between self-reports and third-party judgments; (3) Benchmark detection and forward-looking emotion inference in real-time support. Findings show neutral dominates participant messages; desire and gratitude are the most frequent non-neutral emotions. Agreement is moderate for multilabel emotions and valence, lower for arousal and dominance; self-reports diverge notably from third-party labels, aligning most for neutral, gratitude, and anger. Objective strategies often elicit neutrality or gratitude, while suboptimal strategies increase anger, annoyance, disappointment, desire, and confusion. Some affective strategies (cheerfulness, gratitude) foster positive reciprocity, whereas others (apology, empathy) can also leave desire, anger, or annoyance. Temporal analysis confirms successful conversation-level steering toward prescribed trajectories, most distinctly for negative targets; positive and neutral targets yield similar final valence distributions. Benchmarks highlight the difficulty of forward-looking emotion inference from prior turns, underscoring the complexity of proactive emotion-aware support.

</details>


### [52] [Tracing How Annotators Think: Augmenting Preference Judgments with Reading Processes](https://arxiv.org/abs/2511.21912)
*Karin de Langis,William Walker,Khanh Chi Le,Dongyeop Kang*

Main category: cs.CL

TL;DR: 论文提出了一种捕获标注者阅读过程的标注方法，通过鼠标追踪创建了包含细粒度阅读行为的PreferRead数据集，用于分析偏好标注任务中标注者的决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有标注方法只关注最终标签，忽略了标注者的认知过程。为了更好理解复杂主观NLP任务中的标注者可靠性、决策过程和分歧，需要捕捉标注者在标注过程中的阅读行为。

Method: 提出了一种新的标注框架，不仅记录标签，还通过鼠标追踪捕获标注者的阅读过程（如关注文本的哪些部分、重读或略读）。在偏好标注任务上进行案例研究，创建了PreferRead数据集。

Result: 标注者在大约一半的试验中会重读回答，最常重读最终选择的选项，很少重读提示。阅读行为与标注结果显著相关：重读与更高的标注者间一致性相关，而较长的阅读路径和时间与较低的一致性相关。

Conclusion: 阅读过程为理解复杂主观NLP任务中标注者的可靠性、决策过程和分歧提供了补充的认知维度。该方法有助于更全面地评估标注质量。

Abstract: We propose an annotation approach that captures not only labels but also the reading process underlying annotators' decisions, e.g., what parts of the text they focus on, re-read or skim. Using this framework, we conduct a case study on the preference annotation task, creating a dataset PreferRead that contains fine-grained annotator reading behaviors obtained from mouse tracking. PreferRead enables detailed analysis of how annotators navigate between a prompt and two candidate responses before selecting their preference. We find that annotators re-read a response in roughly half of all trials, most often revisiting the option they ultimately choose, and rarely revisit the prompt. Reading behaviors are also significantly related to annotation outcomes: re-reading is associated with higher inter-annotator agreement, whereas long reading paths and times are associated with lower agreement. These results demonstrate that reading processes provide a complementary cognitive dimension for understanding annotator reliability, decision-making and disagreement in complex, subjective NLP tasks. Our code and data are publicly available.

</details>


### [53] [A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship Attribution on Chinese Lyrics](https://arxiv.org/abs/2511.21930)
*Yuxin Li,Lorraine Xu,Meng Fan Wang*

Main category: cs.CL

TL;DR: 该研究创建了首个平衡的中文歌词作者归属数据集，并开发了领域特定模型，发现结构化流派的归属准确率显著高于抽象流派，微调模型在真实数据上表现更好但受测试集设计影响。


<details>
  <summary>Details</summary>
Motivation: 中文歌词作者归属领域缺乏干净、公开的数据集，需要建立基准和评估框架。

Method: 创建平衡的中文歌词多流派数据集，开发领域特定模型并进行微调，与DeepSeek LLM的零样本推理进行对比，在两个测试集上进行评估。

Result: 结构化流派（如民俗传统）的归属准确率显著高于抽象流派（如爱情浪漫）；微调在真实数据测试集上提升鲁棒性和泛化能力，但在合成增强测试集上增益有限；测试集设计缺陷会掩盖微调的真实效果。

Conclusion: 建立了首个跨流派中文歌词作者归属基准，强调流派敏感评估的重要性，提供公开数据集和分析框架，建议扩大测试集多样性、减少词级数据增强、平衡作者流派代表性、探索领域自适应预训练。

Abstract: We propose a novel study on authorship attribution for Chinese lyrics, a domain where clean, public datasets are sorely lacking. Our contributions are twofold: (1) we create a new, balanced dataset of Chinese lyrics spanning multiple genres, and (2) we develop and fine-tune a domain-specific model, comparing its performance against zero-shot inference using the DeepSeek LLM.
  We test two central hypotheses. First, we hypothesize that a fine-tuned model will outperform a zero-shot LLM baseline. Second, we hypothesize that performance is genre-dependent. Our experiments strongly confirm Hypothesis 2: structured genres (e.g. Folklore & Tradition) yield significantly higher attribution accuracy than more abstract genres (e.g. Love & Romance). Hypothesis 1 receives only partial support: fine-tuning improves robustness and generalization in Test1 (real-world data and difficult genres), but offers limited or ambiguous gains in Test2, a smaller, synthetically-augmented set. We show that the design limitations of Test2 (e.g., label imbalance, shallow lexical differences, and narrow genre sampling) can obscure the true effectiveness of fine-tuning.
  Our work establishes the first benchmark for cross-genre Chinese lyric attribution, highlights the importance of genre-sensitive evaluation, and provides a public dataset and analytical framework for future research. We conclude with recommendations: enlarge and diversify test sets, reduce reliance on token-level data augmentation, balance author representation across genres, and investigate domain-adaptive pretraining as a pathway for improved attribution performance.

</details>


### [54] [Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity](https://arxiv.org/abs/2511.21974)
*Pamela D. Rivière,Sean Trott*

Main category: cs.CL

TL;DR: 研究提出系統化探測注意力機制的流程，利用詞彙歧義來分離詞義消歧的注意力機制，發現不同規模模型有不同特徵：小模型機制較脆弱，大模型有更穩健的消歧頭。


<details>
  <summary>Details</summary>
Motivation: 儘管理論上理解Transformer語言模型中的自注意力矩陣操作，但仍不清楚這些操作如何映射到可解釋的計算或功能，以及個別注意力頭何時發展出專門的注意力模式。需要系統化方法來探測注意力機制。

Method: 採用"發展性"方法：1) 使用Pythia LM檢查點識別消歧性能的轉折點；2) 識別注意力與整體消歧性能共變的頭；3) 通過刺激擾動壓力測試這些頭的穩健性；4) 進行因果分析（消融目標頭）；5) 在14M模型的所有隨機種子上重現發展分析。

Result: 1) 在14M和410M模型中識別出注意力與消歧性能共變的頭；2) 14M模型的穩健性有限，410M模型有多個頭展現驚人的泛化行為；3) 消融目標頭會損害消歧性能（尤其在14M）；4) 消歧受益於多種機制，小模型機制對消歧線索的位置和詞性高度敏感，大模型包含更穩健的消歧頭。

Conclusion: 詞義消歧依賴多種機制，不同規模模型展現不同特徵：小模型機制脆弱且對上下文敏感，大模型發展出更穩健的消歧行為。研究強調採用發展視角探測語言模型機制的價值。

Abstract: Despite an in-principle understanding of self-attention matrix operations in Transformer language models (LMs), it remains unclear precisely how these operations map onto interpretable computations or functions--and how or when individual attention heads develop specialized attention patterns. Here, we present a pipeline to systematically probe attention mechanisms, and we illustrate its value by leveraging lexical ambiguity--where a single word has multiple meanings--to isolate attention mechanisms that contribute to word sense disambiguation. We take a "developmental" approach: first, using publicly available Pythia LM checkpoints, we identify inflection points in disambiguation performance for each LM in the suite; in 14M and 410M, we identify heads whose attention to disambiguating words covaries with overall disambiguation performance across development. We then stress-test the robustness of these heads to stimulus perturbations: in 14M, we find limited robustness, but in 410M, we identify multiple heads with surprisingly generalizable behavior. Then, in a causal analysis, we find that ablating the target heads demonstrably impairs disambiguation performance, particularly in 14M. We additionally reproduce developmental analyses of 14M across all of its random seeds. Together, these results suggest: that disambiguation benefits from a constellation of mechanisms, some of which (especially in 14M) are highly sensitive to the position and part-of-speech of the disambiguating cue; and that larger models (410M) may contain heads with more robust disambiguation behavior. They also join a growing body of work that highlights the value of adopting a developmental perspective when probing LM mechanisms.

</details>


### [55] [AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models](https://arxiv.org/abs/2511.22016)
*Yann Le Beux,Oluchi Audu,Oche D. Ankeli,Dhananjay Balakrishnan,Melissah Weya,Marie D. Ralaiarinosy,Ignatius Ezeani*

Main category: cs.CL

TL;DR: 首个开源非洲刻板印象数据集AfriStereo，包含5000+刻板印象-反刻板印象对，评估显示11个模型中有9个存在显著偏见。


<details>
  <summary>Details</summary>
Motivation: 现有AI偏见评估基准主要反映西方视角，非洲背景代表性不足，导致各种应用中出现有害刻板印象。需要填补这一空白，建立基于当地社会文化背景的评估框架。

Method: 通过塞内加尔、肯尼亚和尼日利亚的社区参与收集1163个刻板印象；使用少样本提示和人在回路验证扩展至5000+对；通过语义聚类和文化知情评审员手动标注进行验证。

Result: 评估11个语言模型，其中9个显示统计显著偏见（BPR 0.63-0.78，p≤0.05），系统性地偏好刻板印象而非反刻板印象，尤其在年龄、职业和性别维度。领域特定模型偏见较弱。

Conclusion: AfriStereo为文化基础偏见评估和缓解研究开辟道路，为AI社区提供构建更公平、情境感知和全球包容NLP技术的关键方法。

Abstract: Existing AI bias evaluation benchmarks largely reflect Western perspectives, leaving African contexts underrepresented and enabling harmful stereotypes in applications across various domains. To address this gap, we introduce AfriStereo, the first open-source African stereotype dataset and evaluation framework grounded in local socio-cultural contexts. Through community engaged efforts across Senegal, Kenya, and Nigeria, we collected 1,163 stereotypes spanning gender, ethnicity, religion, age, and profession. Using few-shot prompting with human-in-the-loop validation, we augmented the dataset to over 5,000 stereotype-antistereotype pairs. Entries were validated through semantic clustering and manual annotation by culturally informed reviewers. Preliminary evaluation of language models reveals that nine of eleven models exhibit statistically significant bias, with Bias Preference Ratios (BPR) ranging from 0.63 to 0.78 (p <= 0.05), indicating systematic preferences for stereotypes over antistereotypes, particularly across age, profession, and gender dimensions. Domain-specific models appeared to show weaker bias in our setup, suggesting task-specific training may mitigate some associations. Looking ahead, AfriStereo opens pathways for future research on culturally grounded bias evaluation and mitigation, offering key methodologies for the AI community on building more equitable, context-aware, and globally inclusive NLP technologies.

</details>


### [56] [ResearchArcade: Graph Interface for Academic Tasks](https://arxiv.org/abs/2511.22036)
*Jingjun Xu,Chongshan Lin,Haofei Yu,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: ResearchArcade是一个基于图结构的统一数据接口，连接多个学术数据源，支持多模态信息，用于开发面向各种学术任务的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 学术研究产生多样化的数据源，随着研究者越来越多地使用机器学习辅助研究任务，需要建立一个统一的数据接口来支持开发面向各种学术任务的机器学习模型，从而更好地支持整个研究过程，加速知识发现。

Method: 提出ResearchArcade，一个基于图的接口，采用连贯的多表格式和图结构来组织不同来源的数据（如ArXiv学术语料和OpenReview同行评审），支持文本、图表等多模态信息，并保留手稿和社区层面的时间演化。统一了多样化的学术任务定义，支持具有不同输入需求的各种模型。

Result: 在六个学术任务上的实验表明，结合跨源和多模态信息能够支持更广泛的任务范围，而融入图结构相比基线方法持续提升性能。

Conclusion: ResearchArcade展示了其有效性，并具有推动研究进展的潜力，为学术机器学习模型开发提供了统一的数据接口解决方案。

Abstract: Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.

</details>


### [57] [Early Risk Prediction with Temporally and Contextually Grounded Clinical Language Processing](https://arxiv.org/abs/2511.22038)
*Rochana Chaturvedi,Yue Zhou,Andrew Boyd,Brian T. Layden,Mudassir Rashid,Lu Cheng,Ali Cinar,Barbara Di Eugenio*

Main category: cs.CL

TL;DR: 提出了两种互补的方法（HiTGNN和ReVeAL）用于从纵向临床笔记中进行时间上下文风险预测，应用于2型糖尿病筛查，在保护隐私的同时提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的临床笔记包含丰富的时间信息，但面临长文本、不规则事件分布、复杂时间依赖、隐私限制和资源限制等NLP挑战，需要有效方法进行慢性病风险预测。

Method: 1. HiTGNN：分层时序图神经网络，整合笔记内时间事件结构、就诊间动态和医学知识，以细粒度时间粒度建模患者轨迹；2. ReVeAL：轻量级测试时框架，将大语言模型的推理能力蒸馏到较小的验证器模型中。

Result: 在2型糖尿病筛查应用中，HiTGNN实现了最高的预测准确性（特别是近期风险），同时保护隐私并减少对大型专有模型的依赖；ReVeAL提高了对真实病例的敏感性并保留了可解释性推理。

Conclusion: 两种方法互补，能有效利用临床笔记的时间结构进行风险预测，消融实验确认了时间结构和知识增强的价值，公平性分析显示HiTGNN在不同亚组中表现更公平。

Abstract: Clinical notes in Electronic Health Records (EHRs) capture rich temporal information on events, clinician reasoning, and lifestyle factors often missing from structured data. Leveraging them for predictive modeling can be impactful for timely identification of chronic diseases. However, they present core natural language processing (NLP) challenges: long text, irregular event distribution, complex temporal dependencies, privacy constraints, and resource limitations. We present two complementary methods for temporally and contextually grounded risk prediction from longitudinal notes. First, we introduce HiTGNN, a hierarchical temporal graph neural network that integrates intra-note temporal event structures, inter-visit dynamics, and medical knowledge to model patient trajectories with fine-grained temporal granularity. Second, we propose ReVeAL, a lightweight, test-time framework that distills the reasoning of large language models into smaller verifier models. Applied to opportunistic screening for Type 2 Diabetes (T2D) using temporally realistic cohorts curated from private and public hospital corpora, HiTGNN achieves the highest predictive accuracy, especially for near-term risk, while preserving privacy and limiting reliance on large proprietary models. ReVeAL enhances sensitivity to true T2D cases and retains explanatory reasoning. Our ablations confirm the value of temporal structure and knowledge augmentation, and fairness analysis shows HiTGNN performs more equitably across subgroups.

</details>


### [58] [A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large Language Models](https://arxiv.org/abs/2511.22109)
*Gia Bao Hoang,Keith J Ransom,Rachel Stephens,Carolyn Semmler,Nicolas Fay,Lewis Mitchell*

Main category: cs.CL

TL;DR: 利用大语言模型结合心理学特征预测社交媒体说服效果，发现认知情绪和分享意愿是信念改变的关键预测因子


<details>
  <summary>Details</summary>
Motivation: 传统心理学信念修正模型主要针对面对面互动，无法有效捕捉社交媒体大规模文本交流中的说服过程，需要开发新模型来预测在线说服效果

Method: 采用混合方法，利用大语言模型生成心理学文献中已研究的特征评分，构建随机森林分类模型预测信息是否会导致信念改变

Result: 测试的八个特征中，认知情绪和分享意愿是信念改变的最强预测因子，模型成功识别了说服性信息的关键特征

Conclusion: 研究揭示了说服性信息的特征，展示了大语言模型如何增强基于心理学理论的说服模型，在在线影响力检测、错误信息缓解和叙事效果评估方面有广泛应用

Abstract: Traditional psychological models of belief revision focus on face-to-face interactions, but with the rise of social media, more effective models are needed to capture belief revision at scale, in this rich text-based online discourse. Here, we use a hybrid approach, utilizing large language models (LLMs) to develop a model that predicts successful persuasion using features derived from psychological experiments.
  Our approach leverages LLM generated ratings of features previously examined in the literature to build a random forest classification model that predicts whether a message will result in belief change. Of the eight features tested, \textit{epistemic emotion} and \textit{willingness to share} were the top-ranking predictors of belief change in the model. Our findings provide insights into the characteristics of persuasive messages and demonstrate how LLMs can enhance models of successful persuasion based on psychological theory. Given these insights, this work has broader applications in fields such as online influence detection and misinformation mitigation, as well as measuring the effectiveness of online narratives.

</details>


### [59] [Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive Samples](https://arxiv.org/abs/2511.22141)
*Shuhei Yamashita,Daiki Shirafuji,Tatsuhiko Saito*

Main category: cs.CL

TL;DR: 提出相似度标准化方法解决视觉语言模型中的模态鸿沟问题，通过伪数据构建计算模态特定统计量，将不同模态的相似度分数标准化到统一尺度，显著提升跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: 当数据库同时包含文本和图像时，视觉语言模型产生的相似度分数在不同模态间存在尺度差异（模态鸿沟），这阻碍了准确的跨模态检索。现有方法通常需要人工标注数据进行微调，成本较高。

Method: 提出相似度标准化方法：1）为每个查询构建伪配对数据（检索余弦相似度最高的文本和图像候选）；2）计算查询与其配对数据在文本和图像模态上的相似度均值和方差；3）使用这些模态特定统计量将所有相似度分数标准化到跨模态的统一尺度上。

Result: 在7个VLM模型和两个多模态QA基准（MMQA和WebQA）上评估，当查询与目标数据属于不同模态时，平均Recall@20提升64%（MMQA）和28%（WebQA）。相比通过图像描述解决模态鸿沟的E5-V方法，本方法更有效地弥合了模态鸿沟。

Conclusion: 提出的相似度标准化方法无需人工标注数据，通过伪数据构建和统计标准化有效解决了VLM中的模态鸿沟问题，显著提升了跨模态检索性能，为多模态检索系统提供了实用解决方案。

Abstract: Advances in vision-language models (VLMs) have enabled effective cross-modality retrieval. However, when both text and images exist in the database, similarity scores would differ in scale by modality. This phenomenon, known as the modality gap, hinders accurate retrieval. Most existing studies address this issue with manually labeled data, e.g., by fine-tuning VLMs on them. In this work, we propose a similarity standardization approach with pseudo data construction. We first compute the mean and variance of the similarity scores between each query and its paired data in text or image modality. Using these modality-specific statistics, we standardize all similarity scores to compare on a common scale across modalities. These statistics are calculated from pseudo pairs, which are constructed by retrieving the text and image candidates with the highest cosine similarity to each query. We evaluate our method across seven VLMs using two multi-modal QA benchmarks (MMQA and WebQA), where each question requires retrieving either text or image data. Our experimental results show that our method significantly improves retrieval performance, achieving average Recall@20 gains of 64% on MMQA and 28% on WebQA when the query and the target data belong to different modalities. Compared to E5-V, which addresses the modality gap through image captioning, we confirm that our method more effectively bridges the modality gap.

</details>


### [60] [C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models](https://arxiv.org/abs/2511.22146)
*Kairong Han,Nuanqiao Shan,Ziyu Zhao,Zijing Hu,Xinpeng Dong,Junjian Ye,Lujia Pan,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 提出C²DLM模型，通过概念级因果图引导扩散语言模型学习因果关系，提升推理能力


<details>
  <summary>Details</summary>
Motivation: 现有自回归语言模型和扩散语言模型在推理能力上存在不足，无法有效建模自然语言中的因果结构。自回归模型受限于从左到右的token预测，扩散模型则完全忽略因果顺序，而人类推理依赖于因果知识和思维。

Method: 从扩散语言模型的完全连接注意力机制出发，首先从教师模型获取概念级因果图，然后显式引导注意力学习概念间的因果关系。通过聚焦因果关系并避免因果反转的困难子目标干扰。

Result: 在COT-OrderPerturb任务中提升12%，训练速度加快约3.2倍；在六个下游推理任务上平均提升1.31%。

Conclusion: C²DLM通过引入概念级因果指导，有效提升了扩散语言模型的推理能力，填补了现有语言模型在因果建模方面的空白。

Abstract: Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \underline{\textbf{C}}ausal \underline{\textbf{C}}oncept-Guided \underline{\textbf{D}}iffusion \underline{\textbf{L}}anguage \underline{\textbf{M}}odel (C$^2$DLM). Starting from DLM's fully connected attention, C$^2$DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C$^2$DLM improves 12\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\% across six downstream reasoning tasks. More details in the repository ~\href{https://github.com/Kairong-Han/C-2-DLM}{here}.

</details>


### [61] [A Theoretically Grounded Hybrid Ensemble for Reliable Detection of LLM-Generated Text](https://arxiv.org/abs/2511.22153)
*Sepyan Purnama Kristanto,Lutfi Hakim*

Main category: cs.CL

TL;DR: 提出一种融合三种互补检测范式的混合集成方法，用于检测LLM生成的文本，在学术文本上实现94.2%的准确率和35%的假阳性降低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速扩散模糊了人类与机器作者身份的界限，对学术诚信和信息可靠性构成实际风险。现有文本检测器通常依赖单一方法范式，泛化能力差且假阳性率高，特别是在高风险的学术文本上。

Method: 提出理论基础的混合集成方法，系统融合三种互补检测范式：1) 基于RoBERTa的transformer分类器用于深度语义特征提取；2) 基于GPT-2的概率检测器使用扰动诱导似然曲率；3) 统计语言特征分析器捕捉文体计量模式。核心创新在于优化的加权投票框架，其中集成权重在概率单纯形上学习以最大化F1分数而非启发式设置。

Result: 在包含30,000个文档的大规模多生成器语料库上评估，系统达到94.2%的准确率和0.978的AUC，在学术文本上假阳性相对降低35%。模型间相关性低(rho ~ 0.35-0.42)，这是方差减少的关键条件。

Conclusion: 该方法为教育和其它高风险领域的实际部署提供了一个更可靠和道德负责任的检测器，通过理论基础的混合集成有效解决了现有检测器的局限性。

Abstract: The rapid proliferation of Large Language Models (LLMs) has blurred the line between human and machine authorship, creating practical risks for academic integrity and information reliability. Existing text detectors typically rely on a single methodological paradigm and suffer from poor generalization and high false positive rates (FPR), especially on high-stakes academic text. We propose a theoretically grounded hybrid ensemble that systematically fuses three complementary detection paradigms: (i) a RoBERTa-based transformer classifier for deep semantic feature extraction, (ii) a GPT-2-based probabilistic detector using perturbation-induced likelihood curvature, and (iii) a statistical linguistic feature analyzer capturing stylometric patterns. The core novelty lies in an optimized weighted voting framework, where ensemble weights are learned on the probability simplex to maximize F1-score rather than set heuristically. We provide a bias-variance analysis and empirically demonstrate low inter-model correlation (rho ~ 0.35-0.42), a key condition for variance reduction. Evaluated on a large-scale, multigenerator corpus of 30,000 documents, our system achieves 94.2% accuracy and an AUC of 0.978, with a 35% relative reduction in false positives on academic text. This yields a more reliable and ethically responsible detector for real-world deployment in education and other high-stakes domains.

</details>


### [62] [RefineBench: Evaluating Refinement Capability of Language Models via Checklists](https://arxiv.org/abs/2511.22173)
*Young-Jun Lee,Seungone Kim,Byung-Kwan Lee,Minkyeong Moon,Yechan Hwang,Jong Myoung Kim,Graham Neubig,Sean Welleck,Ho-Jin Choi*

Main category: cs.CL

TL;DR: 语言模型在引导式精炼中表现优异，但在自我精炼方面仍有局限，需要突破性进展。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否能够自我精炼其响应，因为现实用户交互常涉及精炼请求，而现有研究主要关注可验证任务而非开放性问题。

Method: 引入RefineBench基准测试，包含11个领域的1000个挑战性问题，采用检查表评估框架，评估引导式精炼（提供自然语言反馈）和自我精炼（无指导）两种模式。

Result: 在自我精炼模式下，即使是前沿模型如Gemini 2.5 Pro和GPT-5也仅获得31.3%和29.1%的基准分数，多数模型无法在迭代中持续改进；而在引导式精炼中，大型模型能在五轮内将响应精炼至接近完美水平。

Conclusion: 前沿语言模型需要突破性进展才能自我精炼错误响应，RefineBench为跟踪进展提供了有价值的测试平台。

Abstract: Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.

</details>


### [63] [Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information](https://arxiv.org/abs/2511.22176)
*Lukas Struppek,Dominik Hintersdorf,Hannah Struppek,Daniel Neider,Kristian Kersting*

Main category: cs.CL

TL;DR: F-CoT通过将信息提取与推理过程分离，先组织查询中的关键信息到结构化上下文中，然后引导模型仅在此上下文中推理，从而减少生成token数量2-3倍，同时保持与标准零样本CoT相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型通过生成详细的思维链实现强大推理能力，但这导致过多的token使用和高推理延迟。现有效率方法通常关注模型中心干预（如强化学习或监督微调）来减少冗余，但本文提出无需训练、输入中心的方法。

Method: 受认知心理学启发，提出Focused Chain-of-Thought (F-CoT)，将信息提取与推理过程分离。首先将查询中的关键信息组织成简洁的结构化上下文，然后引导模型仅在此上下文中进行推理，避免关注无关细节。

Result: 在算术文字问题上，F-CoT将生成的token减少2-3倍，同时保持与标准零样本CoT相当的准确性。这证明了结构化输入作为提高LLM推理效率的简单而有效的杠杆。

Conclusion: 结构化输入是提高大型语言模型推理效率的简单而有效的方法，F-CoT通过分离信息提取和推理过程，自然产生更短的推理路径，为训练自由的高效推理提供了新方向。

Abstract: Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.

</details>


### [64] [Beyond Query-Level Comparison: Fine-Grained Reinforcement Learning for Text-to-SQL with Automated Interpretable Critiques](https://arxiv.org/abs/2511.22258)
*Guifeng Wang,Yuanfeng Song,Meng Yang,Tao Zhu,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: RuCo-C是一个用于文本到SQL任务的生成式评估模型，通过自动生成查询特定的评估标准和可解释的批评，提供细粒度评估，并在强化学习训练中通过"渐进探索"策略整合密集奖励反馈。


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL任务存在评估瓶颈：1) 依赖昂贵的人工标注黄金SQL查询；2) 强化学习方法仅使用最终二元执行结果作为奖励信号，这种粗粒度监督忽略了详细的结构和语义错误。

Method: 1) 自动生成查询特定的评估标准，无需人工标注，并将其与可解释的批评相关联；2) 在强化学习训练过程中通过"渐进探索"策略整合密集奖励反馈，动态调整奖励以提升模型性能。

Result: 综合实验表明，RuCo-C在文本到SQL评估方面优于现有方法，取得了显著的性能提升。

Conclusion: RuCo-C通过自动生成细粒度评估标准和可解释批评，解决了文本到SQL任务中评估和奖励机制的瓶颈问题，为模型训练提供了更有效的监督信号。

Abstract: Text-to-SQL, a pivotal natural language processing (NLP) task that converts textual queries into executable SQL, has seen substantial progress in recent years. However, existing evaluation and reward mechanisms used to train and assess the text-to-SQL models remain a critical bottleneck. Current approaches heavily rely on manually annotated gold SQL queries, which are costly to produce and impractical for large-scale evaluation. More importantly, most reinforcement learning (RL) methods in text-to-SQL leverage only the final binary execution outcome as the reward signal, a coarse-grained supervision that overlooks detailed structural and semantic errors from the perspective of rubrics. To address these challenges, we propose RuCo-C, a novel generative judge model for fine-grained, query-specific automatic evaluation using interpretable critiques without human intervention. Our framework first automatically generates query-specific evaluation rubrics for human-free annotation, linking them to interpretable critiques. Subsequently, it integrates densified reward feedback through a "progressive exploration" strategy during the RL training process, which dynamically adjusts the rewards to enhance the model's performance. Comprehensive experiments demonstrate that RuCo-C outperforms existing methods in text-to-SQL evaluation, yielding significant performance gains.

</details>


### [65] [Token-Level Marginalization for Multi-Label LLM Classifiers](https://arxiv.org/abs/2511.22312)
*Anjaneya Praharaj,Jaykumar Kasundra*

Main category: cs.CL

TL;DR: 该研究提出三种新颖的token级概率估计方法，为生成式语言模型在多标签内容安全分类中提供可解释的置信度分数，解决了LLaMA Guard等模型缺乏直接类别概率的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式语言模型（如LLaMA Guard）在多标签内容安全分类中虽然有效，但其生成式架构缺乏直接的类别级概率，这阻碍了模型置信度评估和性能解释，使得内容审核的动态阈值设置和细粒度错误分析变得复杂。

Method: 提出了三种新颖的token级概率估计方法，通过利用token logits来桥接生成式模型与概率输出之间的差距，并在一个经过严格标注的合成生成数据集上进行广泛实验。

Result: 实验证明，利用token logits能显著提高生成式分类器的可解释性和可靠性，实现更细致的内容安全审核，并评估了该框架在不同指令调优模型上的泛化能力。

Conclusion: 该研究提出的token级概率估计方法有效解决了生成式语言模型在多标签内容安全分类中缺乏置信度分数的问题，增强了模型的可解释性和准确性，为内容安全审核提供了更可靠的解决方案。

Abstract: This paper addresses the critical challenge of deriving interpretable confidence scores from generative language models (LLMs) when applied to multi-label content safety classification. While models like LLaMA Guard are effective for identifying unsafe content and its categories, their generative architecture inherently lacks direct class-level probabilities, which hinders model confidence assessment and performance interpretation. This limitation complicates the setting of dynamic thresholds for content moderation and impedes fine-grained error analysis. This research proposes and evaluates three novel token-level probability estimation approaches to bridge this gap. The aim is to enhance model interpretability and accuracy, and evaluate the generalizability of this framework across different instruction-tuned models. Through extensive experimentation on a synthetically generated, rigorously annotated dataset, it is demonstrated that leveraging token logits significantly improves the interpretability and reliability of generative classifiers, enabling more nuanced content safety moderation.

</details>


### [66] [Sentiment Analysis Of Shopee Product Reviews Using Distilbert](https://arxiv.org/abs/2511.22313)
*Zahri Aksa Dautd,Aviv Yuniar Rahman*

Main category: cs.CL

TL;DR: 本研究使用轻量级Transformer模型DistilBERT对Shopee平台约100万条英文产品评论进行情感分析，在保持高准确率（94.8%）的同时，计算时间比BERT减少55%以上，实现了准确率与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 随着电子商务的快速发展，Shopee等平台每天产生海量消费者评论，手动分析效率低下。需要采用计算方法来挖掘这些评论中的客户满意度和偏好信息，因此研究高效的情感分析技术具有重要价值。

Method: 使用DistilBERT（distilbert-base-uncased模型）对约100万条英文Shopee产品评论进行预处理和训练，采用准确率、精确率、召回率和F1分数作为评估指标，并与BERT和SVM等基准模型进行比较。

Result: DistilBERT达到94.8%的准确率，略低于BERT（95.3%），但显著高于SVM（90.2%）。更重要的是，计算时间比BERT减少超过55%，在准确率和效率之间取得了良好平衡。

Conclusion: DistilBERT为大规模电商平台情感分析提供了准确率与计算效率的最佳平衡，适合处理海量评论数据，是实际应用中的理想选择。

Abstract: The rapid growth of digital commerce has led to the accumulation of a massive number of consumer reviews on online platforms. Shopee, as one of the largest e-commerce platforms in Southeast Asia, receives millions of product reviews every day containing valuable information regarding customer satisfaction and preferences. Manual analysis of these reviews is inefficient, thus requiring a computational approach such as sentiment analysis. This study examines the use of DistilBERT, a lightweight transformer-based deep learning model, for sentiment classification on Shopee product reviews. The dataset used consists of approximately one million English-language reviews that have been preprocessed and trained using the distilbert-base-uncased model. Evaluation was conducted using accuracy, precision, recall, and F1-score metrics, and compared against benchmark models such as BERT and SVM. The results show that DistilBERT achieved an accuracy of 94.8%, slightly below BERT (95.3%) but significantly higher than SVM (90.2%), with computation time reduced by more than 55%. These findings demonstrate that DistilBERT provides an optimal balance between accuracy and efficiency, making it suitable for large scale sentiment analysis on e-commerce platforms. Keywords: Sentiment Analysis, DistilBERT, Shopee Reviews, Natural Language Processing, Deep Learning, Transformer Models.

</details>


### [67] [Named Entity Recognition for the Kurdish Sorani Language: Dataset Creation and Comparative Analysis](https://arxiv.org/abs/2511.22315)
*Bakhtawar Abdalla,Rebwar Mala Nabi,Hassan Eshkiki,Fabio Caraffini*

Main category: cs.CL

TL;DR: 为库尔德索拉尼语创建首个NER数据集（64,563标注词元），开发多语言NER工具，发现传统CRF方法（F1=0.825）显著优于BiLSTM模型（F1=0.706），挑战了神经网络在低资源NLP中的优势假设。


<details>
  <summary>Details</summary>
Motivation: 平衡NLP技术的包容性和全球适用性，为低资源、代表性不足的库尔德索拉尼语创建首个命名实体识别数据集，促进语言多样性。

Method: 创建库尔德索拉尼语NER数据集（64,563标注词元），开发多语言NER工具，进行系统比较分析，包括经典机器学习模型（如CRF）和神经网络系统（如BiLSTM）。

Result: 传统CRF方法获得0.825的F1分数，显著优于BiLSTM模型的0.706，挑战了神经网络在NLP中的优势假设，表明在低资源环境下，更简单、计算效率更高的经典框架可以超越神经架构。

Conclusion: 在低资源NLP设置中，简单且计算效率高的经典方法（如CRF）可以优于神经网络架构，这对NLP研究中的资源分配和方法选择具有重要意义。

Abstract: This work contributes towards balancing the inclusivity and global applicability of natural language processing techniques by proposing the first 'name entity recognition' dataset for Kurdish Sorani, a low-resource and under-represented language, that consists of 64,563 annotated tokens. It also provides a tool for facilitating this task in this and many other languages and performs a thorough comparative analysis, including classic machine learning models and neural systems. The results obtained challenge established assumptions about the advantage of neural approaches within the context of NLP. Conventional methods, in particular CRF, obtain F1-scores of 0.825, outperforming the results of BiLSTM-based models (0.706) significantly. These findings indicate that simpler and more computationally efficient classical frameworks can outperform neural architectures in low-resource settings.

</details>


### [68] [Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs](https://arxiv.org/abs/2511.22402)
*Srivarshinee Sridhar,Raghav Kaushik Ravi,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 本文提出了一种评估大语言模型对临床文本中语言不确定性敏感性的方法，通过对比数据集和层间探测指标MSU，发现LLMs对临床不确定性具有结构化、深度依赖的敏感性。


<details>
  <summary>Details</summary>
Motivation: LLMs在临床环境中应用日益广泛，但对语言不确定性的内部表征机制了解甚少。与输出置信度不同，本文关注输入侧对语言不确定性的表征敏感性，这对临床诊断解释和决策制定至关重要。

Method: 构建包含不同认识模态（如"is consistent with" vs. "may be consistent with"）的对比性临床陈述数据集，提出模型不确定性敏感性（MSU）指标，通过层间探测量化激活水平变化。

Result: LLMs对临床不确定性表现出结构化、深度依赖的敏感性，表明认识信息在更深层逐渐编码。模型对不确定性线索的敏感性随网络深度变化。

Conclusion: 研究揭示了LLMs内部如何表征语言不确定性，为模型可解释性和认识可靠性提供了新见解，有助于理解临床环境中LLMs的决策过程。

Abstract: Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g., 'is consistent with' vs. 'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability.

</details>


### [69] [Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource Languages: Do Linguistic Differences Matter?](https://arxiv.org/abs/2511.22482)
*Isabel Gonçalves,Paulo Cavalin,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 研究发现，使用相似方法和数据创建的翻译器性能差异主要源于语言本身差异，而非数据清洗、预训练模型限制、基础模型大小或训练数据量等训练因素。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，使用相似方法和少量数据微调预训练语言模型创建超低资源语言（如濒危土著语言）翻译器时，性能存在显著差异。本研究旨在系统探索这些性能差异的可能原因。

Method: 系统研究可能影响性能的因素：数据清洗程序、预训练模型限制、基础模型大小、训练数据集大小，并研究双向翻译。使用两种相关但具有显著结构语言学特征的巴西土著语言进行实验。

Result: 研究表明，这些训练因素对翻译器性能没有影响或影响非常有限。语言之间的差异可能在通过微调预训练模型创建翻译器的能力中起重要作用。

Conclusion: 翻译器性能差异主要源于语言本身的结构和特性差异，而非训练过程中的技术因素。这对超低资源语言翻译研究具有重要意义。

Abstract: Finetuning pre-trained language models with small amounts of data is a commonly-used method to create translators for ultra-low resource languages such as endangered Indigenous languages. However, previous works have reported substantially different performances with translators created using similar methodology and data. In this work we systematically explored possible causes of the performance difference, aiming to determine whether it was a product of different cleaning procedures, limitations of the pre-trained models, the size of the base model, or the size of the training dataset, studying both directions of translation. Our studies, using two Brazilian Indigenous languages, related but with significant structural linguistic characteristics, indicated none or very limited influence from those training factors, suggesting differences between languages may play a significant role in the ability to produce translators by fine-tuning pre-trained models.

</details>


### [70] [Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State Tracking](https://arxiv.org/abs/2511.22503)
*Katia Vendrame,Bolaji Yusuf,Santosh Kesiraju,Šimon Sedláček,Oldřich Plchot,Jan Černocký*

Main category: cs.CL

TL;DR: 提出联合训练方法，利用现有口语对话状态跟踪数据和跨领域文本数据，实现无需目标领域口语训练数据的跨领域泛化能力


<details>
  <summary>Details</summary>
Motivation: 端到端口语对话状态跟踪面临两大挑战：语音输入处理和数据稀缺。现有方法结合语音基础编码器和大型语言模型，在单领域表现优秀但跨领域泛化能力差，且需要为每个目标领域收集昂贵的口语标注数据。文本对话状态跟踪数据相对容易获取，因此探索如何利用跨领域文本数据提升口语对话状态跟踪的泛化能力

Method: 提出联合训练方法，同时使用可用的口语对话状态跟踪数据和其他领域的书面文本数据进行训练。通过多领域文本数据增强模型的泛化能力，减少对目标领域口语训练数据的依赖

Result: 实验证明该方法有效，能够在不需要目标领域口语训练数据的情况下，获得良好的跨领域对话状态跟踪性能

Conclusion: 通过联合训练口语和跨领域文本数据，可以显著提升口语对话状态跟踪模型的跨领域泛化能力，降低对昂贵口语标注数据的依赖，为实际应用提供更实用的解决方案

Abstract: End-to-end spoken dialogue state tracking (DST) is made difficult by the tandem of having to handle speech input and data scarcity. Combining speech foundation encoders and large language models has been proposed in recent work as to alleviate some of this difficulty. Although this approach has been shown to result in strong spoken DST models, achieving state-of-the-art performance in realistic multi-turn DST, it struggles to generalize across domains and requires annotated spoken DST training data for each domain of interest. However, collecting such data for every target domain is both costly and difficult. Noting that textual DST data is more easily obtained for various domains, in this work, we propose jointly training on available spoken DST data and written textual data from other domains as a way to achieve cross-domain generalization. We conduct experiments which show the efficacy of our proposed method for getting good cross-domain DST performance without relying on spoken training data from the target domains.

</details>


### [71] [Extension Condition "violations" and Merge optimality constraints](https://arxiv.org/abs/2511.22582)
*Matilde Marcolli,Richard Larson,Riny Huijbregts*

Main category: cs.CL

TL;DR: 本文在强最简论框架下，使用合并的数学形式化分析了一系列语言现象，证明这些看似违反扩展条件的现象实际上都能在不违反EC的情况下得到解释。


<details>
  <summary>Details</summary>
Motivation: 许多语言现象（如中心语移动、短语词缀、句法附着、动词-小品词交替、算子-变项现象）常被视为违反扩展条件，本文旨在证明这些现象实际上都能在不违反EC的情况下得到合理解释。

Method: 使用侧向合并推导所有现象，分析不同情况下的最优性违反程度；对于严重违反最优性的情况，提供不涉及EC和SM的替代推导；分析多重wh-前置、罗曼语附着语簇、韩语领属一致等现象与SM的兼容性。

Result: 证明所有现象都能在不违反EC的情况下解释；EC在合并的数学形式化中具有清晰的代数意义，是模型的内在结构约束而非额外假设；最小最优性违反的SM在合并的马尔可夫性质中起结构作用。

Conclusion: 扩展条件是合并数学形式化的内在代数约束，看似违反EC的语言现象实际上都能通过侧向合并等机制得到合理解释，最小最优性违反的SM在语言推导的动态过程中具有结构重要性。

Abstract: We analyze, using the mathematical formulation of Merge within the Strong Minimalist Thesis framework, a set of linguistic phenomena, including head-to-head movement, phrasal affixes and syntactic cliticization, verb-particle alternation, and operator-variable phenomena. These are often regarded as problematic, as violations of the Extension Condition. We show that, in fact, all of these phenomena can be explained without involving any EC violation. We first show that derivations using Sideward Merge are possible for all of these cases: these respect EC, though they involve some amount of optimality violations, with respect to Resource Restrictions cost functions, andthe amount of violation differs among these cases. We show that all the cases that involve large optimality violations can be derived in alternative ways involving neither EC nor the use of SM. The main remaining case (head-to-head movement) only involves SM with minimal violations of optimality (near equilibrium fluctuations). We analyze explicitly also the cases of multiple wh-fronting, clusters of clitics in Romance languages and possessor agreement construction in Korean, and how an explanation of these phenomena based on SM can be made compatible with the colored operad generators for phases and theta roles. We also show that the EC condition has a clear algebraic meaning in the mathematical formulation of Merge and is therefore an intrinsic structural algebraic constraint of the model, rather than an additional assumption. We also show that the minimal optimality violating SM plays a structural role in the Markovian properties of Merge, and we compare different optimality conditions coming from Minimal Search and from Resource Restriction in terms of their effect on the dynamics of the Hopf algebra Markov chain, in a simple explicit example.

</details>


### [72] [Smarter, not Bigger: Fine-Tuned RAG-Enhanced LLMs for Automotive HIL Testing](https://arxiv.org/abs/2511.22584)
*Chao Feng,Zihan Liu,Siddhant Gupta,Gongpei Cui,Jan von der Assen,Burkhard Stiller*

Main category: cs.CL

TL;DR: HIL-GPT：基于检索增强生成（RAG）的系统，通过领域适配的LLM和语义检索整合，优化汽车硬件在环测试中的测试工件利用，在准确性、延迟和成本间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 汽车硬件在环（HIL）测试存在测试工件碎片化和利用率低的问题，需要更高效、可追溯的测试用例和需求检索方法。

Method: 提出HIL-GPT系统，结合领域适配的大语言模型与语义检索，通过启发式挖掘和LLM辅助合成构建领域特定数据集进行嵌入微调，并使用向量索引实现可扩展的检索。

Result: 微调后的紧凑模型（如bge-base-en-v1.5）在准确性、延迟和成本方面优于大型模型；A/B用户研究表明RAG增强助手在帮助性、真实性和满意度方面优于通用LLM。

Conclusion: 研究为工业HIL环境中部署高效、领域对齐的LLM助手提供了见解，挑战了"越大越好"的观念，展示了紧凑模型在特定领域的优势。

Abstract: Hardware-in-the-Loop (HIL) testing is essential for automotive validation but suffers from fragmented and underutilized test artifacts. This paper presents HIL-GPT, a retrieval-augmented generation (RAG) system integrating domain-adapted large language models (LLMs) with semantic retrieval. HIL-GPT leverages embedding fine-tuning using a domain-specific dataset constructed via heuristic mining and LLM-assisted synthesis, combined with vector indexing for scalable, traceable test case and requirement retrieval. Experiments show that fine-tuned compact models, such as \texttt{bge-base-en-v1.5}, achieve a superior trade-off between accuracy, latency, and cost compared to larger models, challenging the notion that bigger is always better. An A/B user study further confirms that RAG-enhanced assistants improve perceived helpfulness, truthfulness, and satisfaction over general-purpose LLMs. These findings provide insights for deploying efficient, domain-aligned LLM-based assistants in industrial HIL environments.

</details>


### [73] [Improving LLM-based Ontology Matching with fine-tuning on synthetic data](https://arxiv.org/abs/2511.22612)
*Guilherme Sousa,Rinaldo Lima,Cassia Trojahn*

Main category: cs.CL

TL;DR: 本文提出了一种结合自动数据集生成和微调的策略，使LLM能够有效执行本体匹配任务，通过搜索空间缩减和合成数据集生成来提升零样本匹配性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到本体匹配流程中，需要探索LLM直接执行本体匹配的能力，并解决训练数据稀缺的问题，以提升模型在零样本设置下的匹配性能。

Method: 1) 采用搜索空间缩减技术选择相关本体子集；2) 自动构建提示；3) 引入基于LLM的合成数据集生成方法，创建本体子模块对及其参考对齐的数据集；4) 使用合成数据对LLM进行微调。

Result: 在OAEI复杂跟踪的Conference、Geolink、Enslaved、Taxon和Hydrography数据集上评估，结果显示在合成数据上微调的LLM相比未微调的基础模型表现出更优的性能。

Conclusion: 提出的结合自动数据集生成和微调的策略能够有效适配LLM用于本体匹配任务，为解决训练数据稀缺问题提供了创新方法，并显著提升了模型性能。

Abstract: Large Language Models (LLMs) are increasingly being integrated into various components of Ontology Matching pipelines. This paper investigates the capability of LLMs to perform ontology matching directly on ontology modules and generate the corresponding alignments. Furthermore, it is explored how a dedicated fine-tuning strategy can enhance the model's matching performance in a zero-shot setting. The proposed method incorporates a search space reduction technique to select relevant subsets from both source and target ontologies, which are then used to automatically construct prompts. Recognizing the scarcity of reference alignments for training, a novel LLM-based approach is introduced for generating a synthetic dataset. This process creates a corpus of ontology submodule pairs and their corresponding reference alignments, specifically designed to fine-tune an LLM for the ontology matching task. The proposed approach was evaluated on the Conference, Geolink, Enslaved, Taxon, and Hydrography datasets from the OAEI complex track. The results demonstrate that the LLM fine-tuned on the synthetically generated data exhibits superior performance compared to the non-fine-tuned base model. The key contribution is a strategy that combines automatic dataset generation with fine-tuning to effectively adapt LLMs for ontology matching tasks.

</details>


### [74] [Modeling Romanized Hindi and Bengali: Dataset Creation and Multilingual LLM Integration](https://arxiv.org/abs/2511.22769)
*Kanchon Gharami,Quazi Sarwar Muhtaseem,Deepti Gupta,Lavanya Elluri,Shafika Showkat Moni*

Main category: cs.CL

TL;DR: 本文针对印地语和孟加拉语开发了一个包含280万对音译数据的数据集，并基于Marian架构预训练了一个多语言seq2seq大语言模型，显著提升了罗马化文本音译的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多语言模型在处理罗马化脚本（用罗马字母表示其他语言语音）时面临挑战，特别是在南亚语境中，印地语和孟加拉语等印欧语系语言在社交媒体上广泛使用罗马化脚本。现有音译数据集缺乏发音和拼写变体的多样性、足够的代码混合数据以及低资源适应能力。

Method: 1) 为印地语和孟加拉语创建了一个包含近180万对印地语音译和100万对孟加拉语音译的数据集；2) 基于Marian架构预训练了一个定制的多语言seq2seq大语言模型。

Result: 实验结果表明，与现有相关模型相比，在BLEU和CER指标上取得了显著改进。

Conclusion: 提出的数据集和模型有效解决了印欧语系语言罗马化脚本音译的挑战，为自然语言处理任务提供了更好的支持。

Abstract: The development of robust transliteration techniques to enhance the effectiveness of transforming Romanized scripts into native scripts is crucial for Natural Language Processing tasks, including sentiment analysis, speech recognition, information retrieval, and intelligent personal assistants. Despite significant advancements, state-of-the-art multilingual models still face challenges in handling Romanized script, where the Roman alphabet is adopted to represent the phonetic structure of diverse languages. Within the South Asian context, where the use of Romanized script for Indo-Aryan languages is widespread across social media and digital communication platforms, such usage continues to pose significant challenges for cutting-edge multilingual models. While a limited number of transliteration datasets and models are available for Indo-Aryan languages, they generally lack sufficient diversity in pronunciation and spelling variations, adequate code-mixed data for large language model (LLM) training, and low-resource adaptation. To address this research gap, we introduce a novel transliteration dataset for two popular Indo-Aryan languages, Hindi and Bengali, which are ranked as the 3rd and 7th most spoken languages worldwide. Our dataset comprises nearly 1.8 million Hindi and 1 million Bengali transliteration pairs. In addition to that, we pre-train a custom multilingual seq2seq LLM based on Marian architecture using the developed dataset. Experimental results demonstrate significant improvements compared to existing relevant models in terms of BLEU and CER metrics.

</details>


### [75] [Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization](https://arxiv.org/abs/2511.22818)
*Vivek Kumar,Pushpraj Singh Rajawat,Eirini Ntoutsi*

Main category: cs.CL

TL;DR: 该研究评估大语言模型在心理治疗中的效能，通过生成动机访谈对话摘要，基于MITI框架设计标注方案，评估模型在复杂心理概念理解上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用和特定领域任务中展现潜力，但在心理学等低资源敏感领域存在敏感性不足、事实错误、共情表达不一致、偏见、幻觉等问题，难以捕捉人类理解的深度和复杂性。

Method: 采用混合方法评估LLMs在心理治疗中的效能：使用LLMs生成动机访谈对话摘要，基于MITI框架（唤起、合作、自主、方向、共情、非评判态度）设计两阶段标注方案，以专家标注为基准，通过渐进提示技术（单样本和少样本提示）构建多分类任务评估模型性能。

Result: 研究结果为LLMs理解复杂心理概念的能力提供了见解，并提出了在治疗环境中减轻"语义漂移"的最佳实践。为MI社区提供了高质量标注数据集以解决低资源领域数据稀缺问题，并为在复杂行为治疗中使用LLMs进行精确上下文解释提供了关键见解。

Conclusion: 该研究不仅通过提供高质量标注数据集解决了低资源领域的数据稀缺问题，还为在复杂行为治疗中使用LLMs进行精确上下文解释提供了重要指导，有助于提升LLMs在敏感心理治疗领域的应用效果。

Abstract: Recent advancements in large language models (LLMs) have shown their potential across both general and domain-specific tasks. However, there is a growing concern regarding their lack of sensitivity, factual incorrectness in responses, inconsistent expressions of empathy, bias, hallucinations, and overall inability to capture the depth and complexity of human understanding, especially in low-resource and sensitive domains such as psychology. To address these challenges, our study employs a mixed-methods approach to evaluate the efficacy of LLMs in psychotherapy. We use LLMs to generate precise summaries of motivational interviewing (MI) dialogues and design a two-stage annotation scheme based on key components of the Motivational Interviewing Treatment Integrity (MITI) framework, namely evocation, collaboration, autonomy, direction, empathy, and a non-judgmental attitude. Using expert-annotated MI dialogues as ground truth, we formulate multi-class classification tasks to assess model performance under progressive prompting techniques, incorporating one-shot and few-shot prompting. Our results offer insights into LLMs' capacity for understanding complex psychological constructs and highlight best practices to mitigate ``semantic drift" in therapeutic settings. Our work contributes not only to the MI community by providing a high-quality annotated dataset to address data scarcity in low-resource domains but also critical insights for using LLMs for precise contextual interpretation in complex behavioral therapy.

</details>


### [76] [RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms](https://arxiv.org/abs/2511.22858)
*Yuya Ishihara,Atsushi Keyaki,Hiroaki Yamada,Ryutaro Ohara,Mihoko Sumida*

Main category: cs.CL

TL;DR: 本文探讨了基于RAG的LLM系统在支持日本医疗诉讼程序时需要满足的三个法律规范要求：检索模块必须遵循禁止使用私人知识原则、生成回答必须忠实于检索上下文、检索必须包含适当的时间戳。


<details>
  <summary>Details</summary>
Motivation: 在诉讼中，专家委员（如医生、建筑师、会计师、工程师）为法官提供专业知识以澄清争议点。考虑用基于RAG的LLM系统替代这些专家角色时，必须严格遵守法律规范约束。

Method: 本文讨论了满足三个法律要求的RAG-based LLM系统设计：1）检索模块必须根据禁止使用私人知识原则检索与争议问题相关的外部知识；2）生成回答必须源自RAG提供的上下文并忠实于该上下文；3）检索模块必须引用具有适当时间戳的外部知识。

Result: 提出了一个符合日本医疗诉讼法律规范的RAG-based LLM系统设计方案，确保系统能够替代专家委员角色，同时满足法律程序的严格要求。

Conclusion: 基于RAG的LLM系统可以支持日本医疗诉讼程序，但必须设计为满足特定的法律规范要求，包括检索的适当性、回答的忠实性以及时间戳的准确性。

Abstract: This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.

</details>


### [77] [JBE-QA: Japanese Bar Exam QA Dataset for Assessing Legal Domain Knowledge](https://arxiv.org/abs/2511.22869)
*Zhihan Cao,Fumihito Nishino,Hiroaki Yamada,Nguyen Ha Thanh,Yusuke Miyao,Ken Satoh*

Main category: cs.CL

TL;DR: JBE-QA是一个日本司法考试问答数据集，用于评估大语言模型的法律知识，包含2015-2024年司法考试选择题，涵盖民法、刑法和宪法，共3464个平衡标注项目。


<details>
  <summary>Details</summary>
Motivation: 现有日本法律领域评估资源主要关注民法，缺乏全面的法律知识评估基准。需要创建覆盖民法、刑法和宪法的综合数据集来评估LLMs在日本法律领域的表现。

Method: 从2015-2024年日本司法考试选择题中提取问题，将每个问题分解为独立的真/假判断，并添加结构化上下文字段。数据集包含3464个平衡标注项目。

Result: 评估了26个LLMs（包括专有模型、开源模型、日本专用模型和推理模型）。结果显示：启用推理的专有模型表现最佳；宪法问题通常比民法或刑法问题更容易。

Conclusion: JBE-QA是首个全面的日本法律领域评估基准，为LLMs在日本法律知识评估提供了标准化工具，揭示了不同模型在法律问题上的性能差异。

Abstract: We introduce JBE-QA, a Japanese Bar Exam Question-Answering dataset to evaluate large language models' legal knowledge. Derived from the multiple-choice (tanto-shiki) section of the Japanese bar exam (2015-2024), JBE-QA provides the first comprehensive benchmark for Japanese legal-domain evaluation of LLMs. It covers the Civil Code, the Penal Code, and the Constitution, extending beyond the Civil Code focus of prior Japanese resources. Each question is decomposed into independent true/false judgments with structured contextual fields. The dataset contains 3,464 items with balanced labels. We evaluate 26 LLMs, including proprietary, open-weight, Japanese-specialised, and reasoning models. Our results show that proprietary models with reasoning enabled perform best, and the Constitution questions are generally easier than the Civil Code or the Penal Code questions.

</details>


### [78] [FEANEL: A Benchmark for Fine-Grained Error Analysis in K-12 English Writing](https://arxiv.org/abs/2511.22883)
*Jingheng Ye,Shen Wang,Jiaqi Chen,Hebin Wang,Deqing Zou,Yanyu Zhu,Jiwei Tang,Hai-Tao Zheng,Ruitong Liu,Haoyang Li,Yanfeng Wang,Qingsong Wen*

Main category: cs.CL

TL;DR: 论文提出了FEANEL基准，用于评估大语言模型在K-12英语写作中细粒度错误分析的能力，发现当前模型存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在教育应用中有巨大潜力，但它们在K-12英语写作中提供细粒度教育反馈的能力尚未得到充分探索，需要系统评估其错误分析和教学技能。

Method: 提出了FEANEL基准，包含1000篇中小学生英语作文，基于语言教育专家共同开发的词性分类法进行错误标注（包括类型、严重程度和解释性反馈），并评估了最先进的大语言模型在该基准上的表现。

Result: 实验结果显示，当前大语言模型在细粒度错误分析能力上存在显著差距，特别是在教育应用方面需要进一步改进。

Conclusion: 需要开发专门的方法来提升大语言模型在教育应用中的细粒度错误分析能力，FEANEL基准为这一研究方向提供了重要的评估工具。

Abstract: Large Language Models (LLMs) have transformed artificial intelligence, offering profound opportunities for educational applications. However, their ability to provide fine-grained educational feedback for K-12 English writing remains underexplored. In this paper, we challenge the error analysis and pedagogical skills of LLMs by introducing the problem of Fine-grained Error Analysis for English Learners and present the Fine-grained Error ANalysis for English Learners (FEANEL) Benchmark. The benchmark comprises 1,000 essays written by elementary and secondary school students, and a well-developed English writing error taxonomy. Each error is annotated by language education experts and categorized by type, severity, and explanatory feedback, using a part-of-speech-based taxonomy they co-developed. We evaluate state-of-the-art LLMs on the FEANEL Benchmark to explore their error analysis and pedagogical abilities. Experimental results reveal significant gaps in current LLMs' ability to perform fine-grained error analysis, highlighting the need for advancements in particular methods for educational applications.

</details>


### [79] [Language-conditioned world model improves policy generalization by reading environmental descriptions](https://arxiv.org/abs/2511.22904)
*Anh Nguyen,Stefan Lee*

Main category: cs.CL

TL;DR: 提出LED-WM方法，通过语言条件化的世界模型提升策略在未见游戏中的泛化能力，无需规划或专家演示


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法证明策略在未见游戏中的泛化能力，要么依赖限制性假设（如容忍推理时规划延迟或需要专家演示）。需要改进语言条件化世界模型的策略泛化能力，同时去除这些假设。

Method: 基于DreamerV3构建语言感知编码器世界模型（LED-WM），使用注意力机制将语言描述显式地关联到观察中的实体。通过与环境交互训练语言条件化世界模型，然后从该模型学习策略，无需规划或专家演示。

Result: 在MESSENGER和MESSENGER-WM两个环境中，LED-WM训练的策略相比其他基线方法，在由新动态和语言描述的未见游戏中表现出更好的泛化能力。还展示了策略可以通过世界模型生成的合成测试轨迹进行微调来改进。

Conclusion: LED-WM方法能够有效提升策略在未见游戏中的泛化能力，无需依赖规划或专家演示，为实际部署前的策略改进提供了新途径。

Abstract: To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying "what to do". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.

</details>


### [80] [Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework](https://arxiv.org/abs/2511.22943)
*Kelaiti Xiao,Liang Yang,Dongyu Zhang,Paerhati Tulajiang,Hongfei Lin*

Main category: cs.CL

TL;DR: 提出一个基于迭代框架的视觉双关语生成系统，通过协调LLM、T2IM和MLLM自动生成和评估成语视觉双关图像，并创建了包含1000个成语的数据集用于基准测试。


<details>
  <summary>Details</summary>
Motivation: 研究成语视觉双关语（将成语的字面意义和比喻意义对齐的图像），需要开发自动生成和评估这类复杂视觉内容的方法，并建立相应的基准数据集。

Method: 提出迭代框架：给定成语后，系统循环执行：(1) LLM生成详细视觉提示，(2) T2IM合成图像，(3) MLLM从图像推断成语，(4) 优化提示直到识别成功或达到步数限制。使用1000个成语作为输入创建数据集。

Result: 实验评估了10个LLM、10个MLLM和1个T2IM（Qwen-Image）。MLLM选择是主要性能驱动因素：GPT准确率最高，Gemini次之，最佳开源MLLM（Gemma）与部分闭源模型竞争。在LLM方面，Claude在提示生成方面表现最佳。

Conclusion: 成功开发了自动生成成语视觉双关语的迭代框架，创建了基准数据集，并系统评估了不同模型在生成和理解任务上的性能，为视觉语言理解研究提供了重要工具和见解。

Abstract: We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.

</details>


### [81] [Training-Free Loosely Speculative Decoding: Accepting Semantically Correct Drafts Beyond Exact Match](https://arxiv.org/abs/2511.22972)
*Jinze Li,Yixing Xu,Guanchen Li,Shuo Yang,Jinfeng Xu,Xuanwu Yin,Dong Li,Edith C. H. Ngai,Emad Barsoum*

Main category: cs.CL

TL;DR: FLy是一种无需训练的解码加速方法，通过放宽验证标准并利用目标模型的自校正能力，在保持99%以上准确率的同时实现2.81-5.07倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在两个主要问题：1）严格的精确匹配验证丢弃了许多语义有效的候选；2）基于训练的方法在分布外任务上性能下降。需要一种无需训练、能泛化到不同模型和领域的方法。

Method: 提出训练自由的宽松推测解码（FLy），采用双层机制：1）熵级门控识别当前token是否允许多个合理替代；2）token级延迟窗口区分真正错误与语义正确的变体。还设计了多级加速策略，不仅加速目标模型，也加速草稿模型本身。

Result: 在Llama-3.1-70B-Instruct上平均实现2.81倍加速，在405B变体上实现5.07倍加速，同时保持目标模型99%以上的准确率。在分布外数据集上，FLy仍然高效，比基于训练的EAGLE-3方法快1.62倍。

Conclusion: FLy是一种无需训练的解码加速方法，通过放宽验证标准和多级加速策略，在保持准确率的同时显著降低推理延迟，且能无缝组合任意草稿-目标模型对，无需超参数重新调整即可跨模型和领域泛化。

Abstract: Large language models (LLMs) achieve strong performance across diverse tasks but suffer from high inference latency due to their autoregressive generation. Speculative Decoding (SPD) mitigates this issue by verifying candidate tokens in parallel from a smaller draft model, yet its strict exact-match verification discards many semantically valid continuations. Moreover, existing training-based SPD methods often suffer from performance degradation on out-of-distribution (OOD) tasks. To this end, we propose Training-Free Loosely Speculative Decoding (FLy), a novel method that loosens the rigid verification criterion by leveraging the target model's self-corrective behavior to judge whether a draft-target mismatch remains semantically valid. FLy introduces a two-tier mechanism: an entropy-level gate that identifies whether the current token allows multiple plausible alternatives or is nearly deterministic, and a token-level deferred window that distinguishes genuine errors from differently worded yet semantically correct variants. To further reduce latency, we design a multi-level acceleration strategy that accelerates not only the target model but also the drafter itself. Owing to its training-free design, FLy composes seamlessly with arbitrary draft-target pairs and generalizes across models and domains without hyperparameter re-tuning. Experiments show that FLy preserves more than 99% of the target model's accuracy while achieving an average 2.81x speedup on Llama-3.1-70B-Instruct and 5.07x speedup on the 405B variant. Notably, on out-of-domain datasets, our method remains highly effective and outperforms the training-based method EAGLE-3 by 1.62x.

</details>


### [82] [Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification](https://arxiv.org/abs/2511.22977)
*Sumit Mamtani,Abhijeet Bhure*

Main category: cs.CL

TL;DR: 该论文研究了将Transformer表示作为假新闻检测的下游评估，比较了编码器-解码器预训练模型作为冻结嵌入器与轻量级分类器配对的性能，发现BERT嵌入结合逻辑回归在LIAR数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer表示在假新闻检测任务中的有效性，通过冻结预训练模型作为嵌入器来分离Transformer架构贡献与分类器复杂性，为验证性任务提供架构中心的稳健基础。

Method: 使用编码器-解码器预训练模型（BERT、GPT-2、Transformer-XL）作为冻结嵌入器，与轻量级分类器配对。通过控制预处理比较池化与填充策略，以及神经网络与线性分类头。在LIAR数据集上进行评估，分析序列长度和聚合方法的影响。

Result: BERT嵌入结合逻辑回归在LIAR数据集分割上优于神经网络基线。上下文自注意力编码能有效迁移，对截断具有鲁棒性，简单最大或平均池化具有优势。注意力基于标记编码器成为验证性任务的稳健架构中心基础。

Conclusion: 基于注意力的标记编码器可作为假新闻检测等验证性任务的稳健、架构中心基础，能够有效分离Transformer贡献与分类器复杂性，为下游任务提供有效的表示学习框架。

Abstract: This paper investigates fake news detection as a downstream evaluation of Transformer representations, benchmarking encoder-only and decoder-only pre-trained models (BERT, GPT-2, Transformer-XL) as frozen embedders paired with lightweight classifiers. Through controlled preprocessing comparing pooling versus padding and neural versus linear heads, results demonstrate that contextual self-attention encodings consistently transfer effectively. BERT embeddings combined with logistic regression outperform neural baselines on LIAR dataset splits, while analyses of sequence length and aggregation reveal robustness to truncation and advantages from simple max or average pooling. This work positions attention-based token encoders as robust, architecture-centric foundations for veracity tasks, isolating Transformer contributions from classifier complexity.

</details>


### [83] [ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?](https://arxiv.org/abs/2511.22978)
*Huaixiao Tou,Ying Zeng,Cong Ma,Muzhi Li,Minghao Li,Weijie Yuan,He Zhang,Kai Jia*

Main category: cs.CL

TL;DR: ShoppingComp是一个具有挑战性的真实世界基准测试，用于严格评估LLM购物代理在三个核心能力上的表现：精确产品检索、专家级报告生成和安全关键决策制定。结果显示当前LLM表现不佳，揭示了研究基准与真实世界部署之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 现有电子商务基准测试无法充分评估LLM购物代理的真实能力，特别是在产品安全和实际部署方面存在不足。需要建立一个更贴近真实购物需求、易于验证的基准来填补这一空白。

Method: 创建了包含120个任务和1,026个场景的ShoppingComp基准，由35位专家策划以反映真实购物需求。基准强调真实产品和易于验证的原则，并引入了产品安全危害识别这一新的评估维度。

Result: 当前最先进的LLM表现很差：GPT-5仅11.22%，Gemini-2.5-Flash仅3.92%。LLM在识别不安全产品使用、避免促销误导等方面存在严重错误，导致有害推荐。

Conclusion: ShoppingComp填补了研究基准与真实世界部署之间的差距，为推进电子商务中可靠实用的代理建立了新标准。结果表明LLM在真实购物场景中仍有很大改进空间。

Abstract: We present ShoppingComp, a challenging real-world benchmark for rigorously evaluating LLM-powered shopping agents on three core capabilities: precise product retrieval, expert-level report generation, and safety critical decision making. Unlike prior e-commerce benchmarks, ShoppingComp introduces highly complex tasks under the principle of guaranteeing real products and ensuring easy verifiability, adding a novel evaluation dimension for identifying product safety hazards alongside recommendation accuracy and report quality. The benchmark comprises 120 tasks and 1,026 scenarios, curated by 35 experts to reflect authentic shopping needs. Results reveal stark limitations of current LLMs: even state-of-the-art models achieve low performance (e.g., 11.22% for GPT-5, 3.92% for Gemini-2.5-Flash). These findings highlight a substantial gap between research benchmarks and real-world deployment, where LLMs make critical errors such as failure to identify unsafe product usage or falling for promotional misinformation, leading to harmful recommendations. ShoppingComp fills the gap and thus establishes a new standard for advancing reliable and practical agents in e-commerce.

</details>


### [84] [Social Perceptions of English Spelling Variation on Twitter: A Comparative Analysis of Human and LLM Responses](https://arxiv.org/abs/2511.23041)
*Dong Nguyen,Laura Rosseel*

Main category: cs.CL

TL;DR: 研究发现人类与大型语言模型对英语在线拼写变体（如funnnn vs. fun）的社会感知（正式性、谨慎性、年龄）存在强相关性，但在评分分布和不同类型变体间存在差异。


<details>
  <summary>Details</summary>
Motivation: 拼写变体影响文本和作者的社会感知（如非正式性、年轻感），研究旨在探索人类与大型语言模型对这种社会感知的评估是否一致。

Method: 基于社会语言学方法，比较LLM和人类对拼写变体在三个关键社会属性（正式性、谨慎性、年龄）上的评分。

Result: 人类与LLM评分总体上存在强相关性，但在评分分布分析以及不同类型拼写变体比较时出现显著差异。

Conclusion: LLM在捕捉拼写变体的社会感知方面与人类有良好对齐，但在某些方面仍存在差异，需要进一步研究模型如何内化社会语言规范。

Abstract: Spelling variation (e.g. funnnn vs. fun) can influence the social perception of texts and their writers: we often have various associations with different forms of writing (is the text informal? does the writer seem young?). In this study, we focus on the social perception of spelling variation in online writing in English and study to what extent this perception is aligned between humans and large language models (LLMs). Building on sociolinguistic methodology, we compare LLM and human ratings on three key social attributes of spelling variation (formality, carefulness, age). We find generally strong correlations in the ratings between humans and LLMs. However, notable differences emerge when we analyze the distribution of ratings and when comparing between different types of spelling variation.

</details>


### [85] [Decoding the Past: Explainable Machine Learning Models for Dating Historical Texts](https://arxiv.org/abs/2511.23056)
*Paulo J. N. Pinto,Armando J. Pinho,Diogo Pratas*

Main category: cs.CL

TL;DR: 使用可解释的特征工程树模型进行历史文本年代分类，整合压缩、词汇结构、可读性、新词检测和距离特征，在世纪和十年尺度上取得显著预测性能


<details>
  <summary>Details</summary>
Motivation: 准确确定历史文本的年代对于文化遗产收藏的组织和解释至关重要，需要开发可解释的机器学习方法来替代黑盒神经网络模型

Method: 整合五种特征类别：压缩特征、词汇结构特征、可读性特征、新词检测特征和距离特征，使用基于树的机器学习模型进行时间分类

Result: 在世纪尺度预测达到76.7%准确率，十年尺度达到26.1%；在宽松时间精度下，世纪top-2准确率96.0%，十年top-10准确率85.8%；AUCROC最高94.8%，AUPRC最高83.3%；特征重要性分析显示距离特征和词汇结构最有效

Conclusion: 特征工程树模型为历史文本年代分类提供了可扩展、可解释的替代方案，不同特征域提供互补的时间信号，19世纪是语言演变的关键转折点，但跨数据集评估显示领域适应挑战

Abstract: Accurately dating historical texts is essential for organizing and interpreting cultural heritage collections. This article addresses temporal text classification using interpretable, feature-engineered tree-based machine learning models. We integrate five feature categories - compression-based, lexical structure, readability, neologism detection, and distance features - to predict the temporal origin of English texts spanning five centuries. Comparative analysis shows that these feature domains provide complementary temporal signals, with combined models outperforming any individual feature set. On a large-scale corpus, we achieve 76.7% accuracy for century-scale prediction and 26.1% for decade-scale classification, substantially above random baselines (20% and 2.3%). Under relaxed temporal precision, performance increases to 96.0% top-2 accuracy for centuries and 85.8% top-10 accuracy for decades. The final model exhibits strong ranking capabilities with AUCROC up to 94.8% and AUPRC up to 83.3%, and maintains controlled errors with mean absolute deviations of 27 years and 30 years, respectively. For authentication-style tasks, binary models around key thresholds (e.g., 1850-1900) reach 85-98% accuracy. Feature importance analysis identifies distance features and lexical structure as most informative, with compression-based features providing complementary signals. SHAP explainability reveals systematic linguistic evolution patterns, with the 19th century emerging as a pivot point across feature domains. Cross-dataset evaluation on Project Gutenberg highlights domain adaptation challenges, with accuracy dropping by 26.4 percentage points, yet the computational efficiency and interpretability of tree-based models still offer a scalable, explainable alternative to neural architectures.

</details>


### [86] [Standard Occupation Classifier -- A Natural Language Processing Approach](https://arxiv.org/abs/2511.23057)
*Sidharth Rony,Jack Patman*

Main category: cs.CL

TL;DR: 使用自然语言处理技术构建职业分类器，通过集成Google BERT和神经网络模型，结合职位标题、描述和技能信息，实现对招聘广告的职业编码自动分类。


<details>
  <summary>Details</summary>
Motivation: 将标准职业分类系统（SOC）与招聘广告大数据结合，可以研究特定职业的劳动力需求。当前需要一种能够自动为招聘广告分配职业代码的分类器，以提供及时准确的劳动力市场演变信息。

Method: 开发多种分类器用于英国ONS SOC和美国O*NET SOC系统，使用不同的语言模型。最终构建集成模型，结合Google BERT和神经网络分类器，同时考虑职位标题、描述和技能信息。

Result: 集成模型在SOC第四层级（较低层级）的分类准确率达到61%，在第三层级的准确率达到72%。该模型能够提供基于招聘广告的及时准确的劳动力市场演变信息。

Conclusion: 自然语言处理技术可以有效构建职业分类器，集成模型在职业编码分类任务中表现最佳，为劳动力市场分析提供了有效的自动化工具。

Abstract: Standard Occupational Classifiers (SOC) are systems used to categorize and classify different types of jobs and occupations based on their similarities in terms of job duties, skills, and qualifications. Integrating these facets with Big Data from job advertisement offers the prospect to investigate labour demand that is specific to various occupations. This project investigates the use of recent developments in natural language processing to construct a classifier capable of assigning an occupation code to a given job advertisement. We develop various classifiers for both UK ONS SOC and US O*NET SOC, using different Language Models. We find that an ensemble model, which combines Google BERT and a Neural Network classifier while considering job title, description, and skills, achieved the highest prediction accuracy. Specifically, the ensemble model exhibited a classification accuracy of up to 61% for the lower (or fourth) tier of SOC, and 72% for the third tier of SOC. This model could provide up to date, accurate information on the evolution of the labour market using job advertisements.

</details>


### [87] [Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework](https://arxiv.org/abs/2511.23059)
*Jiatong Han*

Main category: cs.CL

TL;DR: 本研究采用人机协同框架，通过提示引导DeepSeek V3.1识别《黄帝内经》中的隐喻和转喻，生成认知优化的翻译，相比人工翻译和基线模型翻译在五个认知维度表现最佳。


<details>
  <summary>Details</summary>
Motivation: 中医理论建立在意象思维基础上，现有英译多采用直译，难以让目标语读者重构概念网络并应用于临床实践，需要更有效的翻译方法。

Method: 采用人机协同框架，选取《黄帝内经》四段基础理论文本，通过提示式认知支架引导DeepSeek V3.1识别隐喻和转喻，生成翻译。评估阶段用ChatGPT 5 Pro和Gemini 2.5 Pro模拟三类真实读者，对人工翻译、基线模型翻译和提示调整翻译进行五维度评分，并进行结构化访谈和解释现象学分析。

Result: 提示调整的LLM翻译在所有五个认知维度表现最佳，具有较高的跨模型和跨角色一致性。访谈主题揭示了人工与机器翻译的差异、隐喻和转喻传递的有效策略以及读者的认知偏好。

Conclusion: 本研究为中医等概念密集的古籍翻译提供了一条认知、高效且可复制的人机协同方法路径，能够更好地传达中医理论的意象思维和概念网络。

Abstract: Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.

</details>


### [88] [Accent Placement Models for Rigvedic Sanskrit Text](https://arxiv.org/abs/2511.23088)
*Akhil Rajeev P,Annarao Kulkarni*

Main category: cs.CL

TL;DR: 本研究开发了带重音/无重音《梨俱吠陀》诗节的平行语料库，比较了三种自动重音标注策略：全微调ByT5、从头训练的BiLSTM-CRF基线、以及基于LoRA的参数高效微调。全微调ByT5在所有指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 《梨俱吠陀》作为最古老的梵文文本之一，使用独特的音高重音系统（udātta、anudātta、svarita），这些重音标记编码了旋律和解释线索，但在现代电子文本中经常缺失。需要开发自动重音恢复技术来支持文化遗产语言的研究、教学和数字化应用。

Method: 1. 构建带重音/无重音诗节的平行语料库；2. 比较三种自动重音标注策略：全微调ByT5（直接在Unicode组合标记上操作的字节级Transformer）、从头训练的BiLSTM-CRF序列标注基线、基于LoRA的参数高效微调；3. 使用WER、CER和专门的重音错误率（DER）进行评估。

Result: 全微调ByT5在所有指标（WER、CER、DER）上达到最低错误率；LoRA提供了良好的效率-准确性权衡；BiLSTM-CRF作为透明基线。研究强调了重音恢复的实际要求：Unicode安全预处理、标记感知分词、以及分离字形和重音错误的评估方法。

Conclusion: 本研究为《梨俱吠陀》重音恢复建立了可复现的基线，为下游任务（如重音感知OCR、ASR/吟唱合成、数字学术研究）提供了指导。将遗产语言技术定位为连接计算建模与文献学、教学目标的NLP新兴领域。

Abstract: The Rigveda, among the oldest Indian texts in Vedic Sanskrit, employs a distinctive pitch-accent system : udātta, anudātta, svarita whose marks encode melodic and interpretive cues but are often absent from modern e-texts. This work develops a parallel corpus of accented-unaccented ślokas and conducts a controlled comparison of three strategies for automatic accent placement in Rigvedic verse: (i) full fine-tuning of ByT5, a byte-level Transformer that operates directly on Unicode combining marks, (ii) a from-scratch BiLSTM-CRF sequence-labeling baseline, and (iii) LoRA-based parameter-efficient fine-tuning atop ByT5.
  Evaluation uses Word Error Rate (WER) and Character Error Rate (CER) for orthographic fidelity, plus a task-specific Diacritic Error Rate (DER) that isolates accent edits. Full ByT5 fine-tuning attains the lowest error across all metrics; LoRA offers strong efficiency-accuracy trade-offs, and BiLSTM-CRF serves as a transparent baseline. The study underscores practical requirements for accent restoration - Unicode-safe preprocessing, mark-aware tokenization, and evaluation that separates grapheme from accent errors - and positions heritage-language technology as an emerging NLP area connecting computational modeling with philological and pedagogical aims. Results establish reproducible baselines for Rigvedic accent restoration and provide guidance for downstream tasks such as accent-aware OCR, ASR/chant synthesis, and digital scholarship.

</details>


### [89] [Mind Reading or Misreading? LLMs on the Big Five Personality Test](https://arxiv.org/abs/2511.23101)
*Francesco Di Cursi,Chiara Boldrini,Marco Conti,Andrea Passarella*

Main category: cs.CL

TL;DR: 评估LLM在二元五因素模型下从文本自动预测人格的表现，发现现有模型在零样本设置中表现不稳定，需要精心设计提示和评估指标


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在自动人格预测任务中的实际表现，探索不同模型、数据集和提示策略对预测效果的影响

Method: 使用五个LLM（包括GPT-4和轻量级开源模型）在三个异构数据集（Essays、MyPersonality、Pandora）上测试，采用两种提示策略（基础提示vs包含语言学和心理学线索的增强提示）

Result: 增强提示减少无效输出并改善类别平衡，但引入系统性偏向预测特质存在；开放性和宜人性相对容易检测，外向性和神经质较难；开源模型有时接近GPT-4和先前基准，但没有配置能在零样本二元设置中产生一致可靠的预测

Conclusion: 当前开箱即用的LLM尚不适合自动人格预测任务，需要精心协调提示设计、特质框架和评估指标才能获得可解释的结果

Abstract: We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.

</details>


### [90] [Dripper: Token-Efficient Main HTML Extraction with a Lightweight LM](https://arxiv.org/abs/2511.23119)
*Mengjie Liu,Jiahui Peng,Pei Chu,Jiantao Qiu,Ren Ma,He Zhu,Rui Min,Lindong Lu,Wenchang Ning,Linfeng Hou,Kaiwen Liu,Yuan Qu,Zhenxiang Li,Chao Xu,Zhongying Tu,Wentao Zhang,Conghui He*

Main category: cs.CL

TL;DR: Dripper：基于轻量级语言模型的高效HTML主要内容提取框架，通过HTML简化、序列分类、受控解码等创新，仅用0.6B参数即实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 从网页中准确高效提取主要内容对获取大模型训练数据至关重要。现有基于预训练生成语言模型的方法受限于上下文窗口长度、推理成本和格式幻觉等问题

Method: 1) 设计专门的HTML简化算法，将输入token减少到原始HTML的22%；2) 将主要内容提取重构为语义块序列分类任务；3) 引入受控解码机制通过logits处理器严格约束输出空间；4) 提出包含7800+网页的WebMainBench评估数据集

Result: 仅使用0.6B参数模型，Dripper在所有评估基准上均达到SOTA性能，在WebMainBench数据集上获得81.58%的ROUGE-N F1分数（使用回退策略可达83.13%）

Conclusion: Dripper通过创新的HTML简化、任务重构和受控解码机制，有效解决了现有方法在上下文窗口、推理成本和幻觉方面的问题，实现了高效准确的主要内容提取

Abstract: Accurately and efficiently extracting main content from general web pages is of great significance for obtaining training data for large models. Using well-pre-trained decoder-only generative language models offers excellent document comprehension capabilities, thereby effectively enhancing parsing quality. However, it remains constrained by issues such as context window length, inference cost, and format hallucination. We present Dripper, an efficient HTML main content extraction framework powered by lightweight language models, which addresses these challenges through four key innovations: (1) We design a specialized HTML simplification algorithm that reduces input token count to 22\% compared to raw HTML while preserving critical structural information; (2) We reformulate main content extraction as a semantic block sequence classification task, significantly reducing inference cost; (3) We introduce a controlled decoding mechanism that strictly constrains the output space through logits processors, effectively eliminating hallucination issues common in small-scale models; (4) We propose WebMainBench, an evaluation dataset containing over 7,800 web pages with meticulously human-annotated main content extraction labels. Experimental results demonstrate that using only a 0.6B parameter model, Dripper achieves state-of-the-art performance across all evaluation benchmarks and outperforms all baseline methods, attaining an ROUGE-N F1 score of 81.58\%( 83.13\% with fall-back strategy) on our proposed WebMainBench dataset.

</details>


### [91] [Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2511.23136)
*Yujiao Yang,Jing Lian,Linhui Li*

Main category: cs.CL

TL;DR: MGRS是一种新的推理框架，通过生成多样化推理轨迹、复合验证、构建推理关系图并计算累积成功率，显著提升LLM的推理能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法（如ToT、GoT）存在推理策略多样性有限、搜索分支冗余、异构推理路径整合与纠错不足等问题，限制了LLM的实际应用。

Method: 提出多链图精炼与选择（MGRS）框架：1）生成多个多样化推理轨迹；2）使用复合自验证和交叉验证策略精炼候选响应；3）构建推理关系图并估计中间节点成功率；4）计算累积成功率以选择最可靠答案和推理轨迹。

Result: 在涵盖四个任务的六个基准数据集上，MGRS平均准确率达到82.9%，比最先进基线高出2.1%。在24点游戏中首次实现100%准确率，同时比领先的Forest of Thoughts框架快13.6倍。

Conclusion: MGRS通过多样化推理轨迹生成、复合验证机制和图基选择策略，显著提升了LLM推理增强方法的推理能力和计算效率，解决了现有方法的局限性。

Abstract: The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.

</details>


### [92] [Are LLMs Good Safety Agents or a Propaganda Engine?](https://arxiv.org/abs/2511.23174)
*Neemesh Yadav,Francesco Ortu,Jiarui Liu,Joeun Yook,Bernhard Schölkopf,Rada Mihalcea,Alberto Cazzaniga,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文介绍了PSP数据集，用于探究LLMs拒绝回答行为背后的政治审查因素，发现大多数LLM都存在某种形式的审查机制


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLMs拒绝回答行为背后动机的系统分析——是真正的安全政策还是政治审查？需要区分安全拒绝与政治审查的差异

Method: 构建PSP数据集，包含来自中国的敏感提示和各国被审查的推文；采用数据驱动方法（使PSP隐含化）和表示层面方法（消除政治概念）；使用提示注入攻击测试模型脆弱性

Result: 大多数LLM都存在某种形式的审查行为；通过将审查与对隐含意图内容的拒绝关联，发现模型在不同国家语境下的拒绝分布存在差异

Conclusion: 总结了导致不同模型和国家语境下拒绝分布变化的主要属性，揭示了LLMs中存在的政治审查机制

Abstract: Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an explicitly political context. PSP is built by formatting existing censored content from two data sources, openly available on the internet: sensitive prompts in China generalized to multiple countries, and tweets that have been censored in various countries. We study: 1) impact of political sensitivity in seven LLMs through data-driven (making PSP implicit) and representation-level approaches (erasing the concept of politics); and, 2) vulnerability of models on PSP through prompt injection attacks (PIAs). Associating censorship with refusals on content with masked implicit intent, we find that most LLMs perform some form of censorship. We conclude with summarizing major attributes that can cause a shift in refusal distributions across models and contexts of different countries.

</details>


### [93] [Listwise Preference Optimization with Element-wise Confusions for Aspect Sentiment Quad Prediction](https://arxiv.org/abs/2511.23184)
*Wenna Lai,Haoran Xie,Guandong Xu,Qing Li,S. Joe Qin*

Main category: cs.CL

TL;DR: 提出一种基于推理生成和列表偏好优化的ASQP方法，通过自然语言推理增强元素关系建模和可解释性，显著提升四元组预测准确率


<details>
  <summary>Details</summary>
Motivation: 现有基于标记预测的方法难以建模ASQP任务中四个情感元素（a, c, o, s）之间的复杂关系，在预测高阶元素（如c和s）时性能下降明显，需要更有效的建模方法

Method: 1) 采用基于推理的生成方法，在统一模板中输出四元组和自然语言推理过程；2) 引入列表偏好优化框架，通过句法和语义邻近生成混淆候选，训练模型偏好黄金候选而非竞争替代项

Result: 在四个基准数据集上的实验表明，该方法有效提高了四元组预测准确性和解释一致性

Conclusion: 通过推理生成和列表偏好优化的结合，能够更好地建模ASQP任务中的元素关系，提升结构化预测的性能和可解释性

Abstract: Aspect sentiment quad prediction (ASQP) is inherently challenging to predict a structured quadruple with four core sentiment elements, including aspect term (a), aspect category (c), opinion term (o), and sentiment polarity (s). Prior methods relying on marker-based prediction struggle with modeling the intricate relationships among elements and experience sharp performance declines when predicting higher-order elements (e.g., c and s) under standard supervised fine-tuning. To address these limitations, we employ reasoning-based generation to output both the quadruple and a natural language rationale under element prefixes within a unified template, encouraging explicit relational reasoning and interpretability. To further enhance element-wise alignment, we introduce a listwise preference optimization framework for improving structural validity and relational coherence. Specifically, we generate element-wise confusable candidates via syntactic and semantic proximity, then train the model with listwise objectives to prefer the gold candidates over closely competing alternatives. Extensive experiments on four benchmark datasets demonstrate that our framework effectively improves quadruple prediction accuracy and explanation consistency.

</details>


### [94] [TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies](https://arxiv.org/abs/2511.23225)
*Guang Liang,Jie Shao,Ningyuan Tang,Xinyao Liu,Jianxin Wu*

Main category: cs.CL

TL;DR: TWEO是一种新型非侵入式损失函数，通过解决权重矩阵共线性问题，有效消除Transformer训练中的极端异常值，实现全模型FP8预训练，无需工程技巧或架构修改。


<details>
  <summary>Details</summary>
Motivation: 现代硬件对FP8的原生支持对于训练大型Transformer至关重要，但极端激活异常值严重阻碍了FP8的使用。现有解决方案要么依赖复杂的混合精度工程，要么需要侵入式架构修改。

Method: 论文挑战了异常值是数据驱动的传统观念，证明极端异常值是训练过程中由权重矩阵的结构特性（共线性）产生的机械性伪影。基于此，提出了TWEO（Transformers Without Extreme Outliers），这是一种新颖的非侵入式损失函数，通过简单的损失项有效防止极端异常值。

Result: TWEO将异常值从10000+减少到小于20，实现了全模型FP8预训练，在标准FP8训练灾难性崩溃的情况下，性能与BF16基线相当，同时训练吞吐量提高36%。此外，TWEO实现了硬件友好的W8A8每张量静态量化，在TWEO训练的模型上首次达到SOTA性能。

Conclusion: TWEO通过解决异常值的根本原因——权重矩阵共线性，提供了一种简单有效的解决方案，使FP8训练成为可能，并开启了新的量化范式，为大型Transformer的高效训练提供了突破性进展。

Abstract: Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.

</details>


### [95] [Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models](https://arxiv.org/abs/2511.23235)
*Praveen Gatla,Anushka,Nikita Kanwar,Gouri Sahoo,Rajesh Kumar Mundotiya*

Main category: cs.CL

TL;DR: 首个针对印地语旅游领域的抽取式问答系统研究，专注于瓦拉纳西旅游文化，构建数据集并评估BERT/RoBERTa模型在SFT和LoRA微调下的性能。


<details>
  <summary>Details</summary>
Motivation: 针对印地语旅游领域缺乏语言特定和文化敏感的问答资源的问题，特别是在瓦拉纳西这一具有丰富文化内涵的旅游目的地，需要建立基础系统来支持文化语境化的自然语言处理应用。

Method: 构建包含7,715个印地语问答对的数据集，并通过Llama零样本提示生成27,455个增强对。使用BERT和RoBERTa等基础模型，采用监督微调(SFT)和低秩适应(LoRA)两种微调策略，评估多种BERT变体在低资源领域特定问答任务中的适用性。

Result: LoRA微调在减少98%可训练参数的同时达到85.3%的F1分数，在效率和准确性间取得平衡。RoBERTa+SFT在捕捉文化嵌入术语(如Aarti、Kund)的上下文细微差别方面表现最佳。评估指标(F1、BLEU、ROUGE-L)显示了答案精确度和语言流畅性之间的权衡。

Conclusion: 本研究为印地语旅游问答系统建立了基础基准，强调了LoRA在低资源环境中的作用，并突显了在旅游领域开发文化语境化NLP框架的必要性。

Abstract: This article presents the first comprehensive study on designing a baseline extractive question-answering (QA) system for the Hindi tourism domain, with a specialized focus on the Varanasi-a cultural and spiritual hub renowned for its Bhakti-Bhaav (devotional ethos). Targeting ten tourism-centric subdomains-Ganga Aarti, Cruise, Food Court, Public Toilet, Kund, Museum, General, Ashram, Temple and Travel, the work addresses the absence of language-specific QA resources in Hindi for culturally nuanced applications. In this paper, a dataset comprising 7,715 Hindi QA pairs pertaining to Varanasi tourism was constructed and subsequently augmented with 27,455 pairs generated via Llama zero-shot prompting. We propose a framework leveraging foundation models-BERT and RoBERTa, fine-tuned using Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA), to optimize parameter efficiency and task performance. Multiple variants of BERT, including pre-trained languages (e.g., Hindi-BERT), are evaluated to assess their suitability for low-resource domain-specific QA. Evaluation metrics - F1, BLEU, and ROUGE-L - highlight trade-offs between answer precision and linguistic fluency. Experiments demonstrate that LoRA-based fine-tuning achieves competitive performance (85.3\% F1) while reducing trainable parameters by 98\% compared to SFT, striking a balance between efficiency and accuracy. Comparative analysis across models reveals that RoBERTa with SFT outperforms BERT variants in capturing contextual nuances, particularly for culturally embedded terms (e.g., Aarti, Kund). This work establishes a foundational baseline for Hindi tourism QA systems, emphasizing the role of LORA in low-resource settings and underscoring the need for culturally contextualized NLP frameworks in the tourism domain.

</details>


### [96] [Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs](https://arxiv.org/abs/2511.23271)
*Jiancheng Dong,Pengyue Jia,Jingyu Peng,Maolin Wang,Yuhao Wang,Lixin Su,Xin Sun,Shuaiqiang Wang,Dawei Yin,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 提出一种三阶段训练框架，用单个[BE]令牌替代冗长系统提示，实现3000倍压缩同时保持98%下游性能


<details>
  <summary>Details</summary>
Motivation: 冗长的系统提示会增加推理延迟、计算成本和减少有效上下文长度，需要寻找既能保持行为效果又能大幅压缩提示长度的方法

Method: 提出轻量级三阶段训练框架：1)通过重构训练[BE]编码原始系统提示的自然语言内容；2)将提示的下游行为蒸馏到单个令牌中；3)无需访问模型内部、辅助压缩模型或标注响应

Result: 在三个数据集上的实验表明，单个[BE]令牌可实现高达3000倍的提示长度压缩，同时保持原始系统提示约98%的下游性能

Conclusion: 该方法能显著降低推理成本，几乎将整个上下文窗口留给用户输入，为高效LLM代理系统提供了实用解决方案

Abstract: Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.

</details>


### [97] [MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)](https://arxiv.org/abs/2511.23281)
*Aaron Steiner,Ralph Peeters,Christian Bizer*

Main category: cs.CL

TL;DR: 本文比较了四种LLM网页代理架构（HTML、RAG、MCP、NLWeb）在电商任务中的表现，发现RAG、MCP和NLWeb在效果和效率上均优于传统HTML浏览，其中RAG+GPT 5配置表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前研究探索了LLM代理与网站交互的不同接口（HTML浏览、RAG、MCP、NLWeb），但缺乏在统一控制环境下对这些架构的系统性比较，需要填补这一研究空白。

Method: 构建包含四个模拟电商网站的实验平台，每个网站提供HTML、MCP和NLWeb接口。为每种接口（HTML、RAG、MCP、NLWeb）开发专用代理，执行相同的电商任务序列（从简单产品搜索到复杂查询和结账流程）。使用GPT 4.1、GPT 5、GPT 5 mini和Claude Sonnet 4作为底层LLM进行评估。

Result: RAG、MCP和NLWeb代理在效果和效率上均显著优于HTML代理：平均F1分数从HTML的0.67提升到0.75-0.77；每任务token使用量从约241k降至47k-140k；运行时间从291秒降至50-62秒。最佳配置是RAG+GPT 5，F1分数达0.87，完成率0.79。综合考虑成本，RAG+GPT 5 mini在API费用和性能间提供了良好平衡。

Conclusion: 网页交互接口的选择对基于LLM的网页代理的效果和效率有显著影响。RAG、MCP和NLWeb等现代接口相比传统HTML浏览在多个维度上表现更优，为实际应用提供了更好的选择。

Abstract: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.
  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.

</details>


### [98] [Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models](https://arxiv.org/abs/2511.23319)
*Xiang Hu,Zhanchao Zhou,Ruiqi Liang,Zehuan Li,Wei Wu,Jianguo Li*

Main category: cs.CL

TL;DR: 本文提出HSA-UltraLong模型，通过分层稀疏注意力机制解决超长上下文建模问题，在16M长度上下文上实现90%以上的检索准确率。


<details>
  <summary>Details</summary>
Motivation: 构建"能记住的机器"需要解决超长上下文建模问题，这要求具备稀疏性、随机访问灵活性和长度泛化三个关键特性。

Method: 提出分层稀疏注意力机制，并将其集成到Transformer中构建HSA-UltraLong模型（80亿参数的MoE模型），在超过8万亿token上训练。

Result: 模型在域内长度上表现与全注意力基线相当，在16M长度的上下文检索任务中达到90%以上准确率，展示了处理超长上下文的能力。

Conclusion: HSA-UltraLong为解决超长上下文建模提供了基础，为未来研究贡献了实验见解和开放问题。

Abstract: This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \textbf{sparsity}, \textbf{random-access flexibility}, and \textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.

</details>


### [99] [Tackling a Challenging Corpus for Early Detection of Gambling Disorder: UNSL at MentalRiskES 2025](https://arxiv.org/abs/2511.23325)
*Horacio Thompson,Marcelo Errecalde*

Main category: cs.CL

TL;DR: 本文提出基于CPI+DMC方法的三种模型，用于MentalRiskES 2025挑战赛Task 1，旨在通过社交媒体活动识别赌博障碍高风险用户，其中两个方案在官方结果中位列前两名。


<details>
  <summary>Details</summary>
Motivation: 赌博障碍是一种复杂的行为成瘾，具有严重的生理、心理和社会后果。基于社交媒体的早期风险检测已成为识别心理健康行为早期迹象的关键任务，需要开发更有效的检测系统。

Method: 采用CPI+DMC方法框架，使用SS3、扩展词汇的BERT和SBERT三种模型实现，并结合基于历史用户分析的决策策略，同时考虑预测效果和决策速度两个独立目标。

Result: 在具有挑战性的语料库中，提出的三个方法中有两个在官方结果中获得了前两名的成绩，在决策指标上表现突出，但研究发现区分高风险和低风险用户仍存在困难。

Conclusion: 需要探索改进数据解释和质量的方法，促进更透明可靠的早期风险检测系统，以更好地识别赌博障碍风险。

Abstract: Gambling disorder is a complex behavioral addiction that is challenging to understand and address, with severe physical, psychological, and social consequences. Early Risk Detection (ERD) on the Web has become a key task in the scientific community for identifying early signs of mental health behaviors based on social media activity. This work presents our participation in the MentalRiskES 2025 challenge, specifically in Task 1, aimed at classifying users at high or low risk of developing a gambling-related disorder. We proposed three methods based on a CPI+DMC approach, addressing predictive effectiveness and decision-making speed as independent objectives. The components were implemented using the SS3, BERT with extended vocabulary, and SBERT models, followed by decision policies based on historical user analysis. Although it was a challenging corpus, two of our proposals achieved the top two positions in the official results, performing notably in decision metrics. Further analysis revealed some difficulty in distinguishing between users at high and low risk, reinforcing the need to explore strategies to improve data interpretation and quality, and to promote more transparent and reliable ERD systems for mental disorders.

</details>


### [100] [Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach](https://arxiv.org/abs/2511.23335)
*Shuqi Liu,Han Wu,Guanzhi Deng,Jianshu Chen,Xiaoyang Wang,Linqi Song*

Main category: cs.CL

TL;DR: 提出任务无关的结构化知识猎手，结合语言模型生成能力和知识猎手的高保真度，提升知识增强文本生成的可解释性


<details>
  <summary>Details</summary>
Motivation: 语言模型在生成连贯流畅文本方面表现出色，但缺乏可解释性限制了其在实际应用中的可用性，特别是在需要可靠性和可解释性的知识增强文本生成任务中。现有方法通常使用针对特定数据特征的领域特定知识检索器，限制了其在不同数据类型和任务中的泛化能力。

Method: 直接利用结构化知识的两层架构（高层实体和低层知识三元组），设计任务无关的结构化知识猎手。采用局部-全局交互方案进行结构化知识表示学习，并使用基于层次化Transformer的指针网络作为骨干，选择相关知识三元组和实体。

Result: 在RotoWireFG数据集上的内部知识增强表格到文本生成任务和KdConv数据集上的外部知识增强对话响应生成任务中，模型均表现出有效性。任务无关模型超越了最先进方法和相应语言模型，在基准测试中设定了新标准。

Conclusion: 通过结合语言模型的强大生成能力和知识猎手的高保真度，模型实现了高可解释性，使用户能够理解模型输出生成过程。该方法为知识增强文本生成提供了更通用、可解释的解决方案。

Abstract: Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.

</details>


### [101] [Scaling HuBERT for African Languages: From Base to Large and XL](https://arxiv.org/abs/2511.23370)
*Antoine Caubrière,Elodie Gauthier*

Main category: cs.CL

TL;DR: 论文介绍了SSA-HuBERT-Large和SSA-HuBERT-XL，这是首个完全基于非洲语音训练的大规模模型，并展示了更大架构在低资源非洲语言ASR和LID任务中的显著优势。


<details>
  <summary>Details</summary>
Motivation: 非洲语言在语音处理研究中代表性不足，现有公开模型多为BASE规模，缺乏专门针对非洲语音训练的大规模编码器，需要研究模型容量与数据组成如何影响性能。

Method: 开发了SSA-HuBERT-Large（3.17亿参数）和SSA-HuBERT-XL（9.64亿参数）两个大规模模型，完全基于非洲中心音频训练，同时提供BASE规模对照模型，在撒哈拉以南非洲语言上进行ASR和语言识别的受控实验研究。

Result: 实验表明，更大的架构通过有效利用大规模音频数据集，在ASR和LID任务上显著提升性能，证明了专门针对非洲语音训练的大规模模型的价值。

Conclusion: 这是首个完全基于非洲语音训练的大规模自监督语音编码器，填补了研究空白，证明了更大模型容量在低资源非洲语言处理中的重要性，并开源了模型权重。

Abstract: Despite recent progress in multilingual speech processing, African languages remain under-represented in both research and deployed systems, particularly when it comes to strong, open-weight encoders that transfer well under low-resource supervision. Self-supervised learning has proven especially promising in such settings, yet most publicly released models targeting African speech remain at BASE scale, leaving unanswered whether larger encoders, trained exclusively on Africa-centric audio, offer tangible benefits and how model capacity interacts with data composition. This work addresses that gap by introducing SSA-HuBERT-Large (317M parameters) and SSA-HuBERT-XL (964M parameters), the first large models trained solely on African speech, alongside a BASE size counterpart. We release these models as open weights: see https://huggingface.co/collections/Orange/african-speech-foundation-models. By conducting a carefully controlled experimental study focused exclusively on Sub-Saharan languages, covering automatic speech recognition (ASR) and language identification (LID) tasks, we demonstrate that larger architectures significantly improve performance by effectively leveraging large audio datasets.

</details>


### [102] [Optimizing Multimodal Language Models through Attention-based Interpretability](https://arxiv.org/abs/2511.23375)
*Alexander Sergeev,Evgeny Kotelnikov*

Main category: cs.CL

TL;DR: 提出基于注意力机制的MLMs可解释性方法，通过分析图像关键对象的注意力分数来识别重要注意力头，并应用于参数高效微调(PEFT)，在图像描述任务中仅微调0.01%参数即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态语言模型(MLMs)难以解释，难以确定哪些组件对训练最有效，以平衡效率和性能。虽然参数高效微调(PEFT)方法可以降低计算成本，但需要识别模型中最有效的组件。

Method: 提出基于注意力的可解释性方法：通过分析注意力分数相对于图像令牌的关系，识别关注图像关键对象的注意力头。计算头部影响(HI)分数量化注意力头对关键对象的关注程度。利用HI分数选择最优模型组件进行PEFT，特别针对图像描述任务。

Result: 实验验证了方法的有效性：微调具有最高HI分数的层相比预训练、随机选择或最低HI分数层，能带来最显著的指标变化。仅微调约0.01%的参数就能显著影响图像理解能力。创建了包含图像、关键对象掩码和文本描述的新数据集。

Conclusion: 提出的注意力分析方法能有效识别MLMs中对图像理解关键的组件，为参数高效微调提供了指导，仅需微调极小比例的参数就能显著提升多模态任务的性能，平衡了计算效率和模型效果。

Abstract: Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.

</details>


### [103] [Ambiguity Awareness Optimization: Towards Semantic Disambiguation for Direct Preference Optimization](https://arxiv.org/abs/2511.23391)
*Jian Li,Shenglin Yin,Yujia Zhang,Alan Zhao,Xi Chen,Xiaohui Zhou,Pengfei Xu*

Main category: cs.CL

TL;DR: AAO通过识别偏好对中的语义相似内容并重新加权，减少DPO训练中的歧义，显著提升对齐性能


<details>
  <summary>Details</summary>
Motivation: 研究发现DPO训练中偏好对经常包含相同或语义相似的内容（歧义内容），这些内容可能引入歧义限制对齐效果的进一步提升

Method: 提出Ambiguity Awareness Optimization (AAO)，通过计算偏好对中的语义相似度自动重新加权歧义内容以减少歧义

Result: AAO在多个模型规模和基准数据集上显著超越现有方法，在AlpacaEval 2上比DPO提升8.9分，在Arena-Hard上提升15.0分

Conclusion: AAO通过减少训练中的歧义内容有效提升DPO性能，是一种简单有效的优化方法

Abstract: Direct Preference Optimization (DPO) is a widely used reinforcement learning from human feedback (RLHF) method across various domains. Recent research has increasingly focused on the role of token importance in improving DPO effectiveness. It is observed that identical or semantically similar content (defined as ambiguous content) frequently appears within the preference pairs. We hypothesize that the presence of ambiguous content during DPO training may introduce ambiguity, thereby limiting further improvements in alignment. Through mathematical analysis and proof-of-concept experiments, we reveal that ambiguous content may potentially introduce ambiguities, thereby degrading performance. To address this issue, we introduce Ambiguity Awareness Optimization (AAO), a simple yet effective approach that automatically re-weights ambiguous content to reduce ambiguities by calculating semantic similarity from preference pairs. Through extensive experiments, we demonstrate that AAO consistently and significantly surpasses state-of-the-art approaches in performance, without markedly increasing response length, across multiple model scales and widely adopted benchmark datasets, including AlpacaEval 2, MT-Bench, and Arena-Hard. Specifically, AAO outperforms DPO by up to 8.9 points on AlpacaEval 2 and achieves an improvement of by up to 15.0 points on Arena-Hard.

</details>


### [104] [MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation](https://arxiv.org/abs/2511.23397)
*Mahdi Rahmani,AmirHossein Saffari,Reyhane Rahmani*

Main category: cs.CL

TL;DR: MegaChat：首个完全合成的波斯语问答数据集，用于评估Telegram电商智能销售聊天机器人，采用多智能体架构自动生成高质量对话数据，无需昂贵的人工标注。


<details>
  <summary>Details</summary>
Motivation: 伊朗中小企业越来越多地使用Telegram进行销售，实时互动对转化至关重要。但为低资源语言（如波斯语）开发AI驱动的聊天机器人需要大量高质量问答数据集，这通常成本高昂且资源密集。

Method: 提出一种新颖的自动化多智能体架构，从活跃的Telegram购物频道收集数据，生成具有角色意识的问答对。系统包含专门的问题生成、验证和优化智能体，确保生成真实多样的对话数据。评估时比较了三种经典检索增强生成模型与先进的智能体系统（具有多查询检索、重排序和角色对齐响应合成功能）。

Result: 使用GPT-5.1在六个质量维度上进行评估，结果显示智能体架构在5个不同频道中的4个上优于传统RAG模型，证明了其无需依赖昂贵人工标注或复杂微调即可生成可扩展高质量数据集的能力。

Conclusion: MegaChat为中小企业提供了高效、经济高效的解决方案，用于在专业商业领域构建智能客户互动系统，推动了低资源语言多语言对话AI的进步。

Abstract: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [105] [Bridging Planning and Execution: Multi-Agent Path Finding Under Real-World Deadlines](https://arxiv.org/abs/2511.21886)
*Jingtian Yan,Shuai Zhou,Stephen F. Smith,Jiaoyang Li*

Main category: cs.RO

TL;DR: REMAP是一个执行感知的多智能体路径规划框架，通过ExecTimeNet准确估计执行时间，解决规划与执行之间的差距，在MAPF-RD问题上比基线方法提升20%的解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 传统MAPF问题假设简化的机器人模型，忽略了执行时的动力学约束、通信延迟和控制器变异性等实际因素，导致规划与执行之间存在差距，这对时间敏感应用尤其成问题。

Method: 提出REMAP执行感知MAPF规划框架，集成ExecTimeNet神经网络准确估计基于规划路径的执行时间，可与主流搜索式MAPF规划器（如MAPF-LNS和CBS）结合使用，仅需少量修改。

Result: 在最多300个智能体的基准地图上，REMAP相比基线方法（如恒定执行速度估计器）在解决方案质量上提升高达20%，有效解决了具有现实世界截止时间的MAPF-RD问题。

Conclusion: REMAP框架成功弥合了MAPF规划与执行之间的差距，通过准确估计执行时间，为时间敏感应用提供了更可靠的解决方案，可与现有MAPF方法有效集成。

Abstract: The Multi-Agent Path Finding (MAPF) problem aims to find collision-free paths for multiple agents while optimizing objectives such as the sum of costs or makespan. MAPF has wide applications in domains like automated warehouses, manufacturing systems, and airport logistics. However, most MAPF formulations assume a simplified robot model for planning, which overlooks execution-time factors such as kinodynamic constraints, communication latency, and controller variability. This gap between planning and execution is problematic for time-sensitive applications. To bridge this gap, we propose REMAP, an execution-informed MAPF planning framework that can be combined with leading search-based MAPF planners with minor changes. Our framework integrates the proposed ExecTimeNet to accurately estimate execution time based on planned paths. We demonstrate our method for solving MAPF with Real-world Deadlines (MAPF-RD) problem, where agents must reach their goals before a predefined wall-clock time. We integrate our framework with two popular MAPF methods, MAPF-LNS and CBS. Experiments show that REMAP achieves up to 20% improvement in solution quality over baseline methods (e.g., constant execution speed estimators) on benchmark maps with up to 300 agents.

</details>


### [106] [OpenTwinMap: An Open-Source Digital Twin Generator for Urban Autonomous Driving](https://arxiv.org/abs/2511.21925)
*Alex Richardson,Jonathan Sprinkle*

Main category: cs.RO

TL;DR: OpenTwinMap是一个开源的Python框架，用于生成高保真3D城市数字孪生，旨在降低自动驾驶研究中的仿真门槛。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶研究中，许多公开可用的数字孪生工具与特定仿真器紧密耦合、难以扩展或技术开销大。例如CARLA完全作为Unreal Engine C++插件实现，限制了灵活性和快速原型开发。

Method: 基于Python的开源框架，通过处理LiDAR扫描和OpenStreetMap数据，生成语义分割的静态环境资产（道路网络、地形、城市结构），可导出到Unreal Engine进行自动驾驶仿真。

Result: 目前框架已具备OSM和LiDAR数据预处理、基本道路网格和地形生成功能，并初步支持CARLA集成。强调可扩展性和并行化能力。

Conclusion: OpenTwinMap通过提供灵活、可扩展的Python框架，降低了研究人员在不同城市环境中适应和扩展数字孪生管道的门槛，有助于推进自动驾驶研究。

Abstract: Digital twins of urban environments play a critical role in advancing autonomous vehicle (AV) research by enabling simulation, validation, and integration with emerging generative world models. While existing tools have demonstrated value, many publicly available solutions are tightly coupled to specific simulators, difficult to extend, or introduce significant technical overhead. For example, CARLA-the most widely used open-source AV simulator-provides a digital twin framework implemented entirely as an Unreal Engine C++ plugin, limiting flexibility and rapid prototyping. In this work, we propose OpenTwinMap, an open-source, Python-based framework for generating high-fidelity 3D urban digital twins. The completed framework will ingest LiDAR scans and OpenStreetMap (OSM) data to produce semantically segmented static environment assets, including road networks, terrain, and urban structures, which can be exported into Unreal Engine for AV simulation. OpenTwinMap emphasizes extensibility and parallelization, lowering the barrier for researchers to adapt and scale the pipeline to diverse urban contexts. We describe the current capabilities of the OpenTwinMap, which includes preprocessing of OSM and LiDAR data, basic road mesh and terrain generation, and preliminary support for CARLA integration.

</details>


### [107] [RSPECT: Robust and Scalable Planner for Energy-Aware Coordination of UAV-UGV Teams in Aerial Monitoring](https://arxiv.org/abs/2511.21957)
*Cahit Ikbal Er,Amin Kashiri,Yasin Yazicioglu*

Main category: cs.RO

TL;DR: 提出RSPECT算法，用于规划无人机与地面充电车的协同路径，在不确定性环境下完成长期空中监测任务，实现最小化任务时间。


<details>
  <summary>Details</summary>
Motivation: 解决无人机与地面充电车协同执行长期空中监测任务时的鲁棒路径规划问题。无人机能量有限，需要地面充电车作为移动充电站，同时面临未知障碍、地形、风力等不确定性因素，需要制定无需重大修改的鲁棒计划。

Method: 将问题形式化为混合整数规划（MIP）问题，由于问题NP难，提出RSPECT启发式算法。该算法具有可扩展性和高效性，并提供算法复杂度、可行性和鲁棒性的理论分析。

Result: 通过仿真和实验验证了RSPECT算法的性能。算法能够在不确定性环境下生成鲁棒的无人机-地面车协同路径，实现最小化任务时间的目标。

Conclusion: RSPECT算法为能量受限的无人机与地面充电车协同执行长期空中监测任务提供了有效的鲁棒路径规划解决方案，能够应对环境不确定性，具有实际应用价值。

Abstract: We consider the robust planning of energy-constrained unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), which act as mobile charging stations, to perform long-horizon aerial monitoring missions. More specifically, given a set of points to be visited by the UAVs and desired final positions of the UAV-UGV teams, the objective is to find a robust plan (the vehicle trajectories) that can be realized without a major revision in the face of uncertainty (e.g., unknown obstacles/terrain, wind) to complete this mission in minimum time. We provide a formal description of this problem as a mixed-integer program (MIP), which is NP-hard. Since exact solution methods are computationally intractable for such problems, we propose RSPECT, a scalable and efficient heuristic. We provide theoretical results on the complexity of our algorithm and the feasibility and robustness of resulting plans. We also demonstrate the performance of our method via simulations and experiments.

</details>


### [108] [Constant-Volume Deformation Manufacturing for Material-Efficient Shaping](https://arxiv.org/abs/2511.22042)
*Lei Li,Jiale Gong,Ziyang Li,Hong Wang*

Main category: cs.RO

TL;DR: 提出一种基于体积保持的数字模具范式，通过实时体积一致性建模、几何信息变形预测和误差补偿策略，实现塑料材料的高可预测性成形，达到98%以上的材料利用率。


<details>
  <summary>Details</summary>
Motivation: 传统增材和减材制造依赖离散堆叠或局部去除，限制了连续可控变形，导致体积损失和形状偏差，需要一种能够保持体积一致性的可持续、零浪费的成形方法。

Method: 集成实时体积一致性建模、几何信息变形预测和误差补偿策略的数字模具范式，通过分析成形后点云的变形模式和误差趋势，校正弹性回弹和累积误差。

Result: 在五种代表性几何形状上的实验表明，系统能够高保真地复现目标形状，同时实现超过98%的材料利用率。

Conclusion: 该方法建立了数字驱动、可重复的可持续零浪费成形路径，连接了数字建模、实时传感和自适应成形，推动了下一代可持续和可定制制造的发展。

Abstract: Additive and subtractive manufacturing enable complex geometries but rely on discrete stacking or local removal, limiting continuous and controllable deformation and causing volume loss and shape deviations. We present a volumepreserving digital-mold paradigm that integrates real-time volume-consistency modeling with geometry-informed deformation prediction and an error-compensation strategy to achieve highly predictable shaping of plastic materials. By analyzing deformation patterns and error trends from post-formed point clouds, our method corrects elastic rebound and accumulation errors, maintaining volume consistency and surface continuity. Experiments on five representative geometries demonstrate that the system reproduces target shapes with high fidelity while achieving over 98% material utilization. This approach establishes a digitally driven, reproducible pathway for sustainable, zero-waste shaping of user-defined designs, bridging digital modeling, real-time sensing, and adaptive forming, and advancing next-generation sustainable and customizable manufacturing.

</details>


### [109] [SwordRiding: A Unified Navigation Framework for Quadrotors in Unknown Complex Environments via Online Guiding Vector Fields](https://arxiv.org/abs/2511.22043)
*Xuchen Liu,Ruocheng Li,Bin Xin,Weijia Yao,Qigeng Duan,Jinqiang Cui,Ben M. Chen,Jie Chen*

Main category: cs.RO

TL;DR: 提出基于引导向量场的四旋翼无人机实时导航框架，通过在线构建GVF实现未知复杂环境中的闭环导航，提高抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 现有四旋翼导航框架多为开环运行，难以应对环境不确定性（如风扰、外部扰动），在未知复杂环境中实时适应性不足。

Method: 1) 机载感知模块构建ESDF环境表示；2) 全局规划器生成离散无碰撞路径点；3) 均匀B样条参数化生成平滑参考轨迹；4) 从ESDF和优化B样条轨迹合成自适应GVF；5) 采用闭环导航范式。

Result: 大量仿真和真实实验表明，相比传统方法，该方法显著提高了对外部扰动的鲁棒性，并具有优越的实时性能。

Conclusion: 提出的基于GVF的统一实时导航框架成功解决了四旋翼在未知复杂环境中的实时适应性问题，通过闭环导航范式显著增强了系统鲁棒性。

Abstract: Although quadrotor navigation has achieved high performance in trajectory planning and control, real-time adaptability in unknown complex environments remains a core challenge. This difficulty mainly arises because most existing planning frameworks operate in an open-loop manner, making it hard to cope with environmental uncertainties such as wind disturbances or external perturbations. This paper presents a unified real-time navigation framework for quadrotors in unknown complex environments, based on the online construction of guiding vector fields (GVFs) from discrete reference path points. In the framework, onboard perception modules build a Euclidean Signed Distance Field (ESDF) representation of the environment, which enables obstacle awareness and path distance evaluation. The system first generates discrete, collision-free path points using a global planner, and then parameterizes them via uniform B-splines to produce a smooth and physically feasible reference trajectory. An adaptive GVF is then synthesized from the ESDF and the optimized B-spline trajectory. Unlike conventional approaches, the method adopts a closed-loop navigation paradigm, which significantly enhances robustness under external disturbances. Compared with conventional GVF methods, the proposed approach directly accommodates discretized paths and maintains compatibility with standard planning algorithms. Extensive simulations and real-world experiments demonstrate improved robustness against external disturbances and superior real-time performance.

</details>


### [110] [SoftNash: Entropy-Regularized Nash Games for Non-Fighting Virtual Fixtures](https://arxiv.org/abs/2511.22087)
*Tai Inui,Jee-Hwan Ryu*

Main category: cs.RO

TL;DR: 提出Soft-Nash虚拟夹具，通过单一可解释参数τ软化传统虚拟夹具，在保持精度的同时减少用户冲突、降低认知负荷并提升代理感


<details>
  <summary>Details</summary>
Motivation: 传统虚拟夹具虽然能提高遥操作精度，但会与用户产生"对抗"，增加认知负荷并削弱代理感，需要一种能平衡精度与用户体验的共享控制策略

Method: 基于博弈论的Soft-Nash共享控制策略，通过单一参数τ膨胀夹具的努力权重，从KL正则化信任区域和最大熵视角推导，获得机器人最佳响应的闭式解

Result: 在6自由度触觉设备上的3D跟踪任务中，适度软化（τ≈1-3，特别是τ=2）保持跟踪误差与传统虚拟夹具无统计差异，同时显著减少冲突、降低NASA-TLX负荷、提升代理感

Conclusion: 单参数Soft-Nash策略能在保持精度的同时改善舒适度和感知代理感，为触觉和遥操作中的个性化共享控制提供了实用且可解释的途径

Abstract: Virtual fixtures (VFs) improve precision in teleoperation but often ``fight'' the user, inflating mental workload and eroding the sense of agency. We propose Soft-Nash Virtual Fixtures, a game-theoretic shared-control policy that softens the classic two-player linear-quadratic (LQ) Nash solution by inflating the fixture's effort weight with a single, interpretable scalar parameter $τ$. This yields a continuous dial on controller assertiveness: $τ=0$ recovers a hard, performance-focused Nash / virtual fixture controller, while larger $τ$ reduce gains and pushback, yet preserve the equilibrium structure and continuity of closed-loop stability. We derive Soft-Nash from both a KL-regularized trust-region and a maximum-entropy viewpoint, obtaining a closed-form robot best response that shrinks authority and aligns the fixture with the operator's input as $τ$ grows. We implement Soft-Nash on a 6-DoF haptic device in 3D tracking task ($n=12$). Moderate softness ($τ\approx 1-3$, especially $τ=2$) maintains tracking error statistically indistinguishable from a tuned classic VF while sharply reducing controller-user conflict, lowering NASA-TLX workload, and increasing Sense of Agency (SoAS). A composite BalancedScore that combines normalized accuracy and non-fighting behavior peaks near $τ=2-3$. These results show that a one-parameter Soft-Nash policy can preserve accuracy while improving comfort and perceived agency, providing a practical and interpretable pathway to personalized shared control in haptics and teleoperation.

</details>


### [111] [Design of an Adaptive Modular Anthropomorphic Dexterous Hand for Human-like Manipulation](https://arxiv.org/abs/2511.22100)
*Zelong Zhou,Wenrui Chen,Zeyun Hu,Qiang Diao,Qixin Gao,Yaonan Wang*

Main category: cs.RO

TL;DR: 提出一种基于生物协同原理的模块化灵巧手设计，通过2个驱动器控制4自由度的仿人手指拓扑结构，平衡了驱动复杂性和灵巧性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 生物协同原理被广泛用于灵巧手设计，但过度耦合会降低手的灵巧性。本文旨在解决驱动复杂性和灵巧性之间的权衡问题。

Method: 提出4自由度2驱动的仿人手指拓扑结构，基于此开发自适应模块化灵巧手。通过分析手部生物协同和人类手势，将关节级协调和结构特征转化为模块化手指架构，建立运动学模型分析自适应抓取和手内操作。

Result: 构建了物理原型并进行初步实验，验证了所提出设计和分析的有效性。

Conclusion: 提出的仿人手指拓扑和模块化灵巧手设计成功平衡了驱动复杂性和灵巧性，为灵巧手设计提供了有效解决方案。

Abstract: Biological synergies have emerged as a widely adopted paradigm for dexterous hand design, enabling human-like manipulation with a small number of actuators. Nonetheless, excessive coupling tends to diminish the dexterity of hands. This paper tackles the trade-off between actuation complexity and dexterity by proposing an anthropomorphic finger topology with 4 DoFs driven by 2 actuators, and by developing an adaptive, modular dexterous hand based on this finger topology. We explore the biological basis of hand synergies and human gesture analysis, translating joint-level coordination and structural attributes into a modular finger architecture. Leveraging these biomimetic mappings, we design a five-finger modular hand and establish its kinematic model to analyze adaptive grasping and in-hand manipulation. Finally, we construct a physical prototype and conduct preliminary experiments, which validate the effectiveness of the proposed design and analysis.

</details>


### [112] [3D Affordance Keypoint Detection for Robotic Manipulation](https://arxiv.org/abs/2511.22195)
*Zhiyang Liu,Ruiteng Zhao,Lei Zhou,Chengran Yuan,Yuwei Wu,Sheng Guo,Zhengshen Zhang,Chenchen Liu,Marcelo H Ang*

Main category: cs.RO

TL;DR: 提出FAKP-Net，通过3D关键点四元组增强机器人对物体功能性的理解，同时解决"是什么、在哪里、如何操作"三个问题，超越传统仅关注语义分割的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法将可供性检测视为语义分割任务，只关注"物体能用来做什么"（what），但缺乏对"在哪里操作"（where）和"如何操作"（how）的指导。需要更全面的方法为机器人操作提供完整信息。

Method: 提出基于融合的可供性关键点网络（FAKP-Net），引入3D关键点四元组，利用RGB和深度图像的协同潜力，提供执行位置、方向和范围信息。

Result: 基准测试显示FAKP-Net在可供性分割任务和关键点检测任务上显著优于现有模型。真实世界实验也证明该方法在处理未见过的物体时能可靠完成操作任务。

Conclusion: FAKP-Net通过3D关键点方法全面解决机器人操作中的what、where、how问题，为可供性感知的机器人操作提供了更有效的解决方案。

Abstract: This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts' functionality. The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question. To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent. Benchmark testing demonstrates that FAKP-Net outperforms existing models by significant margins in affordance segmentation task and keypoint detection task. Real-world experiments also showcase the reliability of our method in accomplishing manipulation tasks with previously unseen objects.

</details>


### [113] [Bayesian Decentralized Decision-making for Multi-Robot Systems: Sample-efficient Estimation of Event Rates](https://arxiv.org/abs/2511.22225)
*Gabriel Aguirre,Simay Atasoy Bingöl,Heiko Hamann,Jonas Kuckling*

Main category: cs.RO

TL;DR: 提出一种去中心化贝叶斯框架，让简单机器人群体通过泊松过程估计危险事件率，选择更安全区域，在减少危险暴露的同时高效收敛。


<details>
  <summary>Details</summary>
Motivation: 在危险环境中，群体机器人需要平衡探索、通信和不确定性估计，特别是在直接测量受限或成本高昂的情况下，需要一种能够识别更安全区域的集体决策方法。

Method: 采用去中心化贝叶斯框架，机器人使用共轭先验逐步预测危险事件间隔时间，并推导置信度估计来调整行为，通过泊松过程建模未知危险事件率。

Result: 仿真结果显示，机器人群体能一致选择正确区域，同时通过样本高效性减少危险暴露。相比基准启发式方法，在安全性和收敛速度方面表现更好。

Conclusion: 该框架可扩展集体决策基准测试集，适用于危险动态环境中的自适应风险感知采样和探索应用。

Abstract: Effective collective decision-making in swarm robotics often requires balancing exploration, communication and individual uncertainty estimation, especially in hazardous environments where direct measurements are limited or costly. We propose a decentralized Bayesian framework that enables a swarm of simple robots to identify the safer of two areas, each characterized by an unknown rate of hazardous events governed by a Poisson process. Robots employ a conjugate prior to gradually predict the times between events and derive confidence estimates to adapt their behavior. Our simulation results show that the robot swarm consistently chooses the correct area while reducing exposure to hazardous events by being sample-efficient. Compared to baseline heuristics, our proposed approach shows better performance in terms of safety and speed of convergence. The proposed scenario has potential to extend the current set of benchmarks in collective decision-making and our method has applications in adaptive risk-aware sampling and exploration in hazardous, dynamic environments.

</details>


### [114] [MLATC: Fast Hierarchical Topological Mapping from 3D LiDAR Point Clouds Based on Adaptive Resonance Theory](https://arxiv.org/abs/2511.22238)
*Ryosuke Ofuchi,Yuichiro Toda,Naoki Masuyama,Takayuki Matsuno*

Main category: cs.RO

TL;DR: 提出MLATC方法，通过分层结构加速ATC-DT拓扑建图，实现大规模环境下的实时处理


<details>
  <summary>Details</summary>
Motivation: ATC-DT方法在构建大规模拓扑地图时存在可扩展性问题，其穷举最近邻搜索随着节点数量增加而效率下降

Method: 提出多层ATC（MLATC），将节点组织成层次结构，实现从粗到精的分辨率搜索；采用自适应层添加机制，在低层饱和时自动加深层次

Result: 仿真实验显示MLATC比原始ATC-DT加速拓扑建图，搜索时间随节点数呈亚线性（近似对数）增长；真实校园规模LiDAR数据集实验证实MLATC保持毫秒级每帧运行时间，实现大规模环境实时全局拓扑建图

Conclusion: MLATC显著提升了ATC-DT的计算效率，解决了大规模动态未知环境中实时拓扑建图的可扩展性问题

Abstract: This paper addresses the problem of building global topological maps from 3D LiDAR point clouds for autonomous mobile robots operating in large-scale, dynamic, and unknown environments. Adaptive Resonance Theory-based Topological Clustering with Different Topologies (ATC-DT) builds global topological maps represented as graphs while mitigating catastrophic forgetting during sequential processing. However, its winner selection mechanism relies on an exhaustive nearest-neighbor search over all existing nodes, leading to scalability limitations as the map grows. To address this challenge, we propose a hierarchical extension called Multi-Layer ATC (MLATC). MLATC organizes nodes into a hierarchy, enabling the nearest-neighbor search to proceed from coarse to fine resolutions, thereby drastically reducing the number of distance evaluations per query. The number of layers is not fixed in advance. MLATC employs an adaptive layer addition mechanism that automatically deepens the hierarchy when lower layers become saturated, keeping the number of user-defined hyperparameters low. Simulation experiments on synthetic large-scale environments show that MLATC accelerates topological map building compared to the original ATC-DT and exhibits a sublinear, approximately logarithmic scaling of search time with respect to the number of nodes. Experiments on campus-scale real-world LiDAR datasets confirm that MLATC maintains a millisecond-level per-frame runtime and enables real-time global topological map building in large-scale environments, significantly outperforming the original ATC-DT in terms of computational efficiency.

</details>


### [115] [Soft Fluidic Sheet Transistor for Soft Robotic System Enabling Fluid Logic Operations](https://arxiv.org/abs/2511.22318)
*Yuki Origane,Koya Cho,Hideyuki Tsukagoshi*

Main category: cs.RO

TL;DR: 提出一种软质聚氨酯片状阀门（FST），仅使用气动信号即可实现逻辑运算，类似电子电路中的晶体管，可用于构建流体机器人系统。


<details>
  <summary>Details</summary>
Motivation: 为了实现软体机器人系统的高功能性和灵活性，需要开发能够仅使用气动信号执行逻辑操作的元件，从而减少电气部件的依赖并提高系统的柔性和安全性。

Method: 设计软质聚氨酯片状阀门，当控制腔加压时，主通道沿中心轴压缩、屈曲并被压紧，实现阻塞。这种阀门作为NOT逻辑元件（FST），通过集成多个FST可在单张片上实现正逻辑、NAND、NOR等逻辑运算。

Result: 成功开发出FST并验证其逻辑运算功能，构建了锁存电路（自保持逻辑电路），结合流体检测器和流体执行器，实现了仅使用单管气压就能在遇到障碍物时主动改变姿态的行为。

Conclusion: 提出的FST能够仅使用气动信号实现逻辑运算，为构建完全气动的软体机器人系统提供了有效解决方案，验证了该方法的可行性和有效性。

Abstract: Aiming to achieve both high functionality and flexibility in soft robot system, this paper presents a soft urethane sheet-like valve with an amplifier that can perform logical operations using only pneumatic signals. When the control chamber in the valve is pressurized, the main path is compressed along its central axis, buckling and being pressed,resulting in blockage. This allows control by a pressure signal smaller than that within the main channel. Furthermore, similar to transistors in electrical circuits, when combined, the proposed valve can perform a variety of logical operations. The basic type operates as a NOT logic element, which is named the fluidic sheet transistor (FST). By integrating multiple FSTs, logical operations such as positive logic, NAND, and NOR can be performed on a single sheet. This paper describes the operating principle, fabrication method, and characteristics of the FST,followed by a method for configuring logical operations.Moreover, we demonstrate the construction of a latch circuit(self-holding logic circuit) using FST, introducing a prototype of a fluid robot system that combines a tactile tube as a fluidic detector and fluid actuators. This demonstrates that it is possible to generate behavior that actively changes posture when hitting an obstacle using only air pressure from a single pipe, which verifies the effectiveness of the proposed methods.

</details>


### [116] [Nonholonomic Narrow Dead-End Escape with Deep Reinforcement Learning](https://arxiv.org/abs/2511.22338)
*Denghan Xiong,Yanzhe Zhao,Yutong Chen,Zichun Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于深度强化学习的非完整约束Ackermann车辆窄死胡同逃逸方法，通过生成训练环境并训练策略，相比传统规划器在逃逸成功率和机动次数方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 非完整约束限制了Ackermann车辆的可行速度，使其无法执行几何路径；狭窄死胡同逃逸需要复杂的前进-后退机动序列；传统规划器将全局搜索与局部转向解耦，在狭窄通道中采样效率低且对间隙敏感。

Method: 1) 构建生成器，采样与Ackermann运动学兼容的多阶段前进-后退轨迹，并膨胀其包络以合成保证至少有一个可行逃逸路径的窄死胡同族；2) 构建强制执行运动学约束的训练环境，使用软演员-评论家算法训练策略；3) 与结合全局搜索和非完整转向的经典规划器进行对比评估。

Result: 在参数化的死胡同族中，学习到的策略解决了更大比例的实例，减少了机动次数，在保持可比路径长度和规划时间的同时，在相同的感知和控制限制下表现更优。

Conclusion: 基于深度强化学习的非完整约束Ackermann车辆窄死胡同逃逸方法优于传统规划器，提供了开源实现，为复杂非完整约束环境下的机器人导航提供了有效解决方案。

Abstract: Nonholonomic constraints restrict feasible velocities without reducing configuration-space dimension, which makes collision-free geometric paths generally non-executable for car-like robots. Ackermann steering further imposes curvature bounds and forbids in-place rotation, so escaping from narrow dead ends typically requires tightly sequenced forward and reverse maneuvers. Classical planners that decouple global search and local steering struggle in these settings because narrow passages occupy low-measure regions and nonholonomic reachability shrinks the set of valid connections, which degrades sampling efficiency and increases sensitivity to clearances. We study nonholonomic narrow dead-end escape for Ackermann vehicles and contribute three components. First, we construct a generator that samples multi-phase forward-reverse trajectories compatible with Ackermann kinematics and inflates their envelopes to synthesize families of narrow dead ends that are guaranteed to admit at least one feasible escape. Second, we construct a training environment that enforces kinematic constraints and train a policy using the soft actor-critic algorithm. Third, we evaluate against representative classical planners that combine global search with nonholonomic steering. Across parameterized dead-end families, the learned policy solves a larger fraction of instances, reduces maneuver count, and maintains comparable path length and planning time while under the same sensing and control limits. We provide our project as open source at https://github.com/gitagitty/cisDRL-RobotNav.git

</details>


### [117] [LLM-Based Generalizable Hierarchical Task Planning and Execution for Heterogeneous Robot Teams with Event-Driven Replanning](https://arxiv.org/abs/2511.22354)
*Suraj Borate,Bhavish Rai B,Vipul Pardeshi,Madhu Vadali*

Main category: cs.RO

TL;DR: CoMuRoS是一个分层架构，用于异构机器人团队，结合集中式规划和分布式执行，支持事件驱动的重规划，通过LLM实现自然语言任务解释和代码生成，在硬件和仿真中展示了鲁棒的多机器人和人机协作。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的机器人系统缺乏运行时事件驱动的重规划能力，无法在物理机器人上实现鲁棒的多机器人和人机协作。需要一种能够处理动态环境变化、任务失败和用户意图变更的通用架构。

Method: 采用分层架构：任务管理器LLM解释自然语言目标、分类任务、分配子任务；每个机器人运行本地LLM将原始技能组合成可执行Python代码；机载感知持续监控事件并分类；任务失败或用户意图变化触发重规划。

Result: 硬件实验：自主恢复成功率9/10，协调运输8/8，人辅助恢复5/5；仿真展示意图感知重规划；文本基准测试（22个场景）在任务分配、分类、IoU等方面获得高分（正确率最高0.91）；重规划场景正确率达1.0。

Conclusion: CoMuRoS首次在物理机器人上实现了运行时事件驱动的重规划，提供了鲁棒、灵活的多机器人和人机协作能力，相比现有LLM系统具有显著优势。

Abstract: This paper introduces CoMuRoS (Collaborative Multi-Robot System), a generalizable hierarchical architecture for heterogeneous robot teams that unifies centralized deliberation with decentralized execution, and supports event-driven replanning. A Task Manager LLM interprets natural-language goals, classifies tasks, and allocates subtasks using static rules plus dynamic contexts (task, history, robot and task status, and events).Each robot runs a local LLM that composes executable Python code from primitive skills (ROS2 nodes, policies), while onboard perception (VLMs/image processing) continuously monitors events and classifies them into relevant or irrelevant to the task. Task failures or user intent changes trigger replanning, allowing robots to assist teammates, resume tasks, or request human help. Hardware studies demonstrate autonomous recovery from disruptive events, filtering of irrelevant distractions, and tightly coordinated transport with emergent human-robot cooperation (e.g., multirobot collaborative object recovery success rate: 9/10, coordinated transport: 8/8, human-assisted recovery: 5/5).Simulation studies show intention-aware replanning. A curated textual benchmark spanning 22 scenarios (3 tasks each, around 20 robots) evaluates task allocation, classification, IoU, executability, and correctness, with high average scores (e.g., correctness up to 0.91) across multiple LLMs, a separate replanning set (5 scenarios) achieves 1.0 correctness. Compared with prior LLM-based systems, CoMuRoS uniquely demonstrates runtime, event-driven replanning on physical robots, delivering robust, flexible multi-robot and human-robot collaboration.

</details>


### [118] [BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands](https://arxiv.org/abs/2511.22364)
*Seongwon Cho,Daechul Ahn,Donghyun Shin,Hyeonbeom Choi,San Kim,Jonghyun Choi*

Main category: cs.RO

TL;DR: BINDER是一个双过程框架，通过分离战略规划和连续环境监控来解决开放词汇移动操作中的动态环境适应问题，显著提高了成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法只在离散更新点更新世界表示（如导航目标、路径点），导致机器人在更新间处于"盲区"，造成级联失败：遗漏物体、错误检测延迟、重规划滞后。需要解决动态环境中保持感知与避免昂贵更新之间的权衡问题。

Method: 提出BINDER框架，包含两个互补模块：1) 审慎响应模块（DRM）- 多模态LLM进行战略规划和结构化3D场景更新；2) 即时响应模块（IRM）- VideoLLM进行连续视频流监控。两模块双向协调：DRM指导IRM关注内容，IRM更新记忆、纠正正在执行的动作并在必要时触发重规划。

Result: 在三个真实世界环境中进行动态物体放置评估，BINDER相比最先进的基线方法实现了显著更高的成功率和效率，展示了其在真实世界部署的有效性。

Conclusion: BINDER通过分离战略规划和连续监控的双过程框架，解决了开放词汇移动操作中动态环境适应的关键挑战，实现了在真实世界部署中鲁棒的适应性。

Abstract: Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.

</details>


### [119] [Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion](https://arxiv.org/abs/2511.22445)
*Yikai Tang,Haoran Geng,Sheng Zang,Pieter Abbeel,Jitendra Malik*

Main category: cs.RO

TL;DR: VGDP是一个多模态模仿学习框架，通过互补感知融合模块和模态级dropout平衡RGB和点云特征，在视觉和空间扰动下表现出强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在空间和视觉随机化下泛化能力不足，容易过拟合，需要设计更有效的观察编码器来提升策略泛化能力。

Method: 提出视觉几何扩散策略(VGDP)，采用互补感知融合模块，通过模态级dropout强制平衡RGB和点云线索的使用，交叉注意力仅作为轻量级交互层。

Result: 在18个模拟任务和4个真实任务中，VGDP平均性能提升39.1%，在视觉扰动下平均提升41.5%，在空间设置下平均提升15.2%，显著优于7个基线策略。

Conclusion: 融合潜在空间的表达能力主要来自模态级dropout强制实现的互补性，交叉注意力主要作为轻量级交互机制而非鲁棒性的主要来源。

Abstract: Imitation learning has emerged as a crucial ap proach for acquiring visuomotor skills from demonstrations, where designing effective observation encoders is essential for policy generalization. However, existing methods often struggle to generalize under spatial and visual randomizations, instead tending to overfit. To address this challenge, we propose Visual Geometry Diffusion Policy (VGDP), a multimodal imitation learning framework built around a Complementarity-Aware Fusion Module where modality-wise dropout enforces balanced use of RGB and point-cloud cues, with cross-attention serving only as a lightweight interaction layer. Our experiments show that the expressiveness of the fused latent space is largely induced by the enforced complementarity from modality-wise dropout, with cross-attention serving primarily as a lightweight interaction mechanism rather than the main source of robustness. Across a benchmark of 18 simulated tasks and 4 real-world tasks, VGDP outperforms seven baseline policies with an average performance improvement of 39.1%. More importantly, VGDP demonstrates strong robustness under visual and spatial per turbations, surpassing baselines with an average improvement of 41.5% in different visual conditions and 15.2% in different spatial settings.

</details>


### [120] [RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion](https://arxiv.org/abs/2511.22505)
*Xiujian Liang,Jiacheng Liu,Mingyang Sun,Qichen He,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: 提出RealD²iff框架，通过扩散模型学习从干净深度合成真实噪声深度，实现纯仿真驱动的机器人学习，无需真实传感器数据收集即可构建干净-噪声配对数据集，并实现零样本sim2real机器人操作。


<details>
  <summary>Details</summary>
Motivation: 真实世界机器人操作受视觉sim2real差距限制，仿真中收集的深度观测无法反映真实传感器的复杂噪声模式。传统方法难以准确建模真实深度噪声。

Method: 提出RealD²iff分层扩散框架，将深度噪声分解为全局结构扭曲和细粒度局部扰动。采用频率引导监督(FGS)进行全局结构建模，差异引导优化(DGO)进行局部细化。构建六阶段管道集成到模仿学习中。

Result: RealD²iff能够生成真实世界般的深度数据，无需手动传感器数据收集即可构建干净-噪声配对数据集。实现零样本sim2real机器人操作，显著提高真实世界性能而无需额外微调。

Conclusion: 通过clean-to-noisy范式成功弥合视觉sim2real差距，实现纯仿真驱动的机器人学习。RealD²iff框架为机器人操作提供有效的视觉域适应解决方案。

Abstract: Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.

</details>


### [121] [BUDD-e: an autonomous robotic guide for visually impaired users](https://arxiv.org/abs/2511.22541)
*Jinyang Li,Marcello Farina,Luca Mozzarelli,Luca Cattaneo,Panita Rattamasanaprapai,Eleonora A. Tagarelli,Matteo Corno,Paolo Perego,Giuseppe Andreoni,Emanuele Lettieri*

Main category: cs.RO

TL;DR: 开发了名为BUDD-e的新型导盲机器人原型，并在医院真实场景中由视障志愿者测试，表现出色且用户接受度高。


<details>
  <summary>Details</summary>
Motivation: 为视障用户开发实用的导盲机器人，帮助他们在复杂环境中导航，提高独立性和安全性。

Method: 设计并实现了BUDD-e导盲机器人原型，在米兰Niguarda大都会医院的真实场景中，通过视障志愿者进行实地测试。

Result: 实验结果显示机器人性能卓越，用户接受度高，在真实环境中表现出色。

Conclusion: BUDD-e导盲机器人原型在真实场景测试中证明有效，具有良好的性能和用户接受度，为视障人士提供了实用的导航辅助。

Abstract: This paper describes the design and the realization of a prototype of the novel guide robot BUDD-e for visually impaired users. The robot has been tested in a real scenario with the help of visually disabled volunteers at ASST Grande Ospedale Metropolitano Niguarda, in Milan. The results of the experimental campaign are throughly described in the paper, displaying its remarkable performance and user-acceptance.

</details>


### [122] [Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention](https://arxiv.org/abs/2511.22555)
*Yanbo Mao,Jianlong Fu,Ruoxuan Zhang,Hongxia Xie,Meibao Yao*

Main category: cs.RO

TL;DR: 提出LIBERO-Elegant基准和优雅执行框架，通过离线学习优雅批评器和即时干预机制，在不修改基础VLA策略的情况下提升机器人执行质量


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在机器人操作中表现出执行质量不稳定，这源于人类演示数据的质量参差不齐，其中隐含的任务约束只被部分满足

Method: 1) 定义优雅执行为满足隐含任务约束；2) 通过离线校准Q学习训练优雅批评器评估动作质量；3) 推理时使用即时干预机制在关键时刻选择性干预

Result: 在LIBERO-Elegant基准和真实世界操作任务上的实验表明，优雅批评器显著提升了执行质量，即使对未见任务也有效

Conclusion: 该框架实现了不仅关注任务是否成功，更关注任务执行方式的机器人控制，为提升VLA模型执行质量提供了有效解决方案

Abstract: Vision-Language-Action (VLA) models have enabled notable progress in general-purpose robotic manipulation, yet their learned policies often exhibit variable execution quality. We attribute this variability to the mixed-quality nature of human demonstrations, where the implicit principles that govern how actions should be carried out are only partially satisfied. To address this challenge, we introduce the LIBERO-Elegant benchmark with explicit criteria for evaluating execution quality. Using these criteria, we develop a decoupled refinement framework that improves execution quality without modifying or retraining the base VLA policy. We formalize Elegant Execution as the satisfaction of Implicit Task Constraints (ITCs) and train an Elegance Critic via offline Calibrated Q-Learning to estimate the expected quality of candidate actions. At inference time, a Just-in-Time Intervention (JITI) mechanism monitors critic confidence and intervenes only at decision-critical moments, providing selective, on-demand refinement. Experiments on LIBERO-Elegant and real-world manipulation tasks show that the learned Elegance Critic substantially improves execution quality, even on unseen tasks. The proposed model enables robotic control that values not only whether tasks succeed, but also how they are performed.

</details>


### [123] [Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation](https://arxiv.org/abs/2511.22685)
*Haoyi Wang,Licheng Luo,Yiannis Kantaros,Bruno Sinopoli,Mingyu Cai*

Main category: cs.RO

TL;DR: 提出混合框架，结合强化学习反应式导航与按需多智能体路径规划，解决密集环境中多机器人导航的死锁问题


<details>
  <summary>Details</summary>
Motivation: 多机器人在密集环境导航时，需要在反应式避碰与长期目标达成间平衡。狭窄通道或受限空间中常出现死锁，特别是当强化学习控制策略遇到超出学习分布的新配置时。现有基于RL的方法在未见环境中的泛化能力有限。

Method: 提出混合框架，无缝集成基于RL的反应式导航与按需多智能体路径规划(MAPF)。包含安全层监控智能体进度以检测死锁，触发时启动受影响智能体的协调控制器。框架通过MAPF构建全局可行轨迹，并调节航点进度以减少导航中的智能体间冲突。

Result: 在密集多智能体基准测试中，方法将任务完成率从边缘提升到接近普遍成功，显著减少死锁和碰撞。与分层任务规划结合时，支持异构机器人的协调导航，证明反应式RL导航与选择性MAPF干预结合能实现鲁棒的零样本性能。

Conclusion: 耦合反应式RL导航与选择性MAPF干预能产生鲁棒、零样本性能的解决方案，有效解决多机器人导航中的死锁问题，提升在密集环境中的导航成功率。

Abstract: Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages
  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.
  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.

</details>


### [124] [Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations](https://arxiv.org/abs/2511.22697)
*Chancharik Mitra,Yusen Luo,Raj Saravanan,Dantong Niu,Anirudh Pai,Jesse Thomason,Trevor Darrell,Abrar Anwar,Deva Ramanan,Roei Herzig*

Main category: cs.RO

TL;DR: Robotic Steering：基于机制可解释性的微调方法，通过少样本演示识别并选择性微调与机器人任务物理、视觉、语言需求对齐的特定注意力头，在机器人任务中优于LoRA方法


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言动作模型微调方法缺乏特异性，无论任务的视觉、语言和物理特性如何，都调整相同的参数集。受神经科学中功能特异性的启发，作者认为针对特定任务微调稀疏模型表示更有效

Method: 提出Robotic Steering方法：基于机制可解释性，利用少样本演示识别并选择性微调与机器人任务物理、视觉、语言需求对齐的特定注意力头

Result: 在Franka Emika机械臂上的全面机器人评估表明，Robotic Steering在任务变化下优于LoRA，具有更好的鲁棒性、更低的计算成本和更强的可解释性

Conclusion: Robotic Steering为适应多样化机器人任务提供了一种高效、可解释的微调方法，通过功能特异性原则实现了更好的性能

Abstract: Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.

</details>


### [125] [A Two Degrees-of-Freedom Floor-Based Robot for Transfer and Rehabilitation Applications](https://arxiv.org/abs/2511.22705)
*Ian Lalonde,Jeff Denis,Mathieu Lamy,Camille Martin,Karina Lebel,Alexandre Girard*

Main category: cs.RO

TL;DR: 开发了一种可调节阻抗和垂直/前向力的坐立训练设备，能够适应不同活动能力水平的训练需求，同时保持商业升降辅助设备的转移功能。


<details>
  <summary>Details</summary>
Motivation: 坐立动作能力对提高功能活动性和降低再住院风险至关重要。现有的升降辅助设备和部分体重支撑康复设备无法根据不同活动能力水平调整坐立训练。

Method: 开发了一种坐立训练设备，允许配置不同的阻抗和垂直/前向力，以适应多种训练需求，同时保持商业升降辅助设备的转移能力。

Result: 对具有不同身高体重的健康成年人（男女均有）的实验表明：1）设备对自然坐立运动学影响小；2）能在患者质心提供精确的减重支持；3）可在坐立动作开始时增加虚拟前向弹簧，辅助体重转移到脚部以实现离座。

Conclusion: 该坐立训练设备能够适应不同的训练需求，有效支持坐立动作训练，同时保持升降辅助功能。

Abstract: The ability to accomplish a sit-to-stand (STS) motion is key to increase functional mobility and reduce rehospitalization risks. While raising aid (transfer) devices and partial bodyweight support (rehabilitation) devices exist, both are unable to adjust the STS training to different mobility levels. Therefore, We have developed an STS training device that allows various configurations of impedance and vertical/forward forces to adapt to many training needs while maintaining commercial raising aid transfer capabilities. Experiments with healthy adults (both men and women) of various heights and weights show that the device 1) has a low impact on the natural STS kinematics, 2) can provide precise weight unloading at the patient's center of mass and 3) can add a forward virtual spring to assist the transfer of the bodyweight to the feet for seat-off, at the start of the STS motion.

</details>


### [126] [Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion](https://arxiv.org/abs/2511.22744)
*Rémy Rahem,Wael Suleiman*

Main category: cs.RO

TL;DR: 多视角深度感知框架结合本体感知与双深度流，通过师生蒸馏方法提升四足机器人在动态运动中的环境感知能力和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前腿式机器人主要依赖第一人称视角感知，在视野被遮挡时性能受限。需要多视角互补感知来增强环境意识，实现更敏捷、鲁棒的动态运动

Method: 提出多视角深度感知框架，结合本体感知与双深度流（第一人称和第三人称）。采用师生蒸馏方法训练策略，引入广泛的领域随机化，包括随机远程相机丢失和3D位置扰动

Result: 多视角策略在跨越间隙、台阶下降等动态动作上优于单视角基线，在外视角相机部分或完全不可用时仍能保持稳定性。适度的视角偏差在训练中被良好容忍

Conclusion: 异构视觉反馈显著提高了四足机器人运动的鲁棒性和敏捷性。多视角感知框架为动态腿式运动提供了有效的解决方案

Abstract: Recent progress in legged locomotion has allowed highly dynamic and parkour-like behaviors for robots, similar to their biological counterparts. Yet, these methods mostly rely on egocentric (first-person) perception, limiting their performance, especially when the viewpoint of the robot is occluded. A promising solution would be to enhance the robot's environmental awareness by using complementary viewpoints, such as multiple actors exchanging perceptual information. Inspired by this idea, this work proposes a multi-view depth-based locomotion framework that combines egocentric and exocentric observations to provide richer environmental context during agile locomotion. Using a teacher-student distillation approach, the student policy learns to fuse proprioception with dual depth streams while remaining robust to real-world sensing imperfections. To further improve robustness, we introduce extensive domain randomization, including stochastic remote-camera dropouts and 3D positional perturbations that emulate aerial-ground cooperative sensing. Simulation results show that multi-viewpoints policies outperform single-viewpoint baseline in gap crossing, step descent, and other dynamic maneuvers, while maintaining stability when the exocentric camera is partially or completely unavailable. Additional experiments show that moderate viewpoint misalignment is well tolerated when incorporated during training. This study demonstrates that heterogeneous visual feedback improves robustness and agility in quadrupedal locomotion. Furthermore, to support reproducibility, the implementation accompanying this work is publicly available at https://anonymous.4open.science/r/multiview-parkour-6FB8

</details>


### [127] [CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance](https://arxiv.org/abs/2511.22773)
*Rui Heng Yang,Xuan Zhao,Leo Maxime Brunswic,Montgomery Alban,Mateo Clemente,Tongtong Cao,Jun Jin,Amir Rasouli*

Main category: cs.RO

TL;DR: CAPE提出了一种上下文感知的扩散策略框架，通过先验种子迭代引导精炼过程，在推理时扩展轨迹分布模式，以解决机器人模仿学习中数据获取成本高和碰撞避免任务泛化难的问题。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习中，扩散模型虽然能捕捉多模态轨迹，但需要大规模数据集才能达到最佳性能，而获取这些数据成本高昂。特别是在碰撞避免等挑战性任务中，泛化需要覆盖多种障碍物类型和空间配置，仅通过数据获取不切实际。

Method: CAPE框架通过上下文感知先验和引导在推理时扩展轨迹分布模式。采用先验种子迭代引导精炼过程：首先生成初始轨迹计划并执行短前缀轨迹，然后将剩余轨迹段扰动到中间噪声水平形成轨迹先验（保持任务意图），再通过上下文感知引导去噪迭代扩展模式支持，寻找更平滑、碰撞风险更低的轨迹。

Result: 在杂乱未见过的模拟和真实世界设置中评估CAPE，相比最先进方法分别实现了高达26%和80%的成功率提升，展示了在未见环境中更好的泛化能力。

Conclusion: CAPE通过上下文感知先验和迭代引导精炼，有效扩展了轨迹分布模式，能够在未见环境中采样无碰撞轨迹并保持目标一致性，解决了扩散模型在机器人模仿学习中数据依赖和泛化挑战的问题。

Abstract: In robotics, diffusion models can capture multi-modal trajectories from demonstrations, making them a transformative approach in imitation learning. However, achieving optimal performance following this regiment requires a large-scale dataset, which is costly to obtain, especially for challenging tasks, such as collision avoidance. In those tasks, generalization at test time demands coverage of many obstacles types and their spatial configurations, which are impractical to acquire purely via data. To remedy this problem, we propose Context-Aware diffusion policy via Proximal mode Expansion (CAPE), a framework that expands trajectory distribution modes with context-aware prior and guidance at inference via a novel prior-seeded iterative guided refinement procedure. The framework generates an initial trajectory plan and executes a short prefix trajectory, and then the remaining trajectory segment is perturbed to an intermediate noise level, forming a trajectory prior. Such a prior is context-aware and preserves task intent. Repeating the process with context-aware guided denoising iteratively expands mode support to allow finding smoother, less collision-prone trajectories. For collision avoidance, CAPE expands trajectory distribution modes with collision-aware context, enabling the sampling of collision-free trajectories in previously unseen environments while maintaining goal consistency. We evaluate CAPE on diverse manipulation tasks in cluttered unseen simulated and real-world settings and show up to 26% and 80% higher success rates respectively compared to SOTA methods, demonstrating better generalization to unseen environments.

</details>


### [128] [Improving Robotic Manipulation Robustness via NICE Scene Surgery](https://arxiv.org/abs/2511.22777)
*Sajjad Pakdamansavoji,Mozhgan Pourkeshavarz,Adam Sigal,Zhiyuan Li,Rui Heng Yang,Amir Rasouli*

Main category: cs.RO

TL;DR: NICE框架通过自然图像修复增强视觉多样性，减少模仿学习中的分布外差距，无需额外数据收集或模型训练，在杂乱场景中显著提升机器人操作性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中视觉干扰物会显著降低机器人操作策略的性能和安全性，现有方法需要额外数据收集或模拟器访问，限制了实际应用。

Method: 利用图像生成框架和大语言模型进行三种编辑操作：物体替换、重新风格化和移除干扰物，保持空间关系和动作标签一致性，增强视觉多样性。

Result: 在高度杂乱场景中，空间可供性预测准确率提升超过20%；在含不同数量干扰物的环境中，操作任务成功率平均提高11%；目标混淆降低6%，碰撞率减少7%。

Conclusion: NICE框架有效减少模仿学习的分布外差距，提升机器人操作的视觉鲁棒性和安全性，且无需额外数据收集或模型训练，可直接应用于现有数据集。

Abstract: Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.
  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.

</details>


### [129] [Distracted Robot: How Visual Clutter Undermine Robotic Manipulation](https://arxiv.org/abs/2511.22780)
*Amir Rasouli,Montgomery Alban,Sajjad Pakdamansavoji,Zhiyuan Li,Zhanguang Zhang,Aaron Wu,Xuan Zhao*

Main category: cs.RO

TL;DR: 提出了一种从心理物理学角度评估机器人操作策略在杂乱场景中性能的协议，使用统一的杂乱度量来系统构建评估场景，发现杂乱会显著降低VLA模型性能达34%，不同策略有独特弱点，微调不能完全解决杂乱影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法缺乏对杂乱场景的系统性分析，需要从心理物理学角度建立统一的杂乱度量标准，以全面评估机器人操作策略在真实复杂环境中的性能表现。

Method: 提出统一的杂乱度量方法，考虑环境因素、干扰物数量、特征和排列；在超真实模拟和真实世界中系统构建评估场景；对视觉-语言-动作模型进行广泛实验；分析杂乱度量与性能下降的关系。

Result: 场景杂乱显著降低策略性能达34%；不同VLA策略虽然平均性能相似，但有独特脆弱性和较低的成功场景一致性；杂乱度量是性能下降的有效指标；微调对杂乱影响的改善效果不均。

Conclusion: 需要更系统的评估方法来分析机器人操作策略在杂乱场景中的性能，当前VLA模型对杂乱敏感且存在独特弱点，需要针对性的改进方法。

Abstract: In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.

</details>


### [130] [Safe Autonomous Lane Changing: Planning with Dynamic Risk Fields and Time-Varying Convex Space Generation](https://arxiv.org/abs/2511.22829)
*Zhen Tian,Zhihao Lin*

Main category: cs.RO

TL;DR: 提出了一种用于自动驾驶换道等复杂场景的新型轨迹规划框架，通过将风险感知规划与保证碰撞避免集成到统一优化框架中，实现安全高效的轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹规划方法在复杂动态交通场景中难以同时保证安全性、效率和舒适性，特别是在密集交互环境中需要平衡风险规避与运动平滑性。

Method: 1. 构建动态风险场(DRF)捕捉静态和动态碰撞风险；2. 生成时变凸可行空间确保运动学可行性和安全要求；3. 将轨迹规划问题建模为有限时域最优控制问题，使用约束迭代线性二次调节器(iLQR)算法联合优化轨迹平滑性、控制努力和风险暴露。

Result: 在换道场景中实现更短的换道距离(28.59米)和时间(2.84秒)，保持平滑舒适的加速度模式；在密集环岛环境中相比APF、MPC和RRT基线方法，表现出更大的安全裕度、更低的急动度和更优的曲率平滑性。

Conclusion: 集成的DRF、凸可行空间和约束iLQR求解器为动态交互交通场景提供了安全、高效和舒适的轨迹生成平衡解决方案。

Abstract: This paper presents a novel trajectory planning pipeline for complex driving scenarios like autonomous lane changing, by integrating risk-aware planning with guaranteed collision avoidance into a unified optimization framework. We first construct a dynamic risk fields (DRF) that captures both the static and dynamic collision risks from surrounding vehicles. Then, we develop a rigorous strategy for generating time-varying convex feasible spaces that ensure kinematic feasibility and safety requirements. The trajectory planning problem is formulated as a finite-horizon optimal control problem and solved using a constrained iterative Linear Quadratic Regulator (iLQR) algorithm that jointly optimizes trajectory smoothness, control effort, and risk exposure while maintaining strict feasibility. Extensive simulations demonstrate that our method outperforms traditional approaches in terms of safety and efficiency, achieving collision-free trajectories with shorter lane-changing distances (28.59 m) and times (2.84 s) while maintaining smooth and comfortable acceleration patterns. In dense roundabout environments the planner further demonstrates robust adaptability, producing larger safety margins, lower jerk, and superior curvature smoothness compared with APF, MPC, and RRT based baselines. These results confirm that the integrated DRF with convex feasible space and constrained iLQR solver provides a balanced solution for safe, efficient, and comfortable trajectory generation in dynamic and interactive traffic scenarios.

</details>


### [131] [Threat-Aware UAV Dodging of Human-Thrown Projectiles with an RGB-D Camera](https://arxiv.org/abs/2511.22847)
*Yuying Zhang,Na Fan,Haowen Zheng,Junning Liang,Zongliang Pan,Qifeng Chen,Ximin Lyu*

Main category: cs.RO

TL;DR: 基于RGB-D相机和人体姿态估计的无人机实时躲避系统，通过预测攻击者动作和抛射物轨迹来躲避突然的抛射物攻击


<details>
  <summary>Details</summary>
Motivation: 执行运输和航拍等任务的无人机容易受到人类故意抛射物攻击，躲避这种突然快速抛射物需要超低延迟响应和敏捷机动能力

Method: 受棒球运动启发，利用RGB-D相机结合人体姿态估计与深度信息，预测攻击者运动轨迹和抛射物轨迹，并引入不确定性感知躲避策略

Result: 感知系统实现高预测精度，在有效距离和延迟方面优于基线；躲避策略处理时空不确定性确保无人机安全；真实世界实验证明系统在突发攻击下具有可靠躲避能力和出色鲁棒性

Conclusion: 提出的实时躲避系统能有效应对突然抛射物攻击，在各种场景下表现出色，为无人机安全防护提供了创新解决方案

Abstract: Uncrewed aerial vehicles (UAVs) performing tasks such as transportation and aerial photography are vulnerable to intentional projectile attacks from humans. Dodging such a sudden and fast projectile poses a significant challenge for UAVs, requiring ultra-low latency responses and agile maneuvers. Drawing inspiration from baseball, in which pitchers' body movements are analyzed to predict the ball's trajectory, we propose a novel real-time dodging system that leverages an RGB-D camera. Our approach integrates human pose estimation with depth information to predict the attacker's motion trajectory and the subsequent projectile trajectory. Additionally, we introduce an uncertainty-aware dodging strategy to enable the UAV to dodge incoming projectiles efficiently. Our perception system achieves high prediction accuracy and outperforms the baseline in effective distance and latency. The dodging strategy addresses temporal and spatial uncertainties to ensure UAV safety. Extensive real-world experiments demonstrate the framework's reliable dodging capabilities against sudden attacks and its outstanding robustness across diverse scenarios.

</details>


### [132] [MARVO: Marine-Adaptive Radiance-aware Visual Odometry](https://arxiv.org/abs/2511.22860)
*Sacchin Sundar,Atman Kikani,Aaliya Alam,Sumukh Shrote,A. Nayeemulla Khan,A. Shahina*

Main category: cs.RO

TL;DR: MARVO：一种融合水下图像形成建模、可微分匹配和强化学习优化的物理感知学习集成里程计框架，用于解决水下视觉定位的挑战。


<details>
  <summary>Details</summary>
Motivation: 水下视觉定位面临波长相关衰减、纹理贫乏和非高斯传感器噪声等挑战，传统方法难以应对水下环境的复杂物理特性。

Method: 1. 前端：扩展基于transformer的特征匹配器，加入物理感知辐射适配器补偿颜色通道衰减和对比度损失；2. 后端：将半密集匹配与惯性和压力测量结合在因子图中，使用GTSAM库实现关键帧视觉-惯性-气压估计器；3. 全局优化：引入基于强化学习的位姿图优化器，通过SE(2)上的最优回缩动作超越经典最小二乘求解器的局部最小值。

Result: 框架实现了实时全状态最大后验估计，在浑浊水下环境中产生几何一致的特征对应关系，并通过强化学习优化器改进全局轨迹。

Conclusion: MARVO通过融合物理建模、可微分匹配和强化学习优化，为水下视觉定位提供了一种鲁棒的解决方案，能够有效应对水下环境的独特挑战。

Abstract: Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise. We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization. At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity. These semi dense matches are combined with inertial and pressure measurements inside a factor-graph backend, where we formulate a keyframe-based visual-inertial-barometric estimator using GTSAM library. Each keyframe introduces (i) Pre-integrated IMU motion factors, (ii) MARVO-derived visual pose factors, and (iii) barometric depth priors, giving a full-state MAP estimate in real time. Lastly, we introduce a Reinforcement-Learningbased Pose-Graph Optimizer that refines global trajectories beyond local minima of classical least-squares solvers by learning optimal retraction actions on SE(2).

</details>


### [133] [SUPER-AD: Semantic Uncertainty-aware Planning for End-to-End Robust Autonomous Driving](https://arxiv.org/abs/2511.22865)
*Wonjeong Ryu,Seungjun Yu,Seokha Moon,Hojun Choi,Junsung Park,Jinkyu Kim,Hyunjung Shim*

Main category: cs.RO

TL;DR: 提出了一种基于摄像头的端到端自动驾驶框架，通过估计BEV空间中的偶然不确定性并融入规划，结合车道跟随正则化，实现鲁棒且可解释的轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统存在根本性缺陷：它们假设感知输出完全可靠，即使在模糊或观测不良的场景中也是如此，导致规划器缺乏明确的不确定性度量，限制了系统的安全性和可靠性。

Method: 1. 提出摄像头端到端框架，直接在BEV空间中估计偶然不确定性；2. 生成密集的、不确定性感知的可行驶性地图，捕捉语义结构和几何布局；3. 引入车道跟随正则化，编码车道结构和交通规范，稳定轨迹规划同时保持灵活性。

Result: 在NAVSIM基准测试中达到最先进性能，在具有挑战性的NAVHARD和NAVSAFE子集上取得显著提升，证明了偶然不确定性建模与驾驶先验结合能显著提高摄像头端到端自动驾驶的安全性和可靠性。

Conclusion: 通过将偶然不确定性建模与驾驶先验相结合，本文提出的方法显著推进了摄像头端到端自动驾驶的安全性和可靠性，为处理不确定性条件下的鲁棒规划提供了有效解决方案。

Abstract: End-to-End (E2E) planning has become a powerful paradigm for autonomous driving, yet current systems remain fundamentally uncertainty-blind. They assume perception outputs are fully reliable, even in ambiguous or poorly observed scenes, leaving the planner without an explicit measure of uncertainty. To address this limitation, we propose a camera-only E2E framework that estimates aleatoric uncertainty directly in BEV space and incorporates it into planning. Our method produces a dense, uncertainty-aware drivability map that captures both semantic structure and geometric layout at pixel-level resolution. To further promote safe and rule-compliant behavior, we introduce a lane-following regularization that encodes lane structure and traffic norms. This prior stabilizes trajectory planning under normal conditions while preserving the flexibility needed for maneuvers such as overtaking or lane changes. Together, these components enable robust and interpretable trajectory planning, even under challenging uncertainty conditions. Evaluated on the NAVSIM benchmark, our method achieves state-of-the-art performance, delivering substantial gains on both the challenging NAVHARD and NAVSAFE subsets. These results demonstrate that our principled aleatoric uncertainty modeling combined with driving priors significantly advances the safety and reliability of camera-only E2E autonomous driving.

</details>


### [134] [Seeing before Observable: Potential Risk Reasoning in Autonomous Driving via Vision Language Models](https://arxiv.org/abs/2511.22928)
*Jiaxin Liu,Xiangyu Yan,Liang Peng,Lei Yang,Lingjun Zhang,Yuechen Luo,Yueming Tao,Ashton Yu Xuan Tan,Mu Li,Lei Zhang,Ziqi Zhan,Sai Guo,Hong Wang,Jun Li*

Main category: cs.RO

TL;DR: 提出了PotentialRiskQA数据集和PR-Reasoner框架，用于自动驾驶车辆在风险尚未显现时通过语义推理识别潜在风险


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在识别潜在风险方面存在不足，现有数据集缺乏罕见复杂场景和因果推理链标注，导致系统无法通过细微前兆推断尚未显现的风险

Method: 创建PotentialRiskQA视觉语言数据集，包含结构化场景描述、语义前兆和推断风险结果；提出PR-Reasoner框架，基于视觉语言模型进行车载潜在风险推理

Result: 在PotentialRiskQA数据集上微调的PR-Reasoner在潜在风险推理任务上显著优于基线视觉语言模型

Conclusion: 提出的数据集和模型为开发具有更强预见性和主动安全能力的自动驾驶系统奠定了基础，推动更智能、更具韧性的自动驾驶发展

Abstract: Ensuring safety remains a key challenge for autonomous vehicles (AVs), especially in rare and complex scenarios. One critical but understudied aspect is the \textbf{potential risk} situations, where the risk is \textbf{not yet observable} but can be inferred from subtle precursors, such as anomalous behaviors or commonsense violations. Recognizing these precursors requires strong semantic understanding and reasoning capabilities, which are often absent in current AV systems due to the scarcity of such cases in existing driving or risk-centric datasets. Moreover, current autonomous driving accident datasets often lack annotations of the causal reasoning chains behind incidents, which are essential for identifying potential risks before they become observable. To address these gaps, we introduce PotentialRiskQA, a novel vision-language dataset designed for reasoning about potential risks prior to observation. Each sample is annotated with structured scene descriptions, semantic precursors, and inferred risk outcomes. Based on this dataset, we further propose PR-Reasoner, a vision-language-model-based framework tailored for onboard potential risk reasoning. Experimental results show that fine-tuning on PotentialRiskQA enables PR-Reasoner to significantly enhance its performance on the potential risk reasoning task compared to baseline VLMs. Together, our dataset and model provide a foundation for developing autonomous systems with improved foresight and proactive safety capabilities, moving toward more intelligent and resilient AVs.

</details>


### [135] [Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary](https://arxiv.org/abs/2511.22963)
*Zhirui Liu,Kaiyang Ji,Ke Yang,Jingyi Yu,Ye Shi,Jingya Wang*

Main category: cs.RO

TL;DR: Humanoid-LLA：基于大语言动作模型，将自由形式语言指令映射为类人机器人可执行的全身动作，实现语言泛化与物理可行性的平衡


<details>
  <summary>Details</summary>
Motivation: 类人机器人需要理解自由形式语言指令以实现人机交互、协作任务和通用具身智能，但现有方法在语言条件化全身控制方面存在局限，要么指令简单，要么牺牲动作多样性或物理合理性

Method: 提出Humanoid-LLA模型，包含三个核心组件：1)统一运动词汇表，将人类和类人运动基元对齐到共享离散空间；2)从特权策略蒸馏的词汇导向控制器，确保物理可行性；3)基于强化学习的物理感知微调阶段，使用动力学感知奖励增强鲁棒性和稳定性

Result: 在仿真和真实Unitree G1类人机器人上的广泛评估表明，Humanoid-LLA在保持高物理保真度的同时实现了强大的语言泛化能力，在动作自然性、稳定性和执行成功率方面优于现有语言条件化控制器

Conclusion: Humanoid-LLA成功解决了类人机器人语言条件化全身控制的挑战，实现了表达性语言指令到物理可执行动作的映射，为人机交互和通用具身智能提供了有效解决方案

Abstract: Enabling humanoid robots to follow free-form language commands is critical for seamless human-robot interaction, collaborative task execution, and general-purpose embodied intelligence. While recent advances have improved low-level humanoid locomotion and robot manipulation, language-conditioned whole-body control remains a significant challenge. Existing methods are often limited to simple instructions and sacrifice either motion diversity or physical plausibility. To address this, we introduce Humanoid-LLA, a Large Language Action Model that maps expressive language commands to physically executable whole-body actions for humanoid robots. Our approach integrates three core components: a unified motion vocabulary that aligns human and humanoid motion primitives into a shared discrete space; a vocabulary-directed controller distilled from a privileged policy to ensure physical feasibility; and a physics-informed fine-tuning stage using reinforcement learning with dynamics-aware rewards to enhance robustness and stability. Extensive evaluations in simulation and on a real-world Unitree G1 humanoid show that Humanoid-LLA delivers strong language generalization while maintaining high physical fidelity, outperforming existing language-conditioned controllers in motion naturalness, stability, and execution success rate.

</details>


### [136] [Analytical Inverse Kinematic Solution for "Moz1" NonSRS 7-DOF Robot arm with novel arm angle](https://arxiv.org/abs/2511.22996)
*Ke Chen*

Main category: cs.RO

TL;DR: 提出7自由度Moz1机械臂逆运动学问题的解析解，使用新颖的臂角概念，提供封闭形式解，解决算法奇异性问题，并能获得所有16个解。


<details>
  <summary>Details</summary>
Motivation: 传统SEW角度在某些情况下无法定义，存在算法奇异性问题，需要一种新的方法来解决7自由度机械臂的逆运动学问题，实现完全自运动并处理工作空间内的奇异性。

Method: 采用新颖的臂角表示法，为带有腕部偏移的7-DOF Moz1机械臂提供封闭形式的逆运动学解。该方法能解析地解决冗余度问题，处理传统SEW角度无法定义的情况。

Result: 该方法简单、快速且精确，能够为每个姿态提供完整的解空间（即所有16个解），解决了工作空间内的算法奇异性问题，实现了完全自运动。

Conclusion: 提出的基于新颖臂角的解析解方法有效解决了7自由度机械臂的逆运动学问题，克服了传统方法的局限性，为机械臂控制提供了可靠的理论基础。

Abstract: This paper presents an analytical solution to the inverse kinematic problem(IKP) for the seven degree-of-freedom (7-DOF) Moz1 Robot Arm with offsets on wrist. We provide closed-form solutions with the novel arm angle . it allow fully self-motion and solve the problem of algorithmic singularities within the workspace. It also provides information on how the redundancy is resolved in a new arm angle representation where traditional SEW angle faied to be defined and how singularities are handled. The solution is simple, fast and exact, providing full solution space (i.e. all 16 solutions) per pose.

</details>


### [137] [Adaptive Factor Graph-Based Tightly Coupled GNSS/IMU Fusion for Robust Positionin](https://arxiv.org/abs/2511.23017)
*Elham Ahmadi,Alireza Olama,Petri Välisuo,Heidi Kuusniemi*

Main category: cs.RO

TL;DR: 提出了一种基于因子图的鲁棒自适应GNSS/IMU融合框架，采用Barron损失函数处理非高斯噪声和异常值，在GNSS受限环境中显著提升定位精度


<details>
  <summary>Details</summary>
Motivation: GNSS受限环境（如城市峡谷）中的可靠定位是导航系统的关键挑战。传统的紧耦合GNSS/IMU融合虽然提高了鲁棒性，但仍容易受到非高斯噪声和异常值的影响，需要更强大的鲁棒性解决方案

Method: 提出基于因子图的鲁棒自适应融合框架，将GNSS伪距测量与IMU预积分因子直接集成，并引入Barron损失函数（一种通过单个可调参数统一多个M估计器的通用鲁棒损失函数），自适应降低不可靠GNSS测量的权重

Result: 在UrbanNav数据集上评估，相比标准因子图优化(FGO)减少定位误差达41%，相比扩展卡尔曼滤波器(EKF)基线在城市场景中提升更显著

Conclusion: Barron损失函数能有效增强GNSS/IMU导航在城市场景和信号受限环境中的鲁棒性，提出的自适应框架显著提高了定位系统的抗干扰能力

Abstract: Reliable positioning in GNSS-challenged environments remains a critical challenge for navigation systems. Tightly coupled GNSS/IMU fusion improves robustness but remains vulnerable to non-Gaussian noise and outliers. We present a robust and adaptive factor graph-based fusion framework that directly integrates GNSS pseudorange measurements with IMU preintegration factors and incorporates the Barron loss, a general robust loss function that unifies several m-estimators through a single tunable parameter. By adaptively down weighting unreliable GNSS measurements, our approach improves resilience positioning. The method is implemented in an extended GTSAM framework and evaluated on the UrbanNav dataset. The proposed solution reduces positioning errors by up to 41% relative to standard FGO, and achieves even larger improvements over extended Kalman filter (EKF) baselines in urban canyon environments. These results highlight the benefits of Barron loss in enhancing the resilience of GNSS/IMU-based navigation in urban and signal-compromised environments.

</details>


### [138] [DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management](https://arxiv.org/abs/2511.23030)
*Casimir Feldmann,Maximum Wilder-Smith,Vaishakh Patil,Michael Oechsle,Michael Niemeyer,Keisuke Tateno,Marco Hutter*

Main category: cs.RO

TL;DR: DiskChunGS是一个可扩展的3D高斯泼溅SLAM系统，通过外存方法将场景分割成空间块，仅将活跃区域保留在GPU内存中，从而克服了内存限制，实现了大规模环境的重建。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS SLAM系统受限于GPU内存容量，只能重建小规模环境，无法扩展到大规模场景。需要一种能够克服内存瓶颈的解决方案。

Method: 采用外存方法，将场景分割成空间块，仅将活跃区域保留在GPU内存中，不活跃区域存储在磁盘上。与现有SLAM框架无缝集成，支持姿态估计和闭环检测。

Result: 在室内场景（Replica, TUM-RGBD）、城市驾驶场景（KITTI）和资源受限的Nvidia Jetson平台上验证。成功完成所有11个KITTI序列而不会出现内存故障，同时实现了优越的视觉质量。

Conclusion: DiskChunGS通过算法创新克服了先前3DGS SLAM方法的内存限制，证明了可扩展的3DGS SLAM系统的可行性，能够实现大规模环境的全局一致重建。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.

</details>


### [139] [LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models](https://arxiv.org/abs/2511.23034)
*Zuolei Li,Xingyu Gao,Xiaofan Wang,Jianlong Fu*

Main category: cs.RO

TL;DR: 提出通用潜在动作学习框架，通过结合未来帧重建和动作序列预测，从大规模物体操作视频中学习可转移的潜在动作表示，实现机器人任务的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉重建目标而忽略物理先验，导致学习通用表示的性能不佳。需要从大规模物体操作视频中学习可转移的潜在动作，以增强下游机器人任务的泛化能力。

Method: 提出通用潜在动作学习框架，输入任务指令和多帧图像，同时优化未来帧重建和动作序列预测。将潜在动作分解为可学习的运动和场景token，区分机器人主动运动和环境变化。将学习到的潜在动作蒸馏到最新的VLA模型中。

Result: 在模拟环境（SIMPLER和LIBERO）和真实世界机器人设置中均取得强性能。仅需每个任务10条真实世界轨迹，就能成功完成所有五个挑战性任务，展示了强大的少样本迁移能力。

Conclusion: 通过结合视觉重建和动作预测，能够学习包含丰富物理先验的通用潜在动作表示，实现从大规模视频到下游机器人任务的无缝迁移，显著提升机器人操作的泛化能力。

Abstract: Learning transferable latent actions from large-scale object manipulation videos can significantly enhance generalization in downstream robotics tasks, as such representations are agnostic to different robot embodiments. Existing approaches primarily rely on visual reconstruction objectives while neglecting physical priors, leading to sub-optimal performance in learning universal representations. To address these challenges, we propose a Universal Latent Action Learning framework that takes task instructions and multiple frames as inputs, and optimizes both future frame reconstruction and action sequence prediction. Unlike prior works, incorporating action predictions (e.g., gripper or hand trajectories and orientations) allows the model to capture richer physical priors such as real-world distances and orientations, thereby enabling seamless transferability to downstream tasks. We further decompose the latent actions into learnable motion and scene tokens to distinguish the robot's active movements from environmental changes, thus filtering out irrelevant dynamics. By distilling the learned latent actions into the latest VLA models, we achieve strong performance across both simulated (SIMPLER and LIBERO) and real-world robot settings. Notably, with only 10 real-world trajectories per task collected on a Franka robot, our approach successfully completes all five challenging tasks, demonstrating strong few-shot transferability in robotic manipulation.

</details>


### [140] [Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications](https://arxiv.org/abs/2511.23143)
*Enrico Saccon,Davide De Martini,Matteo Saveriano,Edoardo Lamon,Luigi Palopoli,Marco Roveri*

Main category: cs.RO

TL;DR: 提出结合大语言模型、自动规划和形式验证的框架，从自然语言描述自动构建马尔可夫决策过程并生成最优策略


<details>
  <summary>Details</summary>
Motivation: 传统概率规划方法需要大量手动建模工作，限制了在机器人领域的可访问性和可扩展性。本文旨在通过结合语言模型和形式化方法，简化MDP的创建和使用过程。

Method: 1) 使用LLM从自然语言描述中提取结构化知识，构建Prolog知识库；2) 通过可达性分析自动构建MDP；3) 使用Storm模型检查器合成最优策略；4) 将策略导出为状态-动作表供执行

Result: 在三个人机交互场景中验证了框架的有效性，能够以最小的人工努力生成可执行策略，证明了该方法的可行性

Conclusion: 结合语言模型与形式化方法能够实现更易访问和可扩展的概率规划，为机器人领域提供了新的自动化解决方案

Abstract: We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.

</details>


### [141] [Obstruction reasoning for robotic grasping](https://arxiv.org/abs/2511.23186)
*Runyu Jiao,Matteo Bortolon,Francesco Giuliari,Alice Fasoli,Sergio Povoli,Guofeng Mei,Yiming Wang,Fabio Poiesi*

Main category: cs.RO

TL;DR: UNOGrasp是一个基于学习的视觉语言模型，专门用于机器人抓取中的障碍物推理和可访问性规划，通过多步推理和视觉线索增强，在合成和真实环境中显著提升了抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言具身推理模型在空间理解方面有所进展，但在障碍物推理和可访问性规划方面仍有限制。机器人要在杂乱环境中成功抓取目标物体，不仅需要视觉定位目标，还需要推理哪些障碍物需要先被清除。

Method: 提出了UNOGrasp模型，采用基于目标物体障碍路径的多步推理过程，通过障碍感知的视觉线索锚定每个推理步骤。结合监督学习和强化学习微调，使用可验证的推理奖励。同时构建了UNOBench大规模数据集，基于MetaGraspNetV2，包含超过10万条人工标注的障碍路径，包含障碍比例、接触点和自然语言指令。

Result: 广泛的实验和真实机器人评估表明，UNOGrasp在合成和真实世界环境中显著改善了障碍物推理和抓取成功率，超越了通用模型和专有替代方案。

Conclusion: UNOGrasp通过专门设计的障碍物推理能力和大规模数据集训练，有效解决了杂乱环境中机器人抓取的障碍物清除和可访问性规划问题，为具身智能在复杂环境中的操作提供了有力解决方案。

Abstract: Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.

</details>


### [142] [Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging](https://arxiv.org/abs/2511.23193)
*Yuchen Shi,Huaxin Pei,Yi Zhang,Danya Yao*

Main category: cs.RO

TL;DR: 提出一种针对自动驾驶车辆协同驾驶的多智能体强化学习方法，通过对抗性故障注入和自诊断机制增强系统对观测故障的容错能力。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在协同驾驶中应用受限，主要因为对观测故障的容错能力不足。观测故障（感知数据扰动）会严重损害基于MARL的驾驶系统性能，需要解决对抗性扰动生成和故障缓解两大挑战。

Method: 提出包含两个关键智能体的容错MARL方法：1）对抗性故障注入智能体，协同训练生成扰动以挑战和强化车辆策略；2）容错车辆智能体，具备自诊断能力，利用车辆状态序列的时空相关性检测故障并重建可信观测。

Result: 在模拟高速公路汇入场景中，该方法显著优于基线MARL方法，在各种观测故障模式下实现了接近无故障水平的安全性和效率。

Conclusion: 通过结合对抗性故障注入和自诊断机制，提出的容错MARL方法能有效提升协同驾驶系统对观测故障的鲁棒性，为实际应用提供了重要保障。

Abstract: Multi-Agent Reinforcement Learning (MARL) holds significant promise for enabling cooperative driving among Connected and Automated Vehicles (CAVs). However, its practical application is hindered by a critical limitation, i.e., insufficient fault tolerance against observational faults. Such faults, which appear as perturbations in the vehicles' perceived data, can substantially compromise the performance of MARL-based driving systems. Addressing this problem presents two primary challenges. One is to generate adversarial perturbations that effectively stress the policy during training, and the other is to equip vehicles with the capability to mitigate the impact of corrupted observations. To overcome the challenges, we propose a fault-tolerant MARL method for cooperative on-ramp vehicles incorporating two key agents. First, an adversarial fault injection agent is co-trained to generate perturbations that actively challenge and harden the vehicle policies. Second, we design a novel fault-tolerant vehicle agent equipped with a self-diagnosis capability, which leverages the inherent spatio-temporal correlations in vehicle state sequences to detect faults and reconstruct credible observations, thereby shielding the policy from misleading inputs. Experiments in a simulated highway merging scenario demonstrate that our method significantly outperforms baseline MARL approaches, achieving near-fault-free levels of safety and efficiency under various observation fault patterns.

</details>


### [143] [Field-programmable dynamics in a soft magnetic actuator enabling true random number generation and reservoir computing](https://arxiv.org/abs/2511.23215)
*Eduardo Sergio Oliveros-Mata,Oleksandr V. Pylypovskyi,Eleonora Raimondo,Rico Illing,Yevhen Zabila,Lin Guo,Guannan Mu,Mónica Navarro López,Xu Wang,Georgios Tzortzinis,Angelos Filippatos,Gilbert Santiago Cañón Bermúdez,Francesca Garescì,Giovanni Finocchio,Denys Makarov*

Main category: cs.RO

TL;DR: 该论文展示了复杂动力学在软体机器人中的优势，通过设计可调动态磁致动器，实现了真随机数生成、随机计算和时间序列预测等新功能。


<details>
  <summary>Details</summary>
Motivation: 传统机电系统通常避免复杂和混沌动力学，担心磨损和可控性问题。但作者认为复杂动力学在软体机器人中可能特别有利，能提供传统驱动方法难以实现的新功能。

Method: 设计并实现了具有可调动态机制的弹性磁致动器，能够在数万次循环中无疲劳运行。利用这些致动器进行实验验证。

Result: 成功展示了致动器在真随机数生成和随机计算中的应用，验证了软体机器人作为物理储层执行Mackey-Glass时间序列预测的能力。还展示了仿生眨眼和随机化语音调制等应用。

Conclusion: 探索软体机器人中的复杂动力学将扩展其在软计算、人机交互和协作机器人等领域的应用场景，为软体机器人系统开辟新的功能可能性。

Abstract: Complex and even chaotic dynamics, though prevalent in many natural and engineered systems, has been largely avoided in the design of electromechanical systems due to concerns about wear and controlability. Here, we demonstrate that complex dynamics might be particularly advantageous in soft robotics, offering new functionalities beyond motion not easily achievable with traditional actuation methods. We designed and realized resilient magnetic soft actuators capable of operating in a tunable dynamic regime for tens of thousands cycles without fatigue. We experimentally demonstrated the application of these actuators for true random number generation and stochastic computing. {W}e validate soft robots as physical reservoirs capable of performing Mackey--Glass time series prediction. These findings show that exploring the complex dynamics in soft robotics would extend the application scenarios in soft computing, human-robot interaction and collaborative robots as we demonstrate with biomimetic blinking and randomized voice modulation.

</details>


### [144] [Incorporating Ephemeral Traffic Waves in A Data-Driven Framework for Microsimulation in CARLA](https://arxiv.org/abs/2511.23236)
*Alex Richardson,Azhar Hasan,Gabor Karsai,Jonathan Sprinkle*

Main category: cs.RO

TL;DR: 提出基于CARLA的数据驱动交通微观仿真框架，利用I-24 MOTION真实时空数据重建交通波动态，通过共仿真模块注入真实交通信息，实现感知真实的边界驱动仿真。


<details>
  <summary>Details</summary>
Motivation: 传统微观仿真器校准难以重现大规模交通波等短暂现象，需要新的方法来利用高保真真实世界数据，实现更准确的交通动态模拟。

Method: 1) 在CARLA中自动生成1英里高速公路段对应I-24；2) 使用I-24数据驱动共仿真模块向仿真注入交通信息；3) 以经验数据中的自车为中心，在纵向范围内自动生成"可见"交通；4) 超出范围使用上游和下游的幽灵单元进行边界控制。

Result: 仿真产生的涌现行为与真实交通高度一致，能够在低拥堵和高拥堵场景中模拟波的形成和消散，为交通控制策略、感知驱动自主性和波缓解解决方案评估提供新框架。

Conclusion: 该工作将微观建模与物理实验数据相结合，首次在CARLA中实现了感知真实、边界驱动的经验交通波现象仿真，为交通研究提供了创新的共仿真框架。

Abstract: This paper introduces a data-driven traffic microsimulation framework in CARLA that reconstructs real-world wave dynamics using high-fidelity time-space data from the I-24 MOTION testbed. Calibration of road networks in microsimulators to reproduce ephemeral phenomena such as traffic waves for large-scale simulation is a process that is fraught with challenges. This work reconsiders the existence of the traffic state data as boundary conditions on an ego vehicle moving through previously recorded traffic data, rather than reproducing those traffic phenomena in a calibrated microsim. Our approach is to autogenerate a 1 mile highway segment corresponding to I-24, and use the I-24 data to power a cosimulation module that injects traffic information into the simulation. The CARLA and cosimulation simulations are centered around an ego vehicle sampled from the empirical data, with autogeneration of "visible" traffic within the longitudinal range of the ego vehicle. Boundary control beyond these visible ranges is achieved using ghost cells behind (upstream) and ahead (downstream) of the ego vehicle. Unlike prior simulation work that focuses on local car-following behavior or abstract geometries, our framework targets full time-space diagram fidelity as the validation objective. Leveraging CARLA's rich sensor suite and configurable vehicle dynamics, we simulate wave formation and dissipation in both low-congestion and high-congestion scenarios for qualitative analysis. The resulting emergent behavior closely mirrors that of real traffic, providing a novel cosimulation framework for evaluating traffic control strategies, perception-driven autonomy, and future deployment of wave mitigation solutions. Our work bridges microscopic modeling with physical experimental data, enabling the first perceptually realistic, boundary-driven simulation of empirical traffic wave phenomena in CARLA.

</details>


### [145] [SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot](https://arxiv.org/abs/2511.23300)
*Yara Mahmoud,Jeffrin Sam,Nguyen Khang,Marcelino Fernando,Issatay Tokmurziyev,Miguel Altamirano Cabrera,Muhammad Haris Khan,Artem Lykov,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: SafeHumanoid：基于视觉语言模型和检索增强生成的仿人机器人阻抗控制框架，通过语义理解实现场景感知的安全人机交互


<details>
  <summary>Details</summary>
Motivation: 安全可信的人机交互需要机器人不仅能完成任务，还能根据场景上下文和人类接近程度调节阻抗和速度。现有方法缺乏语义理解能力，难以实现上下文感知的安全控制。

Method: 提出SafeHumanoid框架：1) 使用第一人称视角图像输入；2) 通过结构化VLM提示处理图像；3) 嵌入并匹配验证场景数据库；4) 通过逆运动学映射到关节级阻抗命令；5) 结合RAG技术调度阻抗和速度参数。

Result: 在桌面操作任务（擦拭、物体交接、液体倾倒）中，系统能根据有无人类在场自适应调整刚度、阻尼和速度配置，保持任务成功率的同时提高安全性。当前推理延迟约1.4秒，限制了高动态环境的响应性。

Conclusion: 语义接地的阻抗控制是实现更安全、符合标准的仿人机器人协作的可行路径。虽然当前延迟限制了高动态场景的响应，但SafeHumanoid展示了语义理解在安全人机交互中的潜力。

Abstract: Safe and trustworthy Human Robot Interaction (HRI) requires robots not only to complete tasks but also to regulate impedance and speed according to scene context and human proximity. We present SafeHumanoid, an egocentric vision pipeline that links Vision Language Models (VLMs) with Retrieval-Augmented Generation (RAG) to schedule impedance and velocity parameters for a humanoid robot. Egocentric frames are processed by a structured VLM prompt, embedded and matched against a curated database of validated scenarios, and mapped to joint-level impedance commands via inverse kinematics. We evaluate the system on tabletop manipulation tasks with and without human presence, including wiping, object handovers, and liquid pouring. The results show that the pipeline adapts stiffness, damping, and speed profiles in a context-aware manner, maintaining task success while improving safety. Although current inference latency (up to 1.4 s) limits responsiveness in highly dynamic settings, SafeHumanoid demonstrates that semantic grounding of impedance control is a viable path toward safer, standard-compliant humanoid collaboration.

</details>


### [146] [Design, modelling and experimental validation of bipenniform shape memory alloy-based linear actuator integrable with hydraulic stroke amplification mechanism](https://arxiv.org/abs/2511.23372)
*Kanhaiya Lal Chaurasiya,Ruchira Kumar Pradhan,Yashaswi Sinha,Shivam Gupta,Ujjain Kumar Bidila,Digambar Killedar,Kapil Das Sahu,Bishakh Bhattacharya*

Main category: cs.RO

TL;DR: 开发了一种基于形状记忆合金（SMA）和双羽状结构生物启发的线性致动器，相比传统步进电机致动器，重量减少67%，部件减少80%，成本降低32%，节能19%，产生257N致动力。


<details>
  <summary>Details</summary>
Motivation: 工业对替代传统电磁致动器的需求日益增长，传统系统存在效率有限、体积庞大、设计复杂（内置齿轮机构）、生产和摊销成本高等问题。将生物启发设计原则融入线性致动器可推动下一代自适应、高能效的智能材料致动系统发展。

Method: 结合双羽状结构（在给定生理区域产生高力）和形状记忆合金（高功率重量比）的优势，开发了多层双羽状配置的SMA致动器数学模型，并通过实验验证。采用设计失效模式与影响分析进行失效缓解策略，并实验评估性能。

Result: 与工业开发的步进电机致动器相比，该致动器在15V输入电压下产生257N致动力，满足操作要求。重量减少67%，部件减少80%，成本降低32%，节能19%，且外形尺寸相似，便于与阻尼器和百叶窗组装部署。

Conclusion: 该研究提出了一种基于SMA线圈的先进致动器设计，适用于高力-高行程应用。这种生物启发的SMA线性致动器可应用于建筑自动化控制、空间机器人轻量化致动系统和医疗假体等领域。

Abstract: The increasing industrial demand for alternative actuators over conventional electromagnetism-based systems having limited efficiency, bulky size, complex design due to in-built gear-train mechanisms, and high production and amortization costs necessitates the innovation in new actuator development. Integrating bio-inspired design principles into linear actuators could bring forth the next generation of adaptive and energy efficient smart material-based actuation systems. The present study amalgamates the advantages of bipenniform architecture, which generates high force in the given physiological region and a high power-to-weight ratio of shape memory alloy (SMA), into a novel bio-inspired SMA-based linear actuator. A mathematical model of a multi-layered bipenniform configuration-based SMA actuator was developed and validated experimentally. The current research also caters to the incorporation of failure mitigation strategies using design failure mode and effects analysis along with the experimental assessment of the performance of the developed actuator. The system has been benchmarked against an industry-developed stepper motor-driven actuator. It has shown promising results generating an actuation force of 257 N with 15 V input voltage, meeting the acceptable range for actuation operation. It further exhibits about 67% reduction in the weight of the drive mechanism, with 80% lesser component, 32% cost reduction, and 19% energy savings and similar envelope dimensions for assembly compatibility with dampers and louvers for easy onsite deployment. The study introduces SMA coil-based actuator as an advanced design that can be deployed for high force-high stroke applications. The bio-inspired SMA-based linear actuator has applications ranging from building automation controls to lightweight actuation systems for space robotics and medical prosthesis.

</details>


### [147] [From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products](https://arxiv.org/abs/2511.23407)
*Jan Baumgärtner,Malte Hansjosten,David Hald,Adrian Hauptmannl,Alexander Puchta,Jürgen Fleischer*

Main category: cs.RO

TL;DR: 提出将机器人拆解规划建模为部分可观测马尔可夫决策过程(POMDP)，以处理废旧产品的不确定性，并通过强化学习框架实现自适应拆解


<details>
  <summary>Details</summary>
Motivation: 支持循环经济需要机器人系统能够拆解废旧产品，但现有方法假设产品模型确定且完全可观测，而实际废旧产品常因磨损、腐蚀或未记录的维修而偏离原始设计，存在不确定性

Method: 将拆解规划建模为POMDP，隐藏变量表示不确定的结构或物理属性；提出任务与运动规划框架，从CAD数据、机器人能力和检测结果自动生成POMDP模型；采用强化学习方法近似求解，结合贝叶斯滤波器在执行过程中持续更新对潜在废旧状态的信念

Result: 在两种机器人系统上测试三种产品，证明该概率规划框架在平均拆解时间和方差方面优于确定性基线方法，能够跨不同机器人设置泛化，并成功适应CAD模型的偏差（如缺失或卡住零件）

Conclusion: 将拆解规划建模为POMDP并采用概率规划方法，能够有效处理废旧产品的不确定性，提高机器人拆解的鲁棒性和适应性，为循环经济中的自动化拆解提供了可行方案

Abstract: To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal. Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs. We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state. We present a mathematical formulation of disassembly as a POMDP, in which hidden variables represent uncertain structural or physical properties. Building on this formulation, we propose a task and motion planning framework that automatically derives specific POMDP models from CAD data, robot capabilities, and inspection results. To obtain tractable policies, we approximate this formulation with a reinforcement-learning approach that operates on stochastic action outcomes informed by inspection priors, while a Bayesian filter continuously maintains beliefs over latent EOL conditions during execution. Using three products on two robotic systems, we demonstrate that this probabilistic planning framework outperforms deterministic baselines in terms of average disassembly time and variance, generalizes across different robot setups, and successfully adapts to deviations from the CAD model, such as missing or stuck parts.

</details>
