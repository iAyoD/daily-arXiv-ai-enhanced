{"id": "2509.12367", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12367", "abs": "https://arxiv.org/abs/2509.12367", "authors": ["Daniel Lindmark", "Jonas Andersson", "Kenneth Bodin", "Tora Bodin", "Hugo Börjesson", "Fredrik Nordfeldth", "Martin Servin"], "title": "An integrated process for design and control of lunar robotics using AI and simulation", "comment": "14 pages, 6 figures", "summary": "We envision an integrated process for developing lunar construction\nequipment, where physical design and control are explored in parallel. In this\npaper, we describe a technical framework that supports this process. It relies\non OpenPLX, a readable/writable declarative language that links CAD-models and\nautonomous systems to high-fidelity, real-time 3D simulations of contacting\nmultibody dynamics, machine regolith interaction forces, and non-ideal sensors.\nTo demonstrate its capabilities, we present two case studies, including an\nautonomous lunar rover that combines a vision-language model for navigation\nwith a reinforcement learning-based control policy for locomotion."}
{"id": "2509.12379", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12379", "abs": "https://arxiv.org/abs/2509.12379", "authors": ["Divyam Goel", "Yufei Wang", "Tiancheng Wu", "Guixiu Qiao", "Pavel Piliptchak", "David Held", "Zackory Erickson"], "title": "Geometric Red-Teaming for Robotic Manipulation", "comment": "Accepted at the 9th Annual Conference on Robot Learning (CoRL 2025,\n  Oral)", "summary": "Standard evaluation protocols in robotic manipulation typically assess policy\nperformance over curated, in-distribution test sets, offering limited insight\ninto how systems fail under plausible variation. We introduce Geometric\nRed-Teaming (GRT), a red-teaming framework that probes robustness through\nobject-centric geometric perturbations, automatically generating CrashShapes --\nstructurally valid, user-constrained mesh deformations that trigger\ncatastrophic failures in pre-trained manipulation policies. The method\nintegrates a Jacobian field-based deformation model with a gradient-free,\nsimulator-in-the-loop optimization strategy. Across insertion, articulation,\nand grasping tasks, GRT consistently discovers deformations that collapse\npolicy performance, revealing brittle failure modes missed by static\nbenchmarks. By combining task-level policy rollouts with constraint-aware shape\nexploration, we aim to build a general purpose framework for structured,\nobject-centric robustness evaluation in robotic manipulation. We additionally\nshow that fine-tuning on individual CrashShapes, a process we refer to as\nblue-teaming, improves task success by up to 60 percentage points on those\nshapes, while preserving performance on the original object, demonstrating the\nutility of red-teamed geometries for targeted policy refinement. Finally, we\nvalidate both red-teaming and blue-teaming results with a real robotic arm,\nobserving that simulated CrashShapes reduce task success from 90% to as low as\n22.5%, and that blue-teaming recovers performance to up to 90% on the\ncorresponding real-world geometry -- closely matching simulation outcomes.\nVideos and code can be found on our project website:\nhttps://georedteam.github.io/ ."}
{"id": "2509.12390", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12390", "abs": "https://arxiv.org/abs/2509.12390", "authors": ["Evangelos Psomiadis", "Panagiotis Tsiotras"], "title": "Distributed Event-Triggered Distance-Based Formation Control for Multi-Agent Systems", "comment": "8 pages, 7 figures", "summary": "This paper addresses the problem of collaborative formation control for\nmulti-agent systems with limited resources. We consider a team of robots tasked\nwith achieving a desired formation from arbitrary initial configurations. To\nreduce unnecessary control updates and conserve resources, we propose a\ndistributed event-triggered formation controller that relies on inter-agent\ndistance measurements. Control updates are triggered only when the measurement\nerror exceeds a predefined threshold, ensuring system stability. The proposed\ncontroller is validated through extensive simulations and real-world\nexperiments involving different formations, communication topologies,\nscalability tests, and variations in design parameters, while also being\ncompared against periodic triggering strategies. Results demonstrate that the\nevent-triggered approach significantly reduces control efforts while preserving\nformation performance."}
{"id": "2509.12398", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12398", "abs": "https://arxiv.org/abs/2509.12398", "authors": ["Michael Lorenz", "Bertram Taetz", "Gabriele Bleser-Taetz", "Didier Stricker"], "title": "MinJointTracker: Real-time inertial kinematic chain tracking with joint position estimation and minimal state size", "comment": "10 pages, 2 figures", "summary": "Inertial motion capture is a promising approach for capturing motion outside\nthe laboratory. However, as one major drawback, most of the current methods\nrequire different quantities to be calibrated or computed offline as part of\nthe setup process, such as segment lengths, relative orientations between\ninertial measurement units (IMUs) and segment coordinate frames (IMU-to-segment\ncalibrations) or the joint positions in the IMU frames. This renders the setup\nprocess inconvenient. This work contributes to real-time capable\ncalibration-free inertial tracking of a kinematic chain, i.e. simultaneous\nrecursive Bayesian estimation of global IMU angular kinematics and joint\npositions in the IMU frames, with a minimal state size. Experimental results on\nsimulated IMU data from a three-link kinematic chain (manipulator study) as\nwell as re-simulated IMU data from healthy humans walking (lower body study)\nshow that the calibration-free and lightweight algorithm provides not only\ndrift-free relative but also drift-free absolute orientation estimates with a\nglobal heading reference for only one IMU as well as robust and fast\nconvergence of joint position estimates in the different movement scenarios."}
{"id": "2509.12444", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12444", "abs": "https://arxiv.org/abs/2509.12444", "authors": ["Weiting Feng", "Kyle L. Walker", "Yunjie Yang", "Francesco Giorgio-Serchi"], "title": "Computing forward statics from tendon-length in flexible-joint hyper-redundant manipulators", "comment": "To be presented at IROS 2025, Hangzhou, China", "summary": "Hyper-redundant tendon-driven manipulators offer greater flexibility and\ncompliance over traditional manipulators. A common way of controlling such\nmanipulators relies on adjusting tendon lengths, which is an accessible control\nparameter. This approach works well when the kinematic configuration is\nrepresentative of the real operational conditions. However, when dealing with\nmanipulators of larger size subject to gravity, it becomes necessary to solve a\nstatic force problem, using tendon force as the input and employing a mapping\nfrom the configuration space to retrieve tendon length. Alternatively,\nmeasurements of the manipulator posture can be used to iteratively adjust\ntendon lengths to achieve a desired posture. Hence, either tension measurement\nor state estimation of the manipulator are required, both of which are not\nalways accurately available. Here, we propose a solution by reconciling cables\ntension and length as the input for the solution of the system forward statics.\nWe develop a screw-based formulation for a tendon-driven, multi-segment,\nhyper-redundant manipulator with elastic joints and introduce a forward statics\niterative solution method that equivalently makes use of either tendon length\nor tension as the input. This strategy is experimentally validated using a\ntraditional tension input first, subsequently showing the efficacy of the\nmethod when exclusively tendon lengths are used. The results confirm the\npossibility to perform open-loop control in static conditions using a kinematic\ninput only, thus bypassing some of the practical problems with tension\nmeasurement and state estimation of hyper-redundant systems."}
{"id": "2509.12458", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12458", "abs": "https://arxiv.org/abs/2509.12458", "authors": ["Àlmos Veres-Vitàlyos", "Genis Castillo Gomez-Raya", "Filip Lemic", "Daniel Johannes Bugelnig", "Bernhard Rinner", "Sergi Abadal", "Xavier Costa-Pérez"], "title": "Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles", "comment": "13 pages, 16 figures, 3 tables, 45 references", "summary": "Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for\nnavigating indoor and hard-to-reach areas, yet their significant constraints in\npayload and autonomy have largely prevented their use for complex tasks like\nhigh-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we\nintroduce a novel system architecture that enables fully autonomous,\nhigh-fidelity 3D scanning of static objects using UAVs weighing under 100\ngrams. Our core innovation lies in a dual-reconstruction pipeline that creates\na real-time feedback loop between data capture and flight control. A\nnear-real-time (near-RT) process uses Structure from Motion (SfM) to generate\nan instantaneous pointcloud of the object. The system analyzes the model\nquality on the fly and dynamically adapts the UAV's trajectory to intelligently\ncapture new images of poorly covered areas. This ensures comprehensive data\nacquisition. For the final, detailed output, a non-real-time (non-RT) pipeline\nemploys a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR)\napproach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB)\nlocation data to achieve superior accuracy. We implemented and validated this\narchitecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both\nsingle- and multi-UAV configurations, conclusively show that dynamic trajectory\nadaptation consistently improves reconstruction quality over static flight\npaths. This work demonstrates a scalable and autonomous solution that unlocks\nthe potential of miniaturized UAVs for fine-grained 3D reconstruction in\nconstrained environments, a capability previously limited to much larger\nplatforms."}
{"id": "2509.12468", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12468", "abs": "https://arxiv.org/abs/2509.12468", "authors": ["Shipeng Liu", "Meghana Sagare", "Shubham Patil", "Feifei Qian"], "title": "Bio-inspired tail oscillation enables robot fast crawling on deformable granular terrains", "comment": null, "summary": "Deformable substrates such as sand and mud present significant challenges for\nterrestrial robots due to complex robot-terrain interactions. Inspired by\nmudskippers, amphibious animals that naturally adjust their tail morphology and\nmovement jointly to navigate such environments, we investigate how tail design\nand control can jointly enhance flipper-driven locomotion on granular media.\nUsing a bio-inspired robot modeled after the mudskipper, we experimentally\ncompared locomotion performance between idle and actively oscillating tail\nconfigurations. Tail oscillation increased robot speed by 67% and reduced body\ndrag by 46%. Shear force measurements revealed that this improvement was\nenabled by tail oscillation fluidizing the substrate, thereby reducing\nresistance. Additionally, tail morphology strongly influenced the oscillation\nstrategy: designs with larger horizontal surface areas leveraged the\noscillation-reduced shear resistance more effectively by limiting insertion\ndepth. Based on these findings, we present a design principle to inform tail\naction selection based on substrate strength and tail morphology. Our results\noffer new insights into tail design and control for improving robot locomotion\non deformable substrates, with implications for agricultural robotics, search\nand rescue, and environmental exploration."}
{"id": "2509.12507", "categories": ["cs.RO", "cs.HC", "cs.LG", "68T07, 68T40", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.12507", "abs": "https://arxiv.org/abs/2509.12507", "authors": ["Anna Deichler", "Siyang Wang", "Simon Alexanderson", "Jonas Beskow"], "title": "Learning to Generate Pointing Gestures in Situated Embodied Conversational Agents", "comment": "DOI: 10.3389/frobt.2023.1110534. This is the author's LaTeX version", "summary": "One of the main goals of robotics and intelligent agent research is to enable\nnatural communication with humans in physically situated settings. While recent\nwork has focused on verbal modes such as language and speech, non-verbal\ncommunication is crucial for flexible interaction. We present a framework for\ngenerating pointing gestures in embodied agents by combining imitation and\nreinforcement learning. Using a small motion capture dataset, our method learns\na motor control policy that produces physically valid, naturalistic gestures\nwith high referential accuracy. We evaluate the approach against supervised\nlearning and retrieval baselines in both objective metrics and a virtual\nreality referential game with human users. Results show that our system\nachieves higher naturalness and accuracy than state-of-the-art supervised\nmodels, highlighting the promise of imitation-RL for communicative gesture\ngeneration and its potential application to robots."}
{"id": "2509.12516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12516", "abs": "https://arxiv.org/abs/2509.12516", "authors": ["William Ward", "Sarah Etter", "Jesse Quattrociocchi", "Christian Ellis", "Adam J. Thorpe", "Ufuk Topcu"], "title": "Zero to Autonomy in Real-Time: Online Adaptation of Dynamics in Unstructured Environments", "comment": "Submitted to ICRA 2026", "summary": "Autonomous robots must go from zero prior knowledge to safe control within\nseconds to operate in unstructured environments. Abrupt terrain changes, such\nas a sudden transition to ice, create dynamics shifts that can destabilize\nplanners unless the model adapts in real-time. We present a method for online\nadaptation that combines function encoders with recursive least squares,\ntreating the function encoder coefficients as latent states updated from\nstreaming odometry. This yields constant-time coefficient estimation without\ngradient-based inner-loop updates, enabling adaptation from only a few seconds\nof data. We evaluate our approach on a Van der Pol system to highlight\nalgorithmic behavior, in a Unity simulator for high-fidelity off-road\nnavigation, and on a Clearpath Jackal robot, including on a challenging terrain\nat a local ice rink. Across these settings, our method improves model accuracy\nand downstream planning, reducing collisions compared to static and\nmeta-learning baselines."}
{"id": "2509.12531", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "68T07, 68T40 (Primary) 93C85, 62L20 (Secondary)", "I.2.6; I.2.9; I.4.8; F.2.2"], "pdf": "https://arxiv.org/pdf/2509.12531", "abs": "https://arxiv.org/abs/2509.12531", "authors": ["Scott Jones", "Liyou Zhou", "Sebastian W. Pattinson"], "title": "Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning", "comment": null, "summary": "In visuomotor policy learning, the control policy for the robotic agent is\nderived directly from visual inputs. The typical approach, where a policy and\nvision encoder are trained jointly from scratch, generalizes poorly to novel\nvisual scene changes. Using pre-trained vision models (PVMs) to inform a policy\nnetwork improves robustness in model-free reinforcement learning (MFRL). Recent\ndevelopments in Model-based reinforcement learning (MBRL) suggest that MBRL is\nmore sample-efficient than MFRL. However, counterintuitively, existing work has\nfound PVMs to be ineffective in MBRL. Here, we investigate PVM's effectiveness\nin MBRL, specifically on generalization under visual domain shifts. We show\nthat, in scenarios with severe shifts, PVMs perform much better than a baseline\nmodel trained from scratch. We further investigate the effects of varying\nlevels of fine-tuning of PVMs. Our results show that partial fine-tuning can\nmaintain the highest average task performance under the most extreme\ndistribution shifts. Our results demonstrate that PVMs are highly successful in\npromoting robustness in visual policy learning, providing compelling evidence\nfor their wider adoption in model-based robotic learning applications."}
{"id": "2509.12562", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12562", "abs": "https://arxiv.org/abs/2509.12562", "authors": ["Zhefei Gong", "Shangke Lyu", "Pengxiang Ding", "Wei Xiao", "Donglin Wang"], "title": "Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling", "comment": null, "summary": "Imitation learning (IL) enables efficient skill acquisition from\ndemonstrations but often struggles with long-horizon tasks and high-precision\ncontrol due to compounding errors. Residual policy learning offers a promising,\nmodel-agnostic solution by refining a base policy through closed-loop\ncorrections. However, existing approaches primarily focus on local corrections\nto the base policy, lacking a global understanding of state evolution, which\nlimits robustness and generalization to unseen scenarios. To address this, we\npropose incorporating global dynamics modeling to guide residual policy\nupdates. Specifically, we leverage Koopman operator theory to impose linear\ntime-invariant structure in a learned latent space, enabling reliable state\ntransitions and improved extrapolation for long-horizon prediction and unseen\nenvironments. We introduce KORR (Koopman-guided Online Residual Refinement), a\nsimple yet effective framework that conditions residual corrections on\nKoopman-predicted latent states, enabling globally informed and stable action\nrefinement. We evaluate KORR on long-horizon, fine-grained robotic furniture\nassembly tasks under various perturbations. Results demonstrate consistent\ngains in performance, robustness, and generalization over strong baselines. Our\nfindings further highlight the potential of Koopman-based modeling to bridge\nmodern learning methods with classical control theory."}
{"id": "2509.12594", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12594", "abs": "https://arxiv.org/abs/2509.12594", "authors": ["Titong Jiang", "Xuefeng Jiang", "Yuan Ma", "Xin Wen", "Bailin Li", "Kun Zhan", "Peng Jia", "Yahui Liu", "Sheng Sun", "Xianpeng Lang"], "title": "The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning", "comment": "Under review. Project site:\n  https://liauto-research.github.io/LightVLA", "summary": "We present LightVLA, a simple yet effective differentiable token pruning\nframework for vision-language-action (VLA) models. While VLA models have shown\nimpressive capability in executing real-world robotic tasks, their deployment\non resource-constrained platforms is often bottlenecked by the heavy\nattention-based computation over large sets of visual tokens. LightVLA\naddresses this challenge through adaptive, performance-driven pruning of visual\ntokens: It generates dynamic queries to evaluate visual token importance, and\nadopts Gumbel softmax to enable differentiable token selection. Through\nfine-tuning, LightVLA learns to preserve the most informative visual tokens\nwhile pruning tokens which do not contribute to task execution, thereby\nimproving efficiency and performance simultaneously. Notably, LightVLA requires\nno heuristic magic numbers and introduces no additional trainable parameters,\nmaking it compatible with modern inference frameworks. Experimental results\ndemonstrate that LightVLA outperforms different VLA models and existing token\npruning methods across diverse tasks on the LIBERO benchmark, achieving higher\nsuccess rates with substantially reduced computational overhead. Specifically,\nLightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.9%\nimprovement in task success rate. Meanwhile, we also investigate the learnable\nquery-based token pruning method LightVLA* with additional trainable\nparameters, which also achieves satisfactory performance. Our work reveals that\nas VLA pursues optimal performance, LightVLA spontaneously learns to prune\ntokens from a performance-driven perspective. To the best of our knowledge,\nLightVLA is the first work to apply adaptive visual token pruning to VLA tasks\nwith the collateral goals of efficiency and performance, marking a significant\nstep toward more efficient, powerful and practical real-time robotic systems."}
{"id": "2509.12618", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12618", "abs": "https://arxiv.org/abs/2509.12618", "authors": ["Zekai Zhang", "Weiye Zhu", "Hewei Pan", "Xiangchen Wang", "Rongtao Xu", "Xing Sun", "Feng Zheng"], "title": "ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation", "comment": null, "summary": "The Vision-and-Language Navigation (VLN) task requires an agent to follow\nnatural language instructions and navigate through complex environments.\nExisting MLLM-based VLN methods primarily rely on imitation learning (IL) and\noften use DAgger for post-training to mitigate covariate shift. While\neffective, these approaches incur substantial data collection and training\ncosts. Reinforcement learning (RL) offers a promising alternative. However,\nprior VLN RL methods lack dynamic interaction with the environment and depend\non expert trajectories for reward shaping, rather than engaging in open-ended\nactive exploration. This restricts the agent's ability to discover diverse and\nplausible navigation routes. To address these limitations, we propose\nActiveVLN, a VLN framework that explicitly enables active exploration through\nmulti-turn RL. In the first stage, a small fraction of expert trajectories is\nused for IL to bootstrap the agent. In the second stage, the agent iteratively\npredicts and executes actions, automatically collects diverse trajectories, and\noptimizes multiple rollouts via the GRPO objective. To further improve RL\nefficiency, we introduce a dynamic early-stopping strategy to prune long-tail\nor likely failed trajectories, along with additional engineering optimizations.\nExperiments show that ActiveVLN achieves the largest performance gains over IL\nbaselines compared to both DAgger-based and prior RL-based post-training\nmethods, while reaching competitive performance with state-of-the-art\napproaches despite using a smaller model. Code and data will be released soon."}
{"id": "2509.12620", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12620", "abs": "https://arxiv.org/abs/2509.12620", "authors": ["Yikai Chen", "Zhi Zheng", "Jin Wang", "Bingye He", "Xiangyu Xu", "Jialu Zhang", "Huan Yu", "Guodong Lu"], "title": "PerchMobi^3: A Multi-Modal Robot with Power-Reuse Quad-Fan Mechanism for Air-Ground-Wall Locomotion", "comment": "7 pages, 8 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Achieving seamless integration of aerial flight, ground driving, and wall\nclimbing within a single robotic platform remains a major challenge, as\nexisting designs often rely on additional adhesion actuators that increase\ncomplexity, reduce efficiency, and compromise reliability. To address these\nlimitations, we present PerchMobi^3, a quad-fan, negative-pressure,\nair-ground-wall robot that implements a propulsion-adhesion power-reuse\nmechanism. By repurposing four ducted fans to simultaneously provide aerial\nthrust and negative-pressure adhesion, and integrating them with four actively\ndriven wheels, PerchMobi^3 eliminates dedicated pumps while maintaining a\nlightweight and compact design. To the best of our knowledge, this is the first\nquad-fan prototype to demonstrate functional power reuse for multi-modal\nlocomotion. A modeling and control framework enables coordinated operation\nacross ground, wall, and aerial domains with fan-assisted transitions. The\nfeasibility of the design is validated through a comprehensive set of\nexperiments covering ground driving, payload-assisted wall climbing, aerial\nflight, and cross-mode transitions, demonstrating robust adaptability across\nlocomotion scenarios. These results highlight the potential of PerchMobi^3 as a\nnovel design paradigm for multi-modal robotic mobility, paving the way for\nfuture extensions toward autonomous and application-oriented deployment."}
{"id": "2509.12674", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12674", "abs": "https://arxiv.org/abs/2509.12674", "authors": ["Anna Johansson", "Daniel Lindmark", "Viktor Wiberg", "Martin Servin"], "title": "Safety filtering of robotic manipulation under environment uncertainty: a computational approach", "comment": "8 pages, 8 figures", "summary": "Robotic manipulation in dynamic and unstructured environments requires safety\nmechanisms that exploit what is known and what is uncertain about the world.\nExisting safety filters often assume full observability, limiting their\napplicability in real-world tasks. We propose a physics-based safety filtering\nscheme that leverages high-fidelity simulation to assess control policies under\nuncertainty in world parameters. The method combines dense rollout with nominal\nparameters and parallelizable sparse re-evaluation at critical\nstate-transitions, quantified through generalized factors of safety for stable\ngrasping and actuator limits, and targeted uncertainty reduction through\nprobing actions. We demonstrate the approach in a simulated bimanual\nmanipulation task with uncertain object mass and friction, showing that unsafe\ntrajectories can be identified and filtered efficiently. Our results highlight\nphysics-based sparse safety evaluation as a scalable strategy for safe robotic\nmanipulation under uncertainty."}
{"id": "2509.12702", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12702", "abs": "https://arxiv.org/abs/2509.12702", "authors": ["Hongrui Zhao", "Xunlan Zhou", "Boris Ivanovic", "Negar Mehr"], "title": "UDON: Uncertainty-weighted Distributed Optimization for Multi-Robot Neural Implicit Mapping under Extreme Communication Constraints", "comment": null, "summary": "Multi-robot mapping with neural implicit representations enables the compact\nreconstruction of complex environments. However, it demands robustness against\ncommunication challenges like packet loss and limited bandwidth. While prior\nworks have introduced various mechanisms to mitigate communication disruptions,\nperformance degradation still occurs under extremely low communication success\nrates. This paper presents UDON, a real-time multi-agent neural implicit\nmapping framework that introduces a novel uncertainty-weighted distributed\noptimization to achieve high-quality mapping under severe communication\ndeterioration. The uncertainty weighting prioritizes more reliable portions of\nthe map, while the distributed optimization isolates and penalizes mapping\ndisagreement between individual pairs of communicating agents. We conduct\nextensive experiments on standard benchmark datasets and real-world robot\nhardware. We demonstrate that UDON significantly outperforms existing\nbaselines, maintaining high-fidelity reconstructions and consistent scene\nrepresentations even under extreme communication degradation (as low as 1%\nsuccess rate)."}
{"id": "2509.12714", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12714", "abs": "https://arxiv.org/abs/2509.12714", "authors": ["Kit-Wa Sou", "Junhao Gong", "Shoujie Li", "Chuqiao Lyu", "Ziwu Song", "Shilong Mu", "Wenbo Ding"], "title": "MoiréTac: A Dual-Mode Visuotactile Sensor for Multidimensional Perception Using Moiré Pattern Amplification", "comment": null, "summary": "Visuotactile sensors typically employ sparse marker arrays that limit spatial\nresolution and lack clear analytical force-to-image relationships. To solve\nthis problem, we present \\textbf{Moir\\'eTac}, a dual-mode sensor that generates\ndense interference patterns via overlapping micro-gratings within a transparent\narchitecture. When two gratings overlap with misalignment, they create moir\\'e\npatterns that amplify microscopic deformations. The design preserves optical\nclarity for vision tasks while producing continuous moir\\'e fields for tactile\nsensing, enabling simultaneous 6-axis force/torque measurement, contact\nlocalization, and visual perception. We combine physics-based features\n(brightness, phase gradient, orientation, and period) from moir\\'e patterns\nwith deep spatial features. These are mapped to 6-axis force/torque\nmeasurements, enabling interpretable regression through end-to-end learning.\nExperimental results demonstrate three capabilities: force/torque measurement\nwith R^2 > 0.98 across tested axes; sensitivity tuning through geometric\nparameters (threefold gain adjustment); and vision functionality for object\nclassification despite moir\\'e overlay. Finally, we integrate the sensor into a\nrobotic arm for cap removal with coordinated force and torque control,\nvalidating its potential for dexterous manipulation."}
{"id": "2509.12723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12723", "abs": "https://arxiv.org/abs/2509.12723", "authors": ["Kai Zhang", "Eric Lucet", "Julien Alexandre Dit Sandretto", "Shoubin Chen", "David Filait"], "title": "NAMOUnc: Navigation Among Movable Obstacles with Decision Making on Uncertainty Interval", "comment": "11 pages, ICINCO2025", "summary": "Navigation among movable obstacles (NAMO) is a critical task in robotics,\noften challenged by real-world uncertainties such as observation noise, model\napproximations, action failures, and partial observability. Existing solutions\nfrequently assume ideal conditions, leading to suboptimal or risky decisions.\nThis paper introduces NAMOUnc, a novel framework designed to address these\nuncertainties by integrating them into the decision-making process. We first\nestimate them and compare the corresponding time cost intervals for removing\nand bypassing obstacles, optimizing both the success rate and time efficiency,\nensuring safer and more efficient navigation. We validate our method through\nextensive simulations and real-world experiments, demonstrating significant\nimprovements over existing NAMO frameworks. More details can be found in our\nwebsite: https://kai-zhang-er.github.io/namo-uncertainty/"}
{"id": "2509.12739", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12739", "abs": "https://arxiv.org/abs/2509.12739", "authors": ["Trung Kien La", "Eric Guiffo Kaigom"], "title": "Deep Learning for Model-Free Prediction of Thermal States of Robot Joint Motors", "comment": "$\\copyright$ 2025 the authors. This work has been accepted to the\n  10th IFAC Symposium on Mechatronic Systems & 14th IFAC Symposium on Robotics\n  July 15-18, 2025 || Paris, France for publication under a Creative Commons\n  Licence CC-BY-NC-ND", "summary": "In this work, deep neural networks made up of multiple hidden Long Short-Term\nMemory (LSTM) and Feedforward layers are trained to predict the thermal\nbehavior of the joint motors of robot manipulators. A model-free and scalable\napproach is adopted. It accommodates complexity and uncertainty challenges\nstemming from the derivation, identification, and validation of a large number\nof parameters of an approximation model that is hardly available. To this end,\nsensed joint torques are collected and processed to foresee the thermal\nbehavior of joint motors. Promising prediction results of the machine learning\nbased capture of the temperature dynamics of joint motors of a redundant robot\nwith seven joints are presented."}
{"id": "2509.12740", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12740", "abs": "https://arxiv.org/abs/2509.12740", "authors": ["Eric Guiffo Kaigom"], "title": "Deep Generative and Discriminative Digital Twin endowed with Variational Autoencoder for Unsupervised Predictive Thermal Condition Monitoring of Physical Robots in Industry 6.0 and Society 6.0", "comment": "$\\copyright$ 2025 the authors. This work has been accepted to the to\n  the 10th IFAC Symposium on Mechatronic Systems & 14th IFAC Symposium on\n  Robotics July 15-18, 2025 || Paris, France for publication under a Creative\n  Commons Licence CC-BY-NC-ND", "summary": "Robots are unrelentingly used to achieve operational efficiency in Industry\n4.0 along with symbiotic and sustainable assistance for the work-force in\nIndustry 5.0. As resilience, robustness, and well-being are required in\nanti-fragile manufacturing and human-centric societal tasks, an autonomous\nanticipation and adaption to thermal saturation and burns due to motors\noverheating become instrumental for human safety and robot availability. Robots\nare thereby expected to self-sustain their performance and deliver user\nexperience, in addition to communicating their capability to other agents in\nadvance to ensure fully automated thermally feasible tasks, and prolong their\nlifetime without human intervention. However, the traditional robot shutdown,\nwhen facing an imminent thermal saturation, inhibits productivity in factories\nand comfort in the society, while cooling strategies are hard to implement\nafter the robot acquisition. In this work, smart digital twins endowed with\ngenerative AI, i.e., variational autoencoders, are leveraged to manage\nthermally anomalous and generate uncritical robot states. The notion of thermal\ndifficulty is derived from the reconstruction error of variational\nautoencoders. A robot can use this score to predict, anticipate, and share the\nthermal feasibility of desired motion profiles to meet requirements from\nemerging applications in Industry 6.0 and Society 6.0."}
{"id": "2509.12741", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12741", "abs": "https://arxiv.org/abs/2509.12741", "authors": ["Alexis Yihong Hao", "Yufei Wang", "Navin Sriram Ravie", "Bharath Hegde", "David Held", "Zackory Erickson"], "title": "Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions", "comment": "CoRL 2025", "summary": "Robot-assisted dressing has the potential to significantly improve the lives\nof individuals with mobility impairments. To ensure an effective and\ncomfortable dressing experience, the robot must be able to handle challenging\ndeformable garments, apply appropriate forces, and adapt to limb movements\nthroughout the dressing process. Prior work often makes simplifying assumptions\n-- such as static human limbs during dressing -- which limits real-world\napplicability. In this work, we develop a robot-assisted dressing system\ncapable of handling partial observations with visual occlusions, as well as\nrobustly adapting to arm motions during the dressing process. Given a policy\ntrained in simulation with partial observations, we propose a method to\nfine-tune it in the real world using a small amount of data and multi-modal\nfeedback from vision and force sensing, to further improve the policy's\nadaptability to arm motions and enhance safety. We evaluate our method in\nsimulation with simplified articulated human meshes and in a real world human\nstudy with 12 participants across 264 dressing trials. Our policy successfully\ndresses two long-sleeve everyday garments onto the participants while being\nadaptive to various kinds of arm motions, and greatly outperforms prior\nbaselines in terms of task completion and user feedback. Video are available at\nhttps://dressing-motion.github.io/."}
{"id": "2509.12747", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12747", "abs": "https://arxiv.org/abs/2509.12747", "authors": ["Botao He", "Amir Hossein Shahidzadeh", "Yu Chen", "Jiayi Wu", "Tianrui Guan", "Guofei Chen", "Howie Choset", "Dinesh Manocha", "Glen Chou", "Cornelia Fermuller", "Yiannis Aloimonos"], "title": "NavMoE: Hybrid Model- and Learning-based Traversability Estimation for Local Navigation via Mixture of Experts", "comment": null, "summary": "This paper explores traversability estimation for robot navigation. A key\nbottleneck in traversability estimation lies in efficiently achieving reliable\nand robust predictions while accurately encoding both geometric and semantic\ninformation across diverse environments. We introduce Navigation via Mixture of\nExperts (NAVMOE), a hierarchical and modular approach for traversability\nestimation and local navigation. NAVMOE combines multiple specialized models\nfor specific terrain types, each of which can be either a classical model-based\nor a learning-based approach that predicts traversability for specific terrain\ntypes. NAVMOE dynamically weights the contributions of different models based\non the input environment through a gating network. Overall, our approach offers\nthree advantages: First, NAVMOE enables traversability estimation to adaptively\nleverage specialized approaches for different terrains, which enhances\ngeneralization across diverse and unseen environments. Second, our approach\nsignificantly improves efficiency with negligible cost of solution quality by\nintroducing a training-free lazy gating mechanism, which is designed to\nminimize the number of activated experts during inference. Third, our approach\nuses a two-stage training strategy that enables the training for the gating\nnetworks within the hybrid MoE method that contains nondifferentiable modules.\nExtensive experiments show that NAVMOE delivers a better efficiency and\nperformance balance than any individual expert or full ensemble across\ndifferent domains, improving cross- domain generalization and reducing average\ncomputational cost by 81.2% via lazy gating, with less than a 2% loss in path\nquality."}
{"id": "2509.12754", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12754", "abs": "https://arxiv.org/abs/2509.12754", "authors": ["Saki Hashimoto", "Shoichi Hasegawa", "Tomochika Ishikawa", "Akira Taniguchi", "Yoshinobu Hagiwara", "Lotfi El Hafi", "Tadahiro Taniguchi"], "title": "Toward Ownership Understanding of Objects: Active Question Generation with Large Language Model and Probabilistic Generative Model", "comment": "Submitted to AROB-ISBC 2026 (Journal Track option)", "summary": "Robots operating in domestic and office environments must understand object\nownership to correctly execute instructions such as ``Bring me my cup.''\nHowever, ownership cannot be reliably inferred from visual features alone. To\naddress this gap, we propose Active Ownership Learning (ActOwL), a framework\nthat enables robots to actively generate and ask ownership-related questions to\nusers. ActOwL employs a probabilistic generative model to select questions that\nmaximize information gain, thereby acquiring ownership knowledge efficiently to\nimprove learning efficiency. Additionally, by leveraging commonsense knowledge\nfrom Large Language Models (LLM), objects are pre-classified as either shared\nor owned, and only owned objects are targeted for questioning. Through\nexperiments in a simulated home environment and a real-world laboratory\nsetting, ActOwL achieved significantly higher ownership clustering accuracy\nwith fewer questions than baseline methods. These findings demonstrate the\neffectiveness of combining active inference with LLM-guided commonsense\nreasoning, advancing the capability of robots to acquire ownership knowledge\nfor practical and socially appropriate task execution."}
{"id": "2509.12776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12776", "abs": "https://arxiv.org/abs/2509.12776", "authors": ["Renjie Wang", "Shangke Lyu", "Xin Lang", "Wei Xiao", "Donglin Wang"], "title": "Integrating Trajectory Optimization and Reinforcement Learning for Quadrupedal Jumping with Terrain-Adaptive Landing", "comment": "Accepted by IROS 2025", "summary": "Jumping constitutes an essential component of quadruped robots' locomotion\ncapabilities, which includes dynamic take-off and adaptive landing. Existing\nquadrupedal jumping studies mainly focused on the stance and flight phase by\nassuming a flat landing ground, which is impractical in many real world cases.\nThis work proposes a safe landing framework that achieves adaptive landing on\nrough terrains by combining Trajectory Optimization (TO) and Reinforcement\nLearning (RL) together. The RL agent learns to track the reference motion\ngenerated by TO in the environments with rough terrains. To enable the learning\nof compliant landing skills on challenging terrains, a reward relaxation\nstrategy is synthesized to encourage exploration during landing recovery\nperiod. Extensive experiments validate the accurate tracking and safe landing\nskills benefiting from our proposed method in various scenarios."}
{"id": "2509.12813", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12813", "abs": "https://arxiv.org/abs/2509.12813", "authors": ["Bowen Ye", "Junyue Huang", "Yang Liu", "Xiaozhen Qiao", "Xiang Yin"], "title": "Bridging Perception and Planning: Towards End-to-End Planning for Signal Temporal Logic Tasks", "comment": null, "summary": "We investigate the task and motion planning problem for Signal Temporal Logic\n(STL) specifications in robotics. Existing STL methods rely on pre-defined maps\nor mobility representations, which are ineffective in unstructured real-world\nenvironments. We propose the \\emph{Structured-MoE STL Planner}\n(\\textbf{S-MSP}), a differentiable framework that maps synchronized multi-view\ncamera observations and an STL specification directly to a feasible trajectory.\nS-MSP integrates STL constraints within a unified pipeline, trained with a\ncomposite loss that combines trajectory reconstruction and STL robustness. A\n\\emph{structure-aware} Mixture-of-Experts (MoE) model enables horizon-aware\nspecialization by projecting sub-tasks into temporally anchored embeddings. We\nevaluate S-MSP using a high-fidelity simulation of factory-logistics scenarios\nwith temporally constrained tasks. Experiments show that S-MSP outperforms\nsingle-expert baselines in STL satisfaction and trajectory feasibility. A\nrule-based \\emph{safety filter} at inference improves physical executability\nwithout compromising logical correctness, showcasing the practicality of the\napproach."}
{"id": "2509.12838", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.12838", "abs": "https://arxiv.org/abs/2509.12838", "authors": ["Kento Murata", "Shoichi Hasegawa", "Tomochika Ishikawa", "Yoshinobu Hagiwara", "Akira Taniguchi", "Lotfi El Hafi", "Tadahiro Taniguchi"], "title": "Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models", "comment": "Submitted to AROB-ISBC 2026 (Journal Track option)", "summary": "It is crucial to efficiently execute instructions such as \"Find an apple and\na banana\" or \"Get ready for a field trip,\" which require searching for multiple\nobjects or understanding context-dependent commands. This study addresses the\nchallenging problem of determining which robot should be assigned to which part\nof a task when each robot possesses different situational on-site\nknowledge-specifically, spatial concepts learned from the area designated to it\nby the user. We propose a task planning framework that leverages large language\nmodels (LLMs) and spatial concepts to decompose natural language instructions\ninto subtasks and allocate them to multiple robots. We designed a novel\nfew-shot prompting strategy that enables LLMs to infer required objects from\nambiguous commands and decompose them into appropriate subtasks. In our\nexperiments, the proposed method achieved 47/50 successful assignments,\noutperforming random (28/50) and commonsense-based assignment (26/50).\nFurthermore, we conducted qualitative evaluations using two actual mobile\nmanipulators. The results demonstrated that our framework could handle\ninstructions, including those involving ad hoc categories such as \"Get ready\nfor a field trip,\" by successfully performing task decomposition, assignment,\nsequential planning, and execution."}
{"id": "2509.12846", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12846", "abs": "https://arxiv.org/abs/2509.12846", "authors": ["Junlin Song", "Antoine Richard", "Miguel Olivares-Mendez"], "title": "Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration", "comment": null, "summary": "Visual-inertial fusion is crucial for a large amount of intelligent and\nautonomous applications, such as robot navigation and augmented reality. To\nbootstrap and achieve optimal state estimation, the spatial-temporal\ndisplacements between IMU and cameras must be calibrated in advance. Most\nexisting calibration methods adopt continuous-time state representation, more\nspecifically the B-spline. Despite these methods achieve precise\nspatial-temporal calibration, they suffer from high computational cost caused\nby continuous-time state representation. To this end, we propose a novel and\nextremely efficient calibration method that unleashes the power of\ndiscrete-time state representation. Moreover, the weakness of discrete-time\nstate representation in temporal calibration is tackled in this paper. With the\nincreasing production of drones, cellphones and other visual-inertial\nplatforms, if one million devices need calibration around the world, saving one\nminute for the calibration of each device means saving 2083 work days in total.\nTo benefit both the research and industry communities, our code will be\nopen-source."}
{"id": "2509.12851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12851", "abs": "https://arxiv.org/abs/2509.12851", "authors": ["Antoine Lénat", "Olivier Cheminat", "Damien Chablat", "Camilo Charron"], "title": "A Novel Skill Modeling Approach: Integrating Vergnaud's Scheme with Cognitive Architectures", "comment": null, "summary": "Human-machine interaction is increasingly important in industry, and this\ntrend will only intensify with the rise of Industry 5.0. Human operators have\nskills that need to be adapted when using machines to achieve the best results.\nIt is crucial to highlight the operator's skills and understand how they use\nand adapt them [18]. A rigorous description of these skills is necessary to\ncompare performance with and without robot assistance. Predicate logic, used by\nVergnaud within Piaget's scheme concept, offers a promising approach. However,\nthis theory doesn't account for cognitive system constraints, such as the\ntiming of actions, the limitation of cognitive resources, the parallelization\nof tasks, or the activation of automatic gestures contrary to optimal\nknowledge. Integrating these constraints is essential for representing agent\nskills understanding skill transfer between biological and mechanical\nstructures. Cognitive architectures models [2] address these needs by\ndescribing cognitive structure and can be combined with the scheme for mutual\nbenefit. Welding provides a relevant case study, as it highlights the\nchallenges faced by operators, even highly skilled ones. Welding's complexity\nstems from the need for constant skill adaptation to variable parameters like\npart position and process. This adaptation is crucial, as weld quality, a key\nfactor, is only assessed afterward via destructive testing. Thus, the welder is\nconfronted with a complex perception-decision-action cycle, where the\nevaluation of the impact of his actions is delayed and where errors are\ndefinitive. This dynamic underscores the importance of understanding and\nmodeling the skills of operators."}
{"id": "2509.12858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12858", "abs": "https://arxiv.org/abs/2509.12858", "authors": ["Yidan Lu", "Rurui Yang", "Qiran Kou", "Mengting Chen", "Tao Fan", "Peter Cui", "Yinzhao Dong", "Peng Lu"], "title": "Contrastive Representation Learning for Robust Sim-to-Real Transfer of Adaptive Humanoid Locomotion", "comment": null, "summary": "Reinforcement learning has produced remarkable advances in humanoid\nlocomotion, yet a fundamental dilemma persists for real-world deployment:\npolicies must choose between the robustness of reactive proprioceptive control\nor the proactivity of complex, fragile perception-driven systems. This paper\nresolves this dilemma by introducing a paradigm that imbues a purely\nproprioceptive policy with proactive capabilities, achieving the foresight of\nperception without its deployment-time costs. Our core contribution is a\ncontrastive learning framework that compels the actor's latent state to encode\nprivileged environmental information from simulation. Crucially, this\n``distilled awareness\" empowers an adaptive gait clock, allowing the policy to\nproactively adjust its rhythm based on an inferred understanding of the\nterrain. This synergy resolves the classic trade-off between rigid, clocked\ngaits and unstable clock-free policies. We validate our approach with zero-shot\nsim-to-real transfer to a full-sized humanoid, demonstrating highly robust\nlocomotion over challenging terrains, including 30 cm high steps and 26.5{\\deg}\nslopes, proving the effectiveness of our method. Website:\nhttps://lu-yidan.github.io/cra-loco."}
{"id": "2509.12863", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12863", "abs": "https://arxiv.org/abs/2509.12863", "authors": ["Haozhan Ni", "Jingsong Liang", "Chenyu He", "Yuhong Cao", "Guillaume Sartoretti"], "title": "GRATE: a Graph transformer-based deep Reinforcement learning Approach for Time-efficient autonomous robot Exploration", "comment": null, "summary": "Autonomous robot exploration (ARE) is the process of a robot autonomously\nnavigating and mapping an unknown environment. Recent Reinforcement Learning\n(RL)-based approaches typically formulate ARE as a sequential decision-making\nproblem defined on a collision-free informative graph. However, these methods\noften demonstrate limited reasoning ability over graph-structured data.\nMoreover, due to the insufficient consideration of robot motion, the resulting\nRL policies are generally optimized to minimize travel distance, while\nneglecting time efficiency. To overcome these limitations, we propose GRATE, a\nDeep Reinforcement Learning (DRL)-based approach that leverages a Graph\nTransformer to effectively capture both local structure patterns and global\ncontextual dependencies of the informative graph, thereby enhancing the model's\nreasoning capability across the entire environment. In addition, we deploy a\nKalman filter to smooth the waypoint outputs, ensuring that the resulting path\nis kinodynamically feasible for the robot to follow. Experimental results\ndemonstrate that our method exhibits better exploration efficiency (up to 21.5%\nin distance and 21.3% in time to complete exploration) than state-of-the-art\nconventional and learning-based baselines in various simulation benchmarks. We\nalso validate our planner in real-world scenarios."}
{"id": "2509.12880", "categories": ["cs.RO", "cs.HC", "cs.LG", "68T05, 68T40", "I.2.9; I.2.6; H.5.2"], "pdf": "https://arxiv.org/pdf/2509.12880", "abs": "https://arxiv.org/abs/2509.12880", "authors": ["Anna Deichler", "Siyang Wang", "Simon Alexanderson", "Jonas Beskow"], "title": "Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation", "comment": "Presented at the Context-Awareness in HRI (CONAWA) Workshop, ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI 2022), March 7, 2022", "summary": "Pointing is a key mode of interaction with robots, yet most prior work has\nfocused on recognition rather than generation. We present a motion capture\ndataset of human pointing gestures covering diverse styles, handedness, and\nspatial targets. Using reinforcement learning with motion imitation, we train\npolicies that reproduce human-like pointing while maximizing precision. Results\nshow our approach enables context-aware pointing behaviors in simulation,\nbalancing task performance with natural dynamics."}
{"id": "2509.12890", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12890", "abs": "https://arxiv.org/abs/2509.12890", "authors": ["Malte Probst", "Raphael Wenzel", "Monica Dasi"], "title": "Responsibility and Engagement -- Evaluating Interactions in Social Robot Navigation", "comment": "under review for 2026 IEEE International Conference on Robotics &\n  Automation (ICRA)", "summary": "In Social Robot Navigation (SRN), the availability of meaningful metrics is\ncrucial for evaluating trajectories from human-robot interactions. In the SRN\ncontext, such interactions often relate to resolving conflicts between two or\nmore agents. Correspondingly, the shares to which agents contribute to the\nresolution of such conflicts are important. This paper builds on recent work,\nwhich proposed a Responsibility metric capturing such shares. We extend this\nframework in two directions: First, we model the conflict buildup phase by\nintroducing a time normalization. Second, we propose the related Engagement\nmetric, which captures how the agents' actions intensify a conflict. In a\ncomprehensive series of simulated scenarios with dyadic, group and crowd\ninteractions, we show that the metrics carry meaningful information about the\ncooperative resolution of conflicts in interactions. They can be used to assess\nbehavior quality and foresightedness. We extensively discuss applicability,\ndesign choices and limitations of the proposed metrics."}
{"id": "2509.12912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12912", "abs": "https://arxiv.org/abs/2509.12912", "authors": ["Raphael Wenzel", "Malte Probst"], "title": "Spotting the Unfriendly Robot -- Towards better Metrics for Interactions", "comment": "Presented at 2025 IEEE Conference on Robotics and Automation (ICRA)\n  Workshop: Advances in Social Navigation: Planning, HRI and Beyond", "summary": "Establishing standardized metrics for Social Robot Navigation (SRN)\nalgorithms for assessing the quality and social compliance of robot behavior\naround humans is essential for SRN research. Currently, commonly used\nevaluation metrics lack the ability to quantify how cooperative an agent\nbehaves in interaction with humans. Concretely, in a simple frontal approach\nscenario, no metric specifically captures if both agents cooperate or if one\nagent stays on collision course and the other agent is forced to evade. To\naddress this limitation, we propose two new metrics, a conflict intensity\nmetric and the responsibility metric. Together, these metrics are capable of\nevaluating the quality of human-robot interactions by showing how much a given\nalgorithm has contributed to reducing a conflict and which agent actually took\nresponsibility of the resolution. This work aims to contribute to the\ndevelopment of a comprehensive and standardized evaluation methodology for SRN,\nultimately enhancing the safety, efficiency, and social acceptance of robots in\nhuman-centric environments."}
{"id": "2509.12928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12928", "abs": "https://arxiv.org/abs/2509.12928", "authors": ["Peiwen Yang", "Mingquan Jiang", "Xinyue Shen", "Heping Zhang"], "title": "Spatiotemporal Calibration for Laser Vision Sensor in Hand-eye System Based on Straight-line Constraint", "comment": "Submitted to IEEE RAL", "summary": "Laser vision sensors (LVS) are critical perception modules for industrial\nrobots, facilitating real-time acquisition of workpiece geometric data in\nwelding applications. However, the camera communication delay will lead to a\ntemporal desynchronization between captured images and the robot motions.\nAdditionally, hand-eye extrinsic parameters may vary during prolonged\nmeasurement. To address these issues, we introduce a measurement model of LVS\nconsidering the effect of the camera's time-offset and propose a teaching-free\nspatiotemporal calibration method utilizing line constraints. This method\ninvolves a robot equipped with an LVS repeatedly scanning straight-line fillet\nwelds using S-shaped trajectories. Regardless of the robot's orientation\nchanges, all measured welding positions are constrained to a straight-line,\nrepresented by Plucker coordinates. Moreover, a nonlinear optimization model\nbased on straight-line constraints is established. Subsequently, the\nLevenberg-Marquardt algorithm (LMA) is employed to optimize parameters,\nincluding time-offset, hand-eye extrinsic parameters, and straight-line\nparameters. The feasibility and accuracy of the proposed approach are\nquantitatively validated through experiments on curved weld scanning. We\nopen-sourced the code, dataset, and simulation report at\nhttps://anonymous.4open.science/r/LVS_ST_CALIB-015F/README.md."}
{"id": "2509.12969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12969", "abs": "https://arxiv.org/abs/2509.12969", "authors": ["Jae-Hyun Lee", "Jonghoo Park", "Kyu-Jin Cho"], "title": "Tendon-Based Proprioception in an Anthropomorphic Underactuated Robotic Hand with Series Elastic Actuators", "comment": "8 pages, 10 figures, Supplementary video, Submitted to IEEE Robotics\n  and Automation Letters (RA-L)", "summary": "Anthropomorphic underactuated hands are widely employed for their versatility\nand structural simplicity. In such systems, compact sensing integration and\nproper interpretation aligned with underactuation are crucial for realizing\npractical grasp functionalities. This study proposes an anthropomorphic\nunderactuated hand that achieves comprehensive situational awareness of\nhand-object interaction, utilizing tendon-based proprioception provided by\nseries elastic actuators (SEAs). We developed a compact SEA with high accuracy\nand reliability that can be seamlessly integrated into sensorless fingers. By\ncoupling proprioceptive sensing with potential energy-based modeling, the\nsystem estimates key grasp-related variables, including contact timing, joint\nangles, relative object stiffness, and finger configuration changes indicating\nexternal disturbances. These estimated variables enable grasp posture\nreconstruction, safe handling of deformable objects, and blind grasping with\nproprioceptive-only recognition of objects with varying geometry and stiffness.\nFinger-level experiments and hand-level demonstrations confirmed the\neffectiveness of the proposed approach. The results demonstrate that\ntendon-based proprioception serves as a compact and robust sensing modality for\npractical manipulation without reliance on vision or tactile feedback."}
{"id": "2509.12982", "categories": ["cs.RO", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12982", "abs": "https://arxiv.org/abs/2509.12982", "authors": ["Erblin Isaku", "Hassan Sartaj", "Shaukat Ali", "Beatriz Sanguino", "Tongtong Wang", "Guoyuan Li", "Houxiang Zhang", "Thomas Peyrucain"], "title": "Out of Distribution Detection in Self-adaptive Robots with AI-powered Digital Twins", "comment": "15 pages, 4 figures, 3 tables", "summary": "Self-adaptive robots (SARs) in complex, uncertain environments must\nproactively detect and address abnormal behaviors, including\nout-of-distribution (OOD) cases. To this end, digital twins offer a valuable\nsolution for OOD detection. Thus, we present a digital twin-based approach for\nOOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to\nforecast SAR states and employs reconstruction error and Monte Carlo dropout\nfor uncertainty quantification. By combining reconstruction error with\npredictive variance, the digital twin effectively detects OOD behaviors, even\nin previously unseen conditions. The digital twin also includes an\nexplainability layer that links potential OOD to specific SAR states, offering\ninsights for self-adaptation. We evaluated ODiSAR by creating digital twins of\ntwo industrial robots: one navigating an office environment, and another\nperforming maritime ship navigation. In both cases, ODiSAR forecasts SAR\nbehaviors (i.e., robot trajectories and vessel motion) and proactively detects\nOOD events. Our results showed that ODiSAR achieved high detection performance\n-- up to 98\\% AUROC, 96\\% TNR@TPR95, and 95\\% F1-score -- while providing\ninterpretable insights to support self-adaptation."}
{"id": "2509.13024", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13024", "abs": "https://arxiv.org/abs/2509.13024", "authors": ["Haohan Min", "Zhoujian Li", "Yu Yang", "Jinyu Chen", "Shenghai Yuan"], "title": "DVDP: An End-to-End Policy for Mobile Robot Visual Docking with RGB-D Perception", "comment": null, "summary": "Automatic docking has long been a significant challenge in the field of\nmobile robotics. Compared to other automatic docking methods, visual docking\nmethods offer higher precision and lower deployment costs, making them an\nefficient and promising choice for this task. However, visual docking methods\nimpose strict requirements on the robot's initial position at the start of the\ndocking process. To overcome the limitations of current vision-based methods,\nwe propose an innovative end-to-end visual docking method named DVDP(direct\nvisual docking policy). This approach requires only a binocular RGB-D camera\ninstalled on the mobile robot to directly output the robot's docking path,\nachieving end-to-end automatic docking. Furthermore, we have collected a\nlarge-scale dataset of mobile robot visual automatic docking dataset through a\ncombination of virtual and real environments using the Unity 3D platform and\nactual mobile robot setups. We developed a series of evaluation metrics to\nquantify the performance of the end-to-end visual docking method. Extensive\nexperiments, including benchmarks against leading perception backbones adapted\ninto our framework, demonstrate that our method achieves superior performance.\nFinally, real-world deployment on the SCOUT Mini confirmed DVDP's efficacy,\nwith our model generating smooth, feasible docking trajectories that meet\nphysical constraints and reach the target pose."}
{"id": "2509.13069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13069", "abs": "https://arxiv.org/abs/2509.13069", "authors": ["James C. Ward", "Arthur Richards", "Edmund R. Hunt"], "title": "Practical Handling of Dynamic Environments in Decentralised Multi-Robot Patrol", "comment": null, "summary": "Persistent monitoring using robot teams is of interest in fields such as\nsecurity, environmental monitoring, and disaster recovery. Performing such\nmonitoring in a fully on-line decentralised fashion has significant potential\nadvantages for robustness, adaptability, and scalability of monitoring\nsolutions, including, in principle, the capacity to effectively adapt in\nreal-time to a changing environment. We examine this through the lens of\nmulti-robot patrol, in which teams of patrol robots must persistently minimise\ntime between visits to points of interest, within environments where\ntraversability of routes is highly dynamic. These dynamics must be observed by\npatrol agents and accounted for in a fully decentralised on-line manner. In\nthis work, we present a new method of monitoring and adjusting for environment\ndynamics in a decentralised multi-robot patrol team. We demonstrate that our\nmethod significantly outperforms realistic baselines in highly dynamic\nscenarios, and also investigate dynamic scenarios in which explicitly\naccounting for environment dynamics may be unnecessary or impractical."}
{"id": "2509.13074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13074", "abs": "https://arxiv.org/abs/2509.13074", "authors": ["Simon Fritsch", "Liam Achenbach", "Riccardo Bianco", "Nicola Irmiger", "Gawain Marti", "Samuel Visca", "Chenyu Yang", "Davide Liconti", "Barnabas Gavin Cangan", "Robert Jomar Malate", "Ronan J. Hinchet", "Robert K. Katzschmann"], "title": "Beyond Anthropomorphism: Enhancing Grasping and Eliminating a Degree of Freedom by Fusing the Abduction of Digits Four and Five", "comment": "First five listed authors have equal contribution", "summary": "This paper presents the SABD hand, a 16-degree-of-freedom (DoF) robotic hand\nthat departs from purely anthropomorphic designs to achieve an expanded grasp\nenvelope, enable manipulation poses beyond human capability, and reduce the\nrequired number of actuators. This is achieved by combining the\nadduction/abduction (Add/Abd) joint of digits four and five into a single joint\nwith a large range of motion. The combined joint increases the workspace of the\ndigits by 400\\% and reduces the required DoFs while retaining dexterity.\nExperimental results demonstrate that the combined Add/Abd joint enables the\nhand to grasp objects with a side distance of up to 200 mm. Reinforcement\nlearning-based investigations show that the design enables grasping policies\nthat are effective not only for handling larger objects but also for achieving\nenhanced grasp stability. In teleoperated trials, the hand successfully\nperformed 86\\% of attempted grasps on suitable YCB objects, including\nchallenging non-anthropomorphic configurations. These findings validate the\ndesign's ability to enhance grasp stability, flexibility, and dexterous\nmanipulation without added complexity, making it well-suited for a wide range\nof applications."}
{"id": "2509.13077", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13077", "abs": "https://arxiv.org/abs/2509.13077", "authors": ["Jonathan Külz", "Sehoon Ha", "Matthias Althoff"], "title": "A Design Co-Pilot for Task-Tailored Manipulators", "comment": null, "summary": "Although robotic manipulators are used in an ever-growing range of\napplications, robot manufacturers typically follow a ``one-fits-all''\nphilosophy, employing identical manipulators in various settings. This often\nleads to suboptimal performance, as general-purpose designs fail to exploit\nparticularities of tasks. The development of custom, task-tailored robots is\nhindered by long, cost-intensive development cycles and the high cost of\ncustomized hardware. Recently, various computational design methods have been\ndevised to overcome the bottleneck of human engineering. In addition, a surge\nof modular robots allows quick and economical adaptation to changing industrial\nsettings. This work proposes an approach to automatically designing and\noptimizing robot morphologies tailored to a specific environment. To this end,\nwe learn the inverse kinematics for a wide range of different manipulators. A\nfully differentiable framework realizes gradient-based fine-tuning of designed\nrobots and inverse kinematics solutions. Our generative approach accelerates\nthe generation of specialized designs from hours with optimization-based\nmethods to seconds, serving as a design co-pilot that enables instant\nadaptation and effective human-AI collaboration. Numerical experiments show\nthat our approach finds robots that can navigate cluttered environments,\nmanipulators that perform well across a specified workspace, and can be adapted\nto different hardware constraints. Finally, we demonstrate the real-world\napplicability of our method by setting up a modular robot designed in\nsimulation that successfully moves through an obstacle course."}
{"id": "2509.13095", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13095", "abs": "https://arxiv.org/abs/2509.13095", "authors": ["Zijie Zhao", "Honglei Guo", "Shengqian Chen", "Kaixuan Xu", "Bo Jiang", "Yuanheng Zhu", "Dongbin Zhao"], "title": "Empowering Multi-Robot Cooperation via Sequential World Models", "comment": null, "summary": "Model-based reinforcement learning (MBRL) has shown significant potential in\nrobotics due to its high sample efficiency and planning capability. However,\nextending MBRL to multi-robot cooperation remains challenging due to the\ncomplexity of joint dynamics. To address this, we propose the Sequential World\nModel (SeqWM), a novel framework that integrates the sequential paradigm into\nmodel-based multi-agent reinforcement learning. SeqWM employs independent,\nsequentially structured agent-wise world models to decompose complex joint\ndynamics. Latent rollouts and decision-making are performed through sequential\ncommunication, where each agent generates its future trajectory and plans its\nactions based on the predictions of its predecessors. This design enables\nexplicit intention sharing, enhancing cooperative performance, and reduces\ncommunication overhead to linear complexity. Results in challenging simulated\nenvironments (Bi-DexHands and Multi-Quad) show that SeqWM outperforms existing\nstate-of-the-art model-free and model-based baselines in both overall\nperformance and sample efficiency, while exhibiting advanced cooperative\nbehaviors such as predictive adaptation and role division. Furthermore, SeqWM\nhas been success fully deployed on physical quadruped robots, demonstrating its\neffectiveness in real-world multi-robot systems. Demos and code are available\nat: https://github.com/zhaozijie2022/seqwm-marl"}
{"id": "2509.13109", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13109", "abs": "https://arxiv.org/abs/2509.13109", "authors": ["Fabian Flürenbrock", "Yanick Büchel", "Johannes Köhler", "Marianne Schmid Daners", "Melanie N. Zeilinger"], "title": "Model Predictive Control with Reference Learning for Soft Robotic Intracranial Pressure Waveform Modulation", "comment": null, "summary": "This paper introduces a learning-based control framework for a soft robotic\nactuator system designed to modulate intracranial pressure (ICP) waveforms,\nwhich is essential for studying cerebrospinal fluid dynamics and pathological\nprocesses underlying neurological disorders. A two-layer framework is proposed\nto safely achieve a desired ICP waveform modulation. First, a model predictive\ncontroller (MPC) with a disturbance observer is used for offset-free tracking\nof the system's motor position reference trajectory under safety constraints.\nSecond, to address the unknown nonlinear dependence of ICP on the motor\nposition, we employ a Bayesian optimization (BO) algorithm used for online\nlearning of a motor position reference trajectory that yields the desired ICP\nmodulation. The framework is experimentally validated using a test bench with a\nbrain phantom that replicates realistic ICP dynamics in vitro. Compared to a\npreviously employed proportional-integral-derivative controller, the MPC\nreduces mean and maximum motor position reference tracking errors by 83 % and\n73 %, respectively. In less than 20 iterations, the BO algorithm learns a motor\nposition reference trajectory that yields an ICP waveform with the desired mean\nand amplitude."}
{"id": "2509.13126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13126", "abs": "https://arxiv.org/abs/2509.13126", "authors": ["Miquel Oller", "An Dang", "Nima Fazeli"], "title": "Hydrosoft: Non-Holonomic Hydroelastic Models for Compliant Tactile Manipulation", "comment": null, "summary": "Tactile sensors have long been valued for their perceptual capabilities,\noffering rich insights into the otherwise hidden interface between the robot\nand grasped objects. Yet their inherent compliance -- a key driver of\nforce-rich interactions -- remains underexplored. The central challenge is to\ncapture the complex, nonlinear dynamics introduced by these passive-compliant\nelements. Here, we present a computationally efficient non-holonomic\nhydroelastic model that accurately models path-dependent contact force\ndistributions and dynamic surface area variations. Our insight is to extend the\nobject's state space, explicitly incorporating the distributed forces generated\nby the compliant sensor. Our differentiable formulation not only accounts for\npath-dependent behavior but also enables gradient-based trajectory\noptimization, seamlessly integrating with high-resolution tactile feedback. We\ndemonstrate the effectiveness of our approach across a range of simulated and\nreal-world experiments and highlight the importance of modeling the path\ndependence of sensor dynamics."}
{"id": "2509.13132", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13132", "abs": "https://arxiv.org/abs/2509.13132", "authors": ["Zhihao Zhang", "Chengyang Peng", "Minghao Zhu", "Ekim Yurtsever", "Keith A. Redmill"], "title": "An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios", "comment": null, "summary": "Autonomous driving in dense, dynamic environments requires decision-making\nsystems that can exploit both spatial structure and long-horizon temporal\ndependencies while remaining robust to uncertainty. This work presents a novel\nframework that integrates multi-channel bird's-eye-view occupancy grids with\ntransformer-based sequence modeling for tactical driving in complex roundabout\nscenarios. To address the imbalance between frequent low-risk states and rare\nsafety-critical decisions, we propose the Uncertainty-Weighted Decision\nTransformer (UWDT). UWDT employs a frozen teacher transformer to estimate\nper-token predictive entropy, which is then used as a weight in the student\nmodel's loss function. This mechanism amplifies learning from uncertain,\nhigh-impact states while maintaining stability across common low-risk\ntransitions. Experiments in a roundabout simulator, across varying traffic\ndensities, show that UWDT consistently outperforms other baselines in terms of\nreward, collision rate, and behavioral stability. The results demonstrate that\nuncertainty-aware, spatial-temporal transformers can deliver safer and more\nefficient decision-making for autonomous driving in complex traffic\nenvironments."}
{"id": "2509.13164", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13164", "abs": "https://arxiv.org/abs/2509.13164", "authors": ["Jiawei Wang", "Haowei Sun", "Xintao Yan", "Shuo Feng", "Jun Gao", "Henry X. Liu"], "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving", "comment": "8 pages, 6 figures. Codes and videos are available at\n  https://wjiawei.com/terasim-world-web/", "summary": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical~data synthesis framework for training and evaluation of E2E autonomous\ndriving systems."}
{"id": "2509.13177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13177", "abs": "https://arxiv.org/abs/2509.13177", "authors": ["Salvatore Esposito", "Matías Mattamala", "Daniel Rebain", "Francis Xiatian Zhang", "Kevin Dhaliwal", "Mohsen Khadem", "Subramanian Ramamoorthy"], "title": "ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation", "comment": null, "summary": "Continuum robots are advancing bronchoscopy procedures by accessing complex\nlung airways and enabling targeted interventions. However, their development is\nlimited by the lack of realistic training and test environments: Real data is\ndifficult to collect due to ethical constraints and patient safety concerns,\nand developing autonomy algorithms requires realistic imaging and physical\nfeedback. We present ROOM (Realistic Optical Observation in Medicine), a\ncomprehensive simulation framework designed for generating photorealistic\nbronchoscopy training data. By leveraging patient CT scans, our pipeline\nrenders multi-modal sensor data including RGB images with realistic noise and\nlight specularities, metric depth maps, surface normals, optical flow and point\nclouds at medically relevant scales. We validate the data generated by ROOM in\ntwo canonical tasks for medical robotics -- multi-view pose estimation and\nmonocular depth estimation, demonstrating diverse challenges that\nstate-of-the-art methods must overcome to transfer to these medical settings.\nFurthermore, we show that the data produced by ROOM can be used to fine-tune\nexisting depth estimation models to overcome these challenges, also enabling\nother downstream applications such as navigation. We expect that ROOM will\nenable large-scale data generation across diverse patient anatomies and\nprocedural scenarios that are challenging to capture in clinical settings. Code\nand data: https://github.com/iamsalvatore/room."}
{"id": "2509.13200", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13200", "abs": "https://arxiv.org/abs/2509.13200", "authors": ["Moonyoung Lee", "Dong Ki Kim", "Jai Krishna Bandi", "Max Smith", "Aileen Liao", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "StageACT: Stage-Conditioned Imitation for Robust Humanoid Door Opening", "comment": "7 pages", "summary": "Humanoid robots promise to operate in everyday human environments without\nrequiring modifications to the surroundings. Among the many skills needed,\nopening doors is essential, as doors are the most common gateways in built\nspaces and often limit where a robot can go. Door opening, however, poses\nunique challenges as it is a long-horizon task under partial observability,\nsuch as reasoning about the door's unobservable latch state that dictates\nwhether the robot should rotate the handle or push the door. This ambiguity\nmakes standard behavior cloning prone to mode collapse, yielding blended or\nout-of-sequence actions. We introduce StageACT, a stage-conditioned imitation\nlearning framework that augments low-level policies with task-stage inputs.\nThis effective addition increases robustness to partial observability, leading\nto higher success rates and shorter completion times. On a humanoid operating\nin a real-world office environment, StageACT achieves a 55% success rate on\npreviously unseen doors, more than doubling the best baseline. Moreover, our\nmethod supports intentional behavior guidance through stage prompting, enabling\nrecovery behaviors. These results highlight stage conditioning as a lightweight\nyet powerful mechanism for long-horizon humanoid loco-manipulation."}
{"id": "2509.13239", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13239", "abs": "https://arxiv.org/abs/2509.13239", "authors": ["Tianxu An", "Flavio De Vincenti", "Yuntao Ma", "Marco Hutter", "Stelian Coros"], "title": "Collaborative Loco-Manipulation for Pick-and-Place Tasks with Dynamic Reward Curriculum", "comment": null, "summary": "We present a hierarchical RL pipeline for training one-armed legged robots to\nperform pick-and-place (P&P) tasks end-to-end -- from approaching the payload\nto releasing it at a target area -- in both single-robot and cooperative\ndual-robot settings. We introduce a novel dynamic reward curriculum that\nenables a single policy to efficiently learn long-horizon P&P operations by\nprogressively guiding the agents through payload-centered sub-objectives.\nCompared to state-of-the-art approaches for long-horizon RL tasks, our method\nimproves training efficiency by 55% and reduces execution time by 18.6% in\nsimulation experiments. In the dual-robot case, we show that our policy enables\neach robot to attend to different components of its observation space at\ndistinct task stages, promoting effective coordination via autonomous attention\nshifts. We validate our method through real-world experiments using ANYmal D\nplatforms in both single- and dual-robot scenarios. To our knowledge, this is\nthe first RL pipeline that tackles the full scope of collaborative P&P with two\nlegged manipulators."}
{"id": "2509.13249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13249", "abs": "https://arxiv.org/abs/2509.13249", "authors": ["Ye Li", "Daming Liu", "Yanhe Zhu", "Junming Zhang", "Yongsheng Luo", "Ziqi Wang", "Chenyu Liu", "Jie Zhao"], "title": "Design and Control of a Perching Drone Inspired by the Prey-Capturing Mechanism of Venus Flytrap", "comment": null, "summary": "The endurance and energy efficiency of drones remain critical challenges in\ntheir design and operation. To extend mission duration, numerous studies\nexplored perching mechanisms that enable drones to conserve energy by\ntemporarily suspending flight. This paper presents a new perching drone that\nutilizes an active flexible perching mechanism inspired by the rapid predation\nmechanism of the Venus flytrap, achieving perching in less than 100 ms. The\nproposed system is designed for high-speed adaptability to the perching\ntargets. The overall drone design is outlined, followed by the development and\nvalidation of the biomimetic perching structure. To enhance the system\nstability, a cascade extended high-gain observer (EHGO) based control method is\ndeveloped, which can estimate and compensate for the external disturbance in\nreal time. The experimental results demonstrate the adaptability of the\nperching structure and the superiority of the cascaded EHGO in resisting wind\nand perching disturbances."}
{"id": "2509.13279", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13279", "abs": "https://arxiv.org/abs/2509.13279", "authors": ["Sanjay Oruganti", "Sergei Nirenburg", "Marjorie McShane", "Jesse English", "Michael K. Roberts", "Christian Arndt", "Carlos Gonzalez", "Mingyo Seo", "Luis Sentis"], "title": "HARMONIC: A Content-Centric Cognitive Robotic Architecture", "comment": null, "summary": "This paper introduces HARMONIC, a cognitive-robotic architecture designed for\nrobots in human-robotic teams. HARMONIC supports semantic perception\ninterpretation, human-like decision-making, and intentional language\ncommunication. It addresses the issues of safety and quality of results; aims\nto solve problems of data scarcity, explainability, and safety; and promotes\ntransparency and trust. Two proof-of-concept HARMONIC-based robotic systems are\ndemonstrated, each implemented in both a high-fidelity simulation environment\nand on physical robotic platforms."}
