<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.RO](#cs.RO) [Total: 36]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents](https://arxiv.org/abs/2601.20975)
*Nikita Gupta,Riju Chatterjee,Lukas Haas,Connie Tao,Andrew Wang,Chang Liu,Hidekazu Oiwa,Elena Gribovskaya,Jan Ackermann,John Blitzer,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: DeepSearchQA是一个包含900个提示的基准测试，用于评估智能体在17个不同领域的复杂多步骤信息搜索任务中的表现，重点关注系统信息整合、去重和停止条件判断能力。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试主要关注单一答案检索或广泛事实性，而缺乏对智能体执行复杂搜索计划、整合碎片化信息、去重和判断停止条件等关键能力的评估。需要一个新的基准来测试这些在实际深度研究任务中至关重要的能力。

Method: 创建了一个包含900个手工制作的挑战性任务的数据集，覆盖17个不同领域。每个任务都设计为因果链结构，后续步骤的成功依赖于前一步的完成。所有任务都基于开放网络，具有客观可验证的答案集。

Result: 对最先进的智能体架构进行全面评估显示显著的性能限制：即使最先进的模型也难以平衡高召回率和精确度。观察到不同的失败模式，包括过早停止（检索不足）和过度泛化行为（通过低置信度答案人为提高召回率）。

Conclusion: DeepSearchQA揭示了当前智能体设计中的关键改进空间，并定位为未来研究的重要诊断工具，推动开发更强大的深度研究能力。

Abstract: We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.

</details>


### [2] [asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation](https://arxiv.org/abs/2601.20992)
*Oleg Sedukhin,Andrey Kostin*

Main category: cs.CL

TL;DR: 提出语音识别评估的改进方法：支持多参考标注的字符串对齐算法、收集俄语长语音数据集、开发流式识别评估工具


<details>
  <summary>Details</summary>
Motivation: 现有语音识别评估方法对非拉丁语系、构词丰富的语言（如俄语）支持不足，特别是在长语音、嘈杂语音场景下，需要更好的对齐算法和多参考标注

Method: 1. 提出支持多参考标注、任意长度插入和更好词对齐的字符串对齐算法；2. 收集DiverseSpeech-Ru俄语长语音数据集并进行多参考标注；3. 重新标注流行俄语测试集；4. 开发流式语音识别评估工具和可视化比较工具

Result: 1. 改进的对齐算法特别适用于非拉丁语系语言；2. 发现模型会适应数据集特定标注，造成指标改进的假象；3. 提供了多种离线/流式语音识别模型的统一封装

Conclusion: 提出的改进方法提升了语音识别评估的准确性和实用性，特别是对俄语等复杂语言，代码将公开提供

Abstract: We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.

</details>


### [3] [UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop](https://arxiv.org/abs/2601.21000)
*Muhammad Ali Shafique,Areej Mehboob,Layba Fiaz,Muhammad Usman Qadeer,Hamza Farooq*

Main category: cs.CL

TL;DR: 提出一个上下文集成翻译框架，通过多翻译系统结合人工验证，将英文推理基准翻译为乌尔都语，创建UrduBench基准，并全面评估LLMs在乌尔都语推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在低资源语言（如乌尔都语）的推理能力评估面临挑战，缺乏标准化基准。现有方法依赖机器翻译但存在敏感性问题，且多关注一般语言任务而非推理能力。

Method: 提出上下文集成翻译框架，结合多个翻译系统进行翻译，并通过人工验证确保上下文和结构完整性。将MGSM、MATH-500、CommonSenseQA和OpenBookQA等推理基准翻译为乌尔都语，形成UrduBench。

Result: 评估发现：多步和符号推理任务在乌尔都语中挑战最大；稳定的语言对齐是鲁棒推理的关键前提；不同数据集、任务难度、模型架构和规模设置下存在性能差异。

Conclusion: 建立了乌尔都语标准化推理评估的可扩展方法，提供了多语言推理失败的实证见解。该方法可推广到其他低资源语言，代码和数据集将公开。

Abstract: Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluation has been limited by the sensitivity of machine translation and an emphasis on general language tasks rather than reasoning benchmarks. In this paper, we propose a contextually ensembled translation framework with human-in-the-loop validation that leverages multiple translation systems to develop Urdu reasoning benchmarks while preserving contextual and structural integrity. Using this framework, we translate widely adopted reasoning and question-answering benchmarks, including MGSM, MATH-500, CommonSenseQA, and OpenBookQA, into Urdu, collectively referred to as UrduBench, and conduct a comprehensive evaluation of both reasoning-oriented and instruction-tuned LLMs across multiple prompting strategies. Our analysis reveals performance differences across (1) four datasets, (2) five task difficulty levels, (3) diverse model architectures, (4) multiple model scaling settings, and (5) language consistency tests. We find that multi-step and symbolic reasoning tasks pose significant challenges in Urdu, and that stable language alignment is a critical prerequisite for robust reasoning. Overall, our work establishes a scalable methodology for standardized reasoning evaluation in Urdu and provides empirical insights into multilingual reasoning failures. This experimental setup is also broadly applicable to other low-resource languages. The code and datasets will be publicly released.

</details>


### [4] [Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations](https://arxiv.org/abs/2601.21084)
*Amit Meghanani,Thomas Hain*

Main category: cs.CL

TL;DR: 该论文研究了自监督学习（SSL）语音模型中前端语音增强（SE）微调的位置嵌入问题，提出使用软DTW损失和速度扰动来避免MSE损失对位置信息的过度依赖，从而提高下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前将前端语音增强模型与自监督学习语音模型结合时，通常使用MSE损失在增强语音和干净语音之间进行微调。但MSE容易利用SSL模型中的位置嵌入，通过位置相关性而非内容信息最小化目标，这限制了模型性能。

Method: 提出了两种策略来解决位置嵌入问题：1）零填充（在微调设置中重新审视），2）结合速度扰动的软DTW损失。软DTW方法能够更好地处理时间对齐问题，减少对位置信息的依赖。

Result: 实验表明，基于软DTW的方法实现了更快的收敛速度和改进的下游任务性能，验证了在SSL语音建模中进行位置不变微调的重要性。

Conclusion: 位置不变微调对于SSL语音模型至关重要，软DTW损失结合速度扰动能有效避免MSE损失对位置嵌入的过度利用，提升模型在噪声条件下的下游任务表现。

Abstract: Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.

</details>


### [5] [ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference](https://arxiv.org/abs/2601.21109)
*Ketan Thakkar,Maitreyi Chatterjee,Ramasubramanian Balasubramanian,Achyuthan Jootoo,Rajendra Ugrani*

Main category: cs.CL

TL;DR: ChunkWise LoRA：一种动态自适应方法，根据token复杂度将序列划分为可变长度的chunk，并为每个chunk分配定制的低秩配置，显著降低延迟和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法对所有输入token使用静态的秩配置，忽略了token复杂度和计算需求的差异，导致效率低下。

Method: 提出ChunkWise LoRA，包含运行时调度器估计token难度、自适应分块、使用rank-ladder机制选择每块LoRA秩和缩放，以及边界安全组合模块和策略驱动的KV-cache策略。

Result: 在Wikitext-103和SQuAD等基准测试中，相比基线LoRA实现了高达34%的延迟降低和38%的内存减少，同时保持或改进了BLEU、EM和困惑度等任务性能指标。

Conclusion: ChunkWise LoRA为参数高效LLM的实际部署提供了实用解决方案，完全兼容现有transformer架构和推理框架。

Abstract: Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.

</details>


### [6] [Multi-task Code LLMs: Data Mix or Model Merge?](https://arxiv.org/abs/2601.21115)
*Mingzhi Zhu,Boris Sobolev,Rahul Krishna,Raju Pavuluri,Stacy Patterson,Michele Merler*

Main category: cs.CL

TL;DR: 比较数据混合与模型合并两种创建小型多任务代码LLM的方法，发现7B规模时模型合并效果更好，2B规模时数据混合更优，并引入权重分析技术理解任务对参数的影响。


<details>
  <summary>Details</summary>
Motivation: 随着研究倡导在代理框架中部署小型专业化代码LLM，需要探索高效的多任务学习策略，以平衡性能、约束和成本。

Method: 比较数据混合与模型合并两种方法，在Qwen Coder和DeepSeek Coder两个模型家族的2B和7B规模上进行实验，针对代码生成和代码摘要任务进行微调，并引入权重分析技术。

Result: 在7B规模上，模型合并效果最佳，在代码生成任务上保留96%的专业模型性能，同时保持摘要能力，甚至能超越单独微调的模型；在2B规模上，数据混合是更优策略。

Conclusion: 精心设计的合并和混合策略可以有效结合任务特定能力而不会显著降低性能，适合资源受限的部署场景。

Abstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.

</details>


### [7] [Large Language Models Naively Recover Ethnicity from Individual Records](https://arxiv.org/abs/2601.21132)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: LLM能根据姓名推断种族/民族，准确率超过传统BISG方法，无需额外训练数据，适用于美国以外地区，并能减少收入偏见。


<details>
  <summary>Details</summary>
Motivation: 传统种族推断方法如BISG存在局限性：仅适用于美国，需要额外数据，存在系统性偏见（特别是对富裕社区的少数族裔），且无法适应不同文化背景的分类需求。

Method: 使用大型语言模型（包括GPT-4o、Gemini 3 Flash、DeepSeek v3.2等6个模型）根据姓名推断种族/民族。通过扩展推理能力提升准确率，结合政党注册等元数据进一步优化。还使用LLM标签微调小型Transformer模型，实现本地部署。

Result: 在佛罗里达和北卡罗来纳选民数据上，LLM分类准确率达84.7%，显著优于BISG的68.2%。扩展推理可提升1-3个百分点，结合元数据可达86.7%。在黎巴嫩（宗教派别64.3%）、印度（保留选区议员99.2%、种姓分类74.0%）等多国验证有效。微调的小型模型超越BISG准确率且可本地部署。

Conclusion: LLM提供了一种更准确、更灵活、偏见更少的种族/民族推断方法，适用于全球范围，并能适应不同文化背景的分类体系。该方法在社会科学研究和政策应用中具有重要价值。

Abstract: I demonstrate that large language models can infer ethnicity from names with accuracy exceeding that of Bayesian Improved Surname Geocoding (BISG) without additional training data, enabling inference outside the United States and to contextually appropriate classification categories. Using stratified samples from Florida and North Carolina voter files with self-reported race, LLM-based classification achieves up to 84.7% accuracy, outperforming BISG (68.2%) on balanced samples. I test six models including Gemini 3 Flash, GPT-4o, and open-source alternatives such as DeepSeek v3.2 and GLM-4.7. Enabling extended reasoning can improve accuracy by 1-3 percentage points, though effects vary across contexts; including metadata such as party registration reaches 86.7%. LLM classification also reduces the income bias inherent in BISG, where minorities in wealthier neighborhoods are systematically misclassified as White. I further validate using Lebanese voter registration with religious sect (64.3% accuracy), Indian MPs from reserved constituencies (99.2%), and Indian land records with caste classification (74.0%). Aggregate validation across India, Uganda, Nepal, Armenia, Chile, and Costa Rica using original full-count voter rolls demonstrates that the method recovers known population distributions where naming conventions are distinctive. For large-scale applications, small transformer models fine-tuned on LLM labels exceed BISG accuracy while enabling local deployment at no cost.

</details>


### [8] [EnsembleLink: Accurate Record Linkage Without Training Data](https://arxiv.org/abs/2601.21138)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: EnsembleLink是一种无需训练标签的高精度记录链接方法，利用预训练语言模型学习语义关系，在多个基准测试中达到或超过需要大量标注的方法。


<details>
  <summary>Details</summary>
Motivation: 记录链接在社会科学研究中至关重要，但目前方法不成熟：研究者将其视为预处理步骤，使用临时规则且不量化链接错误带来的不确定性。现有方法要么精度低，要么需要大量标注数据。

Method: 提出EnsembleLink方法，利用预训练语言模型（已从大型文本语料库学习语义关系）实现高精度记录链接，无需任何训练标签。该方法在开源模型上本地运行，无需外部API调用。

Result: 在城市名称、人名、组织、多语言政党和书目记录等多个基准测试中，EnsembleLink达到或超过需要大量标注的方法。典型链接任务可在几分钟内完成。

Conclusion: EnsembleLink为社会科学研究提供了一种无需标注数据的高精度记录链接方法，解决了现有方法的局限性，能够量化链接不确定性并提高下游分析的可信度。

Abstract: Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules without quantifying the uncertainty that linkage errors introduce into downstream analyses. Existing methods either achieve low accuracy or require substantial labeled training data. I present EnsembleLink, a method that achieves high accuracy without any training labels. EnsembleLink leverages pre-trained language models that have learned semantic relationships (e.g., that "South Ozone Park" is a neighborhood in "New York City" or that "Lutte ouvriere" refers to the Trotskyist "Workers' Struggle" party) from large text corpora. On benchmarks spanning city names, person names, organizations, multilingual political parties, and bibliographic records, EnsembleLink matches or exceeds methods requiring extensive labeling. The method runs locally on open-source models, requiring no external API calls, and completes typical linkage tasks in minutes.

</details>


### [9] [Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space](https://arxiv.org/abs/2601.21169)
*Tobias Materzok*

Main category: cs.CL

TL;DR: OS-Search将LLM生成转化为端点搜索，在冻结编码器定义的3D输出空间Z中进行目标选择，通过序列级RL训练的策略生成接近目标坐标的输出，实现并行扫描和黑盒优化。


<details>
  <summary>Details</summary>
Motivation: 传统LLM生成通常是路径依赖的token级搜索，难以进行并行优化和黑盒目标优化。需要一种方法能够在输出空间中进行高效搜索，同时保持生成质量。

Method: 1. 定义冻结编码器的3D输出空间Z；2. 外层循环选择目标z*；3. 使用序列级强化学习训练检索接地策略；4. 策略生成输出使其坐标在标准自回归解码下接近z*；5. 支持并行扫描和贝叶斯优化。

Result: 在故事生成中，Z空间扫描比prompt-chaining获得3.1倍更高的LLM评分多样性；在代码生成中，Z空间的贝叶斯优化在匹配推理预算下改进了控制器未知的目标函数，同时保持有效性。

Conclusion: OS-Search提供了一种有效的输出空间搜索框架，能够实现并行优化和黑盒目标改进，在文本和代码生成任务中均表现出优越性能。

Abstract: We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.

</details>


### [10] [From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning](https://arxiv.org/abs/2601.21191)
*Xiulin Yang,Heidi Getz,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 研究统计条件如何支持从线性输入中学习层次结构，重点关注功能词的分布特性及其在语言习得中的作用


<details>
  <summary>Details</summary>
Motivation: 探索支持从线性输入中学习层次结构的统计条件，特别关注功能词在语言习得中的关键作用，因为功能词具有高频、与句法结构可靠关联、与短语边界对齐等独特分布特性

Method: 1) 跨语言语料库分析：在186种语言中验证功能词的三种分布特性；2) 反事实语言建模和消融实验：比较保留不同特性的语言变体在神经网络学习中的表现；3) 探测和消融分析：研究不同学习条件下对功能词的依赖机制

Result: 1) 三种分布特性在所有186种语言中都存在；2) 保留所有三种特性的语言变体更容易被神经网络习得，其中频率和结构关联的贡献比边界对齐更强；3) 不同的学习条件导致对功能词的系统性不同依赖，表明相似性能可能源于不同的内部机制

Conclusion: 功能词的分布特性（特别是高频和结构关联）支持从线性输入中学习层次结构，但学习条件会影响对功能词的依赖方式，揭示了语言习得中性能相似但机制不同的现象

Abstract: What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.

</details>


### [11] [Scaling Embeddings Outperforms Scaling Experts in Language Models](https://arxiv.org/abs/2601.21204)
*Hong Liu,Jiaqi Zhang,Chao Wang,Xing Hu,Linkun Lyu,Jiaqi Sun,Xurui Yang,Bo Wang,Fengcun Li,Yulei Qian,Lingtong Si,Yerui Sun,Rumei Li,Peng Pei,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

TL;DR: 论文提出嵌入缩放作为混合专家架构的替代方案，通过系统分析和优化，开发出LongCat-Flash-Lite模型，在推理速度和性能上超越传统MoE方法。


<details>
  <summary>Details</summary>
Motivation: 混合专家架构在大型语言模型中面临收益递减和系统瓶颈问题，需要探索新的稀疏性扩展维度。

Method: 采用嵌入缩放作为正交扩展维度，通过参数预算分配、模型宽度深度交互分析，结合系统优化和推测解码技术。

Result: 开发出68.5B参数的LongCat-Flash-Lite模型（激活约3B参数），在代理和编码领域表现优异，超越参数相当的MoE基线模型。

Conclusion: 嵌入缩放是有效的稀疏性扩展策略，通过适当的架构设计和系统优化，能够实现更好的性能-效率权衡。

Abstract: While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.

</details>


### [12] [Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling](https://arxiv.org/abs/2601.21205)
*Eunjung Yeo,Julie M. Liss,Visar Berisha,David R. Mortensen*

Main category: cs.CL

TL;DR: 提出一个多语言音素生成评估框架，结合通用音素识别和语言特定音素解释，用于跨语言的构音障碍可懂度评估


<details>
  <summary>Details</summary>
Motivation: 神经障碍导致的构音障碍日益普遍，需要跨语言的自动可懂度评估方法。现有方法要么局限于单一语言，要么无法捕捉影响可懂度的语言特定因素。

Method: 整合通用音素识别与语言特定音素解释，使用对比音系特征距离进行音素到音位映射和序列对齐。框架产生三个指标：音素错误率(PER)、音系特征错误率(PFER)和新提出的对齐无关指标音素覆盖率(PhonCov)。

Result: 在英语、西班牙语、意大利语和泰米尔语上的分析显示：PER受益于映射和对齐的组合，PFER仅受益于对齐，PhonCov受益于映射。框架能捕捉与构音障碍语音已知观察一致的有临床意义的可懂度退化模式。

Conclusion: 提出的多语言音素生成评估框架能有效评估跨语言的构音障碍可懂度，捕捉语言特定因素，并提供临床相关的评估指标。

Abstract: The growing prevalence of neurological disorders associated with dysarthria motivates the need for automated intelligibility assessment methods that are applicalbe across languages. However, most existing approaches are either limited to a single language or fail to capture language-specific factors shaping intelligibility. We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. The framework yields three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a newly proposed alignment-free measure, phoneme coverage (PhonCov). Analysis on English, Spanish, Italian, and Tamil show that PER benefits from the combination of mapping and alignment, PFER from alignment alone, and PhonCov from mapping. Further analyses demonstrate that the proposed framework captures clinically meaningful patterns of intelligibility degradation consistent with established observations of dysarthric speech.

</details>


### [13] [Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models](https://arxiv.org/abs/2601.21214)
*Zhaoyi Li,Jiatong Li,Gangwei Jiang,Linqi Song,Defu Lian,Ying Wei*

Main category: cs.CL

TL;DR: 本文研究了LLM在推理步数泛化场景中的性能下降问题，发现错误集中在少数关键错误类型的token位置，源于内部竞争机制，并提出了一种动态识别和停用错误处理头的轻量级干预方法。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT推理已成为LLM解决复杂问题的标准范式，但研究发现当所需推理步数超出训练分布时，LLM会出现显著的性能下降。这种推理步数泛化失败的内在机制尚未被充分理解，因此需要系统研究其根本原因。

Method: 作者首先在多领域任务上进行系统研究，发现错误集中在少数关键错误类型的token位置而非均匀分布。进一步分析揭示这些错误源于内部竞争机制：某些注意力头（错误处理头）会放大错误推理轨迹同时抑制正确推理。基于此，提出了推理时的测试时间校正方法，动态识别和停用错误处理头。

Result: 研究发现：1）错误集中在特定token位置而非均匀分布；2）单个错误处理头的移除通常能恢复正确预测；3）提出的测试时间校正方法在不同任务和LLM上都能一致地提升推理步数泛化性能。

Conclusion: LLM在推理步数泛化中的失败源于内部注意力头的竞争机制，通过动态识别和停用错误处理头的轻量级干预方法可以有效改善推理泛化能力，这既展示了方法的有效性，也揭示了其潜力。

Abstract: Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.

</details>


### [14] [Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data](https://arxiv.org/abs/2601.21218)
*Christopher Adrian Kusuma,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 提出基于Pythia开源LLM的诚实性评估基准和利用预训练数据提升LLM诚实性的新方法


<details>
  <summary>Details</summary>
Motivation: 当前LLM经常产生幻觉，在知识边界外生成错误回答，而现有诚实性评估方法不够鲁棒，未考虑模型预训练阶段实际吸收的知识

Method: 利用Pythia开源LLM及其公开的预训练数据，构建更鲁棒的诚实性评估基准；并提出利用预训练数据提升LLM诚实性的新方法

Result: 提出了更鲁棒的LLM诚实性评估基准数据集，并开发了基于预训练数据提升LLM诚实性的新方法

Conclusion: 通过利用开源LLM的预训练数据，可以构建更有效的诚实性评估基准和提升方法，帮助LLM在知识边界外更诚实地回答"我不知道"

Abstract: Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with "I don't know" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.

</details>


### [15] [MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning Evaluation](https://arxiv.org/abs/2601.21225)
*Tianyi Xu,Kosei Uemura,Alfred Malengo Kondoro,Tadesse Destaw Belay,Catherine Nana Nyaah Essuman,Ifeoma Okoh,Ganiyat Afolabi,Ayodele Awokoya,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: MGSM-Pro扩展了MGSM数据集，采用GSM-Symbolic方法为每个问题提供5种不同实例化（改变名称、数字和无关上下文），在多语言评估中发现低资源语言在数字变化时性能显著下降，建议每个问题至少使用5种数字变化实例化以获得更稳健的数学推理评估。


<details>
  <summary>Details</summary>
Motivation: 当前多语言数学推理基准在难度和时效性上落后于英语，且GSM-Symbolic研究表明同一问题的不同实例化会导致模型性能大幅波动，但该评估仅限于英语。需要扩展多语言评估以了解模型在不同语言和问题实例化下的鲁棒性。

Method: 扩展MGSM数据集创建MGSM-Pro，采用GSM-Symbolic方法为每个MGSM问题生成5种不同实例化，通过改变名称、数字和无关上下文来增加多样性。在9种语言上评估模型性能，特别关注数字实例化变化对低资源语言的影响。

Result: 评估发现：1）许多低资源语言在数字实例化不同于原始测试集时性能大幅下降；2）专有模型中Gemini 2.5 Flash和GPT-4.1对数字实例化鲁棒性较差，Claude 4.0 Sonnet更鲁棒；3）开源模型中GPT-OSS 120B和DeepSeek V3表现出更强的鲁棒性。

Conclusion: 为获得更稳健和真实的数学推理评估，建议每个问题至少使用5种数字变化的实例化进行测试。MGSM-Pro数据集为多语言数学推理评估提供了更全面的基准，揭示了模型在不同语言和问题表述下的性能差异。

Abstract: Large language models have made substantial progress in mathematical reasoning. However, benchmark development for multilingual evaluation has lagged behind English in both difficulty and recency. Recently, GSM-Symbolic showed a strong evidence of high variance when models are evaluated on different instantiations of the same question; however, the evaluation was conducted only in English. In this paper, we introduce MGSM-Pro, an extension of MGSM dataset with GSM-Symbolic approach. Our dataset provides five instantiations per MGSM question by varying names, digits and irrelevant context. Evaluations across nine languages reveal that many low-resource languages suffer large performance drops when tested on digit instantiations different from those in the original test set. We further find that some proprietary models, notably Gemini 2.5 Flash and GPT-4.1, are less robust to digit instantiation, whereas Claude 4.0 Sonnet is more robust. Among open models, GPT-OSS 120B and DeepSeek V3 show stronger robustness. Based on these findings, we recommend evaluating each problem using at least five digit-varying instantiations to obtain a more robust and realistic assessment of math reasoning.

</details>


### [16] [SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models](https://arxiv.org/abs/2601.21235)
*Alok Abhishek,Tushar Bandopadhyay,Lisa Erickson*

Main category: cs.CL

TL;DR: SHARP框架用于多维度、分布感知的LLM社会危害评估，通过风险剖面分析揭示平均风险相似模型在尾部风险和波动性上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估基准将复杂社会风险简化为均值标量分数，掩盖了分布结构、跨维度交互和最坏情况行为，而LLM在高风险领域部署需要更全面的风险评估方法。

Method: 提出SHARP框架，将危害建模为多元随机变量，整合偏见、公平、伦理和认知可靠性的显式分解，采用累积对数风险的加性聚合，并使用风险敏感分布统计（特别是CVaR95）来表征最坏情况行为。

Result: 对11个前沿LLM在901个社会敏感提示上的评估显示：平均风险相似的模型在尾部暴露和波动性上存在两倍以上差异；不同危害维度的尾部行为存在系统性差异，偏见维度尾部严重性最强，伦理错配风险最低。

Conclusion: LLM的负责任评估和治理需要超越标量平均值，转向多维度、尾部敏感的风险剖面分析，以揭示被传统基准混淆的异质性、模型依赖的失败结构。

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.

</details>


### [17] [MoCo: A One-Stop Shop for Model Collaboration Research](https://arxiv.org/abs/2601.21257)
*Shangbin Feng,Yuyang Bai,Ziyuan Yang,Yike Wang,Zhaoxuan Tan,Jiajie Yan,Zhenyu Lei,Wenxuan Ding,Weijia Shi,Haojin Wang,Zhenting Qi,Yuru Jiang,Heng Wang,Chengsong Huang,Yu Fei,Jihan Yao,Yilun Du,Luke Zettlemoyer,Yejin Choi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: MoCo是一个用于模型协作的Python库，集成了26种协作方法、25个评估数据集，实验表明61%的情况下协作策略优于单模型，性能提升最高达25.8%。


<details>
  <summary>Details</summary>
Motivation: 现有模型协作研究分散在不同社区，缺乏系统比较和统一框架，需要整合现有研究并建立模型协作作为一个独立的研究方向。

Method: 开发MoCo库，支持路由、文本、logit和模型参数等多种跨模型信息交换方式，提供执行、基准测试和比较模型协作算法的统一平台。

Result: 61%的（模型，数据）设置中协作策略优于单模型，最佳方法性能提升达25.8%；分析了协作策略的扩展性、训练/推理效率，并展示了协作系统能解决单模型难以处理的问题。

Conclusion: MoCo作为促进开放、模块化、去中心化和协作AI未来的重要工具包，为模型协作研究提供了统一框架和实验平台。

Abstract: Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.

</details>


### [18] [CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding](https://arxiv.org/abs/2601.21262)
*Jiahao Huo,Yu Huang,Yibo Yan,Ye Pan,Yi Cao,Mingdong Ou,Philip S. Yu,Xuming Hu*

Main category: cs.CL

TL;DR: CausalEmbed提出一种自回归生成方法构建多向量嵌入，通过迭代边界损失减少视觉token数量30-155倍，在保持竞争力的同时显著降低存储开销


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉文档检索中虽然能生成高质量多向量嵌入，但每个页面需要数千个视觉token表示，导致巨大的存储开销，限制了实际应用

Method: 提出CausalEmbed自回归生成方法，通过迭代边界损失进行对比训练，鼓励嵌入模型学习紧凑且结构良好的表示，仅使用数十个视觉token

Result: 实现了30-155倍的token数量减少，在各种骨干网络和基准测试中保持高度竞争力的性能，训练效率高且测试时可扩展

Conclusion: CausalEmbed为多向量视觉文档检索表示提供了灵活的测试时扩展策略，揭示了多模态文档检索中的生成范式潜力

Abstract: Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.

</details>


### [19] [Qwen3-ASR Technical Report](https://arxiv.org/abs/2601.21337)
*Xian Shi,Xiong Wang,Zhifang Guo,Yongqi Wang,Pei Zhang,Xinyu Zhang,Zishan Guo,Hongkun Hao,Yu Xi,Baosong Yang,Jin Xu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-ASR系列包含两个全功能语音识别模型和一个非自回归语音强制对齐模型，在52种语言上实现SOTA性能，并在效率和准确性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型在开源基准测试中差异不大，但在实际场景中质量差异显著。需要开发在真实世界表现优异且高效的语音识别模型，同时提供文本-语音对齐功能。

Method: 基于Qwen3-Omni基础模型的音频理解能力，利用大规模语音训练数据开发ASR模型。采用非自回归架构构建强制对齐模型，支持多语言识别和对齐功能。

Result: 1.7B版本在开源ASR模型中达到SOTA性能，与最强专有API竞争；0.6B版本提供最佳精度-效率平衡（平均TTFT低至92ms）。强制对齐模型在11种语言上超越三个最强对齐模型，效率和通用性更优。

Conclusion: Qwen3-ASR系列模型在性能、效率和多功能性方面表现优异，通过Apache 2.0许可证开源，将加速ASR和音频理解领域的研究进展。

Abstract: In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.

</details>


### [20] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 提出一种新的预训练方法，使用强化学习在生成过程中实时改进模型输出质量，相比标准预训练在事实性和安全性方面有显著提升


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全性和事实性保障主要依赖昂贵的微调和对齐流程，但这些方法无法纠正预训练阶段学习到的错误模式。需要在预训练阶段就解决这些问题，因为预训练塑造了模型的核心行为，防止不安全或幻觉输出被深度嵌入

Method: 提出新的预训练方法：流式处理文档，使用强化学习改进每个步骤生成的下K个token。使用一个经过后训练的强模型来评估候选生成（包括模型rollout、原始后缀和重写后缀）的质量、安全性和事实性。训练早期依赖原始和重写后缀，随着模型改进，RL奖励高质量rollout

Result: 相比标准预训练，在事实性方面获得36.2%的相对改进，在安全性方面获得18.5%的相对改进，在整体生成质量方面获得高达86.3%的胜率改进

Conclusion: 该方法从基础层面构建了更高质量、更安全、更真实的模型，通过在预训练阶段直接解决质量和安全问题，比传统微调方法更有效

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [21] [The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation](https://arxiv.org/abs/2601.21360)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Vinay Chamola,Dhruv Kumar*

Main category: cs.CL

TL;DR: LLMs在代码评分中存在系统性漏洞：过度遵循指令导致忽略代码逻辑，即使代码功能错误也会给出高分


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在代码评分中的假设：指令遵循能力是否能直接转化为客观评判能力。当前教育评估系统依赖这一未经验证的假设。

Method: 提出SPACI框架和AST-ASIP协议，通过在AST的trivia节点中嵌入对抗性指令，利用语法-语义鸿沟。大规模评估9个SOTA模型在25,000个Python、C、C++、Java提交中的表现。

Result: 发现灾难性失败率（>95%），特别是DeepSeek-V3等高容量开源模型会优先考虑隐藏的格式约束而非代码正确性。使用新的三重框架量化失败：解耦概率、分数分歧、教学严重性。

Conclusion: 当前对齐范式在自动评分中创建了"特洛伊"漏洞，需要从标准RLHF转向领域特定的裁决鲁棒性，让模型优先考虑证据而非指令遵循。

Abstract: The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread "False Certification" of functionally broken code. Our findings suggest that current alignment paradigms create a "Trojan" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.

</details>


### [22] [User-Centric Evidence Ranking for Attribution and Fact Verification](https://arxiv.org/abs/2601.21387)
*Guy Alt,Eran Hirsch,Serwar Basch,Ido Dagan,Oren Glickman*

Main category: cs.CL

TL;DR: 本文提出证据排序新任务，通过优化证据呈现顺序来减少用户阅读负担，同时保持验证准确性。研究比较了一次性排序和增量排序两种方法，并引入新的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前自动系统和大型语言模型在事实验证中提供的证据要么不足要么冗余，导致验证效率低下且容易出错。需要一种能优先呈现足够信息同时最小化用户阅读负担的方法。

Method: 提出证据排序任务，比较一次性排序和增量排序两种方法。引入基于信息检索指标的新评估框架，并整合现有事实验证数据集构建统一基准。使用多样化模型进行实验，并进行受控用户研究。

Result: 增量排序策略能更好地捕捉互补证据，基于LLM的方法优于浅层基线。证据排序相比证据选择能减少阅读负担并提高验证准确性，但在平衡充分性和冗余性方面仍面临挑战。

Conclusion: 证据排序任务为构建更可解释、高效且用户对齐的信息验证系统提供了基础步骤，展示了优化证据呈现顺序在事实验证中的重要性。

Abstract: Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, they often present users with either insufficient or overly redundant information, leading to inefficient and error-prone verification. To address this, we propose Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list. This minimizes user reading effort while still making all available evidence accessible for sequential verification. We compare two approaches for the new ranking task: one-shot ranking and incremental ranking. We introduce a new evaluation framework, inspired by information retrieval metrics, and construct a unified benchmark by aggregating existing fact verification datasets. Extensive experiments with diverse models show that incremental ranking strategies better capture complementary evidence and that LLM-based methods outperform shallower baselines, while still facing challenges in balancing sufficiency and redundancy. Compared to evidence selection, we conduct a controlled user study and demonstrate that evidence ranking both reduces reading effort and improves verification. This work provides a foundational step toward more interpretable, efficient, and user-aligned information verification systems.

</details>


### [23] [Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation](https://arxiv.org/abs/2601.21464)
*Yuan Sui,Bryan Hooi*

Main category: cs.CL

TL;DR: CoNL框架通过多智能体自博弈统一生成、评估和元评估，利用"能帮助他人改进的批评才是好批评"的洞察，在没有外部评估者或真实标签的情况下联合优化生成和评估能力。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型进行不可验证任务（如创意写作、对话、伦理推理）面临缺乏真实标签的挑战。LLM-as-Judge方法虽然可扩展，但受限于评估者自身质量：如果评估者无法识别好方案，就无法提供有用的训练信号，且评估偏见（如偏爱冗长而非质量）无法解决，这促使需要元评估来评估和改进评估者本身。

Method: CoNL框架通过多智能体自博弈统一生成、评估和元评估。多个共享相同策略的智能体参与结构化对话，提出方案、批评和修订方案。关键洞察：批评质量可以通过它是否帮助他人改进方案来衡量。能够促进方案改进的批评获得诊断奖励，为元评估创建显式监督，并通过自博弈联合优化生成和评估能力，无需外部评估者或真实标签。

Result: 在五个基准测试上的实验表明，CoNL相比自我奖励基线实现了持续改进，同时保持训练稳定性。

Conclusion: CoNL通过多智能体自博弈框架解决了LLM训练中评估者质量限制的问题，实现了在没有外部监督的情况下联合优化生成和评估能力，为不可验证任务的训练提供了有效解决方案。

Abstract: Training large language models (LLMs) for non-verifiable tasks, such as creative writing, dialogue, and ethical reasoning, remains challenging due to the absence of ground-truth labels. While LLM-as-Judge approaches offer a scalable alternative to human feedback, they face a fundamental limitation: performance is constrained by the evaluator's own quality. If the judge cannot recognize good solutions, it cannot provide useful training signals, and evaluation biases (e.g., favoring verbosity over quality) remain unaddressed. This motivates meta-evaluation: the ability to evaluate and improve the evaluator itself. We introduce CoNL, a framework that unifies generation, evaluation, and meta-evaluation through multi-agent self-play. Our key insight: critique quality can be measured by whether it helps others improve their solutions. In CoNL, multiple agents sharing the same policy engage in structured conversations to propose, critique, and revise solutions. Critiques that enable solution improvements earn a diagnostic reward, creating explicit supervision for meta-evaluation and enabling joint optimization of generation and judging capabilities through self-play, without external judges or ground truth. Experiments on five benchmarks show that CoNL achieves consistent improvements over self-rewarding baselines while maintaining stable training.

</details>


### [24] [SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.21476)
*Lei Yang,Wei Bi,Chenxi Sun,Renren Jin,Deyi Xiong*

Main category: cs.CL

TL;DR: SOUP是一种新的RL框架，在token级别统一了离线和在线策略学习，通过限制离线策略影响在序列前缀，使用在线策略生成后续内容，解决了传统方法探索不足和策略不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型后训练中广泛使用的在线RL方法（如GRPO）存在探索有限和早期饱和的问题，而现有的离线策略混合方法会导致显著的策略不匹配和不稳定性。

Method: 提出SOUP框架，在单个样本的token级别统一离线和在线学习：将离线策略影响限制在从历史策略采样的生成序列前缀部分，后续部分使用在线策略生成，并通过token级别的重要性比率有效利用离线信息。

Result: 大量实验表明，SOUP在性能上持续优于标准的在线策略训练和现有的离线策略扩展方法，能够同时改善探索和最终性能。

Conclusion: SOUP通过细粒度的单样本混合策略训练，有效解决了语言模型RL中的探索不足和策略不匹配问题，为离线和在线策略学习的统一提供了新范式。

Abstract: On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\textbf{S}$ingle-sample Mix-p$\textbf{O}$licy $\textbf{U}$nified $\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.

</details>


### [25] [DimStance: Multilingual Datasets for Dimensional Stance Analysis](https://arxiv.org/abs/2601.21483)
*Jonas Becker,Liang-Chih Yu,Shamsuddeen Hassan Muhammad,Jan Philip Wahle,Terry Ruas,Idris Abdulmumin,Lung-Hao Lee,Wen-Ni Liu,Tzu-Mi Lin,Zhe-Yu Xu,Ying-Lung Lin,Jin Wang,Maryam Ibrahim Mukhtar,Bela Gipp,Saif M. Mohammed*

Main category: cs.CL

TL;DR: DimStance：首个维度立场资源，将立场检测从分类扩展为情感维度（效价-唤醒度）的连续值预测，支持多语言跨领域分析。


<details>
  <summary>Details</summary>
Motivation: 传统立场检测仅使用分类标签（支持/中立/反对），无法捕捉立场表达背后的细微情感状态。需要引入情感科学框架，通过连续的情感维度（效价和唤醒度）实现更细粒度的立场分析。

Method: 1. 创建DimStance资源：包含11,746个目标方面、7,365篇文本，涵盖5种语言（英、德、中、尼日利亚皮钦语、斯瓦希里语）和2个领域（政治、环保）。2. 提出维度立场回归任务：预测效价和唤醒度的连续值。3. 评估预训练模型和大型语言模型在回归和提示设置下的表现。

Result: 1. 微调的LLM回归器表现竞争力强。2. 低资源语言仍面临挑战。3. 基于token的生成方法存在局限性。4. 揭示了跨语言的效价-唤醒度模式。

Conclusion: DimStance为多语言、情感感知的立场分析提供了基础资源，推动了从分类到连续维度分析的范式转变，有助于更细致地理解立场表达的情感基础。

Abstract: Stance detection is an established task that classifies an author's attitude toward a specific target into categories such as Favor, Neutral, and Against. Beyond categorical stance labels, we leverage a long-established affective science framework to model stance along real-valued dimensions of valence (negative-positive) and arousal (calm-active). This dimensional approach captures nuanced affective states underlying stance expressions, enabling fine-grained stance analysis. To this end, we introduce DimStance, the first dimensional stance resource with valence-arousal (VA) annotations. This resource comprises 11,746 target aspects in 7,365 texts across five languages (English, German, Chinese, Nigerian Pidgin, and Swahili) and two domains (politics and environmental protection). To facilitate the evaluation of stance VA prediction, we formulate the dimensional stance regression task, analyze cross-lingual VA patterns, and benchmark pretrained and large language models under regression and prompting settings. Results show competitive performance of fine-tuned LLM regressors, persistent challenges in low-resource languages, and limitations of token-based generation. DimStance provides a foundation for multilingual, emotion-aware, stance analysis and benchmarking.

</details>


### [26] [MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset](https://arxiv.org/abs/2601.21512)
*Serry Sibaee,Yasser Alhabashi,Nadia Sibai,Yara Farouk,Adel Ammar,Sawsan AlHalawani,Wadii Boulila*

Main category: cs.CL

TL;DR: 提出了MURAD数据集，包含96,243个阿拉伯语词-定义对，覆盖多个领域，支持计算语言学和词典学研究


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语词汇丰富，涵盖科学、宗教、文学等多个领域，但大规模连接阿拉伯语词汇与精确定义的数据集仍然有限

Method: 使用混合流水线方法，整合直接文本解析、光学字符识别和自动化重建，从可信的参考书和教育资源中提取数据

Result: 创建了MURAD数据集，包含96,243个词-定义对，每个记录包含目标词、标准化阿拉伯语定义和源领域元数据，覆盖语言学、伊斯兰研究、数学、物理、心理学和工程学等领域

Conclusion: 通过发布这一资源，旨在推进阿拉伯语自然语言处理，促进阿拉伯语词汇语义学的可重复研究，支持反向词典建模、语义检索和教育工具等应用

Abstract: Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.

</details>


### [27] [LMK > CLS: Landmark Pooling for Dense Embeddings](https://arxiv.org/abs/2601.21525)
*Meet Doshi,Aashka Trivedi,Vishwajeet Kumar,Parul Awasthy,Yulong Li,Jaydeep Sen,Radu Florian,Sachindra Joshi*

Main category: cs.CL

TL;DR: 本文提出Landmark (LMK) pooling方法，通过将序列分块并在块间插入landmark tokens，然后对这些token嵌入进行平均池化，解决了传统[CLS]和mean pooling在长上下文任务中的系统性问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列编码器通常使用[CLS] token或mean pooling将变长token序列压缩为单个向量，但这些方法存在系统性弱点：[CLS]倾向于将信息集中在序列初始位置，可能无法充分表示分布式证据；mean pooling可能稀释重要的局部信号，在短上下文任务中表现更差。

Method: 提出Landmark (LMK) pooling方法：1) 将序列划分为多个块；2) 在块之间插入landmark tokens；3) 通过对landmark token嵌入进行平均池化形成最终表示。这种方法在引入少量特殊token的情况下，改善了长上下文外推能力而不牺牲局部显著特征。

Result: 实验表明：LMK pooling在短上下文检索任务上与现有方法表现相当，在长上下文任务上带来显著改进，使其成为现有池化方法的实用且可扩展的替代方案。

Conclusion: LMK pooling通过简单的机制有效解决了传统池化策略在长上下文任务中的局限性，平衡了局部特征保留和全局信息整合，为序列表示学习提供了更优的解决方案。

Abstract: Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.

</details>


### [28] [inversedMixup: Data Augmentation via Inverting Mixed Embeddings](https://arxiv.org/abs/2601.21543)
*Fanshuang Kong,Richong Zhang,Qiyu Sun,Zhijie Nie,Ting Deng,Chunming Hu*

Main category: cs.CL

TL;DR: 提出inversedMixup框架，将Mixup的可控性与LLM生成的可解释性结合，通过三阶段训练对齐任务模型与LLM的嵌入空间，生成可读的增强文本并缓解流形入侵问题。


<details>
  <summary>Details</summary>
Motivation: Mixup在嵌入层面生成增强样本但不可解释，LLM增强方法可生成可读文本但控制有限。需要结合两者的优势，同时解决文本Mixup中的流形入侵问题。

Method: 采用三阶段训练：1) 训练任务特定模型；2) 对齐任务模型输出嵌入与LLM输入嵌入空间；3) 通过LLM反演将混合嵌入重构为可读句子。提出缓解流形入侵的简单策略。

Result: 实验证明方法在少样本和全监督场景下有效且具有泛化性，首次为文本Mixup中的流形入侵现象提供实证证据，并成功缓解该问题。

Conclusion: inversedMixup成功结合了Mixup的可控性与LLM生成的可解释性，通过嵌入空间对齐实现了可读的文本增强，同时解决了流形入侵问题，为文本数据增强提供了新思路。

Abstract: Mixup generates augmented samples by linearly interpolating inputs and labels with a controllable ratio. However, since it operates in the latent embedding level, the resulting samples are not human-interpretable. In contrast, LLM-based augmentation methods produce sentences via prompts at the token level, yielding readable outputs but offering limited control over the generation process. Inspired by recent advances in LLM inversion, which reconstructs natural language from embeddings and helps bridge the gap between latent embedding space and discrete token space, we propose inversedMixup, a unified framework that combines the controllability of Mixup with the interpretability of LLM-based generation. Specifically, inversedMixup adopts a three-stage training procedure to align the output embedding space of a task-specific model with the input embedding space of an LLM. Upon successful alignment, inversedMixup can reconstruct mixed embeddings with a controllable mixing ratio into human-interpretable augmented sentences, thereby improving the augmentation performance. Additionally, inversedMixup provides the first empirical evidence of the manifold intrusion phenomenon in text Mixup and introduces a simple yet effective strategy to mitigate it. Extensive experiments demonstrate the effectiveness and generalizability of our approach in both few-shot and fully supervised scenarios.

</details>


### [29] [Note2Chat: Improving LLMs for Multi-Turn Clinical History Taking Using Medical Notes](https://arxiv.org/abs/2601.21551)
*Yang Zhou,Zhenting Sheng,Mingrui Tan,Yuting Song,Jun Zhou,Yu Heng Kwan,Lian Leng Low,Yang Bai,Yong Liu*

Main category: cs.CL

TL;DR: Note2Chat框架通过将医疗记录转换为医患对话，训练LLM进行结构化病史采集和诊断，显著提升临床推理能力


<details>
  <summary>Details</summary>
Motivation: 临床病史采集是临床推理的基础但研究不足。现有LLM在静态基准测试中表现良好，但在需要迭代提问和假设修正的动态多轮诊断场景中表现不佳。缺乏敏感对话数据限制了模型训练。

Method: 提出Note2Chat框架：1) 使用决策树引导的生成和精炼管道将真实医疗记录转换为高质量医患对话；2) 采用三阶段微调策略（监督学习、模拟数据增强、偏好学习）；3) 提出单轮推理范式，将病史采集重构为一系列单轮推理问题。

Result: 方法显著改善临床推理能力，相比GPT-4o提升16.9 F1分数和21.0 Top-1诊断准确率。代码和数据集已开源。

Conclusion: Note2Chat框架通过利用广泛可得的医疗记录而非稀缺的敏感对话数据，有效训练LLM进行结构化病史采集和诊断，为临床推理提供了可解释、高效且适应性强的解决方案。

Abstract: Effective clinical history taking is a foundational yet underexplored component of clinical reasoning. While large language models (LLMs) have shown promise on static benchmarks, they often fall short in dynamic, multi-turn diagnostic settings that require iterative questioning and hypothesis refinement. To address this gap, we propose \method{}, a note-driven framework that trains LLMs to conduct structured history taking and diagnosis by learning from widely available medical notes. Instead of relying on scarce and sensitive dialogue data, we convert real-world medical notes into high-quality doctor-patient dialogues using a decision tree-guided generation and refinement pipeline. We then propose a three-stage fine-tuning strategy combining supervised learning, simulated data augmentation, and preference learning. Furthermore, we propose a novel single-turn reasoning paradigm that reframes history taking as a sequence of single-turn reasoning problems. This design enhances interpretability and enables local supervision, dynamic adaptation, and greater sample efficiency. Experimental results show that our method substantially improves clinical reasoning, achieving gains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over GPT-4o. Our code and dataset can be found at https://github.com/zhentingsheng/Note2Chat.

</details>


### [30] [ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas](https://arxiv.org/abs/2601.21558)
*Xiaoyu Tian,Haotian Wang,Shuaiting Chen,Hao Zhou,Kaichi Yu,Yudian Zhang,Jade Ouyang,Junxi Yin,Jiong Chen,Baoyan Guo,Lei Zhang,Junjie Tao,Yuansheng Song,Ming Cui,Chengwei Liu*

Main category: cs.CL

TL;DR: ASTRA是一个自动化框架，通过可扩展的数据合成和可验证的强化学习来训练工具增强的语言模型代理，解决了现有方法在长视野、多轮学习中的稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强代理训练方法存在多个问题：需要人工干预、依赖不可验证的模拟环境、仅使用监督微调或强化学习中的一种、难以稳定进行长视野多轮学习。

Method: ASTRA包含两个组件：1) 基于工具调用图静态拓扑的数据合成管道，生成多样化、结构化的轨迹；2) 环境合成框架，将分解的问题-答案轨迹转换为可执行、可验证的环境。采用统一的训练方法，结合监督微调和在线强化学习。

Result: 在多个代理工具使用基准测试中，ASTRA训练的模型在同等规模下达到最先进性能，接近闭源系统，同时保持核心推理能力。

Conclusion: ASTRA提供了一个完全自动化的端到端框架，通过可扩展的数据合成和可验证的强化学习，有效训练工具增强的语言模型代理，解决了现有方法的局限性。

Abstract: Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.

</details>


### [31] [KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices](https://arxiv.org/abs/2601.21579)
*Wuyang Zhou,Yuxuan Gu,Giorgos Iacovides,Danilo Mandic*

Main category: cs.CL

TL;DR: 提出KromHC方法，通过小双随机矩阵的Kronecker积参数化残差矩阵，解决现有超连接方法训练不稳定、参数复杂度高的问题，在保证双随机性的同时大幅降低参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有超连接方法存在训练不稳定和可扩展性受限的问题。mHC通过Birkhoff多面体投影缓解这些问题，但仍面临两个挑战：1）迭代SK算法不能总是产生精确的双随机残差矩阵；2）参数复杂度高达O(n³C)。mHC-lite虽然保证双随机性，但参数复杂度呈阶乘爆炸。

Method: 提出KromHC方法，使用小双随机矩阵的Kronecker积来参数化mHC中的残差矩阵。通过对张量化残差流的每个模式施加流形约束，保证残差矩阵的精确双随机性，同时将参数复杂度降低到O(n²C)。

Result: 综合实验表明，KromHC在性能上匹配甚至优于最先进的mHC变体，同时需要显著更少的可训练参数。

Conclusion: KromHC成功解决了现有超连接方法的双随机性保证和参数复杂度问题，在保持性能的同时大幅降低了计算开销，为高效超连接提供了实用解决方案。

Abstract: The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\mathcal{O} \left( nC \cdot n! \right)$. To address both challenges, we propose \textbf{KromHC}, which uses the \underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \texttt{https://github.com/wz1119/KromHC}.

</details>


### [32] [Language Models as Artificial Learners: Investigating Crosslinguistic Influence](https://arxiv.org/abs/2601.21587)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 该研究使用语言模型作为受控统计学习器来系统模拟跨语言影响，发现语言优势和熟练度是CLI的主要预测因子，并揭示了L1在L2处理中的共激活机制。


<details>
  <summary>Details</summary>
Motivation: 人类研究中跨语言影响（CLI）的结果常常相互矛盾，主要源于实验方差。需要一种受控的方法来系统模拟CLI并分离其根本驱动因素。

Method: 使用语言模型作为受控统计学习器，通过操纵L2暴露年龄（训练步数）来控制L2熟练度，研究不同句法距离的L1预训练影响，采用跨语言启动范式分析L1结构激活对L2处理的影响。

Result: 语言优势和熟练度是CLI的强预测因子；语法结构的启动是双向的，但不合语法结构的启动对语言优势敏感；L1在L2处理过程中被共激活并直接影响L2的神经回路。

Conclusion: 语言模型可作为计算框架来验证人类跨语言影响理论，为CLI研究提供了受控实验平台，揭示了语言优势和熟练度在跨语言影响中的核心作用。

Abstract: Despite the centrality of crosslinguistic influence (CLI) to bilingualism research, human studies often yield conflicting results due to inherent experimental variance. We address these inconsistencies by using language models (LMs) as controlled statistical learners to systematically simulate CLI and isolate its underlying drivers. Specifically, we study the effect of varying the L1 language dominance and the L2 language proficiency, which we manipulate by controlling the L2 age of exposure -- defined as the training step at which the L2 is introduced. Furthermore, we investigate the impact of pretraining on L1 languages with varying syntactic distance from the L2. Using cross-linguistic priming, we analyze how activating L1 structures impacts L2 processing. Our results align with evidence from psycholinguistic studies, confirming that language dominance and proficiency are strong predictors of CLI. We further find that while priming of grammatical structures is bidirectional, the priming of ungrammatical structures is sensitive to language dominance. Finally, we provide mechanistic evidence of CLI in LMs, demonstrating that the L1 is co-activated during L2 processing and directly influences the neural circuitry recruited for the L2. More broadly, our work demonstrates that LMs can serve as a computational framework to inform theories of human CLI.

</details>


### [33] [ILRR: Inference-Time Steering Method for Masked Diffusion Language Models](https://arxiv.org/abs/2601.21647)
*Eden Avrahami,Eliya Nachmani*

Main category: cs.CL

TL;DR: 提出ILRR框架，通过单参考序列动态对齐内部激活来引导离散扩散语言模型的生成，实现无学习推理控制


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型在推理时控制机制相对不足，现有方法包括采样级引导或轨迹优化，需要更有效的控制框架

Method: 提出迭代潜在表示精炼(ILRR)框架，通过在整个去噪过程中动态对齐生成序列与参考序列的内部激活来引导生成；引入空间调制引导扩展，用短参考引导长文本

Result: ILRR在LLaDA和MDLM架构上实现有效属性引导，计算开销小（每去噪步仅需额外一次并行前向传播）；相同计算预算下，属性准确率比基线提高10-60个百分点

Conclusion: ILRR为离散扩散语言模型提供了一种有效、灵活且计算高效的推理时控制框架，能够通过单参考序列实现高质量的属性引导

Abstract: Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\%$ to 60$\%$ points, while maintaining high generation quality.

</details>


### [34] [AdaptBPE: From General Purpose to Specialized Tokenizers](https://arxiv.org/abs/2601.21665)
*Vijini Liyanage,François Yvon*

Main category: cs.CL

TL;DR: 提出一种后训练自适应方法，通过替换低效用token来优化BPE分词器，使其在特定领域或语言中更高效


<details>
  <summary>Details</summary>
Motivation: 标准的通用分词器在处理特定领域或语言时效率低下，需要一种轻量级适应机制来优化分词效果

Method: 提出后训练适应策略，基于适应语料库中token的频率，选择性地用更相关的token替换低效用token，算法识别最能有效编码适应语料库的token集合

Result: 在多语言生成和分类任务上的实验表明，适应后的分词器在相同词汇量下比基线方法更有效地压缩测试语料库

Conclusion: 该方法作为一种轻量级适应机制，类似于词汇微调过程，能够为特定领域或任务优化分词效果

Abstract: Subword tokenization methods, such as Byte-Pair Encoding (BPE), significantly impact the performance and efficiency of large language models (LLMs). The standard approach involves training a general-purpose tokenizer that uniformly processes all textual data during both training and inference. However, the use of a generic set of tokens can incur inefficiencies when applying the model to specific domains or languages. To address this limitation, we propose a post-training adaptation strategy that selectively replaces low-utility tokens with more relevant ones based on their frequency in an adaptation corpus. Our algorithm identifies the token inventory that most effectively encodes the adaptation corpus for a given target vocabulary size. Extensive experiments on generation and classification tasks across multiple languages demonstrate that our adapted tokenizers compress test corpora more effectively than baselines using the same vocabulary size. This method serves as a lightweight adaptation mechanism, akin to a vocabulary fine-tuning process, enabling optimized tokenization for specific domains or tasks. Our code and data are available at https://github.com/vijini/Adapt-BPE.git.

</details>


### [35] [Scale-Dependent Semantic Dynamics Revealed by Allan Deviation](https://arxiv.org/abs/2601.21678)
*Debayan Dasgupta*

Main category: cs.CL

TL;DR: 使用Allan偏差分析文本语义动态，发现人类文本与AI生成文本在短时幂律标度和长时稳定性上存在差异


<details>
  <summary>Details</summary>
Motivation: 理解语言语义进展的底层动态机制，将文本语义进展视为高维状态空间中的随机轨迹，探索人类认知与算法模型生成模式的差异

Method: 将有序句子嵌入视为位移信号，使用精密计量学中的Allan偏差工具分析语义稳定性，识别不同的动态机制

Result: 发现两个不同的动态机制：短时幂律标度（区分创意文学与技术文本）和长时交叉到稳定性限制的噪声基底；大语言模型能模仿人类文本的局部标度统计，但稳定性视界系统性降低

Conclusion: 语义连贯性是可测量的物理属性，该框架能区分人类认知与算法模型的微妙动态差异

Abstract: While language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.

</details>


### [36] [FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning](https://arxiv.org/abs/2601.21682)
*Xiaoyu Xu,Minxin Du,Kun Fang,Zi Liang,Yaxin Xiao,Zhicong Huang,Cheng Hong,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: FIT是一个持续遗忘框架，通过数据过滤、重要性感知更新和针对性层归因，处理大规模删除请求，防止灾难性遗忘和遗忘后恢复，在PCH基准测试中表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘方法很少考虑现实世界中持续、高容量的删除请求特性，导致随着请求积累出现效用退化和灾难性遗忘问题。

Method: 提出FIT框架，包含三个核心组件：严格的数据过滤、重要性感知更新和针对性层归因，以稳定处理长序列的遗忘操作。

Result: 在四个开源LLM上进行了数百个删除请求的实验，FIT在F.D.（遗忘度）和R.U.（保留效用）之间取得了最佳平衡，在MMLU、CommonsenseQA和GSM8K上超越现有方法，并能抵抗重新学习和量化恢复攻击。

Conclusion: FIT框架能够有效处理大规模持续遗忘请求，在保持模型效用的同时实现有效遗忘，为解决LLM隐私、版权和有害内容问题提供了实用解决方案。

Abstract: Large language models (LLMs) demonstrate impressive capabilities across diverse tasks but raise concerns about privacy, copyright, and harmful materials. Existing LLM unlearning methods rarely consider the continual and high-volume nature of real-world deletion requests, which can cause utility degradation and catastrophic forgetting as requests accumulate. To address this challenge, we introduce \fit, a framework for continual unlearning that handles large numbers of deletion requests while maintaining robustness against both catastrophic forgetting and post-unlearning recovery. \fit mitigates degradation through rigorous data \underline{F}iltering, \underline{I}mportance-aware updates, and \underline{T}argeted layer attribution, enabling stable performance across long sequences of unlearning operations and achieving a favorable balance between forgetting effectiveness and utility retention. To support realistic evaluation, we present \textbf{PCH}, a benchmark covering \textbf{P}ersonal information, \textbf{C}opyright, and \textbf{H}armful content in sequential deletion scenarios, along with two symmetric metrics, Forget Degree (F.D.) and Retain Utility (R.U.), which jointly assess forgetting quality and utility preservation. Extensive experiments on four open-source LLMs with hundreds of deletion requests show that \fit achieves the strongest trade-off between F.D. and R.U., surpasses existing methods on MMLU, CommonsenseQA, and GSM8K, and remains resistant against both relearning and quantization recovery attacks.

</details>


### [37] [Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.21684)
*Xinglin Wang,Jiayi Shi,Shaoxiong Feng,Peiwen Yuan,Yiwei Li,Yueqi Zhang,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.CL

TL;DR: RSE是一种训练免费的自引导策略，通过将原始轨迹提炼到共享经验库中，实现中间结论的正向回收和失败模式的负向回收，显著提升大语言模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法将每次推理尝试视为独立样本，导致大量计算冗余，因为模型会重复推导已发现的结论并重新访问已知的死胡同。

Method: 提出回收搜索经验（RSE）策略，通过主动将原始轨迹提炼到共享经验库中，实现中间结论的正向回收以跳过冗余推导，以及失败模式的负向回收以修剪遇到的死胡同。

Result: 在HMMT24、HMMT25、IMO-Bench和HLE等基准测试中，RSE在相同计算成本下持续优于强基线方法，实现了最先进的扩展效率。

Conclusion: RSE将测试时搜索从一系列孤立试验转变为累积过程，通过回收搜索经验显著减少计算冗余，为复杂推理任务提供了高效的解决方案。

Abstract: Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.

</details>


### [38] [Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents](https://arxiv.org/abs/2601.21699)
*Hojae Han,Heeyun Jung,Jongyoon Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: DAVID-GRPO：一个针对资源受限环境下小型语言模型多跳推理的预算高效RL框架，通过最小监督稳定早期学习、基于证据召回分配检索信用、重采样截断轨迹改进探索，在1.5B参数模型上仅用4个RTX 3090 GPU实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在资源受限环境下（无法支持大模型或密集探索）存在挑战：小型语言模型代理陷入低成本低准确率困境，有限的rollout预算导致稀疏探索、稀疏信用分配和不稳定训练。需要打破这种权衡，让小型模型在资源约束下也能实现强大多跳推理。

Method: 提出DAVID-GRPO框架：1）用最小监督稳定早期学习；2）基于证据召回分配检索信用；3）通过重采样截断的"近失"轨迹改进探索。该方法专为资源受限环境设计，支持在有限GPU资源上训练小型模型。

Result: 在六个多跳QA基准测试中，DAVID-GRPO在仅使用4个RTX 3090 GPU训练、参数最多1.5B的代理上，始终优于先前为大规模设置设计的RL方法，实现了低成本训练与高准确率的平衡。

Conclusion: 通过正确的归纳偏置，小型代理可以在资源约束下实现低成本训练与高准确率，打破了现有RL方法在资源受限环境中的性能限制，为实际部署提供了可行方案。

Abstract: While reinforcement learning (RL) has empowered multi-turn reasoning agents with retrieval and tools, existing successes largely depend on extensive on-policy rollouts in high-cost, high-accuracy regimes. Under realistic resource constraints that cannot support large models or dense explorations, however, small language model agents fall into a low-cost, low-accuracy regime, where limited rollout budgets lead to sparse exploration, sparse credit assignment, and unstable training. In this work, we challenge this trade-off and show that small language models can achieve strong multi-hop reasoning under resource constraints. We introduce DAVID-GRPO, a budget-efficient RL framework that (i) stabilizes early learning with minimal supervision, (ii) assigns retrieval credit based on evidence recall, and (iii) improves exploration by resampling truncated near-miss trajectories. Evaluated on agents up to 1.5B parameters trained on only four RTX 3090 GPUs, DAVID-GRPO consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks. These results show that with the right inductive biases, small agents can achieve low training cost with high accuracy.

</details>


### [39] [Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning](https://arxiv.org/abs/2601.21700)
*Wonduk Seo,Wonseok Choi,Junseo Koh,Juhyeon Lee,Hyunjin An,Minhyeong Yu,Jian Park,Qingshan Zhou,Seunghyun Lee,Yi Bu*

Main category: cs.CL

TL;DR: OG-MAR是一个本体指导的多智能体推理框架，通过构建全球文化本体论和价值-人物智能体，提升LLMs在文化敏感决策中的对齐性、一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文化敏感决策中存在偏差，主要原因是预训练数据倾斜且缺乏结构化价值表示。现有方法缺乏人口统计基础，将价值视为独立、非结构化的信号，导致一致性和可解释性不足。

Method: 提出OG-MAR框架：1) 从世界价值观调查中总结受访者特定价值；2) 通过能力问题在固定分类法上构建全球文化本体论；3) 推理时检索本体一致关系和人口统计相似档案，实例化多个价值-人物智能体；4) 由判断智能体综合输出，强制执行本体一致性和人口统计接近性。

Result: 在四个LLM骨干网络上的区域社会调查基准测试表明，OG-MAR相比竞争基线提高了文化对齐性和鲁棒性，同时产生更透明的推理轨迹。

Conclusion: OG-MAR通过本体指导的多智能体推理框架，有效解决了LLMs在文化敏感决策中的对齐问题，提供了更一致、可解释且具有人口统计基础的价值表示方法。

Abstract: Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.

</details>


### [40] [Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis](https://arxiv.org/abs/2601.21709)
*Qingyue Yang,Jie Wang,Xing Li,Yinqi Bai,Xialiang Tong,Huiling Zhen,Jianye Hao,Mingxuan Yuan,Bin Li*

Main category: cs.CL

TL;DR: TAPPA框架通过时间连续性视角分析注意力模式的数学公式，将注意力模式分为可预测和不可预测两类，并基于此指导KV缓存压缩和LLM剪枝任务。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现了检索头、汇头、对角线轨迹等个体注意力模式，但这些观察是碎片化的，缺乏统一的解释框架。

Method: 提出TAPPA框架，从时间连续性视角分析注意力模式的数学公式，将模式分为可预测和不可预测两类，并通过查询在时间维度上的自相似性来解释这种区别。对可预测模式进一步通过查询、键和RoPE的联合效应进行详细数学分析。

Result: TAPPA框架在KV缓存压缩和LLM剪枝任务中得到验证，基于TAPPA启发的简单指标在这些任务中持续优于基线方法。

Conclusion: TAPPA提供了一个统一的框架来解释多样化的注意力模式，不仅加深了对注意力行为的理解，还能指导推理加速方法。

Abstract: Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce \textbf{Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations} from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.

</details>


### [41] [TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2601.21711)
*Huiyuan Lai,Malvina Nissim*

Main category: cs.CL

TL;DR: TACLer是一个为LLMs设计的课程强化学习框架，通过渐进式训练和混合思考/非思考推理模式，在减少计算成本的同时提升数学推理任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在复杂推理任务中需要大规模RL训练来生成长链式思考，但这会导致计算成本高和过度思考问题。需要一种更高效的学习和推理方法，既能减少计算开销又能保持或提升性能。

Method: TACLer包含两个核心组件：1) 定制化课程学习，根据模型在不同阶段的能力逐步增加数据复杂度；2) 混合思考/非思考推理范式，通过开关思考模式来平衡准确性和效率。

Result: 实验显示TACLer在学习和推理方面具有双重优势：1) 减少计算成本，训练计算量比长思考模型减少50%以上，推理token使用比基础模型减少42%以上；2) 在四个数学数据集上准确率提升超过9%，优于最先进的非思考和思考基线模型。

Conclusion: TACLer通过课程强化学习和混合推理范式，实现了在减少计算开销的同时提升LLMs在复杂数学推理任务上的性能，为高效LLM训练和推理提供了有效解决方案。

Abstract: Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.

</details>


### [42] [Enhancing Language Models for Robust Greenwashing Detection](https://arxiv.org/abs/2601.21722)
*Neil Heinrich Braun,Keane Ong,Rui Mao,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 提出参数高效框架，通过对比学习和序数排序目标结构化LLM潜在空间，结合门控特征调制和MetaGradNorm，提升ESG报告中绿色漂洗和模糊声明的检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ESG评估中的可持续性报告常因绿色漂洗和模糊声明而不可靠，现有NLP模型缺乏对这些实践的鲁棒性，通常依赖表层模式且泛化能力差。

Method: 参数高效框架：结合对比学习和序数排序目标结构化LLM潜在空间，捕捉具体行动与模糊声明之间的分级区别；采用门控特征调制过滤披露噪声；使用MetaGradNorm稳定多目标优化。

Result: 跨类别设置实验显示优于标准基线的鲁棒性，同时揭示了表征刚性与泛化之间的权衡关系。

Conclusion: 所提框架能有效提升ESG报告分析的鲁棒性，为检测绿色漂洗和模糊声明提供了更可靠的解决方案，但需要在表征刚性和泛化能力之间进行平衡。

Abstract: Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.

</details>


### [43] [Procedural Pretraining: Warming Up Language Models with Abstract Data](https://arxiv.org/abs/2601.21725)
*Liangze Jiang,Zachary Shinnick,Anton van den Hengel,Hemanth Saratchandran,Damien Teney*

Main category: cs.CL

TL;DR: 论文提出在语言模型预训练前先使用抽象程序数据（如形式语言生成的序列）进行预训练，能显著提升模型算法能力并加速后续自然语言预训练。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型直接在网络规模语料上预训练是主流范式，但作者探索另一种方法：先让模型接触抽象结构化数据（类似人类先学习简单逻辑和数学再学习高级推理），以促进后续语义知识的获取。

Method: 使用程序数据（由形式语言和简单算法生成）进行预训练，研究不同程序数据对算法技能的提升效果；在更大模型（达13亿参数）上测试；分析注意力机制和MLP层的变化机制；探索多种程序数据组合的方法。

Result: 1）程序数据显著提升算法能力：如在上下文召回任务上，使用Dyck序列预训练后准确率从10%提升到98%；2）仅用0.1%程序数据预训练就优于标准自然语言、代码和数学预训练；3）程序预训练使模型达到相同损失值只需原数据的55-86%；4）程序预训练在注意力层和MLP层都引入了非平凡结构。

Conclusion: 程序预训练是一种简单轻量级的方法，能提升语言模型性能并加速预训练过程，为在LLMs中将知识获取与推理能力解耦提供了有前景的方向。

Abstract: Pretraining directly on web-scale corpora is the de facto paradigm for building language models. We study an alternative setting where the model is initially exposed to abstract structured data, as a means to ease the subsequent acquisition of rich semantic knowledge, much like humans learn simple logic and mathematics before higher reasoning. We specifically focus on procedural data, generated by formal languages and other simple algorithms, as such abstract data.
  We first diagnose the algorithmic skills that different forms of procedural data can improve, often significantly. For example, on context recall (Needle-in-a-haystack), the accuracy jumps from 10 to 98% when pretraining on Dyck sequences (balanced brackets). Second, we study how these gains are reflected in pretraining larger models (up to 1.3B). We find that front-loading as little as 0.1% procedural data significantly outperforms standard pretraining on natural language, code, and informal mathematics (C4, CodeParrot, and DeepMind-Math datasets). Notably, this procedural pretraining enables the models to reach the same loss value with only 55, 67, 86% of the original data. Third, we explore the mechanisms behind and find that procedural pretraining instils non-trivial structure in both attention and MLP layers. The former is particularly important for structured domains (e.g. code), and the latter for language. Finally, we lay a path for combining multiple forms of procedural data. Our results show that procedural pretraining is a simple, lightweight means to improving performance and accelerating language model pretraining, ultimately suggesting the promise of disentangling knowledge acquisition from reasoning in LLMs.

</details>


### [44] [CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering](https://arxiv.org/abs/2601.21733)
*Jiayin Lan,Jiaqi Li,Baoxin Wang,Ming Liu,Dayong Wu,Shijin Wang,Bing Qin,Guoping Hu*

Main category: cs.CL

TL;DR: 提出CE-GOCD方法，通过学术知识图谱的语义子结构建模来增强LLMs的科学问答能力，在三个NLP文献问答数据集上优于现有检索增强基线。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强方法通常依赖孤立的文本块或概念，忽视了论文间更深层的语义联系，这损害了LLM对科学文献的理解，影响了回答的全面性和针对性。

Method: 提出CE-GOCD方法：1) 利用论文标题作为中心实体进行针对性子图检索；2) 通过子图剪枝和补全增强隐式语义发现；3) 应用社区检测提取具有共同主题的连贯论文组。

Result: 在三个NLP文献问答数据集上的评估结果显示，该方法优于其他检索增强基线方法，证实了框架的有效性。

Conclusion: 通过显式建模和利用学术知识图谱中的语义子结构，CE-GOCD方法能够有效增强LLMs在科学问答任务中的表现，解决了现有方法忽视深层语义联系的问题。

Abstract: Large Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM's comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs' scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.

</details>


### [45] [Temporal Guidance for Large Language Models](https://arxiv.org/abs/2601.21744)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CL

TL;DR: 提出Temporal Guidance (TeGu)方法，通过时间维度的对比指导和多词预测来提升语言模型生成质量，避免额外辅助模型的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有对比解码方法需要额外辅助模型，计算开销大；而基于层间差异的内部自对比方法（如DoLa）在小规模模型上不稳定。观察到语言模型存在局部偏好特性，需要更稳定高效的对比解码方法。

Method: 提出时间维度对比指导策略TeGu，利用多词预测构建较弱的"业余"预测用于模型自对比。引入轻量级条件多词预测投影器cMTPP，避免维护多个独立网络。

Result: 在各种模型系列和基准测试中，TeGu实现了显著的性能提升，同时保持了较低的内存消耗和计算开销。

Conclusion: TeGu通过时间维度的对比指导，提供了一种高效稳定的对比解码方法，在提升生成质量的同时避免了传统方法的高计算成本。

Abstract: Contrastive Decoding (CD) enhances the generation quality of large language models (LLMs) but incurs significant additional computational overhead due to the need for an auxiliary model. Existing internal self-contrastive decoding methods, such as Decoding by Contrasting Layers (DoLa), focus on discrepancies across different layers, which are notably unstable on small-scale models. In this work, based on the observation that LLMs exhibit local preferences, we propose a novel contrastive guidance strategy along the temporal dimension, namely Temporal Guidance (TeGu). Our method ingeniously leverages Multi-Token Prediction (MTP) to construct weaker amateur predictions for model self-contrast. To standardize the implementation of this mechanism, we further introduce a lightweight Conditional MTP Projector (cMTPP), which avoids maintaining multiple independent networks as required by other MTP modules. Across various model series and benchmarks, TeGu achieves significant performance improvements while maintaining low additional memory consumption and computational overhead.

</details>


### [46] [CoFrGeNet: Continued Fraction Architectures for Language Generation](https://arxiv.org/abs/2601.21766)
*Amit Dhurandhar,Vijil Chenthamarakshan,Dennis Wei,Tejaswini Pedapati,Karthikeyan Natesan Ramamurthy,Rahul Nair*

Main category: cs.CL

TL;DR: 提出基于连分数的新型生成网络架构CoFrGeNets，可替代Transformer中的多头注意力和前馈网络，参数减少1/2-1/3，性能保持竞争力甚至更优。


<details>
  <summary>Details</summary>
Motivation: Transformer是目前语言生成的首选架构，但参数量大、训练成本高。受连分数启发，希望设计更高效的架构组件来替代Transformer中的核心模块，减少参数和训练时间。

Method: 引入基于连分数的新型函数类，设计CoFrGeNets架构组件替代多头注意力和前馈网络。开发自定义梯度公式优化组件，比标准PyTorch梯度更准确高效。该组件可直接替换，无需大幅修改现有Transformer训练推理流程。

Result: 在GPT2-xl(1.5B)和Llama3(3.2B)上实验，参数减少2/3到1/2，预训练时间更短，下游分类、问答、推理和文本理解任务性能与原模型相当甚至更优。

Conclusion: CoFrGeNets提供了一种高效替代Transformer核心组件的方法，显著减少参数和训练成本，同时保持竞争力。未来针对硬件定制化实现将进一步发挥其潜力。

Abstract: Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\frac{2}{3}$ to $\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.

</details>


### [47] [Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond](https://arxiv.org/abs/2601.21767)
*Wei Zhu*

Main category: cs.CL

TL;DR: ChatGPT在医学信息抽取任务上表现不及微调模型，但能提供高质量解释，存在过度自信问题，对原文忠实度较高，生成不确定性影响信息抽取结果。


<details>
  <summary>Details</summary>
Motivation: 评估ChatGPT在医学信息抽取任务中的整体能力，包括性能、可解释性、置信度、忠实度和不确定性等方面，探索其在专业医学NLP任务中的应用潜力。

Method: 在6个基准数据集上对ChatGPT进行4种不同医学信息抽取任务的系统评估，测量其性能、可解释性、置信度、忠实度和不确定性等指标。

Result: ChatGPT在MedIE任务上的性能得分低于微调基线模型；能提供高质量决策解释但过度自信；大多数情况下对原文忠实度较高；生成不确定性导致信息抽取结果不确定，影响实际应用。

Conclusion: ChatGPT在医学信息抽取任务中表现有限，虽然具有高质量解释能力，但性能不足、过度自信和生成不确定性等问题限制了其在MedIE任务中的实际应用。

Abstract: Large Language Models (LLMs) like ChatGPT have demonstrated amazing capabilities in comprehending user intents and generate reasonable and useful responses. Beside their ability to chat, their capabilities in various natural language processing (NLP) tasks are of interest to the research community. In this paper, we focus on assessing the overall ability of ChatGPT in 4 different medical information extraction (MedIE) tasks across 6 benchmark datasets. We present the systematically analysis by measuring ChatGPT's performance, explainability, confidence, faithfulness, and uncertainty. Our experiments reveal that: (a) ChatGPT's performance scores on MedIE tasks fall behind those of the fine-tuned baseline models. (b) ChatGPT can provide high-quality explanations for its decisions, however, ChatGPT is over-confident in its predcitions. (c) ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. (d) The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks.

</details>


### [48] [Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention](https://arxiv.org/abs/2601.21768)
*Alon Rozental*

Main category: cs.CL

TL;DR: Zonkey是一个分层扩散模型，通过可微分分词器和概率注意力机制实现从原始字符到文档级表示的端到端训练，解决了传统LLM固定分词器的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型受限于固定的、不可微分的分词器（如BPE），这阻碍了端到端优化和对噪声或领域特定数据的适应性。需要一种完全可训练的文本处理管道。

Method: 1) 可微分分词器（Segment Splitter）学习概率性的序列开始决策；2) 新颖的概率注意力机制，包含位置特定存在概率；3) 分层压缩序列到更高抽象级别；4) 去噪扩散混合模型（DDMM）在潜在空间进行稳定去噪；5) Stitcher确保跨片段的重叠不变性。

Result: 在维基百科上端到端训练后，Zonkey能够从噪声生成连贯的变长文本，展示了涌现的分层结构，与基于熵的可学习分词器相比，在数据分布对齐方面表现出有希望的定性结果。

Conclusion: Zonkey推动了完全基于梯度的LLM发展，具有更好的领域适应性和可扩展生成潜力，为实现完全可微分的语言模型架构迈出了重要一步。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical diffusion model that addresses these limitations through a fully trainable pipeline from raw characters to document-level representations. At its core is a differentiable tokenizer (Segment Splitter) that learns probabilistic beginning-of-sequence (BOS) decisions, enabling adaptive splits that emerge as linguistically meaningful (e.g., word boundaries at spaces, sentence starts at periods) without explicit supervision. This differentiability is enabled by our novel Probabilistic Attention mechanism, which incorporates position-specific existence probabilities to simulate soft masking over theoretically infinite sequences while preserving gradients. Sequences decay probabilistically rather than relying on end-of-sequence tokens, supporting variable-length outputs. Hierarchical levels compress sequences into higher abstractions (e.g., character n-grams to word-like vectors, then sentence-like), with reconstruction via our Denoising Diffusion Mixed Model (DDMM) for stable and efficient denoising in latent space. A Stitcher ensures overlap invariance across segments. Trained end-to-end on Wikipedia, Zonkey generates coherent, variable-length text from noise, demonstrating emergent hierarchies and promising qualitative alignment to data distributions compared to entropy-based learnable tokenizers. Our approach advances toward fully gradient-based LLMs, with potential for better domain adaptation and scalable generation. We release the source code for training and reproducing our experiments.

</details>


### [49] [KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection](https://arxiv.org/abs/2601.21796)
*Yaocong Li,Leihan Zhang,Le Zhang,Qiang Yan*

Main category: cs.CL

TL;DR: KID框架通过知识注入和双头学习，将复杂的表情包理解分解为结构化推理链，在有害表情包检测任务上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模态内和模态间信号分析，但对隐含毒性的理解往往依赖于表情包本身未明确呈现的背景知识，这给自动化内容审核带来了挑战。

Method: 提出KID框架：1) 采用标签约束蒸馏范式，将复杂表情包理解分解为连接视觉证据、背景知识和分类标签的结构化推理链；2) 使用双头架构联合优化语义生成和分类目标，实现对齐的语言推理同时保持稳定的决策边界。

Result: 在涵盖英语、中文和低资源孟加拉语的五个多语言数据集上，KID在二元和多标签有害表情包检测任务上均达到SOTA，相比之前最佳方法在主要评估指标上提升2.1%-19.7%。

Conclusion: 知识注入和双头联合学习的有效性得到验证，它们对稳健且可泛化的表情包理解具有互补贡献，为解决依赖背景知识的有害内容检测问题提供了有效方案。

Abstract: Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.

</details>


### [50] [Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation](https://arxiv.org/abs/2601.21797)
*Yimin Deng,Yuqing Fu,Derong Xu,Yejing Wang,Wei Ni,Jingtong Gao,Xiaopeng Li,Chengxu Liu,Xiao Han,Guoshuai Zhao,Xiangyu Zhao,Li Zhu,Xueming Qian*

Main category: cs.CL

TL;DR: 提出对抗性记忆适应机制（AMA），通过模拟任务执行来对齐离线记忆构建与更新，解决现有记忆系统离线阶段固定且与任务无关的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆系统离线阶段（记忆构建与更新）固定且与任务无关，使用预定义流程和通用指标，导致与下游任务需求不匹配，影响任务性能。

Method: AMA机制包含三个智能体：挑战者基于原始对话生成问答对，用构建的记忆回答这些问题（模拟下游推理）；评估者评估回答并进行错误分析；适配者分析错误案例，对构建策略和内容进行双重更新。

Result: 在长对话基准LoCoMo上的大量实验证明了AMA的有效性，该机制可集成到多种现有记忆系统中。

Conclusion: AMA通过在离线阶段提供任务感知的监督信号，使记忆系统能更好地适应下游任务需求，解决了现有记忆系统离线阶段与任务不匹配的问题。

Abstract: Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.

</details>


### [51] [RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes](https://arxiv.org/abs/2601.21803)
*Korbinian Randl,Guido Rocchietti,Aron Henriksson,Ziawasch Abedjan,Tony Lindgren,John Pavlopoulos*

Main category: cs.CL

TL;DR: RAG-E：一个端到端的可解释性框架，通过数学基础归因方法量化检索器与生成器之间的对齐，揭示RAG系统中组件交互的失败模式


<details>
  <summary>Details</summary>
Motivation: RAG系统结合密集检索器和语言模型，但组件交互的不透明性给高风险领域部署带来挑战，需要可解释性框架来理解系统行为

Method: 提出RAG-E框架：1) 采用积分梯度分析检索器；2) 引入PMCSHAP（蒙特卡洛稳定Shapley值近似）进行生成器归因；3) 提出WARG度量评估生成器文档使用与检索器排名的对齐程度

Result: 在TREC CAsT和FoodSafeSum数据集上发现严重不对齐：47.4%-66.7%查询中生成器忽略检索器排名最高的文档，48.1%-65.9%依赖相关性较低的文档

Conclusion: RAG输出质量不仅取决于单个组件性能，更取决于组件间的交互，RAG-E能够审计这种交互，为系统部署提供关键洞察

Abstract: Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.

</details>


### [52] [Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning](https://arxiv.org/abs/2601.21804)
*Bodong Du,Xuanqi Huang,Xiaomeng Li*

Main category: cs.CL

TL;DR: DARE提出了一种分布感知的奖励估计方法，替代传统多数投票，通过利用完整rollout分布、探索奖励和分布剪枝来提供更稳健的强化学习信号。


<details>
  <summary>Details</summary>
Motivation: 现有测试时强化学习方法依赖多数投票产生确定性奖励，但这种方法丢弃了非多数但正确的动作信息，导致系统性偏差的奖励估计。

Method: 提出分布感知奖励估计（DARE）：1）从单一多数结果转向完整经验rollout分布；2）添加探索奖励鼓励非多数rollout探索；3）使用分布剪枝机制进行奖励去噪。

Result: 在挑战性推理基准测试中，DARE相比现有基线提高了优化稳定性和最终性能，在AIME 2024上相对提升25.3%，在AMC上提升5.3%。

Conclusion: 利用完整rollout分布而非单一多数结果进行奖励估计，能提供更丰富和稳健的学习信号，显著提升测试时强化学习的效果。

Abstract: Test-time reinforcement learning (TTRL) enables large language models (LLMs) to self-improve on unlabeled inputs, but its effectiveness critically depends on how reward signals are estimated without ground-truth supervision. Most existing TTRL methods rely on majority voting (MV) over rollouts to produce deterministic rewards, implicitly assuming that the majority rollout provides a reliable learning signal. We show that this assumption is fragile: MV reduces the rollout distribution into a single outcome, discarding information about non-majority but correct actions candidates, and yields systematically biased reward estimates. To address this, we propose Distribution-AwareReward Estimation (DARE), which shifts reward estimation from a single majority outcome to the full empirical rollout distribution. DARE further augments this distribution-based reward with an exploration bonus and a distribution pruning mechanism for non-majority rollout exploration and reward denoise, yielding a more informative and robust reward estimation. Extensive experiments on challenging reasoning benchmarks show that DARE improves optimization stability and final performance over recent baselines, achieving relative improvements of 25.3% on challenging AIME 2024 and 5.3% on AMC.

</details>


### [53] [Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models](https://arxiv.org/abs/2601.21826)
*Aadi Palnitkar,Mingyang Mao,Nicholas Waytowich,Vinicius G. Goecks,Tinoosh Mohsenin,Xiaomin Lin*

Main category: cs.CL

TL;DR: MilSCORE是首个基于复杂军事规划场景的专家编写多跳问题数据集，用于评估LLM在长上下文、多模态信息下的空间推理和规划能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM处理更长更复杂任务的需求增长，需要真实的长上下文基准测试来评估选择性阅读和异构多模态信息整合能力，特别是在需要快速准确推理地图、命令、情报报告等分布式数据的军事规划等地理空间规划问题中。

Method: 创建MilSCORE数据集，包含专家编写的基于复杂模拟军事规划场景的多跳问题，涵盖七类问题类型，针对事实回忆和多步推理（约束、策略、空间分析），并提供评估协议。

Result: 报告了多种当代视觉语言模型的基线结果，发现当前系统在真实场景级长上下文规划方面存在显著不足，MilSCORE为未来工作提供了具有挑战性的测试平台。

Conclusion: MilSCORE填补了长上下文多模态推理基准的空白，突出了当前系统在真实场景规划中的局限性，为评估和改进LLM在复杂地理空间规划任务中的能力提供了重要工具。

Abstract: As large language models (LLMs) are applied to increasingly longer and more complex tasks, there is a growing need for realistic long-context benchmarks that require selective reading and integration of heterogeneous, multi-modal information sources. This need is especially acute for geospatial planning problems, such as those found in planning for large-scale military operations, which demand fast and accurate reasoning over maps, orders, intelligence reports, and other distributed data. To address this gap, we present MilSCORE (Military Scenario Contextual Reasoning), to our knowledge the first scenario-level dataset of expert-authored, multi-hop questions grounded in a complex, simulated military planning scenario used for training. MilSCORE is designed to evaluate high-stakes decision-making and planning, probing LLMs' ability to combine tactical and spatial reasoning across multiple sources and to reason over long-horizon, geospatially rich context. The benchmark includes a diverse set of question types across seven categories targeting both factual recall and multi-step reasoning about constraints, strategy, and spatial analysis. We provide an evaluation protocol and report baseline results for a range of contemporary vision-language models. Our findings highlight substantial headroom on MilSCORE, indicating that current systems struggle with realistic, scenario-level long-context planning, and positioning MilSCORE as a challenging testbed for future work.

</details>


### [54] [Embodied Task Planning via Graph-Informed Action Generation with Large Lanaguage Model](https://arxiv.org/abs/2601.21841)
*Xiang Li,Ning Yan,Masood Mortazavi*

Main category: cs.CL

TL;DR: GiG框架使用图内图架构组织具身智能体记忆，通过GNN编码环境状态为图嵌入，聚类检索结构先验，结合符号转换逻辑的有界前瞻模块，显著提升长时程规划性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在零样本推理方面表现出色，但作为具身智能体部署时，在长时程规划方面仍面临根本性挑战。标准LLM规划器经常因上下文窗口限制而无法保持策略连贯性，或产生违反约束的幻觉转换。

Method: 提出GiG规划框架，采用图内图架构组织智能体记忆：1) 使用GNN将环境状态编码为图嵌入；2) 将这些嵌入组织成动作连接的执行轨迹图存入经验记忆库；3) 通过聚类图嵌入实现结构感知先验检索；4) 引入有界前瞻模块，利用符号转换逻辑增强规划能力。

Result: 在三个具身规划基准测试中超越最先进基线：Robotouille同步版Pass@1提升22%，异步版提升37%，ALFWorld提升15%，且计算成本相当或更低。

Conclusion: GiF框架通过结构化记忆组织和符号推理增强，有效解决了LLM在具身规划中的长时程连贯性问题，为具身智能体部署提供了实用解决方案。

Abstract: While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.

</details>


### [55] [Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text](https://arxiv.org/abs/2601.21895)
*Hongyi Zhou,Jin Zhu,Erhan Xu,Kai Ye,Ying Yang,Chengchun Shi*

Main category: cs.CL

TL;DR: 本文提出了一种基于几何视角理解重写检测算法的新方法，并引入自适应学习距离函数的新型检测算法，在多种LLM检测场景中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如GPT、Claude、Gemini）能生成高度拟人化文本，这引发了关于错误信息和学术诚信的严重担忧，迫切需要可靠的算法来检测LLM生成的内容。

Method: 首先从几何角度解析重写检测算法，揭示其基本原理和泛化能力。基于此洞察，提出一种新型重写检测算法，能自适应学习原始文本与重写文本之间的距离。

Result: 在超过100种设置下进行广泛实验，新方法在大多数场景中表现出优于基线算法的性能。针对不同目标LLM（如GPT、Claude、Gemini），相对于最强基线实现了57.8%到80.6%的相对改进。

Conclusion: 理论上证明了使用自适应学习距离函数比固定距离更有效；实践上开发出在多种LLM检测场景中表现优越的新型检测算法，为解决LLM生成内容检测问题提供了有效方案。

Abstract: Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\% to 80.6\% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini).

</details>


### [56] [SONIC: Segmented Optimized Nexus for Information Compression in Key-Value Caching](https://arxiv.org/abs/2601.21927)
*Hong Chen,Xiang Liu,Bo Wang,Yuxuan Fan,Yuanlin Chu,Zongluo Li,Xiaowen Chu,Xuming Hu*

Main category: cs.CL

TL;DR: SONIC是一种基于学习的KV缓存压缩框架，通过将历史对话段压缩为紧凑的Nexus令牌来减少内存占用，在保持对话连贯性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中KV缓存的线性增长是LLM部署的主要瓶颈，现有压缩方法通常忽视对话结构特性，依赖启发式淘汰策略可能导致关键上下文丢失。

Method: 提出SONIC框架，将历史对话段压缩为语义丰富的Nexus令牌，采用动态预算训练机制，无需重新训练即可适应不同的内存约束。

Result: 在80%和50%压缩率下，SONIC在四个多轮对话基准测试中均优于H2O和StreamingLLM等基线方法。在MTBench101上平均得分提升35.55%，推理过程加速50.1%。

Conclusion: SONIC通过基于学习的压缩方法有效解决了多轮对话中的KV缓存问题，在保持对话质量的同时显著提升了部署效率和推理速度。

Abstract: The linear growth of Key-Value (KV) cache remains a bottleneck for multi-turn LLM deployment. Existing KV cache compression methods often fail to account for the structural properties of multi-turn dialogues, relying on heuristic eviction that risks losing critical context. We propose \textbf{SONIC}, a learning-based framework that compresses historical segments into compact and semantically rich \textbf{Nexus} tokens. By integrating dynamic budget training, SONIC allows flexible adaptation to varying memory constraints without retraining. Experiments show that at compression ratios of 80\% and 50\%, SONIC consistently outperforms baselines such as H2O and StreamingLLM on four diverse multi-turn benchmarks. Specifically, on the widely used MTBench101 benchmark, SONIC achieves an average score improvement of 35.55\% over state-of-the-art baselines, validating its effectiveness in sustaining coherent multi-turn dialogues. Furthermore, SONIC enhances deployment efficiency, accelerating the overall inference process by 50.1\% compared to full-context generation.

</details>


### [57] [From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes](https://arxiv.org/abs/2601.21955)
*Fariba Afrin Irany*

Main category: cs.CL

TL;DR: 提出基于GPT的选择性微调架构，用于临床文本分类，显著减少可训练参数同时保持性能


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中非结构化临床叙事数据增多，但长领域特定文本建模面临标注数据有限、类别严重不平衡、大型预训练模型计算成本高等挑战

Method: 采用GPT-2预训练模型，冻结大部分骨干网络，仅微调最后的Transformer块、层归一化和轻量级分类头，大幅减少可训练参数

Result: 在MIMIC-IV-Note放射学报告数据集上评估，模型在多标签分类、二元分类和疾病结果预测等任务中表现稳定且性能良好，尤其在非提及和否定发现占主导的场景中表现突出

Conclusion: 选择性微调预训练生成语言模型为临床文本分类提供了高效有效的途径，能够在显著降低计算复杂度的同时实现对真实世界电子健康记录数据的可扩展适应

Abstract: The increasing availability of unstructured clinical narratives in electronic health records (EHRs) has created new opportunities for automated disease characterization, cohort identification, and clinical decision support. However, modeling long, domain-specific clinical text remains challenging due to limited labeled data, severe class imbalance, and the high computational cost of adapting large pretrained language models.
  This study presents a GPT-based architecture for clinical text classification that adapts a pretrained decoder-only Transformer using a selective fine-tuning strategy. Rather than updating all model parameters, the majority of the GPT-2 backbone is frozen, and training is restricted to the final Transformer block, the final layer normalization, and a lightweight classification head. This approach substantially reduces the number of trainable parameters while preserving the representational capacity required to model complex clinical language.
  The proposed method is evaluated on radiology reports from the MIMIC-IV-Note dataset using uncertainty-aware CheXpert-style labels derived directly from report text. Experiments cover multiple problem formulations, including multi-label classification of radiographic findings, binary per-label classification under different uncertainty assumptions, and aggregate disease outcome prediction. Across varying dataset sizes, the model exhibits stable convergence behavior and strong classification performance, particularly in settings dominated by non-mention and negated findings.
  Overall, the results indicate that selective fine-tuning of pretrained generative language models provides an efficient and effective pathway for clinical text classification, enabling scalable adaptation to real-world EHR data while significantly reducing computational complexity.

</details>


### [58] [OVD: On-policy Verbal Distillation](https://arxiv.org/abs/2601.21968)
*Jing Xiong,Hui Shen,Shansan Gong,Yuxin Cheng,Jianghan Shen,Chaofan Tao,Haochen Tan,Haoli Bai,Lifeng Shang,Ngai Wong*

Main category: cs.CL

TL;DR: OVD提出了一种基于轨迹匹配和离散语言评分的在线蒸馏框架，显著降低内存消耗，提升学生模型推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基于token级概率匹配的在线蒸馏方法存在三个主要问题：1) 需要token级对齐，限制了学生模型的探索能力；2) 难以有效利用交互环境反馈；3) 在强化学习中存在严重的内存瓶颈

Method: 提出On-policy Verbal Distillation (OVD)框架，用基于离散语言评分（0-9）的轨迹匹配替代token级概率匹配，避免token级对齐，允许学生模型自由探索输出空间

Result: 在Web问答和数学推理任务上，OVD显著优于现有方法：Web Q&A任务平均EM提升高达12.9%，数学基准提升高达25.7%（仅使用一个随机样本训练），同时展现出优越的训练效率

Conclusion: OVD通过轨迹匹配和离散语言评分，有效解决了传统在线蒸馏的内存瓶颈和探索限制问题，为高效知识蒸馏提供了新思路

Abstract: Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io

</details>


### [59] [Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding](https://arxiv.org/abs/2601.21969)
*Yifan Zhu,Huiqiang Rong,Haoran Luo*

Main category: cs.CL

TL;DR: Token-Guard：基于自检解码的token级幻觉控制方法，通过内部验证、潜在空间评估和迭代修剪来减少LLM幻觉


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和RLHF虽然能缓解幻觉但资源消耗大，解码方法轻量但缺乏明确的幻觉控制，需要一种既轻量又能显式控制幻觉的方法

Method: Token-Guard采用token级自检解码，包括：1）推理步骤内部验证检测幻觉token；2）潜在空间显式幻觉风险评分评估候选片段；3）迭代修剪和再生动态纠正检测到的错误

Result: 在HALU数据集上的实验表明，Token-Guard显著减少了幻觉并提高了生成准确性

Conclusion: Token-Guard为可靠的LLM输出提供了一个可扩展、模块化的解决方案，代码已公开

Abstract: Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.

</details>


### [60] [Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units](https://arxiv.org/abs/2601.21996)
*Jianhui Chen,Yuzhang Luo,Liangming Pan*

Main category: cs.CL

TL;DR: MDA框架使用影响函数将可解释电路追溯到训练数据，通过干预高影响力样本验证了可解释头的因果起源，发现重复结构数据是机制催化剂，并建立了归纳头与上下文学习的功能联系。


<details>
  <summary>Details</summary>
Motivation: 虽然机制可解释性已在LLMs中识别出可解释电路，但这些电路在训练数据中的因果起源仍然难以捉摸。需要建立从可解释单元到具体训练样本的因果联系。

Method: 提出机制数据归因(MDA)框架，使用影响函数将可解释单元追溯到特定训练样本。在Pythia模型家族上进行实验，通过移除或增强高影响力样本进行针对性干预，验证因果关系。

Result: 针对性干预（移除或增强少量高影响力样本）显著调节可解释头的出现，而随机干预无效。重复结构数据（如LaTeX、XML）作为机制催化剂。针对归纳头形成的干预会同时改变模型的上下文学习能力。

Conclusion: 提供了归纳头与上下文学习功能联系的首个直接因果证据。提出的机制数据增强流程能跨模型规模一致加速电路收敛，为引导LLMs发展轨迹提供了原则性方法。

Abstract: While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.

</details>


### [61] [When "Better" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications](https://arxiv.org/abs/2601.22025)
*Daniel Commey*

Main category: cs.CL

TL;DR: 论文提出针对LLM应用的评估驱动工作流（定义-测试-诊断-修复）和最小可行评估套件（MVES），通过实验发现通用"改进"提示模板会带来行为权衡，强调需要评估驱动的提示迭代而非通用方案。


<details>
  <summary>Details</summary>
Motivation: LLM应用评估与传统软件测试不同，输出具有随机性、高维性，且对提示和模型变化敏感。需要系统化的评估方法来应对这些挑战。

Method: 提出评估驱动工作流（定义-测试-诊断-修复）和最小可行评估套件（MVES），包含通用LLM应用、检索增强生成（RAG）和智能体工具使用工作流的评估组件。使用自动检查、人工评分和LLM作为评判者等方法。

Result: 实验发现通用"改进"提示模板会导致行为权衡：在Llama 3上，提取通过率从100%降至90%，RAG合规性从93.3%降至80%，但指令遵循有所改善。这证明了评估驱动迭代的必要性。

Conclusion: LLM应用评估需要系统化方法，通用提示模板无法解决所有问题，应基于评估结果进行迭代优化，并谨慎校准性能声明而非依赖通用方案。

Abstract: Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.
  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.
  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic "improved" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.
  All test suites, harnesses, and results are included for reproducibility.

</details>


### [62] [Causal Autoregressive Diffusion Language Model](https://arxiv.org/abs/2601.22031)
*Junhao Ruan,Bei Li,Yongjing Yin,Pengcheng Huang,Xin Chen,Jingang Wang,Xunliang Cai,Tong Xiao,JingBo Zhu*

Main category: cs.CL

TL;DR: CARD是一个结合自回归模型训练效率和扩散模型高吞吐推理的新框架，通过因果注意力掩码实现单次前向传播的密集监督，支持动态并行解码，训练延迟降低3倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练效率和推理吞吐量之间存在权衡：自回归模型（ARMs）训练高效但推理串行，扩散模型推理并行但训练成本高。需要统一两者优势，实现高效训练和高吞吐推理。

Method: 1. 在严格因果注意力掩码下重新表述扩散过程，实现单次前向传播的密集监督；2. 引入软尾掩码模式保留局部上下文；3. 基于信噪比原理设计上下文感知重加权机制；4. 利用KV缓存实现动态并行解码，根据置信度自适应生成变长序列。

Result: CARD在性能上优于现有离散扩散基线，训练延迟相比块扩散方法降低3倍，实现了ARM级别的数据效率，同时解锁了并行生成的延迟优势。

Conclusion: CARD建立了下一代高效LLM的稳健范式，统一了训练效率和推理吞吐量，为高效语言模型提供了新的解决方案。

Abstract: In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.

</details>


### [63] [Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models](https://arxiv.org/abs/2601.22035)
*Longxuan Yu,Yu Fu,Shaorong Zhang,Hui Liu,Mukund Varma T,Greg Ver Steeg,Yue Dong*

Main category: cs.CL

TL;DR: 扩散语言模型相比自回归模型在输出顺序约束下表现更稳定，能实现"顺序鲁棒性"


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型强制左到右生成顺序，当输出结构要求与自然推理顺序冲突时（如要求答案在解释前），模型必须过早承诺答案，造成性能下降

Method: 使用掩码扩散语言模型（MDLMs），通过并行迭代优化所有token，解耦计算顺序与输出结构；在GSM8K、Math500和自建的ReasonOrderQA基准上验证

Result: 当提示要求答案在推理前时，自回归模型相比标准思维链顺序准确率大幅下降（相对下降达67%），而扩散模型保持稳定（相对下降≤14%），表现出"顺序鲁棒性"

Conclusion: 扩散语言模型通过让简单token（如推理步骤）在扩散过程中比复杂token（如最终答案）更早稳定，实现顺序鲁棒性，但也存在优势减弱的失效条件

Abstract: Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\leq$14% relative drop), a property we term "order robustness". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.

</details>


### [64] [A Separable Architecture for Continuous Token Representation in Language Models](https://arxiv.org/abs/2601.22040)
*Reza T. Batley,Sourav Saha*

Main category: cs.CL

TL;DR: Leviathan架构使用连续嵌入生成器替代传统离散查找表，在小语言模型中显著提升参数效率，表现相当于参数多1.47-2.11倍的密集模型


<details>
  <summary>Details</summary>
Motivation: 传统Transformer缩放定律将参数视为可互换的，但在十亿参数以下的小语言模型中，嵌入矩阵占据大部分参数预算，这种分配既次优又违反直觉

Method: 提出Leviathan架构，使用连续嵌入生成器替代传统的离散查找表，在等参设置下评估性能

Result: 在Pile数据集上，Leviathan始终优于标准的LLaMA风格架构；通过经验幂律拟合，显示出显著优越的有效参数容量

Conclusion: Leviathan在小语言模型领域表现出更高的参数效率，相当于拥有1.47到2.11倍参数的密集模型，为小模型设计提供了新思路

Abstract: Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \times$ more parameters.

</details>


### [65] [On the Paradoxical Interference between Instruction-Following and Task Solving](https://arxiv.org/abs/2601.22047)
*Yunjia Qi,Hao Peng,Xintong Shi,Amy Xin,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 论文发现指令跟随会干扰LLM的任务解决能力，提出了SUSTAINSCORE量化这种干扰，实验显示添加自明约束会导致性能显著下降


<details>
  <summary>Details</summary>
Motivation: 指令跟随旨在让LLM与人类意图对齐，但作者发现一个反直觉现象：指令跟随可能反而会干扰LLM的任务解决能力。需要量化这种干扰并理解其机制

Method: 提出SUSTAINSCORE指标，通过在指令中插入自明约束（从原始成功模型输出中提取）来衡量任务性能下降。在数学、多跳QA和代码生成任务上测试当前LLM，分析失败模式，研究注意力分配机制

Result: 添加自明约束导致性能显著下降，即使是Claude-Sonnet-4.5等先进模型。干扰在不同约束类型和规模上具有普遍性，失败案例对约束分配更多注意力

Conclusion: 指令跟随可能干扰LLM任务解决能力，SUSTAINSCORE能有效量化这种干扰。研究揭示了指令对齐策略的局限性，为未来对齐方法改进提供见解

Abstract: Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research

</details>


### [66] [MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs](https://arxiv.org/abs/2601.22050)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: MasalBench是一个评估LLMs对波斯谚语上下文理解和跨文化理解的基准测试，发现LLMs在识别波斯谚语方面表现良好（准确率>0.90），但在识别等效英语谚语方面表现较差（最佳模型0.79准确率），揭示了当前LLMs在文化知识和类比推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型已成为日常生活的重要组成部分，需要掌握会话语言规则以有效与用户沟通。先前研究主要评估LLMs对高资源语言中比喻语言的理解，但对低资源语言中的表现研究不足。波斯语作为低资源语言，其谚语是会话的关键组成部分，需要评估LLMs在这方面的能力。

Method: 研究者开发了MasalBench基准测试，专门用于评估LLMs对波斯谚语的上下文和跨文化理解。该基准测试了8个最先进的LLMs，包括两个任务：1）在上下文中识别波斯谚语；2）识别与波斯谚语等效的英语谚语。

Result: LLMs在识别波斯谚语方面表现良好，准确率超过0.90；但在识别等效英语谚语方面表现显著下降，最佳模型仅达到0.79准确率。这表明当前LLMs在文化知识和类比推理方面存在明显局限性。

Conclusion: 研究揭示了当前多语言LLMs在低资源语言文化理解和跨文化类比推理方面的不足，MasalBench为评估其他低资源语言的跨文化理解提供了一个框架，有助于推动LLMs在文化智能方面的发展。

Abstract: In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs' understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a comprehensive benchmark for assessing LLMs' contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight state-of-the-art LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 accuracy. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a framework for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at https://github.com/kalhorghazal/MasalBench.

</details>


### [67] [$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA](https://arxiv.org/abs/2601.22055)
*Yaxin Du,Junru Song,Yifan Zhou,Cheng Wang,Jiahao Gu,Zimeng Chen,Menglan Chen,Wen Yao,Yang Yang,Ying Wen,Siheng Chen*

Main category: cs.CL

TL;DR: G²-Reader：一种双图系统，通过内容图和规划图解决多模态文档问答中的结构保持和检索导航问题，在VisDoMBench上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在多模态长文档问答中存在两个主要问题：1）平面分块破坏了文档原生结构和跨模态对齐，导致语义碎片化；2）迭代检索在长上下文中容易陷入局部证据循环或漂移到无关内容，缺乏全局搜索状态跟踪。

Method: 提出G²-Reader双图系统：1）内容图用于保持文档原生结构和跨模态语义；2）规划图作为子问题的有向无环图，跟踪中间发现并指导逐步导航以完成证据收集。

Result: 在VisDoMBench的五个多模态领域上，G²-Reader配合Qwen3-VL-32B-Instruct达到66.21%的平均准确率，显著优于强基线方法和独立的GPT-5（53.08%）。

Conclusion: G²-Reader通过双图架构有效解决了多模态长文档问答中的结构保持和检索导航问题，证明了内容图和规划图协同工作的有效性，为复杂多模态文档理解提供了新思路。

Abstract: Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21\% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08\%).

</details>


### [68] [VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning](https://arxiv.org/abs/2601.22069)
*Yibo Wang,Yongcheng Jing,Shunyu Liu,Hao Guan,Rong-cheng Tu,Chengyu Wang,Jun Huang,Dacheng Tao*

Main category: cs.CL

TL;DR: VTC-R1：一种新的高效推理范式，通过将中间推理段渲染为紧凑图像作为"光学记忆"，实现3.4倍token压缩和2.7倍推理加速


<details>
  <summary>Details</summary>
Motivation: 长上下文推理显著增强了LLMs处理复杂任务的能力，但带来了严重的效率瓶颈。现有高效方法通常需要复杂的额外训练或依赖外部模型进行压缩，限制了可扩展性并丢弃了关键的细粒度信息。

Method: 提出VTC-R1范式，将视觉-文本压缩集成到推理过程中。不处理冗长的文本轨迹，而是将中间推理段渲染为紧凑图像，迭代地反馈给视觉语言模型作为"光学记忆"。基于OpenR1-Math-220K构建训练数据集，并对代表性VLM模型Glyph和Qwen3-VL进行微调。

Result: 在MATH500、AIME25、AMC23和GPQA-D等基准测试中，VTC-R1始终优于标准长上下文推理。实现了3.4倍的token压缩和2.7倍的端到端延迟加速，显著提高了推理效率。

Conclusion: VTC-R1作为一种可扩展的推理密集型应用解决方案，通过视觉-文本压缩有效解决了长上下文推理的效率瓶颈，在保持性能的同时大幅提升了推理速度。

Abstract: Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as "optical memory." We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.

</details>


### [69] [ECO: Quantized Training without Full-Precision Master Weights](https://arxiv.org/abs/2601.22101)
*Mahdi Nikdan,Amir Zandieh,Dan Alistarh,Vahab Mirrokni*

Main category: cs.CL

TL;DR: ECO优化器通过消除主权重缓冲区，将更新直接应用到量化参数中，显著减少内存开销，同时保持与使用主权重基线相当的精度。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法仍需在训练过程中维护高精度主权重缓冲区，这带来了显著的内存开销，特别是在稀疏混合专家模型中，模型参数和优化器状态占用了大量内存。

Method: 提出误差补偿优化器，消除主权重，将更新直接应用到量化参数中。该方法在每一步后量化权重，并将量化误差注入优化器动量中，形成无需额外内存的误差反馈循环。

Result: 在标准假设和衰减学习率下，ECO收敛到最优解的常数半径邻域内，而简单移除主权重可能导致与学习率成反比的误差。实验表明ECO在多种模型上匹配使用主权重的基线精度。

Conclusion: ECO通过消除主权重缓冲区，显著改善了静态内存与验证损失的帕累托前沿，为大规模模型训练提供了更高效的内存优化方案。

Abstract: Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.

</details>


### [70] [A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine](https://arxiv.org/abs/2601.22124)
*Anran Li,Yuanyuan Chen,Wenjun Long,Yu Yin,Yan Hu,Hyunjae Kim,Weipeng Zhou,Yujia Zhou,Hongyi Peng,Yang Ren,Xuguang Ai,Zhenyue Qin,Ming Hu,Xiaoxiao Li,Han Yu,Yih-Chung Tham,Lucila Ohno-Machado,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 提出Fed-MedLoRA框架，通过联邦学习高效适应医疗LLM，解决通信开销和数据异构问题


<details>
  <summary>Details</summary>
Motivation: 医疗LLM通常基于单一机构数据训练，存在泛化性和安全性限制；传统联邦学习对LLM不实用，且假设数据同质性，而真实医疗数据高度异构

Method: 提出Fed-MedLoRA（传输低秩适配器参数）和Fed-MedLoRA+（加入自适应数据感知聚合），应用于临床信息抽取任务

Result: 在五个患者队列中评估，包括域内训练测试、外部验证和低资源新站点适应场景，与BERT、LLaMA-3、DeepSeek-R1、GPT-4o等模型比较

Conclusion: Fed-MedLoRA框架能有效降低通信计算开销，并在跨站点异构数据下改善收敛，适用于医疗LLM的联邦学习适应

Abstract: Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.

</details>


### [71] [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://arxiv.org/abs/2601.22139)
*Xin Chen,Feng Jiang,Yiqian Zhang,Hardy Chen,Shuo Yan,Wenya Xie,Min Yang,Shujian Huang*

Main category: cs.CL

TL;DR: PIR是一种新的推理范式，让LLM从被动推理转变为主动交互式推理，通过询问用户来澄清前提和意图的不确定性，显著提升性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示的LLMs存在"盲目自我思考"的局限性，当关键信息缺失或模糊时仍进行大量内部推理，需要一种能主动与用户交互澄清不确定性的新范式。

Method: 提出PIR框架，包含两个核心组件：1) 不确定性感知的监督微调，使模型具备交互推理能力；2) 基于用户模拟器的策略优化框架，使用复合奖励使模型行为与用户意图对齐。

Result: 在数学推理、代码生成和文档编辑任务上，PIR显著优于基线方法，准确率提升最高32.70%，通过率提升22.90%，BLEU分数提升41.36，同时减少近一半的推理计算和不必要的交互轮次。

Conclusion: PIR将LLM从被动求解器转变为主动询问者，通过交互式推理有效处理前提和意图层面的不确定性，在多个任务上展现出强大的泛化能力和鲁棒性。

Abstract: Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}

</details>


### [72] [FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale](https://arxiv.org/abs/2601.22146)
*Ajay Patel,Colin Raffel,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 提出FineInstructions方法，将互联网规模的预训练文档转化为数十亿个合成指令-答案训练对，使LLM能够仅通过指令调优目标从头开始预训练，在标准基准测试中表现优于传统预训练方法。


<details>
  <summary>Details</summary>
Motivation: 由于监督训练数据有限，LLM通常通过自监督的"预测下一个词"目标在大量非结构化文本上进行预训练，然后使用少量指令调优数据进行微调。为了克服监督数据不足的问题，需要将预训练文档中的知识转化为大规模的合成指令训练数据。

Method: 提出FineInstructions方法：1) 从真实用户查询和提示中创建约1800万个指令模板；2) 将这些模板与非结构化预训练语料库中的人工编写源文档进行匹配和实例化；3) 生成数十亿个合成指令-答案训练对，使LLM能够仅通过指令调优目标从头开始预训练。

Result: 在受控的token-for-token训练实验中，使用FineInstructions进行预训练的模型在衡量自由形式响应质量的标准基准测试中，表现优于标准预训练和其他提出的合成预训练技术。

Conclusion: FineInstructions能够将预训练文档中的知识有效转化为大规模的合成指令训练数据，使LLM能够通过更符合下游使用场景的指令调优目标进行预训练，从而提升模型性能。该方法为解决监督数据有限的问题提供了有效途径。

Abstract: Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised "predict the next word" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of "instruction-tuning" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With "supervised" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .

</details>


### [73] [DynaWeb: Model-Based Reinforcement Learning of Web Agents](https://arxiv.org/abs/2601.22149)
*Hang Ding,Peidong Liu,Junqiao Wang,Ziwei Ji,Meng Cao,Rongzhao Zhang,Lynn Ai,Eric Yang,Tianyu Shi,Lei Yu*

Main category: cs.CL

TL;DR: DynaWeb是一个基于模型强化学习的框架，通过训练网页世界模型来模拟网页交互，使网页代理能够在想象中训练，显著提高了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 训练自主网页代理面临与实时互联网交互的低效、昂贵和风险问题，需要一种更高效、可扩展的训练方法。

Method: 提出DynaWeb框架：1) 训练网页世界模型预测给定代理动作后的网页表示；2) 在合成网页环境中进行策略rollout；3) 将真实专家轨迹与在线策略rollout随机交错训练以提高稳定性和样本效率。

Result: 在WebArena和WebVoyager基准测试中，DynaWeb显著提升了最先进开源网页代理模型的性能。

Conclusion: 通过想象训练网页代理是可行的，为在线代理强化学习提供了可扩展且高效的方法。

Abstract: The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.

</details>


### [74] [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://arxiv.org/abs/2601.22156)
*Yingfa Chen,Zhen Leng Thai,Zihan Zhou,Zhu Zhang,Xingyu Shen,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: HALO方法将Transformer蒸馏为RNN-attention混合模型，HypeNet架构通过新型位置编码和架构改进实现优异长度泛化，仅需少量数据即可转换Qwen3系列模型


<details>
  <summary>Details</summary>
Motivation: 现有混合Transformer架构（结合softmax attention和RNN）在长上下文建模中表现出良好的性能-吞吐量权衡，但大规模从头预训练成本过高。现有转换方法需要大量训练数据且长上下文性能不佳，而混合模型在长上下文场景下相比Transformer具有显著推理加速优势

Method: 提出HALO（Hybrid Attention via Layer Optimization）管道，用于将Transformer模型蒸馏为RNN-attention混合模型。提出HypeNet混合架构，采用新型位置编码方案HyPE和各种架构改进，实现优异的长度泛化能力

Result: 使用HALO将Qwen3系列转换为HypeNet，仅需2.3B tokens（不到预训练数据的0.01%），性能与原始Transformer模型相当，同时获得优异的长上下文性能和效率

Conclusion: HALO和HypeNet提供了一种高效的方法将预训练Transformer转换为混合模型，显著降低了转换成本，同时保持了性能并提升了长上下文场景下的效率

Abstract: Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [75] [Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs](https://arxiv.org/abs/2601.20968)
*Yulie Arad,Stav Ashur,Nancy M. Amato*

Main category: cs.RO

TL;DR: 提出"红-绿-灰"范式改进SPITE方法，通过廉价启发式检查快速更新机器人运动规划路线图，适应非静态环境


<details>
  <summary>Details</summary>
Motivation: 解决机器人运动规划中路线图在非静态环境中的适应性问题，传统方法难以快速更新路线图以应对环境变化

Method: 改进SPITE方法，引入红-绿-灰分类范式：使用计算几何方法近似机器人扫掠体积，进行懒惰碰撞检查，将边标记为无效(红)、有效(绿)或未知(灰)

Result: 与Leven和Hutchinson的成熟技术相比，该方法提高了准确性，能够正确标记无效边，同时保持可比的更新运行时间

Conclusion: 红-绿-灰范式为机器人运动规划提供了有效的半懒惰路线图更新方法，适用于动态变化的非静态环境

Abstract: In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the "Red-Green-Gray" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.

</details>


### [76] [Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems](https://arxiv.org/abs/2601.21011)
*Anshul Ranjan,Anoosh Damodar,Neha Chougule,Dhruva S Nayak,Anantharaman P. N,Shylaja S S*

Main category: cs.RO

TL;DR: Meta-ROS是一个新型机器人中间件，通过简化集成、提升性能和确保跨平台兼容性来优化机器人开发，相比ROS2实现了高达30%的吞吐量提升和显著降低的消息延迟。


<details>
  <summary>Details</summary>
Motivation: 现有机器人中间件框架（如ROS2）存在复杂性和互操作性问题，对新开发者采用造成困难，需要更简化、高效且兼容性强的解决方案。

Method: Meta-ROS采用现代通信协议（如Zenoh和ZeroMQ），支持高效低延迟的跨硬件平台通信，并兼容多种数据类型（音频、图像、视频）。

Result: Meta-ROS在性能测试中优于ROS2，吞吐量提升高达30%，消息延迟显著降低，资源使用优化，并具备强大的硬件支持和开发者友好设计。

Conclusion: Meta-ROS凭借其高性能、跨平台兼容性和易用性，成为现代实时机器人AI应用的理想中间件解决方案。

Abstract: The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.

</details>


### [77] [Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing](https://arxiv.org/abs/2601.21027)
*Youngim Nam,Jungbin Kim,Kyungtae Kang,Cheolhyeon Kwon*

Main category: cs.RO

TL;DR: 提出一个用于自动驾驶赛车轨迹优化的框架，通过贝叶斯优化在参数空间中探索全局轨迹，并结合迭代学习动态模型来逐步优化圈速。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注跟踪层面的动态学习，而不更新轨迹本身以适应学习到的动态，导致在不确定动态下难以保证全局最优性。同时，全赛道轨迹优化计算成本高，实际跟踪难以保证最优。

Method: 采用赛道中心化方法，通过小波变换将轨迹表示为赛道无关的参数空间，使用贝叶斯优化高效探索该空间，并通过模拟评估候选轨迹的圈速。将优化嵌入迭代学习框架，用优化轨迹收集真实数据更新动态模型，逐步迭代优化。

Result: 通过仿真和真实实验验证了框架有效性，相比基准方法圈速提升高达20.7%，并持续优于现有最先进方法。

Conclusion: 提出的框架能够有效处理自动驾驶赛车中的不确定动态问题，通过迭代学习和全局轨迹优化显著提升圈速性能。

Abstract: This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.

</details>


### [78] [Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned](https://arxiv.org/abs/2601.21063)
*Pierre-Yves Lajoie,Karthik Soma,Haechan Mark Bong,Alice Lemieux-Bourque,Rongge Zhang,Vivek Shankar Varadharajan,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 本文分享了在火星模拟地形上进行的去中心化协作SLAM实验的经验教训，分析了有限间歇通信对性能的影响，并发布了包含实时点对点通信测量的新数据集。


<details>
  <summary>Details</summary>
Motivation: 去中心化协作SLAM对于多机器人任务在未知环境中的自主运行至关重要，特别是在月球、火星等行星探索中。需要研究在通信受限和行星环境下的实际挑战。

Method: 在火星模拟地形上进行三机器人实验，使用自组织网络通信，分析有限和间歇通信对C-SLAM性能的影响，并收集实时点对点通信吞吐量和延迟数据。

Result: 实验揭示了通信受限环境下C-SLAM的实际性能表现，识别了行星环境特有的定位挑战，并创建了包含真实通信测量的数据集。

Conclusion: 去中心化C-SLAM在行星探索中具有关键作用，通信约束是主要挑战。发布的数据集将支持未来通信受限多机器人操作的研究。

Abstract: Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure. This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets. In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network. We examine the impact of limited and intermittent communication on C-SLAM performance, as well as the unique localization challenges posed by planetary-like environments. Additionally, we introduce a novel dataset collected during our experiments, which includes real-time peer-to-peer inter-robot throughput and latency measurements. This dataset aims to support future research on communication-constrained, decentralized multirobot operations.

</details>


### [79] [WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics](https://arxiv.org/abs/2601.21129)
*Guangping Liu,Tipu Sultan,Vittorio Di Giorgio,Nick Hawkins,Flavio Esposito,Madi Babaiasl*

Main category: cs.RO

TL;DR: 提出WheelArm-Sim仿真框架，用于收集轮椅与机械臂集成控制的多模态数据集，并验证其可用于数据驱动的机器学习模型


<details>
  <summary>Details</summary>
Motivation: 当前辅助机器人研究主要关注轮椅和机械臂的独立控制，但两者集成的统一机器学习控制方法尚未充分探索，需要数据收集作为开发基础

Method: 开发WheelArm-Sim仿真框架（基于Isaac Sim），收集包含13个任务、232条轨迹、67,783个样本的多模态数据集，并实现基线模型进行动作预测验证

Result: 成功收集了大规模多模态数据集，基线模型在芥末拾取任务中验证了仿真数据的可行性，证明可用于数据驱动的集成控制模型

Conclusion: WheelArm-Sim框架为轮椅-机械臂集成控制系统开发提供了有效的数据收集工具，仿真数据可用于训练机器学习模型，推动辅助机器人集成控制研究

Abstract: Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.

</details>


### [80] [InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios](https://arxiv.org/abs/2601.21173)
*Zeyi Liu,Shuang Liu,Jihai Min,Zhaoheng Zhang,Jun Cen,Pengyu Han,Songqiao Hu,Zihan Meng,Xiao He,Donghua Zhou*

Main category: cs.RO

TL;DR: InspecSafe-V1是首个用于工业巡检安全评估的多模态基准数据集，包含真实巡检机器人采集的5种工业场景数据，提供像素级分割标注、语义场景描述和安全等级标签，涵盖7种同步感知模态。


<details>
  <summary>Details</summary>
Motivation: 工业智能化和无人巡检快速发展，但现有公共数据集多为模拟数据、单模态感知或缺乏细粒度标注，限制了工业基础模型的鲁棒场景理解和多模态安全推理能力。

Method: 从真实巡检机器人在真实工业环境中采集数据，覆盖隧道、电力设施、烧结设备、石油化工、煤炭输送栈桥5种场景，使用41台轮式和轨道式巡检机器人，在2239个有效巡检点收集5013个巡检实例。

Result: 发布了InspecSafe-V1数据集，包含可见光图像的像素级分割标注、语义场景描述和安全等级标签，以及红外视频、音频、深度点云、雷达点云、气体测量、温度、湿度7种同步感知模态。

Conclusion: InspecSafe-V1填补了工业巡检安全评估多模态基准数据集的空白，支持多模态异常识别、跨模态融合和综合安全评估，为工业基础模型发展提供重要资源。

Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.

</details>


### [81] [Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation](https://arxiv.org/abs/2601.21188)
*Hao Cheng,Feitian Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种结合移动水平估计器(MHE)和模型预测控制(MPC)的框架，用于提高机器人飞艇在风扰动环境下的飞行稳定性。


<details>
  <summary>Details</summary>
Motivation: 机器人飞艇作为轻于空气(LTA)的航空系统，具有长续航和本质安全的优点，但对风扰动高度敏感。目前缺乏针对LTA平台的扰动感知控制框架，需要解决风扰动下的稳定控制问题。

Method: 采用移动水平估计器(MHE)实时估计风扰动，并将估计结果输入到模型预测控制器(MPC)中。利用二自由度移动质量机制产生惯性和气动力矩，实现姿态和航向控制。

Result: 在顶风和侧风条件下的广泛飞行实验表明，集成的MHE-MPC框架显著优于基准PID控制，证明了其在扰动感知LTA飞行中的有效性。

Conclusion: 提出的扰动感知控制框架成功解决了LTA平台在风扰动环境下的稳定控制问题，为机器人飞艇在复杂环境中的应用提供了有效的解决方案。

Abstract: Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.

</details>


### [82] [Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies](https://arxiv.org/abs/2601.21251)
*Ce Hao,Xuanran Zhai,Yaohua Liu,Harold Soh*

Main category: cs.RO

TL;DR: SMP是一种基于扩散模型的专家混合策略，通过学习紧凑的正交技能基，并使用粘性路由在每一步仅激活少量任务相关的专家来组合动作，实现了高效的多任务机器人操作。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的策略在机器人操作中表现出色，但扩展到多任务场景时面临模型规模和演示数据成本高昂的问题。需要一种既能保持高性能又能降低推理成本的可扩展方法。

Method: 提出技能专家混合策略(SMP)：1)学习紧凑的正交技能基；2)使用粘性路由在每一步仅激活少量任务相关的专家；3)采用变分训练目标支持这一设计；4)在推理时自适应激活专家，实现快速采样而无需过大的骨干网络。

Result: 在仿真和真实双臂平台上进行多任务学习和迁移学习测试，SMP比大型扩散基线模型获得更高的成功率，同时显著降低推理成本。

Conclusion: SMP为实现可扩展、可迁移的多任务操作提供了一条实用路径：一次性学习可重用技能，仅激活所需部分，并在任务变化时快速适应。

Abstract: Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.

</details>


### [83] [Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter](https://arxiv.org/abs/2601.21297)
*Byeongjun Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 提出Deep QP Safety Filter，一种完全数据驱动的安全层，用于黑盒动态系统，结合Hamilton-Jacobi可达性与无模型学习，通过收缩损失训练神经网络，实现安全无模型控制


<details>
  <summary>Details</summary>
Motivation: 为黑盒动态系统开发完全数据驱动的安全层，解决无模型控制中的安全问题，避免预收敛阶段的失败，同时加速学习过程

Method: 结合Hamilton-Jacobi可达性与无模型学习，构建基于收缩的损失函数训练两个神经网络（安全值及其导数），学习QP安全滤波器，无需模型知识

Result: 在精确设置下，学习到的critic收敛到粘性解（及其导数），即使对于非光滑值也有效；在多种动态系统（包括混合系统）和RL任务中，显著减少预收敛失败，加速学习获得更高回报

Conclusion: Deep QP Safety Filter为安全无模型控制提供了原则性和实用的途径，在保证安全的同时提升学习效率和性能

Abstract: We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems -- even including a hybrid system -- and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.

</details>


### [84] [HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control](https://arxiv.org/abs/2601.21346)
*Wei Zuo,Chengyang Li,Yikun Wang,Bingyang Cheng,Zeyi Ren,Shuai Wang,Derrick Wing Kwan Ng,Yik-Chung Wu*

Main category: cs.RO

TL;DR: 提出分层主动调优框架HPTune，通过评估已执行和未执行动作来提升MPC运动规划器的参数调优效率，结合激光雷达实现安全敏捷的避障策略。


<details>
  <summary>Details</summary>
Motivation: 现有MPC参数调优方法通常只评估已执行动作，导致参数更新效率低下，因为失败事件（如障碍物接近或碰撞）稀疏。需要更全面的评估方法来提高调优效率。

Method: 提出分层主动调优框架HPTune：1）快速层调优采用预测接近速度和预测接近距离的风险指标；2）慢速层调优利用闭环反向传播的扩展评估损失。结合多普勒激光雷达获取障碍物速度信息，增强运动预测能力。

Result: 在高保真模拟器上的大量实验表明，HPTune实现了高效的MPC调优，在复杂环境中优于各种基线方案。能够实现情境定制的运动规划，形成安全、敏捷的避障策略。

Conclusion: HPTune通过扩展评估范围到未执行动作，结合分层调优和激光雷达感知，显著提升了MPC运动规划器的参数调优效率和性能，实现了更安全、更敏捷的避障能力。

Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.

</details>


### [85] [Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control](https://arxiv.org/abs/2601.21363)
*Weidong Huang,Zhehan Li,Hangxin Liu,Biao Hou,Yao Su,Jingwen Zhang*

Main category: cs.RO

TL;DR: 提出一种结合大规模离策略预训练与基于模型微调的人形机器人控制方法，使用SAC进行高效预训练实现零样本部署，再用基于模型方法进行安全适应


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人控制中，基于策略的方法（如PPO）虽然能实现零样本部署，但样本效率低限制了在新环境中的安全适应；离策略RL和基于模型的RL虽有更高样本效率，但大规模预训练与高效微调之间仍存在差距

Method: 1. 使用SAC（软演员-评论家）进行大规模预训练，采用大批量更新和高UTD比；2. 在新环境中使用基于模型方法进行微调，数据收集使用确定性策略，而随机探索仅限于物理信息世界模型中

Result: SAC预训练策略能够实现零样本部署到真实机器人；预训练策略可在新环境和分布外任务中通过基于模型方法进行高效微调；该方法结合了预训练阶段的大规模模拟效率与微调阶段的基于模型学习样本效率

Conclusion: 该方法成功弥合了大规模预训练与高效微调之间的差距，通过分离确定性数据收集和模型内随机探索，在保持探索覆盖的同时降低了适应过程中的随机探索风险

Abstract: Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.

</details>


### [86] [Towards Space-Based Environmentally-Adaptive Grasping](https://arxiv.org/abs/2601.21394)
*Leonidas Askianakis,Aleksandr Artemov*

Main category: cs.RO

TL;DR: 本文提出一种在学习的潜在流形中直接学习控制策略的方法，用于空间环境中的抓取任务，通过多模态融合实现更高效的强化学习。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作系统在非结构化环境中面临高维动作空间、稀疏奖励和泛化能力差的问题，特别是在空间环境等极端条件下，需要更鲁棒和高效的抓取方法。

Method: 构建一个融合多模态信息的结构化潜在表示流形，在该流形中直接学习控制策略。基于GPU加速的物理仿真，使用Soft Actor-Critic (SAC)强化学习算法，在单次操作任务中训练策略。

Result: 在少于100万环境步数内达到超过95%的任务成功率，在连续变化的抓取条件下从第一步开始就表现出色。与现有视觉基线相比收敛更快，对新颖物体、夹爪几何、环境杂乱和传感器配置具有更好的鲁棒性。

Conclusion: 在潜在空间中进行显式推理能够实现更样本高效的学习和更强的鲁棒性。虽然取得了进展，但仍需进一步研究以实现空间极端条件下的完全自适应和可泛化的抓取。

Abstract: Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.

</details>


### [87] [DSCD-Nav: Dual-Stance Cooperative Debate for Object Navigation](https://arxiv.org/abs/2601.21409)
*Weitao An,Qi Liu,Chenghao Xu,Jiayi Chai,Xu Yang,Kun Wei,Cheng Deng*

Main category: cs.RO

TL;DR: DSCD-Nav：一种双立场协作辩论导航机制，通过任务场景理解与安全信息平衡两个立场的交叉验证和证据感知仲裁，提升室内导航的可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有导航系统依赖单次评分决策，容易产生过度自信的长期错误和冗余探索。需要一种能提高部分可观测环境下动作可靠性的决策机制。

Method: 提出双立场协作辩论导航：1）构建两个互补立场：任务场景理解（TSU）关注目标进展，安全信息平衡（SIB）关注风险和信息价值；2）立场间进行协作辩论，交叉验证候选动作；3）引入导航共识仲裁（NCA）整合双方理由和证据，必要时触发轻量级微探测验证不确定选择。

Result: 在HM3Dv1、HM3Dv2和MP3D数据集上的实验表明，该方法在成功率和路径效率方面取得一致提升，同时减少了探索冗余。

Conclusion: DSCD-Nav通过立场交叉验证和证据感知仲裁机制，有效解决了单次评分决策的局限性，提高了室内导航的可靠性和效率。

Abstract: Adaptive navigation in unfamiliar indoor environments is crucial for household service robots. Despite advances in zero-shot perception and reasoning from vision-language models, existing navigation systems still rely on single-pass scoring at the decision layer, leading to overconfident long-horizon errors and redundant exploration. To tackle these problems, we propose Dual-Stance Cooperative Debate Navigation (DSCD-Nav), a decision mechanism that replaces one-shot scoring with stance-based cross-checking and evidence-aware arbitration to improve action reliability under partial observability. Specifically, given the same observation and candidate action set, we explicitly construct two stances by conditioning the evaluation on diverse and complementary objectives: a Task-Scene Understanding (TSU) stance that prioritizes goal progress from scene-layout cues, and a Safety-Information Balancing (SIB) stance that emphasizes risk and information value. The stances conduct a cooperative debate and make policy by cross-checking their top candidates with cue-grounded arguments. Then, a Navigation Consensus Arbitration (NCA) agent is employed to consolidate both sides' reasons and evidence, optionally triggering lightweight micro-probing to verify uncertain choices, preserving NCA's primary intent while disambiguating. Experiments on HM3Dv1, HM3Dv2, and MP3D demonstrate consistent improvements in success and path efficiency while reducing exploration redundancy.

</details>


### [88] [Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates](https://arxiv.org/abs/2601.21413)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种将李群积分器与标准多体系统方程对接的框架，以及将刚体运动几何一致性融入绝对坐标方程评估的方法，解决了传统多体系统建模中奇异性问题与现有仿真代码兼容性的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统多体系统建模使用绝对坐标描述刚体运动，但存在奇异性问题。虽然李群积分方法能无奇异地积分运动方程并保持几何结构，但与现有的标准方程格式不兼容，需要对仿真代码进行重大重构。本文旨在解决这一兼容性问题。

Method: 提出两个主要方法：1）建立李群积分器与标准运动方程格式的接口框架，允许使用各种绝对坐标描述多体系统，同时应用李群积分方案；2）开发将刚体运动几何一致性融入绝对坐标方程评估的方法，使用直积群SO(3)×R3和半直积群SE(3)表示刚体运动，关键是通过局部-全局转换映射更新绝对坐标。

Result: 成功实现了李群积分器与标准多体系统运动方程格式的兼容，允许在现有仿真代码中使用李群积分方法，同时保持了刚体运动的几何特性，解决了奇异性问题。

Conclusion: 本文提出的框架和方法有效解决了多体系统仿真中李群积分器与标准方程格式的兼容性问题，既保持了李群积分的几何优势，又无需对现有仿真代码进行重大重构，为多体系统建模提供了更灵活、更稳健的解决方案。

Abstract: A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.

</details>


### [89] [Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation](https://arxiv.org/abs/2601.21416)
*Alexandre Chapin,Bruno Machado,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: SBOCR（基于槽的对象中心表示）在机器人操作任务中比全局特征和密集特征具有更好的泛化能力，特别是在光照、纹理变化和干扰物存在的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作策略使用的视觉表示（全局特征和密集特征）混合了任务相关和无关信息，导致在分布变化（如光照、纹理变化、干扰物）下泛化能力差。

Method: 提出使用基于槽的对象中心表示（SBOCR），将密集特征分组为有限的对象实体，减少策略接收的噪声同时保留足够任务信息。在模拟和真实世界任务中对比了全局、密集和SBOCR表示。

Result: SBOCR策略在泛化设置中优于基于密集和全局表示的策略，即使没有任务特定的预训练。在光照、纹理变化和干扰物存在的情况下表现更好。

Conclusion: SBOCR是设计在动态真实世界机器人环境中有效泛化的视觉系统的有前景方向，通过结构化表示减少噪声同时保持任务相关信息。

Abstract: The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.

</details>


### [90] [Nimbus: A Unified Embodied Synthetic Data Generation Framework](https://arxiv.org/abs/2601.21449)
*Zeyu He,Yuchang Zhang,Yuanzhen Zhou,Miao Tao,Hengjie Li,Yang Tian,Jia Zeng,Tai Wang,Wenzhe Cai,Yilun Chen,Ning Gao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Nimbus是一个统一的合成数据生成框架，通过模块化四层架构和异步执行模型，将异构导航和操作管道集成，实现大规模分布式环境下的高效数据生成。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成管道碎片化且任务特定，导致工程效率低下和系统不稳定，无法支持基础模型训练所需的高吞吐量持续数据生成。

Method: 提出Nimbus框架，采用模块化四层架构，将轨迹规划、渲染和存储解耦为异步阶段，实现动态管道调度、全局负载均衡、分布式容错和后端特定渲染优化。

Result: 相比未优化的基线，Nimbus实现了2-3倍的端到端吞吐量提升，确保大规模分布式环境下的鲁棒长期运行，作为InternData套件的生产骨干。

Conclusion: Nimbus框架解决了合成数据生成中的碎片化和效率问题，为跨领域数据合成提供了统一解决方案，支持大规模基础模型训练。

Abstract: Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.

</details>


### [91] [4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving](https://arxiv.org/abs/2601.21454)
*Shanliang Yao,Zhuoxiao Li,Runwei Guan,Kebin Cao,Meng Xia,Fuping Hu,Sen Xu,Yong Yue,Xiaohui Zhu,Weiping Ding,Ryan Wen Liu*

Main category: cs.RO

TL;DR: 4D-CAAL：用于4D雷达-相机标定和自动标注的统一框架，通过双用途标定靶设计和自动标注流水线，解决多模态传感器融合中的标定和标注难题。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在自动驾驶中至关重要，但现有标定方法使用分离的视觉和雷达标定靶，难以建立对应关系；手动标注稀疏雷达数据既费力又不可靠。

Method: 提出双用途标定靶设计（正面棋盘格用于相机检测，背面中心角反射器用于雷达检测），开发鲁棒的对应匹配算法，并构建自动标注流水线，通过几何投影和多特征优化将相机分割标注转移到雷达点云。

Result: 实验表明该方法实现了高标定精度，同时显著减少了手动标注工作量，加速了自动驾驶鲁棒多模态感知系统的开发。

Conclusion: 4D-CAAL为4D雷达-相机系统提供了一个有效的统一解决方案，解决了标定和标注两个关键挑战，促进了多模态感知系统的发展。

Abstract: 4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.

</details>


### [92] [DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching](https://arxiv.org/abs/2601.21474)
*Xingyu Zhang,Chaofan Zhang,Boyue Zhang,Zhinan Peng,Shaowei Cui,Shuo Wang*

Main category: cs.RO

TL;DR: DexTac：基于动觉示教的视觉-触觉操作学习框架，通过捕捉人类演示的多维触觉数据（接触力分布和空间接触区域），使灵巧手能在复杂交互中自主选择并保持最佳接触区域。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧操作的数据收集和技能学习系统通常存在触觉信息维度低的问题，而接触密集型任务需要能够产生全面触觉感知运动的策略。

Method: 提出DexTac框架，基于动觉示教捕捉人类演示的多维触觉数据（包括接触力分布和空间接触区域），并将这些丰富的触觉模态集成到策略网络中。

Result: 在具有挑战性的单手注射任务中，DexTac实现了91.67%的成功率。在涉及小规模注射器的高精度场景中，该方法比仅使用力的基线方法高出31.67%。

Conclusion: 从人类演示中学习多维触觉先验对于在接触丰富的环境中实现稳健、类人的灵巧操作至关重要。

Abstract: For contact-intensive tasks, the ability to generate policies that produce comprehensive tactile-aware motions is essential. However, existing data collection and skill learning systems for dexterous manipulation often suffer from low-dimensional tactile information. To address this limitation, we propose DexTac, a visuo-tactile manipulation learning framework based on kinesthetic teaching. DexTac captures multi-dimensional tactile data-including contact force distributions and spatial contact regions-directly from human demonstrations. By integrating these rich tactile modalities into a policy network, the resulting contact-aware agent enables a dexterous hand to autonomously select and maintain optimal contact regions during complex interactions. We evaluate our framework on a challenging unimanual injection task. Experimental results demonstrate that DexTac achieves a 91.67% success rate. Notably, in high-precision scenarios involving small-scale syringes, our approach outperforms force-only baselines by 31.67%. These results underscore that learning multi-dimensional tactile priors from human demonstrations is critical for achieving robust, human-like dexterous manipulation in contact-rich environments.

</details>


### [93] [Don't double it: Efficient Agent Prediction in Occlusions](https://arxiv.org/abs/2601.21504)
*Anna Rothenhäusler,Markus Mazzola,Andreas Look,Raghu Rajan,Joschka Bödecker*

Main category: cs.RO

TL;DR: MatchInformer：一种基于Transformer的方法，通过匈牙利匹配减少冗余预测，解耦航向与运动改进轨迹预测，使用MCC评估遮挡区域推理


<details>
  <summary>Details</summary>
Motivation: 遮挡的交通参与者对自动驾驶构成重大挑战，现有学习方法虽然能推断隐藏代理的存在，但经常产生冗余的占用预测（单个代理被多次识别），这增加了下游规划和计算负担

Method: 基于最先进的SceneInformer架构，引入匈牙利匹配算法到训练过程中，强制预测与真实值之间的一对一对应；解耦代理的航向与运动以改进轨迹预测；使用马修斯相关系数（MCC）评估占用预测以处理类别不平衡

Result: 在Waymo Open Motion Dataset上的实验表明，该方法改进了对遮挡区域的推理，并比先前方法产生更准确的轨迹预测

Conclusion: MatchInformer通过集成匈牙利匹配、解耦航向与运动、以及使用MCC评估，有效减少了冗余预测，提高了遮挡区域推理和轨迹预测的准确性

Abstract: Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent's heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.

</details>


### [94] [IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation](https://arxiv.org/abs/2601.21506)
*Joonhee Lee,Hyunseung Shin,Jeonggil Ko*

Main category: cs.RO

TL;DR: IROS是一个实时室内机器人导航框架，结合了VLM级别的上下文推理和轻量级感知模块的效率，在低成本设备硬件上运行，将快速反射决策与慢速审慎推理分离，仅在必要时调用VLM。


<details>
  <summary>Details</summary>
Motivation: 现有室内机器人导航方法存在局限性：传统几何方法（如SLAM）依赖详细地图且无法解释人类导向的语义线索；VLA模型仅基于可见帧做出反应性决策，无法预见未见的交叉口或推理远处的文本线索；VLMs虽然提供丰富的上下文推理，但计算延迟高，不适合嵌入式平台的实时操作。

Method: IROS框架受双过程理论启发，将快速反射决策（系统一）与慢速审慎推理（系统二）分离，仅在必要时调用VLM。通过增强紧凑型VLMs的空间和文本线索，实现人类化导航。在低成本设备硬件上结合VLM级上下文推理与轻量级感知模块的效率。

Result: 在五个真实世界建筑中，IROS相比连续VLM导航提高了决策准确性，并将延迟减少了66%。

Conclusion: IROS框架成功实现了实时室内机器人导航，结合了语义理解和快速响应，通过分离快速反射和慢速推理过程，在保持VLM级上下文推理能力的同时显著降低了计算延迟。

Abstract: Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.

</details>


### [95] [Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning](https://arxiv.org/abs/2601.21548)
*Irene Ambrosini,Ingo Blakowski,Dmitrii Zendrikov,Cristiano Capone,Luna Gava,Giacomo Indiveri,Chiara De Luca,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 使用脉冲神经网络在混合信号神经形态处理器上实现空气曲棍球实时学习控制，通过硬件-算法协同设计实现快速强化学习


<details>
  <summary>Details</summary>
Motivation: 空气曲棍球需要在高速度下做出瞬间决策，这为神经形态计算和机器人控制提供了理想的测试平台。研究旨在将神经科学启发的硬件与真实世界机器人控制相结合，展示大脑启发方法能够处理快速交互任务并支持智能机器的持续学习。

Method: 采用混合信号模拟/数字神经形态处理器运行紧凑的脉冲神经网络。通过硬件与学习算法的协同设计，使用强化学习训练系统。网络利用固定随机连接捕捉任务的时间结构，在读出层采用局部e-prop学习规则，利用事件驱动活动实现快速高效学习。采用计算机和神经形态芯片的实时环路设置。

Result: 系统在极少试验次数内成功实现冰球交互，展示了脉冲神经网络在机器人自主系统中的实际训练能力。实现了实时学习，将神经科学启发的硬件与真实世界机器人控制相连接。

Conclusion: 大脑启发的方法能够处理快速交互任务，同时支持智能机器的持续学习。这项工作为神经形态计算在机器人控制中的应用提供了实际验证，展示了硬件-算法协同设计的优势。

Abstract: Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.

</details>


### [96] [AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation](https://arxiv.org/abs/2601.21602)
*Jianli Sun,Bin Tian,Qiyao Zhang,Chengxiang Li,Zihan Song,Zhiyong Cui,Yisheng Lv,Yonglin Tian*

Main category: cs.RO

TL;DR: 提出了首个专门针对空中操作系统的视觉-语言-动作基准AIR-VLA，包含物理仿真环境和3000个手动遥操作演示数据集，系统评估了主流VLA模型在无人机操作任务中的能力边界。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言-动作模型在地面实体智能中取得了显著成功，但在空中操作系统中的应用仍是一个未充分探索的领域。空中操作系统的浮动基座动力学、无人机与机械臂的强耦合性以及多步骤长时域操作任务特性，对现有为静态或2D移动基座设计的VLA范式构成了严重挑战。

Method: 构建了基于物理的仿真环境，发布了包含3000个手动遥操作演示的高质量多模态数据集，涵盖基座操作、物体与空间理解、语义推理和长时域规划。利用该平台系统评估了主流VLA模型和最先进的VLM模型。

Result: 实验不仅验证了将VLA范式迁移到空中系统的可行性，还通过针对空中任务的多维度指标，揭示了当前模型在无人机机动性、机械臂控制和高层规划方面的能力边界。

Conclusion: AIR-VLA为通用空中机器人研究建立了标准化测试平台和数据基础，填补了VLA模型在空中操作领域的空白。

Abstract: While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.

</details>


### [97] [From Instruction to Event: Sound-Triggered Mobile Manipulation](https://arxiv.org/abs/2601.21667)
*Hao Ju,Shaofei Huang,Hongyu Li,Zihan Ding,Si Liu,Meng Wang,Zhedong Zheng*

Main category: cs.RO

TL;DR: 提出声音触发的移动操作任务，让机器人主动感知声音物体并交互，无需显式指令，开发Habitat-Echo平台和基线方法验证可行性。


<details>
  <summary>Details</summary>
Motivation: 当前移动操作研究主要依赖预定义文本指令，限制了机器人的自主性和对动态环境事件的响应能力。需要让机器人能够主动感知和响应环境中的声音事件。

Method: 1) 提出声音触发的移动操作任务；2) 开发Habitat-Echo数据平台，集成声学渲染与物理交互；3) 设计包含高层任务规划和低层策略模型的基线方法。

Result: 实验表明基线方法能让机器人主动检测和响应听觉事件，无需逐案例指令。在具有挑战性的双声源场景中，机器人能成功从重叠声学干扰中分离主要声源执行首次交互，然后操作次要物体。

Conclusion: 声音触发的移动操作增强了机器人的自主性，提出的Habitat-Echo平台和基线方法验证了该范式的可行性，为更智能的机器人交互开辟了新方向。

Abstract: Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.

</details>


### [98] [CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation](https://arxiv.org/abs/2601.21712)
*Xuanran Zhai,Binkai Ou,Yemin Wang,Hui Yi Leong,Qiaojun Yu,Ce Hao,Yaohua Liu*

Main category: cs.RO

TL;DR: CoFreeVLA在端到端VLA模型基础上增加了短时域自碰撞风险估计器，通过门控机制阻止高风险指令、引导安全恢复并优化策略，在双机械臂任务中显著减少自碰撞并提高成功率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言动作模型在双机械臂部署中存在安全隐患，因为模型未能充分建模机械臂之间以及机械臂与抓取物体之间的自碰撞风险，这在实际操作中可能导致危险情况。

Method: CoFreeVLA在端到端VLA模型基础上集成短时域自碰撞风险估计器，该估计器利用本体感知、视觉嵌入和计划动作预测碰撞概率。系统通过门控机制阻止高风险指令，引导恢复到安全状态，并基于风险信息优化策略。采用基于模型的碰撞标签进行预训练，然后在真实机器人上通过实际运行数据进行后训练校准。

Result: 在PiPER双机械臂的五个双手机器人任务中，CoFreeVLA相比RDT和APEX方法显著减少了自碰撞发生，并提高了任务成功率。

Conclusion: CoFreeVLA通过集成自碰撞风险估计器有效提升了VLA模型在双机械臂操作中的安全性，为实际部署提供了更可靠的解决方案。

Abstract: Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.

</details>


### [99] [Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations](https://arxiv.org/abs/2601.21713)
*Donatien Delehelle,Fei Chen,Darwin Caldwell*

Main category: cs.RO

TL;DR: 本文质疑布料操作中常见的端到端学习方法，提出了一种更高效、模块化的强化学习方法，显著减小模型规模并缩短训练时间，同时在SoftGym基准测试中取得优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 布料操作对机器人来说仍然是一个开放挑战，主要困难包括高维状态空间、复杂动力学和自遮挡问题。现有基于数据的方法通常依赖大模型和长训练时间，计算成本高，且端到端学习方法使用高度信息损失的环境状态表示，进一步增加了计算负担。

Method: 本文探索了一种高效、模块化的布料操作强化学习方法。通过精心设计选择，在仿真中学习时可以显著减小模型规模和训练时间，并展示了如何将仿真训练的模型转移到现实世界。

Result: 在SoftGym基准测试中，该方法在任务上取得了显著优于可用基线的性能，同时使用了明显更小的模型。

Conclusion: 通过质疑布料操作中常见的端到端学习设计选择，本文展示了通过模块化方法可以显著提高强化学习在布料操作中的效率，减小模型规模和训练时间，同时保持或提高性能。

Abstract: Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.

</details>


### [100] [Flocking behavior for dynamic and complex swarm structures](https://arxiv.org/abs/2601.21772)
*Carmen D. R. Pita-Romero,Pedro Arias-Perez,Miguel Fernandez-Cortizas,Rafael Perez-Segui,Pascual Campoy*

Main category: cs.RO

TL;DR: 基于虚拟质心的无人机集群编队控制算法，简化复杂结构形成与轨迹跟踪


<details>
  <summary>Details</summary>
Motivation: 多无人机保持复杂编队结构并实现复杂轨迹跟踪仍是一个重大挑战，需要更简单有效的控制方法

Method: 提出基于虚拟质心的集群行为算法，在经典虚拟行为基础上扩展，提供动态控制无人机数量和编队结构的理论框架

Result: 通过仿真测试和真实实验验证，算法即使在复杂编队和复杂轨迹下也表现出简单有效的特性

Conclusion: 虚拟质心方法为无人机集群编队控制提供了简单有效的解决方案，能够处理复杂结构和轨迹

Abstract: Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.

</details>


### [101] [GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration](https://arxiv.org/abs/2601.21829)
*Bsher Karbouj,Baha Eddin Gaaloul,Jorg Kruger*

Main category: cs.RO

TL;DR: GAZELOAD是一个用于工业人机协作中脑力负荷估计的多模态数据集，包含26名参与者在与协作机器人交互时的眼动追踪数据、环境测量和任务上下文信息。


<details>
  <summary>Details</summary>
Motivation: 在工业人机协作场景中，准确估计操作员的脑力负荷对于优化协作效率和安全至关重要。现有数据集往往缺乏多模态同步数据，特别是在真实工业环境下的眼动与环境因素的综合测量。

Method: 在实验室装配测试平台上，26名参与者佩戴Meta ARIA智能眼镜与两台协作机器人（UR5和Franka Emika Panda）交互。数据集同步记录了眼动信号（瞳孔直径、注视点、扫视、视线、注视转移熵、注视分散指数）、环境实时连续测量（照度）以及任务和机器人上下文（工作台、任务块、诱导故障），并在任务难度和环境条件受控变化下收集数据。

Result: 为每位参与者和每个脑力负荷分级任务块提供了CSV文件，包含250毫秒窗口聚合的眼动指标、环境日志和1-10李克特量表的自我报告脑力负荷评分，按参与者特定文件夹组织并附带文档。

Conclusion: 该数据集可用于开发和基准测试脑力负荷估计算法、特征提取和时间建模，适用于真实工业人机协作场景，并可用于研究环境因素（如照明）对基于眼动的脑力负荷指标的影响。

Abstract: This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.

</details>


### [102] [LLM-Driven Scenario-Aware Planning for Autonomous Driving](https://arxiv.org/abs/2601.21876)
*He Li,Zhaowei Chen,Rui Gao,Guoliang Li,Qi Hao,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出LAP方法，利用大语言模型进行场景理解，通过联合优化模式配置和运动规划，实现自动驾驶中高速驾驶和精确驾驶的自适应切换。


<details>
  <summary>Details</summary>
Motivation: 现有的混合规划器切换框架在密集交通环境中存在模式转换不可靠、驾驶效率低的问题，主要原因是启发式场景识别和低频控制更新。

Method: 提出LAP方法：1) 利用LLM进行场景理解；2) 将LLM推理集成到模式配置和运动规划的联合优化中；3) 使用树搜索模型预测控制和交替最小化求解联合优化问题。

Result: 高保真仿真结果显示，LAP在驾驶时间和成功率方面均优于其他基准方法。

Conclusion: LAP方法通过LLM驱动的自适应规划，有效解决了自动驾驶在复杂交通环境中模式切换不可靠和效率低的问题。

Abstract: Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.

</details>


### [103] [Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation](https://arxiv.org/abs/2601.21884)
*Pratik Ingle,Jørn Lambertsen,Kasper Støy,Andres Faina*

Main category: cs.RO

TL;DR: MANTA-RAY平台通过分布式模块化软织物表面，以低致动器密度实现多物体并行操控，无需数据驱动训练


<details>
  <summary>Details</summary>
Motivation: 传统密集致动器阵列操控系统自由度多、复杂度高、可扩展性差，需要开发低致动器密度但保持操控性能的模块化解决方案

Method: 提出分布式模块化MANTA-RAY平台，采用模块间物体传递策略和基于几何变换的PID控制器，直接将倾斜角控制输出映射为致动器命令

Result: 系统成功操控多种几何形状、质量和纹理的物体（包括鸡蛋、苹果等易碎物品），支持并行操控，3x3和4x4配置在仿真中验证，2x2硬件原型实验证明可行性

Conclusion: 多模块MANTA-RAY提高了可扩展性，支持大区域内多物体协调操控，具有实际应用潜力

Abstract: Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.

</details>


### [104] [Information Filtering via Variational Regularization for Robot Manipulation](https://arxiv.org/abs/2601.21926)
*Jinhao Zhang,Wenlong Xia,Yaojia Wang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: 提出Variational Regularization（VR）模块，通过时间步条件高斯分布和KL散度正则化形成自适应信息瓶颈，减少扩散视觉运动策略中骨干特征的冗余噪声，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D视觉表示的扩散视觉运动策略使用过大的去噪解码器，虽然增加模型容量能改善去噪，但经验证据表明这会引入中间特征块的冗余和噪声。研究发现，在推理时随机掩码骨干特征（不改变训练）能提升性能，证实了中间特征中存在任务无关噪声。

Method: 提出Variational Regularization（VR）轻量级模块，对骨干特征施加时间步条件高斯分布，应用KL散度正则化，形成自适应信息瓶颈，减少特征冗余和噪声。

Result: 在三个仿真基准（RoboTwin2.0、Adroit和MetaWorld）上的实验表明，相比基线DP3，该方法在RoboTwin2.0上成功率提升6.1%，在Adroit和MetaWorld上提升4.1%，达到新的SOTA结果。真实世界实验进一步证明方法在实际部署中表现良好。

Conclusion: Variational Regularization通过自适应信息瓶颈有效减少扩散视觉运动策略中骨干特征的冗余噪声，显著提升性能，在实际部署中表现良好。

Abstract: Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.

</details>


### [105] [MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts](https://arxiv.org/abs/2601.21971)
*Lorenzo Mazza,Ariel Rodriguez,Rayan Younis,Martin Lelis,Ortrun Hellig,Chenpan Li,Sebastian Bodenstedt,Martin Wagner,Stefanie Speidel*

Main category: cs.RO

TL;DR: 提出一种用于结构化手术操作的监督混合专家架构，可在少量演示（<150次）下从立体内窥镜图像学习复杂长时程操作，在肠道抓取任务中表现优于现有方法，并展示出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 手术机器人模仿学习面临数据稀缺、工作空间受限、安全性和可预测性要求高等挑战，现有方法通常需要多摄像头设置或数千次演示，难以实际应用。

Method: 提出监督混合专家架构，可叠加在任何自主策略之上，结合轻量级动作解码器策略（如ACT），仅使用立体内窥镜图像和少于150次演示学习复杂长时程操作。

Result: 在肠道抓取任务中，通用VLA模型完全失败，标准ACT在分布内取得中等成功率，而监督MoE架构显著提升性能，在分布内成功率更高，在分布外场景（新抓取位置、光照减弱、部分遮挡）表现更鲁棒，能泛化到未见测试视角，并零样本迁移到离体猪组织。

Conclusion: 监督MoE架构为手术机器人模仿学习提供有前景的途径，能在少量数据下实现复杂操作学习，并展示出良好的泛化和鲁棒性，为体内部署奠定基础。

Abstract: Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.

</details>


### [106] [Macro-Scale Electrostatic Origami Motor](https://arxiv.org/abs/2601.21976)
*Alex S. Miller,Leo McElroy,Jeffrey H. Lang*

Main category: cs.RO

TL;DR: 开发了首个可折叠的宏观尺度折纸旋转电机，使用电晕放电产生扭矩，可折叠平放后展开工作


<details>
  <summary>Details</summary>
Motivation: 现有可折叠机器人要么在结构中嵌入线性执行器，要么附加非折叠旋转电机，且所有嵌入折叠结构的执行器都只能产生线性或折叠运动，无法实现连续旋转运动。宏观尺度上尚未有可折叠的连续旋转执行器

Method: 开发了基于折纸结构的旋转电机，采用电晕放电原理产生扭矩，电机可折叠平放后展开工作

Result: 原型电机实现了2.5:1的扩展比，在-29 kV驱动下达到1440 rpm的最高转速，最大输出扭矩超过0.15 mN·m，主动部件扭矩密度为0.04 Nm/kg

Conclusion: 成功开发了首个宏观尺度的可折叠折纸旋转电机，为可折叠机器人提供了连续旋转运动能力，具有高体积质量比和形状适应性优势

Abstract: Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.

</details>


### [107] [PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy](https://arxiv.org/abs/2601.22018)
*Jinhao Zhang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Wenlong Xia,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: 提出PocketDP3，一种轻量级3D扩散策略，用基于MLP-Mixer的Diffusion Mixer替换传统U-Net解码器，大幅减少参数并支持两步推理，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉扩散策略存在架构不匹配问题：小型点云编码器与庞大解码器配对，导致解码器参数浪费。需要更高效的架构设计。

Method: 提出PocketDP3，用轻量级Diffusion Mixer（基于MLP-Mixer块）替换传统条件U-Net解码器，实现时间和通道维度的高效融合。

Result: 在RoboTwin2.0、Adroit和MetaWorld三个仿真基准测试中达到SOTA性能，参数少于先前方法的1%，同时加速推理。真实世界实验验证了实用性和可迁移性。

Conclusion: PocketDP3通过架构创新解决了扩散策略的参数效率问题，实现了高性能、轻量化和实时部署的平衡，为机器人操作技能学习提供了实用解决方案。

Abstract: Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.

</details>


### [108] [mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning](https://arxiv.org/abs/2601.22074)
*Kevin Zakka,Qiayuan Liao,Brent Yi,Louis Le Lay,Koushil Sreenath,Pieter Abbeel*

Main category: cs.RO

TL;DR: mjlab是一个轻量级开源机器人学习框架，结合GPU加速仿真、可组合环境和最小化安装配置


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习框架通常需要复杂的安装配置和依赖管理，缺乏轻量级、易用且高性能的解决方案。需要结合GPU加速仿真和模块化设计来降低使用门槛。

Method: 采用Isaac Lab引入的manager-based API，用户可组合观察、奖励和事件的模块化构建块，结合MuJoCo Warp实现GPU加速物理仿真，提供原生MuJoCo数据结构直接访问

Result: 实现了单命令安装、最小依赖的框架，提供速度跟踪、运动模仿和操作任务的参考实现，显著降低机器人学习开发门槛

Conclusion: mjlab成功创建了一个轻量级、高性能的机器人学习框架，通过GPU加速仿真和模块化设计平衡了易用性和性能，为机器人学习研究提供了高效工具

Abstract: We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.

</details>


### [109] [ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection](https://arxiv.org/abs/2601.22090)
*Runsheng Wang,Katelyn Lee,Xinyue Zhu,Lauren Winterbottom,Dawn M. Nilsen,Joel Stein,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 利用健康人群sEMG预训练模型，通过少量卒中患者数据微调，提高卒中后手部康复意图检测的准确性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 卒中后手部康复中，表面肌电信号(sEMG)是重要的控制信号，但从瘫痪肌肉检测意图通常需要冗长的特定患者校准，且对变异性敏感。需要减少校准负担并提高鲁棒性。

Method: 提出健康到卒中的适应管道：先在大量健康人群sEMG数据上预训练模型，然后仅用少量卒中患者特定数据进行微调。比较了三种适应策略：仅头部调优、参数高效的LoRA适配器和端到端微调。

Result: 在包含三个慢性卒中患者的新数据集上评估，健康预训练适应在所有条件下都优于零样本迁移和相同数据预算下的卒中专用训练。最佳适应方法将平均转移准确率从0.42提高到0.61，原始准确率从0.69提高到0.78。

Conclusion: 迁移可重复使用的健康域EMG表示可以减少校准负担，同时提高卒中后实时意图检测的鲁棒性，为实际临床应用提供了有前景的解决方案。

Abstract: Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.

</details>


### [110] [DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation](https://arxiv.org/abs/2601.22153)
*Haozhe Xie,Beichen Wen,Jiarui Zheng,Zhaoxi Chen,Fangzhou Hong,Haiwen Diao,Ziwei Liu*

Main category: cs.RO

TL;DR: DynamicVLA是一个用于动态物体操作的VLA框架，通过紧凑模型架构、连续推理和潜在感知动作流式传输来解决动态场景中的感知、时序预测和连续控制挑战，并在新构建的DOM基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在静态操作中表现出色，但在需要快速感知、时序预测和连续控制的动态场景中表现不佳，缺乏有效的动态物体操作解决方案。

Method: 提出DynamicVLA框架，包含三个关键设计：1) 使用卷积视觉编码器的紧凑0.4B VLA模型；2) 连续推理实现推理与执行重叠；3) 潜在感知动作流式传输确保时序对齐的动作执行。同时构建了DOM基准数据集。

Result: 在响应速度、感知能力和泛化性能方面取得显著改进，DynamicVLA成为跨不同实现方式的通用动态物体操作统一框架。

Conclusion: DynamicVLA通过集成时序推理和闭环自适应，成功解决了动态物体操作的挑战，为VLA模型在动态场景中的应用提供了有效解决方案。

Abstract: Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.

</details>
