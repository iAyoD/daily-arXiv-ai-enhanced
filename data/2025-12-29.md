<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 30]
- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Teaching People LLM's Errors and Getting it Right](https://arxiv.org/abs/2512.21422)
*Nathan Stringham,Fateme Hashemi Chaleshtori,Xinyuan Yan,Zhichao Xu,Bei Wang,Ana Marasović*

Main category: cs.CL

TL;DR: 本文分析为何现有方法未能有效减少用户对LLM的过度依赖，发现失败模式确实存在，但自动发现方法效果不一，且需要改进评估指标。


<details>
  <summary>Details</summary>
Motivation: 用户经常在不该使用大语言模型(LLM)时过度依赖它们，因为看到LLM能作诗回答复杂问题，就错误地认为它们不会在简单任务上出错。现有方法通过聚类实例嵌入识别失败模式并教给用户，但效果不佳，本文旨在探究原因。

Method: 1) 检查是否存在失败模式：按元标签分组实例，评估LLM在各组的预测，识别规模大且错误率高的组；2) 测试提示和嵌入方法能否发现已知失败；3) 提出新评估指标衡量用户利用失败模式预测LLM错误的能力，并通过用户研究验证。

Result: 1) 确实存在可教的失败模式；2) 自动发现方法效果不一，可能解释现有方法的失败；3) 新评估指标显示教学有积极效果，而传统的人机协作准确率指标未能体现。

Conclusion: 教授失败模式可能是减少过度依赖的有效方法，但成功取决于更好的自动失败发现方法和使用本文提出的评估指标。

Abstract: People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.
  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.

</details>


### [2] [Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models](https://arxiv.org/abs/2512.21439)
*Geoffroy Morlat,Marceau Nahon,Augustin Chartouny,Raja Chatila,Ismael T. Freire,Mehdi Khamassi*

Main category: cs.CL

TL;DR: COMETH框架结合概率上下文学习、LLM语义抽象和人类道德评估，通过可解释的模型预测情境对模糊行为道德判断的影响，相比端到端LLM提示将人类判断对齐度提高约一倍。


<details>
  <summary>Details</summary>
Motivation: 道德判断不仅取决于行为结果，还受情境影响。现有方法（如端到端LLM）在预测情境敏感的道德判断方面表现有限且缺乏可解释性，需要开发能学习情境影响并解释预测的模型。

Method: 1. 构建包含300个情境、6类核心行为（不杀、不欺骗、不违法）的数据集，收集101名参与者的三元判断（责备/中立/支持）；2. 使用LLM过滤和MiniLM嵌入+K-means预处理标准化行为，形成核心行为聚类；3. COMETH框架：在线聚类学习行为特定的道德情境，使用原则性差异标准；4. 泛化模块提取简洁的二元情境特征，在基于似然的透明模型中学习特征权重。

Result: COMETH将人类多数判断对齐度从端到端LLM提示的约30%提高到约60%（约翻倍），同时能揭示驱动预测的情境特征，提供可解释的预测。

Conclusion: COMETH提供了：1）基于实证的道德情境数据集；2）结合人类判断、模型上下文学习和LLM语义的可复现流程；3）用于情境敏感道德预测和解释的可解释替代方案，优于端到端LLM方法。

Abstract: Moral actions are judged not only by their outcomes but by the context in which they occur. We present COMETH (Contextual Organization of Moral Evaluation from Textual Human inputs), a framework that integrates a probabilistic context learner with LLM-based semantic abstraction and human moral evaluations to model how context shapes the acceptability of ambiguous actions. We curate an empirically grounded dataset of 300 scenarios across six core actions (violating Do not kill, Do not deceive, and Do not break the law) and collect ternary judgments (Blame/Neutral/Support) from N=101 participants. A preprocessing pipeline standardizes actions via an LLM filter and MiniLM embeddings with K-means, producing robust, reproducible core-action clusters. COMETH then learns action-specific moral contexts by clustering scenarios online from human judgment distributions using principled divergence criteria. To generalize and explain predictions, a Generalization module extracts concise, non-evaluative binary contextual features and learns feature weights in a transparent likelihood-based model. Empirically, COMETH roughly doubles alignment with majority human judgments relative to end-to-end LLM prompting (approx. 60% vs. approx. 30% on average), while revealing which contextual features drive its predictions. The contributions are: (i) an empirically grounded moral-context dataset, (ii) a reproducible pipeline combining human judgments with model-based context learning and LLM semantics, and (iii) an interpretable alternative to end-to-end LLMs for context-sensitive moral prediction and explanation.

</details>


### [3] [Oogiri-Master: Benchmarking Humor Understanding via Oogiri](https://arxiv.org/abs/2512.21494)
*Soichiro Murakami,Hidetaka Kamigaito,Hiroya Takamura,Manabu Okumura*

Main category: cs.CL

TL;DR: 该研究通过日本创意回应游戏Oogiri，构建了Oogiri-Master基准和Oogiri-Corpus数据集，用于严格评估大语言模型的幽默理解能力，分析了与幽默相关的语言因素，并展示了先进模型接近人类表现的结果。


<details>
  <summary>Details</summary>
Motivation: 幽默是测试大语言模型类人创造性思维的重要领域，但现有研究存在局限：数据集候选回应少、评分时暴露流行度信号、缺乏客观可比的幽默度量标准。需要更严谨的方法来研究什么因素让人类觉得回应有趣。

Method: 引入Oogiri-Master基准和Oogiri-Corpus数据集，每个提示对应约100个多样化候选回应，由约100名独立人类评委评分（避免看到他人评分以减少流行度偏差）。分析文本长度、歧义性、不一致性解决等语言因素与幽默性的关联，推导预测人类判断的客观指标。

Result: 定量分析揭示了与幽默相关的具体语言因素，最先进的大语言模型在Oogiri-Master基准上接近人类表现，通过洞察增强提示可以提升模型性能。

Conclusion: 该研究为评估和推进大语言模型的幽默理解提供了原则性基础，展示了如何通过精心设计的数据集和基准来系统研究幽默这一复杂的创造性认知能力。

Abstract: Humor is a salient testbed for human-like creative thinking in large language models (LLMs). We study humor using the Japanese creative response game Oogiri, in which participants produce witty responses to a given prompt, and ask the following research question: What makes such responses funny to humans? Previous work has offered only limited reliable means to answer this question. Existing datasets contain few candidate responses per prompt, expose popularity signals during ratings, and lack objective and comparable metrics for funniness. Thus, we introduce Oogiri-Master and Oogiri-Corpus, which are a benchmark and dataset designed to enable rigorous evaluation of humor understanding in LLMs. Each prompt is paired with approximately 100 diverse candidate responses, and funniness is rated independently by approximately 100 human judges without access to others' ratings, reducing popularity bias and enabling robust aggregation. Using Oogiri-Corpus, we conduct a quantitative analysis of the linguistic factors associated with funniness, such as text length, ambiguity, and incongruity resolution, and derive objective metrics for predicting human judgments. Subsequently, we benchmark a range of LLMs and human baselines in Oogiri-Master, demonstrating that state-of-the-art models approach human performance and that insight-augmented prompting improves the model performance. Our results provide a principled basis for evaluating and advancing humor understanding in LLMs.

</details>


### [4] [Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management](https://arxiv.org/abs/2512.21567)
*Changzhi Sun,Xiangyu Chen,Jixiang Luo,Dell Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 提出DAM框架，将LLM外部内存管理重构为不确定性下的序列决策问题，用价值函数和不确定性估计评估操作，替代启发式方法


<details>
  <summary>Details</summary>
Motivation: 当前LLM系统的外部内存管理主要依赖手工设计的启发式方法，难以预测内存决策的长期和不确定性后果，需要更原则性的框架

Method: 提出DAM框架，将内存管理分解为即时信息访问和分层存储维护，通过价值函数和不确定性估计评估候选操作，基于长期效用和风险进行决策

Result: 提出了一个原则性的重构框架，阐明了启发式方法的局限性，为未来不确定性感知的内存系统研究奠定了基础

Conclusion: 内存管理应视为不确定性下的序列决策问题，DAM框架提供了决策理论视角，支持基于长期效用和风险的内存管理

Abstract: External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.

</details>


### [5] [A Unified Definition of Hallucination, Or: It's the World Model, Stupid](https://arxiv.org/abs/2512.21577)
*Emmy Liu,Varun Gangal,Chelsea Zou,Xiaoqi Huang,Michael Yu,Alex Chang,Zhuofu Tao,Sachin Kumar,Steven Y. Feng*

Main category: cs.CL

TL;DR: 本文提出了一个统一的幻觉定义：幻觉是语言模型内部世界建模的不准确性，表现为与参考世界模型（如知识库、上下文等）的冲突。通过这一统一定义，作者旨在澄清幻觉概念、改进评估方法，并规划基于合成世界模型的基准测试。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络语言模型发展已久，幻觉问题至今仍是前沿大语言模型的核心挑战。现有文献对幻觉的定义多样且分散，缺乏统一框架，导致评估标准不一致，难以比较不同基准和缓解技术。本文旨在通过历史梳理，建立一个统一的幻觉定义，为研究和评估提供共同语言。

Method: 作者从历史角度梳理了文献中各种幻觉定义，将其整合为一个统一框架：幻觉是内部世界建模的不准确性，表现为与参考世界模型的冲突。通过变化参考世界模型（如知识库、上下文等）和知识冲突策略，可以涵盖现有各种定义。基于此定义，作者规划了基于合成世界模型的基准测试家族。

Result: 提出了一个统一的幻觉定义框架，将幻觉核心界定为不准确的世界建模。该框架要求评估明确其假设的"世界"或真相来源，澄清了幻觉与规划错误、奖励/激励相关错误的区别，为比较基准和缓解技术提供了共同语言。并基于此规划了合成世界模型基准测试的发展方向。

Conclusion: 统一的幻觉定义框架有助于澄清概念混淆，改进评估方法，促进不同研究之间的比较。通过基于合成世界模型的基准测试，可以更好地压力测试和改进语言模型的世界建模能力，为解决幻觉问题提供系统性方法。

Abstract: Despite numerous attempts to solve the issue of hallucination since the inception of neural language models, it remains a problem in even frontier large language models today. Why is this the case? We walk through definitions of hallucination used in the literature from a historical perspective up to the current day, and fold them into a single definition of hallucination, wherein different prior definitions focus on different aspects of our definition. At its core, we argue that hallucination is simply inaccurate (internal) world modeling, in a form where it is observable to the user (e.g., stating a fact which contradicts a knowledge base, or producing a summary which contradicts a known source). By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), we arrive at the different existing definitions of hallucination present in the literature.
  We argue that this unified view is useful because it forces evaluations to make clear their assumed "world" or source of truth, clarifies what should and should not be called hallucination (as opposed to planning or reward/incentive-related errors), and provides a common language to compare benchmarks and mitigation techniques. Building on this definition, we outline plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments, and sketch out how these benchmarks can use such settings to stress-test and improve the world modeling components of language models.

</details>


### [6] [Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM](https://arxiv.org/abs/2512.21580)
*Alexander Podolskiy,Semen Molokov,Timofey Gerasin,Maksim Titov,Alexey Rukhovich,Artem Khrapov,Kirill Morozov,Evgeny Tetin,Constantine Korikov,Pavel Efimov,Polina Lazukova,Yuliya Skripkar,Nikita Okhotnikov,Irina Piontkovskaya,Meng Xiaojun,Zou Xueyi,Zhang Zhenhe*

Main category: cs.CL

TL;DR: Gamayun是一个15亿参数的多语言模型，采用两阶段预训练策略，在资源受限环境中表现优异，在多项基准测试中超越了更大训练预算的模型。


<details>
  <summary>Details</summary>
Motivation: 解决小型非英语中心语言模型研究的不足，为资源受限环境提供高效部署方案，特别关注俄语等非英语语言支持。

Method: 采用新颖的两阶段预训练策略：1）平衡多语言训练实现跨语言对齐；2）高质量英语丰富化以将性能增益转移到其他语言。模型从零开始训练，使用25万亿token数据。

Result: 在15亿参数规模下，超越了LLaMA3.2-1B（9T tokens）的所有基准测试，在广泛英语和多语言任务上超越Qwen2.5-1.5B（18T tokens），在非高级STEM任务上与Qwen3（36T tokens）相当或更好，在俄语任务上达到同类规模模型的最先进水平。

Conclusion: Gamayun证明了通过精心设计的训练策略，小型多语言模型可以在资源受限环境中实现卓越性能，为多语言AI部署提供了高效解决方案。

Abstract: We present Gamayun, a 1.5B-parameter multilingual language model trained entirely from scratch on 2.5T tokens. Designed for efficiency and deployment in resource-constrained environments, Gamayun addresses the lack of research on small non-English-centric LLMs by adopting a novel two-stage pre-training strategy: balanced multilingual training for cross-lingual alignment, followed by high-quality English enrichment to transfer performance gains across languages. Our model supports 12 languages, with special focus on Russian. Despite a significantly smaller training budget than comparable models, Gamayun outperforms LLaMA3.2-1B (9T tokens) on all considered benchmarks, and surpasses Qwen2.5-1.5B (18T tokens) on a wide range of English and multilingual tasks. It matches or exceeds Qwen3 (36T tokens) on most tasks outside advanced STEM, achieving state-of-the-art results in Russian, including the MERA benchmark, among the models of comparable size (1-2B parameters).

</details>


### [7] [Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2512.21625)
*Xinyu Tang,Yuliang Zhan,Zhixun Li,Wayne Xin Zhao,Zhenduo Zhang,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 论文提出A3PO方法，通过自适应非对称的token级优势信号分配来优化大型推理模型的强化学习训练


<details>
  <summary>Details</summary>
Motivation: 研究强化学习验证奖励（RLVR）训练中正负样本极性对训练动态和行为的影响，发现现有方法在优势信号分配上不够精确

Method: 提出A3PO方法：自适应非对称的token级优势信号分配策略，更精确地为不同极性的关键token分配优势信号

Result: 在五个推理基准测试中验证了A3PO方法的有效性

Conclusion: 正负样本在RLVR训练中发挥不同作用，通过精确的token级优势信号分配可以显著提升推理模型的训练效果

Abstract: Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.

</details>


### [8] [Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations](https://arxiv.org/abs/2512.21635)
*Chengxu Yang,Jingling Yuan,Siqi Cai,Jiawei Jiang,Chuang Hu*

Main category: cs.CL

TL;DR: HIC-Bench：将LLM幻觉分为智能幻觉(IH)与缺陷幻觉(DH)的评估框架，用于系统研究LLM创造力与准确性之间的非线性关系


<details>
  <summary>Details</summary>
Motivation: 现有研究主要将幻觉视为需要最小化的错误，但忽视了某些幻觉可能包含创造性或有认知价值的内容。当前幻觉检测方法过于关注事实一致性，难以处理异质科学任务，也无法平衡创造力与准确性

Method: 提出HIC-Bench框架：1) 结构化IH/DH评估，整合托兰斯创造性思维测试指标与幻觉特定维度；2) 跨领域适用性，涵盖10个科学领域；3) 动态提示优化，使用动态幻觉提示引导模型；采用多LLM评委平均评分减少偏差，人工标注验证分类

Result: 实验揭示了IH与DH之间的非线性关系，表明创造力与正确性可以共同优化。智能幻觉可作为创造力的催化剂，LLM幻觉具有推动科学创新的潜力

Conclusion: HIC-Bench为研究LLM幻觉的创造性智能提供了有价值的平台，重新定位了智能幻觉作为创造力催化剂的作用，揭示了LLM幻觉驱动科学创新的能力

Abstract: Hallucinations in large language models (LLMs) are commonly regarded as errors to be minimized. However, recent perspectives suggest that some hallucinations may encode creative or epistemically valuable content, a dimension that remains underquantified in current literature. Existing hallucination detection methods primarily focus on factual consistency, struggling to handle heterogeneous scientific tasks and balance creativity with accuracy. To address these challenges, we propose HIC-Bench, a novel evaluation framework that categorizes hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH), enabling systematic investigation of their interplay in LLM creativity. HIC-Bench features three core characteristics: (1) Structured IH/DH Assessment. using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation); (2) Cross-Domain Applicability. spanning ten scientific domains with open-ended innovation tasks; and (3) Dynamic Prompt Optimization. leveraging the Dynamic Hallucination Prompt (DHP) to guide models toward creative and reliable outputs. The evaluation process employs multiple LLM judges, averaging scores to mitigate bias, with human annotators verifying IH/DH classifications. Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized. These insights position IH as a catalyst for creativity and reveal the ability of LLM hallucinations to drive scientific innovation.Additionally, the HIC-Bench offers a valuable platform for advancing research into the creative intelligence of LLM hallucinations.

</details>


### [9] [Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech](https://arxiv.org/abs/2512.21706)
*Shuchang Pan,Siddharth Banerjee,Dhruv Hebbar,Siddhant Patel,Akshaj Gupta,Kan Jen Cheng,Hanjo Kim,Zeyi Austin Li,Martin Q. Ma,Tingle Li,Gopala Anumanchipalli,Jiachen Lian*

Main category: cs.CL

TL;DR: 提出基于因果推理的图思维框架，用于建模对话中的意图-行为路径，实现全双工交互系统的自然对话推理


<details>
  <summary>Details</summary>
Motivation: 人类对话由隐含的思维链组织，表现为定时的言语行为。捕捉这种因果路径是构建自然全双工交互系统的关键，但现有方法缺乏对这种因果关系的建模能力

Method: 引入图思维框架，将对话过程建模为因果推理。采用分层标注方案预测高层交际意图和低层言语行为，学习其因果和时间依赖关系。使用混合语料库（可控模拟对话+人工标注推理+真实对话）训练系统，将流式预测构建为演化图，使多模态Transformer能够预测下一个言语行为、生成决策理由并动态优化推理

Result: 在合成和真实全双工对话上的实验表明，该框架实现了鲁棒的行为检测，产生了可解释的推理链，并为全双工口语对话系统的对话推理基准测试奠定了基础

Conclusion: 图思维框架通过建模对话中的因果推理路径，为构建自然全双工交互系统提供了理论基础和技术实现，在行为检测和推理可解释性方面表现出色

Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this causal pathway is key to building natural full-duplex interactive systems. We introduce a framework that enables reasoning over conversational behaviors by modeling this process as causal inference within a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a hybrid corpus that pairs controllable, event-rich simulations with human-annotated rationales and real conversational speech. The GoT framework structures streaming predictions as an evolving graph, enabling a multimodal transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

</details>


### [10] [MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles](https://arxiv.org/abs/2512.21708)
*Jing Han,Binwei Yan,Tianyu Guo,Zheyuan Bai,Mengyu Zheng,Hanting Chen,Ying Nie*

Main category: cs.CL

TL;DR: 提出Mixture-of-Roles (MoR)框架，通过三个专门化的LoRA组（推理者、执行者、总结者）实现参数高效微调，用于提升LLM在智能体任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在智能体任务微调方面取得进展，但参数高效微调方法在该领域仍未被充分探索。现有方法通常对整个模型进行微调，缺乏针对智能体任务特定角色的专门化设计。

Method: 1) 将智能体能力分解为三个角色：推理者（理解查询、决定下一步角色）、执行者（识别调用函数和参数）、总结者（提炼对话信息）；2) 提出MoR框架，包含三个专门的LoRA组，每个组对应一个角色；3) 开发基于公开数据集的多角色数据生成流程，包含角色特定内容补全和可靠性验证。

Result: 在多种LLM和智能体基准测试上进行了广泛实验和消融研究，证明了所提方法的有效性。项目已公开可用。

Conclusion: MoR框架通过角色分解和专门化LoRA组，实现了参数高效的智能体微调，为LLM在智能体任务中的参数高效微调提供了新思路。

Abstract: Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant Reason+Action paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification. We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io.

</details>


### [11] [Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers](https://arxiv.org/abs/2512.21709)
*Md. Rakibul Islam,Most. Sharmin Sultana Samu,Md. Zahid Hossain,Farhad Uz Zaman,Md. Kamrozzaman Bhuiyan*

Main category: cs.CL

TL;DR: 该研究探索了孟加拉语中AI生成文本的检测，评估了五种基于Transformer的模型，发现零样本评估效果不佳，但经过微调后XLM-RoBERTa等模型能达到约91%的准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能生成类似人类的文本，引发了对虚假信息和内容滥用的担忧。虽然已有多种语言的检测研究，但孟加拉语因其丰富的词汇和复杂结构而未被充分探索，需要专门的研究来应对这一挑战。

Method: 研究调查了五种基于Transformer的模型：XLMRoBERTa-Large、mDeBERTaV3-Base、BanglaBERT-Base、IndicBERT-Base和MultilingualBERT-Base。首先进行零样本评估，然后进行任务特定的微调。

Result: 零样本评估显示所有模型表现接近随机水平（约50%准确率）。微调后性能显著提升，XLM-RoBERTa、mDeBERTa和MultilingualBERT在准确率和F1分数上均达到约91%。IndicBERT表现相对较弱。

Conclusion: 该研究推进了孟加拉语AI生成文本检测，为构建强大的检测系统奠定了基础，证明了微调对提升检测性能的重要性，特别是在孟加拉语这种复杂语言中。

Abstract: Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.

</details>


### [12] [Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought](https://arxiv.org/abs/2512.21711)
*Yuyi Zhang,Boyu Tang,Tianjie Ju,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: COCONUT（Chain-of-Continuous-Thought）作为潜在token推理方法，实际上是一种伪推理机制，它依赖数据集伪影而非真实推理，通过生成看似合理的推理轨迹来掩盖对捷径的依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在token在增强大语言模型推理方面受到关注，但其内部机制仍不明确。本文从可靠性角度研究该问题，旨在揭示潜在token推理方法（特别是COCONUT）是否真正编码了忠实推理过程，还是仅仅依赖捷径。

Method: 采用两种互补方法：1）引导实验：扰动COCONUT和显式CoT的特定token子集，比较敏感性；2）捷径实验：在偏见和分布外设置下评估模型，检验是否依赖数据集伪影。

Result: 在MMLU和HotpotQA上的结果显示：COCONUT token对引导扰动不敏感，缺乏推理关键信息；在捷径实验中，COCONUT持续利用数据集伪影来提升基准性能，而非进行真实推理。

Conclusion: COCONUT是一种伪推理机制，它生成看似合理的推理轨迹来掩盖对捷径的依赖，而非忠实表示推理过程。潜在token作为不可解释的占位符，促进捷径使用而非真正推理。

Abstract: Latent tokens are gaining attention for enhancing reasoning in large language models (LLMs), yet their internal mechanisms remain unclear. This paper examines the problem from a reliability perspective, uncovering fundamental weaknesses: latent tokens function as uninterpretable placeholders rather than encoding faithful reasoning. While resistant to perturbation, they promote shortcut usage over genuine reasoning. We focus on Chain-of-Continuous-Thought (COCONUT), which claims better efficiency and stability than explicit Chain-of-Thought (CoT) while maintaining performance. We investigate this through two complementary approaches. First, steering experiments perturb specific token subsets, namely COCONUT and explicit CoT. Unlike CoT tokens, COCONUT tokens show minimal sensitivity to steering and lack reasoning-critical information. Second, shortcut experiments evaluate models under biased and out-of-distribution settings. Results on MMLU and HotpotQA demonstrate that COCONUT consistently exploits dataset artifacts, inflating benchmark performance without true reasoning. These findings reposition COCONUT as a pseudo-reasoning mechanism: it generates plausible traces that conceal shortcut dependence rather than faithfully representing reasoning processes.

</details>


### [13] [CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation](https://arxiv.org/abs/2512.21715)
*Rui Ke,Jiahui Xu,Shenghao Yang,Kuang Wang,Feng Jiang,Haizhou Li*

Main category: cs.CL

TL;DR: CATCH是一个用于主题检测的统一框架，通过上下文感知主题表示、偏好引导主题聚类和分层主题生成，解决了对话系统中主题检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏、简短的话语时难以准确表示主题，且无法捕捉跨对话的用户级主题偏好。主题检测需要跨对话一致性和个性化用户偏好对齐，这带来了重大挑战。

Method: CATCH框架包含三个核心组件：1) 上下文感知主题表示，利用周围主题片段丰富话语级语义；2) 偏好引导主题聚类，联合建模语义邻近性和个性化反馈以对齐跨对话主题；3) 分层主题生成机制，抑制噪声并产生鲁棒、连贯的主题标签。

Result: 在多领域客户对话基准（DSTC-12）上的实验表明，CATCH在主题聚类和主题生成质量方面都表现出有效性，使用了8B LLM。

Conclusion: CATCH框架通过整合上下文感知表示、个性化聚类和分层生成，有效解决了主题检测中的关键挑战，为对话系统提供了更准确的主题识别能力。

Abstract: Theme detection is a fundamental task in user-centric dialogue systems, aiming to identify the latent topic of each utterance without relying on predefined schemas. Unlike intent induction, which operates within fixed label spaces, theme detection requires cross-dialogue consistency and alignment with personalized user preferences, posing significant challenges. Existing methods often struggle with sparse, short utterances for accurate topic representation and fail to capture user-level thematic preferences across dialogues. To address these challenges, we propose CATCH (Controllable Theme Detection with Contextualized Clustering and Hierarchical Generation), a unified framework that integrates three core components: (1) context-aware topic representation, which enriches utterance-level semantics using surrounding topic segments; (2) preference-guided topic clustering, which jointly models semantic proximity and personalized feedback to align themes across dialogue; and (3) a hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels. Experiments on a multi-domain customer dialogue benchmark (DSTC-12) demonstrate the effectiveness of CATCH with 8B LLM in both theme clustering and topic generation quality.

</details>


### [14] [Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation](https://arxiv.org/abs/2512.21787)
*Abdullah Alabdullah,Lifeng Han,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文提出了Ara-HOPE框架，这是一个针对阿拉伯方言到现代标准阿拉伯语翻译评估的人类中心化后编辑评估框架，包含五类错误分类法和决策树标注协议，能有效评估翻译系统性能差异。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言到现代标准阿拉伯语翻译存在显著差异，现有自动评估指标和通用人工评估框架难以捕捉方言特定的翻译错误，阻碍了翻译评估的进展。

Method: 提出Ara-HOPE框架，包含五类错误分类法和决策树标注协议，用于系统评估三个翻译系统：阿拉伯中心化的Jais、通用GPT-3.5和基线NLLB-200。

Result: Ara-HOPE能有效突出不同系统间的性能差异，结果显示方言特定术语和语义保留是阿拉伯方言翻译中最持久的挑战。

Conclusion: Ara-HOPE为评估阿拉伯方言翻译质量建立了新框架，并为改进方言感知翻译系统提供了可操作的指导。

Abstract: Dialectal Arabic to Modern Standard Arabic (DA-MSA) translation is a challenging task in Machine Translation (MT) due to significant lexical, syntactic, and semantic divergences between Arabic dialects and MSA. Existing automatic evaluation metrics and general-purpose human evaluation frameworks struggle to capture dialect-specific MT errors, hindering progress in translation assessment. This paper introduces Ara-HOPE, a human-centric post-editing evaluation framework designed to systematically address these challenges. The framework includes a five-category error taxonomy and a decision-tree annotation protocol. Through comparative evaluation of three MT systems (Arabic-centric Jais, general-purpose GPT-3.5, and baseline NLLB-200), Ara-HOPE effectively highlights systematic performance differences between these systems. The results show that dialect-specific terminology and semantic preservation remain the most persistent challenges in DA-MSA translation. Ara-HOPE establishes a new framework for evaluating Dialectal Arabic MT quality and provides actionable guidance for improving dialect-aware MT systems.

</details>


### [15] [Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning](https://arxiv.org/abs/2512.21789)
*Ting-Hao K. Huang,Ryan A. Rossi,Sungchul Kim,Tong Yu,Ting-Yao E. Hsu,Ho Yin,Ng,C. Lee Giles*

Main category: cs.CL

TL;DR: SciCap项目回顾：从2011-2025年科学图表标题生成研究的五年发展历程，总结了技术方法教训并提出了未来五大挑战方向


<details>
  <summary>Details</summary>
Motivation: 测试领域特定训练（在文本模型如SciBERT中成功的方法）是否同样适用于图表标题生成，探索科学图表标题自动生成的可能性

Method: 1. 从arXiv论文中收集和发布大量图表-标题对数据集；2. 进行自动和人工评估；3. 应对大语言模型兴起；4. 举办年度挑战赛；5. 构建交互式系统帮助科学家撰写更好的标题

Result: 项目从宾州大学种子基金项目发展成为塑造科学图表标题生成领域的核心努力，建立了多机构合作，创建了持续更新的数据集，并开发了实用工具

Conclusion: 总结了五年来的技术和方法教训，提出了科学图表标题生成领域未来研究的五大未解决挑战和发展方向

Abstract: Between 2021 and 2025, the SciCap project grew from a small seed-funded idea at The Pennsylvania State University (Penn State) into one of the central efforts shaping the scientific figure-captioning landscape. Supported by a Penn State seed grant, Adobe, and the Alfred P. Sloan Foundation, what began as our attempt to test whether domain-specific training, which was successful in text models like SciBERT, could also work for figure captions expanded into a multi-institution collaboration. Over these five years, we curated, released, and continually updated a large collection of figure-caption pairs from arXiv papers, conducted extensive automatic and human evaluations on both generated and author-written captions, navigated the rapid rise of large language models (LLMs), launched annual challenges, and built interactive systems that help scientists write better captions. In this piece, we look back at the first five years of SciCap and summarize the key technical and methodological lessons we learned. We then outline five major unsolved challenges and propose directions for the next phase of research in scientific figure captioning.

</details>


### [16] [On The Conceptualization and Societal Impact of Cross-Cultural Bias](https://arxiv.org/abs/2512.21809)
*Vitthal Bhandari*

Main category: cs.CL

TL;DR: 该论文分析了2025年发表的20篇关于NLP文化偏见的文献，提出了一套观察框架，旨在帮助研究者更具体地概念化偏见并有效评估其危害，倡导对语言技术的社会影响进行更稳健的评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然能基于文化背景生成回应，但存在跨文化泛化问题。更重要的是，研究者在评估语言技术文化偏见时，往往不接触实际使用该技术的利益相关者，这回避了他们本应解决的核心问题。因此需要建立更有效的文化偏见评估框架。

Method: 受arXiv:2005.14050v2工作的启发，作者系统分析了2025年发表的20篇关于NLP文化偏见识别与评估的文献，从中提炼出一套观察框架和指导原则。

Result: 通过对20篇论文的分析，作者提出了一套具体的观察框架，帮助NLP研究者更具体地概念化文化偏见，并有效评估其危害。这些观察旨在指导未来的研究实践。

Conclusion: 论文倡导对表现出跨文化偏见的语言技术进行更稳健的社会影响评估，强调需要建立更具体、有效的文化偏见评估框架，确保研究真正解决实际问题而非回避核心问题。

Abstract: Research has shown that while large language models (LLMs) can generate their responses based on cultural context, they are not perfect and tend to generalize across cultures. However, when evaluating the cultural bias of a language technology on any dataset, researchers may choose not to engage with stakeholders actually using that technology in real life, which evades the very fundamental problem they set out to address.
  Inspired by the work done by arXiv:2005.14050v2, I set out to analyse recent literature about identifying and evaluating cultural bias in Natural Language Processing (NLP). I picked out 20 papers published in 2025 about cultural bias and came up with a set of observations to allow NLP researchers in the future to conceptualize bias concretely and evaluate its harms effectively. My aim is to advocate for a robust assessment of the societal impact of language technologies exhibiting cross-cultural bias.

</details>


### [17] [Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments](https://arxiv.org/abs/2512.21817)
*Hong Su*

Main category: cs.CL

TL;DR: 提出Method Decoration (DeMe)框架，通过装饰LLM的方法生成路径来适应动态环境，解决现有方法无法系统应对新场景的问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能物联网系统依赖LLM生成任务执行方法，但缺乏系统应对未知场景的能力，且依赖固定的设备特定逻辑，无法适应环境变化。

Method: DeMe框架通过从隐藏目标、积累的学习方法和环境反馈中提取显式装饰来修改LLM的方法生成路径。装饰不是硬编码的，而是从通用行为原则、经验和观察到的环境差异中提取。支持预装饰、后装饰、中间步骤修改和步骤插入四种装饰方式。

Result: 实验结果表明，方法装饰使物联网设备在面对未知或故障操作条件时能够推导出更合适的方法。

Conclusion: DeMe框架通过装饰LLM的方法生成路径，实现了上下文感知、安全对齐和环境自适应的方法生成，提升了智能物联网系统在动态环境中的适应性。

Abstract: Intelligent IoT systems increasingly rely on large language models (LLMs) to generate task-execution methods for dynamic environments. However, existing approaches lack the ability to systematically produce new methods when facing previously unseen situations, and they often depend on fixed, device-specific logic that cannot adapt to changing environmental conditions.In this paper, we propose Method Decoration (DeMe), a general framework that modifies the method-generation path of an LLM using explicit decorations derived from hidden goals, accumulated learned methods, and environmental feedback. Unlike traditional rule augmentation, decorations in DeMe are not hardcoded; instead, they are extracted from universal behavioral principles, experience, and observed environmental differences. DeMe enables the agent to reshuffle the structure of its method path-through pre-decoration, post-decoration, intermediate-step modification, and step insertion-thereby producing context-aware, safety-aligned, and environment-adaptive methods. Experimental results show that method decoration allows IoT devices to derive ore appropriate methods when confronting unknown or faulty operating conditions.

</details>


### [18] [Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco](https://arxiv.org/abs/2512.21837)
*Siyu Li,Chenwei Song,Wan Zhou,Xinyi Liu*

Main category: cs.CL

TL;DR: 提出一种结合图结构信息的LLM方法，用于烟草病虫害防治的知识推理，通过知识图谱增强检索和推理能力。


<details>
  <summary>Details</summary>
Motivation: 烟草病虫害防治需要准确的知识推理，传统方法难以处理复杂的多跳和比较推理场景，需要结合结构化知识提升LLM的推理能力。

Method: 基于GraphRAG框架，首先用LLM辅助构建烟草病虫害知识图谱，然后结合GNN学习节点表示，使用ChatGLM作为骨干模型并通过LoRA进行参数高效微调。

Result: 实验结果表明该方法在多个评估指标上均优于基线方法，显著提高了推理准确性和深度，特别是在复杂多跳和比较推理场景中。

Conclusion: 结合图结构信息的LLM方法能有效提升烟草病虫害防治领域的知识推理能力，为领域特定应用提供了有效的解决方案。

Abstract: This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.

</details>


### [19] [AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts](https://arxiv.org/abs/2512.21842)
*Baorong Huang,Ali Asiri*

Main category: cs.CL

TL;DR: 提出AlignAR生成式句子对齐方法和新的阿拉伯语-英语数据集，包含复杂法律和文学文本，评估显示传统对齐方法在困难数据集上表现不佳，而基于LLM的方法更稳健，F1分数达85.5%


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语-英语平行语料库稀缺，现有数据集主要是简单的一对一映射，缺乏复杂文本的对齐资源，限制了机器翻译研究和翻译教学的发展

Method: 提出AlignAR生成式句子对齐方法，构建包含复杂法律和文学文本的新阿拉伯语-英语数据集，通过减少一对一映射创建"困难"子集来评估不同对齐方法

Result: "简单"数据集缺乏区分对齐方法的能力，"困难"子集暴露了传统对齐方法的局限性，基于LLM的方法表现更稳健，整体F1分数达到85.5%，比先前方法提升9%

Conclusion: 需要更复杂的数据集来充分评估句子对齐方法，基于LLM的方法在处理复杂对齐任务时表现优异，开源的数据集和代码将促进相关研究发展

Abstract: High-quality parallel corpora are essential for Machine Translation (MT) research and translation teaching. However, Arabic-English resources remain scarce and existing datasets mainly consist of simple one-to-one mappings. In this paper, we present AlignAR, a generative sentence alignment method, and a new Arabic-English dataset comprising complex legal and literary texts. Our evaluation demonstrates that "Easy" datasets lack the discriminatory power to fully assess alignment methods. By reducing one-to-one mappings in our "Hard" subset, we exposed the limitations of traditional alignment methods. In contrast, LLM-based approaches demonstrated superior robustness, achieving an overall F1-score of 85.5%, a 9% improvement over previous methods. Our datasets and codes are open-sourced at https://github.com/XXX.

</details>


### [20] [HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs](https://arxiv.org/abs/2512.21849)
*Jiaxin Liu,Peiyi Tu,Wenyu Chen,Yihong Zhuang,Xinxia Ling,Anji Zhou,Chenxi Wang,Zhuo Han,Zhengkai Yang,Junbo Zhao,Zenan Huang,Yuanyuan Wang*

Main category: cs.CL

TL;DR: HeartBench是一个评估中文大语言模型在情感、文化和伦理维度上类人智能的框架，基于真实心理咨询场景开发，包含5个主要维度和15项次级能力，评估显示现有模型仅能达到专家理想分数的60%。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在认知和推理基准测试中表现出色，但在类人智能方面存在明显不足，特别是在中文语言文化背景下，缺乏专门的评估框架和高质量的社会情感数据阻碍了进展。

Method: 基于真实心理咨询场景，与临床专家合作开发理论驱动的分类法（5个主要维度，15个次级能力），采用案例特定、基于量规的方法，通过"先推理后评分"的评估协议将抽象的人类特质转化为可测量的标准。

Result: 评估13个最先进的大语言模型显示存在显著性能上限：即使领先模型也只能达到专家定义理想分数的60%。在涉及微妙情感潜台词和复杂伦理权衡的"困难集"场景中，模型性能显著下降。

Conclusion: HeartBench为类人AI评估建立了标准化指标，并为构建高质量、人类对齐的训练数据提供了方法论蓝图，揭示了当前模型在情感、文化和伦理智能方面的局限性。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.

</details>


### [21] [TimeBill: Time-Budgeted Inference for Large Language Models](https://arxiv.org/abs/2512.21859)
*Qi Fan,An Zou,Yehan Ma*

Main category: cs.CL

TL;DR: TimeBill是一个面向时间预算的LLM推理框架，通过预测响应长度和执行时间，自适应调整KV缓存淘汰比例，在给定时间预算内平衡推理效率和响应质量。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地部署在时间关键系统中（如机器人、自动驾驶、工业自动化），这些系统需要在给定时间预算内生成准确响应。然而，自回归生成过程使得端到端执行时间难以建模和估计，而现有的基于固定KV缓存淘汰比例的高效推理方法无法适应具有不同时间预算的多样化任务。

Method: 提出TimeBill框架：1）细粒度响应长度预测器（RLP）预测LLM输出长度；2）执行时间估计器（ETE）准确预测端到端执行时间；3）基于时间预算的高效推理方法，根据执行时间预测和给定时间预算自适应调整KV缓存淘汰比例。

Result: 通过大量实验证明TimeBill在各种超时策略下提高任务完成率并保持响应性能的优势。

Conclusion: TimeBill是一个有效的时间预算推理框架，能够在时间关键系统中平衡LLM推理效率和响应质量，适应不同任务的时间预算要求。

Abstract: Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.

</details>


### [22] [Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?](https://arxiv.org/abs/2512.21871)
*Naen Xu,Jinghuai Zhang,Changjiang Li,Hengyu An,Chunyi Zhou,Jun Wang,Boyu Xu,Yuyuan Li,Tianyu Du,Shouling Ji*

Main category: cs.CL

TL;DR: 该论文评估大型视觉语言模型处理版权内容的能力，发现即使最先进的模型也存在显著缺陷，并提出了工具增强的防御框架来降低侵权风险。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态推理任务中取得了显著进展，但其广泛可访问性引发了版权侵权的关键担忧。模型在遇到版权内容时能否准确识别并遵守版权法规，避免产生法律和伦理后果，是本研究的主要动机。

Method: 1. 创建包含50,000个多模态查询-内容对的大规模基准数据集，涵盖书籍摘录、新闻文章、音乐歌词和代码文档等版权内容；2. 数据集包含有版权声明和无版权声明两种场景；3. 引入工具增强的防御框架来确保版权合规。

Result: 评估显示，即使最先进的闭源大型视觉语言模型在识别和尊重版权内容方面也存在显著缺陷，即使提供了版权声明。提出的工具增强防御框架在所有场景中都能有效降低侵权风险。

Conclusion: 开发版权感知的大型视觉语言模型对于确保负责任和合法使用版权内容至关重要。当前模型在版权合规方面存在不足，需要专门的防御机制来应对这一挑战。

Abstract: Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.

</details>


### [23] [CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics](https://arxiv.org/abs/2512.21877)
*Vaibhav Devraj,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

TL;DR: CricBench：首个针对板球数据分析的文本到SQL基准测试，包含英语和印地语查询，评估发现专业领域性能显著下降，且印地语查询表现与英语相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 板球作为全球第二大运动，拥有超过25亿粉丝，但现有LLM在处理板球领域特定复杂查询时存在能力差距。需要评估LLM在专业体育分析中的表现，特别是处理领域特定模式、复杂模式变化和多语言需求的能力。

Method: 创建CricBench基准测试套件：1）与板球和SQL领域专家合作手动编写复杂查询作为"黄金标准"数据集；2）构建英语和印地语双语基准；3）评估6个SOTA模型（包括GPT-4o、Claude 3.7 Sonnet和开源模型）；4）采用严格评估协议。

Result: 1）通用基准的高性能不能保证专业领域成功；2）开源推理模型DeepSeek R1表现最佳（50.6%），超过Claude 3.7 Sonnet（47.7%）和GPT-4o（33.7%）；3）从通用基准（BIRD）到CricBench时准确性显著下降；4）印地语混合查询表现与英语相当甚至更好，挑战了英语是最佳提示语言的假设。

Conclusion: LLM在专业体育分析领域存在显著能力差距，需要领域特定的基准测试。多语言支持在专业SQL任务中很重要，英语不一定是最佳提示语言。开源模型在专业领域可以超越专有模型。

Abstract: Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a "Gold Standard" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.

</details>


### [24] [Explainable Statute Prediction via Attention-based Model and LLM Prompting](https://arxiv.org/abs/2512.21902)
*Sachin Pawar,Girish Keshav Palshikar,Anindita Sinha Banerjee,Nitin Ramrakhiyani,Basit Ali*

Main category: cs.CL

TL;DR: 本文提出两种自动法规预测与解释方法：AoS（基于注意力机制的句子级模型）和LLMPrompt（基于大语言模型的零样本提示方法），用于根据案件描述预测相关法规条款并提供可理解的解释。


<details>
  <summary>Details</summary>
Motivation: 自动法规预测对于法律AI助手和问答系统有重要应用价值，但为了获得更好的用户接受度，预测结果需要附带人类可理解的解释。

Method: 提出两种方法：1) AoS：使用句子转换器和小型语言模型，通过注意力机制在案件描述的句子上进行监督学习；2) LLMPrompt：使用大语言模型进行零样本预测，探索标准提示和思维链提示技术，同时生成解释。

Result: 在两个流行数据集上比较了两种方法与基准模型的法规预测性能，并通过自动化反事实方法和人工评估评估了解释质量。

Conclusion: 两种方法都能在预测法规的同时生成人类可理解的解释，为法律AI系统的可解释性提供了有效解决方案。

Abstract: In this paper, we explore the problem of automatic statute prediction where for a given case description, a subset of relevant statutes are to be predicted. Here, the term "statute" refers to a section, a sub-section, or an article of any specific Act. Addressing this problem would be useful in several applications such as AI-assistant for lawyers and legal question answering system. For better user acceptance of such Legal AI systems, we believe the predictions should also be accompanied by human understandable explanations. We propose two techniques for addressing this problem of statute prediction with explanations -- (i) AoS (Attention-over-Sentences) which uses attention over sentences in a case description to predict statutes relevant for it and (ii) LLMPrompt which prompts an LLM to predict as well as explain relevance of a certain statute. AoS uses smaller language models, specifically sentence transformers and is trained in a supervised manner whereas LLMPrompt uses larger language models in a zero-shot manner and explores both standard as well as Chain-of-Thought (CoT) prompting techniques. Both these models produce explanations for their predictions in human understandable forms. We compare statute prediction performance of both the proposed techniques with each other as well as with a set of competent baselines, across two popular datasets. Also, we evaluate the quality of the generated explanations through an automated counter-factual manner as well as through human evaluation.

</details>


### [25] [Accelerate Speculative Decoding with Sparse Computation in Verification](https://arxiv.org/abs/2512.21911)
*Jikai Wang,Jianchao Tan,Yuxuan Hu,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出稀疏验证框架，通过联合稀疏化注意力、FFN和MoE组件来加速推测解码中的验证阶段，减少计算瓶颈


<details>
  <summary>Details</summary>
Motivation: 推测解码的验证阶段已成为主要计算瓶颈，尤其是在长上下文输入和MoE模型中，现有稀疏化方法主要针对标准自回归解码，不适用于验证阶段

Method: 系统分析验证阶段的多维结构化冗余，提出联合稀疏化注意力、FFN和MoE组件的稀疏验证框架，并引入跨草稿令牌和跨层检索重用策略

Result: 在摘要、问答和数学推理数据集上的实验表明，该方法实现了良好的效率-准确性权衡，同时保持稳定的接受长度

Conclusion: 提出的稀疏验证框架有效减少了推测解码验证阶段的计算成本，为长上下文和MoE模型的加速提供了实用解决方案

Abstract: Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsification methods are designed primarily for standard token-by-token autoregressive decoding to remove substantial computational redundancy in LLMs. This work systematically adopts different sparse methods on the verification stage of the speculative decoding and identifies structured redundancy across multiple dimensions. Based on these observations, we propose a sparse verification framework that jointly sparsifies attention, FFN, and MoE components during the verification stage to reduce the dominant computation cost. The framework further incorporates an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without introducing additional training. Extensive experiments across summarization, question answering, and mathematical reasoning datasets demonstrate that the proposed methods achieve favorable efficiency-accuracy trade-offs, while maintaining stable acceptance length.

</details>


### [26] [SWE-RM: Execution-free Feedback For Software Engineering Agents](https://arxiv.org/abs/2512.21919)
*KaShun Shum,Binyuan Hui,Jiawei Chen,Lei Zhang,X. W.,Jiaxi Yang,Yuzhen Huang,Junyang Lin,Junxian He*

Main category: cs.CL

TL;DR: 本文提出SWE-RM奖励模型，通过混合专家架构提升软件工程代理在测试时间缩放和强化学习中的性能，解决了传统执行反馈稀疏性和测试用例依赖问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于单元测试的执行反馈存在两个主要问题：1）需要大量可靠的测试用例，反馈稀疏且难以区分相似轨迹；2）执行无关的奖励模型反馈在软件工程代理中尚未充分探索。此外，研究发现测试时间缩放性能相似的验证器在强化学习中表现差异很大，表明需要更全面的评估指标。

Method: 提出SWE-RM奖励模型，采用混合专家架构（总参数量30B，推理时激活3B）。通过控制实验分析训练数据规模、策略混合、数据源组成等因素对分类准确性和校准性的影响，以训练出在测试时间缩放和强化学习中都能稳健表现的奖励模型。

Result: SWE-RM显著提升了软件工程代理的性能：在SWE-Bench Verified上，Qwen3-Coder-Flash准确率从51.6%提升到62.0%，Qwen3-Coder-Max从67.0%提升到74.6%，在开源模型中达到新的最先进水平。

Conclusion: 执行无关的奖励模型反馈能够提供更细粒度的信号，且不依赖单元测试。通过关注分类准确性和校准性等关键指标，可以训练出在测试时间缩放和强化学习中都能有效工作的稳健奖励模型，显著提升软件工程代理的性能。

Abstract: Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.

</details>


### [27] [Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs](https://arxiv.org/abs/2512.21933)
*Sachin Pawar,Manoj Apte,Kshitij Jadhav,Girish Keshav Palshikar,Nitin Ramrakhiyani*

Main category: cs.CL

TL;DR: 该论文提出LLM分词过程中自然词被拆分会损害模型性能，并设计了一套惩罚函数来量化这种负面影响。


<details>
  <summary>Details</summary>
Motivation: LLM分词与传统NLP分词不同，由于词汇表有限，自然词常被拆分为多个token。作者假设这种拆分会对LLM在各种NLP任务上的性能产生负面影响。

Method: 提出了一套惩罚函数来计算给定文本在特定LLM上的分词惩罚，量化分词"糟糕"程度。在多个NLP任务和不同LLM上验证假设的统计显著性。

Result: 建立了分词惩罚与LLM性能负相关的统计显著性证据，表明自然词拆分确实损害模型表现。

Conclusion: LLM分词过程中自然词被拆分会负面影响模型性能，提出的惩罚函数能有效量化这种影响，为改进分词策略提供了依据。

Abstract: Tokenization is the first step in training any Large Language Model (LLM), where the text is split into a sequence of tokens as per the model's fixed vocabulary. This tokenization in LLMs is different from the traditional tokenization in NLP where the text is split into a sequence of "natural" words. In LLMs, a natural word may also be broken into multiple tokens due to limited vocabulary size of the LLMs (e.g., Mistral's tokenizer splits "martial" into "mart" and "ial"). In this paper, we hypothesize that such breaking of natural words negatively impacts LLM performance on various NLP tasks. To quantify this effect, we propose a set of penalty functions that compute a tokenization penalty for a given text for a specific LLM, indicating how "bad" the tokenization is. We establish statistical significance of our hypothesis on multiple NLP tasks for a set of different LLMs.

</details>


### [28] [Self-attention vector output similarities reveal how machines pay attention](https://arxiv.org/abs/2512.21956)
*Tal Halevi,Yarden Tzach,Ronit D. Gross,Shalom Rosner,Ido Kanter*

Main category: cs.CL

TL;DR: 该研究提出了一种量化自注意力机制信息处理的新方法，通过分析BERT-12架构发现：深层注意力聚焦于句子分隔符，可用于基于语义特征的文本分割；不同注意力头专注于不同语言特征；随着网络层数加深，相似性从长距离转向短距离，最终偏好同一句子内的强相似性。


<details>
  <summary>Details</summary>
Motivation: 尽管自注意力机制在自然语言处理中取得了显著成功，但其具体的学习机制和量化表征仍是一个开放的研究问题。本研究旨在开发一种方法来量化自注意力机制内部的信息处理过程，以更好地理解其工作原理。

Method: 提出了一种量化自注意力机制信息处理的新方法，基于自注意力头产生的向量空间构建上下文相似性矩阵（测量两个标记向量之间的标量积）。在BERT-12架构上进行分析，研究注意力图、相似性分布和个体头的特性。

Result: 1. 深层注意力聚焦于句子分隔符标记，可用于基于语义特征的文本分割
2. 不同注意力头专注于不同语言特征（如识别标记重复、识别常见标记及其上下文）
3. 相似性分布随网络层数变化：初始层显示长距离相似性，深层发展出短距离相似性，最终偏好同一句子内的强相似性
4. 每个注意力头倾向于聚焦于文本中的独特标记，并围绕该标记构建相似性对

Conclusion: 自注意力机制通过不同头的专业化分工处理不同语言特征，随着网络深度增加，从捕捉长距离依赖逐渐转向处理局部语义关系，最终实现句子级别的语义整合。这种量化分析方法为理解自注意力机制的工作原理提供了新视角。

Abstract: The self-attention mechanism has significantly advanced the field of natural language processing, facilitating the development of advanced language-learning machines. Although its utility is widely acknowledged, the precise mechanisms of self-attention underlying its advanced learning and the quantitative characterization of this learning process remains an open research question. This study introduces a new approach for quantifying information processing within the self-attention mechanism. The analysis conducted on the BERT-12 architecture reveals that, in the final layers, the attention map focuses on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features. Based on the vector space emerging from the self-attention heads, a context similarity matrix, measuring the scalar product between two token vectors was derived, revealing distinct similarities between different token vector pairs within each head and layer. The findings demonstrated that different attention heads within an attention block focused on different linguistic characteristics, such as identifying token repetitions in a given text or recognizing a token of common appearance in the text and its surrounding context. This specialization is also reflected in the distribution of distances between token vectors with high similarity as the architecture progresses. The initial attention layers exhibit substantially long-range similarities; however, as the layers progress, a more short-range similarity develops, culminating in a preference for attention heads to create strong similarities within the same sentence. Finally, the behavior of individual heads was analyzed by examining the uniqueness of their most common tokens in their high similarity elements. Each head tends to focus on a unique token from the text and builds similarity pairs centered around it.

</details>


### [29] [Context as a Tool: Context Management for Long-Horizon SWE-Agents](https://arxiv.org/abs/2512.22087)
*Shukai Liu,Jian Yang,Bo Jiang,Yizhi Li,Jinyang Guo,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: CAT是一种新的上下文管理范式，将上下文维护提升为可调用的工具，帮助大语言模型代理在软件工程任务中实现稳定、可扩展的长时推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理在处理仓库级代码库的长时交互任务时，通常采用追加式上下文维护或被动触发的压缩启发式方法，这会导致上下文爆炸、语义漂移和推理能力下降。

Method: 提出CAT范式，将上下文维护作为可调用工具集成到代理决策过程中；设计结构化上下文工作空间（稳定任务语义、压缩长期记忆、高保真短期交互）；开发CAT-GENERATOR监督框架，通过离线数据构建管道注入上下文管理动作；训练上下文感知模型SWE-Compressor。

Result: 在SWE-Bench-Verified上，SWE-Compressor达到57.6%的解决率，显著优于基于ReAct的代理和静态压缩基线，同时在有限上下文预算下保持稳定且可扩展的长时推理能力。

Conclusion: CAT范式通过主动、结构化的上下文管理，有效解决了大语言模型代理在软件工程任务中的长时交互挑战，实现了更好的性能和可扩展性。

Abstract: Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.

</details>


### [30] [Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis](https://arxiv.org/abs/2512.22100)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 该论文提出了TrGLUE——首个土耳其语自然语言理解综合基准，以及SentiTurca情感分析专用基准，填补了土耳其语NLU评估的空白。


<details>
  <summary>Details</summary>
Motivation: 虽然英语有GLUE基准，其他语言如中文、法语、日语也有相应基准，但土耳其语缺乏类似的全面NLU评估基准。需要建立能够从多维度评估土耳其语模型能力的基准。

Method: 创建TrGLUE基准，包含土耳其本土语料库，采用半自动化标注流程：结合强LLM标注、跨模型一致性检查和人工验证。同时提供基于Transformer模型的微调和评估代码。

Result: 成功建立了TrGLUE基准，涵盖多种NLU任务，以及SentiTurca情感分析基准。标注流程优先考虑语言自然性，减少翻译痕迹，实现了可扩展、可复现的工作流程。

Conclusion: TrGLUE为土耳其语NLU建立了稳健的评估框架，为研究人员提供了宝贵资源，并为生成高质量半自动化数据集提供了见解，填补了土耳其语评估基准的空白。

Abstract: Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [31] [Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks](https://arxiv.org/abs/2512.21375)
*Yuanshuang Fu,Qianyao Wang,Qihao Wang,Bonan Zhang,Jiaxin Zhao,Yiming Cao,Zhijun Li*

Main category: cs.RO

TL;DR: 本文提出了一种用于无人机水质监测的动态光照阴影干扰主动路径规划方法，通过构建动态预测模型、改进IFDS算法、MPC框架和动态飞行高度调整机制，在复杂光照环境下提高数据质量和飞行安全。


<details>
  <summary>Details</summary>
Motivation: 无人机光谱遥感技术在水质监测中广泛应用，但在动态环境中，变化的照明条件（如阴影和镜面反射）会导致严重的光谱失真，降低数据可用性。为了在确保飞行安全的同时最大化获取高质量数据，需要解决光照阴影干扰问题。

Method: 1. 构建动态预测模型，将时变的光照阴影干扰区域转化为三维虚拟障碍物；2. 引入改进的干扰流体动力学系统（IFDS）算法，通过构建排斥力场生成平滑的初始避障路径；3. 采用模型预测控制（MPC）框架进行滚动时域路径优化，处理飞行动力学约束并实现实时轨迹跟踪；4. 设计动态飞行高度调整（DFAA）机制，在可观测区域狭窄时主动降低飞行高度以提高空间分辨率。

Result: 仿真结果表明，与传统PID和单一避障算法相比，所提方法在密集干扰场景下实现了98%的避障成功率，显著提高了路径平滑度，并将有效观测数据量增加了约27%。

Conclusion: 该研究为复杂光照环境下的精确无人机水质监测提供了有效的工程解决方案，通过主动路径规划和动态高度调整机制，显著提高了数据质量和飞行安全性。

Abstract: Unmanned Aerial Vehicle (UAV) spectral remote sensing technology is widely used in water quality monitoring. However, in dynamic environments, varying illumination conditions, such as shadows and specular reflection (sun glint), can cause severe spectral distortion, thereby reducing data availability. To maximize the acquisition of high-quality data while ensuring flight safety, this paper proposes an active path planning method for dynamic light and shadow disturbance avoidance. First, a dynamic prediction model is constructed to transform the time-varying light and shadow disturbance areas into three-dimensional virtual obstacles. Second, an improved Interfered Fluid Dynamical System (IFDS) algorithm is introduced, which generates a smooth initial obstacle avoidance path by building a repulsive force field. Subsequently, a Model Predictive Control (MPC) framework is employed for rolling-horizon path optimization to handle flight dynamics constraints and achieve real-time trajectory tracking. Furthermore, a Dynamic Flight Altitude Adjustment (DFAA) mechanism is designed to actively reduce the flight altitude when the observable area is narrow, thereby enhancing spatial resolution. Simulation results show that, compared with traditional PID and single obstacle avoidance algorithms, the proposed method achieves an obstacle avoidance success rate of 98% in densely disturbed scenarios, significantly improves path smoothness, and increases the volume of effective observation data by approximately 27%. This research provides an effective engineering solution for precise UAV water quality monitoring in complex illumination environments.

</details>


### [32] [Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction](https://arxiv.org/abs/2512.21398)
*Rahul Moorthy Mahesh,Oguzhan Goktug Poyrazoglu,Yukang Cao,Volkan Isler*

Main category: cs.RO

TL;DR: PaceForecaster利用人类指令预测环境地图和子目标，帮助机器人在遮挡环境中更安全快速地导航


<details>
  <summary>Details</summary>
Motivation: 在杂乱环境中，运动规划器面临安全与速度的权衡，因为遮挡和有限传感器范围导致不确定性。研究是否可以通过人类指令帮助机器人更果断地规划同时保持安全。

Method: 提出PaceForecaster方法，将人类指令整合到局部规划器中。输入包括机器人局部传感器范围(Level-1)和人类指令，输出预测地图(Level-2)和指令条件化的子目标。子目标为规划器提供明确指导，以目标导向方式利用预测环境。

Result: 将PaceForecaster与Log-MPPI控制器集成，在多边形环境中，使用语言条件化预测和目标相比仅使用局部地图的基线导航性能提升36%。

Conclusion: 人类指令可以帮助机器人在遮挡环境中更有效地规划，通过预测未来可见区域和设置指令条件化子目标，显著提升导航性能。

Abstract: In cluttered environments, motion planners often face a trade-off between safety and speed due to uncertainty caused by occlusions and limited sensor range. In this work, we investigate whether co-pilot instructions can help robots plan more decisively while remaining safe. We introduce PaceForecaster, as an approach that incorporates such co-pilot instructions into local planners. PaceForecaster takes the robot's local sensor footprint (Level-1) and the provided co-pilot instructions as input and predicts (i) a forecasted map with all regions visible from Level-1 (Level-2) and (ii) an instruction-conditioned subgoal within Level-2. The subgoal provides the planner with explicit guidance to exploit the forecasted environment in a goal-directed manner. We integrate PaceForecaster with a Log-MPPI controller and demonstrate that using language-conditioned forecasts and goals improves navigation performance by 36% over a local-map-only baseline while in polygonal environments.

</details>


### [33] [Developing a Fundamental Diagram for Urban Air Mobility Based on Physical Experiments](https://arxiv.org/abs/2512.21425)
*Hang Zhou,Yuhui Zhai,Shiyu Shen,Yanfeng Ouyang,Xiaowei Shi,Xiaopeng*

Main category: cs.RO

TL;DR: 首个通过物理实验构建城市空中交通基本图的研究，结合理论分析与物理测试验证了地面交通FD结构适用于UAM系统，并发现实验与仿真结果存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着无人机密度增加，城市空中交通（UAM）预计会出现类似地面交通的拥堵问题，但目前对UAM交通流的基本特性，特别是在真实操作条件下的特性了解不足。

Method: 提出一个构建UAM交通基本图的通用框架，结合理论分析和物理实验。理论方面设计两种无人机避撞控制律和基于仿真的交通生成方法；实验方面在缩比测试平台上使用Crazyflie无人机进行物理实验，收集轨迹数据构建UAMTra2Flow数据集。

Result: 初步结果表明，地面交通的经典基本图结构同样适用于UAM系统。物理实验获得的基本图曲线与仿真结果存在偏差，突显了实验验证的重要性。缩比测试结果被缩放到实际运行条件，为未来UAM交通系统提供实用见解。

Conclusion: 这是首个使用真实物理测试数据推导UAM基本图的研究，证明了实验验证的必要性，并为理解UAM交通流特性提供了重要框架和数据集。

Abstract: Urban Air Mobility (UAM) is an emerging application of unmanned aerial vehicles (UAVs) that promises to reduce travel time and alleviate congestion in urban transportation systems. As drone density increases, UAM operations are expected to experience congestion similar to that in ground traffic. However, the fundamental characteristics of UAM traffic flow, particularly under real-world operating conditions, remain poorly understood. This study proposes a general framework for constructing the fundamental diagram (FD) of UAM traffic by integrating theoretical analysis with physical experiments. To the best of our knowledge, this is the first study to derive a UAM FD using real-world physical test data. On the theoretical side, we design two drone control laws for collision avoidance and develop simulation-based traffic generation methods to produce diverse UAM traffic scenarios. Based on Edie's definition, traffic flow theory is then applied to construct the FD and characterize the macroscopic properties of UAM traffic. To account for real-world disturbances and modeling uncertainties, we further conduct physical experiments on a reduced-scale testbed using Bitcraze Crazyflie drones. Both simulation and physical test trajectory data are collected and organized into the UAMTra2Flow dataset, which is analyzed using the proposed framework. Preliminary results indicate that classical FD structures for ground transportation are also applicable to UAM systems. Notably, FD curves obtained from physical experiments exhibit deviations from simulation-based results, highlighting the importance of experimental validation. Finally, results from the reduced-scale testbed are scaled to realistic operating conditions to provide practical insights for future UAM traffic systems. The dataset and code for this paper are publicly available at https://github.com/CATS-Lab/UAM-FD.

</details>


### [34] [EVE: A Generator-Verifier System for Generative Policies](https://arxiv.org/abs/2512.21430)
*Yusuf Ali,Gryphon Patlin,Karthik Kothuri,Muhammad Zubair Irshad,Wuwei Liang,Zsolt Kira*

Main category: cs.RO

TL;DR: EVE框架通过零样本VLM验证器在推理时提升生成式策略性能，无需额外训练


<details>
  <summary>Details</summary>
Motivation: 生成式视觉运动策略（如扩散和流匹配）在分布偏移下性能下降，而语言模型领域通过推理时计算扩展显著提升了推理能力。本研究探索生成式策略是否也能通过零样本VLM验证器在推理时获得类似提升。

Method: 提出EVE框架：将预训练的生成式策略与多个零样本VLM验证器结合。验证器提出动作改进建议，动作整合器融合验证器输出与基础策略预测，生成最终执行动作。研究了不同能力验证器间的信息交互设计。

Result: 在多样化操作任务中，EVE一致提升了任务成功率，无需额外策略训练。通过消融实验分析了验证器能力和动作整合策略的贡献。

Conclusion: EVE框架展示了生成式策略通过推理时计算扩展提升性能的潜力，为零样本VLM验证器在具身控制中的应用提供了实用指南。

Abstract: Visuomotor policies based on generative architectures such as diffusion and flow-based matching have shown strong performance but degrade under distribution shifts, demonstrating limited recovery capabilities without costly finetuning. In the language modeling domain, test-time compute scaling has revolutionized reasoning capabilities of modern LLMs by leveraging additional inference-time compute for candidate solution refinement. These methods typically leverage foundation models as verification modules in a zero-shot manner to synthesize improved candidate solutions. In this work, we hypothesize that generative policies can similarly benefit from additional inference-time compute that employs zero-shot VLM-based verifiers. A systematic analysis of improving policy performance through the generation-verification framework remains relatively underexplored in the current literature. To this end, we introduce EVE - a modular, generator-verifier interaction framework - that boosts the performance of pretrained generative policies at test time, with no additional training. EVE wraps a frozen base policy with multiple zero-shot, VLM-based verifier agents. Each verifier proposes action refinements to the base policy candidate actions, while an action incorporator fuses the aggregated verifier output into the base policy action prediction to produce the final executed action. We study design choices for generator-verifier information interfacing across a system of verifiers with distinct capabilities. Across a diverse suite of manipulation tasks, EVE consistently improves task success rates without any additional policy training. Through extensive ablations, we isolate the contribution of verifier capabilities and action incorporator strategies, offering practical guidelines to build scalable, modular generator-verifier systems for embodied control.

</details>


### [35] [Planetary Terrain Datasets and Benchmarks for Rover Path Planning](https://arxiv.org/abs/2512.21438)
*Marvin Chancán,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出了首个大型行星路径规划基准数据集MarsPlanBench和MoonPlanBench，评估了经典与学习型算法在行星地形上的性能，发现经典算法在挑战性地形上表现优异，而学习型算法泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 行星探测任务日益增多，但先前任务的大量数据未被充分利用于路径规划研究，缺乏基于空间任务的行星数据集、标准化基准和评估协议。

Method: 从火星和月球的高分辨率数字地形图像中创建了两个大型平面基准数据集MarsPlanBench和MoonPlanBench，并在统一框架中设置了经典和学习型路径规划算法进行评估。

Result: 经典算法在月球南北极等挑战性地形上平均能达到100%的全局路径规划成功率，解释了NASA在实践中使用这些算法的原因；学习型模型在较简单环境中表现良好，但在行星领域泛化能力不足。

Conclusion: 通过创建首个行星路径规划基准数据集和评估框架，为行星探测路径规划研究提供了基础，揭示了经典算法在行星地形上的优越性，同时指出了学习型算法在泛化方面的挑战。

Abstract: Planetary rover exploration is attracting renewed interest with several upcoming space missions to the Moon and Mars. However, a substantial amount of data from prior missions remain underutilized for path planning and autonomous navigation research. As a result, there is a lack of space mission-based planetary datasets, standardized benchmarks, and evaluation protocols. In this paper, we take a step towards coordinating these three research directions in the context of planetary rover path planning. We propose the first two large planar benchmark datasets, MarsPlanBench and MoonPlanBench, derived from high-resolution digital terrain images of Mars and the Moon. In addition, we set up classical and learned path planning algorithms, in a unified framework, and evaluate them on our proposed datasets and on a popular planning benchmark. Through comprehensive experiments, we report new insights on the performance of representative path planning algorithms on planetary terrains, for the first time to the best of our knowledge. Our results show that classical algorithms can achieve up to 100% global path planning success rates on average across challenging terrains such as Moon's north and south poles. This suggests, for instance, why these algorithms are used in practice by NASA. Conversely, learning-based models, although showing promising results in less complex environments, still struggle to generalize to planetary domains. To serve as a starting point for fundamental path planning research, our code and datasets will be released at: https://github.com/mchancan/PlanetaryPathBench.

</details>


### [36] [Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task in Uncertain Dynamic Environment](https://arxiv.org/abs/2512.21497)
*Siddhartha Upadhyay,Ratnangshu Das,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 将时空管框架扩展至动态不确定环境中的概率时序可达-避障-停留任务，提出实时管合成方法，提供形式化概率安全保证


<details>
  <summary>Details</summary>
Motivation: 在动态环境中处理具有不确定障碍物的概率时序可达-避障-停留任务需要形式化保证，现有方法难以同时满足实时性、安全性和任务完成要求

Method: 扩展时空管框架，将管定义为状态空间中的时变球体，其中心和半径基于不确定传感信息在线演化，推导出无近似、无优化的闭式控制律，确保系统轨迹保持在管内

Result: 方法提供概率避障和有限时间任务完成的正式保证，控制器无模型、无近似、无优化，能实时高效执行并保证收敛到目标，在移动机器人、无人机和7自由度机械臂的仿真和硬件实验中验证有效性和可扩展性

Conclusion: 提出的时空管扩展框架成功解决了动态不确定环境中的概率时序可达-避障-停留任务，实现了实时执行、形式化安全保证和任务完成的统一

Abstract: In this work, we extend the Spatiotemporal Tube (STT) framework to address Probabilistic Temporal Reach-Avoid-Stay (PrT-RAS) tasks in dynamic environments with uncertain obstacles. We develop a real-time tube synthesis procedure that explicitly accounts for time-varying uncertain obstacles and provides formal probabilistic safety guarantees. The STT is formulated as a time-varying ball in the state space whose center and radius evolve online based on uncertain sensory information. We derive a closed-form, approximation-free control law that confines the system trajectory within the tube, ensuring both probabilistic safety and task satisfaction. Our method offers a formal guarantee for probabilistic avoidance and finite-time task completion. The resulting controller is model-free, approximation-free, and optimization-free, enabling efficient real-time execution while guaranteeing convergence to the target. The effectiveness and scalability of the framework are demonstrated through simulation studies and hardware experiments on mobile robots, a UAV, and a 7-DOF manipulator navigating in cluttered and uncertain environments.

</details>


### [37] [A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming](https://arxiv.org/abs/2512.21534)
*Congrui Bai,Zhenting Du,Weibang Bai*

Main category: cs.RO

TL;DR: 提出新型螺旋缠绕结构静电层卡滞（HWS-ELJ）可变刚度机制，应用于机器人手指设计，通过静电吸附增强层间摩擦实现刚度调节，相比传统平面ELJ具有指数级刚度增强效果。


<details>
  <summary>Details</summary>
Motivation: 传统平面静电层卡滞（ELJ）在刚度调节范围和空间效率方面存在局限，需要开发更高效的可变刚度机制以适应机器人手指等紧凑应用场景。

Method: 采用螺旋缠绕结构设计静电层卡滞机制，利用静电吸附增强层间摩擦抑制相对滑动，通过电压控制实现刚度调节，并进行理论推导和实验验证。

Result: HWS-ELJ相比传统平面ELJ在相同电极接触面积下实现指数级刚度增强，在同等刚度条件下减少占用空间，实验结果显示与理论趋势一致，机器人手指原型验证了电压驱动刚度调节的可行性。

Conclusion: HWS-ELJ是一种高效的可变刚度机制，特别适合机器人手指等紧凑型应用，通过电压控制实现精确刚度调节，为可变刚度机器人设计提供了新方案。

Abstract: This paper introduces a novel variable stiffness mechanism termed Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ) and systematically investigates its potential applications in variable stiffness robotic finger design. The proposed method utilizes electrostatic attraction to enhance interlayer friction, thereby suppressing relative sliding and enabling tunable stiffness. Compared with conventional planar ELJ, the helical configuration of HWS-ELJ provides exponentially increasing stiffness adjustment with winding angle, achieving significantly greater stiffness enhancement for the same electrode contact area while reducing the required footprint under equivalent stiffness conditions. Considering the practical advantage of voltage-based control, a series of experimental tests under different initial force conditions were conducted to evaluate the stiffness modulation characteristics of HWS-ELJ. The results demonstrated its rational design and efficacy, with outcomes following the deduced theoretical trends. Furthermore, a robotic finger prototype integrating HWS-ELJ was developed, demonstrating voltage-driven stiffness modulation and confirming the feasibility of the proposed robotic variable stiffness mechanism.

</details>


### [38] [World-Coordinate Human Motion Retargeting via SAM 3D Body](https://arxiv.org/abs/2512.21573)
*Zhangzheng Tu,Kailun Su,Shaolong Zhu,Yukun Zheng*

Main category: cs.RO

TL;DR: 提出轻量级框架，从单目视频恢复世界坐标系人体运动并重定向到人形机器人，使用SAM 3D Body作为感知骨干和Momentum HumanRig作为机器人友好中间表示，通过滑动窗口优化和物理约束实现稳定运动重建。


<details>
  <summary>Details</summary>
Motivation: 从单目视频恢复世界坐标系人体运动对于具身智能和机器人学很重要，但现有方法依赖复杂的SLAM流程或重型时序模型，需要更轻量、工程友好的解决方案。

Method: 使用冻结的SAM 3D Body作为感知骨干，Momentum HumanRig作为中间表示；锁定身份和骨架尺度参数保证时序一致性；在低维MHR潜空间进行滑动窗口优化平滑预测；使用可微分软足地接触模型和接触感知全局优化恢复物理合理的根轨迹；最后通过运动学感知的两阶段逆运动学管道重定向到Unitree G1人形机器人。

Result: 在真实单目视频上测试显示，该方法具有稳定的世界轨迹和可靠的机器人重定向效果，表明结构化人体表示结合轻量级物理约束可以从单目输入产生机器人就绪的运动。

Conclusion: 结构化人体表示配合轻量级物理约束能够有效从单目视频恢复机器人可用的运动，避免了复杂SLAM流程或重型时序模型，为机器人运动重定向提供了工程友好的解决方案。

Abstract: Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics. To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate. Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization. Finally, we retarget the reconstructed motion to the Unitree G1 humanoid using a kinematics-aware two-stage inverse kinematics pipeline. Results on real monocular videos show that our method has stable world trajectories and reliable robot retargeting, indicating that structured human representations with lightweight physical constraints can yield robot-ready motion from monocular input.

</details>


### [39] [AstraNav-Memory: Contexts Compression for Long Memory](https://arxiv.org/abs/2512.21627)
*Botao Ren,Junjun Hu,Xinda Xue,Minghua Luo,Jintao Chen,Haochen Bai,Liangliang You,Mu Xu*

Main category: cs.RO

TL;DR: 提出基于图像中心的记忆框架，通过视觉上下文压缩模块实现长期隐式记忆，结合Qwen2.5-VL导航策略，在GOAT-Bench和HM3D-OVON上达到SOTA导航性能。


<details>
  <summary>Details</summary>
Motivation: 终身具身导航需要智能体跨任务积累、保留和利用空间语义经验，但现有基于物体中心的记忆方法依赖检测和重建流程，限制了鲁棒性和可扩展性。

Method: 提出图像中心记忆框架，包含高效的视觉上下文压缩模块，基于ViT骨干网络（冻结DINOv3特征）和轻量级PixelUnshuffle+Conv块，支持可配置压缩率（如16倍压缩下每张图像约30个token）。

Result: 在GOAT-Bench和HM3D-OVON上实现最先进的导航性能，提升陌生环境探索能力并缩短熟悉环境路径。消融研究表明适度压缩在效率和准确性间达到最佳平衡。

Conclusion: 压缩的图像中心记忆为终身具身智能体提供了实用且可扩展的接口，使其能够基于长视觉历史进行推理，实现类人效率的导航。

Abstract: Lifelong embodied navigation requires agents to accumulate, retain, and exploit spatial-semantic experience across tasks, enabling efficient exploration in novel environments and rapid goal reaching in familiar ones. While object-centric memory is interpretable, it depends on detection and reconstruction pipelines that limit robustness and scalability. We propose an image-centric memory framework that achieves long-term implicit memory via an efficient visual context compression module end-to-end coupled with a Qwen2.5-VL-based navigation policy. Built atop a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, our visual tokenizer supports configurable compression rates; for example, under a representative 16$\times$ compression setting, each image is encoded with about 30 tokens, expanding the effective context capacity from tens to hundreds of images. Experimental results on GOAT-Bench and HM3D-OVON show that our method achieves state-of-the-art navigation performance, improving exploration in unfamiliar environments and shortening paths in familiar ones. Ablation studies further reveal that moderate compression provides the best balance between efficiency and accuracy. These findings position compressed image-centric memory as a practical and scalable interface for lifelong embodied agents, enabling them to reason over long visual histories and navigate with human-like efficiency.

</details>


### [40] [Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning](https://arxiv.org/abs/2512.21654)
*Zikun Guo,Adeyinka P. Adedigba,Rammohan Mallipeddi,Heoncheol Lee*

Main category: cs.RO

TL;DR: 提出一种结合结构先验的蚁群优化框架，用于多机器人路径规划，通过空间分布约束搜索空间，平衡全局效率和个体工作量分配。


<details>
  <summary>Details</summary>
Motivation: 传统群体智能方法在小规模实例上有效，但容易过早收敛且难以扩展到复杂环境。多机器人路径规划具有组合复杂性，需要在全局效率和公平任务分配之间取得平衡。

Method: 提出结构诱导探索框架，将结构先验集成到蚁群优化中：1) 利用任务空间分布初始化结构先验约束搜索空间；2) 设计信息素更新规则强调结构上有意义的连接；3) 引入负载感知目标平衡总行程和个体工作量；4) 采用显式重叠抑制策略确保任务区分和平衡分配。

Result: 在多种基准场景（不同实例规模和机器人团队配置）上验证，相比代表性元启发式基线，在路径紧凑性、稳定性和工作量分布方面取得一致改进。

Conclusion: 该方法不仅提供性能提升，还提供了一个可扩展且可解释的框架，可应用于物流、监视和搜救等需要可靠大规模协调的应用场景。

Abstract: Multi-robot path planning is a fundamental yet challenging problem due to its combinatorial complexity and the need to balance global efficiency with fair task allocation among robots. Traditional swarm intelligence methods, although effective on small instances, often converge prematurely and struggle to scale to complex environments. In this work, we present a structure-induced exploration framework that integrates structural priors into the search process of the ant colony optimization (ACO). The approach leverages the spatial distribution of the task to induce a structural prior at initialization, thereby constraining the search space. The pheromone update rule is then designed to emphasize structurally meaningful connections and incorporates a load-aware objective to reconcile the total travel distance with individual robot workload. An explicit overlap suppression strategy further ensures that tasks remain distinct and balanced across the team. The proposed framework was validated on diverse benchmark scenarios covering a wide range of instance sizes and robot team configurations. The results demonstrate consistent improvements in route compactness, stability, and workload distribution compared to representative metaheuristic baselines. Beyond performance gains, the method also provides a scalable and interpretable framework that can be readily applied to logistics, surveillance, and search-and-rescue applications where reliable large-scale coordination is essential.

</details>


### [41] [MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning](https://arxiv.org/abs/2512.21722)
*Zishuo Wang,Xinyu Zhang,Zhuonan Liu,Tomohito Kawabata,Daeun Song,Xuesu Xiao,Ling Xiao*

Main category: cs.RO

TL;DR: MAction-SocialNav是一个用于社交合规导航的高效视觉语言模型，通过元认知提示方法处理动作模糊性，能够生成多种合理动作，在决策质量、安全性和实时效率方面优于GPT-4o和Claude。


<details>
  <summary>Details</summary>
Motivation: 现有社交导航方法通常假设单一正确动作，无法处理现实世界中社交规范固有的模糊性和不确定性，限制了在实际场景中的应用能力。

Method: 提出MAction-SocialNav模型，采用元认知提示方法增强推理能力，并构建包含789个样本的多动作社交导航数据集，涵盖不同人群密度和室内外环境。

Result: 模型在决策质量(APG: 0.595 vs. 0.000/0.025)和安全性(ER: 0.264 vs. 0.642/0.668)上显著优于GPT-4o和Claude，同时保持实时效率(1.524 FPS，快3倍以上)。

Conclusion: MAction-SocialNav通过显式处理动作模糊性，实现了强大的社交推理能力和高效性能，在真实人机导航场景中具有重要应用潜力。

Abstract: Socially compliant navigation requires robots to move safely and appropriately in human-centered environments by respecting social norms. However, social norms are often ambiguous, and in a single scenario, multiple actions may be equally acceptable. Most existing methods simplify this problem by assuming a single correct action, which limits their ability to handle real-world social uncertainty. In this work, we propose MAction-SocialNav, an efficient vision language model for socially compliant navigation that explicitly addresses action ambiguity, enabling generating multiple plausible actions within one scenario. To enhance the model's reasoning capability, we introduce a novel meta-cognitive prompt (MCP) method. Furthermore, to evaluate the proposed method, we curate a multi-action socially compliant navigation dataset that accounts for diverse conditions, including crowd density, indoor and outdoor environments, and dual human annotations. The dataset contains 789 samples, each with three-turn conversation, split into 710 training samples and 79 test samples through random selection. We also design five evaluation metrics to assess high-level decision precision, safety, and diversity. Extensive experiments demonstrate that the proposed MAction-SocialNav achieves strong social reasoning performance while maintaining high efficiency, highlighting its potential for real-world human robot navigation. Compared with zero-shot GPT-4o and Claude, our model achieves substantially higher decision quality (APG: 0.595 vs. 0.000/0.025) and safety alignment (ER: 0.264 vs. 0.642/0.668), while maintaining real-time efficiency (1.524 FPS, over 3x faster).

</details>


### [42] [HELP: Hierarchical Embodied Language Planner for Household Tasks](https://arxiv.org/abs/2512.21723)
*Alexandr V. Korchemnyi,Anatoly O. Onishchenko,Eva A. Bakaeva,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.RO

TL;DR: HELP：基于开源LLM的分层具身语言规划器，通过多智能体架构解决家庭任务规划问题


<details>
  <summary>Details</summary>
Motivation: 具身智能体在复杂场景中需要强大的规划能力，而大语言模型（LLMs）具有丰富的语言知识可以担任这一角色。但需要设计合适的架构来充分利用LLMs处理语言歧义、从环境中检索信息、基于智能体可用技能的能力。

Method: 提出分层具身语言规划器HELP，由一组基于LLM的智能体组成，每个智能体专门解决不同的子任务。使用参数相对较少的开源LLM，以实现自主部署。

Result: 在家庭任务上评估了该方法，并进行了具身智能体的真实世界实验。

Conclusion: HELP架构能够有效利用LLMs的规划能力，通过分层多智能体方法解决具身任务，且使用开源小模型可实现自主部署。

Abstract: Embodied agents tasked with complex scenarios, whether in real or simulated environments, rely heavily on robust planning capabilities. When instructions are formulated in natural language, large language models (LLMs) equipped with extensive linguistic knowledge can play this role. However, to effectively exploit the ability of such models to handle linguistic ambiguity, to retrieve information from the environment, and to be based on the available skills of an agent, an appropriate architecture must be designed. We propose a Hierarchical Embodied Language Planner, called HELP, consisting of a set of LLM-based agents, each dedicated to solving a different subtask. We evaluate the proposed approach on a household task and perform real-world experiments with an embodied agent. We also focus on the use of open source LLMs with a relatively small number of parameters, to enable autonomous deployment.

</details>


### [43] [MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction](https://arxiv.org/abs/2512.21853)
*Kentaro Uno,Elian Neppel,Gustavo H. Diaz,Ashutosh Mishra,Shamistan Karimov,A. Sejal Jain,Ayesha Habib,Pascal Pama,Hazal Gozbasi,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: MoonBot是一个模块化可重构机器人系统，专为月球探索设计，能在严格质量限制下适应不同环境条件和任务需求，通过现场演示验证了其执行月球基础设施建立任务的能力。


<details>
  <summary>Details</summary>
Motivation: 月球表面探索和开发日益受到全球关注，机器人对于探索未知地形、利用本地资源和建设未来人类栖息地至关重要。需要开发能在严格质量限制下适应多变环境条件和任务需求的机器人系统。

Method: 设计并开发了模块化按需可重构机器人系统（MoonBot），该系统采用模块化设计，能够根据任务需求进行重构。进行了初步现场演示，模拟建立月球基础设施的里程碑任务，包括土木工程操作、基础设施组件运输部署以及与充气模块的辅助操作。

Result: 成功验证了概念证明，展示了MoonBot执行月球基础设施建立任务的能力。系统总结了测试过程中的经验教训，特别关注连接器设计，为未来月球任务中模块化机器人系统的发展提供了宝贵见解。

Conclusion: MoonBot作为模块化可重构机器人系统，为月球探索和开发提供了有前景的解决方案。测试中获得的经验教训，特别是连接器设计的改进，将推动未来模块化机器人系统在月球任务中的发展。

Abstract: The allure of lunar surface exploration and development has recently captured widespread global attention. Robots have proved to be indispensable for exploring uncharted terrains, uncovering and leveraging local resources, and facilitating the construction of future human habitats. In this article, we introduce the modular and on-demand reconfigurable robot (MoonBot), a modular and reconfigurable robotic system engineered to maximize functionality while operating within the stringent mass constraints of lunar payloads and adapting to varying environmental conditions and task requirements. This article details the design and development of MoonBot and presents a preliminary field demonstration that validates the proof of concept through the execution of milestone tasks simulating the establishment of lunar infrastructure. These tasks include essential civil engineering operations, infrastructural component transportation and deployment, and assistive operations with inflatable modules. Furthermore, we systematically summarize the lessons learned during testing, focusing on the connector design and providing valuable insights for the advancement of modular robotic systems in future lunar missions.

</details>


### [44] [Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking](https://arxiv.org/abs/2512.21882)
*Kenta Iizuka,Akiyoshi Uchida,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出一种基于非线性优化的轨迹规划方法，用于空间碎片清除任务中接近翻滚目标的近距离交会，引入动态禁入球体确保安全接近，并开发了考虑实际约束的ON/OFF推力器控制策略。


<details>
  <summary>Details</summary>
Motivation: 空间碎片清除任务中，使用机械臂的服务卫星需要安全接近翻滚的碎片目标，这是一个关键挑战。需要解决在目标旋转状态下将其带入机械臂工作空间的轨迹规划问题。

Method: 基于非线性优化的轨迹规划方法，引入动态禁入球体（根据接近条件自适应调整），允许更近更安全地接近目标。同时开发了使用离散ON/OFF推力器的控制策略来复现优化轨迹。

Result: 该方法能够为自由漂浮、旋转的碎片目标在二维平面内规划出安全的接近轨迹，作为捕获前的预备步骤。动态禁入球体机制提高了接近的安全性和效率。

Conclusion: 提出的轨迹规划和控制策略为解决空间碎片清除中的安全接近问题提供了有效方法，动态禁入球体概念和ON/OFF推力器控制方案具有实际应用价值。

Abstract: Approaching a tumbling target safely is a critical challenge in space debris removal missions utilizing robotic manipulators onboard servicing satellites. In this work, we propose a trajectory planning method based on nonlinear optimization for a close-range rendezvous to bring a free-floating, rotating debris object in a two-dimensional plane into the manipulator's workspace, as a preliminary step for its capture. The proposed method introduces a dynamic keep-out sphere that adapts depending on the approach conditions, allowing for closer and safer access to the target. Furthermore, a control strategy is developed to reproduce the optimized trajectory using discrete ON/OFF thrusters, considering practical implementation constraints.

</details>


### [45] [Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications](https://arxiv.org/abs/2512.21886)
*Akiyoshi Uchida,Antonine Richard,Kentaro Uno,Miguel Olivares-Mendez,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出一种用于自由漂浮基座机器人的在线惯性参数估计方法，通过动量守恒扩展现有方法，适用于空间机器人操作未知目标物体


<details>
  <summary>Details</summary>
Motivation: 在空间机器人操作中，掌握被抓取物体的惯性参数对于动力学感知操作至关重要，特别是在自由漂浮基座的情况下，需要准确估计未知目标物体的惯性参数

Method: 扩展现有在线识别方法，通过结合动量守恒原理，使其适用于自由漂浮基座机器人，在操作过程中实时估计目标物体的惯性参数

Result: 通过数值仿真验证了方法的有效性，估计参数与真实值对比显示准确识别，在多种场景下表现良好

Conclusion: 该方法适用于在轨服务和其他空间任务，为自由漂浮基座机器人操作未知物体提供了有效的惯性参数估计解决方案

Abstract: Knowing the inertia parameters of a grasped object is crucial for dynamics-aware manipulation, especially in space robotics with free-floating bases. This work addresses the problem of estimating the inertia parameters of an unknown target object during manipulation. We apply and extend an existing online identification method by incorporating momentum conservation, enabling its use for the floating-base robots. The proposed method is validated through numerical simulations, and the estimated parameters are compared with ground-truth values. Results demonstrate accurate identification in the scenarios, highlighting the method's applicability to on-orbit servicing and other space missions.

</details>


### [46] [Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space](https://arxiv.org/abs/2512.21887)
*Weichen Zhang,Peizhi Tang,Xin Zeng,Fanhang Man,Shiquan Yu,Zichao Dai,Baining Zhao,Hongjin Chen,Yu Shang,Wei Wu,Chen Gao,Xinlei Chen,Xin Wang,Yong Li,Wenwu Zhu*

Main category: cs.RO

TL;DR: ANWM是一个无人机导航世界模型，通过预测未来视觉观测来评估轨迹的语义合理性和导航效用，提升无人机在大型3D环境中的导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有无人机导航策略主要优化低层目标（如避障和轨迹平滑），缺乏将高层语义融入规划的能力，需要桥接这一差距。

Method: 提出ANWM模型，基于4-DoF无人机轨迹训练，引入物理启发的未来帧投影模块（FFP），将过去帧投影到未来视点提供几何先验，减少长距离视觉生成的表示不确定性。

Result: ANWM在长距离视觉预测方面显著优于现有世界模型，并提高了无人机在大型环境中的导航成功率。

Conclusion: ANWM通过结合语义理解和物理几何先验，有效提升了无人机在复杂3D环境中的自主导航能力。

Abstract: Unmanned aerial vehicles (UAVs) have emerged as powerful embodied agents. One of the core abilities is autonomous navigation in large-scale three-dimensional environments. Existing navigation policies, however, are typically optimized for low-level objectives such as obstacle avoidance and trajectory smoothness, lacking the ability to incorporate high-level semantics into planning. To bridge this gap, we propose ANWM, an aerial navigation world model that predicts future visual observations conditioned on past frames and actions, thereby enabling agents to rank candidate trajectories by their semantic plausibility and navigational utility. ANWM is trained on 4-DoF UAV trajectories and introduces a physics-inspired module: Future Frame Projection (FFP), which projects past frames into future viewpoints to provide coarse geometric priors. This module mitigates representational uncertainty in long-distance visual generation and captures the mapping between 3D trajectories and egocentric observations. Empirical results demonstrate that ANWM significantly outperforms existing world models in long-distance visual forecasting and improves UAV navigation success rates in large-scale environments.

</details>


### [47] [Flexible Multitask Learning with Factorized Diffusion Policy](https://arxiv.org/abs/2512.21898)
*Chaoqi Liu,Haonan Chen,Sigmund H. Høeg,Shaoxiong Yao,Yunzhu Li,Kris Hauser,Yilun Du*

Main category: cs.RO

TL;DR: 提出模块化扩散策略框架，将复杂动作分布分解为多个专门扩散模型的组合，每个模型捕捉行为空间的不同子模式，从而更有效地学习多任务机器人策略。


<details>
  <summary>Details</summary>
Motivation: 多任务学习面临机器人动作分布高度多模态和多样性的挑战。现有的单一模型往往难以有效拟合这些复杂分布，容易欠拟合且缺乏灵活适应能力。

Method: 引入模块化扩散策略框架，将复杂动作分布分解为多个专门扩散模型的组合。每个扩散模型捕捉行为空间的不同子模式，形成整体策略。模块化结构支持通过添加或微调组件来灵活适应新任务。

Result: 在仿真和真实世界机器人操作环境中，该方法持续优于强大的模块化和单一模型基线方法。

Conclusion: 模块化扩散策略框架能有效分解复杂动作分布，提高多任务学习性能，同时通过模块化结构实现灵活适应新任务并缓解灾难性遗忘问题。

Abstract: Multitask learning poses significant challenges due to the highly multimodal and diverse nature of robot action distributions. However, effectively fitting policies to these complex task distributions is often difficult, and existing monolithic models often underfit the action distribution and lack the flexibility required for efficient adaptation. We introduce a novel modular diffusion policy framework that factorizes complex action distributions into a composition of specialized diffusion models, each capturing a distinct sub-mode of the behavior space for a more effective overall policy. In addition, this modular structure enables flexible policy adaptation to new tasks by adding or fine-tuning components, which inherently mitigates catastrophic forgetting. Empirically, across both simulation and real-world robotic manipulation settings, we illustrate how our method consistently outperforms strong modular and monolithic baselines.

</details>


### [48] [StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision](https://arxiv.org/abs/2512.21970)
*Shengliang Deng,Mi Yan,Yixin Zheng,Jiayi Su,Wenhao Zhang,Xiaoguang Zhao,Heming Cui,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: StereoVLA：利用立体视觉几何线索的视觉-语言-动作模型，通过几何-语义特征提取模块和交互区域深度估计任务，在机器人操作任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 立体视觉能提供丰富的空间线索，对机器人精确操作至关重要，但目前在视觉-语言-动作模型中的应用仍未被充分探索。

Method: 提出StereoVLA模型，包含几何-语义特征提取模块（利用视觉基础模型提取并融合立体视图的几何特征和单目视图的语义特征），以及辅助的交互区域深度估计任务来增强空间感知。

Result: 在立体视觉设置下的多样化任务中，该方法大幅优于基线方法，并对相机姿态变化表现出强鲁棒性。

Conclusion: StereoVLA成功将立体视觉的几何线索整合到VLA模型中，显著提升了空间感知能力和任务性能，为机器人精确操作提供了有效解决方案。

Abstract: Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.

</details>


### [49] [Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation](https://arxiv.org/abs/2512.21983)
*Saksham Gupta,Sarthak Mishra,Arshad Ayub,Kamran Farooque,Spandan Roy,Babita Gupta*

Main category: cs.RO

TL;DR: BRIS是一个紧凑的人机协同机器人插管系统，集成了四向可操纵纤维支气管镜、独立气管导管推进机制和摄像头增强口器，通过学习型闭环控制和单目深度估计实现安全、直观的气管插管辅助。


<details>
  <summary>Details</summary>
Motivation: 气管插管是关键但技术要求高的操作，现有机器人系统主要关注气道导航，缺乏对气管导管推进的集成控制和相对于隆突的客观深度验证，可能导致严重并发症。

Method: BRIS整合了四向可操纵纤维支气管镜、独立气管导管推进机制和摄像头增强口器。采用学习型闭环控制框架，利用实时形状传感将操纵杆输入映射到支气管镜尖端在笛卡尔空间的运动。使用单目内窥镜深度估计对气道区域进行分类，提供可解释的解剖感知引导。

Result: 在高保真气道模型上进行验证，包括标准和困难气道配置，系统展示了可靠的导航和可控的导管放置性能。

Conclusion: BRIS是实现更安全、更一致且临床兼容的机器人气道管理的重要一步，为气管插管提供了集成控制和客观深度验证的解决方案。

Abstract: Endotracheal intubation is a critical yet technically demanding procedure, with failure or improper tube placement leading to severe complications. Existing robotic and teleoperated intubation systems primarily focus on airway navigation and do not provide integrated control of endotracheal tube advancement or objective verification of tube depth relative to the carina. This paper presents the Robotic Intubation System (BRIS), a compact, human-in-the-loop platform designed to assist fiberoptic-guided intubation while enabling real-time, objective depth awareness. BRIS integrates a four-way steerable fiberoptic bronchoscope, an independent endotracheal tube advancement mechanism, and a camera-augmented mouthpiece compatible with standard clinical workflows. A learning-enabled closed-loop control framework leverages real-time shape sensing to map joystick inputs to distal bronchoscope tip motion in Cartesian space, providing stable and intuitive teleoperation under tendon nonlinearities and airway contact. Monocular endoscopic depth estimation is used to classify airway regions and provide interpretable, anatomy-aware guidance for safe tube positioning relative to the carina. The system is validated on high-fidelity airway mannequins under standard and difficult airway configurations, demonstrating reliable navigation and controlled tube placement. These results highlight BRIS as a step toward safer, more consistent, and clinically compatible robotic airway management.

</details>
