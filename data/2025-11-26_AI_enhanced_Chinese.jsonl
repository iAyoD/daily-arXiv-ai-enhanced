{"id": "2511.19648", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19648", "abs": "https://arxiv.org/abs/2511.19648", "authors": ["Manil Shrestha", "Edward Kim"], "title": "Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search", "comment": null, "summary": "Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6df7\u5408\u7b97\u6cd5\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u591a\u8df3\u95ee\u7b54\u7684\u6548\u7387\u4e0e\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\uff1aLLM\u5f15\u5bfc\u89c4\u5212\u4f7f\u7528\u5355\u6b21LLM\u8c03\u7528\u9884\u6d4b\u5173\u7cfb\u5e8f\u5217\uff0c\u5d4c\u5165\u5f15\u5bfc\u795e\u7ecf\u641c\u7d22\u5b8c\u5168\u6d88\u9664LLM\u8c03\u7528\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8fb9\u8bc4\u5206\u5668\u5b9e\u73b0100\u500d\u52a0\u901f\u3002", "motivation": "\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u51cf\u5c11\u6602\u8d35\u7684LLM\u63a8\u7406\u5f00\u9500\uff0c\u786e\u4fdd\u7b54\u6848\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u4e2d\u7684\u53ef\u9a8c\u8bc1\u6027\u3002", "method": "1) LLM\u5f15\u5bfc\u89c4\u5212\uff1a\u5355\u6b21LLM\u8c03\u7528\u9884\u6d4b\u5173\u7cfb\u5e8f\u5217\uff0c\u901a\u8fc7\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u6267\u884c\uff1b2) \u5d4c\u5165\u5f15\u5bfc\u795e\u7ecf\u641c\u7d22\uff1a\u878d\u5408\u6587\u672c\u548c\u56fe\u5d4c\u5165\u7684\u8f7b\u91cf\u7ea7\u8fb9\u8bc4\u5206\u5668\uff1b3) \u77e5\u8bc6\u84b8\u998f\uff1a\u5c06\u89c4\u5212\u80fd\u529b\u538b\u7f29\u52304B\u53c2\u6570\u6a21\u578b\u4e2d\u3002", "result": "\u5728MetaQA\u4e0a\u8bc4\u4f30\uff0cLLM\u5f15\u5bfc\u89c4\u5212\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff08micro-F1 > 0.90\uff09\uff0c\u5d4c\u5165\u5f15\u5bfc\u795e\u7ecf\u641c\u7d22\u5b9e\u73b0100\u500d\u52a0\u901f\u4e14\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u7387\uff0c\u7ed3\u6784\u5316\u89c4\u5212\u6bd4\u76f4\u63a5\u7b54\u6848\u751f\u6210\u66f4\u5177\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u53ef\u9a8c\u8bc1\u7684\u591a\u8df3\u63a8\u7406\u4e0d\u9700\u8981\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u800c\u662f\u9700\u8981\u7ed3\u5408\u7b26\u53f7\u7ed3\u6784\u4e0e\u5b66\u4e60\u8868\u793a\u7684\u9002\u5f53\u67b6\u6784\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2511.19719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19719", "abs": "https://arxiv.org/abs/2511.19719", "authors": ["Mobina Mehrazar", "Mohammad Amin Yousefi", "Parisa Abolfath Beygi", "Behnam Bahrak"], "title": "Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian", "comment": null, "summary": "Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.", "AI": {"tldr": "\u8bc4\u4f30\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u7c7b\u4e2dLLM\u751f\u6210\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u89e3\u91ca\u4e0e\u4eba\u7c7b\u5224\u65ad\u5b58\u5728\u5206\u6b67\uff0c\u63d0\u793a\u7b56\u7565\u5bf9\u5fe0\u5b9e\u6027\u5f71\u54cd\u6709\u9650\u3002", "motivation": "LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u751f\u6210\u81ea\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u4ee4\u4eba\u62c5\u5fe7\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u7c7b\u4e2d\u7684\u89e3\u91ca\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83LLM\u8bc6\u522b\u7684\u5f71\u54cd\u8bcd\u4e0e\u4eba\u7c7b\u6807\u6ce8\uff0c\u4f7f\u7528\u57fa\u4e8e\u8bcd\u7ea7\u5bf9\u6570\u6982\u7387\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u5fe0\u5b9e\u6027\uff0c\u6d4b\u8bd5\u4e24\u79cd\u63d0\u793a\u7b56\u7565\uff08\u5148\u9884\u6d4b\u540e\u89e3\u91ca vs \u5148\u89e3\u91ca\u540e\u9884\u6d4b\uff09\u3002", "result": "LLM\u5206\u7c7b\u6027\u80fd\u5f3a\u4f46\u751f\u6210\u89e3\u91ca\u4e0e\u5fe0\u5b9e\u63a8\u7406\u504f\u79bb\uff0c\u6a21\u578b\u95f4\u89e3\u91ca\u4e00\u81f4\u6027\u9ad8\u4e8e\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u5f53\u524d\u89e3\u91ca\u65b9\u6cd5\u548c\u6307\u6807\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u786e\u4fddLLM\u5728\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.19739", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19739", "abs": "https://arxiv.org/abs/2511.19739", "authors": ["Richard J. Young", "Alice M. Matthews"], "title": "Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation", "comment": "25 pages, 13 figures, 5 tables", "summary": "Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e8610\u79cd\u57fa\u4e8etransformer\u7684\u5d4c\u5165\u6a21\u578b\u5728\u5fc3\u810f\u75c5\u5b66\u9886\u57df\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7f16\u7801\u5668\u67b6\u6784\uff08\u7279\u522b\u662fBioLinkBERT\uff09\u5728\u9886\u57df\u7279\u5b9a\u6027\u80fd\u4e0a\u4f18\u4e8e\u66f4\u5927\u7684\u89e3\u7801\u5668\u6a21\u578b\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u5c11\u3002", "motivation": "\u9886\u57df\u7279\u5b9a\u7684\u6587\u672c\u5d4c\u5165\u5bf9\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e4b\u95f4\u7684\u7cfb\u7edf\u6bd4\u8f83\u4ecd\u7136\u6709\u9650\u3002", "method": "\u4f7f\u7528Low-Rank Adaptation (LoRA)\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728106,535\u5bf9\u5fc3\u810f\u75c5\u5b66\u6587\u672c\u5bf9\u4e0a\u9002\u914d\u4e8610\u79cdtransformer\u5d4c\u5165\u6a21\u578b\uff0c\u8fd9\u4e9b\u6587\u672c\u5bf9\u6765\u81ea\u6743\u5a01\u533b\u5b66\u6559\u79d1\u4e66\u3002", "result": "\u7f16\u7801\u5668\u67b6\u6784\uff08\u7279\u522b\u662fBioLinkBERT\uff09\u5728\u9886\u57df\u7279\u5b9a\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff08\u5206\u79bb\u5f97\u5206\uff1a0.510\uff09\uff0c\u4f18\u4e8e\u66f4\u5927\u7684\u89e3\u7801\u5668\u6a21\u578b\uff0c\u540c\u65f6\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u66f4\u5927\u8bed\u8a00\u6a21\u578b\u5fc5\u7136\u4ea7\u751f\u66f4\u597d\u9886\u57df\u7279\u5b9a\u5d4c\u5165\u7684\u5047\u8bbe\uff0c\u4e3a\u4e34\u5e8aNLP\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u6240\u6709\u6a21\u578b\u548c\u8d44\u6e90\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2511.19757", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19757", "abs": "https://arxiv.org/abs/2511.19757", "authors": ["Colton Casto", "Anna Ivanova", "Evelina Fedorenko", "Nancy Kanwisher"], "title": "What does it mean to understand language?", "comment": null, "summary": "Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u8bed\u8a00\u7406\u89e3\u9700\u8981\u5c06\u4fe1\u606f\u4ece\u6838\u5fc3\u8bed\u8a00\u7cfb\u7edf\u5bfc\u51fa\u5230\u5176\u4ed6\u5927\u8111\u533a\u57df\uff0c\u4ee5\u6784\u5efa\u4e30\u5bcc\u7684\u5fc3\u7406\u6a21\u578b\u3002", "motivation": "\u8bed\u8a00\u7406\u89e3\u4e0d\u4ec5\u662f\u63d0\u53d6\u8bed\u8a00\u8f93\u5165\u7684\u8868\u5c42\u610f\u4e49\uff0c\u8fd8\u9700\u8981\u6784\u5efa\u6240\u63cf\u8ff0\u60c5\u5883\u7684\u4e30\u5bcc\u5fc3\u7406\u6a21\u578b\u3002\u7531\u4e8e\u5927\u8111\u6838\u5fc3\u8bed\u8a00\u7cfb\u7edf\u7684\u5904\u7406\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u5c06\u4fe1\u606f\u5bfc\u51fa\u5230\u5176\u4ed6\u8111\u533a\u3002", "method": "\u56de\u987e\u73b0\u6709\u8bc1\u636e\u652f\u6301\u8fd9\u4e00\u5047\u8bbe\uff0c\u5e76\u5229\u7528\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7684\u6700\u65b0\u8fdb\u5c55\u6765\u76f4\u63a5\u6d4b\u8bd5\u8be5\u5047\u8bbe\u3002", "result": "\u8bba\u6587\u8ba4\u4e3a\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7684\u8fdb\u5c55\u4e3a\u76f4\u63a5\u68c0\u9a8c\u8fd9\u4e00\u5047\u8bbe\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u548c\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e00\u7814\u7a76\u7b56\u7565\u4e3a\u63ed\u793a\u8bed\u8a00\u7406\u89e3\u7684\u8ba4\u77e5\u548c\u795e\u7ecf\u673a\u5236\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.19528", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19528", "abs": "https://arxiv.org/abs/2511.19528", "authors": ["Rushuai Yang", "Zhiyuan Feng", "Tianxiang Zhang", "Kaixin Wang", "Chuheng Zhang", "Li Zhao", "Xiu Su", "Yi Chen", "Jiang Bian"], "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories", "comment": null, "summary": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.", "AI": {"tldr": "\u63d0\u51faDLR\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u6a21\u5f0f\u53d1\u73b0\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u6210\u529f\u7387\u7684\u673a\u5668\u4eba\u884c\u4e3a\u8f68\u8ff9\uff0c\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u76f8\u6bd4\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u9884\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u64cd\u4f5c\u8f68\u8ff9\u6570\u636e\uff0c\u4f46\u4eba\u5de5\u9065\u64cd\u4f5c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u5b66\u4e60\u6280\u80fd\uff0c\u4f46\u6807\u51c6RL\u8bad\u7ec3\u4f1a\u6536\u655b\u5230\u5355\u4e00\u6267\u884c\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u7684\u6548\u7528\u3002", "method": "\u63d0\u51faDiscover, Learn and Reinforce (DLR)\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6a21\u5f0f\u53d1\u73b0\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u591a\u4e2a\u4e0d\u540c\u4f46\u9ad8\u6210\u529f\u7387\u7684\u884c\u4e3a\u6a21\u5f0f\u7528\u4e8eVLA\u9884\u8bad\u7ec3\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDLR\u751f\u6210\u4e86\u660e\u663e\u66f4\u591a\u6837\u5316\u7684\u8f68\u8ff9\u8bed\u6599\u5e93\uff0c\u4e3a\u540c\u4e00\u4efb\u52a1\u5b66\u4e60\u591a\u79cd\u4e0d\u540c\u4e14\u9ad8\u6210\u529f\u7387\u7684\u7b56\u7565\uff0c\u8986\u76d6\u4e86\u66f4\u5e7f\u6cdb\u7684\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528DLR\u6570\u636e\u9884\u8bad\u7ec3\u7684VLA\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4f7f\u7528\u7b49\u91cf\u6807\u51c6RL\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u591a\u6a21\u5f0f\u5f3a\u5316\u5b66\u4e60\u53ef\u4f5c\u4e3a\u5b9e\u7528\u7684\u3001\u53ef\u6269\u5c55\u7684\u5177\u8eab\u57fa\u7840\u6a21\u578b\u6570\u636e\u5f15\u64ce\uff0cDLR\u5c55\u73b0\u51fa\u6b63\u7684\u6570\u636e\u7f29\u653e\u7279\u6027\uff0c\u800c\u5355\u6a21\u5f0fRL\u7f3a\u4e4f\u8fd9\u79cd\u7279\u6027\u3002"}}
{"id": "2511.19785", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.19785", "abs": "https://arxiv.org/abs/2511.19785", "authors": ["Maureen Herbert", "Katie Sun", "Angelica Lim", "Yasaman Etesam"], "title": "Gender Bias in Emotion Recognition by Large Language Models", "comment": "Accepted at AAAI 2026 Workshop (WS37)", "summary": "The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, \"How does this person feel?\". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u5fc3\u667a\u7406\u8bba\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u53bb\u504f\u7b56\u7565\uff0c\u53d1\u73b0\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u6bd4\u63a8\u7406\u65f6\u63d0\u793a\u5de5\u7a0b\u66f4\u6709\u6548\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30\u548c\u786e\u4fdd\u5176\u516c\u5e73\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u5173\u6ce8\u60c5\u611f\u5fc3\u667a\u7406\u8bba\u9886\u57df\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63cf\u8ff0\u4eba\u7269\u53ca\u5176\u73af\u5883\uff0c\u8be2\u95ee\u6a21\u578b\"\u8fd9\u4e2a\u4eba\u611f\u89c9\u5982\u4f55\uff1f\"\u6765\u68c0\u6d4b\u6027\u522b\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u79cd\u53bb\u504f\u7b56\u7565\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u4f9d\u9760\u63a8\u7406\u65f6\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u51cf\u5c11\u504f\u89c1\uff0c\u800c\u57fa\u4e8e\u8bad\u7ec3\u7684\u53bb\u504f\u5e72\u9884\u624d\u80fd\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u504f\u89c1\u51cf\u5c11\u3002", "conclusion": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684\u516c\u5e73\u6027\u7814\u7a76\u4e2d\uff0c\u9700\u8981\u91c7\u7528\u57fa\u4e8e\u8bad\u7ec3\u7684\u53bb\u504f\u7b56\u7565\uff0c\u800c\u4e0d\u80fd\u4ec5\u4ec5\u4f9d\u8d56\u63a8\u7406\u65f6\u7684\u63d0\u793a\u8c03\u6574\u6765\u6d88\u9664\u6027\u522b\u504f\u89c1\u3002"}}
{"id": "2511.19543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19543", "abs": "https://arxiv.org/abs/2511.19543", "authors": ["Omar Faris", "S\u0142awomir Tadeja", "Fulvio Forni"], "title": "A Virtual Mechanical Interaction Layer Enables Resilient Human-to-Robot Object Handovers", "comment": null, "summary": "Object handover is a common form of interaction that is widely present in collaborative tasks. However, achieving it efficiently remains a challenge. We address the problem of ensuring resilient robotic actions that can adapt to complex changes in object pose during human-to-robot object handovers. We propose the use of Virtual Model Control to create an interaction layer that controls the robot and adapts to the dynamic changes in the handover process. Additionally, we propose the use of augmented reality to facilitate bidirectional communication between humans and robots during handovers. We assess the performance of our controller in a set of experiments that demonstrate its resilience to various sources of uncertainties, including complex changes to the object's pose during the handover. Finally, we performed a user study with 16 participants to understand human preferences for different robot control profiles and augmented reality visuals in object handovers. Our results showed a general preference for the proposed approach and revealed insights that can guide further development in adapting the interaction with the user.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62df\u6a21\u578b\u63a7\u5236\u548c\u589e\u5f3a\u73b0\u5b9e\u7684\u4eba\u673a\u7269\u4f53\u4ea4\u63a5\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u4ea4\u63a5\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u7269\u4f53\u4ea4\u63a5\u8fc7\u7a0b\u4e2d\u7269\u4f53\u59ff\u6001\u590d\u6742\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u865a\u62df\u6a21\u578b\u63a7\u5236\u521b\u5efa\u4ea4\u4e92\u5c42\u6765\u63a7\u5236\u673a\u5668\u4eba\u5e76\u9002\u5e94\u4ea4\u63a5\u8fc7\u7a0b\u7684\u52a8\u6001\u53d8\u5316\uff0c\u540c\u65f6\u5229\u7528\u589e\u5f3a\u73b0\u5b9e\u4fc3\u8fdb\u4eba\u673a\u53cc\u5411\u901a\u4fe1\u3002", "result": "\u63a7\u5236\u5668\u5728\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u53c2\u4e0e\u8005\u666e\u904d\u504f\u597d\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u673a\u7269\u4f53\u4ea4\u63a5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7814\u7a76\u7ed3\u679c\u53ef\u4e3a\u672a\u6765\u4eba\u673a\u4ea4\u4e92\u9002\u5e94\u6027\u53d1\u5c55\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.19816", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19816", "abs": "https://arxiv.org/abs/2511.19816", "authors": ["Saif M. Mohammad"], "title": "Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions", "comment": null, "summary": "Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NRC VAD Lexicon v2\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b10,000\u4e2a\u82f1\u8bed\u591a\u8bcd\u8868\u8fbe\u53ca\u5176\u7ec4\u6210\u8bcd\u7684\u6548\u4ef7\u3001\u5524\u9192\u5ea6\u548c\u652f\u914d\u5ea6\u4eba\u5de5\u8bc4\u5206\u7684\u8bcd\u5178\uff0c\u6269\u5c55\u4e862018\u5e74\u53d1\u5e03\u7684v1\u7248\u672c\uff0c\u65b0\u589e\u4e8625,000\u4e2a\u5355\u8bcd\u6761\u76ee\u3002", "motivation": "\u73b0\u6709\u8bcd\u5178\u59822018\u5e74\u53d1\u5e03\u7684NRC VAD Lexicon\u53ea\u5305\u542b\u5355\u8bcd\u7684VAD\u5173\u8054\u8bc4\u5206\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8bcd\u8868\u8fbe\u7684\u60c5\u611f\u5206\u6790\u652f\u6301\uff0c\u4e14\u9700\u8981\u66f4\u65b02018\u5e74\u540e\u66f4\u5e38\u89c1\u7684\u8bcd\u6c47\u3002", "method": "\u901a\u8fc7\u4eba\u5de5\u8bc4\u5206\u65b9\u6cd5\u6536\u96c610,000\u4e2a\u591a\u8bcd\u8868\u8fbe\u53ca\u5176\u7ec4\u6210\u8bcd\u7684\u6548\u4ef7\u3001\u5524\u9192\u5ea6\u548c\u652f\u914d\u5ea6\u8bc4\u5206\uff0c\u5e76\u6269\u5c55\u4e8625,000\u4e2a\u5355\u8bcd\u7684\u8986\u76d6\u8303\u56f4\u3002", "result": "\u65b0\u8bcd\u5178\u5305\u542b10,000\u4e2a\u591a\u8bcd\u8868\u8fbe\u548c25,000\u4e2a\u5355\u8bcd\u6761\u76ee\uff0c\u8bc4\u5206\u5177\u6709\u9ad8\u5ea6\u53ef\u9760\u6027\uff0c\u53ef\u7528\u4e8e\u5206\u6790\u591a\u8bcd\u8868\u8fbe\u7684\u60c5\u611f\u7279\u5f81\u548c\u60c5\u611f\u7ec4\u5408\u6027\u3002", "conclusion": "NRC VAD Lexicon v2\u652f\u6301NLP\u3001\u5fc3\u7406\u5b66\u3001\u516c\u5171\u536b\u751f\u3001\u6570\u5b57\u4eba\u6587\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u7684\u7814\u7a76\uff0c\u53ef\u901a\u8fc7\u9879\u76ee\u7f51\u9875\u514d\u8d39\u83b7\u53d6\u3002"}}
{"id": "2511.19647", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19647", "abs": "https://arxiv.org/abs/2511.19647", "authors": ["Jennifer Grannen", "Michelle Pan", "Kenneth Llontop", "Cherie Ho", "Mark Zolotas", "Jeannette Bohg", "Dorsa Sadigh"], "title": "Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation", "comment": null, "summary": "Foundation models (FM) have unlocked powerful zero-shot capabilities in vision and language, yet their reliance on internet pretraining data leaves them brittle in unstructured, real-world settings. The messy, real-world data encountered during deployment (e.g. occluded or multilingual text) remains massively underrepresented in existing corpora. Robots, as embodied agents, are uniquely positioned to close this gap: they can act in physical environments to collect large-scale, real-world data that enriches FM training with precisely the examples current models lack. We introduce the Robot-Powered Data Flywheel, a framework that transforms robots from FM consumers into data generators. By deploying robots equipped with FMs in the wild, we enable a virtuous cycle: robots perform useful tasks while collecting real-world data that improves both domain-specific adaptation and domain-adjacent generalization. We instantiate this framework with Scanford, a mobile manipulator deployed in the East Asia Library for 2 weeks. Scanford autonomously scans shelves, identifies books using a vision-language model (VLM), and leverages the library catalog to label images without human annotation. This deployment both aids librarians and produces a dataset to finetune the underlying VLM, improving performance on the domain-specific in-the-wild library setting and on domain-adjacent multilingual OCR benchmarks. Using data collected from 2103 shelves, Scanford improves VLM performance on book identification from 32.0% to 71.8% and boosts domain-adjacent multilingual OCR from 24.8% to 46.6% (English) and 30.8% to 38.0% (Chinese), while saving an ~18.7 hrs of human time. These results highlight how robot-powered data flywheels can both reduce human effort in real deployments and unlock new pathways for continually adapting FMs to the messiness of reality. More details are at: https://scanford-robot.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u9a71\u52a8\u7684\u6570\u636e\u98de\u8f6e\u6846\u67b6\uff0c\u5c06\u673a\u5668\u4eba\u4ece\u57fa\u7840\u6a21\u578b\u6d88\u8d39\u8005\u8f6c\u53d8\u4e3a\u6570\u636e\u751f\u6210\u5668\uff0c\u901a\u8fc7\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u673a\u5668\u4eba\u6536\u96c6\u6570\u636e\u6765\u6539\u8fdb\u57fa\u7840\u6a21\u578b\u7684\u9886\u57df\u9002\u5e94\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u4f9d\u8d56\u4e92\u8054\u7f51\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u975e\u7ed3\u6784\u5316\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8868\u73b0\u8106\u5f31\u3002\u673a\u5668\u4eba\u4f5c\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u6536\u96c6\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5f00\u53d1\u4e86Scanford\u79fb\u52a8\u673a\u68b0\u81c2\uff0c\u5728\u56fe\u4e66\u9986\u90e8\u7f722\u5468\u81ea\u4e3b\u626b\u63cf\u4e66\u67b6\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u4e66\u7c4d\uff0c\u5e76\u5229\u7528\u56fe\u4e66\u9986\u76ee\u5f55\u81ea\u52a8\u6807\u6ce8\u56fe\u50cf\u3002", "result": "\u4ece2103\u4e2a\u4e66\u67b6\u6536\u96c6\u6570\u636e\uff0c\u5c06\u4e66\u7c4d\u8bc6\u522b\u51c6\u786e\u7387\u4ece32.0%\u63d0\u5347\u81f371.8%\uff0c\u591a\u8bed\u8a00OCR\u51c6\u786e\u7387\u4ece24.8%\u63d0\u5347\u81f346.6%\uff08\u82f1\u6587\uff09\u548c30.8%\u63d0\u5347\u81f338.0%\uff08\u4e2d\u6587\uff09\uff0c\u8282\u7701\u7ea618.7\u5c0f\u65f6\u4eba\u5de5\u65f6\u95f4\u3002", "conclusion": "\u673a\u5668\u4eba\u9a71\u52a8\u7684\u6570\u636e\u98de\u8f6e\u65e2\u80fd\u51cf\u5c11\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u4eba\u5de5\u52aa\u529b\uff0c\u53c8\u80fd\u4e3a\u57fa\u7840\u6a21\u578b\u6301\u7eed\u9002\u5e94\u73b0\u5b9e\u590d\u6742\u6027\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.19818", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19818", "abs": "https://arxiv.org/abs/2511.19818", "authors": ["Koena Ronny Mabokela", "Tim Schlippe", "Mpho Raborife", "Turgay Celik"], "title": "Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana", "comment": "Published in the The Fourth Workshop on Processing Emotions, Decisions and Opinions (EDO 2023) at 10th Language & Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics (LTC 2023), Pozna\u0144, Poland, 21-23 April 2023. ISBN: 978-83-232-4176-8", "summary": "Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8868\u60c5\u7b26\u53f7\u548c\u60c5\u611f\u8bcd\u6c47\u7684\u81ea\u52a8\u60c5\u611f\u6807\u6ce8\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u95ee\u9898\uff0c\u5728\u82f1\u8bed\u3001Sepedi\u548cSetswana\u8bed\u8a00\u4e0a\u5206\u522b\u8fbe\u523066%\u300169%\u548c63%\u7684\u6807\u6ce8\u51c6\u786e\u7387\u3002", "motivation": "\u8bb8\u591a\u975e\u6d32\u8bed\u8a00\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u800c\u88ab\u5f52\u7c7b\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u624b\u52a8\u6807\u6ce8\u6587\u672c\u6570\u636e\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7684\u5feb\u901f\u6807\u6ce8\u6d41\u7a0b\u6765\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "method": "\u5229\u7528\u8868\u60c5\u7b26\u53f7\u548c\u60c5\u611f\u8bcd\u6c47\u4fe1\u606f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u81ea\u52a8\u60c5\u611f\u6807\u6ce8\u65b9\u6cd5\uff0c\u5728\u5305\u542b\u82f1\u8bed\u3001Sepedi\u548cSetswana\u63a8\u6587\u7684SAfriSenti\u591a\u8bed\u8a00\u60c5\u611f\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u82f1\u8bed\u63a8\u6587\u6807\u6ce8\u51c6\u786e\u7387\u4e3a66%\uff0cSepedi\u63a8\u6587\u4e3a69%\uff0cSetswana\u63a8\u6587\u4e3a63%\uff0c\u5e73\u5747\u53ea\u9700\u4fee\u6b6334%\u7684\u81ea\u52a8\u751f\u6210\u6807\u7b7e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19651", "abs": "https://arxiv.org/abs/2511.19651", "authors": ["Lishuo Pan", "Mattia Catellani", "Thales C. Silva", "Lorenzo Sabattini", "Nora Ayanian"], "title": "Online Learning-Enhanced High Order Adaptive Safety Control", "comment": "8 pages, 7 figures, submitted to RA-L", "summary": "Control barrier functions (CBFs) are an effective model-based tool to formally certify the safety of a system. With the growing complexity of modern control problems, CBFs have received increasing attention in both optimization-based and learning-based control communities as a safety filter, owing to their provable guarantees. However, success in transferring these guarantees to real-world systems is critically tied to model accuracy. For example, payloads or wind disturbances can significantly influence the dynamics of an aerial vehicle and invalidate the safety guarantee. In this work, we propose an efficient yet flexible online learning-enhanced high-order adaptive control barrier function using Neural ODEs. Our approach improves the safety of a CBF-certified system on the fly, even under complex time-varying model perturbations. In particular, we deploy our hybrid adaptive CBF controller on a 38g nano quadrotor, keeping a safe distance from the obstacle, against 18km/h wind.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecfODE\u7684\u5728\u7ebf\u5b66\u4e60\u589e\u5f3a\u9ad8\u9636\u81ea\u9002\u5e94\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u590d\u6742\u65f6\u53d8\u6a21\u578b\u6270\u52a8\u4e0b\u5b9e\u65f6\u63d0\u9ad8CBF\u8ba4\u8bc1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u5e76\u572838g\u7eb3\u7c73\u56db\u65cb\u7ffc\u4e0a\u6210\u529f\u9a8c\u8bc1\uff0c\u572818km/h\u98ce\u901f\u4e0b\u4fdd\u6301\u4e0e\u969c\u788d\u7269\u7684\u5b89\u5168\u8ddd\u79bb\u3002", "motivation": "\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\u662f\u4fdd\u8bc1\u7cfb\u7edf\u5b89\u5168\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u5176\u5b89\u5168\u4fdd\u8bc1\u7684\u6210\u529f\u8f6c\u79fb\u5230\u73b0\u5b9e\u7cfb\u7edf\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6a21\u578b\u7cbe\u5ea6\u3002\u6709\u6548\u8f7d\u8377\u6216\u98ce\u6270\u52a8\u7b49\u590d\u6742\u65f6\u53d8\u6270\u52a8\u4f1a\u663e\u8457\u5f71\u54cd\u98de\u884c\u5668\u52a8\u529b\u5b66\u5e76\u4f7f\u5b89\u5168\u4fdd\u8bc1\u5931\u6548\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5728\u7ebf\u5b66\u4e60\u589e\u5f3a\u9ad8\u9636\u81ea\u9002\u5e94\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u65b9\u6cd5\uff0c\u4f7f\u7528\u795e\u7ecfODE\u6765\u9002\u5e94\u590d\u6742\u65f6\u53d8\u6a21\u578b\u6270\u52a8\uff0c\u5b9e\u73b0\u6df7\u5408\u81ea\u9002\u5e94CBF\u63a7\u5236\u5668\u3002", "result": "\u572838g\u7eb3\u7c73\u56db\u65cb\u7ffc\u4e0a\u6210\u529f\u90e8\u7f72\u8be5\u6df7\u5408\u81ea\u9002\u5e94CBF\u63a7\u5236\u5668\uff0c\u572818km/h\u98ce\u901f\u4e0b\u80fd\u591f\u4fdd\u6301\u4e0e\u969c\u788d\u7269\u7684\u5b89\u5168\u8ddd\u79bb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u7ebf\u5b66\u4e60\u589e\u5f3aCBF\u7684\u5b89\u5168\u6027\uff0c\u6709\u6548\u5e94\u5bf9\u590d\u6742\u65f6\u53d8\u6a21\u578b\u6270\u52a8\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u7684\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.19852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19852", "abs": "https://arxiv.org/abs/2511.19852", "authors": ["Shi-Wei Dai", "Yan-Wei Shie", "Tsung-Huan Yang", "Lun-Wei Ku", "Yung-Hui Li"], "title": "Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs", "comment": null, "summary": "Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.", "AI": {"tldr": "\u63d0\u51faPersonaPulse\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u89d2\u8272\u626e\u6f14\u63d0\u793a\u6765\u589e\u5f3aLLM\u7684\u4e2a\u6027\u5316\u8868\u8fbe\uff0c\u5229\u7528\u60c5\u5883\u54cd\u5e94\u57fa\u51c6\u8fdb\u884c\u8bc4\u5206\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b", "motivation": "\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u63d0\u793a\u6765\u6fc0\u53d1LLM\u7684\u7279\u5b9a\u4e2a\u6027\u7279\u5f81\uff0c\u4f46\u672a\u4f18\u5316\u8fd9\u4e9b\u63d0\u793a\u4ee5\u6700\u5927\u5316\u4e2a\u6027\u8868\u8fbe", "method": "\u5229\u7528LLM\u5bf9\u4e2a\u6027\u7279\u5f81\u7684\u56fa\u6709\u77e5\u8bc6\u8fed\u4ee3\u589e\u5f3a\u89d2\u8272\u626e\u6f14\u63d0\u793a\uff0c\u540c\u65f6\u6574\u5408\u60c5\u5883\u54cd\u5e94\u57fa\u51c6\u4f5c\u4e3a\u8bc4\u5206\u5de5\u5177\uff0c\u786e\u4fdd\u66f4\u771f\u5b9e\u548c\u60c5\u5883\u5316\u7684\u8bc4\u4f30\u6765\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793aPersonaPulse\u751f\u6210\u7684\u63d0\u793a\u4f18\u4e8e\u57fa\u4e8e\u5fc3\u7406\u5b66\u7814\u7a76\u8bbe\u8ba1\u7684\u5148\u524d\u5de5\u4f5c\uff0c\u63a2\u7d22\u4e86\u6a21\u578b\u5927\u5c0f\u4e0e\u4e2a\u6027\u5efa\u6a21\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u67d0\u4e9b\u4e2a\u6027\u7279\u5f81\u7684\u6fc0\u53d1\u7a0b\u5ea6\u53ef\u901a\u8fc7\u6682\u505c\u4f18\u5316\u8fc7\u7a0b\u90e8\u5206\u63a7\u5236", "conclusion": "\u63d0\u793a\u4f18\u5316\u5728\u5851\u9020LLM\u4e2a\u6027\u8868\u8fbe\u4e2d\u5177\u6709\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u9002\u5e94AI\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u89c1\u89e3"}}
{"id": "2511.19653", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19653", "abs": "https://arxiv.org/abs/2511.19653", "authors": ["Mahmud Suhaimi Ibrahim", "Shantanu Rahman", "Muhammad Samin Hasan", "Minhaj Uddin Ahmad", "Abdullah Abrar"], "title": "Flow-Based Path Planning for Multiple Homogenous UAVs for Outdoor Formation-Flying", "comment": "9 pages, 15 figures, conference", "summary": "Collision-free path planning is the most crucial component in multi-UAV formation-flying (MFF). We use unlabeled homogenous quadcopters (UAVs) to demonstrate the use of a flow network to create complete (inter-UAV) collision-free paths. This procedure has three main parts: 1) Creating a flow network graph from physical GPS coordinates, 2) Finding a path of minimum cost (least distance) using any graph-based path-finding algorithm, and 3) Implementing the Ford-Fulkerson Method to find the paths with the maximum flow (no collision). Simulations of up to 64 UAVs were conducted for various formations, followed by a practical experiment with 3 quadcopters for testing physical plausibility and feasibility. The results of these tests show the efficacy of this method's ability to produce safe, collision-free paths.", "AI": {"tldr": "\u4f7f\u7528\u6d41\u7f51\u7edc\u65b9\u6cd5\u4e3a\u591a\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u89c4\u5212\u65e0\u78b0\u649e\u8def\u5f84\uff0c\u5305\u62ec\u6784\u5efa\u6d41\u7f51\u7edc\u56fe\u3001\u5bfb\u627e\u6700\u5c0f\u6210\u672c\u8def\u5f84\u548c\u5b9e\u73b0\u6700\u5927\u6d41\u8def\u5f84\u5206\u914d\u3002", "motivation": "\u591a\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u4e2d\uff0c\u65e0\u78b0\u649e\u8def\u5f84\u89c4\u5212\u662f\u6700\u5173\u952e\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u9700\u8981\u89e3\u51b3\u65e0\u4eba\u673a\u95f4\u7684\u78b0\u649e\u95ee\u9898\u3002", "method": "1) \u4ece\u7269\u7406GPS\u5750\u6807\u6784\u5efa\u6d41\u7f51\u7edc\u56fe\uff1b2) \u4f7f\u7528\u56fe\u8def\u5f84\u7b97\u6cd5\u5bfb\u627e\u6700\u5c0f\u6210\u672c\u8def\u5f84\uff1b3) \u5e94\u7528Ford-Fulkerson\u65b9\u6cd5\u5bfb\u627e\u6700\u5927\u6d41\u8def\u5f84\uff08\u65e0\u78b0\u649e\uff09\u3002", "result": "\u5bf9\u6700\u591a64\u67b6\u65e0\u4eba\u673a\u8fdb\u884c\u4e86\u5404\u79cd\u7f16\u961f\u6a21\u62df\uff0c\u5e76\u75283\u67b6\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u8fdb\u884c\u4e86\u5b9e\u9645\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u5b89\u5168\u7684\u65e0\u78b0\u649e\u8def\u5f84\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.19858", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19858", "abs": "https://arxiv.org/abs/2511.19858", "authors": ["Farzad Ahmed", "Joniel Augustine Jerome", "Meliha Yetisgen", "\u00d6zlem Uzuner"], "title": "A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction", "comment": null, "summary": "Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.\n  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.\n  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.\n  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u9759\u6001\u968f\u673a\u793a\u4f8b\u3001\u68c0\u7d22\u589e\u5f3a\u52a8\u6001\u63d0\u793a\uff09\u5728\u533b\u7597\u9519\u8bef\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u68c0\u7d22\u589e\u5f3a\u52a8\u6001\u63d0\u793a\u5728\u51cf\u5c11\u5047\u9633\u6027\u3001\u63d0\u9ad8\u53ec\u56de\u7387\u548c\u751f\u6210\u66f4\u51c6\u786e\u4fee\u6b63\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e34\u5e8a\u6587\u6863\u4e2d\u5b58\u5728\u53ef\u80fd\u5f71\u54cd\u60a3\u8005\u5b89\u5168\u7684\u4e8b\u5b9e\u6027\u3001\u8bca\u65ad\u6027\u548c\u7ba1\u7406\u6027\u9519\u8bef\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5e2e\u52a9\u68c0\u6d4b\u548c\u4fee\u6b63\u8fd9\u4e9b\u9519\u8bef\uff0c\u4f46\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u884c\u4e3a\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528MEDEC\u6570\u636e\u96c6\u8bc4\u4f309\u4e2a\u6307\u4ee4\u8c03\u4f18\u7684LLM\uff0c\u6bd4\u8f83\u96f6\u6837\u672c\u63d0\u793a\u3001\u9759\u6001\u968f\u673a\u793a\u4f8b\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u52a8\u6001\u63d0\u793a\u5728\u9519\u8bef\u6807\u8bb0\u68c0\u6d4b\u3001\u9519\u8bef\u53e5\u5b50\u68c0\u6d4b\u548c\u9519\u8bef\u4fee\u6b63\u4e09\u4e2a\u5b50\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u68c0\u7d22\u589e\u5f3a\u52a8\u6001\u63d0\u793a\u5728\u6240\u67099\u4e2aLLM\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5c06\u5047\u9633\u6027\u7387\u964d\u4f4e\u7ea615%\uff0c\u5728\u9519\u8bef\u53e5\u5b50\u68c0\u6d4b\u4e2d\u53ec\u56de\u7387\u63d0\u9ad85-10%\uff0c\u5e76\u751f\u6210\u66f4\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u4fee\u6b63\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u52a8\u6001\u63d0\u793a\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u9759\u6001\u968f\u673a\u793a\u4f8b\u63d0\u793a\uff0c\u4f7f\u7528\u68c0\u7d22\u793a\u4f8b\u53ef\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u51cf\u5c11\u5047\u9633\u6027\u5e76\u589e\u5f3a\u533b\u7597\u9519\u8bef\u4fee\u6b63\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.19655", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19655", "abs": "https://arxiv.org/abs/2511.19655", "authors": ["Shantanu Rahman", "Nayeb Hasin", "Mainul Islam"], "title": "Development of a Testbed for Autonomous Vehicles: Integrating MPC Control with Monocular Camera Lane Detection", "comment": "49 pages, 23 figures", "summary": "Autonomous vehicles are becoming popular day by day not only for autonomous road traversal but also for industrial automation, farming and military. Most of the standard vehicles follow the Ackermann style steering mechanism. This has become to de facto standard for large and long faring vehicles. The local planner of an autonomous vehicle controls the low-level vehicle movement upon which the vehicle will perform its motor actuation. In our work, we focus on autonomous vehicles in road and perform experiments to analyze the effect of low-level controllers in the simulation and a real environment. To increase the precision and stability of trajectory tracking in autonomous cars, a novel method that combines lane identification with Model Predictive Control (MPC) is presented. The research focuses on camera-equipped autonomous vehicles and uses methods like edge recognition, sliding window-based straight-line identification for lane line extraction, and dynamic region of interest (ROI) extraction. Next, to follow the identified lane line, an MPC built on a bicycle vehicle dynamics model is created. A single-lane road simulation model is built using ROS Gazebo and tested in order to verify the controller's performance. The root mean square error between the optimal tracking trajectory and the target trajectory was reduced by 27.65% in the simulation results, demonstrating the high robustness and flexibility of the developed controller.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f66\u9053\u8bc6\u522b\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u8ddf\u8e2a\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u4eff\u771f\u4e2d\u4f7f\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\u4e8627.65%\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u9053\u8def\u5bfc\u822a\u4e2d\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\uff0c\u73b0\u6709\u7684Ackermann\u8f6c\u5411\u673a\u5236\u8f66\u8f86\u9700\u8981\u66f4\u7a33\u5b9a\u548c\u7cbe\u786e\u7684\u4f4e\u7ea7\u63a7\u5236\u5668\u3002", "method": "\u4f7f\u7528\u8fb9\u7f18\u8bc6\u522b\u3001\u6ed1\u52a8\u7a97\u53e3\u76f4\u7ebf\u8bc6\u522b\u8fdb\u884c\u8f66\u9053\u7ebf\u63d0\u53d6\uff0c\u7ed3\u5408\u52a8\u6001\u611f\u5174\u8da3\u533a\u57df(ROI)\u63d0\u53d6\uff0c\u7136\u540e\u57fa\u4e8e\u81ea\u884c\u8f66\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\u6784\u5efaMPC\u63a7\u5236\u5668\u6765\u8ddf\u8e2a\u8bc6\u522b\u51fa\u7684\u8f66\u9053\u7ebf\u3002", "result": "\u5728ROS Gazebo\u4eff\u771f\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u6700\u4f18\u8ddf\u8e2a\u8f68\u8ff9\u4e0e\u76ee\u6807\u8f68\u8ff9\u4e4b\u95f4\u7684\u5747\u65b9\u6839\u8bef\u5dee\u964d\u4f4e\u4e8627.65%\uff0c\u63a7\u5236\u5668\u8868\u73b0\u51fa\u9ad8\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f66\u9053\u8bc6\u522b\u4e0eMPC\u7ed3\u5408\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2511.19957", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19957", "abs": "https://arxiv.org/abs/2511.19957", "authors": ["Tianyi Chen", "Michael Solodko", "Sen Wang", "Jongwoo Ko", "Junheng Hao", "Colby Banbury", "Sara Abdali", "Saeed Amizadeh", "Qing Xiao", "Yinheng Li", "Tianyu Ding", "Kamran Ghasedi Dizaji", "Suzhen Zheng", "Hao Fan", "Justin Wagle", "Pashmina Cameron", "Kazuhito Koishida"], "title": "AppSelectBench: Application-Level Tool Selection Benchmark", "comment": null, "summary": "Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.", "AI": {"tldr": "\u63d0\u51fa\u4e86AppSelectBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUA\uff09\u7684\u5e94\u7528\u9009\u62e9\u80fd\u529b\uff0c\u5305\u542b10\u4e07\u4e2a\u771f\u5b9e\u591a\u6837\u7684\u7528\u6237\u4efb\u52a1\uff0c\u8986\u76d6100\u4e2a\u684c\u9762\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u7ec6\u7c92\u5ea6API\u9009\u62e9\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u5e94\u7528\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u800c\u5e94\u7528\u9009\u62e9\u662fCUA\u6709\u6548\u8fd0\u4f5c\u7684\u57fa\u7840\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u7528\u6237\u4efb\u52a1\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u521b\u5efa\u771f\u5b9e\u3001\u591a\u6837\u3001\u8bed\u4e49\u57fa\u7840\u7684\u7528\u6237\u610f\u56fe\uff0c\u5e76\u63d0\u4f9b\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff08\u968f\u673a\u3001\u542f\u53d1\u5f0f\u3001\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u68c0\u7d22\u589e\u5f3a\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8de8\u5e94\u7528\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u7cfb\u7edf\u6027\u5f31\u70b9\uff0c\u96be\u4ee5\u505a\u51fa\u4e00\u81f4\u7684\u5e94\u7528\u9009\u62e9\u3002", "conclusion": "AppSelectBench\u4e3a\u7814\u7a76\u548c\u63a8\u8fdb\u667a\u80fdCUA\u7684\u5e94\u7528\u7ea7\u63a8\u7406\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u80fd\u529b\u3002"}}
{"id": "2511.19691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19691", "abs": "https://arxiv.org/abs/2511.19691", "authors": ["Thomas Marshall Vielmetti", "Devansh R Agrawal", "Dimitra Panagou"], "title": "Multi-Agent gatekeeper: Safe Flight Planning and Formation Control for Urban Air Mobility", "comment": "13 pages, 4 figures, to appear AIAA SciTech 2026", "summary": "We present Multi-Agent gatekeeper, a framework that provides provable safety guarantees for leader-follower formation control in cluttered 3D environments. Existing methods face a trad-off: online planners and controllers lack formal safety guarantees, while offline planners lack adaptability to changes in the number of agents or desired formation. To address this gap, we propose a hybrid architecture where a single leader tracks a pre-computed, safe trajectory, which serves as a shared trajectory backup set for all follower agents. Followers execute a nominal formation-keeping tracking controller, and are guaranteed to remain safe by always possessing a known-safe backup maneuver along the leader's path. We formally prove this method ensures collision avoidance with both static obstacles and other agents. The primary contributions are: (1) the multi-agent gatekeeper algorithm, which extends our single-agent gatekeeper framework to multi-agent systems; (2) the trajectory backup set for provably safe inter-agent coordination for leader-follower formation control; and (3) the first application of the gatekeeper framework in a 3D environment. We demonstrate our approach in a simulated 3D urban environment, where it achieved a 100% collision-avoidance success rate across 100 randomized trials, significantly outperforming baseline CBF and NMPC methods. Finally, we demonstrate the physical feasibility of the resulting trajectories on a team of quadcopters.", "AI": {"tldr": "\u63d0\u51faMulti-Agent gatekeeper\u6846\u67b6\uff0c\u4e3a3D\u590d\u6742\u73af\u5883\u4e2d\u7684\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7f16\u961f\u63a7\u5236\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u901a\u8fc7\u9886\u5bfc\u8005\u9884\u8ba1\u7b97\u5b89\u5168\u8f68\u8ff9\u4f5c\u4e3a\u5171\u4eab\u5907\u4efd\uff0c\u786e\u4fdd\u8ddf\u968f\u8005\u59cb\u7ec8\u6709\u5b89\u5168\u907f\u969c\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6743\u8861\uff1a\u5728\u7ebf\u89c4\u5212\u5668\u548c\u63a7\u5236\u5668\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\uff0c\u800c\u79bb\u7ebf\u89c4\u5212\u5668\u65e0\u6cd5\u9002\u5e94\u4ee3\u7406\u6570\u91cf\u6216\u671f\u671b\u7f16\u961f\u7684\u53d8\u5316\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u5b89\u5168\u53c8\u5177\u5907\u9002\u5e94\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff1a\u5355\u4e2a\u9886\u5bfc\u8005\u8ddf\u8e2a\u9884\u8ba1\u7b97\u7684\u5b89\u5168\u8f68\u8ff9\u4f5c\u4e3a\u5171\u4eab\u8f68\u8ff9\u5907\u4efd\u96c6\uff0c\u8ddf\u968f\u8005\u6267\u884c\u540d\u4e49\u7f16\u961f\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u59cb\u7ec8\u62e5\u6709\u6cbf\u9886\u5bfc\u8005\u8def\u5f84\u7684\u5df2\u77e5\u5b89\u5168\u5907\u4efd\u673a\u52a8\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df3D\u57ce\u5e02\u73af\u5883\u4e2d\uff0c100\u6b21\u968f\u673a\u8bd5\u9a8c\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u907f\u78b0\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfCBF\u548cNMPC\u65b9\u6cd5\uff0c\u5e76\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u56e2\u961f\u4e0a\u9a8c\u8bc1\u4e86\u8f68\u8ff9\u7684\u7269\u7406\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u5355\u667a\u80fd\u4f53gatekeeper\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4e3a\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7f16\u961f\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u534f\u8c03\u673a\u5236\uff0c\u662fgatekeeper\u6846\u67b6\u57283D\u73af\u5883\u4e2d\u7684\u9996\u6b21\u5e94\u7528\u3002"}}
{"id": "2511.19987", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.19987", "abs": "https://arxiv.org/abs/2511.19987", "authors": ["Xinyu Wang", "Hanwei Wu", "Qingchen Hu", "Zhenghan Tai", "Jingrui Tian", "Lei Ding", "Jijun Chi", "Hailin He", "Tung Sum Thomas Kwok", "Yufei Cui", "Sicheng Lyu", "Muzhi Li", "Mingze Li", "Xinyue Yu", "Ling Zhou", "Peng Lu"], "title": "$\\text{R}^2\\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers", "comment": "13 pages, including 3 figures and 3 tables", "summary": "Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.", "AI": {"tldr": "R2R\u662f\u4e00\u4e2a\u9886\u57df\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u8def\u7531\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u9886\u57df\u4e13\u4e1a\u5316\u95ee\u9898\uff0c\u907f\u514d\u8868\u9762\u5f62\u5f0f\u8fc7\u62df\u5408\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u901a\u7528\u6a21\u578b\u5728\u91d1\u878d\u3001\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u800c\u7b80\u5355\u5fae\u8c03\u4f1a\u5bfc\u81f4\u8868\u9762\u5f62\u5f0f\u8fc7\u62df\u5408\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u4e13\u5bb6\u8def\u7531\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08EAG\uff09\uff0c\u901a\u8fc7\u63a9\u7801\u6700\u5177\u9884\u6d4b\u6027\u7684\u8868\u9762\u7ebf\u7d22\uff0c\u5f3a\u5236\u91cd\u6392\u5e8f\u5668\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u7684\u76f8\u5173\u6027\u6a21\u5f0f\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6f5c\u5728\u8bed\u4e49\u8def\u7531\u5668\u4ece\u51bb\u7ed3\u7684\u89e3\u7801\u5668\u4e2d\u9009\u62e9\u6700\u4f18LoRA\u4e13\u5bb6\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\uff08\u6cd5\u5f8b\u3001\u533b\u7597\u3001\u91d1\u878d\uff09\u548c\u4e0d\u540c\u91cd\u6392\u5e8f\u5668\u9aa8\u5e72\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cR2R\u59cb\u7ec8\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u548c\u5355\u9886\u57df\u5fae\u8c03\u57fa\u7ebf\u3002", "conclusion": "R2R\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8de8\u9886\u57df\u9c81\u68d2\u6027\uff0c\u80fd\u6709\u6548\u5b9e\u73b0\u9886\u57df\u4e13\u4e1a\u5316\u3002"}}
{"id": "2511.19709", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19709", "abs": "https://arxiv.org/abs/2511.19709", "authors": ["Lukas Molnar", "Jin Cheng", "Gabriele Fadini", "Dongho Kang", "Fatemeh Zargarbashi", "Stelian Coros"], "title": "Whole-Body Inverse Dynamics MPC for Legged Loco-Manipulation", "comment": "9 pages, 6 figures, to be published in IEEE Robotics and Automation Letters (Special Issue: Advancements in MPC and Learning Algorithms for Legged Robots)", "summary": "Loco-manipulation demands coordinated whole-body motion to manipulate objects effectively while maintaining locomotion stability, presenting significant challenges for both planning and control. In this work, we propose a whole-body model predictive control (MPC) framework that directly optimizes joint torques through full-order inverse dynamics, enabling unified motion and force planning and execution within a single predictive layer. This approach allows emergent, physically consistent whole-body behaviors that account for the system's dynamics and physical constraints. We implement our MPC formulation using open software frameworks (Pinocchio and CasADi), along with the state-of-the-art interior-point solver Fatrop. In real-world experiments on a Unitree B2 quadruped equipped with a Unitree Z1 manipulator arm, our MPC formulation achieves real-time performance at 80 Hz. We demonstrate loco-manipulation tasks that demand fine control over the end-effector's position and force to perform real-world interactions like pulling heavy loads, pushing boxes, and wiping whiteboards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u8eab\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u9636\u9006\u52a8\u529b\u5b66\u76f4\u63a5\u4f18\u5316\u5173\u8282\u626d\u77e9\uff0c\u5728\u5355\u4e2a\u9884\u6d4b\u5c42\u4e2d\u5b9e\u73b0\u7edf\u4e00\u7684\u8fd0\u52a8\u548c\u529b\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e8680Hz\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5168\u8eab\u8fd0\u52a8\u64cd\u4f5c\u9700\u8981\u534f\u8c03\u7684\u5168\u8eab\u8fd0\u52a8\u6765\u6709\u6548\u64cd\u7eb5\u7269\u4f53\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u7a33\u5b9a\u6027\uff0c\u8fd9\u5bf9\u89c4\u5212\u548c\u63a7\u5236\u90fd\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5168\u8eab\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u9636\u9006\u52a8\u529b\u5b66\u76f4\u63a5\u4f18\u5316\u5173\u8282\u626d\u77e9\uff0c\u91c7\u7528Pinocchio\u3001CasADi\u8f6f\u4ef6\u6846\u67b6\u548cFatrop\u5185\u70b9\u6c42\u89e3\u5668\u5b9e\u73b0\u3002", "result": "\u5728\u914d\u5907\u673a\u68b0\u81c2\u7684Unitree B2\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e8680Hz\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u6210\u529f\u5b8c\u6210\u4e86\u62c9\u52a8\u91cd\u7269\u3001\u63a8\u7bb1\u5b50\u548c\u64e6\u767d\u677f\u7b49\u5b9e\u9645\u4ea4\u4e92\u4efb\u52a1\u3002", "conclusion": "\u8be5MPC\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u7684\u5168\u8eab\u884c\u4e3a\uff0c\u8003\u8651\u4e86\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u7269\u7406\u7ea6\u675f\uff0c\u5728\u5b9e\u65f6\u5168\u8eab\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.19997", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19997", "abs": "https://arxiv.org/abs/2511.19997", "authors": ["Mihir Sahasrabudhe"], "title": "Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test", "comment": "19 pages, 4 figures. Code available at https://github.com/mihirs-0/synass", "summary": "Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a \"reversal curse,\" and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u8bed\u8a00\u5148\u9a8c\u7684\u7eaf\u5408\u6210\u6570\u636e\u4e0a\uff0c\u56e0\u679cTransformer\u6a21\u578b\u4ecd\u8868\u73b0\u51fa\u663e\u8457\u7684\u65b9\u5411\u6027\u4f18\u5316\u5dee\u8ddd\uff0c\u8868\u660e\u65b9\u5411\u6027\u5b66\u4e60\u56f0\u96be\u662f\u67b6\u6784\u672c\u8eab\u56fa\u6709\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3Transformer\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u89c2\u5bdf\u5230\u7684\"\u53cd\u8f6c\u8bc5\u5492\"\u95ee\u9898\uff0c\u6f84\u6e05\u8fd9\u79cd\u65b9\u5411\u6027\u5931\u8d25\u662f\u6e90\u4e8e\u8bed\u8a00\u7edf\u8ba1\u7279\u6027\u8fd8\u662f\u67b6\u6784\u672c\u8eab\u3002", "method": "\u4f7f\u7528\u5177\u6709\u53ef\u8c03\u5206\u652f\u56e0\u5b50K\u7684\u968f\u673a\u5b57\u7b26\u4e32\u6620\u5c04\u6784\u5efa\u5408\u6210\u57fa\u51c6\uff0c\u521b\u5efa\u96f6\u6761\u4ef6\u71b5\u7684\u524d\u5411\u4efb\u52a1\u548c\u5177\u6709\u5206\u6790\u786e\u5b9a\u71b5\u4e0b\u9650\u7684\u9006\u5411\u4efb\u52a1\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4ece\u5934\u8bad\u7ec3\u7684GPT-2\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u65b9\u5411\u6027\u4f18\u5316\u5dee\u8ddd\uff08\u5982K=5\u65f61.16 nats\uff09\uff0c\u8fdc\u5927\u4e8e\u5728\u76f8\u540c\u6570\u636e\u4e0a\u8bad\u7ec3\u7684MLP\uff1b\u9884\u8bad\u7ec3\u521d\u59cb\u5316\u6539\u53d8\u4e86\u4f18\u5316\u884c\u4e3a\u4f46\u672a\u6d88\u9664\u5dee\u8ddd\uff1bLoRA\u5728\u9ad8\u71b5\u9006\u5411\u6620\u5c04\u4e0a\u9047\u5230\u5bb9\u91cf\u74f6\u9888\u3002", "conclusion": "\u7814\u7a76\u5206\u79bb\u51fa\u4e86\u4e00\u4e2a\u4e0e\u8bed\u4e49\u65e0\u5173\u7684\u6700\u5c0f\u65b9\u5411\u6027\u6469\u64e6\u7279\u5f81\uff0c\u8868\u660e\u5373\u4f7f\u79fb\u9664\u8bed\u8a00\u5148\u9a8c\u3001\u8bcd\u9891\u548c\u8bed\u6599\u7ea7\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027\uff0c\u56e0\u679cTransformer\u8bad\u7ec3\u4e2d\u56fa\u6709\u7684\u65b9\u5411\u6027\u504f\u89c1\u4ecd\u7136\u5b58\u5728\u3002"}}
{"id": "2511.19859", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19859", "abs": "https://arxiv.org/abs/2511.19859", "authors": ["Xiangkai Ma", "Lekai Xing", "Han Zhang", "Wenzhong Li", "Sanglu Lu"], "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation", "comment": null, "summary": "Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\\%, 9.6\\% and 12.1\\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.", "AI": {"tldr": "VITA\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u89c6\u89c9\u548c\u52a8\u4f5c\u7684\u5171\u4eab\u79bb\u6563\u6f5c\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u89c2\u5bdf\u4e0e\u4f4e\u7ea7\u52a8\u4f5c\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672cCoT\u5728\u590d\u6742\u7a7a\u95f4\u73af\u5883\u4e2d\u96be\u4ee5\u5145\u5206\u6355\u6349\u573a\u666f\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7684\u89c6\u89c9\u89c2\u5bdf\u4e0e\u4f4e\u7ea7\u52a8\u4f5c\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u3001\u89c6\u89c9\u9884\u6d4b\u4e0e\u52a8\u4f5c\u751f\u6210\u76ee\u6807\u51b2\u7a81\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faVision-Integrated Trajectory Alignment\u6846\u67b6\uff0c\u5b66\u4e60\u89c6\u89c9\u548c\u52a8\u4f5c\u7684\u5171\u4eab\u79bb\u6563\u6f5c\u7a7a\u95f4\uff0c\u901a\u8fc7\u9690\u5f0f\u89c6\u89c9CoT\u540c\u65f6\u89e3\u7801\u672a\u6765\u5e27\u9884\u6d4b\u548c\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5c06\u89c6\u89c9\u52a8\u6001\u5185\u5316\u4e3a\u8fd0\u52a8\u89c4\u5212\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u5728CALVIN\u3001LIBERO\u548cSimplerEnv\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u6bd4\u73b0\u6709\u57fa\u7ebf\u63d0\u534714.5%\u30019.6%\u548c12.1%\uff0c\u5728\u516d\u9879\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523080.5%\u3002", "conclusion": "VITA\u5c55\u793a\u4e86\u4f5c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u611f\u77e5\u548c\u8fd0\u52a8\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2511.20001", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.20001", "abs": "https://arxiv.org/abs/2511.20001", "authors": ["Edward Ajayi", "Martha Kachweka", "Mawuli Deku", "Emily Aiken"], "title": "A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media", "comment": "Accepted for Oral Presentation at the AAAI-26 Bridge Program on AI for Medicine and Healthcare (AIMedHealth). To appear in Proceedings of Machine Learning Research (PMLR)", "summary": "Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous \"split-then-balance\" pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conducted a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAPLLM explainability framework and present a prototype dashboard (\"Social Media Screener\") designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u7c7b\u522b\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u68c0\u6d4b\u5341\u79cd\u4e0d\u540c\u7684\u5fc3\u7406\u5065\u5eb7\u548c\u7f51\u7edc\u6b3a\u51cc\u7c7b\u522b\u3002\u901a\u8fc7\u6bd4\u8f83\u591a\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u9886\u57df\u9002\u5e94\u7684MentalBERT\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u5f00\u53d1\u4e86\u7ed3\u5408SHAPLLM\u89e3\u91ca\u6027\u6846\u67b6\u7684\u539f\u578b\u4eea\u8868\u677f\u3002", "motivation": "\u6570\u5b57\u7a7a\u95f4\u4e2d\u65e5\u76ca\u4e25\u91cd\u7684\u5fc3\u7406\u5065\u5eb7\u6311\u6218\u548c\u7f51\u7edc\u6b3a\u51cc\u95ee\u9898\u9700\u8981\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u4eceTwitter\u548cReddit\u6536\u96c6\u6570\u636e\uff0c\u91c7\u7528\"\u5206\u5272\u540e\u5e73\u8861\"\u6d41\u7a0b\uff0c\u5728\u5e73\u8861\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u73b0\u5b9e\u4e0d\u5e73\u8861\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u3002\u6bd4\u8f83\u4e86\u4f20\u7edf\u8bcd\u6c47\u6a21\u578b\u3001\u6df7\u5408\u65b9\u6cd5\u548c\u591a\u79cd\u7aef\u5230\u7aef\u5fae\u8c03transformer\u6a21\u578b\u3002", "result": "\u7aef\u5230\u7aef\u5fae\u8c03\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u9886\u57df\u9002\u5e94\u7684MentalBERT\u6210\u4e3a\u6700\u4f73\u6a21\u578b\uff0c\u51c6\u786e\u7387\u8fbe\u52300.92\uff0c\u5b8f\u89c2F1\u5206\u6570\u4e3a0.76\uff0c\u8d85\u8d8a\u4e86\u901a\u7528\u5bf9\u5e94\u6a21\u578b\u548c\u96f6\u6837\u672cLLM\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u88ab\u5b9a\u4f4d\u4e3a\u4eba\u5de5\u53c2\u4e0e\u5faa\u73af\u7684\u7b5b\u67e5\u8f85\u52a9\u5de5\u5177\u800c\u975e\u8bca\u65ad\u5de5\u5177\u3002\u672a\u6765\u9700\u8981\u5728\u5728\u7ebf\u5b89\u5168\u548c\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u7684\u4ea4\u53c9\u9886\u57df\u5f00\u53d1\u591a\u6807\u7b7e\u3001\u4e34\u5e8a\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2511.19869", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19869", "abs": "https://arxiv.org/abs/2511.19869", "authors": ["Eito Sato", "Takahiro Wada"], "title": "Human-Centered Cooperative Control Coupling Autonomous and Haptic Shared Control via Control Barrier Function", "comment": null, "summary": "Haptic shared control (HSC) is effective in teleoperation when full autonomy is limited by uncertainty or sensing constraints. However, autonomous control performance achieved by maximizing HSC strength is limited because the dynamics of the joystick and human arm affect the robot's behavior. We propose a cooperative framework coupling a joystick-independent autonomous controller with HSC. A control barrier function ignores joystick inputs within a safe region determined by the human operator in real-time, while HSC is engaged otherwise. A pilot experiment on simulated tasks with tele-operated underwater robot in virtual environment demonstrated improved accuracy and reduced required time over conventional HSC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u72ec\u7acb\u81ea\u4e3b\u63a7\u5236\u5668\u4e0e\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5728\u5b89\u5168\u533a\u57df\u5185\u5ffd\u7565\u64cd\u7eb5\u6746\u8f93\u5165\uff0c\u5728\u865a\u62df\u73af\u5883\u5b9e\u9a8c\u4e2d\u63d0\u5347\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u5728\u6700\u5927\u5316\u63a7\u5236\u5f3a\u5ea6\u65f6\u53d7\u5230\u64cd\u7eb5\u6746\u548c\u4eba\u4f53\u624b\u81c2\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u63a7\u5236\u6027\u80fd\u7684\u63d0\u5347\u3002", "method": "\u5f00\u53d1\u4e86\u534f\u4f5c\u6846\u67b6\uff0c\u5c06\u72ec\u7acb\u4e8e\u64cd\u7eb5\u6746\u7684\u81ea\u4e3b\u63a7\u5236\u5668\u4e0e\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u7ed3\u5408\uff0c\u4f7f\u7528\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5728\u5b89\u5168\u533a\u57df\u5185\u5ffd\u7565\u64cd\u7eb5\u6746\u8f93\u5165\uff0c\u5176\u4ed6\u60c5\u51b5\u4e0b\u542f\u7528\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u3002", "result": "\u5728\u865a\u62df\u73af\u5883\u4e2d\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u6a21\u62df\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u89e6\u89c9\u5171\u4eab\u63a7\u5236\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u4e14\u6240\u9700\u65f6\u95f4\u51cf\u5c11\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u4f5c\u6846\u67b6\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u7684\u5c40\u9650\u6027\uff0c\u5728\u9065\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.20056", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20056", "abs": "https://arxiv.org/abs/2511.20056", "authors": ["Huiyu Bai", "Runze Wang", "Zhuoyun Du", "Yiyang Zhao", "Fengji Zhang", "Haoyu Chen", "Xiaoyong Zhu", "Bo Zheng", "Xuejiao Zhao"], "title": "Online-PVLM: Advancing Personalized VLMs with Online Concept Learning", "comment": "Work in Progress", "summary": "Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.", "AI": {"tldr": "\u63d0\u51fa\u4e86Online-PVLM\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u66f2\u8868\u793a\u5b9e\u73b0\u4e2a\u6027\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5728\u7ebf\u6982\u5ff5\u5b66\u4e60\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5728\u6d4b\u8bd5\u65f6\u751f\u6210\u6982\u5ff5\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u652f\u6301\u5b9e\u65f6\u9002\u5e94\u548c\u5927\u89c4\u6a21\u573a\u666f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u6982\u5ff5\u5b66\u4e60\u5355\u72ec\u7684\u5d4c\u5165\uff0c\u65e0\u6cd5\u652f\u6301\u6d4b\u8bd5\u65f6\u7684\u5b9e\u65f6\u9002\u5e94\uff0c\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u65e0\u6cd5\u9ad8\u6548\u68c0\u7d22\u6982\u5ff5\u5d4c\u5165\u3002", "method": "\u5229\u7528\u53cc\u66f2\u8868\u793a\u5f00\u53d1\u4e86Online-PVLM\u6846\u67b6\uff0c\u91c7\u7528\u514d\u8bad\u7ec3\u7684\u6982\u5ff5\u5d4c\u5165\u751f\u6210\u8303\u5f0f\uff0c\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u5728\u7ebf\u6982\u5ff5\u5b66\u4e60\u3002", "result": "\u5728\u5305\u542b1,292\u4e2a\u6982\u5ff5\u548c\u8d85\u8fc730K\u9ad8\u8d28\u91cf\u5b9e\u4f8b\u7684OP-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4f7f\u4e2a\u6027\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4f7f\u7528\u65e2\u5177\u6709\u53ef\u6269\u5c55\u6027\u53c8\u9ad8\u6548\uff0c\u89e3\u51b3\u4e86\u5b9e\u65f6\u9002\u5e94\u548c\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u6311\u6218\u3002"}}
{"id": "2511.19914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19914", "abs": "https://arxiv.org/abs/2511.19914", "authors": ["Dapeng Zhang", "Fei Shen", "Rui Zhao", "Yinda Chen", "Peng Zhi", "Chenyang Li", "Rui Zhou", "Qingguo Zhou"], "title": "CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model", "comment": null, "summary": "Autonomous driving represents a prominent application of artificial intelligence. Recent approaches have shifted from focusing solely on common scenarios to addressing complex, long-tail situations such as subtle human behaviors, traffic accidents, and non-compliant driving patterns. Given the demonstrated capabilities of large language models (LLMs) in understanding visual and natural language inputs and following instructions, recent methods have integrated LLMs into autonomous driving systems to enhance reasoning, interpretability, and performance across diverse scenarios. However, existing methods typically rely either on real-world data, which is suitable for industrial deployment, or on simulation data tailored to rare or hard case scenarios. Few approaches effectively integrate the complementary advantages of both data sources. To address this limitation, we propose a novel VLM-guided, end-to-end adversarial transfer framework for autonomous driving that transfers long-tail handling capabilities from simulation to real-world deployment, named CoC-VLA. The framework comprises a teacher VLM model, a student VLM model, and a discriminator. Both the teacher and student VLM models utilize a shared base architecture, termed the Chain-of-Causality Visual-Language Model (CoC VLM), which integrates temporal information via an end-to-end text adapter. This architecture supports chain-of-thought reasoning to infer complex driving logic. The teacher and student VLM models are pre-trained separately on simulated and real-world datasets. The discriminator is trained adversarially to facilitate the transfer of long-tail handling capabilities from simulated to real-world environments by the student VLM model, using a novel backpropagation strategy.", "AI": {"tldr": "\u63d0\u51faCoC-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8fc1\u79fb\u5b66\u4e60\u5c06\u6a21\u62df\u73af\u5883\u4e2d\u7684\u957f\u5c3e\u573a\u666f\u5904\u7406\u80fd\u529b\u8f6c\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u8981\u4e48\u4f7f\u7528\u6a21\u62df\u6570\u636e\uff0c\u4f46\u5f88\u5c11\u80fd\u6709\u6548\u6574\u5408\u4e24\u8005\u7684\u4e92\u8865\u4f18\u52bf\u3002\u9700\u8981\u89e3\u51b3\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u957f\u5c3e\u573a\u666f\u5904\u7406\u80fd\u529b\u8fc1\u79fb\u95ee\u9898", "method": "\u57fa\u4e8eVLM\u7684\u7aef\u5230\u7aef\u5bf9\u6297\u8fc1\u79fb\u6846\u67b6\uff0c\u5305\u542b\u6559\u5e08VLM\u3001\u5b66\u751fVLM\u548c\u5224\u522b\u5668\u3002\u4f7f\u7528\u5171\u4eab\u7684CoC VLM\u67b6\u6784\u96c6\u6210\u65f6\u5e8f\u4fe1\u606f\uff0c\u652f\u6301\u94fe\u5f0f\u56e0\u679c\u63a8\u7406", "result": "\u6846\u67b6\u80fd\u591f\u5c06\u6a21\u62df\u73af\u5883\u4e2d\u7684\u957f\u5c3e\u5904\u7406\u80fd\u529b\u6709\u6548\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d", "conclusion": "CoC-VLA\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6a21\u62df\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6574\u5408\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u957f\u5c3e\u573a\u666f\u4e0b\u7684\u5904\u7406\u80fd\u529b"}}
{"id": "2511.20072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20072", "abs": "https://arxiv.org/abs/2511.20072", "authors": ["Xiaopeng Li", "Yuanjin Zheng", "Wanyu Wang", "wenlin zhang", "Pengyue Jia", "Yiqi Wang", "Maolin Wang", "Xuetao Wei", "Xiangyu Zhao"], "title": "MTA: A Merge-then-Adapt Framework for Personalized Large Language Model", "comment": null, "summary": "Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86MTA\u6846\u67b6\u89e3\u51b3\u4e2a\u6027\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u50a8\u6210\u672c\u9ad8\u548c\u7a00\u758f\u6570\u636e\u6027\u80fd\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5143LoRA\u94f6\u884c\u3001\u81ea\u9002\u5e94LoRA\u878d\u5408\u548cLoRA\u5806\u53e0\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u7528\u6237\u5fae\u8c03\u5355\u72ec\u6a21\u5757\u5bfc\u81f4\u5b58\u50a8\u6210\u672c\u7ebf\u6027\u589e\u957f\u4e14\u5bf9\u7a00\u758f\u6570\u636e\u7528\u6237\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u65b9\u6848\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1)\u6784\u5efa\u5171\u4eab\u5143LoRA\u94f6\u884c\u9884\u8bad\u7ec3\u5143\u4e2a\u6027\u5316\u7279\u5f81\uff1b2)\u81ea\u9002\u5e94LoRA\u878d\u5408\u52a8\u6001\u5408\u5e76\u76f8\u5173\u951a\u70b9\u5143LoRA\uff1b3)LoRA\u5806\u53e0\u5728\u5408\u5e76LoRA\u4e0a\u6dfb\u52a0\u8d85\u4f4e\u79e9\u6a21\u5757\u8fdb\u884c\u5c11\u6837\u672c\u5fae\u8c03\u3002", "result": "\u5728LaMP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MTA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86PLLMs\u7684\u5b58\u50a8\u6269\u5c55\u6027\u548c\u7a00\u758f\u6570\u636e\u6027\u80fd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u3002"}}
{"id": "2511.19932", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19932", "abs": "https://arxiv.org/abs/2511.19932", "authors": ["Lidi Zhang", "Han Wu", "Liyu Zhang", "Ruofeng Liu", "Haotian Wang", "Chao Li", "Desheng Zhang", "Yunhuai Liu", "Tian He"], "title": "Collaborate sim and real: Robot Bin Packing Learning in Real-world and Physical Engine", "comment": null, "summary": "The 3D bin packing problem, with its diverse industrial applications, has garnered significant research attention in recent years. Existing approaches typically model it as a discrete and static process, while real-world applications involve continuous gravity-driven interactions. This idealized simplification leads to infeasible deployments (e.g., unstable packing) in practice. Simulations with physical engine offer an opportunity to emulate continuous gravity effects, enabling the training of reinforcement learning (RL) agents to address such limitations and improve packing stability. However, a simulation-to-reality gap persists due to dynamic variations in physical properties of real-world objects, such as various friction coefficients, elasticity, and non-uniform weight distributions. To bridge this gap, we propose a hybrid RL framework that collaborates with physical simulation with real-world data feedback. Firstly, domain randomization is applied during simulation to expose agents to a spectrum of physical parameters, enhancing their generalization capability. Secondly, the RL agent is fine-tuned with real-world deployment feedback, further reducing collapse rates. Extensive experiments demonstrate that our method achieves lower collapse rates in both simulated and real-world scenarios. Large-scale deployments in logistics systems validate the practical effectiveness, with a 35\\% reduction in packing collapse compared to baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u6a21\u62df\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u53cd\u9988\u76f8\u7ed3\u5408\uff0c\u89e3\u51b33D\u88c5\u7bb1\u95ee\u9898\u4e2d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5305\u88c5\u574d\u584c\u7387\u3002", "motivation": "\u73b0\u6709\u76843D\u88c5\u7bb1\u65b9\u6cd5\u901a\u5e38\u5c06\u5176\u5efa\u6a21\u4e3a\u79bb\u6563\u9759\u6001\u8fc7\u7a0b\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u6d89\u53ca\u8fde\u7eed\u91cd\u529b\u9a71\u52a8\u4ea4\u4e92\uff0c\u8fd9\u79cd\u7406\u60f3\u5316\u7b80\u5316\u5bfc\u81f4\u5b9e\u8df5\u4e2d\u51fa\u73b0\u4e0d\u53ef\u884c\u90e8\u7f72\uff08\u5982\u4e0d\u7a33\u5b9a\u5305\u88c5\uff09\u3002\u7269\u7406\u6a21\u62df\u867d\u80fd\u6a21\u62df\u91cd\u529b\u6548\u5e94\uff0c\u4f46\u4ecd\u5b58\u5728\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u5728\u6a21\u62df\u4e2d\u5e94\u7528\u9886\u57df\u968f\u673a\u5316\uff0c\u8ba9\u667a\u80fd\u4f53\u63a5\u89e6\u5404\u79cd\u7269\u7406\u53c2\u6570\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b2\uff09\u5229\u7528\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u53cd\u9988\u5bf9RL\u667a\u80fd\u4f53\u8fdb\u884c\u5fae\u8c03\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u574d\u584c\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u90fd\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u574d\u584c\u7387\u3002\u5728\u7269\u6d41\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5b9e\u9645\u6548\u679c\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u5305\u88c5\u574d\u584c\u51cf\u5c11\u4e8635%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408RL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863D\u88c5\u7bb1\u95ee\u9898\u4e2d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u5dee\u8ddd\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u53cd\u9988\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5305\u88c5\u7a33\u5b9a\u6027\uff0c\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2511.20086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20086", "abs": "https://arxiv.org/abs/2511.20086", "authors": ["Duc Anh Vu", "Thong Nguyen", "Cong-Duy Nguyen", "Viet Anh Nguyen", "Anh Tuan Luu"], "title": "More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering", "comment": "Accepted at the 41st ACM/SIGAPP Symposium On Applied Computing (SAC 2026), Main Conference", "summary": "With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.", "AI": {"tldr": "BiasPrompting\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfcLLMs\u751f\u6210\u5e76\u6279\u5224\u6027\u8bc4\u4f30\u6240\u6709\u53ef\u80fd\u7b54\u6848\u9009\u9879\u7684\u63a8\u7406\uff0c\u6765\u63d0\u5347\u591a\u9879\u9009\u62e9\u9898\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u9879\u9009\u62e9\u9898\u4efb\u52a1\u4e2d\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u7b54\u6848\u9009\u9879\u901a\u5e38\u6ca1\u6709\u4e0a\u4e0b\u6587\u57fa\u7840\u6216\u89e3\u91ca\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5b8c\u6574\u63a2\u7d22\u6240\u6709\u53ef\u80fd\u7b54\u6848\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "BiasPrompting\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u63a8\u7406\u751f\u6210\u9636\u6bb5\uff0c\u6a21\u578b\u4e3a\u6bcf\u4e2a\u7b54\u6848\u9009\u9879\u751f\u6210\u652f\u6301\u6027\u63a8\u7406\uff1b2) \u63a8\u7406\u5f15\u5bfc\u4e00\u81f4\u6027\u9636\u6bb5\uff0c\u7efc\u5408\u751f\u6210\u7684\u63a8\u7406\u6765\u9009\u62e9\u6700\u5408\u7406\u7684\u7b54\u6848\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBiasPrompting\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u7684\u590d\u6742\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u8bbe\u7f6e\u4e2d\u3002", "conclusion": "BiasPrompting\u589e\u5f3a\u4e86LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3a\u5904\u7406\u590d\u6742\u6311\u6218\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2511.19955", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19955", "abs": "https://arxiv.org/abs/2511.19955", "authors": ["Jinxuan Zhu", "Zihao Yan", "Yangyu Xiao", "Jingxiang Guo", "Chenrui Tie", "Xinyi Cao", "Yuhang Zheng", "Lin Shao"], "title": "ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation", "comment": null, "summary": "Contact feedback is essential for contact-rich robotic manipulation, as it allows the robot to detect subtle interaction changes and adjust its actions accordingly. Six-axis force-torque sensors are commonly used to obtain contact feedback, but their high cost and fragility have discouraged many researchers from adopting them in contact-rich tasks. To offer a more cost-efficient and easy-accessible source of contact feedback, we present ShapeForce, a low-cost, plug-and-play soft wrist that provides force-like signals for contact-rich robotic manipulation. Inspired by how humans rely on relative force changes in contact rather than precise force magnitudes, ShapeForce converts external force and torque into measurable deformations of its compliant core, which are then estimated via marker-based pose tracking and converted into force-like signals. Our design eliminates the need for calibration or specialized electronics to obtain exact values, and instead focuses on capturing force and torque changes sufficient for enabling contact-rich manipulation. Extensive experiments across diverse contact-rich tasks and manipulation policies demonstrate that ShapeForce delivers performance comparable to six-axis force-torque sensors at an extremely low cost.", "AI": {"tldr": "ShapeForce\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u5373\u63d2\u5373\u7528\u7684\u8f6f\u624b\u8155\uff0c\u901a\u8fc7\u5c06\u5916\u529b\u626d\u77e9\u8f6c\u6362\u4e3a\u53ef\u6d4b\u91cf\u7684\u5f62\u53d8\u6765\u63d0\u4f9b\u7c7b\u4f3c\u529b\u7684\u4fe1\u53f7\uff0c\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u6027\u80fd\u53ef\u4e0e\u516d\u7ef4\u529b\u626d\u77e9\u4f20\u611f\u5668\u76f8\u5ab2\u7f8e\u4f46\u6210\u672c\u6781\u4f4e\u3002", "motivation": "\u516d\u7ef4\u529b\u626d\u77e9\u4f20\u611f\u5668\u867d\u7136\u5e38\u7528\u4e8e\u83b7\u53d6\u63a5\u89e6\u53cd\u9988\uff0c\u4f46\u6210\u672c\u9ad8\u4e14\u6613\u788e\uff0c\u9650\u5236\u4e86\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u63d0\u4f9b\u66f4\u7ecf\u6d4e\u6613\u7528\u7684\u63a5\u89e6\u53cd\u9988\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u8f6f\u624b\u8155\uff0c\u5c06\u5916\u529b\u626d\u77e9\u8f6c\u6362\u4e3a\u67d4\u987a\u6838\u5fc3\u7684\u5f62\u53d8\uff0c\u901a\u8fc7\u57fa\u4e8e\u6807\u8bb0\u7684\u59ff\u6001\u8ddf\u8e2a\u4f30\u8ba1\u5f62\u53d8\uff0c\u8f6c\u6362\u4e3a\u7c7b\u4f3c\u529b\u7684\u4fe1\u53f7\u3002\u65e0\u9700\u6821\u51c6\u6216\u4e13\u7528\u7535\u5b50\u8bbe\u5907\u5373\u53ef\u83b7\u5f97\u8db3\u591f\u7528\u4e8e\u63a5\u89e6\u64cd\u4f5c\u7684\u53d8\u5316\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u79cd\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u548c\u64cd\u4f5c\u7b56\u7565\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cShapeForce\u80fd\u4ee5\u6781\u4f4e\u6210\u672c\u63d0\u4f9b\u4e0e\u516d\u7ef4\u529b\u626d\u77e9\u4f20\u611f\u5668\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "ShapeForce\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u83b7\u53d6\u7684\u63a5\u89e6\u53cd\u9988\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u7cbe\u786e\u529b\u503c\u800c\u4e13\u6ce8\u4e8e\u6355\u6349\u8db3\u591f\u7684\u53d8\u5316\u4fe1\u53f7\u3002"}}
{"id": "2511.20102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20102", "abs": "https://arxiv.org/abs/2511.20102", "authors": ["Zhenyi Shen", "Junru Lu", "Lin Gui", "Jiazheng Li", "Yulan He", "Di Yin", "Xing Sun"], "title": "SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space", "comment": "28 pages", "summary": "The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.", "AI": {"tldr": "SSA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7a00\u758f\u6ce8\u610f\u529b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u5bf9\u9f50\u7a00\u758f\u548c\u5168\u6ce8\u610f\u529b\uff0c\u5728\u4fdd\u6301\u68af\u5ea6\u6d41\u52a8\u7684\u540c\u65f6\u4fc3\u8fdb\u66f4\u5f3a\u7684\u7a00\u758f\u6027\uff0c\u5728\u7a00\u758f\u548c\u5168\u6ce8\u610f\u529b\u63a8\u7406\u4e0b\u90fd\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5b58\u5728\u68af\u5ea6\u66f4\u65b0\u7f3a\u9677\uff1a\u88ab\u6392\u9664\u7684\u4f4e\u79e9\u952e\u503c\u5bf9\u65e0\u6cd5\u83b7\u5f97\u68af\u5ea6\u66f4\u65b0\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5b66\u4e60\u9002\u5f53\u7684\u6291\u5236\uff0c\u4ea7\u751f\u7a00\u758f\u6027\u4f4e\u4e8e\u5168\u6ce8\u610f\u529b\u7684\u6096\u8bba\u3002", "method": "\u63d0\u51faSSA\u6846\u67b6\uff0c\u540c\u65f6\u8003\u8651\u7a00\u758f\u548c\u5168\u6ce8\u610f\u529b\uff0c\u5728\u6bcf\u4e00\u5c42\u5f3a\u5236\u6267\u884c\u53cc\u5411\u5bf9\u9f50\uff0c\u4fdd\u6301\u6240\u6709token\u7684\u68af\u5ea6\u6d41\u52a8\uff0c\u5e76\u663e\u5f0f\u9f13\u52b1\u7a00\u758f\u6ce8\u610f\u529b\u8f93\u51fa\u4e0e\u5176\u5168\u6ce8\u610f\u529b\u5bf9\u5e94\u7269\u5bf9\u9f50\u3002", "result": "SSA\u5728\u591a\u4e2a\u5e38\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u7a00\u758f\u548c\u5168\u6ce8\u610f\u529b\u63a8\u7406\u4e0b\u90fd\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u80fd\u591f\u5e73\u6ed1\u9002\u5e94\u4e0d\u540c\u7684\u7a00\u758f\u9884\u7b97\uff1b\u6027\u80fd\u968f\u7740\u53ef\u5173\u6ce8token\u6570\u91cf\u589e\u52a0\u800c\u6301\u7eed\u63d0\u5347\uff1b\u5c55\u73b0\u51fa\u6700\u5f3a\u7684\u957f\u4e0a\u4e0b\u6587\u5916\u63a8\u80fd\u529b\u3002", "conclusion": "SSA\u901a\u8fc7\u89e3\u51b3\u68af\u5ea6\u66f4\u65b0\u7f3a\u9677\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7a00\u758f\u6ce8\u610f\u529b\u8bad\u7ec3\uff0c\u652f\u6301\u7075\u6d3b\u7684\u8ba1\u7b97-\u6027\u80fd\u6743\u8861\uff0c\u5e76\u6539\u5584\u4e86\u957f\u4e0a\u4e0b\u6587\u5916\u63a8\u80fd\u529b\u3002"}}
{"id": "2511.20050", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20050", "abs": "https://arxiv.org/abs/2511.20050", "authors": ["Yan Li", "Yingzhao Li", "Gim Hee Lee"], "title": "Active3D: Active High-Fidelity 3D Reconstruction via Hierarchical Uncertainty Quantification", "comment": null, "summary": "In this paper, we present an active exploration framework for high-fidelity 3D reconstruction that incrementally builds a multi-level uncertainty space and selects next-best-views through an uncertainty-driven motion planner. We introduce a hybrid implicit-explicit representation that fuses neural fields with Gaussian primitives to jointly capture global structural priors and locally observed details. Based on this hybrid state, we derive a hierarchical uncertainty volume that quantifies both implicit global structure quality and explicit local surface confidence. To focus optimization on the most informative regions, we propose an uncertainty-driven keyframe selection strategy that anchors high-entropy viewpoints as sparse attention nodes, coupled with a viewpoint-space sliding window for uncertainty-aware local refinement. The planning module formulates next-best-view selection as an Expected Hybrid Information Gain problem and incorporates a risk-sensitive path planner to ensure efficient and safe exploration. Extensive experiments on challenging benchmarks demonstrate that our approach consistently achieves state-of-the-art accuracy, completeness, and rendering quality, highlighting its effectiveness for real-world active reconstruction and robotic perception tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u4e0d\u786e\u5b9a\u6027\u7a7a\u95f4\u548c\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u5b9e\u73b0\u9ad8\u4fdd\u771f3D\u91cd\u5efa\uff0c\u878d\u5408\u795e\u7ecf\u573a\u4e0e\u9ad8\u65af\u57fa\u5143\u7684\u6df7\u5408\u8868\u793a\uff0c\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u91cd\u5efa\u65b9\u6cd5\u5728\u4e3b\u52a8\u63a2\u7d22\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6355\u6349\u5168\u5c40\u7ed3\u6784\u5148\u9a8c\u548c\u5c40\u90e8\u89c2\u6d4b\u7ec6\u8282\uff0c\u5e76\u80fd\u667a\u80fd\u9009\u62e9\u6700\u4f73\u89c2\u6d4b\u89c6\u89d2\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u6df7\u5408\u9690\u5f0f-\u663e\u5f0f\u8868\u793a\u878d\u5408\u795e\u7ecf\u573a\u4e0e\u9ad8\u65af\u57fa\u5143\uff0c\u6784\u5efa\u5206\u5c42\u4e0d\u786e\u5b9a\u6027\u4f53\u79ef\u91cf\u5316\u5168\u5c40\u7ed3\u6784\u8d28\u91cf\u548c\u5c40\u90e8\u8868\u9762\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u5173\u952e\u5e27\u9009\u62e9\u548c\u98ce\u9669\u654f\u611f\u8def\u5f84\u89c4\u5212\u5668\u3002", "result": "\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3001\u5b8c\u6574\u6027\u548c\u6e32\u67d3\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u4e3b\u52a8\u91cd\u5efa\u548c\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u89c6\u89d2\u9009\u62e9\u548c\u6df7\u5408\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20106", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20106", "abs": "https://arxiv.org/abs/2511.20106", "authors": ["Xingfeng Li", "Xiaohan Shi", "Junjie Li", "Yongwei Li", "Masashi Unoki", "Tomoki Toda", "Masato Akagi"], "title": "EM2LDL: A Multilingual Speech Corpus for Mixed Emotion Recognition through Label Distribution Learning", "comment": "Submitted to IEEE Transactions on Affective computing", "summary": "This study introduces EM2LDL, a novel multilingual speech corpus designed to advance mixed emotion recognition through label distribution learning. Addressing the limitations of predominantly monolingual and single-label emotion corpora \\textcolor{black}{that restrict linguistic diversity, are unable to model mixed emotions, and lack ecological validity}, EM2LDL comprises expressive utterances in English, Mandarin, and Cantonese, capturing the intra-utterance code-switching prevalent in multilingual regions like Hong Kong and Macao. The corpus integrates spontaneous emotional expressions from online platforms, annotated with fine-grained emotion distributions across 32 categories. Experimental baselines using self-supervised learning models demonstrate robust performance in speaker-independent gender-, age-, and personality-based evaluations, with HuBERT-large-EN achieving optimal results. By incorporating linguistic diversity and ecological validity, EM2LDL enables the exploration of complex emotional dynamics in multilingual settings. This work provides a versatile testbed for developing adaptive, empathetic systems for applications in affective computing, including mental health monitoring and cross-cultural communication. The dataset, annotations, and baseline codes are publicly available at https://github.com/xingfengli/EM2LDL.", "AI": {"tldr": "EM2LDL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\u63a8\u8fdb\u6df7\u5408\u60c5\u611f\u8bc6\u522b\uff0c\u5305\u542b\u82f1\u8bed\u3001\u666e\u901a\u8bdd\u548c\u7ca4\u8bed\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u652f\u630132\u4e2a\u60c5\u611f\u7c7b\u522b\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u6599\u5e93\u4e3b\u8981\u9650\u4e8e\u5355\u8bed\u8a00\u548c\u5355\u6807\u7b7e\u60c5\u611f\u8bc6\u522b\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u8bed\u6599\u5e93\u9650\u5236\u4e86\u8bed\u8a00\u591a\u6837\u6027\u3001\u65e0\u6cd5\u5efa\u6a21\u6df7\u5408\u60c5\u611f\u4e14\u7f3a\u4e4f\u751f\u6001\u6548\u5ea6\u3002", "method": "\u6784\u5efa\u5305\u542b\u82f1\u8bed\u3001\u666e\u901a\u8bdd\u548c\u7ca4\u8bed\u8868\u8fbe\u6027\u8bdd\u8bed\u7684\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff0c\u6355\u6349\u8bed\u5185\u8bed\u7801\u8f6c\u6362\u73b0\u8c61\uff0c\u6574\u5408\u6765\u81ea\u5728\u7ebf\u5e73\u53f0\u7684\u81ea\u53d1\u60c5\u611f\u8868\u8fbe\uff0c\u5e76\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u57fa\u7ebf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u57fa\u7ebf\u663e\u793a\uff0c\u5728\u8bf4\u8bdd\u8005\u72ec\u7acb\u3001\u6027\u522b\u3001\u5e74\u9f84\u548c\u4eba\u683c\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\uff0cHuBERT-large-EN\u6a21\u578b\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "EM2LDL\u901a\u8fc7\u6574\u5408\u8bed\u8a00\u591a\u6837\u6027\u548c\u751f\u6001\u6548\u5ea6\uff0c\u4e3a\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u590d\u6742\u60c5\u611f\u52a8\u6001\u7684\u63a2\u7d22\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u4e3a\u5f00\u53d1\u9002\u5e94\u6027\u5f3a\u7684\u5171\u60c5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2511.20180", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20180", "abs": "https://arxiv.org/abs/2511.20180", "authors": ["Ryohei Kobayashi", "Kosei Isomoto", "Kosei Yamao", "Soma Fumoto", "Koshun Arimura", "Naoki Yamaguchi", "Akinobu Mizutani", "Tomoya Shiba", "Kouki Kimizuka", "Yuta Ohno", "Ryo Terashima", "Hiromasa Yamaguchi", "Tomoaki Fujino", "Ryoga Maruno", "Wataru Yoshimura", "Kazuhito Mine", "Tang Phu Thien Nhan", "Yuga Yano", "Yuichiro Tanaka", "Takeshi Nishida", "Takashi Morie", "Hakaru Tamukoh"], "title": "Hibikino-Musashi@Home 2025 Team Description Paper", "comment": null, "summary": "This paper provides an overview of the techniques employed by Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team developed a dataset generator for training a robot vision system and an open-source development environment running on a Human Support Robot simulator. The large-language-model-powered task planner selects appropriate primitive skills to perform the task requested by the user. Moreover, the team has focused on research involving brain-inspired memory models for adaptation to individual home environments. This approach aims to provide intuitive and personalized assistance. Additionally, the team contributed to the reusability of the navigation system developed by Pumas in RoboCup2024. The team aimed to design a home service robot to assist humans in their homes and continuously attend competitions to evaluate and improve the developed system.", "AI": {"tldr": "Hibikino-Musashi@Home\u56e2\u961f\u5f00\u53d1\u4e86\u7528\u4e8e\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u8bad\u7ec3\u7684\u6570\u636e\u96c6\u751f\u6210\u5668\u3001\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u89c4\u5212\u5668\u3001\u8111\u542f\u53d1\u8bb0\u5fc6\u6a21\u578b\u548c\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e8\u5728\u4e3a\u5bb6\u5ead\u73af\u5883\u63d0\u4f9b\u4e2a\u6027\u5316\u8f85\u52a9\u670d\u52a1\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u4e3a\u4eba\u7c7b\u63d0\u4f9b\u8f85\u52a9\u670d\u52a1\u7684\u5bb6\u7528\u670d\u52a1\u673a\u5668\u4eba\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u53c2\u52a0\u6bd4\u8d5b\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u6570\u636e\u96c6\u751f\u6210\u5668\u8bad\u7ec3\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\uff1b\u521b\u5efa\u5f00\u6e90\u5f00\u53d1\u73af\u5883\u8fd0\u884c\u5728HSR\u6a21\u62df\u5668\u4e0a\uff1b\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4efb\u52a1\u89c4\u5212\u5668\u9009\u62e9\u539f\u59cb\u6280\u80fd\uff1b\u7814\u7a76\u8111\u542f\u53d1\u8bb0\u5fc6\u6a21\u578b\u4ee5\u9002\u5e94\u4e2a\u4f53\u5bb6\u5ead\u73af\u5883\uff1b\u91cd\u7528Pumas\u5728RoboCup2024\u5f00\u53d1\u7684\u5bfc\u822a\u7cfb\u7edf\u3002", "result": "\u6784\u5efa\u4e86\u5b8c\u6574\u7684\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5305\u62ec\u89c6\u89c9\u8bad\u7ec3\u3001\u4efb\u52a1\u89c4\u5212\u3001\u73af\u5883\u9002\u5e94\u548c\u5bfc\u822a\u7b49\u6838\u5fc3\u529f\u80fd\u6a21\u5757\u3002", "conclusion": "\u8be5\u56e2\u961f\u901a\u8fc7\u591a\u6280\u672f\u878d\u5408\u7684\u65b9\u6cd5\u5f00\u53d1\u4e86\u9762\u5411\u5bb6\u5ead\u73af\u5883\u7684\u670d\u52a1\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u7ade\u8d5b\u6301\u7eed\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u5b9e\u73b0\u4e2a\u6027\u5316\u5bb6\u5ead\u8f85\u52a9\u670d\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.20107", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.20107", "abs": "https://arxiv.org/abs/2511.20107", "authors": ["Huu Tuong Tu", "Ha Viet Khanh", "Tran Tien Dat", "Vu Huan", "Thien Van Luong", "Nguyen Tien Cuong", "Nguyen Thi Thu Trang"], "title": "Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach", "comment": null, "summary": "Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u53d1\u97f3\u9519\u8bef\u68c0\u6d4b\u4e0e\u8bca\u65ad\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3ASR\u6a21\u578b\u548c\u68c0\u7d22\u6280\u672f\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u97f3\u7d20\u7ea7\u6a21\u578b\u6216\u8bc4\u5206\u6a21\u578b\u7684\u590d\u6742\u6027\u3002", "motivation": "\u4f20\u7edf\u53d1\u97f3\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u97f3\u7d20\u7ea7\u6a21\u578b\u6216\u8bc4\u5206\u6a21\u578b\uff0c\u8fc7\u7a0b\u590d\u6742\u4e14\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u66f4\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u7ed3\u5408\u68c0\u7d22\u6280\u672f\u6784\u5efa\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u65e0\u9700\u97f3\u7d20\u7279\u5b9a\u5efa\u6a21\u6216\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728L2-ARCTIC\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u523069.60%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e14\u907f\u514d\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5229\u7528\u9884\u8bad\u7ec3ASR\u6a21\u578b\u548c\u68c0\u7d22\u6280\u672f\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u53d1\u97f3\u9519\u8bef\u68c0\u6d4b\u4e0e\u8bca\u65ad\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u8fc7\u7a0b\u3002"}}
{"id": "2511.20226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20226", "abs": "https://arxiv.org/abs/2511.20226", "authors": ["Yu Sun", "Yaosheng Deng", "Wenjie Mei", "Xiaogang Xiong", "Yang Bai", "Masaki Ogura", "Zeyu Zhou", "Mir Feroskhan", "Michael Yu Wang", "Qiyang Zuo", "Yao Li", "Yunjiang Lou"], "title": "Toward generic control for soft robotic systems", "comment": null, "summary": "Soft robotics has advanced rapidly, yet its control methods remain fragmented: different morphologies and actuation schemes still require task-specific controllers, hindering theoretical integration and large-scale deployment. A generic control framework is therefore essential, and a key obstacle lies in the persistent use of rigid-body control logic, which relies on precise models and strict low-level execution. Such a paradigm is effective for rigid robots but fails for soft robots, where the ability to tolerate and exploit approximate action representations, i.e., control compliance, is the basis of robustness and adaptability rather than a disturbance to be eliminated. Control should thus shift from suppressing compliance to explicitly exploiting it. Human motor control exemplifies this principle: instead of computing exact dynamics or issuing detailed muscle-level commands, it expresses intention through high-level movement tendencies, while reflexes and biomechanical mechanisms autonomously resolve local details. This architecture enables robustness, flexibility, and cross-task generalization. Motivated by this insight, we propose a generic soft-robot control framework grounded in control compliance and validate it across robots with diverse morphologies and actuation mechanisms. The results demonstrate stable, safe, and cross-platform transferable behavior, indicating that embracing control compliance, rather than resisting it, may provide a widely applicable foundation for unified soft-robot control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u67d4\u987a\u6027\u7684\u901a\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u800c\u975e\u6291\u5236\u8fd1\u4f3c\u52a8\u4f5c\u8868\u793a\uff0c\u5b9e\u73b0\u8de8\u5f62\u6001\u548c\u9a71\u52a8\u673a\u5236\u7684\u7a33\u5b9a\u3001\u5b89\u5168\u63a7\u5236\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u788e\u7247\u5316\uff0c\u4e0d\u540c\u5f62\u6001\u548c\u9a71\u52a8\u65b9\u6848\u9700\u8981\u7279\u5b9a\u63a7\u5236\u5668\u3002\u521a\u6027\u63a7\u5236\u903b\u8f91\u4f9d\u8d56\u7cbe\u786e\u6a21\u578b\u548c\u4e25\u683c\u6267\u884c\uff0c\u4e0d\u9002\u5408\u8f6f\u4f53\u673a\u5668\u4eba\u3002\u53d7\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u542f\u53d1\uff0c\u9700\u8981\u4ece\u6291\u5236\u67d4\u987a\u6027\u8f6c\u5411\u663e\u5f0f\u5229\u7528\u67d4\u987a\u6027\u3002", "method": "\u57fa\u4e8e\u63a7\u5236\u67d4\u987a\u6027\u7684\u901a\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u5c42\u8fd0\u52a8\u503e\u5411\u8868\u8fbe\u610f\u56fe\uff0c\u53cd\u5c04\u548c\u751f\u7269\u529b\u5b66\u673a\u5236\u81ea\u4e3b\u89e3\u51b3\u5c40\u90e8\u7ec6\u8282\u3002", "result": "\u5728\u591a\u79cd\u5f62\u6001\u548c\u9a71\u52a8\u673a\u5236\u7684\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u7a33\u5b9a\u3001\u5b89\u5168\u4e14\u8de8\u5e73\u53f0\u53ef\u8f6c\u79fb\u7684\u884c\u4e3a\u3002", "conclusion": "\u63a5\u53d7\u800c\u975e\u62b5\u6297\u63a7\u5236\u67d4\u987a\u6027\uff0c\u53ef\u80fd\u4e3a\u7edf\u4e00\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u5e7f\u6cdb\u5e94\u7528\u57fa\u7840\u3002"}}
{"id": "2511.20120", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20120", "abs": "https://arxiv.org/abs/2511.20120", "authors": ["Somsubhra De", "Harsh Kumar", "Arun Prakash A"], "title": "\"When Data is Scarce, Prompt Smarter\"... Approaches to Grammatical Error Correction in Low-Resource Settings", "comment": "10 pages, 5 figures, 5 tables; Accept-demonstration at BHASHA Workshop, IJCNLP-AACL 2025", "summary": "Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4.1\u3001Gemini-2.5\u3001LLaMA-4\uff09\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00\u8bed\u6cd5\u9519\u8bef\u6821\u6b63\u95ee\u9898\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u53d6\u5f97\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u5370\u5ea6\u8bed\u8a00\u7684\u8bed\u6cd5\u9519\u8bef\u6821\u6b63\u9762\u4e34\u8d44\u6e90\u6709\u9650\u3001\u8bed\u8a00\u591a\u6837\u6027\u548c\u590d\u6742\u5f62\u6001\u5b66\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u7b56\u7565\uff0c\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u8f7b\u91cf\u7ea7\u9002\u914d\u6765\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u5171\u4eab\u4efb\u52a1\u4e2d\u53d6\u5f97\u9886\u5148\u6210\u7ee9\uff1a\u6cf0\u7c73\u5c14\u8bed\u7b2c1\u540d\uff08GLEU: 91.57\uff09\u3001\u5370\u5730\u8bed\u7b2c1\u540d\uff08GLEU: 85.69\uff09\u3001\u6cf0\u5362\u56fa\u8bed\u7b2c2\u540d\uff08GLEU: 85.22\uff09\u3001\u5b5f\u52a0\u62c9\u8bed\u7b2c4\u540d\uff08GLEU: 92.86\uff09\u3001\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bed\u7b2c5\u540d\uff08GLEU: 92.97\uff09\u3002", "conclusion": "\u63d0\u793a\u9a71\u52a8\u7684NLP\u6280\u672f\u975e\u5e38\u6709\u6548\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5353\u8d8a\u7684\u591a\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5f25\u5408\u591a\u8bed\u8a00\u8bed\u6cd5\u9519\u8bef\u6821\u6b63\u4e2d\u7684\u8d44\u6e90\u5dee\u8ddd\u3002"}}
{"id": "2511.20275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20275", "abs": "https://arxiv.org/abs/2511.20275", "authors": ["Chenhui Dong", "Haozhe Xu", "Wenhao Feng", "Zhipeng Wang", "Yanmin Zhou", "Yifei Zhao", "Bin He"], "title": "HAFO: Humanoid Force-Adaptive Control for Intense External Force Interaction Environments", "comment": null, "summary": "Reinforcement learning controllers have made impressive progress in humanoid locomotion and light load manipulation. However, achieving robust and precise motion with strong force interaction remains a significant challenge. Based on the above limitations, this paper proposes HAFO, a dual-agent reinforcement learning control framework that simultaneously optimizes both a robust locomotion strategy and a precise upper-body manipulation strategy through coupled training under external force interaction environments. Simultaneously, we explicitly model the external pulling disturbances through a spring-damper system and achieve fine-grained force control by manipulating the virtual spring. During this process, the reinforcement-learning policy spontaneously generates disturbance-rejection response by exploiting environmental feedback. Moreover, HAFO employs an asymmetric Actor-Critic framework in which the Critic-network access to privileged spring-damping forces guides the actor-network to learn a generalizable, robust policy for resisting external disturbances. The experimental results demonstrate that HAFO achieves stable control of humanoid robot under various strong force interactions, showing remarkable performance in load tasks and ensuring stable robot operation under rope tension disturbances. Project website: hafo-robot.github.io.", "AI": {"tldr": "HAFO\u662f\u4e00\u4e2a\u53cc\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u8bad\u7ec3\u540c\u65f6\u4f18\u5316\u7a33\u5065\u7684\u6b65\u6001\u7b56\u7565\u548c\u7cbe\u786e\u7684\u4e0a\u534a\u8eab\u64cd\u4f5c\u7b56\u7565\uff0c\u5728\u5f3a\u5916\u529b\u4ea4\u4e92\u73af\u5883\u4e0b\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u5728\u4eba\u5f62\u673a\u5668\u4eba\u6b65\u6001\u548c\u8f7b\u8d1f\u8f7d\u64cd\u4f5c\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5f3a\u5916\u529b\u4ea4\u4e92\u4e0b\u5b9e\u73b0\u7a33\u5065\u7cbe\u786e\u8fd0\u52a8\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u53cc\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f39\u7c27-\u963b\u5c3c\u7cfb\u7edf\u663e\u5f0f\u5efa\u6a21\u5916\u90e8\u62c9\u529b\u5e72\u6270\uff0c\u91c7\u7528\u975e\u5bf9\u79f0Actor-Critic\u67b6\u6784\uff0cCritic\u7f51\u7edc\u8bbf\u95ee\u7279\u6743\u5f39\u7c27-\u963b\u5c3c\u529b\u4fe1\u606f\u6307\u5bfcActor\u7f51\u7edc\u5b66\u4e60\u901a\u7528\u7a33\u5065\u7b56\u7565\u3002", "result": "HAFO\u5728\u5404\u79cd\u5f3a\u5916\u529b\u4ea4\u4e92\u4e0b\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7a33\u5b9a\u63a7\u5236\uff0c\u5728\u8d1f\u8f7d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u7ef3\u7d22\u62c9\u529b\u5e72\u6270\u4e0b\u786e\u4fdd\u673a\u5668\u4eba\u7a33\u5b9a\u8fd0\u884c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f3a\u5916\u529b\u4ea4\u4e92\u4e0b\u4eba\u5f62\u673a\u5668\u4eba\u7684\u63a7\u5236\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.20143", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20143", "abs": "https://arxiv.org/abs/2511.20143", "authors": ["Wen-Fang Su", "Hsiao-Wei Chou", "Wen-Yang Lin"], "title": "SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models", "comment": "9 pages, 5 figures", "summary": "Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u50cf\u6570\u636e\u589e\u5f3a\u6280\u672f\u7684\u7f51\u683c\u6807\u6ce8\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u4e0d\u8fde\u7eed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8de8\u53e5\u5b50\u4e0d\u8fde\u7eed\u5b9e\u4f53\u4e0a\u7684\u5206\u5272\u548c\u9057\u6f0f\u95ee\u9898\u3002", "motivation": "\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u4e0d\u8fde\u7eed\u5b9e\u4f53\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6587\u672c\u5206\u5272\u65f6\u5bb9\u6613\u9519\u8bef\u5206\u5272\u6216\u5b8c\u5168\u9057\u6f0f\u8de8\u53e5\u5b50\u7684\u4e0d\u8fde\u7eed\u5b9e\u4f53\uff0c\u4e25\u91cd\u5f71\u54cd\u8bc6\u522b\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8e\u7f51\u683c\u6807\u6ce8\u65b9\u6cd5\uff0c\u6574\u5408\u56fe\u50cf\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982\u88c1\u526a\u3001\u7f29\u653e\u548c\u586b\u5145\uff09\u5230\u7f51\u683c\u6a21\u578b\u4e2d\uff0c\u589e\u5f3a\u5bf9\u4e0d\u8fde\u7eed\u5b9e\u4f53\u7684\u8bc6\u522b\u80fd\u529b\u548c\u5904\u7406\u5206\u5272\u6311\u6218\u7684\u80fd\u529b\u3002", "result": "\u5728CADEC\u3001ShARe13\u548cShARe14\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4f20\u7edf\u5206\u5272\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8de8\u53e5\u5b50\u4e0d\u8fde\u7eed\u5b9e\u4f53\uff0c\u800c\u589e\u5f3a\u7f51\u683c\u6a21\u578b\u6574\u4f53F1\u5206\u6570\u63d0\u53471-2.5%\uff0c\u4e0d\u8fde\u7eed\u5b9e\u4f53\u8bc6\u522b\u63d0\u53473.7-8.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u8fde\u7eed\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u5206\u5272\u548c\u9057\u6f0f\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u56fe\u50cf\u6570\u636e\u589e\u5f3a\u6280\u672f\u5728\u7f51\u683c\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.20292", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20292", "abs": "https://arxiv.org/abs/2511.20292", "authors": ["Dong Wang", "Daniel Casado Herraez", "Stefan May", "Andreas N\u00fcchter"], "title": "Dynamic-ICP: Doppler-Aware Iterative Closest Point Registration for Dynamic Scenes", "comment": "8 pages, 5 figures", "summary": "Reliable odometry in highly dynamic environments remains challenging when it relies on ICP-based registration: ICP assumes near-static scenes and degrades in repetitive or low-texture geometry. We introduce Dynamic-ICP, a Doppler-aware registration framework. The method (i) estimates ego motion from per-point Doppler velocity via robust regression and builds a velocity filter, (ii) clusters dynamic objects and reconstructs object-wise translational velocities from ego-compensated radial measurements, (iii) predicts dynamic points with a constant-velocity model, and (iv) aligns scans using a compact objective that combines point-to-plane geometry residual with a translation-invariant, rotation-only Doppler residual. The approach requires no external sensors or sensor-vehicle calibration and operates directly on FMCW LiDAR range and Doppler velocities. We evaluate Dynamic-ICP on three datasets-HeRCULES, HeLiPR, AevaScenes-focusing on highly dynamic scenes. Dynamic-ICP consistently improves rotational stability and translation accuracy over the state-of-the-art methods. Our approach is also simple to integrate into existing pipelines, runs in real time, and provides a lightweight solution for robust registration in dynamic environments. To encourage further research, the code is available at: https://github.com/JMUWRobotics/Dynamic-ICP.", "AI": {"tldr": "Dynamic-ICP\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u666e\u52d2\u611f\u77e5\u7684\u6fc0\u5149\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u6b8b\u5dee\u548c\u591a\u666e\u52d2\u6b8b\u5dee\uff0c\u5728\u9ad8\u5ea6\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u626b\u63cf\u914d\u51c6\u3002", "motivation": "\u4f20\u7edfICP\u65b9\u6cd5\u5047\u8bbe\u573a\u666f\u9759\u6001\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u91cd\u590d\u7eb9\u7406\u6216\u4f4e\u7eb9\u7406\u51e0\u4f55\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u9760\u5de5\u4f5c\u7684\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u3002", "method": "1) \u901a\u8fc7\u7a33\u5065\u56de\u5f52\u4ece\u591a\u666e\u52d2\u901f\u5ea6\u4f30\u8ba1\u81ea\u8eab\u8fd0\u52a8\u5e76\u6784\u5efa\u901f\u5ea6\u6ee4\u6ce2\u5668\uff1b2) \u805a\u7c7b\u52a8\u6001\u7269\u4f53\u5e76\u4ece\u81ea\u8eab\u8865\u507f\u7684\u5f84\u5411\u6d4b\u91cf\u91cd\u5efa\u7269\u4f53\u5e73\u79fb\u901f\u5ea6\uff1b3) \u4f7f\u7528\u6052\u5b9a\u901f\u5ea6\u6a21\u578b\u9884\u6d4b\u52a8\u6001\u70b9\uff1b4) \u7ed3\u5408\u70b9\u5bf9\u9762\u51e0\u4f55\u6b8b\u5dee\u548c\u65cb\u8f6c\u4e0d\u53d8\u7684\u591a\u666e\u52d2\u6b8b\u5dee\u8fdb\u884c\u626b\u63cf\u914d\u51c6\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cDynamic-ICP\u5728\u65cb\u8f6c\u7a33\u5b9a\u6027\u548c\u5e73\u79fb\u7cbe\u5ea6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u8fd0\u884c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u4e2d\u3002", "conclusion": "Dynamic-ICP\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u6216\u4f20\u611f\u5668-\u8f66\u8f86\u6807\u5b9a\uff0c\u76f4\u63a5\u5728FMCW\u6fc0\u5149\u96f7\u8fbe\u8ddd\u79bb\u548c\u591a\u666e\u52d2\u901f\u5ea6\u4e0a\u64cd\u4f5c\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a33\u5065\u914d\u51c6\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.20182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20182", "abs": "https://arxiv.org/abs/2511.20182", "authors": ["Adilet Metinov", "Gulida M. Kudakeeva", "Gulnara D. Kabaeva"], "title": "KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP", "comment": "3 pages, 1 figure, 2 tables. Preprint", "summary": "Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86KyrgyzBERT\uff0c\u8fd9\u662f\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u5409\u5c14\u5409\u65af\u8bed\u5355\u8bedBERT\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u542b3590\u4e07\u53c2\u6570\u548c\u5b9a\u5236\u5206\u8bcd\u5668\uff0c\u5728\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0e5\u500d\u5927\u7684mBERT\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u5409\u5c14\u5409\u65af\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u57fa\u7840NLP\u5de5\u5177\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6765\u652f\u6301\u8be5\u8bed\u8a00\u7684NLP\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86KyrgyzBERT\u6a21\u578b\uff0c\u4f7f\u7528\u5b9a\u5236\u5206\u8bcd\u5668\u9002\u5e94\u8bed\u8a00\u5f62\u6001\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u7ffb\u8bd1\u548c\u4eba\u5de5\u6807\u6ce8\u521b\u5efa\u4e86kyrgyz-sst2\u60c5\u611f\u5206\u6790\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "KyrgyzBERT\u5728\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u8fbe\u5230F1\u5206\u65700.8280\uff0c\u4e0e\u89c4\u6a21\u59275\u500d\u7684mBERT\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "KyrgyzBERT\u4e3a\u5409\u5c14\u5409\u65af\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u6240\u6709\u6a21\u578b\u3001\u6570\u636e\u548c\u4ee3\u7801\u5747\u5df2\u516c\u5f00\u53d1\u5e03\u4ee5\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2511.20299", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.20299", "abs": "https://arxiv.org/abs/2511.20299", "authors": ["R\u00f3is\u00edn Keenan", "Joost C. Dessing"], "title": "How Robot Kinematics Influence Human Performance in Virtual Robot-to-Human Handover Tasks", "comment": null, "summary": "Recent advancements in robotics have increased the possibilities for integrating robotic systems into human-involved workplaces, highlighting the need to examine and optimize human-robot coordination in collaborative settings. This study explores human-robot interactions during handover tasks using Virtual Reality (VR) to investigate differences in human motor performance across various task dynamics and robot kinematics. A VR-based robot handover simulation afforded safe and controlled assessments of human-robot interactions. In separate experiments, four potential influences on human performance were examined (1) control over task initiation and robot movement synchrony (temporal and spatiotemporal); (2) partner appearance (human versus robotic); (3) robot velocity profiles (minimum jerk, constant velocity, constant acceleration, and biphasic); and (4) the timing of rotational object motion. Findings across experiments emphasize humans benefit from robots providing early and salient visual information about task-relevant object motion, and advantages of human-like smooth robot trajectories. To varying degrees, these manipulations improved predictive accuracy and synchronization during interaction. This suggests that human-robot interactions should be designed to allow humans to leverage their natural capabilities for detecting biological motion, which conversely may reduce the need for costly robotic computations or added cognitive adaptation on the human side.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528VR\u63a2\u7d22\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u7269\u4f53\u4ea4\u63a5\u4efb\u52a1\uff0c\u53d1\u73b0\u4eba\u7c7b\u53d7\u76ca\u4e8e\u673a\u5668\u4eba\u63d0\u4f9b\u65e9\u671f\u89c6\u89c9\u4fe1\u606f\u548c\u7c7b\u4eba\u5e73\u6ed1\u8f68\u8ff9\uff0c\u8fd9\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u540c\u6b65\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u4eba\u7c7b\u5de5\u4f5c\u573a\u6240\uff0c\u9700\u8981\u4f18\u5316\u4eba\u673a\u534f\u8c03\u534f\u4f5c\uff0c\u7279\u522b\u662f\u5728\u7269\u4f53\u4ea4\u63a5\u7b49\u4ea4\u4e92\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7VR\u673a\u5668\u4eba\u4ea4\u63a5\u6a21\u62df\uff0c\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5206\u522b\u6d4b\u8bd5\u56db\u4e2a\u5f71\u54cd\u56e0\u7d20\uff1a\u4efb\u52a1\u542f\u52a8\u63a7\u5236\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u540c\u6b65\u6027\u3001\u4f19\u4f34\u5916\u89c2\u3001\u673a\u5668\u4eba\u901f\u5ea6\u66f2\u7ebf\u3001\u7269\u4f53\u65cb\u8f6c\u8fd0\u52a8\u65f6\u673a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u673a\u5668\u4eba\u63d0\u4f9b\u65e9\u671f\u89c6\u89c9\u4fe1\u606f\u548c\u7c7b\u4eba\u5e73\u6ed1\u8f68\u8ff9\u80fd\u663e\u8457\u6539\u5584\u4eba\u7c7b\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4ea4\u4e92\u540c\u6b65\u6027\u3002", "conclusion": "\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u5e94\u8ba9\u4eba\u7c7b\u80fd\u591f\u5229\u7528\u5176\u751f\u7269\u8fd0\u52a8\u68c0\u6d4b\u7684\u81ea\u7136\u80fd\u529b\uff0c\u8fd9\u53ef\u4ee5\u51cf\u5c11\u673a\u5668\u4eba\u8ba1\u7b97\u6210\u672c\u548c\u4eba\u7c7b\u8ba4\u77e5\u9002\u5e94\u8d1f\u62c5\u3002"}}
{"id": "2511.20233", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20233", "abs": "https://arxiv.org/abs/2511.20233", "authors": ["Chuyi Kong", "Gao Wei", "Jing Ma", "Hongzhan Lin", "Zhiyuan Fan"], "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance", "comment": null, "summary": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.", "AI": {"tldr": "REFLEX\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7684\u5373\u63d2\u5373\u7528\u8303\u5f0f\uff0c\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u548c\u5bf9\u6bd4\u6fc0\u6d3b\u5bf9\u6765\u63d0\u5347\u5224\u51b3\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\uff0c\u4ec5\u9700465\u4e2a\u8bad\u7ec3\u6837\u672c\u5c31\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u6e90\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u65f6\u6027\u3002", "method": "\u5c06\u4e8b\u5b9e\u6838\u67e5\u91cd\u6784\u4e3a\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\uff0c\u8054\u5408\u8bad\u7ec3\u5224\u51b3\u9884\u6d4b\u548c\u89e3\u91ca\u751f\u6210\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6fc0\u6d3b\u5bf9\u6784\u5efa\u5f15\u5bfc\u5411\u91cf\u6765\u5206\u79bb\u771f\u76f8\u7684\u98ce\u683c\u548c\u5b9e\u8d28\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cREFLEX\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4ec5\u7528465\u4e2a\u81ea\u7cbe\u70bc\u8bad\u7ec3\u6837\u672c\u5c31\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u89e3\u91ca\u4fe1\u53f7\u80fd\u63d0\u5347\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\u8fbe7.57%\u3002", "conclusion": "\u5185\u90e8\u89e3\u91ca\u4fe1\u53f7\u5728\u4e8b\u5b9e\u6838\u67e5\u4e2d\u5177\u6709\u53cc\u91cd\u4f5c\u7528\uff1a\u65e2\u80fd\u89e3\u91ca\u53c8\u80fd\u589e\u5f3a\u4e8b\u5b9e\u63a8\u7406\uff0cREFLEX\u8303\u5f0f\u80fd\u6709\u6548\u5904\u7406\u4eba\u7c7b\u672a\u77e5\u7684\u5fae\u5999\u771f\u76f8\u3002"}}
{"id": "2511.20330", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20330", "abs": "https://arxiv.org/abs/2511.20330", "authors": ["Yuhan Wu", "Tiantian Wei", "Shuo Wang", "ZhiChao Wang", "Yanyong Zhang", "Daniel Cremers", "Yan Xia"], "title": "ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation", "comment": null, "summary": "Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86ArtiBench\u57fa\u51c6\u6d4b\u8bd5\u548cArtiBrain\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u6cdb\u5316\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u63a8\u7406\u548c\u81ea\u9002\u5e94\u5e95\u5c42\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u5728\u8de8\u90e8\u4ef6\u3001\u5b9e\u4f8b\u548c\u7c7b\u522b\u7684\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u89e3\u51b3\u957f\u65f6\u7a0b\u3001\u591a\u6b65\u9aa4\u4ea4\u4e92\u4e2d\u7684\u7269\u7406\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faArtiBrain\u6a21\u5757\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8eVLM\u7684\u4efb\u52a1\u63a8\u7406\u5668\u5206\u89e3\u548c\u9a8c\u8bc1\u5b50\u76ee\u6807\uff0c\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u5173\u952e\u5e27\u6267\u884c\u548c\u53ef\u4f9b\u6027\u5f15\u5bfc\u6269\u6563\u7684\u6df7\u5408\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7\u53ef\u4f9b\u6027\u8bb0\u5fc6\u5e93\u79ef\u7d2f\u548c\u4f20\u64ad\u6210\u529f\u6267\u884c\u7ecf\u9a8c\u3002", "result": "\u5728ArtiBench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cArtiBrain\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u3002", "conclusion": "ArtiBrain\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u9ad8\u5c42\u63a8\u7406\u548c\u81ea\u9002\u5e94\u5e95\u5c42\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u7684\u6cdb\u5316\u6311\u6218\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u5173\u8282\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20340", "abs": "https://arxiv.org/abs/2511.20340", "authors": ["Luohe Shi", "Zuchao Li", "Lefei Zhang", "Baoyuan Qi", "Guoming Liu", "Hai Zhao"], "title": "Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios", "comment": "accepted by AAAI-2026", "summary": "Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.", "AI": {"tldr": "SpecFormer\u662f\u4e00\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u7ed3\u5408\u5355\u5411\u548c\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4f4e\u9a8c\u8bc1\u8d44\u6e90\u548c\u8c03\u5ea6\u6210\u672c\u4e0b\u5b9e\u73b0LLM\u63a8\u7406\u52a0\u901f\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u578b\u524d\u7f00\u6811\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5047\u8bbe\u6709\u5927\u91cf\u53ef\u7528\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u5728\u6279\u5904\u7406\u7b49\u4e3b\u6d41\u63a8\u7406\u7cfb\u7edf\u4e2d\uff0c\u53ef\u7528\u7a7a\u95f2\u8ba1\u7b97\u80fd\u529b\u88ab\u538b\u7f29\uff0c\u56e0\u6b64\u9700\u8981\u4f4e\u8d44\u6e90\u3001\u4f4e\u6210\u672c\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\u3002", "method": "\u63d0\u51faSpecFormer\u67b6\u6784\uff0c\u96c6\u6210\u5355\u5411\u548c\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u6a21\u578b\u4ece\u5b8c\u6574\u8f93\u5165\u5e8f\u5217\u63d0\u53d6\u4fe1\u606f\u7684\u80fd\u529b\u548c\u975e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5e76\u884c\u751f\u6210\u4f18\u52bf\u3002", "result": "\u5728\u5404\u79cd\u89c4\u6a21\u6a21\u578b\u7684\u5b9e\u9a8c\u4e2d\uff0cSpecFormer\u4e3aLLM\u63a8\u7406\u6269\u5c55\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u8bad\u7ec3\u9700\u6c42\u66f4\u4f4e\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u5c11\u3002", "conclusion": "SpecFormer\u901a\u8fc7\u65b0\u9896\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5b9e\u73b0\u4e00\u81f4\u7684\u52a0\u901f\u6548\u679c\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20353", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20353", "abs": "https://arxiv.org/abs/2511.20353", "authors": ["Benjamin Sportich", "Kenza Boubakri", "Olivier Simonin", "Alessandro Renzaglia"], "title": "Quality-guided UAV Surface Exploration for 3D Reconstruction", "comment": null, "summary": "Reasons for mapping an unknown environment with autonomous robots are wide-ranging, but in practice, they are often overlooked when developing planning strategies. Rapid information gathering and comprehensive structural assessment of buildings have different requirements and therefore necessitate distinct methodologies. In this paper, we propose a novel modular Next-Best-View (NBV) planning framework for aerial robots that explicitly uses a reconstruction quality objective to guide the exploration planning. In particular, our approach introduces new and efficient methods for view generation and selection of viewpoint candidates that are adaptive to the user-defined quality requirements, fully exploiting the uncertainty encoded in a Truncated Signed Distance field (TSDF) representation of the environment. This results in informed and efficient exploration decisions tailored towards the predetermined objective. Finally, we validate our method via extensive simulations in realistic environments. We demonstrate that it successfully adjusts its behavior to the user goal while consistently outperforming conventional NBV strategies in terms of coverage, quality of the final 3D map and path efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u6a21\u5757\u5316Next-Best-View\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u5efa\u8d28\u91cf\u76ee\u6807\u6307\u5bfc\u63a2\u7d22\u89c4\u5212\uff0c\u5728\u8986\u76d6\u8303\u56f4\u30013D\u5730\u56fe\u8d28\u91cf\u548c\u8def\u5f84\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfNBV\u7b56\u7565\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5efa\u56fe\u9700\u6c42\u5e7f\u6cdb\uff0c\u4f46\u5728\u89c4\u5212\u7b56\u7565\u5f00\u53d1\u4e2d\u5e38\u88ab\u5ffd\u89c6\u3002\u5feb\u901f\u4fe1\u606f\u6536\u96c6\u548c\u5efa\u7b51\u7269\u5168\u9762\u7ed3\u6784\u8bc4\u4f30\u6709\u4e0d\u540c\u7684\u8981\u6c42\uff0c\u9700\u8981\u4e0d\u540c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316NBV\u89c4\u5212\u6846\u67b6\uff0c\u4f7f\u7528\u91cd\u5efa\u8d28\u91cf\u76ee\u6807\u6307\u5bfc\u63a2\u7d22\u89c4\u5212\uff1b\u5f15\u5165\u65b0\u7684\u9ad8\u6548\u89c6\u56fe\u751f\u6210\u548c\u89c6\u70b9\u5019\u9009\u9009\u62e9\u65b9\u6cd5\uff0c\u9002\u5e94\u4e8e\u7528\u6237\u5b9a\u4e49\u7684\u8d28\u91cf\u8981\u6c42\uff1b\u5145\u5206\u5229\u7528TSDF\u8868\u793a\u4e2d\u7f16\u7801\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u6839\u636e\u7528\u6237\u76ee\u6807\u8c03\u6574\u884c\u4e3a\uff0c\u5728\u8986\u76d6\u8303\u56f4\u3001\u6700\u7ec83D\u5730\u56fe\u8d28\u91cf\u548c\u8def\u5f84\u6548\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u4f20\u7edfNBV\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u9884\u5b9a\u76ee\u6807\u505a\u51fa\u77e5\u60c5\u4e14\u9ad8\u6548\u7684\u63a2\u7d22\u51b3\u7b56\uff0c\u4e3a\u4e0d\u540c\u5efa\u56fe\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20344", "abs": "https://arxiv.org/abs/2511.20344", "authors": ["Taewhoo Lee", "Minju Song", "Chanwoong Yoon", "Jungwoo Park", "Jaewoo Kang"], "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models", "comment": "AAAI 2026", "summary": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.", "AI": {"tldr": "LLMs\u80fd\u591f\u7f16\u7801\u7c7b\u6bd4\u5b9e\u4f53\u95f4\u7684\u5173\u7cfb\uff0c\u4f46\u5728\u5c06\u5173\u7cfb\u4fe1\u606f\u5e94\u7528\u4e8e\u65b0\u5b9e\u4f53\u65f6\u5b58\u5728\u56f0\u96be\u3002\u901a\u8fc7\u7b56\u7565\u6027\u5730\u4fee\u8865\u9690\u85cf\u8868\u793a\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6539\u5584\u4fe1\u606f\u4f20\u9012\u3002\u6210\u529f\u7684\u7c7b\u6bd4\u63a8\u7406\u9700\u8981\u5f3a\u7ed3\u6784\u5bf9\u9f50\uff0c\u5931\u8d25\u5219\u901a\u5e38\u6e90\u4e8e\u5bf9\u9f50\u9000\u5316\u6216\u9519\u4f4d\u3002", "motivation": "\u63a2\u7d22LLMs\u662f\u5426\u80fd\u591f\u7f16\u7801\u9ad8\u5c42\u6b21\u5173\u7cfb\u6982\u5ff5\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u6bd4\u8f83\u5c06\u5176\u5e94\u7528\u4e8e\u65b0\u60c5\u5883\uff0c\u8fd9\u662f\u4eba\u7c7b\u8ba4\u77e5\u7684\u6838\u5fc3\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6bd4\u4f8b\u7c7b\u6bd4\u548c\u6545\u4e8b\u7c7b\u6bd4\u4efb\u52a1\uff0c\u5206\u6790LLMs\u5728\u4e0d\u540c\u5c42\u7ea7\u7684\u9690\u85cf\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u7b56\u7565\u6027\u4fee\u8865\u9690\u85cf\u8868\u793a\u6765\u7814\u7a76\u4fe1\u606f\u4f20\u9012\u3002", "result": "LLMs\u80fd\u591f\u6709\u6548\u7f16\u7801\u7c7b\u6bd4\u5b9e\u4f53\u95f4\u7684\u5173\u7cfb\uff0c\u5173\u7cfb\u4fe1\u606f\u5728\u4e2d\u4e0a\u5c42\u4f20\u64ad\uff1b\u4f46\u5728\u5e94\u7528\u5173\u7cfb\u4fe1\u606f\u5230\u65b0\u5b9e\u4f53\u65f6\u5b58\u5728\u56f0\u96be\uff1b\u7b56\u7565\u6027\u4fee\u8865\u9690\u85cf\u8868\u793a\u53ef\u6539\u5584\u4fe1\u606f\u4f20\u9012\uff1b\u6210\u529f\u7684\u7c7b\u6bd4\u63a8\u7406\u9700\u8981\u5f3a\u7ed3\u6784\u5bf9\u9f50\u3002", "conclusion": "LLMs\u5728\u7f16\u7801\u548c\u5e94\u7528\u9ad8\u5c42\u6b21\u5173\u7cfb\u6982\u5ff5\u65b9\u9762\u5c55\u73b0\u51fa\u521d\u6b65\u4f46\u6709\u9650\u7684\u80fd\u529b\uff0c\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u76f8\u4f3c\u6027\u548c\u5dee\u8ddd\u3002"}}
{"id": "2511.20394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20394", "abs": "https://arxiv.org/abs/2511.20394", "authors": ["Shiqian Liu", "Azlan Mohd Zain", "Le-le Mao"], "title": "Improved adaptive wind driven optimization algorithm for real-time path planning", "comment": "23 pages, 4 figures", "summary": "Recently, path planning has achieved remarkable progress in enhancing global search capability and convergence accuracy through heuristic and learning-inspired optimization frameworks. However, real-time adaptability in dynamic environments remains a critical challenge for autonomous navigation, particularly when robots must generate collision-free, smooth, and efficient trajectories under complex constraints. By analyzing the difficulties in dynamic path planning, the Wind Driven Optimization (WDO) algorithm emerges as a promising framework owing to its physically interpretable search dynamics. Motivated by these observations, this work revisits the WDO principle and proposes a variant formulation, Multi-hierarchical adaptive wind driven optimization(MAWDO), that improves adaptability and robustness in time-varying environments. To mitigate instability and premature convergence, a hierarchical-guidance mechanism divides the population into multiple groups guided by individual, regional, and global leaders to balance exploration and exploitation. Extensive evaluations on sixteen benchmark functions show that MAWDO achieves superior optimization accuracy, convergence stability, and adaptability over state-of-the art metaheuristics. In dynamic path planning, MAWDO shortens the path length to 469.28 pixels, improving over Multi-strategy ensemble wind driven optimization(MEWDO), Adaptive wind driven optimization(AWDO) and WDO by 3.51\\%, 11.63\\% and 14.93\\%, and achieves the smallest optimality gap (1.01) with smoothness 0.71 versus 13.50 and 15.67 for AWDO and WDO, leading to smoother, shorter, and collision-free trajectories that confirm its effectiveness for real-time path planning in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u7ea7\u81ea\u9002\u5e94\u98ce\u9a71\u52a8\u4f18\u5316\u7b97\u6cd5(MAWDO)\uff0c\u901a\u8fc7\u5206\u5c42\u5f15\u5bfc\u673a\u5236\u6539\u8fdb\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u8def\u5f84\u89c4\u5212\u4e2d\u5b9e\u73b0\u4e86\u66f4\u77ed\u3001\u66f4\u5e73\u6ed1\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u9002\u5e94\u6027\u662f\u81ea\u4e3b\u5bfc\u822a\u7684\u5173\u952e\u6311\u6218\uff0c\u98ce\u9a71\u52a8\u4f18\u5316\u7b97\u6cd5\u56e0\u5176\u7269\u7406\u53ef\u89e3\u91ca\u7684\u641c\u7d22\u52a8\u6001\u800c\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u4e0d\u7a33\u5b9a\u548c\u65e9\u719f\u6536\u655b\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u98ce\u9a71\u52a8\u4f18\u5316\u539f\u7406\uff0c\u63d0\u51fa\u591a\u5c42\u7ea7\u81ea\u9002\u5e94\u53d8\u4f53MAWDO\uff0c\u91c7\u7528\u5206\u5c42\u5f15\u5bfc\u673a\u5236\u5c06\u79cd\u7fa4\u5212\u5206\u4e3a\u591a\u4e2a\u7ec4\uff0c\u7531\u4e2a\u4f53\u3001\u533a\u57df\u548c\u5168\u5c40\u9886\u5bfc\u8005\u5f15\u5bfc\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u572816\u4e2a\u57fa\u51c6\u51fd\u6570\u4e0aMAWDO\u83b7\u5f97\u4f18\u8d8a\u7684\u4f18\u5316\u7cbe\u5ea6\u3001\u6536\u655b\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff1b\u5728\u52a8\u6001\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u8def\u5f84\u957f\u5ea6\u7f29\u77ed\u81f3469.28\u50cf\u7d20\uff0c\u6bd4MEWDO\u3001AWDO\u548cWDO\u5206\u522b\u63d0\u53473.51%\u300111.63%\u548c14.93%\uff0c\u6700\u4f18\u6027\u5dee\u8ddd\u6700\u5c0f(1.01)\uff0c\u5e73\u6ed1\u5ea6\u4e3a0.71\u3002", "conclusion": "MAWDO\u80fd\u591f\u751f\u6210\u66f4\u5e73\u6ed1\u3001\u66f4\u77ed\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.20399", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20399", "abs": "https://arxiv.org/abs/2511.20399", "authors": ["Abdullah Al Sefat"], "title": "BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali", "comment": null, "summary": "Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.", "AI": {"tldr": "BengaliFig\u662f\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7684\u6311\u6218\u6570\u636e\u96c6\uff0c\u5305\u542b435\u4e2a\u6765\u81ea\u5b5f\u52a0\u62c9\u53e3\u5934\u548c\u6587\u5b66\u4f20\u7edf\u7684\u8c1c\u8bed\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4f4e\u8d44\u6e90\u6587\u5316\u80cc\u666f\u4e0b\u7684\u6bd4\u55bb\u548c\u6587\u5316\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6bd4\u55bb\u548c\u6587\u5316\u63a8\u7406\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u5883\u4e0b\u7684\u8bc4\u4f30\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u5305\u542b435\u4e2a\u72ec\u7279\u8c1c\u8bed\u7684\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u9879\u76ee\u6cbf\u4e94\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\u8fdb\u884c\u6ce8\u91ca\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u611f\u77e5\u7684AI\u8f85\u52a9\u7ba1\u9053\u81ea\u52a8\u8f6c\u6362\u4e3a\u591a\u9879\u9009\u62e9\u683c\u5f0f\u3002", "result": "\u8bc4\u4f30\u516b\u4e2a\u524d\u6cbfLLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5728\u9690\u55bb\u548c\u6587\u5316\u7279\u5b9a\u63a8\u7406\u65b9\u9762\u7684\u4e00\u81f4\u5f31\u70b9\u3002", "conclusion": "BengaliFig\u4e3a\u8bc4\u4f30LLM\u5728\u4f4e\u8d44\u6e90\u6587\u5316\u80cc\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\uff0c\u5e76\u671d\u7740\u5305\u5bb9\u6027\u548c\u4f20\u627f\u610f\u8bc6\u7684NLP\u8bc4\u4f30\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2511.20467", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20467", "abs": "https://arxiv.org/abs/2511.20467", "authors": ["Liangkai Liu", "Weisong Shi", "Kang G. Shin"], "title": "Power-Efficient Autonomous Mobile Robots", "comment": "13 pages, 16 figures", "summary": "This paper presents pNav, a novel power-management system that significantly enhances the power/energy-efficiency of Autonomous Mobile Robots (AMRs) by jointly optimizing their physical/mechanical and cyber subsystems. By profiling AMRs' power consumption, we identify three challenges in achieving CPS (cyber-physical system) power-efficiency that involve both cyber (C) and physical (P) subsystems: (1) variabilities of system power consumption breakdown, (2) environment-aware navigation locality, and (3) coordination of C and P subsystems. pNav takes a multi-faceted approach to achieve power-efficiency of AMRs. First, it integrates millisecond-level power consumption prediction for both C and P subsystems. Second, it includes novel real-time modeling and monitoring of spatial and temporal navigation localities for AMRs. Third, it supports dynamic coordination of AMR software (navigation, detection) and hardware (motors, DVFS driver) configurations. pNav is prototyped using the Robot Operating System (ROS) Navigation Stack, 2D LiDAR, and camera. Our in-depth evaluation with a real robot and Gazebo environments demonstrates a >96% accuracy in predicting power consumption and a 38.1% reduction in power consumption without compromising navigation accuracy and safety.", "AI": {"tldr": "pNav\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u529f\u7387\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u7269\u7406/\u673a\u68b0\u548c\u7f51\u7edc\u5b50\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u529f\u7387/\u80fd\u91cf\u6548\u7387\u3002", "motivation": "\u901a\u8fc7\u5206\u6790AMR\u7684\u529f\u7387\u6d88\u8017\uff0c\u53d1\u73b0\u5b9e\u73b0CPS\u529f\u7387\u6548\u7387\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a\u7cfb\u7edf\u529f\u7387\u6d88\u8017\u5206\u89e3\u7684\u53d8\u5f02\u6027\u3001\u73af\u5883\u611f\u77e5\u5bfc\u822a\u7684\u5c40\u90e8\u6027\u3001\u4ee5\u53ca\u7f51\u7edc\u548c\u7269\u7406\u5b50\u7cfb\u7edf\u7684\u534f\u8c03\u3002", "method": "\u91c7\u7528\u591a\u5c42\u9762\u65b9\u6cd5\uff1a\u96c6\u6210\u6beb\u79d2\u7ea7\u529f\u7387\u6d88\u8017\u9884\u6d4b\u3001\u5b9e\u65f6\u5efa\u6a21\u548c\u76d1\u63a7\u5bfc\u822a\u65f6\u7a7a\u5c40\u90e8\u6027\u3001\u52a8\u6001\u534f\u8c03\u8f6f\u4ef6\u548c\u786c\u4ef6\u914d\u7f6e\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u548cGazebo\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u529f\u7387\u6d88\u8017\u9884\u6d4b\u51c6\u786e\u7387>96%\uff0c\u529f\u7387\u6d88\u8017\u51cf\u5c1138.1%\uff0c\u4e14\u4e0d\u5f71\u54cd\u5bfc\u822a\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "conclusion": "pNav\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86AMR\u529f\u7387\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7f51\u7edc\u548c\u7269\u7406\u5b50\u7cfb\u7edf\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u529f\u7387\u8282\u7701\u3002"}}
{"id": "2511.20409", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20409", "abs": "https://arxiv.org/abs/2511.20409", "authors": ["Md Abdullah Al Kafi", "Raka Moni", "Sumit Kumar Banshal"], "title": "A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines", "comment": null, "summary": "Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9762\u5411\u4efb\u52a1\u7684\u8bcd\u5e72\u63d0\u53d6\u65b9\u6cd5\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542bSES\u3001MPD\u548cANLD\u4e09\u4e2a\u6307\u6807\uff0c\u7528\u4e8e\u533a\u5206\u8bcd\u5e72\u63d0\u53d6\u7684\u6548\u7387\u589e\u76ca\u548c\u8bed\u4e49\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bcd\u5e72\u63d0\u53d6\u65b9\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u8fc7\u5ea6\u8bcd\u5e72\u63d0\u53d6\u53ef\u80fd\u9020\u6210\u7684\u5371\u5bb3\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u4e09\u4e2a\u65b9\u9762\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff1a(1)\u8bcd\u5e72\u63d0\u53d6\u6548\u7528(SES)\u3001(2)\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd(MPD)\u3001(3)\u8bcd\u5e72\u5316\u524d\u540e\u8bcd\u8bed\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6(ANLD)\u3002", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u6bd4\u8f83\u5b5f\u52a0\u62c9\u8bed\u548c\u82f1\u8bed\u8bcd\u5e72\u63d0\u53d6\u5668\uff0c\u53d1\u73b0\u5b5f\u52a0\u62c9\u8bed\u8bcd\u5e72\u63d0\u53d6\u5668\u867d\u7136SES\u9ad8(1.67)\uff0c\u4f46ANLD\u663e\u793a\u5b58\u5728\u6709\u5bb3\u7684\u8fc7\u5ea6\u8bcd\u5e72\u63d0\u53d6(0.26)\uff0c\u5bfc\u81f4\u4e0b\u6e38\u6027\u80fd\u4e0b\u964d\uff1b\u82f1\u8bed\u8bcd\u5e72\u63d0\u53d6\u5668SES\u9002\u4e2d(1.31)\u4f46\u8bed\u4e49\u8ddd\u79bb\u5b89\u5168(ANLD=0.14)\uff0c\u5bf9\u4e0b\u6e38\u6027\u80fd\u6709\u79ef\u6781\u8d21\u732e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u533a\u5206\u8bcd\u5e72\u63d0\u53d6\u6548\u7387\u589e\u76ca\u548c\u8bed\u4e49\u4fdd\u6301\u80fd\u529b\u7684\u6709\u4ef7\u503c\u5de5\u5177\uff0c\u8868\u660e\u9ad8SES\u5e76\u4e0d\u603b\u662f\u6709\u76ca\u7684\uff0c\u9700\u8981\u7ed3\u5408ANLD\u6765\u8bc4\u4f30\u8bcd\u5e72\u63d0\u53d6\u5668\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.20492", "categories": ["cs.RO", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20492", "abs": "https://arxiv.org/abs/2511.20492", "authors": ["Cyrill P\u00fcntener", "Johann Schwabe", "Dominique Garmier", "Jonas Frey", "Marco Hutter"], "title": "Kleinkram: Open Robotic Data Management", "comment": "for associated source code, see https://github.com/leggedrobotics/kleinkram", "summary": "We introduce Kleinkram, a free and open-source system designed to solve the challenge of managing massive, unstructured robotic datasets. Designed as a modular, on-premises cloud solution, Kleinkram enables scalable storage, indexing, and sharing of datasets, ranging from individual experiments to large-scale research collections. Kleinkram natively integrates with standard formats such as ROS bags and MCAP and utilises S3-compatible storage for flexibility. Beyond storage, Kleinkram features an integrated \"Action Runner\" that executes customizable Docker-based workflows for data validation, curation, and benchmarking. Kleinkram has successfully managed over 30 TB of data from diverse robotic systems, streamlining the research lifecycle through a modern web interface and a robust Command Line Interface (CLI).", "AI": {"tldr": "Kleinkram\u662f\u4e00\u4e2a\u514d\u8d39\u5f00\u6e90\u7684\u673a\u5668\u4eba\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u7ba1\u7406\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u5b58\u50a8\u3001\u7d22\u5f15\u548c\u5171\u4eab\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u7ba1\u7406\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u652f\u6301\u4ece\u4e2a\u4f53\u5b9e\u9a8c\u5230\u5927\u89c4\u6a21\u7814\u7a76\u6570\u636e\u96c6\u7684\u7edf\u4e00\u7ba1\u7406\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u672c\u5730\u4e91\u89e3\u51b3\u65b9\u6848\uff0c\u96c6\u6210ROS bags\u548cMCAP\u6807\u51c6\u683c\u5f0f\uff0c\u4f7f\u7528S3\u517c\u5bb9\u5b58\u50a8\uff0c\u5e76\u5305\u542b\u57fa\u4e8eDocker\u7684\u81ea\u5b9a\u4e49\u5de5\u4f5c\u6d41\u6267\u884c\u5668\u3002", "result": "\u6210\u529f\u7ba1\u7406\u4e86\u8d85\u8fc730TB\u6765\u81ea\u4e0d\u540c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6570\u636e\uff0c\u901a\u8fc7\u73b0\u4ee3\u5316Web\u754c\u9762\u548c\u5f3a\u5927CLI\u7b80\u5316\u4e86\u7814\u7a76\u751f\u547d\u5468\u671f\u3002", "conclusion": "Kleinkram\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u673a\u5668\u4eba\u6570\u636e\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u6570\u636e\u9a8c\u8bc1\u3001\u6574\u7406\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u7814\u7a76\u7684\u6570\u636e\u7ba1\u7406\u6548\u7387\u3002"}}
{"id": "2511.20459", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20459", "abs": "https://arxiv.org/abs/2511.20459", "authors": ["Mosab Rezaei", "Mina Rajaei Moghadam", "Abdul Rahman Shaikh", "Hamed Alhoori", "Reva Freedman"], "title": "Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts", "comment": null, "summary": "Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u8bc4\u4f3019\u4e16\u7eaa\u5c0f\u8bf4\u5bb6\u98ce\u683c\u6587\u672c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u548c\u57fa\u4e8etransformer\u7684\u68c0\u6d4b\u5668\u6765\u5206\u6790\u5199\u4f5c\u98ce\u683c\u3002", "motivation": "\u89e3\u51b3\u5728\u6ca1\u6709\u914d\u5bf9\u6570\u636e\u65f6\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u4ee5\u53ca\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u5224\u65ad\u8bc4\u4f30\u98ce\u683c\u6587\u672c\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6700\u5c0f\u5316\u5355token\u63d0\u793a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7279\u5b9a\u4f5c\u8005\u98ce\u683c\u7684\u6587\u672c\uff0c\u5e76\u8bad\u7ec3\u57fa\u4e8etransformer\u7684\u68c0\u6d4b\u5668\u8fdb\u884c\u5206\u7c7b\u548c\u98ce\u683c\u89e3\u91ca\u3002", "result": "\u751f\u6210\u7684\u6587\u672c\u53cd\u6620\u4e86\u4f5c\u8005\u7684\u72ec\u7279\u6a21\u5f0f\uff0cAI\u8bc4\u4f30\u4e3a\u4eba\u7c7b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u98ce\u683c\u6587\u672c\u7684\u751f\u6210\u548c\u8bc4\u4f30\uff0c\u6240\u6709\u5de5\u4f5c\u6210\u679c\u5df2\u5728\u7ebf\u53d1\u5e03\u3002"}}
{"id": "2511.20496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20496", "abs": "https://arxiv.org/abs/2511.20496", "authors": ["Jiaxin Liu", "Min Li", "Wanting Xu", "Liang Li", "Jiaqi Yang", "Laurent Kneip"], "title": "Metric, inertially aligned monocular state estimation via kinetodynamic priors", "comment": null, "summary": "Accurate state estimation for flexible robotic systems poses significant challenges, particular for platforms with dynamically deforming structures that invalidate rigid-body assumptions. This paper tackles this problem and allows to extend existing rigid-body pose estimation methods to non-rigid systems. Our approach hinges on two core assumptions: first, the elastic properties are captured by an injective deformation-force model, efficiently learned via a Multi-Layer Perceptron; second, we solve the platform's inherently smooth motion using continuous-time B-spline kinematic models. By continuously applying Newton's Second Law, our method establishes a physical link between visually-derived trajectory acceleration and predicted deformation-induced acceleration. We demonstrate that our approach not only enables robust and accurate pose estimation on non-rigid platforms, but that the properly modeled platform physics instigate inertial sensing properties. We demonstrate this feasibility on a simple spring-camera system, and show how it robustly resolves the typically ill-posed problem of metric scale and gravity recovery in monocular visual odometry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u521a\u6027\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u521a\u6027\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u53d8\u5f62-\u529b\u6a21\u578b\u548c\u8fde\u7eed\u65f6\u95f4B\u6837\u6761\u8fd0\u52a8\u6a21\u578b\uff0c\u5efa\u7acb\u89c6\u89c9\u8f68\u8ff9\u52a0\u901f\u5ea6\u4e0e\u53d8\u5f62\u8bf1\u5bfc\u52a0\u901f\u5ea6\u4e4b\u95f4\u7684\u7269\u7406\u8054\u7cfb\u3002", "motivation": "\u89e3\u51b3\u67d4\u6027\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7cbe\u786e\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u52a8\u6001\u53d8\u5f62\u7ed3\u6784\u4f7f\u521a\u4f53\u5047\u8bbe\u5931\u6548\u7684\u5e73\u53f0\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\u5b66\u4e60\u53ef\u9006\u7684\u53d8\u5f62-\u529b\u6a21\u578b\uff0c\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4B\u6837\u6761\u8fd0\u52a8\u6a21\u578b\uff0c\u901a\u8fc7\u725b\u987f\u7b2c\u4e8c\u5b9a\u5f8b\u5efa\u7acb\u89c6\u89c9\u8f68\u8ff9\u52a0\u901f\u5ea6\u4e0e\u9884\u6d4b\u53d8\u5f62\u52a0\u901f\u5ea6\u7684\u7269\u7406\u8054\u7cfb\u3002", "result": "\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5728\u975e\u521a\u6027\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9c81\u68d2\u51c6\u786e\u7684\u4f4d\u59ff\u4f30\u8ba1\uff0c\u8fd8\u80fd\u901a\u8fc7\u6b63\u786e\u5efa\u6a21\u7684\u5e73\u53f0\u7269\u7406\u6fc0\u53d1\u60ef\u6027\u4f20\u611f\u7279\u6027\uff0c\u5728\u7b80\u5355\u5f39\u7c27-\u76f8\u673a\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7a33\u5065\u5730\u89e3\u51b3\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u5ea6\u91cf\u5c3a\u5ea6\u548c\u91cd\u529b\u6062\u590d\u8fd9\u4e00\u901a\u5e38\u4e0d\u9002\u5b9a\u7684\u95ee\u9898\u3002"}}
{"id": "2511.20494", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20494", "abs": "https://arxiv.org/abs/2511.20494", "authors": ["Jakub Hoscilowicz", "Artur Janicki"], "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models", "comment": null, "summary": "We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Applications include embedding adversarial images into websites to prevent MLLM-powered agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u5bf9\u6297\u653b\u51fb\u2014\u2014\u5bf9\u6297\u6027\u6df7\u6dc6\u653b\u51fb\uff0c\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u7834\u574f\u6a21\u578b\u4f7f\u5176\u751f\u6210\u4e0d\u8fde\u8d2f\u6216\u81ea\u4fe1\u7684\u9519\u8bef\u8f93\u51fa\uff0c\u53ef\u7528\u4e8e\u963b\u6b62MLLM\u4ee3\u7406\u7684\u53ef\u9760\u8fd0\u884c\u3002", "motivation": "\u73b0\u6709\u653b\u51fb\u4e3b\u8981\u5173\u6ce8\u8d8a\u72f1\u6216\u5b9a\u5411\u9519\u8bef\u5206\u7c7b\uff0c\u800c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u80fd\u7cfb\u7edf\u7834\u574fMLLM\u53ef\u9760\u6027\u7684\u65b0\u578b\u5a01\u80c1\uff0c\u901a\u8fc7\u5728\u7f51\u7ad9\u4e2d\u5d4c\u5165\u5bf9\u6297\u56fe\u50cf\u6765\u963b\u6b62MLLM\u4ee3\u7406\u7684\u6b63\u5e38\u8fd0\u884c\u3002", "method": "\u4f7f\u7528\u5f00\u6e90MLLM\u5c0f\u96c6\u5408\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4e0b\u4e00\u4ee4\u724c\u71b5\u7684\u57fa\u672c\u5bf9\u6297\u6280\u672f\uff08PGD\uff09\u751f\u6210\u5bf9\u6297\u56fe\u50cf\uff0c\u5728\u5b8c\u6574\u56fe\u50cf\u548c\u5bf9\u6297CAPTCHA\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u653b\u51fb\u3002", "result": "\u5728\u81ea\u76d2\u8bbe\u7f6e\u4e0b\uff0c\u5355\u4e2a\u5bf9\u6297\u56fe\u50cf\u80fd\u7834\u574f\u96c6\u5408\u4e2d\u6240\u6709\u6a21\u578b\uff0c\u751f\u6210\u7684\u6270\u52a8\u80fd\u8fc1\u79fb\u5230\u672a\u89c1\u8fc7\u7684\u5f00\u6e90\u6a21\u578b\uff08\u5982Qwen3-VL\uff09\u548c\u4e13\u6709\u6a21\u578b\uff08\u5982GPT-5.1\uff09\u3002", "conclusion": "\u5bf9\u6297\u6027\u6df7\u6dc6\u653b\u51fb\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b0\u578b\u5a01\u80c1\uff0c\u5373\u4f7f\u4f7f\u7528\u57fa\u672c\u5bf9\u6297\u6280\u672f\u4e5f\u80fd\u4ea7\u751f\u53ef\u8fc1\u79fb\u7684\u6270\u52a8\uff0c\u4e25\u91cd\u5a01\u80c1MLLM\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.20570", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20570", "abs": "https://arxiv.org/abs/2511.20570", "authors": ["Tasha Kim", "Oiwi Parker Jones"], "title": "Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics", "comment": "Embodied and Safe-Assured Robotic Systems workshop at NeurIPS 2025", "summary": "Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action.", "AI": {"tldr": "GUARDIAN\u662f\u4e00\u4e2a\u7528\u4e8e\u795e\u7ecf\u4fe1\u53f7\u63a7\u5236\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u795e\u7ecf\u7b26\u53f7\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u8111\u4fe1\u53f7\u89e3\u7801\u4e0e\u7b26\u53f7\u76ee\u6807\u57fa\u7840\u548c\u53cc\u5c42\u7ea7\u8fd0\u884c\u65f6\u76d1\u63a7\uff0c\u786e\u4fdd\u903b\u8f91\u5b89\u5168\u548c\u751f\u7406\u4fe1\u4efb\u3002", "motivation": "\u5b89\u5168\u5173\u952e\u7684\u8f85\u52a9\u7cfb\u7edf\u9700\u8981\u4ece\u795e\u7ecf\u4fe1\u53f7\u76f4\u63a5\u89e3\u7801\u7528\u6237\u610f\u56fe\uff0c\u8fd9\u8981\u6c42\u4e25\u683c\u7684\u53ef\u9760\u6027\u548c\u4fe1\u4efb\u4fdd\u8bc1\u3002", "method": "GUARDIAN\u6846\u67b6\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u8111\u4fe1\u53f7\u89e3\u7801\u3001\u7b26\u53f7\u76ee\u6807\u57fa\u7840\u548c\u53cc\u5c42\u7ea7\u8fd0\u884c\u65f6\u76d1\u63a7\uff0c\u5728BNCI2014\u8fd0\u52a8\u60f3\u8c61EEG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u57289\u540d\u53d7\u8bd5\u8005\u548c5,184\u6b21\u8bd5\u9a8c\u4e2d\u5b9e\u73b0\u4e8694-97%\u7684\u9ad8\u5b89\u5168\u7387\uff0c\u5373\u4f7f\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u67b6\u6784\uff08\u6d4b\u8bd5\u51c6\u786e\u738727-46%\uff0cECE\u7f6e\u4fe1\u5ea6\u6821\u51c6\u8bef\u5dee0.22-0.41\uff09\u3002\u5728\u6a21\u62df\u566a\u58f0\u6d4b\u8bd5\u4e2d\uff0c\u6b63\u786e\u5e72\u9884\u6b21\u6570\u6bd4\u57fa\u7ebf\u63d0\u9ad81.7\u500d\u3002\u76d1\u63a7\u5668\u4ee5100Hz\u9891\u7387\u8fd0\u884c\uff0c\u51b3\u7b56\u5ef6\u8fdf\u4f4e\u4e8e\u6beb\u79d2\u7ea7\u3002", "conclusion": "GUARDIAN\u5c55\u793a\u4e86\u9488\u5bf9\u4fe1\u53f7\u9000\u5316\u7684\u6e10\u8fdb\u54cd\u5e94\uff0c\u5e76\u751f\u6210\u4ece\u610f\u56fe\u3001\u8ba1\u5212\u5230\u884c\u52a8\u7684\u53ef\u5ba1\u8ba1\u8f68\u8ff9\uff0c\u6709\u52a9\u4e8e\u5c06\u795e\u7ecf\u8bc1\u636e\u4e0e\u53ef\u9a8c\u8bc1\u7684\u673a\u5668\u4eba\u884c\u52a8\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2511.20507", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20507", "abs": "https://arxiv.org/abs/2511.20507", "authors": ["Nathan Roll", "Jill Kries", "Flora Jin", "Catherine Wang", "Ann Marie Finley", "Meghan Sumner", "Cory Shain", "Laura Gwilliams"], "title": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models", "comment": null, "summary": "Large language models (LLMs) have emerged as a candidate \"model organism\" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6587\u672c\u5931\u8bed\u75c7\u8bc4\u4f30\u91cf\u8868(TAB)\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u5931\u8bed\u75c7\u8bc4\u4f30\u6539\u7f16\u7684\u6587\u672c\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5931\u8bed\u75c7\u6837\u7f3a\u9677\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u7814\u7a76\u8bed\u8a00\u969c\u788d\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u4f20\u7edf\u4e34\u5e8a\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u9002\u5408\u8bc4\u4f30LLMs\uff0c\u56e0\u4e3a\u5b83\u4eec\u9884\u8bbe\u4e86\u4eba\u7c7b\u8bed\u7528\u538b\u529b\u5e76\u63a2\u6d4b\u4eba\u5de5\u67b6\u6784\u4e2d\u4e0d\u5b58\u5728\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u4ece\u5feb\u901f\u5931\u8bed\u75c7\u8bc4\u4f30\u91cf\u8868(QAB)\u6539\u7f16\u5f00\u53d1\u4e86TAB\uff0c\u5305\u542b\u56db\u4e2a\u5b50\u6d4b\u8bd5\uff1a\u8fde\u8d2f\u6587\u672c\u3001\u5355\u8bcd\u7406\u89e3\u3001\u53e5\u5b50\u7406\u89e3\u548c\u91cd\u590d\u3002\u9a8c\u8bc1\u4e86\u4f7f\u7528Gemini 2.5 Flash\u7684\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u8fbe\u5230\u4e86\u4e0e\u4e13\u5bb6\u4eba\u7c7b\u8bc4\u5206\u8005\u76f8\u5f53\u7684\u53ef\u9760\u6027\uff08\u6a21\u578b-\u5171\u8bc6\u4e00\u81f4\u6027Cohen's kappa = 0.255 vs \u4eba-\u4eba\u4e00\u81f4\u60270.286\uff09\u3002", "conclusion": "TAB\u4f5c\u4e3a\u4e00\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u7684\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\u53d1\u5e03\uff0c\u7528\u4e8e\u5206\u6790\u4eba\u5de5\u7cfb\u7edf\u4e2d\u7684\u8bed\u8a00\u7f3a\u9677\u3002"}}
{"id": "2511.20593", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20593", "abs": "https://arxiv.org/abs/2511.20593", "authors": ["Allen Emmanuel Binny", "Mahathi Anand", "Hugo T. M. Kussaba", "Lingyun Chen", "Shreenabh Agrawal", "Fares J. Abu-Dakka", "Abdalla Swikir"], "title": "Safe and Stable Neural Network Dynamical Systems for Robot Motion Planning", "comment": null, "summary": "Learning safe and stable robot motions from demonstrations remains a challenge, especially in complex, nonlinear tasks involving dynamic, obstacle-rich environments. In this paper, we propose Safe and Stable Neural Network Dynamical Systems S$^2$-NNDS, a learning-from-demonstration framework that simultaneously learns expressive neural dynamical systems alongside neural Lyapunov stability and barrier safety certificates. Unlike traditional approaches with restrictive polynomial parameterizations, S$^2$-NNDS leverages neural networks to capture complex robot motions providing probabilistic guarantees through split conformal prediction in learned certificates. Experimental results on various 2D and 3D datasets -- including LASA handwriting and demonstrations recorded kinesthetically from the Franka Emika Panda robot -- validate S$^2$-NNDS effectiveness in learning robust, safe, and stable motions from potentially unsafe demonstrations.", "AI": {"tldr": "S\u00b2-NNDS\u662f\u4e00\u4e2a\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u6846\u67b6\uff0c\u540c\u65f6\u5b66\u4e60\u795e\u7ecf\u52a8\u529b\u5b66\u7cfb\u7edf\u4ee5\u53ca\u795e\u7ecf\u674e\u96c5\u666e\u8bfa\u592b\u7a33\u5b9a\u6027\u548c\u969c\u788d\u5b89\u5168\u8bc1\u4e66\uff0c\u63d0\u4f9b\u6982\u7387\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u5728\u590d\u6742\u3001\u975e\u7ebf\u6027\u4efb\u52a1\u4e2d\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u5b89\u5168\u7a33\u5b9a\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u3001\u969c\u788d\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u590d\u6742\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u901a\u8fc7\u5206\u88c2\u4fdd\u5f62\u9884\u6d4b\u5728\u5b66\u4e60\u8bc1\u4e66\u4e2d\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u9650\u5236\u6027\u591a\u9879\u5f0f\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd2D\u548c3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cS\u00b2-NNDS\u80fd\u591f\u4ece\u6f5c\u5728\u4e0d\u5b89\u5168\u7684\u6f14\u793a\u4e2d\u5b66\u4e60\u5230\u9c81\u68d2\u3001\u5b89\u5168\u4e14\u7a33\u5b9a\u7684\u8fd0\u52a8\u3002", "conclusion": "S\u00b2-NNDS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u5b89\u5168\u7a33\u5b9a\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u3002"}}
{"id": "2511.20534", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20534", "abs": "https://arxiv.org/abs/2511.20534", "authors": ["Wesley Bian", "Xiaofeng Lin", "Guang Cheng"], "title": "Bridging the Language Gap: Synthetic Voice Diversity via Latent Mixup for Equitable Speech Recognition", "comment": "Accepted at ICML 2025 Workshop on Machine Learning for Audio", "summary": "Modern machine learning models for audio tasks often exhibit superior performance on English and other well-resourced languages, primarily due to the abundance of available training data. This disparity leads to an unfair performance gap for low-resource languages, where data collection is both challenging and costly. In this work, we introduce a novel data augmentation technique for speech corpora designed to mitigate this gap. Through comprehensive experiments, we demonstrate that our method significantly improves the performance of automatic speech recognition systems on low-resource languages. Furthermore, we show that our approach outperforms existing augmentation strategies, offering a practical solution for enhancing speech technology in underrepresented linguistic communities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u97f3\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u65e8\u5728\u6539\u5584\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u5dee\u8ddd", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u82f1\u8bed\u7b49\u8d44\u6e90\u4e30\u5bcc\u8bed\u8a00\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7531\u4e8e\u6570\u636e\u6536\u96c6\u56f0\u96be\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u5b58\u5728\u4e0d\u516c\u5e73\u7684\u6027\u80fd\u5dee\u8ddd", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u97f3\u8bed\u6599\u5e93\u6570\u636e\u589e\u5f3a\u6280\u672f", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u589e\u5f3a\u7b56\u7565", "conclusion": "\u8be5\u6280\u672f\u4e3a\u589e\u5f3a\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u793e\u533a\u7684\u8bed\u97f3\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.20633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20633", "abs": "https://arxiv.org/abs/2511.20633", "authors": ["Jiahui Zhang", "Ze Huang", "Chun Gu", "Zipei Ma", "Li Zhang"], "title": "Reinforcing Action Policies by Prophesying", "comment": "https://LogosRoboticsGroup.github.io/ProphRL", "summary": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.", "AI": {"tldr": "ProphRL\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\u540e\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\u63d0\u53475-17%\u6210\u529f\u7387\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u63d0\u534724-30%\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\u4e3b\u8981\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u6f14\u793a\u6570\u636e\u4e14\u5728\u5206\u5e03\u504f\u79fb\u65f6\u8868\u73b0\u8106\u5f31\u3002\u5f3a\u5316\u5b66\u4e60\u80fd\u76f4\u63a5\u4f18\u5316\u4efb\u52a1\u5956\u52b1\u4f46\u771f\u5b9e\u673a\u5668\u4eba\u4ea4\u4e92\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u6a21\u62df\u5668\u96be\u4ee5\u6784\u5efa\u548c\u8fc1\u79fb\u3002", "method": "\u63d0\u51faProphRL\u6846\u67b6\uff1a1) Prophet - \u5728\u5927\u89c4\u6a21\u5f02\u6784\u673a\u5668\u4eba\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u52a8\u4f5c\u5230\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u53ef\u5feb\u901f\u9002\u5e94\u65b0\u673a\u5668\u4eba\u3001\u7269\u4f53\u548c\u73af\u5883\uff1b2) FA-GRPO - \u9488\u5bf9\u6d41\u5f0f\u52a8\u4f5c\u5934\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff1b3) FlowScale - \u9010\u6b65\u91cd\u52a0\u6743\u65b9\u6cd5\uff0c\u91cd\u65b0\u7f29\u653e\u6d41\u5934\u7684\u6bcf\u6b65\u68af\u5ea6\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b05-17%\u7684\u6210\u529f\u7387\u63d0\u5347\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b024-30%\u7684\u6210\u529f\u7387\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "ProphRL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u6570\u636e\u548c\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\u540e\u8bad\u7ec3\u8def\u5f84\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u548c\u4e13\u95e8\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.20547", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20547", "abs": "https://arxiv.org/abs/2511.20547", "authors": ["Farjana Sultana Mim", "Shuchin Aeron", "Eric Miller", "Kristen Wendell"], "title": "From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding", "comment": null, "summary": "Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6559\u80b2\u5bf9\u8bdd\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u5b66\u751f\u5bf9\u8bdd\u4e2d\u7684\u77e5\u8bc6\u5efa\u6784\u548c\u4efb\u52a1\u4ea7\u51fa\u8bdd\u8bed\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u624b\u52a8\u5206\u6790\u5b66\u751f\u5bf9\u8bdd\u4e2d\u7684\u8bdd\u8bed\u7279\u5f81\u8017\u65f6\u8d39\u529b\uff0c\u9650\u5236\u4e86\u7814\u7a76\u89c4\u6a21\u3002\u5229\u7528NLP\u6280\u672f\u53ef\u4ee5\u81ea\u52a8\u68c0\u6d4b\u8fd9\u4e9b\u7279\u5f81\uff0c\u4e3a\u6559\u80b2\u7814\u7a76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u6d1e\u5bdf\u3002", "method": "\u521b\u5efa\u4e86\u6807\u6ce8\u7684\u6559\u80b2\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5305\u542b\u77e5\u8bc6\u5efa\u6784\u548c\u4efb\u52a1\u4ea7\u51fa\u8bdd\u8bed\u7279\u5f81\uff0c\u5e76\u4f7f\u7528GPT-3.5\u548cLlama-3.1\u7b49\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u57fa\u7ebf\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u672a\u6765\u7814\u7a76\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u867d\u7136\u81ea\u52a8\u68c0\u6d4b\u6559\u80b2\u5bf9\u8bdd\u4e2d\u7684\u8bdd\u8bed\u7279\u5f81\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u4ecd\u9700\u6539\u8fdb\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.20604", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20604", "abs": "https://arxiv.org/abs/2511.20604", "authors": ["Yixin Liu", "Pengfei Liu", "Arman Cohan"], "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges", "comment": "NeurIPS 2025 Camera Ready", "summary": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLMs\u7684\u751f\u6210\u80fd\u529b\u4e0e\u8bc4\u4f30\u80fd\u529b\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u7684\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e86AlignEval\u57fa\u51c6\uff0c\u901a\u8fc7\u8bc4\u4f30LLMs\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u80fd\u529b\u6765\u95f4\u63a5\u8861\u91cf\u5176\u5bf9\u9f50\u6027\u80fd\uff0c\u65e0\u9700\u76f4\u63a5\u8bc4\u4f30\u751f\u6210\u8f93\u51fa\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30LLMs\u5bf9\u9f50\u9700\u8981\u76f4\u63a5\u8bc4\u4f30\u5176\u5f00\u653e\u751f\u6210\u7ed3\u679c\uff0c\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u6216\u5f3aLLM\u8bc4\u5224\u8005\u3002\u672c\u6587\u63a2\u7d22LLMs\u751f\u6210\u4e0e\u8bc4\u4f30\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5206\u6790\u5404\u79cdLLMs\u7684\u751f\u6210-\u8bc4\u4f30\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u4e24\u8005\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002\u57fa\u4e8e\u6b64\u63d0\u51faAlignEval\u57fa\u51c6\uff0c\u901a\u8fc7\u8bc4\u4f30LLMs\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u80fd\u529b\u6765\u95f4\u63a5\u8861\u91cf\u5176\u5bf9\u9f50\u6027\u80fd\u3002", "result": "AlignEval\u57fa\u51c6\u5728\u6355\u6349\u4eba\u7c7b\u504f\u597d\u6392\u540dLLMs\u65b9\u9762\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86AlpacaEval\u548cArena-Hard\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u751f\u6210\u4e0e\u8bc4\u4f30\u80fd\u529b\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u9700\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u7684\u5bf9\u9f50\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.20639", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20639", "abs": "https://arxiv.org/abs/2511.20639", "authors": ["Jiaru Zou", "Xiyuan Yang", "Ruizhong Qiu", "Gaotang Li", "Katherine Tieu", "Pan Lu", "Ke Shen", "Hanghang Tong", "Yejin Choi", "Jingrui He", "James Zou", "Mengdi Wang", "Ling Yang"], "title": "Latent Collaboration in Multi-Agent Systems", "comment": "Project: https://github.com/Gen-Verse/LatentMAS", "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", "AI": {"tldr": "LatentMAS\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u534f\u4f5c\uff0c\u5b9e\u73b0LLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u7eaf\u6f5c\u5728\u534f\u4f5c\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6587\u672c\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5177\u6709\u66f4\u9ad8\u8868\u8fbe\u80fd\u529b\u548c\u65e0\u635f\u4fe1\u606f\u4ea4\u6362\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u4f9d\u8d56\u57fa\u4e8e\u6587\u672c\u7684\u4e2d\u4ecb\u8fdb\u884c\u63a8\u7406\u548c\u901a\u4fe1\uff0c\u4f5c\u8005\u5e0c\u671b\u8ba9\u6a21\u578b\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u534f\u4f5c\uff0c\u63d0\u5347\u7cfb\u7edf\u7ea7\u667a\u80fd\u3002", "method": "\u6bcf\u4e2a\u667a\u80fd\u4f53\u901a\u8fc7\u6700\u540e\u4e00\u5c42\u9690\u85cf\u5d4c\u5165\u8fdb\u884c\u81ea\u56de\u5f52\u6f5c\u5728\u601d\u7ef4\u751f\u6210\uff0c\u5171\u4eab\u6f5c\u5728\u5de5\u4f5c\u5185\u5b58\u4fdd\u5b58\u548c\u4f20\u8f93\u5185\u90e8\u8868\u793a\uff0c\u786e\u4fdd\u65e0\u635f\u4fe1\u606f\u4ea4\u6362\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLatentMAS\u6bd4\u5355\u6a21\u578b\u548c\u57fa\u4e8e\u6587\u672c\u7684MAS\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u9ad814.6%\uff0c\u8f93\u51fatoken\u51cf\u5c1170.8%-83.7%\uff0c\u7aef\u5230\u7aef\u63a8\u7406\u901f\u5ea6\u63d0\u53474-4.3\u500d\u3002", "conclusion": "\u6f5c\u5728\u534f\u4f5c\u6846\u67b6\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u7ea7\u63a8\u7406\u8d28\u91cf\u5e76\u5e26\u6765\u6548\u7387\u589e\u76ca\u3002"}}
