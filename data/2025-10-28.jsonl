{"id": "2510.21732", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21732", "abs": "https://arxiv.org/abs/2510.21732", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "title": "A Robotic Stirring Method with Trajectory Optimization and Adaptive Speed Control for Accurate Pest Counting in Water Traps", "comment": "This paper has been submitted to ICRA 2026 and is currently under\n  review", "summary": "Accurate monitoring of pest population dynamics is crucial for informed\ndecision-making in precision agriculture. Currently, mainstream image-based\npest counting methods primarily rely on image processing combined with machine\nlearning or deep learning for pest counting. However, these methods have\nlimitations and struggle to handle situations involving pest occlusion. To\naddress this issue, this paper proposed a robotic stirring method with\ntrajectory optimization and adaptive speed control for accurate pest counting\nin water traps. First, we developed an automated stirring system for pest\ncounting in yellow water traps based on a robotic arm. Stirring alters the\ndistribution of pests in the yellow water trap, making some of the occluded\nindividuals visible for detection and counting. Then, we investigated the\nimpact of different stirring trajectories on pest counting performance and\nselected the optimal trajectory for pest counting. Specifically, we designed\nsix representative stirring trajectories, including circle, square, triangle,\nspiral, four small circles, and random lines, for the robotic arm to stir. And\nby comparing the overall average counting error and counting confidence of\ndifferent stirring trajectories across various pest density scenarios, we\ndetermined the optimal trajectory. Finally, we proposed a counting\nconfidence-driven closed-loop control system to achieve adaptive-speed\nstirring. It uses changes in pest counting confidence between consecutive\nframes as feedback to adjust the stirring speed. To the best of our knowledge,\nthis is the first study dedicated to investigating the effects of different\nstirring trajectories on object counting in the dynamic liquid environment and\nto implement adaptive-speed stirring for this type of task. Experimental\nresults show ..."}
{"id": "2510.21734", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21734", "abs": "https://arxiv.org/abs/2510.21734", "authors": ["Giovanni Battista Regazzo", "Wim-Alexander Beckers", "Xuan Thao Ha", "Mouloud Ourak", "Johan Vlekken", "Emmanuel Vander Poorten"], "title": "Force-Displacement Profiling for Robot-Assisted Deployment of a Left Atrial Appendage Occluder Using FBG-EM Distal Sensing", "comment": "Presented at the Conference on New Technologies for Computer and\n  Robot Assisted Surgery (CRAS2025)", "summary": "Atrial fibrillation (AF) increases the risk of thromboembolic events due to\nimpaired function of the left atrial appendage (LAA). Left atrial appendage\nclosure (LAAC) is a minimally invasive intervention designed to reduce stroke\nrisk by sealing the LAA with an expandable occluder device. Current deployment\nrelies on manual catheter control and imaging modalities like fluoroscopy and\ntransesophageal echocardiography, which carry limitations including radiation\nexposure and limited positioning precision. In this study, we leverage a\npreviously developed force-sensing delivery sheath integrating fiber Bragg\ngratings (FBGs) at the interface between the catheter and the occluder.\nCombined with electromagnetic (EM) tracking, this setup enables real-time\nmeasurement of interaction forces and catheter tip position during\nrobot-assisted LAAC deployment in an anatomical phantom. We present a novel\nforce-displacement profiling method that characterizes occluder deployment\ndynamics and identifies key procedural steps without relying on ionizing\nradiation. The force profiles reveal low-magnitude interaction forces,\nsuggesting minimal mechanical stress on the surrounding anatomy. This approach\nshows promise in providing clinicians with enhanced intraoperative feedback,\nimproving deployment outcome. Future work will focus on automating deployment\nsteps classification and validating the sensing strategy in dynamic, realistic\nenvironments."}
{"id": "2510.21735", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21735", "abs": "https://arxiv.org/abs/2510.21735", "authors": ["Yuhui Liu", "Shian Wang", "Ansel Panicker", "Kate Embry", "Ayana Asanova", "Tianyi Li"], "title": "A phase-aware AI car-following model for electric vehicles with adaptive cruise control: Development and validation using real-world data", "comment": null, "summary": "Internal combustion engine (ICE) vehicles and electric vehicles (EVs) exhibit\ndistinct vehicle dynamics. EVs provide rapid acceleration, with electric motors\nproducing peak power across a wider speed range, and achieve swift deceleration\nthrough regenerative braking. While existing microscopic models effectively\ncapture the driving behavior of ICE vehicles, a modeling framework that\naccurately describes the unique car-following dynamics of EVs is lacking.\nDeveloping such a model is essential given the increasing presence of EVs in\ntraffic, yet creating an easy-to-use and accurate analytical model remains\nchallenging.\n  To address these gaps, this study develops and validates a Phase-Aware AI\n(PAAI) car-following model specifically for EVs. The proposed model enhances\ntraditional physics-based frameworks with an AI component that recognizes and\nadapts to different driving phases, such as rapid acceleration and regenerative\nbraking. Using real-world trajectory data from vehicles equipped with adaptive\ncruise control (ACC), we conduct comprehensive simulations to validate the\nmodel's performance. The numerical results demonstrate that the PAAI model\nsignificantly improves prediction accuracy over traditional car-following\nmodels, providing an effective tool for accurately representing EV behavior in\ntraffic simulations."}
{"id": "2510.21736", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21736", "abs": "https://arxiv.org/abs/2510.21736", "authors": ["Yuhui Liu", "Samannita Halder", "Shian Wang", "Tianyi Li"], "title": "Learn2Drive: A neural network-based framework for socially compliant automated vehicle control", "comment": null, "summary": "This study introduces a novel control framework for adaptive cruise control\n(ACC) in automated driving, leveraging Long Short-Term Memory (LSTM) networks\nand physics-informed constraints. As automated vehicles (AVs) adopt advanced\nfeatures like ACC, transportation systems are becoming increasingly intelligent\nand efficient. However, existing AV control strategies primarily focus on\noptimizing the performance of individual vehicles or platoons, often neglecting\ntheir interactions with human-driven vehicles (HVs) and the broader impact on\ntraffic flow. This oversight can exacerbate congestion and reduce overall\nsystem efficiency. To address this critical research gap, we propose a neural\nnetwork-based, socially compliant AV control framework that incorporates social\nvalue orientation (SVO). This framework enables AVs to account for their\ninfluence on HVs and traffic dynamics. By leveraging AVs as mobile traffic\nregulators, the proposed approach promotes adaptive driving behaviors that\nreduce congestion, improve traffic efficiency, and lower energy consumption.\nWithin this framework, we define utility functions for both AVs and HVs, which\nare optimized based on the SVO of each AV to balance its own control objectives\nwith broader traffic flow considerations. Numerical results demonstrate the\neffectiveness of the proposed method in adapting to varying traffic conditions,\nthereby enhancing system-wide efficiency. Specifically, when the AV's control\nmode shifts from prioritizing energy consumption to optimizing traffic flow\nefficiency, vehicles in the following platoon experience at least a 58.99%\nincrease in individual energy consumption alongside at least a 38.39%\nimprovement in individual average speed, indicating significant enhancements in\ntraffic dynamics."}
{"id": "2510.21739", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21739", "abs": "https://arxiv.org/abs/2510.21739", "authors": ["Liangqi Yuan", "Chuhao Deng", "Dong-Jun Han", "Inseok Hwang", "Sabine Brunswicker", "Christopher G. Brinton"], "title": "Next-Generation LLM for UAV: From Natural Language to Autonomous Flight", "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), their\ncapabilities in various automation domains, particularly Unmanned Aerial\nVehicle (UAV) operations, have garnered increasing attention. Current research\nremains predominantly constrained to small-scale UAV applications, with most\nstudies focusing on isolated components such as path planning for toy drones,\nwhile lacking comprehensive investigation of medium- and long-range UAV systems\nin real-world operational contexts. Larger UAV platforms introduce distinct\nchallenges, including stringent requirements for airport-based take-off and\nlanding procedures, adherence to complex regulatory frameworks, and specialized\noperational capabilities with elevated mission expectations. This position\npaper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive\ndemonstration and automation roadmap for integrating LLMs into multi-scale UAV\noperations. The NeLV system processes natural language instructions to\norchestrate short-, medium-, and long-range UAV missions through five key\ntechnical components: (i) LLM-as-Parser for instruction interpretation, (ii)\nRoute Planner for Points of Interest (POI) determination, (iii) Path Planner\nfor waypoint generation, (iv) Control Platform for executable trajectory\nimplementation, and (v) UAV monitoring. We demonstrate the system's feasibility\nthrough three representative use cases spanning different operational scales:\nmulti-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the\ncurrent implementation, we establish a five-level automation taxonomy that\ncharts the evolution from current LLM-as-Parser capabilities (Level 1) to fully\nautonomous LLM-as-Autopilot systems (Level 5), identifying technical\nprerequisites and research challenges at each stage."}
{"id": "2510.21744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21744", "abs": "https://arxiv.org/abs/2510.21744", "authors": ["Yanjia Huang", "Shuo Liu", "Sheng Liu", "Qingxiao Xu", "Mingyang Wu", "Xiangbo Gao", "Zhengzhong Tu"], "title": "FORGE-Tree: Diffusion-Forcing Tree Search for Long-Horizon Robot Manipulation", "comment": null, "summary": "Long-horizon robot manipulation tasks remain challenging for\nVision-Language-Action (VLA) policies due to drift and exposure bias, often\ndenoise the entire trajectory with fixed hyperparameters, causing small\ngeometric errors to compound across stages and offering no mechanism to\nallocate extra test-time compute where clearances are tight. To address these\nchallenges, we introduce FORGE-Tree, a plug-in control layer that couples a\nstage-aligned Diffusion Forcing (DF) head with test-time Monte Carlo Tree\nDiffusion (MCTD). With a frozen VLA encoder, DF aligns timesteps to subtask\nstages; during inference we partially denoise only a target segment while\nkeeping other tokens frozen, turning trajectory refinement into a sequence of\nlocal edits. We then apply Monte Carlo Tree Diffusion to select the next\nsegment to refine. A scene graph supplies priors for expansion and geometry\nrelation-aware scoring for rollouts, yielding tree-structured denoising whose\nperformance scales with search budget while preserving the executed prefix.\nEvaluation on LIBERO, FORGE-Tree improves success rate by 13.4 to 17.2 pp over\nthe native VLA baselines with both OpenVLA and Octo-Base. Gains remain\nconsistent under comparable compute budgets, especially on long-horizon\nvariants. Videos available at: https://taco-group.github.io/FORGE-Tree/"}
{"id": "2510.21746", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21746", "abs": "https://arxiv.org/abs/2510.21746", "authors": ["Harris Song", "Long Le"], "title": "Avi: Action from Volumetric Inference", "comment": "NeurIPS 2025 Workshop on Embodied World Models for Decision Making.\n  URL: https://avi-3drobot.github.io/", "summary": "We propose Avi, a novel 3D Vision-Language-Action (VLA) architecture that\nreframes robotic action generation as a problem of 3D perception and spatial\nreasoning, rather than low-level policy learning. While existing VLA models\nprimarily operate on 2D visual inputs and are trained end-to-end on\ntask-specific action policies, Avi leverages 3D point clouds and\nlanguage-grounded scene understanding to compute actions through classical\ngeometric transformations. Most notably, Avi does not train on previous action\ntokens, rather, we build upon a 3D Multi-modal Large Language Model (MLLM) to\ngenerate the next point cloud and explicitly calculate the actions through\nclassical transformations. This approach enables generalizable behaviors that\nare robust to occlusions, camera pose variations, and changes in viewpoint. By\ntreating the robotic decision-making process as a structured reasoning task\nover 3D representations, Avi bridges the gap between high-level language\ninstructions and low-level actuation without requiring opaque policy learning.\nOur preliminary results highlight the potential of 3D vision-language reasoning\nas a foundation for scalable, robust robotic systems. Check it out at\nhttps://avi-3drobot.github.io/."}
{"id": "2510.21751", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21751", "abs": "https://arxiv.org/abs/2510.21751", "authors": ["Van Nam Dinh", "Van Vy Phan", "Thai Son Dang", "Van Du Phan", "The Anh Mai", "Van Chuong Le", "Sy Phuong Ho", "Dinh Tu Duong", "Hung Cuong Ta"], "title": "Real-time Mixed-Integer Quadratic Programming for Driving Behavior-Inspired Speed Bump Optimal Trajectory Planning", "comment": null, "summary": "This paper proposes a novel methodology for trajectory planning in autonomous\nvehicles (AVs), addressing the complex challenge of negotiating speed bumps\nwithin a unified Mixed-Integer Quadratic Programming (MIQP) framework. By\nleveraging Model Predictive Control (MPC), we develop trajectories that\noptimize both the traversal of speed bumps and overall passenger comfort. A key\ncontribution of this work is the formulation of speed bump handling constraints\nthat closely emulate human driving behavior, seamlessly integrating these with\nbroader road navigation requirements. Through extensive simulations in varied\nurban driving environments, we demonstrate the efficacy of our approach,\nhighlighting its ability to ensure smooth speed transitions over speed bumps\nwhile maintaining computational efficiency suitable for real-time deployment.\nThe method's capability to handle both static road features and dynamic\nconstraints, alongside expert human driving, represents a significant step\nforward in trajectory planning for urban"}
{"id": "2510.21758", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21758", "abs": "https://arxiv.org/abs/2510.21758", "authors": ["Kumater Ter", "RexCharles Donatus", "Ore-Ofe Ajayi", "Daniel Udekwe"], "title": "Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review", "comment": null, "summary": "Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems."}
{"id": "2510.21761", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21761", "abs": "https://arxiv.org/abs/2510.21761", "authors": ["Jesse Atuhurra", "Hidetaka Kamigaito", "Taro Watanabe", "Koichiro Yoshino"], "title": "J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception", "comment": "Accepted to IROS2025", "summary": "We introduce J-ORA, a novel multimodal dataset that bridges the gap in robot\nperception by providing detailed object attribute annotations within Japanese\nhuman-robot dialogue scenarios. J-ORA is designed to support three critical\nperception tasks, object identification, reference resolution, and next-action\nprediction, by leveraging a comprehensive template of attributes (e.g.,\ncategory, color, shape, size, material, and spatial relations). Extensive\nevaluations with both proprietary and open-source Vision Language Models (VLMs)\nreveal that incorporating detailed object attributes substantially improves\nmultimodal perception performance compared to without object attributes.\nDespite the improvement, we find that there still exists a gap between\nproprietary and open-source VLMs. In addition, our analysis of object\naffordances demonstrates varying abilities in understanding object\nfunctionality and contextual relationships across different VLMs. These\nfindings underscore the importance of rich, context-sensitive attribute\nannotations in advancing robot perception in dynamic environments. See project\npage at https://jatuhurrra.github.io/J-ORA/."}
{"id": "2510.21771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21771", "abs": "https://arxiv.org/abs/2510.21771", "authors": ["Dharunish Yugeswardeenoo"], "title": "Improving the performance of AI-powered Affordable Robotics for Assistive Tasks", "comment": "6 pages, 5 figures. Accepted to Conference on Robot Learning (CoRL\n  2025), Seoul, Korea", "summary": "By 2050, the global demand for assistive care is expected to reach 3.5\nbillion people, far outpacing the availability of human caregivers. Existing\nrobotic solutions remain expensive and require technical expertise, limiting\naccessibility. This work introduces a low-cost robotic arm for assistive tasks\nsuch as feeding, cleaning spills, and fetching medicine. The system uses\nimitation learning from demonstration videos, requiring no task-specific\nprogramming or manual labeling. The robot consists of six servo motors, dual\ncameras, and 3D-printed grippers. Data collection via teleoperation with a\nleader arm yielded 50,000 video frames across the three tasks. A novel Phased\nAction Chunking Transformer (PACT) captures temporal dependencies and segments\nmotion dynamics, while a Temporal Ensemble (TE) method refines trajectories to\nimprove accuracy and smoothness. Evaluated across five model sizes and four\narchitectures, with ten hours of real-world testing, the system achieved over\n90% task accuracy, up to 40% higher than baselines. PACT enabled a 5x model\nsize reduction while maintaining 75% accuracy. Saliency analysis showed\nreliance on key visual cues, and phase token gradients peaked at critical\ntrajectory moments, indicating effective temporal reasoning. Future work will\nexplore bimanual manipulation and mobility for expanded assistive capabilities."}
{"id": "2510.21773", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21773", "abs": "https://arxiv.org/abs/2510.21773", "authors": ["Van Nam Dinh"], "title": "Real-Time QP Solvers: A Concise Review and Practical Guide Towards Legged Robots", "comment": "6 pages, 1 figure, 2 tables", "summary": "Quadratic programming (QP) underpins real-time robotics by enabling\nefficient, constrained optimization in state estimation, motion planning, and\ncontrol. In legged locomotion and manipulation, essential modules like inverse\ndynamics, Model Predictive Control (MPC), and Whole-Body Control (WBC) are\ninherently QP-based, demanding reliable solutions amid tight timing, energy,\nand computational limits on embedded platforms. This paper presents a\ncomprehensive analysis and benchmarking study of cutting-edge QP solvers for\nlegged robotics. We begin by formulating the standard convex QP and classify\nsolvers into four principal algorithmic approaches, including interior-point\nmethods, active-set strategies, operator splitting schemes, and augmented\nLagrangian approaches. Each solver is examined in terms of algorithmic\nstructure, computational characteristics, and its ability to exploit problem\nstructure and warm-starting. Performance is evaluated using publicly available\nbenchmarks, focusing on metrics such as computation time, constraint\nsatisfaction, and robustness under perturbations. Feature tables and\ncomparisons yield practical guidance for solver selection, underscoring\ntrade-offs in speed, accuracy, and energy efficiency. Our findings emphasize\nthe synergy between solver, task, and hardware, sparse IPMs for long-horizon\nMPC, and dense active-set for high frequency WBC to advance agile, autonomous\nlegged systems, with emerging extensions to nonconvex and distributed QP."}
{"id": "2510.21817", "categories": ["cs.RO", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21817", "abs": "https://arxiv.org/abs/2510.21817", "authors": ["Xiaoyu Liu", "Chaoyou Fu", "Chi Yan", "Chu Wu", "Haihan Gao", "Yi-Fan Zhang", "Shaoqi Dong", "Cheng Qian", "Bin Luo", "Xiuyong Yang", "Guanwu Li", "Yusheng Cai", "Yunhang Shen", "Deqiang Jiang", "Haoyu Cao", "Xing Sun", "Caifeng Shan", "Ran He"], "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting", "comment": "Homepage: https://lxysl.github.io/VITA-E/", "summary": "Current Vision-Language-Action (VLA) models are often constrained by a rigid,\nstatic interaction paradigm, which lacks the ability to see, hear, speak, and\nact concurrently as well as handle real-time user interruptions dynamically.\nThis hinders seamless embodied collaboration, resulting in an inflexible and\nunresponsive user experience. To address these limitations, we introduce\nVITA-E, a novel embodied interaction framework designed for both behavioral\nconcurrency and nearly real-time interruption. The core of our approach is a\ndual-model architecture where two parallel VLA instances operate as an ``Active\nModel'' and a ``Standby Model'', allowing the embodied agent to observe its\nenvironment, listen to user speech, provide verbal responses, and execute\nactions, all concurrently and interruptibly, mimicking human-like multitasking\ncapabilities. We further propose a ``model-as-controller'' paradigm, where we\nfine-tune the VLM to generate special tokens that serve as direct system-level\ncommands, coupling the model's reasoning with the system's behavior.\nExperiments conducted on a physical humanoid platform demonstrate that VITA-E\ncan reliably handle complex interactive scenarios. Our framework is compatible\nwith various dual-system VLA models, achieving an extremely high success rate\non emergency stops and speech interruptions while also successfully performing\nconcurrent speech and action. This represents a significant step towards more\nnatural and capable embodied assistants."}
{"id": "2510.21854", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21854", "abs": "https://arxiv.org/abs/2510.21854", "authors": ["Sourabh Karmakar", "Cameron J. Turner"], "title": "A Literature Review On Stewart-Gough Platform Calibrations A Literature Review On Stewart-Gough Platform Calibrations", "comment": null, "summary": "Researchers have studied Stewart-Gough platforms, also known as Gough-Stewart\nplatforms or hexapod platforms extensively for their inherent fine control\ncharacteristics. Their studies led to the potential deployment opportunities of\nStewart-Gough Platforms in many critical applications such as the medical\nfield, engineering machines, space research, electronic chip manufacturing,\nautomobile manufacturing, etc. Some of these applications need micro and\nnano-level movement control in 3D space for the motions to be precise,\ncomplicated, and repeatable; a Stewart-Gough platform fulfills these challenges\nsmartly. For this, the platform must be more accurate than the specified\napplication accuracy level and thus proper calibration for a parallel robot is\ncrucial. Forward kinematics-based calibration for these hexapod machines\nbecomes unnecessarily complex and inverse kinematics complete this task with\nmuch ease. To experiment with different calibration techniques, various\ncalibration approaches were implemented by using external instruments,\nconstraining one or more motions of the system, and using extra sensors for\nauto or self-calibration. This survey paid attention to those key\nmethodologies, their outcome, and important details related to inverse\nkinematic-based parallel robot calibrations. It was observed during this study\nthat the researchers focused on improving the accuracy of the platform position\nand orientation considering the errors contributed by one source or multiple\nsources. The error sources considered are mainly kinematic and structural, in\nsome cases, environmental factors also are reviewed, however, those\ncalibrations are done under no-load conditions. This study aims to review the\npresent state of the art in this field and highlight the processes and errors\nconsidered for the calibration of Stewart-Gough platforms."}
{"id": "2510.21860", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21860", "abs": "https://arxiv.org/abs/2510.21860", "authors": ["Callum Sharrock", "Lukas Petersson", "Hanna Petersson", "Axel Backlund", "Axel Wennström", "Kristoffer Nordström", "Elias Aronsson"], "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence", "comment": null, "summary": "We present Butter-Bench, a benchmark evaluating large language model (LLM)\ncontrolled robots for practical intelligence, defined as the ability to\nnavigate the messiness of the physical world. Current state-of-the-art robotic\nsystems use a hierarchical architecture with LLMs in charge of high-level\nreasoning, and a Vision Language Action (VLA) model for low-level control.\nButter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs\nhave repeatedly surpassed humans in evaluations requiring analytical\nintelligence, we find humans still outperform LLMs on Butter-Bench. The best\nLLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs\nstruggled the most with multi-step spatial planning and social understanding.\nWe also evaluate LLMs that are fine-tuned for embodied reasoning and conclude\nthat this training does not improve their score on Butter-Bench."}
{"id": "2510.21874", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21874", "abs": "https://arxiv.org/abs/2510.21874", "authors": ["Shuning Zhang"], "title": "A Physics-Informed Neural Network Approach for UAV Path Planning in Dynamic Environments", "comment": "15 pages, 8 figures", "summary": "Unmanned aerial vehicles (UAVs) operating in dynamic wind fields must\ngenerate safe and energy-efficient trajectories under physical and\nenvironmental constraints. Traditional planners, such as A* and kinodynamic\nRRT*, often yield suboptimal or non-smooth paths due to discretization and\nsampling limitations. This paper presents a physics-informed neural network\n(PINN) framework that embeds UAV dynamics, wind disturbances, and obstacle\navoidance directly into the learning process. Without requiring supervised\ndata, the PINN learns dynamically feasible and collision-free trajectories by\nminimizing physical residuals and risk-aware objectives. Comparative\nsimulations show that the proposed method outperforms A* and Kino-RRT* in\ncontrol energy, smoothness, and safety margin, while maintaining similar flight\nefficiency. The results highlight the potential of physics-informed learning to\nunify model-based and data-driven planning, providing a scalable and physically\nconsistent framework for UAV trajectory optimization."}
{"id": "2510.21991", "categories": ["cs.RO", "cs.AI", "68T40, 93C85, 68T07, 68U35"], "pdf": "https://arxiv.org/pdf/2510.21991", "abs": "https://arxiv.org/abs/2510.21991", "authors": ["Mateo Clemente", "Leo Brunswic", "Rui Heng Yang", "Xuan Zhao", "Yasser Khalil", "Haoyu Lei", "Amir Rasouli", "Yinchuan Li"], "title": "Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising", "comment": "16 pages, 11 figure, 2 tables, accepted at Neurips 2025", "summary": "Diffusion models, such as diffusion policy, have achieved state-of-the-art\nresults in robotic manipulation by imitating expert demonstrations. While\ndiffusion models were originally developed for vision tasks like image and\nvideo generation, many of their inference strategies have been directly\ntransferred to control domains without adaptation. In this work, we show that\nby tailoring the denoising process to the specific characteristics of embodied\nAI tasks -- particularly structured, low-dimensional nature of action\ndistributions -- diffusion policies can operate effectively with as few as 5\nneural function evaluations (NFE).\n  Building on this insight, we propose a population-based sampling strategy,\ngenetic denoising, which enhances both performance and stability by selecting\ndenoising trajectories with low out-of-distribution risk. Our method solves\nchallenging tasks with only 2 NFE while improving or matching performance. We\nevaluate our approach across 14 robotic manipulation tasks from D4RL and\nRobomimic, spanning multiple action horizons and inference budgets. In over 2\nmillion evaluations, our method consistently outperforms standard\ndiffusion-based policies, achieving up to 20\\% performance gains with\nsignificantly fewer inference steps."}
{"id": "2510.22030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22030", "abs": "https://arxiv.org/abs/2510.22030", "authors": ["Harsha Karunanayaka", "Siavash Rezazadeh"], "title": "Estimation of Minimum Stride Frequency for the Frontal Plane Stability of Bipedal Systems", "comment": null, "summary": "Stability of bipedal systems in frontal plane is affected by the hip offset,\nto the extent that adjusting stride time using feedforward retraction and\nextension of the legs can lead to stable oscillations without feedback control.\nThis feedforward stabilization can be leveraged to reduce the control effort\nand energy expenditure and increase the locomotion robustness. However, there\nis limited understanding of how key parameters, such as mass, stiffness, leg\nlength, and hip width, affect stability and the minimum stride frequency needed\nto maintain it. This study aims to address these gaps through analyzing how\nindividual model parameters and the system's natural frequency influence the\nminimum stride frequency required to maintain a stable cycle. We propose a\nmethod to predict the minimum stride frequency, and compare the predicted\nstride frequencies with actual values for randomly generated models. The\nfindings of this work provide a better understanding of the frontal plane\nstability mechanisms and how feedforward stabilization can be leveraged to\nreduce the control effort."}
{"id": "2510.22113", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22113", "abs": "https://arxiv.org/abs/2510.22113", "authors": ["Zitiantao Lin", "Yongpeng Sang", "Yang Ye"], "title": "RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation", "comment": "5 pages, 5 figures; Accepted to: 2025 IEEE 4th International\n  Conference on Intelligent Reality (ICIR 2025); Zitiantao Lin and Yongpeng\n  Sang contributed equally to this work (co-first authors). Corresponding\n  author: Yang Ye (y.ye@northeastern.edu)", "summary": "Robotic manipulators are increasingly used to assist individuals with\nmobility impairments in object retrieval. However, the predominant\njoystick-based control interfaces can be challenging due to high precision\nrequirements and unintuitive reference frames. Recent advances in human-robot\ninteraction have explored alternative modalities, yet many solutions still rely\non external screens or restrictive control schemes, limiting their\nintuitiveness and accessibility. To address these challenges, we present an\negocentric, gaze-guided robotic manipulation interface that leverages a\nwearable Mixed Reality (MR) headset. Our system enables users to interact\nseamlessly with real-world objects using natural gaze fixation from a\nfirst-person perspective, while providing augmented visual cues to confirm\nintent and leveraging a pretrained vision model and robotic arm for intent\nrecognition and object manipulation. Experimental results demonstrate that our\napproach significantly improves manipulation accuracy, reduces system latency,\nand achieves single-pass intention and object recognition accuracy greater than\n88% across multiple real-world scenarios. These results demonstrate the\nsystem's effectiveness in enhancing intuitiveness and accessibility,\nunderscoring its practical significance for assistive robotics applications."}
{"id": "2510.22126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22126", "abs": "https://arxiv.org/abs/2510.22126", "authors": ["Guanwen Xie", "Jingzehua Xu", "Jiwei Tang", "Yubo Huang", "Shuai Zhang", "Xiaofan Li"], "title": "EasyUUV: An LLM-Enhanced Universal and Lightweight Sim-to-Real Reinforcement Learning Framework for UUV Attitude Control", "comment": "8 pages, 15 figures", "summary": "Despite recent advances in Unmanned Underwater Vehicle (UUV) attitude\ncontrol, existing methods still struggle with generalizability, robustness to\nreal-world disturbances, and efficient deployment. To address the above\nchallenges, this paper presents EasyUUV, a Large Language Model (LLM)-enhanced,\nuniversal, and lightweight simulation-to-reality reinforcement learning (RL)\nframework for robust attitude control of UUVs. EasyUUV combines parallelized RL\ntraining with a hybrid control architecture, where a learned policy outputs\nhigh-level attitude corrections executed by an adaptive S-Surface controller. A\nmultimodal LLM is further integrated to adaptively tune controller parameters\nat runtime using visual and textual feedback, enabling training-free adaptation\nto unmodeled dynamics. Also, we have developed a low-cost 6-DoF UUV platform\nand applied an RL policy trained through efficient parallelized simulation.\nExtensive simulation and real-world experiments validate the effectiveness and\noutstanding performance of EasyUUV in achieving robust and adaptive UUV\nattitude control across diverse underwater conditions. The source code is\navailable from the following website: https://360zmem.github.io/easyuuv/"}
{"id": "2510.22164", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22164", "abs": "https://arxiv.org/abs/2510.22164", "authors": ["Jianeng Wang", "Matias Mattamala", "Christina Kassab", "Nived Chebrolu", "Guillaume Burger", "Fabio Elnecave", "Marine Petriaux", "Maurice Fallon"], "title": "LT-Exosense: A Vision-centric Multi-session Mapping System for Lifelong Safe Navigation of Exoskeletons", "comment": "8 pages, 4 figures", "summary": "Self-balancing exoskeletons offer a promising mobility solution for\nindividuals with lower-limb disabilities. For reliable long-term operation,\nthese exoskeletons require a perception system that is effective in changing\nenvironments. In this work, we introduce LT-Exosense, a vision-centric,\nmulti-session mapping system designed to support long-term (semi)-autonomous\nnavigation for exoskeleton users. LT-Exosense extends single-session mapping\ncapabilities by incrementally fusing spatial knowledge across multiple\nsessions, detecting environmental changes, and updating a persistent global\nmap. This representation enables intelligent path planning, which can adapt to\nnewly observed obstacles and can recover previous routes when obstructions are\nremoved. We validate LT-Exosense through several real-world experiments,\ndemonstrating a scalable multi-session map that achieves an average\npoint-to-point error below 5 cm when compared to ground-truth laser scans. We\nalso illustrate the potential application of adaptive path planning in\ndynamically changing indoor environments."}
{"id": "2510.22201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22201", "abs": "https://arxiv.org/abs/2510.22201", "authors": ["Minho Park", "Kinam Kim", "Junha Hyung", "Hyojin Jang", "Hoiyeong Jin", "Jooyeol Yun", "Hojoon Lee", "Jaegul Choo"], "title": "ACG: Action Coherence Guidance for Flow-based VLA models", "comment": null, "summary": "Diffusion and flow matching models have emerged as powerful robot policies,\nenabling Vision-Language-Action (VLA) models to generalize across diverse\nscenes and instructions. Yet, when trained via imitation learning, their high\ngenerative capacity makes them sensitive to noise in human demonstrations:\njerks, pauses, and jitter which reduce action coherence. Reduced action\ncoherence causes instability and trajectory drift during deployment, failures\nthat are catastrophic in fine-grained manipulation where precision is crucial.\nIn this paper, we present Action Coherence Guidance (ACG) for VLA models, a\ntraining-free test-time guidance algorithm that improves action coherence and\nthereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and\nreal-world SO-101 tasks, ACG consistently improves action coherence and boosts\nsuccess rates across diverse manipulation tasks. Code and project page are\navailable at https://github.com/DAVIAN-Robotics/ACG and\nhttps://DAVIAN-Robotics.github.io/ACG , respectively."}
{"id": "2510.22204", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22204", "abs": "https://arxiv.org/abs/2510.22204", "authors": ["Weixian Qian", "Sebastian Schroder", "Yao Deng", "Jiaohong Yao", "Linfeng Liang", "Xiao Cheng", "Richard Han", "Xi Zheng"], "title": "Bridging Perception and Reasoning: Dual-Pipeline Neuro-Symbolic Landing for UAVs in Cluttered Environments", "comment": null, "summary": "Autonomous landing in unstructured (cluttered, uneven, and map-poor)\nenvironments is a core requirement for Unmanned Aerial Vehicles (UAVs), yet\npurely vision-based or deep learning models often falter under covariate shift\nand provide limited interpretability. We propose NeuroSymLand, a neuro-symbolic\nframework that tightly couples two complementary pipelines: (i) an offline\npipeline, where Large Language Models (LLMs) and human-in-the-loop refinement\nsynthesize Scallop code from diverse landing scenarios, distilling\ngeneralizable and verifiable symbolic knowledge; and (ii) an online pipeline,\nwhere a compact foundation-based semantic segmentation model generates\nprobabilistic Scallop facts that are composed into semantic scene graphs for\nreal-time deductive reasoning. This design combines the perceptual strengths of\nlightweight foundation models with the interpretability and verifiability of\nsymbolic reasoning. Node attributes (e.g., flatness, area) and edge relations\n(adjacency, containment, proximity) are computed with geometric routines rather\nthan learned, avoiding the data dependence and latency of train-time graph\nbuilders. The resulting Scallop program encodes landing principles (avoid water\nand obstacles; prefer large, flat, accessible regions) and yields calibrated\nsafety scores with ranked Regions of Interest (ROIs) and human-readable\njustifications. Extensive evaluations across datasets, diverse simulation maps,\nand real UAV hardware show that NeuroSymLand achieves higher accuracy, stronger\nrobustness to covariate shift, and superior efficiency compared with\nstate-of-the-art baselines, while advancing UAV safety and reliability in\nemergency response, surveillance, and delivery missions."}
{"id": "2510.22313", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22313", "abs": "https://arxiv.org/abs/2510.22313", "authors": ["Chen Zhiqiang", "Le Gentil Cedric", "Lin Fuling", "Lu Minghao", "Qiao Qiyuan", "Xu Bowen", "Qi Yuhua", "Lu Peng"], "title": "Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via Spatio-Temporal Normal Analysis", "comment": "8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "This paper addresses the challenge of Lidar-Inertial Odometry (LIO) in\ndynamic environments, where conventional methods often fail due to their\nstatic-world assumptions. Traditional LIO algorithms perform poorly when\ndynamic objects dominate the scenes, particularly in geometrically sparse\nenvironments. Current approaches to dynamic LIO face a fundamental challenge:\naccurate localization requires a reliable identification of static features,\nyet distinguishing dynamic objects necessitates precise pose estimation. Our\nsolution breaks this circular dependency by integrating dynamic awareness\ndirectly into the point cloud registration process. We introduce a novel\ndynamic-aware iterative closest point algorithm that leverages spatio-temporal\nnormal analysis, complemented by an efficient spatial consistency verification\nmethod to enhance static map construction. Experimental evaluations demonstrate\nsignificant performance improvements over state-of-the-art LIO systems in\nchallenging dynamic environments with limited geometric structure. The code and\ndataset are available at https://github.com/thisparticle/btsa."}
{"id": "2510.22336", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22336", "abs": "https://arxiv.org/abs/2510.22336", "authors": ["Bo Yue", "Sheng Xu", "Kui Jia", "Guiliang Liu"], "title": "Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery", "comment": null, "summary": "Humanoid robots represent a central frontier in embodied intelligence, as\ntheir anthropomorphic form enables natural deployment in humans' workspace.\nBrain-body co-design for humanoids presents a promising approach to realizing\nthis potential by jointly optimizing control policies and physical morphology.\nWithin this context, fall recovery emerges as a critical capability. It not\nonly enhances safety and resilience but also integrates naturally with\nlocomotion systems, thereby advancing the autonomy of humanoids. In this paper,\nwe propose RoboCraft, a scalable humanoid co-design framework for fall recovery\nthat iteratively improves performance through the coupled updates of control\npolicy and morphology. A shared policy pretrained across multiple designs is\nprogressively finetuned on high-performing morphologies, enabling efficient\nadaptation without retraining from scratch. Concurrently, morphology search is\nguided by human-inspired priors and optimization algorithms, supported by a\npriority buffer that balances reevaluation of promising candidates with the\nexploration of novel designs. Experiments show that \\ourmethod{} achieves an\naverage performance gain of 44.55% on seven public humanoid robots, with\nmorphology optimization drives at least 40% of improvements in co-designing\nfour humanoid robots, underscoring the critical role of humanoid co-design."}
{"id": "2510.22339", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22339", "abs": "https://arxiv.org/abs/2510.22339", "authors": ["Enyi Wang", "Zhen Deng", "Chuanchuan Pan", "Bingwei He", "Jianwei Zhang"], "title": "Estimating Continuum Robot Shape under External Loading using Spatiotemporal Neural Networks", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "This paper presents a learning-based approach for accurately estimating the\n3D shape of flexible continuum robots subjected to external loads. The proposed\nmethod introduces a spatiotemporal neural network architecture that fuses\nmulti-modal inputs, including current and historical tendon displacement data\nand RGB images, to generate point clouds representing the robot's deformed\nconfiguration. The network integrates a recurrent neural module for temporal\nfeature extraction, an encoding module for spatial feature extraction, and a\nmulti-modal fusion module to combine spatial features extracted from visual\ndata with temporal dependencies from historical actuator inputs. Continuous 3D\nshape reconstruction is achieved by fitting B\\'ezier curves to the predicted\npoint clouds. Experimental validation demonstrates that our approach achieves\nhigh precision, with mean shape estimation errors of 0.08 mm (unloaded) and\n0.22 mm (loaded), outperforming state-of-the-art methods in shape sensing for\nTDCRs. The results validate the efficacy of deep learning-based spatiotemporal\ndata fusion for precise shape estimation under loading conditions."}
{"id": "2510.22370", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22370", "abs": "https://arxiv.org/abs/2510.22370", "authors": ["Seyed Ahmad Hosseini Miangoleh", "Amin Jalal Aghdasian", "Farzaneh Abdollahi"], "title": "BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles", "comment": "https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git", "summary": "In this paper, we propose Bootstrapped Language-Image Pretraining-driven\nFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a\nnovel multimodal reinforcement learning (RL) framework for autonomous\nlane-keeping (LK), in which semantic embeddings generated by a vision-language\nmodel (VLM) are directly fused with geometric states, LiDAR observations, and\nProportional-Integral-Derivative-based (PID) control feedback within the agent\nobservation space. The proposed method lets the agent learn driving rules that\nare aware of their surroundings and easy to understand by combining high-level\nscene understanding from the VLM with low-level control and spatial signals.\nOur architecture brings together semantic, geometric, and control-aware\nrepresentations to make policy learning more robust. A hybrid reward function\nthat includes semantic alignment, LK accuracy, obstacle avoidance, and speed\nregulation helps learning to be more efficient and generalizable. Our method is\ndifferent from the approaches that only use semantic models to shape rewards.\nInstead, it directly embeds semantic features into the state representation.\nThis cuts down on expensive runtime inference and makes sure that semantic\nguidance is always available. The simulation results show that the proposed\nmodel is better at LK stability and adaptability than the best vision-based and\nmultimodal RL baselines in a wide range of difficult driving situations. We\nmake our code publicly available."}
{"id": "2510.22420", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22420", "abs": "https://arxiv.org/abs/2510.22420", "authors": ["Mohammad Ali Labbaf Khaniki", "Fateme Taroodi", "Benyamin Safizadeh"], "title": "A Novel Multi-Timescale Stability-Preserving Hierarchical Reinforcement Learning Controller Framework for Adaptive Control in High-Dimensional Dynamical Systems", "comment": null, "summary": "Controlling high-dimensional stochastic systems, critical in robotics,\nautonomous vehicles, and hyperchaotic systems, faces the curse of\ndimensionality, lacks temporal abstraction, and often fails to ensure\nstochastic stability. To overcome these limitations, this study introduces the\nMulti-Timescale Lyapunov-Constrained Hierarchical Reinforcement Learning\n(MTLHRL) framework. MTLHRL integrates a hierarchical policy within a\nsemi-Markov Decision Process (SMDP), featuring a high-level policy for\nstrategic planning and a low-level policy for reactive control, which\neffectively manages complex, multi-timescale decision-making and reduces\ndimensionality overhead. Stability is rigorously enforced using a neural\nLyapunov function optimized via Lagrangian relaxation and multi-timescale\nactor-critic updates, ensuring mean-square boundedness or asymptotic stability\nin the face of stochastic dynamics. The framework promotes efficient and\nreliable learning through trust-region constraints and decoupled optimization.\nExtensive simulations on an 8D hyperchaotic system and a 5-DOF robotic\nmanipulator demonstrate MTLHRL's empirical superiority. It significantly\noutperforms baseline methods in both stability and performance, recording the\nlowest error indices (e.g., Integral Absolute Error (IAE): 3.912 in\nhyperchaotic control and IAE: 1.623 in robotics), achieving faster convergence,\nand exhibiting superior disturbance rejection. MTLHRL offers a theoretically\ngrounded and practically viable solution for robust control of complex\nstochastic systems."}
{"id": "2510.22448", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.22448", "abs": "https://arxiv.org/abs/2510.22448", "authors": ["Pranup Chhetri", "Alejandro Torrejon", "Sergio Eslava", "Luis J. Manso"], "title": "A short methodological review on social robot navigation benchmarking", "comment": "18 pages, 14 of which references. 3 figures, 2 tables", "summary": "Social Robot Navigation is the skill that allows robots to move efficiently\nin human-populated environments while ensuring safety, comfort, and trust.\nUnlike other areas of research, the scientific community has not yet achieved\nan agreement on how Social Robot Navigation should be benchmarked. This is\nnotably important, as the lack of a de facto standard to benchmark Social Robot\nNavigation can hinder the progress of the field and may lead to contradicting\nconclusions. Motivated by this gap, we contribute with a short review focused\nexclusively on benchmarking trends in the period from January 2020 to July\n2025. Of the 130 papers identified by our search using IEEE Xplore, we analysed\nthe 85 papers that met the criteria of the review. This review addresses the\nmetrics used in the literature for benchmarking purposes, the algorithms\nemployed in such benchmarks, the use of human surveys for benchmarking, and how\nconclusions are drawn from the benchmarking results, when applicable."}
{"id": "2510.22465", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22465", "abs": "https://arxiv.org/abs/2510.22465", "authors": ["Sourabh Karmakar", "Cameron J. Turner"], "title": "Forward Kinematics Solution For A General Stewart Platform Through Iteration Based Simulation", "comment": null, "summary": "This paper presents a method to generate feasible, unique forward-kinematic\nsolutions for a general Stewart platform. This is done by using inverse\nkinematics to obtain valid workspace data and corresponding actuator lengths\nfor the moving platform. For parallel kinematic machines, such as the Stewart\nPlatform, inverse kinematics are straight forward, but the forward kinematics\nare complex and generates multiple solutions due to the closed loop structure\nof the kinematic links. In this research, a simple iterative algorithm has been\nused employing modified Denavit-Hartenberg convention. The outcome is\nencouraging as this method generates a single feasible forward kinematic\nsolution for each valid pose with the solved DH parameters and unlike earlier\nforward kinematics solutions, this unique solution does not need to be manually\nverified. Therefore, the forward kinematic solutions can be used directly for\nfurther calculations without the need for manual pose verification. This\ncapability is essential for the six degree of freedom materials testing system\ndeveloped by the authors in their laboratory. The developed system is aimed at\ncharacterizing additively manufactured materials under complex combined\nmultiple loading conditions. The material characterization is done by enabling\nhigh precision force control on the moving platform via in situ calibration of\nthe as-built kinematics of the Stewart Gough Platform."}
{"id": "2510.22504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22504", "abs": "https://arxiv.org/abs/2510.22504", "authors": ["Ciera McFarland", "Antonio Alvarez", "Sarah Taher", "Nathaniel Hanson", "Margaret McGuinness"], "title": "On Steerability Factors for Growing Vine Robots", "comment": null, "summary": "Vine robots extend their tubular bodies by everting material from the tip,\nenabling navigation in complex environments with a minimalist soft body.\nDespite their promise for field applications, especially in the urban search\nand rescue domain, performance is constrained by the weight of attached sensors\nor tools, as well as other design and control choices. This work investigates\nhow tip load, pressure, length, diameter, and fabrication method shape vine\nrobot steerability--the ability to maneuver with controlled curvature--for\nrobots that steer with series pouch motor-style pneumatic actuators. We conduct\ntwo groups of experiments: (1) studying tip load, chamber pressure, length, and\ndiameter in a robot supporting itself against gravity, and (2) studying\nfabrication method and ratio of actuator to chamber pressure in a robot\nsupported on the ground. Results show that steerability decreases with\nincreasing tip load, is best at moderate chamber pressure, increases with\nlength, and is largely unaffected by diameter. Robots with actuators attached\non their exterior begin curving at low pressure ratios, but curvature saturates\nat high pressure ratios; those with actuators integrated into the robot body\nrequire higher pressure ratios to begin curving but achieve higher curvature\noverall. We demonstrate that robots optimized with these principles outperform\nthose with ad hoc parameters in a mobility task that involves maximizing upward\nand horizontal curvatures."}
{"id": "2510.22524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22524", "abs": "https://arxiv.org/abs/2510.22524", "authors": ["Shenbagaraj Kannapiran", "Elena Oikonomou", "Albert Chu", "Spring Berman", "Theodore P. Pavlic"], "title": "Ant-inspired Walling Strategies for Scalable Swarm Separation: Reinforcement Learning Approaches Based on Finite State Machines", "comment": null, "summary": "In natural systems, emergent structures often arise to balance competing\ndemands. Army ants, for example, form temporary \"walls\" that prevent\ninterference between foraging trails. Inspired by this behavior, we developed\ntwo decentralized controllers for heterogeneous robotic swarms to maintain\nspatial separation while executing concurrent tasks. The first is a\nfinite-state machine (FSM)-based controller that uses encounter-triggered\ntransitions to create rigid, stable walls. The second integrates FSM states\nwith a Deep Q-Network (DQN), dynamically optimizing separation through emergent\n\"demilitarized zones.\" In simulation, both controllers reduce mixing between\nsubgroups, with the DQN-enhanced controller improving adaptability and reducing\nmixing by 40-50% while achieving faster convergence."}
{"id": "2510.22568", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.22568", "abs": "https://arxiv.org/abs/2510.22568", "authors": ["Onur Akgün"], "title": "SPIRAL: Self-Play Incremental Racing Algorithm for Learning in Multi-Drone Competitions", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "This paper introduces SPIRAL (Self-Play Incremental Racing Algorithm for\nLearning), a novel approach for training autonomous drones in multi-agent\nracing competitions. SPIRAL distinctively employs a self-play mechanism to\nincrementally cultivate complex racing behaviors within a challenging, dynamic\nenvironment. Through this self-play core, drones continuously compete against\nincreasingly proficient versions of themselves, naturally escalating the\ndifficulty of competitive interactions. This progressive learning journey\nguides agents from mastering fundamental flight control to executing\nsophisticated cooperative multi-drone racing strategies. Our method is designed\nfor versatility, allowing integration with any state-of-the-art Deep\nReinforcement Learning (DRL) algorithms within its self-play framework.\nSimulations demonstrate the significant advantages of SPIRAL and benchmark the\nperformance of various DRL algorithms operating within it. Consequently, we\ncontribute a versatile, scalable, and self-improving learning framework to the\nfield of autonomous drone racing. SPIRAL's capacity to autonomously generate\nappropriate and escalating challenges through its self-play dynamic offers a\npromising direction for developing robust and adaptive racing strategies in\nmulti-agent environments. This research opens new avenues for enhancing the\nperformance and reliability of autonomous racing drones in increasingly complex\nand competitive scenarios."}
{"id": "2510.22570", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.22570", "abs": "https://arxiv.org/abs/2510.22570", "authors": ["Onur Akgün"], "title": "Curriculum-Based Iterative Self-Play for Scalable Multi-Drone Racing", "comment": "13 pages, 5 figures. This paper is currently under review at the\n  journal Engineering Applications of Artificial Intelligence. Supplementary\n  video: https://drive.google.com/file/d/1k7necen2DgIxaYT2alKK8-b20sE_AyDA/view\n  Source code and models: https://doi.org/10.5281/zenodo.17256943", "summary": "The coordination of multiple autonomous agents in high-speed, competitive\nenvironments represents a significant engineering challenge. This paper\npresents CRUISE (Curriculum-Based Iterative Self-Play for Scalable Multi-Drone\nRacing), a reinforcement learning framework designed to solve this challenge in\nthe demanding domain of multi-drone racing. CRUISE overcomes key scalability\nlimitations by synergistically combining a progressive difficulty curriculum\nwith an efficient self-play mechanism to foster robust competitive behaviors.\nValidated in high-fidelity simulation with realistic quadrotor dynamics, the\nresulting policies significantly outperform both a standard reinforcement\nlearning baseline and a state-of-the-art game-theoretic planner. CRUISE\nachieves nearly double the planner's mean racing speed, maintains high success\nrates, and demonstrates robust scalability as agent density increases. Ablation\nstudies confirm that the curriculum structure is the critical component for\nthis performance leap. By providing a scalable and effective training\nmethodology, CRUISE advances the development of autonomous systems for dynamic,\ncompetitive tasks and serves as a blueprint for future real-world deployment."}
{"id": "2510.22600", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22600", "abs": "https://arxiv.org/abs/2510.22600", "authors": ["Huilin Yin", "Zhaolin Yang", "Linchuan Zhang", "Gerhard Rigoll", "Johannes Betz"], "title": "RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and Low-light Environment Resilience", "comment": "13 pages, 11 figures, under review", "summary": "The reliability of Simultaneous Localization and Mapping (SLAM) is severely\nconstrained in environments where visual inputs suffer from noise and low\nillumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM\nframeworks achieve high-fidelity mapping under clean conditions, they remain\nvulnerable to compounded degradations that degrade mapping and tracking\nperformance. A key observation underlying our work is that the original 3DGS\nrendering pipeline inherently behaves as an implicit low-pass filter,\nattenuating high-frequency noise but also risking over-smoothing. Building on\nthis insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for\nnoise and low-light resilience. The framework integrates three innovations: a\nStructure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples\nrendered appearance, depth, and edge cues; an adaptive tracking objective with\nresidual balancing regularization; and a Contrastive Language-Image Pretraining\n(CLIP)-based enhancement module, selectively activated under compounded\ndegradations to restore semantic and structural fidelity. Comprehensive\nexperiments on Replica, TUM, and real-world sequences show that RoGER-SLAM\nconsistently improves trajectory accuracy and reconstruction quality compared\nwith other 3DGS-SLAM systems, especially under adverse imaging conditions."}
{"id": "2510.22680", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22680", "abs": "https://arxiv.org/abs/2510.22680", "authors": ["Shireen Kudukkil Manchingal", "Armand Amaritei", "Mihir Gohad", "Maryam Sultana", "Julian F. P. Kooij", "Fabio Cuzzolin", "Andrew Bradley"], "title": "Uncertainty-Aware Autonomous Vehicles: Predicting the Road Ahead", "comment": null, "summary": "Autonomous Vehicle (AV) perception systems have advanced rapidly in recent\nyears, providing vehicles with the ability to accurately interpret their\nenvironment. Perception systems remain susceptible to errors caused by\noverly-confident predictions in the case of rare events or out-of-sample data.\nThis study equips an autonomous vehicle with the ability to 'know when it is\nuncertain', using an uncertainty-aware image classifier as part of the AV\nsoftware stack. Specifically, the study exploits the ability of Random-Set\nNeural Networks (RS-NNs) to explicitly quantify prediction uncertainty. Unlike\ntraditional CNNs or Bayesian methods, RS-NNs predict belief functions over sets\nof classes, allowing the system to identify and signal uncertainty clearly in\nnovel or ambiguous scenarios. The system is tested in a real-world autonomous\nracing vehicle software stack, with the RS-NN classifying the layout of the\nroad ahead and providing the associated uncertainty of the prediction.\nPerformance of the RS-NN under a range of road conditions is compared against\ntraditional CNN and Bayesian neural networks, with the RS-NN achieving\nsignificantly higher accuracy and superior uncertainty calibration. This\nintegration of RS-NNs into Robot Operating System (ROS)-based vehicle control\npipeline demonstrates that predictive uncertainty can dynamically modulate\nvehicle speed, maintaining high-speed performance under confident predictions\nwhile proactively improving safety through speed reductions in uncertain\nscenarios. These results demonstrate the potential of uncertainty-aware neural\nnetworks - in particular RS-NNs - as a practical solution for safer and more\nrobust autonomous driving."}
{"id": "2510.22699", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22699", "abs": "https://arxiv.org/abs/2510.22699", "authors": ["Matteo El-Hariry", "Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez"], "title": "RL-AVIST: Reinforcement Learning for Autonomous Visual Inspection of Space Targets", "comment": null, "summary": "The growing need for autonomous on-orbit services such as inspection,\nmaintenance, and situational awareness calls for intelligent spacecraft capable\nof complex maneuvers around large orbital targets. Traditional control systems\noften fall short in adaptability, especially under model uncertainties,\nmulti-spacecraft configurations, or dynamically evolving mission contexts. This\npaper introduces RL-AVIST, a Reinforcement Learning framework for Autonomous\nVisual Inspection of Space Targets. Leveraging the Space Robotics Bench (SRB),\nwe simulate high-fidelity 6-DOF spacecraft dynamics and train agents using\nDreamerV3, a state-of-the-art model-based RL algorithm, with PPO and TD3 as\nmodel-free baselines. Our investigation focuses on 3D proximity maneuvering\ntasks around targets such as the Lunar Gateway and other space assets. We\nevaluate task performance under two complementary regimes: generalized agents\ntrained on randomized velocity vectors, and specialized agents trained to\nfollow fixed trajectories emulating known inspection orbits. Furthermore, we\nassess the robustness and generalization of policies across multiple spacecraft\nmorphologies and mission domains. Results demonstrate that model-based RL\noffers promising capabilities in trajectory fidelity, and sample efficiency,\npaving the way for scalable, retrainable control solutions for future space\noperations"}
{"id": "2510.22738", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22738", "abs": "https://arxiv.org/abs/2510.22738", "authors": ["Wentao Guo", "Wenzeng Zhang"], "title": "SCAL for Pinch-Lifting: Complementary Rotational and Linear Prototypes for Environment-Adaptive Grasping", "comment": "Preliminary version presented at the IROS 2025 CIM Workshop, where it\n  was selected as a Best Demo Award (Finalist) and subsequently received the\n  Best Demo Award after oral presentation", "summary": "This paper presents environment-adaptive pinch-lifting built on a\nslot-constrained adaptive linkage (SCAL) and instantiated in two complementary\nfingers: SCAL-R, a rotational-drive design with an active fingertip that folds\ninward after contact to form an envelope, and SCAL-L, a linear-drive design\nthat passively opens on contact to span wide or weak-feature objects. Both\nfingers convert surface following into an upward lifting branch while\nmaintaining fingertip orientation, enabling thin or low-profile targets to be\nraised from supports with minimal sensing and control. Two-finger grippers are\nfabricated via PLA-based 3D printing. Experiments evaluate (i)\ncontact-preserving sliding and pinch-lifting on tabletops, (ii) ramp\nnegotiation followed by lift, and (iii) handling of bulky objects via active\nenveloping (SCAL-R) or contact-triggered passive opening (SCAL-L). Across\ndozens of trials on small parts, boxes, jars, and tape rolls, both designs\nachieve consistent grasps with limited tuning. A quasi-static analysis provides\nclosed-form fingertip-force models for linear parallel pinching and two-point\nenveloping, offering geometry-aware guidance for design and operation. Overall,\nthe results indicate complementary operating regimes and a practical path to\nrobust, environment-adaptive grasping with simple actuation."}
{"id": "2510.22740", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22740", "abs": "https://arxiv.org/abs/2510.22740", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "title": "Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM", "comment": "IEEE International Symposium on Multi-Robot & Multi-Agent Systems\n  (MRS) 2025", "summary": "We consider the distributed pose-graph optimization (PGO) problem, which is\nfundamental in accurate trajectory estimation in multi-robot simultaneous\nlocalization and mapping (SLAM). Conventional iterative approaches linearize a\nhighly non-convex optimization objective, requiring repeated solving of normal\nequations, which often converge to local minima and thus produce suboptimal\nestimates. We propose a scalable, outlier-robust distributed planar PGO\nframework using Multi-Agent Reinforcement Learning (MARL). We cast distributed\nPGO as a partially observable Markov game defined on local pose-graphs, where\neach action refines a single edge's pose estimate. A graph partitioner\ndecomposes the global pose graph, and each robot runs a recurrent\nedge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gating\nto denoise noisy edges. Robots sequentially refine poses through a hybrid\npolicy that utilizes prior action memory and graph embeddings. After local\ngraph correction, a consensus scheme reconciles inter-robot disagreements to\nproduce a globally consistent estimate. Our extensive evaluations on a\ncomprehensive suite of synthetic and real-world datasets demonstrate that our\nlearned MARL-based actors reduce the global objective by an average of 37.5%\nmore than the state-of-the-art distributed PGO framework, while enhancing\ninference efficiency by at least 6X. We also demonstrate that actor replication\nallows a single learned policy to scale effortlessly to substantially larger\nrobot teams without any retraining. Code is publicly available at\nhttps://github.com/herolab-uga/policies-over-poses."}
{"id": "2510.22754", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22754", "abs": "https://arxiv.org/abs/2510.22754", "authors": ["Chunyu Li", "Shoubin Chen", "Dong Li", "Weixing Xue", "Qingquan Li"], "title": "TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments", "comment": "Accepted by the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025", "summary": "Multi-agent cooperative SLAM often encounters challenges in similar indoor\nenvironments characterized by repetitive structures, such as corridors and\nrooms. These challenges can lead to significant inaccuracies in shared location\nidentification when employing point cloud-based techniques. To mitigate these\nissues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that\nintegrates text semantics and WiFi signal features to enhance location\nidentification and loop closure detection. TWC-SLAM comprises a single-agent\nfront-end odometry module based on FAST-LIO2, a location identification and\nloop closure detection module that leverages text semantics and WiFi features,\nand a global mapping module. The agents are equipped with sensors capable of\ncapturing textual information and detecting WiFi signals. By correlating these\ndata sources, TWC-SLAM establishes a common location, facilitating point cloud\nalignment across different agents' maps. Furthermore, the system employs loop\nclosure detection and optimization modules to achieve global optimization and\ncohesive mapping. We evaluated our approach using an indoor dataset featuring\nsimilar corridors, rooms, and text signs. The results demonstrate that TWC-SLAM\nsignificantly improves the performance of cooperative SLAM systems in complex\nenvironments with repetitive architectural features."}
{"id": "2510.22784", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22784", "abs": "https://arxiv.org/abs/2510.22784", "authors": ["Guangyao Shi", "Yuwei Wu", "Vijay Kumar", "Gaurav S. Sukhatme"], "title": "PIP-LLM: Integrating PDDL-Integer Programming with LLMs for Coordinating Multi-Robot Teams Using Natural Language", "comment": null, "summary": "Enabling robot teams to execute natural language commands requires\ntranslating high-level instructions into feasible, efficient multi-robot plans.\nWhile Large Language Models (LLMs) combined with Planning Domain Description\nLanguage (PDDL) offer promise for single-robot scenarios, existing approaches\nstruggle with multi-robot coordination due to brittle task decomposition, poor\nscalability, and low coordination efficiency.\n  We introduce PIP-LLM, a language-based coordination framework that consists\nof PDDL-based team-level planning and Integer Programming (IP) based\nrobot-level planning. PIP-LLMs first decomposes the command by translating the\ncommand into a team-level PDDL problem and solves it to obtain a team-level\nplan, abstracting away robot assignment. Each team-level action represents a\nsubtask to be finished by the team. Next, this plan is translated into a\ndependency graph representing the subtasks' dependency structure. Such a\ndependency graph is then used to guide the robot-level planning, in which each\nsubtask node will be formulated as an IP-based task allocation problem,\nexplicitly optimizing travel costs and workload while respecting robot\ncapabilities and user-defined constraints. This separation of planning from\nassignment allows PIP-LLM to avoid the pitfalls of syntax-based decomposition\nand scale to larger teams. Experiments across diverse tasks show that PIP-LLM\nimproves plan success rate, reduces maximum and average travel costs, and\nachieves better load balancing compared to state-of-the-art baselines."}
{"id": "2510.22789", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22789", "abs": "https://arxiv.org/abs/2510.22789", "authors": ["Abhijeet M. Kulkarni", "Ioannis Poulakakis", "Guoquan Huang"], "title": "Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning", "comment": null, "summary": "Accurate full-body motion prediction is essential for the safe, autonomous\nnavigation of legged robots, enabling critical capabilities like limb-level\ncollision checking in cluttered environments. Simplified kinematic models often\nfail to capture the complex, closed-loop dynamics of the robot and its\nlow-level controller, limiting their predictions to simple planar motion. To\naddress this, we present a learning-based observer-predictor framework that\naccurately predicts this motion. Our method features a neural observer with\nprovable UUB guarantees that provides a reliable latent state estimate from a\nhistory of proprioceptive measurements. This stable estimate initializes a\ncomputationally efficient predictor, designed for the rapid, parallel\nevaluation of thousands of potential trajectories required by modern\nsampling-based planners. We validated the system by integrating our neural\npredictor into an MPPI-based planner on a Vision 60 quadruped. Hardware\nexperiments successfully demonstrated effective, limb-aware motion planning in\na challenging, narrow passage and over small objects, highlighting our system's\nability to provide a robust foundation for high-performance, collision-aware\nplanning on dynamic robotic platforms."}
{"id": "2510.22821", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22821", "abs": "https://arxiv.org/abs/2510.22821", "authors": ["Ricardo Vega", "Connor Mattson", "Kevin Zhu", "Daniel S. Brown", "Cameron Nowzari"], "title": "Analytical Swarm Chemistry: Characterization and Analysis of Emergent Swarm Behaviors", "comment": "9 pages, 8 figures, 1 table", "summary": "Swarm robotics has potential for a wide variety of applications, but\nreal-world deployments remain rare due to the difficulty of predicting emergent\nbehaviors arising from simple local interactions. Traditional engineering\napproaches design controllers to achieve desired macroscopic outcomes under\nidealized conditions, while agent-based and artificial life studies explore\nemergent phenomena in a bottom-up, exploratory manner. In this work, we\nintroduce Analytical Swarm Chemistry, a framework that integrates concepts from\nengineering, agent-based and artificial life research, and chemistry. This\nframework combines macrostate definitions with phase diagram analysis to\nsystematically explore how swarm parameters influence emergent behavior.\nInspired by concepts from chemistry, the framework treats parameters like\nthermodynamic variables, enabling visualization of regions in parameter space\nthat give rise to specific behaviors. Applying this framework to agents with\nminimally viable capabilities, we identify sufficient conditions for behaviors\nsuch as milling and diffusion and uncover regions of the parameter space that\nreliably produce these behaviors. Preliminary validation on real robots\ndemonstrates that these regions correspond to observable behaviors in practice.\nBy providing a principled, interpretable approach, this framework lays the\ngroundwork for predictable and reliable emergent behavior in real-world swarm\nsystems."}
{"id": "2510.22825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22825", "abs": "https://arxiv.org/abs/2510.22825", "authors": ["Nan Zhang"], "title": "Kinematically Controllable Cable Robots with Reconfigurable End-effectors", "comment": "7 pages, 12 figures, Technical Report", "summary": "To enlarge the translational workspace of cable-driven robots, one common\napproach is to increase the number of cables. However, this introduces two\nchallenges: (1) cable interference significantly reduces the rotational\nworkspace, and (2) the solution of tensions in cables becomes non-unique,\nresulting in difficulties for kinematic control of the robot. In this work, we\ndesign structurally simple reconfigurable end-effectors for cable robots. By\nincorporating a spring, a helical-grooved shaft, and a matching nut, relative\nlinear motions between end-effector components are converted into relative\nrotations, thereby expanding the rotational workspace of the mechanism.\nMeanwhile, a bearing is introduced to provide an additional rotational degree\nof freedom, making the mechanism non-redundant. As a result, the robot's motion\ncan be controlled purely through kinematics without additional tension sensing\nand control."}
{"id": "2510.22892", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22892", "abs": "https://arxiv.org/abs/2510.22892", "authors": ["Jingzehua Xu", "Yangyang Li", "Yangfei Chen", "Guanwen Xie", "Shuai Zhang"], "title": "Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning", "comment": null, "summary": "Robotic arms are increasingly deployed in uncertain environments, yet\nconventional control pipelines often become rigid and brittle when exposed to\nperturbations or incomplete information. Virtual Model Control (VMC) enables\ncompliant behaviors by embedding virtual forces and mapping them into joint\ntorques, but its reliance on fixed parameters and limited coordination among\nvirtual components constrains adaptability and may undermine stability as task\nobjectives evolve. To address these limitations, we propose Adaptive VMC with\nLarge Language Model (LLM)- and Lyapunov-Based Reinforcement Learning (RL),\nwhich preserves the physical interpretability of VMC while supporting\nstability-guaranteed online adaptation. The LLM provides structured priors and\nhigh-level reasoning that enhance coordination among virtual components,\nimprove sample efficiency, and facilitate flexible adjustment to varying task\nrequirements. Complementarily, Lyapunov-based RL enforces theoretical stability\nconstraints, ensuring safe and reliable adaptation under uncertainty. Extensive\nsimulations on a 7-DoF Panda arm demonstrate that our approach effectively\nbalances competing objectives in dynamic tasks, achieving superior performance\nwhile highlighting the synergistic benefits of LLM guidance and\nLyapunov-constrained adaptation."}
{"id": "2510.22917", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22917", "abs": "https://arxiv.org/abs/2510.22917", "authors": ["Zecheng Yin", "Hao Zhao", "Zhen Li"], "title": "HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment", "comment": "under review", "summary": "Objective-oriented navigation(ObjNav) enables robot to navigate to target\nobject directly and autonomously in an unknown environment. Effective\nperception in navigation in unknown environment is critical for autonomous\nrobots. While egocentric observations from RGB-D sensors provide abundant local\ninformation, real-time top-down maps offer valuable global context for ObjNav.\nNevertheless, the majority of existing studies focus on a single source, seldom\nintegrating these two complementary perceptual modalities, despite the fact\nthat humans naturally attend to both. With the rapid advancement of\nVision-Language Models(VLMs), we propose Hybrid Perception Navigation\n(HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding\ncapabilities to jointly perceive both local and global information to enhance\nthe effectiveness and intelligence of navigation in unknown environments. In\nboth massive simulation evaluation and real-world validation, our methods\nachieved state-of-the-art performance against popular baselines. Benefiting\nfrom hybrid perception approach, our method captures richer cues and finds the\nobjects more effectively, by simultaneously leveraging information\nunderstanding from egocentric observations and the top-down map. Our ablation\nstudy further proved that either of the hybrid perception contributes to the\nnavigation performance."}
{"id": "2510.22949", "categories": ["cs.RO", "cs.SY", "eess.SY", "93C10", "I.2.9; I.2.8; J.2"], "pdf": "https://arxiv.org/pdf/2510.22949", "abs": "https://arxiv.org/abs/2510.22949", "authors": ["Benedictus C. G. Cinun", "Tua A. Tamba", "Immanuel R. Santjoko", "Xiaofeng Wang", "Michael A. Gunarso", "Bin Hu"], "title": "End-to-End Design and Validation of a Low-Cost Stewart Platform with Nonlinear Estimation and Control", "comment": "24 pages, journal", "summary": "This paper presents the complete design, control, and experimental validation\nof a low-cost Stewart platform prototype developed as an affordable yet capable\nrobotic testbed for research and education. The platform combines off the shelf\ncomponents with 3D printed and custom fabricated parts to deliver full six\ndegrees of freedom motions using six linear actuators connecting a moving\nplatform to a fixed base. The system software integrates dynamic modeling, data\nacquisition, and real time control within a unified framework. A robust\ntrajectory tracking controller based on feedback linearization, augmented with\nan LQR scheme, compensates for the platform's nonlinear dynamics to achieve\nprecise motion control. In parallel, an Extended Kalman Filter fuses IMU and\nactuator encoder feedback to provide accurate and reliable state estimation\nunder sensor noise and external disturbances. Unlike prior efforts that\nemphasize only isolated aspects such as modeling or control, this work delivers\na complete hardware-software platform validated through both simulation and\nexperiments on static and dynamic trajectories. Results demonstrate effective\ntrajectory tracking and real-time state estimation, highlighting the platform's\npotential as a cost effective and versatile tool for advanced research and\neducational applications."}
{"id": "2510.23003", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23003", "abs": "https://arxiv.org/abs/2510.23003", "authors": ["ZhengKai Huang", "YiKun Wang", "ChenYu Hui", "XiaoCheng"], "title": "An Intelligent Water-Saving Irrigation System Based on Multi-Sensor Fusion and Visual Servoing Control", "comment": null, "summary": "This paper introduces an intelligent water-saving irrigation system designed\nto address critical challenges in precision agriculture, such as inefficient\nwater use and poor terrain adaptability. The system integrates advanced\ncomputer vision, robotic control, and real-time stabilization technologies via\na multi-sensor fusion approach. A lightweight YOLO model, deployed on an\nembedded vision processor (K210), enables real-time plant container detection\nwith over 96% accuracy under varying lighting conditions. A simplified hand-eye\ncalibration algorithm-designed for 'handheld camera' robot arm\nconfigurations-ensures that the end effector can be precisely positioned, with\na success rate exceeding 90%. The active leveling system, driven by the\nSTM32F103ZET6 main control chip and JY901S inertial measurement data, can\nstabilize the irrigation platform on slopes up to 10 degrees, with a response\ntime of 1.8 seconds. Experimental results across three simulated agricultural\nenvironments (standard greenhouse, hilly terrain, complex lighting) demonstrate\na 30-50% reduction in water consumption compared to conventional flood\nirrigation, with water use efficiency exceeding 92% in all test cases."}
{"id": "2510.23016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23016", "abs": "https://arxiv.org/abs/2510.23016", "authors": ["Zhuo Li", "Junjia Liu", "Dianxi Li", "Tao Teng", "Miao Li", "Sylvain Calinon", "Darwin Caldwell", "Fei Chen"], "title": "ManiDP: Manipulability-Aware Diffusion Policy for Posture-Dependent Bimanual Manipulation", "comment": "7 pages, 6 figures, Accepted and published in IROS 2025", "summary": "Recent work has demonstrated the potential of diffusion models in robot\nbimanual skill learning. However, existing methods ignore the learning of\nposture-dependent task features, which are crucial for adapting dual-arm\nconfigurations to meet specific force and velocity requirements in dexterous\nbimanual manipulation. To address this limitation, we propose\nManipulability-Aware Diffusion Policy (ManiDP), a novel imitation learning\nmethod that not only generates plausible bimanual trajectories, but also\noptimizes dual-arm configurations to better satisfy posture-dependent task\nrequirements. ManiDP achieves this by extracting bimanual manipulability from\nexpert demonstrations and encoding the encapsulated posture features using\nRiemannian-based probabilistic models. These encoded posture features are then\nincorporated into a conditional diffusion process to guide the generation of\ntask-compatible bimanual motion sequences. We evaluate ManiDP on six real-world\nbimanual tasks, where the experimental results demonstrate a 39.33$\\%$ increase\nin average manipulation success rate and a 0.45 improvement in task\ncompatibility compared to baseline methods. This work highlights the importance\nof integrating posture-relevant robotic priors into bimanual skill diffusion to\nenable human-like adaptability and dexterity."}
{"id": "2510.23057", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23057", "abs": "https://arxiv.org/abs/2510.23057", "authors": ["Oskar Natan", "Jun Miura"], "title": "Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot Navigation", "comment": "Preprint notice, this manuscript has been submitted to IEEE sensors\n  journal for possible publication", "summary": "We present Seq-DeepIPC, a sequential end-to-end perception-to-control model\nfor legged robot navigation in realworld environments. Seq-DeepIPC advances\nintelligent sensing for autonomous legged navigation by tightly integrating\nmulti-modal perception (RGB-D + GNSS) with temporal fusion and control. The\nmodel jointly predicts semantic segmentation and depth estimation, giving\nricher spatial features for planning and control. For efficient deployment on\nedge devices, we use EfficientNet-B0 as the encoder, reducing computation while\nmaintaining accuracy. Heading estimation is simplified by removing the noisy\nIMU and instead computing the bearing angle directly from consecutive GNSS\npositions. We collected a larger and more diverse dataset that includes both\nroad and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative\nand ablation studies show that sequential inputs improve perception and control\nin our models, while other baselines do not benefit. Seq-DeepIPC achieves\ncompetitive or better results with reasonable model size; although GNSS-only\nheading is less reliable near tall buildings, it is robust in open areas.\nOverall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to\nmore versatile and temporally-aware systems. To support future research, we\nwill release the codes to our GitHub repository at\nhttps://github.com/oskarnatan/Seq-DeepIPC."}
{"id": "2510.23059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23059", "abs": "https://arxiv.org/abs/2510.23059", "authors": ["Yongtong Zhu", "Lei Li", "Iggy Qian", "WenBin Zhou", "Ye Yuan", "Qingdu Li", "Na Liu", "Jianwei Zhang"], "title": "Awakening Facial Emotional Expressions in Human-Robot", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2025). 8 pages, 7 figures, IEEE two-column format", "summary": "The facial expression generation capability of humanoid social robots is\ncritical for achieving natural and human-like interactions, playing a vital\nrole in enhancing the fluidity of human-robot interactions and the accuracy of\nemotional expression. Currently, facial expression generation in humanoid\nsocial robots still relies on pre-programmed behavioral patterns, which are\nmanually coded at high human and time costs. To enable humanoid robots to\nautonomously acquire generalized expressive capabilities, they need to develop\nthe ability to learn human-like expressions through self-training. To address\nthis challenge, we have designed a highly biomimetic robotic face with\nphysical-electronic animated facial units and developed an end-to-end learning\nframework based on KAN (Kolmogorov-Arnold Network) and attention mechanisms.\nUnlike previous humanoid social robots, we have also meticulously designed an\nautomated data collection system based on expert strategies of facial motion\nprimitives to construct the dataset. Notably, to the best of our knowledge,\nthis is the first open-source facial dataset for humanoid social robots.\nComprehensive evaluations indicate that our approach achieves accurate and\ndiverse facial mimicry across different test subjects."}
{"id": "2510.23084", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23084", "abs": "https://arxiv.org/abs/2510.23084", "authors": ["Sunyou Hwang", "Christophe De Wagter", "Bart Remes", "Guido de Croon"], "title": "Breaking the Circle: An Autonomous Control-Switching Strategy for Stable Orographic Soaring in MAVs", "comment": "13 pages, 15 figures", "summary": "Orographic soaring can significantly extend the endurance of micro aerial\nvehicles (MAVs), but circling behavior, arising from control conflicts between\nthe longitudinal and vertical axes, increases energy consumption and the risk\nof divergence. We propose a control switching method, named SAOS: Switched\nControl for Autonomous Orographic Soaring, which mitigates circling behavior by\nselectively controlling either the horizontal or vertical axis, effectively\ntransforming the system from underactuated to fully actuated during soaring.\nAdditionally, the angle of attack is incorporated into the INDI controller to\nimprove force estimation. Simulations with randomized initial positions and\nwind tunnel experiments on two MAVs demonstrate that the SAOS improves position\nconvergence, reduces throttle usage, and mitigates roll oscillations caused by\npitch-roll coupling. These improvements enhance energy efficiency and flight\nstability in constrained soaring environments."}
{"id": "2510.23109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23109", "abs": "https://arxiv.org/abs/2510.23109", "authors": ["Bernhard Rameder", "Hubert Gattringer", "Ronald Naderer", "Andreas Mueller"], "title": "An Automated Tape Laying System Employing a Uniaxial Force Control Device", "comment": "Proceedings ECCM21 - 21st European Conference on Composite Materials,\n  Nantes, France, 7-2024", "summary": "This paper deals with the design of a cost effective automated tape laying\nsystem (ATL system) with integrated uniaxial force control to ensure the\nnecessary compaction forces as well as with an accurate temperature control to\nguarantee the used tape being melted appropriate. It is crucial to control the\nsubstrate and the oncoming tape onto a specific temperature level to ensure an\noptimal consolidation between the different layers of the product. Therefore,\nit takes several process steps from the spooled tape on the coil until it is\nfinally tacked onto the desired mold. The different modules are divided into\nthe tape storage spool, a tape-guiding roller, a tape processing unit, a\nheating zone and the consolidation unit. Moreover, a special robot control\nconcept for testing the ATL system is presented. In contrast to many other\nsystems, with this approach, the tape laying device is spatially fixed and the\nshape is moved accordingly by the robot, which allows for handling of rather\ncompact and complex shapes. The functionality of the subsystems and the taping\nprocess itself was finally approved in experimental results using a carbon\nfiber reinforced HDPE tape."}
{"id": "2510.23119", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23119", "abs": "https://arxiv.org/abs/2510.23119", "authors": ["Yi-Lin Wei", "Zhexi Luo", "Yuhao Lin", "Mu Lin", "Zhizhao Liang", "Shuoyu Chen", "Wei-Shi Zheng"], "title": "OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback", "comment": "Project page: https://isee-laboratory.github.io/OmniDexGrasp/", "summary": "Enabling robots to dexterously grasp and manipulate objects based on human\ncommands is a promising direction in robotics. However, existing approaches are\nchallenging to generalize across diverse objects or tasks due to the limited\nscale of semantic dexterous grasp datasets. Foundation models offer a new way\nto enhance generalization, yet directly leveraging them to generate feasible\nrobotic actions remains challenging due to the gap between abstract model\nknowledge and physical robot execution. To address these challenges, we propose\nOmniDexGrasp, a generalizable framework that achieves omni-capabilities in user\nprompting, dexterous embodiment, and grasping tasks by combining foundation\nmodels with the transfer and control strategies. OmniDexGrasp integrates three\nkey modules: (i) foundation models are used to enhance generalization by\ngenerating human grasp images supporting omni-capability of user prompt and\ntask; (ii) a human-image-to-robot-action transfer strategy converts human\ndemonstrations into executable robot actions, enabling omni dexterous\nembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stable\ngrasp execution. Experiments in simulation and on real robots validate the\neffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexterous\nhands, and further results show its extensibility to dexterous manipulation\ntasks."}
{"id": "2510.23121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23121", "abs": "https://arxiv.org/abs/2510.23121", "authors": ["Bharath Santhanam", "Alex Mitrevski", "Santosh Thoduka", "Sebastian Houben", "Teena Hassan"], "title": "Reliable Robotic Task Execution in the Face of Anomalies", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Learned robot policies have consistently been shown to be versatile, but they\ntypically have no built-in mechanism for handling the complexity of open\nenvironments, making them prone to execution failures; this implies that\ndeploying policies without the ability to recognise and react to failures may\nlead to unreliable and unsafe robot behaviour. In this paper, we present a\nframework that couples a learned policy with a method to detect visual\nanomalies during policy deployment and to perform recovery behaviours when\nnecessary, thereby aiming to prevent failures. Specifically, we train an\nanomaly detection model using data collected during nominal executions of a\ntrained policy. This model is then integrated into the online policy execution\nprocess, so that deviations from the nominal execution can trigger a\nthree-level sequential recovery process that consists of (i) pausing the\nexecution temporarily, (ii) performing a local perturbation of the robot's\nstate, and (iii) resetting the robot to a safe state by sampling from a learned\nexecution success model. We verify our proposed method in two different\nscenarios: (i) a door handle reaching task with a Kinova Gen3 arm using a\npolicy trained in simulation and transferred to the real robot, and (ii) an\nobject placing task with a UFactory xArm 6 using a general-purpose policy\nmodel. Our results show that integrating policy execution with anomaly\ndetection and recovery increases the execution success rate in environments\nwith various anomalies, such as trajectory deviations and adversarial human\ninterventions."}
{"id": "2510.23129", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23129", "abs": "https://arxiv.org/abs/2510.23129", "authors": ["Sabino Francesco Roselli", "Ze Zhang", "Knut Åkesson"], "title": "Combining High Level Scheduling and Low Level Control to Manage Fleets of Mobile Robots", "comment": null, "summary": "The deployment of mobile robots for material handling in industrial\nenvironments requires scalable coordination of large fleets in dynamic\nsettings. This paper presents a two-layer framework that combines high-level\nscheduling with low-level control. Tasks are assigned and scheduled using the\ncompositional algorithm ComSat, which generates time-parameterized routes for\neach robot. These schedules are then used by a distributed Model Predictive\nControl (MPC) system in real time to compute local reference trajectories,\naccounting for static and dynamic obstacles. The approach ensures safe,\ncollision-free operation, and supports rapid rescheduling in response to\ndisruptions such as robot failures or environmental changes. We evaluate the\nmethod in simulated 2D environments with varying road capacities and traffic\nconditions, demonstrating high task completion rates and robust behavior even\nunder congestion. The modular structure of the framework allows for\ncomputational tractability and flexibility, making it suitable for deployment\nin complex, real-world industrial scenarios."}
{"id": "2510.23176", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23176", "abs": "https://arxiv.org/abs/2510.23176", "authors": ["Arnav Sukhija", "Lenart Treven", "Jin Cheng", "Florian Dörfler", "Stelian Coros", "Andreas Krause"], "title": "TARC: Time-Adaptive Robotic Control", "comment": null, "summary": "Fixed-frequency control in robotics imposes a trade-off between the\nefficiency of low-frequency control and the robustness of high-frequency\ncontrol, a limitation not seen in adaptable biological systems. We address this\nwith a reinforcement learning approach in which policies jointly select control\nactions and their application durations, enabling robots to autonomously\nmodulate their control frequency in response to situational demands. We\nvalidate our method with zero-shot sim-to-real experiments on two distinct\nhardware platforms: a high-speed RC car and a quadrupedal robot. Our method\nmatches or outperforms fixed-frequency baselines in terms of rewards while\nsignificantly reducing the control frequency and exhibiting adaptive frequency\ncontrol under real-world conditions."}
{"id": "2510.23204", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23204", "abs": "https://arxiv.org/abs/2510.23204", "authors": ["Giulia Pusceddu", "Giulio Antonio Abbo", "Francesco Rea", "Tony Belpaeme", "Alessandra Sciutti"], "title": "If They Disagree, Will You Conform? Exploring the Role of Robots' Value Awareness in a Decision-Making Task", "comment": null, "summary": "This study investigates whether the opinions of robotic agents are more\nlikely to influence human decision-making when the robots are perceived as\nvalue-aware (i.e., when they display an understanding of human principles). We\ndesigned an experiment in which participants interacted with two Furhat robots\n- one programmed to be Value-Aware and the other Non-Value-Aware - during a\nlabeling task for images representing human values. Results indicate that\nparticipants distinguished the Value-Aware robot from the Non-Value-Aware one.\nAlthough their explicit choices did not indicate a clear preference for one\nrobot over the other, participants directed their gaze more toward the\nValue-Aware robot. Additionally, the Value-Aware robot was perceived as more\nloyal, suggesting that value awareness in a social robot may enhance its\nperceived commitment to the group. Finally, when both robots disagreed with the\nparticipant, conformity occurred in about one out of four trials, and\nparticipants took longer to confirm their responses, suggesting that two robots\nexpressing dissent may introduce hesitation in decision-making. On one hand,\nthis highlights the potential risk that robots, if misused, could manipulate\nusers for unethical purposes. On the other hand, it reinforces the idea that\nsocial robots might encourage reflection in ambiguous situations and help users\navoid scams."}
{"id": "2510.23227", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23227", "abs": "https://arxiv.org/abs/2510.23227", "authors": ["Klaus Zauner", "Josef El Dib", "Hubert Gattringer", "Andreas Mueller"], "title": "Workspace Registration and Collision Detection for Industrial Robotics Applications", "comment": null, "summary": "Motion planning for robotic manipulators relies on precise knowledge of the\nenvironment in order to be able to define restricted areas and to take\ncollision objects into account. To capture the workspace, point clouds of the\nenvironment are acquired using various sensors. The collision objects are\nidentified by region growing segmentation and VCCS algorithm. Subsequently the\npoint clusters are approximated. The aim of the present paper is to compare\ndifferent sensors, to illustrate the process from detection to the finished\ncollision environment and to detect collisions between the robot and this\nenvironment."}
{"id": "2510.23234", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23234", "abs": "https://arxiv.org/abs/2510.23234", "authors": ["Klaus Zauner", "Hubert Gattringer", "Andreas Mueller"], "title": "Optimal Dimensioning of Elastic-Link Manipulators regarding Lifetime Estimation", "comment": "Mechanics Based Design of Structures and Machines, December 2024", "summary": "Resourceful operation and design of robots is key for sustainable industrial\nautomation. This will be enabled by lightweight design along with time and\nenergy optimal control of robotic manipulators. Design and control of such\nsystems is intertwined as the control must take into account inherent\nmechanical compliance while the design must accommodate the dynamic\nrequirements demanded by the control. As basis for such design optimization, a\nmethod for estimating the lifetime of elastic link robotic manipulators is\npresented. This is applied to the geometry optimization of flexible serial\nmanipulators performing pick-and-place operations, where the optimization\nobjective is a combination of overall weight and vibration amplitudes. The\nlifetime estimation draws from a fatigue analysis combining the rainflow\ncounting algorithm and the method of critical cutting plane. Tresca hypothesis\nis used to formulate an equivalent stress, and linear damage accumulation is\nassumed. The final robot geometry is selected from a Pareto front as a tradeoff\nof lifetime and vibration characteristic. The method is illustrated for a three\ndegrees of freedom articulated robotic manipulator."}
{"id": "2510.23258", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23258", "abs": "https://arxiv.org/abs/2510.23258", "authors": ["Riko Yokozawa", "Kentaro Fujii", "Yuta Nomura", "Shingo Murata"], "title": "Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation", "comment": "Preprint version", "summary": "Autonomous robotic navigation in real-world environments requires exploration\nto acquire environmental information as well as goal-directed navigation in\norder to reach specified targets. Active inference (AIF) based on the\nfree-energy principle provides a unified framework for these behaviors by\nminimizing the expected free energy (EFE), thereby combining epistemic and\nextrinsic values. To realize this practically, we propose a deep AIF framework\nthat integrates a diffusion policy as the policy model and a multiple timescale\nrecurrent state-space model (MTRSSM) as the world model. The diffusion policy\ngenerates diverse candidate actions while the MTRSSM predicts their\nlong-horizon consequences through latent imagination, enabling action selection\nthat minimizes EFE. Real-world navigation experiments demonstrated that our\nframework achieved higher success rates and fewer collisions compared with the\nbaselines, particularly in exploration-demanding scenarios. These results\nhighlight how AIF based on EFE minimization can unify exploration and\ngoal-directed navigation in real-world robotic settings."}
{"id": "2510.23286", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23286", "abs": "https://arxiv.org/abs/2510.23286", "authors": ["Jin Huang", "Yingqiang Wang", "Haoda Li", "Zichen Liu", "Zhikun Wang", "Ying Chen"], "title": "Precise Time Delay Measurement and Compensation for Tightly Coupled Underwater SINS/piUSBL Navigation", "comment": null, "summary": "In multi-sensor systems, time synchronization between sensors is a\nsignificant challenge, and this issue is particularly pronounced in underwater\nintegrated navigation systems incorporating acoustic positioning. Such systems\nare highly susceptible to time delay, which can significantly degrade accuracy\nwhen measurement and fusion moments are misaligned. To address this challenge,\nthis paper introduces a tightly coupled navigation framework that integrates a\npassive inverted ultra-short baseline (piUSBL) acoustic positioning system, a\nstrapdown inertial navigation system (SINS), and a depth gauge under precise\ntime synchronization. The framework fuses azimuth and slant range from the\npiUSBL with depth data, thereby avoiding poor vertical-angle observability in\nplanar arrays. A novel delay measurement strategy is introduced, combining\nsynchronized timing with acoustic signal processing, which redefines\ndelay-traditionally an unobservable error-into a quantifiable parameter,\nenabling explicit estimation of both acoustic propagation and system processing\ndelays. Simulations and field experiments confirm the feasibility of the\nproposed method, with delay-compensated navigation reducing RMSE by 40.45% and\nmaximum error by 32.55%. These findings show that precise delay measurement and\ncompensation not only enhance underwater navigation accuracy but also establish\na generalizable framework for acoustic positioning integration, offering\nvaluable insights into time alignment and data fusion in latency-sensitive\nmulti-sensor systems."}
{"id": "2510.23329", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23329", "abs": "https://arxiv.org/abs/2510.23329", "authors": ["Shreya Santra", "Thomas Robbins", "Kazuya Yoshida"], "title": "Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon", "comment": "6 pages, 7 figures. Accepted at IEEE iSpaRo 2025", "summary": "Autonomous navigation in unstructured environments is essential for field and\nplanetary robotics, where robots must efficiently reach goals while avoiding\nobstacles under uncertain conditions. Conventional algorithmic approaches often\nrequire extensive environment-specific tuning, limiting scalability to new\ndomains. Deep Reinforcement Learning (DRL) provides a data-driven alternative,\nallowing robots to acquire navigation strategies through direct interactions\nwith their environment. This work investigates the feasibility of DRL policy\ngeneralization across visually and topographically distinct simulated domains,\nwhere policies are trained in terrestrial settings and validated in a zero-shot\nmanner in extraterrestrial environments. A 3D simulation of an agricultural\nrover is developed and trained using Proximal Policy Optimization (PPO) to\nachieve goal-directed navigation and obstacle avoidance in farmland settings.\nThe learned policy is then evaluated in a lunar-like simulated environment to\nassess transfer performance. The results indicate that policies trained under\nterrestrial conditions retain a high level of effectiveness, achieving close to\n50\\% success in lunar simulations without the need for additional training and\nfine-tuning. This underscores the potential of cross-domain DRL-based policy\ntransfer as a promising approach to developing adaptable and efficient\nautonomous navigation for future planetary exploration missions, with the added\nbenefit of minimizing retraining costs."}
{"id": "2510.23357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23357", "abs": "https://arxiv.org/abs/2510.23357", "authors": ["Shaohan Bian", "Ying Zhang", "Guohui Tian", "Zhiqiang Miao", "Edmond Q. Wu", "Simon X. Yang", "Changchun Hua"], "title": "Large language model-based task planning for service robots: A review", "comment": "Submitted to Biomimetic Intelligence and Robotics for possible\n  publication", "summary": "With the rapid advancement of large language models (LLMs) and robotics,\nservice robots are increasingly becoming an integral part of daily life,\noffering a wide range of services in complex environments. To deliver these\nservices intelligently and efficiently, robust and accurate task planning\ncapabilities are essential. This paper presents a comprehensive overview of the\nintegration of LLMs into service robotics, with a particular focus on their\nrole in enhancing robotic task planning. First, the development and\nfoundational techniques of LLMs, including pre-training, fine-tuning,\nretrieval-augmented generation (RAG), and prompt engineering, are reviewed. We\nthen explore the application of LLMs as the cognitive core-`brain'-of service\nrobots, discussing how LLMs contribute to improved autonomy and\ndecision-making. Furthermore, recent advancements in LLM-driven task planning\nacross various input modalities are analyzed, including text, visual, audio,\nand multimodal inputs. Finally, we summarize key challenges and limitations in\ncurrent research and propose future directions to advance the task planning\ncapabilities of service robots in complex, unstructured domestic environments.\nThis review aims to serve as a valuable reference for researchers and\npractitioners in the fields of artificial intelligence and robotics."}
{"id": "2510.23359", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23359", "abs": "https://arxiv.org/abs/2510.23359", "authors": ["Chungeng Tian", "Ning Hao", "Fenghua He"], "title": "T-ESKF: Transformed Error-State Kalman Filter for Consistent Visual-Inertial Navigation", "comment": "This paper was submitted to IEEE RA-L on July 14, 2024, and accepted\n  on December 18, 2024. This version serves as the 'plus edition' of the\n  accepted paper, incorporating supplementary materials for completeness", "summary": "This paper presents a novel approach to address the inconsistency problem\ncaused by observability mismatch in visual-inertial navigation systems (VINS).\nThe key idea involves applying a linear time-varying transformation to the\nerror-state within the Error-State Kalman Filter (ESKF). This transformation\nensures that \\textrr{the unobservable subspace of the transformed error-state\nsystem} becomes independent of the state, thereby preserving the correct\nobservability of the transformed system against variations in linearization\npoints. We introduce the Transformed ESKF (T-ESKF), a consistent VINS estimator\nthat performs state estimation using the transformed error-state system.\nFurthermore, we develop an efficient propagation technique to accelerate the\ncovariance propagation based on the transformation relationship between the\ntransition and accumulated matrices of T-ESKF and ESKF. We validate the\nproposed method through extensive simulations and experiments, demonstrating\nbetter (or competitive at least) performance compared to state-of-the-art\nmethods. The code is available at github.com/HITCSC/T-ESKF."}
{"id": "2510.23386", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23386", "abs": "https://arxiv.org/abs/2510.23386", "authors": ["Alvaro Paz", "Mahdi Hejrati", "Pauli Mustalahti", "Jouni Mattila"], "title": "Full-Dynamics Real-Time Nonlinear Model Predictive Control of Heavy-Duty Hydraulic Manipulator for Trajectory Tracking Tasks", "comment": "This work has been submitted for possible publication in IEEE", "summary": "Heavy-duty hydraulic manipulators (HHMs) operate under strict physical and\nsafety-critical constraints due to their large size, high power, and complex\nnonlinear dynamics. Ensuring that both joint-level and end-effector\ntrajectories remain compliant with actuator capabilities, such as force,\nvelocity, and position limits, is essential for safe and reliable operation,\nyet remains largely underexplored in real-time control frameworks. This paper\npresents a nonlinear model predictive control (NMPC) framework designed to\nguarantee constraint satisfaction throughout the full nonlinear dynamics of\nHHMs, while running at a real-time control frequency of 1 kHz. The proposed\nmethod combines a multiple-shooting strategy with real-time sensor feedback,\nand is supported by a robust low-level controller based on virtual\ndecomposition control (VDC) for precise joint tracking. Experimental validation\non a full-scale hydraulic manipulator shows that the NMPC framework not only\nenforces actuator constraints at the joint level, but also ensures\nconstraint-compliant motion in Cartesian space for the end-effector. These\nresults demonstrate the method's capability to deliver high-accuracy trajectory\ntracking while strictly respecting safety-critical limits, setting a new\nbenchmark for real-time control in large-scale hydraulic systems."}
{"id": "2510.23495", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23495", "abs": "https://arxiv.org/abs/2510.23495", "authors": ["Chenyang Ma", "Kai Lu", "Ruta Desai", "Xavier Puig", "Andrew Markham", "Niki Trigoni"], "title": "COOPERA: Continual Open-Ended Human-Robot Assistance", "comment": "NeurIPS 2025 (Spotlight); Project Page:\n  https://dannymcy.github.io/coopera/", "summary": "To understand and collaborate with humans, robots must account for individual\nhuman traits, habits, and activities over time. However, most robotic\nassistants lack these abilities, as they primarily focus on predefined tasks in\nstructured environments and lack a human model to learn from. This work\nintroduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot\nAssistance, where simulated humans, driven by psychological traits and\nlong-term intentions, interact with robots in complex environments. By\nintegrating continuous human feedback, our framework, for the first time,\nenables the study of long-term, open-ended human-robot collaboration (HRC) in\ndifferent collaborative tasks across various time-scales. Within COOPERA, we\nintroduce a benchmark and an approach to personalize the robot's collaborative\nactions by learning human traits and context-dependent intents. Experiments\nvalidate the extent to which our simulated humans reflect realistic human\nbehaviors and demonstrate the value of inferring and personalizing to human\nintents for open-ended and long-term HRC. Project Page:\nhttps://dannymcy.github.io/coopera/"}
{"id": "2510.23509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23509", "abs": "https://arxiv.org/abs/2510.23509", "authors": ["Weizheng Wang", "Obi Ike", "Soyun Choi", "Sungeun Hong", "Byung-Cheol Min"], "title": "Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model", "comment": null, "summary": "Social robot navigation increasingly relies on large language models for\nreasoning, path planning, and enabling movement in dynamic human spaces.\nHowever, relying solely on LLMs for planning often leads to unpredictable and\nunsafe behaviors, especially in dynamic human spaces, due to limited physical\ngrounding and weak logical consistency. In this work, we introduce NaviWM, a\nsocially-aware robot Navigation World Model that augments LLM reasoning with a\nstructured world model and a logic-driven chain-of-thought process. NaviWM\nconsists of two main components: (1) a spatial-temporal world model that\ncaptures the positions, velocities, and activities of agents in the\nenvironment, and (2) a deductive reasoning module that guides LLMs through a\nmulti-step, logic-based inference process. This integration enables the robot\nto generate navigation decisions that are both socially compliant and\nphysically safe, under well-defined constraints such as personal space,\ncollision avoidance, and timing. Unlike previous methods based on prompting or\nfine-tuning, NaviWM encodes social norms as first-order logic, enabling\ninterpretable and verifiable reasoning. Experiments show that NaviWM improves\nsuccess rates and reduces social violations, particularly in crowded\nenvironments. These results demonstrate the benefit of combining formal\nreasoning with LLMs for robust social navigation. Additional experimental\ndetails and demo videos for this work can be found at:\nhttps://sites.google.com/view/NaviWM."}
{"id": "2510.23511", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23511", "abs": "https://arxiv.org/abs/2510.23511", "authors": ["Bin Xie", "Erjin Zhou", "Fan Jia", "Hao Shi", "Haoqiang Fan", "Haowei Zhang", "Hebei Li", "Jianjian Sun", "Jie Bin", "Junwen Huang", "Kai Liu", "Kaixin Liu", "Kefan Gu", "Lin Sun", "Meng Zhang", "Peilong Han", "Ruitao Hao", "Ruitao Zhang", "Saike Huang", "Songhan Xie", "Tiancai Wang", "Tianle Liu", "Wenbin Tang", "Wenqi Zhu", "Yang Chen", "Yingfei Liu", "Yizhuang Zhou", "Yu Liu", "Yucheng Zhao", "Yunchao Ma", "Yunfei Wei", "Yuxiang Chen", "Ze Chen", "Zeming Li", "Zhao Wu", "Ziheng Zhang", "Ziming Liu", "Ziwei Yan", "Ziyu Zhang"], "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox", "comment": "Authors are listed in alphabetical order. The official website is\n  located at https://dexbotic.com/. Code is available at\n  https://github.com/Dexmal/dexbotic", "summary": "In this paper, we present Dexbotic, an open-source Vision-Language-Action\n(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA\nresearch service for professionals in the field of embodied intelligence. It\noffers a codebase that supports multiple mainstream VLA policies\nsimultaneously, allowing users to reproduce various VLA methods with just a\nsingle environment setup. The toolbox is experiment-centric, where the users\ncan quickly develop new VLA experiments by simply modifying the Exp script.\nMoreover, we provide much stronger pretrained models to achieve great\nperformance improvements for state-of-the-art VLA policies. Dexbotic will\ncontinuously update to include more of the latest pre-trained foundation models\nand cutting-edge VLA models in the industry."}
{"id": "2510.23512", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23512", "abs": "https://arxiv.org/abs/2510.23512", "authors": ["Martin Huber", "Nicola A. Cavalcanti", "Ayoob Davoodi", "Ruixuan Li", "Christopher E. Mower", "Fabio Carrillo", "Christoph J. Laux", "Francois Teyssere", "Thibault Chandanson", "Antoine Harlé", "Elie Saghbiny", "Mazda Farshad", "Guillaume Morel", "Emmanuel Vander Poorten", "Philipp Fürnstahl", "Sébastien Ourselin", "Christos Bergeles", "Tom Vercauteren"], "title": "Localising under the drape: proprioception in the era of distributed surgical robotic system", "comment": null, "summary": "Despite their mechanical sophistication, surgical robots remain blind to\ntheir surroundings. This lack of spatial awareness causes collisions, system\nrecoveries, and workflow disruptions, issues that will intensify with the\nintroduction of distributed robots with independent interacting arms. Existing\ntracking systems rely on bulky infrared cameras and reflective markers,\nproviding only limited views of the surgical scene and adding hardware burden\nin crowded operating rooms. We present a marker-free proprioception method that\nenables precise localisation of surgical robots under their sterile draping\ndespite associated obstruction of visual cues. Our method solely relies on\nlightweight stereo-RGB cameras and novel transformer-based deep learning\nmodels. It builds on the largest multi-centre spatial robotic surgery dataset\nto date (1.4M self-annotated images from human cadaveric and preclinical in\nvivo studies). By tracking the entire robot and surgical scene, rather than\nindividual markers, our approach provides a holistic view robust to occlusions,\nsupporting surgical scene understanding and context-aware control. We\ndemonstrate an example of potential clinical benefits during in vivo breathing\ncompensation with access to tissue dynamics, unobservable under state of the\nart tracking, and accurately locate in multi-robot systems for future\nintelligent interaction. In addition, and compared with existing systems, our\nmethod eliminates markers and improves tracking visibility by 25%. To our\nknowledge, this is the first demonstration of marker-free proprioception for\nfully draped surgical robots, reducing setup complexity, enhancing safety, and\npaving the way toward modular and autonomous robotic surgery."}
{"id": "2510.23521", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23521", "abs": "https://arxiv.org/abs/2510.23521", "authors": ["Anthony Opipari", "Aravindhan K Krishnan", "Shreekant Gayaka", "Min Sun", "Cheng-Hao Kuo", "Arnie Sen", "Odest Chadwicke Jenkins"], "title": "Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation", "comment": "Accepted in IEEE Robotics and Automation Letters September 2025", "summary": "Remembering where object segments were predicted in the past is useful for\nimproving the accuracy and consistency of class-agnostic video segmentation\nalgorithms. Existing video segmentation algorithms typically use either no\nobject-level memory (e.g. FastSAM) or they use implicit memories in the form of\nrecurrent neural network features (e.g. SAM2). In this paper, we augment both\ntypes of segmentation models using an explicit 3D memory and show that the\nresulting models have more accurate and consistent predictions. For this, we\ndevelop an online 3D Gaussian Splatting (3DGS) technique to store predicted\nobject-level segments generated throughout the duration of a video. Based on\nthis 3DGS representation, a set of fusion techniques are developed, named\nFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve\ntheir respective foundation models' predictions. Ablation experiments are used\nto validate the proposed techniques' design and hyperparameter settings.\nResults from both real-world and simulated benchmarking experiments show that\nmodels which use explicit 3D memories result in more accurate and consistent\npredictions than those which use no memory or only implicit neural network\nmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/"}
{"id": "2510.23571", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23571", "abs": "https://arxiv.org/abs/2510.23571", "authors": ["Yash Jangir", "Yidi Zhang", "Kashu Yamazaki", "Chenyu Zhang", "Kuan-Hsun Tu", "Tsung-Wei Ke", "Lei Ke", "Yonatan Bisk", "Katerina Fragkiadaki"], "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation", "comment": "Website: https://robotarenainf.github.io", "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape."}
{"id": "2510.23576", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23576", "abs": "https://arxiv.org/abs/2510.23576", "authors": ["Anqi Li", "Zhiyong Wang", "Jiazhao Zhang", "Minghan Li", "Yunpeng Qi", "Zhibo Chen", "Zhizheng Zhang", "He Wang"], "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility", "comment": null, "summary": "Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties."}
