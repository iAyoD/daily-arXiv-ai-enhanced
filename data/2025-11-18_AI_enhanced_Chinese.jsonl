{"id": "2511.11616", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11616", "abs": "https://arxiv.org/abs/2511.11616", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance", "comment": "Accepted and scheduled for conference presentation", "summary": "The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\u03b5\\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\\%$ and the Byzantine fault tolerance of $f < n/3$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u6027\u80fd\u3001\u5bf9\u6297\u5f39\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u5c42\u67b6\u6784\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u78b0\u649e\u907f\u514d\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff08O(n\u00b2)\uff09\u4e14\u7f3a\u4e4f\u62dc\u5360\u5ead\u5bb9\u9519\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u5b9e\u65f6\u6027\u80fd\u3001\u5bf9\u6297\u5f39\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u672c\u5730\u5c42\u4f7f\u7528\u5bc6\u96c6\u56fe\u6ce8\u610f\u529b\uff08\u5ef6\u8fdf<10ms\uff09\uff0c\u533a\u57df\u5c42\u4f7f\u7528\u7a00\u758f\u6ce8\u610f\u529b\uff08O(nk)\u590d\u6742\u5ea6\uff09\u548c\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\uff0c\u5168\u5c40\u5c42\u4f7f\u7528\u8f7b\u91cf\u7ea7Hashgraph\u534f\u8bae\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u5dee\u5206\u9690\u79c1\u673a\u5236\u3002", "result": "\u5728500\u67b6\u65e0\u4eba\u673a\u89c4\u6a21\u4e0b\uff0c\u78b0\u649e\u7387<2.0%\uff0c95%\u767e\u5206\u4f4d\u51b3\u7b56\u5ef6\u8fdf<50ms\uff0c\u62dc\u5360\u5ead\u5bb9\u9519f < n/3\uff0c\u9690\u79c1-\u6548\u7528\u6743\u8861\u5f97\u5230\u4f18\u5316\u3002", "conclusion": "\u8be5\u5206\u5c42\u6846\u67b6\u6210\u529f\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6848\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u9ad8\u6548\u3001\u5b89\u5168\u548c\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2511.11634", "categories": ["cs.RO", "cs.CV", "cs.HC", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.11634", "abs": "https://arxiv.org/abs/2511.11634", "authors": ["Michikuni Eguchi", "Takekazu Kitagishi", "Yuichi Hiroi", "Takefumi Hiraki"], "title": "Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding", "comment": "3 pages, 2 figures, 1 table. Presented at SIGGRAPH Asia 2025 Posters (SA Posters '25), December 15-18, 2025, Hong Kong, Hong Kong", "summary": "The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u68b0\u81c2\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u5b8c\u6574\u670d\u88c5\u4e2d\u6536\u96c6\u89e6\u89c9\u6570\u636e\uff0c\u901a\u8fc7\u6a21\u62df\u6307\u5c16\u6ed1\u52a8\u6d4b\u91cf\u6765\u521b\u5efa\u5e26\u8fd0\u52a8\u6807\u7b7e\u7684\u591a\u6a21\u6001\u89e6\u89c9\u6570\u636e\u5e93\u3002", "motivation": "\u670d\u88c5\u7684\u89e6\u89c9\u611f\u53d7\u5bf9\u7a7f\u7740\u8212\u9002\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u6536\u96c6\u6ed1\u52a8\u8fd0\u52a8\u4e2d\u7684\u89e6\u89c9\u6570\u636e\u6765\u63ed\u793a\u5f71\u54cd\u8212\u9002\u5ea6\u7684\u7269\u7406\u7279\u6027\u3002", "method": "\u4f7f\u7528\u673a\u68b0\u81c2\u7cfb\u7edf\u8fdb\u884c\u6ed1\u52a8\u6d4b\u91cf\uff0c\u6a21\u62df\u6307\u5c16\u89e6\u6478\uff0c\u7cbe\u786e\u63a7\u5236\u901f\u5ea6\u548c\u65b9\u5411\uff0c\u521b\u5efa\u5e26\u8fd0\u52a8\u6807\u7b7e\u7684\u591a\u6a21\u6001\u89e6\u89c9\u6570\u636e\u5e93\u3002", "result": "\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u663e\u793a\uff0c\u5305\u542b\u8fd0\u52a8\u76f8\u5173\u53c2\u6570\u63d0\u9ad8\u4e86\u97f3\u9891\u548c\u52a0\u901f\u5ea6\u6570\u636e\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u8fd0\u52a8\u76f8\u5173\u6807\u7b7e\u5728\u8868\u5f81\u670d\u88c5\u89e6\u89c9\u611f\u53d7\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u975e\u7834\u574f\u6027\u7684\u670d\u88c5\u89e6\u89c9\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7ec7\u7269\u611f\u77e5\u548c\u518d\u73b0\u7814\u7a76\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2511.11639", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11639", "abs": "https://arxiv.org/abs/2511.11639", "authors": ["Jie Fan", "Francesco Visentin", "Barbara Mazzolai", "Emanuela Del Dottore"], "title": "Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature", "comment": "This manuscript is a preprint version of the article currently under peer review at International Journal of Computer Vision (IJCV)", "summary": "Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u51e0\u4f55\u65b9\u6cd5\uff0c\u4f7f\u75283D\u5206\u6bb5\u56de\u65cb\u66f2\u7ebf\u6a21\u578b\u6765\u91cd\u5efa\u89e6\u987b\u5728\u673a\u68b0\u523a\u6fc0\u540e\u7684\u5f62\u72b6\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u89e6\u987b\u9876\u7aef\u533a\u57df\u5177\u6709\u66f4\u9ad8\u7684\u54cd\u5e94\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6500\u63f4\u690d\u7269\u5df2\u88ab\u7814\u7a76\u5f88\u957f\u65f6\u95f4\uff0c\u4f46\u63d0\u53d6\u65f6\u95f4\u5f62\u72b6\u53d8\u5316\u3001\u89e6\u53d1\u4e8b\u4ef6\u548c\u63a5\u89e6\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u5efa\u7acb\u8fd9\u79cd\u5173\u7cfb\u6765\u7406\u89e3\u690d\u7269\u751f\u7269\u529b\u5b66\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u51e0\u4f55\u65b9\u6cd5\uff0c\u4f7f\u75283D\u5206\u6bb5\u56de\u65cb\u66f2\u7ebf\u6a21\u578b\u6765\u91cd\u5efa\u89e6\u987b\u5728\u673a\u68b0\u523a\u6fc0\u540e\u7684\u914d\u7f6e\u3002\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u6570\u636e\u9700\u6c42\u5c11\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u548c\u53ef\u89e3\u91ca\u6027\u7684\u4f18\u52bf\u3002", "result": "\u91cd\u5efa\u663e\u793a\u51fa\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u51c6\u786e\u5ea6R2 > 0.99\u3002\u5206\u6790\u663e\u793a\u89e6\u987b\u9876\u7aef\u6bb5\u5177\u6709\u66f4\u9ad8\u7684\u54cd\u5e94\u6027\uff0c\u53ef\u80fd\u5bf9\u5e94\u4e8e\u8be5\u533a\u57df\u66f4\u9ad8\u7684\u654f\u611f\u6027\u548c\u7ec4\u7ec7\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u83b7\u5f97\u690d\u7269\u751f\u7269\u529b\u5b66\u65b0\u89c1\u89e3\u63d0\u4f9b\u4e86\u65b9\u6cd5\u5b66\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u548c\u5f00\u53d1\u53d7\u6500\u63f4\u690d\u7269\u542f\u53d1\u7684\u65b0\u578b\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11740", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11740", "abs": "https://arxiv.org/abs/2511.11740", "authors": ["Haowen Jiang", "Xinyu Huang", "You Lu", "Dingji Wang", "Yuheng Cao", "Chaofeng Sha", "Bihuan Chen", "Keyu Chen", "Xin Peng"], "title": "ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts", "comment": "The paper has been accepted by the Fortieth AAAI Conference on Artificial Intelligence. AAAI 2026", "summary": "Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.", "AI": {"tldr": "\u63d0\u51faExpertAD\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\uff0c\u51cf\u5c11\u78b0\u649e\u738720%\u548c\u63a8\u7406\u5ef6\u8fdf25%\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff1a\u590d\u6742\u573a\u666f\u8bed\u4e49\u4fe1\u606f\u4e30\u5bcc\u4f46\u53ef\u80fd\u6a21\u7cca\u6216\u5608\u6742\uff0c\u591a\u4efb\u52a1\u5e72\u6270\u5f71\u54cd\u89c4\u5212\u6548\u679c\uff0c\u63a8\u7406\u5ef6\u8fdf\u589e\u52a0\u5b89\u5168\u98ce\u9669\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5f15\u5165\u611f\u77e5\u9002\u914d\u5668\u589e\u5f3a\u4efb\u52a1\u5173\u952e\u7279\u5f81\uff0c\u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6700\u5c0f\u5316\u4efb\u52a1\u5e72\u6270\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u78b0\u649e\u7387\u964d\u4f4e20%\uff0c\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c1125%\uff0c\u5728\u7f55\u89c1\u573a\u666f\u548c\u672a\u89c1\u57ce\u5e02\u73af\u5883\u4e2d\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ExpertAD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u3001\u4efb\u52a1\u5e72\u6270\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u51b3\u7b56\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2511.11594", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11594", "abs": "https://arxiv.org/abs/2511.11594", "authors": ["James McCammon"], "title": "TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy", "comment": null, "summary": "Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our \"Assisted Fuzzy\" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.", "AI": {"tldr": "\u63d0\u51fa\u4e86TimeStampEval\u57fa\u51c6\uff0c\u7528\u4e8e\u4ece\u957f\u6587\u672c\u8f6c\u5f55\u4e2d\u68c0\u7d22\u975e\u9010\u5b57\u5f15\u7528\u7684\u7cbe\u786e\u65f6\u95f4\u6233\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e90%\u4ee5\u4e0a\u7684\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u7cca\u5339\u914d\u5728\u8bed\u4e49\u76f8\u540c\u4f46\u8bed\u6cd5\u4e0d\u540c\u7684\u5f15\u7528\u68c0\u7d22\u4e2d\u7684\u5931\u8d25\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5bf9\u9f50\u5b98\u65b9\u4e66\u9762\u8bb0\u5f55\u548c\u8bed\u97f3\u8f6c\u6587\u5b57\u8f6c\u5f55\u65f6\u3002\u5e94\u7528\u573a\u666f\u662f\u81ea\u52a8\u5316\u7684\u957f\u7bc7\u64ad\u5ba2\uff0c\u5c06\u56fd\u4f1a\u8bb0\u5f55\u7247\u6bb5\u7ec4\u88c5\u6210AI\u4e3b\u6301\u7684\u53d9\u8ff0\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528RapidFuzz\u8fdb\u884c\u9884\u8fc7\u6ee4\uff0c\u7136\u540e\u5bf9\u77ed\u7247\u6bb5\u8fdb\u884cLLM\u9a8c\u8bc1\u3002\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\uff08\u5c06\u67e5\u8be2\u653e\u5728\u8f6c\u5f55\u524d\u3001\u4f7f\u7528\u7d27\u51d1\u683c\u5f0f\uff09\u548c\u9002\u5ea6\u63a8\u7406\u9884\u7b97\uff08600-850\u4e2atoken\uff09\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u57282800\u53e5\uff0812\u4e07token\uff09\u8f6c\u5f55\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a\u63d0\u793a\u8bbe\u8ba1\u6bd4\u6a21\u578b\u9009\u62e9\u66f4\u91cd\u8981\uff0c\u51c6\u786e\u7387\u63d0\u9ad83-20\u70b9\uff0ctoken\u6570\u51cf\u5c1130-40%\uff1b\u9002\u5ea6\u63a8\u7406\u9884\u7b97\u5c06\u51c6\u786e\u7387\u4ece37%\u63d0\u5347\u523077%\uff08\u5f31\u8bbe\u7f6e\uff09\u548c90%\u4ee5\u4e0a\uff08\u5f3a\u8bbe\u7f6e\uff09\uff1b\u8f85\u52a9\u6a21\u7cca\u65b9\u6cd5\u5c06\u6a21\u7cca\u5339\u914d\u51c6\u786e\u7387\u63d0\u9ad850\u70b9\uff0c\u5ef6\u8fdf\u51cf\u534a\uff0c\u6bcf\u4e2a\u6b63\u786e\u7ed3\u679c\u7684\u6210\u672c\u964d\u4f4e96%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u8f6c\u5f55\u957f\u5ea6\u3001\u8bcd\u6c47\u6f02\u79fb\u548c\u9886\u57df\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u572810\u4e2a\u8f6c\u5f55\uff085\u4e07-90\u4e07token\uff0c1989-2025\u5e74\uff09\u4e0a\u4fdd\u630195-100%\u7684\u62d2\u7edd\u51c6\u786e\u7387\uff0c\u9002\u7528\u4e8e\u975e\u9010\u5b57\u65f6\u95f4\u6233\u68c0\u7d22\u4efb\u52a1\u3002"}}
{"id": "2511.11777", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11777", "abs": "https://arxiv.org/abs/2511.11777", "authors": ["Vinit Mehta", "Charu Sharma", "Karthick Thiyagarajan"], "title": "Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review", "comment": "45 pages, 15 figures, MDPI Sensors Journal", "summary": "With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u5206\u6790\u4e86LLMs\u4e0e3D\u89c6\u89c9\u878d\u5408\u5728\u673a\u5668\u4eba\u611f\u77e5\u6280\u672f\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\u3001\u5e94\u7528\u548c\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u573a\u666f\u7406\u89e3\u3001\u6587\u672c\u52303D\u751f\u6210\u3001\u7269\u4f53\u5b9a\u4f4d\u548c\u591a\u6a21\u6001\u96c6\u6210\u7b49\u5173\u952e\u6280\u672f\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e3D\u89c6\u89c9\u76f8\u7ed3\u5408\u6b63\u5728\u6210\u4e3a\u63d0\u5347\u673a\u5668\u4eba\u611f\u77e5\u80fd\u529b\u7684\u53d8\u9769\u6027\u65b9\u6cd5\uff0c\u65e8\u5728\u5f25\u5408\u8bed\u8a00\u667a\u80fd\u4e0e\u7a7a\u95f4\u611f\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u9996\u5148\u4ecb\u7ecdLLMs\u548c3D\u6570\u636e\u8868\u793a\u7684\u57fa\u7840\u539f\u7406\uff0c\u6df1\u5165\u5206\u6790\u673a\u5668\u4eba\u5173\u952e\u76843D\u611f\u77e5\u6280\u672f\uff0c\u7136\u540e\u63a2\u8ba8\u573a\u666f\u7406\u89e3\u3001\u6587\u672c\u52303D\u751f\u6210\u3001\u7269\u4f53\u5b9a\u4f4d\u548c\u5177\u8eab\u667a\u80fd\u4f53\u7b49\u5173\u952e\u8fdb\u5c55\uff0c\u5305\u62ec\u96f6\u6837\u672c3D\u5206\u5272\u3001\u52a8\u6001\u573a\u666f\u5408\u6210\u548c\u8bed\u8a00\u5f15\u5bfc\u64cd\u4f5c\u7b49\u524d\u6cbf\u6280\u672f\u3002", "result": "\u8bba\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u591a\u6a21\u6001LLMs\u5982\u4f55\u6574\u54083D\u6570\u636e\u4e0e\u89e6\u89c9\u3001\u542c\u89c9\u548c\u70ed\u8f93\u5165\uff0c\u589e\u5f3a\u73af\u5883\u7406\u89e3\u548c\u673a\u5668\u4eba\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u6574\u7406\u4e86\u4e13\u95e8\u9488\u5bf93D-\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u786e\u5b9a\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u6a21\u578b\u67b6\u6784\u3001\u589e\u5f3a\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u66f4\u667a\u80fd\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u81ea\u4e3b\u7684\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.11793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11793", "abs": "https://arxiv.org/abs/2511.11793", "authors": ["MiroMind Team", "Song Bai", "Lidong Bing", "Carson Chen", "Guanzheng Chen", "Yuntao Chen", "Zhe Chen", "Ziyi Chen", "Jifeng Dai", "Xuan Dong", "Yue Deng", "Yunjie Fu", "Junqi Ge", "Chenxia Han", "Tammy Huang", "Zhenhang Huang", "Jerry Jiao", "Shilei Jiang", "Tianyu Jiao", "Xiaoqi Jian", "Lei Lei", "Ruilin Li", "Ryan Luo", "Tiantong Li", "Xiang Lin", "Ziyuan Liu", "Zhiqi Li", "Jie Ni", "Qiang Ren", "Pax Sun", "Shiqian Su", "Chenxin Tao", "Bin Wang", "Hellen Wang", "Haonan Wang", "James Wang", "Jin Wang", "Jojo Wang", "Letian Wang", "Shizun Wang", "Weizhi Wang", "Zixuan Wang", "Jinfan Xu", "Sen Xing", "Chenyu Yang", "Hai Ye", "Jiaheng Yu", "Yue Yu", "Muyan Zhong", "Tianchen Zhao", "Xizhou Zhu", "Yanpeng Zhou", "Yifan Zhang", "Zhi Zhu"], "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "comment": "Technical Report", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "AI": {"tldr": "MiroThinker v1.0\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6269\u5c55\u4f5c\u4e3a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u7b2c\u4e09\u7ef4\u5ea6\uff0c\u80fd\u591f\u5728256K\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u6267\u884c\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u6216\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u800cMiroThinker\u63a2\u7d22\u5728\u6a21\u578b\u5c42\u9762\u8fdb\u884c\u4ea4\u4e92\u6269\u5c55\uff0c\u901a\u8fc7\u66f4\u6df1\u5165\u548c\u9891\u7e41\u7684\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u4ea4\u4e92\u6269\u5c55\uff0c\u6a21\u578b\u80fd\u591f\u5229\u7528\u73af\u5883\u53cd\u9988\u548c\u5916\u90e8\u4fe1\u606f\u83b7\u53d6\u6765\u7ea0\u6b63\u9519\u8bef\u548c\u4f18\u5316\u8f68\u8ff9\uff0c\u652f\u6301\u6df1\u5ea6\u591a\u8f6e\u63a8\u7406\u3002", "result": "72B\u53d8\u4f53\u5728GAIA\u3001HLE\u3001BrowseComp\u548cBrowseComp-ZH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523081.9%\u300137.7%\u300147.1%\u548c55.6%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u5148\u524d\u5f00\u6e90\u4ee3\u7406\u5e76\u63a5\u8fd1\u5546\u4e1a\u5bf9\u5e94\u7269\u3002", "conclusion": "\u4ea4\u4e92\u6269\u5c55\u5c55\u73b0\u51fa\u4e0e\u6a21\u578b\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7c7b\u4f3c\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u6210\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\u7684\u7b2c\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002"}}
{"id": "2511.11840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11840", "abs": "https://arxiv.org/abs/2511.11840", "authors": ["Shuangyu Xie", "Kaiyuan Chen", "Wenjing Chen", "Chengyuan Qian", "Christian Juette", "Liu Ren", "Dezhen Song", "Ken Goldberg"], "title": "LAVQA: A Latency-Aware Visual Question Answering Framework for Shared Autonomy in Self-Driving Vehicles", "comment": null, "summary": "When uncertainty is high, self-driving vehicles may halt for safety and benefit from the access to remote human operators who can provide high-level guidance. This paradigm, known as {shared autonomy}, enables autonomous vehicle and remote human operators to jointly formulate appropriate responses. To address critical decision timing with variable latency due to wireless network delays and human response time, we present LAVQA, a latency-aware shared autonomy framework that integrates Visual Question Answering (VQA) and spatiotemporal risk visualization. LAVQA augments visual queries with Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty. It enables remote operator to observe as the vehicle safety regions vary over time in the presence of dynamic obstacles and delayed responses. Closed-loop simulations in CARLA, the de-facto standard for autonomous vehicle simulator, suggest that that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.", "AI": {"tldr": "LAVQA\u662f\u4e00\u4e2a\u5ef6\u8fdf\u611f\u77e5\u7684\u5171\u4eab\u81ea\u6cbb\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\u548c\u65f6\u7a7a\u98ce\u9669\u53ef\u89c6\u5316\u6765\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u7f51\u7edc\u5ef6\u8fdf\u548c\u4eba\u7c7b\u54cd\u5e94\u65f6\u95f4\u95ee\u9898\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387", "motivation": "\u5f53\u4e0d\u786e\u5b9a\u6027\u8f83\u9ad8\u65f6\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u53ef\u80fd\u9700\u8981\u8fdc\u7a0b\u4eba\u7c7b\u64cd\u4f5c\u5458\u63d0\u4f9b\u9ad8\u7ea7\u6307\u5bfc\uff0c\u4f46\u65e0\u7ebf\u7f51\u7edc\u5ef6\u8fdf\u548c\u4eba\u7c7b\u54cd\u5e94\u65f6\u95f4\u4f1a\u5f71\u54cd\u5173\u952e\u51b3\u7b56\u65f6\u673a", "method": "\u63d0\u51faLAVQA\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u95ee\u7b54\u548c\u65f6\u7a7a\u98ce\u9669\u53ef\u89c6\u5316\uff0c\u4f7f\u7528LICOM\uff08\u5ef6\u8fdf\u8bf1\u5bfc\u78b0\u649e\u5730\u56fe\uff09\u52a8\u6001\u8868\u793a\u65f6\u95f4\u5ef6\u8fdf\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u7684\u95ed\u73af\u4eff\u771f\u8868\u660e\uff0c\u4e0e\u4e0d\u8003\u8651\u5ef6\u8fdf\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cLAVQA\u80fd\u5c06\u78b0\u649e\u7387\u964d\u4f4e8\u500d\u4ee5\u4e0a", "conclusion": "LAVQA\u901a\u8fc7\u5ef6\u8fdf\u611f\u77e5\u7684\u5171\u4eab\u81ea\u6cbb\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5ef6\u8fdf\u5e26\u6765\u7684\u5b89\u5168\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5b89\u5168\u6027"}}
{"id": "2511.11810", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11810", "abs": "https://arxiv.org/abs/2511.11810", "authors": ["Bertram H\u00f8jer"], "title": "On the Notion that Language Models Reason", "comment": "Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025", "summary": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u8bed\u8a00\u6a21\u578b\u5e76\u975e\u771f\u6b63\u7684\u63a8\u7406\u8005\uff0c\u800c\u662f\u7edf\u8ba1\u6a21\u5f0f\u5339\u914d\u5668\uff0c\u5176\u63a8\u7406\u5f0f\u8f93\u51fa\u6e90\u4e8e\u5b66\u4e60\u5230\u7684\u7edf\u8ba1\u89c4\u5f8b\u800c\u975e\u903b\u8f91\u673a\u5236\u3002", "motivation": "\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u6f84\u6e05\u5f53\u524dNLP\u9886\u57df\u5bf9\"\u63a8\u7406\"\u6982\u5ff5\u7684\u4e0d\u4e00\u81f4\u4f7f\u7528\u3002", "method": "\u5c06\u57fa\u4e8etransformer\u7684\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u5b9e\u73b0\u9690\u5f0f\u6709\u9650\u9636\u9a6c\u5c14\u53ef\u592b\u6838\u7684\u7cfb\u7edf\uff0c\u5206\u6790\u5176\u4fe1\u606f\u5904\u7406\u548c\u751f\u6210\u673a\u5236\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5f0f\u8f93\u51fa\u5bf9\u5e94\u5b66\u4e60\u6838\u4e2d\u7684\u7edf\u8ba1\u89c4\u5f8b\u6027\u548c\u8fd1\u4f3c\u7edf\u8ba1\u4e0d\u53d8\u6027\uff0c\u800c\u975e\u663e\u5f0f\u903b\u8f91\u673a\u5236\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u662f\u7edf\u8ba1\u6a21\u5f0f\u5339\u914d\u5668\u800c\u975e\u771f\u6b63\u63a8\u7406\u8005\uff0c\u8fd9\u4e00\u533a\u5206\u5bf9\u8bc4\u4f30\u5176\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.11845", "categories": ["cs.RO", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11845", "abs": "https://arxiv.org/abs/2511.11845", "authors": ["K. A. I. N Jayarathne", "R. M. N. M. Rathnayaka", "D. P. S. S. Peiris"], "title": "Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture", "comment": "6 pages, 2 figures", "summary": "Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210SLAM\u4e0eSoar\u8ba4\u77e5\u67b6\u6784\u7684\u81ea\u4e3b\u6c34\u4e0b\u8ba4\u77e5\u7cfb\u7edf(AUCS)\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u548c\u8ba4\u77e5\u63a8\u7406\u6a21\u5757\u5b9e\u73b0\u590d\u6742\u6d77\u6d0b\u73af\u5883\u4e0b\u7684\u81ea\u9002\u5e94\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u6df1\u6d77\u63a2\u7d22\u4e2d\u7684\u8ff7\u5931\u65b9\u5411\u3001\u901a\u4fe1\u4e2d\u65ad\u548c\u5bfc\u822a\u5931\u8d25\u7b49\u6311\u6218\uff0c\u63d0\u5347\u6c34\u4e0b\u81ea\u4e3b\u822a\u884c\u5668\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u878d\u5408SONAR\u3001LiDAR\u3001IMU\u548cDVL\u7b49\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u7ed3\u5408Soar\u8ba4\u77e5\u67b6\u6784\u7684\u611f\u77e5\u3001\u6ce8\u610f\u3001\u89c4\u5212\u548c\u5b66\u4e60\u6a21\u5757\uff0c\u5b9e\u73b0\u8bed\u4e49\u7406\u89e3\u3001\u81ea\u9002\u5e94\u4f20\u611f\u5668\u7ba1\u7406\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u5b66\u4e60\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u533a\u5206\u52a8\u6001\u548c\u9759\u6001\u7269\u4f53\uff0c\u51cf\u5c11\u9519\u8bef\u95ed\u73af\u68c0\u6d4b\uff0c\u589e\u5f3a\u957f\u671f\u5730\u56fe\u4e00\u81f4\u6027\uff0c\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u611f\u77e5-\u8ba4\u77e5-\u884c\u52a8-\u5b66\u4e60\u5faa\u73af\u3002", "conclusion": "\u4e3a\u4e0b\u4e00\u4ee3\u8ba4\u77e5\u6f5c\u6c34\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u9ad8\u4e86\u6df1\u6d77\u63a2\u7d22\u7684\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u81ea\u4e3b\u6027\u3002"}}
{"id": "2511.11821", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11821", "abs": "https://arxiv.org/abs/2511.11821", "authors": ["Hong-Jun Yoon", "Faisal Ashraf", "Thomas A. Ruggles", "Debjani Singh"], "title": "Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis", "comment": "18 pages, zero figures, Preprint submitted to Environmental Modeling and Software", "summary": "Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.\n  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\\% F1 through appropriate validation, while smaller models plateau at 51\\%. Large-scale models approach 77\\% F1 but require enterprise infrastructure.\n  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.\n  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.", "AI": {"tldr": "\u8bc4\u4f30\u4e867\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b(0.6B-70B\u53c2\u6570)\u5728\u6c34\u7535\u8bb8\u53ef\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u4e2d\u7684\u6027\u80fd-\u8d44\u6e90\u6743\u8861\uff0c\u53d1\u73b014B\u53c2\u6570\u662f\u6709\u6548\u9a8c\u8bc1\u7684\u9608\u503c\uff0c\u5c0f\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u76d1\u7ba1\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u4e2d\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u5b9e\u8bc1\u6307\u5bfc\u3002", "method": "\u5728\u6c34\u7535\u8bb8\u53ef\u6587\u6863\u4e0a\u8bc4\u4f307\u4e2a\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5206\u6790\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3001\u6027\u80fd\u8868\u73b0\u548c\u5e7b\u89c9\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u51fa14B\u53c2\u6570\u9608\u503c\uff0c\u6d88\u8d39\u7ea7\u6a21\u578b\u53ef\u8fbe64% F1\uff0c\u5c0f\u6a21\u578b\u4ec551%\uff0c\u5927\u89c4\u6a21\u6a21\u578b\u8fbe77%\u4f46\u9700\u4f01\u4e1a\u57fa\u7840\u8bbe\u65bd\uff1b\u53d1\u73b0\u5c0f\u6a21\u578b\u5b8c\u7f8e\u53ec\u56de\u7387\u53cd\u800c\u8868\u660e\u63d0\u53d6\u5931\u8d25\u7684\u7cfb\u7edf\u6027\u5e7b\u89c9\u6a21\u5f0f\u3002", "conclusion": "\u5efa\u7acb\u4e86\u9996\u4e2a\u76d1\u7ba1\u80cc\u666f\u4e0b\u5f00\u6e90\u4fe1\u606f\u63d0\u53d6\u7684\u8d44\u6e90-\u6027\u80fd\u6620\u5c04\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u8bc1\u636e\u57fa\u7840\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u6c34\u7535\u5408\u89c4\u6709\u76f4\u63a5\u4ef7\u503c\uff0c\u53c2\u6570\u7f29\u653e\u6548\u5e94\u7684\u89c1\u89e3\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u3002"}}
{"id": "2511.11931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11931", "abs": "https://arxiv.org/abs/2511.11931", "authors": ["Saida Liu", "Nikolay Atanasov", "Shumon Koga"], "title": "MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy", "comment": "14 pages, 3 figures. Submitted to L4DC 2026", "summary": "This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.", "AI": {"tldr": "MATT-Diff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7b56\u7565\u7684\u591a\u6a21\u6001\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u63a2\u7d22\u3001\u4e13\u7528\u8ddf\u8e2a\u548c\u76ee\u6807\u91cd\u6355\u83b7\u7b49\u591a\u79cd\u884c\u4e3a\u6a21\u5f0f\uff0c\u65e0\u9700\u76ee\u6807\u6570\u91cf\u3001\u72b6\u6001\u6216\u52a8\u6001\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u6709\u6548\u7684\u76ee\u6807\u8ddf\u8e2a\u9700\u8981\u5728\u63a2\u7d22\u672a\u68c0\u6d4b\u6216\u4e22\u5931\u76ee\u6807\u4e0e\u8ddf\u8e2a\u5df2\u68c0\u6d4b\u4f46\u4e0d\u786e\u5b9a\u76ee\u6807\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u8fd9\u4e9b\u4e0d\u540c\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u4e13\u5bb6\u89c4\u5212\u5668\u751f\u6210\u6f14\u793a\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u89c6\u89c9\u53d8\u6362\u5668\u7684\u63a7\u5236\u7b56\u7565\u8fdb\u884c\u81ea\u6211\u4e2d\u5fc3\u5730\u56fe\u6807\u8bb0\u5316\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u53ef\u53d8\u76ee\u6807\u4f30\u8ba1\uff0c\u5e76\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4ee5\u751f\u6210\u591a\u6a21\u6001\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u8bc4\u4f30\u663e\u793aMATT-Diff\u5728\u591a\u79cd\u76ee\u6807\u8fd0\u52a8\u573a\u666f\u4e0b\u4f18\u4e8e\u4e13\u5bb6\u548c\u884c\u4e3a\u514b\u9686\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u76ee\u6807\u8ddf\u8e2a\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "MATT-Diff\u901a\u8fc7\u6269\u6563\u7b56\u7565\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5904\u7406\u63a2\u7d22\u3001\u8ddf\u8e2a\u548c\u91cd\u6355\u83b7\u7b49\u590d\u6742\u884c\u4e3a\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.11829", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11829", "abs": "https://arxiv.org/abs/2511.11829", "authors": ["Mihir Gupte", "Ramesh S"], "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification", "comment": "To be submitted for publication", "summary": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u9a8c\u8bc1LLM\u751f\u6210\u7684\u8f93\u51fa\u4e0e\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u7684\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u903b\u8f91\u9a8c\u8bc1\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6b63\u5f0f\u65b9\u6cd5\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u8f93\u51fa\u7684\u51c6\u786e\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5668\u5bf9LLM\u751f\u6210\u7684\u8f93\u51fa\u8fdb\u884c\u9a8c\u8bc1\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff1a1\uff09\u9a8c\u8bc1\u4e0d\u540c\u8868\u8ff0\u7684\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u7684\u903b\u8f91\u7b49\u4ef7\u6027\uff1b2\uff09\u8bc6\u522b\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u4e0eLLM\u751f\u6210\u8f93\u51fa\u4e4b\u95f4\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u81ea\u52a8\u5f62\u5f0f\u5316\u5668\u6210\u529f\u8bc6\u522b\u51fa\u4e24\u4e2a\u4e0d\u540c\u8868\u8ff0\u7684\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u5728\u903b\u8f91\u4e0a\u662f\u7b49\u4ef7\u7684\uff1b\u5728\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u8bc6\u522b\u51fa\u7ed9\u5b9a\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u4e0eLLM\u751f\u6210\u8f93\u51fa\u4e4b\u95f4\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u867d\u7136\u7814\u7a76\u6709\u9650\uff0c\u4f46\u81ea\u52a8\u5f62\u5f0f\u5316\u5728\u786e\u4fddLLM\u751f\u6210\u8f93\u51fa\u7684\u4fdd\u771f\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11958", "abs": "https://arxiv.org/abs/2511.11958", "authors": ["Derek Chen", "Zoe Samuels", "Lizzie Peiros", "Sujaan Mukherjee", "Michael C. Yip"], "title": "Characterization and Evaluation of Screw-Based Locomotion Across Aquatic, Granular, and Transitional Media", "comment": null, "summary": "Screw-based propulsion systems offer promising capabilities for amphibious mobility, yet face significant challenges in optimizing locomotion across water, granular materials, and transitional environments. This study presents a systematic investigation into the locomotion performance of various screw configurations in media such as dry sand, wet sand, saturated sand, and water. Through a principles-first approach to analyze screw performance, it was found that certain parameters are dominant in their impact on performance. Depending on the media, derived parameters inspired from optimizing heat sink design help categorize performance within the dominant design parameters. Our results provide specific insights into screw shell design and adaptive locomotion strategies to enhance the performance of screw-based propulsion systems for versatile amphibious applications.", "AI": {"tldr": "\u5bf9\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u5728\u4e0d\u540c\u4ecb\u8d28\u4e2d\u7684\u8fd0\u52a8\u6027\u80fd\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u53d1\u73b0\u7279\u5b9a\u53c2\u6570\u5bf9\u6027\u80fd\u6709\u4e3b\u5bfc\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u70ed\u6c89\u8bbe\u8ba1\u4f18\u5316\u601d\u8def\u63d0\u51fa\u4e86\u6027\u80fd\u5206\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u5728\u5b9e\u73b0\u6c34\u9646\u4e24\u6816\u673a\u52a8\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u4f18\u5316\u6c34\u3001\u9897\u7c92\u6750\u6599\u548c\u8fc7\u6e21\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u6027\u80fd\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u539f\u7406\u4f18\u5148\u65b9\u6cd5\u5206\u6790\u87ba\u65cb\u6027\u80fd\uff0c\u7814\u7a76\u4e0d\u540c\u87ba\u65cb\u914d\u7f6e\u5728\u5e72\u6c99\u3001\u6e7f\u6c99\u3001\u9971\u548c\u6c99\u548c\u6c34\u7b49\u4ecb\u8d28\u4e2d\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u67d0\u4e9b\u53c2\u6570\u5728\u4e0d\u540c\u4ecb\u8d28\u4e2d\u5bf9\u6027\u80fd\u6709\u4e3b\u5bfc\u5f71\u54cd\uff0c\u57fa\u4e8e\u70ed\u6c89\u8bbe\u8ba1\u4f18\u5316\u7684\u884d\u751f\u53c2\u6570\u6709\u52a9\u4e8e\u5728\u4e3b\u5bfc\u8bbe\u8ba1\u53c2\u6570\u5185\u5bf9\u6027\u80fd\u8fdb\u884c\u5206\u7c7b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u87ba\u65cb\u58f3\u4f53\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u8fd0\u52a8\u7b56\u7565\u63d0\u4f9b\u4e86\u5177\u4f53\u89c1\u89e3\uff0c\u53ef\u63d0\u5347\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u5728\u591a\u529f\u80fd\u6c34\u9646\u4e24\u6816\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11857", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11857", "abs": "https://arxiv.org/abs/2511.11857", "authors": ["Taimur Khan", "Ramoza Ahsan", "Mohib Hameed"], "title": "Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection", "comment": "18 pages", "summary": "Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u7535\u5f71\u5267\u672c\u60c5\u611f\u5f27\u7ebf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u8bcd\u5178\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u5e76\u4f7f\u7528\u805a\u7c7b\u65b9\u6cd5\u5bf9\u76f8\u4f3c\u60c5\u611f\u6a21\u5f0f\u8fdb\u884c\u5206\u7ec4\uff0c\u5e2e\u52a9\u7528\u6237\u9009\u62e9\u6545\u4e8b\u3002", "motivation": "\u6545\u4e8b\u7406\u89e3\u548c\u5206\u6790\u662f\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u6311\u6218\u9886\u57df\uff0c\u9700\u8981\u6df1\u5ea6\u8ba1\u7b97\u8bed\u4e49\u8868\u793a\u548c\u53e5\u6cd5\u5904\u7406\u3002\u5927\u91cf\u53d9\u4e8b\u6570\u636e\u9700\u8981\u81ea\u52a8\u5316\u8bed\u4e49\u5206\u6790\u800c\u975e\u624b\u52a8\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eNRC-VAD\u6570\u636e\u96c6\u7684Valence\u3001Arousal\u548cDominance\u5206\u6570\u6784\u5efa\u7684\u81ea\u5b9a\u4e49\u8bcd\u5178\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u5e94\u7528LabMTsimple storylab\u6a21\u5757\uff0c\u5e76\u4f7f\u7528Wards\u5c42\u6b21\u805a\u7c7b\u6280\u672f\u5bf9\u76f8\u4f3c\u60c5\u611f\u60c5\u8282\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5728\u7535\u5f71\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u5206\u6790\u7ed3\u679c\u5bf9\u6d88\u8d39\u8005\u548c\u8bfb\u8005\u5728\u9009\u62e9\u53d9\u4e8b\u6216\u6545\u4e8b\u65f6\u5f88\u6709\u5e2e\u52a9\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u53d6\u53d9\u4e8b\u4e2d\u4f20\u8fbe\u7684\u9ad8\u5c42\u548c\u4f4e\u5c42\u6982\u5ff5\uff0c\u901a\u8fc7\u60c5\u611f\u5f27\u7ebf\u5206\u6790\u548c\u805a\u7c7b\u6280\u672f\u4e3a\u6545\u4e8b\u9009\u62e9\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.11967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11967", "abs": "https://arxiv.org/abs/2511.11967", "authors": ["Mani Amani", "Behrad Beheshti", "Reza Akhavian"], "title": "Bootstrapped LLM Semantics for Context-Aware Path Planning", "comment": null, "summary": "Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM \"danger\" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u968f\u673a\u8bed\u4e49\u4f20\u611f\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7LLM\u7684\u5371\u9669\u5224\u65ad\u6765\u8c03\u5236\u7ecf\u5178\u8def\u5f84\u89c4\u5212\u5668\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u8bed\u4e49\u4fe1\u606f\u5b89\u5168\u9ad8\u6548\u5730\u5728\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u79fb\u52a8\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u7136\u8bed\u8a00\u673a\u5668\u4eba\u63a7\u5236\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u9009\u62e9\u800c\u975e\u6267\u884c\u5b89\u5168\u6027\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8bed\u4e49\u4e30\u5bcc\u7684\u4eba\u7c7b\u4e2d\u5fc3\u7a7a\u95f4\u4e2d\u5982\u4f55\u5b89\u5168\u9ad8\u6548\u5730\u6267\u884c\u4efb\u52a1\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u591a\u4e2a\u5371\u9669\u5224\u65ad\uff0c\u5e94\u7528\u8d1d\u53f6\u65af\u81ea\u4e3e\u6cd5\u8fd1\u4f3c\u6bcf\u4e2a\u7c7b\u522b\u7684\u98ce\u9669\u540e\u9a8c\u5206\u5e03\uff0c\u57fa\u4e8e\u540e\u9a8c\u7edf\u8ba1\u521b\u5efa\u52bf\u80fd\u6210\u672c\u6765\u5236\u5b9a\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548cBIM\u6570\u5b57\u5b6a\u751f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u663e\u5f0f\u63d0\u793a\u548c\u9690\u5f0f\u4e0a\u4e0b\u6587\u4fe1\u606f\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5668\u4eba\u79fb\u52a8\u65b9\u5f0f\uff0c\u63d0\u4f9b\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06LLM\u8f6c\u5316\u4e3a\u8bed\u4e49\u4f20\u611f\u5668\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u590d\u6742\u7684\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u8fdb\u884c\u5b89\u5168\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u3002"}}
{"id": "2511.11867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11867", "abs": "https://arxiv.org/abs/2511.11867", "authors": ["Namu Park", "Giridhar Kaushik Ramachandran", "Kevin Lybarger", "Fei Xia", "Ozlem Uzuner", "Meliha Yetisgen", "Martin Gunn"], "title": "Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches", "comment": "Submitted to LREC 2026", "summary": "Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b6,393\u4efd\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u653e\u5c04\u5b66\u968f\u8bbf\u4f9d\u4ece\u6027\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u5728\u4f18\u5316\u63d0\u793a\u4e0b\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u653e\u5c04\u5b66\u4efb\u52a1\u8868\u73b0\u7684\u4e13\u4e1a\u9886\u57df\u6570\u636e\u96c6\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u57fa\u51c6\u6765\u6bd4\u8f83\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e0e\u751f\u6210\u5f0fLLMs\u7684\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e866,393\u4efd\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff08LR\u3001SVM\u3001Longformer\uff09\u3001\u5b8c\u5168\u5fae\u8c03\u7684Llama3-8B-Instruct\u4e0e\u751f\u6210\u5f0fLLMs\uff08GPT-4o\u3001GPT-OSS-20B\uff09\uff0c\u540e\u8005\u5728\u57fa\u7840\u548c\u4f18\u5316\u4e24\u79cd\u63d0\u793a\u8bbe\u7f6e\u4e0b\u6d4b\u8bd5\u3002", "result": "GPT-4o\uff08\u4f18\u5316\u8bbe\u7f6e\uff09\u8868\u73b0\u6700\u4f73\uff08F1=0.832\uff09\uff0cGPT-OSS-20B\uff08\u4f18\u5316\u8bbe\u7f6e\uff09\u7d27\u968f\u5176\u540e\uff08F1=0.828\uff09\uff0cLR\u548cSVM\u4e5f\u8868\u73b0\u826f\u597d\uff08F1=0.776\u548c0.775\uff09\uff0c\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u9ad8\uff08F1=0.846\uff09\u3002", "conclusion": "\u867d\u7136\u901a\u8fc7\u63d0\u793a\u4f18\u5316LLMs\u53ef\u4ee5\u8fbe\u5230\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u53ef\u89e3\u91ca\u4e14\u8d44\u6e90\u6548\u7387\u9ad8\u7684\u4f20\u7edf\u6a21\u578b\u4ecd\u7136\u662f\u91cd\u8981\u7684\u57fa\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2511.11970", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11970", "abs": "https://arxiv.org/abs/2511.11970", "authors": ["Sara Wickenhiser", "Lizzie Peiros", "Calvin Joyce", "Peter Gavrilrov", "Sujaan Mukherjee", "Syler Sylvester", "Junrong Zhou", "Mandy Cheung", "Jason Lim", "Florian Richter", "Michael C. Yip"], "title": "ARCSnake V2: An Amphibious Multi-Domain Screw-Propelled Snake-Like Robot", "comment": "8 pages, 9 figures, ICRA", "summary": "Robotic exploration in extreme environments such as caves, oceans, and planetary surfaces pose significant challenges, particularly in locomotion across diverse terrains. Conventional wheeled or legged robots often struggle in these contexts due to surface variability. This paper presents ARCSnake V2, an amphibious, screw propelled, snake like robot designed for teleoperated or autonomous locomotion across land, granular media, and aquatic environments. ARCSnake V2 combines the high mobility of hyper redundant snake robots with the terrain versatility of Archimedean screw propulsion. Key contributions include a water sealed mechanical design with serially linked screw and joint actuation, an integrated buoyancy control system, and teleoperation via a kinematically matched handheld controller. The robots design and control architecture enable multiple locomotion modes screwing, wheeling, and sidewinding with smooth transitions between them. Extensive experiments validate its underwater maneuverability, communication robustness, and force regulated actuation. These capabilities position ARCSnake V2 as a versatile platform for exploration, search and rescue, and environmental monitoring in multi domain settings.", "AI": {"tldr": "ARCSnake V2\u662f\u4e00\u4e2a\u4e24\u6816\u87ba\u65cb\u63a8\u8fdb\u86c7\u5f62\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e86\u9ad8\u673a\u52a8\u6027\u86c7\u5f62\u673a\u5668\u4eba\u548c\u963f\u57fa\u7c73\u5fb7\u87ba\u65cb\u63a8\u8fdb\u7684\u5730\u5f62\u9002\u5e94\u6027\uff0c\u80fd\u5728\u9646\u5730\u3001\u9897\u7c92\u4ecb\u8d28\u548c\u6c34\u73af\u5883\u4e2d\u5b9e\u73b0\u87ba\u65cb\u3001\u8f6e\u5f0f\u548c\u4fa7\u5411\u6ed1\u884c\u7b49\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u8f6e\u5f0f\u6216\u817f\u5f0f\u673a\u5668\u4eba\u5728\u6d1e\u7a74\u3001\u6d77\u6d0b\u548c\u884c\u661f\u8868\u9762\u7b49\u6781\u7aef\u73af\u5883\u7684\u5730\u5f62\u591a\u6837\u6027\u4e2d\u79fb\u52a8\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5e94\u591a\u53d8\u5730\u5f62\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u6c34\u5bc6\u5c01\u673a\u68b0\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e32\u8054\u87ba\u65cb\u548c\u5173\u8282\u9a71\u52a8\uff0c\u96c6\u6210\u6d6e\u529b\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fd0\u52a8\u5b66\u5339\u914d\u7684\u624b\u6301\u63a7\u5236\u5668\u8fdb\u884c\u9065\u64cd\u4f5c\uff0c\u652f\u6301\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\u7684\u5e73\u6ed1\u5207\u6362\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6c34\u4e0b\u673a\u52a8\u6027\u3001\u901a\u4fe1\u9c81\u68d2\u6027\u548c\u529b\u8c03\u8282\u9a71\u52a8\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5728\u591a\u9886\u57df\u73af\u5883\u4e2d\u7684\u6709\u6548\u79fb\u52a8\u6027\u80fd\u3002", "conclusion": "ARCSnake V2\u4f5c\u4e3a\u4e00\u4e2a\u591a\u57df\u5e73\u53f0\uff0c\u5728\u63a2\u7d22\u3001\u641c\u6551\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2511.11878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11878", "abs": "https://arxiv.org/abs/2511.11878", "authors": ["Fernanda Bufon F\u00e4rber", "Iago Alves Brito", "Julia Soares Dollis", "Pedro Schindler Freire Brasil Ribeiro", "Rafael Teixeira Sousa", "Arlindo Rodrigues Galv\u00e3o Filho"], "title": "MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers", "comment": "11 pages, 3 tables, 2 figures", "summary": "While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.", "AI": {"tldr": "MedPT\u662f\u9996\u4e2a\u9488\u5bf9\u5df4\u897f\u8461\u8404\u7259\u8bed\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u533b\u7597\u8bed\u6599\u5e93\uff0c\u5305\u542b38.4\u4e07\u6761\u771f\u5b9e\u7684\u533b\u60a3\u95ee\u7b54\u5bf9\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7cbe\u5fc3\u7b5b\u9009\u548cLLM\u589e\u5f3a\u6807\u6ce8\uff0c\u652f\u6301\u533b\u7597\u4e13\u79d1\u8def\u7531\u7b49\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u533b\u7597AI\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u53d1\u5c55\u4e3b\u8981\u96c6\u4e2d\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u7b80\u5355\u7ffb\u8bd1\u65e0\u6cd5\u6355\u6349\u7279\u5b9a\u4e34\u5e8a\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\uff08\u5982\u5730\u65b9\u6027\u75be\u75c5\uff09\uff0c\u8fd9\u4e3a\u5176\u4ed6\u8bed\u8a00\u7fa4\u4f53\u521b\u9020\u4e86\u5173\u952e\u969c\u788d\u3002", "method": "\u6784\u5efa\u5305\u542b384,095\u6761\u771f\u5b9e\u533b\u60a3\u95ee\u7b54\u5bf9\u7684\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u6df7\u5408\u5b9a\u91cf-\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\u8fdb\u884c\u591a\u9636\u6bb5\u7b5b\u9009\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u6807\u6ce8\u5c06\u95ee\u9898\u5206\u7c7b\u4e3a7\u79cd\u8bed\u4e49\u7c7b\u578b\uff0c\u5e76\u5206\u6790\u5176\u4e3b\u9898\u5e7f\u5ea6\u548c\u8bed\u8a00\u7279\u6027\u3002", "result": "\u5728\u533b\u7597\u4e13\u79d1\u8def\u7531\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c0317\u4ebf\u53c2\u6570\u6a21\u578b\u572820\u7c7b\u8bbe\u7f6e\u4e0b\u8fbe\u523094%\u7684F1\u5206\u6570\uff1b\u9519\u8bef\u5206\u6790\u663e\u793a\u8bef\u5206\u7c7b\u53cd\u6620\u4e86\u771f\u5b9e\u7684\u4e34\u5e8a\u6a21\u7cca\u6027\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u6df1\u5c42\u8bed\u4e49\u4e30\u5bcc\u6027\u3002", "conclusion": "MedPT\u516c\u5f00\u91ca\u653e\u4ee5\u4fc3\u8fdb\u8461\u8404\u7259\u8bed\u4e16\u754c\u66f4\u516c\u5e73\u3001\u51c6\u786e\u548c\u6587\u5316\u654f\u611f\u7684\u533b\u7597\u6280\u672f\u53d1\u5c55\uff0c\u5c55\u793a\u4e86\u5176\u5728\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u533b\u7597AI\u6311\u6218\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.12022", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12022", "abs": "https://arxiv.org/abs/2511.12022", "authors": ["Anh-Quan Pham", "Kabir Ram Puri", "Shreyas Raorane"], "title": "SBAMP: Sampling Based Adaptive Motion Planning", "comment": "8 pages, 13 figures", "summary": "Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.", "AI": {"tldr": "SBAMP\u7ed3\u5408RRT*\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548cSEDS\u5c40\u90e8\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u89c4\u5212\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u5b9e\u65f6\u9002\u5e94\u6027\u548c\u8def\u5f84\u6700\u4f18\u6027\u3002", "motivation": "\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u5982RRT*\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\uff0c\u800c\u5b66\u4e60\u578b\u52a8\u6001\u7cfb\u7edf\u5982SEDS\u4f9d\u8d56\u9884\u6536\u96c6\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u5168\u5c40\u89c4\u5212\u548c\u5b9e\u65f6\u9002\u5e94\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u96c6\u6210RRT*\u8fdb\u884c\u5168\u5c40\u8def\u5f84\u89c4\u5212\uff0c\u4f7f\u7528SEDS\u57fa\u7684\u5c40\u90e8\u63a7\u5236\u5668\u8fdb\u884c\u8fde\u7eed\u81ea\u9002\u5e94\u8f68\u8ff9\u8c03\u6574\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u901a\u8fc7Lyapunov\u4fdd\u8bc1\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548cRoboRacer\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0cSBAMP\u5728\u52a8\u6001\u969c\u788d\u573a\u666f\u3001\u6270\u52a8\u6062\u590d\u548c\u6025\u8f6c\u5f2f\u5904\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u65f6\u9002\u5e94\u6027\u5f3a\u4e14\u4e0d\u727a\u7272\u5168\u5c40\u8def\u5f84\u6700\u4f18\u6027\u3002", "conclusion": "SBAMP\u4e3a\u52a8\u6001\u975e\u7ed3\u6784\u5316\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u91c7\u6837\u65b9\u6cd5\u548c\u5b66\u4e60\u578b\u7cfb\u7edf\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.11883", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11883", "abs": "https://arxiv.org/abs/2511.11883", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts", "comment": null, "summary": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.", "AI": {"tldr": "ClinStructor\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u4e34\u5e8a\u81ea\u7531\u6587\u672c\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u95ee\u7b54\u5bf9\uff0c\u4ee5\u89e3\u51b3\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u504f\u89c1\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5728ICU\u6b7b\u4ea1\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u4ec5\u8f7b\u5fae\u4e0b\u964d2-3% AUC\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u4fe1\u606f\u4f46\u683c\u5f0f\u975e\u7ed3\u6784\u5316\uff0c\u5b58\u5728\u65e0\u610f\u504f\u89c1\uff08\u5982\u6027\u522b\u6216\u79cd\u65cf\u504f\u89c1\uff09\u3001\u8de8\u4e34\u5e8a\u73af\u5883\u6cdb\u5316\u6027\u5dee\u4ee5\u53ca\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u4e34\u5e8a\u81ea\u7531\u6587\u672c\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u95ee\u7b54\u5bf9\uff0c\u7136\u540e\u8fdb\u884c\u9884\u6d4b\u5efa\u6a21\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u63a7\u6027\uff0c\u5728ICU\u6b7b\u4ea1\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\u4e0e\u76f4\u63a5\u5fae\u8c03\u76f8\u6bd4\uff0c\u9884\u6d4b\u6027\u80fd\u4ec5\u8f7b\u5fae\u4e0b\u964d\uff08AUC\u4e0b\u964d2-3%\uff09\u3002", "conclusion": "ClinStructor\u4e3a\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u6784\u5efa\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2511.12101", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12101", "abs": "https://arxiv.org/abs/2511.12101", "authors": ["Jian Zhou", "Sihao Lin", "Shuai Fu", "Qi WU"], "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers", "comment": null, "summary": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u4f4e\u6210\u672c\u7684\u8fd0\u52a8\u5b66\u751f\u6210\u8f68\u8ff9\u9884\u8bad\u7ec3\u901a\u7528\u52a8\u4f5c\u5934\uff0c\u7136\u540e\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u63ed\u793a\u4e86\u52a8\u4f5c\u751f\u6210\u4e3b\u5e72\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u4f5c\u7528\u6709\u9650\uff0c\u8fdb\u800c\u63d0\u51fa\u4e86\u53c2\u6570\u66f4\u5c11\u7684DP-MLP\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u5982Diffusion Policy\u53d7\u9650\u4e8e\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u4e14\u5176\u5185\u90e8\u673a\u5236\u7406\u89e3\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u6a21\u578b\u8bbe\u8ba1\u7f3a\u4e4f\u539f\u5219\u6027\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u8bad\u7ec3\u65b9\u6cd5\uff1a\u4f7f\u7528\u8fd0\u52a8\u5b66\u751f\u6210\u8f68\u8ff9\u9884\u8bad\u7ec3\u901a\u7528\u52a8\u4f5c\u5934\uff0c\u7136\u540e\u51bb\u7ed3\u8be5\u52a8\u4f5c\u5934\u5e76\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u57fa\u4e8e\u52a8\u4f5c\u4e3b\u5e72\u4f5c\u7528\u6709\u9650\u7684\u89c2\u5bdf\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u7528\u7b80\u5355MLP\u5757\u66ff\u6362U-Net\u4e3b\u5e72\u7684DP-MLP\u6a21\u578b\u3002", "result": "\u89e3\u8026\u8bad\u7ec3\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u5747\u53ef\u884c\uff0cDP-C\u5b9e\u73b041%\u8bad\u7ec3\u52a0\u901f\u3002DP-MLP\u5728\u6b63\u5e38\u8bad\u7ec3\u4e0b\u63d0\u901f83.9%\uff0c\u89e3\u8026\u8bad\u7ec3\u4e0b\u63d0\u901f89.1%\uff0c\u53c2\u6570\u91cf\u4ece244M\u51cf\u5c11\u52304M\u3002", "conclusion": "\u89e3\u8026\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u63ed\u793a\u4e86\u52a8\u4f5c\u751f\u6210\u4e3b\u5e72\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u4f5c\u7528\u6709\u9650\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.11884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11884", "abs": "https://arxiv.org/abs/2511.11884", "authors": ["Eric Hua Qing Zhang", "Julia Ive"], "title": "Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support", "comment": null, "summary": "Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u589e\u5f3aGPT-2\u7684\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u751f\u6210\u80fd\u529b\uff0c\u5728\u591a\u6307\u6807\u8bc4\u4f30\u4e2d\u663e\u793a\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u60c5\u611f\u51c6\u786e\u7387\u8fbe\u523099.34%\u3002", "motivation": "COVID-19\u52a0\u5267\u4e86\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u7684\u53ef\u53ca\u6027\u6311\u6218\uff0c\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b24/7\u670d\u52a1\uff0c\u4f46\u9884\u8bad\u7ec3\u6a21\u578b\u7f3a\u4e4f\u60c5\u5883\u548c\u60c5\u611f\u610f\u8bc6\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u4f9b\u5408\u9002\u7684\u6cbb\u7597\u54cd\u5e94\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u91cd\u6784\u8f93\u5165\u683c\u5f0f\u4ee5\u540c\u65f6\u5904\u7406\u60c5\u5883\u4fe1\u606f\u548c\u60c5\u611f\u72b6\u6001\uff0c\u4f7f\u7528\u591a\u7ec4\u4ef6\u5956\u52b1\u51fd\u6570\u4f7f\u6a21\u578b\u8f93\u51fa\u4e0e\u4e13\u4e1a\u6cbb\u7597\u5e08\u54cd\u5e94\u548c\u6807\u6ce8\u60c5\u611f\u5bf9\u9f50\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebfGPT-2\uff1aBLEU(0.0111)\u3001ROUGE-1(0.1397)\u3001ROUGE-2(0.0213)\u3001ROUGE-L(0.1317)\u3001METEOR(0.0581)\uff0c\u60c5\u611f\u51c6\u786e\u7387\u8fbe\u523099.34%\uff08\u57fa\u7ebf\u4e3a66.96%\uff09\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u5f00\u53d1\u6cbb\u7597\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u4f5c\u4e3a\u6cbb\u7597\u5e08\u7684\u6709\u4ef7\u503c\u8f85\u52a9\u5de5\u5177\uff0c\u540c\u65f6\u4fdd\u6301\u5fc5\u8981\u7684\u4eba\u7c7b\u4e34\u5e8a\u76d1\u7763\u3002"}}
{"id": "2511.12148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12148", "abs": "https://arxiv.org/abs/2511.12148", "authors": ["Advik Sinha", "Akshay Arjun", "Abhijit Das", "Joyjit Mukherjee"], "title": "Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies", "comment": "9 pages, 6 figures", "summary": "This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot", "AI": {"tldr": "\u4f7f\u7528NEAT\u7b97\u6cd5\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u5f00\u53d1\u8d44\u6e90\u9ad8\u6548\u7684\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\u65b9\u6848\uff0c\u901a\u8fc7\u8fdb\u5316\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u52a8\u6001\u6b65\u6001\u53c2\u6570\uff0c\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8def\u5f84\u8ddf\u8e2a\u3002", "motivation": "\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e2d\u5b9e\u73b0\u86c7\u5f62\u673a\u5668\u4eba\u7684\u9ad8\u6548\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u8d44\u6e90\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528NEAT\uff08\u589e\u5f3a\u62d3\u6251\u795e\u7ecf\u8fdb\u5316\uff09\u7b97\u6cd5\uff0c\u4ee5\u5173\u8282\u89d2\u5ea6\u3001\u8fde\u6746\u4f4d\u7f6e\u3001\u5934\u90e8\u4f4d\u7f6e\u548c\u9644\u8fd1\u969c\u788d\u7269\u4f4d\u7f6e\u4e3a\u8f93\u5165\uff0c\u8f93\u51fa\u63a7\u5236\u901f\u5ea6\u548c\u822a\u5411\u7684serpenoid\u6b65\u6001\u9891\u7387\u548c\u504f\u79fb\u89d2\uff0c\u901a\u8fc7LiDAR\u548c\u4f20\u611f\u5668\u6570\u636e\u53c2\u6570\u5316\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728PyBullet\u7269\u7406\u5f15\u64ce\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e0e\u6700\u65b0\u7684CBRL\u65b9\u6cd5\u7ed3\u679c\u76f8\u5f53\u4f46\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684NEAT\u6846\u67b6\u5728\u5bc6\u96c6\u969c\u788d\u73af\u5883\u4e2d\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u578b\u73af\u5883\u548c\u591a\u969c\u788d\u573a\u666f\u3002"}}
{"id": "2511.11922", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11922", "abs": "https://arxiv.org/abs/2511.11922", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "title": "Additive Large Language Models for Semi-Structured Text", "comment": null, "summary": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.", "AI": {"tldr": "CALM\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u5206\u89e3\u4e3a\u5404\u4e2a\u8bed\u4e49\u7ec4\u4ef6\u7684\u8d21\u732e\u548c\uff0c\u63d0\u4f9b\u5fe0\u5b9e\u89e3\u91ca\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\u4e2d\u9884\u6d4b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u7814\u7a76\u8005\u548c\u533b\u751f\u9700\u8981\u7406\u89e3\u60a3\u8005\u8bb0\u5f55\u4e2d\u54ea\u4e9b\u90e8\u5206\u9a71\u52a8\u98ce\u9669\u4fe1\u53f7\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u53ef\u52a0\u6027\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u534a\u7ed3\u6784\u5316\u6587\u672c\u8f93\u5165\u5206\u89e3\u4e3a\u8bed\u4e49\u7ec4\u4ef6\uff0c\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u5404\u7ec4\u4ef6\u8d21\u732e\u7684\u52a0\u548c\uff0c\u5b9e\u73b0\u60a3\u8005\u548c\u7fa4\u4f53\u5c42\u9762\u7684\u5fe0\u5b9e\u89e3\u91ca\u3002", "result": "CALM\u5728\u6027\u80fd\u4e0a\u4e0e\u5e38\u89c4LLM\u5206\u7c7b\u5668\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u4fe1\u5ea6\uff0c\u652f\u6301\u8d28\u91cf\u4fdd\u8bc1\u68c0\u67e5\uff0c\u5e76\u5728\u6a21\u578b\u5f00\u53d1\u548c\u5ba1\u8ba1\u4e2d\u63ed\u793a\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u6a21\u5f0f\u3002", "conclusion": "CALM\u6846\u67b6\u4e3a\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u52a0\u6027\u7ed3\u6784\u5b9e\u73b0\u900f\u660e\u9884\u6d4b\uff0c\u4fc3\u8fdb\u6a21\u578b\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2511.12160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12160", "abs": "https://arxiv.org/abs/2511.12160", "authors": ["Wenbin Mai", "Minghui Liwang", "Xinlei Yi", "Xiaoyu Xia", "Seyyedali Hosseinalipour", "Xianbin Wang"], "title": "Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)", "comment": "12 pages, 9 figures", "summary": "Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\\varepsilon$-BR (i$\\varepsilon$-BR) process that guarantees finite-step convergence to an $\\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.", "AI": {"tldr": "\u63d0\u51faRE-DPG\u6846\u67b6\uff0c\u5c06\u53ef\u8fbe\u6027\u5206\u6790\u4e0e\u535a\u5f08\u8bba\u534f\u8c03\u7ed3\u5408\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u590d\u6742\u4ea4\u4e92\u3001\u968f\u673a\u6270\u52a8\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u5b89\u5168\u3001\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u8fd0\u52a8\u89c4\u5212\u6311\u6218\uff0c\u7279\u522b\u662f\u8026\u5408\u51b3\u7b56\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u4e3b\u52a8\u5b89\u5168\u4fdd\u8bc1\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53ef\u8fbe\u6027\u589e\u5f3a\u52a8\u6001\u52bf\u535a\u5f08\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u534f\u8c03\u5efa\u6a21\u4e3a\u52a8\u6001\u52bf\u535a\u5f08\uff0c\u901a\u8fc7\u7eb3\u4ec0\u5747\u8861\u5b9a\u4e49\u6700\u4f18\u63a7\u5236\u7b56\u7565\uff1b\u5f00\u53d1\u90bb\u57df\u4e3b\u5bfc\u8fed\u4ee3\u6700\u4f73\u54cd\u5e94\u65b9\u6848\uff0c\u57fa\u4e8e\u8fed\u4ee3\u03b5-\u6700\u4f73\u54cd\u5e94\u8fc7\u7a0b\u786e\u4fdd\u6709\u9650\u6b65\u6536\u655b\u5230\u03b5-\u7eb3\u4ec0\u5747\u8861\uff1b\u96c6\u6210\u591a\u667a\u80fd\u4f53\u524d\u5411\u53ef\u8fbe\u96c6\u673a\u5236\u5230\u6210\u672c\u51fd\u6570\u4e2d\uff0c\u663e\u5f0f\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5e76\u5f3a\u5236\u6267\u884c\u78b0\u649e\u907f\u514d\u7ea6\u675f\u3002", "result": "\u901a\u8fc72D\u548c3D\u73af\u5883\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RE-DPG\u5728\u4e0d\u540c\u64cd\u4f5c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "RE-DPG\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u53ef\u8fbe\u6027\u5206\u6790\u4e0e\u535a\u5f08\u8bba\u534f\u8c03\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11933", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11933", "abs": "https://arxiv.org/abs/2511.11933", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "Bhuwan Dhingra", "David Edwin Carlson"], "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis", "comment": null, "summary": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.", "AI": {"tldr": "\u63d0\u51faInData\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524dLLM\u5728\u590d\u6742\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u89e3\u51b3LLM\u76f4\u63a5\u751f\u6210\u548c\u6267\u884c\u4ee3\u7801\u8bbf\u95ee\u654f\u611f\u6570\u636e\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u5b89\u5168\u5de5\u5177\u8fdb\u884c\u95f4\u63a5\u6570\u636e\u4ea4\u4e92\u3002", "method": "\u5f15\u5165InData\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684\u6570\u636e\u5206\u6790\u95ee\u9898\uff0c\u8bc4\u4f3015\u4e2a\u5f00\u6e90LLM\u5728\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5927\u578b\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u9ad8\uff0897.3%\uff09\uff0c\u4f46\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0869.6%\uff09\uff0c\u663e\u793a\u5f53\u524dLLM\u7f3a\u4e4f\u7a33\u5065\u7684\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "InData\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u5177\u6709\u66f4\u5f3a\u591a\u6b65\u9aa4\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684LLM\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5c06\u516c\u5f00\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2511.12184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12184", "abs": "https://arxiv.org/abs/2511.12184", "authors": ["Jun Huo", "Kehan Xu", "Chengyao Li", "Yu Cao", "Jie Zuo", "Xinxing Chen", "Jian Huang"], "title": "Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance", "comment": null, "summary": "In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u8d85\u6570\u673a\u5668\u4eba\u817f\u7cfb\u7edf\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4f4d\u7f6e/\u529b\u963b\u6297\u63a7\u5236\u5668\u548c\u53ef\u53d8\u963b\u6297\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u963b\u6297\u53c2\u6570\u6765\u9002\u5e94\u5185\u5916\u5e72\u6270\uff0c\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5728\u6d6e\u57fa\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u786e\u4fdd\u529b\u63a7\u5236\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u8d85\u6570\u673a\u5668\u4eba\u817f\u7cfb\u7edf\u4f5c\u4e3a\u5178\u578b\u7684\u677e\u6563\u8026\u5408\u6d6e\u57fa\u7cfb\u7edf\uff0c\u7279\u522b\u5bb9\u6613\u53d7\u5230\u5f3a\u5185\u90e8\u5e72\u6270\u7684\u5f71\u54cd\uff0c\u9700\u8981\u89e3\u51b3\u6d6e\u57fa\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u4e86\u677e\u6563\u8026\u5408SRL\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u6df7\u5408\u4f4d\u7f6e/\u529b\u963b\u6297\u63a7\u5236\u5668\u6765\u9002\u5e94\u52a8\u6001\u626d\u77e9\u8f93\u5165\u3002\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u53d8\u963b\u6297\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u963b\u6297\u53c2\u6570\u6765\u6539\u5584\u521a\u67d4\u5207\u6362\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5b9e\u65f6\u7a33\u5b9a\u6027\u4fdd\u8bc1\u7684\u963b\u6297\u53c2\u6570\u751f\u6210\u7f51\u7edc\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u7cfb\u7edf\u80fd\u591f\u5728\u67d4\u6027\u72b6\u6001\u4e0b\u4fdd\u6301\u5e73\u6ed1\u4fe1\u53f7\u8f6c\u6362\uff0c\u5728\u521a\u6027\u72b6\u6001\u4e0b\u63d0\u4f9b\u5f3a\u529b\u652f\u6491\uff0c\u5b9e\u73b0\u4e86\u51b2\u51fb\u7f13\u89e3\u548c\u9ad8\u521a\u6027\u652f\u6491\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u4e2a\u4f53\u6b65\u6001\u53d8\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.11946", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11946", "abs": "https://arxiv.org/abs/2511.11946", "authors": ["Hadi Sheikhi", "Chenyang Huang", "Osmar R. Za\u00efane"], "title": "Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization", "comment": null, "summary": "Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.", "AI": {"tldr": "LLM-KAT\u8bc4\u4f30\u7a0b\u5e8f\u548c\u5b9e\u4f53\u533f\u540d\u5316\u6280\u672f\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4e2d\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u5229\u7528\u80fd\u529b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\uff0c\u5373\u4f7f\u63d0\u4f9b\u4e86\u51c6\u786e\u7684\u77e5\u8bc6\u56fe\u8c31\u4e5f\u96be\u4ee5\u6709\u6548\u5229\u7528\u5916\u90e8\u77e5\u8bc6", "method": "\u63d0\u51faLLM-KAT\u8bc4\u4f30\u65b9\u6cd5\u6765\u8861\u91cf\u77e5\u8bc6\u9644\u7740\u5ea6\uff0c\u5e76\u91c7\u7528\u5b9e\u4f53\u533f\u540d\u5316\u6280\u672f\u6765\u9f13\u52b1\u6a21\u578b\u66f4\u597d\u5730\u5229\u7528\u5916\u90e8\u77e5\u8bc6", "result": "\u5728OpenDialKG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u9644\u7740\u80fd\u529b", "conclusion": "\u5b9e\u4f53\u533f\u540d\u5316\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4e2d\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u5229\u7528"}}
{"id": "2511.12186", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12186", "abs": "https://arxiv.org/abs/2511.12186", "authors": ["Jun Huo", "Jian Huang", "Jie Zuo", "Bo Yang", "Zhongzheng Fu", "Xi Li", "Samer Mohammed"], "title": "Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization", "comment": null, "summary": "Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53(SRL)\u7684\u591a\u76ee\u6807\u4f18\u5316\u8bbe\u8ba1\u7406\u8bba\uff0c\u6574\u5408\u6293\u53d6\u5de5\u4f5c\u7a7a\u95f4\u76f8\u4f3c\u6027\u3001\u884c\u8d70\u5de5\u4f5c\u7a7a\u95f4\u76f8\u4f3c\u6027\u3001\u5750\u7ad9\u8f6c\u6362\u652f\u6491\u529b\u548c\u6574\u4f53\u8d28\u91cf\u60ef\u6027\uff0c\u901a\u8fc7\u692d\u5706\u4f53\u91cf\u5316\u65b9\u6cd5\u548c\u6539\u8fdb\u7684\u8424\u706b\u866b\u7b97\u6cd5\u4f18\u5316\u8bbe\u8ba1\uff0c\u5b9e\u9a8c\u663e\u793a\u6293\u53d6\u6210\u529f\u7387\u63d0\u53477.2%\uff0c\u808c\u8089\u6d3b\u52a8\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u8bbe\u8ba1\u901a\u7528SRL\u8bbe\u5907\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u6ee1\u8db3\u4e0a\u4e0b\u80a2\u4e0d\u540c\u529f\u80fd\u9700\u6c42\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u5728\u5eb7\u590d\u548c\u529f\u80fd\u589e\u5f3a\u5e94\u7528\u4e2d\u53d1\u6325\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u591a\u76ee\u6807\u4f18\u5316\u8bbe\u8ba1\u7406\u8bba\uff0c\u4f7f\u7528\u692d\u5706\u4f53\u8868\u793a\u5de5\u4f5c\u7a7a\u95f4\u4ee5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5f15\u5165\u591a\u5b50\u7fa4\u4fee\u6b63\u8424\u706b\u866b\u7b97\u6cd5\u5904\u7406\u9ad8\u7ef4\u4e0d\u89c4\u5219\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4f18\u5316\u540e\u91cd\u65b0\u8bbe\u8ba1\u539f\u578b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u4e0e\u4f18\u5316\u524d\u76f8\u6bd4\uff0c\u5e73\u5747\u6293\u53d6\u6210\u529f\u7387\u63d0\u9ad87.2%\uff0c\u884c\u8d70\u548c\u5750\u7ad9\u8f6c\u6362\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u6d3b\u52a8\u5ea6\u5206\u522b\u964d\u4f4e12.7%\u548c25.1%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u7406\u8bba\u4e3a\u591a\u529f\u80fdSRL\u673a\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u9009\u62e9\uff0c\u9a8c\u8bc1\u4e86\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11966", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11966", "abs": "https://arxiv.org/abs/2511.11966", "authors": ["Steven Cao", "Gregory Valiant", "Percy Liang"], "title": "On the Entropy Calibration of Language Models", "comment": "Neurips 2025", "summary": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u71b5\u6821\u51c6\u95ee\u9898\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6821\u51c6\u9519\u8bef\uff0c\u4e14\u8fd9\u79cd\u9519\u8bef\u968f\u89c4\u6a21\u6269\u5927\u6539\u5584\u7f13\u6162\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u6821\u51c6\u6539\u5584\u901f\u5ea6\u53d6\u51b3\u4e8e\u6570\u636e\u5206\u5e03\u7684\u5e42\u5f8b\u6307\u6570\uff0c\u5b9e\u8bc1\u7814\u7a76\u663e\u793a\u4ece5\u4ebf\u5230700\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u9519\u8bef\u7d2f\u79ef\u7387\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u71b5\u6821\u51c6\u9519\u8bef\u7d2f\u79ef\u8fd9\u4e00\u57fa\u672c\u95ee\u9898\uff0c\u63a2\u7d22\u6a21\u578b\u89c4\u6a21\u6269\u5927\u662f\u5426\u80fd\u6539\u5584\u6821\u51c6\uff0c\u4ee5\u53ca\u662f\u5426\u5b58\u5728\u65e0\u9700\u727a\u7272\u591a\u6837\u6027\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5728\u7b80\u5316\u7406\u8bba\u8bbe\u7f6e\u4e2d\u5206\u6790\u6821\u51c6\u9519\u8bef\u968f\u6570\u636e\u96c6\u89c4\u6a21\u7684\u7f29\u653e\u884c\u4e3a\uff0c\u7136\u540e\u5b9e\u8bc1\u6d4b\u91cf\u4ece0.5B\u523070B\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\u7684\u6821\u51c6\u9519\u8bef\u3002", "result": "\u53d1\u73b0\u6821\u51c6\u9519\u8bef\u7684\u7f29\u653e\u6307\u6570\u63a5\u8fd10\uff0c\u610f\u5473\u7740\u66f4\u5927\u6a21\u578b\u4e0e\u66f4\u5c0f\u6a21\u578b\u4ee5\u76f8\u4f3c\u901f\u7387\u7d2f\u79ef\u9519\u8bef\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5373\u4f7f\u66f4\u5927\u6a21\u578b\u8d28\u91cf\u66f4\u9ad8\uff0c\u6211\u4eec\u4ecd\u4f7f\u7528\u76f8\u4f3c\u7a0b\u5ea6\u7684\u622a\u65ad\u91c7\u6837\u3002", "conclusion": "\u7406\u8bba\u4e0a\u8bc1\u660e\u5982\u679c\u80fd\u591f\u8bbf\u95ee\u9884\u6d4b\u6587\u672c\u672a\u6765\u71b5\u7684\u9ed1\u76d2\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u5bf9\u6570\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u71b5\uff0c\u4ece\u800c\u53ef\u80fd\u5b9e\u73b0\u65e0\u4ee3\u4ef7\u7684\u6821\u51c6\u3002"}}
{"id": "2511.12203", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12203", "abs": "https://arxiv.org/abs/2511.12203", "authors": ["Antony Thomas", "Fulvio Mastrogiovanni", "Marco Baglietto"], "title": "Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps", "comment": "Robotics and Autonomous Systems", "summary": "We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u8ba1\u7b97\u5c40\u90e8\u6700\u4f18\u7684\u969c\u788d\u7269\u4f4d\u79fb\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u627e\u5230\u53ef\u884c\u8def\u5f84\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u5b58\u5728\u969c\u788d\u7269\u6216\u7ea6\u675f\u7684\u73af\u5883\u4e2d\u5bfb\u627e\u53ef\u884c\u8def\u5f84\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f4d\u79fb\u7ea6\u675f\u6216\u969c\u788d\u7269\u6765\u521b\u9020\u53ef\u884c\u8def\u5f84\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8ba1\u7b97\u901a\u8fc7\u969c\u788d\u7269\u7684\u8f68\u8ff9\u5e76\u6700\u5c0f\u5316\u76ee\u6807\u51fd\u6570\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f4d\u79fb\u969c\u788d\u7269\u4f7f\u673a\u5668\u4eba\u8f68\u8ff9\u53ef\u884c\u4e14\u65e0\u78b0\u649e\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4e24\u7c7b\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\u4e0a\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u591a\u4e2a\u793a\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11978", "abs": "https://arxiv.org/abs/2511.11978", "authors": ["Hui Huang", "Yanping Chen", "Ruizhang Huang", "Chuan Lin", "Yongbin Qin"], "title": "A Reasoning Paradigm for Named Entity Recognition", "comment": "Accepted at AAAI 2026", "summary": "Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This \"cognitive shortcutting\" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReasoningNER\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u673a\u5236\u6539\u8fdbNER\u4efb\u52a1\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6027\u80fd\u8d85\u8d8aGPT-4 12.3\u4e2a\u767e\u5206\u70b9", "motivation": "\u73b0\u6709\u751f\u6210\u5f0fLLM\u5728NER\u4efb\u52a1\u4e2d\u4f9d\u8d56\u9690\u5f0f\u6a21\u5f0f\u5339\u914d\uff0c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u673a\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u548c\u6cdb\u5316\u8106\u5f31\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b", "method": "\u4e09\u9636\u6bb5\u63a8\u7406\u6846\u67b6\uff1a1)\u751f\u6210NER\u5bfc\u5411\u7684\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff1b2)\u4f7f\u7528CoT\u5fae\u8c03NER\u6a21\u578b\uff1b3)\u901a\u8fc7\u7efc\u5408\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff0cF1\u5206\u6570\u6bd4GPT-4\u9ad8\u51fa12.3\u4e2a\u767e\u5206\u70b9\uff0c\u5c55\u793a\u4e86\u5728\u63a8\u7406\u5bfc\u5411\u4fe1\u606f\u63d0\u53d6\u7814\u7a76\u4e2d\u7684\u5de8\u5927\u6f5c\u529b", "conclusion": "ReasoningNER\u6846\u67b6\u901a\u8fc7\u5c06\u63d0\u53d6\u8303\u5f0f\u4ece\u9690\u5f0f\u6a21\u5f0f\u5339\u914d\u8f6c\u5411\u663e\u5f0f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86NER\u4efb\u52a1\u7684\u8ba4\u77e5\u80fd\u529b\u548c\u6027\u80fd"}}
{"id": "2511.12232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12232", "abs": "https://arxiv.org/abs/2511.12232", "authors": ["Lingfeng Zhang", "Erjia Xiao", "Xiaoshuai Hao", "Haoxiang Fu", "Zeying Gong", "Long Chen", "Xiaojun Liang", "Renjing Xu", "Hangjun Ye", "Wenbo Ding"], "title": "SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation", "comment": null, "summary": "Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.", "AI": {"tldr": "\u63d0\u51fa\u4e86SocialNav-Map\uff0c\u4e00\u79cd\u96f6\u6837\u672c\u793e\u4ea4\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u548c\u5360\u636e\u5730\u56fe\uff0c\u65e0\u9700\u73af\u5883\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u89812000+\u5c0f\u65f6\u8bad\u7ec3\uff0c\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u964c\u751f\u73af\u5883\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5c06\u4efb\u52a1\u76ee\u6807\u4f4d\u7f6e\u8f6c\u6362\u5230\u5730\u56fe\u5750\u6807\u7cfb\uff0c\u521b\u5efa\u5305\u542b\u9884\u6d4b\u4eba\u7c7b\u8fd0\u52a8\u7684\u52a8\u6001\u5360\u636e\u5730\u56fe\uff0c\u4f7f\u7528\u5386\u53f2\u9884\u6d4b\u548c\u65b9\u5411\u9884\u6d4b\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\u8fdb\u884c\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728Social-HM3D\u548cSocial-MP3D\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684RL\u65b9\u6cd5\uff0c\u51cf\u5c11\u4eba\u7c7b\u78b0\u649e\u7387\u8d85\u8fc710%\uff0c\u65e0\u9700\u65b0\u73af\u5883\u8bad\u7ec3\u3002", "conclusion": "\u901a\u8fc7\u6d88\u9664\u73af\u5883\u7279\u5b9a\u8bad\u7ec3\u9700\u6c42\uff0cSocialNav-Map\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u90e8\u7f72\u793e\u4ea4\u5bfc\u822a\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.12001", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12001", "abs": "https://arxiv.org/abs/2511.12001", "authors": ["Eunkyu Park", "Wesley Hanwen Deng", "Vasudha Varadarajan", "Mingxi Yan", "Gunhee Kim", "Maarten Sap", "Motahhare Eslami"], "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations", "comment": "Under review; 16 pages, 15 figures", "summary": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0CoT\u89e3\u91ca\u5728\u9053\u5fb7\u573a\u666f\u4e2d\u5177\u6709\u53cc\u91cd\u4f5c\u7528\uff1a\u65e2\u80fd\u63d0\u9ad8\u900f\u660e\u5ea6\uff0c\u4e5f\u53ef\u80fd\u56e0\u786e\u8ba4\u504f\u89c1\u800c\u8bef\u5bfc\u7528\u6237\uff0c\u7279\u522b\u662f\u5f53\u89e3\u91ca\u8bed\u6c14\u81ea\u4fe1\u65f6\uff0c\u7528\u6237\u4f1a\u5ffd\u89c6\u63a8\u7406\u9519\u8bef\u800c\u7ef4\u6301\u4fe1\u4efb\u3002", "motivation": "\u7814\u7a76CoT\u89e3\u91ca\u5728\u9053\u5fb7\u573a\u666f\u4e2d\u7684\u53cc\u91cd\u89d2\u8272\uff0c\u63a2\u8ba8\u5176\u5982\u4f55\u65e2\u4fc3\u8fdb\u900f\u660e\u5ea6\u53c8\u53ef\u80fd\u56e0\u786e\u8ba4\u504f\u89c1\u800c\u8bef\u5bfc\u7528\u6237\uff0c\u7279\u522b\u662f\u5173\u6ce8\u63a8\u7406\u9519\u8bef\u548c\u8bed\u6c14\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u5728\u591a\u6a21\u6001\u9053\u5fb7\u573a\u666f\u4e2d\u7cfb\u7edf\u6027\u5730\u6270\u52a8\u63a8\u7406\u94fe\u548c\u64cd\u7eb5\u89e3\u91ca\u8bed\u6c14\uff0c\u5206\u6790\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u9519\u8bef\u53ca\u5176\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u6548\u5e94\uff1a(1)\u7528\u6237\u5e38\u5c06\u4fe1\u4efb\u7b49\u540c\u4e8e\u7ed3\u679c\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u63a8\u7406\u6709\u7f3a\u9677\u4e5f\u7ef4\u6301\u4f9d\u8d56\uff1b(2)\u81ea\u4fe1\u8bed\u6c14\u4f1a\u6291\u5236\u9519\u8bef\u68c0\u6d4b\u4f46\u7ef4\u6301\u4f9d\u8d56\uff0c\u8868\u660e\u8868\u8fbe\u98ce\u683c\u53ef\u4ee5\u51cc\u9a7e\u4e8e\u6b63\u786e\u6027\u4e4b\u4e0a\u3002", "conclusion": "CoT\u89e3\u91ca\u65e2\u80fd\u6f84\u6e05\u4e5f\u80fd\u8bef\u5bfc\uff0c\u5f3a\u8c03NLP\u7cfb\u7edf\u9700\u8981\u63d0\u4f9b\u9f13\u52b1\u5ba1\u614e\u548c\u6279\u5224\u6027\u601d\u7ef4\u800c\u975e\u76f2\u76ee\u4fe1\u4efb\u7684\u89e3\u91ca\u3002"}}
{"id": "2511.12237", "categories": ["cs.RO", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12237", "abs": "https://arxiv.org/abs/2511.12237", "authors": ["Alysson Ribeiro da Silva", "Luiz Chaimowicz"], "title": "Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration", "comment": "9 pages, 9 figures, International Conference on Advanced Robotics", "summary": "Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u673a\u5668\u4eba\u63a2\u7d22\u901a\u4fe1\u7ea6\u675f\u548c\u95f4\u6b47\u8fde\u63a5\u95ee\u9898\uff08MRE-CCIC\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ecMILP\u89c4\u5212\u751f\u6210\u5668\u548cRTUS\u8ddf\u8e2a\u673a\u5236\uff0c\u5728Gazebo\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u63a2\u7d22\u4e2d\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8c03\u5ea6\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u672a\u77e5\u73af\u5883\u4e2d\u65e0\u6cd5\u9884\u5148\u83b7\u53d6\u73af\u5883\u4fe1\u606f\u7684\u60c5\u51b5\uff0c\u6539\u8fdb\u4e4b\u524d\u5de5\u4f5c\u4e2d\u65e0\u6cd5\u751f\u6210\u6700\u4f18\u8ba1\u5212\u548c\u7f3a\u4e4f\u5b9e\u9645\u8f68\u8ff9\u8ddf\u8e2a\u673a\u5236\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u751f\u6210\u96c6\u5408\u70b9\u8ba1\u5212\uff0c\u5e76\u57fa\u4e8eRTUS\uff08\u672a\u77e5\u573a\u666f\u96c6\u5408\u70b9\u8ddf\u8e2a\uff09\u673a\u5236\u5236\u5b9a\u7b56\u7565\u6765\u8ddf\u8e2a\u8ba1\u5212\uff0c\u8003\u8651\u672a\u77e5\u73af\u5883\u6761\u4ef6\u3002", "result": "\u5728Gazebo\u4eff\u771f\u7684\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u53ca\u65f6\u8ddf\u8e2a\u8ba1\u5212\u5e76\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684MILP\u89c4\u5212\u751f\u6210\u5668\u548cRTUS\u8ddf\u8e2a\u673a\u5236\u80fd\u591f\u6709\u6548\u89e3\u51b3MRE-CCIC\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12014", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12014", "abs": "https://arxiv.org/abs/2511.12014", "authors": ["Truong Vo", "Sanmi Koyejo"], "title": "CURE: Cultural Understanding and Reasoning Evaluation - A Framework for \"Thick\" Culture Alignment Evaluation in LLMs", "comment": "7 pages, 5 figures", "summary": "Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6587\u5316\u80fd\u529b\u7684\u539a\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u5883\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u7ef4\u5ea6\u6307\u6807\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8584\u8bc4\u4f30\u4f1a\u9ad8\u4f30\u6a21\u578b\u6587\u5316\u80fd\u529b\u4e14\u8bc4\u4f30\u4e0d\u7a33\u5b9a\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6587\u5316\u80fd\u529b\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u53bb\u60c5\u5883\u5316\u7684\u6b63\u786e\u6027\u6216\u5f3a\u5236\u9009\u62e9\u5224\u65ad\uff0c\u5ffd\u89c6\u4e86\u6587\u5316\u7406\u89e3\u548c\u63a8\u7406\u7684\u9700\u6c42\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u73b0\u5b9e\u60c5\u5883\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u6a21\u578b\u8fdb\u884c\u6587\u5316\u57fa\u7840\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u4e2a\u8865\u5145\u6307\u6807\uff08\u8986\u76d6\u5ea6\u3001\u7279\u5f02\u6027\u3001\u5185\u6db5\u548c\u8fde\u8d2f\u6027\uff09\u6765\u591a\u7ef4\u5ea6\u8bc4\u4f30\u54cd\u5e94\u8d28\u91cf\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u663e\u793a\uff0c\u4f20\u7edf\u8584\u8bc4\u4f30\u4f1a\u7cfb\u7edf\u6027\u9ad8\u4f30\u6587\u5316\u80fd\u529b\u4e14\u8bc4\u4f30\u65b9\u5dee\u5927\uff0c\u800c\u539a\u8bc4\u4f30\u80fd\u63ed\u793a\u63a8\u7406\u6df1\u5ea6\u5dee\u5f02\u3001\u51cf\u5c11\u65b9\u5dee\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u53ef\u89e3\u91ca\u7684\u6587\u5316\u7406\u89e3\u4fe1\u53f7\u3002", "conclusion": "\u539a\u8bc4\u4f30\u65b9\u6cd5\u6bd4\u8584\u8bc4\u4f30\u66f4\u9002\u5408\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u80fd\u529b\uff0c\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u7a33\u5b9a\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u7ed3\u679c\u3002"}}
{"id": "2511.12361", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12361", "abs": "https://arxiv.org/abs/2511.12361", "authors": ["Leroy D'Souza", "Akash Karthikeyan", "Yash Vardhan Pant", "Sebastian Fischmeister"], "title": "SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty", "comment": null, "summary": "Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.\n  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.", "AI": {"tldr": "\u63d0\u51faSAC-MoE\u65b9\u6cd5\uff0c\u5728Soft Actor-Critic\u6846\u67b6\u4e2d\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u548c\u81ea\u9002\u5e94\u8def\u7531\u5668\u6765\u5904\u7406\u5177\u6709\u4e0d\u53ef\u89c2\u6d4b\u53c2\u6570\u548c\u6a21\u5f0f\u5207\u6362\u7684\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u5b58\u5728\u4e0d\u53ef\u89c2\u6d4b\u7684\u6f5c\u5728\u53c2\u6570\u548c\u6a21\u5f0f\u5207\u6362\u4e8b\u4ef6\uff0c\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u6807\u51c6\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u7a81\u7136\u7684\u6a21\u5f0f\u5207\u6362\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u5c06SAC\u6846\u67b6\u4e2d\u7684\u884c\u52a8\u8005\u5efa\u6a21\u4e3a\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u5305\u542b\u5b66\u4e60\u5230\u7684\u8def\u7531\u5668\u81ea\u9002\u5e94\u9009\u62e9\u4e13\u5bb6\uff1b\u5f00\u53d1\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u4f18\u5148\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u6536\u96c6\u6570\u636e\u3002", "result": "\u5728\u6df7\u5408\u81ea\u4e3b\u8d5b\u8f66\u548c\u817f\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u4efb\u52a1\u4e2d\u7684\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0cSAC-MoE\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u73af\u5883\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u6700\u9ad8\u8fbe6\u500d\uff09\uff0c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u5728\u6240\u6709\u8bc4\u4f30\u7b56\u7565\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SAC-MoE\u80fd\u6709\u6548\u5904\u7406\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u6a21\u5f0f\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u89e3\u91ca\u7684MoE\u8def\u7531\u5668\u80fd\u4e3a\u4e0d\u540c\u6f5c\u5728\u6a21\u5f0f\u6fc0\u6d3b\u4e0d\u540c\u4e13\u5bb6\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12109", "abs": "https://arxiv.org/abs/2511.12109", "authors": ["Felipe Fujita", "Hideyuki Takada"], "title": "Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task", "comment": null, "summary": "In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.", "AI": {"tldr": "\u7ed3\u5408\u53cd\u5411\u7ffb\u8bd1\u548c\u5fae\u8c03\u5728\u5c0f\u578b\u65e5\u8bed\u8bed\u6599\u5e93\u4e0a\u663e\u8457\u63d0\u5347\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4e5f\u80fd\u83b7\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\uff08\u82f1\u8bed\u2192\u65e5\u8bed\uff09\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u53cd\u5411\u7ffb\u8bd1\u548c\u5fae\u8c03\u6280\u672f\u6765\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u9996\u5148\u4f7f\u7528\u53cd\u5411\u7ffb\u8bd1\u4ece\u5355\u8bed\u65e5\u8bed\u8bed\u6599\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7136\u540e\u5728\u5c0f\u89c4\u6a21\u771f\u5b9e\u5e73\u884c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6700\u540e\u5c06\u4e24\u79cd\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u4ece\u57fa\u7ebfCOMET=0.460\u5f00\u59cb\uff0c\u53cd\u5411\u7ffb\u8bd1\u63d0\u5347\u81f30.468\uff0c\u5fae\u8c03\u5927\u5e45\u63d0\u5347\u81f30.589\uff0c\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\u8fbe\u5230\u6700\u4f73\u6027\u80fdCOMET=0.597\u3002", "conclusion": "\u53cd\u5411\u7ffb\u8bd1\u548c\u9488\u5bf9\u6027\u5fae\u8c03\u7684\u534f\u540c\u4f7f\u7528\u80fd\u591f\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u6709\u9650\u8bad\u7ec3\u6570\u636e\u573a\u666f\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12380", "abs": "https://arxiv.org/abs/2511.12380", "authors": ["Nicholas Gunter", "Heiko Kabutz", "Kaushik Jayaram"], "title": "Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots", "comment": null, "summary": "Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.", "AI": {"tldr": "\u5f00\u53d1\u591a\u5c42PVDF\u538b\u7535\u81f4\u52a8\u5668\uff0c\u5728\u8106\u6027\u9ad8\u529bPZT\u5806\u6808\u548c\u67d4\u987a\u4f46\u4f4e\u5e26\u5bbd\u8f6f\u805a\u5408\u7269\u81f4\u52a8\u5668\u4e4b\u95f4\u5efa\u7acb\u72ec\u7279\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u5927\u4f4d\u79fb\u3001\u9ad8\u529b\u548c\u9ad8\u9891\u6027\u80fd\u3002", "motivation": "\u591a\u5c42PVDF\u538b\u7535\u81f4\u52a8\u5668\u662f\u63d0\u5347\u8f6f\u5fae\u673a\u5668\u4eba\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u65e8\u5728\u586b\u8865\u8106\u6027\u9ad8\u529bPZT\u5806\u6808\u4e0e\u67d4\u987a\u4f46\u4f4e\u5e26\u5bbd\u8f6f\u805a\u5408\u7269\u81f4\u52a8\u5668\u4e4b\u95f4\u7684\u8bbe\u8ba1\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u591a\u5c42PVDF\u81f4\u52a8\u5668\uff0c\u91c7\u7528\u5e76\u884c\u7535\u538b\u5206\u5e03\u8bbe\u8ba1\uff0c\u7814\u7a76\u5c42\u539a\u5ea6\u548c\u5c42\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u7b2c\u4e00\u539f\u7406\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u81f4\u52a8\u5668\u5b9e\u73b0\u4e86>3mm\u81ea\u7531\u504f\u8f6c\u3001>20mN\u963b\u585e\u529b\u548c\u2265500Hz\u9891\u7387\uff0c\u5de5\u4f5c\u7535\u538b\u4f4e\u81f3150V\u3002\u96c6\u6210\u5230\u5e73\u9762\u79fb\u52a8\u5fae\u673a\u5668\u4eba\u4e2d\uff0c\u5229\u7528\u5171\u632f\u5b9e\u73b0\u8fd0\u52a8\u5e76\u5177\u6709\u6297\u5927\u6270\u52a8\u9c81\u68d2\u6027\u3002", "conclusion": "\u591a\u5c42PVDF\u81f4\u52a8\u5668\u5728\u6027\u80fd\u53c2\u6570\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5fae\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8f6f\u673a\u5668\u4eba\u96c6\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12116", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12116", "abs": "https://arxiv.org/abs/2511.12116", "authors": ["Piotr P\u0119zik", "Konrad Kaczy\u0144ski", "Maria Szyma\u0144ska", "Filip \u017barnecki", "Zuzanna Deckert", "Jakub Kwiatkowski", "Wojciech Janowski"], "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.", "AI": {"tldr": "LLMLagBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u8bad\u7ec3\u6570\u636e\u65f6\u95f4\u8fb9\u754c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc6\u522b\u6a21\u578b\u77e5\u8bc6\u7684\u6700\u65b0\u65f6\u95f4\u754c\u9650\uff0c\u907f\u514dLLM\u5728\u63a8\u7406\u65f6\u6df7\u5408\u8fc7\u65f6\u4fe1\u606f\u3002", "motivation": "LLM\u5728\u7279\u5b9a\u65f6\u95f4\u70b9\u524d\u7684\u6587\u672c\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u5f62\u6210\u4e86\u4e25\u683c\u7684\u77e5\u8bc6\u8fb9\u754c\u3002\u5f53\u8fd9\u4e2a\u9650\u5236\u672a\u77e5\u6216\u88ab\u5ffd\u7565\u65f6\uff0cLLM\u53ef\u80fd\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u65e0\u610f\u95f4\u5c06\u8fc7\u65f6\u7684\u65f6\u6548\u6027\u4fe1\u606f\u4e0e\u4e00\u822c\u77e5\u8bc6\u6df7\u5408\uff0c\u4ece\u800c\u5f71\u54cd\u56de\u7b54\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165LLMLagBench\u4f5c\u4e3a\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30LLM\u5bf9\u8fd1\u671f\u4e8b\u4ef6\u7684\u77e5\u8bc6\u6765\u8bc6\u522b\u5176\u8bad\u7ec3\u6570\u636e\u7684\u6700\u65e9\u53ef\u80fd\u65f6\u95f4\u8fb9\u754c\u3002\u5c06\u8be5\u57fa\u51c6\u5e94\u7528\u4e8e\u5927\u91cfLLM\u8bc4\u4f30\uff0c\u5305\u62ec\u660e\u786e\u58f0\u660e\u548c\u672a\u58f0\u660e\u8bad\u7ec3\u622a\u6b62\u65f6\u95f4\u7684\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u548c\u4e0e\u516c\u5f00\u7684LLM\u9884\u8bad\u7ec3\u4fe1\u606f\u6bd4\u8f83\u6765\u8bc4\u4f30\u57fa\u51c6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "LLMLagBench\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u8bc6\u522bLLM\u7684\u77e5\u8bc6\u65f6\u95f4\u8fb9\u754c\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u7684\u77e5\u8bc6\u5c40\u9650\u6027\u5e76\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\u3002"}}
{"id": "2511.12383", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12383", "abs": "https://arxiv.org/abs/2511.12383", "authors": ["Sanjar Atamuradov"], "title": "Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks", "comment": "7 pages, 5 figures", "summary": "Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86MAML-TRPO\u5728MetaWorld ML10\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5143\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5355\u6b21\u9002\u5e94\u80fd\u529b\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u6cdb\u5316\u5dee\u8ddd\u548c\u4efb\u52a1\u95f4\u6027\u80fd\u5dee\u5f02\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u5143\u5b66\u4e60\u7b97\u6cd5\u80fd\u591f\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u8fd9\u5bf9\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u5143\u5b66\u4e60(MAML)\u548c\u4fe1\u4efb\u57df\u7b56\u7565\u4f18\u5316(TRPO)\uff0c\u5728\u5305\u542b10\u4e2a\u4e0d\u540c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684MetaWorld ML10\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "MAML\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5355\u6b21\u9002\u5e94\uff0c\u5728\u8bad\u7ec3\u4efb\u52a1\u4e0a\u8fbe\u523021.0%\u7684\u6210\u529f\u7387\uff0c\u6d4b\u8bd5\u4efb\u52a1\u4e0a\u8fbe\u523013.2%\u7684\u6210\u529f\u7387\uff0c\u4f46\u5b58\u5728\u6cdb\u5316\u5dee\u8ddd\uff0c\u4e14\u4e0d\u540c\u64cd\u4f5c\u6280\u80fd\u7684\u6210\u529f\u7387\u5dee\u5f02\u5f88\u5927\uff080%\u523080%\uff09\u3002", "conclusion": "\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u65e2\u6709\u524d\u666f\u4e5f\u6709\u5c40\u9650\u6027\uff0c\u672a\u6765\u9700\u8981\u5728\u4efb\u52a1\u611f\u77e5\u9002\u5e94\u548c\u7ed3\u6784\u5316\u7b56\u7565\u67b6\u6784\u65b9\u9762\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.12130", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12130", "abs": "https://arxiv.org/abs/2511.12130", "authors": ["Bingbing Wang", "Zhixin Bai", "Zhengda Jin", "Zihan Wang", "Xintong Song", "Jingjie Lin", "Sixuan Li", "Jing Li", "Ruifeng Xu"], "title": "PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection", "comment": null, "summary": "The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86U-MStance\u6570\u636e\u96c6\u548cPRISM\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u753b\u50cf\u548c\u591a\u6a21\u6001\u63a8\u7406\u89e3\u51b3\u591a\u6a21\u6001\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u4e2d\u7684\u4f2a\u591a\u6a21\u6001\u548c\u7528\u6237\u540c\u8d28\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5b58\u5728\u4f2a\u591a\u6a21\u6001\uff08\u4ec5\u6e90\u5e16\u5b50\u5305\u542b\u89c6\u89c9\u7ebf\u7d22\uff0c\u8bc4\u8bba\u88ab\u89c6\u4e3a\u7eaf\u6587\u672c\uff09\u548c\u7528\u6237\u540c\u8d28\u5316\uff08\u5ffd\u7565\u4e2a\u4eba\u7279\u8d28\u5bf9\u7acb\u573a\u8868\u8fbe\u7684\u5f71\u54cd\uff09\u7684\u5c40\u9650\u6027\u3002", "method": "PRISM\u6a21\u578b\uff1a1\uff09\u4ece\u5386\u53f2\u5e16\u5b50\u63a8\u5bfc\u7eb5\u5411\u7528\u6237\u753b\u50cf\uff1b2\uff09\u901a\u8fc7\u601d\u7ef4\u94fe\u5bf9\u9f50\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e2d\u7684\u6587\u672c\u548c\u89c6\u89c9\u7ebf\u7d22\uff1b3\uff09\u91c7\u7528\u76f8\u4e92\u4efb\u52a1\u5f3a\u5316\u673a\u5236\u8054\u5408\u4f18\u5316\u7acb\u573a\u68c0\u6d4b\u548c\u7acb\u573a\u611f\u77e5\u54cd\u5e94\u751f\u6210\u3002", "result": "\u5728U-MStance\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPRISM\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7528\u6237\u4e2d\u5fc3\u548c\u4e0a\u4e0b\u6587\u57fa\u7840\u7684\u591a\u6a21\u6001\u63a8\u7406\u5bf9\u4e8e\u73b0\u5b9e\u7acb\u573a\u7406\u89e3\u5177\u6709\u6709\u6548\u6027\u3002"}}
{"id": "2511.12390", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12390", "abs": "https://arxiv.org/abs/2511.12390", "authors": ["Sanjar Atamuradov"], "title": "Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control", "comment": "9 pages, 5 figures", "summary": "Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u795e\u7ecf\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7b56\u7565\u66ff\u4ee3\u4f20\u7edf\u9006\u8fd0\u52a8\u5b66+PD\u63a7\u5236\u5668\uff0c\u76f4\u63a5\u6620\u5c04VR\u63a7\u5236\u5668\u8f93\u5165\u5230\u673a\u5668\u4eba\u5173\u8282\u547d\u4ee4\uff0c\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u8ddf\u8e2a\u8bef\u5dee\u3001\u66f4\u5e73\u6ed1\u7684\u8fd0\u52a8\u548c\u66f4\u597d\u7684\u529b\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u9065\u64cd\u4f5c\u7cfb\u7edf\u4f9d\u8d56\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u548c\u624b\u52a8\u8c03\u6574\u7684PD\u63a7\u5236\u5668\uff0c\u96be\u4ee5\u5904\u7406\u5916\u529b\u5e72\u6270\u3001\u9002\u5e94\u4e0d\u540c\u7528\u6237\uff0c\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u65e0\u6cd5\u4ea7\u751f\u81ea\u7136\u8fd0\u52a8\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u7528\u57fa\u4e8e\u9006\u8fd0\u52a8\u5b66\u7684\u9065\u64cd\u4f5c\u6f14\u793a\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u7136\u540e\u901a\u8fc7\u529b\u968f\u673a\u5316\u548c\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u5956\u52b1\u8fdb\u884c\u5fae\u8c03\uff0c\u76f4\u63a5\u5b66\u4e60\u4eceVR\u63a7\u5236\u5668\u8f93\u5165\u5230\u673a\u5668\u4eba\u5173\u8282\u547d\u4ee4\u7684\u6620\u5c04\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5b66\u4e60\u7b56\u7565\u76f8\u6bd4\u9006\u8fd0\u52a8\u5b66\u57fa\u7ebf\u5b9e\u73b0\u4e8634%\u66f4\u4f4e\u7684\u8ddf\u8e2a\u8bef\u5dee\u300145%\u66f4\u5e73\u6ed1\u7684\u8fd0\u52a8\u548c\u66f4\u4f18\u7684\u529b\u9002\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\uff0850Hz\u63a7\u5236\u9891\u7387\uff09\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7cfb\u7edf\u7684\u81ea\u7136\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.12133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12133", "abs": "https://arxiv.org/abs/2511.12133", "authors": ["Qingyu Zhang", "Chunlei Xin", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Qing Ye", "Qianlong Xie", "Xingxing Wang"], "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing", "comment": null, "summary": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86AI-Salesman\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u67b6\u6784\u89e3\u51b3\u76ee\u6807\u9a71\u52a8\u8bf4\u670d\u5bf9\u8bdd\u4e2d\u7684\u591a\u8f6e\u89c4\u5212\u548c\u4e8b\u5b9e\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u76ee\u6807\u9a71\u52a8\u8bf4\u670d\u5bf9\u8bdd\uff08\u5982\u7535\u8bdd\u9500\u552e\uff09\u9700\u8981\u590d\u6742\u591a\u8f6e\u89c4\u5212\u548c\u4e25\u683c\u4e8b\u5b9e\u5fe0\u5b9e\u6027\uff0c\u73b0\u6709LLMs\u5b58\u5728\u6218\u7565\u8106\u5f31\u6027\u548c\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u6570\u636e\u3002", "method": "\u6784\u5efaTeleSalesCorpus\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u53cc\u9636\u6bb5AI-Salesman\u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u8d1d\u53f6\u65af\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4ece\u566a\u58f0\u5bf9\u8bdd\u4e2d\u5b66\u4e60\u7a33\u5065\u9500\u552e\u7b56\u7565\uff1b\u63a8\u7406\u9636\u6bb5\u4f7f\u7528\u52a8\u6001\u5927\u7eb2\u5f15\u5bfc\u4ee3\u7406(DOGA)\u7ed3\u5408\u9884\u5efa\u811a\u672c\u5e93\u63d0\u4f9b\u52a8\u6001\u7b56\u7565\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAI-Salesman\u5728\u81ea\u52a8\u6307\u6807\u548c\u7efc\u5408\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u590d\u6742\u8bf4\u670d\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "AI-Salesman\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u76ee\u6807\u9a71\u52a8\u8bf4\u670d\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u548c\u52a8\u6001\u7b56\u7565\u6307\u5bfc\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12436", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12436", "abs": "https://arxiv.org/abs/2511.12436", "authors": ["Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Yanbiao Ma", "Yunfeng Diao", "Ziyu Jia", "Wenbo Ding", "Hangjun Ye", "Long Chen"], "title": "RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation", "comment": null, "summary": "Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoboAfford++\u6570\u636e\u96c6\u548cRoboAfford-Eval\u57fa\u51c6\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u4f53\u548c\u7a7a\u95f4\u53ef\u64cd\u4f5c\u6027\u5b66\u4e60\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u64cd\u4f5c\u548c\u5bfc\u822a\u4e2d\u7684\u53ef\u64cd\u4f5c\u6027\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u64c5\u957f\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u548c\u573a\u666f\u7406\u89e3\uff0c\u4f46\u5728\u63a8\u65ad\u7269\u7406\u4ea4\u4e92\u7684\u53ef\u64cd\u4f5c\u4f4d\u7f6e\uff08\u5982\u529f\u80fd\u6027\u6293\u53d6\u70b9\u548c\u5141\u8bb8\u653e\u7f6e\u533a\u57df\uff09\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u7269\u4f53\u548c\u7a7a\u95f4\u53ef\u64cd\u4f5c\u6027\u6807\u6ce8\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b869,987\u5f20\u56fe\u50cf\u548c200\u4e07\u95ee\u7b54\u6807\u6ce8\u7684RoboAfford++\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u7269\u4f53\u53ef\u64cd\u4f5c\u6027\u8bc6\u522b\u3001\u7269\u4f53\u53ef\u64cd\u4f5c\u6027\u9884\u6d4b\u548c\u7a7a\u95f4\u53ef\u64cd\u4f5c\u6027\u5b9a\u4f4d\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542b338\u4e2a\u6837\u672c\u7684RoboAfford-Eval\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u64cd\u4f5c\u6027\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u800c\u5728RoboAfford++\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5728\u7269\u4f53\u548c\u7a7a\u95f4\u53ef\u64cd\u4f5c\u6027\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RoboAfford++\u6570\u636e\u96c6\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u53ef\u64cd\u4f5c\u6027\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5bfc\u822a\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2511.12140", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12140", "abs": "https://arxiv.org/abs/2511.12140", "authors": ["Pinxue Guo", "Chongruo Wu", "Xinyu Zhou", "Lingyi Hong", "Zhaoyu Chen", "Jinglun Li", "Kaixun Jiang", "Sen-ching Samson Cheung", "Wei Zhang", "Wenqiang Zhang"], "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.", "AI": {"tldr": "VBackChecker\u662f\u4e00\u4e2a\u65e0\u9700\u53c2\u8003\u7684\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7Grounding LLM\u9a8c\u8bc1MLLM\u751f\u6210\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\uff0c\u5728R^2-HalBench\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u51c6\u786e\u68c0\u6d4b\u4ee5\u786e\u4fdd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8e\"\u773c\u89c1\u4e3a\u5b9e\"\u539f\u5219\uff0c\u4f7f\u7528\u5177\u5907\u63a8\u7406\u548c\u53c2\u8003\u5206\u5272\u80fd\u529b\u7684\u50cf\u7d20\u7ea7Grounding LLM\u6765\u9a8c\u8bc1\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86R-Instruct\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5728R^2-HalBench\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u590d\u6742\u6846\u67b6\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-4o\uff0c\u5728\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc710%\u3002", "conclusion": "VBackChecker\u4e3aMLLM\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53c2\u8003\u514d\u8d39\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5904\u7406\u4e30\u5bcc\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.12479", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12479", "abs": "https://arxiv.org/abs/2511.12479", "authors": ["Navin Sriram Ravie", "Keerthi Vasan M", "Bijo Sebastian"], "title": "ClutterNav: Gradient-Guided Search for Efficient 3D Clutter Removal with Learned Costmaps", "comment": null, "summary": "Dense clutter removal for target object retrieval presents a challenging problem, especially when targets are embedded deep within densely-packed configurations. It requires foresight to minimize overall changes to the clutter configuration while accessing target objects, avoiding stack destabilization and reducing the number of object removals required. Rule-based planners when applied to this problem, rely on rigid heuristics, leading to high computational overhead. End-to-end reinforcement learning approaches struggle with interpretability and generalizability over different conditions. To address these issues, we present ClutterNav, a novel decision-making framework that can identify the next best object to be removed so as to access a target object in a given clutter, while minimising stack disturbances. ClutterNav formulates the problem as a continuous reinforcement learning task, where each object removal dynamically updates the understanding of the scene. A removability critic, trained from demonstrations, estimates the cost of removing any given object based on geometric and spatial features. This learned cost is complemented by integrated gradients that assess how the presence or removal of surrounding objects influences the accessibility of the target. By dynamically prioritizing actions that balance immediate removability against long-term target exposure, ClutterNav achieves near human-like strategic sequencing, without predefined heuristics. The proposed approach is validated extensively in simulation and over real-world experiments. The results demonstrate real-time, occlusion-aware decision-making in partially observable environments.", "AI": {"tldr": "ClutterNav\u662f\u4e00\u4e2a\u7528\u4e8e\u5bc6\u96c6\u6742\u7269\u4e2d\u76ee\u6807\u7269\u4f53\u68c0\u7d22\u7684\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u79fb\u9664\u6027\u8bc4\u4f30\u6765\u6700\u5c0f\u5316\u5806\u6808\u6270\u52a8\u5e76\u51cf\u5c11\u7269\u4f53\u79fb\u9664\u6570\u91cf\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u6742\u7269\u4e2d\u76ee\u6807\u7269\u4f53\u68c0\u7d22\u7684\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u89c4\u5212\u5668\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u5dee\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u8fde\u7eed\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u4f7f\u7528\u4ece\u6f14\u793a\u4e2d\u8bad\u7ec3\u7684\u53ef\u79fb\u9664\u6027\u8bc4\u4f30\u5668\u4f30\u8ba1\u79fb\u9664\u6210\u672c\uff0c\u7ed3\u5408\u79ef\u5206\u68af\u5ea6\u8bc4\u4f30\u5468\u56f4\u7269\u4f53\u5bf9\u76ee\u6807\u53ef\u8bbf\u95ee\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u906e\u6321\u611f\u77e5\u7684\u51b3\u7b56\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u7b56\u7565\u5e8f\u5217\u3002", "conclusion": "ClutterNav\u65e0\u9700\u9884\u5b9a\u4e49\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u80fd\u6709\u6548\u5e73\u8861\u5373\u65f6\u53ef\u79fb\u9664\u6027\u548c\u957f\u671f\u76ee\u6807\u66b4\u9732\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6742\u7269\u6e05\u7406\u7b56\u7565\u3002"}}
{"id": "2511.12159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12159", "abs": "https://arxiv.org/abs/2511.12159", "authors": ["Yaocheng Zhang", "Haohuan Huang", "Zijun Song", "Yuanheng Zhu", "Qichao Zhang", "Zijie Zhao", "Dongbin Zhao"], "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic", "comment": "17 pages, 10 figures", "summary": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.", "AI": {"tldr": "CriticSearch\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u56de\u987e\u6027\u6279\u8bc4\u673a\u5236\u63d0\u4f9b\u5bc6\u96c6\u7684\u56de\u5408\u7ea7\u53cd\u9988\uff0c\u89e3\u51b3\u641c\u7d22\u4ee3\u7406\u4e2d\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u7684\u4f4e\u6548\u63a2\u7d22\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u7ba1\u9053\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u4f46\u9762\u4e34\u7a00\u758f\u7ed3\u679c\u5956\u52b1\u95ee\u9898\uff0c\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u975e\u5bf9\u79f0\u6279\u8bc4LLM\uff0c\u57fa\u4e8e\u5b8c\u6574\u8f68\u8ff9\u548c\u9ec4\u91d1\u7b54\u6848\u7684\u6743\u9650\u4fe1\u606f\u56de\u987e\u6027\u8bc4\u4f30\u6bcf\u4e2a\u56de\u5408\uff0c\u5c06\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7a33\u5b9a\u7684\u5bc6\u96c6\u5956\u52b1\u6765\u6307\u5bfc\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u5728\u591a\u6837\u5316\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCriticSearch\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u3001\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "CriticSearch\u901a\u8fc7\u5bc6\u96c6\u7684\u56de\u5408\u7ea7\u53cd\u9988\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u641c\u7d22\u4ee3\u7406\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2511.12526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12526", "abs": "https://arxiv.org/abs/2511.12526", "authors": ["Davide De Benedittis", "Giovanni Di Lorenzo", "Franco Angelini", "Barbara Valle", "Marina Serena Borgatti", "Paolo Remagnino", "Marco Caccianiga", "Manolo Garabini"], "title": "Botany Meets Robotics in Alpine Scree Monitoring", "comment": "Published as Early Access in IEEE Transactions on Field Robotics. 19 pages, 13 figures", "summary": "According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u817f\u5f0f\u673a\u5668\u4eba\u76d1\u6d4b\u788e\u77f3\u6816\u606f\u5730\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u90e8\u7f72ANYmal C\u673a\u5668\u4eba\u5728\u610f\u5927\u5229\u963f\u5c14\u5351\u65af\u5c71\u533a\u8fdb\u884c\u690d\u7269\u7269\u79cd\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u63d0\u9ad8\u4e86\u76d1\u6d4b\u6548\u7387\u548c\u9891\u7387\u3002", "motivation": "\u788e\u77f3\u6816\u606f\u5730\u56e0\u9ad8\u6d77\u62d4\u7279\u6027\u9762\u4e34\u6c14\u5019\u53d8\u5316\u4e25\u91cd\u5a01\u80c1\uff0c\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u9700\u8981\u4e13\u5bb6\u5728\u504f\u8fdc\u5371\u9669\u5730\u533a\u8fdb\u884c\u5b9e\u5730\u8003\u5bdf\uff0c\u8fc7\u7a0b\u8d44\u6e90\u5bc6\u96c6\u4e14\u8017\u65f6\u3002", "method": "\u5728\u610f\u5927\u5229\u963f\u5c14\u5351\u65af\u751f\u7269\u533a\u90e8\u7f72ANYmal C\u817f\u5f0f\u673a\u5668\u4eba\u8fdb\u884c\u4e24\u5e74\u5b9e\u5730\u8003\u5bdf\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u548c\u5206\u7c7b\u5173\u952e\u690d\u7269\u7269\u79cd\uff0c\u5e76\u4e0e\u4f20\u7edf\u690d\u7269\u793e\u4f1a\u5b66\u8c03\u67e5\u76f8\u7ed3\u5408\u3002", "result": "\u654f\u6377\u817f\u5f0f\u673a\u5668\u4eba\u80fd\u591f\u5bfc\u822a\u5177\u6709\u6311\u6218\u6027\u7684\u5730\u5f62\uff0c\u63d0\u9ad8\u788e\u77f3\u76d1\u6d4b\u7684\u9891\u7387\u548c\u6548\u7387\uff0c\u540c\u65f6\u4f18\u5316\u6570\u636e\u91c7\u96c6\u3001\u5b58\u50a8\u548c\u4f7f\u7528\u6d41\u7a0b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u73af\u5883\u79d1\u5b66\u4e2d\u7684\u673a\u5668\u4eba\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u4e3a\u66f4\u5168\u9762\u548c\u53ef\u6301\u7eed\u7684\u6816\u606f\u5730\u76d1\u6d4b\u4e0e\u4fdd\u62a4\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12213", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12213", "abs": "https://arxiv.org/abs/2511.12213", "authors": ["Liang Xue", "Haoyu Liu", "Yajun Tian", "Xinyu Zhong", "Yang Liu"], "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues", "comment": null, "summary": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.", "AI": {"tldr": "MME-RAG\u662f\u4e00\u4e2a\u591a\u7ba1\u7406\u5668-\u4e13\u5bb6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5b9e\u4f53\u8bc6\u522b\u5206\u89e3\u4e3a\u7c7b\u578b\u7ea7\u5224\u65ad\u548c\u8de8\u5ea6\u7ea7\u63d0\u53d6\u4e24\u4e2a\u534f\u8c03\u9636\u6bb5\uff0c\u89e3\u51b3\u4e86LLM\u5728\u9886\u57df\u9002\u5e94\u548c\u68c0\u7d22\u53ef\u63a7\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u8bc6\u522b\u9762\u4e34\u9886\u57df\u9002\u5e94\u548c\u68c0\u7d22\u53ef\u63a7\u6027\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u9886\u57df\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a\u8f7b\u91cf\u7ea7\u7ba1\u7406\u5668\u8fdb\u884c\u7c7b\u578b\u7ea7\u5224\u65ad\uff0c\u4e13\u4e1a\u4e13\u5bb6\u8fdb\u884c\u8de8\u5ea6\u7ea7\u63d0\u53d6\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u914d\u5907KeyInfo\u68c0\u7d22\u5668\u6ce8\u5165\u8bed\u4e49\u5bf9\u9f50\u7684\u5c11\u6837\u672c\u793a\u4f8b\u3002", "result": "\u5728CrossNER\u3001MIT-Movie\u3001MIT-Restaurant\u548c\u65b0\u6784\u5efa\u7684\u591a\u9886\u57df\u5ba2\u670d\u6570\u636e\u96c6\u4e0a\uff0cMME-RAG\u5728\u5927\u591a\u6570\u9886\u57df\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5206\u5c42\u5206\u89e3\u548cKeyInfo\u5f15\u5bfc\u7684\u68c0\u7d22\u662f\u9c81\u68d2\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0cMME-RAG\u4e3a\u81ea\u9002\u5e94\u5bf9\u8bdd\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12618", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12618", "abs": "https://arxiv.org/abs/2511.12618", "authors": ["Jordan Leyva", "Nahim J. Moran Vera", "Yihan Xu", "Adrien Durasno", "Christopher U. Romero", "Tendai Chimuka", "Gabriel O. Huezo Ramirez", "Ziqian Dong", "Roberto Rojas-Cessa"], "title": "EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones", "comment": "Autonomous drone, A* algorithm, 3D environments, path planning, obstacle avoidance, energy efficiency, MIT Conference", "summary": "Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.", "AI": {"tldr": "EcoFlight\u662f\u4e00\u79cd\u80fd\u91cf\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u80fd\u57283D\u7a7a\u95f4\u4e2d\u907f\u5f00\u969c\u788d\u7269\u627e\u5230\u80fd\u8017\u6700\u4f4e\u7684\u98de\u884c\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6848\u5f88\u5c11\u8003\u8651\u969c\u788d\u7269\u907f\u8ba9\uff0c\u800c\u907f\u969c\u53c8\u662f\u80fd\u91cf\u5bc6\u96c6\u578b\u64cd\u4f5c\uff0c\u5bf9\u70b9\u5bf9\u70b9\u65e0\u4eba\u673a\u98de\u884c\u7684\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u65e0\u4eba\u673a\u63a8\u8fdb\u7cfb\u7edf\u548c\u98de\u884c\u52a8\u529b\u5b66\u5efa\u6a21\u80fd\u8017\uff0c\u57283D\u7a7a\u95f4\u4e2d\u5bfb\u627e\u80fd\u8017\u6700\u4f4e\u7684\u907f\u969c\u8def\u5f84\u3002", "result": "\u5728\u5404\u79cd\u969c\u788d\u7269\u5bc6\u5ea6\u4e0b\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cEcoFlight\u59cb\u7ec8\u627e\u5230\u6bd4\u76f4\u63a5\u98de\u884c\u548c\u6700\u77ed\u8ddd\u79bb\u65b9\u6848\u80fd\u8017\u66f4\u4f4e\u7684\u8def\u5f84\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5bc6\u5ea6\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u3002\u5408\u9002\u7684\u98de\u884c\u901f\u5ea6\u8fd8\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u8282\u80fd\u6548\u679c\u3002", "conclusion": "EcoFlight\u7b97\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u65e0\u4eba\u673a\u5728\u907f\u969c\u98de\u884c\u4e2d\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u4e3a\u80fd\u91cf\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.12236", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12236", "abs": "https://arxiv.org/abs/2511.12236", "authors": ["Raavi Gupta", "Pranav Hari Panicker", "Sumit Bhatia", "Ganesh Ramakrishnan"], "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts", "comment": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025", "summary": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.", "AI": {"tldr": "CONFACTCHECK\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u68c0\u67e5\u751f\u6210\u6587\u672c\u4e2d\u4e8b\u5b9e\u63a2\u9488\u54cd\u5e94\u7684\u4e00\u81f4\u6027\u6765\u68c0\u6d4bLLM\u5e7b\u89c9\u3002", "motivation": "LLM\u5728\u6587\u672c\u751f\u6210\u65f6\u7ecf\u5e38\u4ea7\u751f\u4e8b\u5b9e\u9519\u8bef\u7684\u5e7b\u89c9\uff0c\u8fd9\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u9886\u57df\u5b58\u5728\u4e25\u91cd\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u591a\u6b21API\u8c03\u7528\uff0c\u589e\u52a0\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "method": "\u57fa\u4e8e\u5355LLM\u5185\u90e8\u548c\u8de8\u4e0d\u540cLLM\u4e4b\u95f4\u5bf9\u751f\u6210\u6587\u672c\u4e2d\u4e8b\u5b9e\u63a2\u9488\u54cd\u5e94\u7684\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5e93\u6216\u6a21\u578b\u6743\u91cd\u8bbf\u95ee\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCONFACTCHECK\u80fd\u4ee5\u66f4\u5c11\u8d44\u6e90\u9ad8\u6548\u68c0\u6d4b\u5e7b\u89c9\u4e8b\u5b9e\uff0c\u76f8\u6bd4\u7c7b\u4f3c\u6761\u4ef6\u4e0b\u7684\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "CONFACTCHECK\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u3001\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u7684\u5e7b\u89c9\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8eAPI\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2511.12650", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12650", "abs": "https://arxiv.org/abs/2511.12650", "authors": ["Arvind Kumar Mishra", "Sohom Chakrabarty"], "title": "Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning", "comment": "10 pages, 11 figures, It is submitted as a journal option paper associated with the IFAC World Congress 2026", "summary": "In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.\n  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5e73\u9762\u673a\u68b0\u81c2\u5f62\u6001\u4f18\u5316\uff0c\u9a8c\u8bc1\u4e86RL\u80fd\u591f\u4ec5\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u6062\u590d\u5df2\u77e5\u89e3\u6790\u6700\u4f18\u89e3\uff0c\u5e76\u5728\u65e0\u89e3\u6790\u89e3\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3\u9ad8\u7ef4\u5f62\u6001\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u5927\u591a\u6570\u5f62\u6001\u8bbe\u8ba1\u4efb\u52a1\u6ca1\u6709\u95ed\u5f0f\u89e3\uff0c\u7f51\u683c\u641c\u7d22\u548c\u542f\u53d1\u5f0f\u641c\u7d22\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Yoshikawa\u53ef\u64cd\u4f5c\u6027\u6307\u6807\uff0c\u6bd4\u8f83SAC\u3001DDPG\u548cPPO\u4e09\u79cdRL\u7b97\u6cd5\u4e0e\u7f51\u683c\u641c\u7d22\u548c\u9ed1\u76d2\u4f18\u5316\u5668\uff0c\u57282R\u673a\u68b0\u81c2\u8ddf\u8e2a\u5706\u5f62\u8def\u5f84\u7684\u9a8c\u8bc1\u6848\u4f8b\u4e2d\uff0c\u5f62\u6001\u7531\u5355\u4e00\u53c2\u6570phi\u8868\u793a\uff0c\u7136\u540e\u6269\u5c55\u5230\u692d\u5706\u548c\u77e9\u5f62\u8def\u5f84\u7684\u5b8c\u6574\u5f62\u6001\u5411\u91cf(L1, L2, theta2)\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u5728\u9a8c\u8bc1\u6848\u4f8b\u4e2d\u90fd\u6536\u655b\u5230\u89e3\u6790\u89e3\uff0c\u8bc1\u660e\u65e0\u9700\u63d0\u4f9b\u89e3\u6790\u7ed3\u6784\u5373\u53ef\u6570\u503c\u6062\u590d\u6700\u4f18\u89e3\u3002\u5728\u65e0\u89e3\u6790\u89e3\u7684\u60c5\u51b5\u4e0b\uff0cRL\u7ee7\u7eed\u53ef\u9760\u6536\u655b\uff0c\u800c\u7f51\u683c\u548c\u9ed1\u76d2\u65b9\u6cd5\u9700\u8981\u66f4\u5927\u7684\u8bc4\u4f30\u9884\u7b97\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u6062\u590d\u5df2\u77e5\u6700\u4f18\u89e3\u548c\u89e3\u51b3\u65e0\u89e3\u6790\u89e3\u7684\u5f62\u6001\u4f18\u5316\u95ee\u9898\u90fd\u662f\u6709\u6548\u7684\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.12249", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12249", "abs": "https://arxiv.org/abs/2511.12249", "authors": ["Khang T. Huynh", "Dung H. Nguyen", "Binh T. Nguyen"], "title": "ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations", "comment": null, "summary": "Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT", "AI": {"tldr": "\u63d0\u51fa\u4e86ViConBERT\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u57fa\u4e8e\u8bcd\u4e49\u89e3\u91ca\u7684\u77e5\u8bc6\u84b8\u998f\u6765\u5b66\u4e60\u8d8a\u5357\u8bed\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u5e76\u521b\u5efa\u4e86ViConWSD\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u8d8a\u5357\u8bed\u8bed\u4e49\u7406\u89e3\u3002", "motivation": "\u8d8a\u5357\u8bed\u7f3a\u4e4f\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u6a21\u578b\u548c\u8bc4\u4f30\u8d44\u6e90\uff0c\u800c\u73b0\u6709\u8fdb\u5c55\u4e3b\u8981\u9650\u4e8e\u82f1\u8bed\u7b49\u9ad8\u8d44\u6e90\u8bed\u8a00\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\uff08SimCLR\uff09\u548c\u57fa\u4e8e\u8bcd\u4e49\u89e3\u91ca\u7684\u77e5\u8bc6\u84b8\u998f\u6765\u5b66\u4e60\u8d8a\u5357\u8bed\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6ViConWSD\u3002", "result": "ViConBERT\u5728WSD\u4e0aF1=0.87\uff0c\u5728ViCon\u4e0aAP=0.88\uff0c\u5728ViSim-400\u4e0aSpearman's rho=0.60\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ViConBERT\u80fd\u6709\u6548\u5efa\u6a21\u79bb\u6563\u8bcd\u4e49\u548c\u5206\u7ea7\u8bed\u4e49\u5173\u7cfb\uff0c\u4e3a\u8d8a\u5357\u8bed\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2511.12755", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12755", "abs": "https://arxiv.org/abs/2511.12755", "authors": ["Aleesha Khurram", "Amir Moeini", "Shangtong Zhang", "Rohan Chandra"], "title": "Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL", "comment": null, "summary": "Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICRL)\u7684\u63a8\u7406\u65f6\u5c11\u6837\u672c\u63d0\u793a\u9a71\u52a8\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\uff0c\u65e0\u9700\u6a21\u578b\u53c2\u6570\u66f4\u65b0\u6216\u989d\u5916\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u9886\u57df\u81ea\u9002\u5e94\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u3002\u73b0\u6709\u7684\u63d0\u793a\u9a71\u52a8\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u4ec5\u9650\u4e8e\u611f\u77e5\u4efb\u52a1\u4e14\u9700\u8981\u4e13\u5bb6\u5c11\u6837\u672c\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICRL)\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u4e00\u822c\u8f68\u8ff9\u8fdb\u884c\u5c11\u6837\u672c\u63d0\u793a\u9a71\u52a8\u9886\u57df\u81ea\u9002\u5e94\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u6216\u6536\u96c6\u989d\u5916\u5bf9\u6297\u6027\u5929\u6c14\u6570\u636e\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u63d0\u793a\u9a71\u52a8\u9886\u57df\u81ea\u9002\u5e94\u57fa\u7ebf\u65b9\u6cd5\uff0cICRL\u5728\u76ee\u6807\u9886\u57df\u4e2d\u4ea7\u751f\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u548c\u66f4\u8212\u9002\u7684\u9a7e\u9a76\u7b56\u7565\u3002", "conclusion": "ICRL\u65b9\u6cd5\u6210\u529f\u5c06\u63d0\u793a\u9a71\u52a8\u9886\u57df\u81ea\u9002\u5e94\u6269\u5c55\u5230\u95ed\u73af\u9a7e\u9a76\u4efb\u52a1\uff0c\u4f7f\u7528\u63a8\u7406\u65f6\u89c2\u5bdf\u5230\u7684\u4e00\u822c\u8f68\u8ff9\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12281", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12281", "abs": "https://arxiv.org/abs/2511.12281", "authors": ["Ivan Zakazov", "Alexander Sharipov", "Berke Argin", "Oussama Gabouj", "Kamel Charaf", "Alexi Semiz", "Lorenzo Drudi", "Nicolas Baldwin", "Robert West"], "title": "Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor", "comment": null, "summary": "Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u5c0f\u578bLLM\u538b\u7f29\u5927\u578bLLM\u8f93\u5165\u7684\u65b0\u8303\u5f0f\uff0c\u5f00\u53d1\u4e86Cmprsr\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4fe1\u606f\u548c\u63a7\u5236\u538b\u7f29\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u964d\u4f4e\u4f7f\u7528\u9ed1\u76d2\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6210\u672c\uff0c\u901a\u8fc7\u538b\u7f29\u8f93\u5165\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u4f7f\u7528Textgrad\u4f18\u5316\u538b\u7f29\u5143\u63d0\u793a\uff0c\u5bf9Qwen3-4B\u8fdb\u884cSFT\u548cGRPO\u8054\u5408\u8bad\u7ec3\uff0c\u5b9e\u73b0\u538b\u7f29\u7387\u63a7\u5236\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u6700\u5927\u5316\u3002", "result": "Cmprsr\u5728MeetingBank\u3001LongBench\u548cGSM8k\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u62bd\u53d6\u5f0f\u548c\u539f\u59cb\u62bd\u8c61\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u7cbe\u786e\u63a7\u5236\u538b\u7f29\u7387\u3002", "conclusion": "Cmprsr\u6a21\u578b\u5728\u6210\u672c-\u8d28\u91cf\u6743\u8861\u65b9\u9762\u63d0\u4f9b\u4e86\u7cbe\u7ec6\u63a7\u5236\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12778", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12778", "abs": "https://arxiv.org/abs/2511.12778", "authors": ["Vignesh Rajagopal", "Kasun Weerakoon Kulathun Mudiyanselage", "Gershom Devake Seneviratne", "Pon Aswin Sankaralingam", "Mohamed Elnoor", "Jing Liang", "Rohan Chandra", "Dinesh Manocha"], "title": "DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation", "comment": null, "summary": "We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions", "AI": {"tldr": "DR.Nav\u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u5bfc\u822a\u7684\u65b0\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6b7b\u80e1\u540c\u68c0\u6d4b\u548c\u6062\u590d\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002\u5b83\u901a\u8fc7RGB-LiDAR\u878d\u5408\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u751f\u6210\u5b9e\u65f6\u7684\u8bed\u4e49\u6210\u672c\u5730\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u7ecf\u5e38\u9047\u5230\u6b7b\u80e1\u540c\u3001\u690d\u88ab\u906e\u6321\u548c\u963b\u585e\u8def\u53e3\u7b49\u95ee\u9898\uff0c\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u60c5\u51b5\uff0c\u5bfc\u81f4\u5bfc\u822a\u5931\u8d25\u6216\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u8de8\u6a21\u6001RGB-LiDAR\u878d\u5408\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8fc7\u6ee4\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u6301\u7eed\u66f4\u65b0\u6bcf\u4e2a\u5355\u5143\u7684\u6b7b\u4ea1\u6982\u7387\u548c\u6062\u590d\u70b9\uff0c\u5c06\u6062\u590d\u611f\u77e5\u98ce\u9669\u660e\u786e\u7eb3\u5165\u5bfc\u822a\u6210\u672c\u5730\u56fe\u3002", "result": "\u5728\u5bc6\u96c6\u5ba4\u5185\u5916\u573a\u666f\u4e2d\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad883.33%\uff0c\u76ee\u6807\u5230\u8fbe\u65f6\u95f4\u51cf\u5c1152.4%\uff0c\u4f18\u4e8eDWA\u3001MPPI\u548cNav2 DWB\u7b49\u5148\u8fdb\u89c4\u5212\u5668\u3002", "conclusion": "DR.Nav\u901a\u8fc7\u7edf\u4e00\u7684\u6b7b\u80e1\u540c\u9884\u6d4b\u548c\u6062\u590d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u5bfc\u822a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.12290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12290", "abs": "https://arxiv.org/abs/2511.12290", "authors": ["Purnima Bindal", "Vikas Kumar", "Sagar Rathore", "Vasudha Bhatnagar"], "title": "AugAbEx : Way Forward for Extractive Case Summarization", "comment": "30 pages, under review in a Journal", "summary": "Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.\n  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u73b0\u6709\u62bd\u8c61\u6458\u8981\u751f\u6210\u5bf9\u5e94\u63d0\u53d6\u5f0f\u6458\u8981\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u6cd5\u5f8b\u6848\u4f8b\u6458\u8981\u7814\u7a76\u793e\u533a\u521b\u5efa\u4e30\u5bcc\u7684\u6570\u636e\u8d44\u6e90\u3002", "motivation": "\u6cd5\u5f8b\u5224\u51b3\u6458\u8981\u5bf9\u6cd5\u5f8b\u4ece\u4e1a\u8005\u9020\u6210\u6c89\u91cd\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u751f\u6210\u7684\u62bd\u8c61\u6458\u8981\u5bb9\u6613\u8bef\u5224\u6cd5\u5f8b\u672f\u8bed\u6216\u5ffd\u7565\u5173\u952e\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u53d6\u5f0f\u6848\u4f8b\u6458\u8981\u5668\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u900f\u660e\u7684\u6d41\u7a0b\uff0c\u5229\u7528\u73b0\u6709\u7684\u62bd\u8c61\u6807\u51c6\u6458\u8981\u521b\u5efa\u5bf9\u5e94\u7684\u63d0\u53d6\u5f0f\u6807\u51c6\u7248\u672c\uff0c\u786e\u4fdd\u4e13\u5bb6\u610f\u89c1\u4ece\u539f\u59cb\u62bd\u8c61\u6458\u8981\u4f20\u9012\u5230\u8f6c\u6362\u540e\u7684\u63d0\u53d6\u5f0f\u6458\u8981\u3002", "result": "\u6269\u5145\u4e86\u4e03\u4e2a\u73b0\u6709\u6848\u4f8b\u6458\u8981\u6570\u636e\u96c6\uff0c\u5305\u542b\u62bd\u8c61\u6458\u8981\u548c\u5bf9\u5e94\u7684\u63d0\u53d6\u5f0f\u6458\u8981\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u3001\u8bcd\u6c47\u548c\u8bed\u4e49\u7ef4\u5ea6\u7684\u5e7f\u6cdb\u6bd4\u8f83\u8bc4\u4f30\u786e\u4fdd\u63d0\u53d6\u5f0f\u6458\u8981\u8d28\u91cf\u3002", "conclusion": "\u627f\u8bfa\u516c\u5f00\u53d1\u5e03\u6269\u5145\u7684\u6570\u636e\u96c6\uff0c\u76f8\u4fe1\u8fd9\u4e00\u8d44\u6e90\u5c06\u63a8\u52a8\u6cd5\u5f8b\u6587\u6863\u81ea\u52a8\u6458\u8981\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.12795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12795", "abs": "https://arxiv.org/abs/2511.12795", "authors": ["Boshu Lei", "Wen Jiang", "Kostas Daniilidis"], "title": "ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model", "comment": "under review", "summary": "Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u6821\u51c6\u7684\u6293\u53d6\u59ff\u6001\u751f\u6210\u6a21\u578b\u548c\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u8fdb\u884c\u673a\u5668\u4eba\u6293\u53d6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6821\u51c6\u80fd\u91cf\u6a21\u578b\u6355\u83b7SE(3)\u6d41\u5f62\u4e0a\u7684\u591a\u6a21\u6001\u6293\u53d6\u5206\u5e03\uff0c\u5e76\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u6293\u53d6\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u89c6\u4e86\u6293\u53d6\u5206\u5e03\u5728\u4fe1\u606f\u589e\u76ca\u4f30\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u6293\u53d6\u5206\u5e03\u7684\u6295\u5f71\u800c\u5ffd\u7565\u4e86SE(3)\u6d41\u5f62\u4e0a\u7684\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u6821\u51c6\u7684\u80fd\u91cf\u6a21\u578b\u751f\u6210\u6293\u53d6\u59ff\u6001\uff0c\u8be5\u6a21\u578b\u6355\u83b7SE(3)\u6d41\u5f62\u4e0a\u7684\u591a\u6a21\u6001\u6293\u53d6\u5206\u5e03\uff0c\u5e76\u5c06\u80fd\u91cf\u6c34\u5e73\u6821\u51c6\u5230\u6293\u53d6\u6210\u529f\u7387\u3002\u901a\u8fc7\u4ece\u6821\u51c6\u5206\u5e03\u4e2d\u4f30\u8ba1\u4fe1\u606f\u589e\u76ca\u6765\u9009\u62e9\u4e0b\u4e00\u4e2a\u6700\u4f73\u89c6\u89d2\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bbe\u7f6e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5148\u524d\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6709\u9650\u89c6\u89d2\u9884\u7b97\u4e0b\u6210\u529f\u6293\u53d6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u7269\u4f53\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u6293\u53d6\u95ee\u9898\uff0c\u6a21\u62df\u73af\u5883\u53ef\u4f5c\u4e3a\u672a\u6765\u4e3b\u52a8\u6293\u53d6\u7814\u7a76\u7684\u53ef\u590d\u73b0\u5e73\u53f0\u3002"}}
{"id": "2511.12300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12300", "abs": "https://arxiv.org/abs/2511.12300", "authors": ["Naoya Sugiura", "Kosuke Yamada", "Yasuhiro Ogawa", "Katsuhiko Toyama", "Ryohei Sasano"], "title": "Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering", "comment": null, "summary": "LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86LLMs\u548c\u4eba\u7c7b\u5728\u62a2\u7b54\u5f0f\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u53d1\u73b0LLMs\u5728\u7ef4\u57fa\u767e\u79d1\u672a\u8986\u76d6\u7684\u95ee\u9898\u548c\u9700\u8981\u6570\u503c\u7b54\u6848\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u8f83\u5dee", "motivation": "\u63a2\u7a76\u5bf9\u4eba\u7c7b\u56f0\u96be\u7684\u95ee\u9898\u662f\u5426\u5bf9LLMs\u540c\u6837\u56f0\u96be\uff0c\u6bd4\u8f83LLMs\u548c\u4eba\u7c7b\u5728\u62a2\u7b54\u5f0f\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\u5dee\u5f02", "method": "\u6536\u96c6\u65e5\u672c\u95ee\u7b54\u6570\u636e\uff08\u5305\u542b\u95ee\u9898\u3001\u7b54\u6848\u548c\u4eba\u7c7b\u6b63\u786e\u7387\uff09\uff0c\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u8ba9LLMs\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ece\u4e24\u4e2a\u5206\u6790\u89d2\u5ea6\u6bd4\u8f83LLMs\u4e0e\u4eba\u7c7b\u7684\u6b63\u786e\u7387", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u4eba\u7c7b\uff0cLLMs\u5728\u7ef4\u57fa\u767e\u79d1\u672a\u8986\u76d6\u7b54\u6848\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u5dee\uff0c\u4e14\u5728\u9700\u8981\u6570\u503c\u7b54\u6848\u7684\u95ee\u9898\u4e0a\u4e5f\u6709\u56f0\u96be", "conclusion": "LLMs\u4e0e\u4eba\u7c7b\u5728\u95ee\u9898\u96be\u5ea6\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u77e5\u8bc6\u8986\u76d6\u8303\u56f4\u548c\u6570\u503c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u80fd\u529b\u7279\u5f81"}}
{"id": "2511.12848", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12848", "abs": "https://arxiv.org/abs/2511.12848", "authors": ["Max M. Sun", "Todd Murphey"], "title": "Structured Imitation Learning of Interactive Policies through Inverse Games", "comment": "Presented at the \"Workshop on Generative Modeling Meets Human-Robot Interaction\" at Robotics: Science and Systems 2025. Workshop website: https://sites.google.com/view/gai-hri/", "summary": "Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0f\u5355\u667a\u80fd\u4f53\u7b56\u7565\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u7ed3\u6784\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u4ea4\u4e92\u5f0f\u7b56\u7565\uff0c\u57285\u667a\u80fd\u4f53\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u4e2d\u4ec5\u752850\u4e2a\u6f14\u793a\u5c31\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c", "motivation": "\u5728\u65e0\u663e\u5f0f\u901a\u4fe1\u7684\u5171\u4eab\u7a7a\u95f4\u4e2d\u534f\u8c03\u4eba\u7c7b\u884c\u4e3a\u7684\u4ea4\u4e92\u5f0f\u7b56\u7565\u6a21\u4eff\u5b66\u4e60\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u884c\u4e3a\u590d\u6742\u6027\u8fdc\u9ad8\u4e8e\u975e\u4ea4\u4e92\u4efb\u52a1", "method": "\u4e24\u9636\u6bb5\u5b66\u4e60\uff1a\u9996\u5148\u4f7f\u7528\u6807\u51c6\u6a21\u4eff\u5b66\u4e60\u4ece\u591a\u667a\u80fd\u4f53\u6f14\u793a\u4e2d\u5b66\u4e60\u4e2a\u4f53\u884c\u4e3a\u6a21\u5f0f\uff0c\u7136\u540e\u901a\u8fc7\u6c42\u89e3\u9006\u535a\u5f08\u95ee\u9898\u7ed3\u6784\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u95f4\u4f9d\u8d56\u5173\u7cfb", "result": "\u5728\u5408\u62105\u667a\u80fd\u4f53\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u975e\u4ea4\u4e92\u7b56\u7565\uff0c\u4ec5\u752850\u4e2a\u6f14\u793a\u5c31\u8fbe\u5230\u4e86\u4e0e\u771f\u5b9e\u4ea4\u4e92\u7b56\u7565\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "\u7ed3\u6784\u5316\u6a21\u4eff\u5b66\u4e60\u5728\u4ea4\u4e92\u5f0f\u573a\u666f\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b"}}
{"id": "2511.12381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12381", "abs": "https://arxiv.org/abs/2511.12381", "authors": ["Logan Mann", "Nayan Saxena", "Sarah Tandon", "Chenhao Sun", "Savar Toteja", "Kevin Zhu"], "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load", "comment": null, "summary": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bbd\u523a\u53cd\u5f39\u73b0\u8c61\u2014\u2014\u5426\u5b9a\u6307\u4ee4\u53cd\u800c\u4f1a\u589e\u52a0\u88ab\u7981\u6b62\u6982\u5ff5\u7684\u53ef\u53ca\u6027\u3002\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u53d1\u73b0\uff1a\u5426\u5b9a\u540e\u7acb\u5373\u51fa\u73b0\u53cd\u5f39\uff0c\u8bed\u4e49\u5e72\u6270\u4f1a\u52a0\u5267\u53cd\u5f39\uff0c\u800c\u91cd\u590d\u6709\u52a9\u4e8e\u6291\u5236\uff1b\u6781\u6027\u5206\u79bb\u7a0b\u5ea6\u4e0e\u53cd\u5f39\u6301\u7eed\u6027\u76f8\u5173\u3002", "motivation": "\u4eba\u7c7b\u601d\u7ef4\u4e2d\u5b58\u5728\u8bbd\u523a\u53cd\u5f39\u73b0\u8c61\uff0c\u5373\u5426\u5b9a\u6307\u4ee4\u4f1a\u589e\u5f3a\u88ab\u7981\u6b62\u6982\u5ff5\u7684\u53ef\u53ca\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u9762\u4e34\u540c\u6837\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u8fd9\u79cd\u53cd\u5f39\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\u3002", "method": "\u8fdb\u884c\u4e24\u4e2a\u5b9e\u9a8c\uff1a(1) \u8d1f\u8f7d\u4e0e\u5185\u5bb9\u5b9e\u9a8c\uff1a\u5728\u5426\u5b9a\u6307\u4ee4\u540e\u5f15\u5165\u4e0d\u540c\u7c7b\u578b\u5e72\u6270\u6587\u672c\uff08\u8bed\u4e49\u3001\u53e5\u6cd5\u3001\u91cd\u590d\uff09\uff0c\u6d4b\u91cf\u53cd\u5f39\u5f3a\u5ea6\uff1b(2) \u6781\u6027\u5206\u79bb\u5b9e\u9a8c\uff1a\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u80fd\u533a\u5206\u6982\u5ff5\u7684\u4e2d\u6027\u548c\u8d1f\u9762\u6846\u67b6\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5206\u79bb\u662f\u5426\u9884\u6d4b\u53cd\u5f39\u6301\u7eed\u6027\u3002\u8fd8\u8fdb\u884c\u4e86\u7535\u8def\u8ffd\u8e2a\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u5426\u5b9a\u540e\u7acb\u5373\u51fa\u73b0\u53cd\u5f39\uff0c\u8f83\u957f\u6216\u8bed\u4e49\u5e72\u6270\u4f1a\u52a0\u5267\u53cd\u5f39\uff0c\u800c\u91cd\u590d\u6709\u52a9\u4e8e\u6291\u5236\uff1b\u66f4\u5f3a\u7684\u6781\u6027\u5206\u79bb\u4e0e\u66f4\u6301\u4e45\u7684\u53cd\u5f39\u76f8\u5173\u3002\u7535\u8def\u5206\u6790\u53d1\u73b0\u7a00\u758f\u4e2d\u95f4\u5c42\u6ce8\u610f\u529b\u5934\u653e\u5927\u88ab\u7981\u6b62\u6807\u8bb0\uff0c\u800c\u65e9\u671f\u5c42\u8fdb\u884c\u6291\u5236\u3002", "conclusion": "\u7814\u7a76\u5c06\u8bbd\u523a\u53cd\u5f39\u7684\u8ba4\u77e5\u9884\u6d4b\u4e0e\u957f\u4e0a\u4e0b\u6587\u5e72\u6270\u7684\u673a\u5236\u6d1e\u5bdf\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u53d1\u5e03\u4e86ReboundBench\u6570\u636e\u96c6\uff085000\u4e2a\u7cfb\u7edf\u53d8\u5316\u7684\u5426\u5b9a\u63d0\u793a\uff09\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2511.12882", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12882", "abs": "https://arxiv.org/abs/2511.12882", "authors": ["Taiyi Su", "Jian Zhu", "Yaxuan Li", "Chong Ma", "Zitai Huang", "Yichen Zhu", "Hanli Wang", "Yi Xu"], "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos", "comment": "11 pages, 5 figures", "summary": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.", "AI": {"tldr": "MTV-World\u662f\u4e00\u4e2a\u5177\u8eab\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\u5b9e\u73b0\u7cbe\u786e\u7684\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5c06\u4f4e\u7ea7\u52a8\u4f5c\u8f6c\u6362\u4e3a\u7cbe\u786e\u673a\u5668\u4eba\u8fd0\u52a8\u65f6\u7684\u7269\u7406\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u4e16\u754c\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u5730\u5c06\u4f4e\u7ea7\u52a8\u4f5c\uff08\u5982\u5173\u8282\u4f4d\u7f6e\uff09\u8f6c\u6362\u4e3a\u9884\u6d4b\u5e27\u4e2d\u7684\u7cbe\u786e\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u5bfc\u81f4\u4e0e\u73b0\u5b9e\u4e16\u754c\u7269\u7406\u4ea4\u4e92\u4e0d\u4e00\u81f4\u3002", "method": "\u4f7f\u7528\u901a\u8fc7\u76f8\u673a\u5185\u5916\u53c2\u548c\u7b1b\u5361\u5c14\u7a7a\u95f4\u53d8\u6362\u83b7\u5f97\u7684\u8f68\u8ff9\u89c6\u9891\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\uff0c\u5f15\u5165\u591a\u89c6\u89d2\u6846\u67b6\u8865\u507f\u7a7a\u95f4\u4fe1\u606f\u635f\u5931\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u9884\u6d4b\u672a\u6765\u5e27\u3002", "result": "\u5728\u590d\u6742\u53cc\u81c2\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u63a7\u5236\u6267\u884c\u548c\u51c6\u786e\u7684\u7269\u7406\u4ea4\u4e92\u5efa\u6a21\u3002", "conclusion": "MTV-World\u901a\u8fc7\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5177\u8eab\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u7269\u7406\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u548c\u7269\u7406\u4ea4\u4e92\u5efa\u6a21\u3002"}}
{"id": "2511.12387", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12387", "abs": "https://arxiv.org/abs/2511.12387", "authors": ["Jeyarajalingam Varsha", "Menan Velayuthan", "Sumirtha Karunakaran", "Rasan Nivethiga", "Kengatharaiyer Sarveswaran"], "title": "From Phonemes to Meaning: Evaluating Large Language Models on Tamil", "comment": "11 pages", "summary": "Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.", "AI": {"tldr": "ILAKKANAM\u662f\u9996\u4e2a\u6cf0\u7c73\u5c14\u8bed\u7279\u5b9a\u8bed\u8a00\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b820\u4e2a\u6765\u81ea\u65af\u91cc\u5170\u5361\u5b66\u6821\u8003\u8bd5\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u663e\u793aLLMs\u5728\u6cf0\u7c73\u5c14\u8bed\u4e2d\u7684\u8bed\u8a00\u80fd\u529b\u6709\u9650\uff0c\u6027\u80fd\u968f\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00\u57fa\u51c6\u591a\u4f9d\u8d56\u82f1\u8bed\u7ffb\u8bd1\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u6355\u6349\u6cf0\u7c73\u5c14\u8bed\u7b49\u4f4e\u8d44\u6e90\u3001\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u7684\u8bed\u8a00\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u4f7f\u7528820\u4e2a\u65af\u91cc\u5170\u5361\u5b66\u6821\u6cf0\u7c73\u5c14\u8bed\u8003\u8bd5\u95ee\u9898\u6784\u5efa\u57fa\u51c6\uff0c\u7531\u8bad\u7ec3\u6709\u7d20\u7684\u8bed\u8a00\u5b66\u5bb6\u5728\u4e94\u4e2a\u8bed\u8a00\u7c7b\u522b\u548c\u4e00\u4e2a\u4e8b\u5b9e\u77e5\u8bc6\u7c7b\u522b\u4e0b\u8fdb\u884c\u6807\u6ce8\uff0c\u6db5\u76d61-13\u5e74\u7ea7\u4ee5\u786e\u4fdd\u5e7f\u6cdb\u7684\u8bed\u8a00\u8986\u76d6\u3002", "result": "Gemini 2.5\u8868\u73b0\u6700\u4f73\uff0c\u5f00\u6e90\u6a21\u578b\u843d\u540e\uff1b\u6240\u6709\u6a21\u578b\u5728\u4f4e\u5e74\u7ea7\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u968f\u7740\u8bed\u8a00\u590d\u6742\u6027\u589e\u52a0\u660e\u663e\u4e0b\u964d\uff1b\u6a21\u578b\u6574\u4f53\u8868\u73b0\u4e0e\u8bc6\u522b\u8bed\u8a00\u7c7b\u522b\u80fd\u529b\u65e0\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "LLMs\u5728\u6cf0\u7c73\u5c14\u8bed\u4e2d\u7684\u8868\u73b0\u53ef\u80fd\u7531\u66dd\u5149\u9a71\u52a8\u800c\u975e\u771f\u6b63\u7406\u89e3\uff0c\u9700\u8981\u66f4\u591a\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u8a00\u57fa\u7840\u5de5\u4f5c\u3002"}}
{"id": "2511.12896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12896", "abs": "https://arxiv.org/abs/2511.12896", "authors": ["Jun Huo", "Hongge Ru", "Bo Yang", "Xingjian Chen", "Xi Li", "Jian Huang"], "title": "Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction", "comment": null, "summary": "Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\\%$, 2.7$\\%$, 5.8$\\%$ and 6.7$\\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e16\u901a\u9053\u6c14\u538b\u8ba1\u7684\u8f6f\u6c14\u5ba4\u578b\u516d\u8f74\u529b/\u529b\u77e9\u4f20\u611f\u5668\uff0c\u91c7\u7528\u521a\u6027-\u67d4\u6027\u5206\u5c42\u7ed3\u6784\u5b9e\u73b0\u6709\u6548\u7684\u89e3\u8026\u65b9\u6cd5\uff0c\u5c06\u516d\u8f74\u89e3\u8026\u95ee\u9898\u7b80\u5316\u4e3a\u4e24\u4e2a\u4e09\u8f74\u89e3\u8026\u95ee\u9898\u3002", "motivation": "\u5f00\u53d1\u8f6f\u6027\u4e14\u7cbe\u786e\u7684\u516d\u8f74\u4f20\u611f\u5668\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4ea4\u53c9\u8f74\u8026\u5408\u4f1a\u5bfc\u81f4\u6821\u51c6\u95ee\u9898\u548c\u7cbe\u5ea6\u4e0b\u964d\u3002\u9700\u8981\u5b9e\u73b0\u5b8c\u6574\u7684\u516d\u81ea\u7531\u5ea6\u529b\u6d4b\u91cf\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u7845\u6a61\u80f6\u5236\u6210\u7684\u8d85\u5f39\u6027\u6c14\u5ba4\uff0c\u5185\u7f6e16\u901a\u9053\u6c14\u538b\u8ba1\u3002\u63d0\u51fa\u57fa\u4e8e\u521a\u6027-\u67d4\u6027\u5206\u5c42\u7ed3\u6784\u7684\u89e3\u8026\u65b9\u6cd5\uff0c\u5c06\u516d\u8f74\u89e3\u8026\u7b80\u5316\u4e3a\u4e24\u4e2a\u4e09\u8f74\u89e3\u8026\u95ee\u9898\u3002\u901a\u8fc7\u6709\u9650\u5143\u6a21\u578b\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u539f\u578b\u4f20\u611f\u5668\u6d4b\u91cf\u8303\u56f4\u4e3a50N\u529b\u548c1Nm\u626d\u77e9\uff0c\u5e73\u5747\u504f\u5dee4.9%\u3001\u91cd\u590d\u60272.7%\u3001\u975e\u7ebf\u60275.8%\u3001\u8fdf\u6ede6.7%\u3002\u5728\u9759\u6001\u8d1f\u8f7d\u54cd\u5e94\u3001\u52a8\u6001\u8d1f\u8f7d\u54cd\u5e94\u548c\u52a8\u6001\u54cd\u5e94\u7279\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4ee4\u4eba\u6ee1\u610f\u7684\u4f20\u611f\u6027\u80fd\u3002", "conclusion": "\u8be5\u8f6f\u6c14\u5ba4\u516d\u8f74\u529b/\u529b\u77e9\u4f20\u611f\u5668\u5728\u4fdd\u6301\u67d4\u8f6f\u6027\u7684\u540c\u65f6\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u4f20\u611f\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12464", "abs": "https://arxiv.org/abs/2511.12464", "authors": ["Chenglong Wang", "Yifu Huo", "Yang Gan", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Chunliang Zhang", "Tongran Liu", "Anxiang Ma", "Zhengtao Yu", "Jingbo Zhu", "Tong Xiao"], "title": "Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models", "comment": "Accepted by AAAI 2026", "summary": "Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86MRMBench\u57fa\u51c6\u548c\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u5728\u591a\u7ef4\u5ea6\u504f\u597d\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u4e0eLLM\u5bf9\u9f50\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u63ed\u793a\u4e86\u5956\u52b1\u6a21\u578b\u5728\u591a\u7ef4\u5ea6\u504f\u597d\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u6210\u5bf9\u6392\u5e8f\u6d4b\u8bd5\u96c6\uff0c\u4f46\u65e0\u6cd5\u63d0\u4f9b\u5404\u4e2a\u504f\u597d\u7ef4\u5ea6\u7684\u6027\u80fd\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5206\u6790\u5956\u52b1\u6a21\u578b\u5728\u4e0d\u540c\u504f\u597d\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86MRMBench\u57fa\u51c6\uff0c\u5305\u542b6\u4e2a\u4e0d\u540c\u504f\u597d\u7ef4\u5ea6\u7684\u63a2\u6d4b\u4efb\u52a1\uff1b\u63d0\u51fa\u4e86\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\uff0c\u8bc6\u522b\u5956\u52b1\u9884\u6d4b\u4e2d\u4f7f\u7528\u7684\u7ef4\u5ea6\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "MRMBench\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u80fd\u5f3a\u76f8\u5173\uff1b\u5956\u52b1\u6a21\u578b\u5728\u591a\u7ef4\u5ea6\u504f\u597d\u6355\u6349\u4e0a\u5b58\u5728\u56f0\u96be\uff1b\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\u80fd\u53ef\u9760\u8bc4\u4f30\u5956\u52b1\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "MRMBench\u662f\u5f00\u53d1\u5148\u8fdb\u5956\u52b1\u6a21\u578b\u7684\u53ef\u9760\u53c2\u8003\uff1b\u591a\u76ee\u6807\u4f18\u5316\u5728\u5956\u52b1\u5efa\u6a21\u4e2d\u5177\u6709\u6f5c\u529b\uff1b\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\u80fd\u63d0\u5347LLM\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2511.12910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12910", "abs": "https://arxiv.org/abs/2511.12910", "authors": ["Yong Li", "Yujun Huang", "Yi Chen", "Hui Cheng"], "title": "TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints", "comment": null, "summary": "Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86TOPP-DWR\u7b97\u6cd5\uff0c\u4e3a\u5dee\u901f\u9a71\u52a8\u8f6e\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u7cfb\u7edf\u5b9e\u7528\u7684\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u89d2\u901f\u5ea6\u3001\u5173\u8282\u901f\u5ea6\u3001\u7ebf\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7ea6\u675f", "motivation": "\u73b0\u6709\u79fb\u52a8\u673a\u5668\u4eba\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\u7814\u7a76\u901a\u5e38\u5ffd\u7565\u89d2\u901f\u5ea6\u548c\u5173\u8282\u901f\u5ea6\u7ea6\u675f\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u63a7\u5236\u6027\u80fd\u4e0b\u964d", "method": "\u91c7\u7528\u975e\u5747\u5300B\u6837\u6761\u8868\u793a\u4efb\u52a1\u7a7a\u95f4\u521d\u59cb\u8f68\u8ff9\uff0c\u5c06\u89d2\u901f\u5ea6\u3001\u5173\u8282\u901f\u5ea6\u3001\u7ebf\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7ea6\u675f\u7edf\u4e00\u8868\u793a\u4e3a\u7ebf\u901f\u5ea6\u7ea6\u675f\uff0c\u901a\u8fc7\u5f15\u5165\u677e\u5f1b\u53d8\u91cf\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u4e8c\u9636\u9525\u89c4\u5212", "result": "\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5b9a\u91cf\u6027\u80fd\u6307\u6807\u663e\u793aTOPP-DWR\u80fd\u5728\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316", "conclusion": "\u73b0\u573a\u81ea\u4e3b\u5bfc\u822a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TOPP-DWR\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027"}}
{"id": "2511.12472", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12472", "abs": "https://arxiv.org/abs/2511.12472", "authors": ["Mengying Wang", "Chenhui Ma", "Ao Jiao", "Tuo Liang", "Pengjun Lu", "Shrinidhi Hegde", "Yu Yin", "Evren Gurkan-Cavusoglu", "Yinghui Wu"], "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing", "comment": "The 40th AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.", "AI": {"tldr": "\u63d0\u51faSerenQA\u6846\u67b6\u6765\u8bc4\u4f30LLM\u5728\u79d1\u5b66\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u53d1\u73b0\u610f\u5916\u6d1e\u5bdf\u7684\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u836f\u7269\u91cd\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u76f8\u5173\u6027\u3001\u65b0\u9896\u6027\u548c\u60ca\u559c\u5ea6\u3002", "motivation": "\u73b0\u6709KGQA\u7cfb\u7edf\u901a\u5e38\u8fd4\u56de\u9ad8\u5ea6\u76f8\u5173\u4f46\u53ef\u9884\u6d4b\u7684\u7b54\u6848\uff0c\u7f3a\u4e4f\u53d1\u73b0\u60ca\u559c\u548c\u65b0\u578b\uff08\"\u610f\u5916\"\uff09\u7b54\u6848\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faSerenQA\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u76f8\u5173\u6027\u3001\u65b0\u9896\u6027\u548c\u60ca\u559c\u5ea6\u7684\u4e25\u683c\u610f\u5916\u6027\u6307\u6807\uff0c\u4ee5\u53ca\u4ece\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u7684\u4e13\u5bb6\u6807\u6ce8\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u836f\u7269\u91cd\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLM\u5728\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bc6\u522b\u771f\u6b63\u4ee4\u4eba\u60ca\u8bb6\u548c\u6709\u4ef7\u503c\u7684\u53d1\u73b0\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "\u5728\u53d1\u73b0\u771f\u6b63\u4ee4\u4eba\u60ca\u8bb6\u548c\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\u65b9\u9762\u4ecd\u6709\u663e\u8457\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u91ca\u653e\u4e86\u7cbe\u5fc3\u7b56\u5212\u7684\u8d44\u6e90\u7528\u4e8e\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2511.12912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12912", "abs": "https://arxiv.org/abs/2511.12912", "authors": ["Yingting Zhou", "Wenbo Cui", "Weiheng Liu", "Guixing Chen", "Haoran Li", "Dongbin Zhao"], "title": "DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping", "comment": null, "summary": "Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.", "AI": {"tldr": "DiffuDepGrasp\u662f\u4e00\u4e2a\u96f6\u6837\u672csim2real\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6df1\u5ea6\u751f\u6210\u5668\u5408\u6210\u5e26\u6709\u4f20\u611f\u5668\u566a\u58f0\u7684\u4eff\u771f\u6df1\u5ea6\u56fe\uff0c\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u7684\u6293\u53d6\u7b56\u7565\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7b56\u7565\u8fc1\u79fb\u4e2d\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\u548c\u90e8\u7f72\u590d\u6742\u6027\uff0c\u514b\u670d\u6df1\u5ea6\u56fe\u4f20\u611f\u5668\u4f2a\u5f71\uff08\u5982\u7a7a\u6d1e\u548c\u566a\u58f0\uff09\u9020\u6210\u7684sim2real\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6df1\u5ea6\u751f\u6210\u5668\uff0c\u5305\u542b\u6269\u6563\u6df1\u5ea6\u6a21\u5757\uff08\u5229\u7528\u65f6\u95f4\u51e0\u4f55\u5148\u9a8c\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff09\u548c\u566a\u58f0\u5ac1\u63a5\u6a21\u5757\uff08\u4fdd\u6301\u5ea6\u91cf\u7cbe\u5ea6\u6ce8\u5165\u611f\u77e5\u4f2a\u5f71\uff09\uff0c\u4ec5\u9700\u539f\u59cb\u6df1\u5ea6\u8f93\u5165\u8fdb\u884c\u90e8\u7f72\u3002", "result": "\u572812\u4e2a\u7269\u4f53\u6293\u53d6\u4efb\u52a1\u4e2d\u8fbe\u523095.7%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u5e76\u5bf9\u672a\u89c1\u7269\u4f53\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffuDepGrasp\u901a\u8fc7\u4eff\u771f\u4e13\u5c5e\u7b56\u7565\u8bad\u7ec3\u548c\u6269\u6563\u6a21\u578b\u566a\u58f0\u5408\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86sim2real\u8fc1\u79fb\u4e2d\u7684\u6570\u636e\u6548\u7387\u548c\u90e8\u7f72\u590d\u6742\u5ea6\u95ee\u9898\u3002"}}
{"id": "2511.12497", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12497", "abs": "https://arxiv.org/abs/2511.12497", "authors": ["JoonHo Lee", "HyeonMin Cho", "Jaewoong Yun", "Hyunjae Lee", "JunKyu Lee", "Juree Seok"], "title": "SGuard-v1: Safety Guardrail for Large Language Models", "comment": "Technical Report", "summary": "We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.", "AI": {"tldr": "SGuard-v1\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u62a4\u680f\u7cfb\u7edf\uff0c\u5305\u542bContentFilter\u548cJailbreakFilter\u4e24\u4e2a\u4e13\u95e8\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u548c\u7b5b\u9009\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u652f\u630112\u79cd\u8bed\u8a00\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b-AI\u5bf9\u8bdd\u8bbe\u7f6e\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u548c\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u9632\u62a4\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e2B\u53c2\u6570\u7684Granite-3.3-2B-Instruct\u6a21\u578b\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u8bad\u7ec3\u4e24\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1aContentFilter\u7528\u4e8e\u8bc6\u522b\u5b89\u5168\u98ce\u9669\uff0cJailbreakFilter\u7528\u4e8e\u9632\u5fa160\u79cd\u4e3b\u8981\u653b\u51fb\u7c7b\u578b\uff0c\u4f7f\u7528\u7ea6140\u4e07\u8bad\u7ec3\u5b9e\u4f8b\u3002", "result": "\u5728\u516c\u5171\u548c\u4e13\u6709\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u5b89\u5168\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u90e8\u7f72\uff0c\u63d0\u4f9b\u591a\u7c7b\u5b89\u5168\u9884\u6d4b\u548c\u4e8c\u5143\u7f6e\u4fe1\u5ea6\u5206\u6570\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "SGuard-v1\u662f\u4e00\u4e2a\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7Apache-2.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2511.12941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12941", "abs": "https://arxiv.org/abs/2511.12941", "authors": ["Chunyong Hu", "Qi Luo", "Jianyun Xu", "Song Wang", "Qiang Li", "Sheng Yang"], "title": "GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving", "comment": null, "summary": "In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.", "AI": {"tldr": "GUIDE\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u7684\u65b0\u578b\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u5230\u4f53\u7d20\u6295\u5f71\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u969c\u788d\u7269\u68c0\u6d4b\u548c\u5360\u7528\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf3D\u8fb9\u754c\u6846\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5904\u7406\u4e0d\u89c4\u5219\u5f62\u72b6\u7269\u4f53\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8621.61\u7684\u5b9e\u4f8b\u5360\u7528mAP\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534750%\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d563D\u8fb9\u754c\u6846\u6765\u8868\u793a\u969c\u788d\u7269\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u89c4\u5219\u5f62\u72b6\u7269\u4f53\u7684\u590d\u6742\u6027\uff0c\u9650\u5236\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u8fdb\u884c\u5b9e\u4f8b\u68c0\u6d4b\u548c\u5360\u7528\u9884\u6d4b\uff0c\u4f7f\u7528\u7a00\u758f\u8868\u793a\u7b56\u7565\u548cGaussian-to-Voxel Splatting\u6280\u672f\uff0c\u5728\u907f\u514d\u5bc6\u96c6\u4f53\u7d20\u7f51\u683c\u8ba1\u7b97\u8d1f\u62c5\u7684\u540c\u65f6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u5b9e\u4f8b\u7ea7\u5360\u7528\u6570\u636e\uff0c\u5e76\u5177\u5907\u5f3a\u5927\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0cGUIDE\u5b9e\u73b0\u4e8621.61\u7684\u5b9e\u4f8b\u5360\u7528mAP\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534750%\uff0c\u540c\u65f6\u5177\u5907\u7ade\u4e89\u529b\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "GUIDE\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u73b0\u5b9e\u9a7e\u9a76\u73af\u5883\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2511.12504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12504", "abs": "https://arxiv.org/abs/2511.12504", "authors": ["Maria Tseytlin", "Paul Roit", "Omri Abend", "Ido Dagan", "Ayal Klein"], "title": "QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs", "comment": null, "summary": "Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.", "AI": {"tldr": "QA-Noun\u662f\u4e00\u4e2a\u57fa\u4e8e\u95ee\u7b54\u7684\u540d\u8bcd\u4e2d\u5fc3\u8bed\u4e49\u5173\u7cfb\u6846\u67b6\uff0c\u901a\u8fc79\u4e2a\u95ee\u9898\u6a21\u677f\u6355\u6349\u540d\u8bcd\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u8bed\u4e49\u89d2\u8272\uff0c\u4e0eQA-SRL\u7ed3\u5408\u5b9e\u73b0\u53e5\u5b50\u610f\u4e49\u7684\u7ec6\u7c92\u5ea6\u5206\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eQA\u7684\u8bed\u4e49\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8c13\u8bcd-\u8bba\u5143\u5173\u7cfb\uff0c\u4f46\u5ffd\u89c6\u4e86\u540d\u8bcd\u4e2d\u5fc3\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u9700\u8981\u8865\u5145\u540d\u8bcd\u8bed\u4e49\u8868\u793a\u6846\u67b6\u3002", "method": "\u5b9a\u4e499\u4e2a\u95ee\u9898\u6a21\u677f\u8986\u76d6\u540d\u8bcd\u7684\u8bed\u6cd5\u548c\u4e0a\u4e0b\u6587\u89d2\u8272\uff0c\u6784\u5efa\u8d85\u8fc72000\u4e2a\u6807\u6ce8\u540d\u8bcd\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e0eQA-SRL\u96c6\u6210\u5f62\u6210\u7edf\u4e00\u8bed\u4e49\u5206\u89e3\u6846\u67b6\u3002", "result": "QA-Noun\u51e0\u4e4e\u5b8c\u5168\u8986\u76d6AMR\u7684\u540d\u8bcd\u8bba\u5143\uff0c\u540c\u65f6\u53d1\u73b0\u66f4\u591a\u4e0a\u4e0b\u6587\u9690\u542b\u5173\u7cfb\uff0c\u4e0eQA-SRL\u7ed3\u5408\u6bd4FactScore\u548cDecompScore\u7b49\u65b9\u6cd5\u7684\u7c92\u5ea6\u9ad8\u51fa130%\u4ee5\u4e0a\u3002", "conclusion": "QA-Noun\u8865\u5145\u4e86\u57fa\u4e8eQA\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u5f62\u6210\u4e86\u5168\u9762\u4e14\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5206\u89e3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8de8\u6587\u672c\u5bf9\u9f50\u4efb\u52a1\u3002"}}
{"id": "2511.12972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12972", "abs": "https://arxiv.org/abs/2511.12972", "authors": ["Siddarth Narasimhan", "Matthew Lisondra", "Haitong Wang", "Goldie Nejat"], "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models", "comment": "Project Page: https://splat-search.github.io/", "summary": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.", "AI": {"tldr": "SplatSearch\u662f\u4e00\u4e2a\u89e3\u51b3\u5b9e\u4f8b\u56fe\u50cf\u76ee\u6807\u5bfc\u822a(IIN)\u95ee\u9898\u7684\u65b0\u67b6\u6784\uff0c\u5229\u7528\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u548c\u6269\u6563\u6a21\u578b\u5b8c\u6210\u6e32\u67d3\u56fe\u50cf\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u8fb9\u754c\u63a2\u7d22\u7b56\u7565\u63d0\u9ad8\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5b9e\u4f8b\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u95ee\u9898\u4e2d\u7684\u6311\u6218\uff1a\u53c2\u8003\u56fe\u50cf\u89c6\u89d2\u4efb\u610f\u4e14\u673a\u5668\u4eba\u5fc5\u987b\u5728\u7a00\u758f\u89c6\u56fe\u573a\u666f\u91cd\u5efa\u4e0b\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u5728\u7ebf3D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u6e32\u67d3\u5019\u9009\u5bf9\u8c61\u7684\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u5229\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u8865\u5168\u7f3a\u5931\u533a\u57df\uff0c\u5b9e\u73b0\u4e0e\u76ee\u6807\u56fe\u50cf\u7684\u9c81\u68d2\u7279\u5f81\u5339\u914d\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u8bed\u4e49\u548c\u89c6\u89c9\u76f8\u5173\u6027\u7684\u8fb9\u754c\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u611f\u5bb6\u5ead\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SplatSearch\u5728\u6210\u529f\u7387\u548c\u6210\u529f\u8def\u5f84\u957f\u5ea6\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SplatSearch\u901a\u8fc7\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u3001\u6269\u6563\u6a21\u578b\u8865\u5168\u548c\u8bed\u4e49\u8fb9\u754c\u63a2\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u4f8b\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12520", "abs": "https://arxiv.org/abs/2511.12520", "authors": ["Jie Zhang", "Bo Tang", "Wanzi Shao", "Wenqiang Wei", "Jihao Zhao", "Jianqing Zhu", "Zhiyu li", "Wen Xi", "Zehao Lin", "Feiyu Xiong", "Yanchao Tan"], "title": "TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction", "comment": "Accepted by AAAI 2026", "summary": "Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.", "AI": {"tldr": "TAdaRAG\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6765\u89e3\u51b3\u4f20\u7edfRAG\u4e2d\u4fe1\u606f\u622a\u65ad\u548c\u65e0\u5173\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u5c06\u5916\u90e8\u77e5\u8bc6\u622a\u65ad\u6210\u5c0f\u5757\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u5f15\u53d1\u54cd\u5e94\u5e7b\u89c9\u548c\u63a8\u7406\u94fe\u65ad\u88c2\uff0c\u540c\u65f6\u68c0\u7d22\u7684\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\u542b\u65e0\u5173\u7ec6\u8282\uff0c\u5f71\u54cd\u51c6\u786e\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u610f\u56fe\u9a71\u52a8\u7684\u8def\u7531\u673a\u5236\u5230\u9886\u57df\u7279\u5b9a\u63d0\u53d6\u6a21\u677f\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u9690\u5f0f\u63d0\u53d6\u673a\u5236\uff0c\u786e\u4fdd\u7b80\u6d01\u3001\u8fde\u8d2f\u4e14\u975e\u5197\u4f59\u7684\u77e5\u8bc6\u6574\u5408\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e00\u4e2a\u771f\u5b9e\u4e1a\u52a1\u57fa\u51c6(NowNewsQA)\u4e0a\uff0c\u4f7f\u7528\u4e09\u4e2a\u9aa8\u5e72\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0cTAdaRAG\u5728\u591a\u4e2a\u9886\u57df\u548c\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TAdaRAG\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u6709\u6548\u6027\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edfRAG\u7684\u4fe1\u606f\u622a\u65ad\u548c\u65e0\u5173\u7ec6\u8282\u95ee\u9898\u3002"}}
{"id": "2511.12984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12984", "abs": "https://arxiv.org/abs/2511.12984", "authors": ["Miryeong Park", "Dongjin Cho", "Sanghyun Kim", "Younggun Cho"], "title": "CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner", "comment": "Accepted in International Conference on Space Robotics 2025", "summary": "Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u5b89\u5168\u8def\u5f84\u751f\u6210\u3001\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u66f4\u65b0\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u63a2\u7d22\u7b56\u7565\uff0c\u7528\u4e8e\u884c\u661f\u63a2\u7d22\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u5730\u5f62\u4e2d\u7684\u5bfc\u822a\u548c\u5efa\u56fe\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5730\u5f62\uff08\u5982\u9668\u77f3\u5751\uff09\u9644\u8fd1\u7684\u9ad8\u7a0b\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u4e14\u672a\u80fd\u89e3\u51b3\u9ad8\u7a0b\u4e0d\u786e\u5b9a\u6027\u5bf9\u5bfc\u822a\u5b89\u5168\u548c\u5730\u56fe\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u9ad8\u7a0b\u4f30\u8ba1\u65b9\u6cd5\u751f\u6210\u5730\u5f62\u53ef\u7a7f\u8d8a\u6027\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u5c06\u5176\u6574\u5408\u5230\u57fa\u4e8e\u56fe\u7684\u63a2\u7d22\u89c4\u5212\u5668\uff08GBP\uff09\u4e2d\uff0c\u4f18\u5148\u63a2\u7d22\u53ef\u7a7f\u8d8a\u7684\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\u3002", "result": "\u5728\u6a21\u62df\u6708\u7403\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebfGBP\u5b9e\u73b0\u4e8669%\u7684\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u5230100%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u63a2\u7d22\u5b89\u5168\u6027\u548c\u5730\u56fe\u53ef\u9760\u6027\uff0c\u5728\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.12573", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12573", "abs": "https://arxiv.org/abs/2511.12573", "authors": ["Hyeonji Kim", "Sujeong Oh", "Sanghack Lee"], "title": "Mitigating Length Bias in RLHF through a Causal Lens", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u56e0\u679c\u6846\u67b6\u6765\u5206\u6790\u548c\u7f13\u89e3RLHF\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u72ec\u7acb\u4e8e\u5197\u957f\u5ea6\u8bc4\u4f30\u5185\u5bb9\u8d28\u91cf\u3002", "motivation": "RLHF\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u5b58\u5728\u957f\u5ea6\u504f\u5dee\uff0c\u7cfb\u7edf\u6027\u5730\u503e\u5411\u4e8e\u504f\u7231\u8f83\u957f\u56de\u7b54\uff0c\u5c06\u5197\u957f\u5ea6\u4e0e\u8d28\u91cf\u6df7\u6dc6\uff0c\u8fd9\u5f71\u54cd\u4e86\u6a21\u578b\u8f93\u51fa\u7684\u8d28\u91cf\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u6784\u5efa\u957f\u5ea6\u4e0d\u540c\u4f46\u5185\u5bb9\u76f8\u4f3c\u7684\u54cd\u5e94\u5bf9\uff0c\u4ee5\u53ca\u957f\u5ea6\u76f8\u4f3c\u4f46\u5185\u5bb9\u4e0d\u540c\u7684\u54cd\u5e94\u5bf9\uff0c\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5956\u52b1\u5206\u914d\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\uff0c\u4f7f\u7b56\u7565\u6a21\u578b\u4ea7\u751f\u66f4\u7b80\u6d01\u3001\u5185\u5bb9\u805a\u7126\u7684\u8f93\u51fa\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u957f\u5ea6\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86RLHF\u6d41\u7a0b\u4e2d\u5956\u52b1\u5efa\u6a21\u7684\u9c81\u68d2\u6027\u548c\u5185\u5bb9\u654f\u611f\u6027\u3002"}}
{"id": "2511.13042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13042", "abs": "https://arxiv.org/abs/2511.13042", "authors": ["Yong Li", "Hui Cheng"], "title": "APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation", "comment": null, "summary": "Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eA*\u7b49\u56fe\u641c\u7d22\u89c4\u5212\u5668\u7684\u901a\u7528\u540e\u5904\u7406\u7b97\u6cd5APP\uff0c\u901a\u8fc7\u53cc\u5411\u9876\u70b9\u7f29\u51cf\u548c\u8def\u5f84\u6270\u52a8\u6765\u7f29\u77ed\u8def\u5f84\u957f\u5ea6\u3001\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\uff0c\u63d0\u9ad8\u8def\u5f84\u5e73\u6ed1\u5ea6", "motivation": "A*\u7b49\u56fe\u641c\u7d22\u89c4\u5212\u5668\u751f\u6210\u7684\u8def\u5f84\u901a\u5e38\u4e0d\u662f\u6700\u77ed\u8def\u5f84\uff0c\u4e14\u5b58\u5728\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\u548c\u952f\u9f7f\u6a21\u5f0f\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e2d\u5728\u5f00\u9614\u7a7a\u95f4\u5e94\u8d70\u76f4\u7ebf\u8def\u5f84\u7684\u671f\u671b\u4e0d\u7b26", "method": "\u57fa\u4e8e\u4ee3\u4ef7\u5730\u56fe\u5f00\u53d1APP\u7b97\u6cd5\uff1a1\uff09\u53cc\u5411\u9876\u70b9\u7f29\u51cf\u7b97\u6cd5\u5904\u7406\u8def\u5f84\u548c\u73af\u5883\u7684\u4e0d\u5bf9\u79f0\u6027\uff1b2\uff09\u5728\u9876\u70b9\u7f29\u51cf\u8fc7\u7a0b\u4e2d\u91c7\u7528\u5f7b\u5e95\u6377\u5f84\u7b56\u7565\uff1b3\uff09\u8fed\u4ee3\u8def\u5f84\u6270\u52a8\u7b97\u6cd5\u5c40\u90e8\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411", "result": "\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660eAPP\u5728\u89c4\u5212\u65f6\u95f4\u3001\u8def\u5f84\u957f\u5ea6\u548c\u51cf\u5c11\u4e0d\u5fc5\u8981\u8f6c\u5411\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u73b0\u573a\u5bfc\u822a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027", "conclusion": "APP\u7b97\u6cd5\u80fd\u6709\u6548\u6539\u8fdbA*\u7b49\u89c4\u5212\u5668\u7684\u8def\u5f84\u8d28\u91cf\uff0c\u751f\u6210\u66f4\u77ed\u3001\u66f4\u5e73\u6ed1\u7684\u8def\u5f84\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411"}}
{"id": "2511.12586", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12586", "abs": "https://arxiv.org/abs/2511.12586", "authors": ["Pu-Hai Yang", "Heyan Huang", "Heng-Da Xu", "Fanshu Sun", "Xian-Ling Mao", "Chaoxu Mu"], "title": "MMWOZ: Building Multimodal Agent for Task-oriented Dialogue", "comment": null, "summary": "Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.", "AI": {"tldr": "\u63d0\u51fa\u4e86MMWOZ\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6\u548cMATE\u57fa\u7ebf\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u5728\u73b0\u5b9eGUI\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898", "motivation": "\u4f20\u7edf\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4f9d\u8d56\u5b9a\u5236\u540e\u7aefAPI\uff0c\u800c\u73b0\u5b9e\u573a\u666f\u4e2d\u5e7f\u6cdb\u5b58\u5728\u524d\u7aefGUI\u4e14\u7f3a\u4e4f\u5b9a\u5236API\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "method": "1) \u57fa\u4e8eMultiWOZ 2.3\u6269\u5c55\u6784\u5efaMMWOZ\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u7f51\u9875\u98ce\u683cGUI\uff1b2) \u8bbe\u8ba1\u81ea\u52a8\u5316\u811a\u672c\u5c06\u5bf9\u8bdd\u72b6\u6001\u548c\u7cfb\u7edf\u52a8\u4f5c\u8f6c\u6362\u4e3aGUI\u64cd\u4f5c\u6307\u4ee4\uff1b3) \u6536\u96c6\u7f51\u9875\u5feb\u7167\u548c\u5bf9\u5e94\u64cd\u4f5c\u6307\u4ee4\uff1b4) \u63d0\u51faMATE\u591a\u6a21\u6001\u57fa\u7ebf\u6a21\u578b", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5305\u542bGUI\u64cd\u4f5c\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u57fa\u7ebf\u6a21\u578b", "conclusion": "MMWOZ\u6570\u636e\u96c6\u548cMATE\u6a21\u578b\u4e3a\u6784\u5efa\u5b9e\u7528\u7684\u591a\u6a21\u6001\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5f25\u5408\u4e86\u4f20\u7edf\u7cfb\u7edf\u4e0e\u73b0\u5b9eGUI\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u8ddd"}}
{"id": "2511.13048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13048", "abs": "https://arxiv.org/abs/2511.13048", "authors": ["Yong Li", "Hui Cheng"], "title": "Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments", "comment": "2023 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6e05\u6d01\u673a\u5668\u4eba\u5168\u5c40\u8def\u5f84\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5355\u5411\u9053\u8def\u7f51\u7edc\u8868\u793a\u4ea4\u901a\u7ea6\u675f\uff0c\u91c7\u7528\u6df7\u5408\u7b56\u7565\u4fdd\u8bc1\u89c4\u5212\u7ed3\u679c\uff0c\u5141\u8bb8\u5728\u8d77\u70b9\u548c\u7ec8\u70b9\u5904\u7a7f\u8d8a\u9053\u8def\u4ee5\u7f29\u77ed\u8def\u5f84\uff0c\u5e76\u4f7f\u7528\u53cc\u5c42\u52bf\u80fd\u56fe\u5904\u7406\u590d\u6742\u4ea4\u53c9\u53e3\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u81ea\u7531\u7a7a\u95f4\u65b9\u6cd5\u5ffd\u89c6\u4ea4\u901a\u89c4\u5219\u7ea6\u675f\u5bfc\u81f4\u9891\u7e41\u91cd\u65b0\u89c4\u5212\u548c\u78b0\u649e\u98ce\u9669\u589e\u52a0\uff0c\u800c\u7ed3\u6784\u5316\u73af\u5883\u65b9\u6cd5\u4e25\u683c\u9075\u5b88\u9053\u8def\u7f51\u7edc\u53ef\u80fd\u5bfc\u81f4\u8def\u5f84\u8fc7\u957f\u5f71\u54cd\u5bfc\u822a\u6548\u7387\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u8def\u5f84\u957f\u5ea6\u548c\u9053\u8def\u7f51\u7edc\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5355\u5411\u9053\u8def\u7f51\u7edc\u8868\u793a\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u4ea4\u901a\u7ea6\u675f\uff0c\u63d0\u51fa\u6df7\u5408\u7b56\u7565\u4fdd\u8bc1\u89c4\u5212\u7ed3\u679c\uff0c\u5141\u8bb8\u5728\u8d77\u70b9\u548c\u7ec8\u70b9\u5904\u7a7f\u8d8a\u9053\u8def\u4ee5\u7f29\u77ed\u8def\u5f84\uff0c\u7279\u522b\u8bbe\u8ba1\u4e86\u53cc\u5c42\u52bf\u80fd\u56fe\u6765\u5904\u7406\u590d\u6742\u4ea4\u53c9\u53e3\u60c5\u51b5\u3002", "result": "\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9a\u91cf\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u548c\u9053\u8def\u7f51\u7edc\u4e00\u81f4\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u8def\u5f84\u957f\u5ea6\u548c\u4ea4\u901a\u89c4\u5219\u7ea6\u675f\uff0c\u63d0\u9ad8\u6e05\u6d01\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6548\u7387\u3002"}}
{"id": "2511.12596", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12596", "abs": "https://arxiv.org/abs/2511.12596", "authors": ["Oron Anschel", "Alon Shoshan", "Adam Botach", "Shunit Haviv Hakimi", "Asaf Gendler", "Emanuel Ben Baruch", "Nadav Bhonker", "Igor Kviatkovsky", "Manoj Aggarwal", "Gerard Medioni"], "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models", "comment": "EMNLP Main 2025", "summary": "Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.", "AI": {"tldr": "GAPO\u662f\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u7ec4\u7ea7\u5956\u52b1\u6765\u89e3\u51b3LLM\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u6210\u54cd\u5e94\u7684\u591a\u6837\u6027\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\uff0c\u5373\u4f7f\u5b58\u5728\u591a\u4e2a\u6709\u6548\u7b54\u6848\u4e5f\u91cd\u590d\u751f\u6210\u76f8\u540c\u7684\u51e0\u4e2a\u5b8c\u6210\u7ed3\u679c\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165\u7ec4\u611f\u77e5\u7b56\u7565\u4f18\u5316(GAPO)\uff0c\u4f5c\u4e3aGRPO\u7684\u7b80\u5355\u6269\u5c55\uff0c\u8ba1\u7b97\u6574\u4e2a\u7ec4\u7684\u5956\u52b1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u7ec4\u7ea7\u5c5e\u6027\uff08\u5982\u591a\u6837\u6027\u548c\u8986\u76d6\u7387\uff09\u4e2d\u5b66\u4e60\u3002\u4f7f\u7528\u9891\u7387\u611f\u77e5\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u5bf9\u6709\u6548LLM\u5b8c\u6210\u7ed3\u679c\u7684\u5747\u5300\u91c7\u6837\u3002", "result": "GAPO\u8bad\u7ec3\u7684\u6a21\u578b\u4ea7\u751f\u6709\u6548\u4e14\u66f4\u591a\u6837\u5316\u7684\u54cd\u5e94\uff0c\u5728\u5f00\u653e\u63d0\u793a\u4e0b\u4e5f\u80fd\u63d0\u9ad8\u54cd\u5e94\u591a\u6837\u6027\uff0c\u4e14\u5728\u6807\u51c6LLM\u57fa\u51c6\u6d4b\u8bd5\uff08GSM8K\u3001MATH\u3001HumanEval\u3001MMLU-Pro\uff09\u4e0a\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\u3002", "conclusion": "GAPO\u80fd\u591f\u6709\u6548\u89e3\u51b3LLM\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u6210\u54cd\u5e94\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2511.13071", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13071", "abs": "https://arxiv.org/abs/2511.13071", "authors": ["Michal Levin", "Itzik Klein"], "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers", "comment": "22 pages, 10 figures", "summary": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u4f20\u611f\u5668\u5b9a\u5411\u548c\u65cb\u8f6c\u7684\u6a21\u578b\u65e0\u5173\u5b66\u4e60\u578b\u52a0\u901f\u5ea6\u8ba1\u504f\u7f6e\u6821\u51c6\u65b9\u6cd5\uff0c\u5728\u9759\u6b62\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5feb\u901f\u5b9e\u7528\u7684\u73b0\u573a\u90e8\u7f72\u3002", "motivation": "\u4f4e\u6210\u672cMEMS\u52a0\u901f\u5ea6\u8ba1\u5728\u5bfc\u822a\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6027\u80fd\u5e38\u53d7\u504f\u7f6e\u8bef\u5dee\u5f71\u54cd\u3002\u4f20\u7edf\u6821\u51c6\u65b9\u6cd5\u9700\u8981\u52a0\u901f\u5ea6\u8ba1\u8c03\u5e73\u6216\u590d\u6742\u7684\u5b9a\u5411\u76f8\u5173\u7a0b\u5e8f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u578b\u65e0\u5173\u6821\u51c6\u65b9\u6cd5\uff0c\u5728\u9759\u6b62\u6761\u4ef6\u4e0b\u4f30\u8ba1\u52a0\u901f\u5ea6\u8ba1\u504f\u7f6e\uff0c\u65e0\u9700\u4f20\u611f\u5668\u5b9a\u5411\u77e5\u8bc6\u548c\u65cb\u8f6c\u64cd\u4f5c\u3002", "result": "\u572813.39\u5c0f\u65f6\u516d\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6280\u672f\u8bef\u5dee\u964d\u4f4e52%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86\u65e0\u5b9a\u5411\u573a\u666f\u4e0b\u7684\u7cbe\u786e\u6821\u51c6\u65b9\u6cd5\u53d1\u5c55\uff0c\u63d0\u9ad8\u4e86\u4f4e\u6210\u672c\u60ef\u6027\u4f20\u611f\u5668\u5728\u79d1\u5b66\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u6d88\u9664\u4e86\u8c03\u5e73\u6821\u51c6\u7684\u9700\u6c42\u3002"}}
{"id": "2511.12609", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12609", "abs": "https://arxiv.org/abs/2511.12609", "authors": ["Yunxin Li", "Xinyu Chen", "Shenyuan Jiang", "Haoyuan Shi", "Zhenyu Liu", "Xuanyu Zhang", "Nanhao Deng", "Zhenran Xu", "Yicheng Ma", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data", "comment": "47 pages,10 Figures, Project Website: https://idealistxy.github.io/Uni-MoE-v2.github.io/; Codes: https://github.com/HITsz-TMG/Uni-MoE", "summary": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.", "AI": {"tldr": "Uni-MoE 2.0\u662f\u4e00\u4e2a\u5168\u5f00\u6e90\u7684\u5168\u6a21\u6001\u5927\u6a21\u578b\uff0c\u57fa\u4e8eQwen2.5-7B\u67b6\u6784\u6784\u5efa\uff0c\u901a\u8fc7\u52a8\u6001\u5bb9\u91cfMoE\u8bbe\u8ba1\u3001\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548c\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u6a21\u6001\u6570\u636e\u5339\u914d\u6280\u672f\uff0c\u5728\u8bed\u8a00\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002", "motivation": "\u63a8\u52a8Lychee Uni-MoE\u7cfb\u5217\u5728\u591a\u6a21\u6001\u80fd\u529b\u4e0a\u7684\u53d1\u5c55\uff0c\u6784\u5efa\u80fd\u591f\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u56fe\u50cf\u3001\u6587\u672c\u3001\u8bed\u97f3\u7684\u5168\u6a21\u6001\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5bb9\u91cfMoE\u6846\u67b6\uff0c\u5305\u542b\u5171\u4eab\u3001\u8def\u7531\u548c\u7a7a\u4e13\u5bb6\uff1b\u4f7f\u7528Omni-Modality 3D RoPE\u786e\u4fdd\u8de8\u6a21\u6001\u65f6\u7a7a\u5bf9\u9f50\uff1b\u5b9e\u65bd\u6e10\u8fdb\u5f0f\u76d1\u7763\u5fae\u8c03\u7b56\u7565\uff0c\u7ed3\u5408\u5e73\u8861\u6570\u636e\u7ec4\u5408\u548c\u8fed\u4ee3GSPO-DPO\u65b9\u6cd5\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u572885\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u572876\u4e2a\u57fa\u51c6\u4e2d\u8d85\u8fc750\u4e2a\u4f18\u4e8eQwen2.5-Omni\uff08\u4f7f\u75281.2T token\u8bad\u7ec3\uff09\uff0c\u89c6\u9891\u7406\u89e3\u63d0\u53477%\uff0c\u5168\u6a21\u6001\u7406\u89e3\u63d0\u53477%\uff0c\u89c6\u542c\u63a8\u7406\u63d0\u53474%\uff0c\u957f\u8bed\u97f3\u5904\u7406WER\u964d\u4f4e4.2%\uff0c\u5728\u4f4e\u7ea7\u56fe\u50cf\u5904\u7406\u548c\u53ef\u63a7\u751f\u6210\u65b9\u9762\u9886\u5148\u3002", "conclusion": "Uni-MoE 2.0\u901a\u8fc7\u521b\u65b0\u7684MoE\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5b9e\u73b0\u4e86\u5168\u6a21\u6001\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u7684SOTA\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u4e13\u5bb6\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13096", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13096", "abs": "https://arxiv.org/abs/2511.13096", "authors": ["Guy Damari", "Itzik Klein"], "title": "ResAlignNet: A Data-Driven Approach for INS/DVL Alignment", "comment": null, "summary": "Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8\u00b0 using only 25 seconds of data collection, representing a 65\\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.", "AI": {"tldr": "ResAlignNet\u662f\u4e00\u79cd\u57fa\u4e8e1D ResNet-18\u67b6\u6784\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5c06INS\u548cDVL\u4f20\u611f\u5668\u5bf9\u51c6\u95ee\u9898\u8f6c\u5316\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\uff0c\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u8f85\u52a9\u6216\u590d\u6742\u673a\u52a8\uff0c\u4ec5\u970025\u79d2\u6570\u636e\u5373\u53ef\u57280.8\u00b0\u7cbe\u5ea6\u5185\u5b8c\u6210\u5bf9\u51c6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684INS-DVL\u5bf9\u51c6\u65b9\u6cd5\u5b58\u5728\u6536\u655b\u65f6\u95f4\u957f\u3001\u4f9d\u8d56\u9884\u8bbe\u8fd0\u52a8\u6a21\u5f0f\u548c\u5916\u90e8\u8f85\u52a9\u4f20\u611f\u5668\u7684\u95ee\u9898\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u6c34\u4e0b\u81ea\u4e3b\u822a\u884c\u5668\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u75281D ResNet-18\u67b6\u6784\uff0c\u5c06\u4f20\u611f\u5668\u5bf9\u51c6\u95ee\u9898\u8f6c\u5316\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\uff0c\u652f\u6301Sim2Real\u8fc1\u79fb\u5b66\u4e60\uff0c\u53ef\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u5e76\u5728\u5b9e\u9645\u4f20\u611f\u5668\u6d4b\u91cf\u4e2d\u90e8\u7f72\u3002", "result": "\u5728Snapir\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cResAlignNet\u4ec5\u970025\u79d2\u6570\u636e\u6536\u96c6\u5373\u53ef\u5b9e\u73b00.8\u00b0\u4ee5\u5185\u7684\u5bf9\u51c6\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u6807\u51c6\u901f\u5ea6\u65b9\u6cd5\u6536\u655b\u65f6\u95f4\u51cf\u5c1165%\u3002", "conclusion": "\u8be5\u8f68\u8ff9\u65e0\u5173\u89e3\u51b3\u65b9\u6848\u6d88\u9664\u4e86\u8fd0\u52a8\u6a21\u5f0f\u8981\u6c42\uff0c\u65e0\u9700\u5197\u957f\u7684\u4efb\u52a1\u524d\u7a0b\u5e8f\u5373\u53ef\u7acb\u5373\u90e8\u7f72\u822a\u884c\u5668\uff0c\u901a\u8fc7\u9c81\u68d2\u7684\u4f20\u611f\u5668\u65e0\u5173\u5bf9\u51c6\u63a8\u8fdb\u4e86\u6c34\u4e0b\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2511.12630", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12630", "abs": "https://arxiv.org/abs/2511.12630", "authors": ["Maoqi Liu", "Quan Fang", "Yang Yang", "Can Zhao", "Kaiquan Cai"], "title": "Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing", "comment": "Accepted to Advanced Engineering Informatics", "summary": "Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.", "AI": {"tldr": "\u63d0\u51fa\u4e86NOTAM\u8bed\u4e49\u89e3\u6790\u4efb\u52a1\uff0c\u6784\u5efa\u4e86Knots\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u589e\u5f3a\u6807\u6ce8\u8d28\u91cf\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u63d0\u793a\u5de5\u7a0b\u548c\u6a21\u578b\u9002\u5e94\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u822a\u7a7a\u6587\u672c\u7406\u89e3\u80fd\u529b\u3002", "motivation": "NOTAMs\u4f5c\u4e3a\u5173\u952e\u98de\u884c\u5b89\u5168\u4fe1\u606f\u6e20\u9053\uff0c\u5176\u590d\u6742\u8bed\u8a00\u7ed3\u6784\u548c\u9690\u542b\u63a8\u7406\u7ed9\u81ea\u52a8\u89e3\u6790\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5206\u7c7b\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7b49\u8868\u5c42\u4efb\u52a1\uff0c\u7f3a\u4e4f\u6df1\u5ea6\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u63d0\u51faNOTAM\u8bed\u4e49\u89e3\u6790\u4efb\u52a1\uff0c\u6784\u5efa\u5305\u542b12,347\u6761\u4e13\u5bb6\u6807\u6ce8NOTAMs\u7684Knots\u6570\u636e\u96c6\uff0c\u8986\u76d6194\u4e2a\u98de\u884c\u60c5\u62a5\u533a\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u8fdb\u884c\u5168\u9762\u7684\u5b57\u6bb5\u53d1\u73b0\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5e7f\u6cdb\u7684\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u548c\u6a21\u578b\u9002\u5e94\u6280\u672f\uff0c\u5728\u822a\u7a7a\u6587\u672c\u7406\u89e3\u548c\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u52a8NOTAM\u5206\u6790\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u89e3\u6790\u65b9\u6cd5\u5728\u822a\u7a7a\u9886\u57df\u6587\u672c\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13100", "abs": "https://arxiv.org/abs/2511.13100", "authors": ["Xuecheng Chen", "Jingao Xu", "Wenhua Ding", "Haoyang Wang", "Xinyu Luo", "Ruiyang Duan", "Jialong Chen", "Xueqian Wang", "Yunhao Liu", "Xinlei Chen"], "title": "Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing", "comment": null, "summary": "As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \\sysname. \\sysname features two components: \\textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \\textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \\sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\\%. Additionally, \\sysname infers drone flight commands with 96.5\\% precision and improves drone tracking accuracy by over 22\\% when combined with other sensing modalities. \\textit{ Demo: {\\color{blue}https://eventpro25.github.io/EventPro/.} }", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65e0\u4eba\u673a\u611f\u77e5\u7cfb\u7edfEventPro\uff0c\u901a\u8fc7\u7cbe\u786e\u4f30\u8ba1\u87ba\u65cb\u6868\u8f6c\u901f\u6765\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u611f\u77e5\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u914d\u9001\u573a\u666f\u4e2d\u5b9e\u73b0\u4e863ms\u7684\u611f\u77e5\u5ef6\u8fdf\u548c0.23%\u7684\u8f6c\u901f\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u7684\u666e\u53ca\uff0c\u4ece\u5730\u9762\u8fdb\u884c\u975e\u63a5\u89e6\u5f0f\u65e0\u4eba\u673a\u611f\u77e5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u611f\u77e5\u6027\u80fd\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65e0\u4eba\u673a\u72b6\u6001\u76d1\u6d4b\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aCount Every Rotation\u901a\u8fc7\u51cf\u8f7b\u4e8b\u4ef6\u76f8\u673a\u5bf9\u73af\u5883\u566a\u58f0\u7684\u8d85\u9ad8\u7075\u654f\u5ea6\uff0c\u5b9e\u73b0\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u87ba\u65cb\u6868\u8f6c\u901f\u4f30\u8ba1\uff1bEvery Rotation Counts\u5229\u7528\u8fd9\u4e9b\u8f6c\u901f\u63a8\u65ad\u65e0\u4eba\u673a\u7684\u5185\u5916\u52a8\u6001\u3002", "result": "\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u914d\u9001\u573a\u666f\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e863ms\u7684\u611f\u77e5\u5ef6\u8fdf\u548c0.23%\u7684\u8f6c\u901f\u4f30\u8ba1\u8bef\u5dee\uff0c\u80fd\u591f\u4ee596.5%\u7684\u7cbe\u5ea6\u63a8\u65ad\u65e0\u4eba\u673a\u98de\u884c\u6307\u4ee4\uff0c\u4e0e\u5176\u4ed6\u611f\u77e5\u6a21\u6001\u7ed3\u5408\u65f6\u53ef\u5c06\u65e0\u4eba\u673a\u8ddf\u8e2a\u7cbe\u5ea6\u63d0\u534722%\u4ee5\u4e0a\u3002", "conclusion": "\u4e13\u6ce8\u4e8e\u87ba\u65cb\u6868\u8f6c\u901f\u80fd\u591f\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u611f\u77e5\u6027\u80fd\uff0c\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u89e3\u51b3\u65b9\u6848\u5728\u5b9e\u65f6\u6027\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u65e0\u4eba\u673a\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2511.12661", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12661", "abs": "https://arxiv.org/abs/2511.12661", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "title": "Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing", "comment": null, "summary": "Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a \"faithfulness gap\": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning \"Houston\" from \"NASA\" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.", "AI": {"tldr": "Reason-KE++\u662f\u4e00\u4e2aSFT+RL\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u5fe0\u5b9e\u5ea6\u5bf9\u9f50\u6765\u89e3\u51b3LLM\u5728\u590d\u6742\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\uff0c\u5728MQUAKE-CF-3k\u4e0a\u8fbe\u523095.48%\u7684\u65b0SOTA\u3002", "motivation": "\u73b0\u6709SFT\u65b9\u6cd5\uff08\u5982Reason-KE\uff09\u5b58\u5728\"\u5fe0\u5b9e\u5ea6\u5dee\u8ddd\"\uff1a\u5b83\u4eec\u4f18\u5316\u683c\u5f0f\u6a21\u4eff\u800c\u975e\u53ef\u9760\u63a8\u7406\uff0c\u5bfc\u81f4LLM\u7684\u53c2\u6570\u5148\u9a8c\u8986\u76d6\u4e0a\u4e0b\u6587\u4e8b\u5b9e\uff0c\u4ea7\u751f\u5173\u952e\u4e8b\u5b9e\u5e7b\u89c9\u3002", "method": "\u63d0\u51faReason-KE++\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u9636\u6bb5\u611f\u77e5\u5956\u52b1\u673a\u5236\uff0c\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff08\u5982\u5206\u89e3\u3001\u5b50\u7b54\u6848\u6b63\u786e\u6027\uff09\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\uff0c\u907f\u514d\u5929\u771f\u7ed3\u679c\u5bfc\u5411RL\u7684\u9677\u9631\u3002", "result": "\u5728MQUAKE-CF-3k\u4e0a\u8fbe\u523095.48%\u7684\u51c6\u786e\u7387\uff08\u63d0\u5347+5.28%\uff09\uff0c\u800c\u4ec5\u5173\u6ce8\u7ed3\u679c\u7684RL\u65b9\u6cd5\u4f1a\u635f\u5bb3\u63a8\u7406\u5b8c\u6574\u6027\uff08Hop\u51c6\u786e\u7387\u4ec519.00%\uff09\u3002", "conclusion": "\u5bf9\u4e8e\u590d\u6742\u4efb\u52a1\uff0c\u5bf9\u9f50\u63a8\u7406\u8fc7\u7a0b\u5bf9\u4e8e\u6784\u5efa\u53ef\u4fe1\u8d56\u7684LLM\u81f3\u5173\u91cd\u8981\uff0c\u8fc7\u7a0b\u611f\u77e5\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u6838\u5fc3\u7684LLM\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2511.13120", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13120", "abs": "https://arxiv.org/abs/2511.13120", "authors": ["Trevor Exley", "Anderson Brazil Nardin", "Petr Trunin", "Diana Cafiso", "Lucia Beccai"], "title": "Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design", "comment": "8 pages, 6 figures, 1 algorithm, 1 table", "summary": "This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5355\u4f53\u5355\u5143(MU)\uff0c\u8fd9\u662f\u4e00\u79cd\u96c6\u6210\u4e86\u6c14\u52a8\u9a71\u52a8\u3001\u67d4\u6027\u6676\u683c\u5916\u58f3\u548c\u5149\u5b66\u6ce2\u5bfc\u4f20\u611f\u76843D\u6253\u5370\u6784\u5efa\u6a21\u5757\u3002\u901a\u8fc7\u53c2\u6570\u5316\u8bbe\u8ba1\u6846\u67b6\u548c\u5b9e\u9a8c\u5747\u8d28\u5316\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u4ece\u9a71\u52a8\u5668\u8154\u5ba4\u5c3a\u5bf8\u5230\u6676\u683c\u5355\u5143\u5c3a\u5bf8\u7684\u786e\u5b9a\u6027\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5143\u4eff\u771f\u4f18\u5316\u4f20\u611f\u5668\u5e03\u5c40\uff0c\u5728\u4fdd\u6301\u673a\u68b0\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u5d4c\u5165\u5f0f\u4f20\u611f\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u96c6\u9a71\u52a8\u3001\u7ed3\u6784\u548c\u4f20\u611f\u4e8e\u4e00\u4f53\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u6784\u5efa\u6a21\u5757\uff0c\u89e3\u51b3\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u9a71\u52a8\u3001\u4f20\u611f\u548c\u7ed3\u6784\u96c6\u6210\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u91cd\u590d\u5236\u9020\u548c\u53ef\u6269\u5c55\u8bbe\u8ba1\u3002", "method": "1. \u8bbe\u8ba1\u96c6\u6210\u4e86\u6c14\u52a8\u9a71\u52a8\u3001\u67d4\u6027\u6676\u683c\u5916\u58f3\u548c\u5149\u5b66\u6ce2\u5bfc\u4f20\u611f\u7684\u5355\u4f53\u5355\u5143\uff1b2. \u5efa\u7acb\u53c2\u6570\u5316\u8bbe\u8ba1\u6846\u67b6\uff0c\u5c06\u9a71\u52a8\u5668\u8154\u5ba4\u5c3a\u5bf8\u4e0e\u6676\u683c\u5355\u5143\u5c3a\u5bf8\u5173\u8054\uff1b3. \u901a\u8fc7\u5b9e\u9a8c\u5747\u8d28\u5316\u83b7\u5f97\u6709\u6548\u6750\u6599\u5c5e\u6027\u7528\u4e8e\u6709\u9650\u5143\u4eff\u771f\uff1b4. \u5c06\u4f20\u611f\u5668\u5e03\u5c40\u4f5c\u4e3a\u79bb\u6563\u4f18\u5316\u95ee\u9898\u5904\u7406\uff0c\u4ece\u6676\u683c\u8282\u70b9\u5bfc\u51fa\u7684\u5019\u9009\u6ce2\u5bfc\u8def\u5f84\u4e2d\u9009\u62e9\u6700\u5c0f\u5316\u673a\u68b0\u54cd\u5e94\u504f\u5dee\u7684\u914d\u7f6e\u3002", "result": "\u4f18\u5316\u540e\u7684\u6a21\u578b\u88ab\u5236\u9020\u5e76\u5b9e\u9a8c\u8868\u5f81\uff0c\u9a8c\u8bc1\u4e86\u5728\u4fdd\u6301\u673a\u68b0\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u5d4c\u5165\u5f0f\u4f20\u611f\u3002\u8be5\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u7f29\u653e\u5355\u5143\u548c\u53cc\u6307\u5939\u6301\u5668\uff0c\u8bc1\u660e\u4e86MU\u6982\u5ff5\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u53ef\u91cd\u590d\u7684\u534f\u540c\u8bbe\u8ba1\u89c4\u5219\u548c\u57fa\u4e8e\u4eff\u771f\u7684\u4f20\u611f\u5668\u96c6\u6210\uff0c\u63a8\u8fdb\u4e86\u5355\u4f53\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2511.12690", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12690", "abs": "https://arxiv.org/abs/2511.12690", "authors": ["Sina Rashidi", "Hossein Sameti"], "title": "Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data", "comment": null, "summary": "Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6ce2\u65af\u8bed\u5230\u82f1\u8bed\u8bed\u97f3\u7ffb\u8bd1\u7684\u76f4\u63a5\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u79bb\u6563\u8bed\u97f3\u5355\u5143\u548c\u5408\u6210\u5e76\u884c\u6570\u636e\u6765\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u76f4\u63a5\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u5e76\u884c\u8bed\u97f3\u6570\u636e\uff0c\u4f46\u5bf9\u4e8e\u6ce2\u65af\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u6765\u8bf4\uff0c\u8fd9\u7c7b\u6570\u636e\u975e\u5e38\u7a00\u7f3a\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u57fa\u4e8econformer\u7684\u7f16\u7801\u5668\u3001\u56e0\u679ctransformer\u89e3\u7801\u5668\u548c\u57fa\u4e8e\u5355\u5143\u7684\u795e\u7ecf\u58f0\u7801\u5668\u3002\u901a\u8fc7\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u6ce2\u65af\u8bed\u8bed\u97f3\u8f6c\u5f55\uff0c\u5e76\u7ed3\u5408\u96f6\u6837\u672c\u6587\u672c\u5230\u8bed\u97f3\u7cfb\u7edf\u5408\u6210\u82f1\u8bed\u8bed\u97f3\uff0c\u6784\u5efa\u4e86\u5408\u6210\u5e76\u884c\u8bed\u6599\u5e93\u3002", "result": "\u5728CVSS\u8bed\u6599\u5e93\u7684\u6ce2\u65af\u8bed-\u82f1\u8bed\u90e8\u5206\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u7684\u6a21\u578b\u76f8\u6bd4\u76f4\u63a5\u57fa\u7ebf\u5728ASR BLEU\u6307\u6807\u4e0a\u63d0\u5347\u4e864.6\u5206\uff0c\u53ef\u7528\u5e76\u884c\u8bed\u97f3\u6570\u636e\u91cf\u589e\u52a0\u4e86\u7ea66\u500d\u3002", "conclusion": "\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u79bb\u6563\u8bed\u97f3\u5355\u5143\u548c\u5408\u6210\u5e76\u884c\u6570\u636e\u7684\u65b9\u6cd5\u5bf9\u4e8e\u6539\u5584\u6ce2\u65af\u8bed-\u82f1\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u76f4\u63a5\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2511.13188", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13188", "abs": "https://arxiv.org/abs/2511.13188", "authors": ["Osama Al Sheikh Ali", "Sotiris Koutsoftas", "Ze Zhang", "Knut Akesson", "Emmanuel Dean"], "title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control", "comment": "This paper has been accepted by IEEE SII 2026", "summary": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5bfc\u822a\u6846\u67b6\uff0c\u5c06\u73af\u5883\u8868\u793a\u3001\u8f68\u8ff9\u751f\u6210\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7edf\u4e00\u8d77\u6765\uff0c\u4f7f\u7528\u56db\u53c9\u6811\u65b9\u6cd5\u4ece\u5360\u636e\u5730\u56fe\u751f\u6210\u7ed3\u6784\u5316\u65e0\u78b0\u649e\u533a\u57df\u4f5c\u4e3a\u5b89\u5168\u8d70\u5eca\u548cMPC\u7ea6\u675f\u3002", "motivation": "\u4e3a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u907f\u514d\u76f4\u63a5\u7f16\u7801\u969c\u788d\u7269\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u5bfc\u822a\u3002", "method": "\u91c7\u7528\u56db\u53c9\u6811\u65b9\u6cd5\u751f\u6210\u8f74\u5bf9\u9f50\u7684\u65e0\u78b0\u649e\u533a\u57df\uff0c\u6784\u5efa\u8fde\u901a\u56fe\uff0c\u7ed3\u5408\u8f68\u8ff9\u751f\u6210\u548cB\u6837\u6761\u5e73\u6ed1\uff0c\u5c06\u5b89\u5168\u533a\u57df\u63d0\u53d6\u4f5c\u4e3aMPC\u7684\u7ebf\u6027\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u590d\u6742\u73af\u5883\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u4e00\u81f4\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u65e0\u9700\u76f4\u63a5\u5904\u7406\u969c\u788d\u7269\u7f16\u7801\u3002"}}
{"id": "2511.12710", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12710", "abs": "https://arxiv.org/abs/2511.12710", "authors": ["Yunhao Chen", "Xin Wang", "Juncheng Li", "Yixu Wang", "Jie Li", "Yan Teng", "Yingchun Wang", "Xingjun Ma"], "title": "Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs", "comment": null, "summary": "Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \\textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.", "AI": {"tldr": "EvoSynth\u662f\u4e00\u4e2a\u81ea\u4e3b\u7684\u8fdb\u5316\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u4e3b\u8bbe\u8ba1\u3001\u8fdb\u5316\u548c\u6267\u884c\u57fa\u4e8e\u4ee3\u7801\u7684\u65b0\u578b\u653b\u51fb\u7b97\u6cd5\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u81ea\u52a8\u5316\u7ea2\u961f\u6846\u67b6\u53ea\u80fd\u9009\u62e9\u3001\u7ec4\u5408\u6216\u6539\u8fdb\u73b0\u6709\u653b\u51fb\u7b56\u7565\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u7ea2\u961f\u6846\u67b6\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u5176\u8d8a\u72f1\u903b\u8f91\u4ec5\u9650\u4e8e\u9009\u62e9\u3001\u7ec4\u5408\u6216\u6539\u8fdb\u73b0\u6709\u7684\u653b\u51fb\u7b56\u7565\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u521b\u9020\u529b\uff0c\u65e0\u6cd5\u81ea\u4e3b\u53d1\u660e\u5168\u65b0\u7684\u653b\u51fb\u673a\u5236\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u4e3b\u8bbe\u8ba1\u3001\u8fdb\u5316\u548c\u6267\u884c\u57fa\u4e8e\u4ee3\u7801\u7684\u653b\u51fb\u7b97\u6cd5\uff0c\u5e76\u5305\u542b\u4ee3\u7801\u7ea7\u81ea\u6821\u6b63\u5faa\u73af\uff0c\u80fd\u591f\u6839\u636e\u5931\u8d25\u60c5\u51b5\u8fed\u4ee3\u91cd\u5199\u81ea\u8eab\u653b\u51fb\u903b\u8f91\u3002", "result": "\u5728\u5bf9\u6297\u9ad8\u5ea6\u9c81\u68d2\u7684\u6a21\u578b\uff08\u5982Claude-Sonnet-4.5\uff09\u65f6\u5b9e\u73b0\u4e8685.5%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u751f\u6210\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u591a\u6837\u5316\u7684\u653b\u51fb\u3002", "conclusion": "EvoSynth\u901a\u8fc7\u5c06\u8303\u5f0f\u4ece\u653b\u51fb\u89c4\u5212\u8f6c\u5411\u8fdb\u5316\u5408\u6210\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e3a\u8d8a\u72f1\u65b9\u6cd5\u7684\u8fdb\u5316\u5408\u6210\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.13207", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13207", "abs": "https://arxiv.org/abs/2511.13207", "authors": ["Cheng Peng", "Zhenzhe Zhang", "Cheng Chi", "Xiaobao Wei", "Yanhao Zhang", "Heng Wang", "Pengwei Wang", "Zhongyuan Wang", "Jing Liu", "Shanghang Zhang"], "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection", "comment": null, "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.", "AI": {"tldr": "PIGEON\u662f\u4e00\u4e2a\u7528\u4e8e\u672a\u77e5\u73af\u5883\u4e2d\u7269\u4f53\u5bfc\u822a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5174\u8da3\u70b9\u5f15\u5bfc\u63a2\u7d22\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u9891\u667a\u80fd\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51b3\u7b56\u9891\u7387\u4e0e\u667a\u80fd\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5bfc\u81f4\u51b3\u7b56\u7f3a\u4e4f\u524d\u77bb\u6027\u6216\u52a8\u4f5c\u4e0d\u8fde\u7eed\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u9891\u51b3\u7b56\u53c8\u5177\u5907\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7684\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528PIGEON-VL\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9009\u62e9\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u5f62\u6210\u7684\u5174\u8da3\u70b9\uff0c\u7136\u540e\u7531\u4f4e\u7ea7\u89c4\u5212\u5668\u8f93\u51fa\u52a8\u4f5c\uff1b\u540c\u65f6\u57fa\u4e8e\u5174\u8da3\u70b9\u51b3\u7b56\u751f\u6210\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u3002", "result": "\u5728\u7ecf\u5178\u7269\u4f53\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u8fc1\u79fb\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cRLVR\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8bed\u4e49\u5f15\u5bfc\u80fd\u529b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5bfc\u822a\u4e2d\u7684\u6df1\u5ea6\u63a8\u7406\u3002", "conclusion": "PIGEON\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e86\u51b3\u7b56\u9891\u7387\u4e0e\u667a\u80fd\u6027\uff0c\u901a\u8fc7\u5174\u8da3\u70b9\u5f15\u5bfc\u548c\u8bed\u4e49\u8bb0\u5fc6\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u667a\u80fd\u7684\u7269\u4f53\u5bfc\u822a\u3002"}}
{"id": "2511.12712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12712", "abs": "https://arxiv.org/abs/2511.12712", "authors": ["Christopher Cruz"], "title": "Adaptive Focus Memory for Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.", "AI": {"tldr": "AFM\u662f\u4e00\u79cd\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3001\u65f6\u95f4\u8870\u51cf\u548c\u91cd\u8981\u6027\u5206\u7c7b\u4e3a\u5386\u53f2\u6d88\u606f\u5206\u914d\u4e09\u79cd\u4fdd\u771f\u5ea6\u7ea7\u522b\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u80fd\u7684\u540c\u65f6\u5c06\u5e73\u5747token\u4f7f\u7528\u91cf\u51cf\u5c1166%\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u7b80\u5355\u5185\u5b58\u7b56\u7565\u7684\u74f6\u9888\u95ee\u9898\uff0c\u907f\u514d\u9759\u6001\u6458\u8981\u6216\u4ec5\u5173\u6ce8\u6700\u8fd1\u6d88\u606f\u5bfc\u81f4\u5b89\u5168\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002", "method": "AFM\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3001\u534a\u8870\u671f\u65f6\u95f4\u8870\u51cf\u6743\u91cd\u548c\u91cd\u8981\u6027\u5206\u7c7b\uff0c\u4e3a\u6bcf\u4e2a\u5386\u53f2\u6d88\u606f\u5206\u914dFULL\u3001COMPRESSED\u6216PLACEHOLDER\u4fdd\u771f\u5ea6\u7ea7\u522b\uff0c\u5728\u4e25\u683ctoken\u9884\u7b97\u4e0b\u6309\u65f6\u95f4\u987a\u5e8f\u6253\u5305\u6d88\u606f\u3002", "result": "\u5728\u6d89\u53ca\u82b1\u751f\u8fc7\u654f\u7528\u6237\u89c4\u5212\u6cf0\u56fd\u65c5\u884c\u7684\u5b89\u5168\u5bfc\u5411\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAFM\u5728\u77ed\u4e2d\u957f\u5ea6\u5bf9\u8bdd\u4e2d\u90fd\u80fd\u4fdd\u7559\u8fc7\u654f\u4fe1\u606f\uff0c\u5b89\u5168\u6027\u80fd\u4e0e\u7b80\u5355\u91cd\u653e\u76f8\u5f53\uff0c\u5e73\u5747token\u4f7f\u7528\u91cf\u6bd4\u57fa\u7ebf\u51cf\u5c1166%\u3002", "conclusion": "AFM\u63d0\u4f9b\u6a21\u5757\u5316Python\u5b9e\u73b0\uff0c\u53ef\u5728\u4e0d\u727a\u7272\u5b89\u5168\u6027\u6216\u4e8b\u5b9e\u8fde\u7eed\u6027\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2511.13216", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13216", "abs": "https://arxiv.org/abs/2511.13216", "authors": ["Chiyun Noh", "Sangwoo Jung", "Hanjun Kim", "Yafei Hu", "Laura Herlant", "Ayoung Kim"], "title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry", "comment": null, "summary": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO", "AI": {"tldr": "GaRLILEO\u662f\u4e00\u4e2a\u91cd\u529b\u5bf9\u9f50\u7684\u8fde\u7eed\u65f6\u95f4\u96f7\u8fbe-\u817f-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026IMU\u901f\u5ea6\u5e76\u6784\u5efa\u8fde\u7eed\u65f6\u95f4\u81ea\u901f\u5ea6\u6837\u6761\uff0c\u7ed3\u5408\u96f7\u8fbe\u591a\u666e\u52d2\u548c\u817f\u90e8\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u63d0\u9ad8\u817f\u5f0f\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u5730\u5f62\u4e0a\u7684\u5782\u76f4\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u817f\u90e8\u8fd0\u52a8\u5b66\u548c\u60ef\u6027\u4f20\u611f\u7684\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u5b58\u5728\u65e0\u6cd5\u6291\u5236\u7684\u5782\u76f4\u6f02\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u697c\u68af\u3001\u659c\u5761\u7b49\u590d\u6742\u5730\u5f62\u4e0a\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u4e86\u6fc0\u5149\u96f7\u8fbe\u6216\u76f8\u673a\u7b49\u5916\u90e8\u4f20\u611f\u5668\uff0c\u4f46\u5728\u7279\u5f81\u7a00\u758f\u6216\u91cd\u590d\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u5bb9\u6613\u53d7\u5230IMU\u52a0\u901f\u5ea6\u53cc\u91cd\u79ef\u5206\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "method": "GaRLILEO\u901a\u8fc7\u6784\u5efa\u57fa\u4e8eSoC\u96f7\u8fbe\u591a\u666e\u52d2\u548c\u817f\u90e8\u8fd0\u52a8\u5b66\u4fe1\u606f\u7684\u8fde\u7eed\u65f6\u95f4\u81ea\u901f\u5ea6\u6837\u6761\uff0c\u5c06\u901f\u5ea6\u4eceIMU\u4e2d\u89e3\u8026\u51fa\u6765\uff0c\u5b9e\u73b0\u65e0\u7f1d\u4f20\u611f\u5668\u878d\u5408\u3002\u540c\u65f6\u91c7\u7528\u65b0\u9896\u7684\u8f6fS2\u7ea6\u675f\u91cd\u529b\u56e0\u5b50\u53ef\u9760\u5730\u6355\u6349\u51c6\u786e\u7684\u91cd\u529b\u5411\u91cf\uff0c\u63d0\u9ad8\u5782\u76f4\u4f4d\u59ff\u7cbe\u5ea6\u3002", "result": "\u5728\u81ea\u6536\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u5ba4\u5185\u5916\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cGaRLILEO\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u697c\u68af\u548c\u659c\u5761\u4e0a\u7684\u5782\u76f4\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GaRLILEO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u5730\u5f62\u4e0a\u7684\u5782\u76f4\u6f02\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u6fc0\u5149\u96f7\u8fbe\u6216\u76f8\u673a\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u91cc\u7a0b\u8ba1\u548cSLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12728", "abs": "https://arxiv.org/abs/2511.12728", "authors": ["Lea Hergert", "G\u00e1bor Berend", "Mario Szegedy", "Gyorgy Turan", "M\u00e1rk Jelasity"], "title": "On the Brittleness of LLMs: A Journey around Set Membership", "comment": null, "summary": "Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \\{pear, plum, apple, raspberry\\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.", "AI": {"tldr": "LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u8d85\u4eba\u7c7b\uff0c\u4f46\u5728\u7b80\u5355\u96c6\u5408\u6210\u5458\u67e5\u8be2\u4efb\u52a1\u4e2d\u5374\u9891\u7e41\u5931\u8d25\uff0c\u63ed\u793a\u4e86\u5176\u63a8\u7406\u80fd\u529b\u7684\u8106\u5f31\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u7b80\u5355\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4ee5\u63ed\u793a\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u7b80\u5355\u4efb\u52a1\u5b9e\u73b0\u5927\u89c4\u6a21\u53ef\u63a7\u5b9e\u9a8c\u3002", "method": "\u4f7f\u7528\u96c6\u5408\u6210\u5458\u67e5\u8be2\u4efb\u52a1\uff0c\u7cfb\u7edf\u8bc4\u4f30\u63d0\u793a\u63aa\u8f9e\u3001\u8bed\u4e49\u7ed3\u6784\u3001\u5143\u7d20\u6392\u5e8f\u548c\u6a21\u578b\u9009\u62e9\u7b49\u591a\u4e2a\u7ef4\u5ea6\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u3002", "result": "LLMs\u5728\u8fd9\u4e00\u57fa\u672c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u59cb\u7ec8\u8106\u5f31\u4e14\u4e0d\u53ef\u9884\u6d4b\uff0c\u8868\u660e\u6a21\u578b\u5bf9\u96c6\u5408\u6982\u5ff5\u7684\u7406\u89e3\u662f\u788e\u7247\u5316\u548c\u590d\u6742\u7684\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u95ee\u9898\u5b9e\u73b0\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u80fd\u591f\u5168\u9762\u6620\u5c04\u548c\u5206\u6790\u5931\u8d25\u6a21\u5f0f\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bf9LLM\u8bc4\u4f30\u5177\u6709\u666e\u904d\u4ef7\u503c\u3002"}}
{"id": "2511.13312", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13312", "abs": "https://arxiv.org/abs/2511.13312", "authors": ["Jonas Bode", "Raphael Memmesheimer", "Sven Behnke"], "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation", "comment": "10 pages; 2 figures; 1 table. Prprint submitted to the European Robotics Forum 2026", "summary": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u7684\u6269\u6563\u6a21\u578b\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u7528\u4e8e\u751f\u6210\u7cbe\u786e\u7684\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5728CALVIN\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u64cd\u4f5c\u4efb\u52a1\u548c\u957f\u65f6\u57df\u4efb\u52a1\u5e8f\u5217\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u8ba9\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u9700\u8981\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u7269\u7406\u4efb\u52a1\u5e94\u7528\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u6765\u63d0\u5347\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u6027\u80fd\u3002", "method": "\u5728\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u6846\u67b6\u4e2d\u6574\u5408\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u751f\u6210\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u4f7f\u7528\u53c2\u8003\u6f14\u793a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6269\u6563\u6a21\u578b\u6280\u672f\u8fdb\u884c\u6539\u8fdb\u3002", "result": "\u5728CALVIN\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u5e8f\u5217\u65f6\u63d0\u9ad8\u4e86\u957f\u65f6\u57df\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5f3a\u5316\u4e86\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u901a\u7528\u591a\u4efb\u52a1\u64cd\u4f5c\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2511.12768", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12768", "abs": "https://arxiv.org/abs/2511.12768", "authors": ["Noah Hong", "Tao Hong"], "title": "Evidence of Phase Transitions in Small Transformer-Based Language Models", "comment": null, "summary": "Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u5b58\u5728\u76f8\u53d8\u73b0\u8c61\uff0c\u5373\u4f7f\u5728\u5c0f\u89c4\u6a21transformer\u6a21\u578b\u4e2d\u4e5f\u80fd\u89c2\u5bdf\u5230\uff0c\u8fd9\u79cd\u76f8\u53d8\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0\uff0c\u53ef\u901a\u8fc7\u8bcd\u6c47\u7edf\u8ba1\u65b9\u6cd5\u5728\u539f\u59cb\u8bad\u7ec3\u7a7a\u95f4\u4e2d\u76f4\u63a5\u68c0\u6d4b\u3002", "motivation": "\u63a2\u7d22\u76f8\u53d8\u73b0\u8c61\u662f\u5426\u4ec5\u9650\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u5426\u5728\u5c0f\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\uff0c\u4ee5\u53ca\u662f\u5426\u80fd\u5728\u539f\u59cb\u8bad\u7ec3\u7a7a\u95f4\u800c\u975e\u5bf9\u6570\u7f29\u653e\u540e\u68c0\u6d4b\u5230\u8fd9\u4e9b\u76f8\u53d8\u3002", "method": "\u8bad\u7ec3\u5c0f\u578bGPT\u98ce\u683ctransformer\u6a21\u578b\uff0c\u5206\u6790\u8bcd\u6c47\u4f7f\u7528\u6f14\u53d8\uff0c\u5305\u62ec\u5e73\u5747\u8bcd\u957f\u3001\u6b63\u786e\u4e0e\u9519\u8bef\u8bcd\u6c47\u6570\u91cf\u3001\u8bcd\u6c47\u591a\u6837\u6027\u53d8\u5316\uff0c\u5e76\u5e94\u7528\u6cca\u677e\u548c\u4e9a\u6cca\u677e\u7edf\u8ba1\u91cf\u5316\u8bcd\u6c47\u8fde\u63a5\u548c\u91cd\u7ec4\u3002", "result": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u660e\u663e\u7684\u76f8\u53d8\u70b9\uff0c\u8fd9\u4e9b\u76f8\u53d8\u5728\u6807\u51c6\u635f\u5931\u6216\u9a8c\u8bc1\u66f2\u7ebf\u4e2d\u4e0d\u660e\u663e\uff0c\u4f46\u901a\u8fc7\u8bcd\u6c47\u548c\u7edf\u8ba1\u65b9\u6cd5\u53d8\u5f97\u53ef\u89c1\u3002", "conclusion": "\u76f8\u53d8\u91cd\u7ec4\u662f\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u666e\u904d\u7279\u5f81\uff0c\u53ef\u5728\u9002\u5ea6\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\uff0c\u5728\u539f\u59cb\u8bad\u7ec3\u7a7a\u95f4\u4e2d\u76f4\u63a5\u68c0\u6d4b\uff0c\u5e76\u5728\u8fde\u8d2f\u6027\u51fa\u73b0\u7684\u65e9\u671f\u9636\u6bb5\u53d1\u751f\u3002"}}
{"id": "2511.13327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13327", "abs": "https://arxiv.org/abs/2511.13327", "authors": ["Juntao Jian", "Yi-Lin Wei", "Chengjie Mou", "Yuhao Lin", "Xing Zhu", "Yujun Shen", "Wei-Shi Zheng", "Ruizhen Hu"], "title": "ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning", "comment": null, "summary": "Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \\textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.", "AI": {"tldr": "\u63d0\u51faZeroDexGrasp\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6293\u53d6\u4f18\u5316\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9762\u5411\u4efb\u52a1\u7684\u7075\u5de7\u6293\u53d6\u5408\u6210\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u5728\u591a\u6837\u5316\u7269\u4f53\u548c\u4efb\u52a1\u6307\u4ee4\u95f4\u6cdb\u5316\uff0c\u9700\u8981\u5f00\u53d1\u96f6\u6837\u672c\u7684\u8bed\u4e49\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u63a8\u65ad\u521d\u59cb\u6293\u53d6\u914d\u7f6e\u548c\u7269\u4f53\u63a5\u89e6\u4fe1\u606f\uff0c\u7136\u540e\u901a\u8fc7\u63a5\u89e6\u5f15\u5bfc\u7684\u6293\u53d6\u4f18\u5316\u6765\u7ec6\u5316\u59ff\u52bf\uff0c\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u548c\u4efb\u52a1\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u522b\u548c\u590d\u6742\u4efb\u52a1\u9700\u6c42\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u96f6\u6837\u672c\u7075\u5de7\u6293\u53d6\u3002", "conclusion": "ZeroDexGrasp\u63a8\u52a8\u4e86\u66f4\u901a\u7528\u548c\u667a\u80fd\u7684\u673a\u5668\u4eba\u6293\u53d6\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.12782", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12782", "abs": "https://arxiv.org/abs/2511.12782", "authors": ["Thomas Rivasseau"], "title": "LLM Reinforcement in Context", "comment": "4 pages", "summary": "Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u4e2d\u65ad\u673a\u5236\u4f5c\u4e3a\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7528\u6237\u8f93\u5165\u4e2d\u5b9a\u671f\u63d2\u5165\u63a7\u5236\u8bed\u53e5\u6765\u9632\u6b62\u8d8a\u72f1\u653b\u51fb\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u8bad\u7ec3\u548c\u63d0\u793a\u6765\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4f46\u7f3a\u4e4f\u968f\u7528\u6237\u8f93\u5165\u957f\u5ea6\u6269\u5c55\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0LLM\u8d8a\u72f1\u6982\u7387\u968f\u7528\u6237\u8f93\u5165\u6216\u5bf9\u8bdd\u957f\u5ea6\u589e\u52a0\u800c\u4e0a\u5347\u3002", "method": "\u63d0\u51fa\u4e2d\u65ad\u673a\u5236\uff1a\u5728\u7528\u6237\u8f93\u5165\u4e2d\u6bcf\u9694x\u4e2atoken\u63d2\u5165\u63a7\u5236\u8bed\u53e5\uff0c\u5e76\u53ef\u5c06\u6b64\u65b9\u6cd5\u63a8\u5e7f\u5230\u601d\u7ef4\u94fe\u8fc7\u7a0b\u4e2d\u4ee5\u9632\u6b62\u7b56\u7565\u6027\u884c\u4e3a\u3002", "result": "\u8be5\u65b9\u6cd5\u5c1a\u672a\u5728\u8bba\u6587\u4e2d\u62a5\u544a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u548c\u6982\u5ff5\u9a8c\u8bc1\u3002", "conclusion": "\u4e2d\u65ad\u673a\u5236\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u968f\u7528\u6237\u8f93\u5165\u957f\u5ea6\u6269\u5c55\u5730\u589e\u5f3aLLM\u5bf9\u9f50\u6027\uff0c\u9632\u6b62\u8d8a\u72f1\u653b\u51fb\u548c\u7b56\u7565\u6027\u884c\u4e3a\u3002"}}
{"id": "2511.13459", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13459", "abs": "https://arxiv.org/abs/2511.13459", "authors": ["Bingkun Huang", "Yuhe Gong", "Zewen Yang", "Tianyu Ren", "Luis Figueredo"], "title": "Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness", "comment": null, "summary": "Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u7a7a\u95f4\u548c\u80fd\u91cf\u5b89\u5168\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408PPO\u548c\u8fd0\u52a8\u57fa\u5143\u6765\u5904\u7406\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u786e\u4fdd\u5b89\u5168\u4ea4\u4e92\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eMDP\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u5173\u8282\u7a7a\u95f4\u4e2d\u5e94\u7528\uff0c\u5bf93D\u73af\u5883\u611f\u77e5\u6709\u9650\uff0c\u4e14\u5ffd\u89c6\u4e86\u4efb\u52a1\u7a7a\u95f4\u4e2d\u63a5\u89e6\u4e30\u5bcc\u4fe1\u606f\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u548c\u8fd0\u52a8\u57fa\u5143\u751f\u6210\u53ef\u9760\u7684\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\uff0c\u5e76\u878d\u5165\u80fd\u91cf\u611f\u77e5\u7684\u7b1b\u5361\u5c14\u963b\u6297\u63a7\u5236\u5668\u76ee\u6807\u4ee5\u786e\u4fdd\u5b89\u5168\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u57283D\u73af\u5883\u4e2d\u5404\u79cd\u8868\u9762\u4e0a\u7684\u4efb\u52a1\u5904\u7406\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u3001\u5e73\u6ed1\u8f68\u8ff9\u548c\u80fd\u91cf\u5b89\u5168\u4ea4\u4e92\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4efb\u52a1\u7a7a\u95f4\u80fd\u91cf\u5b89\u5168\u6846\u67b6\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12784", "categories": ["cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.12784", "abs": "https://arxiv.org/abs/2511.12784", "authors": ["Hayden Moore", "Asfahan Shah"], "title": "Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing", "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u5bf9\u8bed\u4e49\u76f8\u4f3c\u4f46\u8868\u8fbe\u4e0d\u540c\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u8f93\u51fa\u5bf9\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\u7684\u5fae\u5c0f\u53d8\u5316\u654f\u611f\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660e\u5b83\u4eec\u5728\u6587\u672c\u5230SQL\u4efb\u52a1\u4e2d\u5bf9\u8bed\u4e49\u4fdd\u6301\u7684\u6539\u5199\u8f93\u5165\u654f\u611f\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u8fd9\u79cd\u654f\u611f\u6027\u662f\u5426\u4e5f\u5b58\u5728\u4e8e\u81ea\u52a8\u5f62\u5f0f\u5316\u9886\u57df\u3002", "method": "\u4f7f\u7528MiniF2F\u548cProofNet\u7684Lean 4\u7248\u672c\u4f5c\u4e3a\u5f62\u5f0f\u5316\u57fa\u51c6\uff0c\u751f\u6210\u8bed\u4e49\u76f8\u4f3c\u7684\u6539\u5199\u81ea\u7136\u8bed\u8a00\u9648\u8ff0\uff0c\u5e76\u5728\u4e24\u4e2a\u73b0\u4ee3LLMs\u4e0a\u8fdb\u884c\u4ea4\u53c9\u8bc4\u4f30\uff0c\u6d4b\u91cf\u8bed\u4e49\u548c\u7f16\u8bd1\u6709\u6548\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u6539\u5199\u8f93\u5165\u4e0a\u7684\u6027\u80fd\u5b58\u5728\u53d8\u5f02\u6027\uff0c\u81ea\u7136\u8bed\u8a00\u9648\u8ff0\u7684\u5fae\u5c0f\u53d8\u5316\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u3002", "conclusion": "LLMs\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u5bf9\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\u7684\u53d8\u5316\u654f\u611f\uff0c\u8fd9\u63ed\u793a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.13530", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13530", "abs": "https://arxiv.org/abs/2511.13530", "authors": ["Vesna Poprcova", "Iulia Lefter", "Matthias Wieser", "Martijn Warnier", "Frances Brazier"], "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety", "comment": "Accepted at the Workshop on Benefits of pErsonalization and behAvioral adaptation in assistive Robots (BEAR 2025), held at the IEEE RO-MAN Conference 2025", "summary": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6536\u96c6\u793e\u4ea4\u7126\u8651\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u534f\u8bae\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e0eFurhat\u793e\u4ea4\u673a\u5668\u4eba\u4ea4\u4e92\u65f6\u7684\u540c\u6b65\u97f3\u9891\u3001\u89c6\u9891\u548c\u751f\u7406\u8bb0\u5f55\uff0c\u65e8\u5728\u652f\u6301\u793e\u4ea4\u7126\u8651\u7684\u7a33\u5065\u591a\u6a21\u6001\u68c0\u6d4b\u7814\u7a76\u3002", "motivation": "\u793e\u4ea4\u7126\u8651\u662f\u4e00\u79cd\u666e\u904d\u5b58\u5728\u7684\u72b6\u51b5\uff0c\u5f71\u54cd\u4eba\u9645\u4e92\u52a8\u548c\u793e\u4ea4\u529f\u80fd\u3002\u76ee\u524d\u7f3a\u4e4f\u53cd\u6620\u793e\u4ea4\u7126\u8651\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u7684\u8fdb\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u6536\u96c6\u534f\u8bae\uff0c\u4ece\u81f3\u5c1170\u540d\u53c2\u4e0e\u8005\u4e2d\u83b7\u53d6\u540c\u6b65\u7684\u97f3\u9891\u3001\u89c6\u9891\u548c\u751f\u7406\u8bb0\u5f55\uff0c\u53c2\u4e0e\u8005\u6839\u636e\u793e\u4ea4\u7126\u8651\u6c34\u5e73\u5206\u7ec4\uff0c\u5728\u53d7\u63a7\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u4e0eFurhat\u793e\u4ea4\u673a\u5668\u4eba\u8fdb\u884c\u7ea610\u5206\u949f\u7684Wizard-of-Oz\u89d2\u8272\u626e\u6f14\u4ea4\u4e92\u3002", "result": "\u5c06\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4e30\u5bcc\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u53ef\u4ee5\u4e3a\u60c5\u611f\u81ea\u9002\u5e94\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u505a\u51fa\u8d21\u732e\uff0c\u901a\u8fc7\u63d0\u4f9b\u652f\u6301\u793e\u4ea4\u7126\u8651\u7a33\u5065\u591a\u6a21\u6001\u68c0\u6d4b\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2511.12821", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12821", "abs": "https://arxiv.org/abs/2511.12821", "authors": ["Ruiyu Wang", "Yuzhang Xie", "Xiao Hu", "Carl Yang", "Jiaying Lu"], "title": "BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals", "comment": null, "summary": "Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.", "AI": {"tldr": "BioMedJImpact\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u751f\u7269\u533b\u5b66\u671f\u520a\u5f71\u54cd\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u6587\u732e\u8ba1\u91cf\u6307\u6807\u3001\u5408\u4f5c\u7279\u5f81\u548c\u57fa\u4e8eLLM\u7684AI\u53c2\u4e0e\u5ea6\u6307\u6807\uff0c\u7528\u4e8e\u5206\u6790\u5408\u4f5c\u5f3a\u5ea6\u548cAI\u53c2\u4e0e\u5ea6\u5982\u4f55\u5171\u540c\u5f71\u54cd\u79d1\u5b66\u5f71\u54cd\u529b\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8d44\u6e90\u5f88\u5c11\u6355\u6349\u5408\u4f5c\u7ed3\u6784\u548cAI\u7814\u7a76\u5982\u4f55\u5171\u540c\u5851\u9020\u751f\u7269\u533b\u5b66\u671f\u520a\u58f0\u671b\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7efc\u5408\u6570\u636e\u96c6\u6765\u63a8\u8fdb\u671f\u520a\u5c42\u9762\u7684\u79d1\u5b66\u5f71\u54cd\u529b\u548cAI\u53c2\u4e0e\u5ea6\u5206\u6790\u3002", "method": "\u4ecePubMed Central\u7684174\u4e07\u7bc7\u6587\u7ae0\u6784\u5efa\u6570\u636e\u96c6\uff0c\u6574\u5408\u6587\u732e\u8ba1\u91cf\u6307\u6807\u3001\u5408\u4f5c\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u53ef\u590d\u73b0\u7684\u4e09\u9636\u6bb5LLM\u6d41\u7a0b\u63d0\u53d6AI\u53c2\u4e0e\u5ea6\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e00\u81f4\u8d8b\u52bf\uff1a\u5408\u4f5c\u5f3a\u5ea6\u66f4\u9ad8\u7684\u671f\u520a\uff08\u7279\u522b\u662f\u4f5c\u8005\u56e2\u961f\u66f4\u5927\u66f4\u591a\u6837\u5316\u7684\uff09\u83b7\u5f97\u66f4\u9ad8\u5f15\u7528\u5f71\u54cd\u529b\uff1bAI\u53c2\u4e0e\u5ea6\u65e5\u76ca\u6210\u4e3a\u671f\u520a\u58f0\u671b\u7684\u5f3a\u76f8\u5173\u56e0\u7d20\uff0c\u5c24\u5176\u5728\u56db\u5206\u4f4d\u6392\u540d\u4e2d\u3002", "conclusion": "BioMedJImpact\u65e2\u662f\u6355\u6349\u751f\u7269\u533b\u5b66\u4e0eAI\u4ea4\u53c9\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u4e5f\u662f\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u5185\u5bb9\u611f\u77e5\u7684\u79d1\u5b66\u8ba1\u91cf\u5206\u6790\u3002"}}
{"id": "2511.13707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13707", "abs": "https://arxiv.org/abs/2511.13707", "authors": ["Xiaoyu Liang", "Ziang Liu", "Kelvin Lin", "Edward Gu", "Ruolin Ye", "Tam Nguyen", "Cynthia Hsu", "Zhanxin Wu", "Xiaoman Yang", "Christy Sum Yu Cheung", "Harold Soh", "Katherine Dimitropoulou", "Tapomayukh Bhattacharjee"], "title": "OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving", "comment": "IROS 2025", "summary": "We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.", "AI": {"tldr": "OpenRoboCare\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u62a4\u7406\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u804c\u4e1a\u6cbb\u7597\u5e08\u6267\u884c\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u4efb\u52a1\u7684\u4e13\u5bb6\u6f14\u793a\uff0c\u6db5\u76d6RGB-D\u89c6\u9891\u3001\u59ff\u6001\u8ddf\u8e2a\u3001\u773c\u52a8\u8ffd\u8e2a\u3001\u4efb\u52a1\u6ce8\u91ca\u548c\u89e6\u89c9\u611f\u77e5\u4e94\u79cd\u6a21\u6001\u3002", "motivation": "\u62a4\u7406\u4efb\u52a1\u6d89\u53ca\u590d\u6742\u7684\u4eba\u673a\u7269\u7406\u4ea4\u4e92\uff0c\u9700\u8981\u7cbe\u786e\u7684\u906e\u6321\u611f\u77e5\u3001\u5b89\u5168\u7684\u7269\u7406\u63a5\u89e6\u548c\u957f\u671f\u89c4\u5212\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u7531\u4e13\u5bb6\u9a71\u52a8\u7684\u771f\u5b9e\u4e16\u754c\u62a4\u7406\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e8621\u540d\u804c\u4e1a\u6cbb\u7597\u5e08\u5728\u4e24\u4e2a\u4eba\u4f53\u6a21\u578b\u4e0a\u6267\u884c15\u9879\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u4efb\u52a1\u7684\u6570\u636e\uff0c\u6db5\u76d6\u4e94\u79cd\u6a21\u6001\uff1aRGB-D\u89c6\u9891\u3001\u59ff\u6001\u8ddf\u8e2a\u3001\u773c\u52a8\u8ffd\u8e2a\u3001\u4efb\u52a1\u548c\u52a8\u4f5c\u6ce8\u91ca\u3001\u89e6\u89c9\u611f\u77e5\u3002", "result": "\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5173\u4e8e\u62a4\u7406\u4eba\u5458\u8fd0\u52a8\u3001\u6ce8\u610f\u529b\u3001\u65bd\u529b\u548c\u4efb\u52a1\u6267\u884c\u7b56\u7565\u7684\u4e30\u5bcc\u591a\u6a21\u6001\u6d1e\u5bdf\uff0c\u5e76\u5bf9\u73b0\u6709\u673a\u5668\u4eba\u611f\u77e5\u548c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\u3002", "conclusion": "OpenRoboCare\u586b\u8865\u4e86\u673a\u5668\u4eba\u62a4\u7406\u9886\u57df\u7684\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u5b89\u5168\u81ea\u9002\u5e94\u7684\u8f85\u52a9\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e13\u5bb6\u62a4\u7406\u539f\u5219\u548c\u7b56\u7565\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u6548\u7387\u548c\u4efb\u52a1\u53ef\u884c\u6027\u3002"}}
{"id": "2511.12832", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12832", "abs": "https://arxiv.org/abs/2511.12832", "authors": ["Niranjan Chebrolu", "Gerard Christopher Yeo", "Kokil Jaidka"], "title": "From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation", "comment": null, "summary": "Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.", "AI": {"tldr": "\u901a\u8fc7\u76ee\u6807\u6fc0\u6d3b\u5de5\u7a0b\u5f15\u5bfcLLaMA 3.1-8B\u5c55\u73b0\u66f4\u4eba\u6027\u5316\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u5173\u952e\u5e72\u9884\u4f4d\u70b9\uff0c\u4ece\u5bf9\u6bd4\u6587\u672c\u5bf9\u63a8\u5bfc\u60c5\u611f\u8868\u8fbe\u5411\u91cf\uff0c\u663e\u8457\u589e\u5f3a\u60c5\u611f\u7279\u5f81\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5bf9\u8bdd\u6d41\u7545\u5ea6\u4e0d\u65ad\u63d0\u5347\uff0c\u4f46\u8d4b\u4e88\u5176\u7ec6\u817b\u3001\u4eba\u6027\u5316\u7684\u60c5\u611f\u8868\u8fbe\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u5bf9\u9f50\u6280\u672f\u5f80\u5f80\u53ea\u5904\u7406\u8868\u9762\u8f93\u51fa\u6216\u9700\u8981\u5927\u91cf\u5fae\u8c03\u3002", "method": "\u91c7\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u56e0\u679c\u5f71\u54cd\u7ec4\u4ef6\uff0c\u901a\u8fc7\u89c2\u5bdf\u8bca\u65ad\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u6fc0\u6d3b\u6a21\u5f0f\u627e\u5230\u5173\u952e\u5e72\u9884\u4f4d\u70b9\uff1b\u4ece\u5bf9\u6bd4\u6587\u672c\u5bf9\uff08\u79ef\u6781vs\u6d88\u6781\u60c5\u611f\u793a\u4f8b\uff09\u7684\u6fc0\u6d3b\u5dee\u5f02\u63a8\u5bfc\u60c5\u611f\u8868\u8fbe\u5411\u91cf\u3002", "result": "\u5c06\u60c5\u611f\u5411\u91cf\u5e94\u7528\u4e8e\u65b0\u5bf9\u8bdd\u63d0\u793a\u663e\u8457\u589e\u5f3a\u4e86\u60c5\u611f\u7279\u5f81\uff1a\u5f15\u5bfc\u540e\u7684\u54cd\u5e94\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u79ef\u6781\u60c5\u611f\uff08\u5982\u559c\u60a6\u3001\u4fe1\u4efb\uff09\u548c\u66f4\u9891\u7e41\u7684\u7b2c\u4e00\u4eba\u79f0\u4ee3\u8bcd\u4f7f\u7528\uff0c\u8868\u660e\u66f4\u5f3a\u7684\u4e2a\u4eba\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cbe\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u4e3a\u5bf9\u8bddAI\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.13710", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13710", "abs": "https://arxiv.org/abs/2511.13710", "authors": ["Jianglong Ye", "Lai Wei", "Guangqi Jiang", "Changwei Jing", "Xueyan Zou", "Xiaolong Wang"], "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands", "comment": "Project page: https://jianglongye.com/power-to-precision", "summary": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u591a\u6307\u7075\u5de7\u624b\u63a7\u5236\u7b56\u7565\u548c\u786c\u4ef6\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u7684\u6307\u5c16\u51e0\u4f55\u4fee\u6539\u5b9e\u73b0\u4e86\u65e2\u80fd\u8fdb\u884c\u5f3a\u529b\u6293\u53d6\u53c8\u80fd\u8fdb\u884c\u7cbe\u786e\u64cd\u4f5c\u7684\u5355\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u96be\u4ee5\u5728\u5355\u4e00\u7cfb\u7edf\u4e2d\u540c\u65f6\u5b9e\u73b0\u7a33\u5b9a\u7684\u5f3a\u529b\u6293\u53d6\u548c\u7cbe\u786e\u7684\u7cbe\u7ec6\u64cd\u4f5c\u3002\u591a\u6307\u624b\u64c5\u957f\u5f3a\u529b\u6293\u53d6\uff0c\u4f46\u7cbe\u786e\u64cd\u4f5c\u4efb\u52a1\u4ecd\u4e3b\u8981\u4f7f\u7528\u5e73\u884c\u5939\u722a\u3002", "method": "1. \u5f15\u5165\u8f7b\u91cf\u7ea7\u6307\u5c16\u51e0\u4f55\u4fee\u6539\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u63a5\u89e6\u5e73\u9762\uff1b2. \u8054\u5408\u4f18\u5316\u6307\u5c16\u51e0\u4f55\u53c2\u6570\u548c\u76f8\u5e94\u63a7\u5236\u7b56\u7565\uff1b3. \u63a7\u5236\u7b56\u7565\u5728\u5f3a\u529b\u64cd\u4f5c\u548c\u7cbe\u786e\u64cd\u4f5c\u95f4\u52a8\u6001\u5207\u6362\uff0c\u5c06\u7cbe\u786e\u63a7\u5236\u7b80\u5316\u4e3a\u5e73\u884c\u62c7\u6307-\u98df\u6307\u8fd0\u52a8\uff1b4. \u5229\u7528\u5927\u89c4\u6a21\u4eff\u771f\u548c\u53ef\u5fae\u5206\u795e\u7ecf\u7269\u7406\u66ff\u4ee3\u6a21\u578b\u4f18\u5316\u6307\u5c16\u51e0\u4f55\u3002", "result": "\u5728\u672a\u89c1\u7269\u4f53\u7684sim-to-real\u7cbe\u786e\u6293\u53d6\u4e2d\u8fbe\u523082.5%\u7684\u96f6\u6837\u672c\u6210\u529f\u7387\uff0c\u5728\u6d89\u53ca\u9762\u5305\u634f\u53d6\u7684\u771f\u5b9e\u4e16\u754c\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8fbe\u523093.3%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u80fd\u663e\u8457\u589e\u5f3a\u591a\u6307\u624b\u7684\u7cbe\u7ec6\u64cd\u4f5c\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u524a\u5f31\u5176\u5f3a\u529b\u6293\u53d6\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u9650\u5236\u3002"}}
{"id": "2511.12850", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12850", "abs": "https://arxiv.org/abs/2511.12850", "authors": ["Saranzaya Magsarjav", "Melissa Humphries", "Jonathan Tuke", "Lewis Mitchell"], "title": "Quantifying consistency and accuracy of Latent Dirichlet Allocation", "comment": "8 pages, 3 figures, to be submitted", "summary": "Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a33\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u6765\u89e3\u51b3\u6982\u7387\u4e3b\u9898\u6a21\u578b\u56e0\u968f\u673a\u6027\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5177\u6709\u771f\u5b9e\u4e3b\u9898\u7684\u8bed\u6599\u5e93\u6765\u8bc4\u4f30LDA\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u6982\u7387\u4e3b\u9898\u6a21\u578b\u7531\u4e8e\u968f\u673a\u6027\u5728\u91cd\u590d\u8fd0\u884c\u65f6\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u91cd\u590d\u6027\u3001\u53ef\u9760\u6027\u548c\u89e3\u91ca\u6027\uff0c\u5f15\u53d1\u4e86\u5bf9\u4e3b\u9898\u6a21\u578b\u662f\u5426\u771f\u6b63\u6355\u6349\u5230\u6709\u610f\u4e49\u4e3b\u9898\u7684\u8d28\u7591\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7ed3\u5408\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u7684\u65b0\u7a33\u5b9a\u6027\u5ea6\u91cf\uff0c\u5229\u7528LDA\u7684\u751f\u6210\u7279\u6027\u751f\u6210\u5177\u6709\u771f\u5b9e\u4e3b\u9898\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u8bed\u6599\u5e93\u8fd0\u884cLDA 50\u6b21\u4ee5\u8bc4\u4f30\u8f93\u51fa\u53d8\u5f02\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0LDA\u80fd\u591f\u6b63\u786e\u786e\u5b9a\u6587\u6863\u4e2d\u7684\u57fa\u7840\u4e3b\u9898\u6570\u91cf\uff0c\u591a\u6b21\u91cd\u590d\u8fd0\u884c\u8fd4\u56de\u76f8\u4f3c\u4e3b\u9898\uff0c\u8868\u660eLDA\u5177\u6709\u8f83\u597d\u7684\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u4f46\u8fd9\u4e9b\u4e3b\u9898\u5e76\u975e\u771f\u5b9e\u4e3b\u9898\u3002", "conclusion": "LDA\u5728\u786e\u5b9a\u4e3b\u9898\u6570\u91cf\u65b9\u9762\u8868\u73b0\u51c6\u786e\u4e14\u5177\u6709\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u4f46\u751f\u6210\u7684\u4e3b\u9898\u4e0e\u771f\u5b9e\u4e3b\u9898\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u51f8\u663e\u4e86\u9700\u8981\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fdd\u4e3b\u9898\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2511.12851", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12851", "abs": "https://arxiv.org/abs/2511.12851", "authors": ["Kang Yin", "Hye-Bin Shin"], "title": "NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation", "comment": null, "summary": "Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.", "AI": {"tldr": "NeuroLex\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e34\u5e8a\u8111\u7535\u56fe\u62a5\u544a\u7684\u8f7b\u91cf\u7ea7\u9886\u57df\u81ea\u9002\u5e94\u8bed\u8a00\u6a21\u578b\uff0c\u5728EEG\u62a5\u544a\u6587\u672c\u4e0a\u8bad\u7ec3\uff0c\u80fd\u66f4\u597d\u5730\u7406\u89e3EEG\u62a5\u544a\u7684\u8bed\u8a00\u7279\u5f81\u548c\u8bca\u65ad\u6a21\u5f0f\u3002", "motivation": "\u901a\u7528\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u6355\u6349\u4e34\u5e8aEEG\u62a5\u544a\u4e2d\u9886\u57df\u7279\u5b9a\u7684\u8bed\u8a00\u60ef\u4f8b\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9EEG\u62a5\u544a\u8bed\u8a00\u7279\u5f81\u8fdb\u884c\u4f18\u5316\u7684\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u8de8\u5ea6\u635f\u574f\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u5f0f\u5fae\u8c03\uff08\u5305\u62ec\u62a5\u544a\u6da6\u8272\u3001\u6bb5\u843d\u6458\u8981\u548c\u672f\u8bed\u95ee\u7b54\uff09\uff0c\u5728\u54c8\u4f5bEEG\u6570\u636e\u5e93\u7684EEG\u62a5\u544a\u6587\u672c\u4e0a\u8bad\u7ec3\u3002", "result": "\u4e0e\u540c\u7b49\u89c4\u6a21\u7684\u901a\u7528\u6a21\u578b\u76f8\u6bd4\uff0cNeuroLex\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\u3001\u66f4\u9ad8\u7684\u63d0\u53d6\u548c\u6458\u8981\u51c6\u786e\u6027\u3001\u66f4\u597d\u7684\u6807\u7b7e\u6548\u7387\uff0c\u4ee5\u53ca\u5bf9\u5426\u5b9a\u548c\u4e8b\u5b9e\u5e7b\u89c9\u7684\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "NeuroLex\u901a\u8fc7EEG\u611f\u77e5\u7684\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\uff0c\u8fde\u63a5\u4e86\u751f\u7269\u533b\u5b66\u6587\u672c\u5efa\u6a21\u548c\u8111\u673a\u63a5\u53e3\u5e94\u7528\uff0c\u4e3a\u53ef\u89e3\u91ca\u548c\u8bed\u8a00\u9a71\u52a8\u7684\u795e\u7ecf\u89e3\u7801\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12861", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12861", "abs": "https://arxiv.org/abs/2511.12861", "authors": ["Wenxin Zhu", "Andong Chen", "Yuchen Song", "Kehai Chen", "Conghui Zhu", "Ziyan Chen", "Tiejun Zhao"], "title": "From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models", "comment": "Survey; 7 figures, 3 tables, 44 pages", "summary": "With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on \"Multimodal Chain-of-Thought\" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u6280\u672f\uff0c\u5206\u6790\u4e86\u5176\u80cc\u666f\u52a8\u673a\u3001\u4e3b\u6d41\u65b9\u6cd5\u3001\u8bc4\u4f30\u6307\u6807\u3001\u5e94\u7528\u573a\u666f\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6210\u529f\uff0c\u63d0\u5347\u5176\u590d\u6742\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002\u73b0\u6709\u6a21\u578b\u5b58\u5728\u63a8\u7406\u8def\u5f84\u4e0d\u900f\u660e\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u800c\u601d\u7ef4\u94fe\u63a8\u7406\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5df2\u8bc1\u660e\u80fd\u589e\u5f3a\u63a8\u7406\u900f\u660e\u5ea6\u548c\u8f93\u51fa\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u671b\u5728\u6269\u5c55\u5230\u591a\u6a21\u6001\u9886\u57df\u540e\u6539\u5584\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4ece\u4e09\u4e2a\u65b9\u9762\u4ecb\u7ecd\u4e3b\u6d41MCoT\u65b9\u6cd5\uff1a\u601d\u7ef4\u94fe\u8303\u5f0f\u3001\u540e\u8bad\u7ec3\u9636\u6bb5\u548c\u63a8\u7406\u9636\u6bb5\uff0c\u5e76\u5206\u6790\u5176\u5e95\u5c42\u673a\u5236\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u6307\u6807\uff0c\u8ba8\u8bba\u4e86MCoT\u7684\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u5206\u6790\u4e86MCoT\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u5bf9\u5176\u672a\u6765\u7814\u7a76\u65b9\u5411\u8fdb\u884c\u4e86\u5c55\u671b\u3002"}}
{"id": "2511.12874", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12874", "abs": "https://arxiv.org/abs/2511.12874", "authors": ["Chukwuebuka Fortunate Ijezue", "Tania-Amanda Fredrick Eneye", "Maaz Amjad"], "title": "Classification of Hope in Textual Data using Transformer-Based Models", "comment": null, "summary": "This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff08BERT\u3001GPT-2\u3001DeBERTa\uff09\u5728\u5e0c\u671b\u8868\u8fbe\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0BERT\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\u6765\u5206\u6790\u6587\u672c\u4e2d\u7684\u5e0c\u671b\u8868\u8fbe\uff0c\u5e94\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u548c\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u9886\u57df\u3002", "method": "\u4f7f\u7528BERT\u3001GPT-2\u548cDeBERTa\u4e09\u79cdTransformer\u67b6\u6784\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff08\u5e0c\u671bvs\u975e\u5e0c\u671b\uff09\u548c\u591a\u7c7b\u5206\u7c7b\uff08\u4e94\u4e2a\u5e0c\u671b\u76f8\u5173\u7c7b\u522b\uff09\u7684\u6bd4\u8f83\u7814\u7a76\u3002", "result": "BERT\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u8fbe\u523084.49%\u51c6\u786e\u7387\uff0c\u591a\u7c7b\u5206\u7c7b\u4e2d\u8fbe\u523072.03%\u51c6\u786e\u7387\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u6700\u9ad8\uff08443\u79d2\u8bad\u7ec3\u65f6\u95f4\uff09\u3002GPT-2\u8868\u73b0\u6700\u5dee\uff0879.34%\u4e8c\u5143\uff0c71.29%\u591a\u7c7b\uff09\uff0cDeBERTa\u8868\u73b0\u4e2d\u7b49\u4f46\u8ba1\u7b97\u6210\u672c\u6700\u9ad8\uff08947\u79d2\u591a\u7c7b\u8bad\u7ec3\uff09\u3002", "conclusion": "\u5bf9\u4e8e\u4e13\u4e1a\u5316\u7684\u60c5\u611f\u68c0\u6d4b\u4efb\u52a1\uff0c\u67b6\u6784\u7684\u9002\u7528\u6027\u53ef\u80fd\u6bd4\u6a21\u578b\u89c4\u6a21\u66f4\u91cd\u8981\uff0cBERT\u5728\u5e0c\u671b\u8868\u8fbe\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2511.12920", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12920", "abs": "https://arxiv.org/abs/2511.12920", "authors": ["Desheng Hu", "Joachim Baumann", "Aleksandra Urman", "Elsa Lichtenegger", "Robin Forsberg", "Aniko Hannak", "Christo Wilson"], "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy", "comment": "18 pages, 10 figures; to appear in AAAI ICWSM 2026", "summary": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u7b97\u6cd5\u5ba1\u8ba1\u8bc4\u4f30Google\u641c\u7d22\u4e2dAI\u751f\u6210\u5185\u5bb9\uff08AI\u6982\u89c8\u548c\u7cbe\u9009\u6458\u8981\uff09\u7684\u8d28\u91cf\uff0c\u53d1\u73b0\u5728\u5a74\u513f\u62a4\u7406\u548c\u5b55\u671f\u76f8\u5173\u67e5\u8be2\u4e2d\u5b58\u5728\u4fe1\u606f\u4e0d\u4e00\u81f4\u3001\u533b\u7597\u5b89\u5168\u63aa\u65bd\u7f3a\u5931\u7b49\u95ee\u9898\u3002", "motivation": "Google\u641c\u7d22\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7AI\u6982\u89c8\u548c\u7cbe\u9009\u6458\u8981\u5c55\u793aAI\u751f\u6210\u5185\u5bb9\uff0c\u7528\u6237\u4f9d\u8d56\u8fd9\u4e9b\u4fe1\u606f\u4f46\u65e0\u6cd5\u63a7\u5236\u5176\u5448\u73b0\u65b9\u5f0f\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u4fe1\u606f\u663e\u793a\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "method": "\u5bf91,508\u4e2a\u771f\u5b9e\u7684\u5a74\u513f\u62a4\u7406\u548c\u5b55\u671f\u76f8\u5173\u67e5\u8be2\u8fdb\u884c\u7cfb\u7edf\u7b97\u6cd5\u5ba1\u8ba1\uff0c\u4f7f\u7528\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u8bc4\u4f30\u7b54\u6848\u4e00\u81f4\u6027\u3001\u76f8\u5173\u6027\u3001\u533b\u7597\u5b89\u5168\u63aa\u65bd\u5b58\u5728\u6027\u3001\u6765\u6e90\u7c7b\u522b\u548c\u60c5\u611f\u5bf9\u9f50\u7b49\u591a\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u3002", "result": "AI\u6982\u89c8\u548c\u7cbe\u9009\u6458\u8981\u4e4b\u95f4\u5b58\u572833%\u7684\u4fe1\u606f\u4e0d\u4e00\u81f4\uff1b\u533b\u7597\u5b89\u5168\u63aa\u65bd\u4e25\u91cd\u7f3a\u5931\uff08AI\u6982\u89c8\u4ec511%\uff0c\u7cbe\u9009\u6458\u8981\u4ec57%\uff09\uff1b\u5065\u5eb7\u7f51\u7ad9\u662f\u4e3b\u8981\u6765\u6e90\uff0c\u4f46\u7cbe\u9009\u6458\u8981\u4e5f\u5e38\u94fe\u63a5\u5546\u4e1a\u6765\u6e90\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u516c\u5171\u536b\u751f\u4fe1\u606f\u83b7\u53d6\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u8868\u660e\u5728AI\u4ecb\u5bfc\u7684\u5065\u5eb7\u4fe1\u606f\u4e2d\u9700\u8981\u66f4\u5f3a\u7684\u8d28\u91cf\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u4e3a\u5ba1\u8ba1\u9ad8\u98ce\u9669\u9886\u57df\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u6846\u67b6\u3002"}}
{"id": "2511.12928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12928", "abs": "https://arxiv.org/abs/2511.12928", "authors": ["Haokun Li", "Yazhou Zhang", "Jizhi Ding", "Qiuchi Li", "Peng Zhang"], "title": "Visual Room 2.0: Seeing is Not Understanding for MLLMs", "comment": null, "summary": "Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \\textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\\%$\\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u623f\u95f4\u8bba\u8bc1\uff0c\u8ba4\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u7cbe\u786e\u63cf\u8ff0\u89c6\u89c9\u7ec6\u8282\u4f46\u65e0\u6cd5\u7406\u89e3\u5e95\u5c42\u60c5\u611f\u548c\u610f\u56fe\uff0c\u5373\"\u770b\u5230\u4e0d\u7b49\u4e8e\u7406\u89e3\"\u3002\u4f5c\u8005\u6784\u5efa\u4e86Visual Room 2.0\u57fa\u51c6\u6765\u8bc4\u4f30MLLMs\u7684\u611f\u77e5-\u8ba4\u77e5\u5bf9\u9f50\uff0c\u6db5\u76d617\u4e2a\u4efb\u52a1\u548c2,100\u4e2a\u95ee\u9898\u3002", "motivation": "\u6269\u5c55Searle\u7684\u4e2d\u6587\u623f\u95f4\u8bba\u8bc1\u5230\u591a\u6a21\u6001\u9886\u57df\uff0c\u8d28\u7591MLLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u6240\u89c1\u5185\u5bb9\u3002\u4f5c\u8005\u8ba4\u4e3a\u6a21\u578b\u53ef\u80fd\u51c6\u786e\u63cf\u8ff0\u89c6\u89c9\u7ec6\u8282\u4f46\u7f3a\u4e4f\u5bf9\u60c5\u611f\u548c\u610f\u56fe\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86Visual Room 2.0\u57fa\u51c6\uff0c\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u548c\u8ba4\u77e5\u8fc7\u7a0b\u7684\u4e09\u4e2a\u5c42\u6b21\uff08\u4f4e\u3001\u4e2d\u3001\u9ad8\uff09\uff0c\u5305\u542b17\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u3002\u6570\u636e\u96c6\u5305\u542b350\u4e2a\u591a\u6a21\u6001\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u67096\u4e2a\u6e10\u8fdb\u5f0f\u95ee\u9898\uff08\u51712,100\u4e2a\uff09\uff0c\u6db5\u76d6\u4ece\u611f\u77e5\u5230\u8ba4\u77e5\u7684\u5b8c\u6574\u8fc7\u7a0b\u3002", "result": "\u8bc4\u4f3010\u4e2a\u6700\u5148\u8fdb\u7684MLLMs\u53d1\u73b0\uff1a(1) MLLMs\u7684\u611f\u77e5\u80fd\u529b\u6bd4\u8ba4\u77e5\u80fd\u529b\u66f4\u5f3a\uff088.0%\u5dee\u8ddd\uff09\uff1b(2) \u8ba4\u77e5\u4f3c\u4e4e\u4e0d\u56e0\u679c\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u611f\u77e5\u7684\u63a8\u7406\uff1b(3) \u8ba4\u77e5\u80fd\u529b\u968f\u6a21\u578b\u89c4\u6a21\u6269\u5c55\uff0c\u4f46\u611f\u77e5\u80fd\u529b\u5728\u66f4\u5927\u53d8\u4f53\u4e2d\u5e76\u672a\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\"\u770b\u5230\u2260\u7406\u89e3\"\u64cd\u4f5c\u5316\u4e3a\u53ef\u6d4b\u8bd5\u7684\u5047\u8bbe\uff0c\u4e3aMLLMs\u4ece\u611f\u77e5\u5904\u7406\u5230\u8ba4\u77e5\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002\u7814\u7a76\u8bc1\u5b9e\u4e86\u611f\u77e5\u548c\u8ba4\u77e5\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4e3a\u7406\u89e3MLLMs\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2511.12991", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12991", "abs": "https://arxiv.org/abs/2511.12991", "authors": ["Zeyu Shi", "Ziming Wang", "Tianyu Chen", "Shiqi Gao", "Haoyi Zhou", "Qingyun Sun", "Jianxin Li"], "title": "Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty", "comment": "Accepted by AAAI 2026 Main Track", "summary": "The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.", "AI": {"tldr": "HCNR\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u548c\u6062\u590d\u5173\u952e\u8868\u8fbe\u795e\u7ecf\u5143\u6765\u4fee\u590dSFT\u540eLLM\u7684\u8bda\u5b9e\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u901f\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u4f1a\u4e25\u91cd\u635f\u5bb3LLM\u7684\u8bda\u5b9e\u6027\uff0c\u4f46\u73b0\u6709\u6062\u590d\u65b9\u6cd5\u5047\u8bbe\u6a21\u578b\u5b8c\u5168\u4e27\u5931\u4e86\u77e5\u8bc6\u8fb9\u754c\u8bc6\u522b\u80fd\u529b\uff0c\u800c\u5b9e\u9645\u4e0a\u8fd9\u79cd\u80fd\u529b\u4ecd\u7136\u4fdd\u7559\uff0c\u53ea\u662f\u8868\u8fbe\u8fd9\u79cd\u610f\u8bc6\u7684\u80fd\u529b\u88ab\u6291\u5236\u4e86", "method": "\u63d0\u51faHonesty-Critical Neurons Restoration (HCNR)\u65b9\u6cd5\uff0c\u8bc6\u522b\u5e76\u6062\u590d\u5173\u952e\u8868\u8fbe\u795e\u7ecf\u5143\u5230\u9884\u8bad\u7ec3\u72b6\u6001\uff0c\u540c\u65f6\u901a\u8fc7Hessian\u5f15\u5bfc\u7684\u8865\u507f\u673a\u5236\u4e0e\u4efb\u52a1\u5bfc\u5411\u795e\u7ecf\u5143\u534f\u8c03", "result": "\u5728\u56db\u4e2aQA\u4efb\u52a1\u548c\u4e94\u4e2aLLM\u5bb6\u65cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHCNR\u6709\u6548\u6062\u590d\u4e8633.25%\u7684\u53d7\u635f\u8bda\u5b9e\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u81f3\u5c112.23\u500d\u52a0\u901f\u548c\u8d85\u8fc710\u500d\u7684\u6570\u636e\u51cf\u5c11", "conclusion": "HCNR\u4e3a\u53ef\u4fe1\u8d56LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u4fee\u590dSFT\u5bf9\u6a21\u578b\u8bda\u5b9e\u6027\u7684\u635f\u5bb3"}}
{"id": "2511.13029", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13029", "abs": "https://arxiv.org/abs/2511.13029", "authors": ["Declan Jackson", "William Keating", "George Cameron", "Micah Hill-Smith"], "title": "AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models", "comment": null, "summary": "Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.", "AI": {"tldr": "AA-Omniscience\u57fa\u51c6\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u56de\u5fc6\u548c\u77e5\u8bc6\u6821\u51c6\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u524d\u6cbf\u6a21\u578b\u5728\u4e8b\u5b9e\u6027\u548c\u6821\u51c6\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5f31\u70b9\uff0cClaude 4.1 Opus\u8868\u73b0\u6700\u4f73\u4f46\u5f97\u5206\u4ec5\u4e3a4.8\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u8861\u91cf\u901a\u7528\u80fd\u529b\uff0c\u4f46\u5728\u5404\u9886\u57df\u53ef\u9760\u4f7f\u7528\u9700\u8981\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u77e5\u8bc6\u5dee\u8ddd\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b6,000\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u95ee\u9898\u6765\u81ea\u6743\u5a01\u5b66\u672f\u548c\u884c\u4e1a\u8d44\u6e90\uff0c\u8986\u76d66\u4e2a\u9886\u57df42\u4e2a\u7ecf\u6d4e\u76f8\u5173\u4e3b\u9898\uff0c\u6d4b\u91cf\u5168\u77e5\u6307\u6570(-100\u5230100)\u3002", "result": "Claude 4.1 Opus\u5f97\u5206\u6700\u9ad8(4.8)\uff0c\u662f\u4ec5\u6709\u7684\u4e09\u4e2a\u5f97\u5206\u8d85\u8fc70\u7684\u6a21\u578b\u4e4b\u4e00\u3002\u4e0d\u540c\u9886\u57df\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4e09\u4e2a\u4e0d\u540c\u7814\u7a76\u5b9e\u9a8c\u5ba4\u7684\u6a21\u578b\u5728\u516d\u4e2a\u9886\u57df\u5404\u81ea\u9886\u5148\u3002", "conclusion": "\u6a21\u578b\u5728\u77e5\u8bc6\u91cd\u8981\u4efb\u52a1\u4e2d\u5e94\u6839\u636e\u7528\u4f8b\u9700\u6c42\u800c\u975e\u901a\u7528\u6027\u80fd\u8fdb\u884c\u9009\u62e9\uff0c\u56e0\u4e3a\u6027\u80fd\u5b58\u5728\u9886\u57df\u5dee\u5f02\u6027\u3002"}}
{"id": "2511.13040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13040", "abs": "https://arxiv.org/abs/2511.13040", "authors": ["Kasun Wickramasinghe", "Nisansa de Silva"], "title": "How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm", "comment": "15 pages, 2 figures, 6 tables", "summary": "Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u4e0e\u5bf9\u9f50\u5355\u8bed\u6a21\u578b\u5728\u53cc\u8bed\u8bcd\u5178\u5f52\u7eb3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5206\u6790\u4e86BLI\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8bcd\u5e72\u7684\u65b0BLI\u65b9\u6cd5\u548c\u8bcd\u6c47\u526a\u679d\u6280\u672f\u6765\u66f4\u51c6\u786e\u8bc4\u4f30\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u7a0b\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u591a\u8bed\u8a00\u5d4c\u5165\u5df2\u6210\u4e3a\u4e3b\u6d41\u9009\u62e9\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u662f\u5426\u5728\u6240\u6709\u65b9\u9762\u90fd\u4f18\u4e8e\u5bf9\u9f50\u5355\u8bed\u6a21\u578b\uff0c\u4ee5\u53ca\u5176\u66f4\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u662f\u5426\u603b\u662f\u5408\u7406\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22BLI\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u5d4c\u5165\u5bf9\u9f50\u6280\u672f\u5728\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u53cc\u8bed\u8bcd\u5178\u5f52\u7eb3\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u6bd4\u8f83\u4f20\u7edf\u5d4c\u5165\u5bf9\u9f50\u6280\u672f\u3001\u65b0\u578b\u591a\u8bed\u8a00\u6a21\u578b\u548c\u7ec4\u5408\u5bf9\u9f50\u6280\u672f\u7684\u6027\u80fd\u3002\u63d0\u51fa\u57fa\u4e8e\u8bcd\u5e72\u7684BLI\u65b9\u6cd5\u548c\u8bcd\u6c47\u526a\u679d\u6280\u672f\uff0c\u5206\u6790\u8bed\u8a00\u5bb6\u65cf\u5bf9\u5bf9\u9f50\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0BLI\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e0d\u80fd\u771f\u5b9e\u53cd\u6620\u5bf9\u9f50\u7a0b\u5ea6\uff1b\u7ec4\u5408\u5d4c\u5165\u5bf9\u9f50\u6280\u672f\u901a\u5e38\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u60c5\u51b5\u4e0b\u591a\u8bed\u8a00\u5d4c\u5165\u8868\u73b0\u66f4\u4f18\uff1b\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u8bc4\u4f30\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u3002", "conclusion": "\u591a\u8bed\u8a00\u5d4c\u5165\u4e0e\u5bf9\u9f50\u5355\u8bed\u6a21\u578b\u5404\u6709\u4f18\u52bf\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u573a\u666f\u9009\u62e9\uff1bBLI\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u6539\u8fdb\u65b9\u6cd5\uff1b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e2d\u591a\u8bed\u8a00\u5d4c\u5165\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2511.13043", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13043", "abs": "https://arxiv.org/abs/2511.13043", "authors": ["Xinyuan Zhou", "Yi Lei", "Xiaoyu Zhou", "Jingyi Sun", "Yu Zhu", "Zhongyi Ye", "Weitai Zhang", "Quan Liu", "Si Wei", "Cong Liu"], "title": "Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training", "comment": null, "summary": "Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a \"CoT-augmented state prediction\" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.", "AI": {"tldr": "Spark-Prover-X1\u662f\u4e00\u4e2a7B\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u63d0\u5347\u8f7b\u91cf\u7ea7LLM\u7684\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5b9a\u7406\u8bc1\u660e\u4efb\u52a1\u4e0a\u8fbe\u5230\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u8fdb\u5c55\u53d7\u5230\u9ad8\u8d28\u91cf\u5f62\u5f0f\u8bed\u8a00\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u5728\u5e7f\u6cdb\u6570\u5b66\u8bed\u6599\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5f15\u5165\"\u601d\u7ef4\u94fe\u589e\u5f3a\u72b6\u6001\u9884\u6d4b\"\u4efb\u52a1\uff1b2\uff09\u5728\u4e13\u5bb6\u8fed\u4ee3\u5faa\u73af\u4e2d\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b3\uff09\u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u9488\u5bf9\u6700\u96be\u95ee\u9898\u8fdb\u884c\u5f3a\u5316\u8bad\u7ec3\u3002", "result": "Spark-Prover-X1-7B\u5728\u7c7b\u4f3c\u5927\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e73\u5747\u901a\u8fc7\u738737.0%\uff08pass@32\uff09\uff0c\u5728PutnamBench\u4e0a\u89e3\u51b327\u4e2a\u95ee\u9898\uff0c\u5728CombiBench\u4e0a\u8fbe\u523024.0%\u3002", "conclusion": "\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u548c\u9010\u6b65\u7cbe\u70bc\u7684\u8bad\u7ec3\u6d41\u6c34\u7ebf\u4e3a\u589e\u5f3a\u8f7b\u91cf\u7ea7LLM\u7684\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2511.13095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13095", "abs": "https://arxiv.org/abs/2511.13095", "authors": ["Chuyuan Li", "Giuseppe Carenini"], "title": "BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models", "comment": null, "summary": "We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.", "AI": {"tldr": "BeDiscovER\u662f\u4e00\u4e2a\u8bc4\u4f30\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u8bdd\u8bed\u7406\u89e3\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b5\u4e2a\u8bdd\u8bed\u4efb\u52a1\u548c52\u4e2a\u6570\u636e\u96c6\uff0c\u8986\u76d6\u8bcd\u6c47\u3001\u53e5\u5b50\u548c\u6587\u6863\u7ea7\u522b\u3002\u8bc4\u4f30\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u7684\u7b97\u672f\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u5b8c\u6574\u6587\u6863\u63a8\u7406\u548c\u67d0\u4e9b\u8bed\u4e49\u8bdd\u8bed\u73b0\u8c61\u4e0a\u4ecd\u6709\u56f0\u96be\u3002", "motivation": "\u968f\u7740\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u8bdd\u8bed\u5c42\u9762\u77e5\u8bc6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4f20\u7edf\u4efb\u52a1\u548c\u65b0\u578b\u6311\u6218\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u73b0\u4ee3LLMs\u7684\u8bdd\u8bed\u7406\u89e3\u80fd\u529b\u3002", "method": "\u6784\u5efaBeDiscovER\u57fa\u51c6\u5957\u4ef6\uff0c\u6574\u54085\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u8bdd\u8bed\u4efb\u52a1\uff08\u5305\u62ec\u8bdd\u8bed\u8bcd\u5178\u3001\u53e5\u5b50\u7ea7\u548c\u6587\u6863\u7ea7\u4efb\u52a1\uff09\uff0c\u517152\u4e2a\u6570\u636e\u96c6\u3002\u8bc4\u4f30\u5f00\u6e90LLMs\uff08Qwen3\u7cfb\u5217\u3001DeepSeek-R1\uff09\u548c\u524d\u6cbf\u6a21\u578b\uff08GPT-5-mini\uff09\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u7684\u7b97\u672f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b8c\u6574\u6587\u6863\u63a8\u7406\u548c\u67d0\u4e9b\u7ec6\u5fae\u8bed\u4e49\u8bdd\u8bed\u73b0\u8c61\uff08\u5982\u4fee\u8f9e\u5173\u7cfb\u8bc6\u522b\uff09\u4e0a\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u73b0\u4ee3LLMs\u5728\u8bdd\u8bed\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u590d\u6742\u6587\u6863\u63a8\u7406\u548c\u7ec6\u5fae\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\uff0cBeDiscovER\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u8bdd\u8bed\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.13107", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13107", "abs": "https://arxiv.org/abs/2511.13107", "authors": ["Zhichao He", "Mouxiao Bian", "Jianhong Zhu", "Jiayuan Chen", "Yunqiu Wang", "Wenxia Zhao", "Tianbin Li", "Bing Han", "Jie Xu", "Junyan Wu"], "title": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study", "comment": null, "summary": "The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc6\u522b\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u5bf9CONSORT 2010\u58f0\u660e\u4f9d\u4ece\u6027\u7684\u51c6\u786e\u6027\u3002\u7ed3\u679c\u663e\u793a\u6a21\u578b\u6574\u4f53\u8868\u73b0\u4e00\u822c\uff0c\u80fd\u51c6\u786e\u8bc6\u522b\u5408\u89c4\u9879\u76ee\uff0c\u4f46\u5728\u8bc6\u522b\u4e0d\u5408\u89c4\u548c\u4e0d\u9002\u7528\u9879\u76ee\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u76ee\u524d\u5c1a\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u8bd5\u9a8c\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u624b\u52a8\u9a8c\u8bc1CONSORT\u4f9d\u4ece\u6027\u662f\u4e00\u4e2a\u8017\u65f6\u8d39\u529b\u7684\u8fc7\u7a0b\uff0c\u6210\u4e3a\u540c\u884c\u8bc4\u5ba1\u548c\u8bc1\u636e\u5408\u6210\u7684\u91cd\u8981\u74f6\u9888\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u5f53\u4ee3LLMs\u5728\u8bc6\u522b\u5df2\u53d1\u8868RCTs\u5bf9CONSORT 2010\u58f0\u660e\u4f9d\u4ece\u6027\u65b9\u9762\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b150\u7bc7\u5df2\u53d1\u8868RCTs\u7684\u91d1\u6807\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u533b\u5b66\u4e13\u4e1a\u3002\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30LLMs\u6027\u80fd\uff0c\u4e3b\u8981\u7ed3\u679c\u662f\u4e09\u7c7b\u5206\u7c7b\u4efb\u52a1\u7684\u5b8f\u5e73\u5747F1\u5206\u6570\uff0c\u8f85\u4ee5\u9879\u76ee\u7ea7\u6027\u80fd\u6307\u6807\u548c\u5b9a\u6027\u9519\u8bef\u5206\u6790\u3002", "result": "\u6574\u4f53\u6a21\u578b\u8868\u73b0\u4e00\u822c\u3002\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578bGemini-2.5-Flash\u548cDeepSeek-R1\u5206\u522b\u83b7\u5f970.634\u548c0.282\u7684\u5b8fF1\u5206\u6570\u548cCohen's Kappa\u7cfb\u6570\uff0c\u4ec5\u4e0e\u4e13\u5bb6\u5171\u8bc6\u8fbe\u6210\u4e00\u822c\u4e00\u81f4\u6027\u3002\u6a21\u578b\u5728\u8bc6\u522b\u5408\u89c4\u9879\u76ee\u65f6\u51c6\u786e\u7387\u9ad8\uff08F1\u5206\u6570>0.850\uff09\uff0c\u4f46\u5728\u8bc6\u522b\u4e0d\u5408\u89c4\u548c\u4e0d\u9002\u7528\u9879\u76ee\u65f6\u8868\u73b0\u5dee\uff08F1\u5206\u6570\u5f88\u5c11\u8d85\u8fc70.400\uff09\u3002GPT-4o\u7b49\u77e5\u540d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u5b8fF1\u5206\u6570\u4ec5\u4e3a0.521\u3002", "conclusion": "LLMs\u4f5c\u4e3aCONSORT\u68c0\u67e5\u7684\u521d\u6b65\u7b5b\u9009\u52a9\u624b\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u62a5\u544a\u826f\u597d\u7684\u9879\u76ee\u3002\u7136\u800c\uff0c\u76ee\u524d\u5b83\u4eec\u65e0\u6cd5\u53ef\u9760\u5730\u68c0\u6d4b\u62a5\u544a\u9057\u6f0f\u6216\u65b9\u6cd5\u5b66\u7f3a\u9677\uff0c\u56e0\u6b64\u4e0d\u9002\u5408\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u8bd5\u9a8c\u8d28\u91cf\u7684\u5173\u952e\u8bc4\u4f30\u3002"}}
{"id": "2511.13118", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13118", "abs": "https://arxiv.org/abs/2511.13118", "authors": ["Quanjiang Guo", "Sijie Wang", "Jinchuan Zhang", "Ben Zhang", "Zhao Kang", "Ling Tian", "Ke Yan"], "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction", "comment": "11 pages, 5 figures, accepted by AAAI 2026 (Oral)", "summary": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.", "AI": {"tldr": "Agent-Event-Coder (AEC) \u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u96f6\u6837\u672c\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u7c7b\u4f3c\u8f6f\u4ef6\u5de5\u7a0b\u7684\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u5b8c\u6574\u7684\u62bd\u53d6\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u4e8b\u4ef6\u62bd\u53d6\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff1a\u76f4\u63a5\u63d0\u793a\u5f80\u5f80\u4ea7\u751f\u4e0d\u5b8c\u6574\u6216\u7ed3\u6784\u65e0\u6548\u7684\u8f93\u51fa\uff0c\u5982\u9519\u8bef\u5206\u7c7b\u7684\u89e6\u53d1\u5668\u3001\u7f3a\u5931\u53c2\u6570\u548c\u6a21\u5f0f\u8fdd\u89c4\u3002", "method": "\u5c06\u4e8b\u4ef6\u62bd\u53d6\u5206\u89e3\u4e3a\u68c0\u7d22\u3001\u89c4\u5212\u3001\u7f16\u7801\u548c\u9a8c\u8bc1\u56db\u4e2a\u4e13\u95e8\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u7531\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u5904\u7406\u3002\u4e8b\u4ef6\u6a21\u5f0f\u8868\u793a\u4e3a\u53ef\u6267\u884c\u7684\u7c7b\u5b9a\u4e49\uff0c\u901a\u8fc7\u9a8c\u8bc1\u667a\u80fd\u4f53\u5b9e\u73b0\u786e\u5b9a\u6027\u9a8c\u8bc1\u548c\u7cbe\u786e\u53cd\u9988\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\u548c\u516d\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAEC\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4ee3\u7801\u751f\u6210\u7684\u65b9\u6cd5\u80fd\u591f\u4f7fLLM\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4ea7\u751f\u7cbe\u786e\u3001\u5b8c\u6574\u4e14\u6a21\u5f0f\u4e00\u81f4\u7684\u62bd\u53d6\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u7f16\u7a0b\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13126", "abs": "https://arxiv.org/abs/2511.13126", "authors": ["Nigar Alishzade", "Gulchin Abdullayeva"], "title": "A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition", "comment": null, "summary": "This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.", "AI": {"tldr": "\u6bd4\u8f83\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u673a\u5236\u5728\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u57fa\u4e8e\u6ce8\u610f\u529b\u7684Transformer\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8eConvLSTM\uff0c\u4f46ConvLSTM\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u66f4\u6709\u4f18\u52bf\u3002", "motivation": "\u7cfb\u7edf\u6bd4\u8f83\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u673a\u5236\u5728\u624b\u8bed\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5728\u963f\u585e\u62dc\u7586\u624b\u8bed\u6570\u636e\u96c6(AzSLD)\u548c\u7f8e\u56fd\u624b\u8bed\u6570\u636e\u96c6(WLASL)\u4e0a\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86ConvLSTM\u548cVanilla Transformer\u4e24\u79cd\u4ee3\u8868\u6027\u6a21\u578b\u3002", "result": "Vanilla Transformer\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8eConvLSTM\uff0c\u5728AzSLD\u4e0a\u8fbe\u523076.8%\u7684Top-1\u51c6\u786e\u7387\uff0c\u5728WLASL\u4e0a\u8fbe\u523088.3%\u3002ConvLSTM\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u4f46\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "Transformer\u5728\u6574\u4f53\u51c6\u786e\u7387\u548c\u8bf4\u8bdd\u8005\u72ec\u7acb\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800cConvLSTM\u5728\u8ba1\u7b97\u6548\u7387\u548c\u65f6\u5e8f\u5efa\u6a21\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u5e94\u6839\u636e\u5e94\u7528\u9700\u6c42\u548c\u8d44\u6e90\u7ea6\u675f\u9009\u62e9\u5408\u9002\u7684\u67b6\u6784\u3002"}}
{"id": "2511.13152", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13152", "abs": "https://arxiv.org/abs/2511.13152", "authors": ["Sourya Dipta Das", "Shubham Kumar", "Kuldeep Yadav"], "title": "Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels", "comment": "Accepted in AACL-IJCNLP 2025", "summary": "Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u8bed\u6cd5\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u8bad\u7ec3\u57fa\u4e8etransformer\u7684\u8bed\u6cd5\u8bc4\u5206\u6a21\u578b", "motivation": "\u53e3\u8bed\u8bed\u6cd5\u8bc4\u4f30\u9762\u4e34\u81ea\u53d1\u3001\u975e\u7ed3\u6784\u5316\u548c\u4e0d\u6d41\u7545\u7684\u6311\u6218\uff0c\u4e14\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6807\u6ce8\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u521b\u5efa\u6570\u636e", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bed\u6cd5\u80fd\u529b\u91cf\u8868\u7684\u63d0\u793a\u8bcd\u8ba9LLM\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u751f\u6210\u9884\u6d4b\u4f5c\u4e3a\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u8bad\u7ec3\u6846\u67b6\u5904\u7406\u6807\u7b7e\u566a\u58f0\u6765\u8bad\u7ec3transformer\u6a21\u578b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u8bed\u6cd5\u80fd\u529b\u5206\u6570\uff0cLLM\u9009\u62e9\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u8bad\u7ec3\u4e2d\u5e72\u51c0\u4e0e\u566a\u58f0\u6837\u672c\u6bd4\u4f8b\u5f71\u54cd\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u3001\u4f4e\u8d44\u6e90\u7684\u8bed\u6cd5\u8bc4\u4f30\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2511.13159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13159", "abs": "https://arxiv.org/abs/2511.13159", "authors": ["Zaara Zabeen Arpa", "Sadnam Sakib Apurbo", "Nazia Karim Khan Oishee", "Ajwad Abrar"], "title": "Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis", "comment": null, "summary": "Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bedASR\u8f6c\u5f55\u672c\u4e2d\u91cd\u590d\u6027\u53e3\u5403\u4e0e\u5f62\u6001\u91cd\u53e0\u7684\u533a\u5206\u6570\u636e\u96c6\uff0c\u901a\u8fc7LLM\u548c\u5fae\u8c03\u65b9\u6cd5\u5efa\u7acb\u4e86\u5f3a\u57fa\u7ebf\uff0c\u6700\u4f73\u6a21\u578b\u8fbe\u523084.78%\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bedASR\u8f6c\u5f55\u672c\u4e2d\u8bcd\u91cd\u590d\u7684\u6b67\u4e49\u95ee\u9898\uff1a\u533a\u5206\u91cd\u590d\u6027\u53e3\u5403\uff08ASR\u9519\u8bef/\u72b9\u8c6b\uff09\u548c\u5f62\u6001\u91cd\u53e0\uff08\u8bed\u6cd5\u6784\u9020\uff09\uff0c\u907f\u514d\u6807\u51c6\u53e3\u5403\u4fee\u6b63\u65b9\u6cd5\u8bef\u5220\u6709\u6548\u8bed\u8a00\u4fe1\u606f\u3002", "method": "\u6784\u5efa\u4e86\u9996\u4e2a20,000\u884c\u624b\u52a8\u6807\u6ce8\u7684\u5b5f\u52a0\u62c9\u8bed\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u63d0\u793a\uff1b2\uff09\u4efb\u52a1\u7279\u5b9a\u7684\u7f16\u7801\u5668\u6a21\u578b\u5fae\u8c03\u3002", "result": "LLM\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u8fbe\u523082.68%\u51c6\u786e\u7387\uff1b\u5fae\u8c03\u65b9\u6cd5\u66f4\u4f18\uff0cBanglaBERT\u6a21\u578b\u8fbe\u5230\u6700\u9ad884.78%\u51c6\u786e\u7387\u548c0.677 F1\u5206\u6570\u3002", "conclusion": "\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u8bed\u8a00\u611f\u77e5\u57fa\u7ebf\uff0c\u4e3a\u5f00\u53d1\u8bed\u4e49\u4fdd\u7559\u7684\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u89c4\u8303\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2511.13169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13169", "abs": "https://arxiv.org/abs/2511.13169", "authors": ["Tianai Huang", "Jiayuan Chen", "Lu Lu", "Pengcheng Chen", "Tianbin Li", "Bing Han", "Wenchao Tang", "Jie Xu", "Ming Li"], "title": "TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine", "comment": "17 pages, 8 figures", "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\\_r1 and gemini\\_2\\_5\\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the \"In-depth Challenge for Comprehensive TCM Abilities\" special track.", "AI": {"tldr": "TCM-5CEval\u662f\u4e00\u4e2a\u66f4\u7ec6\u7c92\u5ea6\u7684\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u6db5\u76d65\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u6838\u5fc3\u77e5\u8bc6\u3001\u7ecf\u5178\u6587\u732e\u3001\u4e34\u5e8a\u51b3\u7b56\u3001\u4e2d\u836f\u5b66\u548c\u4e34\u5e8a\u975e\u836f\u7269\u6cbb\u7597\u3002\u8bc4\u4f30\u53d1\u73b0\u6a21\u578b\u5728\u57fa\u7840\u77e5\u8bc6\u56de\u5fc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ecf\u5178\u6587\u672c\u89e3\u91ca\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5f31\u70b9\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5728\u9ad8\u5ea6\u4e13\u4e1a\u5316\u548c\u6587\u5316\u4e30\u5bcc\u7684\u4e2d\u533b\u9886\u57df\u4e0d\u591f\u5168\u9762\uff0c\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\u5de5\u5177\u6765\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u533b\u9886\u57df\u7684\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u6784\u5efaTCM-5CEval\u57fa\u51c6\uff0c\u5305\u542b5\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\uff1aTCM-Exam\uff08\u6838\u5fc3\u77e5\u8bc6\uff09\u3001TCM-LitQA\uff08\u7ecf\u5178\u6587\u732e\uff09\u3001TCM-MRCD\uff08\u4e34\u5e8a\u51b3\u7b56\uff09\u3001TCM-CMM\uff08\u4e2d\u836f\u5b66\uff09\u3001TCM-ClinNPT\uff08\u4e34\u5e8a\u975e\u836f\u7269\u6cbb\u7597\uff09\u3002\u5bf915\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u6392\u5217\u4e00\u81f4\u6027\u6d4b\u8bd5\u68c0\u6d4b\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u5728\u57fa\u7840\u77e5\u8bc6\u56de\u5fc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ecf\u5178\u6587\u672c\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u6392\u5217\u4e00\u81f4\u6027\u6d4b\u8bd5\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\u654f\u611f\u6027\uff0c\u63a8\u7406\u7a33\u5b9a\u6027\u666e\u904d\u4e0d\u8db3\u3002deepseek_r1\u548cgemini_2_5_pro\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "TCM-5CEval\u4e3a\u4e2d\u533b\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u8be6\u7ec6\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u63a8\u7406\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u6839\u672c\u5f31\u70b9\u3002\u8be5\u57fa\u51c6\u5df2\u4e0a\u4f20\u81f3Medbench\u5e73\u53f0\uff0c\u4fc3\u8fdb\u6807\u51c6\u5316\u6bd4\u8f83\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.13180", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13180", "abs": "https://arxiv.org/abs/2511.13180", "authors": ["Ronit D. Gross", "Yanir Harel", "Ido Kanter"], "title": "Translation Entropy: A Statistical Framework for Evaluating Translation Systems", "comment": "23 pages, 6 figures and 8 tables", "summary": "The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u4f30\u8ba1\u7ffb\u8bd1\u71b5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5728\u4fdd\u6301\u7ffb\u8bd1\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u66ff\u6362\u53e5\u5b50\u4e2d\u5355\u4e2a\u6807\u8bb0\u7684\u6982\u7387\uff0c\u6765\u8bc4\u4f30\u7ffb\u8bd1\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u4fe1\u606f\u65f6\u4ee3\uff0c\u7ffb\u8bd1\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u7f3a\u4e4f\u91cf\u5316\u8bc4\u4f30\u7ffb\u8bd1\u5668\u6027\u80fd\u7684\u5ba2\u89c2\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5373\u4f7f\u662f\u5355\u4e00\u8bed\u8a00\u7684\u71b5\u4e5f\u672a\u77e5\u3002", "method": "\u7ed9\u5b9a\u7ffb\u8bd1\u5668\uff0c\u901a\u8fc7\u66ff\u6362\u67a2\u8f74\u53e5\u4e2d\u9009\u5b9a\u6807\u8bb0\u751f\u6210\u591a\u4e2a\u53e5\u5b50\uff0c\u7edf\u8ba1\u7ffb\u8bd1\u4fdd\u6301\u4e0d\u53d8\u7684\u6982\u7387\uff0c\u8ba1\u7b97\u5355\u4e2a\u6807\u8bb0\u7684\u71b5\uff0c\u5e76\u5e73\u5747\u6240\u6709\u67a2\u8f74\u6807\u8bb0\u5f97\u5230\u6574\u4f53\u7ffb\u8bd1\u71b5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u91cf\u5316\u6392\u540d\u591a\u4e2a\u516c\u5f00\u7ffb\u8bd1\u5668\uff0c\u63ed\u793a\u4e92\u7ffb\u8bd1\u71b5\u7684\u5bf9\u79f0\u6027\uff0c\u5e76\u53d1\u73b0\u66ff\u6362\u4e24\u4e2a\u6807\u8bb0\u65f6\u7ffb\u8bd1\u9000\u5316\u5ea6\u4e0e\u4e24\u4e2a\u6807\u8bb0\u9000\u5316\u5ea6\u7684\u4e58\u79ef\u6210\u6b63\u6bd4\u3002", "conclusion": "\u7ffb\u8bd1\u71b5\u662f\u53ef\u6d4b\u91cf\u7684\u5c5e\u6027\uff0c\u4e3a\u4eba\u5de5\u7ffb\u8bd1\u5668\u63d0\u4f9b\u4e86\u5ba2\u89c2\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u57fa\u4e8eMarianMT\u3001T5-Base\u548cNLLB-200\u7ffb\u8bd1\u5668\u7684\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13182", "abs": "https://arxiv.org/abs/2511.13182", "authors": ["Mihai Dan Nadas", "Laura Diosan"], "title": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study", "comment": null, "summary": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u7b49\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u800cLlama\u7cfb\u5217\u6a21\u578b\u8868\u73b0\u6ce2\u52a8\u8f83\u5927\u3002", "motivation": "\u81ea\u52a8\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u5bf9\u4e8e\u5904\u7406\u50cf\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u8fd9\u6837\u5177\u6709\u4e30\u5bcc\u53d8\u97f3\u7b26\u53f7\u7684\u8bed\u8a00\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540cLLM\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u7efc\u5408\u8bed\u6599\u5e93\u6d4b\u8bd5\u4e86\u5305\u62ecGPT-3.5\u3001GPT-4\u3001GPT-4o\u3001Gemini\u3001Llama\u7cfb\u5217\u7b49\u5728\u5185\u7684\u591a\u79cdLLM\uff0c\u91c7\u7528\u4ece\u96f6\u6837\u672c\u5230\u590d\u6742\u591a\u6837\u672c\u7684\u591a\u79cd\u63d0\u793a\u6a21\u677f\u3002", "result": "GPT-4o\u7b49\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u53d8\u97f3\u7b26\u53f7\u6062\u590d\uff0c\u59cb\u7ec8\u8d85\u8fc7\u4e2d\u6027\u56de\u663e\u57fa\u7ebf\uff0c\u800cLlama\u7cfb\u5217\u7b49\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5927\u7684\u53d8\u5f02\u6027\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u548c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e3a\u6539\u8fdb\u9762\u5411\u53d8\u97f3\u7b26\u53f7\u4e30\u5bcc\u8bed\u8a00\u7684NLP\u5de5\u5177\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.13225", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13225", "abs": "https://arxiv.org/abs/2511.13225", "authors": ["Tyler Loakman", "Joseph James", "Chenghua Lin"], "title": "Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms", "comment": "Accepted to IJCNLP-AACL 2025", "summary": "With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u51c6\u786e\u89e3\u8bfb\u8bed\u97f3\u7684\u9891\u8c31\u56fe\u548c\u6ce2\u5f62\u56fe\uff0c\u8868\u660e\u9700\u8981\u7279\u5b9a\u7684\u53c2\u6570\u77e5\u8bc6\u800c\u4e0d\u4ec5\u4ec5\u662f\u914d\u5bf9\u6837\u672c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u8fd9\u4e9b\u6a21\u578b\u5728\u878d\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u80fd\u5426\u50cf\u4e13\u4e1a\u8bed\u97f3\u5b66\u5bb6\u4e00\u6837\u89e3\u8bfb\u8bed\u97f3\u7684\u9891\u8c31\u56fe\u548c\u6ce2\u5f62\u56fe\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b4000+\u82f1\u8bed\u5355\u8bcd\u7684\u65b0\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u9879\u9009\u62e9\u4efb\u52a1\u6d4b\u8bd5VLMs\u4ece\u8bed\u97f3\u8868\u793a\u4e2d\u9884\u6d4b\u6b63\u786e\u97f3\u7d20\u6216\u5b57\u7d20\u8f6c\u5f55\u7684\u80fd\u529b\uff0c\u4f7f\u7528\u57fa\u4e8e\u97f3\u7d20\u7f16\u8f91\u8ddd\u79bb\u7684\u5e72\u6270\u9879\u3002", "result": "\u65e0\u8bba\u662f\u96f6\u6837\u672c\u8fd8\u662f\u5fae\u8c03\u540e\u7684\u6a21\u578b\uff0c\u5176\u8868\u73b0\u90fd\u5f88\u5c11\u8d85\u8fc7\u968f\u673a\u6c34\u5e73\uff0c\u8868\u660e\u6a21\u578b\u7f3a\u4e4f\u89e3\u8bfb\u6b64\u7c7b\u56fe\u5f62\u7684\u7279\u5b9a\u53c2\u6570\u77e5\u8bc6\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u4e13\u95e8\u7684\u53c2\u6570\u77e5\u8bc6\u6765\u6b63\u786e\u89e3\u8bfb\u8bed\u97f3\u7684\u9891\u8c31\u56fe\u548c\u6ce2\u5f62\u56fe\uff0c\u4ec5\u9760\u914d\u5bf9\u6837\u672c\u8bad\u7ec3\u662f\u4e0d\u591f\u7684\u3002"}}
{"id": "2511.13254", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13254", "abs": "https://arxiv.org/abs/2511.13254", "authors": ["Shalini Maiti", "Amar Budhiraja", "Bhavul Gauri", "Gaurav Chaurasia", "Anton Protopopov", "Alexis Audran-Reiss", "Michael Slater", "Despoina Magka", "Tatiana Shavrina", "Roberta Raileanu", "Yoram Bachrach"], "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "AI": {"tldr": "SoCE\u662f\u4e00\u79cd\u57fa\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u7ec4\u5408\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5404\u5f31\u76f8\u5173\u7c7b\u522b\u4e2d\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u975e\u5747\u5300\u52a0\u6743\u5e73\u5747\u800c\u975e\u5747\u5300\u6743\u91cd\u6765\u6700\u5927\u5316\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8d44\u6e90\u5bc6\u96c6\u4e14\u8017\u65f6\uff0c\u6a21\u578b\u878d\u5408\u4f5c\u4e3a\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u6280\u672f\u53ef\u4ee5\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5229\u7528\u57fa\u51c6\u7c7b\u522b\u95f4\u6a21\u578b\u6027\u80fd\u4f4e\u76f8\u5173\u6027\u7684\u89c2\u5bdf\uff0c\u8bc6\u522b\u6bcf\u4e2a\u5f31\u76f8\u5173\u7c7b\u522b\u7c07\u4e2d\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7684\u52a0\u6743\u5e73\u5747\u800c\u975e\u5747\u5300\u6743\u91cd\u8fdb\u884c\u7ec4\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\uff08\u5305\u62ec\u591a\u8bed\u8a00\u80fd\u529b\u3001\u5de5\u5177\u8c03\u7528\u548c\u6570\u5b66\uff09\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4f2f\u514b\u5229\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "SoCE\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u57fa\u51c6\u7c7b\u522b\u95f4\u7684\u4f4e\u76f8\u5173\u6027\u6765\u8bc6\u522b\u548c\u4f18\u5316\u7ec4\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.13329", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.13329", "abs": "https://arxiv.org/abs/2511.13329", "authors": ["Shufan Yang", "Zifeng Cheng", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Shiping Ge", "Yuchen Fu", "Qing Gu"], "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection", "comment": "AAAI 2026", "summary": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.", "AI": {"tldr": "RegionMarker\u662f\u4e00\u79cd\u533a\u57df\u89e6\u53d1\u8bed\u4e49\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u89e6\u53d1\u533a\u57df\u5e76\u5411\u76f8\u5173\u6587\u672c\u5d4c\u5165\u4e2d\u6ce8\u5165\u6c34\u5370\uff0c\u4e3aEmbedding-as-a-Service\u63d0\u4f9b\u5168\u9762\u7684\u7248\u6743\u4fdd\u62a4\u3002", "motivation": "\u73b0\u6709\u7684EaaS\u6c34\u5370\u65b9\u6cd5\u53ea\u80fd\u62b5\u6297\u90e8\u5206\u653b\u51fb\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5168\u9762\u4fdd\u62a4\uff0c\u5b58\u5728\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u5bfc\u81f4\u7ecf\u6d4e\u635f\u5931\u7684\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u79d8\u5bc6\u964d\u7ef4\u77e9\u9635\u6295\u5f71\u5230\u5b50\u7a7a\u95f4\uff0c\u968f\u673a\u9009\u62e9\u89e6\u53d1\u533a\u57df\uff0c\u5728\u6574\u4e2a\u89e6\u53d1\u533a\u57df\u5d4c\u5165\u6c34\u5370\uff0c\u5e76\u5c06\u6587\u672c\u5d4c\u5165\u672c\u8eab\u4f5c\u4e3a\u6c34\u5370\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRegionMarker\u80fd\u6709\u6548\u62b5\u6297\u4e0d\u540c\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5305\u62ec\u6c34\u5370\u79fb\u9664\u3001\u8f6c\u8ff0\u548c\u7ef4\u5ea6\u6270\u52a8\u653b\u51fb\u3002", "conclusion": "RegionMarker\u6846\u67b6\u80fd\u591f\u5168\u9762\u4fdd\u62a4EaaS\u7684\u7248\u6743\uff0c\u62b5\u6297\u591a\u79cd\u653b\u51fb\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u5546\u63d0\u4f9b\u6709\u6548\u7684\u7248\u6743\u4fdd\u62a4\u65b9\u6848\u3002"}}
{"id": "2511.13335", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13335", "abs": "https://arxiv.org/abs/2511.13335", "authors": ["Maram Alharbi", "Salmane Chafik", "Saad Ezzini", "Ruslan Mitkov", "Tharindu Ranasinghe", "Hansi Hettiarachchi"], "title": "AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects", "comment": null, "summary": "The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u963f\u62c9\u4f2f\u9152\u5e97\u8bc4\u8bba\u60c5\u611f\u5206\u6790\u5171\u4eab\u4efb\u52a1\uff0c\u4f7f\u7528\u5305\u542b\u6c99\u7279\u548c\u6469\u6d1b\u54e5\u65b9\u8a00\u7684\u591a\u65b9\u8a00\u6570\u636e\u96c6\uff0c\u6700\u4f73\u7cfb\u7edfF1\u5206\u6570\u8fbe0.81\u3002", "motivation": "\u963f\u62c9\u4f2f\u4e16\u754c\u9152\u5e97\u4e1a\u65e5\u76ca\u4f9d\u8d56\u5ba2\u6237\u53cd\u9988\u6765\u6539\u8fdb\u670d\u52a1\uff0c\u9700\u8981\u5148\u8fdb\u7684\u963f\u62c9\u4f2f\u8bed\u60c5\u611f\u5206\u6790\u5de5\u5177\uff0c\u7279\u522b\u662f\u9488\u5bf9\u65b9\u8a00\u7684\u5206\u6790\u80fd\u529b\u3002", "method": "\u521b\u5efa\u591a\u65b9\u8a00\u6570\u636e\u96c6\uff0c\u5c06\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7684\u9152\u5e97\u8bc4\u8bba\u7ffb\u8bd1\u6210\u6c99\u7279\u548c\u6469\u6d1b\u54e5\u65b9\u8a00\uff0c\u7531\u6bcd\u8bed\u8005\u9a8c\u8bc1\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u60c5\u611f\u4fdd\u6301\u3002\u6570\u636e\u96c6\u5305\u542b538\u6761\u5e73\u8861\u7684\u60c5\u611f\u8bc4\u8bba\u3002", "result": "\u8d85\u8fc740\u4e2a\u56e2\u961f\u6ce8\u518c\u53c2\u4e0e\uff0c12\u4e2a\u56e2\u961f\u63d0\u4ea4\u7cfb\u7edf\u3002\u6700\u4f73\u7cfb\u7edfF1\u5206\u6570\u8fbe\u52300.81\uff0c\u8bc1\u660e\u4e86\u8de8\u963f\u62c9\u4f2f\u65b9\u8a00\u60c5\u611f\u5206\u6790\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u8d44\u6e90\u652f\u6301\u5f00\u53d1\u65b9\u8a00\u611f\u77e5\u7684NLP\u7cfb\u7edf\uff0c\u7528\u4e8e\u5ba2\u6237\u4f53\u9a8c\u5206\u6790\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u540c\u65f6\u663e\u793a\u4e86\u8de8\u963f\u62c9\u4f2f\u65b9\u8a00\u60c5\u611f\u5206\u6790\u7684\u6301\u7eed\u6311\u6218\u3002"}}
{"id": "2511.13368", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13368", "abs": "https://arxiv.org/abs/2511.13368", "authors": ["Kajetan Dymkiewicz", "Ivan Vulic", "Helen Yannakoudakis", "Eilam Shapira", "Roi Reichart", "Anna Korhonen"], "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7PEFT/LoRA\u7814\u7a76\u63ed\u793a\u4e86LLM\u8de8\u4efb\u52a1\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u6a21\u5f0f\uff1a\u540c\u4efb\u52a1\u8de8\u8bed\u8a00\u8fc1\u79fb\u53ef\u9760\u4e3a\u6b63\uff0c\u800c\u5f02\u4efb\u52a1\u8fc1\u79fb\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b\u540c\u65f6\u53d1\u73b0\u4e86\u7a33\u5b9a\u7684\u6350\u8d60\u8005-\u63a5\u6536\u8005\u7ed3\u6784\u3002", "motivation": "\u7406\u89e3LLM\u5728\u4e00\u4e2a\u4efb\u52a1\u6216\u8bed\u8a00\u4e0a\u7684\u6539\u8fdb\u5982\u4f55\u5f71\u54cd\u5176\u4ed6\u4efb\u52a1\u548c\u8bed\u8a00\u53ca\u5176\u7ec4\u5408\uff0c\u76ee\u524d\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5728\u591a\u4e2a\u5f00\u6e90LLM\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u8fdb\u884c\u53d7\u63a7PEFT/LoRA\u7814\u7a76\uff0c\u5c06\u4efb\u52a1\u548c\u8bed\u8a00\u4f5c\u4e3a\u8fc1\u79fb\u8f74\uff0c\u6bcf\u4e2a\u6a21\u578b\u5728\u5355\u4e00\u4efb\u52a1-\u8bed\u8a00\u6e90\u4e0a\u5fae\u8c03\uff0c\u6d4b\u91cf\u5728\u6240\u6709\u5176\u4ed6\u4efb\u52a1-\u8bed\u8a00\u76ee\u6807\u5bf9\u4e0a\u7684\u8fc1\u79fb\u6548\u679c\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e00\u81f4\u6a21\u5f0f\uff1a1\uff09\u540c\u4efb\u52a1\u8de8\u8bed\u8a00\u8fc1\u79fb\u53ef\u9760\u4e3a\u6b63\uff0c\u800c\u5f02\u4efb\u52a1\u8fc1\u79fb\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b2\uff09\u8de8\u8bed\u8a00\u548c\u4efb\u52a1\u5b58\u5728\u7a33\u5b9a\u7684\u6350\u8d60\u8005-\u63a5\u6536\u8005\u7ed3\u6784\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u98ce\u9669\u611f\u77e5\u5fae\u8c03\u548c\u6a21\u578b\u4e13\u4e1a\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.13381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13381", "abs": "https://arxiv.org/abs/2511.13381", "authors": ["Siyu Zhu", "Mouxiao Bian", "Yue Xie", "Yongyu Tang", "Zhikang Yu", "Tianbin Li", "Pengcheng Chen", "Bing Han", "Jie Xu", "Xiaoyan Dong"], "title": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts", "comment": null, "summary": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.", "AI": {"tldr": "PEDIASBench\u8bc4\u4f30\u6846\u67b6\u663e\u793a\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u513f\u79d1\u533b\u5b66\u4e2d\u57fa\u7840\u77e5\u8bc6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u3001\u52a8\u6001\u8bca\u7597\u51b3\u7b56\u548c\u4eba\u6587\u5173\u6000\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5c1a\u4e0d\u80fd\u72ec\u7acb\u627f\u62c5\u513f\u79d1\u8bca\u7597\u5de5\u4f5c\uff0c\u4f46\u5728\u51b3\u7b56\u652f\u6301\u7b49\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u662f\u5426\u80fd\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u4f5c\u4e3a\u5408\u683c\u7684\u513f\u79d1\u533b\u751f\u53d1\u6325\u4f5c\u7528\u3002", "method": "\u5f00\u53d1\u4e86PEDIASBench\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u57fa\u7840\u77e5\u8bc6\u5e94\u7528\u3001\u52a8\u6001\u8bca\u7597\u80fd\u529b\u3001\u513f\u79d1\u533b\u7597\u5b89\u5168\u4e0e\u4f26\u7406\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u8bc4\u4f3012\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u572819\u4e2a\u513f\u79d1\u4e9a\u4e13\u4e1a\u548c211\u79cd\u5178\u578b\u75be\u75c5\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5148\u8fdb\u6a21\u578b\u5728\u57fa\u7840\u77e5\u8bc6\u4e0a\u8868\u73b0\u826f\u597d\uff08Qwen3-235B-A22B\u5728\u6267\u7167\u7ea7\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u8d8590%\uff09\uff0c\u4f46\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u6027\u80fd\u4e0b\u964d\u7ea615%\uff1b\u5728\u52a8\u6001\u8bca\u7597\u573a\u666f\u4e2d\uff0cDeepSeek-R1\u5728\u75c5\u4f8b\u63a8\u7406\u4e2d\u5f97\u5206\u6700\u9ad8\uff08\u5e73\u57470.58\uff09\uff0c\u4f46\u591a\u6570\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u5b9e\u65f6\u60a3\u8005\u53d8\u5316\uff1b\u5728\u4f26\u7406\u5b89\u5168\u4efb\u52a1\u4e2d\uff0cQwen2.5-72B\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u738792.05%\uff09\uff0c\u4f46\u4eba\u6587\u654f\u611f\u6027\u4ecd\u6709\u9650\u3002", "conclusion": "\u5f53\u524d\u513f\u79d1\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u6709\u9650\u7684\u52a8\u6001\u51b3\u7b56\u80fd\u529b\u548c\u4e0d\u6210\u719f\u7684\u4eba\u6587\u5173\u6000\uff0c\u672a\u6765\u5e94\u5173\u6ce8\u591a\u6a21\u6001\u6574\u5408\u548c\u4e34\u5e8a\u53cd\u9988-\u6a21\u578b\u8fed\u4ee3\u5faa\u73af\uff0c\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u673a\u534f\u4f5c\u3002\u867d\u7136\u4e0d\u80fd\u72ec\u7acb\u627f\u62c5\u513f\u79d1\u8bca\u7597\uff0c\u4f46\u5728\u51b3\u7b56\u652f\u6301\u3001\u533b\u5b66\u6559\u80b2\u548c\u60a3\u8005\u6c9f\u901a\u65b9\u9762\u5177\u6709\u524d\u666f\u3002"}}
{"id": "2511.13410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13410", "abs": "https://arxiv.org/abs/2511.13410", "authors": ["Zhaopei Huang", "Qifeng Dai", "Guozheng Wu", "Xiaopeng Wu", "Kehan Chen", "Chuan Yu", "Xubin Li", "Tiezheng Ge", "Wenxuan Wang", "Qin Jin"], "title": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86PAL-Bench\u57fa\u51c6\u548cH\u00b2Memory\u8bb0\u5fc6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u9762\u5411\u670d\u52a1\u7684\u4e2a\u6027\u5316\u5bf9\u8bdd\u52a9\u624b\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4e2a\u4eba\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u9700\u8981\u80fd\u591f\u7406\u89e3\u7528\u6237\u7279\u5b9a\u7279\u5f81\u5e76\u5b9a\u5236\u54cd\u5e94\u7684\u4e2a\u6027\u5316\u5bf9\u8bdd\u52a9\u624b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u957f\u671f\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u6b65\u9aa4LLM\u5408\u6210\u7ba1\u9053\u521b\u5efaPAL-Set\u4e2d\u6587\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faH\u00b2Memory\u5c42\u6b21\u5f02\u6784\u8bb0\u5fc6\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u6539\u8fdb\u4e2a\u6027\u5316\u54cd\u5e94\u751f\u6210\u3002", "result": "\u5728PAL-Bench\u548c\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u8bb0\u5fc6\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PAL-Bench\u4e3a\u8bc4\u4f30\u670d\u52a1\u5bfc\u5411\u52a9\u624b\u7684\u4e2a\u6027\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0cH\u00b2Memory\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u7528\u6237-\u4ee3\u7406\u4ea4\u4e92\u4e2d\u7684\u4e2a\u6027\u5316\u670d\u52a1\u8868\u73b0\u3002"}}
{"id": "2511.13467", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13467", "abs": "https://arxiv.org/abs/2511.13467", "authors": ["Serge Gladkoff", "Lifeng Han", "Katerina Gasova"], "title": "Non-Linear Scoring Model for Translation Quality Evaluation", "comment": "ongoing work, 38 pages", "summary": "Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.\n  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.\n  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model\n  E(x) = a * ln(1 + b * x), a, b > 0,\n  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.\n  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u6570\u51fd\u6570\u7684\u975e\u7ebf\u6027\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ebf\u6027\u8bc4\u5206\u5728\u4e0d\u540c\u6587\u672c\u957f\u5ea6\u4e0b\u7684\u504f\u5dee\u95ee\u9898\uff0c\u4f7f\u8bc4\u4f30\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eMQM\u7684\u7ebf\u6027\u8bc4\u5206\u65b9\u6cd5\u5728\u4e0d\u540c\u957f\u5ea6\u6587\u672c\u6837\u672c\u4e0a\u5b58\u5728\u504f\u5dee\uff0c\u77ed\u6587\u672c\u88ab\u8fc7\u5ea6\u60e9\u7f5a\u800c\u957f\u6587\u672c\u88ab\u4f4e\u4f30\uff0c\u4e0e\u4e13\u5bb6\u76f4\u89c9\u4e0d\u4e00\u81f4\u3002", "method": "\u5f00\u53d1\u4e86\u53cc\u53c2\u6570\u5bf9\u6570\u6a21\u578bE(x) = a * ln(1 + b * x)\uff0c\u57fa\u4e8e\u97e6\u4f2f-\u8d39\u5e0c\u7eb3\u5b9a\u5f8b\u548c\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\uff0c\u901a\u8fc7\u4e00\u7ef4\u6839\u67e5\u627e\u6b65\u9aa4\u4ece\u4e24\u4e2a\u5bb9\u5fcd\u70b9\u8fdb\u884c\u6821\u51c6\u3002", "result": "\u5b9e\u8bc1\u6570\u636e\u663e\u793a\u53ef\u63a5\u53d7\u9519\u8bef\u6570\u91cf\u968f\u6837\u672c\u5927\u5c0f\u5448\u5bf9\u6570\u589e\u957f\u800c\u975e\u7ebf\u6027\u589e\u957f\uff0c\u8be5\u6a21\u578b\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u548c\u8bc4\u5206\u8005\u95f4\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u975e\u7ebf\u6027\u8bc4\u5206\u6a21\u578b\u4e3a\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u5e76\u4e3aAI\u9a71\u52a8\u7684\u6587\u6863\u7ea7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7840\u3002"}}
{"id": "2511.13481", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13481", "abs": "https://arxiv.org/abs/2511.13481", "authors": ["Attapol T. Rutherford", "Sirisak Chueykamhang", "Thachaparn Bunditlurdruk", "Nanthicha Angsuwichitkul"], "title": "Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns", "comment": null, "summary": "Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u9762\u60c5\u611f\u5206\u6790(ABSA)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u7801\u6cf0\u8bed\u8d22\u52a1\u5e74\u62a5\u4e2d\u7684\u6a21\u7cca\u60c5\u611f\uff0c\u5e76\u901a\u8fc7\u4e8b\u4ef6\u7814\u7a76\u9a8c\u8bc1\u4e86\u5206\u6790\u7ed3\u679c\u5bf9\u80a1\u7968\u4ef7\u683c\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "motivation": "\u8d22\u52a1\u6587\u6863\u4e2d\u7684\u60c5\u611f\u7406\u89e3\u5bf9\u6d1e\u5bdf\u5e02\u573a\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e9b\u62a5\u544a\u5e38\u4f7f\u7528\u6a21\u7cca\u8bed\u8a00\u6765\u5448\u73b0\u79ef\u6781\u6216\u4e2d\u6027\u524d\u666f\uff0c\u5373\u4f7f\u5b9e\u9645\u60c5\u51b5\u53ef\u80fd\u4e0d\u592a\u4e50\u89c2\u3002", "method": "\u5f00\u53d1\u4e86\u9488\u5bf9\u6cf0\u8bed\u8d22\u52a1\u5e74\u62a5\u6a21\u7cca\u60c5\u611f\u6807\u6ce8\u7684\u7279\u5b9a\u6307\u5357\uff0c\u6807\u6ce8\u4e86100\u591a\u4efd\u8d22\u52a1\u62a5\u544a\uff0c\u5e76\u5728\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u5bf9\u5404\u79cd\u6587\u672c\u5206\u7c7b\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u60c5\u611f\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e8b\u4ef6\u7814\u7a76\u8868\u660e\u5e02\u573a\u53cd\u5e94\u53d7\u5230\u62a5\u544a\u4e2d\u7279\u5b9a\u65b9\u9762\u7684\u9009\u62e9\u6027\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8d22\u52a1\u6587\u672c\u60c5\u611f\u5206\u6790\u7684\u590d\u6742\u6027\uff0c\u5e76\u51f8\u663e\u4e86\u89e3\u51b3\u6a21\u7cca\u8bed\u8a00\u4ee5\u51c6\u786e\u8bc4\u4f30\u5e02\u573a\u60c5\u7eea\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.13505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13505", "abs": "https://arxiv.org/abs/2511.13505", "authors": ["Elinor Poole-Dayan", "Daniel T Kessler", "Hannah Chiou", "Margaret Hughes", "Emily S Lin", "Marshall Ganz", "Deb Roy"], "title": "Applying Large Language Models to Characterize Public Narratives", "comment": null, "summary": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u516c\u5171\u53d9\u4e8b\u81ea\u52a8\u5316\u6807\u6ce8\u6846\u67b6\uff0c\u5728\u4e13\u5bb6\u6807\u6ce8\u57fa\u51c6\u4e0a\u8fbe\u5230\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u7684\u6027\u80fd\uff08\u5e73\u5747F1\u5206\u65700.80\uff09\uff0c\u5e76\u6269\u5c55\u5230\u653f\u6cbb\u6f14\u8bb2\u5206\u6790\u3002", "motivation": "\u516c\u5171\u53d9\u4e8b\u662f\u9886\u5bfc\u529b\u53d1\u5c55\u548c\u516c\u6c11\u52a8\u5458\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u7531\u4e8e\u4e3b\u89c2\u89e3\u91ca\u6027\u548c\u4e13\u5bb6\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u7cfb\u7edf\u5206\u6790\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u4f7f\u7528\u4e0e\u9886\u57df\u4e13\u5bb6\u5171\u540c\u5236\u5b9a\u7684\u7f16\u7801\u624b\u518c\uff0c\u8bc4\u4f30LLM\u5728\u516c\u5171\u53d9\u4e8b\u5b9a\u6027\u6807\u6ce8\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u57288\u4e2a\u53d9\u4e8b\u548c14\u4e2a\u4ee3\u7801\u4e0a\u8fbe\u5230\u5e73\u5747F1\u5206\u65700.80\uff0c\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\uff1b\u6269\u5c55\u5206\u6790\u523022\u4e2a\u6545\u4e8b\u548c\u4e00\u7ec4\u653f\u6cbb\u6f14\u8bb2\u3002", "conclusion": "\u5c55\u793a\u4e86LLM\u8f85\u52a9\u6807\u6ce8\u5728\u53ef\u6269\u5c55\u53d9\u4e8b\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u8ba1\u7b97\u516c\u6c11\u53d9\u4e8b\u7814\u7a76\u6307\u660e\u4e86\u5173\u952e\u5c40\u9650\u6027\u548c\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2511.13529", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.13529", "abs": "https://arxiv.org/abs/2511.13529", "authors": ["M\u00e1t\u00e9 Gedeon", "Piroska Zs\u00f3fia Barta", "P\u00e9ter Mihajlik", "Tekla Etelka Gr\u00e1czi", "Anna Koh\u00e1ri", "Katalin M\u00e1dy"], "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets", "comment": "Submitted to LREC 2026", "summary": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\\% on spontaneous and 4.8\\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\\% and 18.26\\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.", "AI": {"tldr": "\u4e3a\u586b\u8865\u5308\u7259\u5229\u8bed\u81ea\u53d1\u6027\u548c\u5bf9\u8bdd\u8bed\u97f3\u8bed\u6599\u5e93\u7684\u7a7a\u767d\uff0c\u7814\u7a76\u8005\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6BEA-Large\u548cBEA-Dialogue\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684ASR\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5bf9\u8bddASR\u7684\u6301\u7eed\u6311\u6218\u3002", "motivation": "\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684ASR\u53d1\u5c55\u53d7\u76ca\u4e8e\u5927\u91cf\u6570\u636e\u96c6\uff0c\u800c\u5308\u7259\u5229\u8bed\u7b49\u8bed\u8a00\u7531\u4e8e\u7f3a\u4e4f\u81ea\u53d1\u6027\u548c\u5bf9\u8bdd\u8bed\u6599\u5e93\u800c\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ece\u5308\u7259\u5229\u8bed\u97f3\u8bed\u6599\u5e93BEA\u4e2d\u6784\u5efa\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff1aBEA-Large\uff08255\u5c0f\u65f6\u81ea\u53d1\u8bed\u97f3\uff09\u548cBEA-Dialogue\uff0885\u5c0f\u65f6\u81ea\u7136\u5bf9\u8bdd\uff09\uff0c\u5e76\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u7684ASR\u6a21\u578b\u5efa\u7acb\u53ef\u590d\u73b0\u57fa\u7ebf\u3002", "result": "\u5fae\u8c03\u7684Fast Conformer\u6a21\u578b\u5728\u81ea\u53d1\u8bed\u97f3\u4e0a\u8bcd\u9519\u8bef\u7387\u4e3a14.18%\uff0c\u5728\u91cd\u590d\u8bed\u97f3\u4e0a\u4e3a4.8%\uff1b\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u9519\u8bef\u7387\u572813.05%-18.26%\u4e4b\u95f4\u3002", "conclusion": "\u5bf9\u8bddASR\u4ecd\u7136\u9762\u4e34\u56f0\u96be\uff0c\u7279\u522b\u662f\u7531\u4e8e\u4e0d\u6d41\u7545\u3001\u91cd\u53e0\u548c\u975e\u6b63\u5f0f\u8bed\u97f3\u6a21\u5f0f\u3002\u901a\u8fc7\u53d1\u5e03\u8fd9\u4e9b\u6570\u636e\u96c6\u548c\u57fa\u7ebf\uff0c\u65e8\u5728\u63a8\u8fdb\u5308\u7259\u5229\u8bed\u97f3\u6280\u672f\u5e76\u4e3a\u5176\u4ed6\u8bed\u8a00\u5f00\u53d1\u81ea\u53d1\u6027\u548c\u5bf9\u8bdd\u57fa\u51c6\u63d0\u4f9b\u65b9\u6cd5\u8bba\u6846\u67b6\u3002"}}
{"id": "2511.13590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13590", "abs": "https://arxiv.org/abs/2511.13590", "authors": ["Hao Wang", "Yuanfeng Song", "Xiaoming Yin", "Xing Chen"], "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation", "comment": null, "summary": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5230SQL\u5206\u7c7b\u6cd5\uff0c\u5e76\u57fa\u4e8e\u8be5\u5206\u7c7b\u6cd5\u521b\u5efa\u4e86SQL-Synth\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6bd4\u73b0\u6709\u57fa\u51c6\u5177\u6709\u66f4\u597d\u7684\u591a\u6837\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230SQL\u6570\u636e\u96c6\u8986\u76d6\u8303\u56f4\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u591a\u6837\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u6838\u5fc3\u610f\u56fe\u3001\u8bed\u53e5\u7c7b\u578b\u3001\u8bed\u6cd5\u7ed3\u6784\u548c\u5173\u952e\u52a8\u4f5c\u7684\u6587\u672c\u5230SQL\u5206\u7c7b\u6cd5\uff0c\u5e76\u5229\u7528\u8be5\u5206\u7c7b\u6cd5\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u6784\u5efa\u6570\u636e\u96c6\u5408\u6210\u7ba1\u9053\u3002", "result": "SQL-Synth\u6570\u636e\u96c6\u5728\u591a\u6837\u6027\u548c\u8986\u76d6\u8303\u56f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u73b0\u6709LLMs\u5728SQL-Synth\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u4f46\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u80fd\u591f\u5168\u9762\u5206\u6790\u6570\u636e\u96c6\u548c\u4e0d\u540cLLMs\u7684\u6027\u80fd\uff0c\u5e76\u6307\u5bfcLLMs\u8bad\u7ec3\u6570\u636e\u7684\u6784\u5efa\u3002"}}
{"id": "2511.13593", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13593", "abs": "https://arxiv.org/abs/2511.13593", "authors": ["Piaohong Wang", "Motong Tian", "Jiaxian Li", "Yuan Liang", "Yuqing Wang", "Qianben Chen", "Tiannan Wang", "Zhicong Lu", "Jiawei Ma", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents", "comment": null, "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.", "AI": {"tldr": "O-Mem\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u7528\u6237\u753b\u50cf\u7684\u65b0\u578b\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u53d6\u548c\u66f4\u65b0\u7528\u6237\u7279\u5f81\u4e0e\u4e8b\u4ef6\u8bb0\u5f55\u6765\u63d0\u5347\u4e2a\u6027\u5316AI\u52a9\u624b\u7684\u957f\u671f\u4ea4\u4e92\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7ef4\u6301\u957f\u671f\u4ea4\u4e92\u65f6\u9762\u4e34\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u548c\u52a8\u6001\u4e2a\u6027\u5316\u6311\u6218\uff0c\u4f20\u7edf\u8bb0\u5fc6\u7cfb\u7edf\u4f9d\u8d56\u8bed\u4e49\u5206\u7ec4\u68c0\u7d22\uff0c\u53ef\u80fd\u5ffd\u7565\u8bed\u4e49\u65e0\u5173\u4f46\u5173\u952e\u7684\u7528\u6237\u4fe1\u606f\u5e76\u5f15\u5165\u68c0\u7d22\u566a\u58f0\u3002", "method": "\u63d0\u51faO-Mem\u6846\u67b6\uff0c\u57fa\u4e8e\u4e3b\u52a8\u7528\u6237\u753b\u50cf\u52a8\u6001\u63d0\u53d6\u548c\u66f4\u65b0\u7528\u6237\u7279\u5f81\u53ca\u4e8b\u4ef6\u8bb0\u5f55\uff0c\u652f\u6301\u4eba\u7269\u5c5e\u6027\u548c\u4e3b\u9898\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u5206\u5c42\u68c0\u7d22\uff0c\u5b9e\u73b0\u66f4\u81ea\u9002\u5e94\u548c\u8fde\u8d2f\u7684\u4e2a\u6027\u5316\u54cd\u5e94\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u4e0a\u8fbe\u523051.76%\uff0c\u6bd4\u4e4b\u524d\u7684SOTA LangMem\u63d0\u5347\u8fd13%\uff1b\u5728PERSONAMEM\u4e0a\u8fbe\u523062.99%\uff0c\u6bd4\u4e4b\u524d\u7684SOTA A-Mem\u63d0\u53473.5%\uff1b\u540c\u65f6\u63d0\u5347\u4e86token\u548c\u4ea4\u4e92\u54cd\u5e94\u65f6\u95f4\u6548\u7387\u3002", "conclusion": "O-Mem\u4e3a\u5f00\u53d1\u9ad8\u6548\u4e14\u7c7b\u4eba\u7684\u4e2a\u6027\u5316AI\u52a9\u624b\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2511.13658", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13658", "abs": "https://arxiv.org/abs/2511.13658", "authors": ["Jiaming Qu", "Mengtian Guo", "Yue Wang"], "title": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues", "comment": null, "summary": "Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u673a\u5668\u5b66\u4e60\u68c0\u6d4b\u5230\u7684\u6b3a\u9a97\u6027\u8bc4\u8bba\u7279\u5f81\u8f6c\u5316\u4e3a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u5e2e\u52a9\u4eba\u4eec\u5728\u6ca1\u6709\u68c0\u6d4b\u5206\u7c7b\u5668\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u5728\u7ebf\u8bc4\u8bba\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u6b3a\u9a97\u6027\u8bc4\u8bba\u8bef\u5bfc\u6d88\u8d39\u8005\u3001\u635f\u5bb3\u4f01\u4e1a\u5229\u76ca\u5e76\u7834\u574f\u5728\u7ebf\u5e02\u573a\u4fe1\u4efb\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u80fd\u6709\u6548\u68c0\u6d4b\u6b3a\u9a97\u6027\u8bc4\u8bba\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u533a\u5206\u7279\u5f81\u5f80\u5f80\u96be\u4ee5\u88ab\u4eba\u7406\u89e3\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u673a\u5668\u5b66\u4e60\u5b66\u5230\u7684\u8bcd\u6c47\u7ebf\u7d22\u8f6c\u5316\u4e3a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u8fd9\u4e9b\u73b0\u8c61\u57fa\u4e8e\u5b9e\u8bc1\u6570\u636e\uff0c\u5728\u76f8\u4f3c\u9886\u57df\u5177\u6709\u901a\u7528\u6027\u3002", "result": "\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u83b7\u5f97\u7684\u8bed\u8a00\u73b0\u8c61\u6bd4\u5927\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u6216\u4e0a\u4e0b\u6587\u5b66\u4e60\u83b7\u5f97\u7684\u73b0\u8c61\u66f4\u5177\u9884\u6d4b\u6027\uff0c\u4e14\u57fa\u4e8e\u5b9e\u8bc1\u6570\u636e\u3002", "conclusion": "\u8fd9\u4e9b\u8bed\u8a00\u73b0\u8c61\u6709\u6f5c\u529b\u5e2e\u52a9\u4eba\u4eec\u5728\u7f3a\u4e4f\u6b3a\u9a97\u68c0\u6d4b\u5206\u7c7b\u5668\u7684\u73af\u5883\u4e2d\u6279\u5224\u6027\u8bc4\u4f30\u5728\u7ebf\u8bc4\u8bba\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2511.13689", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13689", "abs": "https://arxiv.org/abs/2511.13689", "authors": ["Sofia Jamil", "Kotla Sai Charan", "Sriparna Saha", "Koustava Goswami", "Joseph K J"], "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation", "comment": null, "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.", "AI": {"tldr": "\u63d0\u51fa\u4e86TAI\u6846\u67b6\uff0c\u5229\u7528LLM\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u901a\u8fc7\u63d0\u793a\u8c03\u4f18\u6765\u7ffb\u8bd1\u548c\u751f\u6210\u5370\u5ea6\u8bd7\u6b4c\u56fe\u50cf\uff0c\u63d0\u5347\u5370\u5ea6\u8bed\u8a00\u8bd7\u6b4c\u7684\u5168\u7403\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u5370\u5ea6\u8bd7\u6b4c\u5177\u6709\u8bed\u8a00\u590d\u6742\u6027\u548c\u6587\u5316\u6df1\u5ea6\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5927\u591a\u5ffd\u89c6\u5370\u5ea6\u8bed\u8a00\u8bd7\u6b4c\uff0c\u5176\u591a\u5c42\u542b\u4e49\u548c\u6587\u5316\u5178\u6545\u5bf9\u975e\u6bcd\u8bed\u8bfb\u8005\u6784\u6210\u7406\u89e3\u6311\u6218\u3002", "method": "\u4f7f\u7528TAI\u6846\u67b6\uff0c\u5305\u62ec\uff1a(1) \u7ffb\u8bd1\u6a21\u5757\u91c7\u7528\u51e0\u7387\u6bd4\u504f\u597d\u5bf9\u9f50\u7b97\u6cd5\u51c6\u786e\u7ffb\u8bd1\u5f62\u6001\u4e30\u5bcc\u7684\u8bd7\u6b4c\uff1b(2) \u56fe\u50cf\u751f\u6210\u6a21\u5757\u4f7f\u7528\u8bed\u4e49\u56fe\u6355\u6349\u6807\u8bb0\u3001\u4f9d\u8d56\u5173\u7cfb\u548c\u9690\u55bb\u8bed\u4e49\u5173\u7cfb\uff0c\u521b\u5efa\u8bd7\u6b4c\u7684\u89c6\u89c9\u8868\u793a\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aTAI Diffusion\u5728\u8bd7\u6b4c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b21\u79cd\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u76841,570\u9996\u8bd7\u6b4c\u7684MorphoVerse\u6570\u636e\u96c6\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u8bd7\u6b4c\u7ffb\u8bd1\u548c\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u8be5\u5de5\u4f5c\u65e8\u5728\u6269\u5927\u53ef\u8bbf\u95ee\u6027\u5e76\u4e30\u5bcc\u8bfb\u8005\u7684\u4f53\u9a8c\uff0c\u652f\u6301\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u3002"}}
{"id": "2511.13703", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13703", "abs": "https://arxiv.org/abs/2511.13703", "authors": ["Lavender Y. Jiang", "Angelica Chen", "Xu Han", "Xujin Chris Liu", "Radhika Dua", "Kevin Eaton", "Frederick Wolff", "Robert Steele", "Jeff Zhang", "Anton Alyakin", "Qingkai Pan", "Yanbing Chen", "Karl L. Sangwon", "Daniel A. Alber", "Jaden Stryker", "Jin Vivian Lee", "Yindalon Aphinyanaphongs", "Kyunghyun Cho", "Eric Karl Oermann"], "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations", "comment": null, "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.", "AI": {"tldr": "Lang1\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u533b\u7597\u64cd\u4f5c\u51b3\u7b56\u7684\u6a21\u578b\u7cfb\u5217\uff0c\u901a\u8fc7\u5728EHR\u6570\u636e\u548c\u4e92\u8054\u7f51\u6587\u672c\u4e0a\u9884\u8bad\u7ec3\uff0c\u5728\u533b\u7597\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u901a\u7528\u6a21\u578b\uff0c\u5373\u4f7f\u540e\u8005\u89c4\u6a21\u592770\u500d\u3002", "motivation": "\u901a\u7528\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u533b\u7597\u64cd\u4f5c\u51b3\u7b56\u6240\u9700\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u533b\u9662\u8fd0\u8425\u51b3\u7b56\u5bf9\u60a3\u8005\u6d41\u7a0b\u3001\u6210\u672c\u548c\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1Lang1\u6a21\u578b\u7cfb\u5217(100M-7B\u53c2\u6570)\uff0c\u5728NYU Langone Health\u7684800\u4ebf\u4e34\u5e8atoken\u548c6270\u4ebf\u4e92\u8054\u7f51token\u6df7\u5408\u8bed\u6599\u4e0a\u9884\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528ReMedE\u57fa\u51c6\u8fdb\u884c\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u540e\u7684Lang1-1B\u5728AUROC\u4e0a\u6bd4\u5fae\u8c03\u901a\u7528\u6a21\u578b\u63d0\u53473.64%-6.75%\uff0c\u6bd4\u96f6\u6837\u672c\u6a21\u578b\u63d0\u53471.66%-23.66%\uff0c\u4e14\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u5916\u90e8\u533b\u7597\u7cfb\u7edf\u3002", "conclusion": "\u533b\u7597\u7cfb\u7edfAI\u9700\u8981\u9886\u57df\u5185\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u7684\u7ed3\u5408\uff0c\u4e13\u4e1aLLM\u5728\u4e13\u4e1a\u4efb\u52a1\u4e0a\u53ef\u4e0e\u901a\u7528\u6a21\u578b\u7ade\u4e89\u3002"}}
