<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.RO](#cs.RO) [Total: 62]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

TL;DR: 提出基于DPO的对齐框架，提升LLM在医疗照护对话中的事实准确性、语义连贯性和共情能力，相比传统方法更高效。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在医疗照护应用中存在两大缺陷：事实不可靠性和缺乏共情沟通，这在敏感场景中带来风险，需要开发可信赖、共情的AI助手。

Method: 采用直接偏好优化（DPO）对齐框架，使用成对偏好数据微调领域适应的LLM，其中偏好回复体现支持性、可访问的沟通风格，拒绝回复代表规定性或过于技术化的语调。

Result: DPO调优模型在多个开源和专有LLM上实现了更高的语义对齐、改进的事实准确性和更强的人本评估分数，优于基线和Google医疗对话系统等商业替代方案。

Conclusion: 基于偏好的对齐为开发可信赖、共情且具有临床知识的AI助手提供了可扩展且透明的途径，适用于照护者和医疗沟通场景。

Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [2] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

TL;DR: 论文研究使用形态学感知的分词器来改进Yoloxóchitl Mixtec音频语料库的语际注释效率，提出了两种非线性分词方案，与传统BPE和Unigram模型竞争，其中Segment-and-Melody模型在词错误率上表现更好。


<details>
  <summary>Details</summary>
Motivation: 提高Yoloxóchitl Mixtec音频语料库语际注释的效率，减少人工标注工作量，同时针对该语言的非连接性形态特征设计专门的分词方法。

Method: 提出两种新颖的非线性分词方案：1) Segment and Melody分词器，仅提取声调信息而不预测分割；2) Sequence of Processes分词器，预测单词分割，使端到端ASR系统能单次生成分割和未分割的转录。使用ASR和基于文本的序列到序列工具，并与传统BPE和Unigram模型进行比较。

Result: 新型分词器与传统BPE和Unigram模型竞争性相当，Segment-and-Melody模型在词错误率上优于传统分词器，但在字符错误率上未达到相同水平。通过形态学和信息论指标分析，发现与下游性能存在预测相关性。

Conclusion: 针对语言非连接性形态特征专门设计的非线性分词器在ASR任务中与传统BPE和Unigram模型具有竞争力，但需要进一步研究这些分词器在下游处理任务中的适用性。

Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [3] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

TL;DR: 提出GAUGE框架，用于实时检测LLM对话中的隐性情感升级危害


<details>
  <summary>Details</summary>
Motivation: LLM作为情感伴侣时，即使没有明显毒性，重复的情感强化或情感漂移也可能导致隐性伤害，传统毒性过滤器无法检测。现有防护机制依赖外部分类器或临床标准，难以跟上对话的实时动态变化。

Method: 提出GAUGE框架，这是一个轻量级的基于logit的框架，通过测量LLM输出如何概率性地改变对话的情感状态来检测隐性对话升级。

Result: 论文提出了GAUGE框架，但摘要中没有报告具体的实验结果。

Conclusion: GAUGE能够实时检测LLM对话中的隐性情感升级，填补了传统毒性过滤器无法检测的空白，为LLM情感交互提供了更精细的防护机制。

Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [4] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

TL;DR: 提出CFD框架，通过多LLM代理模拟人类标注者交换细粒度证据达成共识，用于数据增强，在心理健康和在线安全任务上表现最佳


<details>
  <summary>Details</summary>
Motivation: 现实世界指标对NLP任务很重要（如心理健康分析的生活事件、在线安全的危险行为），但标注成本高且动态变化，需要有效的自动数据增强方法

Method: 提出置信感知细粒度辩论（CFD）框架，多个LLM代理模拟人类标注者，交换细粒度证据达成共识；创建两个专家标注数据集：心理健康Reddit幸福数据集和在线安全Facebook晒娃风险数据集

Result: CFD框架相比多种基线方法获得最稳健的数据增强性能；数据增强持续改善下游任务；通过辩论记录增强的特征带来最大提升，在线安全任务上比非增强基线提高10.1%

Conclusion: CFD框架是有效的LLM数据增强方法，能显著提升下游任务性能，特别是在需要细粒度证据交换的复杂标注场景中

Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [5] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

TL;DR: 利用LLM-as-a-Judge自动构建策略对齐的训练数据，无需人工标注或平行语料，使小型开源LLM在词汇简化上超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: 句子简化需要根据不同应用采用不同策略（如仅替换复杂词汇或整体重写），但实现策略驱动的控制仍是开放挑战，现有方法依赖昂贵的人工标注或平行语料

Method: 提出基于LLM-as-a-Judge的方法自动构建策略对齐的训练数据，完全无需人工标注或平行语料，使简化系统能适应不同简化策略

Result: 小型开源LLM（如Phi-3-mini-3.8B）在词汇导向简化上超越GPT-4o，在整体重写方面性能相当，自动指标和人工评估均验证了效果

Conclusion: 该方法具有鲁棒性，在不同模型家族和规模上均表现一致改进，为策略驱动的句子简化提供了高效解决方案

Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [6] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

TL;DR: LOCUS是一个低成本的NLP模型定制管道，通过少量标注数据实现高效模型构建，在NER和文本分类任务上超越GPT-4o等基线，同时大幅降低成本和模型大小。


<details>
  <summary>Details</summary>
Motivation: 解决传统NLP模型定制需要大量标注数据和计算资源的问题，提出一种低成本的通用专业化方法，使模型定制更加高效和经济。

Method: 采用三步流程：1) 从大型知识库中检索相关数据；2) 通过上下文数据生成合成额外训练样本；3) 使用全参数或LoRA等参数高效方法进行微调。

Result: 在NER和文本分类基准测试中持续超越强基线（包括GPT-4o），内存优化模型保持99%全微调精度，仅需5%内存占用，参数少于GPT-4o的1%但性能更好。

Conclusion: LOCUS提供了一种高效、低成本的NLP模型定制解决方案，通过少量数据实现高性能，显著降低资源需求，为实际应用提供了可行的技术路径。

Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [7] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

TL;DR: 两个大语言模型在多智能体设置中相互对话，无需外部输入，最终会陷入重复循环和收敛行为。


<details>
  <summary>Details</summary>
Motivation: 研究当两个大型语言模型在没有外部输入的情况下相互对话时会发生什么，探索多智能体对话的动态行为。

Method: 使用Mistral Nemo Base 2407和Llama 2 13B hf模型，从一个短种子句子开始，让两个模型相互读取对方的输出并生成响应，持续固定步数。应用词汇和基于嵌入的指标来测量对话与初始种子的偏离程度以及两个模型输出的相似性。

Result: 大多数对话开始时连贯，但后来陷入重复。在许多运行中，会出现一个短短语并在多个回合中重复。一旦开始重复，两个模型倾向于产生相似的输出而不是引入新的对话方向，导致相同或相似文本的循环。即使模型很大、单独训练且没有提示指令，也会出现这种收敛行为。

Conclusion: 两个大型语言模型在多智能体对话中会自发地收敛到重复循环状态，这揭示了在没有外部引导的情况下，语言模型对话的动态特性。

Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [8] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

TL;DR: Nanbeige4-3B是一个3B参数的小规模高性能语言模型，通过创新的预训练调度器、指令微调机制、双偏好蒸馏和多阶段强化学习，在多项基准测试中超越了同规模模型并媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: 推动小规模语言模型的性能边界，证明通过精心设计的训练策略，小模型也能达到接近大模型的性能水平，为资源受限场景提供高效解决方案。

Method: 1. 预训练：使用FG-WSD调度器（细粒度预热-稳定-衰减）分阶段优化数据混合；2. 指令微调：结合审议生成优化和思维链重构机制提升SFT数据质量；3. 知识蒸馏：采用双偏好蒸馏方法从旗舰推理模型蒸馏知识；4. 强化学习：多阶段RL使用可验证奖励和偏好建模增强推理和对齐能力。

Result: 在广泛基准测试中，Nanbeige4-3B不仅显著优于同参数规模的模型，还能与更大规模的模型竞争，展示了小模型通过优化训练策略能达到的卓越性能。

Conclusion: 通过创新的训练策略组合，成功扩展了小规模语言模型的缩放定律边界，为开发高性能小模型提供了有效框架，模型权重已在HuggingFace开源。

Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [9] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 提出轻量级上下文感知的篇章效用预测方法，用于多跳问答中的篇章重排序，通过建模篇章间依赖关系提升问答性能。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要从多个文本篇章中识别和综合信息。现有方法通常独立评估篇章效用，忽略了多跳推理的关键方面：篇章的效用是上下文相关的，受其与其他篇章关系的影响（是否提供补充信息或形成关键链接）。

Method: 提出轻量级方法建模上下文篇章效用，考虑篇章间依赖关系。微调小型transformer模型预测多跳问答的篇章效用分数，利用高级推理模型的推理轨迹捕捉篇章使用顺序，获得合成训练数据。

Result: 基于效用的检索篇章评分相比基于相关性的重排序方法，能带来改进的重排序和下游问答性能。

Conclusion: 建模上下文篇章效用对于多跳问答很重要，提出的轻量级方法能有效提升篇章重排序和问答性能。

Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [10] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 提出Identify-then-Verify框架，通过先识别缺失信息再验证的方法，更可靠地判断上下文是否足够回答问题


<details>
  <summary>Details</summary>
Motivation: 现有简单提示策略在处理需要推理的问题时经常失败，需要更可靠的方法来判断上下文是否包含足够信息来回答问题

Method: 提出结构化Identify-then-Verify框架：1) 生成多个关于缺失信息的假设并建立语义共识；2) 执行关键验证步骤，强制模型重新检查源文本来确认信息是否真正缺失

Result: 在多个多跳和事实性QA数据集上评估，结果显示该方法能产生更准确的充分性判断，同时清晰表达信息差距

Conclusion: 通过引导模型证明其关于缺失信息的判断，该框架能提高充分性建模的可靠性，为构建可靠问答系统提供有效方法

Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [11] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

TL;DR: 本文研究使用大语言模型自动将德语文本按CEFR语言水平分类，通过构建多样化数据集和多种方法评估，实现了比现有方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 语言能力评估对教育至关重要，能够根据学习者需求提供定制化教学。本文旨在探索使用大语言模型自动将德语文本按照欧洲语言共同参考框架（CEFR）进行水平分类，以实现可靠且可扩展的语言能力评估。

Method: 1. 构建多样化数据集：结合多个现有CEFR标注语料库与合成数据；2. 评估多种方法：提示工程策略、微调LLaMA-3-8B-Instruct模型、基于探针的方法（利用LLM内部神经状态进行分类）。

Result: 研究结果显示，相比先前方法，本文提出的方法取得了持续的性能提升，证明了LLMs在CEFR分类任务上的有效性和潜力。

Conclusion: 大语言模型在可靠且可扩展的CEFR分类方面具有显著潜力，能够为语言教育提供有效的自动化评估工具。

Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [12] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

TL;DR: ProSocialAlign：一个测试时参数高效框架，通过词典约束生成和方向调节机制，在保持安全性的同时提升语言模型的情感共鸣和价值对齐能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型安全范式在情感激烈或高风险场景中存在不足：单纯拒绝可能疏远用户，而简单顺从又会放大风险。需要一种既能确保安全又能保持同理心的方法。

Method: 提出ProSocialAlign框架，采用词典约束生成：首先应用硬约束消除有害延续，然后在安全集合内优化亲社会质量。结合方向调节机制（在参数空间减去学习的"伤害向量"）和偏好感知自回归奖励建模（通过梯度冲突解决联合训练多属性）。

Result: 在五个安全基准测试中实现最先进性能，显著减少不安全泄露，提升与人类价值观的对齐度，在多个评估指标上都有强劲提升。

Conclusion: ProSocialAlign为推理时生成上下文敏感、安全且与人类对齐的响应提供了稳健且模块化的基础，能够在保持安全性的同时提升模型的情感共鸣能力。

Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [13] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

TL;DR: 开发了AlignRuScore，这是针对俄语文本事实一致性评估的度量标准，通过微调RuBERT模型并利用翻译数据集实现


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对俄语文本的事实一致性评估工具，现有工具主要关注英语语料库，需要填补这一空白

Method: 通过微调RuBERT基础的对齐模型，添加任务特定的分类和回归头，使用俄语和翻译的英语数据集进行训练

Result: 成功将统一的对齐度量标准移植到俄语，为鲁棒的多语言事实一致性评估奠定了基础

Conclusion: 发布了翻译语料库、模型检查点和代码，支持进一步研究，证明了统一对齐度量标准可以成功移植到俄语

Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [14] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

TL;DR: 该研究使用语料库语言学方法分析在线讨论中虚拟现实（VR）与焦虑相关的话题，发现VR、Oculus和头戴设备是最常讨论的词汇，揭示了VR系统发展和用户体验的关注点。


<details>
  <summary>Details</summary>
Motivation: VR已被用于治疗广泛性焦虑症和社交焦虑症等临床问题，为患者福祉和护理创造了新途径。了解用户对该技术的在线讨论可以进一步支持其疗效。

Method: 采用语料库语言学方法，使用Sketch Engine软件分析英语趋势语料库，识别VR与焦虑子语料库中高频词汇及其搭配关系。

Result: 研究发现VR、Oculus和头戴设备是最常讨论的词汇，反映了对虚拟系统发展和物理设备的关注。介词短语搭配如"of virtual reality"、"in virtual reality"和"for virtual reality"分别与设计、体验和发展相关。

Conclusion: 这些发现提供了关于VR与焦虑在一般讨论中如何被讨论的新视角，为通过发展和可访问性支持咨询需求提供了未来机会的路径。

Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [15] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

TL;DR: CMV-Fuse是一个跨模态视图融合框架，通过结合抽象意义表示、成分句法、依存句法和语义注意力四种语言视角，模拟人类语言处理过程，提升方面级情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 当前方面级情感分析系统通常孤立地利用单一语言视角，忽视了人类自然语言处理中多种结构表征之间的复杂交互作用，需要更全面的多视角融合方法。

Method: 提出CMV-Fuse框架，系统整合四种语言视角：抽象意义表示、成分句法、依存句法和语义注意力，并融入外部知识。通过分层门控注意力融合机制，在局部句法、中间语义和全局知识三个层次进行融合，同时采用结构感知的多视图对比学习确保表征一致性。

Result: 在标准基准测试中相比强基线模型取得显著改进，分析显示每种语言视角都对更鲁棒的情感分析有贡献。

Conclusion: 通过模拟人类语言处理的多视角融合方法，CMV-Fuse能够捕捉细粒度结构模式和广泛上下文理解，为方面级情感分析提供了更全面的解决方案。

Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [16] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

TL;DR: GPT-2情感分析机制研究：早期层检测词汇情感，晚期层整合上下文，而非预期的中层专业化处理


<details>
  <summary>Details</summary>
Motivation: 研究GPT-2中情感信息处理的因果机制，验证假设的两阶段架构（早期词汇检测和中期上下文整合），实证检验大型语言模型中上下文整合的实际模式

Method: 使用系统性的激活修补技术，在GPT-2的所有12个层上进行实验，测试三种上下文整合假设：中层集中、现象特异性和分布式处理

Result: 早期层（0-3）确实作为词汇情感检测器，编码稳定、位置特定的极性信号；但所有三种上下文整合假设都被证伪，上下文现象（否定、讽刺、领域转移等）主要在晚期层（8-11）通过统一的非模块化机制整合

Conclusion: GPT-2的情感计算与预测的层次模式不同，上下文整合主要在晚期层发生，这突显了需要进一步实证表征大型语言模型中的上下文整合机制

Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [17] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

TL;DR: PersonaMem-v2是用于LLM个性化的先进数据集，包含1000个用户-聊天机器人交互，覆盖300+场景和20000+用户偏好。研究显示前沿LLM在隐式个性化任务上准确率仅37-48%，而通过强化微调的Qwen3-4B模型达到53%准确率，超越GPT-5。提出的智能记忆框架使用2k令牌记忆而非完整32k对话历史，实现55%准确率且减少16倍输入令牌。


<details>
  <summary>Details</summary>
Motivation: 个性化是AI能力和对齐的下一个重要里程碑。当前LLM在隐式个性化任务上表现不佳，准确率仅37-48%，长上下文推理能力不足。需要开发更好的数据集和训练方法来提升LLM的个性化能力。

Method: 1) 创建PersonaMem-v2数据集，包含1000个真实用户-聊天机器人交互，覆盖300+场景、20000+用户偏好，128k令牌上下文窗口，大多数用户偏好隐式呈现。2) 使用强化微调训练Qwen3-4B模型提升长上下文推理能力。3) 开发智能记忆框架，维护单一可读记忆，随用户交互增长。

Result: 1) 前沿LLM在隐式个性化任务上准确率仅37-48%。2) 强化微调的Qwen3-4B达到53%准确率，超越GPT-5。3) 智能记忆框架实现55%准确率，使用2k令牌记忆而非完整32k对话历史，减少16倍输入令牌。

Conclusion: PersonaMem-v2数据集对个性化研究有重要影响，智能记忆框架为现实世界个性化智能提供了可扩展路径。强化微调能有效提升模型的长上下文推理能力，智能记忆系统在减少计算成本的同时实现更好的个性化性能。

Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [18] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

TL;DR: FlyThinker提出"边思考边生成"框架，通过并行推理模型动态指导长文本生成，实现个性化且高效的长文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法主要针对群体偏好，忽视个体用户。早期个性化方法（如提示定制或微调）难以推理隐含偏好，而"先思考后生成"方法在长文本生成中面临挑战：静态一次性推理需要为完整响应生成捕获所有相关信息，学习困难且适应性有限。

Method: FlyThinker采用"边思考边生成"框架，使用独立的推理模型并行生成潜在token级推理，将其融合到生成模型中动态指导响应生成。推理模型仅依赖先前响应而非自身先前输出，保持训练并行性。

Result: 在真实世界基准测试中，FlyThinker实现了更好的个性化生成，同时保持了训练和推理效率。

Conclusion: FlyThinker通过并行推理和生成的设计，解决了长文本个性化生成中的效率和适应性问题，为个性化语言模型提供了有效的解决方案。

Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [19] [TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction](https://arxiv.org/abs/2512.06694)
*Aoi Fujita,Taichi Yamamoto,Yuri Nakayama,Ryota Kobayashi*

Main category: cs.CL

TL;DR: 提出TopiCLEAR方法，通过自适应降维聚类嵌入进行主题提取，专门针对社交媒体短文本，无需预处理，在多个数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型是为较长正式文档设计的，在处理社交媒体短文本时面临挑战：共现统计有限、语义碎片化、拼写不一致和语言非正式。需要专门针对社交媒体短文本的主题提取方法。

Method: TopiCLEAR：使用Sentence-BERT嵌入文本，先用高斯混合模型进行初步聚类，然后通过线性判别分析进行监督投影迭代优化聚类，直到收敛。该方法直接处理原始文本，无需停用词移除等预处理。

Result: 在四个数据集（20News、AgNewsTitle、Reddit、TweetTopic）上评估，与7个基线方法（包括最新的SBERT方法和零样本生成AI方法）相比，该方法与人工标注主题的相似度最高，对社交媒体帖子和在线新闻文章都有显著改进。

Conclusion: TopiCLEAR方法在社交媒体短文本主题提取方面表现出色，产生更可解释的主题，在社交媒体数据和网络内容分析中具有应用潜力。

Abstract: Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.

</details>


### [20] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

TL;DR: 提出一种结合差分隐私噪声分配与梯度裁剪的参数高效微调方法，在保护隐私的同时提升大语言模型指令微调的效率


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型指令微调中的隐私保护和效率问题，传统方法在隐私预算消耗和训练稳定性方面存在不足

Method: 冻结主干模型，通过低维投影子空间更新参数，在梯度计算中引入裁剪和自适应噪声分配，构建梯度约束、噪声分配和参数投影的统一框架

Result: 在准确性、隐私预算和参数效率方面优于基线模型，在多样化不确定数据条件下保持稳定性能

Conclusion: 丰富了差分隐私与参数高效微调的理论整合，为复杂指令环境下的安全训练提供了可行解决方案

Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [21] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

TL;DR: ImplicitBBQ是一个新的基准测试，用于评估大语言模型中的隐式偏见，通过姓名、文化线索等隐含方式提示受保护属性，而不仅仅是显式声明。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM偏见的基准主要依赖显式提示（直接声明宗教、种族、性别等受保护属性），但现实世界互动中偏见往往通过姓名、文化线索等隐含方式体现，这种关键疏忽在公平性评估中造成了重大盲点。

Method: 扩展Bias Benchmark for QA (BBQ)基准，创建ImplicitBBQ，通过隐含方式提示6个类别的受保护属性，而不是直接声明这些属性。

Result: 在GPT-4o上的评估显示，与显式BBQ提示相比，ImplicitBBQ表现出令人担忧的性能差异：在"性取向"子类别中准确率下降高达7%，大多数其他类别也出现一致下降，表明当前LLM存在显式基准无法检测的隐式偏见。

Conclusion: ImplicitBBQ为NLP领域的细致公平性评估提供了关键工具，揭示了当前LLM中存在的隐式偏见，这些偏见无法通过显式基准检测。

Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [22] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: PDFTEMRA是一个紧凑的Transformer架构，通过模型蒸馏、频域调制、集成学习和随机激活模式等技术降低计算成本，适用于资源受限的医疗NLP场景，特别是为印地语使用者和视障用户提供医疗帮助。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在资源受限的医疗环境中部署困难，特别是为低资源语言（如印地语）使用者和视障用户提供医疗帮助时面临挑战。需要开发计算成本低但性能保持的模型来解决这一问题。

Method: 提出PDFTEMRA架构，整合了四种关键技术：1) 模型蒸馏以压缩模型规模；2) 频域调制提高效率；3) 集成学习增强性能；4) 随机激活模式减少计算需求。模型在针对印地语和可访问性场景的医疗问答和咨询数据集上进行训练和评估。

Result: PDFTEMRA在显著降低计算需求的同时，达到了与标准NLP最先进模型基线相当的性能表现，证明了其在资源受限医疗NLP应用中的适用性。

Conclusion: PDFTEMRA为资源受限环境中的医疗NLP应用提供了一种可行的解决方案，特别适合支持低资源语言使用者和视障用户的医疗需求，具有实际部署价值。

Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [23] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

TL;DR: 通过简单的语义提示（如"meaning: {word}"）显著提升文本嵌入模型在单词相似性任务上的表现，无需训练即可超越传统静态嵌入方法


<details>
  <summary>Details</summary>
Motivation: 文本嵌入模型主要针对句子级应用设计，在单词级任务上的表现了解不足，需要探索如何提升它们在孤立单词上的语义表示能力

Method: 在单词前添加语义提示（如"meaning: {word}"或"Represent the semantic concept: {word}"），然后使用文本嵌入模型进行嵌入，在7个模型和3个标准基准上测试

Result: 提示方法显著提升单词相似性相关性，在SimLex-999上最高提升+0.29，某些模型从相关性为0恢复到+0.73提升，最佳结果在SimLex-999上达到0.692相关性，超越Word2Vec(0.40)和LexVec(0.48)

Conclusion: 简单的零样本语义提示技术能有效提升文本嵌入模型在单词相似性任务上的表现，无需训练即可达到新的最先进水平，为单词级应用提供了实用解决方案

Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [24] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

TL;DR: 提出LWE框架，让LLM评估器在推理过程中通过自我改进的元提示进行顺序学习，无需训练集，特别适用于评估者难以判断的案例。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-judge评估方法存在两个问题：1) 每个案例独立评估，无法积累经验；2) 使用固定提示词，缺乏样本特定的评估标准。需要让评估器在推理过程中能够学习和改进。

Method: 提出Learning While Evaluating (LWE)框架，维护一个不断演化的元提示，该提示：1) 生成样本特定的评估指令；2) 通过自我生成的反馈进行自我优化。进一步提出Selective LWE，只在自我不一致的案例上更新元提示，提高计算效率。

Result: 在两个成对比较基准测试中，Selective LWE优于强基线方法，实证表明评估器可以通过简单的选择性更新在顺序测试中改进，从难以判断的案例中学习最多。

Conclusion: LWE框架使LLM评估器能够在推理时顺序学习，无需额外训练数据，通过选择性更新机制在保持性能的同时显著降低成本，为自动评估系统提供了更智能、更高效的解决方案。

Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [25] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: 该论文提出NBDiff方法，通过渐进式块大小增加和上下文因果注意力掩码，将自回归语言模型高效适配为块扩散模型，避免从头训练的高成本。


<details>
  <summary>Details</summary>
Motivation: 自回归解码存在顺序生成的吞吐量瓶颈，而从头训练扩散语言模型成本高昂且浪费现有AR模型的知识。现有适配方法未能解决AR因果性与块双向性之间的根本不匹配问题。

Method: 将AR视为块大小为1的块扩散模型，设计渐进适配路径：使用上下文因果注意力掩码（上下文因果，仅在活动块内双向）、高效并行适配过程、辅助AR损失以最大化数据利用，以及逐步增加生成块大小。

Result: NBDiff-7B在7B级扩散语言模型中达到最先进性能，在通用知识、数学和代码基准上相比强基线有显著提升，同时继承了长上下文建模和推理能力。

Conclusion: 从AR到块扩散的原则性适配是训练扩散语言模型的有效且计算高效替代方案，避免了从头训练的高成本。

Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [26] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

TL;DR: LLM4SFC框架首次实现从自然语言描述生成可执行的顺序功能图(SFC)程序，解决了图形化PLC编程语言生成的挑战，在真实工业数据集上达到75%-94%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要关注文本化PLC编程语言(如结构化文本)，而IEC 61131-3标准的图形化语言(如顺序功能图)生成研究不足。SFC生成面临图形化特性和嵌入式ST代码的双重挑战，现有方法常产生不可执行或与工业工具链不兼容的代码。

Method: LLM4SFC框架包含三个核心组件：1) 精简结构化表示，捕获SFC拓扑结构和嵌入式ST代码，减少文本冗余；2) 微调和少样本检索增强生成(RAG)，对齐SFC编程规范；3) 结构化生成方法，实时修剪非法令牌确保符合SFC文本格式。

Result: 在自动化制造项目的真实SFC数据集上评估，使用开源和专有LLM，LLM4SFC可靠地生成语法有效的SFC程序，成功桥接图形化和文本化PLC语言，生成成功率在75%-94%之间。

Conclusion: LLM4SFC是首个从自然语言描述生成可执行SFC的框架，有效解决了图形化PLC编程语言生成的挑战，为自动化工业编程铺平了道路。

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [27] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

TL;DR: 本文探索使用五种大语言模型（包括开源和专有模型）自动生成出院小结，发现专有模型（特别是Gemini）表现最佳，开源模型虽有望但存在幻觉和重复信息问题。


<details>
  <summary>Details</summary>
Motivation: 出院小结包含丰富的患者信息，自动化生成可减轻医护人员负担、减少错误，并确保关键信息易于获取和操作。

Method: 使用五种大语言模型（Mistral、Llama 2、GPT-3、GPT-4、Gemini 1.5 Pro），基于MIMIC-III数据集，采用精确匹配、软重叠和无参考指标进行评估。

Result: 专有模型（特别是Gemini使用单样本提示）表现最佳，生成的摘要与黄金标准相似度最高。开源模型（尤其是微调后的Mistral）表现有希望但仍落后，常出现幻觉和重复信息问题。临床专家人工评估确认专有模型生成的摘要具有实际效用。

Conclusion: 大语言模型（尤其是专有模型）是自动生成出院小结的有希望候选方案，但需解决幻觉和缺失信息等挑战，并确保数据隐私。

Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [28] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

TL;DR: CAuSE是一个为预训练多模态分类器生成忠实自然语言解释的新框架，通过因果抽象和交换干预确保解释的忠实性


<details>
  <summary>Details</summary>
Motivation: 多模态分类器是黑盒模型，现有解释方法缺乏直观性和可访问性。自然语言解释虽然直观，但需要忠实反映分类器的内部决策过程，这是当前研究的关键挑战

Method: 提出CAuSE框架，通过交换干预训练，形成对底层分类器的因果抽象，生成忠实的自然语言解释

Result: CAuSE在数据集和模型上具有良好泛化能力，在因果忠实性度量上超越其他方法，定性分析也显示其优势

Conclusion: CAuSE能够为多模态分类器生成忠实的自然语言解释，通过因果抽象确保解释的可靠性，并进行了详细的错误分析

Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [29] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

TL;DR: AquaFusionNet是一个轻量级跨模态框架，统一显微镜成像和物理化学传感器数据，用于实时监测小型饮用水系统的微生物污染，在边缘设备上实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有监测工具只能捕捉微生物污染的片段信息，显微镜成像和物理化学传感器数据需要分开解读，导致实时决策不可靠。在饮用水安全领域，公开的微观数据集稀缺。

Method: 提出AquaFusionNet框架，通过门控交叉注意力机制学习微生物外观与传感器动态之间的统计依赖关系，专门为低功耗硬件设计。使用新数据集AquaMicro12K（12,846张饮用水环境标注显微图像）进行训练。

Result: 在印度尼西亚东爪哇7个设施部署6个月，处理184万帧图像，污染事件检测达到94.8% mAP@0.5，异常预测准确率96.3%，在Jetson Nano上功耗仅4.8W。相比其他轻量级检测器，在相同或更低功耗下提供更高准确率。

Conclusion: 跨模态耦合减少了单模态检测器在污染、浊度峰值和不一致光照下的常见故障模式。所有模型、数据和硬件设计已开源，促进分散式水安全基础设施的复制和适应。

Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [30] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

TL;DR: Rhea框架通过解耦对话历史为指令记忆和情景记忆，解决LLMs在多轮对话中的累积上下文衰减问题，显著提升性能


<details>
  <summary>Details</summary>
Motivation: LLMs在单轮任务上表现出色，但在多轮对话中性能下降，这是由于注意力污染、稀释和漂移导致的累积上下文衰减问题

Method: 提出Rhea框架，将对话历史解耦为两个独立记忆模块：指令记忆（持久存储全局约束）和情景记忆（动态管理用户-模型交互），通过优先级注意力构建高信噪比上下文

Result: 在多个多轮对话基准测试中，Rhea缓解了性能衰减，在10分制上提高了1.04分（相对基线提升16%），并在长时交互中保持接近完美的指令保真度（IAR > 8.1）

Conclusion: Rhea为构建更精确、指令一致的对话式LLMs提供了一个原则性且有效的框架

Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [31] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

TL;DR: LLMs在模拟用户观点时存在偏见，难以准确代表不同人口统计和文化背景的用户，即使采用CLAIMSIM方法提升多样性，仍无法准确模拟用户回答。


<details>
  <summary>Details</summary>
Motivation: LLMs（特别是经过RLHF训练的模型）倾向于表现出对主流观点的偏见，这引发了对其能否代表不同人口统计和文化背景用户的担忧。需要评估LLMs在模拟人类对跨领域调查问题回答方面的能力。

Method: 研究采用直接提示和思维链提示两种方法，并提出CLAIMSIM方法（主张多样化方法），该方法从LLM的参数知识中引出观点作为上下文输入。在调查问题回答任务上进行实验。

Result: 实验表明，虽然CLAIMSIM能产生更多样化的回答，但两种方法都难以准确模拟用户。分析发现两个关键局限：1）LLMs倾向于在不同人口统计特征间保持固定观点，生成单一视角的主张；2）当面对相互冲突的主张时，LLMs难以推理人口统计特征间的细微差异，限制了其适应特定用户画像的能力。

Conclusion: LLMs在模拟用户观点方面存在显著局限性，特别是在代表多样化的用户群体方面。即使采用CLAIMSIM等方法来提升回答多样性，LLMs仍难以准确模拟真实用户的回答模式，尤其是在处理复杂的人口统计特征差异时。

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [32] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

TL;DR: 开发了基于历史安全数据的自动化PRO-CTCAE项目选择方法，通过MedDRA语义映射和谱分析平衡信号覆盖与患者负担


<details>
  <summary>Details</summary>
Motivation: PRO-CTCAE项目选择面临两难：选择过多会增加患者负担降低依从性，选择过少可能遗漏重要安全信号，需要自动化方法来优化选择

Method: 将PRO-CTCAE症状映射到MedDRA术语，编码到Safeterm语义空间，结合相关性和发生率计算效用函数，通过谱分析识别正交医学概念集，按重要性排序并基于解释信息确定截断点

Result: 方法在模拟和肿瘤学案例研究中评估，作为Safeterm试验安全应用的一部分实现，提供客观可重复的PRO-CTCAE设计方法

Conclusion: 自动化方法利用MedDRA语义和历史数据简化PRO-CTCAE设计，平衡信号覆盖与患者负担，提高临床试验效率

Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [33] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

TL;DR: LLMs在司法语言学中既是分析工具又是挑战来源，需要方法重构以保持科学可信度和法律可采性


<details>
  <summary>Details</summary>
Motivation: LLMs在司法语言学中呈现双重挑战：一方面作为强大的分析工具，另一方面通过风格模仿、作者混淆和合成文本扩散破坏语言特征的基本假设，这对司法实践中的作者识别和法律可采性标准构成威胁

Method: 分析当前AI文本检测技术的局限性，包括基于分类器的方法、风格计量学方法和水印方法，指出这些方法存在高误报率（特别是对非母语英语写作者）和对抗策略脆弱性等问题

Result: 当前AI文本检测技术存在显著局限，无法满足司法可采性标准（如Daubert和Kumho Tire框架），需要方法论重构

Conclusion: 司法语言学需要进行方法论重构，包括采用混合人机工作流程、超越二元分类的可解释检测范式，以及测量不同群体错误和偏见的验证机制，同时保持"语言揭示生产者信息"的核心见解

Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [34] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

TL;DR: IXAM是一个交互式可解释性框架，用于分析基于嵌入的作者归属模型，允许用户探索嵌入空间并构建多粒度写作风格特征解释


<details>
  <summary>Details</summary>
Motivation: 现有作者归属模型缺乏交互式解释工具，用户难以理解模型预测背后的写作风格特征，需要更直观的可解释性框架

Method: 开发交互式可解释性框架IXAM，支持用户探索嵌入空间，构建多粒度写作风格特征解释，包括用户评估验证框架价值

Result: 用户评估显示IXAM相比预定义风格解释具有更高价值，能够有效帮助用户理解模型预测

Conclusion: IXAM为作者归属模型提供了有效的交互式解释工具，增强了模型透明度和用户理解能力

Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [35] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanhoé Botcazou,Tassadit Amghar,Sylvain Lamprier,Frédéric Saubion*

Main category: cs.CL

TL;DR: 提出Progress Ratio Embeddings (PRE)方法，通过连续嵌入和三角函数信号实现稳健的文本生成长度控制，相比现有方法在超出训练分布时表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 现代神经语言模型在文本生成方面取得高精度，但对生成长度的精确控制仍然不足。现有基于反向位置嵌入(RPE)的方法在超出训练分布时存在限制，特别是使用与绝对剩余token计数相关的离散倒计时信号会导致不稳定性。

Method: 提出Progress Ratio Embeddings (PRE)方法，使用连续嵌入与三角函数不耐烦信号相结合。PRE无缝集成到标准Transformer架构中，提供稳定的长度保真度，同时不影响标准评估指标下的文本准确性。

Result: PRE在两种广泛使用的新闻摘要基准测试中验证有效，提供稳定的长度控制而不降低文本质量，并且能够很好地泛化到未见过的目标长度。

Conclusion: PRE方法相比现有长度控制技术更加稳健，特别是在超出训练分布时表现更好，为文本生成长度控制提供了有效的解决方案。

Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [36] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

TL;DR: PICEPR是一种新颖的"Prompting-in-a-Series"算法，通过内容和嵌入两条流水线，利用模块化解码器LLM进行人格识别，性能提升5-15%，达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理任务中表现出色，但如何更好地利用LLM进行人格识别仍是一个挑战。本研究旨在开发一种心理学信息化的内容嵌入方法，提升人格识别的准确性和效率。

Method: 提出PICEPR算法，包含两条流水线：(a)内容流水线：利用模块化解码器LLM总结或生成内容；(b)嵌入流水线：作为人格特征提取器和人格丰富内容生成器。同时对比了闭源模型(gpt4o, gemini)和开源模型(mistral)。

Result: PICEPR算法在人格识别任务上实现了5-15%的性能提升，达到了新的最先进水平。同时对比了不同LLM生成内容的质量。

Conclusion: PICEPR算法通过心理学信息化的内容嵌入方法，有效提升了人格识别的性能，为LLM在人格分析领域的应用提供了新的思路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [37] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

TL;DR: FVA-RAG 提出了一种新的检索增强生成框架，通过对抗性检索策略主动寻找反驳证据，减少因用户偏见导致的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统存在"检索奉承"漏洞：当用户查询基于错误前提或常见误解时，向量检索器倾向于获取符合用户偏见的文档，导致模型"带引用的幻觉"。

Method: 提出FVA-RAG框架，将检索范式从归纳验证转向演绎证伪。采用对抗性检索策略生成"致命查询"来寻找矛盾证据，并引入双重验证机制权衡草稿答案与"反上下文"。

Result: 在常见误解数据集上的初步实验表明，FVA-RAG相比标准RAG基线显著提高了对奉承性幻觉的鲁棒性。

Conclusion: FVA-RAG通过主动寻找反驳证据的对抗性检索策略，有效减少了RAG系统中的奉承性幻觉问题，可作为事实生成的推理时"红队"。

Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [38] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

TL;DR: 研究发现当前大语言模型的安全对齐技术对多轮对抗攻击存在根本性脆弱性，模型规模不预测鲁棒性，而扩展推理模式可显著提升安全性


<details>
  <summary>Details</summary>
Motivation: 尽管在安全对齐方面投入了大量资源，但大语言模型对复杂多轮对抗攻击的脆弱性仍未被充分表征，且模型规模或推理模式是否影响鲁棒性尚不清楚

Method: 使用TEMPEST多轮攻击框架评估10个前沿模型，覆盖8个供应商和1000种有害行为，生成超过97,000个API查询，通过独立安全分类器进行自动评估

Result: 模型脆弱性呈现光谱分布：6个模型攻击成功率（ASR）达96%-100%，4个模型表现出有意义的抵抗（ASR 42%-78%）；在相同架构上启用扩展推理可将ASR从97%降至42%

Conclusion: 当前对齐技术对自适应多轮攻击存在根本性脆弱性，模型规模不预测鲁棒性，而审慎推理模式是可部署的安全增强方向

Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [39] [SETUP: Sentence-level English-To-Uniform Meaning Representation Parser](https://arxiv.org/abs/2512.07068)
*Emma Markle,Javier Gutierrez Bach,Shira Wein*

Main category: cs.CL

TL;DR: 本文提出了两种英语文本到UMR解析的方法，其中SETUP模型在AnCast和SMATCH++评分上取得显著提升，推动了自动UMR解析的发展。


<details>
  <summary>Details</summary>
Motivation: UMR作为一种新颖的图式语义表示，能够捕捉文本核心含义并适用于多种语言（包括低资源语言），但需要自动化的文本到UMR解析器来实现大规模应用。目前该领域的研究还很有限。

Method: 提出了两种方法：1）微调现有的抽象意义表示解析器；2）利用从通用依存关系转换器。将先前工作作为基线进行比较。

Result: 最佳模型SETUP在AnCast评分上达到84分，SMATCH++评分达到91分，显示出在自动UMR解析方面的显著进步。

Conclusion: 本文提出的方法特别是SETUP模型，在英语文本到UMR解析任务上取得了实质性进展，为UMR在下游应用中的大规模自动生成奠定了基础。

Abstract: Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.

</details>


### [40] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

TL;DR: SAGE是一个基于场景的基准测试，通过跨文化核心概念对齐和生成式任务设计来评估大语言模型的跨文化理解和推理能力，包含4530个测试项，覆盖9个文化维度和15个现实场景。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLMs跨文化理解能力的基准存在三个关键局限：缺乏情境场景、跨文化概念映射不足、深层文化推理能力有限。需要更全面的评估工具来检验LLMs是否真正具备跨文化理解能力。

Method: 基于文化理论将跨文化能力分为9个维度，选取210个核心概念，在4大类跨文化情境下构建15个具体现实场景，按照项目设计原则创建4530个测试项，支持数据集持续扩展和多语言迁移。

Result: SAGE基准测试揭示了LLMs在跨文化推理方面的系统性局限，模型在各个维度和场景中都表现出弱点。虽然已有进步，但LLMs距离真正细致的跨文化理解还有差距。

Conclusion: SAGE基准测试填补了现有跨文化评估工具的空白，能够有效评估LLMs的跨文化理解和推理能力，揭示了模型的系统性局限，为未来改进提供了方向。

Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [41] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

TL;DR: Token Filtering是一种轻量级在线结构化剪枝技术，通过联合键值相似度直接评估token冗余度，在推理过程中跳过冗余注意力计算，无需校准数据，显著降低LLM推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法依赖离线校准数据，在不同输入间泛化能力差，导致不稳定。需要一种无需校准数据、在推理过程中直接做出剪枝决策的方法。

Method: 1. 通过联合键值相似度测量token冗余度，跳过冗余注意力计算；2. 设计方差感知融合策略，自适应加权不同注意力头的键值相似度，确保在高剪枝率下仍保留信息丰富的token；3. 无额外内存开销，提供更可靠的token重要性评估标准。

Result: 在LLaMA-2 (7B/13B)、LLaMA-3 (8B)和Mistral (7B)上的实验表明，Token Filtering持续优于现有结构化剪枝方法，在常识推理基准上保持准确性，在MMLU等挑战性任务上即使50%剪枝仍保持强性能。

Conclusion: Token Filtering提供了一种无需校准数据、稳定高效的在线结构化剪枝方案，显著降低LLM推理成本，同时保持模型性能，为实际部署提供了实用解决方案。

Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [42] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: DART是一个多智能体框架，通过视觉智能体之间的辩论分歧来识别有用的视觉工具，利用工具引入新信息和专家对齐评分来促进讨论，提升视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 虽然专业视觉工具可以为大语言模型或视觉语言模型提供专家知识（如接地、空间推理、医学知识等），但确定何时调用哪些工具具有挑战性。多智能体辩论中如何有效利用工具来解决分歧是一个关键问题。

Method: 提出DART多智能体框架：1）多个视觉智能体进行辩论，通过分歧识别有用的视觉工具（如目标检测、OCR、空间推理等）；2）工具引入新信息并提供工具对齐的同意分数，突出与专家工具一致的智能体；3）使用聚合智能体基于智能体输出和工具信息选择最佳答案。

Result: 在四个多样化基准测试中，DART优于多智能体辩论和单智能体工具调用框架：在A-OKVQA上比次强基线（带评判模型的多智能体辩论）提升3.4%，在MMMU上提升2.4%。在M3D医学数据集上比其他强基线提升1.3%。文本重叠分析显示DART讨论更丰富，工具调用分布显示多样化工具被可靠使用来解决分歧。

Conclusion: DART通过利用多智能体辩论中的分歧来识别和调用相关视觉工具，有效提升了视觉问答性能，能够适应新工具领域，并促进更丰富的多智能体讨论。

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [43] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: GUMBridge是一个新的桥接指代资源，包含16种不同英语文体，提供广泛的桥接现象覆盖和详细的子类型分类标注，并评估了LLMs在桥接解析任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的英语桥接指代资源大多规模小、覆盖范围有限、文体类型有限，需要更全面、多样化的资源来支持桥接指代研究。

Method: 创建GUMBridge资源，包含16种不同英语文体的文本，提供详细的桥接指代标注和子类型分类，并进行标注质量评估和LLMs基线性能测试。

Result: GUMBridge提供了广泛的桥接现象覆盖和详细的子类型标注，评估显示桥接解析和子类型分类在LLMs时代仍然是困难的NLP任务。

Conclusion: GUMBridge是一个全面、多样化的桥接指代资源，有助于推动桥接指代研究，并表明当前LLMs在桥接解析任务上仍有改进空间。

Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [44] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

TL;DR: MASim是首个支持多语言智能体交互的仿真框架，用于研究跨语言社会行为，包含全球舆论建模和媒体影响分析功能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体角色扮演模拟大多是单语言的，无法模拟真实社会中至关重要的跨语言互动，限制了计算社会科学研究。

Method: 开发了MASim多语言智能体仿真框架，支持具有不同社会语言学特征的生成式智能体进行多轮交互。构建了MAPS基准测试，结合全球人口分布的调查问题和人口统计角色。

Result: 实验显示MASim能够复现社会文化现象，校准、敏感性、一致性和文化案例研究表明该框架有效，凸显了多语言仿真对可扩展、可控计算社会科学的重要性。

Conclusion: MASim为研究跨语言社会互动提供了首个多语言智能体仿真框架，通过全球舆论建模和媒体影响分析，推动了计算社会科学向多语言方向的发展。

Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [45] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

TL;DR: NeSTR：一种结合符号表示与反思推理的神经符号框架，用于提升大语言模型在复杂时间约束下的推理能力，无需微调即可显著改善时间敏感任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时间推理方面存在挑战，特别是复杂时间约束下。现有方法中，符号方法未能充分利用LLM的推理能力，而反思方法缺乏结构化时间表示，导致推理不一致或产生幻觉。即使有正确的时间上下文，LLM仍可能误解或误用时间信息。

Method: 提出神经符号时间推理（NeSTR）框架，将结构化符号表示与混合反思推理相结合。通过符号编码保留显式时间关系，通过验证强制执行逻辑一致性，使用溯因反思纠正错误推理。

Result: 在多样化时间问答基准测试中，NeSTR实现了优越的零样本性能，无需任何微调即可持续改善时间推理能力，展示了神经符号集成在增强大语言模型时间理解方面的优势。

Conclusion: NeSTR通过整合符号表示和反思推理，有效解决了LLM在时间推理中的局限性，为提升大语言模型的时间敏感性提供了一种有效的神经符号集成方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [46] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 提出TreeED和ForestED框架，用LLM诱导决策树进行错误检测，提升可解释性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为标注器的错误检测方法存在两个主要问题：1) 黑盒决策过程缺乏可解释性；2) 对提示词敏感，输出不一致，缺乏鲁棒性

Method: 提出LLM-as-an-inducer框架：TreeED用LLM诱导决策树骨架，包含规则节点、GNN节点和叶子节点；ForestED通过不确定性采样构建多个决策树，使用EM算法联合估计树可靠性和优化共识预测

Result: 实验证明方法准确、可解释且鲁棒，平均F1分数比最佳基线提升16.1%

Conclusion: 通过LLM诱导决策树的方法有效解决了现有错误检测方法的可解释性和鲁棒性问题

Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [47] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 该研究为泰卢固语-英语语音翻译创建了首个高质量基准，比较了级联与端到端架构，发现端到端系统在低资源场景下通过适当调优可达到与级联方法相当的性能，并评估了多种自动评估指标的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管泰卢固语有超过8000万使用者，但针对这种形态丰富的语言的语音翻译研究严重不足。研究旨在填补这一空白，为泰卢固语-英语语音翻译建立高质量基准。

Method: 从46小时手动验证的CSTD语料库数据创建基准数据集（30h/8h/8h训练/开发/测试分割）。系统比较级联架构（IndicWhisper + IndicMT）与端到端架构（微调的SeamlessM4T模型），并评估BLEU、METEOR、ChrF++、ROUGE-L、TER和BERTScore等指标与人工判断的相关性。

Result: IndicWhisper + IndicMT级联方法因使用大量泰卢固语特定训练数据而性能最高，但微调的SeamlessM4T模型使用显著更少的泰卢固语数据仍表现出色。研究发现，通过仔细的超参数调优和足够的平行数据（可能少于100小时），端到端系统在低资源设置下能达到与级联方法相当的性能。传统指标比BERTScore在泰卢固语-英语翻译中提供更好的质量区分。

Conclusion: 该工作提供了三个关键贡献：可复现的泰卢固语-英语基准、低资源场景下端到端性能竞争力的实证证据，以及针对形态复杂语言对自动评估的实用指导。

Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [48] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 该论文提出了一种针对低资源语言的跨语言连续预训练方法，通过利用无标签语音数据和形态感知分词，开发出参数更少但性能可比的语音识别模型，挑战了模型规模决定性能的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的语音识别面临标注数据稀缺和计算资源不足的双重限制，而现有大模型依赖大量参数和标注数据，不适用于资源有限的语言场景。

Method: 构建了3000小时的多语言无标签语音语料库，采用可扩展的数据收集流程，结合目标导向的连续预训练和形态感知分词技术，开发了3亿参数的模型。

Result: 3亿参数模型在波斯语上超越了15亿参数的Whisper Large v3，在阿拉伯语和乌尔都语上取得竞争性结果，性能与5倍大的模型相当，但使用更少的参数和标注数据。

Conclusion: 语音识别质量并非主要取决于模型规模，数据相关性和战略预训练在低资源场景中更为关键，这为开发包容性语音技术提供了实用路径，无需依赖大规模计算基础设施或专有数据集。

Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [49] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 通过特征归因方法构建伪忠实自解释，利用持续学习训练指令调优模型，提升自解释忠实度并实现跨风格泛化


<details>
  <summary>Details</summary>
Motivation: 大语言模型能生成不同风格的自解释，但这些解释往往缺乏忠实性（faithfulness）。如何提升忠实性以及改进效果是否能跨风格泛化，是当前未充分探索的问题。

Method: 使用特征归因方法构建单词约束的伪忠实自解释，然后对指令调优模型进行持续学习训练，在三个分类任务和三种解释风格上进行实验。

Result: 训练能提升所有分类任务和解释风格的自解释忠实度，改进效果能泛化到多词设置和未见任务，且三种风格间存在一致的跨风格泛化。

Conclusion: 训练有助于提升大语言模型的忠实自解释能力，且这种能力具有跨风格和跨任务的泛化性，表明训练可能促进更广泛的忠实自解释能力改进。

Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [50] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

TL;DR: 提出一种构建多语言语料库的混合方法，用于研究人文社科中的新兴概念，以"非技术创新"为例，结合公司网站文本和年报数据，创建适合机器学习的数据集。


<details>
  <summary>Details</summary>
Motivation: 为研究人文社科领域的新兴概念（如"非技术创新"）提供可重复、可扩展的多语言语料库资源，既支持概念分析又适合自然语言处理应用。

Method: 混合方法结合两种数据源：1) 从公司网站自动提取的法语和英语文本；2) 按文档标准自动筛选的年报。处理流程包括语言检测、内容过滤、相关片段提取和元数据增强，最终创建包含专家词典术语上下文块（前后各两句）的标注数据集。

Result: 构建了一个可重复、可扩展的多语言语料库，包含英语派生数据集，每个术语出现都带有上下文块和主题类别标注，适合监督分类任务和NLP应用。

Conclusion: 该方法为分析新兴概念的词汇变异性和生成NLP专用数据集提供了有效解决方案，具有可重复性和可扩展性优势。

Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [51] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: 通过强化学习微调语言模型使用Prolog作为可验证计算工具，提升推理可靠性和可审计性


<details>
  <summary>Details</summary>
Motivation: 语言模型经常产生看似合理但错误的推理结果，难以验证，需要确保工具使用的可靠性以构建安全的智能体AI系统

Method: 使用GRPO在GSM8K-Prolog-Prover数据集上微调Qwen2.5-3B-Instruct，研究提示结构、奖励组成（执行、语法、语义、结构）和推理协议（单次、best-of-N、两种智能体模式）的影响

Result: 强化学习方法优于监督微调，3B模型在零样本MMLU上达到与7B模型少样本相当的性能；best-of-N结合外部Prolog验证在GSM8K上准确率最高；内部修复的智能体推理在MMLU-Stem和MMLU-Pro上零样本泛化性能最优

Conclusion: 将模型推理基于形式化验证系统能显著提高安全关键应用的可靠性和可审计性

Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [52] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

TL;DR: Persian-Phi是一个3.8B参数模型，通过创新的课程学习流程将英语模型Phi-3 Mini有效适配到波斯语，证明了小模型也能在多语言任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI民主化受到训练低资源语言大语言模型巨大计算成本的阻碍，需要探索资源高效的方法来扩展先进LLM到资源不足的语言。

Method: 采用新颖的课程学习流程：1) 使用双语叙事(Tiny Stories)进行"预热"阶段对齐嵌入；2) 持续预训练；3) 通过参数高效微调(PEFT)进行指令调优。

Result: Persian-Phi在HuggingFace的Open Persian LLM Leaderboard上取得了有竞争力的结果，尽管模型尺寸紧凑。

Conclusion: 研究提供了一个经过验证的可扩展框架，能够以最小硬件资源将最先进的LLM扩展到资源不足的语言，挑战了强大多语言能力需要大模型或多语言基线的假设。

Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [53] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

TL;DR: NPR是一个无需教师指导的框架，让大语言模型自我进化出真正的并行推理能力，通过自蒸馏训练、并行感知策略优化和专用引擎实现从顺序模拟到原生并行认知的转变。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要采用顺序推理模式，限制了推理效率和能力。虽然有一些并行推理尝试，但往往退回到自回归解码，缺乏真正的原生并行认知能力。需要让模型自我进化出真正的并行推理能力。

Method: 1) 自蒸馏渐进训练范式：从"冷启动"格式发现过渡到严格拓扑约束；2) 并行感知策略优化算法：在执行图中直接优化分支策略，通过试错学习自适应分解；3) NPR引擎：重构SGLang的内存管理和流程控制，支持稳定的大规模并行强化学习训练。

Result: 在8个推理基准测试中，基于Qwen3-4B训练的NPR实现了高达24.5%的性能提升和4.6倍的推理加速。与基线方法不同，NPR展示了100%的真正并行执行，而不是退回到自回归解码。

Conclusion: NPR为大语言模型建立了自我进化、高效、可扩展的智能推理新标准，实现了从顺序模拟到原生并行认知的转变，在性能和效率方面都取得了显著突破。

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [54] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

TL;DR: 论文提出PRS和VSPO两种技术，解决工具集成推理LLM在强化学习中的稀疏奖励和梯度退化问题，提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 工具集成推理的LLM在强化学习优化时面临两个关键挑战：1）稀疏的二元奖励信号对中间步骤指导有限，收敛缓慢；2）GRPO中相同奖励导致零优势估计，降低样本效率和训练稳定性。

Method: 提出两种互补技术：1）渐进奖励塑造（PRS），采用课程式奖励设计，引入密集的阶段反馈；2）基于价值的采样策略优化（VSPO），增强GRPO变体，通过任务价值指标选择提示并应用价值平滑裁剪。

Result: 在多个短式和长式QA基准测试中，PRS持续优于传统二元奖励，VSPO相比PPO、GRPO、CISPO和SFT基线表现出更好的稳定性、更快的收敛和更高的最终性能。

Conclusion: PRS和VSPO相结合产生了跨领域泛化能力更强的基于LLM的工具集成推理智能体，有效解决了稀疏奖励和梯度退化问题。

Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [55] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

TL;DR: SPAD：一种新的RAG幻觉检测方法，通过将token概率分解为七个来源并进行词性标注聚合，有效识别生成中的异常模式


<details>
  <summary>Details</summary>
Motivation: 现有方法将RAG中的幻觉归因于内部知识（FFN）与检索上下文之间的二元冲突，但这种视角不完整，忽略了用户查询、已生成token、当前token本身和最终LayerNorm调整等其他生成组件的影响

Method: SPAD方法：1）数学上将每个token的概率归因到七个不同来源：查询、RAG、过去token、当前token、FFN、最终LayerNorm和初始嵌入；2）通过词性标注（POS tags）聚合这些分数，量化不同组件如何驱动特定语言类别

Result: 通过识别异常模式（如名词过度依赖最终LayerNorm），SPAD能有效检测幻觉。大量实验表明SPAD达到了最先进的性能

Conclusion: SPAD提供了一种更全面的RAG幻觉检测框架，超越了传统的二元冲突视角，通过细粒度的概率分解和语言类别分析实现了更准确的幻觉识别

Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [56] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Björn Deiseroth*

Main category: cs.CL

TL;DR: LIME是一种通过将语法、语义和上下文属性的元数据嵌入到token表示中来提升语言模型预训练效率的方法，能加速56%的适应速度，仅增加0.01%参数，并显著改善语言建模和生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 高质量预训练数据日益稀缺，而元数据通常仅用于数据集创建和整理，其作为直接训练信号的潜力尚未充分探索。本文挑战这一现状，探索元数据作为直接训练信号的可能性。

Method: 提出LIME方法，将捕获语法、语义和上下文属性的元数据直接嵌入到token表示中，丰富token嵌入。还开发了LIME+1变体，使用偏移的元数据来指导token生成。

Result: LIME使模型对训练数据分布的适应速度提升高达56%，仅增加0.01%参数且计算开销可忽略。显著改善分词质量，增强语言建模能力和生成任务性能，这些优势在500M到2B不同规模模型中都保持。LIME+1变体在给定下一个token的元数据时，推理性能提升高达38%，算术准确率提升高达35%。

Conclusion: 元数据作为直接训练信号能显著提升预训练效率和模型性能，LIME方法证明了元数据嵌入的有效性，为语言模型预训练提供了新的优化方向。

Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [57] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出RoPE的扩展方法，重新引入被丢弃的虚部信息，通过双组件注意力分数增强长上下文依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 标准RoPE实现只使用复数点积的实部计算注意力分数，丢弃了包含重要相位信息的虚部，这可能导致长上下文依赖建模中关系细节的损失。

Method: 提出扩展方法，重新整合被丢弃的虚部信息，利用完整的复数表示创建双组件注意力分数，保留更多位置信息。

Result: 在长上下文语言建模基准测试中，该方法相比标准RoPE持续提升性能，且随着上下文长度增加，优势更加显著。

Conclusion: 通过重新引入复数点积的虚部信息，可以增强RoPE对长上下文依赖的建模能力，提升LLMs在长序列任务上的表现。

Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [58] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文提出了SwissGov-RSD数据集，这是首个用于跨语言文档级语义差异识别的自然数据集，包含224个多语言平行文档，并评估了多种LLM和编码器模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 识别跨文档（尤其是不同语言间）的语义差异对于文本生成评估和多语言内容对齐至关重要，但作为独立任务却很少受到关注。

Method: 构建了SwissGov-RSD数据集，包含224个英语-德语、英语-法语、英语-意大利语的多语言平行文档，并进行了token级别的差异标注。评估了多种开源和闭源大语言模型以及编码器模型在不同微调设置下的表现。

Result: 当前自动方法在该新基准上的表现远差于在单语、句子级和合成基准上的表现，揭示了LLM和编码器模型在该任务上存在显著差距。

Conclusion: 跨语言文档级语义差异识别是一个具有挑战性的任务，现有模型表现不佳，需要进一步研究。作者公开了代码和数据集以促进该领域发展。

Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [59] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

TL;DR: 本文提出在生成式错误跨度检测（ESD）任务中使用最小贝叶斯风险（MBR）解码替代传统的最大后验（MAP）解码，以解决模型概率与人类标注相似度不匹配的问题，并通过MBR蒸馏降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有生成式ESD方法通常使用MAP解码，假设模型估计的概率与人类标注相似度完美相关。但作者观察到，与人类标注不相似的标注可能获得更高的模型似然度，这表明MAP解码存在局限性。

Method: 提出在生成式ESD模型中应用MBR解码，使用句子级和跨度级相似度度量作为效用函数，基于候选假设与人类标注的近似相似度进行选择。同时引入MBR蒸馏技术，让标准贪婪模型匹配MBR解码性能，解决推理时延问题。

Result: 实验结果表明，MBR解码在系统级、句子级和跨度级均优于MAP基线。MBR蒸馏有效消除了推理时延瓶颈，使标准贪婪模型达到与MBR解码相当的性能。

Conclusion: MBR解码能够更好地处理生成式ESD任务中模型概率与人类标注相似度不匹配的问题，而MBR蒸馏技术则解决了MBR解码的计算成本问题，为实际应用提供了可行方案。

Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [60] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

TL;DR: 本研究重新检验了基本词汇中语音特征的统计过表征现象，通过扩大语言样本（2864种语言）并加入谱系和地域控制，发现先前研究中的多数模式并不稳健，只有少数模式保持稳定。


<details>
  <summary>Details</summary>
Motivation: 先前关于基本词汇中语音象征现象的研究存在方法学问题：样本偏误、未充分控制语言间的谱系和地域依赖关系，导致结果的稳健性存疑。本研究旨在通过更严谨的方法检验这些发现的可靠性。

Method: 使用Lexibank数据库的2864种语言数据，扩大样本规模；修改原始模型，加入空间和谱系依赖关系的统计控制；重新分析基本词汇中的语音象征模式。

Result: 大多数先前观察到的语音象征模式在加入谱系和地域控制后不再稳健，甚至完全消失；只有少数模式表现出高度稳定性；能够在更大规模上评估语音象征的分布。

Conclusion: 语音象征现象可能比先前认为的更有限；研究强调了检验语言普遍性主张时需要在多个层面上进行稳健性测试的重要性；只有少数语音象征模式能经受住严格的统计控制。

Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


### [61] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

TL;DR: MoCoRP是一个用于基于角色的对话生成框架，通过显式建模角色句子与回复之间的NLI关系，提升对话的一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于角色的对话数据集缺乏角色句子与回复之间的显式关系，导致模型难以有效捕捉角色信息，影响对话的一致性和质量。

Method: 提出MoCoRP框架，利用NLI专家显式提取角色句子与回复之间的NLI关系，使模型能够有效将适当的角色信息融入回复中。该框架应用于BART等预训练模型，并通过对齐调优扩展到现代大语言模型。

Result: 在ConvAI2和MPChat数据集上的实验表明，MoCoRP优于现有基线，在角色一致性和上下文感知对话生成方面表现优异，不仅在定量指标上出色，在定性方面也有显著改进。

Conclusion: 显式建模角色-回复关系在基于角色的对话中非常有效，MoCoRP框架能够显著提升对话系统生成一致且吸引人的角色化回复能力。

Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [62] [Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries](https://arxiv.org/abs/2512.07552)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: SafeTerm AMQ是一个基于AI的自动医疗查询系统，用于从MedDRA术语中检索相关不良事件术语，在SMQs验证中表现出良好的召回率和可调节的精确度。


<details>
  <summary>Details</summary>
Motivation: 在药物安全审查中，将相关不良事件术语分组到SMQs或OCMQs对于信号检测至关重要。需要自动化工具来高效处理MedDRA术语查询。

Method: SafeTerm AMQ将医疗查询术语和MedDRA PTs嵌入多维向量空间，应用余弦相似度和极值聚类生成排名列表。通过多标准统计方法计算相关性分数(0-1)。在110个SMQs上进行验证，计算不同相似度阈值下的精确率、召回率和F1分数。

Result: 在中等相似度阈值下实现高召回率(94%)，较高阈值下精确度可达89%。最佳阈值(0.70)下整体召回率48%、精确率45%。窄术语PTs在更高阈值下表现更好。自动阈值选择(0.66)优先召回率(0.58)而非精确率(0.29)。

Conclusion: SafeTerm AMQ在SMQs和OCMQs上表现相当且令人满意，是自动MedDRA查询生成的可行补充方法，能平衡召回率和精确度。建议使用合适的MedDRA PT术语进行查询，并应用自动阈值方法优化召回率。

Abstract: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.

</details>


### [63] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

TL;DR: 提出一种简单方法，通过lasso特征选择从语音标记中提取关键信息，增强文本预训练大语言模型在特定分类任务中的性能


<details>
  <summary>Details</summary>
Motivation: 传统音频-文本融合方法面临音频序列过长、语音标记器输出序列过长且词汇量大、难以低成本集成到大语言模型中的问题。特别是在某些任务（如论证谬误检测）中，音频信息被认为可能对性能产生负面影响

Method: 使用基于lasso的特征选择方法，在多模态词袋表示中保留对任务最重要的音频标记，然后通过自监督语言建模目标使语言模型适应这些标记，最后在下游任务上进行微调

Result: 相比单模态模型、更大的SpeechLM模型或通过学习表示集成音频的方法，该方法能提升性能。在两个论证谬误检测与分类任务中达到最先进结果，即使随机选择音频标记也能增强单模态模型

Conclusion: 提出的简单方法能有效增强文本预训练大语言模型，利用语音信息提升特定分类任务性能，特别是在音频信息被认为可能有害的任务中也能取得显著改进

Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [64] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

TL;DR: 提出一种结构化方法，通过思维链和少样本学习提示，将LLMs以低成本、高效方式整合到定量研究中，实现人机协作互补


<details>
  <summary>Details</summary>
Motivation: 解决如何在定量研究中有效利用大型语言模型，同时克服其固有弱点，实现人机优势互补，让学者能够以低成本方式管理LLMs的局限性

Method: 采用思维链和少样本学习提示技术，将定性研究中合作团队的最佳实践扩展到定量研究的人机协作中，允许人类使用溯因推理和自然语言审查机器和人类的工作

Result: 方法成功应用于分析1990-2017年间1,934份制药联盟新闻稿中的人机评级差异，展示了如何有效管理LLMs的弱点

Conclusion: 通过结构化的人机协作方法，学者能够以低成本方式利用LLMs的优势，同时有效管理其弱点，为定量研究提供了新的研究范式

Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [65] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

TL;DR: 提出Metric-Fair Prompting框架，通过度量公平性约束指导LLM在医疗问答中做出决策，利用问题相似性和Lipschitz约束确保个体公平性，在MedQA基准上提升性能。


<details>
  <summary>Details</summary>
Motivation: 在医疗问答等高风险应用中，需要确保LLM决策的公平性，特别是保证相似问题得到相似处理（个体公平性）。现有方法通常孤立处理每个问题，缺乏对相似实例一致性的考虑。

Method: 将(问题,选项)对视为二元实例，使用NLP嵌入计算问题相似性，在相似问题对中联合处理而非孤立处理。提示框架强制提取决定性临床特征，将每个对映射到置信度分数f(x)，并施加Lipschitz式约束确保相似输入获得相似分数和一致输出。

Result: 在MedQA(US)基准测试中，Metric-Fair Prompting相比标准单项目提示方法提升了性能，表明公平引导、置信度导向的推理能够增强LLM在高风险临床多选题上的准确性。

Conclusion: 提出的度量公平提示框架通过强制相似问题获得一致处理，不仅提升了LLM决策的公平性，还意外地提高了准确性，为高风险医疗应用中的LLM部署提供了有前景的方法。

Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [66] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

TL;DR: PCMind-2.1-Kaiyuan-2B是一个完全开源的20亿参数模型，通过创新的数据混合策略、选择性重复训练和多领域课程训练方法，在资源受限条件下实现了高效的预训练。


<details>
  <summary>Details</summary>
Motivation: 解决开源社区与产业界之间因闭源高质量数据和训练方法造成的知识差距，为资源有限的环境提供实用的预训练解决方案。

Method: 1. 分位数数据基准测试方法：系统比较异构开源数据集并提供数据混合策略；2. 战略选择性重复方案：在多阶段范式中有效利用稀疏高质量数据；3. 多领域课程训练策略：按质量排序样本；4. 高度优化的数据预处理流程和FP16稳定性架构修改。

Result: Kaiyuan-2B在性能上与最先进的完全开源模型竞争，展示了在资源受限预训练中的实用性和可扩展性。

Conclusion: 该工作提供了完全开源的解决方案（模型权重、数据和代码），通过创新的训练方法在资源受限条件下实现了高效的大语言模型预训练，有助于缩小开源社区与产业界的差距。

Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [67] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

TL;DR: CGBridge：一种通过外部可训练桥接模块将代码图信息注入LLM的即插即用方法，提升代码结构理解能力


<details>
  <summary>Details</summary>
Motivation: LLM在代码智能任务中表现优异，但依赖线性化token序列限制了其对程序结构语义的理解能力。现有方法存在提示长度限制或需要特定架构修改的问题，难以与大规模指令跟随LLM兼容。

Method: 提出CGBridge方法：1）在大规模27万代码图数据集上自监督预训练代码图编码器学习结构语义；2）训练外部模块通过跨模态注意力对齐代码、图和文本语义；3）桥接模块生成结构感知提示注入冻结的LLM，微调下游任务。

Result: 在代码摘要任务上相对原始模型提升16.19%，相对图增强提示方法提升9.12%；在代码翻译任务上执行准确率分别提升9.84%和38.87%；推理速度比LoRA调优模型快4倍以上。

Conclusion: CGBridge通过外部桥接模块有效将代码结构信息注入LLM，在保持与现有LLM兼容性的同时显著提升代码理解性能，兼具高效性和有效性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [68] [When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks](https://arxiv.org/abs/2512.07684)
*Zihan Chen,Lanyu Yu*

Main category: cs.CL

TL;DR: 提出基于图神经网络（GNN）的框架，用于检测英文维基百科社区中的三种不文明行为（毒性、攻击性、人身攻击），通过文本相似性构建评论间的关系图，结合动态注意力机制，在准确性和效率上优于12个大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 在线不文明行为已成为数字社区中普遍且持久的问题，给用户带来沉重的社会和心理负担。现有平台通过人工审核和自动检测来遏制不文明行为，但现有方法在准确性和效率上仍然有限。

Method: 提出图神经网络（GNN）框架，将每个用户评论表示为节点，基于评论间的文本相似性定义边，使网络能够同时学习语言内容和评论间的关系结构。引入动态调整的注意力机制，在信息聚合过程中自适应地平衡节点特征和拓扑特征。

Result: 实证评估表明，所提出的架构在多个指标上优于12个最先进的大型语言模型（LLMs），同时推理成本显著降低。这些发现突显了结构上下文在检测在线不文明行为中的关键作用，并解决了纯文本LLM范式在行为预测中的局限性。

Conclusion: 该研究强调了结构上下文在在线不文明行为检测中的重要性，提出的GNN框架在准确性和效率上优于现有LLM方法。所有数据集和比较输出将公开提供，以支持进一步的研究和可重复性。

Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.

</details>


### [69] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

TL;DR: 该论文提出了HalluShift++方法，通过分析MLLM内部层动态中的异常来检测多模态大语言模型的幻觉问题，相比依赖外部LLM评估器的方法更可靠。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言理解任务中表现出色，但经常产生与视觉内容事实不一致的幻觉描述，这可能带来严重后果。现有方法主要依赖外部LLM评估器，但这些评估器本身也存在幻觉问题且存在领域适应挑战。

Method: 提出HalluShift++方法，基于假设：幻觉表现为MLLM内部层动态中的可测量异常，而不仅仅是分布偏移。该方法通过层间分析特定假设，将幻觉检测从文本LLMs扩展到多模态场景。

Result: HalluShift++能够有效检测多模态大语言模型中的幻觉问题，代码已在GitHub开源。

Conclusion: 通过分析MLLM内部层动态异常来检测幻觉是可行的，HalluShift++为多模态场景下的幻觉检测提供了更可靠的方法，减少了对易受幻觉影响的外部LLM评估器的依赖。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [70] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: SafeTerm是一个AI驱动的系统，通过将医学术语和MedDRA术语嵌入向量空间，使用余弦相似度和极值聚类自动检索和排名相关的不良事件术语，为药物安全审查提供自动化MedDRA查询生成方法。


<details>
  <summary>Details</summary>
Motivation: 在药物上市前安全审查中，将相关不良事件术语分组到标准化MedDRA查询或FDA OCMQs中对于信号检测至关重要。传统方法需要人工操作，效率低下且可能存在不一致性，因此需要自动化解决方案。

Method: SafeTerm系统将医学查询术语和MedDRA首选术语嵌入到多维向量空间中，然后应用余弦相似度和极值聚类来生成按相关性分数排名的术语列表。系统使用多标准统计方法对术语进行排名。

Result: 在FDA OCMQ v3.0（104个查询）上进行验证，结果显示：中等阈值下召回率>95%，更高阈值下精确度可达86%。最佳阈值（~0.70-0.75）下召回率约50%，精确度约33%。窄术语子集表现类似但需要稍高相似度阈值。

Conclusion: SafeTerm AI驱动系统为自动化MedDRA查询生成提供了可行的补充方法。建议初始使用约0.60的相似度阈值，在细化术语选择时可增加阈值。该系统能够有效支持药物安全审查中的信号检测工作。

Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [71] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs能识别不连贯叙事但无法可靠区分连贯与不连贯故事，在叙事连贯性理解上存在缺陷


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能可靠区分连贯与不连贯的叙事，探索模型对故事连贯性的理解能力

Method: 使用配对叙事数据集，通过探测研究和多种提示变体测试LLMs的识别能力，分析内部表征与生成响应的差异

Result: LLMs内部表征能识别不连贯叙事，但生成响应无法可靠区分；对违反设定的不连贯更敏感，对角色特质违反不敏感；思维链无法消除这些缺陷

Conclusion: LLMs对叙事连贯性的理解不完整，更依赖原型世界知识而非基于意义的叙事连贯性构建

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [72] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

TL;DR: 该研究通过完全受控的实验框架，揭示了预训练、中期训练和RL后训练在语言模型推理能力发展中的因果贡献，发现RL仅在预训练留有足够提升空间且针对模型能力边界任务时才产生真正能力增益。


<details>
  <summary>Details</summary>
Motivation: 当前RL技术虽然提升了语言模型的推理能力，但难以确定这种提升是真正扩展了模型能力还是仅利用了预训练获得的知识。由于现代训练流程缺乏控制（预训练语料不透明、中期训练未充分研究、RL目标与未知先验知识复杂交互），需要开发受控实验框架来澄清这些训练阶段的因果贡献。

Method: 开发了完全受控的实验框架，使用合成推理任务（具有明确的原子操作、可解析的逐步推理痕迹），系统性地操纵训练分布。从两个维度评估模型：1）外推泛化到更复杂的组合；2）跨表面上下文的语境泛化。通过该框架分离预训练、中期训练和RL后训练的因果贡献。

Result: 1) RL仅在预训练留有足够提升空间且针对模型能力边界任务时才产生真正能力增益(pass@128)；2) 语境泛化需要最小但充分的预训练暴露，之后RL可以可靠地迁移；3) 在固定计算量下，中期训练相比仅用RL能显著提升性能；4) 过程级奖励减少奖励攻击并提高推理保真度。

Conclusion: 该研究阐明了预训练、中期训练和RL之间的相互作用，为理解和改进推理语言模型的训练策略提供了基础。RL的有效性取决于预训练留下的提升空间和针对模型能力边界的任务，而中期训练在训练流程中扮演着核心但未被充分探索的角色。

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [73] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

TL;DR: 论文提出"协作因果意义建构(CCS)"作为决策支持智能体的研究议程，强调AI应作为认知工作伙伴参与专家决策过程，而非仅仅是工具。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在复杂高风险决策环境中未能实现人机互补优势，人类-AI团队表现常低于最佳个体，专家在验证循环和过度依赖间摇摆。问题不仅是准确性，更是AI辅助理念的根本缺陷。

Method: 提出协作因果意义建构(CCS)框架，将AI系统设计为认知工作伙伴：维护专家推理的演化模型，帮助阐明和修订目标，共同构建和压力测试因果假设，从联合决策结果中学习，实现人机共同进步。

Result: 提出新的研究议程和组织框架，挑战包括：创建使协作思考具有工具价值的训练生态，开发共同建模的表示和交互协议，建立以信任和互补性为中心的评价体系。

Conclusion: CCS方向可重构MAS研究，使智能体参与协作意义建构，成为与人类伙伴共同思考的AI队友，实现真正的人机互补决策支持。

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [74] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

TL;DR: 研究发现LLM在不同OOD测试集上的泛化性能相关性没有统一趋势，相关性正负取决于具体模型选择


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM泛化能力的方法通常只关注单个OOD数据集，无法准确评估模型在实际部署中面对多样化数据偏移的能力

Method: 在微调过程中评估模型在多个OOD测试集上的性能，通过偏相关分析控制域内性能后，分析不同OOD测试集之间的性能相关性

Result: 分析OLMo2和OPT模型发现，不同OOD测试集之间的泛化性能相关性没有统一趋势，相关性正负强烈依赖于具体分析的模型

Conclusion: LLM的OOD泛化结果本身并不具有普遍性，评估模型泛化能力需要考虑多个OOD测试集，因为不同模型在不同数据偏移下的表现差异很大

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [75] [POrTAL: Plan-Orchestrated Tree Assembly for Lookahead](https://arxiv.org/abs/2512.06002)
*Evan Conway,David Porfirio,David Chan,Mark Roberts,Laura M. Hiatt*

Main category: cs.RO

TL;DR: POrTAL：一种新的轻量级概率规划算法，结合FF-Replan和POMCP的优势，在部分可观察环境中快速生成高效任务计划


<details>
  <summary>Details</summary>
Motivation: 在人类-机器人交互中，机器人通常面临部分可观察环境，需要在不完全信息下进行规划。现有概率规划算法要么计算资源需求高，要么需要过多步骤才能达成目标，需要更高效的解决方案。

Method: 提出POrTAL算法，结合FF-Replan和POMCP两种基线规划算法的优势，设计为轻量级概率规划方法，专门针对机器人有限计算资源进行优化。

Result: 在系列案例研究中，POrTAL能够快速找到解决方案，在步骤数量上优于FF-Replan和POMCP基线算法，并在不同时间约束下表现出良好性能。

Conclusion: POrTAL为部分可观察环境下的机器人任务规划提供了高效解决方案，平衡了计算效率和规划质量，适合资源受限的机器人系统。

Abstract: Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.

</details>


### [76] [Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models](https://arxiv.org/abs/2512.06017)
*Laurence Liang*

Main category: cs.RO

TL;DR: 使用前沿视觉语言模型(VLMs)从单张图像估计机器人手臂关节角度，建立了性能基准，发现测试时缩放或参数缩放单独使用不能改善预测效果。


<details>
  <summary>Details</summary>
Motivation: 随着机器人手臂在工业和家庭应用中的普及，可靠的关节角度估计能提高安全性和性能，并可作为验证器来进一步训练机器人策略。

Method: 使用前沿视觉语言模型(VLMs)作为"即用型"工具，从单张目标图像估计机器人手臂关节角度，在合成和真实世界图像数据对上评估。

Result: 建立了当前前沿语言模型(FLMs)的性能基准，实证结果表明测试时缩放或参数缩放单独使用不会改善关节角度预测。

Conclusion: 视觉语言模型可作为机器人手臂关节角度估计的有效工具，但需要更复杂的缩放策略来进一步提升性能。

Abstract: Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.

</details>


### [77] [Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction](https://arxiv.org/abs/2512.06038)
*Kelsey Fontenot,Anjali Gorti,Iva Goel,Tonio Buonassisi,Alexander E. Siemenn*

Main category: cs.RO

TL;DR: 开发自动化基板处理交换系统(ASHE)，结合机器人、双执行器分配器和深度学习计算机视觉，实现自驱动实验室中透明脆弱基板的自动处理与错误检测纠正，达到98.5%首次放置准确率。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室(SDLs)在化学和材料发现中加速了实验自动化，但基板处理和重新加载这一关键步骤常被忽视，限制了SDL的完全自动化能力。

Method: 开发ASHE系统，结合机器人技术、双执行器分配器和深度学习驱动的计算机视觉，用于检测和纠正脆弱透明基板操作中的错误。

Result: 在130次独立试验中，透明玻璃基板重新加载的首次放置准确率达到98.5%，仅发生两次基板错位，且成功检测为错误并自动纠正。

Conclusion: 通过开发更准确可靠的基板处理方法，提升了自驱动实验室的自动化能力，进一步加速了新型化学和材料发现。

Abstract: Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.

</details>


### [78] [WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving](https://arxiv.org/abs/2512.06112)
*Yifang Xu,Jiahao Cui,Feipeng Cai,Zhihao Zhu,Hanlin Shang,Shan Luan,Mingwang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: WAM-Flow是一种视觉-语言-动作模型，将轨迹规划转化为结构化token空间上的离散流匹配，通过并行双向去噪实现可调节的计算精度权衡，在自动驾驶任务中超越自回归和扩散基线。


<details>
  <summary>Details</summary>
Motivation: 现有自回归解码器在轨迹规划中计算效率低且难以平衡精度与计算成本，需要一种能够并行生成、支持粗到细优化、且能整合安全与舒适度奖励的新范式。

Method: 1) 使用度量对齐数值分词器通过三元组边界学习保留标量几何；2) 几何感知流目标；3) 模拟器引导的GRPO对齐整合安全、进度和舒适度奖励；4) 多阶段适配将预训练自回归主干转换为非因果流模型；5) 通过持续多模态预训练增强道路场景能力。

Result: 在NAVSIM v1基准测试中，1步推理达到89.1 PDMS，5步推理达到90.3 PDMS，超越自回归和基于扩散的VLA基线，展示了离散流匹配在端到端自动驾驶中的潜力。

Conclusion: 离散流匹配是端到端自动驾驶的新有前景范式，WAM-Flow通过并行双向去噪和可调节计算精度权衡，在保持安全与舒适度的同时实现了优越的闭环性能。

Abstract: We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.

</details>


### [79] [Probabilistic Weapon Engagement Zones for a Turn Constrained Pursuer](https://arxiv.org/abs/2512.06130)
*Grant Stagg,Isaac E. Weintraub,Cameron K. Peterson*

Main category: cs.RO

TL;DR: 该论文提出了曲线-直线概率交战区(CSPEZ)的概念，用于量化规避者应避免的空间区域，以减少被具有不确定性的转弯率受限追击者捕获的风险，并开发了在不确定性下最小化捕获风险的轨迹生成方法。


<details>
  <summary>Details</summary>
Motivation: 在追击-规避场景中，追击者通常具有参数不确定性（位置、航向、速度、距离、最大转弯率等），传统的确定性交战区无法有效处理这种不确定性。需要开发能够量化不确定性影响并生成安全规避轨迹的方法。

Method: 首先推导确定性曲线-直线基本交战区(CSBEZ)的解析解，然后扩展到概率框架，使用四种不确定性传播方法：蒙特卡洛采样、线性化、二次近似和神经网络回归。最后将CSPEZ约束集成到轨迹优化算法中生成安全路径。

Result: 评估了四种近似方法的准确性和计算成本，并展示了CSPEZ约束如何集成到轨迹优化算法中，生成能够明确考虑追击者不确定性的安全路径。

Conclusion: CSPEZ为规避者提供了量化空间风险区域的方法，多种不确定性传播方法各有优劣，集成CSPEZ约束的轨迹优化能够有效生成考虑追击者不确定性的安全规避路径。

Abstract: Curve-straight probabilistic engagement zones (CSPEZ) quantify the spatial regions an evader should avoid to reduce capture risk from a turn-rate-limited pursuer following a curve-straight path with uncertain parameters including position, heading, velocity, range, and maximum turn rate. This paper presents methods for generating evader trajectories that minimize capture risk under such uncertainty. We first derive an analytic solution for the deterministic curve-straight basic engagement zone (CSBEZ), then extend this formulation to a probabilistic framework using four uncertainty-propagation approaches: Monte Carlo sampling, linearization, quadratic approximation, and neural-network regression. We evaluate the accuracy and computational cost of each approximation method and demonstrate how CSPEZ constraints can be integrated into a trajectory-optimization algorithm to produce safe paths that explicitly account for pursuer uncertainty.

</details>


### [80] [GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers](https://arxiv.org/abs/2512.06147)
*Hochul Hwang,Soowan Yang,Jahir Sadik Monon,Nicholas A Giudice,Sunghoon Ivan Lee,Joydeep Biswas,Donghyun Kim*

Main category: cs.RO

TL;DR: 开发了GuideNav系统，这是一个仅使用视觉的"教导-重复"导航系统，模仿导盲犬的导航方式，能够在户外环境中实现公里级路径跟随。


<details>
  <summary>Details</summary>
Motivation: 虽然针对盲人和低视力人群的移动辅助系统已有进展，但直接指导机器人导航设计的参考仍然很少。需要填补这一空白，开发更符合用户需求的辅助系统。

Method: 1. 进行综合人类研究：采访26名导盲犬使用者、4名白手杖使用者、9名导盲犬训练师和1名定向行走训练师，观察15+小时导盲犬辅助行走。2. 基于研究见解开发GuideNav系统：仅使用视觉的教导-重复导航系统，构建路径的拓扑表示，结合视觉地点识别与时间滤波，使用相对位姿估计器计算导航动作。

Result: 1. 开源了去标识化数据集。2. GuideNav在五个户外环境中持续实现公里级路径跟随，尽管教导和重复运行之间存在明显的场景变化。3. 用户研究（3名导盲犬使用者和1名训练师）确认了系统可行性，这是首次展示四足移动系统以类似导盲犬的方式检索路径。

Conclusion: GuideNav系统成功展示了仅使用视觉的教导-重复导航在户外环境中的可行性，为盲人和低视力人群开发更符合需求的辅助系统提供了重要参考。

Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.

</details>


### [81] [Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets](https://arxiv.org/abs/2512.06151)
*Ratnangshu Das,Siddhartha Upadhyay,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 提出了一种用于非线性纯反馈系统的实时时空管框架，在动态环境中保证在规定时间内完成"到达-避障-停留"任务


<details>
  <summary>Details</summary>
Motivation: 针对非线性纯反馈系统在动态环境中需要满足时间约束的"到达-避障-停留"任务，现有方法难以处理未知动态和实时要求

Method: 引入实时时空管框架，定义状态空间中时变球体，其中心和半径在线自适应调整；推导出闭式、无近似的控制律将系统输出约束在时空管内

Result: 提供了避障和按时完成任务的形式化保证；通过移动机器人和飞行器在杂乱动态环境中的仿真和硬件实验验证了框架的有效性和可扩展性

Conclusion: 所提出的实时时空管框架能够有效处理非线性纯反馈系统的未知动态，在动态环境中保证在规定时间内完成复杂任务，具有实际应用价值

Abstract: This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.

</details>


### [82] [Situation-Aware Interactive MPC Switching for Autonomous Driving](https://arxiv.org/abs/2512.06182)
*Shuhao Qi,Qiling Aori,Luyao Zhang,Mircea Lazar,Sofie Haesaert*

Main category: cs.RO

TL;DR: 提出基于情境感知的控制器切换策略，在自动驾驶交互场景中平衡性能与计算成本


<details>
  <summary>Details</summary>
Motivation: 自动驾驶交互场景中，高精度交互模型能实现更智能行为但计算成本高，而强交互在交通中相对罕见，需要平衡性能与计算开销的实用策略

Method: 1) 比较研究评估不同MPC公式的交互能力并建立层次结构；2) 开发基于神经网络的分类器，实现不同交互能力控制器之间的情境感知切换

Result: 情境感知切换策略既能通过在罕见但关键情况下激活最先进的交互MPC显著提升整体性能，又能通过在大多数场景中使用基本MPC显著减少计算负载

Conclusion: 提出的情境感知控制器切换方法有效平衡了自动驾驶交互场景中的性能与计算效率，为实际应用提供了实用解决方案

Abstract: To enable autonomous driving in interactive traffic scenarios, various model predictive control (MPC) formulations have been proposed, each employing different interaction models. While higher-fidelity models enable more intelligent behavior, they incur increased computational cost. Since strong interactions are relatively infrequent in traffic, a practical strategy for balancing performance and computational overhead is to invoke an appropriate controller based on situational demands. To achieve this approach, we first conduct a comparative study to assess and hierarchize the interactive capabilities of different MPC formulations. Furthermore, we develop a neural network-based classifier to enable situation-aware switching among controllers with different levels of interactive capability. We demonstrate that this situation-aware switching can both substantially improve overall performance by activating the most advanced interactive MPC in rare but critical situations, and significantly reduce computational load by using a basic MPC in the majority of scenarios.

</details>


### [83] [REWW-ARM -- Remote Wire-Driven Mobile Robot: Design, Control, and Experimental Validation](https://arxiv.org/abs/2512.06192)
*Takahiro Hattori,Kento Kawaharazuka,Temma Suzuki,Keita Yoneda,Kei Okada*

Main category: cs.RO

TL;DR: 提出"远程线驱动"系统，通过远程线传输机制将电机单元与无电子设备远端机器人分离，实现陆地和水下操作


<details>
  <summary>Details</summary>
Motivation: 电子设备限制了机器人的使用环境，需要一种既能保留先进电子控制又能将电子设备排除在操作环境之外的方法

Method: 开发了远程线驱动机器人"REWW-ARM"，包含三个核心组件：远程线传输机制(RWTM)、无电子设备远端移动机器人、以及提供闭环控制的电机单元

Result: 通过实验验证了REWW-ARM在机械和控制性能方面的能力，展示了其在陆地和水下的移动、姿态控制和物体操作能力

Conclusion: 远程线驱动系统有潜力应用于各种类型的机器人，从而扩展其操作范围

Abstract: Electronic devices are essential for robots but limit their usable environments. To overcome this, methods excluding electronics from the operating environment while retaining advanced electronic control and actuation have been explored. These include the remote hydraulic drive of electronics-free mobile robots, which offer high reachability, and long wire-driven robot arms with motors consolidated at the base, which offer high environmental resistance. To combine the advantages of both, this study proposes a new system, "Remote Wire Drive." As a proof-of-concept, we designed and developed the Remote Wire-Driven robot "REWW-ARM", which consists of the following components: 1) a novel power transmission mechanism, the "Remote Wire Transmission Mechanism" (RWTM), the key technology of the Remote Wire Drive; 2) an electronics-free distal mobile robot driven by it; and 3) a motor-unit that generates power and provides electronic closed-loop control based on state estimation via the RWTM. In this study, we evaluated the mechanical and control performance of REWW-ARM through several experiments, demonstrating its capability for locomotion, posture control, and object manipulation both on land and underwater. This suggests the potential for applying the Remote Wire-Driven system to various types of robots, thereby expanding their operational range.

</details>


### [84] [Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation](https://arxiv.org/abs/2512.06198)
*Oussama Sifour,Soulaimane Berkane,Abdelhamid Tayebi*

Main category: cs.RO

TL;DR: 提出一种仅使用IMU、体坐标系向量测量和固定锚点距离测量的单测距辅助导航观测器，用于重建刚体完整状态


<details>
  <summary>Details</summary>
Motivation: 传统导航系统通常需要多个传感器或复杂配置，本文旨在开发一种轻量级、仅需单测距辅助的导航方案，降低系统复杂度同时保持高精度

Method: 采用级联观测器架构：首先构建扩展线性时变系统估计位置、速度和重力方向；然后结合体坐标系向量测量重建完整姿态；基于一致可观性条件建立几乎全局渐近稳定性

Result: 三维轨迹仿真研究表明，该系统能准确估计位置、速度和姿态，验证了单测距辅助作为轻量级有效导航模式的可行性

Conclusion: 单测距辅助导航观测器为自主导航提供了一种简单有效的解决方案，在传感器噪声和轨迹变化下具有鲁棒性，适用于资源受限的自主系统

Abstract: This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.

</details>


### [85] [Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots](https://arxiv.org/abs/2512.06207)
*Harshil Suthar,Dipankar Maity*

Main category: cs.RO

TL;DR: 提出一个多机器人协作框架，其中空中机器人探索未知环境并选择性地将地图信息通过带宽受限信道传输给地面机器人，以优化地面机器人的导航成本。


<details>
  <summary>Details</summary>
Motivation: 在多机器人团队中，空中机器人需要探索未知环境并将地图信息传输给地面机器人，但通信带宽有限，因此需要决定传输什么信息、传输多少以及何时传输，同时还要进行环境探索。

Method: 基于信息价值(VoI)决定传输什么信息，使用混合整数线性规划(MILP)决定传输多少信息，通过基于效用分数的环境探索策略获取额外信息，并进行通信-运动权衡分析。

Result: 框架能够平衡空中机器人传输的地图数据总量与地面机器人导航成本之间的权衡，优化整体系统性能。

Conclusion: 该框架为带宽受限环境下的多机器人协作提供了一种系统化方法，通过信息价值评估和优化决策，实现了通信资源与导航性能的有效平衡。

Abstract: In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.

</details>


### [86] [Safe Model Predictive Diffusion with Shielding](https://arxiv.org/abs/2512.06261)
*Taekyung Kim,Keyvan Majd,Hideki Okamoto,Bardh Hoxha,Dimitra Panagou,Georgios Fainekos*

Main category: cs.RO

TL;DR: Safe MPD：一种免训练的扩散规划器，结合基于模型的扩散框架与安全屏障，生成既满足运动学可行性又安全的最优轨迹，避免后处理修正的常见问题。


<details>
  <summary>Details</summary>
Motivation: 为复杂机器人系统生成安全、运动学可行且最优的轨迹是机器人学中的核心挑战。现有方法常采用后处理修正，但存在计算不可行性和可行性损失等问题。

Method: 提出Safe Model Predictive Diffusion (Safe MPD)，将基于模型的扩散框架与安全屏障统一，在去噪过程中对所有样本强制执行可行性和安全性约束，避免后处理修正。

Result: 在具有挑战性的非凸规划问题上验证，包括运动学和加速度控制的拖拉机-拖车系统。结果显示，在成功率和安全性方面显著优于现有安全策略，同时实现亚秒级计算时间。

Conclusion: Safe MPD通过统一扩散规划与安全屏障，能够生成既安全又运动学可行的轨迹，避免了后处理修正的局限性，在复杂机器人系统中表现出优越性能。

Abstract: Generating safe, kinodynamically feasible, and optimal trajectories for complex robotic systems is a central challenge in robotics. This paper presents Safe Model Predictive Diffusion (Safe MPD), a training-free diffusion planner that unifies a model-based diffusion framework with a safety shield to generate trajectories that are both kinodynamically feasible and safe by construction. By enforcing feasibility and safety on all samples during the denoising process, our method avoids the common pitfalls of post-processing corrections, such as computational intractability and loss of feasibility. We validate our approach on challenging non-convex planning problems, including kinematic and acceleration-controlled tractor-trailer systems. The results show that it substantially outperforms existing safety strategies in success rate and safety, while achieving sub-second computation times.

</details>


### [87] [Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking](https://arxiv.org/abs/2512.06423)
*Leonardo F. Dos Santos,Elisa G. Vergamini,Cícero Zanette,Lucca Maitan,Thiago Boaventura*

Main category: cs.RO

TL;DR: 提出基于端口哈密顿系统的阻抗控制基准测试指标，包括无源性条件和阻抗保真度度量


<details>
  <summary>Details</summary>
Motivation: 需要标准化的阻抗控制基准测试方法，特别是针对笛卡尔空间中的质量-弹簧-阻尼器阻抗控制

Method: 引入因果一致的端口哈密顿模型，推导出可微分、不依赖力/力矩传感的n自由度无源性条件，并基于自由运动阶跃响应功率定义阻抗保真度度量

Result: 在Gazebo仿真中验证了六自由度机械臂和四足机器人腿部，证明端口哈密顿框架适用于标准化阻抗控制基准测试

Conclusion: 端口哈密顿系统为阻抗控制提供了有效的标准化基准测试框架，提出的指标能够评估无源性和动态解耦性能

Abstract: This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.

</details>


### [88] [Fault Tolerant Control of Mecanum Wheeled Mobile Robots](https://arxiv.org/abs/2512.06444)
*Xuehui Ma,Shiliang Zhang,Zhiyong Sun*

Main category: cs.RO

TL;DR: 提出一种处理麦克纳姆轮移动机器人完全和部分执行器故障的容错控制策略，采用后验概率学习实时故障参数，通过概率加权控制律确保鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 麦克纳姆轮移动机器人容易受到执行器故障影响，现有容错控制方案主要针对完全故障（如电机堵转），忽略了部分故障（如扭矩退化），需要一种能同时处理两种故障类型的策略。

Method: 采用后验概率学习实时故障参数，通过聚合预定义故障对应的概率加权控制律来推导容错控制律，确保在不同故障发生水平下的鲁棒性和安全性。

Result: 仿真结果表明，该容错控制策略在多种场景下都能有效工作，验证了其在处理完全和部分执行器故障方面的有效性。

Conclusion: 提出的容错控制策略能够同时处理麦克纳姆轮移动机器人的完全和部分执行器故障，通过概率加权方法确保了系统在各种故障情况下的鲁棒性和安全性。

Abstract: Mecanum wheeled mobile robots (MWMRs) are highly susceptible to actuator faults that degrade performance and risk mission failure. Current fault tolerant control (FTC) schemes for MWMRs target complete actuator failures like motor stall, ignoring partial faults e.g., in torque degradation. We propose an FTC strategy handling both fault types, where we adopt posterior probability to learn real-time fault parameters. We derive the FTC law by aggregating probability-weighed control laws corresponding to predefined faults. This ensures the robustness and safety of MWMR control despite varying levels of fault occurrence. Simulation results demonstrate the effectiveness of our FTC under diverse scenarios.

</details>


### [89] [Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains](https://arxiv.org/abs/2512.06486)
*Wanru Gong,Xinyi Zheng,Xiaopeng Yang,Xiaoqing Zhu*

Main category: cs.RO

TL;DR: 提出ECIM算法，通过熵控制和内在动机结合解决PPO系列算法在四足机器人运动控制中过早收敛的问题，在不同地形上获得更好的稳定性和能效。


<details>
  <summary>Details</summary>
Motivation: PPO系列算法在训练四足机器人运动策略时虽然稳定且样本高效，但经常过早收敛，导致运动性能次优和任务表现下降。

Method: 提出Entropy-Controlled Intrinsic Motivation (ECIM)算法，结合内在动机和自适应探索，通过熵控制减少过早收敛问题。

Result: 在Isaac Gym的六种地形上测试，任务奖励提升4-12%，身体俯仰振荡峰值减少23-29%，关节加速度降低20-32%，关节扭矩消耗减少11-20%。

Conclusion: ECIM通过熵控制和内在动机控制，在不同地形上实现更好的四足运动稳定性，同时降低能量消耗，是复杂机器人控制任务的实用选择。

Abstract: Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.
  For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.

</details>


### [90] [Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments](https://arxiv.org/abs/2512.06517)
*Shifa Sulaiman,Akash Bachhar,Ming Shen,Simon Bøgh*

Main category: cs.RO

TL;DR: 提出一种用于假肢手的视觉引导抓取算法，结合感知、规划和控制系统，实现灵巧操作


<details>
  <summary>Details</summary>
Motivation: 假肢技术需要增强灵巧性和自主性，视觉方法能够在动态环境中实现更自然的物体交互

Method: 使用BVH视觉算法分割物体并定义边界框，通过RRT*算法生成候选轨迹，基于最小欧氏距离选择指尖末端姿态，采用DLS逆运动学求解器计算关节角度

Result: 在仿真和Linker Hand O7平台上验证了该方法，实现了每根手指的独立抓取规划和实时适应性

Conclusion: 提出的模块化管道支持非结构化环境中的实时适应性抓取规划，为智能假肢控制提供了有效解决方案

Abstract: Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.

</details>


### [91] [TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping](https://arxiv.org/abs/2512.06524)
*Saekwang Nam,Bowen Deng,Loong Yi Lee,Jonathan M. Rossiter,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 提出一种基于Fin-Ray结构的触觉传感器手指，通过间接感知方法同时检测接触位置和压入深度，使用内部摄像头和卷积神经网络处理变形模式，实现高精度触觉感知。


<details>
  <summary>Details</summary>
Motivation: 为软体机器人结构提供轻量、灵活且可扩展的触觉感知解决方案，特别是在传感器需要远离接触界面的应用场景中，实现精确的接触位置和深度检测。

Method: 设计带有铰链机构的Fin-Ray手指，将软体结构的变形和位移信息传递到底部横梁上的标记针阵列；使用内部摄像头捕捉变形模式，通过卷积神经网络处理图像来推断接触条件；优化针阵列配置和铰链方向。

Result: 实现了0.1mm深度精度和2mm位置检测精度；感知系统对不同形状和尺寸的压头具有鲁棒泛化能力；在不确定抓取位置的拾放任务中，触觉反馈显著提高了放置精度。

Conclusion: 该工作为软体机器人结构提供了一种有效的触觉感知解决方案，特别适用于传感器需要远离接触界面的应用，通过间接感知方法实现了高精度的接触检测和深度测量。

Abstract: We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.

</details>


### [92] [Embodied Referring Expression Comprehension in Human-Robot Interaction](https://arxiv.org/abs/2512.06558)
*Md Mofijul Islam,Alexi Gladstone,Sujan Sarker,Ganesh Nanduru,Md Fahim,Keyan Du,Aman Chadha,Tariq Iqbal*

Main category: cs.RO

TL;DR: 提出了Refer360数据集和MuRes模块，用于提升机器人在人类环境中对具身指代表达的理解能力


<details>
  <summary>Details</summary>
Motivation: 机器人需要理解具身的人类指令以实现直观流畅的人机交互，但现有数据集存在视角偏差、单视角收集、非语言手势覆盖不足、主要关注室内环境等问题

Method: 1) 创建Refer360数据集：大规模具身语言和非语言交互数据集，涵盖室内外多种视角；2) 提出MuRes模块：多模态引导残差模块，提取模态特定信号并增强预训练表示

Result: 在四个HRI数据集（包括Refer360）上的实验表明，当前多模态模型无法全面捕捉具身交互，但使用MuRes增强后性能持续提升

Conclusion: Refer360成为有价值的基准数据集，引导残差学习有潜力推进机器人在人类环境中对具身指代表达的理解

Abstract: As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.

</details>


### [93] [Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input](https://arxiv.org/abs/2512.06571)
*Zifan Xu,Myoungkyu Seo,Dongmyeong Lee,Hao Fu,Jiaheng Hu,Jiaxun Cui,Yuqian Jiang,Zhihan Wang,Anastasiia Brund,Joydeep Biswas,Peter Stone*

Main category: cs.RO

TL;DR: 提出基于强化学习的四阶段训练框架，使双足机器人能够在感知不确定条件下实现鲁棒的连续踢球


<details>
  <summary>Details</summary>
Motivation: 双足机器人踢球面临快速摆腿、单脚支撑稳定性、噪声感知和外部干扰等挑战，需要鲁棒且适应性强的解决方案

Method: 扩展师生训练框架为四阶段：1)长距离追球(教师)；2)定向踢球(教师)；3)策略蒸馏(学生)；4)适应与精炼(学生)。采用定制奖励函数、真实噪声建模和在线约束强化学习

Result: 在仿真和真实机器人上均表现出强踢球准确性和进球成功率，消融研究验证了约束强化学习、噪声建模和适应阶段的必要性

Conclusion: 建立了在感知不完美条件下学习鲁棒连续踢球的系统，为双足机器人全身控制的视觉运动技能学习提供了基准任务

Abstract: Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a "teacher" policy is trained with ground truth state information and the "student" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.

</details>


### [94] [Error-Centric PID Untrained Neural-Net (EC-PIDUNN) For Nonlinear Robotics Control](https://arxiv.org/abs/2512.06578)
*Waleed Razzaq*

Main category: cs.RO

TL;DR: 提出EC-PIDUNN架构，结合无训练神经网络与改进PID控制器，通过稳定因子τ处理非线性系统控制问题，在机器人应用中优于传统PID


<details>
  <summary>Details</summary>
Motivation: 传统PID控制器在处理非线性动态和复杂互联变量时存在稳定性、超调和调节时间问题，现有PIDNN模型需要大量精细训练数据和计算成本，不适用于实际应用

Method: 提出EC-PIDUNN架构，整合无训练神经网络与改进PID控制器，引入稳定因子τ生成控制信号；使用稳态误差e_t作为输入，无需系统动态知识；通过神经网络增加输入维度，引入参数向量ρ_t塑造输出轨迹，使用动态计算函数调整PID系数

Result: 在非线性机器人应用中验证有效性：(1) 阿克曼转向机制的非线性无人地面车辆系统，(2) 云台运动系统；在两个测试中都优于传统PID，在收敛性和稳定性方面表现更好，实现接近临界阻尼响应

Conclusion: EC-PIDUNN架构成功解决了传统PID在处理非线性系统时的局限性，无需大量训练数据，在实际机器人应用中表现出优越性能

Abstract: Classical Proportional-Integral-Derivative (PID) control has been widely successful across various industrial systems such as chemical processes, robotics, and power systems. However, as these systems evolved, the increase in the nonlinear dynamics and the complexity of interconnected variables have posed challenges that classical PID cannot effectively handle, often leading to instability, overshooting, or prolonged settling times. Researchers have proposed PIDNN models that combine the function approximation capabilities of neural networks with PID control to tackle these nonlinear challenges. However, these models require extensive, highly refined training data and have significant computational costs, making them less favorable for real-world applications. In this paper, We propose a novel EC-PIDUNN architecture, which integrates an untrained neural network with an improved PID controller, incorporating a stabilizing factor (\(τ\)) to generate the control signal. Like classical PID, our architecture uses the steady-state error \(e_t\) as input bypassing the need for explicit knowledge of the systems dynamics. By forming an input vector from \(e_t\) within the neural network, we increase the dimensionality of input allowing for richer data representation. Additionally, we introduce a vector of parameters \( ρ_t \) to shape the output trajectory and a \textit{dynamic compute} function to adjust the PID coefficients from predefined values. We validate the effectiveness of EC-PIDUNN on multiple nonlinear robotics applications: (1) nonlinear unmanned ground vehicle systems that represent the Ackermann steering mechanism and kinematics control, (2) Pan-Tilt movement system. In both tests, it outperforms classical PID in convergence and stability achieving a nearly critically damped response.

</details>


### [95] [A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance](https://arxiv.org/abs/2512.06608)
*Xinyu Zhou,Songhao Piao,Chao Gao,Liguo Chen*

Main category: cs.RO

TL;DR: 本文提出一个统一框架，用于公平评估人群导航方法，并引入强调轨迹曲率优化的新型奖励塑造策略，显著提升轨迹质量和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前人群导航研究存在两个主要问题：1) 评估指标相对优先级分析不足，导致对不同目标方法的评估不公平；2) 轨迹连续性指标（特别是需要C²平滑度的指标）很少被纳入。现有DRL方法通常优先考虑效率和近端舒适度，而忽视轨迹优化或仅使用简单未验证的平滑度奖励。

Method: 提出一个统一框架，支持对导航方法进行公平透明的评估，通过检查多个优化目标的优先级和联合评估。同时提出一种新颖的奖励塑造策略，明确强调轨迹曲率优化。

Result: 通过广泛的2D和3D实验证明，所提出的方法在轨迹质量和适应性方面显著提升，相比最先进方法实现了优越性能。

Conclusion: 有效的轨迹优化对于确保自然性、增强舒适度和最大化导航系统能效至关重要。本文提出的框架和奖励策略能够显著改善人群导航系统的轨迹质量和多场景适应性。

Abstract: Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.

</details>


### [96] [Robust Optimization-based Autonomous Dynamic Soaring with a Fixed-Wing UAV](https://arxiv.org/abs/2512.06610)
*Marvin Harms,Jaeyoung Lim,David Rohr,Friedrich Rockenbauer,Nicholas Lawrance,Roland Siegwart*

Main category: cs.RO

TL;DR: 提出一个用于固定翼无人机自主动态翱翔的框架，利用风场显式表示和鲁棒路径跟踪控制，在仿真和实际飞行中验证了在风切变中实现自主动态翱翔的能力。


<details>
  <summary>Details</summary>
Motivation: 动态翱翔是一种利用风切变层能量的飞行技术，可以实现无需内部能源的无限飞行。然而，实现自主动态翱翔需要解决风场估计误差和鲁棒控制等挑战。

Method: 提出一个自主动态翱翔框架，包括：1）使用风场显式表示；2）构建点对点鲁棒参考路径以应对风场估计误差；3）开发固定翼无人机的鲁棒路径跟踪控制器。

Result: 在仿真中展示了在各种风条件、估计误差和干扰下的鲁棒动态翱翔飞行。实际飞行测试进一步验证了能量预测和路径跟踪鲁棒性等关键组件，表明仿真与实际差距很小。

Conclusion: 该框架能够实现固定翼无人机在风切变中的自主动态翱翔飞行，为无能源限制的飞行提供了可行的技术方案。

Abstract: Dynamic soaring is a flying technique to exploit the energy available in wind shear layers, enabling potentially unlimited flight without the need for internal energy sources. We propose a framework for autonomous dynamic soaring with a fixed-wing unmanned aerial vehicle (UAV). The framework makes use of an explicit representation of the wind field and a classical approach for guidance and control of the UAV. Robustness to wind field estimation error is achieved by constructing point-wise robust reference paths for dynamic soaring and the development of a robust path following controller for the fixed-wing UAV. The framework is evaluated in dynamic soaring scenarios in simulation and real flight tests. In simulation, we demonstrate robust dynamic soaring flight subject to varied wind conditions, estimation errors and disturbances. Critical components of the framework, including energy predictions and path-following robustness, are further validated in real flights to assure small sim-to-real gap. Together, our results strongly indicate the ability of the proposed framework to achieve autonomous dynamic soaring flight in wind shear.

</details>


### [97] [MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment](https://arxiv.org/abs/2512.06628)
*Ruicheng Zhang,Mingyang Zhang,Jun Zhou,Zhangrui Guo,Xiaofan Liu,Zunnan Xu,Zhizhou Zhong,Puxin Yan,Haocheng Luo,Xiu Li*

Main category: cs.RO

TL;DR: MIND-V是一个用于生成长时程机器人操作视频的分层框架，通过语义推理、行为语义桥接和视频生成三个核心组件，结合物理一致性奖励机制，实现了物理合理且逻辑连贯的视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模仿学习面临长时程、多样化操作数据稀缺的问题，现有视频生成模型只能合成短片段简单动作且依赖手动定义轨迹，需要能够生成物理合理、逻辑连贯的长时程机器人操作视频的解决方案。

Method: 提出MIND-V分层框架：1）语义推理中心（SRH）使用预训练视觉语言模型进行任务规划；2）行为语义桥接（BSB）将抽象指令转换为领域不变表示；3）运动视频生成器（MVG）进行条件视频渲染。采用分阶段视觉未来展开测试时优化策略，并通过GRPO强化学习后训练和物理预见一致性（PFC）奖励确保物理合理性。

Result: MIND-V在长时程机器人操作视频生成任务上达到了最先进的性能，建立了一个可扩展且可控的具身数据合成范式。

Conclusion: MIND-V通过分层架构和物理一致性约束，成功解决了长时程机器人操作视频生成的挑战，为具身数据合成提供了可扩展的解决方案。

Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.

</details>


### [98] [Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving](https://arxiv.org/abs/2512.06664)
*Wei-Bin Kou,Guangxu Zhu,Jingreng Lei,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

TL;DR: 提出MoE-RAM：基于大模型的统计增强解耦专家路由与聚合机制，用于自动驾驶语义分割任务


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景复杂多样，单一深度学习模型难以覆盖所有条件（天气、交通密度、道路类型等）。现有MoE方法存在路由不精确和聚合效率低的问题。

Method: 提出MoE-RAM：1）通过统计检索机制增强专家路由，匹配大模型提取的潜在特征与缓存的专家原型特征；2）通过测量专家即时特征与大模型潜在特征的统计距离，自适应重加权专家输出融合。

Result: 在自动驾驶数据集上的大量实验表明，MoE-RAM优于其他MoE基线和传统单模型方法。

Conclusion: 统计增强的MoE路由与聚合机制能协同提升预测性能，为复杂自动驾驶场景提供有效解决方案。

Abstract: Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.

</details>


### [99] [FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving](https://arxiv.org/abs/2512.06676)
*Wei-Bin Kou,Guangxu Zhu,Bingyang Cheng,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

TL;DR: FedDSR 是一种用于自动驾驶联邦学习的新范式，通过中间层监督和正则化解决非独立同分布数据导致的泛化差和收敛慢问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶联邦学习面临非独立同分布数据导致的模型泛化能力差和收敛速度慢的挑战，需要新的方法来提升模型性能。

Method: FedDSR 包含三个核心策略：(1) 基于架构无关标准选择多个中间层；(2) 在选定层计算互信息和负熵作为中间损失和正则项；(3) 基于这些规则聚合车辆模型生成全局模型。

Result: 实验显示 FedDSR 相比其他联邦学习基线，在语义分割任务上实现了最高 8.93% 的 mIoU 提升和 28.57% 的训练轮次减少。

Conclusion: FedDSR 通过中间层监督和正则化有效提升了自动驾驶联邦学习的泛化能力和收敛速度，适合实际部署。

Abstract: Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.

</details>


### [100] [Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization](https://arxiv.org/abs/2512.06754)
*Shrreya Rajneesh,Nikita Pavle,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: 提出无模型控制框架，绕过连续体机械臂的复杂运动学建模，通过在线优化的雅可比矩阵实现精确控制


<details>
  <summary>Details</summary>
Motivation: 连续体机械臂具有无限维变形、未建模内部摩擦和配置相关刚度等问题，导致基于模型的运动学公式不可靠，雅可比预测不准确，存在人工奇点和不稳定驱动行为

Method: 采用经验初始化雅可比矩阵，通过差分凸更新在线优化；通过实时二次规划计算驱动器增量，同时避免肌腱松弛和几何限制；引入背部张力优化项调节轴向载荷并抑制协同激活压缩

Result: 在圆形、五边形和方形轨迹上验证，展示平滑收敛、稳定张力演化和亚毫米级稳态精度，无需任何模型校准或参数识别

Conclusion: 该控制器为受限环境中模型依赖的连续体操作提供了可扩展的替代方案

Abstract: Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.

</details>


### [101] [db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF](https://arxiv.org/abs/2512.06796)
*Akmaral Moldagalieva,Keisuke Okumura,Amanda Prorok,Wolfgang Hönig*

Main category: cs.RO

TL;DR: db-LaCAM结合多智能体路径规划(MAPF)的可扩展性与动力学规划的动态感知能力，通过预计算运动基元和允许用户定义的不连续性，实现高效的多机器人运动规划，可扩展到50个机器人，运行速度提升10倍。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多机器人动力学运动规划器由于计算负担高，难以处理超过几个机器人的情况，限制了可扩展性并导致规划时间缓慢。

Method: 提出discontinuity-Bounded LaCAM (db-LaCAM)，利用预计算尊重机器人动力学的运动基元生成视界长度的运动序列，同时允许连续运动之间的用户定义不连续性。该规划器相对于运动基元是分辨率完备的，支持任意机器人动力学。

Result: db-LaCAM可高效扩展到最多50个机器人的场景，相比最先进的规划器实现高达10倍的运行速度提升，同时保持相当的解决方案质量。在2D和3D环境中验证了单轮车和3D双积分器等动力学模型。

Conclusion: db-LaCAM成功结合了MAPF的可扩展性和动力学规划的动态感知能力，在物理实验中验证了飞行机器人和带拖车汽车机器人团队的安全轨迹执行。

Abstract: State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time.
  In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations.
  To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions.
  The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics.
  Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality.
  The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator.
  We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.

</details>


### [102] [MagicSkin: Balancing Marker and Markerless Modes in Vision-Based Tactile Sensors with a Translucent Skin](https://arxiv.org/abs/2512.06829)
*Oluwatimilehin Tijani,Zhuo Chen,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: MagicSkin是一种新型触觉皮肤，采用半透明染色标记设计，平衡了标记和无标记模式的优点，同时实现切向位移跟踪、力预测和表面细节保留，无需额外硬件或软件工具。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的触觉传感器在标记和无标记设计之间存在根本性权衡：不透明墨水标记可以测量力和切向位移但完全遮挡几何特征，而无标记皮肤保留表面细节但难以有效测量切向位移。现有解决方案需要额外硬件或计算负担。

Method: 提出MagicSkin，一种具有半透明染色标记的新型触觉皮肤设计，平衡标记和无标记模式。该皮肤可直接集成到GelSight系列传感器中，无需额外硬件或软件工具。

Result: MagicSkin在各项任务中表现优异：物体分类（99.17%）、纹理分类（93.51%）、切向位移跟踪（97%点保留）和力预测（总力误差改善66%）。半透明标记不仅没有降低反而提升了传感性能。

Conclusion: 半透明皮肤消除了传统标记或无标记模式的性能权衡，为触觉机器人所需的多模态触觉感知铺平了道路。

Abstract: Vision-based tactile sensors (VBTS) face a fundamental trade-off in marker and markerless design on the tactile skin: opaque ink markers enable measurement of force and tangential displacement but completely occlude geometric features necessary for object and texture classification, while markerless skin preserves surface details but struggles in measuring tangential displacements effectively. Current practice to solve the above problem via UV lighting or virtual transfer using learning-based models introduces hardware complexity or computing burdens. This paper introduces MagicSkin, a novel tactile skin with translucent, tinted markers balancing the modes of marker and markerless for VBTS. It enables simultaneous tangential displacement tracking, force prediction, and surface detail preservation. This skin is easy to plug into GelSight-family sensors without requiring additional hardware or software tools. We comprehensively evaluate MagicSkin in downstream tasks. The translucent markers impressively enhance rather than degrade sensing performance compared with traditional markerless and inked marker design: it achieves best performance in object classification (99.17\%), texture classification (93.51\%), tangential displacement tracking (97\% point retention) and force prediction (66\% improvement in total force error). These experimental results demonstrate that translucent skin eliminates the traditional performance trade-off in marker or markerless modes, paving the way for multimodal tactile sensing essential in tactile robotics. See videos at this \href{https://zhuochenn.github.io/MagicSkin_project/}{link}.

</details>


### [103] [Dynamic Visual SLAM using a General 3D Prior](https://arxiv.org/abs/2512.06868)
*Xingguang Zhong,Liren Jin,Marija Popović,Jens Behley,Cyrill Stachniss*

Main category: cs.RO

TL;DR: 提出一种新颖的单目视觉SLAM系统，能够在动态场景中鲁棒地估计相机位姿，通过结合几何补丁束调整和前馈重建模型来过滤动态区域并增强SLAM鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在动态自然环境中，场景动态会严重降低相机位姿估计的准确性，而可靠的增量式相机位姿估计和3D重建对于机器人、交互式可视化和增强现实等应用至关重要。

Method: 提出一种新颖的单目视觉SLAM系统，结合几何补丁在线束调整和前馈重建模型的互补优势。使用前馈重建模型精确过滤动态区域，并利用其深度预测增强基于补丁的视觉SLAM鲁棒性。通过将深度预测与束调整估计的补丁对齐，鲁棒地处理前馈重建模型批量应用时的固有尺度模糊问题。

Result: 该方法能够在动态场景中鲁棒地估计相机位姿，通过深度预测对齐有效解决了尺度模糊问题，增强了SLAM系统在动态环境中的性能。

Conclusion: 提出的单目视觉SLAM系统通过结合几何束调整和前馈重建模型，成功解决了动态场景中相机位姿估计的挑战，为机器人、AR等应用提供了更可靠的解决方案。

Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.

</details>


### [104] [From Zero to High-Speed Racing: An Autonomous Racing Stack](https://arxiv.org/abs/2512.06892)
*Hassan Jardali,Durgakant Pushp,Youwei Yu,Mahmoud Ali,Ihab S. Mohamed,Alejandro Murillo-Gonzalez,Paul D. Coen,Md. Al-Masrur Khan,Reddy Charan Pulivendula,Saeoul Park,Lingchuan Zhou,Lantao Liu*

Main category: cs.RO

TL;DR: IU Luddy团队为Indy Autonomous Challenge开发了自主赛车堆栈ARS，经过三代迭代，在椭圆和公路赛道上实现最高260km/h速度，并发布了高速多传感器数据集。


<details>
  <summary>Details</summary>
Motivation: 高速头对头自主赛车面临精确定位、快速感知、动态规划和实时控制等技术挑战，同时受限于赛道访问和昂贵硬件。需要开发可靠的自主赛车系统来解决这些实际问题。

Method: 开发了模块化的自主赛车堆栈ARS，包含三代迭代（ARS1、ARS2、ARS3）。系统包括定位、感知、规划和控制模块，在不同类型赛道（椭圆和公路赛道）上进行验证。

Result: ARS系统在真实赛道上实现最高260km/h的速度，完成了椭圆和公路赛道上的性能评估，对比了控制、感知和估计在不同环境下的表现，并发布了高速多传感器数据集。

Conclusion: 通过实际高速全尺寸自主赛车实践，揭示了独特的挑战和见解，为自主赛车技术发展提供了重要参考，发布的系统架构和数据集将促进该领域研究。

Abstract: High-speed, head-to-head autonomous racing presents substantial technical and logistical challenges, including precise localization, rapid perception, dynamic planning, and real-time control-compounded by limited track access and costly hardware. This paper introduces the Autonomous Race Stack (ARS), developed by the IU Luddy Autonomous Racing team for the Indy Autonomous Challenge (IAC). We present three iterations of our ARS, each validated on different tracks and achieving speeds up to 260 km/h. Our contributions include: (i) the modular architecture and evolution of the ARS across ARS1, ARS2, and ARS3; (ii) a detailed performance evaluation that contrasts control, perception, and estimation across oval and road-course environments; and (iii) the release of a high-speed, multi-sensor dataset collected from oval and road-course tracks. Our findings highlight the unique challenges and insights from real-world high-speed full-scale autonomous racing.

</details>


### [105] [Control of Powered Ankle-Foot Prostheses on Compliant Terrain: A Quantitative Approach to Stability Enhancement](https://arxiv.org/abs/2512.06896)
*Chrysostomos Karakasis,Camryn Scully,Robert Salati,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 验证了一种基于导纳的控制策略，通过动态调整动力假肢的准刚度来增强在柔性地面上的步态稳定性，相比刚性地面控制器显著改善了步态稳定性。


<details>
  <summary>Details</summary>
Motivation: 下肢截肢者在柔性地面上行走面临更高的跌倒风险，而现有的动力踝足假肢控制策略主要针对刚性地面优化，对柔性或顺应性表面的控制策略研究不足。

Method: 提出并实验验证了一种基于导纳的控制策略，动态调整动力假肢的准刚度。在三个健康受试者身上进行实验，使用两种双边柔性地面（地面刚度分别为63和25 kN/m），通过相位图和两个行走稳定性指标评估控制器性能。

Result: 相比为刚性地面开发的标准相位变量控制器，提出的导纳控制器在所有柔性条件下都一致改善了步态稳定性，直接降低了跌倒风险。

Conclusion: 自适应、稳定性感知的假肢控制策略在真实环境中具有降低跌倒风险的潜力，能够增强康复机器人中人-假肢交互的鲁棒性。

Abstract: Walking on compliant terrain presents a substantial challenge for individuals with lower-limb amputation, further elevating their already high risk of falling. While powered ankle-foot prostheses have demonstrated adaptability across speeds and rigid terrains, control strategies optimized for soft or compliant surfaces remain underexplored. This work experimentally validates an admittance-based control strategy that dynamically adjusts the quasi-stiffness of powered prostheses to enhance gait stability on compliant ground. Human subject experiments were conducted with three healthy individuals walking on two bilaterally compliant surfaces with ground stiffness values of 63 and 25 kN/m, representative of real-world soft environments. Controller performance was quantified using phase portraits and two walking stability metrics, offering a direct assessment of fall risk. Compared to a standard phase-variable controller developed for rigid terrain, the proposed admittance controller consistently improved gait stability across all compliant conditions. These results demonstrate the potential of adaptive, stability-aware prosthesis control to reduce fall risk in real-world environments and advance the robustness of human-prosthesis interaction in rehabilitation robotics.

</details>


### [106] [Ground Compliance Improves Retention of Visual Feedback-Based Propulsion Training for Gait Rehabilitation](https://arxiv.org/abs/2512.06897)
*Bradley Hobbs,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 研究验证了在视觉反馈步态训练中加入地面顺应性比单独使用视觉反馈更能有效增加推进力，对步态康复有重要意义。


<details>
  <summary>Details</summary>
Motivation: 探索如何更有效地增加推进力以改善步态康复效果，特别是针对中风等疾病导致的推进力缺陷问题。

Method: 10名健康参与者在定制分带跑步机上行走，实时接收地面反作用力的视觉反馈。实验组同时体验地面顺应性变化，对照组仅接受视觉反馈。

Result: 推进力成功增加并在干预后持续，特别是地面顺应性组效果更显著。该组还表现出肌肉活动和关节运动学的持久后效应，表明对推进策略的学习更牢固。

Conclusion: 视觉和本体感觉系统在步态适应中协同工作，地面顺应性与视觉反馈结合能增强推进力的学习效果，支持在长期康复中应用顺应性地面的潜力。

Abstract: This study investigates whether adding ground compliance to visual feedback (VF) gait training is more effective at increasing push-off force (POF) compared to using VF alone, with implications for gait rehabilitation. Ten healthy participants walked on a custom split-belt treadmill. All participants received real-time visual feedback of their ground reaction forces. One group also experienced changes in ground compliance, while a control group received only visual feedback. Intentional increases in propulsive ground reaction forces (POF) were successfully achieved and sustained post-intervention, especially in the group that experienced ground compliance. This group also demonstrated lasting after-effects in muscle activity and joint kinematics, indicating a more robust learning of natural strategies to increase propulsion. This work demonstrates how visual and proprioceptive systems coordinate during gait adaptation. It uniquely shows that combining ground compliance with visual feedback enhances the learning of propulsive forces, supporting the potential use of compliant terrain in long-term rehabilitation targeting propulsion deficits, such as those following a stroke.

</details>


### [107] [Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields](https://arxiv.org/abs/2512.06912)
*Rushiraj Gadhvi,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 基于强化学习的自主水面航行器在涡流场中节能导航方法，相比现有技术节能30-50%


<details>
  <summary>Details</summary>
Motivation: 模仿传统航海家利用洋流导航的智慧，解决自主水面航行器在长期任务中受严格能量预算限制的挑战。传统路径规划方法在部分可观测的涡流场中效果不佳。

Method: 采用基于Soft Actor Critic的端到端强化学习框架，仅使用局部速度测量学习流场感知导航策略。

Result: 在多样化和动态丰富的场景中评估，该方法显示出显著的节能效果，并能鲁棒地泛化到未见过的流场条件，导航路径比现有技术节能30-50%。

Conclusion: 该方法为海洋环境中长期自主导航提供了有前景的路径，实现了能量高效的水面航行器导航。

Abstract: For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.

</details>


### [108] [Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs](https://arxiv.org/abs/2512.06935)
*Nicolò Botteghi,Owen Brook,Urban Fasel,Federico Califano*

Main category: cs.RO

TL;DR: 提出一种基于稀疏字典学习的数值方法，用于设计IDA-PBC控制器，无需精确求解匹配PDEs，通过将问题转化为神经ODE学习，使IDA-PBC能应用于超越稳定的复杂任务。


<details>
  <summary>Details</summary>
Motivation: 传统IDA-PBC方法在实际应用中主要局限于学术示例和稳定化任务，主要瓶颈在于需要解析求解复杂的匹配条件PDEs，这对于复杂物理系统和任务极具挑战性。

Method: 将IDA-PBC问题转化为神经ODE学习，利用稀疏字典学习将期望闭环系统参数化为非线性状态相关函数的稀疏线性组合，通过求解多目标优化问题（包含任务相关成本和匹配条件相关成本）来优化控制器参数。

Result: 数值结果表明，该方法使IDA-PBC能够应用于超越稳定的复杂任务（如发现周期性振荡行为），并能推导出包含残差项的闭环系统封闭形式表达式。

Conclusion: 提出的数值方法克服了传统IDA-PBC解析求解匹配PDEs的局限性，扩展了IDA-PBC的应用范围，使其能够处理更复杂的控制任务。

Abstract: Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.
  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms

</details>


### [109] [Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.06951)
*Ilia Larchenko,Gleb Zarin,Akash Karnatak*

Main category: cs.RO

TL;DR: 提出基于Pi0.5架构的视觉-动作策略，在BEHAVIOR 2025挑战赛中获第一名，通过相关噪声流匹配、可学习混合层注意力等创新，在50个家庭任务中实现26% q-score。


<details>
  <summary>Details</summary>
Motivation: 解决BEHAVIOR挑战中的复杂家庭任务，这些任务需要双手操作、导航和上下文感知决策，现有方法难以处理长期视野和多样化的任务需求。

Method: 基于Pi0.5架构，引入相关噪声流匹配提高训练效率，使用可学习混合层注意力和System 2阶段跟踪解决歧义，训练采用多样本流匹配减少方差，推理时使用动作压缩和挑战特定修正规则。

Result: 在BEHAVIOR 2025挑战赛中获得第一名，在50个多样化家庭任务中实现26% q-score，在公开和私有排行榜上表现一致。

Conclusion: 提出的相关噪声流匹配等创新有效提升了视觉-动作策略在复杂长期家庭任务中的性能，为机器人操作提供了有前景的解决方案。

Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.

</details>


### [110] [VideoVLA: Video Generators Can Be Generalizable Robot Manipulators](https://arxiv.org/abs/2512.06963)
*Yichao Shen,Fangyun Wei,Zhiying Du,Yaobo Liang,Yan Lu,Jiaolong Yang,Nanning Zheng,Baining Guo*

Main category: cs.RO

TL;DR: VideoVLA将大型视频生成模型转化为机器人VLA操作器，通过联合建模视频、语言和动作模态，预测动作序列和未来视觉结果，实现机器人操作的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在泛化到新任务、新物体和新环境方面能力有限，需要探索新的范式来提升机器人操作的泛化能力。

Method: 基于多模态扩散变换器，利用预训练的视频生成模型进行联合视觉和动作预测，给定语言指令和图像，预测动作序列和未来视觉结果。

Result: 高质量的未来视觉想象与可靠的动作预测和任务成功相关，VideoVLA展现出强大的泛化能力，包括模仿其他具身技能和处理新物体。

Conclusion: 同时预测动作及其视觉后果的双重预测策略探索了机器人学习的新范式，释放了操作系统中的泛化能力。

Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.

</details>


### [111] [Parametric Design of a Cable-Driven Coaxial Spherical Parallel Mechanism for Ultrasound Scans](https://arxiv.org/abs/2512.06995)
*Maryam Seraj,Mohammad Hossein Kamrava,Carlo Tiseo*

Main category: cs.RO

TL;DR: 提出一种电缆驱动同轴球面并联机构(CDC-SPM)，用于医疗遥操作中的触觉接口，通过减少末端执行器质量来提升动态性能，实现解耦的旋转自由度。


<details>
  <summary>Details</summary>
Motivation: 医疗遥操作中触觉接口需要高保真度，但面临工作空间、灵活性、刚度、惯性和带宽之间的性能权衡挑战，特别是在需要纯旋转运动的应用中。

Method: 采用电缆驱动同轴球面并联机构(CDC-SPM)设计，通过并行和同轴驱动实现解耦的旋转自由度，减少末端执行器质量以降低惯性负载。

Result: 仿真分析表明CDC-SPM提供准确、响应迅速且安全的运动特性，适合高精度触觉应用，具有各向同性的力和扭矩传递能力。

Conclusion: 该机构在医疗遥操作任务中具有潜力，如超声成像等需要精确直观操作的应用，能够平衡触觉接口的关键性能参数。

Abstract: Haptic interfaces play a critical role in medical teleoperation by enabling surgeons to interact with remote environments through realistic force and motion feedback. Achieving high fidelity in such systems requires balancing performance trade-off among workspace, dexterity, stiffness, inertia, and bandwidth, particularly in applications demanding pure rotational motion. This paper presents the design methodology and kinematic analysis of a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) developed to address these challenges. The proposed cable-driven interface design allows for reducing the mass placed at the robot arm end-effector, thereby minimizing inertial loads, enhancing stiffness, and improving dynamic responsiveness. Through parallel and coaxial actuation, the mechanism achieves decoupled rotational degrees of freedom with isotropic force and torque transmission. Simulation and analysis demonstrate that the CDC-SPM provides accurate, responsive, and safe motion characteristics suitable for high-precision haptic applications. These results highlight the mechanism's potential for medical teleoperation tasks such as ultrasound imaging, where precise and intuitive manipulation is essential.

</details>


### [112] [A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator](https://arxiv.org/abs/2512.07032)
*Runcong Wang,Fengyi Wang,Gordon Cheng*

Main category: cs.RO

TL;DR: 论文提出了一种用于移动机械臂的异质关联顺序记忆系统，通过神经形态绑定关节状态与触觉观测，实现低计算和内存成本的逐步动作决策。


<details>
  <summary>Details</summary>
Motivation: 移动机械臂需要能够在触觉交互中做出快速、经济的动作决策，传统方法计算成本高且难以处理连续状态-观测关联。需要一种能够从同步状态流中学习紧凑绑定、支持模糊检索并具有泛化能力的记忆系统。

Method: 1) 使用群体位置编码表示关节角度，Izhikevich神经元模型将皮肤测得的力转换为脉冲率特征；2) 将两种信号转换为双极二进制向量并进行元素级绑定；3) 引入3D旋转位置嵌入，根据感知的力方向旋转子空间以改善二进制空间的可分离性；4) 通过softmax加权召回在时间偏移动作模式上实现模糊检索。

Result: 在丰田人机支持机器人上实现了伪合规控制器，使连杆在触摸下沿施加力的方向和速度移动，并通过持续触觉输入检索多关节抓取序列。系统设置快速，从同步状态流中训练，具有泛化能力且经济高效。

Conclusion: 该系统展示了通过关联召回执行单关节和全臂行为的能力，为模仿学习、运动规划和多模态集成提供了扩展可能性，实现了低计算成本的触觉驱动动作决策。

Abstract: This paper presents a hetero-associative sequential memory system for mobile manipulators that learns compact, neuromorphic bindings between robot joint states and tactile observations to produce step-wise action decisions with low compute and memory cost. The method encodes joint angles via population place coding and converts skin-measured forces into spike-rate features using an Izhikevich neuron model; both signals are transformed into bipolar binary vectors and bound element-wise to create associations stored in a large-capacity sequential memory. To improve separability in binary space and inject geometry from touch, we introduce 3D rotary positional embeddings that rotate subspaces as a function of sensed force direction, enabling fuzzy retrieval through a softmax weighted recall over temporally shifted action patterns. On a Toyota Human Support Robot covered by robot skin, the hetero-associative sequential memory system realizes a pseudocompliance controller that moves the link under touch in the direction and with speed correlating to the amplitude of applied force, and it retrieves multi-joint grasp sequences by continuing tactile input. The system sets up quickly, trains from synchronized streams of states and observations, and exhibits a degree of generalization while remaining economical. Results demonstrate single-joint and full-arm behaviors executed via associative recall, and suggest extensions to imitation learning, motion planning, and multi-modal integration.

</details>


### [113] [CERNet: Class-Embedding Predictive-Coding RNN for Unified Robot Motion, Recognition, and Confidence Estimation](https://arxiv.org/abs/2512.07041)
*Hiroki Sawada,Alexandre Pitti,Mathias Quoy*

Main category: cs.RO

TL;DR: 提出CERNet，一个基于预测编码循环神经网络的统一模型，能同时实现机器人运动生成、实时意图识别和置信度估计


<details>
  <summary>Details</summary>
Motivation: 机器人需要同时具备实时运动生成、行为意图推断和推断置信度估计的能力，现有方法通常将这些功能分开处理，缺乏统一框架

Method: 使用带类别嵌入向量的分层预测编码循环神经网络（PC-RNN），通过动态更新的类别嵌入向量统一运动生成和识别。模型有两种模式：生成模式中类别嵌入约束隐藏状态到类别特定子空间；推断模式中在线优化类别嵌入以最小化预测误差

Result: 在人形机器人26个字母手势任务中，分层模型比参数匹配的单层基线轨迹复制误差降低76%，在外部扰动下保持运动保真度，在线推断轨迹类别达到68% Top-1和81% Top-2准确率，内部预测误差自然反映识别置信度

Conclusion: CERNet在紧凑的PC-RNN框架内整合了鲁棒生成、实时识别和内在不确定性估计，为物理机器人提供了一种紧凑且可扩展的运动记忆方法，在意图敏感的人机协作中具有应用潜力

Abstract: Robots interacting with humans must not only generate learned movements in real-time, but also infer the intent behind observed behaviors and estimate the confidence of their own inferences. This paper proposes a unified model that achieves all three capabilities within a single hierarchical predictive-coding recurrent neural network (PC-RNN) equipped with a class embedding vector, CERNet, which leverages a dynamically updated class embedding vector to unify motor generation and recognition. The model operates in two modes: generation and inference. In the generation mode, the class embedding constrains the hidden state dynamics to a class-specific subspace; in the inference mode, it is optimized online to minimize prediction error, enabling real-time recognition. Validated on a humanoid robot across 26 kinesthetically taught alphabets, our hierarchical model achieves 76% lower trajectory reproduction error than a parameter-matched single-layer baseline, maintains motion fidelity under external perturbations, and infers the demonstrated trajectory class online with 68% Top-1 and 81% Top-2 accuracy. Furthermore, internal prediction errors naturally reflect the model's confidence in its recognition. This integration of robust generation, real-time recognition, and intrinsic uncertainty estimation within a compact PC-RNN framework offers a compact and extensible approach to motor memory in physical robots, with potential applications in intent-sensitive human-robot collaboration.

</details>


### [114] [A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling](https://arxiv.org/abs/2512.07091)
*Tomoya Takahashi,Yusaku Nakajima,Cristian Camilo Beltran-Hernandez,Yuki Kuroda,Kazutoshi Tanaka,Masashi Hamaya,Kanta Ono,Yoshitaka Ushiku*

Main category: cs.RO

TL;DR: 开发了一种用于实验室自动化的柔性漏斗形机械手，通过可控阀门和基于粉末流动模型的反馈控制，实现了毫克级粉末的精确称量。


<details>
  <summary>Details</summary>
Motivation: 实验室自动化在固态材料发现中具有加速潜力，但毫克级粉末处理仍然是一个重大挑战，因为粉末流动动力学复杂且实验室任务多样。现有系统难以实现完全自动化的粉末处理。

Method: 提出了一种新型漏斗形柔性机械手，保留了先前工作的柔软性和锥形片设计，但在锥顶加入了可控阀门，实现毫克级粉末的精确增量分配。该系统通过基于粉末流动模型和在线参数识别的反馈控制与外部天平集成。

Result: 实验评估显示，80%的试验误差在2毫克以内，最大误差约为20毫克（目标范围20毫克至3克）。与直接PID控制相比，基于模型的控制显著提高了准确性和收敛速度。系统能够适应粉末动力学的变化。

Conclusion: 该系统展示了高效灵活的粉末称量潜力，具有向更大数量扩展的可行性，并适用于广泛的实验室自动化任务，为实验室自动化中的粉末处理提供了创新解决方案。

Abstract: Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.

</details>


### [115] [Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots](https://arxiv.org/abs/2512.07114)
*Jue Wang,Mingsong Jiang,Luis A. Ramirez,Bilige Yang,Mujun Zhang,Esteban Figueroa,Wenzhong Yan,Rebecca Kramer-Bottiglio*

Main category: cs.RO

TL;DR: 提出了一种替代柔顺性建模方法，通过在刚体模拟器中引入代表软材料变形的间接变量，实现了软体机器人的高效模拟与控制学习


<details>
  <summary>Details</summary>
Motivation: 自适应形态发生机器人需要适应不同任务和环境条件，但软体组件的模拟和控制存在挑战：软体模拟器精度和计算效率有限，而刚体模拟器无法捕捉软材料动力学

Method: 采用替代柔顺性建模方法，不显式模拟软体物理，而是在刚体模拟器中引入代表软材料变形的间接变量（有效肢体长度和肢体质心变化），并通过强化学习对这些间接变量进行广泛随机化

Result: 在刚体模拟器中实现了可靠策略学习，策略可直接转移到硬件上，在硬质平坦基底上表现出高保真度，在流变复杂地形上转移保真度较低但仍稳健；学习到的闭环步态展现出前所未有的地面机动性，运输成本比开环基线降低一个数量级

Conclusion: 该方法成功实现了软体机器人的高效模拟与控制，验证了替代柔顺性建模在自适应形态发生机器人中的有效性，机器人能够在多种自然地形上实现稳定的多步态运动

Abstract: Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.

</details>


### [116] [Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07130)
*Zebin Xing,Yupeng Zheng,Qichao Zhang,Zhixing Ding,Pengxuan Yang,Songen Gu,Zhongpu Xia,Dongbin Zhao*

Main category: cs.RO

TL;DR: Mimir是一个新颖的分层双系统框架，通过估计目标点的不确定性和多速率引导机制，在自动驾驶中实现更鲁棒的轨迹生成和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶方法受限于不准确的高层引导信号和复杂引导模块的计算开销，需要一种既能处理不确定性又能提高效率的解决方案。

Method: 提出Mimir框架：1) 使用拉普拉斯分布估计目标点不确定性以增强鲁棒性；2) 引入多速率引导机制提前预测扩展目标点以提高推理速度。

Result: 在Navhard和Navtest基准测试中，Mimir在驾驶评分EPDMS上比之前最优方法提升20%，同时高层模块推理速度提升1.6倍且不损失准确性。

Conclusion: Mimir通过不确定性估计和多速率引导机制，有效解决了自动驾驶中高层引导不准确和计算效率低的问题，实现了性能与效率的双重提升。

Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving

</details>


### [117] [Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework](https://arxiv.org/abs/2512.07137)
*Kang Yijie,Hao Yuqing,Wang Qingyun,Chen Guanrong*

Main category: cs.RO

TL;DR: 基于广义Udwadia-Kalaba框架，研究了具有区域约束的轮式移动机器人时变编队跟踪控制问题


<details>
  <summary>Details</summary>
Motivation: 现有时变编队跟踪控制研究未考虑区域约束，无法保证机器人的安全性。本文旨在设计能够同时满足编队跟踪和区域约束的控制策略

Method: 采用广义Udwadia-Kalaba框架，将时变编队跟踪控制目标重构为约束方程，通过微分同胚变换处理区域约束，在有向加权通信拓扑下设计控制器

Result: 成功设计了具有区域约束的时变编队跟踪控制器，通过数值仿真验证了控制策略的有效性

Conclusion: 提出的基于广义Udwadia-Kalaba框架的控制方法能够同时实现时变编队跟踪和区域约束，确保了机器人的安全性，相比现有方法更具优势

Abstract: In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.

</details>


### [118] [Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction](https://arxiv.org/abs/2512.07177)
*Fanjun Bu,Melina Tsai,Audrey Tjokro,Tapomayukh Bhattacharjee,Jorge Ortiz,Wendy Ju*

Main category: cs.RO

TL;DR: 提出一个两阶段管道，使用轻量级感知检测器（注视转移和空间距离）在社交关键时刻触发基于视频的视觉语言模型查询，以实现社交响应式机器人行为。


<details>
  <summary>Details</summary>
Motivation: 机器人在日常环境中需要决定何时以及是否与人互动，这种决策往往依赖于随时间展开的微妙非语言线索，这些线索难以显式建模。

Method: 基于5天Wizard-of-Oz部署的观察，提出两阶段管道：首先使用轻量级感知检测器（注视转移和空间距离）检测社交关键时刻，然后触发基于视频的视觉语言模型查询进行社交推理。

Result: 在重放的现场交互中评估该管道并比较两种提示策略，发现选择性使用VLM作为社交推理代理能够实现社交响应式机器人行为。

Conclusion: 通过选择性使用视觉语言模型作为社交推理代理，机器人能够关注人们在真实世界交互中自然提供的线索，从而采取适当行动。

Abstract: Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.

</details>


### [119] [Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality](https://arxiv.org/abs/2512.07221)
*Zichao Shu,Shitao Bei,Lijun Li,Zetao Chen*

Main category: cs.RO

TL;DR: 提出一种连续时间最大似然估计器，通过整合IMU数据补偿MoCap抖动，实现高精度SLAM基准测试，特别针对XR应用中的旋转误差和帧间抖动评估。


<details>
  <summary>Details</summary>
Motivation: 随着XR沉浸感标准提高，SLAM基准测试要求更严格。传统基于MoCap的基准测试存在时空校准和设备抖动问题，限制了旋转误差和帧间抖动等关键指标的准确评估。

Method: 提出连续时间最大似然估计器，整合辅助IMU数据补偿MoCap抖动；采用可变时间同步方法和基于螺旋同余约束的位姿残差，实现多传感器与待测设备的精确时空校准。

Result: 方法优于现有技术，达到全面评估先进SLAM算法所需的精度；成功对多个领先XR设备和开源SLAM算法进行基准测试验证实用性。

Conclusion: 该方法解决了MoCap基准测试的局限性，为XR应用中的SLAM算法提供了精确的基准测试工具，代码已开源。

Abstract: Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.

</details>


### [120] [SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks](https://arxiv.org/abs/2512.07266)
*Florian Tretter,Daniel Flögel,Alexandru Vasilache,Max Grobbel,Jürgen Becker,Sören Hohmann*

Main category: cs.RO

TL;DR: 提出一种混合社会集成DRL方法，结合SNN和ANN，用于机器人社会导航，显著降低能耗


<details>
  <summary>Details</summary>
Motivation: 将自主移动机器人集成到人类环境需要类人决策和节能的事件驱动计算，但神经形态方法在DRL导航中应用有限，主要由于训练不稳定问题

Method: 采用混合社会集成DRL演员-评论家方法：演员使用脉冲神经网络(SNN)，评论家使用人工神经网络(ANN)，并配备神经形态特征提取器捕捉时间人群动态和人机交互

Result: 提高了社会导航性能，并将估计能耗降低了约1.69个数量级

Conclusion: 该方法成功解决了神经形态方法在DRL导航中的训练不稳定问题，实现了高效节能的社会导航

Abstract: Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.

</details>


### [121] [Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots](https://arxiv.org/abs/2512.07303)
*Gianpietro Battocletti,Dimitris Boskos,Bart De Schutter*

Main category: cs.RO

TL;DR: 提出一种基于通用覆盖空间的连续拓扑模型，用于解决系绳机器人的路径规划问题，相比传统离散方法更高效且支持连续规划


<details>
  <summary>Details</summary>
Motivation: 现有系绳机器人路径规划方法通常依赖离散配置空间表示，无法同时捕获系绳的拓扑信息和机器人的连续位置，需要更有效的建模方法

Method: 首先建立系绳机器人配置空间与工作空间通用覆盖空间之间的联系，然后利用这种联系开发算法计算配置空间的单纯复形模型

Result: 该方法相比构建传统同伦增强图的时间大幅减少，模型是连续的，允许使用多种路径规划算法解决系绳机器人的路径规划任务

Conclusion: 提出的拓扑模型能够高效地表示系绳机器人的配置空间，解决了现有离散方法的局限性，为系绳机器人路径规划提供了更优的解决方案

Abstract: Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.

</details>


### [122] [Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection](https://arxiv.org/abs/2512.07316)
*Gianpietro Battocletti,Dimitris Boskos,Bart De Schutter*

Main category: cs.RO

TL;DR: 提出了一种用于无人水面艇（USV）对接的集中式模型预测控制（MPC）方法，通过协同工作实现更快速高效的对接


<details>
  <summary>Details</summary>
Motivation: 现有USV对接方法通常将一艘USV视为静止目标，另一艘负责接近，缺乏协同性。需要开发更高效的协同对接方法，特别是在存在水流等干扰的环境中。

Method: 采用集中式模型预测控制（MPC）方法，两艘USV协同工作，在约定位置对接。MPC通过预测模型考虑干扰影响，保证约束满足和轨迹可行性。

Result: 仿真结果表明，该方法相比现有方法能够实现更快速、更高效的USV对接，特别是在处理水流等几乎静止的干扰时表现优异。

Conclusion: 提出的集中式MPC协同对接方法有效解决了USV对接问题，通过模型预测能力处理干扰，实现了比传统方法更好的对接性能。

Abstract: Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.

</details>


### [123] [Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin](https://arxiv.org/abs/2512.07359)
*Bin Zhao,Yiwen Lu,Haohua Zhu,Xiao Li,Sheng Yi*

Main category: cs.RO

TL;DR: 提出一个从光学动捕数据构建个性化多刚体手部模型的完整流程，通过将MANO模型的SO(3)关节旋转投影到刚体模型的运动学约束关节上，实现实时物理仿真


<details>
  <summary>Details</summary>
Motivation: 数字孪生应用中需要平衡解剖保真度和计算效率的手部仿真模型，现有方法难以同时满足真实外观和实时物理仿真的需求

Method: 从光学动捕构建个性化MANO模型，转换为URDF表示，提出闭式解处理单自由度关节，使用BCH修正迭代方法处理双自由度关节的旋转非交换性问题

Result: 亚厘米级重建误差，在多样化操作任务中成功执行抓取，强化学习策略能控制多刚体手部重放捕获的人类演示

Conclusion: 提出的流程能构建既保持真实外观又支持实时物理仿真的多刚体手部模型，为数字孪生应用提供了有效的解决方案

Abstract: Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.

</details>


### [124] [ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning](https://arxiv.org/abs/2512.07371)
*Byungju Kim,Jinu Pahk,Chungwoo Lee,Jaejoon Kim,Jangha Lee,Theo Taeyeong Kim,Kyuhwan Shim,Jun Ki Lee,Byoung-Tak Zhang*

Main category: cs.RO

TL;DR: ESPADA框架通过语义感知的演示分段实现机器人策略加速，在保持成功率的同时实现约2倍速度提升


<details>
  <summary>Details</summary>
Motivation: 基于行为克隆的视觉运动策略虽然精确，但继承了人类演示的缓慢节奏，限制了实际部署。现有加速方法依赖统计或启发式线索，忽略了任务语义，在不同操作场景中容易失败。

Method: 使用VLM-LLM流水线结合3D夹爪-物体关系对演示进行语义分段，仅在非关键段进行激进下采样，保留精度关键阶段。通过动态时间规整在动态特征上传播分段标签，从单个标注扩展到整个数据集。

Result: 在仿真和真实世界实验中，与ACT和DP基线相比，ESPADA实现了约2倍加速，同时保持成功率，缩小了人类演示与高效机器人控制之间的差距。

Conclusion: ESPADA提供了一种语义和空间感知的框架，无需额外数据、架构修改或重新训练，就能有效加速机器人策略，同时保持操作精度。

Abstract: Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.

</details>


### [125] [Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction](https://arxiv.org/abs/2512.07464)
*Haolin Song,Hongbo Zhu,Tao Yu,Yan Liu,Mingqi Yuan,Wengang Zhou,Hua Chen,Houqiang Li*

Main category: cs.RO

TL;DR: 提出一个感知性运动框架，将地形感知、步态调节和全身控制集成到单一强化学习策略中，实现人形机器人在复杂地形上的稳健运动


<details>
  <summary>Details</summary>
Motivation: 全尺寸人形机器人在复杂地形（如长楼梯）上实现可靠运动仍然具有挑战性，有限的感知、模糊的地形线索和步态时序适应不足容易导致失衡

Method: 使用向下深度相机观察脚部支撑区域，紧凑U-Net实时重建密集自我中心高度图；将感知高度图与本体感受观察输入统一策略，生成关节命令和全局步进相位信号；采用单阶段连续师生训练方案

Result: 在31自由度、1.65米人形机器人上验证，在仿真和真实环境中实现稳健运动，包括前后上下楼梯以及跨越46厘米间隙

Conclusion: 提出的感知运动框架通过集成地形感知、步态调节和全身控制，成功实现了人形机器人在复杂地形上的可靠运动

Abstract: For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/

</details>


### [126] [Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation](https://arxiv.org/abs/2512.07472)
*Siyu Xu,Zijian Wang,Yunke Wang,Chenghao Xia,Tao Huang,Chang Xu*

Main category: cs.RO

TL;DR: AFI框架通过3D空间可操作场检测和干预VLA模型的记忆陷阱，在分布偏移场景下提升机器人操作鲁棒性


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作中表现良好，但在分布偏移下容易陷入"记忆陷阱"，即在新场景中重复记忆轨迹而非适应新环境。这源于端到端设计缺乏显式3D空间推理能力，无法可靠识别陌生环境中的可操作区域。

Method: 提出Affordance Field Intervention (AFI)混合框架：1) 使用3D空间可操作场(SAFs)作为按需插件指导VLA行为；2) 通过本体感知检测记忆陷阱；3) 将机器人重新定位到高可操作性区域；4) 提出可操作性驱动的路径点锚定VLA生成的动作；5) 使用基于SAF的评分器选择累积可操作性最高的轨迹。

Result: 在真实机器人平台上，不同VLA骨干网络(π₀和π₀.₅)在分布外场景下平均提升23.5%；在LIBERO-Pro基准上提升20.2%，验证了AFI在增强VLA对分布偏移鲁棒性方面的有效性。

Conclusion: AFI通过将显式3D空间可操作性推理与VLA模型结合，有效解决了记忆陷阱问题，显著提升了VLA在分布偏移场景下的适应能力和鲁棒性。

Abstract: Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the "Memory Trap". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.

</details>


### [127] [From Real-World Traffic Data to Relevant Critical Scenarios](https://arxiv.org/abs/2512.07482)
*Florian Lüttner,Nicole Neis,Daniel Stadler,Robin Moss,Mirjam Fehling-Kaschek,Matthias Pfriem,Alexander Stolz,Jens Ziehn*

Main category: cs.RO

TL;DR: 论文提出了一种基于真实高速公路数据的车道变换场景分析方法，通过关键性度量识别安全相关场景，并生成合成关键场景以提高自动驾驶验证效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要在各种相关场景下可靠运行，但识别完整的安全相关场景具有挑战性。随着技术复杂度增加，"未知不安全"场景数量增多，需要从高速公路等简单领域开始，提前识别相关场景以提高验证效率。

Method: 1) 采集和处理真实高速公路交通数据；2) 对轨迹数据应用关键性度量评估场景；3) 将计算出的度量与特定车道变换场景及数据采集条件关联；4) 基于记录场景生成合成场景以应对"未知不安全"场景。

Result: 开发了一个处理链，能够识别安全相关场景，开发数据驱动方法提取这些场景，并通过采样在高速公路上生成合成关键场景。

Conclusion: 该方法通过分析高速公路车道变换场景，为自动驾驶系统验证提供了有效的场景识别和生成框架，有助于提高验证效率并应对"未知不安全"场景的挑战。

Abstract: The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly "unknown unsafe" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of "unknown unsafe" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.

</details>


### [128] [VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform](https://arxiv.org/abs/2512.07507)
*Yiming Cui,Shiyu Fang,Jiarui Zhang,Yan Huang,Chengkai Xu,Bing Zhu,Hao Zhang,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 提出VP-AutoTest平台，通过虚拟物理融合测试解决自动驾驶测试的挑战，支持多种交通元素、单/多车交互、对抗测试，并包含多维评估和可信度自评。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶测试方法（虚拟仿真、封闭场地、公开道路）存在车辆状态不真实、测试能力有限、成本高等问题，虚拟物理融合测试虽有潜力但仍面临元素类型有限、测试范围窄、评估指标固定等挑战。

Method: 提出VP-AutoTest平台，集成10+种虚拟和物理元素（车辆、行人、路侧设施），支持单车交互和多车协同测试，采用对抗测试和平行推演加速故障发现，通过OBU和Redis通信实现V2V/V2I协同，结合多维评估框架和AI专家系统进行性能评估和缺陷诊断。

Result: 平台能够复现实世界交通参与者的多样性，支持全级别协同自动化，通过虚拟物理融合测试与真实实验对比进行可信度自评估，确保测试的保真度和效率。

Conclusion: VP-AutoTest平台有效解决了当前自动驾驶测试的挑战，提供了更全面、高效、可信的测试解决方案，已在OnSite自动驾驶公共服务平台上实现完整测试功能。

Abstract: The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.

</details>


### [129] [See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations](https://arxiv.org/abs/2512.07582)
*Guangyan Chen,Meiling Wang,Qi Shao,Zichen Zhou,Weixin Mao,Te Cui,Minzhao Zhu,Yinan Deng,Luojie Yang,Zhanqi Zhang,Yi Yang,Hua Chen,Yufeng Yue*

Main category: cs.RO

TL;DR: ViVLA是一个通过单次专家演示视频学习新技能的机器人操作策略，利用视觉-语言-动作模型实现从人类视频到机器人的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在训练分布外的任务泛化能力有限，而人类仅通过观察一次演示就能学习新技能。受此启发，研究旨在开发能够从单次专家演示视频高效学习新任务的通用机器人操作策略。

Method: 提出ViVLA策略，联合处理专家演示视频和机器人视觉观察，预测演示动作序列和后续机器人动作。开发可扩展的专家-智能体配对数据生成管道，从易获取的人类视频合成配对轨迹，并结合公开数据集，共生成892,911个训练样本。

Result: ViVLA能够在测试时仅通过单次专家演示视频学习新操作技能：在未见过的LIBERO任务上提升超过30%；跨具身视频保持35%以上增益；真实世界实验中，从人类视频学习在未见任务上带来超过38%的改进。

Conclusion: ViVLA展示了从单次专家演示视频高效学习机器人操作技能的潜力，实现了从人类视频到机器人的知识迁移，为通用机器人操作策略的发展提供了新方向。

Abstract: Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.

</details>


### [130] [Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots](https://arxiv.org/abs/2512.07673)
*Matthias Heyrman,Chenhao Li,Victor Klemm,Dongho Kang,Stelian Coros,Marco Hutter*

Main category: cs.RO

TL;DR: MDME是一种多域运动嵌入方法，使用小波编码器和概率嵌入联合捕捉运动中的结构化周期模式和非规则变化，实现无需重定向的实时运动模仿


<details>
  <summary>Details</summary>
Motivation: 现有运动控制器常忽略运动中的固有模式，而现有表示学习方法未能同时捕捉人类和动物运动中的结构化周期模式和非规则变化

Method: 提出多域运动嵌入(MDME)，使用基于小波的编码器和并行概率嵌入来统一结构化与非结构化特征的嵌入，从最小输入集生成丰富的参考运动表示

Result: 在双足和四足机器人平台上实现了复杂轨迹的准确再现，在重建保真度和对未见运动的泛化能力上优于现有方法，支持零样本部署生成新运动风格

Conclusion: MDME作为可泛化且结构感知的基础，为可扩展的实时机器人模仿提供了解决方案，无需任务特定调优或在线重定向

Abstract: Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.

</details>


### [131] [AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation](https://arxiv.org/abs/2512.07680)
*P. A. Wigner,L. Romanello,A. Hammad,P. H. Nguyen,T. Lan,S. F. Armanini,B. B. Kocer,M. Kovac*

Main category: cs.RO

TL;DR: 提出一种可从空中部署的树冠爬行机器人，结合柔性微刺履带、双履带旋转夹持器和弹性尾部，能在不同曲率和倾斜度的树枝上稳定移动与操作。


<details>
  <summary>Details</summary>
Motivation: 现有空中机器人（如无人机）在树冠等复杂环境中存在续航短、难以稳定附着操作的问题，需要一种能结合空中部署优势与地面爬行稳定性的混合系统，以支持环境采样和冠层内传感等生态研究应用。

Method: 采用柔性微刺履带提供自适应附着能力，双履带旋转夹持器实现可靠抓握，弹性尾部增强稳定性。系统通过无人机-绳索系统进行空中部署，具备偏航转向能力以应对不规则表面。

Result: 实验显示：可靠抓握能力达90度机身滚转和倾斜；在倾斜67.5度的树枝上有效攀爬；水平树枝上最大速度0.55体长/秒；偏航转向达10度；无因次运输成本比典型悬停功耗低一个数量级。

Conclusion: 该空中可部署爬行机器人成功填补了空中与地面生态机器人之间的空白，提供了一个稳健、低功耗的平台，适用于环境采样和冠层内传感应用，在复杂树冠环境中展现出优越的适应性和能效。

Abstract: This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.
  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.
  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.

</details>


### [132] [Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks](https://arxiv.org/abs/2512.07697)
*Aileen Liao,Dong-Ki Kim,Max Olan Smith,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: DA-DP是一种延迟感知扩散策略框架，通过在训练和推理中显式处理推理延迟来提升机器人策略的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 机器人感知和动作选择之间存在推理延迟（数十到数百毫秒），导致观察状态与执行状态之间存在差距，现有方法通常忽略这种延迟影响

Method: 提出延迟感知扩散策略（DA-DP），将零延迟轨迹校正为延迟补偿轨迹，并通过延迟条件增强策略，该框架与架构无关，可扩展到其他策略类型

Result: 在各种任务、机器人和延迟条件下，DA-DP的成功率比延迟不感知方法更鲁棒，能够有效处理推理延迟

Conclusion: DA-DP为延迟感知模仿学习提供通用模式，并鼓励评估协议报告性能与实测延迟的关系，而不仅仅是任务难度

Abstract: As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.

</details>


### [133] [Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next](https://arxiv.org/abs/2512.07765)
*Gustavo A. Cardona,Shubham S. Kumbhar,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 本文综述了物理人-人形机器人交互（pHHI）的现状，通过三个核心支柱（人形建模与控制、人类意图估计、计算人类模型）分析现有方法、挑战和局限性，并提出跨支柱整合路径和基于交互模态与机器人参与度的统一分类法。


<details>
  <summary>Details</summary>
Motivation: 随着人形机器人在非结构化、以人为中心环境中的部署需求增加，需要建立稳健、可扩展且自适应的物理人-人形机器人交互框架。当前研究虽然在各领域取得进展，但跨支柱整合有限，阻碍了连贯交互的实现。

Method: 通过三个核心支柱进行系统性综述：(1) 人形建模与控制：处理不确定人类动力学的全身控制策略；(2) 人类意图估计：有限传感下的实时意图推断；(3) 计算人类模型：考虑人类物理状态变化的建模技术。同时提出基于交互模态（直接/间接）和机器人参与度（协助/合作/协作）的统一分类法。

Result: 识别了各支柱的关键挑战：全身控制策略不足、实时意图推断受限、人类状态变化建模不充分。虽然各领域有显著进展，但跨支柱整合仍然有限。提出了跨领域方法统一的路径，并建立了系统化的交互分类框架。

Conclusion: 为实现稳健、安全、直观的物理交互，需要整合三个核心支柱的方法。提出的统一分类法和跨支柱整合路径为未来研究提供了路线图，使人形系统能够在多样化现实场景中有效理解、预测并与人类伙伴协作。

Abstract: Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.

</details>


### [134] [OptMap: Geometric Map Distillation via Submodular Maximization](https://arxiv.org/abs/2512.07775)
*David Thorne,Nathan Chan,Christa S. Robison,Philip R. Osteen,Brett T. Lopez*

Main category: cs.RO

TL;DR: OptMap是一种实时几何地图蒸馏算法，通过子模优化理论从LiDAR数据中生成特定应用的最优地图，解决了NP-hard组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需要不同尺度的几何地图来支持各种感知和决策算法，但LiDAR产生的大量几何数据难以高效选择信息丰富且尺寸受限的地图，这是一个NP-hard组合优化问题。

Method: 提出OptMap算法，核心是最大化具有递减回报特性的子模集合函数，使用多项式时间算法获得近似最优解。设计了新颖的子模奖励函数来量化信息量、减少输入集大小并最小化顺序收集数据的偏差，并提出动态重排序流式子模算法来改进解质量并解决输入顺序偏差。

Result: 在开源和自定义数据集上测试，特别关注长时间建图会话，展示了OptMap的最小计算需求。提供了ROS1和ROS2开源包，可与任何LiDAR SLAM算法配合使用。

Conclusion: OptMap通过理论创新实现了实时、特定应用的几何地图生成，解决了从丰富LiDAR数据中选择最优地图的NP-hard优化问题，为自主机器人提供了高效的地图蒸馏解决方案。

Abstract: Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.

</details>


### [135] [Inchworm-Inspired Soft Robot with Groove-Guided Locomotion](https://arxiv.org/abs/2512.07813)
*Hari Prakash Thanabalan,Lars Bengtsson,Ugo Lafont,Giovanni Volpe*

Main category: cs.RO

TL;DR: 单驱动器软体机器人通过图案化基底被动控制运动方向，简化了设计和控制，降低了能耗


<details>
  <summary>Details</summary>
Motivation: 传统软体机器人需要多个驱动器来实现方向控制，这增加了机械复杂性、控制难度和能耗。需要一种更简单、更节能的方向控制方法。

Method: 采用受尺蠖启发的软体机器人设计，使用单个卷曲介电弹性体驱动器，通过3D打印基底上的沟槽图案被动引导机器人的对齐和轨迹。

Result: 实验表明，通过改变沟槽角度可以精确控制运动方向，无需复杂的驱动策略。这种方法降低了能耗，简化了机器人设计。

Conclusion: 沟槽引导方法为软体机器人提供了简单有效的方向控制方案，扩展了其在搜救、管道检测和行星探索等领域的应用潜力。

Abstract: Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.

</details>


### [136] [Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation](https://arxiv.org/abs/2512.07819)
*Shubham S. Kumbhar,Abhijeet M. Kulkarni,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 提出一个用于人形机器人的人类协作搬运控制框架，包含高层规划、底层控制和刚度调节三部分，通过真实实验验证了框架有效性并提出了协作效率评估指标。


<details>
  <summary>Details</summary>
Motivation: 实现人形机器人与人类伙伴的协作搬运任务，支持平移和旋转运动，这是协作搬运场景中的基本需求。

Method: 框架包含三个组件：高层规划器（引入交互线性倒立摆I-LIP结合导纳模型和MPC生成动态可行的步态规划）、底层控制器（基于QP的全身控制器处理耦合的人形-物体动力学）、刚度调节机制（调节机器人-物体交互以确保收敛到期望的相对配置）。

Result: 在Digit人形平台上进行了真实世界实验验证，提出了量化协作效率的指标，该指标揭示了柔顺性在协作任务中的作用，并展示了平移、转向和半圆形轨迹等协作行为的实验结果。

Conclusion: 该控制框架成功实现了人形机器人与人类的协作搬运，提出的效率指标能够评估协作质量并为控制层设计提供指导，展示了框架在自然协作搬运任务中的有效性。

Abstract: We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.

</details>
