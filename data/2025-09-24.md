<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 55]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies](https://arxiv.org/abs/2509.18282)
*Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li*

Main category: cs.RO

TL;DR: PEEK提出了一种基于视觉语言模型（VLM）的中间表示方法，通过预测末端执行器路径和任务相关掩码来提升机器人操作策略的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人操作策略经常因为需要同时学习注意力位置、动作选择和执行方式而泛化失败。作者认为高级推理可以卸载给VLM，让策略专注于动作执行。

Method: PEEK微调VLM来预测统一的基于点的中间表示：1）末端执行器路径（指定动作），2）任务相关掩码（指定注意力位置）。这些标注直接叠加在机器人观察上，使表示具有策略无关性和跨架构可转移性。

Result: 在真实世界评估中，PEEK显著提升了零样本泛化能力，包括仅模拟训练的3D策略实现41.4倍改进，大型VLA和小型操作策略分别获得2-3.5倍提升。

Conclusion: 通过让VLM吸收语义和视觉复杂性，PEEK为操作策略提供了所需的最小提示——在哪里、做什么和怎么做。

Abstract: Robotic manipulation policies often fail to generalize because they must
simultaneously learn where to attend, what actions to take, and how to execute
them. We argue that high-level reasoning about where and what can be offloaded
to vision-language models (VLMs), leaving policies to specialize in how to act.
We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which
fine-tunes VLMs to predict a unified point-based intermediate representation:
1. end-effector paths specifying what actions to take, and 2. task-relevant
masks indicating where to focus. These annotations are directly overlaid onto
robot observations, making the representation policy-agnostic and transferable
across architectures. To enable scalable training, we introduce an automatic
annotation pipeline, generating labeled data across 20+ robot datasets spanning
9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot
generalization, including a 41.4x real-world improvement for a 3D policy
trained only in simulation, and 2-3.5x gains for both large VLAs and small
manipulation policies. By letting VLMs absorb semantic and visual complexity,
PEEK equips manipulation policies with the minimal cues they need--where, what,
and how. Website at https://peek-robot.github.io/.

</details>


### [2] [Fine-Tuning Robot Policies While Maintaining User Privacy](https://arxiv.org/abs/2509.18311)
*Benjamin A. Christie,Sagar Parekh,Dylan P. Losey*

Main category: cs.RO

TL;DR: PRoP是一个用于个性化且私密的机器人策略的模型无关框架，通过为每个用户分配唯一密钥来数学变换机器人网络权重，确保只有正确密钥才能访问个性化行为。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略需要个性化调整以匹配用户偏好，但个性化过程中会泄露用户数据，因此需要开发既能个性化又能保护隐私的机器人策略。

Method: 为每个用户分配唯一密钥，用该密钥数学变换机器人网络权重，只有正确密钥才能激活个性化策略，错误密钥则恢复基线行为。

Result: PRoP在模仿学习、强化学习和分类任务中表现良好，保持原始策略架构和行为，实验性能优于现有基于编码器的方法。

Conclusion: PRoP提供了一种有效的隐私保护个性化机器人策略解决方案，具有实际应用优势。

Abstract: Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.

</details>


### [3] [Haptic Communication in Human-Human and Human-Robot Co-Manipulation](https://arxiv.org/abs/2509.18327)
*Katherine H. Allen,Chris Rogers,Elaine S. Short*

Main category: cs.RO

TL;DR: 该研究比较了人-人和人-机器人协作操纵物体时的运动差异，发现人-人协作更加流畅，并提出了通过改进机器人的触觉信号传递能力来提升机器人协作性能。


<details>
  <summary>Details</summary>
Motivation: 研究人类在协作操纵物体时如何通过触觉信号进行沟通，并将这种沟通模式应用于改进机器人协作能力。

Method: 通过IMU传感器记录人-人协作和人-机器人协作时的物体运动轨迹，并结合问卷调查收集主观评价数据。

Result: 人-人协作在流畅性方面显著优于人-机器人协作，IMU数据能够客观捕捉到两种条件下的运动差异。

Conclusion: 研究结果表明需要改进机器人发送和接收类人触觉信号的能力，以提升其在物理任务中的协作性能。

Abstract: When a human dyad jointly manipulates an object, they must communicate about
their intended motion plans. Some of that collaboration is achieved through the
motion of the manipulated object itself, which we call "haptic communication."
In this work, we captured the motion of human-human dyads moving an object
together with one participant leading a motion plan about which the follower is
uninformed. We then captured the same human participants manipulating the same
object with a robot collaborator. By tracking the motion of the shared object
using a low-cost IMU, we can directly compare human-human shared manipulation
to the motion of those same participants interacting with the robot.
Intra-study and post-study questionnaires provided participant feedback on the
collaborations, indicating that the human-human collaborations are
significantly more fluent, and analysis of the IMU data indicates that it
captures objective differences in the motion profiles of the conditions. The
differences in objective and subjective measures of accuracy and fluency
between the human-human and human-robot trials motivate future research into
improving robot assistants for physical tasks by enabling them to send and
receive anthropomorphic haptic signals.

</details>


### [4] [The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020](https://arxiv.org/abs/2509.18330)
*Marsette Vona*

Main category: cs.RO

TL;DR: 本文介绍了Landform contextual mesh技术，该技术融合了火星2020探测车拍摄的数千张2D和3D图像数据，以及火星勘测轨道飞行器提供的轨道高程和彩色地图，生成交互式3D地形可视化。


<details>
  <summary>Details</summary>
Motivation: 为火星任务科学家提供战术和战略规划的交互式3D地形可视化工具，同时向公众开放部分数据以增强科学传播。

Method: 自动为每个探测车位置构建上下文网格，融合多源数据（探测车图像、轨道高程图、彩色地图），通过任务地面数据处理系统生成。

Result: 成功开发出Landform contextual mesh系统，该系统已集成到ASTTRO工具中供科学家使用，并在"Explore with Perseverance"网站上向公众开放。

Conclusion: 该技术为火星探测任务提供了有效的3D地形可视化解决方案，支持科学决策和公众参与。

Abstract: The Landform contextual mesh fuses 2D and 3D data from up to thousands of
Mars 2020 rover images, along with orbital elevation and color maps from Mars
Reconnaissance Orbiter, into an interactive 3D terrain visualization.
Contextual meshes are built automatically for each rover location during
mission ground data system processing, and are made available to mission
scientists for tactical and strategic planning in the Advanced Science
Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also
deployed to the "Explore with Perseverance" public access website.

</details>


### [5] [Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation](https://arxiv.org/abs/2509.18342)
*Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara*

Main category: cs.RO

TL;DR: 提出了一种用于葡萄园环境的语义粒子滤波器，通过融合LiDAR扫描和语义地标（葡萄树干和支撑杆）来解决重复行几何结构导致的定位问题。


<details>
  <summary>Details</summary>
Motivation: 在结构化户外环境（如葡萄园）中，由于重复的行几何结构和感知混叠，传统的LiDAR定位方法经常失败，需要更稳定的定位解决方案。

Method: 使用语义粒子滤波器，将检测到的地标投影到鸟瞰图并与LiDAR扫描融合生成语义观测。创新性地使用语义墙连接相邻地标形成伪结构约束，并在语义稀疏区域引入自适应GPS先验。

Result: 在真实葡萄园实验中，该方法能够在正确的行内保持定位，从AMCL失败的偏差中恢复，并优于RTAB-Map等基于视觉的SLAM方法。

Conclusion: 所提出的语义粒子滤波器通过结合语义地标和自适应GPS先验，有效解决了葡萄园环境中的定位挑战，为农业机器人提供了可靠的定位解决方案。

Abstract: Accurate localisation is critical for mobile robots in structured outdoor
environments, yet LiDAR-based methods often fail in vineyards due to repetitive
row geometry and perceptual aliasing. We propose a semantic particle filter
that incorporates stable object-level detections, specifically vine trunks and
support poles into the likelihood estimation process. Detected landmarks are
projected into a birds eye view and fused with LiDAR scans to generate semantic
observations. A key innovation is the use of semantic walls, which connect
adjacent landmarks into pseudo-structural constraints that mitigate row
aliasing. To maintain global consistency in headland regions where semantics
are sparse, we introduce a noisy GPS prior that adaptively supports the filter.
Experiments in a real vineyard demonstrate that our approach maintains
localisation within the correct row, recovers from deviations where AMCL fails,
and outperforms vision-based SLAM methods such as RTAB-Map.

</details>


### [6] [AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback](https://arxiv.org/abs/2509.18384)
*Yunhao Yang,Junyuan Hong,Gabriel Jacob Perin,Zhiwen Fan,Li Yin,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: LAD-VF是一个无需微调的框架，利用形式化验证反馈进行自动提示工程，通过文本损失函数迭代优化提示而非模型参数，显著提升LLM驱动规划系统的规范合规性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的规划系统在物理世界部署时经常违反安全约束，传统对齐方法需要昂贵的人工标注或资源密集的微调，需要一种更高效且可扩展的解决方案。

Method: 提出LAD-VF框架，集成形式化验证反馈的文本损失函数与LLM-AutoDiff，通过迭代提示工程而非模型微调来优化系统性能。

Result: 在机器人导航和操作任务中，LAD-VF将成功率从60%提升至90%以上，显著增强了规范合规性。

Conclusion: LAD-VF为构建可信赖、可形式化验证的LLM驱动控制系统提供了一条可扩展且可解释的路径。

Abstract: Large language models (LLMs) can translate natural language instructions into
executable action plans for robotics, autonomous driving, and other domains.
Yet, deploying LLM-driven planning in the physical world demands strict
adherence to safety and regulatory constraints, which current models often
violate due to hallucination or weak alignment. Traditional data-driven
alignment methods, such as Direct Preference Optimization (DPO), require costly
human labeling, while recent formal-feedback approaches still depend on
resource-intensive fine-tuning. In this paper, we propose LAD-VF, a
fine-tuning-free framework that leverages formal verification feedback for
automated prompt engineering. By introducing a formal-verification-informed
text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts
rather than model parameters. This yields three key benefits: (i) scalable
adaptation without fine-tuning; (ii) compatibility with modular LLM
architectures; and (iii) interpretable refinement via auditable prompts.
Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF
substantially enhances specification compliance, improving success rates from
60% to over 90%. Our method thus presents a scalable and interpretable pathway
toward trustworthy, formally-verified LLM-driven control systems.

</details>


### [7] [Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections](https://arxiv.org/abs/2509.18407)
*Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston*

Main category: cs.RO

TL;DR: 本文提出了一种基于部分可观测马尔可夫决策过程（POMDP）的驾驶员辅助框架，用于无控制交叉口的通行权推理，通过概率规划器显著提高了碰撞避免率。


<details>
  <summary>Details</summary>
Motivation: 无控制交叉口由于通行权规则模糊、视线遮挡和驾驶员行为不可预测等因素，是道路事故的高发区域。现有研究主要关注自动驾驶车辆，但缺乏针对人工驾驶车辆的辅助导航系统。

Method: 开发了驾驶员辅助框架，采用POMDP进行建模，在自定义仿真测试平台上比较了四种决策方法：确定性有限状态机（FSM）和三种概率规划器（QMDP、POMCP、DESPOT）。

Result: 概率规划器显著优于基于规则的基线方法，在部分可观测条件下实现了高达97.5%的无碰撞导航率，其中POMCP优先考虑安全性，DESPOT在效率和运行时间可行性之间取得平衡。

Conclusion: 研究强调了不确定性感知规划在驾驶员辅助中的重要性，为未来在真实交通环境中集成传感器融合和环境感知模块以实现实时部署提供了动力。

Abstract: Uncontrolled intersections account for a significant fraction of roadway
crashes due to ambiguous right-of-way rules, occlusions, and unpredictable
driver behavior. While autonomous vehicle research has explored
uncertainty-aware decision making, few systems exist to retrofit human-operated
vehicles with assistive navigation support. We present a driver-assist
framework for right-of-way reasoning at uncontrolled intersections, formulated
as a Partially Observable Markov Decision Process (POMDP). Using a custom
simulation testbed with stochastic traffic agents, pedestrians, occlusions, and
adversarial scenarios, we evaluate four decision-making approaches: a
deterministic finite state machine (FSM), and three probabilistic planners:
QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform
the rule-based baseline, achieving up to 97.5 percent collision-free navigation
under partial observability, with POMCP prioritizing safety and DESPOT
balancing efficiency and runtime feasibility. Our findings highlight the
importance of uncertainty-aware planning for driver assistance and motivate
future integration of sensor fusion and environment perception modules for
real-time deployment in realistic traffic environments.

</details>


### [8] [Latent Action Pretraining Through World Modeling](https://arxiv.org/abs/2509.18428)
*Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid*

Main category: cs.RO

TL;DR: LAWM是一个模型无关的框架，通过世界建模从无标签视频数据中学习潜在动作表示，用于自监督预训练模仿学习模型，在保持高效性的同时实现跨任务、环境和具身的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型依赖大规模人工标注的动作数据集，模型体积庞大，难以在现实世界部署。需要更高效、实用的预训练方法。

Method: 提出LAWM框架，通过世界建模从无标签视频（机器人记录或人类日常动作视频）中学习潜在动作表示，实现自监督预训练。

Result: 在LIBERO基准测试和真实世界设置中，LAWM优于使用真实机器人动作训练的模型和类似预训练方法，同时显著更高效实用。

Conclusion: LAWM框架提供了一种高效实用的自监督预训练方法，能够有效迁移到不同任务、环境和具身设置，解决了现有VLA模型部署困难的问题。

Abstract: Vision-Language-Action (VLA) models have gained popularity for learning
robotic manipulation tasks that follow language instructions. State-of-the-art
VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually
labeled action datasets collected through teleoperation. More recent
approaches, including LAPA and villa-X, introduce latent action representations
that enable unsupervised pretraining on unlabeled datasets by modeling abstract
visual changes between frames. Although these methods have shown strong
results, their large model sizes make deployment in real-world settings
challenging. In this work, we propose LAWM, a model-agnostic framework to
pretrain imitation learning models in a self-supervised way, by learning latent
action representations from unlabeled video data through world modeling. These
videos can be sourced from robot recordings or videos of humans performing
actions with everyday objects. Our framework is designed to be effective for
transferring across tasks, environments, and embodiments. It outperforms models
trained with ground-truth robotics actions and similar pretraining methods on
the LIBERO benchmark and real-world setup, while being significantly more
efficient and practical for real-world settings.

</details>


### [9] [PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction](https://arxiv.org/abs/2509.18447)
*Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: PrioriTouch是一个用于在物理人机交互中处理多接触点优先级控制的框架，通过学习和排序控制目标来适应个体接触偏好，特别适用于护理场景。


<details>
  <summary>Details</summary>
Motivation: 在物理人机交互中，特别是护理任务中，不同身体部位可能有相互冲突的力需求，需要一种能够处理多接触点优先级的方法来确保安全性和舒适性。

Method: 结合学习排序方法和分层操作空间控制，利用仿真循环滚动进行数据高效的安全探索，从用户研究中获取个性化舒适阈值。

Result: 通过仿真和真实世界实验验证，PrioriTouch能够适应用户接触偏好，保持任务性能，并提高安全性和舒适性。

Conclusion: PrioriTouch框架为多接触点物理人机交互提供了一种有效的优先级控制方法，在护理等场景中具有广泛应用前景。

Abstract: Physical human-robot interaction (pHRI) requires robots to adapt to
individual contact preferences, such as where and how much force is applied.
Identifying preferences is difficult for a single contact; with whole-arm
interaction involving multiple simultaneous contacts between the robot and
human, the challenge is greater because different body parts can impose
incompatible force requirements. In caregiving tasks, where contact is frequent
and varied, such conflicts are unavoidable. With multiple preferences across
multiple contacts, no single solution can satisfy all objectives--trade-offs
are inherent, making prioritization essential. We present PrioriTouch, a
framework for ranking and executing control objectives across multiple
contacts. PrioriTouch can prioritize from a general collection of controllers,
making it applicable not only to caregiving scenarios such as bed bathing and
dressing but also to broader multi-contact settings. Our method combines a
novel learning-to-rank approach with hierarchical operational space control,
leveraging simulation-in-the-loop rollouts for data-efficient and safe
exploration. We conduct a user study on physical assistance preferences, derive
personalized comfort thresholds, and incorporate them into PrioriTouch. We
evaluate PrioriTouch through extensive simulation and real-world experiments,
demonstrating its ability to adapt to user contact preferences, maintain task
performance, and enhance safety and comfort. Website:
https://emprise.cs.cornell.edu/prioritouch.

</details>


### [10] [Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands](https://arxiv.org/abs/2509.18455)
*Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: 本文提出了一种基于几何感知的多指灵巧手推拉操作（GD2P）方法，通过接触引导采样生成多样化手部姿态，利用物理模拟筛选，并训练扩散模型来预测可行的操作姿态，实现了对难以抓取物体的非抓取式操作。


<details>
  <summary>Details</summary>
Motivation: 现有的非抓取式操作主要依赖平行夹爪或工具，而多指灵巧手能提供更丰富的接触模式和稳定性，但非抓取式操作的动力学建模困难。因此，需要一种能利用灵巧手优势的方法来处理多样化的物体。

Method: 通过接触引导采样生成多样化手部姿态，使用物理模拟进行筛选，训练条件扩散模型根据物体几何预测可行姿态，最后使用标准运动规划器选择和执行推拉动作。

Result: 在Allegro Hand上进行了840次真实世界实验，结果表明GD2P为训练灵巧非抓取操作策略提供了可扩展的途径。在LEAP Hand上的进一步演示也验证了其对不同手部形态的适用性。

Conclusion: GD2P方法有效利用了多指灵巧手的优势，为非抓取式操作提供了可行的解决方案，其预训练模型和数据集（包含130万手部姿态和2300个物体）将开源以促进进一步研究。

Abstract: Nonprehensile manipulation, such as pushing and pulling, enables robots to
move, align, or reposition objects that may be difficult to grasp due to their
geometry, size, or relationship to the robot or the environment. Much of the
existing work in nonprehensile manipulation relies on parallel-jaw grippers or
tools such as rods and spatulas. In contrast, multi-fingered dexterous hands
offer richer contact modes and versatility for handling diverse objects to
provide stable support over the objects, which compensates for the difficulty
of modeling the dynamics of nonprehensile manipulation. Therefore, we propose
Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile
manipulation with dexterous robotic hands. We study pushing and pulling by
framing the problem as synthesizing and learning pre-contact dexterous hand
poses that lead to effective manipulation. We generate diverse hand poses via
contact-guided sampling, filter them using physics simulation, and train a
diffusion model conditioned on object geometry to predict viable poses. At test
time, we sample hand poses and use standard motion planners to select and
execute pushing and pulling actions. We perform 840 real-world experiments with
an Allegro Hand, comparing our method to baselines. The results indicate that
GD2P offers a scalable route for training dexterous nonprehensile manipulation
policies. We further demonstrate GD2P on a LEAP Hand, highlighting its
applicability to different hand morphologies. Our pre-trained models and
dataset, including 1.3 million hand poses across 2.3k objects, will be
open-source to facilitate further research. Our project website is available
at: geodex2p.github.io.

</details>


### [11] [A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems](https://arxiv.org/abs/2509.18460)
*Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh*

Main category: cs.RO

TL;DR: 提出了一种基于反事实推理的感知系统故障检测与隔离框架，利用分析冗余而非物理冗余，通过被动和主动方法实现故障诊断。


<details>
  <summary>Details</summary>
Motivation: 感知系统对自主系统决策至关重要，但感知故障具有环境依赖性且在多阶段管道中传播，传统方法难以有效检测和隔离这些故障。

Method: 采用反事实推理构建感知可靠性测试，将主动故障检测与隔离建模为因果多臂赌博机问题，使用蒙特卡洛树搜索和上置信界算法优化控制输入以最大化有效信息。

Result: 在机器人探索场景中验证了方法的有效性，能够成功隔离由传感器损坏、动态场景和感知退化引起的故障。

Conclusion: 该框架为感知系统提供了一种有效的故障诊断方法，特别适用于环境依赖性和多阶段传播的感知故障场景。

Abstract: Perception systems provide a rich understanding of the environment for
autonomous systems, shaping decisions in all downstream modules. Hence,
accurate detection and isolation of faults in perception systems is important.
Faults in perception systems pose particular challenges: faults are often tied
to the perceptual context of the environment, and errors in their multi-stage
pipelines can propagate across modules. To address this, we adopt a
counterfactual reasoning approach to propose a framework for fault detection
and isolation (FDI) in perception systems. As opposed to relying on physical
redundancy (i.e., having extra sensors), our approach utilizes analytical
redundancy with counterfactual reasoning to construct perception reliability
tests as causal outcomes influenced by system states and fault scenarios.
Counterfactual reasoning generates reliability test results under hypothesized
faults to update the belief over fault hypotheses. We derive both passive and
active FDI methods. While the passive FDI can be achieved by belief updates,
the active FDI approach is defined as a causal bandit problem, where we utilize
Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find
control inputs that maximize a detection and isolation metric, designated as
Effective Information (EI). The mentioned metric quantifies the informativeness
of control inputs for FDI. We demonstrate the approach in a robot exploration
scenario, where a space robot performing vision-based navigation actively
adjusts its attitude to increase EI and correctly isolate faults caused by
sensor damage, dynamic scenes, and perceptual degradation.

</details>


### [12] [Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task](https://arxiv.org/abs/2509.18463)
*Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel*

Main category: cs.RO

TL;DR: 本文提出了一种基于奖励函数突变的新框架，通过在高斯噪声作用下对奖励函数权重进行变异，在机器人液体倾倒任务中实现了多样化的技能学习。


<details>
  <summary>Details</summary>
Motivation: 受人类运动控制中成本效益权衡模型的启发，研究旨在探索如何通过故意突变奖励函数来产生机器人操作任务中的多样化技能变异，而不仅仅是优化单一目标。

Method: 开发了基于高斯噪声的奖励函数突变框架，奖励函数包含准确性、时间和努力三个关键项。在NVIDIA Isaac Sim仿真环境中使用Franka Emika Panda机械臂进行液体倾倒实验，采用近端策略优化算法。

Result: 不同权重配置的突变奖励函数产生了广泛的行为策略：从原始倾倒任务执行的变化到新颖技能，如容器边缘清洁、液体混合和浇水等意外有用的任务。

Conclusion: 该方法为机器人系统在执行特定任务的多样化学习提供了有前景的方向，同时可能为未来任务衍生出有意义的技能。

Abstract: This paper explores how deliberate mutations of reward function in
reinforcement learning can produce diversified skill variations in robotic
manipulation tasks, examined with a liquid pouring use case. To this end, we
developed a new reward function mutation framework that is based on applying
Gaussian noise to the weights of the different terms in the reward function.
Inspired by the cost-benefit tradeoff model from human motor control, we
designed the reward function with the following key terms: accuracy, time, and
effort. The study was performed in a simulation environment created in NVIDIA
Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a
glass with a liquid that needed to be poured into a container. The
reinforcement learning algorithm was based on Proximal Policy Optimization. We
systematically explored how different configurations of mutated weights in the
rewards function would affect the learned policy. The resulting policies
exhibit a wide range of behaviours: from variations in execution of the
originally intended pouring task to novel skills useful for unexpected tasks,
such as container rim cleaning, liquid mixing, and watering. This approach
offers promising directions for robotic systems to perform diversified learning
of specific tasks, while also potentially deriving meaningful skills for future
tasks.

</details>


### [13] [RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain](https://arxiv.org/abs/2509.18466)
*Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao*

Main category: cs.RO

TL;DR: 提出了一种针对双足机器人在粗糙和光滑地形上行走的RL增强MPC框架，通过参数化系统动力学、摆动腿控制器和步态频率，结合MPC和RL的优势。


<details>
  <summary>Details</summary>
Motivation: MPC在双足行走中有效但难以建模复杂地形交互，RL在多样化地形上训练稳健策略但缺乏约束保证，需要结合两者优势。

Method: 参数化基于单刚体动力学的MPC的三个关键组件：系统动力学、摆动腿控制器和步态频率，在NVIDIA IsaacLab中进行双足机器人仿真验证。

Result: 实验结果表明，RL增强的MPC框架在楼梯、踏脚石和低摩擦表面等多样化地形上，比基线MPC和RL产生更自适应和稳健的行为。

Conclusion: RL增强的MPC框架成功结合了MPC的约束保证和RL的地形适应性，为双足机器人在挑战性环境中的行走提供了有效解决方案。

Abstract: Model predictive control (MPC) has demonstrated effectiveness for humanoid
bipedal locomotion; however, its applicability in challenging environments,
such as rough and slippery terrain, is limited by the difficulty of modeling
terrain interactions. In contrast, reinforcement learning (RL) has achieved
notable success in training robust locomotion policies over diverse terrain,
yet it lacks guarantees of constraint satisfaction and often requires
substantial reward shaping. Recent efforts in combining MPC and RL have shown
promise of taking the best of both worlds, but they are primarily restricted to
flat terrain or quadrupedal robots. In this work, we propose an RL-augmented
MPC framework tailored for bipedal locomotion over rough and slippery terrain.
Our method parametrizes three key components of
single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,
and gait frequency. We validate our approach through bipedal robot simulations
in NVIDIA IsaacLab across various terrains, including stairs, stepping stones,
and low-friction surfaces. Experimental results demonstrate that our
RL-augmented MPC framework produces significantly more adaptive and robust
behaviors compared to baseline MPC and RL.

</details>


### [14] [Spatial Envelope MPC: High Performance Driving without a Reference](https://arxiv.org/abs/2509.18506)
*Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal*

Main category: cs.RO

TL;DR: 提出了一种基于包络线的模型预测控制框架，使自动驾驶车辆能够在没有预定义参考轨迹的情况下处理高性能驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的规划和控制框架主要基于参考轨迹，这在车辆需要达到动态极限的高性能驾驶场景中表现受限，因为跟随预定义参考轨迹可能不是最优甚至不可行。

Method: 首先开发了针对优化控制的计算高效车辆动力学模型和连续可微的数学公式，准确捕捉整个可行驶包络线。然后结合强化学习和优化技术解决包络规划问题。

Result: 通过仿真和真实世界实验验证，该框架在赛车、紧急避障和越野导航等多种任务中表现出高性能。

Conclusion: 该框架展示了在不同场景下的可扩展性和广泛适用性，为高性能自动驾驶提供了有效的解决方案。

Abstract: This paper presents a novel envelope based model predictive control (MPC)
framework designed to enable autonomous vehicles to handle high performance
driving across a wide range of scenarios without a predefined reference. In
high performance autonomous driving, safe operation at the vehicle's dynamic
limits requires a real time planning and control framework capable of
accounting for key vehicle dynamics and environmental constraints when
following a predefined reference trajectory is suboptimal or even infeasible.
State of the art planning and control frameworks, however, are predominantly
reference based, which limits their performance in such situations. To address
this gap, this work first introduces a computationally efficient vehicle
dynamics model tailored for optimization based control and a continuously
differentiable mathematical formulation that accurately captures the entire
drivable envelope. This novel model and formulation allow for the direct
integration of dynamic feasibility and safety constraints into a unified
planning and control framework, thereby removing the necessity for predefined
references. The challenge of envelope planning, which refers to maximally
approximating the safe drivable area, is tackled by combining reinforcement
learning with optimization techniques. The framework is validated through both
simulations and real world experiments, demonstrating its high performance
across a variety of tasks, including racing, emergency collision avoidance and
off road navigation. These results highlight the framework's scalability and
broad applicability across a diverse set of scenarios.

</details>


### [15] [LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA](https://arxiv.org/abs/2509.18576)
*Zeyi Kang,Liang He,Yanxin Zhang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 提出轻量级LCMF级联注意力框架，通过多级跨模态参数共享机制解决多模态语义学习中的异构数据融合和计算效率问题


<details>
  <summary>Details</summary>
Motivation: 解决多模态语义学习在异构数据有效融合和资源受限环境计算效率方面的技术挑战，为具身智能中的人机交互应用提供高效解决方案

Method: 在Mamba模块中引入多级跨模态参数共享机制，结合交叉注意力和选择性参数共享状态空间模型的优势，实现异构模态的高效融合和语义互补对齐

Result: 在VQA任务中达到74.29%准确率，在EQA视频任务中达到LLM Agents分布集群的中等性能水平，FLOPs相比基线平均减少4.35倍，参数量仅为166.51M（图像-文本）和219M（视频-文本）

Conclusion: LCMF框架为资源受限场景下的人机交互应用提供了具有强大多模态决策泛化能力的高效解决方案

Abstract: Multimodal semantic learning plays a critical role in embodied intelligence,
especially when robots perceive their surroundings, understand human
instructions, and make intelligent decisions. However, the field faces
technical challenges such as effective fusion of heterogeneous data and
computational efficiency in resource-constrained environments. To address these
challenges, this study proposes the lightweight LCMF cascaded attention
framework, introducing a multi-level cross-modal parameter sharing mechanism
into the Mamba module. By integrating the advantages of Cross-Attention and
Selective parameter-sharing State Space Models (SSMs), the framework achieves
efficient fusion of heterogeneous modalities and semantic complementary
alignment. Experimental results show that LCMF surpasses existing multimodal
baselines with an accuracy of 74.29% in VQA tasks and achieves competitive
mid-tier performance within the distribution cluster of Large Language Model
Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a
4.35-fold reduction in FLOPs relative to the average of comparable baselines
while using only 166.51M parameters (image-text) and 219M parameters
(video-text), providing an efficient solution for Human-Robot Interaction (HRI)
applications in resource-constrained scenarios with strong multimodal decision
generalization capabilities.

</details>


### [16] [VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)
*Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: VLN-Zero是一个两阶段的视觉语言导航框架，利用视觉语言模型构建符号场景图，实现零样本神经符号导航。该框架通过快速探索、符号推理和缓存执行，在未见环境中实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖穷举探索或僵化的导航策略，泛化能力差。需要一种能够在未见环境中快速适应且具有良好泛化能力的导航框架。

Method: 两阶段框架：探索阶段使用结构化提示引导VLM搜索构建紧凑场景图；部署阶段使用神经符号规划器在场景图上推理生成可执行计划，并通过缓存执行模块加速适应。

Result: VLN-Zero在零样本模型中成功率提高2倍，超越大多数微调基线，到达目标位置时间减半，VLM调用减少55%。

Conclusion: 该框架克服了现有视觉语言导航方法的计算低效和泛化能力差的问题，实现了在未见环境中鲁棒且可扩展的决策。

Abstract: Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.

</details>


### [17] [Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills](https://arxiv.org/abs/2509.18597)
*Yuan Meng,Zhenguo Sun,Max Fest,Xukun Li,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了一种人机协作框架，通过将人类修正编码为可重用技能来解决LLM在机器人操作代码生成中的局限性，包括噪声、固定原语限制和长时程任务处理困难。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的机器人代码生成方法存在噪声大、受限于固定原语、上下文窗口有限、难以处理长时程任务等问题，且闭环反馈中的修正知识存储格式不当，限制了泛化能力并导致灾难性遗忘。

Method: 采用人机协作框架，将人类修正编码为可重用技能，结合外部存储和检索增强生成技术，配备提示机制实现动态重用。

Result: 在Ravens、Franka Kitchen和MetaWorld等环境以及真实场景中的实验显示，该框架达到0.93的成功率（比基线高27%），修正轮次效率提升42%，并能稳健解决需要规划超过20个原语的极端长时程任务。

Conclusion: 该人机协作框架有效解决了LLM在机器人代码生成中的关键挑战，显著提升了成功率和效率，特别是在复杂长时程任务中表现出色。

Abstract: Large language models (LLMs)-based code generation for robotic manipulation
has recently shown promise by directly translating human instructions into
executable code, but existing methods remain noisy, constrained by fixed
primitives and limited context windows, and struggle with long-horizon tasks.
While closed-loop feedback has been explored, corrected knowledge is often
stored in improper formats, restricting generalization and causing catastrophic
forgetting, which highlights the need for learning reusable skills. Moreover,
approaches that rely solely on LLM guidance frequently fail in extremely
long-horizon scenarios due to LLMs' limited reasoning capability in the robotic
domain, where such issues are often straightforward for humans to identify. To
address these challenges, we propose a human-in-the-loop framework that encodes
corrections into reusable skills, supported by external memory and
Retrieval-Augmented Generation with a hint mechanism for dynamic reuse.
Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world
settings, show that our framework achieves a 0.93 success rate (up to 27%
higher than baselines) and a 42% efficiency improvement in correction rounds.
It can robustly solve extremely long-horizon tasks such as "build a house",
which requires planning over 20 primitives.

</details>


### [18] [End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2509.18608)
*Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出一种基于端到端学习的导航系统，使用深度强化学习策略将原始3D LiDAR数据直接映射到控制命令，解决农业冠层下环境中的可靠导航问题。


<details>
  <summary>Details</summary>
Motivation: 解决农业冠层环境下GNSS不可靠、行间杂乱和光照变化等导航挑战，避免对标记数据集或手动设计控制接口的依赖。

Method: 采用体素降采样策略将LiDAR输入尺寸减少95.83%，在仿真环境中训练深度强化学习策略，直接处理原始3D LiDAR数据生成控制命令。

Result: 在仿真验证中，直线行种植场景达到100%成功率，但随着行弯曲度增加性能逐渐下降，测试了不同正弦频率和振幅下的表现。

Conclusion: 该方法展示了在仿真环境中训练的学习策略能够有效处理农业环境导航问题，为实际应用提供了可行方案。

Abstract: Reliable navigation in under-canopy agricultural environments remains a
challenge due to GNSS unreliability, cluttered rows, and variable lighting. To
address these limitations, we present an end-to-end learning-based navigation
system that maps raw 3D LiDAR data directly to control commands using a deep
reinforcement learning policy trained entirely in simulation. Our method
includes a voxel-based downsampling strategy that reduces LiDAR input size by
95.83%, enabling efficient policy learning without relying on labeled datasets
or manually designed control interfaces. The policy was validated in
simulation, achieving a 100% success rate in straight-row plantations and
showing a gradual decline in performance as row curvature increased, tested
across varying sinusoidal frequencies and amplitudes.

</details>


### [19] [PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving](https://arxiv.org/abs/2509.18609)
*Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr*

Main category: cs.RO

TL;DR: PIE是一个端到端运动规划框架，通过先进感知、推理和意图建模动态捕捉自车与周围智能体交互，在NAVSIM基准测试中超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 端到端运动规划虽然简化了自动驾驶流程，但场景理解和有效预测决策仍是部署的主要挑战。

Method: 采用双向Mamba融合解决多模态数据压缩损失，推理增强解码器结合Mamba和Mixture-of-Experts优化轨迹推断，动作-运动交互模块利用周围智能体状态预测优化自车规划。

Result: 在NAVSIM基准测试中，不使用集成和数据增强技术，获得88.9 PDM分数和85.6 EPDM分数，超越现有最优方法。

Conclusion: PIE能够可靠生成可行且高质量的自车轨迹，定量和定性分析验证了其有效性。

Abstract: End-to-end motion planning is promising for simplifying complex autonomous
driving pipelines. However, challenges such as scene understanding and
effective prediction for decision-making continue to present substantial
obstacles to its large-scale deployment. In this paper, we present PIE, a
pioneering framework that integrates advanced perception, reasoning, and
intention modeling to dynamically capture interactions between the ego vehicle
and surrounding agents. It incorporates a bidirectional Mamba fusion that
addresses data compression losses in multimodal fusion of camera and LiDAR
inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and
Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize
adaptive trajectory inference. PIE adopts an action-motion interaction module
to effectively utilize state predictions of surrounding agents to refine ego
planning. The proposed framework is thoroughly validated on the NAVSIM
benchmark. PIE, without using any ensemble and data augmentation techniques,
achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of
prior state-of-the-art methods. Comprehensive quantitative and qualitative
analyses demonstrate that PIE is capable of reliably generating feasible and
high-quality ego trajectories.

</details>


### [20] [SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones](https://arxiv.org/abs/2509.18610)
*Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager*

Main category: cs.RO

TL;DR: SINGER是一个基于大视觉语言模型的开源词汇无人机导航系统，仅使用机载传感和计算实现语言引导的自主导航，通过模拟器训练和轻量级策略实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇自主无人机导航的挑战，包括大规模演示数据稀缺、无人机实时控制需求以及缺乏可靠的外部姿态估计模块。

Method: 使用三个核心组件：(i)基于高斯泼溅的光真实语言嵌入飞行模拟器，(ii)RRT启发的多轨迹生成专家用于无碰撞导航演示，(iii)轻量级端到端视觉运动策略进行实时闭环控制。

Result: 在硬件飞行实验中，SINGER在未见环境和未见语言条件目标物体上表现出优异的零样本模拟到真实迁移性能，平均到达查询目标率提高23.33%，视野保持率提高16.67%，碰撞减少10%。

Conclusion: SINGER成功实现了仅使用机载传感的语言引导自主无人机导航，为开放词汇无人机导航提供了有效解决方案。

Abstract: Large vision-language models have driven remarkable progress in
open-vocabulary robot policies, e.g., generalist robot manipulation policies,
that enable robots to complete complex tasks specified in natural language.
Despite these successes, open-vocabulary autonomous drone navigation remains an
unsolved challenge due to the scarcity of large-scale demonstrations, real-time
control demands of drones for stabilization, and lack of reliable external pose
estimation modules. In this work, we present SINGER for language-guided
autonomous drone navigation in the open world using only onboard sensing and
compute. To train robust, open-vocabulary navigation policies, SINGER leverages
three central components: (i) a photorealistic language-embedded flight
simulator with minimal sim-to-real gap using Gaussian Splatting for efficient
data generation, (ii) an RRT-inspired multi-trajectory generation expert for
collision-free navigation demonstrations, and these are used to train (iii) a
lightweight end-to-end visuomotor policy for real-time closed-loop control.
Through extensive hardware flight experiments, we demonstrate superior
zero-shot sim-to-real transfer of our policy to unseen environments and unseen
language-conditioned goal objects. When trained on ~700k-1M observation action
pairs of language conditioned visuomotor data and deployed on hardware, SINGER
outperforms a velocity-controlled semantic guidance baseline by reaching the
query 23.33% more on average, and maintains the query in the field of view
16.67% more on average, with 10% fewer collisions.

</details>


### [21] [The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving](https://arxiv.org/abs/2509.18626)
*Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone*

Main category: cs.RO

TL;DR: 该论文提出了一种基于事故报告检索的自动驾驶决策系统，通过将事故叙述转换为自我中心视角的统一表示，在决策时检索相关先例来改进安全性能边界附近的决策。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的自动驾驶系统主要在无事故数据上训练，对安全性能边界附近的指导不足。真实事故报告包含所需的对比证据，但难以使用，因为叙述是非结构化、第三人称且与传感器视图关联性差。

Method: 将事故叙述标准化为自我中心语言，将日志和事故转换为统一的场景-动作表示用于检索。在决策时，系统通过从统一索引中检索相关先例来裁决提议的动作；还包含一个反事实扩展，提出合理替代方案并为每个方案检索结果，在决策前进行跨结果推理。

Result: 在nuScenes基准测试中，先例检索显著改善了校准，上下文偏好动作的召回率从24%提高到53%。反事实变体在保持这些收益的同时，在风险附近使决策更加敏锐。

Conclusion: 通过将事故报告转换为可检索的统一表示，该系统能够有效利用事故数据改进自动驾驶系统在安全边界附近的决策性能，特别是在高风险场景下表现出更好的决策能力。

Abstract: Learning-based autonomous driving systems are trained mostly on incident-free
data, offering little guidance near safety-performance boundaries. Real crash
reports contain precisely the contrastive evidence needed, but they are hard to
use: narratives are unstructured, third-person, and poorly grounded to sensor
views. We address these challenges by normalizing crash narratives to
ego-centric language and converting both logs and crashes into a unified
scene-action representation suitable for retrieval. At decision time, our
system adjudicates proposed actions by retrieving relevant precedents from this
unified index; an agentic counterfactual extension proposes plausible
alternatives, retrieves for each, and reasons across outcomes before deciding.
On a nuScenes benchmark, precedent retrieval substantially improves
calibration, with recall on contextually preferred actions rising from 24% to
53%. The counterfactual variant preserves these gains while sharpening
decisions near risk.

</details>


### [22] [Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training](https://arxiv.org/abs/2509.18631)
*Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu*

Main category: cs.RO

TL;DR: 提出了一种统一的仿真与真实世界协同训练框架，通过少量真实演示和大量仿真数据学习泛化性强的机器人操作策略，核心是学习域不变的任务相关特征空间。


<details>
  <summary>Details</summary>
Motivation: 行为克隆在机器人操作中表现良好，但真实世界演示成本高且难以规模化获取。仿真数据虽然可扩展，但存在仿真到现实的领域差距问题。

Method: 采用最优传输（OT）损失在协同训练框架中对齐跨域的观察-动作联合分布，并扩展到非平衡OT框架处理仿真与真实数据不平衡问题。

Result: 在挑战性操作任务上验证，可提升真实世界成功率高达30%，并能泛化到仅仿真中见过的场景。

Conclusion: 该方法通过联合分布对齐有效缩小仿真与现实差距，显著提升策略的泛化能力。

Abstract: Behavior cloning has shown promise for robot manipulation, but real-world
demonstrations are costly to acquire at scale. While simulated data offers a
scalable alternative, particularly with advances in automated demonstration
generation, transferring policies to the real world is hampered by various
simulation and real domain gaps. In this work, we propose a unified
sim-and-real co-training framework for learning generalizable manipulation
policies that primarily leverages simulation and only requires a few real-world
demonstrations. Central to our approach is learning a domain-invariant,
task-relevant feature space. Our key insight is that aligning the joint
distributions of observations and their corresponding actions across domains
provides a richer signal than aligning observations (marginals) alone. We
achieve this by embedding an Optimal Transport (OT)-inspired loss within the
co-training framework, and extend this to an Unbalanced OT framework to handle
the imbalance between abundant simulation data and limited real-world examples.
We validate our method on challenging manipulation tasks, showing it can
leverage abundant simulation data to achieve up to a 30% improvement in the
real-world success rate and even generalize to scenarios seen only in
simulation.

</details>


### [23] [Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments](https://arxiv.org/abs/2509.18636)
*Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao*

Main category: cs.RO

TL;DR: 提出了一种基于可变形虚拟结构（DVS）的无人机编队规划方法，能够在狭窄环境中处理无人机数量变化的情况，实现快速编队恢复和环境适应性。


<details>
  <summary>Details</summary>
Motivation: 解决在狭窄环境中无人机数量变化时编队维护困难的问题，传统方法难以收敛到期望配置。

Method: 采用Lloyd算法进行均匀分区和匈牙利算法进行分配（PAAS），结合基于基元的路径搜索和非线性轨迹优化来规划DVS的时空轨迹，实现自适应转换和仿射变换。

Result: 在模拟环境中支持最多15%的无人机加入或离开，并能快速恢复期望编队形状，相比现有方法展现出更快的编队恢复能力和环境适应性。

Conclusion: 该方法在真实世界实验中验证了有效性和鲁棒性，为动态无人机编队规划提供了实用解决方案。

Abstract: Formation maintenance with varying number of drones in narrow environments
hinders the convergence of planning to the desired configurations. To address
this challenge, this paper proposes a formation planning method guided by
Deformable Virtual Structures (DVS) with continuous spatiotemporal
transformation. Firstly, to satisfy swarm safety distance and preserve
formation shape filling integrity for irregular formation geometries, we employ
Lloyd algorithm for uniform $\underline{PA}$rtitioning and Hungarian algorithm
for $\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal
trajectory involving DVS is planned using primitive-based path search and
nonlinear trajectory optimization. The DVS trajectory achieves adaptive
transitions with respect to a varying number of drones while ensuring
adaptability to narrow environments through affine transformation. Finally,
each agent conducts distributed trajectory planning guided by desired
spatiotemporal positions within the DVS, while incorporating collision
avoidance and dynamic feasibility requirements. Our method enables up to 15\%
of swarm numbers to join or leave in cluttered environments while rapidly
restoring the desired formation shape in simulation. Compared to cutting-edge
formation planning method, we demonstrate rapid formation recovery capacity and
environmental adaptability. Real-world experiments validate the effectiveness
and resilience of our formation planning method.

</details>


### [24] [Do You Need Proprioceptive States in Visuomotor Policies?](https://arxiv.org/abs/2509.18644)
*Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出了一种无状态策略（State-free Policy），通过移除本体感知状态输入，仅基于视觉观察预测动作，从而解决了传统模仿学习中策略过度依赖本体感知状态导致的空间泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于模仿学习的视觉运动策略通常同时使用视觉观察和本体感知状态进行精确控制，但研究发现这种做法会使策略过度依赖本体感知状态输入，导致对训练轨迹的过拟合和空间泛化能力差。

Method: 提出无状态策略，移除本体感知状态输入，仅基于视觉观察预测动作。策略构建在相对末端执行器动作空间中，并通过双广角腕部摄像头提供完整的任务相关视觉观察。

Result: 实证结果表明，无状态策略在空间泛化方面显著优于基于状态的策略：在真实世界任务中，高度泛化的平均成功率从0%提升到85%，水平泛化从6%提升到64%。同时还在数据效率和跨具身适应方面表现出优势。

Conclusion: 无状态策略通过减少对本体感知状态的依赖，显著提高了机器人操作任务的空间泛化能力，增强了在真实世界部署的实用性。

Abstract: Imitation-learning-based visuomotor policies have been widely used in robot
manipulation, where both visual observations and proprioceptive states are
typically adopted together for precise control. However, in this study, we find
that this common practice makes the policy overly reliant on the proprioceptive
state input, which causes overfitting to the training trajectories and results
in poor spatial generalization. On the contrary, we propose the State-free
Policy, removing the proprioceptive state input and predicting actions only
conditioned on visual observations. The State-free Policy is built in the
relative end-effector action space, and should ensure the full task-relevant
visual observations, here provided by dual wide-angle wrist cameras. Empirical
results demonstrate that the State-free policy achieves significantly stronger
spatial generalization than the state-based policy: in real-world tasks such as
pick-and-place, challenging shirt-folding, and complex whole-body manipulation,
spanning multiple robot embodiments, the average success rate improves from 0\%
to 85\% in height generalization and from 6\% to 64\% in horizontal
generalization. Furthermore, they also show advantages in data efficiency and
cross-embodiment adaptation, enhancing their practicality for real-world
deployment.

</details>


### [25] [SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer](https://arxiv.org/abs/2509.18648)
*Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause*

Main category: cs.RO

TL;DR: SPiDR是一种基于悲观域随机化的安全强化学习算法，用于解决模拟到现实转换中的安全问题，通过将模拟到现实的差距不确定性纳入安全约束来保证现实环境中的安全性。


<details>
  <summary>Details</summary>
Motivation: 现实世界部署强化学习时安全性是主要挑战，模拟器训练存在模拟到现实的差距，传统鲁棒安全RL方法难以与可扩展训练流程兼容，而流行的域随机化方法在实践中往往导致不安全行为。

Method: SPiDR使用域随机化技术将模拟到现实差距的不确定性纳入安全约束，采用悲观策略来确保在现实条件下的安全性，同时保持与现有训练流程的高度兼容性。

Result: 通过在模拟到模拟基准测试和两个不同的真实机器人平台上的广泛实验，证明SPiDR能够有效确保模拟到现实差距下的安全性，同时保持强大的性能表现。

Conclusion: SPiDR提供了一种具有可证明保证的安全模拟到现实转换方法，在保持高性能的同时有效解决了现实部署中的安全问题。

Abstract: Safety remains a major concern for deploying reinforcement learning (RL) in
real-world applications. Simulators provide safe, scalable training
environments, but the inevitable sim-to-real gap introduces additional safety
concerns, as policies must satisfy constraints in real-world conditions that
differ from simulation. To address this challenge, robust safe RL techniques
offer principled methods, but are often incompatible with standard scalable
training pipelines. In contrast, domain randomization, a simple and popular
sim-to-real technique, stands out as a promising alternative, although it often
results in unsafe behaviors in practice. We present SPiDR, short for
Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with
provable guarantees for safe sim-to-real transfer. SPiDR uses domain
randomization to incorporate the uncertainty about the sim-to-real gap into the
safety constraints, making it versatile and highly compatible with existing
training pipelines. Through extensive experiments on sim-to-sim benchmarks and
two distinct real-world robotic platforms, we demonstrate that SPiDR
effectively ensures safety despite the sim-to-real gap while maintaining strong
performance.

</details>


### [26] [Distributionally Robust Safe Motion Planning with Contextual Information](https://arxiv.org/abs/2509.18666)
*Kaizer Rahaman,Simran Kumari,Ashish R. Hota*

Main category: cs.RO

TL;DR: 提出一种结合上下文信息的分布鲁棒碰撞避免方法，通过条件核均值嵌入在RKHS中建模障碍物未来轨迹的条件分布，构建分布不确定性集合，并在运动规划中引入分布鲁棒约束。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞避免方法往往忽略上下文信息或缺乏对分布不确定性的鲁棒性，导致在复杂场景中性能受限。

Method: 使用条件核均值嵌入在RKHS中表示障碍物轨迹的条件分布，构建基于经验估计的分布模糊集，并在模型预测控制框架中引入分布鲁棒碰撞避免约束。

Result: 仿真结果表明，该方法在多个挑战性场景中比不考虑上下文信息和分布鲁棒性的方法具有更好的碰撞避免性能。

Conclusion: 所提出的分布鲁棒方法通过有效利用上下文信息和处理分布不确定性，显著提升了碰撞避免的可靠性。

Abstract: We present a distributionally robust approach for collision avoidance by
incorporating contextual information. Specifically, we embed the conditional
distribution of future trajectory of the obstacle conditioned on the motion of
the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional
kernel mean embedding operator. Then, we define an ambiguity set containing all
distributions whose embedding in the RKHS is within a certain distance from the
empirical estimate of conditional mean embedding learnt from past data.
Consequently, a distributionally robust collision avoidance constraint is
formulated, and included in the receding horizon based motion planning
formulation of the ego agent. Simulation results show that the proposed
approach is more successful in avoiding collision compared to approaches that
do not include contextual information and/or distributional robustness in their
formulation in several challenging scenarios.

</details>


### [27] [N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout](https://arxiv.org/abs/2509.18671)
*Kaixin Chai,Hyunjun Lee,Joseph J. Lim*

Main category: cs.RO

TL;DR: N2M是一个过渡模块，用于在机器人到达任务区域后引导其到更适合操作的初始姿态，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 移动操作中，导航模块只关注到达任务区域，而不考虑下游操作策略偏好的初始姿态，导致两者不匹配。

Method: N2M仅依赖自我中心观察，无需全局或历史信息，能实时适应环境变化，具有高视角鲁棒性和广泛适用性。

Result: 在PnPCounterToCab任务中，N2M将平均成功率从3%提升到54%；在Toybox Handover任务中，仅用15个数据样本就能在未见环境中提供可靠预测。

Conclusion: N2M通过解决导航与操作之间的姿态偏好不匹配问题，显著提升了移动操作系统的性能和泛化能力。

Abstract: In mobile manipulation, the manipulation policy has strong preferences for
initial poses where it is executed. However, the navigation module focuses
solely on reaching the task area, without considering which initial pose is
preferable for downstream manipulation. To address this misalignment, we
introduce N2M, a transition module that guides the robot to a preferable
initial pose after reaching the task area, thereby substantially improving task
success rates. N2M features five key advantages: (1) reliance solely on
ego-centric observation without requiring global or historical information; (2)
real-time adaptation to environmental changes; (3) reliable prediction with
high viewpoint robustness; (4) broad applicability across diverse tasks,
manipulation policies, and robot hardware; and (5) remarkable data efficiency
and generalizability. We demonstrate the effectiveness of N2M through extensive
simulation and real-world experiments. In the PnPCounterToCab task, N2M
improves the averaged success rate from 3% with the reachability-based baseline
to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable
predictions even in unseen environments with only 15 data samples, showing
remarkable data efficiency and generalizability.

</details>


### [28] [3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space](https://arxiv.org/abs/2509.18676)
*Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee*

Main category: cs.RO

TL;DR: 本文提出3D Flow Diffusion Policy（3D FDP），一种利用场景级3D流作为结构化中间表示来捕捉精细局部运动线索的新框架，在机器人操作任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖直接观察到动作的映射或将感知输入压缩为全局或物体中心特征，往往忽略了精确和接触丰富操作所需的关键局部运动线索。

Method: 预测采样查询点的时间轨迹，并在统一的扩散架构中基于这些交互感知流来生成动作，将操作建立在局部动态基础上同时考虑动作的全局场景后果。

Result: 在MetaWorld基准测试的50个任务中达到最先进性能，特别是在中等和困难设置中表现优异；在8个真实机器人任务中持续优于现有基线方法。

Conclusion: 3D流作为学习可泛化视觉运动策略的强大结构化先验，支持开发更鲁棒和通用的机器人操作系统。

Abstract: Learning robust visuomotor policies that generalize across diverse objects
and interaction dynamics remains a central challenge in robotic manipulation.
Most existing approaches rely on direct observation-to-action mappings or
compress perceptual inputs into global or object-centric features, which often
overlook localized motion cues critical for precise and contact-rich
manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework
that leverages scene-level 3D flow as a structured intermediate representation
to capture fine-grained local motion cues. Our approach predicts the temporal
trajectories of sampled query points and conditions action generation on these
interaction-aware flows, implemented jointly within a unified diffusion
architecture. This design grounds manipulation in localized dynamics while
enabling the policy to reason about broader scene-level consequences of
actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP
achieves state-of-the-art performance across 50 tasks, particularly excelling
on medium and hard settings. Beyond simulation, we validate our method on eight
real-robot tasks, where it consistently outperforms prior baselines in
contact-rich and non-prehensile scenarios. These results highlight 3D flow as a
powerful structural prior for learning generalizable visuomotor policies,
supporting the development of more robust and versatile robotic manipulation.
Robot demonstrations, additional results, and code can be found at
https://sites.google.com/view/3dfdp/home.

</details>


### [29] [Query-Centric Diffusion Policy for Generalizable Robotic Assembly](https://arxiv.org/abs/2509.18686)
*Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao*

Main category: cs.RO

TL;DR: 提出Query-centric Diffusion Policy (QDP)分层框架，通过查询机制连接高层规划和底层控制，提升机器人装配任务的性能


<details>
  <summary>Details</summary>
Motivation: 机器人装配任务因零件交互复杂性和接触丰富环境中的噪声敏感性而具有挑战性，现有分层策略存在高层技能查询与底层执行不匹配的问题

Method: QDP框架使用包含物体、接触点和技能信息的查询来识别任务相关组件并指导底层策略，利用点云观测提高策略鲁棒性

Result: 在FurnitureBench仿真和真实环境实验中，QDP在技能精度和长时程成功率方面表现优异，在插入和拧螺丝任务中比无结构化查询的基线方法成功率提高50%以上

Conclusion: QDP通过查询中心机制有效解决了分层策略中的不匹配问题，为构建通用机器人提供了可行的解决方案

Abstract: The robotic assembly task poses a key challenge in building generalist robots
due to the intrinsic complexity of part interactions and the sensitivity to
noise perturbations in contact-rich settings. The assembly agent is typically
designed in a hierarchical manner: high-level multi-part reasoning and
low-level precise control. However, implementing such a hierarchical policy is
challenging in practice due to the mismatch between high-level skill queries
and low-level execution. To address this, we propose the Query-centric
Diffusion Policy (QDP), a hierarchical framework that bridges high-level
planning and low-level control by utilizing queries comprising objects, contact
points, and skill information. QDP introduces a query-centric mechanism that
identifies task-relevant components and uses them to guide low-level policies,
leveraging point cloud observations to improve the policy's robustness. We
conduct comprehensive experiments on the FurnitureBench in both simulation and
real-world settings, demonstrating improved performance in skill precision and
long-horizon success rate. In the challenging insertion and screwing tasks, QDP
improves the skill-wise success rate by over 50% compared to baselines without
structured queries.

</details>


### [30] [Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation](https://arxiv.org/abs/2509.18734)
*Nishant Doshi,Amey Sutvani,Sanket Gujar*

Main category: cs.RO

TL;DR: 使用强化学习训练配备深度相机的虚拟四轴飞行器在城市环境中导航


<details>
  <summary>Details</summary>
Motivation: 自主飞行器在城市环境中面临GPS精度下降、狭窄空间和动态障碍物等挑战，需要可靠的导航能力

Method: 采用强化学习方法，训练配备深度相机的虚拟四轴飞行器在模拟城市环境中进行导航

Result: 论文提出了基于强化学习的导航方法，但未在摘要中展示具体实验结果

Conclusion: 强化学习是解决城市环境中自主飞行器导航挑战的有效方法

Abstract: One of the challenges faced by Autonomous Aerial Vehicles is reliable
navigation through urban environments. Factors like reduction in precision of
Global Positioning System (GPS), narrow spaces and dynamically moving obstacles
make the path planning of an aerial robot a complicated task. One of the skills
required for the agent to effectively navigate through such an environment is
to develop an ability to avoid collisions using information from onboard depth
sensors. In this paper, we propose Reinforcement Learning of a virtual
quadcopter robot agent equipped with a Depth Camera to navigate through a
simulated urban environment.

</details>


### [31] [MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning](https://arxiv.org/abs/2509.18757)
*Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka*

Main category: cs.RO

TL;DR: MV-UMI框架通过整合第三人称视角与第一人称视角相机，解决了手持抓取器在模仿学习中场景上下文捕获不足的问题，提高了机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 手持抓取器作为数据收集工具虽然直观且可扩展，但仅依赖第一人称视角手腕相机难以捕获足够的场景上下文，限制了机器人操作任务的学习范围。

Method: 提出MV-UMI框架，整合第三人称视角与第一人称视角相机，减轻人类演示与机器人部署之间的领域偏移，同时保持手持设备的跨具身优势。

Result: 实验结果显示，MV-UMI在需要广泛场景理解的子任务中性能提升约47%，通过消融研究验证了方法的有效性。

Conclusion: MV-UMI框架扩展了手持抓取器系统可学习的操作任务范围，且不损害其固有的跨具身优势。

Abstract: Recent advances in imitation learning have shown great promise for developing
robust robot manipulation policies from demonstrations. However, this promise
is contingent on the availability of diverse, high-quality datasets, which are
not only challenging and costly to collect but are often constrained to a
specific robot embodiment. Portable handheld grippers have recently emerged as
intuitive and scalable alternatives to traditional robotic teleoperation
methods for data collection. However, their reliance solely on first-person
view wrist-mounted cameras often creates limitations in capturing sufficient
scene contexts. In this paper, we present MV-UMI (Multi-View Universal
Manipulation Interface), a framework that integrates a third-person perspective
with the egocentric camera to overcome this limitation. This integration
mitigates domain shifts between human demonstration and robot deployment,
preserving the cross-embodiment advantages of handheld data-collection devices.
Our experimental results, including an ablation study, demonstrate that our
MV-UMI framework improves performance in sub-tasks requiring broad scene
understanding by approximately 47% across 3 tasks, confirming the effectiveness
of our approach in expanding the range of feasible manipulation tasks that can
be learned using handheld gripper systems, without compromising the
cross-embodiment advantages inherent to such systems.

</details>


### [32] [VGGT-DP: Generalizable Robot Control via Vision Foundation Models](https://arxiv.org/abs/2509.18778)
*Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang*

Main category: cs.RO

TL;DR: VGGT-DP是一个视觉模仿学习框架，通过整合预训练3D感知模型的几何先验和本体感觉反馈，提高机器人的空间理解和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注策略设计，但忽视了视觉编码器的结构和能力，限制了空间理解和泛化。受生物视觉系统启发，需要结合视觉和本体感觉线索来实现鲁棒控制。

Method: 采用VGGT作为视觉编码器，引入本体感觉引导的视觉学习策略，设计帧级token重用机制减少推理延迟，并应用随机token剪枝增强策略鲁棒性。

Result: 在MetaWorld挑战任务上的实验表明，VGGT-DP显著优于DP和DP3等强基线方法，特别是在精度要求高和长时程场景中表现突出。

Conclusion: VGGT-DP通过整合几何先验和本体感觉反馈，有效提升了视觉模仿学习的空间定位和闭环控制性能，为机器人操作技能学习提供了更优的解决方案。

Abstract: Visual imitation learning frameworks allow robots to learn manipulation
skills from expert demonstrations. While existing approaches mainly focus on
policy design, they often neglect the structure and capacity of visual
encoders, limiting spatial understanding and generalization. Inspired by
biological vision systems, which rely on both visual and proprioceptive cues
for robust control, we propose VGGT-DP, a visuomotor policy framework that
integrates geometric priors from a pretrained 3D perception model with
proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer
(VGGT) as the visual encoder and introduce a proprioception-guided visual
learning strategy to align perception with internal robot states, improving
spatial grounding and closed-loop control. To reduce inference latency, we
design a frame-wise token reuse mechanism that compacts multi-view tokens into
an efficient spatial representation. We further apply random token pruning to
enhance policy robustness and reduce overfitting. Experiments on challenging
MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines
such as DP and DP3, particularly in precision-critical and long-horizon
scenarios.

</details>


### [33] [Human-Interpretable Uncertainty Explanations for Point Cloud Registration](https://arxiv.org/abs/2509.18786)
*Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel*

Main category: cs.RO

TL;DR: 本文提出了一种名为GP-CA的新方法，用于解决点云配准问题中的不确定性量化与归因问题，该方法通过主动学习发现新的不确定性来源，并在多个数据集和真实机器人实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法如ICP在传感器噪声、姿态估计误差和部分重叠等不确定性条件下容易失败，需要一种能够量化并解释配准不确定性的方法。

Method: 开发了高斯过程概念归因（GP-CA）方法，利用主动学习来发现配准问题中的不确定性来源，并通过查询信息实例来改进模型。

Result: 在三个公开数据集和真实机器人实验中验证了GP-CA的有效性，该方法在运行时间、样本效率和准确性方面均优于现有最先进方法。

Conclusion: GP-CA不仅能够有效量化配准不确定性，还能实现有效的故障恢复行为，提高机器人感知的鲁棒性。

Abstract: In this paper, we address the point cloud registration problem, where
well-known methods like ICP fail under uncertainty arising from sensor noise,
pose-estimation errors, and partial overlap due to occlusion. We develop a
novel approach, Gaussian Process Concept Attribution (GP-CA), which not only
quantifies registration uncertainty but also explains it by attributing
uncertainty to well-known sources of errors in registration problems. Our
approach leverages active learning to discover new uncertainty sources in the
wild by querying informative instances. We validate GP-CA on three publicly
available datasets and in our real-world robot experiment. Extensive ablations
substantiate our design choices. Our approach outperforms other
state-of-the-art methods in terms of runtime, high sample-efficiency with
active learning, and high accuracy. Our real-world experiment clearly
demonstrates its applicability. Our video also demonstrates that GP-CA enables
effective failure-recovery behaviors, yielding more robust robotic perception.

</details>


### [34] [Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations](https://arxiv.org/abs/2509.18793)
*Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein*

Main category: cs.RO

TL;DR: 本文提出了一种基于Kubernetes的需求驱动应用管理方法，用于解决大规模协同智能交通系统中动态环境下的应用编排挑战。


<details>
  <summary>Details</summary>
Motivation: 随着车辆自动化和互联程度的提高，协同智能交通系统需要高效的云原生技术来管理动态环境中的应用，但现有方法在资源利用效率方面存在不足。

Method: 开发了一个基于Kubernetes和ROS 2的应用管理框架，采用需求驱动的方法来自动化微服务的部署、重配置、更新和扩展等流程。

Result: 该方法能够根据C-ITS中不同实体的需求动态协调应用管理，减少计算资源消耗和网络流量，并在集体环境感知用例中进行了验证。

Conclusion: 提出的需求驱动应用管理框架有效解决了大规模C-ITS中的动态应用编排问题，为智能交通系统的云原生应用管理提供了可行方案。

Abstract: Vehicles are becoming increasingly automated and interconnected, enabling the
formation of cooperative intelligent transport systems (C-ITS) and the use of
offboard services. As a result, cloud-native techniques, such as microservices
and container orchestration, play an increasingly important role in their
operation. However, orchestrating applications in a large-scale C-ITS poses
unique challenges due to the dynamic nature of the environment and the need for
efficient resource utilization. In this paper, we present a demand-driven
application management approach that leverages cloud-native techniques -
specifically Kubernetes - to address these challenges. Taking into account the
demands originating from different entities within the C-ITS, the approach
enables the automation of processes, such as deployment, reconfiguration,
update, upgrade, and scaling of microservices. Executing these processes on
demand can, for example, reduce computing resource consumption and network
traffic. A demand may include a request for provisioning an external supporting
service, such as a collective environment model. The approach handles changing
and new demands by dynamically reconciling them through our proposed
application management framework built on Kubernetes and the Robot Operating
System (ROS 2). We demonstrate the operation of our framework in the C-ITS use
case of collective environment perception and make the source code of the
prototypical framework publicly available at
https://github.com/ika-rwth-aachen/application_manager .

</details>


### [35] [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://arxiv.org/abs/2509.18830)
*Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu*

Main category: cs.RO

TL;DR: DexSkin是一种柔软、可适应的电容式电子皮肤，能够实现灵敏、局部化和可校准的触觉传感，可用于机器人灵巧操作任务的学习。


<details>
  <summary>Details</summary>
Motivation: 人类皮肤提供丰富的触觉感知能力，能够在大面积曲面区域定位有意和无意的接触事件。为灵巧机器人操作系统复制这些触觉感知能力仍然是一个长期挑战。

Method: 开发了DexSkin电子皮肤系统，将其安装在平行夹爪手指上，提供几乎整个手指表面的触觉覆盖。在示范学习框架下评估其在挑战性操作任务中的能力，并展示了传感器实例间的模型迁移能力。

Result: DexSkin在需要整个手指表面传感覆盖的操作任务中表现出色，如物体在手中重新定向和将弹性带缠绕在盒子上。同时证明其适用于在线强化学习。

Conclusion: DexSkin展示了其在学习真实世界、接触丰富的操作任务中的适用性和实用性，为机器人触觉感知提供了有效的解决方案。

Abstract: Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.

</details>


### [36] [Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation](https://arxiv.org/abs/2509.18865)
*Masato Kobayashi,Thanpimon Buamanee*

Main category: cs.RO

TL;DR: Bi-VLA是一个基于双边控制的模仿学习框架，通过视觉-语言融合实现多任务动作生成，解决了传统双边控制方法只能处理单一任务的局限性


<details>
  <summary>Details</summary>
Motivation: 传统双边控制方法虽然利用关节角度、速度、扭矩和视觉信息进行精确操作，但需要针对每个任务训练特定模型，限制了方法的通用性

Method: Bi-VLA结合了领导者-跟随者双边控制的机器人关节数据（角度、速度、扭矩）与视觉特征和自然语言指令，通过SigLIP和FiLM-based融合技术实现多模态信息融合

Result: 在真实机器人实验中，Bi-VLA成功解读视觉-语言组合，相比传统双边控制模仿学习方法提高了任务成功率，特别是在需要语言补充提示和仅凭视觉可区分的两类任务中都表现出色

Conclusion: Bi-VLA解决了先前双边控制方法的单任务限制，实证表明结合视觉和语言能显著增强系统的通用性，在真实世界任务中验证了其有效性

Abstract: We propose Bilateral Control-Based Imitation Learning via Vision-Language
Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral
control-based imitation learning to handle more than one task within a single
model. Conventional bilateral control methods exploit joint angle, velocity,
torque, and vision for precise manipulation but require task-specific models,
limiting their generality. Bi-VLA overcomes this limitation by utilizing robot
joint angle, velocity, and torque data from leader-follower bilateral control
with visual features and natural language instructions through SigLIP and
FiLM-based fusion. We validated Bi-VLA on two task types: one requiring
supplementary language cues and another distinguishable solely by vision.
Real-robot experiments showed that Bi-VLA successfully interprets
vision-language combinations and improves task success rates compared to
conventional bilateral control-based imitation learning. Our Bi-VLA addresses
the single-task limitation of prior bilateral approaches and provides empirical
evidence that combining vision and language significantly enhances versatility.
Experimental results validate the effectiveness of Bi-VLA in real-world tasks.
For additional material, please visit the website:
https://mertcookimg.github.io/bi-vla/

</details>


### [37] [Lang2Morph: Language-Driven Morphological Design of Robotic Hands](https://arxiv.org/abs/2509.18937)
*Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes*

Main category: cs.RO

TL;DR: Lang2Morph是一个基于大语言模型的机器人手形态设计框架，能够将自然语言任务描述转化为可3D打印的特定任务形态设计。


<details>
  <summary>Details</summary>
Motivation: 传统机器人手设计依赖专家启发式方法和手动调整，自动化优化方法计算密集且依赖仿真。大语言模型具有广泛的人类-物体交互知识和强大生成能力，为零样本设计推理提供了有前景的替代方案。

Method: Lang2Morph包含两个主要阶段：(i) 形态设计：将任务映射为语义标签、结构语法和OPH兼容参数；(ii) 选择与精炼：基于语义对齐和尺寸兼容性评估设计候选，并在需要时应用LLM引导的精炼。

Result: 实验结果表明，该方法能够生成多样化且与任务相关的形态设计。

Conclusion: 这是首个基于LLM的任务条件化机器人手设计框架，展示了语言驱动设计方法的可行性。

Abstract: Designing robotic hand morphologies for diverse manipulation tasks requires
balancing dexterity, manufacturability, and task-specific functionality. While
open-source frameworks and parametric tools support reproducible design, they
still rely on expert heuristics and manual tuning. Automated methods using
optimization are often compute-intensive, simulation-dependent, and rarely
target dexterous hands. Large language models (LLMs), with their broad
knowledge of human-object interactions and strong generative capabilities,
offer a promising alternative for zero-shot design reasoning. In this paper, we
present Lang2Morph, a language-driven pipeline for robotic hand design. It uses
LLMs to translate natural-language task descriptions into symbolic structures
and OPH-compatible parameters, enabling 3D-printable task-specific
morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks
into semantic tags, structural grammars, and OPH-compatible parameters; and
(ii) Selection and Refinement, which evaluates design candidates based on
semantic alignment and size compatibility, and optionally applies LLM-guided
refinement when needed. We evaluate Lang2Morph across varied tasks, and results
show that our approach can generate diverse, task-relevant morphologies. To our
knowledge, this is the first attempt to develop an LLM-based framework for
task-conditioned robotic hand design.

</details>


### [38] [Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations](https://arxiv.org/abs/2509.18953)
*Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao*

Main category: cs.RO

TL;DR: Eva-VLA是首个系统评估VLA模型鲁棒性的统一框架，通过将离散物理变化转化为连续优化问题，揭示了VLA模型在真实世界部署中的严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作中表现出潜力，但其对真实世界物理变化的鲁棒性尚未得到充分探索，需要系统评估方法来弥合实验室成功与部署准备之间的差距。

Method: 将物理变化分解为三个关键领域：物体3D变换、光照变化和对抗性补丁；引入连续黑盒优化框架，将离散物理变化转化为参数优化问题，系统探索最坏情况场景。

Result: 实验显示所有变化类型都导致超过60%的失败率，物体变换在长时程任务中导致高达97.8%的失败，暴露了VLA模型的严重脆弱性。

Conclusion: Eva-VLA框架揭示了VLA模型在真实世界部署中的关键差距，同时为强化VLA基机器人操作模型提供了实用路径。

Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for
robotic manipulation, yet their robustness to real-world physical variations
remains critically underexplored. To bridge this gap, we propose Eva-VLA, the
first unified framework that systematically evaluates the robustness of VLA
models by transforming discrete physical variations into continuous
optimization problems. However, comprehensively assessing VLA robustness
presents two key challenges: (1) how to systematically characterize diverse
physical variations encountered in real-world deployments while maintaining
evaluation reproducibility, and (2) how to discover worst-case scenarios
without prohibitive real-world data collection costs efficiently. To address
the first challenge, we decompose real-world variations into three critical
domains: object 3D transformations that affect spatial reasoning, illumination
variations that challenge visual perception, and adversarial patches that
disrupt scene understanding. For the second challenge, we introduce a
continuous black-box optimization framework that transforms discrete physical
variations into parameter optimization, enabling systematic exploration of
worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models
across multiple benchmarks reveal alarming vulnerabilities: all variation types
trigger failure rates exceeding 60%, with object transformations causing up to
97.8% failure in long-horizon tasks. Our findings expose critical gaps between
controlled laboratory success and unpredictable deployment readiness, while the
Eva-VLA framework provides a practical pathway for hardening VLA-based robotic
manipulation models against real-world deployment challenges.

</details>


### [39] [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](https://arxiv.org/abs/2509.18954)
*Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.RO

TL;DR: 提出了一种基于深度学习的框架，用于在ICP匹配前估计注册误差协方差，无需参考地图即可提供可靠的6自由度误差协方差估计，从而提升LiDAR定位和SLAM的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统ICP算法在特征稀少环境和动态场景中容易产生误差，现有不确定性估计方法依赖手工模型或简化假设，而深度学习方法要么需要预建地图，要么只能进行二元分类，无法准确建模不确定性。

Method: 开发数据驱动框架，利用深度学习在ICP匹配前预测注册误差协方差，将每个LiDAR扫描与可靠的6自由度误差协方差估计关联，实现与卡尔曼滤波的无缝集成。

Result: 在KITTI数据集上的大量实验表明，该方法能准确预测协方差，应用于基于预建地图的定位或SLAM时，能减少定位误差并提高鲁棒性。

Conclusion: 所提出的深度学习框架能够有效解决ICP算法的不确定性估计问题，为LiDAR定位系统提供了更可靠的不确定性建模方法。

Abstract: LiDAR-based localization and SLAM often rely on iterative matching
algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align
sensor data with pre-existing maps or previous scans. However, ICP is prone to
errors in featureless environments and dynamic scenes, leading to inaccurate
pose estimation. Accurately predicting the uncertainty associated with ICP is
crucial for robust state estimation but remains challenging, as existing
approaches often rely on handcrafted models or simplified assumptions.
Moreover, a few deep learning-based methods for localizability estimation
either depend on a pre-built map, which may not always be available, or provide
a binary classification of localizable versus non-localizable, which fails to
properly model uncertainty. In this work, we propose a data-driven framework
that leverages deep learning to estimate the registration error covariance of
ICP before matching, even in the absence of a reference map. By associating
each LiDAR scan with a reliable 6-DoF error covariance estimate, our method
enables seamless integration of ICP within Kalman filtering, enhancing
localization accuracy and robustness. Extensive experiments on the KITTI
dataset demonstrate the effectiveness of our approach, showing that it
accurately predicts covariance and, when applied to localization using a
pre-built map or SLAM, reduces localization errors and improves robustness.

</details>


### [40] [Category-Level Object Shape and Pose Estimation in Less Than a Millisecond](https://arxiv.org/abs/2509.18979)
*Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone*

Main category: cs.RO

TL;DR: 提出了一种快速局部求解器，用于物体形状和姿态估计，只需要类别级物体先验，并能提供高效的全局最优性证明。


<details>
  <summary>Details</summary>
Motivation: 物体形状和姿态估计是机器人技术的基础问题，支持从操作到场景理解和导航等任务。需要一种快速且能保证全局最优性的方法。

Method: 使用学习的前端检测稀疏的类别级语义关键点，用线性主动形状模型表示未知形状，通过最大后验优化问题同时求解位置、方向和形状。使用自洽场迭代法高效求解特征值问题。

Result: 求解器每次迭代约需100微秒，能快速剔除异常值。在合成数据和真实场景（包括两个公共数据集和无人机跟踪场景）上进行了测试。

Conclusion: 该方法实现了快速且可证明全局最优的物体形状和姿态估计，代码已开源。

Abstract: Object shape and pose estimation is a foundational robotics problem,
supporting tasks from manipulation to scene understanding and navigation. We
present a fast local solver for shape and pose estimation which requires only
category-level object priors and admits an efficient certificate of global
optimality. Given an RGB-D image of an object, we use a learned front-end to
detect sparse, category-level semantic keypoints on the target object. We
represent the target object's unknown shape using a linear active shape model
and pose a maximum a posteriori optimization problem to solve for position,
orientation, and shape simultaneously. Expressed in unit quaternions, this
problem admits first-order optimality conditions in the form of an eigenvalue
problem with eigenvector nonlinearities. Our primary contribution is to solve
this problem efficiently with self-consistent field iteration, which only
requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector
pair at each iterate. Solving a linear system for the corresponding Lagrange
multipliers gives a simple global optimality certificate. One iteration of our
solver runs in about 100 microseconds, enabling fast outlier rejection. We test
our method on synthetic data and a variety of real-world settings, including
two public datasets and a drone tracking scenario. Code is released at
https://github.com/MIT-SPARK/Fast-ShapeAndPose.

</details>


### [41] [Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)
*Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou*

Main category: cs.RO

TL;DR: 这篇论文是对视觉语言动作（VLA）模型的综述性研究，分析了VLA从传统策略控制向通用机器人技术的范式转变，系统分类了现有VLA方法并探讨了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: VLA模型的出现标志着机器人控制从传统策略方法向通用机器人的范式转变，需要系统梳理这一快速发展的领域，为研究者提供清晰的分类框架和发展方向。

Method: 通过分析300多项最新研究，将VLA方法分为自回归、扩散、强化学习、混合和专门方法等范式，并详细考察其动机、核心策略和实现方式。

Result: 建立了VLA方法的系统分类体系，介绍了基础数据集、基准测试和仿真平台，为领域研究提供了全面的参考框架。

Conclusion: VLA模型在通用机器人领域具有巨大潜力，但仍面临可扩展性和通用性等关键挑战，需要进一步研究来推动该领域的发展。

Abstract: The emergence of Vision Language Action (VLA) models marks a paradigm shift
from traditional policy-based control to generalized robotics, reframing Vision
Language Models (VLMs) from passive sequence generators into active agents for
manipulation and decision-making in complex, dynamic environments. This survey
delves into advanced VLA methods, aiming to provide a clear taxonomy and a
systematic, comprehensive review of existing research. It presents a
comprehensive analysis of VLA applications across different scenarios and
classifies VLA approaches into several paradigms: autoregression-based,
diffusion-based, reinforcement-based, hybrid, and specialized methods; while
examining their motivations, core strategies, and implementations in detail. In
addition, foundational datasets, benchmarks, and simulation platforms are
introduced. Building on the current VLA landscape, the review further proposes
perspectives on key challenges and future directions to advance research in VLA
models and generalizable robotics. By synthesizing insights from over three
hundred recent studies, this survey maps the contours of this rapidly evolving
field and highlights the opportunities and challenges that will shape the
development of scalable, general-purpose VLA methods.

</details>


### [42] [Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion](https://arxiv.org/abs/2509.19023)
*Shuai Liu,Meng Cheng Lau*

Main category: cs.RO

TL;DR: ROM-GRL是一个两阶段强化学习框架，用于人形机器人行走，无需运动捕捉数据或复杂奖励设计。第一阶段训练4自由度简化模型生成步态模板，第二阶段用这些轨迹指导全身策略学习。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要大量运动捕捉数据或复杂奖励设计的问题，通过简化模型指导来桥接纯奖励方法和模仿学习方法之间的差距。

Method: 第一阶段：使用PPO训练4自由度简化模型生成能量高效步态模板；第二阶段：使用SAC和对抗判别器训练全身策略，确保步态特征分布与简化模型匹配。

Result: 在1米/秒和4米/秒速度下，ROM-GRL产生稳定、对称的步态，跟踪误差显著低于纯奖励基线方法。

Conclusion: ROM-GRL通过将轻量级简化模型指导融入高维策略，实现了无需人类演示的通用、自然的人形机器人行为。

Abstract: We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a
two-stage reinforcement learning framework for humanoid walking that requires
no motion capture data or elaborate reward shaping. In the first stage, a
compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via
Proximal Policy Optimization. This generates energy-efficient gait templates.
In the second stage, those dynamically consistent trajectories guide a
full-body policy trained with Soft Actor--Critic augmented by an adversarial
discriminator, ensuring the student's five-dimensional gait feature
distribution matches the ROM's demonstrations. Experiments at 1
meter-per-second and 4 meter-per-second show that ROM-GRL produces stable,
symmetric gaits with substantially lower tracking error than a pure-reward
baseline. By distilling lightweight ROM guidance into high-dimensional
policies, ROM-GRL bridges the gap between reward-only and imitation-based
locomotion methods, enabling versatile, naturalistic humanoid behaviors without
any human demonstrations.

</details>


### [43] [TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors](https://arxiv.org/abs/2509.19037)
*Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang*

Main category: cs.RO

TL;DR: TacEva是一个用于视觉触觉传感器(VBTS)的全面评估框架，通过定义标准化性能指标和实验流程来解决VBTS性能评估缺乏标准化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉触觉传感器在传感机制、结构尺寸等参数上存在差异，导致性能差异显著，但由于缺乏标准化指标，难以针对特定任务进行优化选择和微调。

Method: 开发了TacEva框架，定义了一套性能指标来捕捉典型应用场景中的关键特性，并为每个指标设计了结构化的实验流程以确保一致性和可重复性量化。

Result: 该框架应用于多种具有不同传感机制的VBTS，结果表明能够对每种设计进行全面评估，并为每个性能维度提供定量指标。

Conclusion: TacEva使研究人员能够按任务预选最合适的VBTS，同时为VBTS设计的优化提供性能指导的见解。

Abstract: Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because
of the high spatial resolution they offer and their relatively low
manufacturing costs. However, variations in their sensing mechanisms,
structural dimension, and other parameters lead to significant performance
disparities between existing VBTSs. This makes it challenging to optimize them
for specific tasks, as both the initial choice and subsequent fine-tuning are
hindered by the lack of standardized metrics. To address this issue, TacEva is
introduced as a comprehensive evaluation framework for the quantitative
analysis of VBTS performance. The framework defines a set of performance
metrics that capture key characteristics in typical application scenarios. For
each metric, a structured experimental pipeline is designed to ensure
consistent and repeatable quantification. The framework is applied to multiple
VBTSs with distinct sensing mechanisms, and the results demonstrate its ability
to provide a thorough evaluation of each design and quantitative indicators for
each performance dimension. This enables researchers to pre-select the most
appropriate VBTS on a task by task basis, while also offering
performance-guided insights into the optimization of VBTS design. A list of
existing VBTS evaluation methods and additional evaluations can be found on our
website: https://stevenoh2003.github.io/TacEva/

</details>


### [44] [ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation](https://arxiv.org/abs/2509.19047)
*Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee*

Main category: cs.RO

TL;DR: 提出ManipForce手持系统捕获高频力扭矩和RGB数据，并开发频率感知多模态Transformer（FMT）用于接触丰富的操作任务，相比仅RGB基线显著提升性能


<details>
  <summary>Details</summary>
Motivation: 接触丰富的操作任务需要精确控制交互力，但现有模仿学习方法主要依赖视觉演示，缺乏力扭矩数据

Method: 使用ManipForce系统采集RGB和力扭矩数据，开发FMT模型通过频率和模态感知嵌入编码异步信号，在Transformer扩散策略中通过双向交叉注意力融合多模态数据

Result: 在6个真实世界接触丰富操作任务中平均成功率83%，显著优于仅RGB基线，消融实验证实高频力扭矩数据和跨模态集成提升策略性能

Conclusion: 高频力扭矩数据和多模态融合对于高精度和稳定接触任务至关重要，ManipForce和FMT为接触丰富操作提供了有效解决方案

Abstract: Contact-rich manipulation tasks such as precision assembly require precise
control of interaction forces, yet existing imitation learning methods rely
mainly on vision-only demonstrations. We propose ManipForce, a handheld system
designed to capture high-frequency force-torque (F/T) and RGB data during
natural human demonstrations for contact-rich manipulation. Building on these
demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).
FMT encodes asynchronous RGB and F/T signals using frequency- and
modality-aware embeddings and fuses them via bi-directional cross-attention
within a transformer diffusion policy. Through extensive experiments on six
real-world contact-rich manipulation tasks - such as gear assembly, box
flipping, and battery insertion - FMT trained on ManipForce demonstrations
achieves robust performance with an average success rate of 83% across all
tasks, substantially outperforming RGB-only baselines. Ablation and
sampling-frequency analyses further confirm that incorporating high-frequency
F/T data and cross-modal integration improves policy performance, especially in
tasks demanding high precision and stable contact.

</details>


### [45] [SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions](https://arxiv.org/abs/2509.19076)
*Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet*

Main category: cs.RO

TL;DR: SlicerROS2是一个结合3D Slicer和ROS的软件模块，用于医学机器人研究的标准集成方法。新版设计提供更好的模块化、低级功能访问和Python API支持。


<details>
  <summary>Details</summary>
Motivation: 为医学图像引导机器人干预研究提供标准化的软件集成解决方案，解决3D Slicer和ROS系统之间的集成问题。

Method: 重新设计和重写SlicerROS2模块，增强模块化设计，提供对低级功能的访问，支持3D Slicer的Python API，并改进数据传输协议。

Result: 开发了四个应用案例，在真实的图像引导机器人场景中验证了SlicerROS2核心功能的有效性。

Conclusion: 新版SlicerROS2为医学机器人研究提供了更强大和灵活的集成平台，支持更复杂的图像引导机器人应用开发。

Abstract: Image-guided robotic interventions involve the use of medical imaging in
tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer
and robot operating system (ROS) in pursuit of a standard integration approach
for medical robotics research. The first release of SlicerROS2 demonstrated the
feasibility of using the C++ API from 3D Slicer and ROS to load and visualize
robots in real time. Since this initial release, we've rewritten and redesigned
the module to offer greater modularity, access to low-level features, access to
3D Slicer's Python API, and better data transfer protocols. In this paper, we
introduce this new design as well as four applications that leverage the core
functionalities of SlicerROS2 in realistic image-guided robotics scenarios.

</details>


### [46] [World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2509.19080)
*Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: World4RL是一个利用扩散模型作为高保真模拟器的框架，用于在想象环境中优化预训练的机器人操作策略，避免真实世界交互的成本和风险。


<details>
  <summary>Details</summary>
Motivation: 机器人操作策略通常通过模仿学习初始化，但受限于专家数据的稀缺性和覆盖范围。强化学习可以优化策略，但真实机器人训练成本高且不安全，而模拟器训练存在仿真到现实的差距。

Method: 提出World4RL框架，预训练扩散世界模型捕捉多任务数据集中的多样化动态，在冻结的世界模型中完全优化策略。设计专门针对机器人操作的双热动作编码方案，采用扩散骨干网络提高建模保真度。

Result: 广泛的仿真和真实世界实验表明，World4RL提供高保真环境建模，实现一致的策略优化，相比模仿学习和其他基线方法显著提高成功率。

Conclusion: World4RL展示了扩散模型作为世界模型在机器人操作策略优化中的有效性，为安全高效的策略训练提供了新途径。

Abstract: Robotic manipulation policies are commonly initialized through imitation
learning, but their performance is limited by the scarcity and narrow coverage
of expert data. Reinforcement learning can refine polices to alleviate this
limitation, yet real-robot training is costly and unsafe, while training in
simulators suffers from the sim-to-real gap. Recent advances in generative
models have demonstrated remarkable capabilities in real-world simulation, with
diffusion models in particular excelling at generation. This raises the
question of how diffusion model-based world models can be combined to enhance
pre-trained policies in robotic manipulation. In this work, we propose
World4RL, a framework that employs diffusion-based world models as
high-fidelity simulators to refine pre-trained policies entirely in imagined
environments for robotic manipulation. Unlike prior works that primarily employ
world models for planning, our framework enables direct end-to-end policy
optimization. World4RL is designed around two principles: pre-training a
diffusion world model that captures diverse dynamics on multi-task datasets and
refining policies entirely within a frozen world model to avoid online
real-world interactions. We further design a two-hot action encoding scheme
tailored for robotic manipulation and adopt diffusion backbones to improve
modeling fidelity. Extensive simulation and real-world experiments demonstrate
that World4RL provides high-fidelity environment modeling and enables
consistent policy refinement, yielding significantly higher success rates
compared to imitation learning and other baselines. More visualization results
are available at https://world4rl.github.io/.

</details>


### [47] [FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.19102)
*Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: FunCanon框架将长时程操作任务分解为动作块序列，通过功能对象规范化实现姿态感知和类别泛化，使用FuncDiffuser扩散策略在功能对齐数据上训练，提升模仿学习的可扩展性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决端到端演示学习导致的任务特定策略泛化能力差的问题，通过功能分解和规范化实现策略的组合性和重用性。

Method: 将长时程操作任务分解为动作块（执行者、动词、对象），进行功能对象规范化实现功能对齐和自动操作轨迹迁移，使用对象中心和动作中心的FuncDiffuser扩散策略训练。

Result: 在仿真和真实世界基准测试中展示了类别级泛化、跨任务行为重用和稳健的仿真到现实部署能力。

Conclusion: 功能规范化为复杂操作领域的可扩展模仿学习提供了强归纳偏置。

Abstract: General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.

</details>


### [48] [Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation](https://arxiv.org/abs/2509.19105)
*Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir*

Main category: cs.RO

TL;DR: RS-Net是一个深度神经网络，能够从RGB图像预测光谱特征，从而获取地形材料和摩擦系数信息，用于机器人户外导航规划。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖几何或语义标签无法区分视觉相似但材料属性不同的表面，而光谱传感器虽然能提供材料信息但存在硬件集成复杂、成本高、计算密集等问题。

Method: 提出RS-Net神经网络，从RGB图像块预测光谱特征，然后将光谱特征映射到地形标签和摩擦系数，集成到轮式机器人的运动规划和四足机器人的MPC控制中。

Result: 开发了一个框架，在训练时学习任务相关的物理属性，测试时仅需RGB传感即可实现准确的物理交互预测。

Conclusion: RS-Net成功弥合了RGB传感的便利性与光谱数据丰富材料信息之间的差距，为机器人户外导航提供了实用的解决方案。

Abstract: Successful navigation in outdoor environments requires accurate prediction of
the physical interactions between the robot and the terrain. To this end,
several methods rely on geometric or semantic labels to classify traversable
surfaces. However, such labels cannot distinguish visually similar surfaces
that differ in material properties. Spectral sensors enable inference of
material composition from surface reflectance measured across multiple
wavelength bands. Although spectral sensing is gaining traction in robotics,
widespread deployment remains constrained by the need for custom hardware
integration, high sensor costs, and compute-intensive processing pipelines. In
this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),
a deep neural network designed to bridge the gap between the accessibility of
RGB sensing and the rich material information provided by spectral data. RS-Net
predicts spectral signatures from RGB patches, which we map to terrain labels
and friction coefficients. The resulting terrain classifications are integrated
into a sampling-based motion planner for a wheeled robot operating in outdoor
environments. Likewise, the friction estimates are incorporated into a
contact-force-based MPC for a quadruped robot navigating slippery surfaces.
Thus, we introduce a framework that learns the task-relevant physical property
once during training and thereafter relies solely on RGB sensing at test time.
The code is available at https://github.com/prajapatisarvesh/RS-Net.

</details>


### [49] [BiGraspFormer: End-to-End Bimanual Grasp Transformer](https://arxiv.org/abs/2509.19142)
*Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee*

Main category: cs.RO

TL;DR: BiGraspFormer是一个统一的端到端transformer框架，直接从物体点云生成协调的双臂抓取，解决了现有方法中协调问题如碰撞风险和力分布不平衡。


<details>
  <summary>Details</summary>
Motivation: 现有双臂抓取方法要么只关注单臂抓取，要么采用分离的抓取生成和双臂评估阶段，导致协调问题。需要一种能够直接生成协调双臂抓取的方法。

Method: 提出Single-Guided Bimanual策略，先用transformer解码器生成多样的单抓取候选，然后通过专门的注意力机制利用学习到的特征联合预测双臂姿态和质量分数。

Result: 在仿真实验和真实世界验证中，BiGraspFormer始终优于现有方法，同时保持高效推理速度（<0.05秒）。

Conclusion: BiGraspFormer框架有效解决了双臂抓取的协调问题，证明了该方法的有效性。

Abstract: Bimanual grasping is essential for robots to handle large and complex
objects. However, existing methods either focus solely on single-arm grasping
or employ separate grasp generation and bimanual evaluation stages, leading to
coordination problems including collision risks and unbalanced force
distribution. To address these limitations, we propose BiGraspFormer, a unified
end-to-end transformer framework that directly generates coordinated bimanual
grasps from object point clouds. Our key idea is the Single-Guided Bimanual
(SGB) strategy, which first generates diverse single grasp candidates using a
transformer decoder, then leverages their learned features through specialized
attention mechanisms to jointly predict bimanual poses and quality scores. This
conditioning strategy reduces the complexity of the 12-DoF search space while
ensuring coordinated bimanual manipulation. Comprehensive simulation
experiments and real-world validation demonstrate that BiGraspFormer
consistently outperforms existing methods while maintaining efficient inference
speed (<0.05s), confirming the effectiveness of our framework. Code and
supplementary materials are available at https://sites.google.com/bigraspformer

</details>


### [50] [A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination](https://arxiv.org/abs/2509.19168)
*Mark Gonzales,Ethan Oh,Joseph Moore*

Main category: cs.RO

TL;DR: 本文提出了一种基于采样的重规划方法，能够处理多模态策略分布，通过交叉熵方法优化多模态策略，提高了对局部最小值的鲁棒性和解空间的探索效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决多机器人规划中的局部最小值问题和死锁情况，需要一种能够考虑多种可能策略的规划方法，以提高系统的鲁棒性和效率。

Method: 使用重规划视野和基于采样的规划器，结合交叉熵方法优化多模态策略分布，在共同成本函数下进行优化。

Result: 数值模拟显示多模态策略显著提高了陷阱环境和多机器人避碰中的成功率，硬件实验验证了方法的实时可行性和实际性能。

Conclusion: 该方法能够有效处理多机器人协作规划问题，避免集中式优化的计算复杂度，同时提高系统的鲁棒性和探索能力。

Abstract: In this paper, we present a receding-horizon, sampling-based planner capable
of reasoning over multimodal policy distributions. By using the cross-entropy
method to optimize a multimodal policy under a common cost function, our
approach increases robustness against local minima and promotes effective
exploration of the solution space. We show that our approach naturally extends
to multi-robot collision-free planning, enables agents to share diverse
candidate policies to avoid deadlocks, and allows teams to minimize a global
objective without incurring the computational complexity of centralized
optimization. Numerical simulations demonstrate that employing multiple modes
significantly improves success rates in trap environments and in multi-robot
collision avoidance. Hardware experiments further validate the approach's
real-time feasibility and practical performance.

</details>


### [51] [MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap](https://arxiv.org/abs/2509.19169)
*Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan*

Main category: cs.RO

TL;DR: MagiClaw是一个多功能双指末端执行器，通过软多面体网络和嵌入式摄像头实现视觉力估计，结合iPhone提供多模态数据，解决人机操作技能转移中的领域差距问题。


<details>
  <summary>Details</summary>
Motivation: 解决人类演示到机器人执行之间的传感和形态学"领域差距"问题，降低收集高保真接触丰富数据集的难度。

Method: 设计MagiClaw双指末端执行器，集成了软多面体网络和嵌入式摄像头进行6自由度力估计，结合iPhone提供6D姿态、RGB视频和LiDAR深度图等多模态数据。

Result: 开发了统一的系统架构，能够实现实时遥操作、离线策略学习和混合现实界面沉浸式控制。

Conclusion: MagiClaw系统降低了收集高质量接触数据集的门槛，加速了通用化操作策略的开发。

Abstract: The transfer of manipulation skills from human demonstration to robotic
execution is often hindered by a "domain gap" in sensing and morphology. This
paper introduces MagiClaw, a versatile two-finger end-effector designed to
bridge this gap. MagiClaw functions interchangeably as both a handheld tool for
intuitive data collection and a robotic end-effector for policy deployment,
ensuring hardware consistency and reliability. Each finger incorporates a Soft
Polyhedral Network (SPN) with an embedded camera, enabling vision-based
estimation of 6-DoF forces and contact deformation. This proprioceptive data is
fused with exteroceptive environmental sensing from an integrated iPhone, which
provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS
application, MagiClaw streams synchronized, multi-modal data for real-time
teleoperation, offline policy learning, and immersive control via mixed-reality
interfaces. We demonstrate how this unified system architecture lowers the
barrier to collecting high-fidelity, contact-rich datasets and accelerates the
development of generalizable manipulation policies. Please refer to the iOS app
at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.

</details>


### [52] [Proactive-reactive detection and mitigation of intermittent faults in robot swarms](https://arxiv.org/abs/2509.19246)
*Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: 本文提出了一种针对机器人群体中间歇性故障的主动-反应式检测与缓解策略，利用自组织备份层和多路网络中的分布式共识来解决间歇性故障问题。


<details>
  <summary>Details</summary>
Motivation: 间歇性故障是周期性出现和消失的瞬时错误，对可靠性和协调性构成重大挑战。现有研究主要关注永久性故障，而间歇性故障在机器人群体自组织网络中难以检测，因为网络拓扑是瞬态且不可预测的。

Method: 采用主动-反应式策略：主动方面，机器人在故障发生前自组织动态备份路径；反应方面，使用一次性似然比检验比较多路网络中不同路径的信息，实现早期故障检测。检测到故障后，通信以自组织方式临时重路由。

Result: 在形成控制中故障位置数据的代表性场景中验证了该方法，证明间歇性故障不会干扰期望形成的收敛，具有高故障检测精度和低误报率。

Conclusion: 该方法有效解决了机器人群体中间歇性故障的检测与缓解问题，通过自组织备份层和分布式共识实现了高可靠性的故障处理。

Abstract: Intermittent faults are transient errors that sporadically appear and
disappear. Although intermittent faults pose substantial challenges to
reliability and coordination, existing studies of fault tolerance in robot
swarms focus instead on permanent faults. One reason for this is that
intermittent faults are prohibitively difficult to detect in the fully
self-organized ad-hoc networks typical of robot swarms, as their network
topologies are transient and often unpredictable. However, in the recently
introduced self-organizing nervous systems (SoNS) approach, robot swarms are
able to self-organize persistent network structures for the first time, easing
the problem of detecting intermittent faults. To address intermittent faults in
robot swarms that have persistent networks, we propose a novel
proactive-reactive strategy to detection and mitigation, based on
self-organized backup layers and distributed consensus in a multiplex network.
Proactively, the robots self-organize dynamic backup paths before faults occur,
adapting to changes in the primary network topology and the robots' relative
positions. Reactively, robots use one-shot likelihood ratio tests to compare
information received along different paths in the multiplex network, enabling
early fault detection. Upon detection, communication is temporarily rerouted in
a self-organized way, until the detected fault resolves. We validate the
approach in representative scenarios of faulty positional data occurring during
formation control, demonstrating that intermittent faults are prevented from
disrupting convergence to desired formations, with high fault detection
accuracy and low rates of false positives.

</details>


### [53] [Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces](https://arxiv.org/abs/2509.19261)
*Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani*

Main category: cs.RO

TL;DR: 提出了一种模仿引导的双手机器人规划框架，通过稳定的抓取流形采样策略和分层运动架构，实现无缝抓取转换和优化的运动性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人操作时不同抓取类型之间平滑转换的挑战，特别是在存在外部力和复杂运动约束的情况下。现有方法往往无法有效处理变化的外部力并优化运动性能。

Method: 采用模仿引导的双手机器人规划框架，包括稳定的抓取流形采样策略实现单双手抓取无缝转换，以及分层双阶段运动架构结合模仿学习的全局路径生成器和二次规划驱动的局部规划器。

Result: 在力密集型任务评估中，该方法显著提高了抓取转换效率和运动性能。

Conclusion: 所提出的方法能够有效增强机器人操作的稳定性和灵巧性，为动态环境中的复杂操作任务提供了可行的解决方案。

Abstract: Robotic manipulation in dynamic environments often requires seamless
transitions between different grasp types to maintain stability and efficiency.
However, achieving smooth and adaptive grasp transitions remains a challenge,
particularly when dealing with external forces and complex motion constraints.
Existing grasp transition strategies often fail to account for varying external
forces and do not optimize motion performance effectively. In this work, we
propose an Imitation-Guided Bimanual Planning Framework that integrates
efficient grasp transition strategies and motion performance optimization to
enhance stability and dexterity in robotic manipulation. Our approach
introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for
seamless transitions between uni-manual and bi-manual grasps, reducing
computational costs and regrasping inefficiencies. Additionally, a Hierarchical
Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path
Generator with a Quadratic Programming-driven Local Planner to ensure real-time
motion feasibility, obstacle avoidance, and superior manipulability. The
proposed method is evaluated through a series of force-intensive tasks,
demonstrating significant improvements in grasp transition efficiency and
motion performance. A video demonstrating our simulation results can be viewed
at
\href{https://youtu.be/3DhbUsv4eDo}{\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.

</details>


### [54] [SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration](https://arxiv.org/abs/2509.19292)
*Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: SOE是一个用于机器人操作任务的策略自改进框架，通过在有效动作流形上进行约束性探索，解决了传统随机扰动方法的安全性和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器人策略探索方法通常依赖随机扰动，这会导致不安全和不稳定的行为，限制了探索效果。需要一种既能保证安全性又能提高探索效率的方法。

Method: SOE学习任务相关因素的紧凑潜在表示，将探索约束在有效动作流形上，可以作为插件模块与任意策略模型集成，同时支持人工引导探索。

Result: 在仿真和真实世界任务中的广泛实验表明，SOE在任务成功率、探索平滑性和安全性、样本效率方面均优于现有方法。

Conclusion: 在流形上的探索为样本高效策略自改进提供了一种原则性方法，SOE框架具有安全、多样且有效的特点。

Abstract: Intelligent agents progress by continually refining their capabilities
through actively exploring environments. Yet robot policies often lack
sufficient exploration capability due to action mode collapse. Existing methods
that encourage exploration typically rely on random perturbations, which are
unsafe and induce unstable, erratic behaviors, thereby limiting their
effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a
framework that enhances policy exploration and improvement in robotic
manipulation. SOE learns a compact latent representation of task-relevant
factors and constrains exploration to the manifold of valid actions, ensuring
safety, diversity, and effectiveness. It can be seamlessly integrated with
arbitrary policy models as a plug-in module, augmenting exploration without
degrading the base policy performance. Moreover, the structured latent space
enables human-guided exploration, further improving efficiency and
controllability. Extensive experiments in both simulation and real-world tasks
demonstrate that SOE consistently outperforms prior methods, achieving higher
task success rates, smoother and safer exploration, and superior sample
efficiency. These results establish on-manifold exploration as a principled
approach to sample-efficient policy self-improvement. Project website:
https://ericjin2002.github.io/SOE

</details>


### [55] [Residual Off-Policy RL for Finetuning Behavior Cloning Policies](https://arxiv.org/abs/2509.19301)
*Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi*

Main category: cs.RO

TL;DR: 提出了一个结合行为克隆和强化学习的残差学习框架，利用BC策略作为基础，通过样本高效的离策略RL学习轻量级残差修正，实现了在真实世界人形机器人上的成功RL训练。


<details>
  <summary>Details</summary>
Motivation: 行为克隆受限于人类演示质量和数据收集成本，而强化学习在真实机器人上训练存在样本效率低、安全问题和稀疏奖励等挑战。需要结合两者的优势来开发实用的真实世界RL方法。

Method: 使用残差学习框架，将BC策略作为黑盒基础策略，通过样本高效的离策略RL学习每步的轻量级残差修正，仅需稀疏二元奖励信号。

Result: 在高自由度系统上有效改进了操作策略，在仿真和真实世界中都取得了成功，特别是在人形机器人上实现了首个成功的真实世界RL训练，在各种视觉任务中达到最先进性能。

Conclusion: 该方法为在真实世界部署RL提供了一条实用路径，结合了BC和RL的优势，解决了真实机器人RL训练的关键挑战。

Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor
control policies. However, these approaches are limited by the quality of human
demonstrations, the manual effort required for data collection, and the
diminishing returns from increasing offline data. In comparison, reinforcement
learning (RL) trains an agent through autonomous interaction with the
environment and has shown remarkable success in various domains. Still,
training RL policies directly on real-world robots remains challenging due to
sample inefficiency, safety concerns, and the difficulty of learning from
sparse rewards for long-horizon tasks, especially for high-degree-of-freedom
(DoF) systems. We present a recipe that combines the benefits of BC and RL
through a residual learning framework. Our approach leverages BC policies as
black-box bases and learns lightweight per-step residual corrections via
sample-efficient off-policy RL. We demonstrate that our method requires only
sparse binary reward signals and can effectively improve manipulation policies
on high-degree-of-freedom (DoF) systems in both simulation and the real world.
In particular, we demonstrate, to the best of our knowledge, the first
successful real-world RL training on a humanoid robot with dexterous hands. Our
results demonstrate state-of-the-art performance in various vision-based tasks,
pointing towards a practical pathway for deploying RL in the real world.
Project website: https://residual-offpolicy-rl.github.io

</details>
