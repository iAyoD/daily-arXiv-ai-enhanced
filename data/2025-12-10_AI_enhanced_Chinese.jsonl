{"id": "2512.08082", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08082", "abs": "https://arxiv.org/abs/2512.08082", "authors": ["Vala Vakilian", "Zimeng Wang", "Ankit Singh Rawat", "Christos Thrampoulidis"], "title": "Short-Context Dominance: How Much Local Context Natural Language Actually Needs?", "comment": "38 pages, 7 figures, includes appendix and references", "summary": "We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u5047\u8bf4\uff0c\u53d1\u73b075-80%\u7684\u957f\u5e8f\u5217\u4ec5\u9700\u6700\u540e96\u4e2atoken\u5373\u53ef\u51c6\u786e\u9884\u6d4b\uff0c\u5f00\u53d1DaMCL\u68c0\u6d4b\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u7684\u6311\u6218\u5e8f\u5217\uff0c\u5e76\u63d0\u51fa\u89e3\u7801\u7b97\u6cd5\u63d0\u5347\u957f\u8ddd\u79bb\u76f8\u5173token\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u5047\u8bf4\uff1a\u9a8c\u8bc1\u5927\u591a\u6570\u5e8f\u5217\u662f\u5426\u4ec5\u9700\u5c40\u90e8\u524d\u7f00\u5373\u53ef\u9884\u6d4b\u4e0b\u4e00\u4e2atoken\uff0c\u5e76\u89e3\u51b3\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u5e26\u6765\u7684\u504f\u89c1\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u7edf\u8ba1\u9884\u8a00\u673a\u6d4b\u91cf\u6700\u5c0f\u4e0a\u4e0b\u6587\u957f\u5ea6(MCL)\uff1b\u63d0\u51fa\u65e0\u9700\u5b9e\u9645\u4e0b\u4e00\u4e2atoken\u77e5\u8bc6\u7684\u5206\u5e03\u611f\u77e5MCL(DaMCL)\uff1b\u5f00\u53d1\u89e3\u7801\u7b97\u6cd5\u68c0\u6d4b\u5e76\u63d0\u5347\u957f\u8ddd\u79bb\u76f8\u5173token\u3002", "result": "\u57281-7k token\u7684\u957f\u6587\u6863\u5e8f\u5217\u4e2d\uff0c75-80%\u4ec5\u9700\u6700\u540e96\u4e2atoken\uff1bDaMCL\u80fd\u6709\u6548\u68c0\u6d4b\u957f\u4e0a\u4e0b\u6587\u5e8f\u5217\uff1b\u89e3\u7801\u7b97\u6cd5\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u4f46\u901a\u8fc7DaMCL\u68c0\u6d4b\u6311\u6218\u5e8f\u5217\u5e76\u8c03\u6574\u89e3\u7801\u7b56\u7565\uff0c\u53ef\u4ee5\u7f13\u89e3\u7531\u6b64\u4ea7\u751f\u7684\u504f\u89c1\uff0c\u63d0\u5347\u6a21\u578b\u5728\u9700\u8981\u957f\u8ddd\u79bb\u4f9d\u8d56\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2512.08088", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08088", "abs": "https://arxiv.org/abs/2512.08088", "authors": ["Eliot Brenner", "Dominic Seyler", "Manjunath Hegde", "Andrei Simion", "Koustuv Dasgupta", "Bing Xiang"], "title": "Adaptation of Embedding Models to Financial Filings via LLM Distillation", "comment": "In proceedings of LLM-Finance 2025 : The 2nd IEEE International Workshop on Large Language Models for Finance", "summary": "Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\\texttt{@}$5, 44.6% improvement in mean DCG$\\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u500b\u53ef\u64f4\u5c55\u7684\u8a13\u7df4\u6d41\u7a0b\uff0c\u4f7f\u7528\u901a\u7528\u6aa2\u7d22\u5d4c\u5165\u6a21\u578b\u4f5c\u70ba\u57fa\u790e\uff0c\u5f9e\u7121\u6a19\u8a3b\u8a9e\u6599\u8a13\u7df4\u5c08\u7528\u6a21\u578b\uff0c\u5728\u91d1\u878d\u9818\u57df\u6aa2\u7d22\u4efb\u52d9\u4e0a\u986f\u8457\u63d0\u5347\u6548\u80fd\u3002", "motivation": "\u5118\u7ba1\u751f\u6210\u5f0f\u5927\u8a9e\u8a00\u6a21\u578b\u6709\u9032\u5c55\uff0c\u4f46\u5c08\u7528\u5c0d\u8a71AI\u7684\u5be6\u969b\u61c9\u7528\u4ecd\u53d7\u9650\u65bc\u8a08\u7b97\u6210\u672c\u3001\u5ef6\u9072\u8981\u6c42\u548c\u7279\u5b9a\u9818\u57df\u76f8\u95dc\u6027\u6e2c\u91cf\u7684\u9700\u6c42\u3002\u73fe\u6709\u5d4c\u5165\u6a21\u578b\u5728\u524d\u5169\u500b\u9650\u5236\u4e0a\u8868\u73fe\u826f\u597d\uff0c\u4f46\u5728\u91d1\u878d\u7b49\u5c08\u696d\u9818\u57df\u7684\u4fe1\u606f\u6aa2\u7d22\u4e0a\u8868\u73fe\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u500b\u53ef\u64f4\u5c55\u7684\u8a13\u7df4\u6d41\u7a0b\uff0c\u4f7f\u7528\u901a\u7528\u6aa2\u7d22\u5d4c\u5165\u6a21\u578b\u4f5c\u70ba\u57fa\u790e\uff0c\u5f9e\u7121\u6a19\u8a3b\u8a9e\u6599\u8a13\u7df4\u5c08\u7528\u6a21\u578b\u3002\u65b9\u6cd5\u63a1\u7528\u5e2b\u751f\u6a21\u578b\u4e92\u52d5\uff0c\u4ea4\u932f\u9032\u884c\u57fa\u65bc\u6aa2\u7d22\u7684\u56f0\u96e3\u6b63\u8ca0\u4f8b\u6316\u6398\u8207\u8fed\u4ee3\u91cd\u65b0\u8a13\u7df4\uff0c\u4f7f\u7528LLM\u5224\u65b7\u7684\u76f8\u95dc\u6027\u5c07\u9818\u57df\u77e5\u8b58\u84b8\u993e\u5230\u7dca\u6e4a\u7684\u6aa2\u7d22\u5668\u4e2d\u3002", "result": "\u572814\u7a2e\u8ca1\u52d9\u6587\u4ef6\u985e\u578b\u768421,800\u500b\u67e5\u8a62-\u6587\u4ef6\u5c0d\u4e0a\uff0cMRR@5\u5e73\u5747\u63d0\u534727.7%\uff0cmean DCG@5\u63d0\u534744.6%\uff1b\u5728FinanceBench\u76844\u500b\u6587\u4ef6\u985e\u5225\u4e2d\uff0c3\u500b\u985e\u5225\u7684NDCG\u6709\u6240\u6539\u5584\u3002", "conclusion": "\u8a72\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u500b\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5728\u4e0d\u9700\u4eba\u5de5\u6a19\u8a3b\u7684\u60c5\u6cc1\u4e0b\uff0c\u5f4c\u5408\u901a\u7528\u6a21\u578b\u8207\u5c08\u696d\u9818\u57df\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u7279\u5225\u9069\u7528\u65bc\u91d1\u878d\u7b49\u5c08\u696d\u9818\u57df\u7684\u6aa2\u7d22\u4efb\u52d9\u3002"}}
{"id": "2512.08094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08094", "abs": "https://arxiv.org/abs/2512.08094", "authors": ["Zifan Jiang", "Youngjoon Jang", "Liliane Momeni", "G\u00fcl Varol", "Sarah Ebling", "Andrew Zisserman"], "title": "Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing", "comment": null, "summary": "The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.", "AI": {"tldr": "SEA\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5b57\u5e55\u5bf9\u9f50\u6846\u67b6\uff0c\u53ef\u5c06\u53e3\u8bed\u6587\u672c\u4e0e\u624b\u8bed\u89c6\u9891\u65f6\u95f4\u6233\u5bf9\u9f50\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df\uff0c\u65e0\u9700\u7279\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7279\u5b9a\u8bed\u8a00\u6216\u6570\u636e\u96c6\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u8de8\u8bed\u8a00\u548c\u9886\u57df\u7684\u901a\u7528\u5b57\u5e55\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "SEA\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a1) \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5206\u5272\u89c6\u9891\u5e27\u5e8f\u5217\u4e3a\u5355\u4e2a\u624b\u8bed\uff1b2) \u5c06\u6bcf\u4e2a\u624b\u8bed\u89c6\u9891\u7247\u6bb5\u5d4c\u5165\u5230\u4e0e\u6587\u672c\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\uff1b3) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u52a8\u6001\u89c4\u5212\u7a0b\u5e8f\u8fdb\u884c\u5bf9\u9f50\uff0cCPU\u4e0a\u5206\u949f\u7ea7\u5904\u7406\u5c0f\u65f6\u957f\u89c6\u9891\u3002", "result": "\u5728\u56db\u4e2a\u624b\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5bf9\u9f50\u6027\u80fd\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u5e73\u884c\u6570\u636e\u7528\u4e8e\u624b\u8bed\u5904\u7406\u7814\u7a76\u3002", "conclusion": "SEA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u901a\u7528\u5b57\u5e55\u5bf9\u9f50\u6846\u67b6\uff0c\u652f\u6301\u4ece\u5c0f\u578b\u8bcd\u5178\u5230\u5927\u578b\u8fde\u7eed\u8bed\u6599\u5e93\u7684\u5404\u79cd\u573a\u666f\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.08123", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08123", "abs": "https://arxiv.org/abs/2512.08123", "authors": ["Sampriti Soor", "Suklav Ghosh", "Arijit Sur"], "title": "Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation", "comment": "10 pages", "summary": "Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable \"soft\" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u5bf9\u6297\u540e\u7f00\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u77ed\u4ee4\u724c\u5e8f\u5217\uff084-10\u4e2a\u4ee4\u724c\uff09\u6765\u964d\u4f4e\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u578b\u7684\u826f\u597d\u8fc1\u79fb\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u63d0\u793a\u65b9\u6cd5\u901a\u5e38\u662f\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u6216\u6a21\u578b\u4f18\u5316\u7684\uff0c\u5bfc\u81f4\u7ed3\u679c\u96be\u4ee5\u6bd4\u8f83\u4e14\u8fc1\u79fb\u6027\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u95f4\u6709\u6548\u8f6c\u79fb\u3002", "method": "\u4f7f\u7528Gumbel-Softmax\u677e\u5f1b\u5b66\u4e60\u53ef\u5fae\u5206\u7684\"\u8f6f\"\u540e\u7f00\uff0c\u7136\u540e\u79bb\u6563\u5316\u7528\u4e8e\u63a8\u7406\u3002\u8bad\u7ec3\u65f6\u6700\u5927\u5316\u6807\u7b7e\u533a\u57df\u7684\u6821\u51c6\u4ea4\u53c9\u71b5\uff0c\u540c\u65f6\u63a9\u7801\u9ec4\u91d1\u4ee4\u724c\u9632\u6b62\u4fe1\u606f\u6cc4\u6f0f\uff0c\u5e76\u52a0\u5165\u71b5\u6b63\u5219\u5316\u907f\u514d\u5d29\u6e83\u3002", "result": "\u5728\u60c5\u611f\u5206\u6790\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u91ca\u4e49\u68c0\u6d4b\u3001\u5e38\u8bc6\u95ee\u7b54\u548c\u7269\u7406\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528Qwen2-1.5B\u3001Phi-1.5\u548cTinyLlama-1.1B\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5355\u4e2a\u540e\u7f00\u5728\u591a\u4e2a\u6a21\u578b\u95f4\u6709\u6548\u8f6c\u79fb\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u51c6\u786e\u7387\u548c\u6821\u51c6\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "\u901a\u7528\u5bf9\u6297\u540e\u7f00\u662f\u4e00\u79cd\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u67b6\u6784\u95f4\u4fdd\u6301\u653b\u51fb\u6548\u679c\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u6a21\u578b\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2512.07969", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07969", "abs": "https://arxiv.org/abs/2512.07969", "authors": ["Alan Papalia", "Nikolas Sanderson", "Haoyu Han", "Heng Yang", "Hanumant Singh", "Michael Everett"], "title": "Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization", "comment": "8 pages, submitted for review", "summary": "Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \\emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \\emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \\textbf{2$\\times$--35$\\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5177\u6709\u89c4\u8303\u5bf9\u79f0\u6027\u7684\u673a\u5668\u4eba\u611f\u77e5\u95ee\u9898\u7684\u53d8\u91cf\u6295\u5f71\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528\u53ef\u5206\u79bb\u6027\u548c\u7a00\u758f\u6027\uff0c\u6784\u5efa\u77e9\u9635\u65e0\u5173\u7684\u8212\u5c14\u8865\u7b97\u5b50\uff0c\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb2-35\u500d\u7684\u8fd0\u884c\u901f\u5ea6\u3002", "motivation": "\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u5927\u89c4\u6a21\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u901a\u5e38\u5177\u6709\u53ef\u5206\u79bb\u6027\u7ed3\u6784\uff08\u67d0\u4e9b\u53d8\u91cf\u7ebf\u6027\u51fa\u73b0\uff09\uff0c\u53d8\u91cf\u6295\u5f71\u65b9\u6cd5\u53ef\u4ee5\u5145\u5206\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u3002\u7136\u800c\uff0c\u89c4\u8303\u5bf9\u79f0\u6027\uff08\u5982\u5168\u5c40\u5e73\u79fb\u548c\u65cb\u8f6c\u4e0d\u53d8\u6027\uff09\u5728\u611f\u77e5\u95ee\u9898\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u7ed9\u4f20\u7edf\u53d8\u91cf\u6295\u5f71\u65b9\u6cd5\u5e26\u6765\u4e86\u8ba1\u7b97\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u5177\u6709\u89c4\u8303\u5bf9\u79f0\u6027\u95ee\u9898\u7684\u53d8\u91cf\u6295\u5f71\u65b9\u6848\uff0c\u8054\u5408\u5229\u7528\u53ef\u5206\u79bb\u6027\u548c\u7a00\u758f\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u4e00\u6b21\u6027\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u6784\u5efa\u77e9\u9635\u65e0\u5173\u7684\u8212\u5c14\u8865\u7b97\u5b50\uff0c\u652f\u6301\u9ad8\u6548\u8ba1\u7b97\u7ea6\u7b80\u95ee\u9898\u7684\u6210\u672c\u3001\u68af\u5ea6\u548cHessian-\u5411\u91cf\u79ef\uff0c\u5e76\u80fd\u4e0e\u6807\u51c6\u8fed\u4ee3NLS\u6c42\u89e3\u5668\u96c6\u6210\u3002\u8bba\u6587\u63d0\u4f9b\u4e86\u65b9\u6cd5\u9002\u7528\u7684\u7cbe\u786e\u6761\u4ef6\uff0c\u5e76\u63cf\u8ff0\u4e86\u5f53\u6761\u4ef6\u90e8\u5206\u6ee1\u8db3\u65f6\u7684\u6269\u5c55\u65b9\u6848\u3002", "result": "\u5728SLAM\u3001SNL\u548cSfM\u7684\u5408\u6210\u548c\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e862-35\u500d\u7684\u8fd0\u884c\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002\u4f5c\u8005\u53d1\u5e03\u4e86\u5f00\u6e90\u7684C++\u5b9e\u73b0\u548c\u6240\u6709\u5b9e\u9a8c\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u89c4\u8303\u5bf9\u79f0\u6027\u7ed9\u53d8\u91cf\u6295\u5f71\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528\u53ef\u5206\u79bb\u6027\u548c\u7a00\u758f\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u5927\u89c4\u6a21\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6027\u80fd\u3002"}}
{"id": "2512.08131", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08131", "abs": "https://arxiv.org/abs/2512.08131", "authors": ["Sampriti Soor", "Suklav Ghosh", "Arijit Sur"], "title": "Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward", "comment": "5 pages", "summary": "Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u901a\u8fc7PPO\u7b97\u6cd5\u4f18\u5316\u540e\u7f00\u7b56\u7565\uff0c\u5229\u7528\u6821\u51c6\u4ea4\u53c9\u71b5\u5956\u52b1\u63d0\u9ad8\u8de8\u4efb\u52a1\u548c\u6a21\u578b\u7684\u8fc1\u79fb\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u77ed\u5bf9\u6297\u6027\u540e\u7f00\u7684\u653b\u51fb\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u68af\u5ea6\u641c\u7d22\u6216\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u8106\u5f31\u4e14\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u6216\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u5bf9\u6297\u6027\u540e\u7f00\u89c6\u4e3a\u7b56\u7565\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u51bb\u7ed3\u6a21\u578b\u4f5c\u4e3a\u5956\u52b1\u8bc4\u4f30\u5668\u3002\u901a\u8fc7\u6821\u51c6\u4ea4\u53c9\u71b5\u5956\u52b1\u6765\u6d88\u9664\u6807\u7b7e\u504f\u5dee\uff0c\u5e76\u805a\u5408\u4e0d\u540c\u8868\u9762\u5f62\u5f0f\u4ee5\u63d0\u9ad8\u8fc1\u79fb\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u7684NLP\u57fa\u51c6\u6570\u636e\u96c6\uff08\u6db5\u76d6\u60c5\u611f\u5206\u6790\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u91ca\u4e49\u548c\u5e38\u8bc6\u63a8\u7406\uff09\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\uff08Qwen2-1.5B Instruct\u3001TinyLlama-1.1B Chat\u548cPhi-1.5\uff09\u3002\u7ed3\u679c\u663e\u793a\uff0cRL\u8bad\u7ec3\u7684\u540e\u7f00\u80fd\u6301\u7eed\u964d\u4f4e\u6a21\u578b\u51c6\u786e\u7387\uff0c\u4e14\u6bd4\u7c7b\u4f3c\u7c7b\u578b\u7684\u5148\u524d\u5bf9\u6297\u6027\u89e6\u53d1\u5668\u5728\u8de8\u4efb\u52a1\u548c\u6a21\u578b\u95f4\u5177\u6709\u66f4\u597d\u7684\u8fc1\u79fb\u6548\u679c\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u751f\u6210\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u8fd9\u4e9b\u540e\u7f00\u4e0d\u4ec5\u80fd\u5728\u5355\u4e00\u4efb\u52a1\u4e0a\u653b\u51fb\u6a21\u578b\uff0c\u8fd8\u80fd\u66f4\u597d\u5730\u8fc1\u79fb\u5230\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5f3a\u7684\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07976", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07976", "abs": "https://arxiv.org/abs/2512.07976", "authors": ["Lazar Milikic", "Manthan Patel", "Jonas Frey"], "title": "VLD: Visual Language Goal Distance for Reinforcement Learning Navigation", "comment": null, "summary": "Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-\"where to go\"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.", "AI": {"tldr": "\u63d0\u51faVision-Language Distance (VLD)\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u5b66\u4e60\u4e0e\u7b56\u7565\u5b66\u4e60\u89e3\u8026\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8ddd\u79bb\u9884\u6d4b\u5668\u5904\u7406\u591a\u6a21\u6001\u76ee\u6807\uff0c\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3RL\u7b56\u7565\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5bfc\u822a", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u56fe\u50cf\u5230\u52a8\u4f5c\u7684\u5bfc\u822a\u7b56\u7565\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u5dee\u8ddd(sim-to-real gap)\uff1b2) \u5e26\u52a8\u4f5c\u6807\u7b7e\u7684\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236", "method": "\u63d0\u51faVLD\u6846\u67b6\uff1a1) \u5728\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\u81ea\u76d1\u7763\u7684\u8ddd\u79bb\u5230\u76ee\u6807\u9884\u6d4b\u5668\uff0c\u652f\u6301\u56fe\u50cf\u548c\u6587\u672c\u76ee\u6807\uff1b2) \u5728\u4eff\u771f\u4e2d\u4f7f\u7528\u7279\u6743\u51e0\u4f55\u8ddd\u79bb\u4fe1\u53f7\u8bad\u7ec3RL\u7b56\u7565\uff0c\u6ce8\u5165\u566a\u58f0\u6a21\u62df\u8ddd\u79bb\u9884\u6d4b\u5668\u7684\u4e0d\u786e\u5b9a\u6027\uff1b3) \u90e8\u7f72\u65f6\u7b56\u7565\u4f7f\u7528VLD\u9884\u6d4b\uff0c\u7ee7\u627f\u5927\u89c4\u6a21\u89c6\u89c9\u8bad\u7ec3\u7684\u8bed\u4e49\u76ee\u6807\u4fe1\u606f", "result": "VLD\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u5bfc\u822a\u6027\u80fd\uff0c\u652f\u6301\u7075\u6d3b\u7684\u76ee\u6807\u6a21\u6001(\u56fe\u50cf\u548c\u6587\u672c)\uff0c\u4f18\u4e8eViNT\u548cVIP\u7b49\u73b0\u6709\u65f6\u5e8f\u8ddd\u79bb\u65b9\u6cd5\u3002\u901a\u8fc7\u5e8f\u6570\u4e00\u81f4\u6027\u8bc4\u4f30\u8ddd\u79bb\u51fd\u6570\uff0c\u8bc1\u660e\u89e3\u8026\u8bbe\u8ba1\u6709\u6548", "conclusion": "VLD\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u89e3\u8026\u611f\u77e5\u5b66\u4e60\u548c\u7b56\u7565\u5b66\u4e60\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u89c6\u89c9\u8bad\u7ec3\u7684\u8bed\u4e49\u7406\u89e3\u4e0e\u4eff\u771f\u4e2d\u5b66\u4e60\u7684\u9c81\u68d2\u4f4e\u7ea7\u5bfc\u822a\u884c\u4e3a\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u6a21\u6001\u5bfc\u822a\u7b56\u7565"}}
{"id": "2512.08193", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.08193", "abs": "https://arxiv.org/abs/2512.08193", "authors": ["Jiwoo Park", "Ruoqi Liu", "Avani Jagdale", "Andrew Srisuwananukorn", "Jing Zhao", "Lang Li", "Ping Zhang", "Sachin Kumar"], "title": "ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access", "comment": null, "summary": "We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.", "AI": {"tldr": "ClinicalTrialsHub\u662f\u4e00\u4e2a\u6574\u5408ClinicalTrials.gov\u548cPubMed\u6570\u636e\u7684\u4ea4\u4e92\u5f0f\u5e73\u53f0\uff0c\u901a\u8fc7LLM\u81ea\u52a8\u63d0\u53d6\u7ed3\u6784\u5316\u8bd5\u9a8c\u4fe1\u606f\uff0c\u5c06\u7ed3\u6784\u5316\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u8bbf\u95ee\u91cf\u63d0\u534783.8%", "motivation": "\u73b0\u6709\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u5206\u6563\u5728ClinicalTrials.gov\u548cPubMed\u7b49\u4e0d\u540c\u5e73\u53f0\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7ed3\u6784\u5316\u8bbf\u95ee\u65b9\u5f0f\uff0c\u9650\u5236\u4e86\u60a3\u8005\u3001\u4e34\u5e8a\u533b\u751f\u3001\u7814\u7a76\u4eba\u5458\u548c\u653f\u7b56\u5236\u5b9a\u8005\u83b7\u53d6\u5b8c\u6574\u8bd5\u9a8c\u8bc1\u636e\u7684\u80fd\u529b", "method": "\u4f7f\u7528GPT-5.1\u548cGemini-3-Pro\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u89e3\u6790PubMed\u5168\u6587\u7814\u7a76\u6587\u7ae0\u63d0\u53d6\u7ed3\u6784\u5316\u8bd5\u9a8c\u4fe1\u606f\uff0c\u5c06\u7528\u6237\u67e5\u8be2\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u5e93\u641c\u7d22\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u7b54\u6848\u94fe\u63a5\u5230\u5177\u4f53\u6e90\u53e5", "result": "\u76f8\u6bd4\u4ec5\u4f9d\u8d56ClinicalTrials.gov\uff0c\u7cfb\u7edf\u5c06\u7ed3\u6784\u5316\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u8bbf\u95ee\u91cf\u63d0\u9ad8\u4e8683.8%\uff1b\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u3001\u4e34\u5e8a\u7814\u7a76\u4eba\u5458\u548c\u836f\u5b66/\u62a4\u7406\u5b66\u535a\u58eb\u751f\u7684\u7528\u6237\u7814\u7a76\u4ee5\u53ca\u81ea\u52a8\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u4fe1\u606f\u63d0\u53d6\u548c\u95ee\u7b54\u80fd\u529b", "conclusion": "ClinicalTrialsHub\u901a\u8fc7\u6574\u5408\u548c\u7ed3\u6784\u5316\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc1\u636e\u83b7\u53d6\u6548\u7387\uff0c\u6709\u6f5c\u529b\u4fc3\u8fdb\u5faa\u8bc1\u533b\u5b66\u53d1\u5c55\uff0c\u4e3a\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u66f4\u597d\u7684\u652f\u6301"}}
{"id": "2512.07998", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07998", "abs": "https://arxiv.org/abs/2512.07998", "authors": ["Mostafa Kamali Tabrizi", "Mingshi Chi", "Bir Bikram Dey", "Yu Qing Yuan", "Markus D. Solbach", "Yiqian Liu", "Michael Jenkin", "John K. Tsotsos"], "title": "DIJIT: A Robotic Head for an Active Observer", "comment": null, "summary": "We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.", "AI": {"tldr": "DIJIT\u662f\u4e00\u4e2a\u4e13\u4e3a\u79fb\u52a8\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u53cc\u76ee\u673a\u5668\u4eba\u5934\u90e8\uff0c\u5177\u67099\u4e2a\u673a\u68b0\u81ea\u7531\u5ea6\u548c4\u4e2a\u5149\u5b66\u81ea\u7531\u5ea6\uff0c\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u773c-\u5934-\u9888\u8fd0\u52a8\uff0c\u7528\u4e8e\u4e3b\u52a8\u89c6\u89c9\u7814\u7a76\u548c\u4eba\u673a\u89c6\u89c9\u5bf9\u6bd4\u7814\u7a76\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u773c-\u5934-\u9888\u8fd0\u52a8\u7684\u673a\u5668\u4eba\u5934\u90e8\uff0c\u7528\u4e8e\u7814\u7a76\u4e3b\u52a8\u89c6\u89c9\u3001\u4eba\u7c7b\u89c6\u89c9\u673a\u5236\u4ee5\u53ca\u4eba\u673a\u89c6\u89c9\u5dee\u5f02\uff0c\u7279\u522b\u662f\u63a2\u7d22\u773c\u52a8\u548c\u5934\u52a8\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u5177\u67099\u4e2a\u673a\u68b0\u81ea\u7531\u5ea6\u548c4\u4e2a\u5149\u5b66\u81ea\u7531\u5ea6\u7684\u53cc\u76ee\u673a\u5668\u4eba\u5934\u90e8DIJIT\uff0c\u5b9e\u73b0\u4e86\u4eba\u7c7b\u6c34\u5e73\u7684\u8fd0\u52a8\u8303\u56f4\u548c\u901f\u5ea6\uff0c\u652f\u6301\u6c47\u805a\u7acb\u4f53\u89c6\u89c9\u6240\u9700\u7684\u8fd0\u52a8\uff08\u8f90\u8f8f\u3001\u5171\u8f6d\u3001\u65cb\u8f6c\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u626b\u89c6\u76f8\u673a\u8fd0\u52a8\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u76f8\u673a\u65b9\u5411\u4e0e\u7535\u673a\u503c\u4e4b\u95f4\u7684\u76f4\u63a5\u5173\u7cfb\u3002", "result": "DIJIT\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u4eba\u7c7b\u8fd0\u52a8\u7684\u626b\u89c6\u76f8\u673a\u8fd0\u52a8\u7cbe\u5ea6\uff0c\u5176\u673a\u68b0\u8bbe\u8ba1\u53c2\u6570\u4e0e\u4eba\u7c7b\u6027\u80fd\u76f8\u5f53\uff0c\u4e3a\u4e3b\u52a8\u89c6\u89c9\u7814\u7a76\u548c\u4eba\u673a\u89c6\u89c9\u5bf9\u6bd4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u9a8c\u5e73\u53f0\u3002", "conclusion": "DIJIT\u662f\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u7684\u673a\u5668\u4eba\u5934\u90e8\u5e73\u53f0\uff0c\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u773c-\u5934-\u9888\u8fd0\u52a8\uff0c\u4e3a\u4e3b\u52a8\u89c6\u89c9\u7814\u7a76\u3001\u4eba\u7c7b\u89c6\u89c9\u673a\u5236\u63a2\u7d22\u4ee5\u53ca\u4eba\u673a\u89c6\u89c9\u5dee\u5f02\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5176\u626b\u89c6\u8fd0\u52a8\u65b9\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2512.08404", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08404", "abs": "https://arxiv.org/abs/2512.08404", "authors": ["Sjoerd B. Stolwijk", "Mark Boukes", "Damian Trilling"], "title": "Are generative AI text annotations systematically biased?", "comment": "9 pages, 6 figures, 1 table; version submitted to the International Communication Association Annual Conference in Cape Town 2026", "summary": "This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6982\u5ff5\u6027\u590d\u5236Boukes(2024)\u7684\u624b\u52a8\u6807\u6ce8\uff0c\u53d1\u73b0GLLMs\u5728F1\u5206\u6570\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4e0e\u4eba\u5de5\u6807\u6ce8\u5728\u6d41\u884c\u5ea6\u3001\u4e0b\u6e38\u7ed3\u679c\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff08GLLMs\u4e4b\u95f4\u4e00\u81f4\u6027\u9ad8\u4e8e\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u4e00\u81f4\u6027\uff09", "motivation": "\u8c03\u67e5\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b(GLLMs)\u5728\u6587\u672c\u6807\u6ce8\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u504f\u89c1\u95ee\u9898\uff0c\u901a\u8fc7\u6982\u5ff5\u6027\u590d\u5236\u5148\u524d\u7814\u7a76\u7684\u624b\u52a8\u6807\u6ce8\u6765\u8bc4\u4f30GLLMs\u7684\u6807\u6ce8\u8d28\u91cf\u4e0e\u53ef\u9760\u6027", "method": "\u4f7f\u7528\u591a\u79cdGLLMs\uff08Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b\uff09\u914d\u5408\u4e94\u79cd\u4e0d\u540c\u7684\u63d0\u793a\u8bcd\uff0c\u5bf9\u4e94\u4e2a\u6982\u5ff5\uff08\u653f\u6cbb\u5185\u5bb9\u3001\u4e92\u52a8\u6027\u3001\u7406\u6027\u3001\u4e0d\u6587\u660e\u6027\u3001\u610f\u8bc6\u5f62\u6001\uff09\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u4e0eBoukes(2024)\u7684\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u5bf9\u6bd4", "result": "GLLMs\u5728F1\u5206\u6570\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4e0e\u4eba\u5de5\u6807\u6ce8\u5728\u6d41\u884c\u5ea6\u4f30\u8ba1\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u4e0b\u6e38\u7ed3\u679c\u663e\u8457\u4e0d\u540c\uff1bGLLMs\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5b83\u4eec\u5f7c\u6b64\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u9ad8\u4e8e\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u4e00\u81f4\u6027\uff1bF1\u5206\u6570\u5dee\u5f02\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u504f\u89c1\u7a0b\u5ea6", "conclusion": "\u867d\u7136GLLMs\u5728\u6280\u672f\u6307\u6807\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u6807\u6ce8\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u5b58\u5728\u5b9e\u8d28\u6027\u5dee\u5f02\uff0c\u4ec5\u4f9d\u8d56F1\u5206\u6570\u8bc4\u4f30GLLMs\u6807\u6ce8\u8d28\u91cf\u53ef\u80fd\u4e0d\u591f\u5145\u5206\uff0c\u9700\u8981\u8003\u8651\u504f\u89c1\u5bf9\u4e0b\u6e38\u5206\u6790\u7684\u5f71\u54cd"}}
{"id": "2512.08028", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08028", "abs": "https://arxiv.org/abs/2512.08028", "authors": ["Lampis Papakostas", "Aristeidis Geladaris", "Athanasios Mastrogeorgiou", "Jim Sharples", "Gautier Hattenberger", "Panagiotis Chatzakos", "Panagiotis Polygerinos"], "title": "Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations", "comment": null, "summary": "This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u707e\u5bb3\u6551\u63f4\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u5ef6\u957f\u98de\u884c\u65f6\u95f4\uff0c\u91c7\u7528\u5c40\u90e8ESDF\u5730\u56fe\u8fdb\u884c\u907f\u969c\uff0c\u7ed3\u5408TSP\u53d8\u4f53\u4f18\u5316\u5174\u8da3\u70b9\u8986\u76d6\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5728\u91ce\u706b\u7b49\u707e\u5bb3\u573a\u666f\u4e2d\uff0c\u7b2c\u4e00\u54cd\u5e94\u8005\u9700\u8981\u9ad8\u6548\u7684\u73af\u5883\u611f\u77e5\u548c\u6551\u63f4\u652f\u6301\u3002\u4f20\u7edf\u5355\u65e0\u4eba\u673a\u7cfb\u7edf\u5b58\u5728\u98de\u884c\u65f6\u95f4\u6709\u9650\u3001\u6570\u636e\u53ef\u7528\u6027\u4e0d\u8db3\u3001\u78b0\u649e\u98ce\u9669\u9ad8\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u96c6\u7fa4\u7cfb\u7edf\u6765\u63d0\u5347\u707e\u5bb3\u54cd\u5e94\u80fd\u529b\u3002", "method": "1) \u91c7\u7528\u591a\u667a\u80fd\u4f53\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u7cfb\u7edf\u5ef6\u957f\u98de\u884c\u65f6\u95f4\u548c\u589e\u5f3a\u6570\u636e\u53ef\u7528\u6027\uff1b2) \u57fa\u4e8e\u5c40\u90e8\u6b27\u51e0\u91cc\u5f97\u7b26\u53f7\u8ddd\u79bb\u573a(ESDF)\u5730\u56fe\u7684\u81ea\u4e3b\u5bfc\u822a\u6846\u67b6\u5b9e\u73b0\u907f\u969c\u548c\u96c6\u7fa4\u7f16\u961f\u4fdd\u6301\uff1b3) \u5f15\u5165\u65c5\u884c\u5546\u95ee\u9898(TSP)\u53d8\u4f53\u4f18\u5316\u533a\u57df\u8986\u76d6\uff0c\u6839\u636e\u73af\u5883\u884c\u4e3a\u548c\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u9884\u5206\u914d\u503c\u4f18\u5148\u8986\u76d6\u5174\u8da3\u70b9(POIs)\u3002", "result": "\u901a\u8fc7\u4e0d\u540c\u96c6\u7fa4\u89c4\u6a21\u7684\u4eff\u771f\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u80fd\u591f\u6700\u5927\u5316\u533a\u57df\u8986\u76d6\uff0c\u540c\u65f6\u786e\u4fdd\u65e0\u4eba\u673a\u4e4b\u95f4\u4ee5\u53ca\u65e0\u4eba\u673a\u4e0e\u969c\u788d\u7269\u4e4b\u95f4\u7684\u78b0\u649e\u907f\u514d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4efb\u52a1\u5931\u8d25\u98ce\u9669\u3002", "conclusion": "\u8be5\u65e0\u4eba\u673a\u96c6\u7fa4\u7cfb\u7edf\u4e3a\u707e\u5bb3\u54cd\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f20\u611f\u3001\u667a\u80fd\u907f\u969c\u548c\u4f18\u5316\u8986\u76d6\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6551\u63f4\u884c\u52a8\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.08440", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08440", "abs": "https://arxiv.org/abs/2512.08440", "authors": ["Jani\u00e7a Hackenbuchner", "Arda Tezcan", "Joke Daems"], "title": "What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models", "comment": null, "summary": "Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5bf9\u6bd4\u89e3\u91ca\u548c\u663e\u8457\u6027\u5f52\u56e0\u65b9\u6cd5\uff0c\u5206\u6790\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u4e2d\u6027\u522b\u504f\u89c1\u7684\u6765\u6e90\uff0c\u901a\u8fc7\u6027\u522b\u6a21\u7cca\u7684\u81ea\u7136\u6e90\u6570\u636e\uff0c\u8bc6\u522b\u5f71\u54cd\u76ee\u6807\u8bed\u8a00\u6027\u522b\u5c48\u6298\u9009\u62e9\u7684\u6e90\u8bcd\uff0c\u5e76\u4e0e\u4eba\u7c7b\u611f\u77e5\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7406\u89e3\u9ed1\u76d2\u6a21\u578b\u51b3\u7b56\uff0c\u4f46\u5bf9\u673a\u5668\u7ffb\u8bd1\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\u7814\u7a76\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u8d85\u8d8a\u7b80\u5355\u7684\u504f\u89c1\u6d4b\u91cf\uff0c\u63a2\u7d22\u6027\u522b\u504f\u89c1\u7684\u6839\u6e90\u3002", "method": "\u4f7f\u7528\u6027\u522b\u6a21\u7cca\u7684\u81ea\u7136\u6e90\u6570\u636e\uff0c\u91c7\u7528\u5bf9\u6bd4\u89e3\u91ca\u548c\u663e\u8457\u6027\u5f52\u56e0\u65b9\u6cd5\uff0c\u5206\u6790\u6e90\u53e5\u4e2d\u54ea\u4e9b\u8f93\u5165\u8bcd\u5f71\u54cd\u7ffb\u8bd1\u6a21\u578b\u9009\u62e9\u7279\u5b9a\u6027\u522b\u5c48\u6298\u3002\u9996\u5148\u89e3\u51b3\u7f3a\u4e4f\u8bc4\u5206\u9608\u503c\u7684\u95ee\u9898\uff0c\u6bd4\u8f83\u4e0d\u540c\u5f52\u56e0\u6c34\u5e73\u4e0b\u6e90\u8bcd\u5bf9\u6a21\u578b\u6027\u522b\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u663e\u8457\u6e90\u8bcd\u4e0e\u4eba\u7c7b\u6027\u522b\u611f\u77e5\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u91cd\u53e0\uff0c\u63d0\u4f9b\u4e86\u663e\u8457\u8bcd\u7684\u8bed\u8a00\u5b66\u5206\u6790\u3002\u5c55\u793a\u4e86\u7406\u89e3\u6a21\u578b\u7ffb\u8bd1\u51b3\u7b56\u5728\u6027\u522b\u65b9\u9762\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u8fd9\u4e0e\u4eba\u7c7b\u51b3\u7b56\u7684\u6bd4\u8f83\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u7406\u89e3\u6a21\u578b\u7ffb\u8bd1\u51b3\u7b56\u5728\u6027\u522b\u65b9\u9762\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u4fe1\u606f\u5e94\u88ab\u7528\u4e8e\u51cf\u8f7b\u6027\u522b\u504f\u89c1\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5229\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7f13\u89e3\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6027\u522b\u504f\u89c1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.08052", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08052", "abs": "https://arxiv.org/abs/2512.08052", "authors": ["Pedro Santana"], "title": "An Introduction to Deep Reinforcement and Imitation Learning", "comment": null, "summary": "Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u548c\u6df1\u5ea6\u6a21\u4eff\u5b66\u4e60(DIL)\u5728\u5177\u8eab\u667a\u80fd\u4f53\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u65b9\u6cd5\u8bb2\u89e3\u57fa\u7840\u7b97\u6cd5\uff0c\u6db5\u76d6\u4ece\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5230PPO\u548cGAIL\u7b49\u5173\u952e\u6280\u672f\u3002", "motivation": "\u5177\u8eab\u667a\u80fd\u4f53\uff08\u5982\u673a\u5668\u4eba\u548c\u865a\u62df\u89d2\u8272\uff09\u9700\u8981\u89e3\u51b3\u590d\u6742\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u624b\u52a8\u8bbe\u8ba1\u63a7\u5236\u5668\u5f88\u56f0\u96be\uff0c\u56e0\u6b64\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff08\u7279\u522b\u662fDRL\u548cDIL\uff09\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u7684\u6587\u732e\u7814\u7a76\u65b9\u6cd5\uff0c\u81ea\u5305\u542b\u5730\u5448\u73b0\u5fc5\u8981\u7684\u6570\u5b66\u548c\u673a\u5668\u5b66\u4e60\u6982\u5ff5\u3002\u91cd\u70b9\u4ecb\u7ecd\u4e00\u5c0f\u5957\u57fa\u7840\u7b97\u6cd5\u548c\u6280\u672f\uff0c\u5305\u62ec\uff1aDRL\u90e8\u5206\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3001REINFORCE\u3001PPO\uff1bDIL\u90e8\u5206\u7684\u884c\u4e3a\u514b\u9686\u3001DAgger\u3001GAIL\u3002", "result": "\u6587\u6863\u63d0\u4f9b\u4e86\u5bf9DRL\u548cDIL\u5728\u5177\u8eab\u667a\u80fd\u4f53\u63a7\u5236\u4e2d\u7684\u7cfb\u7edf\u6027\u4ecb\u7ecd\uff0c\u5efa\u7acb\u4e86\u4ece\u7406\u8bba\u57fa\u7840\u5230\u5b9e\u8df5\u7b97\u6cd5\u7684\u5b8c\u6574\u77e5\u8bc6\u6846\u67b6\uff0c\u4e3a\u8bfb\u8005\u6df1\u5165\u7406\u89e3\u8fd9\u4e9b\u6280\u672f\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6df1\u5ea6\u4f18\u5148\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5730\u4ecb\u7ecd\u4e86DRL\u548cDIL\u5728\u5177\u8eab\u667a\u80fd\u4f53\u63a7\u5236\u4e2d\u7684\u6838\u5fc3\u7b97\u6cd5\uff0c\u4e3a\u5b66\u4e60\u548c\u5e94\u7528\u8fd9\u4e9b\u6280\u672f\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u6df1\u5ea6\u7406\u89e3\u800c\u975e\u5e7f\u6cdb\u8986\u76d6\u3002"}}
{"id": "2512.08480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08480", "abs": "https://arxiv.org/abs/2512.08480", "authors": ["Ju-Young Kim", "Ji-Hong Park", "Se-Yeon Lee", "Sujin Park", "Gun-Woo Kim"], "title": "Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models", "comment": "in Korean language, Published in the Proceedings of the 37th Annual Conference on Human and Language Technology, 2025, pp. 714-719. (English translation assisted by GPT)", "summary": "Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u8f6f\u5f52\u7eb3\u504f\u7f6e\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u5b9a\u4e49\u63a8\u7406\u89c6\u89d2\u6765\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u7528\u4e8e\u97e9\u8bed\u4e0d\u5f53\u8a00\u8bba\u68c0\u6d4b\uff0c\u76f8\u6bd4\u6807\u51c6\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u7ea63.89%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u7ebf\u6e38\u620f\u548c\u793e\u533a\u4e2d\u533f\u540d\u73af\u5883\u4e0b\u7684\u4e0d\u5f53\u8a00\u8bba\u7ecf\u5e38\u5347\u7ea7\u4e3a\u8a00\u8bed\u66b4\u529b\u548c\u72af\u7f6a\u884c\u4e3a\uff0c\u9700\u8981\u68c0\u6d4b\u6280\u672f\u6765\u6784\u5efa\u66f4\u5b89\u5168\u7684\u4ea4\u6d41\u73af\u5883\u3002\u867d\u7136\u97e9\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5728\u4e0d\u5f53\u8a00\u8bba\u68c0\u6d4b\u65b9\u9762\u7684\u5e94\u7528\u7814\u7a76\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8f6f\u5f52\u7eb3\u504f\u7f6e\u65b9\u6cd5\uff0c\u660e\u786e\u5b9a\u4e49\u63a8\u7406\u89c6\u89d2\u6765\u5f15\u5bfc\u63a8\u7406\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u7406\u6027\u51b3\u7b56\u5e76\u9632\u6b62\u63a8\u7406\u9519\u8bef\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u5bf9\u97e9\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u8fdb\u884c\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\u7684\u5b9a\u91cf\u6027\u80fd\u6bd4\u8f83\u548c\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "Kanana-1.5\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523087.0046\uff0c\u76f8\u6bd4\u6807\u51c6\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u7ea63.89%\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6a21\u4eff\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u8fd8\u80fd\u901a\u8fc7\u7ea6\u675f\u63a8\u7406\u89c6\u89d2\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u4e00\u81f4\u7684\u5224\u65ad\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f6f\u5f52\u7eb3\u504f\u7f6e\u65b9\u6cd5\u901a\u8fc7\u7ea6\u675f\u63a8\u7406\u89c6\u89d2\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u66f4\u7cbe\u786e\u548c\u4e00\u81f4\u7684\u5224\u65ad\uff0c\u5728\u4e0d\u5f53\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u77e5\u8bc6\u6a21\u4eff\u3002"}}
{"id": "2512.08145", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08145", "abs": "https://arxiv.org/abs/2512.08145", "authors": ["Haoran Wang", "Zhuohang Chen", "Guang Li", "Bo Ma", "Chuanghuang Li"], "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models", "comment": null, "summary": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u667a\u80fd\u4f53LLM\u6846\u67b6\u7684\u4eba\u673a\u65e0\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u5206\u79bb\u7684\u67b6\u6784\uff0c\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u4ea4\u4e92\u6d41\u7545\u5ea6\u548c\u4efb\u52a1\u6267\u884c\u7075\u6d3b\u6027\u3002", "motivation": "\u5f53\u524d\u65e0\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u6b63\u4ece\u5de5\u7a0b\u5e08\u9a71\u52a8\u8f6c\u5411\u7528\u6237\u9a71\u52a8\uff0c\u4f46\u7f3a\u4e4f\u7528\u6237\u4e0e\u65e0\u4eba\u673a\u4e4b\u95f4\u7684\u901a\u7528\u8bed\u8a00\uff0c\u73b0\u6709\u57fa\u4e8eLLM\u7684HUI\u6846\u67b6\u5728\u6df7\u5408\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u96be\u4ee5\u9002\u5e94\u590d\u6742\u573a\u666f\u3002", "method": "\u63d0\u51fa\u53cc\u667a\u80fd\u4f53HUI\u6846\u67b6\uff0c\u6784\u5efa\u4e24\u4e2a\u72ec\u7acb\u7684LLM\u667a\u80fd\u4f53\uff08\u4efb\u52a1\u89c4\u5212\u667a\u80fd\u4f53\u548c\u6267\u884c\u667a\u80fd\u4f53\uff09\uff0c\u5e94\u7528\u4e0d\u540c\u7684\u63d0\u793a\u5de5\u7a0b\u5206\u522b\u5904\u7406\u4efb\u52a1\u7406\u89e3\u3001\u89c4\u5212\u548c\u6267\u884c\u3002", "result": "\u6784\u5efa\u4e86\u8986\u76d6\u56db\u4e2a\u5178\u578b\u65e0\u4eba\u673a\u5e94\u7528\u573a\u666f\u7684\u4efb\u52a1\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u4e09\u4e2a\u72ec\u7acb\u6307\u6807\u91cf\u5316HUI\u6846\u67b6\u6027\u80fd\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u63d0\u5347\u4e86HUI\u7684\u6d41\u7545\u5ea6\u548c\u4efb\u52a1\u6267\u884c\u7075\u6d3b\u6027\uff0c\u6709\u6548\u6ee1\u8db3\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\u3002", "conclusion": "\u53cc\u667a\u80fd\u4f53LLM\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u7684\u5206\u79bb\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709HUI\u6846\u67b6\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u4e3a\u4eba\u673a\u65e0\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u4e2a\u6027\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08545", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.08545", "abs": "https://arxiv.org/abs/2512.08545", "authors": ["Indrajit Kar", "Kalathur Chenchu Kishore Kumar"], "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks", "comment": "22 pages, 2 tables, 9 figures", "summary": "Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u572864\u00d764\u7f51\u683c\u4e0a\u5206\u5e03\u8f7b\u91cf\u7ea7\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7a7a\u95f4\u8bfe\u7a0b\u5b66\u4e60\u9010\u6b65\u6269\u5c55\u64cd\u4f5c\u533a\u57df\uff0c\u7ed3\u5408NLL\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u548cThompson\u91c7\u6837\u8bfe\u7a0b\u7ba1\u7406\u5668\uff0c\u5728\u7a7a\u95f4\u5316\u6c49\u8bfa\u5854\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3001\u66f4\u5c11\u7684oracle\u4f7f\u7528\u548c\u66f4\u5f3a\u7684\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5206\u89e3\u590d\u6742\u4efb\u52a1\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u957f\u65f6\u7a0b\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u56f0\u96be\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u65f6\u7a0b\u63a8\u7406\u3001\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u65b0\u67b6\u6784\u3002", "method": "1. \u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u572864\u00d764\u7f51\u683c\u4e0a\u5206\u5e03\u8f7b\u91cf\u7ea7\u667a\u80fd\u4f53\uff0c\u7531\u9009\u62e9\u6027oracle\u652f\u6301\uff1b2. \u7a7a\u95f4\u8bfe\u7a0b\u5b66\u4e60\uff1a\u9010\u6b65\u6269\u5c55\u7f51\u683c\u64cd\u4f5c\u533a\u57df\uff0c\u8ba9\u667a\u80fd\u4f53\u5148\u638c\u63e1\u4e2d\u5fc3\u7b80\u5355\u4efb\u52a1\u518d\u5904\u7406\u8fb9\u7f18\u56f0\u96be\u4efb\u52a1\uff1b3. \u7f6e\u4fe1\u5ea6\u96c6\u6210\uff1a\u4f7f\u7528\u8d1f\u5bf9\u6570\u4f3c\u7136(NLL)\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\uff1b4. Thompson\u91c7\u6837\u8bfe\u7a0b\u7ba1\u7406\u5668\uff1a\u57fa\u4e8e\u80fd\u529b\u548cNLL\u9a71\u52a8\u7684\u5956\u52b1\u4fe1\u53f7\u81ea\u9002\u5e94\u9009\u62e9\u8bad\u7ec3\u533a\u57df\u3002", "result": "\u5728\u7a7a\u95f4\u5316\u6c49\u8bfa\u5854\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\uff1a1. \u6539\u8fdb\u7684\u7a33\u5b9a\u6027\uff1b2. \u51cf\u5c11\u7684oracle\u4f7f\u7528\uff1b3. \u901a\u8fc7\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u66f4\u5f3a\u7684\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7ed3\u5408\u7a7a\u95f4\u8bfe\u7a0b\u5b66\u4e60\u548c\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u957f\u65f6\u7a0b\u63a8\u7406\u4efb\u52a1\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u7a7a\u95f4\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08170", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.08170", "abs": "https://arxiv.org/abs/2512.08170", "authors": ["Haoxin Zhang", "Shuaixin Li", "Xiaozhou Zhu", "Hongbo Chen", "Wen Yao"], "title": "RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features", "comment": null, "summary": "In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65e0\u9700\u6807\u5b9a\u677f\u3001\u517c\u5bb9\u591a\u79cdLiDAR\u548c\u76f8\u673a\u4f20\u611f\u5668\u7684\u6807\u5b9a\u5de5\u5177\u5305\uff0c\u4ec5\u9700\u5355\u5bf9\u6fc0\u5149\u70b9\u4e91\u548c\u56fe\u50cf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u52a0\u6743\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5916\u53c2\u6807\u5b9a\u3002", "motivation": "\u73b0\u6709LiDAR-\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u6807\u5b9a\u677f\u6216\u521d\u59cb\u53d8\u6362\u4f30\u8ba1\uff0c\u4e14\u5728\u4f20\u611f\u5668\u5916\u53c2\u504f\u5dee\u8f83\u5927\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u9c81\u68d2\u7684\u6807\u5b9a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u65e0\u6807\u5b9a\u677f\u73af\u5883\u3002", "method": "1) \u4f7f\u7528Gluestick\u7ba1\u9053\u5efa\u7acb2D-3D\u70b9\u548c\u7ebf\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u81ea\u52a8\u521d\u59cb\u4f30\u8ba1\uff1b2) \u5b9a\u91cf\u5206\u6790\u7279\u5f81\u5206\u5e03\u5bf9\u6807\u5b9a\u7ed3\u679c\u7684\u5f71\u54cd\uff1b3) \u57fa\u4e8e\u5206\u6790\u6307\u6807\u81ea\u9002\u5e94\u52a0\u6743\u6bcf\u4e2a\u7279\u5f81\u7684\u4ee3\u4ef7\uff0c\u8fc7\u6ee4\u52a3\u8d28\u7279\u5f81\u7684\u4e0d\u826f\u5f71\u54cd\u3002", "result": "\u5728\u5ba4\u5185\u5916\u591a\u79cdLiDAR-\u76f8\u673a\u4f20\u611f\u5668\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u5b9a\u677f\u3001\u65e0\u9700\u521d\u59cb\u53d8\u6362\u7684LiDAR-\u76f8\u673a\u6807\u5b9a\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u52a0\u6743\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5916\u53c2\u6807\u5b9a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2512.08617", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08617", "abs": "https://arxiv.org/abs/2512.08617", "authors": ["Lifeng Han", "Paul Rayson", "Suzan Verberne", "Andrew Moore", "Goran Nenadic"], "title": "HealthcareNLP: where are we and what is next?", "comment": "Accepted Tutorial by LREC 2026 https://lrec2026.info/", "summary": "This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is \"Introductory to CL/NLP topics (HealthcareNLP)\" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u533b\u7597\u9886\u57dfNLP\u5e94\u7528\u7684\u6559\u7a0b\u8bba\u6587\uff0c\u65e8\u5728\u7cfb\u7edf\u4ecb\u7ecdHealthcareNLP\u7684\u91cd\u8981\u5b50\u9886\u57df\uff0c\u5305\u62ec\u6570\u636e\u8d44\u6e90\u5c42\u3001NLP\u8bc4\u4f30\u5c42\u548c\u60a3\u8005\u5c42\u4e09\u4e2a\u5c42\u6b21\uff0c\u5e76\u5305\u542b\u5b9e\u8df5\u73af\u8282\u3002", "motivation": "\u73b0\u6709\u533b\u7597NLP\u9886\u57df\u7684\u7efc\u8ff0\u8981\u4e48\u5ffd\u7565\u4e86\u4e00\u4e9b\u91cd\u8981\u4efb\u52a1\uff08\u5982\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u53ef\u89e3\u91ca\u6027\u4e34\u5e8aNLP\uff09\uff0c\u8981\u4e48\u9057\u6f0f\u4e86\u5173\u952e\u65b9\u6cd5\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\uff09\u3002\u56e0\u6b64\u9700\u8981\u63d0\u4f9b\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u5165\u95e8\u6559\u7a0b\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u5c42\u6b21\u7ed3\u6784\u7ec4\u7ec7\u6559\u7a0b\u5185\u5bb9\uff1a1) \u6570\u636e/\u8d44\u6e90\u5c42\uff08\u6807\u6ce8\u6307\u5357\u3001\u4f26\u7406\u5ba1\u6279\u3001\u5408\u6210\u6570\u636e\u7b49\uff09\uff1b2) NLP\u8bc4\u4f30\u5c42\uff08NER\u3001\u5173\u7cfb\u62bd\u53d6\u3001\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u53ca\u53ef\u89e3\u91ca\u65b9\u6cd5\uff09\uff1b3) \u60a3\u8005\u5c42\uff08\u60a3\u8005\u53c2\u4e0e\u3001\u5065\u5eb7\u7d20\u517b\u3001\u7ffb\u8bd1\u7b80\u5316\u7b49\u4efb\u52a1\uff09\u3002\u5305\u542b\u5b9e\u8df5\u73af\u8282\u8ba9\u89c2\u4f17\u4f7f\u7528HealthcareNLP\u5e94\u7528\u3002", "result": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684HealthcareNLP\u6559\u7a0b\u6846\u67b6\uff0c\u6db5\u76d6\u4ece\u6570\u636e\u51c6\u5907\u5230\u60a3\u8005\u5e94\u7528\u7684\u5168\u6d41\u7a0b\uff0c\u9488\u5bf9\u533b\u7597NLP\u4ece\u4e1a\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u5b66\u751f\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u5373\u53ef\u53c2\u4e0e\u3002", "conclusion": "\u8be5\u6559\u7a0b\u4e3a\u533b\u7597\u9886\u57dfNLP\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5165\u95e8\u6307\u5357\uff0c\u901a\u8fc7\u4e09\u5c42\u67b6\u6784\u7cfb\u7edf\u6027\u5730\u4ecb\u7ecd\u4e86HealthcareNLP\u7684\u5173\u952e\u65b9\u9762\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2512.08186", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08186", "abs": "https://arxiv.org/abs/2512.08186", "authors": ["Meng Wei", "Chenyang Wan", "Jiaqi Peng", "Xiqian Yu", "Yuqiang Yang", "Delin Feng", "Wenzhe Cai", "Chenming Zhu", "Tai Wang", "Jiangmiao Pang", "Xihui Liu"], "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation", "comment": null, "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.", "AI": {"tldr": "DualVLN\uff1a\u9996\u4e2a\u53cc\u7cfb\u7edf\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u9ad8\u7ea7\u63a8\u7406\u4e0e\u4f4e\u7ea7\u52a8\u4f5c\u6267\u884c\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u5b9e\u65f6\u52a8\u6001\u73af\u5883\u5bfc\u822a", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u76f4\u63a5\u5c06\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u6620\u5c04\u5230\u77ed\u671f\u79bb\u6563\u52a8\u4f5c\uff0c\u5bfc\u81f4\u8fd0\u52a8\u788e\u7247\u5316\u3001\u5ef6\u8fdf\u9ad8\uff0c\u4e14\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u969c\u788d\u7269\u907f\u8ba9\u7b49\u73b0\u5b9e\u6311\u6218", "method": "\u63d0\u51fa\u53cc\u7cfb\u7edf\u67b6\u6784\uff1aSystem 2\uff08\u57fa\u4e8eVLM\u7684\u5168\u5c40\u89c4\u5212\u5668\uff09\u901a\u8fc7\u56fe\u50cf\u57fa\u7840\u63a8\u7406\u9884\u6d4b\u4e2d\u671f\u8def\u5f84\u70b9\u76ee\u6807\uff1bSystem 1\uff08\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6761\u4ef6\u6269\u6563\u53d8\u6362\u5668\u7b56\u7565\uff09\u5229\u7528System 2\u7684\u663e\u5f0f\u50cf\u7d20\u76ee\u6807\u548c\u6f5c\u5728\u7279\u5f81\u751f\u6210\u5e73\u6ed1\u51c6\u786e\u8f68\u8ff9", "result": "\u5728\u6240\u6709VLN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u5c55\u793a\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7a33\u5065\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u5b9e\u65f6\u9002\u5e94\u80fd\u529b", "conclusion": "\u53cc\u7cfb\u7edf\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5b9e\u65f6\u63a7\u5236\u548c\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u5c40\u90e8\u51b3\u7b56\uff0c\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u4fdd\u7559\u4e86VLM\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u5c40\u90e8\u5bfc\u822a"}}
{"id": "2512.08646", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.08646", "abs": "https://arxiv.org/abs/2512.08646", "authors": ["Maximilian Kreutner", "Jens Rupprecht", "Georg Ahnert", "Ahmed Salem", "Markus Strohmaier"], "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models", "comment": "The Python package is available at https://github.com/dess-mannheim/QSTN/", "summary": "We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.", "AI": {"tldr": "QSTN\u662f\u4e00\u4e2a\u5f00\u6e90Python\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u95ee\u5377\u5f0f\u63d0\u793a\u7cfb\u7edf\u751f\u6210LLM\u54cd\u5e94\uff0c\u652f\u6301\u865a\u62df\u8c03\u67e5\u548c\u6807\u6ce8\u4efb\u52a1\uff0c\u5305\u542b\u65e0\u4ee3\u7801\u754c\u9762\u548c\u4f4e\u6210\u672c\u5927\u89c4\u6a21\u8bc4\u4f30\u529f\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u7814\u7a76\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u751f\u6210\u95ee\u5377\u54cd\u5e94\u7684\u5de5\u5177\uff0c\u9700\u8981\u652f\u6301\u865a\u62df\u8c03\u67e5\u548c\u6807\u6ce8\u4efb\u52a1\uff0c\u540c\u65f6\u786e\u4fdd\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5f00\u53d1\u5f00\u6e90Python\u6846\u67b6QSTN\uff0c\u652f\u6301\u95ee\u5377\u5448\u73b0\u3001\u63d0\u793a\u6270\u52a8\u548c\u54cd\u5e94\u751f\u6210\u65b9\u6cd5\u7684\u8bc4\u4f30\uff0c\u63d0\u4f9b\u65e0\u4ee3\u7801\u7528\u6237\u754c\u9762\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\uff08\u8d85\u8fc74000\u4e07\u8c03\u67e5\u54cd\u5e94\uff09\u8bc4\u4f30\u4e0d\u540c\u56e0\u7d20\u5bf9\u54cd\u5e94\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u95ee\u9898\u7ed3\u6784\u548c\u54cd\u5e94\u751f\u6210\u65b9\u6cd5\u5bf9LLM\u751f\u6210\u8c03\u67e5\u54cd\u5e94\u4e0e\u4eba\u7c7b\u7b54\u6848\u7684\u4e00\u81f4\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e14\u53ef\u4ee5\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u83b7\u5f97\u3002\u6846\u67b6\u652f\u6301\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bbe\u7f6e\u3002", "conclusion": "QSTN\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u9ad8LLM\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5f3a\u5927\u7684\u5b9e\u9a8c\u5de5\u5177\uff0c\u652f\u6301\u672a\u6765LLM\u7814\u7a76\u7684\u7a33\u5065\u53d1\u5c55\u3002"}}
{"id": "2512.08188", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.08188", "abs": "https://arxiv.org/abs/2512.08188", "authors": ["Wenjiang Xu", "Cindy Wang", "Rui Fang", "Mingkang Zhang", "Lusong Li", "Jing Xu", "Jiayuan Gu", "Zecui Zeng", "Rui Chen"], "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model", "comment": "Website at https://embodied-tree-of-thoughts.github.io", "summary": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .", "AI": {"tldr": "\u63d0\u51faEToT\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u6a21\u62df\u7684\u6570\u5b57\u5b6a\u751f\u4f5c\u4e3a\u5177\u8eab\u4e16\u754c\u6a21\u578b\uff0c\u5c06\u64cd\u4f5c\u89c4\u5212\u6784\u5efa\u4e3a\u6811\u641c\u7d22\uff0c\u7ed3\u5408\u5148\u9a8c\u5206\u652f\u548c\u53cd\u601d\u5206\u652f\u673a\u5236\uff0c\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u751f\u6210\u53ef\u9760\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u8ba1\u5212\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u7269\u7406\u57fa\u7840\uff0c\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u4e14\u65e0\u6cd5\u4fdd\u6301\u957f\u65f6\u7a0b\u7269\u7406\u7ea6\u675f\u7684\u4e00\u81f4\u6027\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u5177\u8eab\u4e16\u754c\u6a21\u578b\u6765\u652f\u6301\u673a\u5668\u4eba\u64cd\u4f5c\u89c4\u5212\u3002", "method": "\u63d0\u51faEmbodied Tree of Thoughts (EToT)\u6846\u67b6\uff0c\u91c7\u7528Real2Sim2Real\u65b9\u6cd5\uff0c\u5229\u7528\u7269\u7406\u4ea4\u4e92\u6570\u5b57\u5b6a\u751f\u4f5c\u4e3a\u5177\u8eab\u4e16\u754c\u6a21\u578b\u3002\u89c4\u5212\u8fc7\u7a0b\u901a\u8fc7\u4e24\u79cd\u5206\u652f\u673a\u5236\uff1a\u5148\u9a8c\u5206\u652f\uff08\u57fa\u4e8e\u8bed\u4e49\u7a7a\u95f4\u5206\u6790\u751f\u6210\u5019\u9009\u8def\u5f84\uff09\u548c\u53cd\u601d\u5206\u652f\uff08\u5229\u7528VLM\u8bca\u65ad\u6a21\u62df\u4e2d\u7684\u6267\u884c\u5931\u8d25\u5e76\u8fed\u4ee3\u4f18\u5316\u89c4\u5212\u6811\uff09\u3002", "result": "\u5728\u77ed\u65f6\u7a0b\u548c\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u5957\u4ef6\u4e0a\u9a8c\u8bc1\uff0cEToT\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u9884\u6d4b\u7269\u7406\u52a8\u529b\u5b66\u5e76\u9002\u5e94\u6f5c\u5728\u5931\u8d25\u3002", "conclusion": "EToT\u901a\u8fc7\u7269\u7406\u6a21\u62df\u7684\u6570\u5b57\u5b6a\u751f\u5c06\u9ad8\u5c42\u63a8\u7406\u4e0e\u7269\u7406\u7ea6\u675f\u76f8\u7ed3\u5408\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u7269\u7406\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08659", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08659", "abs": "https://arxiv.org/abs/2512.08659", "authors": ["Bohao Yang", "Rui Yang", "Joshua M. Biro", "Haoyuan Wang", "Jessica L. Handley", "Brianna Richardson", "Sophia Bessias", "Nicoleta Economou-Zavlanos", "Armando D. Bedoya", "Monica Agrawal", "Michael M. Zavlanos", "Anand Chowdhury", "Raj M. Ratwani", "Kai Sun", "Kathryn I. Pollak", "Michael J. Pencina", "Chuan Hong"], "title": "An Agentic AI System for Multi-Framework Communication Coding", "comment": null, "summary": "Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.", "AI": {"tldr": "MOSAIC\u662f\u4e00\u4e2a\u57fa\u4e8eLangGraph\u7684\u591a\u6846\u67b6\u7ed3\u6784\u5316AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e34\u5e8a\u6c9f\u901a\u5206\u6790\uff0c\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u4ee3\u7406\u5b9e\u73b0\u81ea\u52a8\u5316\u6807\u6ce8\uff0c\u5728\u98ce\u6e7f\u75c5\u5b66\u548c\u5987\u4ea7\u79d1\u9886\u57df\u8fbe\u52300.928\u7684F1\u5206\u6570\u3002", "motivation": "\u4e34\u5e8a\u6c9f\u901a\u5bf9\u60a3\u8005\u7ed3\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u60a3\u8005-\u63d0\u4f9b\u8005\u5bf9\u8bdd\u52b3\u52a8\u5bc6\u96c6\u3001\u4e0d\u4e00\u81f4\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4efb\u52a1\u6a21\u578b\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u6c9f\u901a\u6846\u67b6\u548c\u4e34\u5e8a\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eLangGraph\u67b6\u6784\u7684MOSAIC\u7cfb\u7edf\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u4ee3\u7406\uff1a\u8ba1\u5212\u4ee3\u7406\uff08\u4ee3\u7801\u672c\u9009\u62e9\u548c\u6d41\u7a0b\u89c4\u5212\uff09\u3001\u66f4\u65b0\u4ee3\u7406\uff08\u7ef4\u62a4\u6700\u65b0\u68c0\u7d22\u6570\u636e\u5e93\uff09\u3001\u6807\u6ce8\u4ee3\u7406\uff08\u5e94\u7528\u4ee3\u7801\u672c\u5f15\u5bfc\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u52a8\u6001\u5c11\u6837\u672c\u63d0\u793a\uff09\u3001\u9a8c\u8bc1\u4ee3\u7406\uff08\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u53cd\u9988\uff09\u3002", "result": "\u572826\u4e2a\u8bad\u7ec3\u8f6c\u5f55\u672c\u548c50\u4e2a\u6d4b\u8bd5\u8f6c\u5f55\u672c\u4e0a\u8bc4\u4f30\uff0cMOSAIC\u603b\u4f53F1\u5206\u6570\u4e3a0.928\u3002\u98ce\u6e7f\u75c5\u5b66\u5b50\u96c6\u8868\u73b0\u6700\u4f73\uff08F1=0.962\uff09\uff0c\u60a3\u8005\u884c\u4e3a\u7c7b\u522b\uff08\u5982\u63d0\u95ee\u3001\u8868\u8fbe\u504f\u597d\u3001\u8868\u73b0\u81ea\u4fe1\uff09\u8868\u73b0\u6700\u5f3a\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793aMOSAIC\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "MOSAIC\u7cfb\u7edf\u901a\u8fc7\u591a\u4ee3\u7406\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4e34\u5e8a\u6c9f\u901a\u6807\u6ce8\u7684\u53ef\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u4e0d\u540c\u4e34\u5e8a\u9886\u57df\u548c\u6c9f\u901a\u6846\u67b6\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2512.08206", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08206", "abs": "https://arxiv.org/abs/2512.08206", "authors": ["Duo Zhang", "Junshan Huang", "Jingjin Yu"], "title": "High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement", "comment": "ICRA 2026 Submission", "summary": "We propose Synchronous Dual-Arm Rearrangement Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal configurations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR-M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state-of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware.", "AI": {"tldr": "SDAR\u662f\u4e00\u4e2a\u7528\u4e8e\u684c\u9762\u91cd\u6392\u7684\u53cc\u81c2\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u5bc6\u96c6\u6210\u7684\u4efb\u52a1\u89c4\u5212\u5668\u548c\u540c\u6b65\u53cc\u81c2\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u5728\u590d\u6742\u7ea0\u7f20\u573a\u666f\u4e2d\u5b9e\u73b0100%\u6210\u529f\u7387\u548c\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u53cc\u81c2\u673a\u5668\u4eba\u5728\u7d27\u5bc6\u534f\u4f5c\u73af\u5883\u4e0b\u91cd\u6392\u9ad8\u5ea6\u7ea0\u7f20\u7269\u4f53\u7684\u6311\u6218\uff0c\u4f20\u7edf\u5355\u81c2\u89c4\u5212\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u540c\u6b65\u534f\u4f5c\u9700\u6c42\u3002", "method": "SDAR\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) SDAR-T\u4efb\u52a1\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5206\u89e3\u5168\u5c40\u5bf9\u8c61\u4f9d\u8d56\u56fe\u751f\u6210\u4f18\u5316\u7684\u53cc\u81c2\u4efb\u52a1\u8ba1\u5212\uff1b2) SDAR-M\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u91c7\u7528\u5206\u5c42\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u5229\u7528GPU SIMD\u5de5\u5177\u7b5b\u9009\u6700\u4f73\u540c\u6b65\u53cc\u81c2\u8fd0\u52a8\u8ba1\u5212\u3002", "result": "\u5728\u590d\u6742\u3001\u975e\u5355\u8c03\u3001\u957f\u89c6\u91ce\u7684\u684c\u9762\u91cd\u6392\u4efb\u52a1\u4e2d\u5b9e\u73b0100%\u6210\u529f\u7387\uff0c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u8fdc\u8d85\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728UR-5e\u53cc\u81c2\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u9760\u7684\u5b9e\u9645\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "SDAR\u6846\u67b6\u901a\u8fc7\u7d27\u5bc6\u96c6\u6210\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u81c2\u534f\u4f5c\u91cd\u6392\u9ad8\u5ea6\u7ea0\u7f20\u7269\u4f53\u7684\u6311\u6218\uff0c\u5728\u6210\u529f\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u5907\u5b9e\u9645\u673a\u5668\u4eba\u90e8\u7f72\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.08713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08713", "abs": "https://arxiv.org/abs/2512.08713", "authors": ["Ekhi Azurmendi", "Xabier Arregi", "Oier Lopez de Lacalle"], "title": "Automatic Essay Scoring and Feedback Generation in Basque Language Learning", "comment": "Submitted to LREC 2026", "summary": "This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.", "AI": {"tldr": "\u9996\u4e2a\u5df4\u65af\u514b\u8bed\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e0e\u53cd\u9988\u751f\u6210\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5305\u542b3200\u7bc7CEFR C1\u7ea7\u522b\u4f5c\u6587\uff0c\u4e13\u5bb6\u6807\u6ce8\u8bc4\u5206\u4e0e\u8be6\u7ec6\u53cd\u9988\u3002\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\u5728\u8bc4\u5206\u4e00\u81f4\u6027\u548c\u53cd\u9988\u8d28\u91cf\u4e0a\u8d85\u8d8aGPT-5\u7b49\u95ed\u6e90\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5df4\u65af\u514b\u8bed\uff09\u5efa\u7acb\u900f\u660e\u3001\u53ef\u590d\u73b0\u4e14\u6559\u80b2\u57fa\u7840\u624e\u5b9e\u7684NLP\u7814\u7a76\u8d44\u6e90\uff0c\u586b\u8865\u5df4\u65af\u514b\u8bed\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e0e\u53cd\u9988\u751f\u6210\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u6536\u96c63200\u7bc7\u5df4\u65af\u514b\u8bedC1\u7ea7\u522b\u4f5c\u6587\uff0c\u4e13\u5bb6\u6807\u6ce8\u4e94\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u5206\u548c\u8be6\u7ec6\u53cd\u9988\u3002\u5fae\u8c03RoBERTa-EusCrawl\u548cLatxa 8B/70B\u7b49\u5f00\u6e90\u6a21\u578b\uff0c\u63d0\u51fa\u7ed3\u5408\u81ea\u52a8\u4e00\u81f4\u6027\u6307\u6807\u548c\u4e13\u5bb6\u9a8c\u8bc1\u7684\u65b0\u578b\u53cd\u9988\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u7f16\u7801\u5668\u6a21\u578b\u5728\u81ea\u52a8\u8bc4\u5206\u4e2d\u4fdd\u6301\u9ad8\u53ef\u9760\u6027\uff0cSFT\u5fae\u8c03\u7684Latxa\u6a21\u578b\u5728\u8bc4\u5206\u4e00\u81f4\u6027\u548c\u53cd\u9988\u8d28\u91cf\u4e0a\u8d85\u8d8aGPT-5\u3001Claude Sonnet 4.5\u7b49\u95ed\u6e90\u7cfb\u7edf\uff0c\u80fd\u751f\u6210\u4e0e\u8bc4\u5206\u6807\u51c6\u5bf9\u9f50\u3001\u5177\u6709\u6559\u5b66\u610f\u4e49\u7684\u53cd\u9988\uff0c\u8bc6\u522b\u66f4\u591a\u9519\u8bef\u7c7b\u578b\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4e3a\u5df4\u65af\u514b\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u5efa\u7acb\u4e86\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u6559\u80b2NLP\u7814\u7a76\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5f00\u6e90\u6a21\u578b\u5728\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u548c\u53cd\u9988\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.08233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08233", "abs": "https://arxiv.org/abs/2512.08233", "authors": ["Timothy Chen", "Marcus Dominguez-Kuhne", "Aiden Swann", "Xu Liu", "Mac Schwager"], "title": "Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior", "comment": null, "summary": "Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6846\u67b6\u4ece\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4e2d\u5b66\u4e60\u7a7a\u95f4\u53d8\u5316\u7684\u98ce\u9669\u6a21\u578b\uff0c\u901a\u8fc7VLM\u5148\u9a8c\u548cViT\u4f3c\u7136\u51fd\u6570\u751f\u6210\u50cf\u7d20\u7ea7\u98ce\u9669\u56fe\u50cf\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u89c4\u5212\u4efb\u52a1", "motivation": "\u4eba\u7c7b\u5bf9\u5b89\u5168\u7684\u7406\u89e3\u4e0d\u662f\u4e8c\u5143\u4fe1\u53f7\uff0c\u800c\u662f\u8fde\u7eed\u3001\u4e0a\u4e0b\u6587\u548c\u7a7a\u95f4\u76f8\u5173\u7684\u98ce\u9669\u6982\u5ff5\u3002\u4e3a\u4e86\u8ba9\u81ea\u4e3b\u7cfb\u7edf\u80fd\u591f\u5185\u5316\u7c7b\u4f3c\u4eba\u7c7b\u7684\u98ce\u9669\u8ba4\u77e5\uff0c\u9700\u8981\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u63d0\u53d6\u9690\u5f0f\u98ce\u9669\u6a21\u578b", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u98ce\u9669\u6846\u67b6\uff1a1) \u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5148\u9a8c\u77e5\u8bc6\uff1b2) \u5b66\u4e60ViT\u4f3c\u7136\u51fd\u6570\u5c06\u9884\u8bad\u7ec3\u7279\u5f81\u6620\u5c04\u5230\u50cf\u7d20\u5bf9\u9f50\u7684\u98ce\u9669\u503c\uff1b3) \u8f93\u5165RGB\u56fe\u50cf\u548c\u67e5\u8be2\u5bf9\u8c61\u5b57\u7b26\u4e32\uff0c\u8f93\u51fa\u50cf\u7d20\u5bc6\u96c6\u7684\u98ce\u9669\u56fe\u50cf", "result": "\u6846\u67b6\u80fd\u591f\u751f\u6210\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u7684\u60c5\u5883\u5316\u98ce\u9669\u4f30\u8ba1\uff0c\u53ef\u6cdb\u5316\u5230\u65b0\u5bf9\u8c61\u548c\u4e0a\u4e0b\u6587\uff0c\u5e76\u652f\u6301\u5feb\u901f\u9002\u5e94\u989d\u5916\u89c2\u5bdf\u6216\u5e38\u8bc6\u89c4\u5219\u3002\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\uff0c\u53ef\u4f5c\u4e3a\u89c6\u89c9\u8fd0\u52a8\u89c4\u5212\u7684\u4ef7\u503c\u9884\u6d4b\u5668\u6216\u4e0e\u7ecf\u5178\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u7ed3\u5408", "conclusion": "\u8be5\u6846\u67b6\u662f\u4f7f\u81ea\u4e3b\u7cfb\u7edf\u5185\u5316\u4eba\u7c7b\u98ce\u9669\u8ba4\u77e5\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u5177\u6709\u6269\u5c55\u5230\u66f4\u5927\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6f5c\u529b\uff0c\u8d1d\u53f6\u65af\u6846\u67b6\u652f\u6301\u5feb\u901f\u9002\u5e94\u65b0\u89c2\u5bdf\u548c\u5e38\u8bc6\u89c4\u5219"}}
{"id": "2512.08777", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08777", "abs": "https://arxiv.org/abs/2512.08777", "authors": ["David Samuel", "Lilja \u00d8vrelid", "Erik Velldal", "Andrey Kutuzov"], "title": "Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages", "comment": null, "summary": "We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokm\u00e5l and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5373\u4f7f\u4f7f\u7528\u4e0d\u6d41\u5229\u7684\u5956\u52b1\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u7545\u6027\uff0c\u65e0\u9700\u76ee\u6807\u8bed\u8a00\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u3002", "motivation": "\u504f\u597d\u4f18\u5316\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u548c\u4e2d\u6587\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u6bcd\u8bed\u8005\u64b0\u5199\u7684\u6570\u636e\u96c6\u548c\u751f\u6210\u6d41\u7545\u5408\u6210\u6570\u636e\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u76ee\u6807\u8bed\u8a00\u6307\u4ee4\u6570\u636e\u7684\u6d41\u7545\u504f\u597d\u5bf9\u9f50\u6a21\u578b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7b56\u7565\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e0e\u4e24\u79cd\u5e38\u89c1\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff1a\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4ee5\u53ca\u591a\u8bed\u8a00\u5fae\u8c03\u3002\u4ee5\u632a\u5a01\u535a\u514b\u9a6c\u5c14\u8bed\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u901a\u8fc7\u6bcd\u8bed\u8005\u8bc4\u4f30\u6d41\u7545\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u57fa\u4e8e\u7b56\u7565\u7684\u8bad\u7ec3\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u4f18\u4e8e\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\uff0c\u4e14\u4e0d\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u504f\u597d\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u76ee\u6807\u8bed\u8a00\u7684\u6307\u4ee4\u6570\u636e\uff0c\u57fa\u4e8e\u7b56\u7565\u7684\u8bad\u7ec3\u662f\u5173\u952e\u4f18\u52bf\u3002"}}
{"id": "2512.08248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08248", "abs": "https://arxiv.org/abs/2512.08248", "authors": ["Ahan Basu", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Learning Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks using Physics-Informed Neural Networks", "comment": null, "summary": "This paper presents a Spatiotemporal Tube (STT)-based control framework for general control-affine MIMO nonlinear pure-feedback systems with unknown dynamics to satisfy prescribed time reach-avoid-stay tasks under external disturbances. The STT is defined as a time-varying ball, whose center and radius are jointly approximated by a Physics-Informed Neural Network (PINN). The constraints governing the STT are first formulated as loss functions of the PINN, and a training algorithm is proposed to minimize the overall violation. The PINN being trained on certain collocation points, we propose a Lipschitz-based validity condition to formally verify that the learned PINN satisfies the conditions over the continuous time horizon. Building on the learned STT representation, an approximation-free closed-form controller is defined to guarantee satisfaction of the T-RAS specification. Finally, the effectiveness and scalability of the framework are validated through two case studies involving a mobile robot and an aerial vehicle navigating through cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u7a7a\u7ba1\u9053\u7684\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u672a\u77e5\u52a8\u6001\u7684MIMO\u975e\u7ebf\u6027\u7eaf\u53cd\u9988\u7cfb\u7edf\uff0c\u5728\u5916\u90e8\u6270\u52a8\u4e0b\u5b9e\u73b0\u89c4\u5b9a\u65f6\u95f4\u7684\u5230\u8fbe-\u907f\u969c-\u505c\u7559\u4efb\u52a1", "motivation": "\u9488\u5bf9\u5177\u6709\u672a\u77e5\u52a8\u6001\u3001\u5916\u90e8\u6270\u52a8\u548c\u590d\u6742\u4efb\u52a1\u89c4\u683c\uff08\u5230\u8fbe-\u907f\u969c-\u505c\u7559\uff09\u7684\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u9c81\u68d2\u63a7\u5236\u6846\u67b6", "method": "\u5b9a\u4e49\u65f6\u7a7a\u7ba1\u9053\u4f5c\u4e3a\u65f6\u53d8\u7403\u4f53\uff0c\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u8054\u5408\u903c\u8fd1\u5176\u4e2d\u5fc3\u548c\u534a\u5f84\uff0c\u901a\u8fc7\u635f\u5931\u51fd\u6570\u7ea6\u675f\u8bad\u7ec3\uff0c\u63d0\u51fa\u57fa\u4e8eLipschitz\u7684\u6709\u6548\u6027\u9a8c\u8bc1\u6761\u4ef6\uff0c\u5e76\u8bbe\u8ba1\u65e0\u8fd1\u4f3c\u7684\u95ed\u5f0f\u63a7\u5236\u5668", "result": "\u6846\u67b6\u5728\u79fb\u52a8\u673a\u5668\u4eba\u548c\u98de\u884c\u5668\u7684\u907f\u969c\u5bfc\u822a\u6848\u4f8b\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u4fdd\u8bc1T-RAS\u4efb\u52a1\u89c4\u683c\u7684\u6ee1\u8db3", "conclusion": "\u63d0\u51fa\u7684STT\u63a7\u5236\u6846\u67b6\u4e3a\u672a\u77e5\u52a8\u6001\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u8bc1\u590d\u6742\u65f6\u7a7a\u4efb\u52a1\u89c4\u683c\u7684\u6ee1\u8db3\uff0c\u5e76\u901a\u8fc7\u5f62\u5f0f\u9a8c\u8bc1\u786e\u4fdd\u8fde\u7eed\u65f6\u95f4\u8303\u56f4\u5185\u7684\u6709\u6548\u6027"}}
{"id": "2512.08786", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08786", "abs": "https://arxiv.org/abs/2512.08786", "authors": ["Mahmoud Srewa", "Tianyu Zhao", "Salma Elmalaki"], "title": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs", "comment": null, "summary": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u805a\u5408\u7b56\u7565\u6765\u5e73\u8861\u5bf9\u9f50\u8d28\u91cf\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u6807\u51c6\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u96be\u4ee5\u5145\u5206\u4ee3\u8868\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\uff0c\u9700\u8981\u89e3\u51b3LLM\u5bf9\u9f50\u4e2d\u7684\u516c\u5e73\u6027\u4e0e\u8d28\u91cf\u6743\u8861\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\u6846\u67b6\uff0c\u5404\u7ec4\u672c\u5730\u8bc4\u4f30\u751f\u6210\u7ed3\u679c\u5e76\u4ea7\u751f\u5956\u52b1\u4fe1\u53f7\uff0c\u670d\u52a1\u5668\u805a\u5408\u7ec4\u7ea7\u5956\u52b1\u800c\u4e0d\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u3002\u8bc4\u4f30\u6807\u51c6\u805a\u5408\u6280\u672f\uff08\u6700\u5c0f\u3001\u6700\u5927\u3001\u5e73\u5747\u503c\uff09\u5e76\u5f15\u5165\u57fa\u4e8e\u5386\u53f2\u5bf9\u9f50\u6027\u80fd\u52a8\u6001\u8c03\u6574\u504f\u597d\u6743\u91cd\u7684\u81ea\u9002\u5e94\u65b9\u6848\u3002", "result": "\u5728\u57fa\u4e8ePPO\u7684RLHF\u7ba1\u9053\u4e0a\u8fdb\u884c\u7684\u95ee\u7b54\u4efb\u52a1\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u5bf9\u9f50\u5206\u6570\u7684\u540c\u65f6\uff0c\u59cb\u7ec8\u5b9e\u73b0\u66f4\u4f18\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u8bc4\u4f30LLM\u5728\u591a\u6837\u5316\u4eba\u7fa4\u4e2d\u884c\u4e3a\u7684\u7a33\u5065\u65b9\u6cd5\uff0c\u5e76\u4e3a\u5f00\u53d1\u771f\u6b63\u591a\u5143\u5316\u548c\u516c\u5e73\u5bf9\u9f50\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08271", "categories": ["cs.RO", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.08271", "abs": "https://arxiv.org/abs/2512.08271", "authors": ["Srijan Dokania", "Dharini Raghavan"], "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation", "comment": "Published and Presented at 3rd Workshop on Human-Centric Multilateral Teleoperation in ICRA 2025", "summary": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.", "AI": {"tldr": "Zero-Splat TeleAssist\uff1a\u96f6\u6837\u672c\u4f20\u611f\u5668\u878d\u5408\u7ba1\u9053\uff0c\u5c06\u666e\u901aCCTV\u89c6\u9891\u6d41\u8f6c\u6362\u4e3a\u5171\u4eab6\u81ea\u7531\u5ea6\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u8fb9\u9065\u64cd\u4f5c", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u4e2d\u9700\u8981\u5b9e\u65f6\u5168\u5c40\u4f4d\u7f6e\u548c\u59ff\u6001\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f7f\u7528\u6807\u8bb0\u7269\u6216\u6df1\u5ea6\u4f20\u611f\u5668\uff0c\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\u548c\u590d\u6742\u5ea6", "method": "\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u52a0\u6743PCA\u59ff\u6001\u63d0\u53d6\u548c3D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u4ece\u666e\u901a\u76d1\u63a7\u89c6\u9891\u6d41\u6784\u5efa6\u81ea\u7531\u5ea6\u4e16\u754c\u6a21\u578b", "result": "\u4e3a\u6bcf\u4e2a\u64cd\u4f5c\u5458\u63d0\u4f9b\u591a\u4e2a\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u5168\u5c40\u4f4d\u7f6e\u548c\u59ff\u6001\u4fe1\u606f\uff0c\u65e0\u9700\u6807\u8bb0\u7269\u6216\u6df1\u5ea6\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u4ea4\u4e92\u4e3a\u4e2d\u5fc3\u7684\u9065\u64cd\u4f5c", "conclusion": "Zero-Splat TeleAssist\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u96f6\u6837\u672c\u7684\u4f20\u611f\u5668\u878d\u5408\u65b9\u6848\uff0c\u5c06\u666e\u901a\u76d1\u63a7\u7cfb\u7edf\u5347\u7ea7\u4e3a\u652f\u6301\u591a\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u76846\u81ea\u7531\u5ea6\u4e16\u754c\u6a21\u578b\u7cfb\u7edf"}}
{"id": "2512.08814", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08814", "abs": "https://arxiv.org/abs/2512.08814", "authors": ["Yifan Lyu", "Liang Zhang"], "title": "Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts", "comment": null, "summary": "Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a \"posts -> user vector -> labels\" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).", "AI": {"tldr": "ROME\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u5fc3\u7406\u95ee\u5377\u56de\u7b54\uff0c\u5c06\u5fc3\u7406\u5b66\u77e5\u8bc6\u6ce8\u5165\u4eba\u683c\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u7a00\u7f3a\u548c\u8bed\u4e49\u6620\u5c04\u4e0d\u660e\u786e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u683c\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u683c\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\"\u5e16\u5b50->\u7528\u6237\u5411\u91cf->\u6807\u7b7e\"\u7684\u5efa\u6a21\u8303\u5f0f\uff0c\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u4e86\u6587\u672c\u7f16\u7801\u80fd\u529b\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u6807\u7b7e\u7a00\u7f3a\u6027\u4ee5\u53ca\u7528\u6237\u8bed\u8a00\u4e0e\u62bd\u8c61\u5fc3\u7406\u6982\u5ff5\u4e4b\u95f4\u8bed\u4e49\u6620\u5c04\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "ROME\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89d2\u8272\u626e\u6f14\u80fd\u529b\uff0c\u6a21\u62df\u7528\u6237\u5bf9\u6807\u51c6\u5316\u5fc3\u7406\u95ee\u5377\u7684\u56de\u7b54\uff0c\u5c06\u81ea\u7531\u5f62\u5f0f\u7684\u7528\u6237\u5e16\u5b50\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u95ee\u5377\u7684\u8bc1\u636e\u3002\u91c7\u7528\u95ee\u9898\u6761\u4ef6\u5316\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u8054\u5408\u5904\u7406\u5e16\u5b50\u548c\u95ee\u9898\u8868\u793a\uff0c\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5c06\u95ee\u5377\u56de\u7b54\u4f5c\u4e3a\u4eba\u683c\u68c0\u6d4b\u7684\u8f85\u52a9\u4efb\u52a1\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cROME\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Kaggle\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8615.41%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ROME\u901a\u8fc7\u663e\u5f0f\u6ce8\u5165\u5fc3\u7406\u5b66\u77e5\u8bc6\uff0c\u5c06\u95ee\u5377\u56de\u7b54\u4f5c\u4e3a\u4e2d\u95f4\u76d1\u7763\u4fe1\u53f7\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u8bed\u4e49\u63a8\u7406\u94fe\u6765\u6307\u5bfc\u6587\u672c\u5230\u4eba\u683c\u7684\u6620\u5c04\u5b66\u4e60\uff0c\u4e3a\u4eba\u683c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08280", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.08280", "abs": "https://arxiv.org/abs/2512.08280", "authors": ["Haldun Balim", "Na Li", "Yilun Du"], "title": "Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making", "comment": null, "summary": "Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.", "AI": {"tldr": "MPDiffuser\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u79bb\u7ebf\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212\u5668\u3001\u52a8\u529b\u5b66\u6a21\u578b\u548c\u6392\u5e8f\u5668\u7684\u7ec4\u5408\u8bbe\u8ba1\uff0c\u751f\u6210\u65e2\u7b26\u5408\u4efb\u52a1\u76ee\u6807\u53c8\u52a8\u529b\u5b66\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u79bb\u7ebf\u51b3\u7b56\u4e2d\u5e38\u4ea7\u751f\u52a8\u529b\u5b66\u4e0d\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4fdd\u8bc1\u4efb\u52a1\u5bf9\u9f50\u548c\u52a8\u529b\u5b66\u53ef\u884c\u6027\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u5f0f\u6a21\u578b\uff1a1\uff09\u89c4\u5212\u5668\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\u5bf9\u9f50\u8f68\u8ff9\uff1b2\uff09\u52a8\u529b\u5b66\u6a21\u578b\u786e\u4fdd\u7cfb\u7edf\u52a8\u529b\u5b66\u4e00\u81f4\u6027\uff1b3\uff09\u6392\u5e8f\u5668\u9009\u62e9\u6700\u4f18\u884c\u4e3a\u3002\u4f7f\u7528\u4ea4\u66ff\u6269\u6563\u91c7\u6837\u65b9\u6848\uff0c\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u9010\u6b65\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u5728D4RL\u548cDSRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u80fd\u9002\u5e94\u65b0\u52a8\u529b\u5b66\uff0c\u5e76\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "MPDiffuser\u901a\u8fc7\u7ec4\u5408\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u51b3\u7b56\u4e2d\u8f68\u8ff9\u53ef\u884c\u6027\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u53ef\u6269\u5c55\u5230\u9ad8\u7ef4\u89c6\u89c9\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2512.08819", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08819", "abs": "https://arxiv.org/abs/2512.08819", "authors": ["Ferdinand Kapl", "Emmanouil Angelis", "Tobias H\u00f6ppe", "Kaitlin Maile", "Johannes von Oswald", "Nino Scherrer", "Stefan Bauer"], "title": "Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis", "comment": null, "summary": "Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csord\u00e1s et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.", "AI": {"tldr": "\u901a\u8fc7\u6e10\u8fdb\u589e\u52a0Transformer\u6df1\u5ea6\u8bad\u7ec3\u53ef\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u672c\u6587\u63ed\u793a\u4e86\u5176\u673a\u5236\uff1a\u7f13\u89e3\"\u6df1\u5ea6\u8bc5\u5492\"\uff0c\u6539\u5584\u6b8b\u5dee\u6d41\u7ed3\u6784\uff0c\u5f62\u6210\u53ef\u7f6e\u6362\u8ba1\u7b97\u5757", "motivation": "\u867d\u7136MIDAS\u5df2\u8bc1\u660e\u6e10\u8fdb\u589e\u52a0Transformer\u6df1\u5ea6\u80fd\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5176\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u7406\u89e3\u8fd9\u79cd\u6027\u80fd\u63d0\u5347\u80cc\u540e\u7684\u8ba1\u7b97\u673a\u5236\uff0c\u7279\u522b\u662f\u4e0e\"\u6df1\u5ea6\u8bc5\u5492\"\u73b0\u8c61\u7684\u5173\u7cfb", "method": "\u91c7\u7528\u6df1\u5ea6\u5206\u6790\u6280\u672f\uff0c\u7814\u7a76\u6e10\u8fdb\u4e2d\u95f4\u5806\u53e0\u751f\u957f\u65b9\u5f0f\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6df1\u5ea6\u5229\u7528\u6548\u7387\u3001\u6539\u53d8\u6b8b\u5dee\u6d41\u7ed3\u6784\uff0c\u5e76\u4fc3\u8fdb\u53ef\u7f6e\u6362\u8ba1\u7b97\u5757\u7684\u5f62\u6210\u3002\u540c\u65f6\u63d0\u51faMIDAS\u7684\u8f7b\u91cf\u7ea7\u6539\u8fdb\u65b9\u6848", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6e10\u8fdb\u6df1\u5ea6\u751f\u957f\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u6a21\u578b\u6df1\u5ea6\uff0c\u6539\u53d8\u6b8b\u5dee\u6d41\u7ed3\u6784\uff0c\u4fc3\u8fdb\u53ef\u7f6e\u6362\u8ba1\u7b97\u5757\u7684\u5f62\u6210\u3002\u63d0\u51fa\u7684\u6539\u8fdb\u65b9\u6848\u5728\u4e0b\u6e38\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd", "conclusion": "\u6e10\u8fdb\u589e\u52a0\u6a21\u578b\u6df1\u5ea6\u80fd\u5f62\u6210\u72ec\u7279\u7684\u8ba1\u7b97\u7535\u8def\uff0c\u514b\u670d\u6807\u51c6\u975e\u751f\u957f\u6a21\u578b\u4e2d\u6df1\u5ea6\u5229\u7528\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e3aTransformer\u67b6\u6784\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u5236\u7406\u89e3"}}
{"id": "2512.08333", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08333", "abs": "https://arxiv.org/abs/2512.08333", "authors": ["Yajat Yadav", "Zhiyuan Zhou", "Andrew Wagenmaker", "Karl Pertsch", "Sergey Levine"], "title": "Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging", "comment": null, "summary": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u578b\u6743\u91cd\u63d2\u503c\u65b9\u6cd5\uff0c\u5728\u5fae\u8c03\u65b0\u4efb\u52a1\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u5355\u4e00\u7b56\u7565\u65e2\u80fd\u5b66\u4e60\u65b0\u6280\u80fd\u53c8\u4e0d\u4e22\u5931\u539f\u6709\u80fd\u529b\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5728\u5fae\u8c03\u65b0\u4efb\u52a1\u65f6\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u7279\u5b9a\u6f14\u793a\uff0c\u65e2\u5931\u53bb\u539f\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u53c8\u65e0\u6cd5\u5728\u65b0\u4efb\u52a1\u5185\u90e8\u6cdb\u5316\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u5fae\u8c03\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7b80\u5355\u6709\u6548\u7684\u7b56\u7565\uff1a\u5c06\u5fae\u8c03\u540e\u7684\u6a21\u578b\u6743\u91cd\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u8fdb\u884c\u63d2\u503c\u5408\u5e76\uff0c\u4ea7\u751f\u5355\u4e00\u6a21\u578b\u7ee7\u627f\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u5e76\u7a33\u5065\u5b66\u4e60\u65b0\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u65b0\u4efb\u52a1\u7684\u5206\u5e03\u5916\u53d8\u5316\u4e0a\u4f18\u4e8e\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6a21\u578b\uff0c\u5e76\u80fd\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u65b0\u6280\u80fd\u800c\u4e0d\u727a\u7272\u539f\u6709\u80fd\u529b\u3002", "conclusion": "\u6743\u91cd\u63d2\u503c\u5408\u5e76\u662f\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u8ba9\u5355\u4e00\u7b56\u7565\u65e2\u5b66\u4e60\u65b0\u4efb\u52a1\u6cdb\u5316\uff0c\u53c8\u4fdd\u7559\u9884\u8bad\u7ec3\u7684\u5e7f\u6cdb\u80fd\u529b\uff0c\u652f\u6301\u7ec8\u8eab\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2512.08892", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08892", "abs": "https://arxiv.org/abs/2512.08892", "authors": ["Guangzhi Xiong", "Zhenghao He", "Bohan Liu", "Sanchit Sinha", "Aidong Zhang"], "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.", "AI": {"tldr": "RAGLens\uff1a\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u548c\u7279\u5f81\u9009\u62e9\u7684\u8f7b\u91cf\u7ea7RAG\u5e7b\u89c9\u68c0\u6d4b\u5668\uff0c\u5229\u7528LLM\u5185\u90e8\u8868\u793a\u51c6\u786e\u8bc6\u522b\u4e0d\u5fe0\u5b9e\u8f93\u51fa\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u5916\u90e8LLM\u67e5\u8be2", "motivation": "RAG\u867d\u7136\u901a\u8fc7\u68c0\u7d22\u8bc1\u636e\u63d0\u9ad8\u4e86LLM\u7684\u4e8b\u5b9e\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u5fe0\u5b9e\u6027\u5931\u8d25\u95ee\u9898\uff08\u751f\u6210\u5185\u5bb9\u4e0e\u6e90\u4fe1\u606f\u77db\u76fe\u6216\u8d85\u51fa\u8303\u56f4\uff09\u3002\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u68c0\u6d4b\u5668\uff0c\u8981\u4e48\u4f9d\u8d56\u5916\u90e8LLM\u5224\u65ad\u5bfc\u81f4\u9ad8\u63a8\u7406\u6210\u672c\uff0c\u800c\u57fa\u4e8eLLM\u5185\u90e8\u8868\u793a\u7684\u65b9\u6cd5\u51c6\u786e\u7387\u6709\u9650\u3002", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u89e3\u8026LLM\u5185\u90e8\u6fc0\u6d3b\uff0c\u8bc6\u522bRAG\u5e7b\u89c9\u89e6\u53d1\u7279\u5f81\uff1b\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u7279\u5f81\u9009\u62e9\u548c\u52a0\u6027\u7279\u5f81\u5efa\u6a21\u6784\u5efaRAGLens\u68c0\u6d4b\u5668\uff0c\u8f7b\u91cf\u7ea7\u5730\u5229\u7528LLM\u5185\u90e8\u8868\u793a\u6807\u8bb0\u4e0d\u5fe0\u5b9eRAG\u8f93\u51fa\u3002", "result": "RAGLens\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u4f9d\u636e\uff0c\u80fd\u591f\u6709\u6548\u8fdb\u884c\u4e8b\u540e\u7f13\u89e3\uff1b\u7814\u7a76\u8fd8\u63ed\u793a\u4e86LLM\u4e2d\u5e7b\u89c9\u76f8\u5173\u4fe1\u53f7\u7684\u5206\u5e03\u65b0\u89c1\u89e3\u3002", "conclusion": "RAGLens\u901a\u8fc7\u7ed3\u5408\u7a00\u758f\u81ea\u7f16\u7801\u5668\u548c\u7cfb\u7edf\u5316\u7279\u5f81\u9009\u62e9\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684RAG\u5e7b\u89c9\u68c0\u6d4b\uff0c\u4e3a\u7406\u89e3\u548c\u7f13\u89e3RAG\u5fe0\u5b9e\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.08405", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08405", "abs": "https://arxiv.org/abs/2512.08405", "authors": ["Fan Zhang", "Michael Gienger"], "title": "Learning Robot Manipulation from Audio World Models", "comment": null, "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u751f\u6210\u5f0f\u6f5c\u5728\u6d41\u5339\u914d\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u97f3\u9891\u89c2\u6d4b\uff0c\u4f7f\u673a\u5668\u4eba\u7b56\u7565\u80fd\u591f\u63a8\u7406\u957f\u671f\u540e\u679c\uff0c\u5728\u9700\u8981\u611f\u77e5\u771f\u5b9e\u4e16\u754c\u97f3\u9891\u6216\u97f3\u4e50\u4fe1\u53f7\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u65e0\u524d\u77bb\u7684\u65b9\u6cd5\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\u9700\u8981\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4f8b\u5982\u704c\u6c34\u4efb\u52a1\u4e2d\u4ec5\u51ed\u89c6\u89c9\u4fe1\u606f\u53ef\u80fd\u6a21\u7cca\u6216\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u57fa\u4e8e\u97f3\u9891\u7684\u65f6\u95f4\u6f14\u5316\u8fdb\u884c\u63a8\u7406\uff0c\u8003\u8651\u5176\u7269\u7406\u7279\u6027\u548c\u97f3\u9ad8\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5f0f\u6f5c\u5728\u6d41\u5339\u914d\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u97f3\u9891\u89c2\u6d4b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6574\u5408\u5230\u673a\u5668\u4eba\u7b56\u7565\u4e2d\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u63a8\u7406\u957f\u671f\u540e\u679c\u3002", "result": "\u5728\u4e24\u4e2a\u9700\u8981\u611f\u77e5\u771f\u5b9e\u4e16\u754c\u97f3\u9891\u6216\u97f3\u4e50\u4fe1\u53f7\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u7cfb\u7edf\u76f8\u6bd4\u65e0\u524d\u77bb\u7684\u65b9\u6cd5\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u6210\u529f\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u5b66\u4e60\u4e0d\u4ec5\u4f9d\u8d56\u591a\u6a21\u6001\u8f93\u5165\uff0c\u66f4\u5173\u952e\u7684\u662f\u51c6\u786e\u9884\u6d4b\u4f53\u73b0\u5185\u5728\u8282\u594f\u6a21\u5f0f\u7684\u672a\u6765\u97f3\u9891\u72b6\u6001\u3002"}}
{"id": "2512.08476", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08476", "abs": "https://arxiv.org/abs/2512.08476", "authors": ["Po-An Shih", "Shao-Hua Wang", "Yung-Che Li", "Chia-Heng Tu", "Chih-Han Chang"], "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems", "comment": null, "summary": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u786c\u4ef6/\u8f6f\u4ef6\u914d\u7f6e\u4f18\u5316\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u81ea\u52a8\u5206\u6790\u6267\u884c\u8f93\u51fa\uff0c\u76f8\u6bd4\u9057\u4f20\u7b97\u6cd5\u627e\u5230\u66f4\u591a\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u3002", "motivation": "\u4f20\u7edf\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6a21\u6001\u6267\u884c\u8f93\u51fa\u548c\u590d\u6742\u6027\u80fd\u6743\u8861\uff0c\u9700\u8981\u4eba\u5de5\u53c2\u4e0e\u8bc4\u4f30\u6b63\u786e\u6027\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u81ea\u52a8\u5316\u7a0b\u5ea6\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u96c6\u6210\u591a\u6a21\u6001\u63a8\u7406\u30013D\u4eff\u771f\u548c\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u4e13\u7528LLM\u4ee3\u7406\u5904\u7406\u7528\u6237\u8f93\u5165\u89e3\u91ca\u3001\u8bbe\u8ba1\u70b9\u751f\u6210\u3001\u6267\u884c\u7f16\u6392\u4ee5\u53ca\u89c6\u89c9/\u6587\u672c\u8f93\u51fa\u5206\u6790\u3002", "result": "\u5728\u673a\u5668\u4eba\u51fa\u79df\u8f66\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u76f8\u6bd4\u9057\u4f20\u7b97\u6cd5\u57fa\u7ebf\uff0c\u5728\u76f8\u540c\u63a2\u7d22\u9884\u7b97\u4e0b\u8bc6\u522b\u51fa\u66f4\u591a\u5e15\u7d2f\u6258\u6700\u4f18\u3001\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bfc\u822a\u65f6\u95f4\u51cf\u5c11\uff0c\u8bc1\u660e\u4e86LLM\u65b9\u6cd5\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bbe\u8ba1\u81ea\u52a8\u5316\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9e\u73b0\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u7684\u81ea\u52a8\u5316\u548c\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2512.08481", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08481", "abs": "https://arxiv.org/abs/2512.08481", "authors": ["Yixiang Lin", "Tiancheng Yang", "Jonathan Eden", "Ying Tan"], "title": "Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception", "comment": "9 pages, 6 figures", "summary": "Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\\% to 90\\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a \"trade-off\" group that modulated their physical responses according to disturbance likelihood, and an \"always-compensate\" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers.", "AI": {"tldr": "\u4eba\u7c7b\u5728\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u4e2d\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u53cd\u5e94\u5b58\u5728\u4e2a\u4f53\u5dee\u5f02\uff0c\u53ef\u5206\u4e3a\"\u6743\u8861\u578b\"\u548c\"\u603b\u662f\u8865\u507f\u578b\"\u4e24\u79cd\u884c\u4e3a\u6a21\u5f0f\uff0c\u9700\u8981\u7d2f\u79ef\u524d\u666f\u7406\u8bba\u7b49\u53ef\u89e3\u91ca\u6a21\u578b\u6765\u6539\u8fdb\u673a\u5668\u4eba\u63a7\u5236\u5668\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edfpHRI\u63a7\u5236\u6846\u67b6\u57fa\u4e8e\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff0c\u5047\u8bbe\u4eba\u7c7b\u884c\u4e3a\u6700\u5c0f\u5316\u6210\u672c\u51fd\u6570\uff0c\u4f46\u4eba\u7c7b\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u884c\u4e3a\u5e38\u504f\u79bb\u6700\u4f18\u6a21\u5f0f\u3002\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u4eba\u7c7b\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u884c\u4e3a\uff0c\u4ee5\u8bbe\u8ba1\u66f4\u5b89\u5168\u6709\u6548\u7684\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u3002", "method": "\u5b9e\u65bd\u7269\u7406\u8026\u5408\u76ee\u6807\u5230\u8fbe\u4efb\u52a1\uff0c\u673a\u5668\u4eba\u4ee5\u7cfb\u7edf\u53d8\u5316\u6982\u7387\uff0810%\u523090%\uff09\u63d0\u4f9b\u534f\u52a9\u6216\u5e72\u6270\u3002\u5206\u6790\u53c2\u4e0e\u8005\u7684\u529b\u8f93\u5165\u548c\u51b3\u7b56\u7b56\u7565\uff0c\u8bc6\u522b\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u4e24\u79cd\u660e\u663e\u7684\u884c\u4e3a\u96c6\u7fa4\uff1a\"\u6743\u8861\u578b\"\u7fa4\u4f53\u6839\u636e\u5e72\u6270\u53ef\u80fd\u6027\u8c03\u6574\u7269\u7406\u53cd\u5e94\uff0c\"\u603b\u662f\u8865\u507f\u578b\"\u7fa4\u4f53\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u98ce\u9669\u89c4\u907f\uff0c\u4e0d\u8003\u8651\u6982\u7387\u3002\u4eba\u7c7b\u51b3\u7b56\u9ad8\u5ea6\u4e2a\u6027\u5316\uff0c\u6982\u7387\u611f\u77e5\u53ef\u80fd\u4e0e\u5b9e\u9645\u503c\u4e0d\u540c\u3002", "conclusion": "\u9700\u8981\u7d2f\u79ef\u524d\u666f\u7406\u8bba\u7b49\u66f4\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\u6a21\u578b\u6765\u51c6\u786e\u6355\u6349\u8fd9\u4e9b\u884c\u4e3a\uff0c\u4e3a\u672a\u6765\u81ea\u9002\u5e94\u673a\u5668\u4eba\u63a7\u5236\u5668\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u4eba\u7c7b\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4e2a\u4f53\u5316\u51b3\u7b56\u6a21\u5f0f\u3002"}}
{"id": "2512.08518", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.08518", "abs": "https://arxiv.org/abs/2512.08518", "authors": ["Nadezhda Kushina", "Ko Watanabe", "Aarthi Kannan", "Ashita Ashok", "Andreas Dengel", "Karsten Berns"], "title": "SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking", "comment": null, "summary": "Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot \"Ameca\" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u773c\u52a8\u8ffd\u8e2a\u548c\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u4eba\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\u7684\u8212\u9002\u5ea6\uff0c\u53d1\u73b0\u51b3\u7b56\u6811\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u6700\u5c0f\u77b3\u5b54\u76f4\u5f84\u662f\u5173\u952e\u9884\u6d4b\u56e0\u5b50\uff0c\u8868\u660e\u4eba\u673a\u4e92\u52a8\u7684\u8212\u9002\u5ea6\u9608\u503c\u4e0e\u4eba\u9645\u4e92\u52a8\u4e0d\u540c\u3002", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u9700\u8981\u9002\u5e94\u4eba\u7c7b\u7684\u8fd1\u4f53\u7a7a\u95f4\u89c4\u8303\u4ee5\u786e\u4fdd\u7528\u6237\u8212\u9002\u5ea6\u548c\u53c2\u4e0e\u5ea6\u3002\u867d\u7136\u5148\u524d\u7814\u7a76\u8868\u660e\u773c\u52a8\u8ffd\u8e2a\u7279\u5f81\u80fd\u53ef\u9760\u4f30\u8ba1\u4eba\u9645\u4e92\u52a8\u4e2d\u7684\u8212\u9002\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4eba\u5f62\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u673a\u5668\u4eba\"Ameca\"\u5728\u56db\u4e2a\u5b9e\u9a8c\u63a7\u5236\u8ddd\u79bb\uff080.5\u7c73\u81f32.0\u7c73\uff09\u4e0b\uff0c\u901a\u8fc7\u79fb\u52a8\u773c\u52a8\u8ffd\u8e2a\u548c\u4e3b\u89c2\u62a5\u544a\uff08N=19\uff09\u8bc4\u4f30\u7528\u6237\u8212\u9002\u5ea6\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e\u6ce8\u89c6\u7279\u5f81\u4f30\u8ba1\u8212\u9002\u5ea6\u3002", "result": "\u4e0e\u5148\u524d\u4eba\u9645\u4e92\u52a8\u7814\u7a76\u4e2dTransformer\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u4e0d\u540c\uff0c\u51b3\u7b56\u6811\u5206\u7c7b\u5668\u53d6\u5f97\u4e86\u6700\u9ad8\u6027\u80fd\uff08F1\u5206\u6570=0.73\uff09\uff0c\u6700\u5c0f\u77b3\u5b54\u76f4\u5f84\u88ab\u786e\u5b9a\u4e3a\u6700\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u673a\u4e92\u52a8\u4e2d\u7684\u751f\u7406\u8212\u9002\u5ea6\u9608\u503c\u4e0e\u4eba\u9645\u4e92\u52a8\u52a8\u6001\u4e0d\u540c\uff0c\u53ef\u4ee5\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u903b\u8f91\u6a21\u578b\u6709\u6548\u5efa\u6a21\u3002"}}
{"id": "2512.08541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08541", "abs": "https://arxiv.org/abs/2512.08541", "authors": ["Nils Gehrke", "David Brecht", "Dominik Kulmer", "Dheer Patel", "Frank Diermeyer"], "title": "vEDGAR -- Can CARLA Do HiL?", "comment": null, "summary": "Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR", "AI": {"tldr": "\u5f00\u53d1\u4e86vEDGAR\u6846\u67b6\uff0c\u5c06CARLA\u6a21\u62df\u5668\u6269\u5c55\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86CARLA\u5728\u5b9e\u65f6\u5168\u4f20\u611f\u5668\u5806\u6808\u4eff\u771f\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "CARLA\u7b49\u5f00\u6e90\u6a21\u62df\u5668\u5728\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5b8c\u6574\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u5806\u6808\u7684\u5b9e\u65f6\u4eff\u771f\u80fd\u529b\uff0c\u65e0\u6cd5\u652f\u6301\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u3002\u8fd9\u9650\u5236\u4e86CARLA\u5728\u6574\u4e2a\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u5e94\u7528\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u9700\u6c42\u5e76\u6307\u5b9a\u4eff\u771f\u67b6\u6784\uff0c\u7136\u540e\u5b9e\u73b0vEDGAR\u8f6f\u4ef6\u6846\u67b6\uff0c\u5c06CARLA\u6a21\u62df\u5668\u6269\u5c55\u4e3a\u652f\u6301\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u7684\u5e73\u53f0\uff0c\u5305\u62ec\u5bf9\u4e13\u7528\u786c\u4ef6\u548c\u5b9e\u65f6\u6027\u80fd\u7684\u652f\u6301\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u5f00\u6e90\u7684vEDGAR\u6846\u67b6\u548c\u4fee\u6539\u7248CARLA\u5206\u652f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e13\u7528\u786c\u4ef6\u7684\u5b9e\u65f6\u4eff\u771f\u6d4b\u8bd5\uff0c\u80fd\u591f\u8bc6\u522b\u7cfb\u7edf\u6781\u9650\uff0c\u586b\u8865\u4e86CARLA\u5728\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u65b9\u9762\u7684\u7a7a\u767d\u3002", "conclusion": "vEDGAR\u6846\u67b6\u8bc1\u660e\u4e86CARLA\u53ef\u4ee5\u6269\u5c55\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\uff0c\u4f7f\u5176\u6210\u4e3a\u8d2f\u7a7f\u6574\u4e2a\u5f00\u53d1\u6d41\u7a0b\u7684\u4e00\u81f4\u8bc4\u4f30\u5de5\u5177\uff0c\u5927\u5927\u63d0\u5347\u4e86\u5f00\u6e90\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u6548\u7387\u3002"}}
{"id": "2512.08548", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08548", "abs": "https://arxiv.org/abs/2512.08548", "authors": ["Yuchi Zhang", "Churui Sun", "Shiqi Liang", "Diyuan Liu", "Chao Ji", "Wei-Nan Zhang", "Ting Liu"], "title": "Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations", "comment": null, "summary": "Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u57fa\u7840\u7684\u8bed\u8a00\u8868\u793a\u6765\u89c4\u8303\u5316\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u901a\u8fc7\u5f3a\u8c03\u65b9\u5411\u6027\u800c\u975e\u6570\u503c\u5c3a\u5ea6\u6765\u7f13\u89e3\u5206\u5e03\u504f\u79fb\uff0c\u63d0\u9ad8\u9884\u8bad\u7ec3\u8868\u793a\u7684\u53ef\u6cdb\u5316\u6027", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u67b6\u6784\u9762\u4e34\u4e25\u91cd\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4efb\u52a1\u95f4\u52a8\u4f5c\u547d\u4ee4\u7684\u6570\u503c\u53d8\u5316\uff0c\u963b\u788d\u4e86\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u6709\u6548\u8fc1\u79fb", "method": "\u63d0\u51fa\u8bed\u4e49\u57fa\u7840\u7684\u8bed\u8a00\u8868\u793a\u6765\u89c4\u8303\u5316\u52a8\u4f5c\uff0c\u5f3a\u8c03\u65b9\u5411\u6027\u800c\u975e\u6570\u503c\u5c3a\u5ea6\uff0c\u8fd9\u79cd\u8fd0\u52a8\u8868\u793a\u5ffd\u7565\u6570\u503c\u5c3a\u5ea6\u6548\u5e94\uff0c\u4e13\u6ce8\u4e8e\u65b9\u5411\u6027\uff0c\u7f29\u5c0f\u52a8\u4f5c\u6807\u8bb0\u4e0e\u6807\u51c6\u8bcd\u6c47\u6807\u8bb0\u7684\u7279\u5f81\u8ddd\u79bb", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6cdb\u5316\u6027\u80fd\u548c\u53ef\u8fc1\u79fb\u6027", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u57fa\u7840\u7684\u8bed\u8a00\u8868\u793a\u89c4\u8303\u5316\u52a8\u4f5c\uff0c\u6709\u6548\u7f13\u89e3\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u9884\u8bad\u7ec3\u8868\u793a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4fc3\u8fdb\u8de8\u5e73\u53f0\u548c\u4efb\u52a1\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u77e5\u8bc6\u8fc1\u79fb"}}
{"id": "2512.08574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08574", "abs": "https://arxiv.org/abs/2512.08574", "authors": ["Vit Kratky", "Robert Penicka", "Parakh M. Gupta", "Ondrej Prochazka", "Martin Saska"], "title": "RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight", "comment": "8 pages, 8 figures", "summary": "This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials.", "AI": {"tldr": "\u57fa\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u548c\u65f6\u95f4\u76f8\u5173\u4e92\u60e0\u901f\u5ea6\u7ea6\u675f\uff08RVCs\uff09\u7684\u65e0\u4eba\u673a\u4e92\u907f\u78b0\u649e\u65b9\u6cd5\uff0c\u65e0\u9700\u8fc7\u591a\u901a\u4fe1\uff0c\u8fd0\u884c\u9891\u7387\u8fbe100Hz\uff0c\u572810\u67b6\u65e0\u4eba\u673a\u300125m/s\u901f\u5ea6\u573a\u666f\u4e2d\u5b9e\u73b0\u65e0\u78b0\u649e\u5bfc\u822a\uff0c\u98de\u884c\u65f6\u95f4\u51cf\u5c1131%\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u4e92\u907f\u78b0\u649e\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u901a\u4fe1\uff0c\u800c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4ec5\u4f9d\u8d56\u53ef\u89c2\u6d4b\u4fe1\u606f\u3001\u65e0\u9700\u8fc7\u591a\u901a\u4fe1\u7684\u9ad8\u6548\u78b0\u649e\u907f\u514d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u654f\u6377\u65e0\u4eba\u673a\u98de\u884c\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u7ed3\u5408\u65f6\u95f4\u76f8\u5173\u4e92\u60e0\u901f\u5ea6\u7ea6\u675f\uff08RVCs\uff09\uff0c\u901a\u8fc7\u8ba1\u7b97\u9ad8\u6548\u7684RVCs\u7b97\u6cd5\uff0c\u5c06\u7ea6\u675f\u76f4\u63a5\u96c6\u6210\u5230\u63a7\u5236\u5668\u7ea7\u522b\u7684NMPC\u95ee\u9898\u4e2d\uff0c\u6574\u4e2a\u6d41\u6c34\u7ebf\u8fd0\u884c\u9891\u7387\u8fbe100Hz\u3002", "result": "\u5728\u6a21\u62df10\u67b6\u65e0\u4eba\u673a\u3001\u901f\u5ea6\u8fbe25m/s\u7684\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u4ee5\u53ca\u5728\u52a0\u901f\u5ea6\u8fbe30m/s\u00b2\u7684\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u6240\u6709\u8bd5\u9a8c\u5747\u5b9e\u73b0\u65e0\u78b0\u649e\u5bfc\u822a\u3002\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u98de\u884c\u65f6\u95f4\u51cf\u5c1131%\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eNMPC\u548cRVCs\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u65e0\u901a\u4fe1\u4f9d\u8d56\u7684\u65e0\u4eba\u673a\u4e92\u907f\u78b0\u649e\uff0c\u8fd0\u884c\u9891\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u654f\u6377\u98de\u884c\uff0c\u5728\u590d\u6742\u591a\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.08580", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08580", "abs": "https://arxiv.org/abs/2512.08580", "authors": ["Peijun Tang", "Shangjin Xie", "Binyan Sun", "Baifu Huang", "Kuncheng Luo", "Haotian Yang", "Weiqi Jin", "Jianan Wang"], "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning", "comment": "49 pages, 25 figures", "summary": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.", "AI": {"tldr": "Lumo-1\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u9884\u8bad\u7ec3\u5c06AI\u63a8\u7406\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u7269\u7406\u52a8\u4f5c\u76f8\u7ed3\u5408\uff0c\u5728\u5177\u8eab\u63a8\u7406\u548c\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u867d\u7136\u4e92\u8054\u7f51\u89c4\u6a21\u6570\u636e\u4f7fAI\u7cfb\u7edf\u5177\u5907\u4e86\u5e7f\u6cdb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5c06\u8fd9\u4e9b\u80fd\u529b\u843d\u5730\u5230\u7269\u7406\u52a8\u4f5c\u4e2d\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4eba\u7c7b\u884c\u52a8\u5177\u6709\u4e0a\u4e0b\u6587\u548c\u610f\u56fe\uff0c\u63a8\u7406\u5728\u5176\u4e2d\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u673a\u5668\u4eba\u7684\"\u601d\u7ef4\"\uff08\u63a8\u7406\uff09\u4e0e\"\u624b\"\uff08\u52a8\u4f5c\uff09\u7edf\u4e00\u8d77\u6765", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u9884\u8bad\u7ec3\u6d41\u7a0b\uff1a1\uff09\u5728\u7cbe\u9009\u7684\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u4e0a\u7ee7\u7eed\u9884\u8bad\u7ec3VLM\uff0c\u589e\u5f3a\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff1b2\uff09\u5728\u8de8\u5177\u8eab\u673a\u5668\u4eba\u6570\u636e\u548c\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff1b3\uff09\u5728Astribot S1\u53cc\u624b\u673a\u5668\u4eba\u6536\u96c6\u7684\u8f68\u8ff9\u4e0a\u8fdb\u884c\u5e26\u63a8\u7406\u8fc7\u7a0b\u7684\u52a8\u4f5c\u8bad\u7ec3\u3002\u6700\u540e\u6574\u5408\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u63a8\u7406-\u52a8\u4f5c\u4e00\u81f4\u6027", "result": "Lumo-1\u5728\u5177\u8eab\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8fd9\u662f\u901a\u7528\u673a\u5668\u4eba\u63a7\u5236\u7684\u5173\u952e\u7ec4\u4ef6\u3002\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u663e\u793a\uff0cLumo-1\u5728\u5e7f\u6cdb\u6311\u6218\u6027\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u5bf9\u65b0\u7269\u4f53\u548c\u73af\u5883\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u957f\u65f6\u57df\u4efb\u52a1\u548c\u9700\u8981\u7b56\u7565\u3001\u6982\u5ff5\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u4eba\u7c7b\u81ea\u7136\u6307\u4ee4\u54cd\u5e94\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "conclusion": "Lumo-1\u6210\u529f\u5730\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u6269\u5c55\u5230\u5177\u8eab\u63a8\u7406\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\"\u601d\u7ef4\"\u4e0e\"\u52a8\u4f5c\"\u7684\u7edf\u4e00\uff0c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.08630", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.08630", "abs": "https://arxiv.org/abs/2512.08630", "authors": ["Marta Manzoni", "Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems", "comment": null, "summary": "This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach.", "AI": {"tldr": "\u4f7f\u7528\u591a\u4efb\u52a1\u8d1d\u53f6\u65af\u4f18\u5316\u8c03\u6574\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u5206\u6563\u8f68\u8ff9\u751f\u6210\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u4e0d\u540c\u573a\u666f\u95f4\u5173\u7cfb\uff0c\u6bd4\u8f83\u5e73\u5747\u4efb\u52a1\u4f18\u5316\u548c\u5355\u4efb\u52a1\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u5728\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\uff0c\u5206\u6563\u8f68\u8ff9\u751f\u6210\u7b97\u6cd5\u7684\u53c2\u6570\u8c03\u4f18\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u968f\u7740\u65e0\u4eba\u673a\u6570\u91cf\u589e\u52a0\uff0c\u4f18\u5316\u6210\u672c\u6025\u5267\u4e0a\u5347\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8de8\u4e0d\u540c\u4ea4\u4e92\u573a\u666f\u5171\u4eab\u4fe1\u606f\u7684\u9ad8\u6548\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u4efb\u52a1\u5b9a\u4e49\u4e3a\u7279\u5b9a\u65e0\u4eba\u673a\u95f4\u4ea4\u4e92\u6570\u91cf\u7684\u8f68\u8ff9\u751f\u6210\u573a\u666f\u3002\u4f7f\u7528\u591a\u4efb\u52a1\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u4efb\u52a1\u95f4\u5173\u7cfb\uff0c\u5b9e\u73b0\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u4f20\u9012\u3002\u6bd4\u8f83\u4e24\u79cd\u7b56\u7565\uff1a\u4f18\u5316\u6240\u6709\u4efb\u52a1\u7684\u5e73\u5747\u4efb\u52a1\u65f6\u95f4\u548c\u5355\u72ec\u4f18\u5316\u6bcf\u4e2a\u4efb\u52a1\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u4efb\u52a1\u4f18\u5316\u968f\u7740\u7fa4\u4f53\u89c4\u6a21\u589e\u957f\u80fd\u5b9e\u73b0\u9010\u6e10\u7f29\u77ed\u7684\u4efb\u52a1\u65f6\u95f4\uff0c\u4f46\u9700\u8981\u6bd4\u5e73\u5747\u4efb\u52a1\u65b9\u6cd5\u663e\u8457\u66f4\u591a\u7684\u4f18\u5316\u65f6\u95f4\u3002", "conclusion": "\u591a\u4efb\u52a1\u8d1d\u53f6\u65af\u4f18\u5316\u4e3a\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u8f68\u8ff9\u751f\u6210\u53c2\u6570\u8c03\u4f18\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5e73\u5747\u4efb\u52a1\u4f18\u5316\u7b56\u7565\u5728\u4f18\u5316\u6548\u7387\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u800c\u5355\u4efb\u52a1\u4f18\u5316\u5728\u6027\u80fd\u63d0\u5347\u65b9\u9762\u8868\u73b0\u66f4\u597d\u4f46\u4ee3\u4ef7\u66f4\u9ad8\u3002"}}
{"id": "2512.08653", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08653", "abs": "https://arxiv.org/abs/2512.08653", "authors": ["Doumegna Mawuto Koudjo Felix", "Xianjia Yu", "Zhuo Zou", "Tomi Westerlund"], "title": "A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation", "comment": null, "summary": "Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4f20\u611f\u5668\u611f\u77e5\u7684\u6fc0\u5149\u96f7\u8fbe\u9000\u5316\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u70b9\u4e91\u4e0a\u6a21\u62df\u53ef\u89e3\u91ca\u7684\u9000\u5316\u6548\u679c\uff0c\u4ee5\u53ef\u63a7\u548c\u53ef\u91cd\u590d\u7684\u65b9\u5f0f\u6d4b\u8bd5SLAM\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6fc0\u5149\u96f7\u8fbeSLAM\u9c81\u68d2\u6027\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u7269\u7406\u57fa\u7840\uff0c\u8981\u4e48\u65e0\u6cd5\u6355\u6349\u4f20\u611f\u5668\u7279\u5b9a\u7684\u884c\u4e3a\u3002\u6fc0\u5149\u96f7\u8fbeSLAM\u7cfb\u7edf\u5bf9\u906e\u6321\u3001\u566a\u58f0\u548c\u89c6\u573a\u9000\u5316\u7b49\u6076\u52a3\u6761\u4ef6\u9ad8\u5ea6\u654f\u611f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7269\u7406\u57fa\u7840\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f20\u611f\u5668\u611f\u77e5\u7684\u73b0\u8c61\u5b66\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u771f\u5b9e\u70b9\u4e91\u4e0a\u6a21\u62df\u53ef\u89e3\u91ca\u7684\u6fc0\u5149\u96f7\u8fbe\u9000\u5316\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u6bcf\u70b9\u7684\u51e0\u4f55\u3001\u5f3a\u5ea6\u548c\u65f6\u95f4\u7ed3\u6784\uff0c\u540c\u65f6\u5e94\u7528\u7ed3\u6784\u5316\u4e22\u5931\u3001\u89c6\u573a\u51cf\u5c11\u3001\u9ad8\u65af\u566a\u58f0\u3001\u906e\u6321\u63a9\u7801\u3001\u7a00\u758f\u5316\u548c\u8fd0\u52a8\u5931\u771f\u3002\u6846\u67b6\u5177\u6709\u81ea\u4e3b\u4e3b\u9898\u548c\u4f20\u611f\u5668\u68c0\u6d4b\u3001\u6a21\u5757\u5316\u914d\u7f6e\uff08\u56db\u4e2a\u4e25\u91cd\u7ea7\u522b\uff09\u548c\u5b9e\u65f6\u6027\u80fd\uff08\u6bcf\u5e27\u5c0f\u4e8e20\u6beb\u79d2\uff09\u3002", "result": "\u5728\u4e09\u79cd\u6fc0\u5149\u96f7\u8fbe\u67b6\u6784\u548c\u4e94\u79cd\u6700\u5148\u8fdb\u7684SLAM\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86\u7531\u4f20\u611f\u5668\u8bbe\u8ba1\u548c\u73af\u5883\u80cc\u666f\u5851\u9020\u7684\u72ec\u7279\u9c81\u68d2\u6027\u6a21\u5f0f\u3002\u5f00\u6e90\u5b9e\u73b0\u4e3a\u5728\u7269\u7406\u6709\u610f\u4e49\u7684\u9000\u5316\u573a\u666f\u4e0b\u57fa\u51c6\u6d4b\u8bd5\u6fc0\u5149\u96f7\u8fbeSLAM\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f20\u611f\u5668\u611f\u77e5\u3001\u7269\u7406\u57fa\u7840\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u6fc0\u5149\u96f7\u8fbeSLAM\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u4e3a\u53ef\u63a7\u548c\u53ef\u91cd\u590d\u7684SLAM\u538b\u529b\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.08656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08656", "abs": "https://arxiv.org/abs/2512.08656", "authors": ["Lauritz Rismark Fosso", "Herman Bi\u00f8rn Amundsen", "Marios Xanthidis", "Sveinung Johan Ohrem"], "title": "Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes", "comment": "7 pages, 4 figures", "summary": "Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions.", "AI": {"tldr": "\u9996\u4e2a\u96f6\u6837\u672csim2real\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u901f\u5ea6\u63a7\u5236\u5668\uff0c\u4ec5\u97003\u5206\u949f\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0AUV\u76846\u81ea\u7531\u5ea6\u654f\u6377\u673a\u52a8\u548c\u8def\u5f84\u8ddf\u8e2a", "motivation": "\u5168\u5411\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\uff08AUV\uff09\u5177\u5907\u786c\u4ef6\u4e0a\u7684\u654f\u6377\u673a\u52a8\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u6c34\u9759\u529b\u5b66\u548c\u6c34\u52a8\u529b\u5b66\u3001\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u8f7d\u8377\u53d8\u5316\u5bfc\u81f4\u7684\u52a8\u6001\u7279\u6027\u9891\u7e41\u6539\u53d8\uff0c\u63a7\u5236\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u5e73\u53f0\u914d\u7f6e\u7cbe\u5fc3\u8c03\u4f18\u63a7\u5236\u5668\uff0c\u4e14\u5728\u4e0d\u540c\u8f7d\u8377\u548c\u6c34\u52a8\u529b\u6761\u4ef6\u4e0b\u9700\u8981\u91cd\u65b0\u8c03\u4f18\uff0c\u5bfc\u81f46\u81ea\u7531\u5ea6\u654f\u6377\u673a\u52a8\u5728\u5b9e\u8df5\u4e2d\u5f88\u5c11\u4f7f\u7528\u3002", "method": "\u63d0\u51faSim2Swim\u65b9\u6cd5\uff0c\u53d7\u6700\u5148\u8fdb\u7684DRL\u4f4d\u7f6e\u63a7\u5236\u542f\u53d1\uff0c\u5229\u7528\u57df\u968f\u673a\u5316\u548c\u5927\u89c4\u6a21\u5e76\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u540e\u5904\u7406\u6216\u8c03\u4f18\u5373\u53ef\u4e3a\u4e0d\u540c\u7279\u6027\u7684AUV\u6536\u655b\u5230\u53ef\u73b0\u573a\u90e8\u7f72\u7684\u63a7\u5236\u7b56\u7565\u3002\u8fd9\u662f\u9996\u4e2a\u901a\u7528\u7684\u96f6\u6837\u672csim2real\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u901f\u5ea6\u63a7\u5236\u5668\u3002", "result": "\u5728\u591a\u79cd\u914d\u7f6e\u7684\u6c60\u8bd5\u9a8c\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u9ad8\u5ea6\u654f\u6377\u8fd0\u52a8\u7684\u9c81\u68d2\u63a7\u5236\u6027\u80fd\u3002\u8bad\u7ec3\u65f6\u95f4\u4ec5\u97003\u5206\u949f\uff0c\u5c31\u80fd\u5b9e\u73b0\u8def\u5f84\u8ddf\u8e2a\u548c6\u81ea\u7531\u5ea6\u654f\u6377\u673a\u52a8\u3002", "conclusion": "Sim2Swim\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u9996\u4e2a\u96f6\u6837\u672csim2real DRL\u901f\u5ea6\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5904\u7406AUV\u7684\u590d\u6742\u52a8\u6001\u7279\u6027\u53d8\u5316\uff0c\u4e3a\u6c34\u4e0b\u822a\u884c\u5668\u7684\u654f\u6377\u673a\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08661", "abs": "https://arxiv.org/abs/2512.08661", "authors": ["Ziyue Zheng", "Yongce Liu", "Hesheng Wang", "Zhongqiang Ren"], "title": "Ergodic Trajectory Planning with Dynamic Sensor Footprints", "comment": "12 figures", "summary": "This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8003\u8651\u52a8\u6001\u4f20\u611f\u5668\u8db3\u8ff9\u7684\u904d\u5386\u89c4\u5212\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u8f68\u8ff9\u548c\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u904d\u5386\u6027\u3002", "motivation": "\u73b0\u6709\u904d\u5386\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u70b9\u4f20\u611f\u5668\u6216\u56fa\u5b9a\u5f62\u72b6\u5206\u8fa8\u7387\u7684\u4f20\u611f\u5668\u8db3\u8ff9\uff0c\u800c\u5b9e\u9645\u4e2d\u4f20\u611f\u5668\u8db3\u8ff9\u4f1a\u968f\u673a\u5668\u4eba\u8fd0\u52a8\u52a8\u6001\u53d8\u5316\uff08\u5982\u65e0\u4eba\u673a\u4fef\u4ef0\u89d2\u3001\u9ad8\u5ea6\u53d8\u5316\u5bfc\u81f4\u89c6\u91ce\u53d8\u5316\uff09\uff0c\u9700\u8981\u65b0\u7684\u89c4\u5212\u6846\u67b6\u6765\u5904\u7406\u8fd9\u79cd\u52a8\u6001\u4f20\u611f\u5668\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u8003\u8651\u52a8\u6001\u4f20\u611f\u5668\u8db3\u8ff9\u7684\u65b0\u5ea6\u91cf\u6307\u6807\uff0c\u5206\u6790\u7406\u8bba\u5c40\u90e8\u6700\u4f18\u6761\u4ef6\uff0c\u5e76\u5f00\u53d1\u6570\u503c\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u673a\u5668\u4eba\u8f68\u8ff9\u548c\u4f20\u611f\u5668\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u540c\u65f6\u4f18\u5316\u8f68\u8ff9\u548c\u4f20\u611f\u5668\u8db3\u8ff9\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u904d\u5386\u6027\uff0c\u5e76\u5728\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u6210\u529f\u5e94\u7528\u4e8e3D\u7a7a\u95f4\u7269\u4f53\u8986\u76d6\u3002", "conclusion": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u52a8\u6001\u4f20\u611f\u5668\u8db3\u8ff9\u4e0b\u7684\u904d\u5386\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u65b0\u5ea6\u91cf\u548c\u4f18\u5316\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406\u4f20\u611f\u5668\u914d\u7f6e\u53d8\u5316\uff0c\u4e3a\u5b9e\u9645\u673a\u5668\u4eba\u4fe1\u606f\u91c7\u96c6\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08688", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08688", "abs": "https://arxiv.org/abs/2512.08688", "authors": ["Mark Pustilnik", "Francesco Borrelli"], "title": "Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility", "comment": null, "summary": "This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u5bfc\u822a\u7684\u52a8\u6001\u535a\u5f08\u6846\u67b6\uff0c\u5f15\u5165\u975e\u5f52\u4e00\u5316\u5747\u8861\u7ed3\u6784\u5904\u7406\u5171\u4eab\u5b89\u5168\u7ea6\u675f\uff0c\u5e76\u5d4c\u5165\u5230\u6eda\u52a8\u65f6\u57df\u6700\u4f18\u63a7\u5236\u4e2d", "motivation": "\u5728\u5b58\u5728\u969c\u788d\u7269\u7684\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u9700\u8981\u4eba\u673a\u534f\u4f5c\u5b8c\u6210\u5171\u540c\u4efb\u52a1\uff0c\u540c\u65f6\u6ee1\u8db3\u5171\u4eab\u5b89\u5168\u7ea6\u675f\uff08\u5982\u907f\u78b0\u548c\u95f4\u8ddd\u4fdd\u6301\uff09\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u53cc\u65b9\u8d21\u732e\u76f8\u540c\u52aa\u529b\uff0c\u4f46\u5b9e\u9645\u4e2d\u4eba\u548c\u673a\u5668\u4eba\u53ef\u80fd\u6709\u4e0d\u540c\u7684\u80fd\u529b\u548c\u8d23\u4efb\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u5408\u4f5c\u673a\u5236\u3002", "method": "1. \u5efa\u7acb\u4eba\u673a\u534f\u4f5c\u5bfc\u822a\u7684\u52a8\u6001\u535a\u5f08\u6a21\u578b\uff1b2. \u5f15\u5165\u975e\u5f52\u4e00\u5316\u5747\u8861\u7ed3\u6784\u5904\u7406\u5171\u4eab\u5b89\u5168\u7ea6\u675f\uff0c\u5141\u8bb8\u53cc\u65b9\u4ee5\u4e0d\u540c\u52aa\u529b\u6c34\u5e73\u6267\u884c\u5b89\u5168\u8981\u6c42\uff1b3. \u5c06\u8be5\u5747\u8861\u7ed3\u6784\u5d4c\u5165\u5230\u6eda\u52a8\u65f6\u57df\u6700\u4f18\u63a7\u5236\u6846\u67b6\u4e2d\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u5171\u4eab\u5b89\u5168\u7ea6\u675f\u7684\u7075\u6d3b\u534f\u4f5c\u6846\u67b6\uff0c\u5141\u8bb8\u4eba\u548c\u673a\u5668\u4eba\u6839\u636e\u5404\u81ea\u80fd\u529b\u4ee5\u4e0d\u540c\u52aa\u529b\u6c34\u5e73\u53c2\u4e0e\u5b89\u5168\u7ea6\u675f\u7684\u6267\u884c\uff0c\u76f8\u6bd4\u4f20\u7edf\u5f52\u4e00\u5316\u65b9\u6cd5\u66f4\u5177\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u901a\u8fc7\u975e\u5f52\u4e00\u5316\u5747\u8861\u7ed3\u6784\u548c\u6eda\u52a8\u65f6\u57df\u6700\u4f18\u63a7\u5236\u7684\u7ed3\u5408\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9002\u5e94\u5b9e\u9645\u5e94\u7528\u4e2d\u53cc\u65b9\u80fd\u529b\u5dee\u5f02\u7684\u60c5\u51b5\u3002"}}
{"id": "2512.08754", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08754", "abs": "https://arxiv.org/abs/2512.08754", "authors": ["Jason Hughes", "Marcel Hussing", "Edward Zhang", "Shenbagaraj Kannapiran", "Joshua Caswell", "Kenneth Chaney", "Ruichen Deng", "Michaela Feehery", "Agelos Kratimenos", "Yi Fan Li", "Britny Major", "Ethan Sanchez", "Sumukh Shrote", "Youkang Wang", "Jeremy Wang", "Daudi Zein", "Luying Zhang", "Ruijun Zhang", "Alex Zhou", "Tenzi Zhouga", "Jeremy Cannon", "Zaffir Qasim", "Jay Yelon", "Fernando Cladera", "Kostas Daniilidis", "Camillo J. Taylor", "Eric Eaton"], "title": "A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models", "comment": "Technical Report for the DARPA Triage Challenge PRONTO team", "summary": "This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u4ef6\u8fdc\u7a0b\u521d\u7ea7\u5206\u8bca\u7684\u5f02\u6784\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7531\u65e0\u4eba\u673a\u548c\u5730\u9762\u673a\u5668\u4eba\u534f\u540c\u5de5\u4f5c\uff0c\u5728\u4e0d\u5371\u53ca\u6551\u63f4\u4eba\u5458\u7684\u60c5\u51b5\u4e0b\u5b9a\u4f4d\u53d7\u5bb3\u8005\u3001\u8bc4\u4f30\u4f24\u60c5\u5e76\u786e\u5b9a\u533b\u7597\u4f18\u5148\u7ea7\u3002", "motivation": "\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u4ef6\u4e2d\uff0c\u4f20\u7edf\u6551\u63f4\u65b9\u5f0f\u4f1a\u5371\u53ca\u6551\u63f4\u4eba\u5458\u751f\u547d\uff0c\u4e14\u7f3a\u4e4f\u5b8c\u6574\u7684\u8fdc\u7a0b\u5206\u8bca\u7cfb\u7edf\u3002\u73b0\u6709\u673a\u5668\u4eba\u7814\u7a76\u591a\u5173\u6ce8\u63a2\u7d22\u6216\u6709\u9650\u7684\u533b\u7597\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5b8c\u6210\u5b8c\u6574\u7684\u5206\u8bca\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u673a\u5668\u4eba\u534f\u540c\u7684\u5f02\u6784\u7cfb\u7edf\uff1a\u65e0\u4eba\u673a\u8d1f\u8d23\u5b9a\u4f4d\u53d7\u5bb3\u8005\u5e76\u63d0\u4f9b\u4fef\u89c6\u56fe\uff1b\u5730\u9762\u673a\u5668\u4eba\u914d\u5907\u4e13\u4e1a\u4f20\u611f\u5668\u6d4b\u91cf\u751f\u547d\u4f53\u5f81\u3001\u68c0\u6d4b\u548c\u5b9a\u4f4d\u8eab\u4f53\u635f\u4f24\u3002\u7cfb\u7edf\u6574\u5408\u4e86\u53d7\u5bb3\u8005\u5b9a\u4f4d\u3001\u751f\u547d\u4f53\u5f81\u6d4b\u91cf\u3001\u4f24\u60c5\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u3001\u7cbe\u795e\u72b6\u6001\u8bc4\u4f30\u548c\u6570\u636e\u6574\u5408\u7b49\u5b8c\u6574\u5206\u8bca\u6d41\u7a0b\u3002", "result": "\u8be5\u7cfb\u7edf\u4f5c\u4e3aDARPA\u5206\u8bca\u6311\u6218\u8d5b\u7684\u4e00\u90e8\u5206\u5f00\u53d1\uff0c\u5c55\u793a\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5982\u4f55\u5728\u707e\u96be\u54cd\u5e94\u573a\u666f\u4e2d\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\uff0c\u6700\u5927\u9650\u5ea6\u5730\u633d\u6551\u751f\u547d\u3002", "conclusion": "\u5f02\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u652f\u6301\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u4ef6\u7684\u8fdc\u7a0b\u521d\u7ea7\u5206\u8bca\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u548c\u5730\u9762\u673a\u5668\u4eba\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u5728\u4e0d\u5371\u53ca\u6551\u63f4\u4eba\u5458\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u5b8c\u6574\u7684\u5206\u8bca\u6d41\u7a0b\uff0c\u4e3a\u707e\u96be\u54cd\u5e94\u63d0\u4f9b\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08767", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08767", "abs": "https://arxiv.org/abs/2512.08767", "authors": ["Mohammed Elseiagy", "Tsige Tadesse Alemayoh", "Ranulfo Bezerra", "Shotaro Kojima", "Kazunori Ohno"], "title": "Data-Driven Dynamic Parameter Learning of manipulator robots", "comment": "Accepted for publication at SII 2026. 6 pages, 7 figures. Code is available at: https://github.com/MohamedAlsiagy/dynamic_parameter_est", "summary": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u52a8\u6001\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u673a\u5668\u4eba\u6a21\u578b\u548c\u8f68\u8ff9\u6570\u636e\uff0c\u7ed3\u5408\u96c5\u53ef\u6bd4\u7279\u5f81\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u53c2\u6570\u4f30\u8ba1\uff0c\u63d0\u5347\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u662f\u6839\u672c\u6311\u6218\uff0c\u51c6\u786e\u7684\u52a8\u6001\u53c2\u6570\u4f30\u8ba1\u5bf9\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u3001\u771f\u5b9e\u4eff\u771f\u548c\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u673a\u5668\u4eba\u7ed3\u6784\u548c\u4ea4\u4e92\uff0c\u800c\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u52a8\u6001\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6784\u5efa\u81ea\u52a8\u5316\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u751f\u62108,192\u4e2a\u5177\u6709\u4e0d\u540c\u60ef\u6027\u548c\u6469\u64e6\u7279\u6027\u7684\u673a\u5668\u4eba\u6a21\u578b\uff0c\u4f7f\u7528\u96c5\u53ef\u6bd4\u63a8\u5bfc\u7684\u7279\u5f81\u4e30\u5bcc\u8f68\u8ff9\u6570\u636e\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u6700\u4f73\u914d\u7f6e\uff08\u5e8f\u5217\u957f\u5ea664\u300164Hz\u91c7\u6837\u7387\u30014\u5c42\u300132\u5934\uff09\u8fbe\u5230\u9a8c\u8bc1R2\u4e3a0.8633\u3002\u8d28\u91cf\u548c\u60ef\u6027\u4f30\u8ba1\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u5e93\u4ed1\u6469\u64e6\u8fbe\u5230\u4e2d\u9ad8\u7cbe\u5ea6\uff0c\u800c\u7c98\u6027\u6469\u64e6\u548c\u8fdc\u7aef\u8fde\u6746\u8d28\u5fc3\u4f30\u8ba1\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "Transformer\u7ed3\u5408\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u548c\u8fd0\u52a8\u5b66\u7279\u5f81\u589e\u5f3a\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u51c6\u786e\u7684\u52a8\u6001\u53c2\u6570\u4f30\u8ba1\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002"}}
{"id": "2512.08813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08813", "abs": "https://arxiv.org/abs/2512.08813", "authors": ["Connor York", "Zachary R Madin", "Paul O'Dowd", "Edmund R Hunt"], "title": "Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks", "comment": "Accepted to SAC '26. To appear, DOI: https://doi.org/10.1145/3748522.3779970", "summary": "Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization (\"patrollers\" and \"searchers\") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task.", "AI": {"tldr": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8fde\u7eed\u4efb\u52a1\u4e2d\u88ab\u7d27\u6025\u5b50\u4efb\u52a1\u4e2d\u65ad\u65f6\u9762\u4e34\u6027\u80fd\u6743\u8861\uff0c\u7814\u7a76\u901a\u8fc7\u89d2\u8272\u4e13\u4e1a\u5316\uff08\u5de1\u903b\u8005\u548c\u641c\u7d22\u8005\uff09\u548c\u611f\u77e5\u5f02\u8d28\u6027\u6765\u4f18\u5316\u8fd9\u79cd\u6743\u8861", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6267\u884c\u8fde\u7eed\u4efb\u52a1\u65f6\uff0c\u5982\u679c\u88ab\u7d27\u6025\u3001\u65f6\u95f4\u5173\u952e\u7684\u5b50\u4efb\u52a1\u4e2d\u65ad\uff0c\u4f1a\u9762\u4e34\u6027\u80fd\u6743\u8861\u95ee\u9898\u3002\u672c\u6587\u7814\u7a76\u5982\u4f55\u5728\u533a\u57df\u5de1\u903b\u548c\u5b9a\u4f4d\u5f02\u5e38\u65e0\u7ebf\u7535\u4fe1\u53f7\u8fd9\u4e24\u4e2a\u65f6\u95f4\u51b2\u7a81\u7684\u4efb\u52a1\u4e4b\u95f4\u627e\u5230\u6700\u4f18\u5e73\u8861", "method": "\u901a\u8fc7\u6a21\u62df\u8bc4\u4f30\u4e24\u79cd\u5f02\u8d28\u6027\u7b56\u7565\uff1a1\uff09\u884c\u4e3a\u5f02\u8d28\u6027\uff08\u89d2\u8272\u4e13\u4e1a\u5316\uff1a\u5de1\u903b\u8005\u548c\u641c\u7d22\u8005\uff09\uff0c2\uff09\u611f\u77e5\u5f02\u8d28\u6027\uff08\u53ea\u6709\u641c\u7d22\u8005\u80fd\u611f\u77e5\u65e0\u7ebf\u7535\u4fe1\u53f7\uff09\u3002\u5728\u4e0d\u540c\u56e2\u961f\u7ec4\u6210\u4e0b\u8bc6\u522b\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861", "result": "\u884c\u4e3a\u5f02\u8d28\u6027\u56e2\u961f\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u6700\u5e73\u8861\u7684\u6743\u8861\u3002\u5f53\u611f\u77e5\u80fd\u529b\u53d7\u9650\u65f6\uff0c\u53ea\u6709\u4e00\u534a\u673a\u5668\u4eba\u5177\u5907\u611f\u77e5\u80fd\u529b\u7684\u5f02\u8d28\u6027\u56e2\u961f\u4e0e\u540c\u8d28\u6027\u56e2\u961f\u8868\u73b0\u76f8\u5f53\uff0c\u8fd9\u4e3a\u9650\u5236\u4f20\u611f\u5668\u90e8\u7f72\u63d0\u4f9b\u4e86\u6210\u672c\u8282\u7ea6\u4f9d\u636e", "conclusion": "\u9884\u90e8\u7f72\u89d2\u8272\u548c\u611f\u77e5\u4e13\u4e1a\u5316\u662f\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u5bf9\u65f6\u95f4\u51b2\u7a81\u4efb\u52a1\u65f6\u7684\u91cd\u8981\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\uff0c\u901a\u8fc7\u8c03\u6574\u884c\u4e3a\u5f02\u8d28\u6027\u7a0b\u5ea6\u53ef\u4ee5\u8c03\u8282\u7cfb\u7edf\u6027\u80fd\u504f\u5411\u7279\u5b9a\u4efb\u52a1"}}
{"id": "2512.08877", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08877", "abs": "https://arxiv.org/abs/2512.08877", "authors": ["Ryan LeRoy", "Jack Kolb"], "title": "IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams", "comment": "4 pages, 3 figures, appendix", "summary": "Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve.", "AI": {"tldr": "\u5728\u5f02\u6784\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0c\u7814\u7a76\u53d1\u73b0\u7b80\u5355\u7684IPPO\u81ea\u535a\u5f08\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u672a\u89c1\u8fc7\u7684\u961f\u53cb\u7b97\u6cd5\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u4e0e\u4e13\u95e8\u8bbe\u8ba1\u7684\u591a\u6837\u5316\u8bad\u7ec3\u65b9\u6cd5RPT\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u81ea\u535a\u5f08\u8bad\u7ec3\u662f\u5426\u771f\u6b63\u5b66\u4e60\u5230\u57fa\u4e8e\u6e38\u620f\u672c\u8d28\u7684\u901a\u7528\u534f\u8c03\u7b56\u7565\uff0c\u8fd8\u662f\u4ec5\u4ec5\u8fc7\u62df\u5408\u5230\u8bad\u7ec3\u4f19\u4f34\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u65cb\u8f6c\u7b56\u7565\u8bad\u7ec3(RPT)\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8f6e\u6362\u4e0d\u540c\u5b66\u4e60\u7b97\u6cd5\u7684\u5f02\u6784\u961f\u53cb\u7b56\u7565\uff0c\u8ba9\u667a\u80fd\u4f53\u63a5\u89e6\u66f4\u5e7f\u6cdb\u7684\u4f19\u4f34\u7b56\u7565\uff1b\u4f7f\u7528HeMAC\u5f02\u6784\u591a\u667a\u80fd\u4f53\u6311\u6218\u73af\u5883\uff0c\u5305\u542b\u89c2\u5bdf\u8005\u548c\u65e0\u4eba\u673a\u4e24\u79cd\u4e92\u8865\u80fd\u529b\u7684\u667a\u80fd\u4f53\u3002", "result": "\u5f53\u4e0e\u4fdd\u7559\u7684\u961f\u53cb\u7b56\u7565(DDQN)\u4e00\u8d77\u6e38\u620f\u65f6\uff0cRPT\u4e0e\u6807\u51c6\u7684\u81ea\u535a\u5f08\u57fa\u7ebfIPPO\u8868\u73b0\u76f8\u4f3c\uff0c\u8868\u660eIPPO\u57fa\u7ebf\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u961f\u53cb\u7b97\u6cd5\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u4e2d\u6ca1\u6709\u7ecf\u5386\u961f\u53cb\u591a\u6837\u6027\u3002", "conclusion": "\u7b80\u5355\u7684IPPO\u57fa\u7ebf\u65b9\u6cd5\u53ef\u80fd\u5df2\u7ecf\u5177\u5907\u4e86\u4e13\u95e8\u8bbe\u8ba1\u7684\u591a\u6837\u5316\u8bad\u7ec3\u65b9\u6cd5\u6240\u671f\u671b\u8fbe\u5230\u7684\u5bf9\u65b0\u961f\u53cb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u8868\u660e\u5728\u5f02\u6784\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\uff0c\u81ea\u535a\u5f08\u8bad\u7ec3\u53ef\u80fd\u6bd4\u9884\u671f\u66f4\u5177\u6cdb\u5316\u6027\u3002"}}
{"id": "2512.08920", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08920", "abs": "https://arxiv.org/abs/2512.08920", "authors": ["Jessica Yin", "Haozhi Qi", "Youngsun Wi", "Sayantan Kundu", "Mike Lambeta", "William Yang", "Changhao Wang", "Tingfan Wu", "Jitendra Malik", "Tess Hellebrekers"], "title": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer", "comment": "Project website: https://jessicayin.github.io/osmo_tactile_glove/", "summary": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.", "AI": {"tldr": "OSMO\u662f\u4e00\u79cd\u5f00\u6e90\u53ef\u7a7f\u6234\u89e6\u89c9\u624b\u5957\uff0c\u7528\u4e8e\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u7684\u6280\u80fd\u8f6c\u79fb\uff0c\u901a\u8fc712\u4e2a\u4e09\u8f74\u89e6\u89c9\u4f20\u611f\u5668\u6536\u96c6\u63a5\u89e6\u4fe1\u53f7\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ec5\u51ed\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u5b66\u4e60\u6267\u884c\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u4e3a\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u89c6\u9891\u672c\u8eab\u65e0\u6cd5\u6355\u6349\u64cd\u4f5c\u4e2d\u5173\u952e\u7684\u4e30\u5bcc\u63a5\u89e6\u4fe1\u53f7\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u638c\u63e1\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1OSMO\u5f00\u6e90\u53ef\u7a7f\u6234\u89e6\u89c9\u624b\u5957\uff0c\u914d\u590712\u4e2a\u4e09\u8f74\u89e6\u89c9\u4f20\u611f\u5668\u5206\u5e03\u5728\u6307\u5c16\u548c\u624b\u638c\uff0c\u517c\u5bb9\u5148\u8fdb\u7684\u624b\u90e8\u8ddf\u8e2a\u65b9\u6cd5\u8fdb\u884c\u91ce\u5916\u6570\u636e\u6536\u96c6\u3002\u901a\u8fc7\u4e3a\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u914d\u5907\u76f8\u540c\u7684\u624b\u5957\uff0c\u6700\u5c0f\u5316\u89c6\u89c9\u548c\u89e6\u89c9\u7684\u4f53\u73b0\u5dee\u8ddd\uff0c\u5b9e\u73b0\u8fde\u7eed\u526a\u5207\u529b\u548c\u6cd5\u5411\u529b\u53cd\u9988\u7684\u8f6c\u79fb\u3002", "result": "\u4ec5\u4f7f\u7528OSMO\u6536\u96c6\u7684\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u7b56\u7565\uff08\u65e0\u9700\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\uff09\u80fd\u591f\u6267\u884c\u5177\u6709\u6311\u6218\u6027\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\u3002\u5728\u9700\u8981\u6301\u7eed\u63a5\u89e6\u538b\u529b\u7684\u771f\u5b9e\u4e16\u754c\u64e6\u62ed\u4efb\u52a1\u4e2d\uff0c\u89e6\u89c9\u611f\u77e5\u7b56\u7565\u8fbe\u523072%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u89c6\u89c9\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "OSMO\u901a\u8fc7\u6700\u5c0f\u5316\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4f53\u73b0\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u89e6\u89c9\u6280\u80fd\u8f6c\u79fb\uff0c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u786c\u4ef6\u8bbe\u8ba1\u4fc3\u8fdb\u793e\u533a\u91c7\u7528\u3002"}}
