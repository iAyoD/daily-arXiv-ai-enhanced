<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos](https://arxiv.org/abs/2509.09769)
*Rutav Shah,Shuijing Liu,Qi Wang,Zhenyu Jiang,Sateesh Kumar,Mingyo Seo,Roberto Martín-Martín,Yuke Zhu*

Main category: cs.RO

TL;DR: MimicDroid是一个使用人类游戏视频作为训练数据的人形机器人模仿学习系统，通过上下文学习实现少样本适应新任务，在模拟和真实环境中都取得了优异性能


<details>
  <summary>Details</summary>
Motivation: 解决当前上下文学习方法依赖人工遥操作数据的可扩展性问题，利用人类游戏视频作为更丰富、可扩展的训练数据源

Method: 从人类游戏视频中提取相似操作行为的轨迹对，训练策略预测一个轨迹的动作条件于另一个轨迹；通过人体姿态重定向和随机补丁掩码来弥合人机差异

Result: 在开源模拟基准测试中超越最先进方法，在真实世界中实现了近两倍的成功率提升

Conclusion: 人类游戏视频是训练人形机器人上下文学习能力的有效数据源，MimicDroid展示了在少样本学习中的强大适应能力

Abstract: We aim to enable humanoid robots to efficiently solve new manipulation tasks
from a few video examples. In-context learning (ICL) is a promising framework
for achieving this goal due to its test-time data efficiency and rapid
adaptability. However, current ICL methods rely on labor-intensive teleoperated
data for training, which restricts scalability. We propose using human play
videos -- continuous, unlabeled videos of people interacting freely with their
environment -- as a scalable and diverse training data source. We introduce
MimicDroid, which enables humanoids to perform ICL using human play videos as
the only training data. MimicDroid extracts trajectory pairs with similar
manipulation behaviors and trains the policy to predict the actions of one
trajectory conditioned on the other. Through this process, the model acquired
ICL capabilities for adapting to novel objects and environments at test time.
To bridge the embodiment gap, MimicDroid first retargets human wrist poses
estimated from RGB videos to the humanoid, leveraging kinematic similarity. It
also applies random patch masking during training to reduce overfitting to
human-specific cues and improve robustness to visual differences. To evaluate
few-shot learning for humanoids, we introduce an open-source simulation
benchmark with increasing levels of generalization difficulty. MimicDroid
outperformed state-of-the-art methods and achieved nearly twofold higher
success rates in the real world. Additional materials can be found on:
ut-austin-rpl.github.io/MimicDroid

</details>


### [2] [MIMo grows! Simulating body and sensory development in a multimodal infant model](https://arxiv.org/abs/2509.09805)
*Francisco M. López,Miles Lenz,Marco G. Fedozzi,Arthur Aubret,Jochen Triesch*

Main category: cs.RO

TL;DR: MIMo v2是一个多模态婴儿模型，通过模拟从出生到24个月的身体生长、运动能力发展、视觉发育和感觉运动延迟，提高了发育机器人建模的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的发育机器人和仿真平台通常针对特定年龄段设计，无法捕捉婴儿发育过程中不断变化的能力和约束条件。

Method: 开发了MIMo v2模型，包含：1）从出生到24个月的身体生长模型和运动强度增加；2）具有发展性视觉敏锐度的中央凹视觉；3）模拟信号传输延迟的感觉运动延迟；4）逆向运动学模块和随机环境生成器。

Result: 新版本的MIMo能够更真实地模拟各种感觉运动发育方面，并与第三方仿真和学习库保持兼容。

Conclusion: MIMo v2提供了一个更全面的婴儿发育模型框架，有助于更准确地研究婴儿感觉运动发育过程，代码已在官方仓库开源。

Abstract: Infancy is characterized by rapid body growth and an explosive change of
sensory and motor abilities. However, developmental robots and simulation
platforms are typically designed in the image of a specific age, which limits
their ability to capture the changing abilities and constraints of developing
infants. To address this issue, we present MIMo v2, a new version of the
multimodal infant model. It includes a growing body with increasing actuation
strength covering the age range from birth to 24 months. It also features
foveated vision with developing visual acuity as well as sensorimotor delays
modeling finite signal transmission speeds to and from an infant's brain.
Further enhancements of this MIMo version include an inverse kinematics module,
a random environment generator and updated compatiblity with third-party
simulation and learning libraries. Overall, this new MIMo version permits
increased realism when modeling various aspects of sensorimotor development.
The code is available on the official repository
(https://github.com/trieschlab/MIMo).

</details>


### [3] [Using the Pepper Robot to Support Sign Language Communication](https://arxiv.org/abs/2509.09889)
*Giulia Botta,Marco Botta,Cristina Gena,Alessandro Mazzei,Massimo Donini,Alberto Lillo*

Main category: cs.RO

TL;DR: 研究探索商业社交机器人Pepper能否产生可理解的意大利手语(LIS)符号和短句，通过用户研究发现大多数孤立符号能被正确识别，但完整句子识别率较低。


<details>
  <summary>Details</summary>
Motivation: 社交机器人在公共和辅助环境中应用日益增多，但针对聋人用户的可访问性研究不足。意大利手语(LIS)作为成熟自然语言，具有复杂的手势和非手势成分，让机器人使用LIS交流可以促进更包容的人机交互。

Method: 与聋人学生和手语专家合作，通过手动动画技术或MATLAB逆向运动学求解器，在Pepper机器人上共同设计实现了52个LIS符号。对12名精通LIS的参与者(包括聋人和听力正常者)进行探索性用户研究，包含15个单项选择视频符号识别任务和2个开放式短句问题。

Result: 大多数孤立符号能被正确识别，但由于Pepper的有限关节活动能力和时间限制，完整句子识别率显著较低。研究表明即使是商业社交机器人也能智能地执行LIS符号子集。

Conclusion: 商业社交机器人如Pepper能够执行可理解的LIS符号子集，为更包容的交互设计提供机会。未来开发应解决多模态增强(如基于屏幕的支持或表情丰富的虚拟形象)，并通过参与式设计让聋人用户参与以改进机器人表现力和可用性。

Abstract: Social robots are increasingly experimented in public and assistive settings,
but their accessibility for Deaf users remains quite underexplored. Italian
Sign Language (LIS) is a fully-fledged natural language that relies on complex
manual and non-manual components. Enabling robots to communicate using LIS
could foster more inclusive human robot interaction, especially in social
environments such as hospitals, airports, or educational settings. This study
investigates whether a commercial social robot, Pepper, can produce
intelligible LIS signs and short signed LIS sentences. With the help of a Deaf
student and his interpreter, an expert in LIS, we co-designed and implemented
52 LIS signs on Pepper using either manual animation techniques or a MATLAB
based inverse kinematics solver. We conducted a exploratory user study
involving 12 participants proficient in LIS, both Deaf and hearing.
Participants completed a questionnaire featuring 15 single-choice video-based
sign recognition tasks and 2 open-ended questions on short signed sentences.
Results shows that the majority of isolated signs were recognized correctly,
although full sentence recognition was significantly lower due to Pepper's
limited articulation and temporal constraints. Our findings demonstrate that
even commercially available social robots like Pepper can perform a subset of
LIS signs intelligibly, offering some opportunities for a more inclusive
interaction design. Future developments should address multi-modal enhancements
(e.g., screen-based support or expressive avatars) and involve Deaf users in
participatory design to refine robot expressivity and usability.

</details>


### [4] [Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision](https://arxiv.org/abs/2509.09893)
*Hanbit Oh,Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Yukiyasu Domae*

Main category: cs.RO

TL;DR: SART框架通过单次人类演示和自主轨迹增强，在保证安全的前提下高效学习机器人策略，显著减少人工干预和数据需求


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要大量演示或随机探索，存在安全风险且需要频繁人工重置环境，特别是在空间受限任务中容易发生碰撞

Method: 两阶段框架：1）单次人类演示并标注关键路径点的精度边界；2）机器人在边界内自主生成多样化、无碰撞轨迹并与原始演示连接

Result: 在仿真和真实机器人操作任务中，SART相比仅使用人类演示训练的策略获得了显著更高的成功率

Conclusion: SART通过最小化人工干预同时确保安全，有效提高了数据收集效率，为单次演示学习提供了可行的解决方案

Abstract: Imitation learning is a promising paradigm for training robot agents;
however, standard approaches typically require substantial data acquisition --
via numerous demonstrations or random exploration -- to ensure reliable
performance. Although exploration reduces human effort, it lacks safety
guarantees and often results in frequent collisions -- particularly in
clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual
environmental resets and imposing additional human burden. This study proposes
Self-Augmented Robot Trajectory (SART), a framework that enables policy
learning from a single human demonstration, while safely expanding the dataset
through autonomous augmentation. SART consists of two stages: (1) human
teaching only once, where a single demonstration is provided and precision
boundaries -- represented as spheres around key waypoints -- are annotated,
followed by one environment reset; (2) robot self-augmentation, where the robot
generates diverse, collision-free trajectories within these boundaries and
reconnects to the original demonstration. This design improves the data
collection efficiency by minimizing human effort while ensuring safety.
Extensive evaluations in simulation and real-world manipulation tasks show that
SART achieves substantially higher success rates than policies trained solely
on human-collected demonstrations. Video results available at
https://sites.google.com/view/sart-il .

</details>


### [5] [Detection of Anomalous Behavior in Robot Systems Based on Machine Learning](https://arxiv.org/abs/2509.09953)
*Mahfuzul I. Nissan,Sharmin Aktar*

Main category: cs.RO

TL;DR: 基于机器学习的机器人系统日志异常检测方法，比较了逻辑回归、支持向量机和自编码器在不同机器人场景下的性能表现


<details>
  <summary>Details</summary>
Motivation: 确保机器人系统的安全可靠运行至关重要，尽管有严格的设计和工程实践，系统仍可能出现故障导致安全风险

Method: 使用CoppeliaSim收集两种不同场景的日志数据，比较评估逻辑回归、支持向量机和自编码器等多种机器学习模型

Result: 在四旋翼无人机场景中逻辑回归表现最佳，在Pioneer机器人场景中自编码器效果最好，表明最优模型选择取决于具体应用场景

Conclusion: 研究强调了比较方法的价值，并展示了自编码器在检测复杂机器人系统异常方面的特殊优势，模型选择需要根据具体应用场景而定

Abstract: Ensuring the safe and reliable operation of robotic systems is paramount to
prevent potential disasters and safeguard human well-being. Despite rigorous
design and engineering practices, these systems can still experience
malfunctions, leading to safety risks. In this study, we present a machine
learning-based approach for detecting anomalies in system logs to enhance the
safety and reliability of robotic systems. We collected logs from two distinct
scenarios using CoppeliaSim and comparatively evaluated several machine
learning models, including Logistic Regression (LR), Support Vector Machine
(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context
(Context 1) and a Pioneer robot context (Context 2). Results showed that while
LR demonstrated superior performance in Context 1, the Autoencoder model proved
to be the most effective in Context 2. This highlights that the optimal model
choice is context-dependent, likely due to the varying complexity of anomalies
across different robotic platforms. This research underscores the value of a
comparative approach and demonstrates the particular strengths of autoencoders
for detecting complex anomalies in robotic systems.

</details>


### [6] [Gaussian path model library for intuitive robot motion programming by demonstration](https://arxiv.org/abs/2509.10007)
*Samuli Soutukorva,Markku Suomalainen,Martin Kollingbaum,Tapio Heikkilä*

Main category: cs.RO

TL;DR: 提出基于高斯路径模型的系统，从教学数据生成路径形状模型，用于分类人类演示路径并实现直观的机器人运动编程


<details>
  <summary>Details</summary>
Motivation: 通过建立多种形状的高斯路径模型库，使人类演示能够用于直观的机器人运动编程，提高编程效率和自然性

Method: 从教学数据生成高斯路径模型，建立多形状模型库进行分类，并通过几何分析实现现有模型的演示修改

Result: 开发了完整的系统，能够生成、分类和修改高斯路径模型，支持人类演示到机器人运动的转换

Conclusion: 高斯路径模型系统为基于演示的机器人编程提供了有效方法，通过模型库和几何修改实现了直观的运动编程

Abstract: This paper presents a system for generating Gaussian path models from
teaching data representing the path shape. In addition, methods for using these
path models to classify human demonstrations of paths are introduced. By
generating a library of multiple Gaussian path models of various shapes, human
demonstrations can be used for intuitive robot motion programming. A method for
modifying existing Gaussian path models by demonstration through geometric
analysis is also presented.

</details>


### [7] [Towards simulation-based optimization of compliant fingers for high-speed connector assembly](https://arxiv.org/abs/2509.10012)
*Richard Matthias Hartisch,Alexander Rother,Jörg Krüger,Kevin Haninger*

Main category: cs.RO

TL;DR: 提出基于仿真的柔顺机构设计工具，通过优化柔顺手指参数来提高插入任务中的容错窗口和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 机械柔顺性是动态接触丰富操作的关键设计参数，传统设计方法要么依赖硬件迭代耗时，要么使用简化模型无法处理复杂操作任务目标

Method: 开发基于仿真的设计工具，考虑接触和摩擦建模，优化结构化柔顺手指的设计参数以减少插入任务中的失败情况

Result: 优化参数可将容错范围提高2.29倍，能够补偿高达8.6mm的工作件变化，但趋势具有任务特异性

Conclusion: 柔顺机构设计需要针对特定应用考虑几何和动力学特性，仿真工具能够有效支持这种应用特定的设计优化

Abstract: Mechanical compliance is a key design parameter for dynamic contact-rich
manipulation, affecting task success and safety robustness over contact
geometry variation. Design of soft robotic structures, such as compliant
fingers, requires choosing design parameters which affect geometry and
stiffness, and therefore manipulation performance and robustness. Today, these
parameters are chosen through either hardware iteration, which takes
significant development time, or simplified models (e.g. planar), which can't
address complex manipulation task objectives. Improvements in dynamic
simulation, especially with contact and friction modeling, present a potential
design tool for mechanical compliance. We propose a simulation-based design
tool for compliant mechanisms which allows design with respect to task-level
objectives, such as success rate. This is applied to optimize design parameters
of a structured compliant finger to reduce failure cases inside a tolerance
window in insertion tasks. The improvement in robustness is then validated on a
real robot using tasks from the benchmark NIST task board. The finger stiffness
affects the tolerance window: optimized parameters can increase tolerable
ranges by a factor of 2.29, with workpiece variation up to 8.6 mm being
compensated. However, the trends remain task-specific. In some tasks, the
highest stiffness yields the widest tolerable range, whereas in others the
opposite is observed, motivating need for design tools which can consider
application-specific geometry and dynamics.

</details>


### [8] [Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping](https://arxiv.org/abs/2509.10032)
*Marawan Khalil,Fabian Arzberger,Andreas Nüchter*

Main category: cs.RO

TL;DR: 本文提出了两种球形测绘系统（轻量非驱动型和驱动型），评估了在球形机器人高动态运动下LIO算法的性能退化问题


<details>
  <summary>Details</summary>
Motivation: 球形机器人在危险或受限环境中具有独特优势，但其高动态运动可能影响测绘精度，需要评估现有LIO算法在这种特殊运动模式下的表现

Method: 开发两种球形测绘系统（非驱动和驱动型），配备Livox Mid-360固态LiDAR，在资源受限硬件上运行LIO算法，通过对比生成点云与地面真实地图来评估精度

Result: 最先进的LIO算法性能因球形运动引入的高动态性而恶化，导致全局不一致的地图和有时不可恢复的漂移

Conclusion: 球形机器人的特殊运动模式对现有LIO算法提出了挑战，需要开发更适合高动态球形运动的定位与建图算法

Abstract: Spherical robots offer unique advantages for mapping applications in
hazardous or confined environments, thanks to their protective shells and
omnidirectional mobility. This work presents two complementary spherical
mapping systems: a lightweight, non-actuated design and an actuated variant
featuring internal pendulum-driven locomotion. Both systems are equipped with a
Livox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)
algorithms on resource-constrained hardware. We assess the mapping accuracy of
these systems by comparing the resulting 3D point-clouds from the LIO
algorithms to a ground truth map. The results indicate that the performance of
state-of-the-art LIO algorithms deteriorates due to the high dynamic movement
introduced by the spherical locomotion, leading to globally inconsistent maps
and sometimes unrecoverable drift.

</details>


### [9] [TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model](https://arxiv.org/abs/2509.10063)
*Xiyan Huang,Zhe Xu,Chenxi Xiao*

Main category: cs.RO

TL;DR: TwinTac系统结合物理触觉传感器及其数字孪生模型，解决了触觉感知在机器人技能学习中的仿真缺失问题，通过真实到仿真的方法实现跨域数据一致性。


<details>
  <summary>Details</summary>
Motivation: 机器人技能学习通常依赖仿真生成交互数据，但触觉传感器的仿真模型缺失阻碍了触觉感知在策略学习中的应用，限制了触觉驱动策略的发展。

Method: 设计高灵敏度物理触觉传感器，采用真实到仿真方法收集同步跨域数据（有限元结果和物理传感器输出），训练神经网络将仿真数据映射到真实传感器响应。

Result: 实验评估显示物理传感器具有高灵敏度，数字孪生能一致复现物理传感器输出；物体分类任务证明仿真数据能有效增强真实数据，提高准确率。

Conclusion: TwinTac系统成功弥合了跨域学习任务的差距，展示了数字孪生触觉传感器在机器人技能学习中的潜力。

Abstract: Robot skill acquisition processes driven by reinforcement learning often rely
on simulations to efficiently generate large-scale interaction data. However,
the absence of simulation models for tactile sensors has hindered the use of
tactile sensing in such skill learning processes, limiting the development of
effective policies driven by tactile perception. To bridge this gap, we present
TwinTac, a system that combines the design of a physical tactile sensor with
its digital twin model. Our hardware sensor is designed for high sensitivity
and a wide measurement range, enabling high quality sensing data essential for
object interaction tasks. Building upon the hardware sensor, we develop the
digital twin model using a real-to-sim approach. This involves collecting
synchronized cross-domain data, including finite element method results and the
physical sensor's outputs, and then training neural networks to map simulated
data to real sensor responses. Through experimental evaluation, we
characterized the sensitivity of the physical sensor and demonstrated the
consistency of the digital twin in replicating the physical sensor's output.
Furthermore, by conducting an object classification task, we showed that
simulation data generated by our digital twin sensor can effectively augment
real-world data, leading to improved accuracy. These results highlight
TwinTac's potential to bridge the gap in cross-domain learning tasks.

</details>


### [10] [Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation](https://arxiv.org/abs/2509.10065)
*Hauzi Cao,Jiahao Shen,Zhengzhen Li,Qinquan Ren,Shiyu Zhao*

Main category: cs.RO

TL;DR: 提出了一种基于预设轨迹和二次规划的空中机械臂运动学跟踪控制框架，确保末端执行器在预设时间内到达目标位置并满足物理约束


<details>
  <summary>Details</summary>
Motivation: 现有运动学跟踪控制方法（如PD反馈或跟踪误差反馈策略）无法在指定时间约束内实现跟踪目标，需要解决这一局限性

Method: 包含两个关键组件：基于用户定义预设轨迹的末端执行器跟踪控制，以及基于二次规划的参考分配方法，同时考虑空中机械臂的物理约束

Result: 通过三个实验验证，结果表明该方法能有效保证目标位置在预设时间内到达，且跟踪误差保持在反映任务要求的性能包络内

Conclusion: 所提出的控制框架具有吸引人的特性，能够确保末端执行器在预设时间内到达期望位置，同时通过二次规划避免违反物理限制的解决方案

Abstract: This paper studies the kinematic tracking control problem for aerial
manipulators. Existing kinematic tracking control methods, which typically
employ proportional-derivative feedback or tracking-error-based feedback
strategies, may fail to achieve tracking objectives within specified time
constraints. To address this limitation, we propose a novel control framework
comprising two key components: end-effector tracking control based on a
user-defined preset trajectory and quadratic programming-based reference
allocation. Compared with state-of-the-art approaches, the proposed method has
several attractive features. First, it ensures that the end-effector reaches
the desired position within a preset time while keeping the tracking error
within a performance envelope that reflects task requirements. Second,
quadratic programming is employed to allocate the references of the quadcopter
base and the Delta arm, while considering the physical constraints of the
aerial manipulator, thus preventing solutions that may violate physical
limitations. The proposed approach is validated through three experiments.
Experimental results demonstrate the effectiveness of the proposed algorithm
and its capability to guarantee that the target position is reached within the
preset time.

</details>


### [11] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: 提出了HHI-Assist数据集和基于Transformer的条件去噪扩散模型，用于预测物理交互场景中的人体运动，以提升辅助机器人的安全性


<details>
  <summary>Details</summary>
Motivation: 劳动力短缺和人口老龄化需要辅助机器人，但机器人需要准确的人体运动预测来提供安全响应，而物理交互中的耦合动力学复杂性使这成为挑战

Method: 收集人类-人类交互辅助任务的动作捕捉数据集(HHI-Assist)，并开发基于条件Transformer的去噪扩散模型来预测交互代理的姿态

Result: 模型有效捕捉了护理者和被护理者之间的耦合动力学，相比基线方法有改进，并在未见场景中表现出强泛化能力

Conclusion: 通过推进交互感知的运动预测和引入新数据集，这项工作有潜力显著增强机器人辅助策略

Abstract: The increasing labor shortage and aging population underline the need for
assistive robots to support human care recipients. To enable safe and
responsive assistance, robots require accurate human motion prediction in
physical interaction scenarios. However, this remains a challenging task due to
the variability of assistive settings and the complexity of coupled dynamics in
physical interactions. In this work, we address these challenges through two
key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of
human-human interactions in assistive tasks; and (2) a conditional
Transformer-based denoising diffusion model for predicting the poses of
interacting agents. Our model effectively captures the coupled dynamics between
caregivers and care receivers, demonstrating improvements over baselines and
strong generalization to unseen scenarios. By advancing interaction-aware
motion prediction and introducing a new dataset, our work has the potential to
significantly enhance robotic assistance policies. The dataset and code are
available at: https://sites.google.com/view/hhi-assist/home

</details>


### [12] [Efficient Learning-Based Control of a Legged Robot in Lunar Gravity](https://arxiv.org/abs/2509.10128)
*Philip Arm,Oliver Fischer,Joseph Church,Adrian Fuhrer,Hendrik Kolvenbach,Marco Hutter*

Main category: cs.RO

TL;DR: 提出基于强化学习的足式机器人控制方法，通过重力缩放功率优化奖励函数，实现从月球重力到超地球重力环境的高效运动控制，功率消耗降低23-36%。


<details>
  <summary>Details</summary>
Motivation: 行星探测机器人面临严格的功率和热预算限制，需要能在多种重力环境下高效工作的能量优化控制方法。

Method: 使用强化学习框架，设计重力缩放的功率优化奖励函数，开发运动控制和基座姿态控制器，并设计恒力弹簧卸载系统进行月球重力实验验证。

Result: 在地球重力下功率消耗23.4W（比基线降低23%），月球重力下12.2W（降低36%），成功在1.62-19.62 m/s²重力范围内验证控制器的可扩展性。

Conclusion: 该方法为足式机器人提供了跨多种重力环境开发功率高效运动控制器的可扩展解决方案，适用于行星探测任务。

Abstract: Legged robots are promising candidates for exploring challenging areas on
low-gravity bodies such as the Moon, Mars, or asteroids, thanks to their
advanced mobility on unstructured terrain. However, as planetary robots' power
and thermal budgets are highly restricted, these robots need energy-efficient
control approaches that easily transfer to multiple gravity environments. In
this work, we introduce a reinforcement learning-based control approach for
legged robots with gravity-scaled power-optimized reward functions. We use our
approach to develop and validate a locomotion controller and a base pose
controller in gravity environments from lunar gravity (1.62 m/s2) to a
hypothetical super-Earth (19.62 m/s2). Our approach successfully scales across
these gravity levels for locomotion and base pose control with the
gravity-scaled reward functions. The power-optimized locomotion controller
reached a power consumption for locomotion of 23.4 W in Earth gravity on a
15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.
Additionally, we designed a constant-force spring offload system that allowed
us to conduct real-world experiments on legged locomotion in lunar gravity. In
lunar gravity, the power-optimized control policy reached 12.2 W, 36 % less
than a baseline controller which is not optimized for power efficiency. Our
method provides a scalable approach to developing power-efficient locomotion
controllers for legged robots across multiple gravity levels.

</details>


### [13] [CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion](https://arxiv.org/abs/2509.10139)
*Santiago Montiel-Marín,Angel Llamazares,Miguel Antunes-García,Fabio Sánchez-García,Luis M. Bergasa*

Main category: cs.RO

TL;DR: CaR1是一种新颖的相机-雷达融合架构，用于BEV车辆分割，通过网格化雷达编码和自适应融合机制，在nuScenes数据集上达到57.6 IoU的竞争性性能


<details>
  <summary>Details</summary>
Motivation: 相机-雷达融合为自动驾驶系统提供了比LiDAR更具成本效益的替代方案，相机提供丰富的语义信息但深度不可靠，雷达提供稀疏但可靠的位置和运动信息

Method: 基于BEVFusion构建，采用网格化雷达编码将点云离散化为结构化BEV特征，并使用自适应融合机制动态平衡传感器贡献

Result: 在nuScenes数据集上实现了竞争性的分割性能（57.6 IoU），与最先进方法相当

Conclusion: CaR1证明了相机-雷达融合在BEV车辆分割任务中的有效性，为低成本自动驾驶感知提供了可行方案

Abstract: Camera-radar fusion offers a robust and cost-effective alternative to
LiDAR-based autonomous driving systems by combining complementary sensing
capabilities: cameras provide rich semantic cues but unreliable depth, while
radar delivers sparse yet reliable position and motion information. We
introduce CaR1, a novel camera-radar fusion architecture for BEV vehicle
segmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar
encoding that discretizes point clouds into structured BEV features and an
adaptive fusion mechanism that dynamically balances sensor contributions.
Experiments on nuScenes demonstrate competitive segmentation performance (57.6
IoU), on par with state-of-the-art methods. Code is publicly available
\href{https://www.github.com/santimontiel/car1}{online}.

</details>


### [14] [DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning](https://arxiv.org/abs/2509.10247)
*Xinhong Zhang,Runqing Wang,Yunfan Ren,Jian Sun,Hao Fang,Jie Chen,Gang Wang*

Main category: cs.RO

TL;DR: DiffAero是一个轻量级、GPU加速的完全可微分四旋翼飞行器仿真框架，支持环境级和智能体级并行，集成多种动力学模型和传感器，在消费级硬件上数小时即可学习鲁棒飞行策略


<details>
  <summary>Details</summary>
Motivation: 现有仿真器存在CPU-GPU数据传输瓶颈，无法高效支持可微分和混合学习算法的研究，需要开发高性能的GPU原生仿真平台来加速四旋翼控制策略学习

Method: 开发完全并行化的GPU原生仿真框架，支持多种动力学模型（IMU、深度相机、LiDAR）和飞行任务，消除CPU-GPU数据传输瓶颈，提供统一的可微分训练接口

Result: 相比现有仿真器，DiffAero在仿真吞吐量上实现了数量级的提升，结合混合学习算法可在消费级硬件上数小时内学习到鲁棒的飞行策略，并通过真实飞行实验验证

Conclusion: DiffAero不仅提供了高性能仿真，还作为研究平台支持可微分和混合学习算法的探索，为四旋翼控制策略学习提供了高效的解决方案

Abstract: This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully
differentiable simulation framework designed for efficient quadrotor control
policy learning. DiffAero supports both environment-level and agent-level
parallelism and integrates multiple dynamics models, customizable sensor stacks
(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,
GPU-native training interface. By fully parallelizing both physics and
rendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and
delivers orders-of-magnitude improvements in simulation throughput. In contrast
to existing simulators, DiffAero not only provides high-performance simulation
but also serves as a research platform for exploring differentiable and hybrid
learning algorithms. Extensive benchmarks and real-world flight experiments
demonstrate that DiffAero and hybrid learning algorithms combined can learn
robust flight policies in hours on consumer-grade hardware. The code is
available at https://github.com/flyingbitac/diffaero.

</details>


### [15] [GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning](https://arxiv.org/abs/2509.10305)
*Yutong Shen,Ruizhe Xia,Bokai Yan,Shunqi zhang,Pengrui Xiang,Sicheng He,Yixin Xu*

Main category: cs.RO

TL;DR: GundamQ是一个多尺度时空Q网络，通过时空感知模块和自适应策略优化模块，解决了机器人路径规划中时间依赖性建模不足和探索-利用平衡效率低的问题，在动态环境中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度强化学习的路径规划方法存在两个根本局限：多尺度时间依赖性建模不足导致动态场景适应性差，以及探索-利用平衡效率低导致路径质量下降。

Method: 提出GundamQ框架，包含两个关键模块：(i)时空感知模块分层提取多粒度空间特征和多尺度时间依赖性；(ii)自适应策略优化模块平衡探索与利用，通过约束策略更新优化路径平滑度和碰撞概率。

Result: 在动态环境实验中，GundamQ实现了15.3%的成功率提升和21.7%的整体路径质量提高，显著优于现有最先进方法。

Conclusion: GundamQ通过改进时空感知和策略优化，有效解决了动态环境中机器人路径规划的关键挑战，为复杂环境下的自主导航提供了有力解决方案。

Abstract: In dynamic and uncertain environments, robotic path planning demands accurate
spatiotemporal environment understanding combined with robust decision-making
under partial observability. However, current deep reinforcement learning-based
path planning methods face two fundamental limitations: (1) insufficient
modeling of multi-scale temporal dependencies, resulting in suboptimal
adaptability in dynamic scenarios, and (2) inefficient exploration-exploitation
balance, leading to degraded path quality. To address these challenges, we
propose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path
Planning. The framework comprises two key modules: (i) the Spatiotemporal
Perception module, which hierarchically extracts multi-granularity spatial
features and multi-scale temporal dependencies ranging from instantaneous to
extended time horizons, thereby improving perception accuracy in dynamic
environments; and (ii) the Adaptive Policy Optimization module, which balances
exploration and exploitation during training while optimizing for smoothness
and collision probability through constrained policy updates. Experiments in
dynamic environments demonstrate that GundamQ achieves a 15.3\% improvement in
success rate and a 21.7\% increase in overall path quality, significantly
outperforming existing state-of-the-art methods.

</details>


### [16] [Robot guide with multi-agent control and automatic scenario generation with LLM](https://arxiv.org/abs/2509.10317)
*Elizaveta D. Moskovskaya,Anton D. Moscowsky*

Main category: cs.RO

TL;DR: 开发了一种混合控制架构，结合多智能体资源管理系统和基于大语言模型的自动行为场景生成，用于人形导游机器人，以克服传统系统手动配置、灵活性低和机器人行为不自然的问题。


<details>
  <summary>Details</summary>
Motivation: 传统导游机器人系统依赖手动调整行为场景，存在手动配置繁琐、灵活性低和机器人行为不够自然的问题，需要一种自动化且更自然的控制方法。

Method: 采用两阶段生成过程：首先生成风格化叙述，然后将非语言动作标签整合到文本中。多智能体系统确保并行动作执行时的协调和冲突解决，并在主要操作完成后维持默认行为。

Result: 试验结果表明，该方法在自动化和社会机器人控制系统扩展方面具有潜力，能够实现更自然的机器人行为。

Conclusion: 提出的混合控制架构成功克服了传统系统的局限性，为社交机器人控制系统的自动化和规模化提供了有效解决方案。

Abstract: The work describes the development of a hybrid control architecture for an
anthropomorphic tour guide robot, combining a multi-agent resource management
system with automatic behavior scenario generation based on large language
models. The proposed approach aims to overcome the limitations of traditional
systems, which rely on manual tuning of behavior scenarios. These limitations
include manual configuration, low flexibility, and lack of naturalness in robot
behavior. The process of preparing tour scenarios is implemented through a
two-stage generation: first, a stylized narrative is created, then non-verbal
action tags are integrated into the text. The multi-agent system ensures
coordination and conflict resolution during the execution of parallel actions,
as well as maintaining default behavior after the completion of main
operations, contributing to more natural robot behavior. The results obtained
from the trial demonstrate the potential of the proposed approach for
automating and scaling social robot control systems.

</details>


### [17] [Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System](https://arxiv.org/abs/2509.10349)
*Weiyan Lu,Huizhe Li,Yuhao Fang,Zhexuan Zhou,Junda Wu,Yude Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Unmanned aerial vehicles (UAVs) with suspended payloads offer significant
advantages for aerial transportation in complex and cluttered environments.
However, existing systems face critical limitations, including unreliable
perception of the cable-payload dynamics, inefficient planning in large-scale
environments, and the inability to guarantee whole-body safety under cable
bending and external disturbances. This paper presents Acetrans, an Autonomous,
Corridor-based, and Efficient UAV suspended transport system that addresses
these challenges through a unified perception, planning, and control framework.
A LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and
cable shape under taut and bent modes, enabling robust whole-body state
estimation and real-time filtering of cable point clouds. To enhance planning
scalability, we introduce the Multi-size-Aware Configuration-space Iterative
Regional Inflation (MACIRI) algorithm, which generates safe flight corridors
while accounting for varying UAV and payload geometries. A spatio-temporal,
corridor-constrained trajectory optimization scheme is then developed to ensure
dynamically feasible and collision-free trajectories. Finally, a nonlinear
model predictive controller (NMPC) augmented with cable-bending constraints
provides robust whole-body safety during execution. Simulation and experimental
results validate the effectiveness of Acetrans, demonstrating substantial
improvements in perception accuracy, planning efficiency, and control safety
compared to state-of-the-art methods.

</details>


### [18] [Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States](https://arxiv.org/abs/2509.10405)
*Nicholas Carlotti,Mirko Nava,Alessandro Giusti*

Main category: cs.RO

TL;DR: 提出了一种无需位姿标签或机器人形状先验知识的单目RGB相对位姿估计模型，通过LED状态预测任务实现自监督学习


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要位姿标签或CAD模型的监督依赖问题，实现无需外部基础设施或人工监督的自主训练

Method: 使用配备多个LED的机器人，通过预测图像中每个LED状态的任务来学习机器人位置、距离和相对方位，训练时已知LED状态和大致视角方向

Result: 与需要监督的方法性能相当，具有良好的领域泛化能力，并能处理多机器人位姿估计

Conclusion: 该方法证明了通过简单的视觉线索（LED状态）可以实现有效的自监督相对位姿估计，为机器人自主导航提供了新思路

Abstract: We introduce a model for monocular RGB relative pose estimation of a ground
robot that trains from scratch without pose labels nor prior knowledge about
the robot's shape or appearance. At training time, we assume: (i) a robot
fitted with multiple LEDs, whose states are independent and known at each
frame; (ii) knowledge of the approximate viewing direction of each LED; and
(iii) availability of a calibration image with a known target distance, to
address the ambiguity of monocular depth estimation. Training data is collected
by a pair of robots moving randomly without needing external infrastructure or
human supervision. Our model trains on the task of predicting from an image the
state of each LED on the robot. In doing so, it learns to predict the position
of the robot in the image, its distance, and its relative bearing. At inference
time, the state of the LEDs is unknown, can be arbitrary, and does not affect
the pose estimation performance. Quantitative experiments indicate that our
approach: is competitive with SoA approaches that require supervision from pose
labels or a CAD model of the robot; generalizes to different domains; and
handles multi-robot pose estimation.

</details>


### [19] [TASC: Task-Aware Shared Control for Teleoperated Manipulation](https://arxiv.org/abs/2509.10416)
*Ze Fu,Pinhao Song,Yutong Hu,Renaud Detry*

Main category: cs.RO

TL;DR: TASC是一个任务感知共享控制框架，通过视觉输入构建开放词汇交互图来推断用户意图，提供旋转辅助，无需预定义知识即可支持日常操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决通用长时域共享控制中的两个关键挑战：理解和推断任务级用户意图，以及在不同对象和任务间泛化辅助功能。

Method: 构建开放词汇交互图表示功能对象关系，使用视觉语言模型预测空间约束，在抓取和对象交互过程中提供旋转辅助的共享控制策略。

Result: 在仿真和真实世界实验中，TASC相比先前方法提高了任务效率并减少了用户输入工作量。

Conclusion: 这是第一个支持日常操作任务且具有零样本泛化能力的共享控制框架，实现了无需预定义知识的通用操作辅助。

Abstract: We present TASC, a Task-Aware Shared Control framework for teleoperated
manipulation that infers task-level user intent and provides assistance
throughout the task. To support everyday tasks without predefined knowledge,
TASC constructs an open-vocabulary interaction graph from visual input to
represent functional object relationships, and infers user intent accordingly.
A shared control policy then provides rotation assistance during both grasping
and object interaction, guided by spatial constraints predicted by a
vision-language model. Our method addresses two key challenges in
general-purpose, long-horizon shared control: (1) understanding and inferring
task-level user intent, and (2) generalizing assistance across diverse objects
and tasks. Experiments in both simulation and the real world demonstrate that
TASC improves task efficiency and reduces user input effort compared to prior
methods. To the best of our knowledge, this is the first shared control
framework that supports everyday manipulation tasks with zero-shot
generalization. The code that supports our experiments is publicly available at
https://github.com/fitz0401/tasc.

</details>


### [20] [DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training](https://arxiv.org/abs/2509.10426)
*Jianxin Shi,Zengqi Peng,Xiaolong Chen,Tianyu Wo,Jun Ma*

Main category: cs.RO

TL;DR: DECAMP是一个解耦的上下文感知预训练框架，用于多智能体运动预测，通过分离行为模式学习和潜在特征重建，在Argoverse 2基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在标注数据稀缺和多智能体预测场景中表现不佳，需要解决表示学习与预训练任务纠缠的问题。

Method: 提出解耦的上下文感知预训练框架，将行为模式学习与潜在特征重建分离，结合上下文感知表示学习和协作空间运动预训练任务。

Result: 在Argoverse 2基准测试中展现出优越性能，证明在多智能体运动预测中的有效性。

Conclusion: DECAMP是首个用于自动驾驶多智能体运动预测的上下文自动编码器框架，代码和模型将公开。

Abstract: Trajectory prediction is a critical component of autonomous driving,
essential for ensuring both safety and efficiency on the road. However,
traditional approaches often struggle with the scarcity of labeled data and
exhibit suboptimal performance in multi-agent prediction scenarios. To address
these challenges, we introduce a disentangled context-aware pre-training
framework for multi-agent motion prediction, named DECAMP. Unlike existing
methods that entangle representation learning with pretext tasks, our framework
decouples behavior pattern learning from latent feature reconstruction,
prioritizing interpretable dynamics and thereby enhancing scene representation
for downstream prediction. Additionally, our framework incorporates
context-aware representation learning alongside collaborative spatial-motion
pretext tasks, which enables joint optimization of structural and intentional
reasoning while capturing the underlying dynamic intentions. Our experiments on
the Argoverse 2 benchmark showcase the superior performance of our method, and
the results attained underscore its effectiveness in multi-agent motion
forecasting. To the best of our knowledge, this is the first context
autoencoder framework for multi-agent motion forecasting in autonomous driving.
The code and models will be made publicly available.

</details>


### [21] [Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction](https://arxiv.org/abs/2509.10444)
*Chaerim Moon,Joohyung Kim*

Main category: cs.RO

TL;DR: 本文提出了一种运动规划层概念，用于减少超级机器人肢体(SRLs)操作时产生的力矩，从而改善人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 作为可穿戴设备，超级机器人肢体在操作时产生的力矩会作为外部扭矩作用于人体。当力矩增大时，需要更多肌肉单元来维持平衡，这会导致肌肉零空间减少，影响人机交互效果。

Method: 开发了一个运动规划层，通过修改给定轨迹，在期望的角加速度和位置偏差限制内优化运动，以减少产生的力矩。使用简化的人体和机器人系统模型进行仿真验证。

Result: 仿真结果表明，该运动规划方法能够有效减少操作时产生的力矩。

Conclusion: 提出的运动规划层概念能够减少超级机器人肢体产生的力矩，从而增强人机交互性能，为可穿戴机器人系统的优化提供了有效解决方案。

Abstract: Supernumerary Robotic Limbs (SRLs) can enhance human capability within close
proximity. However, as a wearable device, the generated moment from its
operation acts on the human body as an external torque. When the moments
increase, more muscle units are activated for balancing, and it can result in
reduced muscular null space. Therefore, this paper suggests a concept of a
motion planning layer that reduces the generated moment for enhanced
Human-Robot Interaction. It modifies given trajectories with desirable angular
acceleration and position deviation limits. Its performance to reduce the
moment is demonstrated through the simulation, which uses simplified human and
robotic system models.

</details>


### [22] [GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/abs/2509.10454)
*Hang Yin,Haoyu Wei,Xiuwei Xu,Wenxuan Guo,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出了一种无需训练的视觉语言导航框架，通过将导航指令分解为空间约束图并进行优化求解，实现零样本连续环境导航


<details>
  <summary>Details</summary>
Motivation: 现有零样本VLN方法主要针对离散环境或在连续模拟器中需要无监督训练，难以泛化到真实世界场景，需要开发无需训练即可在连续环境中工作的框架

Method: 将导航指令分解为显式空间约束，构建空间约束库，将人类指令解析为有向无环图，通过约束求解器进行图约束优化确定路径点位置，采用导航树和回溯机制处理无解或多解情况

Result: 在标准基准测试中相比最先进的零样本VLN方法显著提高了成功率和导航效率，真实世界实验表明能有效泛化到新环境和指令集

Conclusion: 该框架为构建更鲁棒和自主的导航系统铺平了道路，实现了无需训练即可在连续环境中进行零样本视觉语言导航

Abstract: In this paper, we propose a training-free framework for vision-and-language
navigation (VLN). Existing zero-shot VLN methods are mainly designed for
discrete environments or involve unsupervised training in continuous simulator
environments, which makes it challenging to generalize and deploy them in
real-world scenarios. To achieve a training-free framework in continuous
environments, our framework formulates navigation guidance as graph constraint
optimization by decomposing instructions into explicit spatial constraints. The
constraint-driven paradigm decodes spatial semantics through constraint
solving, enabling zero-shot adaptation to unseen environments. Specifically, we
construct a spatial constraint library covering all types of spatial
relationship mentioned in VLN instructions. The human instruction is decomposed
into a directed acyclic graph, with waypoint nodes, object nodes and edges,
which are used as queries to retrieve the library to build the graph
constraints. The graph constraint optimization is solved by the constraint
solver to determine the positions of waypoints, obtaining the robot's
navigation path and final goal. To handle cases of no solution or multiple
solutions, we construct a navigation tree and the backtracking mechanism.
Extensive experiments on standard benchmarks demonstrate significant
improvements in success rate and navigation efficiency compared to
state-of-the-art zero-shot VLN methods. We further conduct real-world
experiments to show that our framework can effectively generalize to new
environments and instruction sets, paving the way for a more robust and
autonomous navigation framework.

</details>
