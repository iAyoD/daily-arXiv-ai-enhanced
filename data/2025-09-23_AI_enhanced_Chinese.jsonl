{"id": "2509.16261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16261", "abs": "https://arxiv.org/abs/2509.16261", "authors": ["Shuocheng Yang", "Zikun Xu", "Jiahao Wang", "Shahid Nawaz", "Jianqiang Wang", "Shaobing Xu"], "title": "RaFD: Flow-Guided Radar Detection for Robust Autonomous Driving", "comment": null, "summary": "Radar has shown strong potential for robust perception in autonomous driving;\nhowever, raw radar images are frequently degraded by noise and \"ghost\"\nartifacts, making object detection based solely on semantic features highly\nchallenging. To address this limitation, we introduce RaFD, a radar-based\nobject detection framework that estimates inter-frame bird's-eye-view (BEV)\nflow and leverages the resulting geometric cues to enhance detection accuracy.\nSpecifically, we design a supervised flow estimation auxiliary task that is\njointly trained with the detection network. The estimated flow is further\nutilized to guide feature propagation from the previous frame to the current\none. Our flow-guided, radar-only detector achieves achieves state-of-the-art\nperformance on the RADIATE dataset, underscoring the importance of\nincorporating geometric information to effectively interpret radar signals,\nwhich are inherently ambiguous in semantics.", "AI": {"tldr": "RaFD\u662f\u4e00\u4e2a\u57fa\u4e8e\u96f7\u8fbe\u7684\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1\u5e27\u95f4\u9e1f\u77b0\u56fe\u6d41\u5e76\u5229\u7528\u51e0\u4f55\u7ebf\u7d22\u6765\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5728RADIATE\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u539f\u59cb\u96f7\u8fbe\u56fe\u50cf\u7ecf\u5e38\u53d7\u5230\u566a\u58f0\u548c\"\u9b3c\u5f71\"\u4f2a\u5f71\u7684\u5f71\u54cd\uff0c\u4ec5\u57fa\u4e8e\u8bed\u4e49\u7279\u5f81\u7684\u76ee\u6807\u68c0\u6d4b\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5f15\u5165\u51e0\u4f55\u4fe1\u606f\u6765\u6709\u6548\u89e3\u91ca\u8bed\u4e49\u4e0a\u56fa\u6709\u7684\u6a21\u7cca\u96f7\u8fbe\u4fe1\u53f7\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u76d1\u7763\u6d41\u4f30\u8ba1\u8f85\u52a9\u4efb\u52a1\uff0c\u4e0e\u68c0\u6d4b\u7f51\u7edc\u8054\u5408\u8bad\u7ec3\u3002\u4f30\u8ba1\u7684\u6d41\u7528\u4e8e\u6307\u5bfc\u4ece\u524d\u4e00\u5e27\u5230\u5f53\u524d\u5e27\u7684\u7279\u5f81\u4f20\u64ad\uff0c\u5229\u7528\u51e0\u4f55\u7ebf\u7d22\u589e\u5f3a\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728RADIATE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5c06\u51e0\u4f55\u4fe1\u606f\u7eb3\u5165\u96f7\u8fbe\u4fe1\u53f7\u89e3\u91ca\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u4fe1\u606f\u7684\u6d41\u5f15\u5bfc\u96f7\u8fbe\u68c0\u6d4b\u5668\u80fd\u591f\u6709\u6548\u89e3\u51b3\u96f7\u8fbe\u4fe1\u53f7\u8bed\u4e49\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.16353", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16353", "abs": "https://arxiv.org/abs/2509.16353", "authors": ["Shaoting Peng", "Dakarai Crowder", "Wenzhen Yuan", "Katherine Driggs-Campbell"], "title": "Tactile-Based Human Intent Recognition for Robot Assistive Navigation", "comment": null, "summary": "Robot assistive navigation (RAN) is critical for enhancing the mobility and\nindependence of the growing population of mobility-impaired individuals.\nHowever, existing systems often rely on interfaces that fail to replicate the\nintuitive and efficient physical communication observed between a person and a\nhuman caregiver, limiting their effectiveness. In this paper, we introduce\nTac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a\nStretch 3 mobile manipulator to provide a more natural and efficient interface\nfor human navigational intent recognition. To robustly classify the tactile\ndata, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an\nalgorithm that explicitly models the sensor's cylindrical geometry and is\nconsequently robust to the natural rotational shifts present in a user's grasp.\nComprehensive experiments were conducted to demonstrate the effectiveness of\nour classification algorithm and the overall system. Results show that CK-SVM\nachieved superior classification accuracy on both simulated (97.1%) and\nreal-world (90.8%) datasets compared to four baseline models. Furthermore, a\npilot study confirmed that users more preferred the Tac-Nav tactile interface\nover conventional joystick and voice-based controls.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Tac-Nav\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\uff0c\u4f7f\u7528\u5706\u67f1\u5f62\u89e6\u89c9\u76ae\u80a4\u548cCK-SVM\u7b97\u6cd5\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\u7f3a\u4e4f\u4eba\u7c7b\u770b\u62a4\u8005\u4e0e\u7528\u6237\u4e4b\u95f4\u76f4\u89c2\u7684\u7269\u7406\u6c9f\u901a\u65b9\u5f0f\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6709\u6548\u6027\u3002", "method": "\u5f00\u53d1\u4e86Tac-Nav\u7cfb\u7edf\uff0c\u91c7\u7528\u5706\u67f1\u5f62\u89e6\u89c9\u76ae\u80a4\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5706\u67f1\u6838\u652f\u6301\u5411\u91cf\u673a(CK-SVM)\u7b97\u6cd5\u6765\u8bc6\u522b\u7528\u6237\u5bfc\u822a\u610f\u56fe\u3002", "result": "CK-SVM\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.1%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u523090.8%\uff0c\u4f18\u4e8e\u56db\u4e2a\u57fa\u7ebf\u6a21\u578b\u3002\u7528\u6237\u7814\u7a76\u663e\u793aTac-Nav\u6bd4\u4f20\u7edf\u64cd\u7eb5\u6746\u548c\u8bed\u97f3\u63a7\u5236\u66f4\u53d7\u6b22\u8fce\u3002", "conclusion": "Tac-Nav\u7cfb\u7edf\u901a\u8fc7\u89e6\u89c9\u754c\u9762\u548c\u51e0\u4f55\u611f\u77e5\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u7684\u81ea\u7136\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.16398", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16398", "abs": "https://arxiv.org/abs/2509.16398", "authors": ["Francesco Argenziano", "Miguel Saavedra-Ruiz", "Sacha Morin", "Daniele Nardi", "Liam Paull"], "title": "Dynamic Objects Relocalization in Changing Environments with Flow Matching", "comment": null, "summary": "Task and motion planning are long-standing challenges in robotics, especially\nwhen robots have to deal with dynamic environments exhibiting long-term\ndynamics, such as households or warehouses. In these environments, long-term\ndynamics mostly stem from human activities, since previously detected objects\ncan be moved or removed from the scene. This adds the necessity to find such\nobjects again before completing the designed task, increasing the risk of\nfailure due to missed relocalizations. However, in these settings, the nature\nof such human-object interactions is often overlooked, despite being governed\nby common habits and repetitive patterns. Our conjecture is that these cues can\nbe exploited to recover the most likely objects' positions in the scene,\nhelping to address the problem of unknown relocalization in changing\nenvironments. To this end we propose FlowMaps, a model based on Flow Matching\nthat is able to infer multimodal object locations over space and time. Our\nresults present statistical evidence to support our hypotheses, opening the way\nto more complex applications of our approach. The code is publically available\nat https://github.com/Fra-Tsuna/flowmaps", "AI": {"tldr": "FlowMaps\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63a8\u65ad\u591a\u6a21\u6001\u7269\u4f53\u4f4d\u7f6e\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u4e2d\u56e0\u7269\u4f53\u79fb\u52a8\u5bfc\u81f4\u7684\u91cd\u65b0\u5b9a\u4f4d\u95ee\u9898\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\uff08\u5982\u5bb6\u5ead\u6216\u4ed3\u5e93\uff09\u4e2d\uff0c\u4eba\u7c7b\u6d3b\u52a8\u5bfc\u81f4\u7269\u4f53\u4f4d\u7f6e\u9891\u7e41\u53d8\u5316\uff0c\u589e\u52a0\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u5931\u8d25\u7684\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\u7684\u89c4\u5f8b\u6027\u6a21\u5f0f\u3002", "method": "\u63d0\u51faFlowMaps\u6a21\u578b\uff0c\u5229\u7528\u6d41\u5339\u914d\u6280\u672f\u6765\u63a8\u65ad\u7269\u4f53\u5728\u65f6\u7a7a\u4e0a\u7684\u591a\u6a21\u6001\u4f4d\u7f6e\u5206\u5e03\uff0c\u80fd\u591f\u6355\u6349\u4eba\u7c7b\u6d3b\u52a8\u7684\u91cd\u590d\u6027\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63d0\u4f9b\u4e86\u7edf\u8ba1\u8bc1\u636e\u652f\u6301\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u53ef\u4ee5\u5229\u7528\u4eba\u7c7b\u4ea4\u4e92\u6a21\u5f0f\u6765\u9884\u6d4b\u7269\u4f53\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7269\u4f53\u91cd\u65b0\u5b9a\u4f4d\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4e3a\u66f4\u590d\u6742\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.16412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16412", "abs": "https://arxiv.org/abs/2509.16412", "authors": ["Zihao Deng", "Peng Gao", "Williard Joshua Jose", "Maggie Wigness", "John Rogers", "Brian Reily", "Christopher Reardon", "Hao Zhang"], "title": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation", "comment": null, "summary": "Coordinated multi-robot navigation is essential for robots to operate as a\nteam in diverse environments. During navigation, robot teams usually need to\nmaintain specific formations, such as circular formations to protect human\nteammates at the center. However, in complex scenarios such as narrow\ncorridors, rigidly preserving predefined formations can become infeasible.\nTherefore, robot teams must be capable of dynamically splitting into smaller\nsubteams and adaptively controlling the subteams to navigate through such\nscenarios while preserving formations. To enable this capability, we introduce\na novel method for SubTeaming and Adaptive Formation (STAF), which is built\nupon a unified hierarchical learning framework: (1) high-level deep graph cut\nfor team splitting, (2) intermediate-level graph learning for facilitating\ncoordinated navigation among subteams, and (3) low-level policy learning for\ncontrolling individual mobile robots to reach their goal positions while\navoiding collisions. To evaluate STAF, we conducted extensive experiments in\nboth indoor and outdoor environments using robotics simulations and physical\nrobot teams. Experimental results show that STAF enables the novel capability\nfor subteaming and adaptive formation control, and achieves promising\nperformance in coordinated multi-robot navigation through challenging\nscenarios. More details are available on the project website:\nhttps://hcrlab.gitlab.io/project/STAF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u8c03\u5bfc\u822a\u7684\u5b50\u56e2\u961f\u81ea\u9002\u5e94\u7f16\u961f\u65b9\u6cd5\uff08STAF\uff09\uff0c\u80fd\u591f\u5728\u590d\u6742\u573a\u666f\u4e2d\u52a8\u6001\u5206\u5272\u56e2\u961f\u5e76\u81ea\u9002\u5e94\u63a7\u5236\u7f16\u961f", "motivation": "\u591a\u673a\u5668\u4eba\u5bfc\u822a\u9700\u8981\u4fdd\u6301\u7279\u5b9a\u7f16\u961f\uff0c\u4f46\u5728\u72ed\u7a84\u8d70\u5eca\u7b49\u590d\u6742\u573a\u666f\u4e2d\uff0c\u521a\u6027\u4fdd\u6301\u9884\u5b9a\u4e49\u7f16\u961f\u53d8\u5f97\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u52a8\u6001\u5206\u5272\u56e2\u961f\u5e76\u81ea\u9002\u5e94\u63a7\u5236", "method": "\u57fa\u4e8e\u7edf\u4e00\u5206\u5c42\u5b66\u4e60\u6846\u67b6\uff1a\u9ad8\u5c42\u6df1\u5ea6\u56fe\u5207\u5272\u7528\u4e8e\u56e2\u961f\u5206\u5272\uff0c\u4e2d\u5c42\u56fe\u5b66\u4e60\u4fc3\u8fdb\u5b50\u56e2\u961f\u95f4\u534f\u8c03\u5bfc\u822a\uff0c\u5e95\u5c42\u7b56\u7565\u5b66\u4e60\u63a7\u5236\u5355\u4e2a\u79fb\u52a8\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u5e76\u907f\u514d\u78b0\u649e", "result": "\u5728\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTAF\u5b9e\u73b0\u4e86\u5b50\u56e2\u961f\u548c\u81ea\u9002\u5e94\u7f16\u961f\u63a7\u5236\u7684\u65b0\u80fd\u529b\uff0c\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u534f\u8c03\u591a\u673a\u5668\u4eba\u5bfc\u822a\u6027\u80fd", "conclusion": "STAF\u65b9\u6cd5\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u534f\u8c03\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u573a\u666f\u9700\u6c42"}}
{"id": "2509.16434", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16434", "abs": "https://arxiv.org/abs/2509.16434", "authors": ["Ritvik Singh", "Karl Van Wyk", "Pieter Abbeel", "Jitendra Malik", "Nathan Ratliff", "Ankur Handa"], "title": "End-to-end RL Improves Dexterous Grasping Policies", "comment": "See our blog post: https://e2e4robotics.com/", "summary": "This work explores techniques to scale up image-based end-to-end learning for\ndexterous grasping with an arm + hand system. Unlike state-based RL,\nvision-based RL is much more memory inefficient, resulting in relatively low\nbatch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is\nstill an attractive method as unlike the more commonly used techniques which\ndistill state-based policies into vision networks, end-to-end RL can allow for\nemergent active vision behaviors. We identify a key bottleneck in training\nthese policies is the way most existing simulators scale to multiple GPUs using\ntraditional data parallelism techniques. We propose a new method where we\ndisaggregate the simulator and RL (both training and experience buffers) onto\nseparate GPUs. On a node with four GPUs, we have the simulator running on three\nof them, and PPO running on the fourth. We are able to show that with the same\nnumber of GPUs, we can double the number of existing environments compared to\nthe previous baseline of standard data parallelism. This allows us to train\nvision-based environments, end-to-end with depth, which were previously\nperforming far worse with the baseline. We train and distill both depth and\nstate-based policies into stereo RGB networks and show that depth distillation\nleads to better results, both in simulation and reality. This improvement is\nlikely due to the observability gap between state and vision policies which\ndoes not exist when distilling depth policies into stereo RGB. We further show\nthat the increased batch size brought about by disaggregated simulation also\nimproves real world performance. When deploying in the real world, we improve\nupon the previous state-of-the-art vision-based results using our end-to-end\npolicies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u62df\u5668\u548cRL\u8bad\u7ec3\u5206\u79bb\u5230\u4e0d\u540c\u7684GPU\u4e0a\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u7075\u5de7\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u73b0\u5b9e\u4e16\u754c\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u7684\u7aef\u5230\u7aefRL\u867d\u7136\u80fd\u591f\u4ea7\u751f\u4e3b\u52a8\u89c6\u89c9\u884c\u4e3a\uff0c\u4f46\u7531\u4e8e\u5185\u5b58\u6548\u7387\u4f4e\u5bfc\u81f4\u6279\u91cf\u5927\u5c0f\u53d7\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325PPO\u7b49\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u73b0\u6709\u6a21\u62df\u5668\u7684\u6570\u636e\u5e76\u884c\u65b9\u6cd5\u5b58\u5728\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u6a21\u62df\u5668\u548cRL\u8bad\u7ec3\u7684\u65b9\u6cd5\uff1a\u5728\u56dbGPU\u8282\u70b9\u4e0a\uff0c\u4e09\u4e2aGPU\u8fd0\u884c\u6a21\u62df\u5668\uff0c\u4e00\u4e2aGPU\u8fd0\u884cPPO\u8bad\u7ec3\u548c\u4f53\u9a8c\u7f13\u51b2\u533a\u3002\u540c\u65f6\u91c7\u7528\u6df1\u5ea6\u84b8\u998f\u6280\u672f\u5c06\u6df1\u5ea6\u548c\u72b6\u6001\u7b56\u7565\u84b8\u998f\u5230\u7acb\u4f53RGB\u7f51\u7edc\u4e2d\u3002", "result": "\u76f8\u6bd4\u6807\u51c6\u6570\u636e\u5e76\u884c\u57fa\u7ebf\uff0c\u76f8\u540cGPU\u6570\u91cf\u4e0b\u73af\u5883\u6570\u91cf\u7ffb\u500d\uff1b\u6df1\u5ea6\u84b8\u998f\u6bd4\u72b6\u6001\u84b8\u998f\u6548\u679c\u66f4\u597d\uff1b\u89e3\u8026\u6a21\u62df\u65b9\u6cd5\u5e26\u6765\u7684\u6279\u91cf\u589e\u52a0\u63d0\u5347\u4e86\u73b0\u5b9e\u4e16\u754c\u6027\u80fd\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u89c6\u89c9\u65b9\u6cd5SOTA\u3002", "conclusion": "\u89e3\u8026\u6a21\u62df\u5668\u4e0eRL\u8bad\u7ec3\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9RL\u7684\u5185\u5b58\u74f6\u9888\uff0c\u6df1\u5ea6\u84b8\u998f\u7b56\u7565\u4f18\u4e8e\u72b6\u6001\u84b8\u998f\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u89c6\u89c9RL\u5728\u7075\u5de7\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u548c\u73b0\u5b9e\u6027\u80fd\u3002"}}
{"id": "2509.16445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16445", "abs": "https://arxiv.org/abs/2509.16445", "authors": ["Naoki Yokoyama", "Sehoon Ha"], "title": "FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning", "comment": null, "summary": "Enabling robotic assistants to navigate complex environments and locate\nobjects described in free-form language is a critical capability for real-world\ndeployment. While foundation models, particularly Vision-Language Models\n(VLMs), offer powerful semantic understanding, effectively adapting their\nweb-scale knowledge for embodied decision-making remains a key challenge. We\npresent FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that\ndirectly fine-tunes pre-trained VLM as the navigation policy. In contrast to\nmethods that use foundation models primarily in a zero-shot manner or for map\nannotation, FiLM-Nav learns to select the next best exploration frontier by\nconditioning directly on raw visual trajectory history and the navigation goal.\nLeveraging targeted simulated embodied experience allows the VLM to ground its\npowerful pre-trained representations in the specific dynamics and visual\npatterns relevant to goal-driven navigation. Critically, fine-tuning on a\ndiverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary\nspatial reasoning task proves essential for achieving robustness and broad\ngeneralization. FiLM-Nav sets a new state-of-the-art in both SPL and success\nrate on HM3D ObjectNav among open-vocabulary methods, and sets a\nstate-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating\nstrong generalization to unseen object categories. Our work validates that\ndirectly fine-tuning VLMs on diverse simulated embodied data is a highly\neffective pathway towards generalizable and efficient semantic navigation\ncapabilities.", "AI": {"tldr": "FiLM-Nav\u901a\u8fc7\u76f4\u63a5\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5bfc\u822a\u7b56\u7565\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7531\u8bed\u8a00\u63cf\u8ff0\u7684\u7269\u4f53\u5bfc\u822a\uff0c\u5728HM3D ObjectNav\u548cOVON\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u57fa\u7840\u6a21\u578b\uff08\u7279\u522b\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u5177\u6709\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5c06\u5176\u7f51\u7edc\u89c4\u6a21\u77e5\u8bc6\u9002\u5e94\u5230\u5177\u8eab\u51b3\u7b56\u4e2d\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u76f4\u63a5\u5229\u7528VLM\u8fdb\u884c\u5bfc\u822a\u51b3\u7b56\u7684\u65b9\u6cd5\u3002", "method": "FiLM-Nav\u76f4\u63a5\u5fae\u8c03\u9884\u8bad\u7ec3\u7684VLM\u4f5c\u4e3a\u5bfc\u822a\u7b56\u7565\uff0c\u57fa\u4e8e\u539f\u59cb\u89c6\u89c9\u8f68\u8ff9\u5386\u53f2\u548c\u5bfc\u822a\u76ee\u6807\u9009\u62e9\u6700\u4f73\u63a2\u7d22\u8fb9\u754c\u3002\u901a\u8fc7\u7ed3\u5408ObjectNav\u3001OVON\u3001ImageNav\u548c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u5316\u6570\u636e\u6df7\u5408\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728HM3D ObjectNav\u4e2d\uff0cFiLM-Nav\u5728\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86SPL\u548c\u6210\u529f\u7387\u7684\u6700\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684HM3D-OVON\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bbe\u7f6e\u4e86\u6700\u65b0\u7684SPL\u8bb0\u5f55\uff0c\u5c55\u793a\u4e86\u5bf9\u672a\u89c1\u7269\u4f53\u7c7b\u522b\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u76f4\u63a5\u5728\u591a\u6837\u5316\u6a21\u62df\u5177\u8eab\u6570\u636e\u4e0a\u5fae\u8c03VLM\u662f\u5b9e\u73b0\u53ef\u6cdb\u5316\u548c\u9ad8\u6548\u8bed\u4e49\u5bfc\u822a\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16469", "abs": "https://arxiv.org/abs/2509.16469", "authors": ["Guglielmo Cervettini", "Roberto Mauceri", "Alex Coppola", "Fabio Bergonti", "Luca Fiorio", "Marco Maggiali", "Daniele Pucci"], "title": "A Framework for Optimal Ankle Design of Humanoid Robots", "comment": "This paper has been accepted for publication at the 2025 IEEE-RAS\n  24th International Conference on Humanoid Robots (Humanoids), Seoul, 2025", "summary": "The design of the humanoid ankle is critical for safe and efficient ground\ninteraction. Key factors such as mechanical compliance and motor mass\ndistribution have driven the adoption of parallel mechanism architectures.\nHowever, selecting the optimal configuration depends on both actuator\navailability and task requirements. We propose a unified methodology for the\ndesign and evaluation of parallel ankle mechanisms. A multi-objective\noptimization synthesizes the mechanism geometry, the resulting solutions are\nevaluated using a scalar cost function that aggregates key performance metrics\nfor cross-architecture comparison. We focus on two representative\narchitectures: the Spherical-Prismatic-Universal (SPU) and the\nRevolute-Spherical-Universal (RSU). For both, we resolve the kinematics, and\nfor the RSU, introduce a parameterization that ensures workspace feasibility\nand accelerates optimization. We validate our approach by redesigning the ankle\nof an existing humanoid robot. The optimized RSU consistently outperforms both\nthe original serial design and a conventionally engineered RSU, reducing the\ncost function by up to 41% and 14%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5e76\u8054\u8e1d\u5173\u8282\u673a\u6784\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5408\u6210\u673a\u6784\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u4f7f\u7528\u6807\u91cf\u6210\u672c\u51fd\u6570\u8bc4\u4f30\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u6bd4\u8f83\u4e86SPU\u548cRSU\u4e24\u79cd\u67b6\u6784\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u8e1d\u5173\u8282\u8bbe\u8ba1\u5bf9\u5b89\u5168\u9ad8\u6548\u7684\u5730\u9762\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u673a\u68b0\u67d4\u987a\u6027\u548c\u7535\u673a\u8d28\u91cf\u5206\u5e03\u7b49\u5173\u952e\u56e0\u7d20\u63a8\u52a8\u4e86\u5e76\u8054\u673a\u6784\u67b6\u6784\u7684\u91c7\u7528\uff0c\u4f46\u6700\u4f18\u914d\u7f6e\u9009\u62e9\u53d6\u51b3\u4e8e\u6267\u884c\u5668\u53ef\u7528\u6027\u548c\u4efb\u52a1\u8981\u6c42\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u5408\u6210\u673a\u6784\u51e0\u4f55\u5f62\u72b6\uff0c\u4f7f\u7528\u6807\u91cf\u6210\u672c\u51fd\u6570\u805a\u5408\u5173\u952e\u6027\u80fd\u6307\u6807\u8fdb\u884c\u8de8\u67b6\u6784\u6bd4\u8f83\uff0c\u91cd\u70b9\u7814\u7a76\u4e86SPU\u548cRSU\u4e24\u79cd\u4ee3\u8868\u6027\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u5b66\u95ee\u9898\u5e76\u4e3aRSU\u5f15\u5165\u4e86\u786e\u4fdd\u5de5\u4f5c\u7a7a\u95f4\u53ef\u884c\u6027\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u73b0\u6709\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8e1d\u5173\u8282\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u4f18\u5316\u7684RSU\u5728\u6210\u672c\u51fd\u6570\u4e0a\u5206\u522b\u6bd4\u539f\u59cb\u4e32\u884c\u8bbe\u8ba1\u548c\u4f20\u7edf\u8bbe\u8ba1\u7684RSU\u964d\u4f4e\u4e8641%\u548c14%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f18\u5316\u5e76\u8054\u8e1d\u5173\u8282\u673a\u6784\u8bbe\u8ba1\uff0cRSU\u67b6\u6784\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\uff0c\u4e3a\u673a\u5668\u4eba\u8e1d\u5173\u8282\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16482", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.DS", "physics.app-ph", "49", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.16482", "abs": "https://arxiv.org/abs/2509.16482", "authors": ["Pranav Tiwari", "Soumyodipta Nath"], "title": "Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems", "comment": "6 Pages, 8 Figures. First two authors have contributed equally", "summary": "Coordinated path following in multi-agent systems is a key challenge in\nrobotics, with applications in automated logistics, surveillance, and\ncollaborative exploration. Traditional formation control techniques often rely\non time-parameterized trajectories and path integrals, which can result in\nsynchronization issues and rigid behavior. In this work, we address the problem\nof sequential path following, where agents maintain fixed spatial separation\nalong a common trajectory, guided by a leader under centralized control. We\nintroduce Robot Conga, a leader-follower control strategy that updates each\nagent's desired state based on the leader's spatial displacement rather than\ntime, assuming access to a global position reference, an assumption valid in\nindoor environments equipped with motion capture, vision-based tracking, or UWB\nlocalization systems. The algorithm was validated in simulation using both\nTurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate\ntrajectory tracking, stable inter-agent spacing, and fast convergence, with all\nagents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped\ncase, and almost instantaneously in the TurtleBot3 implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRobot Conga\u7b97\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u4f4d\u79fb\u800c\u975e\u65f6\u95f4\u53c2\u6570\u7684\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u987a\u5e8f\u8def\u5f84\u8ddf\u968f\u95ee\u9898\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u5728\u5171\u540c\u8f68\u8ff9\u4e0a\u4fdd\u6301\u56fa\u5b9a\u7a7a\u95f4\u95f4\u9694\u3002", "motivation": "\u4f20\u7edf\u7f16\u961f\u63a7\u5236\u6280\u672f\u4f9d\u8d56\u65f6\u95f4\u53c2\u6570\u5316\u8f68\u8ff9\u548c\u8def\u5f84\u79ef\u5206\uff0c\u5bb9\u6613\u5bfc\u81f4\u540c\u6b65\u95ee\u9898\u548c\u521a\u6027\u884c\u4e3a\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u987a\u5e8f\u8def\u5f84\u8ddf\u968f\u95ee\u9898\uff0c\u4f7f\u667a\u80fd\u4f53\u5728\u4e2d\u592e\u63a7\u5236\u4e0b\u6cbf\u5171\u540c\u8f68\u8ff9\u4fdd\u6301\u56fa\u5b9a\u7a7a\u95f4\u5206\u79bb\u3002", "method": "\u91c7\u7528\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u63a7\u5236\u7b56\u7565\uff0c\u57fa\u4e8e\u9886\u5bfc\u8005\u7684\u7a7a\u95f4\u4f4d\u79fb\u800c\u975e\u65f6\u95f4\u66f4\u65b0\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u671f\u671b\u72b6\u6001\uff0c\u5047\u8bbe\u5b58\u5728\u5168\u5c40\u4f4d\u7f6e\u53c2\u8003\uff08\u9002\u7528\u4e8e\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u6355\u6349\u3001\u89c6\u89c9\u8ddf\u8e2a\u6216UWB\u5b9a\u4f4d\u7cfb\u7edf\uff09\u3002", "result": "\u5728TurtleBot3\u548c\u56db\u8db3\u673a\u5668\u4eba\uff08Laikago\uff09\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u51c6\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3001\u7a33\u5b9a\u7684\u667a\u80fd\u4f53\u95f4\u8ddd\u548c\u5feb\u901f\u6536\u655b\uff0c\u56db\u8db3\u673a\u5668\u4eba\u60c5\u51b5\u4e0b\u6240\u6709\u667a\u80fd\u4f53\u5728250\u4e2a\u65f6\u95f4\u6b65\uff08\u7ea60.25\u79d2\uff09\u5185\u5bf9\u9f50\uff0cTurtleBot3\u5b9e\u73b0\u51e0\u4e4e\u77ac\u65f6\u5bf9\u9f50\u3002", "conclusion": "Robot Conga\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u987a\u5e8f\u8def\u5f84\u8ddf\u968f\u95ee\u9898\uff0c\u901a\u8fc7\u7a7a\u95f4\u4f4d\u79fb\u63a7\u5236\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u540c\u6b65\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u7269\u6d41\u3001\u76d1\u63a7\u548c\u534f\u4f5c\u63a2\u7d22\u7b49\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.16492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16492", "abs": "https://arxiv.org/abs/2509.16492", "authors": ["Tinapat Limsila", "Mehul Sharma", "Paulo Garcia"], "title": "Substrate-Timing-Independence for Meta-State Stability of Distributed Robotic Swarms", "comment": null, "summary": "Emergent properties in distributed systems arise due to timing\nunpredictability; asynchronous state evolution within each sub-system may lead\nthe macro-system to faulty meta-states. Empirical validation of correctness is\noften prohibitively expensive, as the size of the state-space is too large to\nbe tractable. In robotic swarms this problem is exacerbated, when compared to\nsoftware systems, by the variability of the implementation substrate across the\ndesign, or even the deployment, process. We present an approach for formally\nreasoning about the correctness of robotic swarm design in a\nsubstrate-timing-independent way. By leveraging concurrent process calculi\n(namely, Communicating Sequential Processes), we introduce a methodology that\ncan automatically identify possible causes of faulty meta-states and correct\nsuch designs such that meta-states are consistently stable, even in the\npresence of timing variability due to substrate changes. We evaluate this\napproach on a robotic swarm with a clearly identified fault, realized in both\nsimulation and reality. Results support the research hypothesis, showing that\nthe swarm reaches an illegal meta-state before the correction is applied, but\nbehaves consistently correctly after the correction. Our techniques are\ntransferable across different design methodologies, contributing to the toolbox\nof formal methods for roboticists.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e76\u53d1\u8fdb\u7a0b\u6f14\u7b97\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u673a\u5668\u4eba\u7fa4\u4f53\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u5143\u72b6\u6001\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u548c\u4fee\u6b63\u8bbe\u8ba1\u6765\u786e\u4fdd\u7cfb\u7edf\u5728\u65f6\u5e8f\u53d8\u5316\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u6d8c\u73b0\u7279\u6027\u7531\u4e8e\u65f6\u5e8f\u4e0d\u53ef\u9884\u6d4b\u6027\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u8fdb\u5165\u9519\u8bef\u5143\u72b6\u6001\uff0c\u800c\u673a\u5668\u4eba\u7fa4\u4f53\u7cfb\u7edf\u7531\u4e8e\u5b9e\u73b0\u57fa\u8d28\u7684\u53d8\u5f02\u6027\u4f7f\u5f97\u7ecf\u9a8c\u9a8c\u8bc1\u53d8\u5f97\u6781\u5176\u56f0\u96be\u3002", "method": "\u5229\u7528\u901a\u4fe1\u987a\u5e8f\u8fdb\u7a0b\uff08CSP\uff09\u7b49\u5e76\u53d1\u8fdb\u7a0b\u6f14\u7b97\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u8d28\u65f6\u5e8f\u65e0\u5173\u7684\u5f62\u5f0f\u5316\u63a8\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u9519\u8bef\u5143\u72b6\u6001\u7684\u539f\u56e0\u5e76\u4fee\u6b63\u8bbe\u8ba1\u3002", "result": "\u5728\u5177\u6709\u660e\u786e\u6545\u969c\u7684\u673a\u5668\u4eba\u7fa4\u4f53\u7cfb\u7edf\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4fee\u6b63\u524d\u7cfb\u7edf\u4f1a\u5230\u8fbe\u975e\u6cd5\u5143\u72b6\u6001\uff0c\u800c\u4fee\u6b63\u540e\u7cfb\u7edf\u884c\u4e3a\u59cb\u7ec8\u4fdd\u6301\u6b63\u786e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u8de8\u4e0d\u540c\u8bbe\u8ba1\u65b9\u6cd5\u5b66\u8f6c\u79fb\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u5bb6\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u5de5\u5177\u7bb1\uff0c\u6709\u52a9\u4e8e\u786e\u4fdd\u673a\u5668\u4eba\u7fa4\u4f53\u7cfb\u7edf\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2509.16532", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16532", "abs": "https://arxiv.org/abs/2509.16532", "authors": ["Run Yu", "Yangdi Liu", "Wen-Da Wei", "Chen Li"], "title": "No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning", "comment": null, "summary": "Recently,vision-based robotic manipulation has garnered significant attention\nand witnessed substantial advancements. 2D image-based and 3D point cloud-based\npolicy learning represent two predominant paradigms in the field, with recent\nstudies showing that the latter consistently outperforms the former in terms of\nboth policy performance and generalization, thereby underscoring the value and\nsignificance of 3D information. However, 3D point cloud-based approaches face\nthe significant challenge of high data acquisition costs, limiting their\nscalability and real-world deployment. To address this issue, we propose a\nnovel framework NoReal3D: which introduces the 3DStructureFormer, a learnable\n3D perception module capable of transforming monocular images into\ngeometrically meaningful pseudo-point cloud features, effectively fused with\nthe 2D encoder output features. Specially, the generated pseudo-point clouds\nretain geometric and topological structures so we design a pseudo-point cloud\nencoder to preserve these properties, making it well-suited for our framework.\nWe also investigate the effectiveness of different feature fusion\nstrategies.Our framework enhances the robot's understanding of 3D spatial\nstructures while completely eliminating the substantial costs associated with\n3D point cloud acquisition.Extensive experiments across various tasks validate\nthat our framework can achieve performance comparable to 3D point cloud-based\nmethods, without the actual point cloud data.", "AI": {"tldr": "\u63d0\u51faNoReal3D\u6846\u67b6\uff0c\u901a\u8fc73DStructureFormer\u5c06\u5355\u76ee\u56fe\u50cf\u8f6c\u6362\u4e3a\u51e0\u4f55\u6709\u610f\u4e49\u7684\u4f2a\u70b9\u4e91\u7279\u5f81\uff0c\u907f\u514d\u5b9e\u96453D\u70b9\u4e91\u91c7\u96c6\u7684\u9ad8\u6210\u672c\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e3D\u70b9\u4e91\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33D\u70b9\u4e91\u65b9\u6cd5\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u63013D\u4fe1\u606f\u7684\u4f18\u52bf\u30022D\u56fe\u50cf\u65b9\u6cd5\u6027\u80fd\u8f83\u5dee\uff0c\u800c3D\u70b9\u4e91\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u597d\u4f46\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u4f7f\u75283DStructureFormer\u6a21\u5757\u5c06\u5355\u76ee\u56fe\u50cf\u8f6c\u6362\u4e3a\u4fdd\u7559\u51e0\u4f55\u548c\u62d3\u6251\u7ed3\u6784\u7684\u4f2a\u70b9\u4e91\u7279\u5f81\uff0c\u4e0e2D\u7f16\u7801\u5668\u8f93\u51fa\u7279\u5f81\u878d\u5408\uff0c\u5e76\u8bbe\u8ba1\u4f2a\u70b9\u4e91\u7f16\u7801\u5668\u6765\u4fdd\u6301\u8fd9\u4e9b\u5c5e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u53ef\u4ee5\u8fbe\u5230\u4e0e3D\u70b9\u4e91\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u5b9e\u9645\u70b9\u4e91\u6570\u636e\u3002", "conclusion": "NoReal3D\u6846\u67b6\u5728\u6d88\u96643D\u70b9\u4e91\u91c7\u96c6\u6210\u672c\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bf93D\u7a7a\u95f4\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u89c6\u89c9\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16550", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16550", "abs": "https://arxiv.org/abs/2509.16550", "authors": ["Yinghao Wu", "Shuhong Hou", "Haowen Zheng", "Yichen Li", "Weiyi Lu", "Xun Zhou", "Yitian Shao"], "title": "TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation", "comment": "8 pages, 7 figures", "summary": "Robotic manipulation tasks such as inserting a key into a lock or plugging a\nUSB device into a port can fail when visual perception is insufficient to\ndetect misalignment. In these situations, touch sensing is crucial for the\nrobot to monitor the task's states and make precise, timely adjustments.\nCurrent touch sensing solutions are either insensitive to detect subtle changes\nor demand excessive sensor data. Here, we introduce TranTac, a data-efficient\nand low-cost tactile sensing and control framework that integrates a single\ncontact-sensitive 6-axis inertial measurement unit within the elastomeric tips\nof a robotic gripper for completing fine insertion tasks. Our customized\nsensing system can detect dynamic translational and torsional deformations at\nthe micrometer scale, enabling the tracking of visually imperceptible pose\nchanges of the grasped object. By leveraging transformer-based encoders and\ndiffusion policy, TranTac can imitate human insertion behaviors using transient\ntactile cues detected at the gripper's tip during insertion processes. These\ncues enable the robot to dynamically control and correct the 6-DoF pose of the\ngrasped object. When combined with vision, TranTac achieves an average success\nrate of 79% on object grasping and insertion tasks, outperforming both\nvision-only policy and the one augmented with end-effector 6D force/torque\nsensing. Contact localization performance is also validated through\ntactile-only misaligned insertion tasks, achieving an average success rate of\n88%. We assess the generalizability by training TranTac on a single prism-slot\npair and testing it on unseen data, including a USB plug and a metal key, and\nfind that the insertion tasks can still be completed with an average success\nrate of nearly 70%. The proposed framework may inspire new robotic tactile\nsensing systems for delicate manipulation tasks.", "AI": {"tldr": "TranTac\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u6570\u636e\u9ad8\u6548\u7684\u591a\u6a21\u6001\u89e6\u89c9\u4f20\u611f\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u62106\u8f74IMU\u4f20\u611f\u5668\u5728\u673a\u5668\u4eba\u5939\u722a\u5c16\u7aef\uff0c\u7ed3\u5408Transformer\u7f16\u7801\u5668\u548c\u6269\u6563\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u77ac\u6001\u89e6\u89c9\u7ebf\u7d22\u7684\u7cbe\u7ec6\u63d2\u5165\u4efb\u52a1\u63a7\u5236\u3002", "motivation": "\u5728\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u4ee5\u68c0\u6d4b\u5fae\u5c0f\u9519\u4f4d\u7684\u60c5\u51b5\u4e0b\uff08\u5982\u94a5\u5319\u63d2\u5165\u9501\u5b54\u3001USB\u63d2\u5165\u7aef\u53e3\uff09\uff0c\u89e6\u89c9\u4f20\u611f\u5bf9\u4e8e\u673a\u5668\u4eba\u76d1\u63a7\u4efb\u52a1\u72b6\u6001\u548c\u8fdb\u884c\u7cbe\u786e\u8c03\u6574\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u89e6\u89c9\u4f20\u611f\u65b9\u6848\u8981\u4e48\u5bf9\u7ec6\u5fae\u53d8\u5316\u4e0d\u654f\u611f\uff0c\u8981\u4e48\u9700\u8981\u8fc7\u591a\u4f20\u611f\u5668\u6570\u636e\u3002", "method": "\u5728\u673a\u5668\u4eba\u5939\u722a\u7684\u5f39\u6027\u5c16\u7aef\u5185\u96c6\u6210\u5355\u4e2a\u63a5\u89e6\u654f\u611f\u76846\u8f74IMU\u4f20\u611f\u5668\uff0c\u68c0\u6d4b\u5fae\u7c73\u7ea7\u7684\u52a8\u6001\u5e73\u79fb\u548c\u626d\u8f6c\u53d8\u5f62\uff1b\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\u548c\u6269\u6563\u7b56\u7565\u6765\u6a21\u4eff\u4eba\u7c7b\u63d2\u5165\u884c\u4e3a\uff1b\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u4fe1\u606f\u8fdb\u884c6-DoF\u59ff\u6001\u63a7\u5236\u3002", "result": "\u7ed3\u5408\u89c6\u89c9\u65f6\u5e73\u5747\u6210\u529f\u738779%\uff0c\u4f18\u4e8e\u7eaf\u89c6\u89c9\u7b56\u7565\u548c\u672b\u7aef\u6267\u884c\u56686D\u529b/\u529b\u77e9\u4f20\u611f\u589e\u5f3a\u7b56\u7565\uff1b\u4ec5\u89e6\u89c9\u7684\u9519\u4f4d\u63d2\u5165\u4efb\u52a1\u5e73\u5747\u6210\u529f\u738788%\uff1b\u5728\u672a\u89c1\u8fc7\u7684USB\u63d2\u5934\u548c\u91d1\u5c5e\u94a5\u5319\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747\u6210\u529f\u7387\u63a5\u8fd170%\u3002", "conclusion": "TranTac\u6846\u67b6\u4e3a\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u5668\u4eba\u89e6\u89c9\u4f20\u611f\u7cfb\u7edf\u601d\u8def\uff0c\u5c55\u793a\u4e86\u6570\u636e\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u591a\u6a21\u6001\u4f20\u611f\u4e0e\u63a7\u5236\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.16611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16611", "abs": "https://arxiv.org/abs/2509.16611", "authors": ["Xiwei Zhao", "Yiwei Wang", "Yansong Wu", "Fan Wu", "Teng Sun", "Zhonghua Miao", "Sami Haddadin", "Alois Knoll"], "title": "Video-to-BT: Generating Reactive Behavior Trees from Human Demonstration Videos for Robotic Assembly", "comment": null, "summary": "Modern manufacturing demands robotic assembly systems with enhanced\nflexibility and reliability. However, traditional approaches often rely on\nprogramming tailored to each product by experts for fixed settings, which are\ninherently inflexible to product changes and lack the robustness to handle\nvariations. As Behavior Trees (BTs) are increasingly used in robotics for their\nmodularity and reactivity, we propose a novel hierarchical framework,\nVideo-to-BT, that seamlessly integrates high-level cognitive planning with\nlow-level reactive control, with BTs serving both as the structured output of\nplanning and as the governing structure for execution. Our approach leverages a\nVision-Language Model (VLM) to decompose human demonstration videos into\nsubtasks, from which Behavior Trees are generated. During the execution, the\nplanned BTs combined with real-time scene interpretation enable the system to\noperate reactively in the dynamic environment, while VLM-driven replanning is\ntriggered upon execution failure. This closed-loop architecture ensures\nstability and adaptivity. We validate our framework on real-world assembly\ntasks through a series of experiments, demonstrating high planning reliability,\nrobust performance in long-horizon assembly tasks, and strong generalization\nacross diverse and perturbed conditions. Project website:\nhttps://video2bt.github.io/video2bt_page/", "AI": {"tldr": "\u63d0\u51faVideo-to-BT\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u751f\u6210\u884c\u4e3a\u6811\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u7684\u9ad8\u5c42\u8ba4\u77e5\u89c4\u5212\u4e0e\u4f4e\u5c42\u53cd\u5e94\u63a7\u5236\u7684\u96c6\u6210", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u88c5\u914d\u7cfb\u7edf\u4f9d\u8d56\u4e13\u5bb6\u7f16\u7a0b\uff0c\u7f3a\u4e4f\u5bf9\u4ea7\u54c1\u53d8\u5316\u7684\u7075\u6d3b\u6027\u548c\u5904\u7406\u53d8\u5f02\u7684\u9c81\u68d2\u6027\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u53ef\u9760\u7684\u65b9\u6cd5", "method": "\u4f7f\u7528Vision-Language Model\u5206\u89e3\u6f14\u793a\u89c6\u9891\u751f\u6210\u884c\u4e3a\u6811\uff0c\u7ed3\u5408\u5b9e\u65f6\u573a\u666f\u89e3\u91ca\u5b9e\u73b0\u53cd\u5e94\u5f0f\u63a7\u5236\uff0c\u5931\u8d25\u65f6\u89e6\u53d1VLM\u9a71\u52a8\u7684\u91cd\u65b0\u89c4\u5212", "result": "\u5728\u771f\u5b9e\u88c5\u914d\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u9ad8\u89c4\u5212\u53ef\u9760\u6027\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u80fd\uff0c\u4ee5\u53ca\u5728\u591a\u6837\u5316\u548c\u6270\u52a8\u6761\u4ef6\u4e0b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684\u95ed\u73af\u67b6\u6784\u786e\u4fdd\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u81ea\u9002\u5e94\u6027\uff0c\u4e3a\u67d4\u6027\u5236\u9020\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.16614", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16614", "abs": "https://arxiv.org/abs/2509.16614", "authors": ["Bojan Deraji\u0107", "Sebastian Bernhard", "Wolfgang H\u00f6nig"], "title": "ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks", "comment": null, "summary": "Control barrier functions (CBFs) have been demonstrated as an effective\nmethod for safety-critical control of autonomous systems. Although CBFs are\nsimple to deploy, their design remains challenging, motivating the development\nof learning-based approaches. Yet, issues such as suboptimal safe sets,\napplicability in partially observable environments, and lack of rigorous safety\nguarantees persist. In this work, we propose observation-conditioned neural\nCBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately\nrecover the maximal safe sets. We exploit certain mathematical properties of\nthe HJ value function, ensuring that the predicted safe set never intersects\nwith the observed failure set. Moreover, we leverage a hypernetwork-based\narchitecture that is particularly suitable for the design of\nobservation-conditioned safety filters. The proposed method is examined both in\nsimulation and hardware experiments for a ground robot and a quadcopter. The\nresults show improved success rates and generalization to out-of-domain\nenvironments compared to the baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eHamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u7684\u89c2\u6d4b\u6761\u4ef6\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b89\u5168\u5173\u952e\u63a7\u5236\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u80fd\u591f\u8fd1\u4f3c\u6062\u590d\u6700\u5927\u5b89\u5168\u96c6\u5e76\u786e\u4fdd\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u867d\u7136\u90e8\u7f72\u7b80\u5355\u4f46\u8bbe\u8ba1\u56f0\u96be\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u96c6\u6b21\u4f18\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u9002\u7528\u6027\u5dee\u3001\u7f3a\u4e4f\u4e25\u683c\u5b89\u5168\u4fdd\u8bc1\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528HJ\u503c\u51fd\u6570\u7684\u6570\u5b66\u7279\u6027\uff0c\u786e\u4fdd\u9884\u6d4b\u5b89\u5168\u96c6\u4e0d\u4e0e\u89c2\u6d4b\u6545\u969c\u96c6\u76f8\u4ea4\uff0c\u91c7\u7528\u8d85\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u89c2\u6d4b\u6761\u4ef6\u5b89\u5168\u6ee4\u6ce2\u5668\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5730\u9762\u673a\u5668\u4eba\u548c\u56db\u65cb\u7ffc\u98de\u884c\u5668\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u5bf9\u57df\u5916\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u8bc1\u3002"}}
{"id": "2509.16615", "categories": ["cs.RO", "68T40", "I.2.9; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.16615", "abs": "https://arxiv.org/abs/2509.16615", "authors": ["Jelle Luijkx", "Runyu Ma", "Zlatan Ajanovi\u0107", "Jens Kober"], "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning", "comment": "8 pages, 7 figures", "summary": "Reinforcement learning (RL) is a promising approach for robotic manipulation,\nbut it can suffer from low sample efficiency and requires extensive exploration\nof large state-action spaces. Recent methods leverage the commonsense knowledge\nand reasoning abilities of large language models (LLMs) to guide exploration\ntoward more meaningful states. However, LLMs can produce plans that are\nsemantically plausible yet physically infeasible, yielding unreliable behavior.\nWe introduce LLM-TALE, a framework that uses LLMs' planning to directly steer\nRL exploration. LLM-TALE integrates planning at both the task level and the\naffordance level, improving learning efficiency by directing agents toward\nsemantically meaningful actions. Unlike prior approaches that assume optimal\nLLM-generated plans or rewards, LLM-TALE corrects suboptimality online and\nexplores multimodal affordance-level plans without human supervision. We\nevaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing\nimprovements in both sample efficiency and success rates over strong baselines.\nReal-robot experiments indicate promising zero-shot sim-to-real transfer. Code\nand supplementary material are available at https://llm-tale.github.io.", "AI": {"tldr": "LLM-TALE\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u80fd\u529b\u76f4\u63a5\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7ea7\u548c\u53ef\u4f9b\u6027\u7ea7\u89c4\u5212\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u4e14\u9700\u8981\u5927\u91cf\u63a2\u7d22\uff0c\u73b0\u6709\u65b9\u6cd5\u5229\u7528LLM\u7684\u5e38\u8bc6\u77e5\u8bc6\u6307\u5bfc\u63a2\u7d22\u4f46\u4f1a\u4ea7\u751f\u7269\u7406\u4e0d\u53ef\u884c\u7684\u8ba1\u5212\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u884c\u4e3a\u3002", "method": "LLM-TALE\u6846\u67b6\u96c6\u6210\u4efb\u52a1\u7ea7\u548c\u53ef\u4f9b\u6027\u7ea7\u89c4\u5212\uff0c\u5728\u7ebf\u7ea0\u6b63\u6b21\u4f18\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u5373\u53ef\u63a2\u7d22\u591a\u6a21\u6001\u53ef\u4f9b\u6027\u7ea7\u8ba1\u5212\u3002", "result": "\u5728\u6807\u51c6RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cpick-and-place\u4efb\u52a1\u7684\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\u5747\u6709\u63d0\u5347\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u663e\u793a\u51fa\u6709\u524d\u666f\u7684\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "LLM-TALE\u901a\u8fc7LLM\u89c4\u5212\u76f4\u63a5\u6307\u5bfcRL\u63a2\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7269\u7406\u4e0d\u53ef\u884c\u8ba1\u5212\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.16638", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16638", "abs": "https://arxiv.org/abs/2509.16638", "authors": ["Jinrui Han", "Weiji Xie", "Jiakun Zheng", "Jiyuan Shi", "Weinan Zhang", "Ting Xiao", "Chenjia Bai"], "title": "KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control", "comment": null, "summary": "Learning versatile whole-body skills by tracking various human motions is a\nfundamental step toward general-purpose humanoid robots. This task is\nparticularly challenging because a single policy must master a broad repertoire\nof motion skills while ensuring stability over long-horizon sequences. To this\nend, we present VMS, a unified whole-body controller that enables humanoid\nrobots to learn diverse and dynamic behaviors within a single policy. Our\nframework integrates a hybrid tracking objective that balances local motion\nfidelity with global trajectory consistency, and an Orthogonal\nMixture-of-Experts (OMoE) architecture that encourages skill specialization\nwhile enhancing generalization across motions. A segment-level tracking reward\nis further introduced to relax rigid step-wise matching, enhancing robustness\nwhen handling global displacements and transient inaccuracies. We validate VMS\nextensively in both simulation and real-world experiments, demonstrating\naccurate imitation of dynamic skills, stable performance over minute-long\nsequences, and strong generalization to unseen motions. These results highlight\nthe potential of VMS as a scalable foundation for versatile humanoid whole-body\ncontrol. The project page is available at\nhttps://kungfubot2-humanoid.github.io.", "AI": {"tldr": "VMS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u5668\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u5b66\u4e60\u591a\u6837\u5316\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u7ed3\u5408\u6df7\u5408\u8ddf\u8e2a\u76ee\u6807\u548c\u6b63\u4ea4\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u5b9e\u73b0\u7a33\u5b9a\u4e14\u901a\u7528\u7684\u8fd0\u52a8\u6280\u80fd\u6a21\u4eff\u3002", "motivation": "\u5b66\u4e60\u8ddf\u8e2a\u5404\u79cd\u4eba\u7c7b\u8fd0\u52a8\u7684\u5168\u8eab\u6280\u80fd\u662f\u8fc8\u5411\u901a\u7528\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5355\u4e00\u7b56\u7565\u4e2d\u540c\u65f6\u638c\u63e1\u5e7f\u6cdb\u8fd0\u52a8\u6280\u80fd\u5e76\u786e\u4fdd\u957f\u671f\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faVMS\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u5e73\u8861\u5c40\u90e8\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u5168\u5c40\u8f68\u8ff9\u4e00\u81f4\u6027\u7684\u6df7\u5408\u8ddf\u8e2a\u76ee\u6807\uff1b2\uff09\u9f13\u52b1\u6280\u80fd\u4e13\u4e1a\u5316\u540c\u65f6\u589e\u5f3a\u8fd0\u52a8\u6cdb\u5316\u80fd\u529b\u7684\u6b63\u4ea4\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff1b3\uff09\u653e\u677e\u4e25\u683c\u9010\u6b65\u5339\u914d\u7684\u5206\u6bb5\u7ea7\u8ddf\u8e2a\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86VMS\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u52a8\u6001\u6280\u80fd\u7684\u51c6\u786e\u6a21\u4eff\u3001\u5206\u949f\u7ea7\u5e8f\u5217\u7684\u7a33\u5b9a\u6027\u80fd\u4ee5\u53ca\u5bf9\u672a\u89c1\u8fd0\u52a8\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VMS\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u901a\u7528\u5168\u8eab\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16757", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16757", "abs": "https://arxiv.org/abs/2509.16757", "authors": ["Haoyang Weng", "Yitang Li", "Nikhil Sobanbabu", "Zihan Wang", "Zhengyi Luo", "Tairan He", "Deva Ramanan", "Guanya Shi"], "title": "HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos", "comment": "website: hdmi-humanoid.github.io", "summary": "Enabling robust whole-body humanoid-object interaction (HOI) remains\nchallenging due to motion data scarcity and the contact-rich nature. We present\nHDMI (HumanoiD iMitation for Interaction), a simple and general framework that\nlearns whole-body humanoid-object interaction skills directly from monocular\nRGB videos. Our pipeline (i) extracts and retargets human and object\ntrajectories from unconstrained videos to build structured motion datasets,\n(ii) trains a reinforcement learning (RL) policy to co-track robot and object\nstates with three key designs: a unified object representation, a residual\naction space, and a general interaction reward, and (iii) zero-shot deploys the\nRL policies on real humanoid robots. Extensive sim-to-real experiments on a\nUnitree G1 humanoid demonstrate the robustness and generality of our approach:\nHDMI achieves 67 consecutive door traversals and successfully performs 6\ndistinct loco-manipulation tasks in the real world and 14 tasks in simulation.\nOur results establish HDMI as a simple and general framework for acquiring\ninteractive humanoid skills from human videos.", "AI": {"tldr": "HDMI\u662f\u4e00\u4e2a\u4ece\u5355\u76eeRGB\u89c6\u9891\u4e2d\u5b66\u4e60\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u6280\u80fd\u7684\u7b80\u5355\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u4eba\u7c7b\u8fd0\u52a8\u8f68\u8ff9\u3001\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72", "motivation": "\u89e3\u51b3\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u5305\u62ec\u8fd0\u52a8\u6570\u636e\u7a00\u7f3a\u548c\u63a5\u89e6\u5bc6\u96c6\u6027\u95ee\u9898\uff0c\u65e8\u5728\u76f4\u63a5\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u4ea4\u4e92\u6280\u80fd", "method": "\u4e09\u6b65\u6d41\u7a0b\uff1a1\uff09\u4ece\u65e0\u7ea6\u675f\u89c6\u9891\u4e2d\u63d0\u53d6\u5e76\u91cd\u5b9a\u5411\u4eba\u7c7b\u548c\u7269\u4f53\u8f68\u8ff9\u6784\u5efa\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff1b2\uff09\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u91c7\u7528\u7edf\u4e00\u7269\u4f53\u8868\u793a\u3001\u6b8b\u5dee\u52a8\u4f5c\u7a7a\u95f4\u548c\u901a\u7528\u4ea4\u4e92\u5956\u52b1\uff1b3\uff09\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u96f6\u6837\u672c\u90e8\u7f72\u7b56\u7565", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u4eff\u771f\u5230\u771f\u5b9e\u5b9e\u9a8c\u663e\u793a\uff1a\u5b9e\u73b067\u6b21\u8fde\u7eed\u95e8\u7a7f\u8d8a\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6210\u529f\u6267\u884c6\u4e2a\u4e0d\u540c\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u5728\u4eff\u771f\u4e2d\u6267\u884c14\u4e2a\u4efb\u52a1", "conclusion": "HDMI\u662f\u4e00\u4e2a\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u83b7\u53d6\u4ea4\u4e92\u5f0f\u4eba\u5f62\u673a\u5668\u4eba\u6280\u80fd\u7684\u7b80\u5355\u901a\u7528\u6846\u67b6\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027"}}
{"id": "2509.16773", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.16773", "abs": "https://arxiv.org/abs/2509.16773", "authors": ["Mohamad Mofeed Chaar", "Jamal Raiyn", "Galia Weidl"], "title": "Improve bounding box in Carla Simulator", "comment": "9 pages, 12 figures,VEHITS Conference 2024", "summary": "The CARLA simulator (Car Learning to Act) serves as a robust platform for\ntesting algorithms and generating datasets in the field of Autonomous Driving\n(AD). It provides control over various environmental parameters, enabling\nthorough evaluation. Development bounding boxes are commonly utilized tools in\ndeep learning and play a crucial role in AD applications. The predominant\nmethod for data generation in the CARLA Simulator involves identifying and\ndelineating objects of interest, such as vehicles, using bounding boxes. The\noperation in CARLA entails capturing the coordinates of all objects on the map,\nwhich are subsequently aligned with the sensor's coordinate system at the ego\nvehicle and then enclosed within bounding boxes relative to the ego vehicle's\nperspective. However, this primary approach encounters challenges associated\nwith object detection and bounding box annotation, such as ghost boxes.\nAlthough these procedures are generally effective at detecting vehicles and\nother objects within their direct line of sight, they may also produce false\npositives by identifying objects that are obscured by obstructions. We have\nenhanced the primary approach with the objective of filtering out unwanted\nboxes. Performance analysis indicates that the improved approach has achieved\nhigh accuracy.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CARLA\u6a21\u62df\u5668\u4e2d\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u751f\u6210\u7684\u8fb9\u754c\u6846\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u4ee5\u8fc7\u6ee4\u8bef\u68c0\u6846\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "CARLA\u6a21\u62df\u5668\u4e2d\u539f\u6709\u7684\u8fb9\u754c\u6846\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8bef\u68c0\u95ee\u9898\uff0c\u5982\u68c0\u6d4b\u5230\u88ab\u906e\u6321\u7269\u4f53\u7684\u865a\u5047\u8fb9\u754c\u6846\uff08ghost boxes\uff09\uff0c\u8fd9\u5f71\u54cd\u4e86\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u3002", "method": "\u5728\u539f\u6709\u65b9\u6cd5\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u6539\u8fdb\u7b97\u6cd5\u6765\u8fc7\u6ee4\u6389\u4e0d\u60f3\u8981\u7684\u8fb9\u754c\u6846\uff0c\u5177\u4f53\u5305\u62ec\u4f18\u5316\u5bf9\u8c61\u68c0\u6d4b\u548c\u8fb9\u754c\u6846\u6807\u6ce8\u6d41\u7a0b\uff0c\u51cf\u5c11\u56e0\u906e\u6321\u5bfc\u81f4\u7684\u8bef\u68c0\u3002", "result": "\u6027\u80fd\u5206\u6790\u8868\u660e\uff0c\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8bef\u68c0\u8fb9\u754c\u6846\u7684\u6570\u91cf\u3002", "conclusion": "\u6539\u8fdb\u7684\u8fb9\u754c\u6846\u751f\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86CARLA\u6a21\u62df\u5668\u4e2d\u6570\u636e\u751f\u6210\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u7684\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\u6784\u5efa\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2509.16812", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16812", "abs": "https://arxiv.org/abs/2509.16812", "authors": ["Priyanshu Agrawal", "Shalabh Gupta", "Zongyuan Shen"], "title": "SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree", "comment": null, "summary": "This paper presents SMART-3D, an extension of the SMART algorithm to 3D\nenvironments. SMART-3D is a tree-based adaptive replanning algorithm for\ndynamic environments with fast moving obstacles. SMART-3D morphs the underlying\ntree to find a new path in real-time whenever the current path is blocked by\nobstacles. SMART-3D removed the grid decomposition requirement of the SMART\nalgorithm by replacing the concept of hot-spots with that of hot-nodes, thus\nmaking it computationally efficient and scalable to 3D environments. The\nhot-nodes are nodes which allow for efficient reconnections to morph the\nexisting tree to find a new safe and reliable path. The performance of SMART-3D\nis evaluated by extensive simulations in 2D and 3D environments populated with\nrandomly moving dynamic obstacles. The results show that SMART-3D achieves high\nsuccess rates and low replanning times, thus highlighting its suitability for\nreal-time onboard applications.", "AI": {"tldr": "SMART-3D\u662fSMART\u7b97\u6cd5\u57283D\u73af\u5883\u4e2d\u7684\u6269\u5c55\uff0c\u4e00\u79cd\u57fa\u4e8e\u6811\u7684\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u7684\u52a8\u6001\u73af\u5883\u3002", "motivation": "\u5c06SMART\u7b97\u6cd5\u6269\u5c55\u52303D\u73af\u5883\uff0c\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u7528\u70ed\u8282\u70b9\u6982\u5ff5\u66ff\u4ee3\u70ed\u70b9\u6982\u5ff5\uff0c\u79fb\u9664\u7f51\u683c\u5206\u89e3\u8981\u6c42\uff0c\u4f7f\u7b97\u6cd5\u80fd\u591f\u5b9e\u65f6\u53d8\u5f62\u5e95\u5c42\u6811\u7ed3\u6784\u4ee5\u5bfb\u627e\u65b0\u8def\u5f84\u3002\u70ed\u8282\u70b9\u5141\u8bb8\u9ad8\u6548\u91cd\u65b0\u8fde\u63a5\u6765\u53d8\u5f62\u73b0\u6709\u6811\u3002", "result": "\u57282D\u548c3D\u73af\u5883\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u6a21\u62df\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793aSMART-3D\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u548c\u4f4e\u91cd\u89c4\u5212\u65f6\u95f4\u3002", "conclusion": "SMART-3D\u9002\u7528\u4e8e\u5b9e\u65f6\u673a\u8f7d\u5e94\u7528\uff0c\u5177\u6709\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.16830", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16830", "abs": "https://arxiv.org/abs/2509.16830", "authors": ["Omkar Patil", "Prabin Rath", "Kartikay Pangaonkar", "Eric Rosen", "Nakul Gopalan"], "title": "Factorizing Diffusion Policies for Observation Modality Prioritization", "comment": "14 pages; website: https://fdp-policy.github.io/fdp-policy/", "summary": "Diffusion models have been extensively leveraged for learning robot skills\nfrom demonstrations. These policies are conditioned on several observational\nmodalities such as proprioception, vision and tactile. However, observational\nmodalities have varying levels of influence for different tasks that diffusion\npolices fail to capture. In this work, we propose 'Factorized Diffusion\nPolicies' abbreviated as FDP, a novel policy formulation that enables\nobservational modalities to have differing influence on the action diffusion\nprocess by design. This results in learning policies where certain observations\nmodalities can be prioritized over the others such as $\\texttt{vision>tactile}$\nor $\\texttt{proprioception>vision}$. FDP achieves modality prioritization by\nfactorizing the observational conditioning for diffusion process, resulting in\nmore performant and robust policies. Our factored approach shows strong\nperformance improvements in low-data regimes with $15\\%$ absolute improvement\nin success rate on several simulated benchmarks when compared to a standard\ndiffusion policy that jointly conditions on all input modalities. Moreover, our\nbenchmark and real-world experiments show that factored policies are naturally\nmore robust with $40\\%$ higher absolute success rate across several visuomotor\ntasks under distribution shifts such as visual distractors or camera\nocclusions, where existing diffusion policies fail catastrophically. FDP thus\noffers a safer and more robust alternative to standard diffusion policies for\nreal-world deployment. Videos are available at\nhttps://fdp-policy.github.io/fdp-policy/ .", "AI": {"tldr": "\u63d0\u51fa\u4e86Factorized Diffusion Policies (FDP)\uff0c\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u516c\u5f0f\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4f7f\u89c2\u6d4b\u6a21\u6001\u5728\u52a8\u4f5c\u6269\u6563\u8fc7\u7a0b\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u5f71\u54cd\u529b\uff0c\u4ece\u800c\u5b66\u4e60\u5230\u66f4\u4f18\u5148\u8003\u8651\u67d0\u4e9b\u89c2\u6d4b\u6a21\u6001\u7684\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u7b56\u7565\u5728\u6761\u4ef6\u5316\u591a\u4e2a\u89c2\u6d4b\u6a21\u6001\uff08\u5982\u672c\u4f53\u611f\u89c9\u3001\u89c6\u89c9\u548c\u89e6\u89c9\uff09\u65f6\uff0c\u65e0\u6cd5\u6355\u6349\u4e0d\u540c\u6a21\u6001\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u4e0d\u540c\u5f71\u54cd\u529b\u6c34\u5e73\u3002", "method": "FDP\u901a\u8fc7\u56e0\u5b50\u5316\u6269\u6563\u8fc7\u7a0b\u7684\u89c2\u6d4b\u6761\u4ef6\u5316\u6765\u5b9e\u73b0\u6a21\u6001\u4f18\u5148\u7ea7\uff0c\u4f7f\u67d0\u4e9b\u89c2\u6d4b\u6a21\u6001\u53ef\u4ee5\u4f18\u5148\u4e8e\u5176\u4ed6\u6a21\u6001\u3002", "result": "\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\uff0cFDP\u76f8\u6bd4\u6807\u51c6\u6269\u6563\u7b56\u7565\u5728\u591a\u4e2a\u6a21\u62df\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8615%\u7684\u7edd\u5bf9\u6210\u529f\u7387\u63d0\u5347\uff1b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\uff08\u5982\u89c6\u89c9\u5e72\u6270\u6216\u76f8\u673a\u906e\u6321\uff09\uff0cFDP\u7684\u7edd\u5bf9\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6269\u6563\u7b56\u7565\u9ad840%\u3002", "conclusion": "FDP\u4e3a\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u6bd4\u6807\u51c6\u6269\u6563\u7b56\u7565\u66f4\u5b89\u5168\u3001\u66f4\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.16834", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16834", "abs": "https://arxiv.org/abs/2509.16834", "authors": ["Jingxi Xu"], "title": "Robot Learning with Sparsity and Scarcity", "comment": null, "summary": "Unlike in language or vision, one of the fundamental challenges in robot\nlearning is the lack of access to vast data resources. We can further break\ndown the problem into (1) data sparsity from the angle of data representation\nand (2) data scarcity from the angle of data quantity. In this thesis, I will\ndiscuss selected works on two domains: (1) tactile sensing and (2)\nrehabilitation robots, which are exemplars of data sparsity and scarcity,\nrespectively. Tactile sensing is an essential modality for robotics, but\ntactile data are often sparse, and for each interaction with the physical\nworld, tactile sensors can only obtain information about the local area of\ncontact. I will discuss my work on learning vision-free tactile-only\nexploration and manipulation policies through model-free reinforcement learning\nto make efficient use of sparse tactile information. On the other hand,\nrehabilitation robots are an example of data scarcity to the extreme due to the\nsignificant challenge of collecting biosignals from disabled-bodied subjects at\nscale for training. I will discuss my work in collaboration with the medical\nschool and clinicians on intent inferral for stroke survivors, where a hand\northosis developed in our lab collects a set of biosignals from the patient and\nuses them to infer the activity that the patient intends to perform, so the\northosis can provide the right type of physical assistance at the right moment.\nMy work develops machine learning algorithms that enable intent inferral with\nminimal data, including semi-supervised, meta-learning, and generative AI\nmethods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u89e6\u89c9\u611f\u77e5\uff08\u6570\u636e\u7a00\u758f\u6027\uff09\u548c\u5eb7\u590d\u673a\u5668\u4eba\uff08\u6570\u636e\u7a00\u7f3a\u6027\uff09\u4e24\u4e2a\u9886\u57df\uff0c\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u8d44\u6e90\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u6570\u636e\u8868\u793a\u4e0a\u7684\u7a00\u758f\u6027\u548c\u6570\u636e\u6570\u91cf\u4e0a\u7684\u7a00\u7f3a\u6027\u3002\u89e6\u89c9\u611f\u77e5\u6570\u636e\u7a00\u758f\uff0c\u5eb7\u590d\u673a\u5668\u4eba\u6570\u636e\u6781\u5ea6\u7a00\u7f3a\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u5bf9\u4e8e\u89e6\u89c9\u611f\u77e5\uff0c\u91c7\u7528\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6765\u5b66\u4e60\u4ec5\u4f9d\u8d56\u89e6\u89c9\u7684\u63a2\u7d22\u548c\u64cd\u4f5c\u7b56\u7565\uff1b\u5bf9\u4e8e\u5eb7\u590d\u673a\u5668\u4eba\uff0c\u5f00\u53d1\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u548c\u751f\u6210\u5f0fAI\u7b49\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5b9e\u73b0\u610f\u56fe\u63a8\u65ad\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u9ad8\u6548\u5229\u7528\u7a00\u758f\u89e6\u89c9\u4fe1\u606f\u7684\u7b56\u7565\uff0c\u4ee5\u53ca\u80fd\u591f\u5728\u6781\u5c11\u91cf\u6570\u636e\u4e0b\u51c6\u786e\u63a8\u65ad\u4e2d\u98ce\u60a3\u8005\u610f\u56fe\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u758f\u548c\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u89e6\u89c9\u611f\u77e5\u548c\u5eb7\u590d\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.16858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16858", "abs": "https://arxiv.org/abs/2509.16858", "authors": ["Soon Jynn Chu", "Raju Gottumukkala", "Alan Barhorst"], "title": "Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics", "comment": "Submitted to conference", "summary": "The ability of social robots to respond to human emotions is crucial for\nbuilding trust and acceptance in human-robot collaborative environments.\nHowever, developing such capabilities through online reinforcement learning is\nsometimes impractical due to the prohibitive cost of data collection and the\nrisk of generating unsafe behaviors. In this paper, we study the use of offline\nreinforcement learning as a practical and efficient alternative. This technique\nuses pre-collected data to enable emotion-adaptive social robots. We present a\nsystem architecture that integrates multimodal sensing and recognition,\ndecision-making, and adaptive responses. Using a limited dataset from a\nhuman-robot game-playing scenario, we establish a benchmark for comparing\noffline reinforcement learning algorithms that do not require an online\nenvironment. Our results show that BCQ and CQL are more robust to data\nsparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN.\nThis work establishes a foundation for benchmarking offline RL in\nemotion-adaptive robotics and informs future deployment in real-world HRI. Our\nfindings provide empirical insight into the performance of offline\nreinforcement learning algorithms in data-constrained HRI. This work\nestablishes a foundation for benchmarking offline RL in emotion-adaptive\nrobotics and informs its future deployment in real-world HRI, such as in\nconversational agents, educational partners, and personal assistants, require\nreliable emotional responsiveness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4f7f\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u4f7f\u793e\u4ea4\u673a\u5668\u4eba\u80fd\u591f\u57fa\u4e8e\u9884\u6536\u96c6\u6570\u636e\u8fdb\u884c\u60c5\u611f\u81ea\u9002\u5e94\u54cd\u5e94\uff0c\u5728\u6570\u636e\u7a00\u758f\u60c5\u51b5\u4e0bBCQ\u548cCQL\u7b97\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u793e\u4ea4\u673a\u5668\u4eba\u60c5\u611f\u54cd\u5e94\u5f00\u53d1\u4e2d\u6210\u672c\u9ad8\u6602\u4e14\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u591a\u6a21\u6001\u611f\u77e5\u8bc6\u522b\u3001\u51b3\u7b56\u5236\u5b9a\u548c\u81ea\u9002\u5e94\u54cd\u5e94\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u6e38\u620f\u573a\u666f\u7684\u6709\u9650\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "BCQ\u548cCQL\u7b97\u6cd5\u5bf9\u6570\u636e\u7a00\u758f\u6027\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4NFQ\u3001DQN\u548cDDQN\u83b7\u5f97\u66f4\u9ad8\u7684\u72b6\u6001-\u52a8\u4f5c\u503c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u60c5\u611f\u81ea\u9002\u5e94\u673a\u5668\u4eba\u4e2d\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u771f\u5b9e\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.16871", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16871", "abs": "https://arxiv.org/abs/2509.16871", "authors": ["Yitian Shi", "Zicheng Guo", "Rosa Wolf", "Edgar Welte", "Rania Rayyes"], "title": "HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness", "comment": "under review", "summary": "We propose Hand-Object\\emph{(HO)GraspFlow}, an affordance-centric approach\nthat retargets a single RGB with hand-object interaction (HOI) into multi-modal\nexecutable parallel jaw grasps without explicit geometric priors on target\nobjects. Building on foundation models for hand reconstruction and vision, we\nsynthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned\non the following three complementary cues: RGB foundation features as visual\nsemantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.\nOur approach demonstrates high fidelity in grasp synthesis without explicit HOI\ncontact input or object geometry, while maintaining strong contact and taxonomy\nrecognition. Another controlled comparison shows that \\emph{HOGraspFlow}\nconsistently outperforms diffusion-based variants (\\emph{HOGraspDiff}),\nachieving high distributional fidelity and more stable optimization in $SE(3)$.\nWe demonstrate a reliable, object-agnostic grasp synthesis from human\ndemonstrations in real-world experiments, where an average success rate of over\n$83\\%$ is achieved.", "AI": {"tldr": "HOGraspFlow\u662f\u4e00\u79cd\u57fa\u4e8eRGB\u56fe\u50cf\u7684\u624b-\u7269\u4f53\u4ea4\u4e92(HOI)\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u65e0\u9700\u7269\u4f53\u51e0\u4f55\u5148\u9a8c\u5373\u53ef\u751f\u6210\u591a\u6a21\u6001\u5e73\u884c\u5939\u722a\u6293\u53d6\u59ff\u52bf", "motivation": "\u89e3\u51b3\u4ece\u5355\u5f20RGB\u56fe\u50cf\u4e2d\u63d0\u53d6\u53ef\u6267\u884c\u6293\u53d6\u59ff\u52bf\u7684\u6311\u6218\uff0c\u907f\u514d\u5bf9\u76ee\u6807\u7269\u4f53\u663e\u5f0f\u51e0\u4f55\u5148\u9a8c\u7684\u4f9d\u8d56", "method": "\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u624b\u90e8\u91cd\u5efa\u548c\u89c6\u89c9\u5206\u6790\uff0c\u4f7f\u7528\u53bb\u566a\u6d41\u5339\u914d(FM)\u5728SE(3)\u7a7a\u95f4\u4e2d\u5408\u6210\u6293\u53d6\u59ff\u52bf\uff0c\u7ed3\u5408RGB\u57fa\u7840\u7279\u5f81\u3001HOI\u63a5\u89e6\u91cd\u5efa\u548c\u6293\u53d6\u7c7b\u578b\u5206\u7c7b\u611f\u77e5\u5148\u9a8c\u4e09\u4e2a\u4e92\u8865\u7ebf\u7d22", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8d85\u8fc783%\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u53d8\u4f53(HOGraspDiff)\u5728SE(3)\u7a7a\u95f4\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u5206\u5e03\u4fdd\u771f\u5ea6\u548c\u66f4\u7a33\u5b9a\u7684\u4f18\u5316", "conclusion": "HOGraspFlow\u80fd\u591f\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b9e\u73b0\u53ef\u9760\u3001\u7269\u4f53\u65e0\u5173\u7684\u6293\u53d6\u5408\u6210\uff0c\u5c55\u793a\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u6293\u53d6\u5408\u6210\u80fd\u529b"}}
{"id": "2509.16894", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16894", "abs": "https://arxiv.org/abs/2509.16894", "authors": ["Zhijie Qiao", "Haowei Li", "Zhong Cao", "Henry X. Liu"], "title": "End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth Racing", "comment": null, "summary": "F1Tenth is a widely adopted reduced-scale platform for developing and testing\nautonomous racing algorithms, hosting annual competitions worldwide. With high\noperating speeds, dynamic environments, and head-to-head interactions,\nautonomous racing requires algorithms that diverge from those in classical\nautonomous driving. Training such algorithms is particularly challenging: the\nneed for rapid decision-making at high speeds severely limits model capacity.\nTo address this, we propose End2Race, a novel end-to-end imitation learning\nalgorithm designed for head-to-head autonomous racing. End2Race leverages a\nGated Recurrent Unit (GRU) architecture to capture continuous temporal\ndependencies, enabling both short-term responsiveness and long-term strategic\nplanning. We also adopt a sigmoid-based normalization function that transforms\nraw LiDAR scans into spatial pressure tokens, facilitating effective model\ntraining and convergence. The algorithm is extremely efficient, achieving an\ninference time of less than 0.5 milliseconds on a consumer-class GPU.\nExperiments in the F1Tenth simulator demonstrate that End2Race achieves a 94.2%\nsafety rate across 2,400 overtaking scenarios, each with an 8-second time\nlimit, and successfully completes overtakes in 59.2% of cases. This surpasses\nprevious methods and establishes ours as a leading solution for the F1Tenth\nracing testbed. Code is available at\nhttps://github.com/michigan-traffic-lab/End2Race.", "AI": {"tldr": "End2Race\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8eF1Tenth\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u5e73\u53f0\u7684\u5934\u5bf9\u5934\u6bd4\u8d5b\uff0c\u901a\u8fc7GRU\u67b6\u6784\u548cLiDAR\u6570\u636e\u5f52\u4e00\u5316\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u9700\u8981\u4e0e\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u4e0d\u540c\u7684\u7b97\u6cd5\uff0c\u9ad8\u901f\u64cd\u4f5c\u548c\u52a8\u6001\u73af\u5883\u9650\u5236\u4e86\u6a21\u578b\u5bb9\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "\u91c7\u7528Gated Recurrent Unit(GRU)\u67b6\u6784\u6355\u6349\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f7f\u7528\u57fa\u4e8esigmoid\u7684\u5f52\u4e00\u5316\u51fd\u6570\u5c06LiDAR\u626b\u63cf\u8f6c\u6362\u4e3a\u7a7a\u95f4\u538b\u529b\u6807\u8bb0\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u3002", "result": "\u5728F1Tenth\u6a21\u62df\u5668\u4e2d\uff0cEnd2Race\u57282400\u4e2a\u8d85\u8f66\u573a\u666f\u4e2d\u8fbe\u523094.2%\u7684\u5b89\u5168\u7387\uff0c59.2%\u7684\u8d85\u8f66\u6210\u529f\u7387\uff0c\u63a8\u7406\u65f6\u95f4\u5c0f\u4e8e0.5\u6beb\u79d2\u3002", "conclusion": "End2Race\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u4e3aF1Tenth\u8d5b\u8f66\u6d4b\u8bd5\u5e73\u53f0\u7684\u9886\u5148\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u901f\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.16920", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16920", "abs": "https://arxiv.org/abs/2509.16920", "authors": ["Ettilla Mohiuddin Eumi", "Hussein Abbass", "Nadine Marcus"], "title": "SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for Robotic Swarms", "comment": "This paper has been accepted and presented at the 16th International\n  Conference on Swarm Intelligence (ICSI 2025), held on July 11-15, 2025, in\n  Yokohama, Japan", "summary": "Traditional Human-Swarm Interaction (HSI) methods often lack intuitive\nreal-time adaptive interfaces, making decision making slower and increasing\ncognitive load while limiting command flexibility. To solve this, we present\nSwarmChat, a context-aware, multimodal interaction system powered by Large\nLanguage Models (LLMs). SwarmChat enables users to issue natural language\ncommands to robotic swarms using multiple modalities, such as text, voice, or\nteleoperation. The system integrates four LLM-based modules: Context Generator,\nIntent Recognition, Task Planner, and Modality Selector. These modules\ncollaboratively generate context from keywords, detect user intent, adapt\ncommands based on real-time robot state, and suggest optimal communication\nmodalities. Its three-layer architecture offers a dynamic interface with both\nfixed and customizable command options, supporting flexible control while\noptimizing cognitive effort. The preliminary evaluation also shows that the\nSwarmChat's LLM modules provide accurate context interpretation, relevant\nintent recognition, and effective command delivery, achieving high user\nsatisfaction.", "AI": {"tldr": "SwarmChat\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u4eba\u673a\u7fa4\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u5b9e\u73b0\u673a\u5668\u4eba\u96c6\u7fa4\u7684\u76f4\u89c2\u63a7\u5236", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4eba\u673a\u7fa4\u4ea4\u4e92\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u89c2\u5b9e\u65f6\u81ea\u9002\u5e94\u754c\u9762\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5bfc\u81f4\u51b3\u7b56\u901f\u5ea6\u6162\u3001\u8ba4\u77e5\u8d1f\u8377\u589e\u52a0\u4e14\u547d\u4ee4\u7075\u6d3b\u6027\u6709\u9650", "method": "\u7cfb\u7edf\u96c6\u6210\u56db\u4e2aLLM\u6a21\u5757\uff08\u4e0a\u4e0b\u6587\u751f\u6210\u5668\u3001\u610f\u56fe\u8bc6\u522b\u3001\u4efb\u52a1\u89c4\u5212\u5668\u548c\u6a21\u6001\u9009\u62e9\u5668\uff09\uff0c\u91c7\u7528\u4e09\u5c42\u67b6\u6784\u652f\u6301\u6587\u672c\u3001\u8bed\u97f3\u6216\u9065\u64cd\u4f5c\u7b49\u591a\u79cd\u4ea4\u4e92\u65b9\u5f0f", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793aSwarmChat\u7684LLM\u6a21\u5757\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca\u3001\u76f8\u5173\u610f\u56fe\u8bc6\u522b\u548c\u6709\u6548\u547d\u4ee4\u4f20\u9012\uff0c\u83b7\u5f97\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6", "conclusion": "SwarmChat\u901a\u8fc7LLM\u9a71\u52a8\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\uff0c\u4e3a\u4eba\u673a\u7fa4\u4ea4\u4e92\u63d0\u4f9b\u4e86\u52a8\u6001\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u7075\u6d3b\u6027\u548c\u7528\u6237\u4f53\u9a8c"}}
{"id": "2509.16963", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16963", "abs": "https://arxiv.org/abs/2509.16963", "authors": ["Chengjin Wang", "Yanmin Zhou", "Zhipeng Wang", "Zheng Yan", "Feng Luan", "Shuo Jiang", "Runjie Shen", "Hongrui Sang", "Bin He"], "title": "A Reliable Robot Motion Planner in Complex Real-world Environments via Action Imagination", "comment": null, "summary": "Humans and animals can make real-time adjustments to movements by imagining\ntheir action outcomes to prevent unanticipated or even catastrophic motion\nfailures in unknown unstructured environments. Action imagination, as a refined\nsensorimotor strategy, leverages perception-action loops to handle physical\ninteraction-induced uncertainties in perception and system modeling within\ncomplex systems. Inspired by the action-awareness capability of animal\nintelligence, this study proposes an imagination-inspired motion planner (I-MP)\nframework that specifically enhances robots' action reliability by imagining\nplausible spatial states for approaching. After topologizing the workspace,\nI-MP build perception-action loop enabling robots autonomously build contact\nmodels. Leveraging fixed-point theory and Hausdorff distance, the planner\ncomputes convergent spatial states under interaction characteristics and\nmission constraints. By homogenously representing multi-dimensional\nenvironmental characteristics through work, the robot can approach the imagined\nspatial states via real-time computation of energy gradients. Consequently,\nexperimental results demonstrate the practicality and robustness of I-MP in\ncomplex cluttered environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u52a8\u7269\u667a\u80fd\u542f\u53d1\u7684\u60f3\u8c61\u529b\u9a71\u52a8\u8fd0\u52a8\u89c4\u5212\u5668\uff08I-MP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u60f3\u8c61\u53ef\u80fd\u7684\u7a7a\u95f4\u72b6\u6001\u6765\u589e\u5f3a\u673a\u5668\u4eba\u5728\u672a\u77e5\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u52a8\u4f5c\u53ef\u9760\u6027\u3002", "motivation": "\u53d7\u4eba\u7c7b\u548c\u52a8\u7269\u80fd\u591f\u901a\u8fc7\u60f3\u8c61\u52a8\u4f5c\u7ed3\u679c\u6765\u5b9e\u65f6\u8c03\u6574\u8fd0\u52a8\u4ee5\u9632\u6b62\u610f\u5916\u8fd0\u52a8\u5931\u8d25\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u7269\u7406\u4ea4\u4e92\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u3002", "method": "I-MP\u6846\u67b6\u901a\u8fc7\u62d3\u6251\u5316\u5de5\u4f5c\u7a7a\u95f4\uff0c\u6784\u5efa\u611f\u77e5-\u52a8\u4f5c\u5faa\u73af\u4f7f\u673a\u5668\u4eba\u81ea\u4e3b\u5efa\u7acb\u63a5\u89e6\u6a21\u578b\u3002\u5229\u7528\u4e0d\u52a8\u70b9\u7406\u8bba\u548cHausdorff\u8ddd\u79bb\u8ba1\u7b97\u5728\u4ea4\u4e92\u7279\u6027\u548c\u4efb\u52a1\u7ea6\u675f\u4e0b\u7684\u6536\u655b\u7a7a\u95f4\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u529f\u6765\u540c\u8d28\u5316\u8868\u793a\u591a\u7ef4\u73af\u5883\u7279\u5f81\uff0c\u5b9e\u65f6\u8ba1\u7b97\u80fd\u91cf\u68af\u5ea6\u6765\u63a5\u8fd1\u60f3\u8c61\u7684\u7a7a\u95f4\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cI-MP\u5728\u590d\u6742\u6742\u4e71\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u60f3\u8c61\u529b\u9a71\u52a8\u8fd0\u52a8\u89c4\u5212\u5668\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u672a\u77e5\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u52a8\u4f5c\u53ef\u9760\u6027\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u4ea4\u4e92\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.16966", "categories": ["cs.RO", "cs.NA", "math.DG", "math.GR", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.16966", "abs": "https://arxiv.org/abs/2509.16966", "authors": ["Andreas Mueller"], "title": "Geometric Interpolation of Rigid Body Motions", "comment": null, "summary": "The problem of interpolating a rigid body motion is to find a spatial\ntrajectory between a prescribed initial and terminal pose. Two variants of this\ninterpolation problem are addressed. The first is to find a solution that\nsatisfies initial conditions on the k-1 derivatives of the rigid body twist.\nThis is called the kth-order initial value trajectory interpolation problem\n(k-IV-TIP). The second is to find a solution that satisfies conditions on the\nrigid body twist and its k-1 derivatives at the initial and terminal pose. This\nis called the kth-order boundary value trajectory interpolation problem\n(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and\nup to the 4th time derivative are prescribed. Further, a solution to the\n1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The\nlatter is a novel cubic interpolation between two spatial configurations with\ngiven initial and terminal twist. This interpolation is automatically identical\nto the minimum acceleration curve when the twists are set to zero. The general\napproach to derive higher-order solutions is presented. Numerical results are\nshown for two examples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u521a\u4f53\u8fd0\u52a8\u63d2\u503c\u95ee\u9898\u7684\u4e24\u79cd\u53d8\u4f53\uff1ak\u9636\u521d\u503c\u8f68\u8ff9\u63d2\u503c\u95ee\u9898\uff08k-IV-TIP\uff09\u548ck\u9636\u8fb9\u503c\u8f68\u8ff9\u63d2\u503c\u95ee\u9898\uff08k-BV-TIP\uff09\uff0c\u5e76\u7ed9\u51fa\u4e86k=1\u52304\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u6b21\u63d2\u503c\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u521a\u4f53\u8fd0\u52a8\u5728\u7ed9\u5b9a\u521d\u59cb\u548c\u7ec8\u7aef\u4f4d\u59ff\u4e4b\u95f4\u7684\u7a7a\u95f4\u8f68\u8ff9\u63d2\u503c\u95ee\u9898\uff0c\u6ee1\u8db3\u5bf9\u521a\u4f53\u626d\u8f6c\u53ca\u5176\u9ad8\u9636\u5bfc\u6570\u5728\u8fb9\u754c\u5904\u7684\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86k-IV-TIP\u548ck-BV-TIP\u4e24\u79cd\u63d2\u503c\u95ee\u9898\u6846\u67b6\uff0c\u5206\u522b\u5904\u7406\u521d\u503c\u6761\u4ef6\u548c\u8fb9\u503c\u6761\u4ef6\u3002\u5bf9\u4e8ek=1\u52304\u7684\u60c5\u51b5\u7ed9\u51fa\u4e86\u5177\u4f53\u89e3\u6cd5\uff0c\u7279\u522b\u9488\u5bf91-IV-TBP\u95ee\u9898\u63d0\u51fa\u4e86\u4e09\u6b21\u63d2\u503c\u65b9\u6cd5\u3002", "result": "\u6210\u529f\u63a8\u5bfc\u51fa\u9ad8\u9636\u63d2\u503c\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5f53\u626d\u8f6c\u8bbe\u7f6e\u4e3a\u96f6\u65f6\uff0c\u4e09\u6b21\u63d2\u503c\u81ea\u52a8\u9000\u5316\u4e3a\u6700\u5c0f\u52a0\u901f\u5ea6\u66f2\u7ebf\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u521a\u4f53\u8fd0\u52a8\u63d2\u503c\u7684\u7cfb\u7edf\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u5230\u66f4\u9ad8\u9636\u63d2\u503c\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u521a\u4f53\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u5b66\u5de5\u5177\u3002"}}
{"id": "2509.16998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16998", "abs": "https://arxiv.org/abs/2509.16998", "authors": ["Nishka Khendry", "Christos Margadji", "Sebastian W. Pattinson"], "title": "IDfRA: Self-Verification for Iterative Design in Robotic Assembly", "comment": null, "summary": "As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA),\nwhich is designing products for efficient automated assembly, is increasingly\nimportant. Traditional approaches to DfRA rely on manual planning, which is\ntime-consuming, expensive and potentially impractical for complex objects.\nLarge language models (LLM) have exhibited proficiency in semantic\ninterpretation and robotic task planning, stimulating interest in their\napplication to the automation of DfRA. But existing methodologies typically\nrely on heuristic strategies and rigid, hard-coded physics simulators that may\nnot translate into real-world assembly contexts. In this work, we present\nIterative Design for Robotic Assembly (IDfRA), a framework using iterative\ncycles of planning, execution, verification, and re-planning, each informed by\nself-assessment, to progressively enhance design quality within a fixed yet\ninitially under-specified environment, thereby eliminating the physics\nsimulation with the real world itself. The framework accepts as input a target\nstructure together with a partial environmental representation. Through\nsuccessive refinement, it converges toward solutions that reconcile semantic\nfidelity with physical feasibility. Empirical evaluation demonstrates that\nIDfRA attains 73.3\\% top-1 accuracy in semantic recognisability, surpassing the\nbaseline on this metric. Moreover, the resulting assembly plans exhibit robust\nphysical feasibility, achieving an overall 86.9\\% construction success rate,\nwith design quality improving across iterations, albeit not always\nmonotonically. Pairwise human evaluation further corroborates the advantages of\nIDfRA relative to alternative approaches. By integrating self-verification with\ncontext-aware adaptation, the framework evidences strong potential for\ndeployment in unstructured manufacturing scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86IDfRA\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u89c4\u5212-\u6267\u884c-\u9a8c\u8bc1-\u91cd\u89c4\u5212\u5faa\u73af\uff0c\u5229\u7528\u673a\u5668\u4eba\u81ea\u8eab\u8fdb\u884c\u8bbe\u8ba1\u9a8c\u8bc1\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u7269\u7406\u4eff\u771f\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u88c5\u914d\u8bbe\u8ba1\u4f9d\u8d56\u4eba\u5de5\u89c4\u5212\uff0c\u8017\u65f6\u6602\u8d35\u4e14\u590d\u6742\u5bf9\u8c61\u4e0d\u5b9e\u7528\uff1b\u73b0\u6709LLM\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u7b56\u7565\u548c\u786c\u7f16\u7801\u7269\u7406\u4eff\u771f\uff0c\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u88c5\u914d\u573a\u666f", "method": "IDfRA\u6846\u67b6\u91c7\u7528\u8fed\u4ee3\u5faa\u73af\uff1a\u89c4\u5212\u3001\u6267\u884c\u3001\u9a8c\u8bc1\u548c\u91cd\u89c4\u5212\uff0c\u6bcf\u4e2a\u73af\u8282\u90fd\u57fa\u4e8e\u81ea\u6211\u8bc4\u4f30\uff0c\u5728\u56fa\u5b9a\u4f46\u521d\u59cb\u672a\u5b8c\u5168\u6307\u5b9a\u7684\u73af\u5883\u4e2d\u9010\u6b65\u63d0\u5347\u8bbe\u8ba1\u8d28\u91cf", "result": "IDfRA\u5728\u8bed\u4e49\u53ef\u8bc6\u522b\u6027\u4e0a\u8fbe\u523073.3%\u7684top-1\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\uff1b\u88c5\u914d\u8ba1\u5212\u6574\u4f53\u6784\u5efa\u6210\u529f\u7387\u8fbe\u523086.9%\uff0c\u8bbe\u8ba1\u8d28\u91cf\u968f\u8fed\u4ee3\u63d0\u5347", "conclusion": "\u901a\u8fc7\u81ea\u6211\u9a8c\u8bc1\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u9002\u5e94\u7684\u7ed3\u5408\uff0c\u8be5\u6846\u67b6\u5728\u975e\u7ed3\u6784\u5316\u5236\u9020\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u90e8\u7f72\u6f5c\u529b"}}
{"id": "2509.17010", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17010", "abs": "https://arxiv.org/abs/2509.17010", "authors": ["Rajpal Singh", "Aditya Singh", "Chidre Shravista Kashyap", "Jishnu Keshavan"], "title": "Generalized Momenta-Based Koopman Formalism for Robust Control of Euler-Lagrangian Systems", "comment": null, "summary": "This paper presents a novel Koopman operator formulation for Euler Lagrangian\ndynamics that employs an implicit generalized momentum-based state space\nrepresentation, which decouples a known linear actuation channel from state\ndependent dynamics and makes the system more amenable to linear Koopman\nmodeling. By leveraging this structural separation, the proposed formulation\nonly requires to learn the unactuated dynamics rather than the complete\nactuation dependent system, thereby significantly reducing the number of\nlearnable parameters, improving data efficiency, and lowering overall model\ncomplexity. In contrast, conventional explicit formulations inherently couple\ninputs with the state dependent terms in a nonlinear manner, making them more\nsuitable for bilinear Koopman models, which are more computationally expensive\nto train and deploy. Notably, the proposed scheme enables the formulation of\nlinear models that achieve superior prediction performance compared to\nconventional bilinear models while remaining substantially more efficient. To\nrealize this framework, we present two neural network architectures that\nconstruct Koopman embeddings from actuated or unactuated data, enabling\nflexible and efficient modeling across different tasks. Robustness is ensured\nthrough the integration of a linear Generalized Extended State Observer (GESO),\nwhich explicitly estimates disturbances and compensates for them in real time.\nThe combined momentum-based Koopman and GESO framework is validated through\ncomprehensive trajectory tracking simulations and experiments on robotic\nmanipulators, demonstrating superior accuracy, robustness, and learning\nefficiency relative to state of the art alternatives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u52a8\u91cf\u7684\u9690\u5f0fKoopman\u7b97\u5b50\u65b9\u6cd5\uff0c\u7528\u4e8e\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u901a\u8fc7\u89e3\u8026\u7ebf\u6027\u9a71\u52a8\u901a\u9053\u548c\u72b6\u6001\u4f9d\u8d56\u52a8\u529b\u5b66\uff0c\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u663e\u5f0fKoopman\u6a21\u578b\u5c06\u8f93\u5165\u4e0e\u72b6\u6001\u4f9d\u8d56\u9879\u975e\u7ebf\u6027\u8026\u5408\uff0c\u9700\u8981\u5b66\u4e60\u5b8c\u6574\u7cfb\u7edf\uff0c\u5bfc\u81f4\u53c2\u6570\u591a\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7ebf\u6027Koopman\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9690\u5f0f\u5e7f\u4e49\u52a8\u91cf\u72b6\u6001\u7a7a\u95f4\u8868\u793a\uff0c\u5206\u79bb\u7ebf\u6027\u9a71\u52a8\u901a\u9053\uff1b\u63d0\u51fa\u4e24\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6784\u5efaKoopman\u5d4c\u5165\uff1b\u96c6\u6210\u7ebf\u6027\u5e7f\u4e49\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668(GESO)\u8fdb\u884c\u5b9e\u65f6\u6270\u52a8\u4f30\u8ba1\u548c\u8865\u507f\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u4e0a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u53cc\u7ebf\u6027\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u5b66\u4e60\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u91cf\u57faKoopman\u4e0eGESO\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7ebf\u6027\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u52a8\u529b\u5b66\u7cfb\u7edf\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17042", "abs": "https://arxiv.org/abs/2509.17042", "authors": ["Zengqi Peng", "Yusen Xie", "Yubin Wang", "Rui Yang", "Qifeng Chen", "Jun Ma"], "title": "Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning", "comment": null, "summary": "The advancement of foundation models fosters new initiatives for policy\nlearning in achieving safe and efficient autonomous driving. However, a\ncritical bottleneck lies in the manual engineering of reward functions and\ntraining curricula for complex and dynamic driving tasks, which is a\nlabor-intensive and time-consuming process. To address this problem, we propose\nOGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning\nframework that leverages vision-language model (VLM)-based multi-agent\ncollaboration. Our framework capitalizes on advanced reasoning and multimodal\nunderstanding capabilities of VLMs to construct a hierarchical agent system.\nSpecifically, a centralized orchestrator plans high-level training objectives,\nwhile a generation module employs a two-step analyze-then-generate process for\nefficient generation of reward-curriculum pairs. A reflection module then\nfacilitates iterative optimization based on the online evaluation. Furthermore,\na dedicated memory module endows the VLM agents with the capabilities of\nlong-term memory. To enhance robustness and diversity of the generation\nprocess, we introduce a parallel generation scheme and a human-in-the-loop\ntechnique for augmentation of the reward observation space. Through efficient\nmulti-agent cooperation and leveraging rich multimodal information, OGR enables\nthe online evolution of reinforcement learning policies to acquire\ninteraction-aware driving skills. Extensive experiments in the CARLA simulator\ndemonstrate the superior performance, robust generalizability across distinct\nurban scenarios, and strong compatibility with various RL algorithms. Further\nreal-world experiments highlight the practical viability and effectiveness of\nour framework. The source code will be available upon acceptance of the paper.", "AI": {"tldr": "OGR\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u5c42\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u751f\u6210\u5956\u52b1\u51fd\u6570\u548c\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u7684\u74f6\u9888\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u4e2d\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u548c\u8bad\u7ec3\u8bfe\u7a0b\u7684\u9ad8\u6210\u672c\u3001\u8017\u65f6\u95ee\u9898\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u4f18\u52bf\u5b9e\u73b0\u81ea\u52a8\u5316\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u63d0\u51faOGR\u6846\u67b6\uff0c\u5305\u542b\u7f16\u6392\u5668\u3001\u751f\u6210\u6a21\u5757\u3001\u53cd\u601d\u6a21\u5757\u548c\u8bb0\u5fc6\u6a21\u5757\u3002\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5229\u7528VLM\u7684\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u6790-\u751f\u6210\u4e24\u6b65\u6cd5\u81ea\u52a8\u751f\u6210\u5956\u52b1-\u8bfe\u7a0b\u5bf9\uff0c\u5e76\u652f\u6301\u5728\u7ebf\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e0e\u591a\u79cdRL\u7b97\u6cd5\u517c\u5bb9\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "OGR\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548cVLM\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u9ad8\u6548\u5728\u7ebf\u5b66\u4e60\uff0c\u4e3a\u81ea\u52a8\u5316\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17053", "categories": ["cs.RO", "68T40, 93C85", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.17053", "abs": "https://arxiv.org/abs/2509.17053", "authors": ["Haizhou Ge", "Yufei Jia", "Zheng Li", "Yue Li", "Zhixing Chen", "Ruqi Huang", "Guyue Zhou"], "title": "FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks", "comment": null, "summary": "Contact-rich manipulation is crucial for robots to perform tasks requiring\nprecise force control, such as insertion, assembly, and in-hand manipulation.\nHowever, most imitation learning (IL) policies remain position-centric and lack\nexplicit force awareness, and adding force/torque sensors to collaborative\nrobot arms is often costly and requires additional hardware design. To overcome\nthese issues, we propose FILIC, a Force-guided Imitation Learning framework\nwith impedance torque control. FILIC integrates a Transformer-based IL policy\nwith an impedance controller in a dual-loop structure, enabling compliant\nforce-informed, force-executed manipulation. For robots without force/torque\nsensors, we introduce a cost-effective end-effector force estimator using joint\ntorque measurements through analytical Jacobian-based inversion while\ncompensating with model-predicted torques from a digital twin. We also design\ncomplementary force feedback frameworks via handheld haptics and VR\nvisualization to improve demonstration quality. Experiments show that FILIC\nsignificantly outperforms vision-only and joint-torque-based methods, achieving\nsafer, more compliant, and adaptable contact-rich manipulation. Our code can be\nfound in https://github.com/TATP-233/FILIC.", "AI": {"tldr": "FILIC\u662f\u4e00\u4e2a\u529b\u5f15\u5bfc\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u963b\u6297\u626d\u77e9\u63a7\u5236\u548c\u53cc\u73af\u7ed3\u6784\u5b9e\u73b0\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u65e0\u9700\u6602\u8d35\u7684\u529b/\u626d\u77e9\u4f20\u611f\u5668", "motivation": "\u89e3\u51b3\u5f53\u524d\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7f3a\u4e4f\u529b\u611f\u77e5\u80fd\u529b\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4e3a\u534f\u4f5c\u673a\u5668\u4eba\u81c2\u6dfb\u52a0\u529b/\u626d\u77e9\u4f20\u611f\u5668\u6210\u672c\u9ad8\u6602\u7684\u6311\u6218", "method": "\u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u548c\u963b\u6297\u63a7\u5236\u5668\uff0c\u4f7f\u7528\u5173\u8282\u626d\u77e9\u6d4b\u91cf\u901a\u8fc7\u96c5\u53ef\u6bd4\u9006\u77e9\u9635\u4f30\u8ba1\u672b\u7aef\u6267\u884c\u5668\u529b\uff0c\u5e76\u8bbe\u8ba1\u624b\u6301\u89e6\u89c9\u548cVR\u53ef\u89c6\u5316\u529b\u53cd\u9988\u6846\u67b6", "result": "FILIC\u663e\u8457\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u89c6\u89c9\u548c\u5173\u8282\u626d\u77e9\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u67d4\u987a\u548c\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c", "conclusion": "FILIC\u6846\u67b6\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u4f20\u611f\u5668"}}
{"id": "2509.17057", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17057", "abs": "https://arxiv.org/abs/2509.17057", "authors": ["Masaki Murooka", "Tomohiro Motoda", "Ryoichi Nakajo", "Hanbit Oh", "Koshi Makihara", "Keisuke Shirai", "Yukiyasu Domae"], "title": "RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments", "comment": null, "summary": "RoboManipBaselines is an open framework for robot imitation learning that\nunifies data collection, training, and evaluation across simulation and real\nrobots. We introduce it as a platform enabling systematic benchmarking of\ndiverse tasks, robots, and multimodal policies with emphasis on integration,\ngenerality, extensibility, and reproducibility.", "AI": {"tldr": "RoboManipBaselines\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u7684\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u5bf9\u591a\u6837\u5316\u4efb\u52a1\u3001\u673a\u5668\u4eba\u548c\u591a\u6a21\u6001\u7b56\u7565\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f3a\u8c03\u96c6\u6210\u6027\u3001\u901a\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\u3001\u673a\u5668\u4eba\u548c\u591a\u6a21\u6001\u7b56\u7565\u57fa\u51c6\u6d4b\u8bd5\u7684\u5e73\u53f0\u3002", "conclusion": "RoboManipBaselines\u4f5c\u4e3a\u4e00\u4e2a\u96c6\u6210\u5e73\u53f0\uff0c\u80fd\u591f\u4fc3\u8fdb\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u9886\u57df\u7684\u7cfb\u7edf\u5316\u7814\u7a76\u548c\u6bd4\u8f83\u3002"}}
{"id": "2509.17080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17080", "abs": "https://arxiv.org/abs/2509.17080", "authors": ["Ruiguo Zhong", "Ruoyu Yao", "Pei Liu", "Xiaolong Chen", "Rui Yang", "Jun Ma"], "title": "CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving", "comment": null, "summary": "Accurate trajectory prediction and motion planning are crucial for autonomous\ndriving systems to navigate safely in complex, interactive environments\ncharacterized by multimodal uncertainties. However, current\ngeneration-then-evaluation frameworks typically construct multiple plausible\ntrajectory hypotheses but ultimately adopt a single most likely outcome,\nleading to overconfident decisions and a lack of fallback strategies that are\nvital for safety in rare but critical scenarios. Moreover, the usual decoupling\nof prediction and planning modules could result in socially inconsistent or\nunrealistic joint trajectories, especially in highly interactive traffic. To\naddress these challenges, we propose a contingency-aware diffusion planner\n(CoPlanner), a unified framework that jointly models multi-agent interactive\ntrajectory generation and contingency-aware motion planning. Specifically, the\npivot-conditioned diffusion mechanism anchors trajectory sampling on a\nvalidated, shared short-term segment to preserve temporal consistency, while\nstochastically generating diverse long-horizon branches that capture multimodal\nmotion evolutions. In parallel, we design a contingency-aware multi-scenario\nscoring strategy that evaluates candidate ego trajectories across multiple\nplausible long-horizon evolution scenarios, balancing safety, progress, and\ncomfort. This integrated design preserves feasible fallback options and\nenhances robustness under uncertainty, leading to more realistic\ninteraction-aware planning. Extensive closed-loop experiments on the nuPlan\nbenchmark demonstrate that CoPlanner consistently surpasses state-of-the-art\nmethods on both Val14 and Test14 datasets, achieving significant improvements\nin safety and comfort under both reactive and non-reactive settings. Code and\nmodel will be made publicly available upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e94\u6025\u611f\u77e5\u6269\u6563\u89c4\u5212\u5668\uff08CoPlanner\uff09\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8f68\u8ff9\u751f\u6210\u548c\u5e94\u6025\u611f\u77e5\u8fd0\u52a8\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9884\u6d4b\u4e0e\u89c4\u5212\u6a21\u5757\u89e3\u8026\u5bfc\u81f4\u7684\u8fc7\u5ea6\u81ea\u4fe1\u51b3\u7b56\u548c\u7f3a\u4e4f\u5e94\u6025\u7b56\u7565\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u751f\u6210-\u8bc4\u4f30\u6846\u67b6\u901a\u5e38\u53ea\u91c7\u7528\u6700\u53ef\u80fd\u7684\u7ed3\u679c\uff0c\u5bfc\u81f4\u5728\u5173\u952e\u573a\u666f\u4e2d\u7f3a\u4e4f\u5e94\u6025\u7b56\u7565\uff1b2\uff09\u9884\u6d4b\u4e0e\u89c4\u5212\u6a21\u5757\u89e3\u8026\u53ef\u80fd\u5bfc\u81f4\u793e\u4f1a\u4e0d\u4e00\u81f4\u6216\u4e0d\u73b0\u5b9e\u7684\u8054\u5408\u8f68\u8ff9\u3002", "method": "CoPlanner\u91c7\u7528\u67a2\u8f74\u6761\u4ef6\u6269\u6563\u673a\u5236\uff0c\u5728\u9a8c\u8bc1\u7684\u5171\u4eab\u77ed\u671f\u6bb5\u4e0a\u951a\u5b9a\u8f68\u8ff9\u91c7\u6837\u4ee5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u968f\u673a\u751f\u6210\u591a\u6837\u5316\u7684\u957f\u671f\u5206\u652f\u4ee5\u6355\u6349\u591a\u6a21\u6001\u8fd0\u52a8\u6f14\u5316\u3002\u5e76\u884c\u8bbe\u8ba1\u4e86\u5e94\u6025\u611f\u77e5\u591a\u573a\u666f\u8bc4\u5206\u7b56\u7565\uff0c\u8bc4\u4f30\u5019\u9009\u81ea\u8f66\u8f68\u8ff9\u5728\u591a\u4e2a\u53ef\u80fd\u957f\u671f\u6f14\u5316\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u95ed\u73af\u5b9e\u9a8c\u8868\u660e\uff0cCoPlanner\u5728Val14\u548cTest14\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u53cd\u5e94\u6027\u548c\u975e\u53cd\u5e94\u6027\u8bbe\u7f6e\u4e0b\u5747\u5b9e\u73b0\u4e86\u5b89\u5168\u548c\u8212\u9002\u6027\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CoPlanner\u7684\u7edf\u4e00\u6846\u67b6\u901a\u8fc7\u4fdd\u6301\u53ef\u884c\u7684\u5e94\u6025\u9009\u9879\u548c\u589e\u5f3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u73b0\u5b9e\u7684\u4ea4\u4e92\u611f\u77e5\u89c4\u5212\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17125", "abs": "https://arxiv.org/abs/2509.17125", "authors": ["Liang Heng", "Jiadong Xu", "Yiwen Wang", "Xiaoqi Li", "Muhe Cai", "Yan Shen", "Juan Zhu", "Guanghui Ren", "Hao Dong"], "title": "Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation", "comment": null, "summary": "Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)\nrequire a robot to manipulate objects with precise semantic and geometric\nreasoning. Existing approaches either rely on pre-collected demonstrations that\nstruggle to capture complex geometric constraints or generate goal-state\nobservations to capture semantic and geometric knowledge, but fail to\nexplicitly couple object transformation with action prediction, resulting in\nerrors due to generative noise. To address these limitations, we propose\nImagine2Act, a 3D imitation-learning framework that incorporates semantic and\ngeometric constraints of objects into policy learning to tackle high-precision\nmanipulation tasks. We first generate imagined goal images conditioned on\nlanguage instructions and reconstruct corresponding 3D point clouds to provide\nrobust semantic and geometric priors. These imagined goal point clouds serve as\nadditional inputs to the policy model, while an object-action consistency\nstrategy with soft pose supervision explicitly aligns predicted end-effector\nmotion with generated object transformation. This design enables Imagine2Act to\nreason about semantic and geometric relationships between objects and predict\naccurate actions across diverse tasks. Experiments in both simulation and the\nreal world demonstrate that Imagine2Act outperforms previous state-of-the-art\npolicies. More visualizations can be found at\nhttps://sites.google.com/view/imagine2act.", "AI": {"tldr": "Imagine2Act\u662f\u4e00\u4e2a3D\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u60f3\u8c61\u7684\u76ee\u6807\u56fe\u50cf\u548c\u70b9\u4e91\u6765\u6574\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7ea6\u675f\uff0c\u89e3\u51b3\u5173\u7cfb\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9884\u6536\u96c6\u7684\u6f14\u793a\u6570\u636e\u96be\u4ee5\u6355\u6349\u590d\u6742\u51e0\u4f55\u7ea6\u675f\uff0c\u8981\u4e48\u751f\u6210\u76ee\u6807\u72b6\u6001\u89c2\u5bdf\u4f46\u672a\u80fd\u663e\u5f0f\u8026\u5408\u7269\u4f53\u53d8\u6362\u4e0e\u52a8\u4f5c\u9884\u6d4b\uff0c\u5bfc\u81f4\u751f\u6210\u566a\u58f0\u5f15\u8d77\u7684\u9519\u8bef", "method": "\u9996\u5148\u751f\u6210\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u7684\u60f3\u8c61\u76ee\u6807\u56fe\u50cf\u5e76\u91cd\u5efa\u5bf9\u5e943D\u70b9\u4e91\uff0c\u5c06\u8fd9\u4e9b\u70b9\u4e91\u4f5c\u4e3a\u7b56\u7565\u6a21\u578b\u7684\u989d\u5916\u8f93\u5165\uff0c\u540c\u65f6\u4f7f\u7528\u8f6f\u59ff\u6001\u76d1\u7763\u7684\u5bf9\u8c61-\u52a8\u4f5c\u4e00\u81f4\u6027\u7b56\u7565\u6765\u5bf9\u9f50\u9884\u6d4b\u7684\u672b\u7aef\u6267\u884c\u5668\u8fd0\u52a8\u4e0e\u751f\u6210\u7684\u7269\u4f53\u53d8\u6362", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u90fd\u8868\u660eImagine2Act\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u7b56\u7565", "conclusion": "Imagine2Act\u80fd\u591f\u63a8\u7406\u7269\u4f53\u95f4\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u5173\u7cfb\uff0c\u5e76\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u9884\u6d4b\u51c6\u786e\u52a8\u4f5c"}}
{"id": "2509.17141", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17141", "abs": "https://arxiv.org/abs/2509.17141", "authors": ["Jingjing Chen", "Hongjie Fang", "Chenxi Wang", "Shiquan Wang", "Cewu Lu"], "title": "History-Aware Visuomotor Policy Learning via Point Tracking", "comment": null, "summary": "Many manipulation tasks require memory beyond the current observation, yet\nmost visuomotor policies rely on the Markov assumption and thus struggle with\nrepeated states or long-horizon dependencies. Existing methods attempt to\nextend observation horizons but remain insufficient for diverse memory\nrequirements. To this end, we propose an object-centric history representation\nbased on point tracking, which abstracts past observations into a compact and\nstructured form that retains only essential task-relevant information. Tracked\npoints are encoded and aggregated at the object level, yielding a compact\nhistory representation that can be seamlessly integrated into various\nvisuomotor policies. Our design provides full history-awareness with high\ncomputational efficiency, leading to improved overall task performance and\ndecision accuracy. Through extensive evaluations on diverse manipulation tasks,\nwe show that our method addresses multiple facets of memory requirements - such\nas task stage identification, spatial memorization, and action counting, as\nwell as longer-term demands like continuous and pre-loaded memory - and\nconsistently outperforms both Markovian baselines and prior history-based\napproaches. Project website: http://tonyfang.net/history", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u8ddf\u8e2a\u7684\u7269\u4f53\u4e2d\u5fc3\u5386\u53f2\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e2d\u7684\u8bb0\u5fc6\u9700\u6c42\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u8fc7\u53bb\u89c2\u5bdf\u62bd\u8c61\u4e3a\u7d27\u51d1\u7684\u7ed3\u6784\u5316\u5f62\u5f0f\uff0c\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u548c\u51b3\u7b56\u51c6\u786e\u6027\u3002", "motivation": "\u8bb8\u591a\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u8d85\u8d8a\u5f53\u524d\u89c2\u5bdf\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u4f46\u5927\u591a\u6570\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4f9d\u8d56\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\uff0c\u96be\u4ee5\u5904\u7406\u91cd\u590d\u72b6\u6001\u6216\u957f\u65f6\u4f9d\u8d56\u5173\u7cfb\u3002\u73b0\u6709\u65b9\u6cd5\u5c1d\u8bd5\u6269\u5c55\u89c2\u5bdf\u89c6\u91ce\u4f46\u4ecd\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u7684\u8bb0\u5fc6\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u70b9\u8ddf\u8e2a\u7684\u7269\u4f53\u4e2d\u5fc3\u5386\u53f2\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u8ddf\u8e2a\u70b9\u7f16\u7801\u5e76\u5728\u7269\u4f53\u7ea7\u522b\u805a\u5408\uff0c\u751f\u6210\u7d27\u51d1\u7684\u5386\u53f2\u8868\u793a\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e2d\u3002", "result": "\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u52a1\u9636\u6bb5\u8bc6\u522b\u3001\u7a7a\u95f4\u8bb0\u5fc6\u3001\u52a8\u4f5c\u8ba1\u6570\u7b49\u591a\u79cd\u8bb0\u5fc6\u9700\u6c42\uff0c\u4ee5\u53ca\u8fde\u7eed\u548c\u9884\u52a0\u8f7d\u8bb0\u5fc6\u7b49\u957f\u671f\u9700\u6c42\uff0c\u6027\u80fd\u4f18\u4e8e\u9a6c\u5c14\u53ef\u592b\u57fa\u7ebf\u548c\u73b0\u6709\u5386\u53f2\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7269\u4f53\u4e2d\u5fc3\u5386\u53f2\u8868\u793a\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5386\u53f2\u611f\u77e5\u80fd\u529b\uff0c\u5177\u6709\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6574\u4f53\u4efb\u52a1\u6027\u80fd\u548c\u51b3\u7b56\u51c6\u786e\u6027\u3002"}}
{"id": "2509.17195", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17195", "abs": "https://arxiv.org/abs/2509.17195", "authors": ["Damian Owerko", "Frederic Vatnsdal", "Saurav Agarwal", "Vijay Kumar", "Alejandro Ribeiro"], "title": "MAST: Multi-Agent Spatial Transformer for Learning to Collaborate", "comment": null, "summary": "This article presents a novel multi-agent spatial transformer (MAST) for\nlearning communication policies in large-scale decentralized and collaborative\nmulti-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from:\n(i) partial observable states as robots make only localized perception, (ii)\nlimited communication range with no central server, and (iii) independent\nexecution of actions. The robots need to optimize a common task-specific\nobjective, which, under the restricted setting, must be done using a\ncommunication policy that exhibits the desired collaborative behavior. The\nproposed MAST is a decentralized transformer architecture that learns\ncommunication policies to compute abstract information to be shared with other\nagents and processes the received information with the robot's own\nobservations. The MAST extends the standard transformer with new positional\nencoding strategies and attention operations that employ windowing to limit the\nreceptive field for MRS. These are designed for local computation,\nshift-equivariance, and permutation equivariance, making it a promising\napproach for DC-MRS. We demonstrate the efficacy of MAST on decentralized\nassignment and navigation (DAN) and decentralized coverage control. Efficiently\ntrained using imitation learning in a centralized setting, the decentralized\nMAST policy is robust to communication delays, scales to large teams, and\nperforms better than the baselines and other learning-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u7a7a\u95f4\u53d8\u6362\u5668\uff08MAST\uff09\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08DC-MRS\uff09\u4e2d\u5b66\u4e60\u901a\u4fe1\u7b56\u7565\u3002MAST\u901a\u8fc7\u6539\u8fdb\u7684\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5c40\u90e8\u89c2\u6d4b\u3001\u6709\u9650\u901a\u4fe1\u8303\u56f4\u548c\u72ec\u7acb\u6267\u884c\u52a8\u4f5c\u7b49\u6311\u6218\u3002", "motivation": "DC-MRS\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5c40\u90e8\u53ef\u89c2\u6d4b\u72b6\u6001\u3001\u6709\u9650\u901a\u4fe1\u8303\u56f4\uff08\u65e0\u4e2d\u592e\u670d\u52a1\u5668\uff09\u4ee5\u53ca\u72ec\u7acb\u6267\u884c\u52a8\u4f5c\u3002\u673a\u5668\u4eba\u9700\u8981\u5728\u53d7\u9650\u73af\u5883\u4e0b\u901a\u8fc7\u901a\u4fe1\u7b56\u7565\u4f18\u5316\u5171\u540c\u4efb\u52a1\u76ee\u6807\uff0c\u5b9e\u73b0\u671f\u671b\u7684\u534f\u4f5c\u884c\u4e3a\u3002", "method": "MAST\u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u53d8\u6362\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u65b0\u7684\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u548c\u91c7\u7528\u7a97\u53e3\u5316\u9650\u5236\u611f\u53d7\u91ce\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\uff0c\u5b66\u4e60\u901a\u4fe1\u7b56\u7565\u6765\u8ba1\u7b97\u8981\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u5171\u4eab\u7684\u62bd\u8c61\u4fe1\u606f\uff0c\u5e76\u5c06\u63a5\u6536\u5230\u7684\u4fe1\u606f\u4e0e\u673a\u5668\u4eba\u81ea\u8eab\u89c2\u6d4b\u76f8\u7ed3\u5408\u5904\u7406\u3002", "result": "\u5728\u53bb\u4e2d\u5fc3\u5316\u5206\u914d\u5bfc\u822a\uff08DAN\uff09\u548c\u53bb\u4e2d\u5fc3\u5316\u8986\u76d6\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAST\u7b56\u7565\u5bf9\u901a\u4fe1\u5ef6\u8fdf\u5177\u6709\u9c81\u68d2\u6027\uff0c\u53ef\u6269\u5c55\u5230\u5927\u578b\u56e2\u961f\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u5176\u4ed6\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "MAST\u901a\u8fc7\u5c40\u90e8\u8ba1\u7b97\u3001\u5e73\u79fb\u7b49\u53d8\u6027\u548c\u7f6e\u6362\u7b49\u53d8\u6027\u7279\u6027\uff0c\u4e3aDC-MRS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2509.17198", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17198", "abs": "https://arxiv.org/abs/2509.17198", "authors": ["Baoshan Song", "Weisong Wen", "Qi Zhang", "Bing Xu", "Li-Ta Hsu"], "title": "Certifiably Optimal Doppler Positioning using Opportunistic LEO Satellites", "comment": "This manuscript has been submitted to IEEE Transactions on Aerospace\n  and Electronic Systems (TAES). The current version is uploaded to arXiv for\n  open access and reference purposes only", "summary": "To provide backup and augmentation to global navigation satellite system\n(GNSS), Doppler shift from Low Earth Orbit (LEO) satellites can be employed as\nsignals of opportunity (SOP) for position, navigation and timing (PNT). Since\nthe Doppler positioning problem is non-convex, local searching methods may\nproduce two types of estimates: a global optimum without notice or a local\noptimum given an inexact initial estimate. As exact initialization is\nunavailable in some unknown environments, a guaranteed global optimization\nmethod in no need of initialization becomes necessary. To achieve this goal, we\npropose a certifiably optimal LEO Doppler positioning method by utilizing\nconvex optimization. In this paper, the certifiable positioning method is\nimplemented through a graduated weight approximation (GWA) algorithm and\nsemidefinite programming (SDP) relaxation. To guarantee the optimality, we\nderive the necessary conditions for optimality in ideal noiseless cases and\nsufficient noise bounds conditions in noisy cases. Simulation and real tests\nare conducted to evaluate the effectiveness and robustness of the proposed\nmethod. Specially, the real test using Iridium-NEXT satellites shows that the\nproposed method estimates an certifiably optimal solution with an 3D\npositioning error of 140 m without initial estimates while Gauss-Newton and\nDog-Leg are trapped in local optima when the initial point is equal or larger\nthan 1000 km away from the ground truth. Moreover, the certifiable estimation\ncan also be used as initialization in local searching methods to lower down the\n3D positioning error to 130 m.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u53ef\u8bc1\u660e\u6700\u4f18LEO\u591a\u666e\u52d2\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7GWA\u7b97\u6cd5\u548cSDP\u677e\u5f1b\u5b9e\u73b0\uff0c\u65e0\u9700\u521d\u59cb\u4f30\u8ba1\u5373\u53ef\u83b7\u5f97\u5168\u5c40\u6700\u4f18\u89e3\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\u65e0\u6cd5\u83b7\u5f97\u7cbe\u786e\u521d\u59cb\u4f30\u8ba1\u65f6\uff0c\u4f20\u7edf\u7684\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\u53ef\u80fd\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u521d\u59cb\u5316\u7684\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6bd5\u4e1a\u6743\u91cd\u8fd1\u4f3c(GWA)\u7b97\u6cd5\u548c\u534a\u5b9a\u89c4\u5212(SDP)\u677e\u5f1b\u6280\u672f\uff0c\u63a8\u5bfc\u4e86\u7406\u60f3\u65e0\u566a\u60c5\u51b5\u4e0b\u7684\u6700\u4f18\u6027\u5fc5\u8981\u6761\u4ef6\u548c\u6709\u566a\u60c5\u51b5\u4e0b\u7684\u5145\u5206\u566a\u58f0\u8fb9\u754c\u6761\u4ef6\u3002", "result": "\u4f7f\u7528Iridium-NEXT\u536b\u661f\u7684\u771f\u5b9e\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u521d\u59cb\u4f30\u8ba1\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u53ef\u8bc1\u660e\u6700\u4f18\u89e3\uff0c3D\u5b9a\u4f4d\u8bef\u5dee\u4e3a140\u7c73\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5728\u521d\u59cb\u70b9\u8ddd\u79bb\u771f\u5b9e\u4f4d\u7f6e1000\u516c\u91cc\u65f6\u4f1a\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5168\u5c40\u6700\u4f18\u5b9a\u4f4d\u89e3\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\u7684\u521d\u59cb\u5316\uff0c\u5c063D\u5b9a\u4f4d\u8bef\u5dee\u8fdb\u4e00\u6b65\u964d\u4f4e\u5230130\u7c73\u3002"}}
{"id": "2509.17204", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17204", "abs": "https://arxiv.org/abs/2509.17204", "authors": ["James R. Han", "Mithun Vanniasinghe", "Hshmat Sahak", "Nicholas Rhinehart", "Timothy D. Barfoot"], "title": "Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation", "comment": "8 pages. Under review at ICRA 2026", "summary": "Scaling Reinforcement Learning to in-the-wild social robot navigation is both\ndata-intensive and unsafe, since policies must learn through direct interaction\nand inevitably encounter collisions. Offline Imitation learning (IL) avoids\nthese risks by collecting expert demonstrations safely, training entirely\noffline, and deploying policies zero-shot. However, we find that naively\napplying Behaviour Cloning (BC) to social navigation is insufficient; achieving\nstrong performance requires careful architectural and training choices. We\npresent Ratatouille, a pipeline and model architecture that, without changing\nthe data, reduces collisions per meter by 6 times and improves success rate by\n3 times compared to naive BC. We validate our approach in both simulation and\nthe real world, where we collected over 11 hours of data on a dense university\ncampus. We further demonstrate qualitative results in a public food court. Our\nfindings highlight that thoughtful IL design, rather than additional data, can\nsubstantially improve safety and reliability in real-world social navigation.\nVideo: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.", "AI": {"tldr": "Ratatouille\u662f\u4e00\u4e2a\u7528\u4e8e\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u7ba1\u9053\u548c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u9009\u62e9\uff0c\u76f8\u6bd4\u7b80\u5355\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u78b0\u649e\u7387\u964d\u4f4e6\u500d\uff0c\u6210\u529f\u7387\u63d0\u9ad83\u500d\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u8fdb\u884c\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5f3a\u5316\u5b66\u4e60\u65e2\u6570\u636e\u5bc6\u96c6\u53c8\u4e0d\u5b89\u5168\uff0c\u56e0\u4e3a\u7b56\u7565\u5fc5\u987b\u901a\u8fc7\u76f4\u63a5\u4ea4\u4e92\u5b66\u4e60\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u9047\u5230\u78b0\u649e\u3002\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u53ef\u4ee5\u5b89\u5168\u5730\u6536\u96c6\u4e13\u5bb6\u6f14\u793a\uff0c\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3\uff0c\u5e76\u96f6\u6837\u672c\u90e8\u7f72\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86Ratatouille\u7ba1\u9053\u548c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u9009\u62e9\u6765\u6539\u8fdb\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u800c\u4e0d\u9700\u8981\u6539\u53d8\u6570\u636e\u3002\u5728\u5bc6\u96c6\u5927\u5b66\u6821\u56ed\u6536\u96c6\u4e86\u8d85\u8fc711\u5c0f\u65f6\u7684\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u4e2d\uff0c\u76f8\u6bd4\u7b80\u5355\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u78b0\u649e\u7387\u964d\u4f4e6\u500d\uff0c\u6210\u529f\u7387\u63d0\u9ad83\u500d\u3002\u8fd8\u5728\u516c\u5171\u7f8e\u98df\u5e7f\u573a\u5c55\u793a\u4e86\u5b9a\u6027\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6df1\u601d\u719f\u8651\u7684\u6a21\u4eff\u5b66\u4e60\u8bbe\u8ba1\uff08\u800c\u975e\u589e\u52a0\u6570\u636e\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u73b0\u5b9e\u4e16\u754c\u793e\u4ea4\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.17210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17210", "abs": "https://arxiv.org/abs/2509.17210", "authors": ["Shaunak A. Mehta", "Dylan P. Losey"], "title": "Combining Performance and Passivity in Linear Control of Series Elastic Actuators", "comment": null, "summary": "When humans physically interact with robots, we need the robots to be both\nsafe and performant. Series elastic actuators (SEAs) fundamentally advance\nsafety by introducing compliant actuation. On the one hand, adding a spring\nmitigates the impact of accidental collisions between human and robot; but on\nthe other hand, this spring introduces oscillations and fundamentally decreases\nthe robot's ability to perform precise, accurate motions. So how should we\ntrade off between physical safety and performance? In this paper, we enumerate\nthe different linear control and mechanical configurations for series elastic\nactuators, and explore how each choice affects the rendered compliance,\npassivity, and tracking performance. While prior works focus on load side\ncontrol, we find that actuator side control has significant benefits. Indeed,\nsimple PD controllers on the actuator side allow for a much wider range of\ncontrol gains that maintain safety, and combining these with a damper in the\nelastic transmission yields high performance. Our simulations and real world\nexperiments suggest that, by designing a system with low physical stiffness and\nhigh controller gains, this solution enables accurate performance while also\nensuring user safety during collisions.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\uff08SEAs\uff09\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u4f4e\u7269\u7406\u521a\u5ea6\u548c\u9ad8\u63a7\u5236\u5668\u589e\u76ca\u7684\u8bbe\u8ba1\u65b9\u6848\uff0c\u5b9e\u73b0\u7cbe\u786e\u6027\u80fd\u540c\u65f6\u786e\u4fdd\u78b0\u649e\u5b89\u5168\u3002", "motivation": "\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u7269\u7406\u4ea4\u4e92\u65f6\uff0c\u9700\u8981\u673a\u5668\u4eba\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u5177\u5907\u9ad8\u6027\u80fd\u3002\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\u901a\u8fc7\u5f15\u5165\u67d4\u6027\u9a71\u52a8\u63d0\u9ad8\u5b89\u5168\u6027\uff0c\u4f46\u5f39\u7c27\u4f1a\u5f15\u5165\u632f\u8361\u5e76\u964d\u4f4e\u8fd0\u52a8\u7cbe\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u5728\u7269\u7406\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u679a\u4e3e\u4e86\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\u7684\u4e0d\u540c\u7ebf\u6027\u63a7\u5236\u548c\u673a\u68b0\u914d\u7f6e\uff0c\u63a2\u7d22\u6bcf\u79cd\u9009\u62e9\u5bf9\u67d4\u987a\u6027\u3001\u88ab\u52a8\u6027\u548c\u8ddf\u8e2a\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u6267\u884c\u5668\u4fa7\u63a7\u5236\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7b80\u5355\u7684PD\u63a7\u5236\u5668\u914d\u5408\u5f39\u6027\u4f20\u52a8\u4e2d\u7684\u963b\u5c3c\u5668\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4f4e\u7269\u7406\u521a\u5ea6\u548c\u9ad8\u63a7\u5236\u5668\u589e\u76ca\u7684\u7cfb\u7edf\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u5728\u786e\u4fdd\u78b0\u649e\u65f6\u7528\u6237\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u6027\u80fd\u3002", "conclusion": "\u6267\u884c\u5668\u4fa7\u63a7\u5236\u7ed3\u5408\u5f39\u6027\u4f20\u52a8\u963b\u5c3c\u5668\u7684\u8bbe\u8ba1\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.17213", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17213", "abs": "https://arxiv.org/abs/2509.17213", "authors": ["Yassine Kebbati", "Naima Ait-Oufroukh", "Vincent Vigneron", "Dalil Ichala"], "title": "Neural Network and ANFIS based auto-adaptive MPC for path tracking in autonomous vehicles", "comment": null, "summary": "Self-driving cars operate in constantly changing environments and are exposed\nto a variety of uncertainties and disturbances. These factors render classical\ncontrollers ineffective, especially for lateral control. Therefore, an adaptive\nMPC controller is designed in this paper for the path tracking task, tuned by\nan improved particle swarm optimization algorithm. Online parameter adaptation\nis performed using Neural Networks and ANFIS. The designed controller showed\npromising results compared to standard MPC in triple lane change and trajectory\ntracking scenarios. Code can be found here:\nhttps://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u8def\u5f84\u8ddf\u8e2a\u7684\u81ea\u9002\u5e94MPC\u63a7\u5236\u5668\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u8c03\u53c2\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u548cANFIS\u5b9e\u73b0\u5728\u7ebf\u53c2\u6570\u81ea\u9002\u5e94\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u9762\u4e34\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\uff0c\u8fd9\u4f7f\u5f97\u4f20\u7edf\u63a7\u5236\u5668\u5728\u6a2a\u5411\u63a7\u5236\u65b9\u9762\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u81ea\u9002\u5e94MPC\u63a7\u5236\u5668\uff0c\u4f7f\u7528\u6539\u8fdb\u7684\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u53c2\u6570\u8c03\u4f18\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548cANFIS\u5b9e\u73b0\u5728\u7ebf\u53c2\u6570\u81ea\u9002\u5e94\u3002", "result": "\u5728\u4e09\u6b21\u6362\u9053\u548c\u8f68\u8ff9\u8ddf\u8e2a\u573a\u666f\u4e2d\uff0c\u6240\u8bbe\u8ba1\u7684\u63a7\u5236\u5668\u76f8\u6bd4\u6807\u51c6MPC\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u81ea\u9002\u5e94MPC\u63a7\u5236\u5668\u5728\u81ea\u52a8\u9a7e\u9a76\u8def\u5f84\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.17244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17244", "abs": "https://arxiv.org/abs/2509.17244", "authors": ["Frederic Vatnsdal", "Romina Garcia Camargo", "Saurav Agarwal", "Alejandro Ribeiro"], "title": "Scalable Multi Agent Diffusion Policies for Coverage Control", "comment": null, "summary": "We propose MADP, a novel diffusion-model-based approach for collaboration in\ndecentralized robot swarms. MADP leverages diffusion models to generate samples\nfrom complex and high-dimensional action distributions that capture the\ninterdependencies between agents' actions. Each robot conditions policy\nsampling on a fused representation of its own observations and perceptual\nembeddings received from peers. To evaluate this approach, we task a team of\nholonomic robots piloted by MADP to address coverage control-a canonical multi\nagent navigation problem. The policy is trained via imitation learning from a\nclairvoyant expert on the coverage control problem, with the diffusion process\nparameterized by a spatial transformer architecture to enable decentralized\ninference. We evaluate the system under varying numbers, locations, and\nvariances of importance density functions, capturing the robustness demands of\nreal-world coverage tasks. Experiments demonstrate that our model inherits\nvaluable properties from diffusion models, generalizing across agent densities\nand environments, and consistently outperforming state-of-the-art baselines.", "AI": {"tldr": "MADP\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u4eba\u7fa4\u4f53\u534f\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u590d\u6742\u9ad8\u7ef4\u52a8\u4f5c\u5206\u5e03\u6837\u672c\uff0c\u6bcf\u4e2a\u673a\u5668\u4eba\u57fa\u4e8e\u81ea\u8eab\u89c2\u5bdf\u548c\u540c\u4f34\u611f\u77e5\u5d4c\u5165\u7684\u878d\u5408\u8868\u793a\u6765\u91c7\u6837\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u95ee\u9898\uff0c\u7279\u522b\u662f\u8986\u76d6\u63a7\u5236\u8fd9\u4e00\u5178\u578b\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u95ee\u9898\uff0c\u9700\u8981\u80fd\u591f\u6355\u6349\u667a\u80fd\u4f53\u95f4\u52a8\u4f5c\u4f9d\u8d56\u5173\u7cfb\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u52a8\u4f5c\u5206\u5e03\uff0c\u901a\u8fc7\u7a7a\u95f4\u53d8\u6362\u5668\u67b6\u6784\u53c2\u6570\u5316\u6269\u6563\u8fc7\u7a0b\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u63a8\u7406\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u4ece\u8986\u76d6\u63a7\u5236\u95ee\u9898\u7684\u5168\u77e5\u4e13\u5bb6\u8fdb\u884c\u7b56\u7565\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMADP\u7ee7\u627f\u4e86\u6269\u6563\u6a21\u578b\u7684\u4f18\u826f\u7279\u6027\uff0c\u80fd\u591f\u8de8\u667a\u80fd\u4f53\u5bc6\u5ea6\u548c\u73af\u5883\u6cdb\u5316\uff0c\u5728\u5404\u79cd\u91cd\u8981\u6027\u5bc6\u5ea6\u51fd\u6570\u7684\u6570\u91cf\u3001\u4f4d\u7f6e\u548c\u65b9\u5dee\u6761\u4ef6\u4e0b\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MADP\u4e3a\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u4eba\u7fa4\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u5728\u8986\u76d6\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.17274", "categories": ["cs.RO", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17274", "abs": "https://arxiv.org/abs/2509.17274", "authors": ["Alexandros Ntagkas", "Constantinos Tsakonas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "Learning and Optimization with 3D Orientations", "comment": "9 pages, 11 figures", "summary": "There exist numerous ways of representing 3D orientations. Each\nrepresentation has both limitations and unique features. Choosing the best\nrepresentation for one task is often a difficult chore, and there exist\nconflicting opinions on which representation is better suited for a set of\nfamily of tasks. Even worse, when dealing with scenarios where we need to learn\nor optimize functions with orientations as inputs and/or outputs, the set of\npossibilities (representations, loss functions, etc.) is even larger and it is\nnot easy to decide what is best for each scenario. In this paper, we attempt to\na) present clearly, concisely and with unified notation all available\nrepresentations, and \"tricks\" related to 3D orientations (including Lie Group\nalgebra), and b) benchmark them in representative scenarios. The first part\nfeels like it is missing from the robotics literature as one has to read many\ndifferent textbooks and papers in order have a concise and clear understanding\nof all possibilities, while the benchmark is necessary in order to come up with\nrecommendations based on empirical evidence. More precisely, we experiment with\nthe following settings that attempt to cover most widely used scenarios in\nrobotics: 1) direct optimization, 2) imitation/supervised learning with a\nneural network controller, 3) reinforcement learning, and 4) trajectory\noptimization using differential dynamic programming. We finally provide\nguidelines depending on the scenario, and make available a reference\nimplementation of all the orientation math described.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u4e863D\u65b9\u5411\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u79cd\u673a\u5668\u4eba\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u4e0d\u540c\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5b9e\u8bc1\u7684\u63a8\u8350\u6307\u5357\u3002", "motivation": "\u73b0\u6709\u76843D\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u4f17\u591a\u4e14\u5404\u6709\u4f18\u52a3\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u68b3\u7406\u548c\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u9009\u62e9\u5408\u9002\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "1\uff09\u7edf\u4e00\u6e05\u6670\u5730\u4ecb\u7ecd\u6240\u6709\u53ef\u7528\u76843D\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u53ca\u76f8\u5173\u6570\u5b66\u6280\u5de7\uff1b2\uff09\u5728\u56db\u79cd\u5178\u578b\u673a\u5668\u4eba\u573a\u666f\uff08\u76f4\u63a5\u4f18\u5316\u3001\u6a21\u4eff\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u8f68\u8ff9\u4f18\u5316\uff09\u4e2d\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u4e0d\u540c\u8868\u793a\u65b9\u6cd5\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u6839\u636e\u4e0d\u540c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u9009\u62e9\u6307\u5357\uff0c\u5e76\u5f00\u6e90\u4e86\u6240\u6709\u65b9\u5411\u6570\u5b66\u7684\u53c2\u8003\u5b9e\u73b0\u3002"}}
{"id": "2509.17287", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17287", "abs": "https://arxiv.org/abs/2509.17287", "authors": ["Gokul B. Nair", "Alejandro Fontan", "Michael Milford", "Tobias Fischer"], "title": "Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation", "comment": "8 Pages, 4 Figures, Under Review", "summary": "Visual teach-and-repeat navigation enables robots to autonomously traverse\npreviously demonstrated paths by comparing current sensory input with recorded\ntrajectories. However, conventional frame-based cameras fundamentally limit\nsystem responsiveness: their fixed frame rates (typically 30-60 Hz) create\ninherent latency between environmental changes and control responses. Here we\npresent the first event-camera-based visual teach-and-repeat system. To achieve\nthis, we develop a frequency-domain cross-correlation framework that transforms\nthe event stream matching problem into computationally efficient Fourier space\nmultiplications, capable of exceeding 300Hz processing rates, an order of\nmagnitude faster than frame-based approaches. By exploiting the binary nature\nof event frames and applying image compression techniques, we further enhance\nthe computational speed of the cross-correlation process without sacrificing\nlocalization accuracy. Extensive experiments using a Prophesee EVK4 HD event\ncamera mounted on an AgileX Scout Mini robot demonstrate successful autonomous\nnavigation across 4000+ meters of indoor and outdoor trajectories. Our system\nachieves ATEs below 24 cm while maintaining consistent high-frequency control\nupdates. Our evaluations show that our approach achieves substantially higher\nupdate rates compared to conventional frame-based systems, underscoring the\npractical viability of event-based perception for real-time robotic navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u89c6\u89c9\u793a\u6559-\u91cd\u590d\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u9891\u57df\u4e92\u76f8\u5173\u6846\u67b6\u5b9e\u73b0\u8d85\u8fc7300Hz\u7684\u5904\u7406\u901f\u7387\uff0c\u6bd4\u4f20\u7edf\u5e27\u5f0f\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u76f8\u673a\uff0830-60Hz\uff09\u7684\u56fa\u5b9a\u5e27\u7387\u9650\u5236\u4e86\u7cfb\u7edf\u54cd\u5e94\u6027\uff0c\u5728\u73af\u5883\u53d8\u5316\u548c\u63a7\u5236\u54cd\u5e94\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u5ef6\u8fdf\u3002\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u9891\u7279\u6027\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u9891\u57df\u4e92\u76f8\u5173\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u6d41\u5339\u914d\u95ee\u9898\u8f6c\u5316\u4e3a\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5085\u91cc\u53f6\u7a7a\u95f4\u4e58\u6cd5\uff1b\u5229\u7528\u4e8b\u4ef6\u5e27\u7684\u4e8c\u8fdb\u5236\u7279\u6027\u5e76\u5e94\u7528\u56fe\u50cf\u538b\u7f29\u6280\u672f\u63d0\u5347\u8ba1\u7b97\u901f\u5ea6\u3002", "result": "\u5728AgileX Scout Mini\u673a\u5668\u4eba\u4e0a\u4f7f\u7528Prophesee EVK4 HD\u4e8b\u4ef6\u76f8\u673a\u8fdb\u884c\u4e864000+\u7c73\u7684\u5ba4\u5185\u5916\u8f68\u8ff9\u5b9e\u9a8c\uff0cATE\u4f4e\u4e8e24cm\uff0c\u4fdd\u6301\u9ad8\u9891\u63a7\u5236\u66f4\u65b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u5e27\u5f0f\u7cfb\u7edf\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u66f4\u65b0\u901f\u7387\uff0c\u8bc1\u660e\u4e86\u4e8b\u4ef6\u611f\u77e5\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2509.17299", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17299", "abs": "https://arxiv.org/abs/2509.17299", "authors": ["Dorian Tsai", "Christopher A. Brunner", "Riki Lamont", "F. Mikaela Nordborg", "Andrea Severati", "Java Terry", "Karen Jackel", "Matthew Dunbabin", "Tobias Fischer", "Scarlett Raine"], "title": "Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)", "comment": "9 pages, 7 figures", "summary": "Coral aquaculture for reef restoration requires accurate and continuous spawn\ncounting for resource distribution and larval health monitoring, but current\nmethods are labor-intensive and represent a critical bottleneck in the coral\nproduction pipeline. We propose the Coral Spawn and Larvae Imaging Camera\nSystem (CSLICS), which uses low cost modular cameras and object detectors\ntrained using human-in-the-loop labeling approaches for automated spawn\ncounting in larval rearing tanks. This paper details the system engineering,\ndataset collection, and computer vision techniques to detect, classify and\ncount coral spawn. Experimental results from mass spawning events demonstrate\nan F1 score of 82.4\\% for surface spawn detection at different embryogenesis\nstages, 65.3\\% F1 score for sub-surface spawn detection, and a saving of 5,720\nhours of labor per spawning event compared to manual sampling methods at the\nsame frequency. Comparison of manual counts with CSLICS monitoring during a\nmass coral spawning event on the Great Barrier Reef demonstrates CSLICS'\naccurate measurement of fertilization success and sub-surface spawn counts.\nThese findings enhance the coral aquaculture process and enable upscaling of\ncoral reef restoration efforts to address climate change threats facing\necosystems like the Great Barrier Reef.", "AI": {"tldr": "\u63d0\u51faCSLICS\u7cfb\u7edf\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u6a21\u5757\u5316\u6444\u50cf\u5934\u548c\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u65b9\u6cd5\u5b9e\u73b0\u73ca\u745a\u4ea7\u5375\u81ea\u52a8\u8ba1\u6570\uff0c\u663e\u8457\u8282\u7701\u4eba\u5de5\u65f6\u95f4\u5e76\u63d0\u9ad8\u73ca\u745a\u517b\u6b96\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u73ca\u745a\u517b\u6b96\u4e2d\u7684\u4ea7\u5375\u8ba1\u6570\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u6210\u4e3a\u73ca\u745a\u751f\u4ea7\u6d41\u7a0b\u7684\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u73ca\u745a\u7901\u6062\u590d\u5de5\u4f5c\u7684\u89c4\u6a21\u3002", "method": "\u5f00\u53d1Coral Spawn and Larvae Imaging Camera System (CSLICS)\uff0c\u91c7\u7528\u4f4e\u6210\u672c\u6a21\u5757\u5316\u6444\u50cf\u5934\uff0c\u7ed3\u5408\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u8bad\u7ec3\u7684\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u5b9e\u73b0\u73ca\u745a\u4ea7\u5375\u7684\u81ea\u52a8\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u8ba1\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u8868\u9762\u4ea7\u5375\u68c0\u6d4bF1\u5206\u657082.4%\uff0c\u6c34\u4e0b\u4ea7\u5375\u68c0\u6d4bF1\u5206\u657065.3%\uff0c\u76f8\u6bd4\u4eba\u5de5\u65b9\u6cd5\u6bcf\u4e2a\u4ea7\u5375\u4e8b\u4ef6\u8282\u77015,720\u5c0f\u65f6\u52b3\u52a8\u65f6\u95f4\u3002\u5728Great Barrier Reef\u7684\u5927\u89c4\u6a21\u4ea7\u5375\u4e8b\u4ef6\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3002", "conclusion": "CSLICS\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u73ca\u745a\u517b\u6b96\u8fc7\u7a0b\u7684\u6548\u7387\uff0c\u80fd\u591f\u652f\u6301\u73ca\u745a\u7901\u6062\u590d\u5de5\u4f5c\u7684\u89c4\u6a21\u5316\uff0c\u4ee5\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u5bf9\u751f\u6001\u7cfb\u7edf\u7684\u5a01\u80c1\u3002"}}
{"id": "2509.17308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17308", "abs": "https://arxiv.org/abs/2509.17308", "authors": ["Kazutoshi Tanaka", "Tomoya Takahashi", "Masashi Hamaya"], "title": "Pose Estimation of a Cable-Driven Serpentine Manipulator Utilizing Intrinsic Dynamics via Physical Reservoir Computing", "comment": "9 pages, 7 figures. Accepted at IROS 2025. This is the preprint\n  version", "summary": "Cable-driven serpentine manipulators hold great potential in unstructured\nenvironments, offering obstacle avoidance, multi-directional force application,\nand a lightweight design. By placing all motors and sensors at the base and\nemploying plastic links, we can further reduce the arm's weight. To demonstrate\nthis concept, we developed a 9-degree-of-freedom cable-driven serpentine\nmanipulator with an arm length of 545 mm and a total mass of only 308 g.\nHowever, this design introduces flexibility-induced variations, such as cable\nslack, elongation, and link deformation. These variations result in\ndiscrepancies between analytical predictions and actual link positions, making\npose estimation more challenging. To address this challenge, we propose a\nphysical reservoir computing based pose estimation method that exploits the\nmanipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir.\nExperimental results show a mean pose error of 4.3 mm using our method,\ncompared to 4.4 mm with a baseline long short-term memory network and 39.5 mm\nwith an analytical approach. This work provides a new direction for control and\nperception strategies in lightweight cable-driven serpentine manipulators\nleveraging their intrinsic dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\u7684\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8f7b\u91cf\u5316\u7f06\u9a71\u86c7\u5f62\u673a\u68b0\u81c2\u7684\u67d4\u6027\u53d8\u5f62\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002", "motivation": "\u7f06\u9a71\u86c7\u5f62\u673a\u68b0\u81c2\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5177\u6709\u907f\u969c\u3001\u591a\u5411\u65bd\u529b\u548c\u8f7b\u91cf\u5316\u7b49\u4f18\u52bf\uff0c\u4f46\u8f7b\u91cf\u5316\u8bbe\u8ba1\u5f15\u5165\u4e86\u7535\u7f06\u677e\u5f1b\u3001\u4f38\u957f\u548c\u8fde\u6746\u53d8\u5f62\u7b49\u67d4\u6027\u53d8\u5316\uff0c\u5bfc\u81f4\u89e3\u6790\u9884\u6d4b\u4e0e\u5b9e\u9645\u4f4d\u59ff\u5b58\u5728\u5dee\u5f02\uff0c\u4f4d\u59ff\u4f30\u8ba1\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e869\u81ea\u7531\u5ea6\u7f06\u9a71\u86c7\u5f62\u673a\u68b0\u81c2\uff08\u81c2\u957f545mm\uff0c\u603b\u8d28\u91cf308g\uff09\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\u7684\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u673a\u68b0\u81c2\u56fa\u6709\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4f5c\u4e3a\u9ad8\u7ef4\u50a8\u5c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u4f4d\u59ff\u8bef\u5dee\u4e3a4.3mm\uff0c\u4f18\u4e8eLSTM\u7f51\u7edc\u76844.4mm\u548c\u89e3\u6790\u65b9\u6cd5\u768439.5mm\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5229\u7528\u8f7b\u91cf\u5316\u7f06\u9a71\u86c7\u5f62\u673a\u68b0\u81c2\u56fa\u6709\u52a8\u529b\u5b66\u7684\u63a7\u5236\u548c\u611f\u77e5\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.17321", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17321", "abs": "https://arxiv.org/abs/2509.17321", "authors": ["Pawe\u0142 Budzianowski", "Emilia Wi\u015bnios", "Gracjan G\u00f3ral", "Igor Kulakov", "Viktor Petrenko", "Krzysztof Walas"], "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation", "comment": null, "summary": "Data scarcity remains one of the most limiting factors in driving progress in\nrobotics. However, the amount of available robotics data in the wild is growing\nexponentially, creating new opportunities for large-scale data utilization.\nReliable temporal task completion prediction could help automatically annotate\nand curate this data at scale. The Generative Value Learning (GVL) approach was\nrecently proposed, leveraging the knowledge embedded in vision-language models\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\ndiverse challenging manipulation tasks involving both robotic and human\nembodiments. We evaluate the capabilities of publicly available open-source\nfoundation models, showing that open-source model families significantly\nunderperform closed-source counterparts, achieving only approximately $70\\%$ of\ntheir performance on temporal progress prediction tasks. Furthermore, we\ndemonstrate how OpenGVL can serve as a practical tool for automated data\ncuration and filtering, enabling efficient quality assessment of large-scale\nrobotics datasets. We release the benchmark along with the complete codebase at\n\\href{github.com/budzianowski/opengvl}{OpenGVL}.", "AI": {"tldr": "OpenGVL\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4efb\u52a1\u8fdb\u5ea6\u9884\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5927\u89c4\u6a21\u6570\u636e\u81ea\u52a8\u6807\u6ce8\u548c\u7b5b\u9009\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u800c\u91ce\u5916\u53ef\u7528\u7684\u673a\u5668\u4eba\u6570\u636e\u6b63\u5728\u6307\u6570\u7ea7\u589e\u957f\u3002\u53ef\u9760\u7684\u65f6\u95f4\u4efb\u52a1\u5b8c\u6210\u9884\u6d4b\u53ef\u4ee5\u5e2e\u52a9\u81ea\u52a8\u6807\u6ce8\u548c\u6574\u7406\u8fd9\u4e9b\u5927\u89c4\u6a21\u6570\u636e\u3002", "method": "\u57fa\u4e8e\u751f\u6210\u4ef7\u503c\u5b66\u4e60(GVL)\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u77e5\u8bc6\u4ece\u89c6\u89c9\u89c2\u5bdf\u4e2d\u9884\u6d4b\u4efb\u52a1\u8fdb\u5ea6\u3002OpenGVL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u5728\u591a\u6837\u5316\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5bb6\u65cf\u5728\u65f6\u95f4\u8fdb\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u4ec5\u8fbe\u5230\u95ed\u6e90\u6a21\u578b\u7ea670%\u7684\u6027\u80fd\u3002OpenGVL\u88ab\u8bc1\u660e\u53ef\u4ee5\u4f5c\u4e3a\u81ea\u52a8\u5316\u6570\u636e\u6574\u7406\u548c\u7b5b\u9009\u7684\u5b9e\u7528\u5de5\u5177\u3002", "conclusion": "OpenGVL\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5e03\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f00\u6e90\u6a21\u578b\u5728\u4efb\u52a1\u8fdb\u5ea6\u9884\u6d4b\u65b9\u9762\u4e0e\u95ed\u6e90\u6a21\u578b\u5b58\u5728\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2509.17340", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17340", "abs": "https://arxiv.org/abs/2509.17340", "authors": ["Xin Chen", "Rui Huang", "Longbin Tang", "Lin Zhao"], "title": "AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation", "comment": null, "summary": "Agile mapless navigation in cluttered 3D environments poses significant\nchallenges for autonomous drones. Conventional mapping-planning-control\npipelines incur high computational cost and propagate estimation errors. We\npresent AERO-MPPI, a fully GPU-accelerated framework that unifies perception\nand planning through an anchor-guided ensemble of Model Predictive Path\nIntegral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR\npoint-cloud representation that rapidly extracts spatially distributed\n\"anchors\" as look-ahead intermediate endpoints, from which we construct\npolynomial trajectory guides to explore distinct homotopy path classes. At each\nplanning step, we run multiple MPPI instances in parallel and evaluate them\nwith a two-stage multi-objective cost that balances collision avoidance and\ngoal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI\nachieves real-time onboard operation and mitigates the local-minima failures of\nsingle-MPPI approaches. Extensive simulations in forests, verticals, and\ninclines demonstrate sustained reliable flight above 7 m/s, with success rates\nabove 80% and smoother trajectories compared to state-of-the-art baselines.\nReal-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX\n16G confirm that AERO-MPPI runs in real time onboard and consistently achieves\nsafe, agile, and robust flight in complex cluttered environments. The code will\nbe open-sourced upon acceptance of the paper.", "AI": {"tldr": "AERO-MPPI\u662f\u4e00\u4e2a\u5b8c\u5168GPU\u52a0\u901f\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u951a\u70b9\u5f15\u5bfc\u7684MPPI\u4f18\u5316\u5668\u96c6\u5408\u7edf\u4e00\u611f\u77e5\u548c\u89c4\u5212\uff0c\u5728\u590d\u67423D\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u654f\u6377\u65e0\u5730\u56fe\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u7684\u5730\u56fe-\u89c4\u5212-\u63a7\u5236\u6d41\u6c34\u7ebf\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f1a\u4f20\u64ad\u4f30\u8ba1\u8bef\u5dee\uff0c\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u65e0\u5730\u56fe\u5bfc\u822a\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u591a\u5206\u8fa8\u7387LiDAR\u70b9\u4e91\u8868\u793a\u5feb\u901f\u63d0\u53d6\u7a7a\u95f4\u5206\u5e03\u7684\u951a\u70b9\u4f5c\u4e3a\u524d\u77bb\u4e2d\u95f4\u7aef\u70b9\uff0c\u6784\u5efa\u591a\u9879\u5f0f\u8f68\u8ff9\u5f15\u5bfc\u63a2\u7d22\u4e0d\u540c\u540c\u4f26\u8def\u5f84\u7c7b\uff0c\u5728\u6bcf\u4e2a\u89c4\u5212\u6b65\u9aa4\u5e76\u884c\u8fd0\u884c\u591a\u4e2aMPPI\u5b9e\u4f8b\u5e76\u4f7f\u7528\u4e24\u9636\u6bb5\u591a\u76ee\u6807\u6210\u672c\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u68ee\u6797\u3001\u5782\u76f4\u548c\u503e\u659c\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u4eff\u771f\u663e\u793a\u6301\u7eed\u53ef\u9760\u98de\u884c\u901f\u5ea6\u8d85\u8fc77m/s\uff0c\u6210\u529f\u7387\u8d85\u8fc780%\uff0c\u8f68\u8ff9\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u66f4\u5e73\u6ed1\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u5b9eAERO-MPPI\u5728\u677f\u8f7d\u5b9e\u65f6\u8fd0\u884c\uff0c\u5728\u590d\u6742\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u654f\u6377\u548c\u9c81\u68d2\u7684\u98de\u884c\u3002", "conclusion": "AERO-MPPI\u6846\u67b6\u901a\u8fc7GPU\u52a0\u901f\u548c\u5e76\u884cMPPI\u4f18\u5316\u5668\u6709\u6548\u89e3\u51b3\u4e86\u5355MPPI\u65b9\u6cd5\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u590d\u67423D\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u53ef\u9760\u65e0\u4eba\u673a\u5bfc\u822a\u3002"}}
{"id": "2509.17350", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17350", "abs": "https://arxiv.org/abs/2509.17350", "authors": ["Haoran Zhou", "Yangwei You", "Shuaijun Wang"], "title": "DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception", "comment": "8 pages, 7 figures", "summary": "Dynamic in air handover is a fundamental challenge for dual-arm robots,\nrequiring accurate perception, precise coordination, and natural motion. Prior\nmethods often rely on dynamics models, strong priors, or depth sensing,\nlimiting generalization and naturalness. We present DyDexHandover, a novel\nframework that employs multi-agent reinforcement learning to train an end to\nend RGB based policy for bimanual object throwing and catching. To achieve more\nhuman-like behavior, the throwing policy is guided by a human policy\nregularization scheme, encouraging fluid and natural motion, and enhancing the\ngeneralization capability of the policy. A dual arm simulation environment was\nbuilt in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly\n99 percent success on training objects and 75 percent on unseen objects, while\ngenerating human-like throwing and catching behaviors. To our knowledge, it is\nthe first method to realize dual-arm in-air handover using only raw RGB\nperception.", "AI": {"tldr": "DyDexHandover\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684RGB\u89c6\u89c9\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u53cc\u624b\u673a\u5668\u4eba\u7684\u7a7a\u4e2d\u7269\u4f53\u629b\u63a5\u4efb\u52a1\uff0c\u65e0\u9700\u4f9d\u8d56\u52a8\u529b\u5b66\u6a21\u578b\u6216\u6df1\u5ea6\u611f\u77e5\uff0c\u80fd\u591f\u751f\u6210\u7c7b\u4eba\u5316\u7684\u81ea\u7136\u8fd0\u52a8\u3002", "motivation": "\u89e3\u51b3\u53cc\u624b\u673a\u5668\u4eba\u7a7a\u4e2d\u4ea4\u63a5\u7269\u4f53\u7684\u57fa\u672c\u6311\u6218\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u52a8\u529b\u5b66\u6a21\u578b\u3001\u5f3a\u5148\u9a8c\u6216\u6df1\u5ea6\u611f\u77e5\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u8fd0\u52a8\u81ea\u7136\u6027\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7aef\u5230\u7aef\u7684RGB\u89c6\u89c9\u7b56\u7565\uff0c\u901a\u8fc7\u4eba\u7c7b\u7b56\u7565\u6b63\u5219\u5316\u65b9\u6848\u5f15\u5bfc\u629b\u63b7\u7b56\u7565\uff0c\u9f13\u52b1\u6d41\u7545\u81ea\u7136\u7684\u8fd0\u52a8\uff0c\u5e76\u5728Isaac Sim\u4e2d\u6784\u5efa\u53cc\u81c2\u4eff\u771f\u73af\u5883\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5728\u8bad\u7ec3\u7269\u4f53\u4e0a\u8fbe\u5230\u8fd199%\u7684\u6210\u529f\u7387\uff0c\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u8fbe\u523075%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u751f\u6210\u7c7b\u4eba\u5316\u7684\u629b\u63a5\u884c\u4e3a\uff0c\u662f\u9996\u4e2a\u4ec5\u4f7f\u7528\u539f\u59cbRGB\u611f\u77e5\u5b9e\u73b0\u53cc\u624b\u673a\u5668\u4eba\u7a7a\u4e2d\u4ea4\u63a5\u7684\u65b9\u6cd5\u3002", "conclusion": "DyDexHandover\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8eRGB\u89c6\u89c9\u7684\u53cc\u624b\u673a\u5668\u4eba\u7a7a\u4e2d\u7269\u4f53\u629b\u63a5\uff0c\u5177\u6709\u9ad8\u6210\u529f\u7387\u548c\u7c7b\u4eba\u5316\u8fd0\u52a8\u7279\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u52a8\u6001\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17381", "abs": "https://arxiv.org/abs/2509.17381", "authors": ["Yongliang Wang", "Hamidreza Kasaei"], "title": "Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators", "comment": "Project page available at: https://sites.google.com/view/ftp4rm/home", "summary": "Generating obstacle-free trajectories for robotic manipulators in\nunstructured and cluttered environments remains a significant challenge.\nExisting motion planning methods often require additional computational effort\nto generate the final trajectory by solving kinematic or dynamic equations.\nThis paper highlights the strong potential of model-free reinforcement learning\nmethods over model-based approaches for obstacle-free trajectory planning in\njoint space. We propose a fast trajectory planning system for manipulators that\ncombines vision-based path planning in task space with reinforcement\nlearning-based obstacle avoidance in joint space. We divide the framework into\ntwo key components. The first introduces an innovative vision-based trajectory\nplanner in task space, leveraging the large-scale fast segment anything (FSA)\nmodel in conjunction with basis spline (B-spline)-optimized kinodynamic path\nsearching. The second component enhances the proximal policy optimization (PPO)\nalgorithm by integrating action ensembles (AE) and policy feedback (PF), which\ngreatly improve precision and stability in goal-reaching and obstacle avoidance\nwithin the joint space. These PPO enhancements increase the algorithm's\nadaptability across diverse robotic tasks, ensuring consistent execution of\ncommands from the first component by the manipulator, while also enhancing both\nobstacle avoidance efficiency and reaching accuracy. The experimental results\ndemonstrate the effectiveness of PPO enhancements, as well as\nsimulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real)\ntransfer, in improving model robustness and planner efficiency in complex\nscenarios. These enhancements allow the robot to perform obstacle avoidance and\nreal-time trajectory planning in obstructed environments. Project page\navailable at: https://sites.google.com/view/ftp4rm/home", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u4efb\u52a1\u7a7a\u95f4\u8def\u5f84\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u5173\u8282\u7a7a\u95f4\u907f\u969c\u7684\u5feb\u901f\u8f68\u8ff9\u89c4\u5212\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdbPPO\u7b97\u6cd5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u907f\u969c\u6548\u7387\u548c\u76ee\u6807\u5230\u8fbe\u7cbe\u5ea6\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u548c\u6742\u4e71\u73af\u5883\u4e2d\u4e3a\u673a\u68b0\u81c2\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u989d\u5916\u8ba1\u7b97\u6765\u6c42\u89e3\u8fd0\u52a8\u5b66\u6216\u52a8\u529b\u5b66\u65b9\u7a0b\u3002\u672c\u6587\u5f3a\u8c03\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5173\u8282\u7a7a\u95f4\u907f\u969c\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u89c6\u89c9\u7684\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u5feb\u901f\u5206\u5272\u6a21\u578b\u548cB\u6837\u6761\u4f18\u5316\u7684\u8fd0\u52a8\u5b66\u8def\u5f84\u641c\u7d22\uff1b2\uff09\u6539\u8fdb\u7684PPO\u7b97\u6cd5\uff0c\u96c6\u6210\u52a8\u4f5c\u96c6\u6210\u548c\u7b56\u7565\u53cd\u9988\uff0c\u63d0\u9ad8\u5173\u8282\u7a7a\u95f4\u4e2d\u7684\u76ee\u6807\u5230\u8fbe\u7cbe\u5ea6\u548c\u907f\u969c\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePPO\u589e\u5f3a\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u4eff\u771f\u5230\u4eff\u771f\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u89c4\u5212\u5668\u6548\u7387\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u6709\u969c\u788d\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u65f6\u907f\u969c\u548c\u8f68\u8ff9\u89c4\u5212\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u673a\u68b0\u81c2\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5feb\u901f\u65e0\u78b0\u649e\u8f68\u8ff9\u89c4\u5212\uff0c\u901a\u8fc7\u89c6\u89c9\u8def\u5f84\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u907f\u969c\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u6548\u7387\u548c\u673a\u5668\u4eba\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2509.17387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17387", "abs": "https://arxiv.org/abs/2509.17387", "authors": ["Ziqing Zou", "Cong Wang", "Yue Hu", "Xiao Liu", "Bowen Xu", "Rong Xiong", "Changjie Fan", "Yingfeng Chen", "Yue Wang"], "title": "High-Precision and High-Efficiency Trajectory Tracking for Excavators Based on Closed-Loop Dynamics", "comment": null, "summary": "The complex nonlinear dynamics of hydraulic excavators, such as time delays\nand control coupling, pose significant challenges to achieving high-precision\ntrajectory tracking. Traditional control methods often fall short in such\napplications due to their inability to effectively handle these nonlinearities,\nwhile commonly used learning-based methods require extensive interactions with\nthe environment, leading to inefficiency. To address these issues, we introduce\nEfficientTrack, a trajectory tracking method that integrates model-based\nlearning to manage nonlinear dynamics and leverages closed-loop dynamics to\nimprove learning efficiency, ultimately minimizing tracking errors. We validate\nour method through comprehensive experiments both in simulation and on a\nreal-world excavator. Comparative experiments in simulation demonstrate that\nour method outperforms existing learning-based approaches, achieving the\nhighest tracking precision and smoothness with the fewest interactions.\nReal-world experiments further show that our method remains effective under\nload conditions and possesses the ability for continual learning, highlighting\nits practical applicability. For implementation details and source code, please\nrefer to https://github.com/ZiqingZou/EfficientTrack.", "AI": {"tldr": "EfficientTrack\u662f\u4e00\u79cd\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\u6765\u5904\u7406\u6db2\u538b\u6316\u6398\u673a\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u5229\u7528\u95ed\u73af\u52a8\u529b\u5b66\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\uff0c\u6700\u5c0f\u5316\u8ddf\u8e2a\u8bef\u5dee\u3002", "motivation": "\u6db2\u538b\u6316\u6398\u673a\u7684\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff08\u5982\u65f6\u95f4\u5ef6\u8fdf\u548c\u63a7\u5236\u8026\u5408\uff09\u7ed9\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u5e26\u6765\u6311\u6218\uff0c\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u975e\u7ebf\u6027\u7279\u6027\uff0c\u800c\u5e38\u7528\u5b66\u4e60\u578b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u73af\u5883\u4ea4\u4e92\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faEfficientTrack\u65b9\u6cd5\uff0c\u96c6\u6210\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\u6765\u7ba1\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u5e76\u5229\u7528\u95ed\u73af\u52a8\u529b\u5b66\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c11\u7684\u4ea4\u4e92\u6b21\u6570\u5b9e\u73b0\u6700\u9ad8\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5e73\u6ed1\u5ea6\uff1b\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u4ecd\u6709\u6548\uff0c\u5e76\u5177\u5907\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "EfficientTrack\u65b9\u6cd5\u5728\u6db2\u538b\u6316\u6398\u673a\u8f68\u8ff9\u8ddf\u8e2a\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u95ee\u9898\u5e76\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2509.17389", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17389", "abs": "https://arxiv.org/abs/2509.17389", "authors": ["Lois Liow", "Jonty Milford", "Emre Uygun", "Andre Farinha", "Vinoth Viswanathan", "Josh Pinskier", "David Howard"], "title": "3D Printable Soft Liquid Metal Sensors for Delicate Manipulation Tasks", "comment": "8 pages, 4 figures", "summary": "Robotics and automation are key enablers to increase throughput in ongoing\nconservation efforts across various threatened ecosystems. Cataloguing,\ndigitisation, husbandry, and similar activities require the ability to interact\nwith delicate, fragile samples without damaging them. Additionally,\nlearning-based solutions to these tasks require the ability to safely acquire\ndata to train manipulation policies through, e.g., reinforcement learning. To\naddress these twin needs, we introduce a novel method to print free-form,\nhighly sensorised soft 'physical twins'. We present an automated design\nworkflow to create complex and customisable 3D soft sensing structures on\ndemand from 3D scans or models. Compared to the state of the art, our soft\nliquid metal sensors faithfully recreate complex natural geometries and display\nexcellent sensing properties suitable for validating performance in delicate\nmanipulation tasks. We demonstrate the application of our physical twins as\n'sensing corals': high-fidelity, 3D printed replicas of scanned corals that\neliminate the need for live coral experimentation, whilst increasing data\nquality, offering an ethical and scalable pathway for advancing autonomous\ncoral handling and soft manipulation broadly. Through extensive bench-top\nmanipulation and underwater grasping experiments, we show that our sensing\ncoral is able to detect grasps under 0.5 N, effectively capturing the delicate\ninteractions and light contact forces required for coral handling. Finally, we\nshowcase the value of our physical twins across two demonstrations: (i)\nautomated coral labelling for lab identification and (ii) robotic coral\naquaculture. Sensing physical twins such as ours can provide richer grasping\nfeedback than conventional sensors providing experimental validation of prior\nto deployment in handling fragile and delicate items.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6253\u5370\u9ad8\u7075\u654f\u5ea6\u8f6f\u4f53'\u7269\u7406\u5b6a\u751f\u4f53'\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6001\u4fdd\u62a4\u4e2d\u7684\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\uff0c\u7279\u522b\u662f\u73ca\u745a\u5904\u7406\u3002\u8be5\u65b9\u6cd5\u901a\u8fc73D\u626b\u63cf\u521b\u5efa\u53ef\u5b9a\u5236\u7684\u8f6f\u4f53\u4f20\u611f\u7ed3\u6784\uff0c\u66ff\u4ee3\u6d3b\u4f53\u73ca\u745a\u5b9e\u9a8c\u3002", "motivation": "\u751f\u6001\u4fdd\u62a4\u5de5\u4f5c\u9700\u8981\u4e0e\u8106\u5f31\u6837\u672c\u4ea4\u4e92\u800c\u4e0d\u635f\u574f\u5b83\u4eec\uff0c\u540c\u65f6\u9700\u8981\u5b89\u5168\u83b7\u53d6\u6570\u636e\u6765\u8bad\u7ec3\u64cd\u4f5c\u7b56\u7565\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u5bf9\u7cbe\u7ec6\u64cd\u4f5c\u7684\u4f20\u611f\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece3D\u626b\u63cf\u6216\u6a21\u578b\u521b\u5efa\u590d\u6742\u7684\u53ef\u5b9a\u52363D\u8f6f\u4f53\u4f20\u611f\u7ed3\u6784\uff0c\u4f7f\u7528\u8f6f\u4f53\u6db2\u6001\u91d1\u5c5e\u4f20\u611f\u5668\u7cbe\u786e\u518d\u73b0\u81ea\u7136\u51e0\u4f55\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4f20\u611f\u73ca\u745a\u80fd\u591f\u68c0\u6d4b\u4f4e\u4e8e0.5N\u7684\u6293\u53d6\u529b\uff0c\u6709\u6548\u6355\u6349\u73ca\u745a\u5904\u7406\u6240\u9700\u7684\u7cbe\u7ec6\u4ea4\u4e92\u548c\u8f7b\u5fae\u63a5\u89e6\u529b\u3002\u5728\u81ea\u52a8\u73ca\u745a\u6807\u8bb0\u548c\u673a\u5668\u4eba\u73ca\u745a\u517b\u6b96\u4e24\u4e2a\u6f14\u793a\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8fd9\u79cd\u7269\u7406\u5b6a\u751f\u4f53\u6bd4\u4f20\u7edf\u4f20\u611f\u5668\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u6293\u53d6\u53cd\u9988\uff0c\u4e3a\u5904\u7406\u8106\u5f31\u7269\u54c1\u63d0\u4f9b\u4e86\u4f26\u7406\u548c\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u53ef\u5728\u90e8\u7f72\u524d\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2509.17390", "categories": ["cs.RO", "68T40, 68U05", "I.6.8"], "pdf": "https://arxiv.org/pdf/2509.17390", "abs": "https://arxiv.org/abs/2509.17390", "authors": ["Junzhe Wu", "Yufei Jia", "Yiyi Yan", "Zhixing Chen", "Tiao Tan", "Zifan Wang", "Guangyu Wang"], "title": "FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR", "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic\nrendering, its vast ecosystem of assets remains incompatible with\nhigh-performance LiDAR simulation, a critical tool for robotics and autonomous\ndriving. We present \\textbf{FGGS-LiDAR}, a framework that bridges this gap with\na truly plug-and-play approach. Our method converts \\textit{any} pretrained\n3DGS model into a high-fidelity, watertight mesh without requiring\nLiDAR-specific supervision or architectural alterations. This conversion is\nachieved through a general pipeline of volumetric discretization and Truncated\nSigned Distance Field (TSDF) extraction. We pair this with a highly optimized,\nGPU-accelerated ray-casting module that simulates LiDAR returns at over 500\nFPS. We validate our approach on indoor and outdoor scenes, demonstrating\nexceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for\ngeometrically accurate depth sensing, our framework extends their utility\nbeyond visualization and unlocks new capabilities for scalable, multimodal\nsimulation. Our open-source implementation is available at\nhttps://github.com/TATP-233/FGGS-LiDAR.", "AI": {"tldr": "FGGS-LiDAR\u662f\u4e00\u4e2a\u5c06\u9884\u8bad\u7ec3\u76843D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u8f6c\u6362\u4e3a\u9ad8\u4fdd\u771f\u6c34\u5bc6\u7f51\u683c\u7684\u6846\u67b6\uff0c\u65e0\u9700\u6fc0\u5149\u96f7\u8fbe\u7279\u5b9a\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u6fc0\u5149\u96f7\u8fbe\u6a21\u62df", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u7136\u9769\u65b0\u4e86\u771f\u5b9e\u611f\u6e32\u67d3\uff0c\u4f46\u5176\u8d44\u4ea7\u751f\u6001\u7cfb\u7edf\u4e0e\u9ad8\u6027\u80fd\u6fc0\u5149\u96f7\u8fbe\u6a21\u62df\u4e0d\u517c\u5bb9\uff0c\u800c\u6fc0\u5149\u96f7\u8fbe\u6a21\u62df\u662f\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u7684\u5173\u952e\u5de5\u5177", "method": "\u901a\u8fc7\u4f53\u79ef\u79bb\u6563\u5316\u548c\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u573a\u63d0\u53d6\u7684\u901a\u7528\u6d41\u7a0b\u8f6c\u63623DGS\u6a21\u578b\uff0c\u5e76\u914d\u5408\u9ad8\u5ea6\u4f18\u5316\u7684GPU\u52a0\u901f\u5149\u7ebf\u6295\u5c04\u6a21\u5757", "result": "\u5728\u5ba4\u5185\u5916\u573a\u666f\u9a8c\u8bc1\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u6fc0\u5149\u96f7\u8fbe\u6a21\u62df\u901f\u5ea6\u8d85\u8fc7500FPS", "conclusion": "\u8be5\u6846\u67b6\u4f7f3DGS\u8d44\u4ea7\u80fd\u591f\u76f4\u63a5\u7528\u4e8e\u51e0\u4f55\u7cbe\u786e\u7684\u6df1\u5ea6\u611f\u77e5\uff0c\u5c06\u5176\u5e94\u7528\u8303\u56f4\u6269\u5c55\u5230\u53ef\u89c6\u5316\u4e4b\u5916\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u6a21\u62df\u89e3\u9501\u65b0\u80fd\u529b"}}
{"id": "2509.17435", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17435", "abs": "https://arxiv.org/abs/2509.17435", "authors": ["Xiaoyu Wang", "Yan Rui Tan", "William Leong", "Sunan Huang", "Rodney Teo", "Cheng Xiang"], "title": "GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a Low-Cost RGB Camera", "comment": null, "summary": "This paper proposes an image-based visual servoing (IBVS) framework for UAV\nnavigation and collision avoidance using only an RGB camera. While UAV\nnavigation has been extensively studied, it remains challenging to apply IBVS\nin missions involving multiple visual targets and collision avoidance. The\nproposed method achieves navigation without explicit path planning, and\ncollision avoidance is realized through AI-based monocular depth estimation\nfrom RGB images. Unlike approaches that rely on stereo cameras or external\nworkstations, our framework runs fully onboard a Jetson platform, ensuring a\nself-contained and deployable system. Experimental results validate that the\nUAV can navigate across multiple AprilTags and avoid obstacles effectively in\nGPS-denied environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9\u4f3a\u670d\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528RGB\u76f8\u673a\u5b9e\u73b0\u65e0\u4eba\u673a\u5bfc\u822a\u548c\u907f\u969c\uff0c\u65e0\u9700\u663e\u5f0f\u8def\u5f84\u89c4\u5212\uff0c\u901a\u8fc7AI\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5b9e\u73b0\u907f\u969c\uff0c\u5b8c\u5168\u5728Jetson\u5e73\u53f0\u4e0a\u8fd0\u884c\u3002", "motivation": "\u867d\u7136\u65e0\u4eba\u673a\u5bfc\u822a\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u6d89\u53ca\u591a\u4e2a\u89c6\u89c9\u76ee\u6807\u548c\u907f\u969c\u7684\u4efb\u52a1\u4e2d\u5e94\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9\u4f3a\u670d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7acb\u4f53\u76f8\u673a\u6216\u5916\u90e8\u5de5\u4f5c\u7ad9\uff0c\u7f3a\u4e4f\u81ea\u5305\u542b\u7684\u53ef\u90e8\u7f72\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9\u4f3a\u670d\u6846\u67b6\uff0c\u901a\u8fc7AI\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4eceRGB\u56fe\u50cf\u4e2d\u83b7\u53d6\u6df1\u5ea6\u4fe1\u606f\u5b9e\u73b0\u907f\u969c\uff0c\u65e0\u9700\u663e\u5f0f\u8def\u5f84\u89c4\u5212\uff0c\u6574\u4e2a\u7cfb\u7edf\u5b8c\u5168\u5728Jetson\u5e73\u53f0\u4e0a\u8fd0\u884c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u65e0\u4eba\u673a\u80fd\u591f\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u6709\u6548\u5bfc\u822a\u901a\u8fc7\u591a\u4e2aAprilTag\u5e76\u907f\u5f00\u969c\u788d\u7269\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u5305\u542b\u3001\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ec5\u4f7f\u7528RGB\u76f8\u673a\u5373\u53ef\u5b9e\u73b0\u65e0\u4eba\u673a\u5bfc\u822a\u548c\u907f\u969c\uff0c\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2509.17450", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17450", "abs": "https://arxiv.org/abs/2509.17450", "authors": ["Ying Feng", "Hongjie Fang", "Yinong He", "Jingjing Chen", "Chenxi Wang", "Zihao He", "Ruonan Liu", "Cewu Lu"], "title": "Learning Dexterous Manipulation with Quantized Hand State", "comment": null, "summary": "Dexterous robotic hands enable robots to perform complex manipulations that\nrequire fine-grained control and adaptability. Achieving such manipulation is\nchallenging because the high degrees of freedom tightly couple hand and arm\nmotions, making learning and control difficult. Successful dexterous\nmanipulation relies not only on precise hand motions, but also on accurate\nspatial positioning of the arm and coordinated arm-hand dynamics. However, most\nexisting visuomotor policies represent arm and hand actions in a single\ncombined space, which often causes high-dimensional hand actions to dominate\nthe coupled action space and compromise arm control. To address this, we\npropose DQ-RISE, which quantizes hand states to simplify hand motion prediction\nwhile preserving essential patterns, and applies a continuous relaxation that\nallows arm actions to diffuse jointly with these compact hand states. This\ndesign enables the policy to learn arm-hand coordination from data while\npreventing hand actions from overwhelming the action space. Experiments show\nthat DQ-RISE achieves more balanced and efficient learning, paving the way\ntoward structured and generalizable dexterous manipulation. Project website:\nhttp://rise-policy.github.io/DQ-RISE/", "AI": {"tldr": "DQ-RISE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\uff0c\u901a\u8fc7\u91cf\u5316\u624b\u90e8\u72b6\u6001\u6765\u7b80\u5316\u52a8\u4f5c\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u81c2-\u624b\u534f\u8c03\u5b66\u4e60\u80fd\u529b", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5c06\u624b\u81c2\u548c\u624b\u90e8\u52a8\u4f5c\u8868\u793a\u5728\u5355\u4e00\u7ec4\u5408\u7a7a\u95f4\u4e2d\uff0c\u5bfc\u81f4\u9ad8\u7ef4\u624b\u90e8\u52a8\u4f5c\u4e3b\u5bfc\u8026\u5408\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5f71\u54cd\u624b\u81c2\u63a7\u5236\u7cbe\u5ea6", "method": "\u91cf\u5316\u624b\u90e8\u72b6\u6001\u4ee5\u7b80\u5316\u624b\u90e8\u8fd0\u52a8\u9884\u6d4b\uff0c\u540c\u65f6\u5e94\u7528\u8fde\u7eed\u677e\u5f1b\u65b9\u6cd5\u4f7f\u624b\u81c2\u52a8\u4f5c\u80fd\u591f\u4e0e\u8fd9\u4e9b\u7d27\u51d1\u7684\u624b\u90e8\u72b6\u6001\u8054\u5408\u6269\u6563", "result": "\u5b9e\u9a8c\u8868\u660eDQ-RISE\u5b9e\u73b0\u4e86\u66f4\u5e73\u8861\u548c\u9ad8\u6548\u7684\u5b66\u4e60\uff0c\u4e3a\u7ed3\u6784\u5316\u548c\u53ef\u6cdb\u5316\u7684\u7075\u5de7\u64cd\u4f5c\u94fa\u5e73\u4e86\u9053\u8def", "conclusion": "DQ-RISE\u901a\u8fc7\u5206\u79bb\u548c\u91cf\u5316\u624b\u90e8\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7075\u5de7\u64cd\u4f5c\u4e2d\u81c2-\u624b\u534f\u8c03\u63a7\u5236\u7684\u6311\u6218"}}
{"id": "2509.17572", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.17572", "abs": "https://arxiv.org/abs/2509.17572", "authors": ["Vishnu Deo Mishra", "S Ganga Prasath"], "title": "Morphologies of a sagging elastica with intrinsic sensing and actuation", "comment": null, "summary": "The morphology of a slender soft-robot can be modified by sensing its shape\nvia sensors and exerting moments via actuators embedded along its body. The\nactuating moments required to morph these soft-robots to a desired shape are\noften difficult to compute due to the geometric non-linearity associated with\nthe structure, the errors in modeling the experimental system, and the\nlimitations in sensing and feedback/actuation capabilities. In this article, we\nexplore the effect of a simple feedback strategy (actuation being proportional\nto the sensed curvature) on the shape of a soft-robot, modeled as an elastica.\nThe finite number of sensors and actuators, often seen in experiments, is\ncaptured in the model via filters of specified widths. Using proportional\nfeedback, we study the simple task of straightening the device by compensating\nfor the sagging introduced by its self-weight. The device undergoes a hierarchy\nof morphological instabilities defined in the phase-space given by the\ngravito-bending number, non-dimensional sensing/feedback gain, and the scaled\nwidth of the filter. For complex shape-morphing tasks, given a perfect model of\nthe device with limited sensing and actuating capabilities, we find that a\ntrade-off arises (set by the sensor spacing & actuator size) between capturing\nthe long and short wavelength features. We show that the error in\nshape-morphing is minimal for a fixed filter width when we choose an\nappropriate actuating gain (whose magnitude goes as a square of the filter\nwidth). Our model provides a quantitative lens to study and design slender soft\ndevices with limited sensing and actuating capabilities for complex maneuvering\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5f62\u6001\u63a7\u5236\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u6bd4\u4f8b\u53cd\u9988\u7b56\u7565\uff08\u9a71\u52a8\u529b\u77e9\u4e0e\u611f\u77e5\u66f2\u7387\u6210\u6b63\u6bd4\uff09\u6765\u8865\u507f\u81ea\u91cd\u5f15\u8d77\u7684\u4e0b\u5782\uff0c\u5206\u6790\u4f20\u611f\u5668\u548c\u81f4\u52a8\u5668\u6570\u91cf\u9650\u5236\u5bf9\u5f62\u6001\u63a7\u5236\u7684\u5f71\u54cd\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u901a\u8fc7\u4f20\u611f\u5668\u611f\u77e5\u5f62\u72b6\u5e76\u901a\u8fc7\u81f4\u52a8\u5668\u65bd\u52a0\u529b\u77e9\u6765\u6539\u53d8\u5f62\u6001\uff0c\u4f46\u7531\u4e8e\u51e0\u4f55\u975e\u7ebf\u6027\u3001\u5efa\u6a21\u8bef\u5dee\u4ee5\u53ca\u4f20\u611f/\u81f4\u52a8\u80fd\u529b\u9650\u5236\uff0c\u8ba1\u7b97\u6240\u9700\u7684\u9a71\u52a8\u529b\u77e9\u975e\u5e38\u56f0\u96be\u3002", "method": "\u5c06\u8f6f\u4f53\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u5f39\u6027\u6746\uff0c\u4f7f\u7528\u6bd4\u4f8b\u53cd\u9988\u7b56\u7565\uff0c\u8003\u8651\u6709\u9650\u4f20\u611f\u5668\u548c\u81f4\u52a8\u5668\u7684\u5f71\u54cd\uff08\u901a\u8fc7\u6307\u5b9a\u5bbd\u5ea6\u7684\u6ee4\u6ce2\u5668\u6a21\u62df\uff09\uff0c\u7814\u7a76\u5728\u8865\u507f\u81ea\u91cd\u4e0b\u5782\u4efb\u52a1\u4e2d\u7684\u5f62\u6001\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u53d1\u73b0\u5f62\u6001\u4e0d\u7a33\u5b9a\u6027\u5728\u76f8\u7a7a\u95f4\uff08\u91cd\u529b\u5f2f\u66f2\u6570\u3001\u65e0\u91cf\u7eb2\u4f20\u611f/\u53cd\u9988\u589e\u76ca\u3001\u7f29\u653e\u6ee4\u6ce2\u5668\u5bbd\u5ea6\uff09\u4e2d\u5448\u73b0\u5c42\u6b21\u7ed3\u6784\u3002\u5728\u590d\u6742\u5f62\u6001\u4efb\u52a1\u4e2d\uff0c\u5b58\u5728\u957f\u6ce2\u548c\u77ed\u6ce2\u7279\u5f81\u6355\u83b7\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5f53\u9009\u62e9\u9002\u5f53\u7684\u9a71\u52a8\u589e\u76ca\u65f6\uff0c\u5f62\u6001\u8bef\u5dee\u6700\u5c0f\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7814\u7a76\u548c\u8bbe\u8ba1\u5177\u6709\u6709\u9650\u4f20\u611f\u548c\u81f4\u52a8\u80fd\u529b\u7684\u7ec6\u957f\u8f6f\u4f53\u8bbe\u5907\u63d0\u4f9b\u4e86\u5b9a\u91cf\u5206\u6790\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u673a\u52a8\u5e94\u7528\u3002"}}
{"id": "2509.17582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17582", "abs": "https://arxiv.org/abs/2509.17582", "authors": ["Vassil Atanassov", "Wanming Yu", "Siddhant Gangapurwala", "James Wilson", "Ioannis Havoutis"], "title": "GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation Skills on Legged Robots", "comment": "You can find an associated video here: https://youtu.be/o8Dd44MkG2E", "summary": "Most modern approaches to quadruped locomotion focus on using Deep\nReinforcement Learning (DRL) to learn policies from scratch, in an end-to-end\nmanner. Such methods often fail to scale, as every new problem or application\nrequires time-consuming and iterative reward definition and tuning. We present\nGeneralist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained\nwith Deep Reinforcement Learning that is capable of tracking arbitrary contact\npoints on a quadruped robot. The strength of our approach is that it provides a\ngeneral and modular low-level controller that can be reused for a wider range\nof high-level tasks, without the need to re-train new controllers from scratch.\nWe demonstrate the scalability and robustness of our method by evaluating on a\nwide range of locomotion and manipulation tasks in a common framework and under\na single generalist policy. These include a variety of gaits, traversing\ncomplex terrains (eg. stairs and slopes) as well as previously unseen\nstepping-stones and narrow beams, and interacting with objects (eg. pushing\nbuttons, tracking trajectories). Our framework acquires new behaviors more\nefficiently, simply by combining a task-specific high-level contact planner and\nthe pre-trained generalist policy. A supplementary video can be found at\nhttps://youtu.be/o8Dd44MkG2E.", "AI": {"tldr": "GeCCo\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u901a\u7528\u63a5\u89e6\u6761\u4ef6\u7b56\u7565\uff0c\u80fd\u591f\u8ddf\u8e2a\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u7684\u4efb\u610f\u63a5\u89e6\u70b9\uff0c\u907f\u514d\u4e86\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u91cd\u65b0\u8bad\u7ec3\u63a7\u5236\u5668\u7684\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u548c\u8c03\u6574\u5956\u52b1\u51fd\u6570\uff0c\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u4f4e\u5c42\u63a7\u5236\u5668\u6765\u652f\u6301\u591a\u79cd\u9ad8\u5c42\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u63a5\u89e6\u6761\u4ef6\u7b56\u7565\uff08GeCCo\uff09\uff0c\u8be5\u7b56\u7565\u80fd\u591f\u8ddf\u8e2a\u4efb\u610f\u63a5\u89e6\u70b9\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5c06\u9ad8\u5c42\u63a5\u89e6\u89c4\u5212\u5668\u4e0e\u9884\u8bad\u7ec3\u7684\u4f4e\u5c42\u7b56\u7565\u7ed3\u5408\u3002", "result": "\u5728\u591a\u79cd\u6b65\u6001\u3001\u590d\u6742\u5730\u5f62\uff08\u697c\u68af\u3001\u659c\u5761\u3001\u72ed\u7a84\u6a2a\u6881\uff09\u548c\u7269\u4f53\u4ea4\u4e92\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u83b7\u53d6\u65b0\u884c\u4e3a\u3002", "conclusion": "GeCCo\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u6a21\u5757\u5316\u7684\u4f4e\u5c42\u63a7\u5236\u5668\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5b66\u4e60\u65b0\u4efb\u52a1\u7684\u6548\u7387\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u63a7\u5236\u5668\u3002"}}
{"id": "2509.17666", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17666", "abs": "https://arxiv.org/abs/2509.17666", "authors": ["Mimo Shirasaka", "Cristian C. Beltran-Hernandez", "Masashi Hamaya", "Yoshitaka Ushiku"], "title": "Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery", "comment": null, "summary": "Object insertion tasks are prone to failures under pose uncertainties and\nenvironmental variations, traditionally requiring manual finetuning or\ncontroller retraining. We present a novel approach for robust and resilient\nobject insertion using a passively compliant soft wrist that enables safe\ncontact absorption through large deformations, without high-frequency control\nor force sensing. Our method structures insertion as compliance-enabled contact\nformations, sequential contact states that progressively constrain degrees of\nfreedom, and integrates automated failure recovery strategies. Our key insight\nis that wrist compliance permits safe, repeated recovery attempts; hence, we\nrefer to it as compliance-enabled failure recovery. We employ a pre-trained\nvision-language model (VLM) that assesses each skill execution from terminal\nposes and images, identifies failure modes, and proposes recovery actions by\nselecting skills and updating goals. In simulation, our method achieved an 83%\nsuccess rate, recovering from failures induced by randomized\nconditions--including grasp misalignments up to 5 degrees, hole-pose errors up\nto 20mm, fivefold increases in friction, and previously unseen\nsquare/rectangular pegs--and we further validate the approach on a real robot.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u88ab\u52a8\u67d4\u987a\u8f6f\u8155\u7684\u9c81\u68d2\u7269\u4f53\u63d2\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u67d4\u987a\u6027\u5b9e\u73b0\u5b89\u5168\u63a5\u89e6\u5438\u6536\u548c\u81ea\u52a8\u6545\u969c\u6062\u590d\uff0c\u65e0\u9700\u9ad8\u9891\u63a7\u5236\u6216\u529b\u4f20\u611f\u3002", "motivation": "\u4f20\u7edf\u7269\u4f53\u63d2\u5165\u4efb\u52a1\u5728\u59ff\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u73af\u5883\u53d8\u5316\u4e0b\u5bb9\u6613\u5931\u8d25\uff0c\u9700\u8981\u624b\u52a8\u8c03\u4f18\u6216\u63a7\u5236\u5668\u91cd\u8bad\u7ec3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u548c\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u88ab\u52a8\u67d4\u987a\u8f6f\u8155\u5b9e\u73b0\u5b89\u5168\u63a5\u89e6\u5438\u6536\uff0c\u5c06\u63d2\u5165\u8fc7\u7a0b\u6784\u5efa\u4e3a\u67d4\u987a\u6027\u652f\u6301\u7684\u63a5\u89e6\u5f62\u6210\u5e8f\u5217\uff0c\u96c6\u6210\u81ea\u52a8\u6545\u969c\u6062\u590d\u7b56\u7565\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6280\u80fd\u6267\u884c\u3001\u8bc6\u522b\u6545\u969c\u6a21\u5f0f\u5e76\u63d0\u51fa\u6062\u590d\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u8fbe\u523083%\u7684\u6210\u529f\u7387\uff0c\u80fd\u591f\u4ece\u968f\u673a\u5316\u6761\u4ef6\u4e0b\u7684\u6545\u969c\u4e2d\u6062\u590d\uff0c\u5305\u62ec5\u5ea6\u6293\u53d6\u504f\u5dee\u300120mm\u5b54\u4f4d\u8bef\u5dee\u30015\u500d\u6469\u64e6\u529b\u589e\u52a0\u4ee5\u53ca\u672a\u89c1\u8fc7\u7684\u65b9\u5f62/\u77e9\u5f62\u63d2\u5934\u3002", "conclusion": "\u67d4\u987a\u6027\u652f\u6301\u7684\u6545\u969c\u6062\u590d\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u4f53\u63d2\u5165\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u5f39\u6027\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u90fd\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2509.17683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17683", "abs": "https://arxiv.org/abs/2509.17683", "authors": ["Jonas Gruetter", "Lorenzo Terenzi", "Pascal Egli", "Marco Hutter"], "title": "Towards Learning Boulder Excavation with Hydraulic Excavators", "comment": null, "summary": "Construction sites frequently require removing large rocks before excavation\nor grading can proceed. Human operators typically extract these boulders using\nonly standard digging buckets, avoiding time-consuming tool changes to\nspecialized grippers. This task demands manipulating irregular objects with\nunknown geometries in harsh outdoor environments where dust, variable lighting,\nand occlusions hinder perception. The excavator must adapt to varying soil\nresistance--dragging along hard-packed surfaces or penetrating soft\nground--while coordinating multiple hydraulic joints to secure rocks using a\nshovel. Current autonomous excavation focuses on continuous media (soil,\ngravel) or uses specialized grippers with detailed geometric planning for\ndiscrete objects. These approaches either cannot handle large irregular rocks\nor require impractical tool changes that interrupt workflow. We train a\nreinforcement learning policy in simulation using rigid-body dynamics and\nanalytical soil models. The policy processes sparse LiDAR points (just 20 per\nrock) from vision-based segmentation and proprioceptive feedback to control\nstandard excavator buckets. The learned agent discovers different strategies\nbased on soil resistance: dragging along the surface in hard soil and\npenetrating directly in soft conditions. Field tests on a 12-ton excavator\nachieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to\n83% for human operators. This demonstrates that standard construction equipment\ncan learn complex manipulation despite sparse perception and challenging\noutdoor conditions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8ba9\u6807\u51c6\u6316\u6398\u673a\u94f2\u6597\u81ea\u4e3b\u63d0\u53d6\u5927\u578b\u4e0d\u89c4\u5219\u5ca9\u77f3\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6076\u52a3\u7684\u5ba4\u5916\u73af\u5883\u4e2d\u5904\u7406\u7a00\u758f\u611f\u77e5\u6570\u636e\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u7684\u571f\u58e4\u6761\u4ef6\u3002", "motivation": "\u5efa\u7b51\u5de5\u5730\u9700\u8981\u79fb\u9664\u5927\u578b\u5ca9\u77f3\u624d\u80fd\u8fdb\u884c\u6316\u6398\u6216\u5e73\u6574\u4f5c\u4e1a\uff0c\u4f46\u73b0\u6709\u81ea\u4e3b\u6316\u6398\u65b9\u6cd5\u8981\u4e48\u53ea\u80fd\u5904\u7406\u8fde\u7eed\u4ecb\u8d28\uff08\u571f\u58e4\u3001\u783e\u77f3\uff09\uff0c\u8981\u4e48\u9700\u8981\u4f7f\u7528\u4e13\u95e8\u7684\u6293\u53d6\u5668\u8fdb\u884c\u8be6\u7ec6\u7684\u51e0\u4f55\u89c4\u5212\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u5904\u7406\u5927\u578b\u4e0d\u89c4\u5219\u5ca9\u77f3\uff0c\u8981\u4e48\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u5de5\u5177\u66f4\u6362\uff0c\u4e2d\u65ad\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u4f7f\u7528\u521a\u4f53\u52a8\u529b\u5b66\u548c\u89e3\u6790\u571f\u58e4\u6a21\u578b\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5904\u7406\u6765\u81ea\u89c6\u89c9\u5206\u5272\u7684\u7a00\u758fLiDAR\u70b9\uff08\u6bcf\u4e2a\u5ca9\u77f3\u4ec520\u4e2a\u70b9\uff09\u548c\u672c\u4f53\u611f\u53d7\u53cd\u9988\uff0c\u4ee5\u63a7\u5236\u6807\u51c6\u6316\u6398\u673a\u94f2\u6597\u3002", "result": "\u572812\u5428\u6316\u6398\u673a\u4e0a\u7684\u73b0\u573a\u6d4b\u8bd5\u663e\u793a\uff0c\u5728\u5404\u79cd\u5ca9\u77f3\uff080.4-0.7\u7c73\uff09\u548c\u571f\u58e4\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e8670%\u7684\u6210\u529f\u7387\uff0c\u800c\u4eba\u7c7b\u64cd\u4f5c\u5458\u7684\u6210\u529f\u7387\u4e3a83%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6807\u51c6\u5efa\u7b51\u8bbe\u5907\u53ef\u4ee5\u5728\u7a00\u758f\u611f\u77e5\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u5ba4\u5916\u6761\u4ef6\u4e0b\u5b66\u4e60\u590d\u6742\u7684\u64cd\u4f5c\u6280\u80fd\u3002"}}
{"id": "2509.17750", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17750", "abs": "https://arxiv.org/abs/2509.17750", "authors": ["Inkyu Jang", "Jonghae Park", "Chams E. Mballo", "Sihyun Cho", "Claire J. Tomlin", "H. Jin Kim"], "title": "EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering", "comment": "Workshop on Safe and Robust Robot Learning for Operation in the Real\n  World (SAFE-ROL) at CoRL 2025", "summary": "We present EigenSafe, an operator-theoretic framework for learning-enabled\nsafety-critical control for stochastic systems. In many robotic systems where\ndynamics are best modeled as stochastic systems due to factors such as sensing\nnoise and environmental disturbances, it is challenging for conventional\nmethods such as Hamilton-Jacobi reachability and control barrier functions to\nprovide a holistic measure of safety. We derive a linear operator governing the\ndynamic programming principle for safety probability, and find that its\ndominant eigenpair provides information about safety for both individual states\nand the overall closed-loop system. The proposed learning framework, called\nEigenSafe, jointly learns this dominant eigenpair and a safe backup policy in\nan offline manner. The learned eigenfunction is then used to construct a safety\nfilter that detects potentially unsafe situations and falls back to the backup\npolicy. The framework is validated in three simulated stochastic\nsafety-critical control tasks.", "AI": {"tldr": "EigenSafe\u662f\u4e00\u4e2a\u57fa\u4e8e\u7b97\u5b50\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u968f\u673a\u7cfb\u7edf\u7684\u5b66\u4e60\u578b\u5b89\u5168\u5173\u952e\u63a7\u5236\uff0c\u901a\u8fc7\u4e3b\u5bfc\u7279\u5f81\u5bf9\u63d0\u4f9b\u5b89\u5168\u6982\u7387\u4fe1\u606f\u5e76\u6784\u5efa\u5b89\u5168\u8fc7\u6ee4\u5668\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u611f\u77e5\u566a\u58f0\u548c\u73af\u5883\u5e72\u6270\u7b49\u56e0\u7d20\uff0c\u52a8\u529b\u5b66\u901a\u5e38\u5efa\u6a21\u4e3a\u968f\u673a\u7cfb\u7edf\u3002\u4f20\u7edf\u65b9\u6cd5\u5982Hamilton-Jacobi\u53ef\u8fbe\u6027\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u96be\u4ee5\u63d0\u4f9b\u5168\u9762\u7684\u5b89\u5168\u5ea6\u91cf\u3002", "method": "\u63a8\u5bfc\u4e86\u5b89\u5168\u6982\u7387\u52a8\u6001\u89c4\u5212\u539f\u7406\u7684\u7ebf\u6027\u7b97\u5b50\uff0c\u53d1\u73b0\u5176\u4e3b\u5bfc\u7279\u5f81\u5bf9\u80fd\u63d0\u4f9b\u72b6\u6001\u548c\u95ed\u73af\u7cfb\u7edf\u7684\u5b89\u5168\u4fe1\u606f\u3002EigenSafe\u6846\u67b6\u79bb\u7ebf\u5b66\u4e60\u4e3b\u5bfc\u7279\u5f81\u5bf9\u548c\u5b89\u5168\u5907\u4efd\u7b56\u7565\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u7279\u5f81\u51fd\u6570\u6784\u5efa\u5b89\u5168\u8fc7\u6ee4\u5668\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u62df\u7684\u968f\u673a\u5b89\u5168\u5173\u952e\u63a7\u5236\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "EigenSafe\u4e3a\u968f\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u578b\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u68c0\u6d4b\u6f5c\u5728\u4e0d\u5b89\u5168\u60c5\u51b5\u5e76\u56de\u9000\u5230\u5907\u4efd\u7b56\u7565\u3002"}}
{"id": "2509.17759", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17759", "abs": "https://arxiv.org/abs/2509.17759", "authors": ["Chengbo Yuan", "Rui Zhou", "Mengzhen Liu", "Yingdong Hu", "Shengjie Wang", "Li Yi", "Chuan Wen", "Shanghang Zhang", "Yang Gao"], "title": "MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies", "comment": null, "summary": "Scaling real robot data is a key bottleneck in imitation learning, leading to\nthe use of auxiliary data for policy training. While other aspects of robotic\nmanipulation such as image or language understanding may be learned from\ninternet-based datasets, acquiring motion knowledge remains challenging. Human\ndata, with its rich diversity of manipulation behaviors, offers a valuable\nresource for this purpose. While previous works show that using human data can\nbring benefits, such as improving robustness and training efficiency, it\nremains unclear whether it can realize its greatest advantage: enabling robot\npolicies to directly learn new motions for task completion. In this paper, we\nsystematically explore this potential through multi-task human-robot\ncotraining. We introduce MotionTrans, a framework that includes a data\ncollection system, a human data transformation pipeline, and a weighted\ncotraining strategy. By cotraining 30 human-robot tasks simultaneously, we\ndirecly transfer motions of 13 tasks from human data to deployable end-to-end\nrobot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot\nmanner. MotionTrans also significantly enhances pretraining-finetuning\nperformance (+40% success rate). Through ablation study, we also identify key\nfactors for successful motion learning: cotraining with robot data and broad\ntask-related motion coverage. These findings unlock the potential of\nmotion-level learning from human data, offering insights into its effective use\nfor training robotic manipulation policies. All data, code, and model weights\nare open-sourced https://motiontrans.github.io/.", "AI": {"tldr": "MotionTrans\u662f\u4e00\u4e2a\u901a\u8fc7\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u4eba\u7c7b\u6570\u636e\u4e2d\u5b66\u4e60\u8fd0\u52a8\u77e5\u8bc6\u5e76\u8fc1\u79fb\u5230\u673a\u5668\u4eba\u7b56\u7565\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e8613\u4e2a\u4efb\u52a1\u7684\u96f6\u6837\u672c\u8fd0\u52a8\u8fc1\u79fb\uff0c\u5176\u4e2d9\u4e2a\u4efb\u52a1\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u4eba\u7c7b\u4e30\u5bcc\u7684\u64cd\u4f5c\u884c\u4e3a\u6570\u636e\u6765\u76f4\u63a5\u5b66\u4e60\u65b0\u7684\u8fd0\u52a8\u6280\u80fd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u63d0\u51faMotionTrans\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u7cfb\u7edf\u3001\u4eba\u7c7b\u6570\u636e\u8f6c\u6362\u7ba1\u9053\u548c\u52a0\u6743\u534f\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u540c\u65f6\u8bad\u7ec330\u4e2a\u4eba\u7c7b-\u673a\u5668\u4eba\u4efb\u52a1\u6765\u5b9e\u73b0\u8fd0\u52a8\u77e5\u8bc6\u7684\u8fc1\u79fb\u3002", "result": "\u6210\u529f\u5c0613\u4e2a\u4efb\u52a1\u7684\u8fd0\u52a8\u4ece\u4eba\u7c7b\u6570\u636e\u76f4\u63a5\u8fc1\u79fb\u5230\u53ef\u90e8\u7f72\u7684\u7aef\u5230\u7aef\u673a\u5668\u4eba\u7b56\u7565\uff0c9\u4e2a\u4efb\u52a1\u5b9e\u73b0\u96f6\u6837\u672c\u975e\u5e73\u51e1\u6210\u529f\u7387\uff0c\u9884\u8bad\u7ec3-\u5fae\u8c03\u6027\u80fd\u63d0\u534740%\u3002", "conclusion": "\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u4e86\u6210\u529f\u8fd0\u52a8\u5b66\u4e60\u7684\u5173\u952e\u56e0\u7d20\uff1a\u4e0e\u673a\u5668\u4eba\u6570\u636e\u534f\u540c\u8bad\u7ec3\u548c\u5e7f\u6cdb\u7684\u4efb\u52a1\u76f8\u5173\u8fd0\u52a8\u8986\u76d6\uff0c\u63ed\u793a\u4e86\u4ece\u4eba\u7c7b\u6570\u636e\u4e2d\u8fdb\u884c\u8fd0\u52a8\u7ea7\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17760", "categories": ["cs.RO", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17760", "abs": "https://arxiv.org/abs/2509.17760", "authors": ["Austin Wilson", "Sahar Kapasi", "Zane Greene", "Alexis E. Block"], "title": "Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research", "comment": null, "summary": "Many research groups face challenges when legacy (unsupported) robotic\nplatforms lose manufacturer support and cannot accommodate modern sensing,\nspeech, and interaction capabilities. We present the Enhanced NAO, a\nrevitalized version of Aldebaran's NAO robot that uses upgraded microphones,\nRGB-D and thermal cameras, and additional compute resources in a fully\nself-contained package. This system combines cloud and local models for\nperception and dialogue, while preserving the NAO's expressive body and\nbehaviors. In a pilot validation study, the Enhanced NAO delivered\nsignificantly higher conversational quality and stronger user preference\ncompared to the NAO AI Edition, without increasing response latency. Key\nupgrades, such as beamforming microphones and low-latency audio processing,\nreduced artifacts like self-hearing and improved multi-party separation.\nExpanded visual and thermal sensing established a foundation for future\ninteraction capabilities. Beyond the NAO, our framework provides a\nplatform-agnostic strategy for extending the lifespan and research utility of\nlegacy robots, ensuring they remain valuable tools for human-robot interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u589e\u5f3a\u7248NAO\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u5347\u7ea7\u9ea6\u514b\u98ce\u3001RGB-D\u548c\u70ed\u6210\u50cf\u76f8\u673a\u4ee5\u53ca\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e3a\u8fc7\u65f6\u7684\u673a\u5668\u4eba\u5e73\u53f0\u63d0\u4f9b\u73b0\u4ee3\u5316\u611f\u77e5\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u8d28\u91cf\u548c\u7528\u6237\u504f\u597d\u3002", "motivation": "\u8bb8\u591a\u7814\u7a76\u56e2\u961f\u9762\u4e34\u4f20\u7edf\u673a\u5668\u4eba\u5e73\u53f0\u56e0\u5236\u9020\u5546\u505c\u6b62\u652f\u6301\u800c\u65e0\u6cd5\u9002\u5e94\u73b0\u4ee3\u611f\u77e5\u3001\u8bed\u97f3\u548c\u4ea4\u4e92\u80fd\u529b\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5ef6\u957f\u8fd9\u4e9b\u673a\u5668\u4eba\u7684\u4f7f\u7528\u5bff\u547d\u548c\u7814\u7a76\u4ef7\u503c\u3002", "method": "\u5f00\u53d1\u589e\u5f3a\u7248NAO\u7cfb\u7edf\uff0c\u96c6\u6210\u4e91\u548c\u672c\u5730\u6a21\u578b\u8fdb\u884c\u611f\u77e5\u548c\u5bf9\u8bdd\uff0c\u4fdd\u7559NAO\u7684\u8868\u8fbe\u6027\u8eab\u4f53\u548c\u884c\u4e3a\uff0c\u540c\u65f6\u5347\u7ea7\u786c\u4ef6\u5305\u62ec\u6ce2\u675f\u6210\u5f62\u9ea6\u514b\u98ce\u3001RGB-D\u76f8\u673a\u3001\u70ed\u6210\u50cf\u76f8\u673a\u548c\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5728\u9a8c\u8bc1\u7814\u7a76\u4e2d\uff0c\u589e\u5f3a\u7248NAO\u76f8\u6bd4NAO AI\u7248\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u8bdd\u8d28\u91cf\u548c\u7528\u6237\u504f\u597d\uff0c\u540c\u65f6\u6ca1\u6709\u589e\u52a0\u54cd\u5e94\u5ef6\u8fdf\uff0c\u6539\u5584\u4e86\u591a\u53c2\u4e0e\u8005\u5206\u79bb\u5e76\u51cf\u5c11\u4e86\u81ea\u542c\u7b49\u4f2a\u5f71\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5ef6\u957f\u4f20\u7edf\u673a\u5668\u4eba\u7684\u5bff\u547d\u548c\u7814\u7a76\u6548\u7528\u63d0\u4f9b\u4e86\u5e73\u53f0\u65e0\u5173\u7684\u7b56\u7565\uff0c\u786e\u4fdd\u5b83\u4eec\u7ee7\u7eed\u6210\u4e3a\u4eba\u673a\u4ea4\u4e92\u7684\u6709\u4ef7\u503c\u5de5\u5177\u3002"}}
{"id": "2509.17783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17783", "abs": "https://arxiv.org/abs/2509.17783", "authors": ["Yibo Peng", "Jiahao Yang", "Shenhao Yan", "Ziyu Huang", "Shuang Li", "Shuguang Cui", "Yiming Zhao", "Yatong Han"], "title": "RoboSeek: You Need to Interact with Your Objects", "comment": null, "summary": "Optimizing and refining action execution through\n  exploration and interaction is a promising way for robotic\n  manipulation. However, practical approaches to interaction driven robotic\nlearning are still underexplored, particularly for\n  long-horizon tasks where sequential decision-making, physical\n  constraints, and perceptual uncertainties pose significant chal lenges.\nMotivated by embodied cognition theory, we propose\n  RoboSeek, a framework for embodied action execution that\n  leverages interactive experience to accomplish manipulation\n  tasks. RoboSeek optimizes prior knowledge from high-level\n  perception models through closed-loop training in simulation\n  and achieves robust real-world execution via a real2sim2real\n  transfer pipeline. Specifically, we first replicate real-world\n  environments in simulation using 3D reconstruction to provide\n  visually and physically consistent environments., then we train\n  policies in simulation using reinforcement learning and the\n  cross-entropy method leveraging visual priors. The learned\n  policies are subsequently deployed on real robotic platforms\n  for execution. RoboSeek is hardware-agnostic and is evaluated\n  on multiple robotic platforms across eight long-horizon ma nipulation tasks\ninvolving sequential interactions, tool use, and\n  object handling. Our approach achieves an average success rate\n  of 79%, significantly outperforming baselines whose success\n  rates remain below 50%, highlighting its generalization and\n  robustness across tasks and platforms. Experimental results\n  validate the effectiveness of our training framework in complex,\n  dynamic real-world settings and demonstrate the stability of the\n  proposed real2sim2real transfer mechanism, paving the way for\n  more generalizable embodied robotic learning. Project Page:\n  https://russderrick.github.io/Roboseek/", "AI": {"tldr": "RoboSeek\u662f\u4e00\u4e2a\u57fa\u4e8e\u5177\u8eab\u8ba4\u77e5\u7406\u8bba\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u4eff\u771f\u8bad\u7ec3\u548creal2sim2real\u8fc1\u79fb\u5b9e\u73b0\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u9c81\u68d2\u6267\u884c\uff0c\u57288\u4e2a\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u738779%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4ea4\u4e92\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u63a2\u7d22\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u987a\u5e8f\u51b3\u7b56\u3001\u7269\u7406\u7ea6\u675f\u548c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002\u53d7\u5177\u8eab\u8ba4\u77e5\u7406\u8bba\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4ea4\u4e92\u7ecf\u9a8c\u6765\u4f18\u5316\u52a8\u4f5c\u6267\u884c\u3002", "method": "\u9996\u5148\u901a\u8fc73D\u91cd\u5efa\u5728\u4eff\u771f\u4e2d\u590d\u73b0\u771f\u5b9e\u73af\u5883\uff0c\u7136\u540e\u5229\u7528\u89c6\u89c9\u5148\u9a8c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4ea4\u53c9\u71b5\u65b9\u6cd5\u8bad\u7ec3\u7b56\u7565\uff0c\u6700\u540e\u901a\u8fc7real2sim2real\u8fc1\u79fb\u7ba1\u9053\u5c06\u5b66\u4e60\u5230\u7684\u7b56\u7565\u90e8\u7f72\u5230\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u6267\u884c\u3002", "result": "\u5728\u6d89\u53ca\u987a\u5e8f\u4ea4\u4e92\u3001\u5de5\u5177\u4f7f\u7528\u548c\u7269\u4f53\u5904\u7406\u76848\u4e2a\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cRoboSeek\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523079%\uff0c\u663e\u8457\u4f18\u4e8e\u6210\u529f\u7387\u4f4e\u4e8e50%\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u8de8\u4efb\u52a1\u548c\u5e73\u53f0\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u8bad\u7ec3\u6846\u67b6\u5728\u590d\u6742\u52a8\u6001\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u6240\u63d0real2sim2real\u8fc1\u79fb\u673a\u5236\u7684\u7a33\u5b9a\u6027\uff0c\u4e3a\u66f4\u5177\u6cdb\u5316\u6027\u7684\u5177\u8eab\u673a\u5668\u4eba\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.17812", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17812", "abs": "https://arxiv.org/abs/2509.17812", "authors": ["Yitaek Kim", "Casper Hewson Rask", "Christoffer Sloth"], "title": "Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation", "comment": "This paper has submitted to Dexterous Humanoid Manipulation Workshop,\n  Humanoid 2025", "summary": "This paper proposes Tac2Motion, a contact-aware reinforcement learning\nframework to facilitate the learning of contact-rich in-hand manipulation\ntasks, such as removing a lid. To this end, we propose tactile sensing-based\nreward shaping and incorporate the sensing into the observation space through\nembedding. The designed rewards encourage an agent to ensure firm grasping and\nsmooth finger gaiting at the same time, leading to higher data efficiency and\nrobust performance compared to the baseline. We verify the proposed framework\non the opening a lid scenario, showing generalization of the trained policy\ninto a couple of object types and various dynamics such as torsional friction.\nLastly, the learned policy is demonstrated on the multi-fingered robot, Shadow\nRobot, showing that the control policy can be transferred to the real world.\nThe video is available: https://youtu.be/poeJBPR7urQ.", "AI": {"tldr": "Tac2Motion\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a5\u89e6\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u63a5\u89e6\u4e30\u5bcc\u7684\u7075\u5de7\u624b\u64cd\u4f5c\u4efb\u52a1\uff0c\u5982\u6253\u5f00\u76d6\u5b50\u3002\u901a\u8fc7\u89e6\u89c9\u611f\u77e5\u7684\u5956\u52b1\u5851\u9020\u548c\u5d4c\u5165\u89c2\u5bdf\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u63a5\u89e6\u4e30\u5bcc\u7684\u7075\u5de7\u624b\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u6253\u5f00\u76d6\u5b50\uff09\u7684\u5b66\u4e60\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u89e6\u89c9\u611f\u77e5\u7684\u5956\u52b1\u5851\u9020\u65b9\u6cd5\uff0c\u5c06\u89e6\u89c9\u611f\u77e5\u5d4c\u5165\u89c2\u5bdf\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u7262\u56fa\u6293\u63e1\u548c\u6d41\u7545\u7684\u624b\u6307\u6b65\u6001\u3002", "result": "\u5728\u5f00\u76d6\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u7b56\u7565\u5bf9\u4e0d\u540c\u7269\u4f53\u7c7b\u578b\u548c\u52a8\u529b\u5b66\uff08\u5982\u626d\u8f6c\u6469\u64e6\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728Shadow Robot\u591a\u6307\u673a\u5668\u4eba\u4e0a\u6210\u529f\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u8fc1\u79fb\u3002", "conclusion": "Tac2Motion\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u771f\u5b9e\u4e16\u754c\u9002\u7528\u6027\u3002"}}
{"id": "2509.17850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17850", "abs": "https://arxiv.org/abs/2509.17850", "authors": ["Xiao Zhou", "Zengqi Peng", "Jun Ma"], "title": "SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model", "comment": null, "summary": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for\nautonomous driving systems to avoid misguided decisions and potential\naccidents. However, achieving reliable predictions in highly dynamic and\ncomplex traffic scenarios remains a significant challenge. One of the key\nimpediments lies in the limited effectiveness of current approaches to capture\nthe multi-modal behaviors of drivers, which leads to predicted trajectories\nthat deviate from actual future motions. To address this issue, we propose\nSocialTraj, a novel trajectory prediction framework integrating social\npsychology principles through social value orientation (SVO). By utilizing\nBayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we\nobtain the critical social context to infer the future interaction trend. To\nensure modal consistency in predicted behaviors, the estimated SVOs of SVs are\nembedded into a conditional denoising diffusion model that aligns generated\ntrajectories with historical driving styles. Additionally, the planned future\ntrajectory of the ego vehicle (EV) is explicitly incorporated to enhance\ninteraction modeling. Extensive experiments on NGSIM and HighD datasets\ndemonstrate that SocialTraj is capable of adapting to highly dynamic and\ninteractive scenarios while generating socially compliant and behaviorally\nconsistent trajectory predictions, outperforming existing baselines. Ablation\nstudies demonstrate that dynamic SVO estimation and explicit ego-planning\ncomponents notably improve prediction accuracy and substantially reduce\ninference time.", "AI": {"tldr": "\u63d0\u51faSocialTraj\u6846\u67b6\uff0c\u901a\u8fc7\u793e\u4f1a\u4ef7\u503c\u53d6\u5411\uff08SVO\uff09\u6574\u5408\u793e\u4f1a\u5fc3\u7406\u5b66\u539f\u7406\uff0c\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5468\u56f4\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u793e\u4f1a\u5408\u89c4\u6027\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u96be\u4ee5\u6709\u6548\u6355\u6349\u9a7e\u9a76\u5458\u7684\u591a\u6a21\u6001\u884c\u4e3a\uff0c\u5bfc\u81f4\u9884\u6d4b\u8f68\u8ff9\u4e0e\u5b9e\u9645\u672a\u6765\u8fd0\u52a8\u5b58\u5728\u504f\u5dee\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u9006\u5f3a\u5316\u5b66\u4e60\u4f30\u8ba1\u5468\u56f4\u8f66\u8f86\u7684SVO\uff0c\u5c06\u4f30\u8ba1\u7684SVO\u5d4c\u5165\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u5e76\u663e\u5f0f\u7ed3\u5408\u81ea\u8f66\u89c4\u5212\u8f68\u8ff9\u6765\u589e\u5f3a\u4ea4\u4e92\u5efa\u6a21\u3002", "result": "\u5728NGSIM\u548cHighD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSocialTraj\u80fd\u591f\u9002\u5e94\u9ad8\u5ea6\u52a8\u6001\u548c\u4ea4\u4e92\u573a\u666f\uff0c\u751f\u6210\u793e\u4f1a\u5408\u89c4\u4e14\u884c\u4e3a\u4e00\u81f4\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001SVO\u4f30\u8ba1\u548c\u663e\u5f0f\u81ea\u8f66\u89c4\u5212\u7ec4\u4ef6\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u793e\u4f1a\u5fc3\u7406\u5b66\u539f\u7406\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.17877", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17877", "abs": "https://arxiv.org/abs/2509.17877", "authors": ["Richard Kuhlmann", "Jakob Wolfram", "Boyang Sun", "Jiaxu Xing", "Davide Scaramuzza", "Marc Pollefeys", "Cesar Cadena"], "title": "Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection", "comment": null, "summary": "Autonomous inspection is a central problem in robotics, with applications\nranging from industrial monitoring to search-and-rescue. Traditionally,\ninspection has often been reduced to navigation tasks, where the objective is\nto reach a predefined location while avoiding obstacles. However, this\nformulation captures only part of the real inspection problem. In real-world\nenvironments, the inspection targets may become visible well before their exact\ncoordinates are reached, making further movement both redundant and\ninefficient. What matters more for inspection is not simply arriving at the\ntarget's position, but positioning the robot at a viewpoint from which the\ntarget becomes observable. In this work, we revisit inspection from a\nperception-aware perspective. We propose an end-to-end reinforcement learning\nframework that explicitly incorporates target visibility as the primary\nobjective, enabling the robot to find the shortest trajectory that guarantees\nvisual contact with the target without relying on a map. The learned policy\nleverages both perceptual and proprioceptive sensing and is trained entirely in\nsimulation, before being deployed to a real-world robot. We further develop an\nalgorithm to compute ground-truth shortest inspection paths, which provides a\nreference for evaluation. Through extensive experiments, we show that our\nmethod outperforms existing classical and learning-based navigation approaches,\nyielding more efficient inspection trajectories in both simulated and\nreal-world settings. The project is avialable at\nhttps://sight-over-site.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u611f\u77e5\u611f\u77e5\u68c0\u67e5\u6846\u67b6\uff0c\u5c06\u76ee\u6807\u53ef\u89c1\u6027\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u627e\u5230\u4fdd\u8bc1\u4e0e\u76ee\u6807\u89c6\u89c9\u63a5\u89e6\u7684\u6700\u77ed\u8f68\u8ff9\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u5730\u56fe\u3002", "motivation": "\u4f20\u7edf\u68c0\u67e5\u65b9\u6cd5\u7b80\u5316\u4e3a\u5bfc\u822a\u4efb\u52a1\uff0c\u53ea\u5173\u6ce8\u5230\u8fbe\u9884\u5b9a\u4f4d\u7f6e\uff0c\u4f46\u5b9e\u9645\u68c0\u67e5\u4e2d\u76ee\u6807\u53ef\u80fd\u5728\u5230\u8fbe\u7cbe\u786e\u5750\u6807\u524d\u5c31\u53ef\u89c1\uff0c\u8fdb\u4e00\u6b65\u79fb\u52a8\u65e2\u5197\u4f59\u53c8\u4f4e\u6548\u3002\u771f\u6b63\u7684\u68c0\u67e5\u5173\u952e\u662f\u8ba9\u673a\u5668\u4eba\u5b9a\u4f4d\u5230\u80fd\u591f\u89c2\u5bdf\u76ee\u6807\u7684\u4f4d\u7f6e\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u660e\u786e\u5c06\u76ee\u6807\u53ef\u89c1\u6027\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\uff0c\u5229\u7528\u611f\u77e5\u548c\u672c\u4f53\u611f\u77e5\u4fe1\u606f\uff0c\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u7b56\u7565\u540e\u90e8\u7f72\u5230\u771f\u5b9e\u673a\u5668\u4eba\u3002\u8fd8\u5f00\u53d1\u4e86\u8ba1\u7b97\u771f\u5b9e\u6700\u77ed\u68c0\u67e5\u8def\u5f84\u7684\u7b97\u6cd5\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u7ecf\u5178\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u4ea7\u751f\u66f4\u9ad8\u6548\u7684\u68c0\u67e5\u8f68\u8ff9\u3002", "conclusion": "\u4ece\u611f\u77e5\u611f\u77e5\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u68c0\u67e5\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u68c0\u67e5\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u68c0\u67e5\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17884", "abs": "https://arxiv.org/abs/2509.17884", "authors": ["Arun L. Bishop", "Juan Alvarez-Padilla", "Sam Schoedel", "Ibrahima Sory Sow", "Juee Chandrachud", "Sheitej Sharma", "Will Kraus", "Beomyeong Park", "Robert J. Griffin", "John M. Dolan", "Zachary Manchester"], "title": "The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control", "comment": "Accepted to IEEE Humanoids 2025. For videos and code visit\n  https://linearwalking.github.io/", "summary": "When do locomotion controllers require reasoning about nonlinearities? In\nthis work, we show that a whole-body model-predictive controller using a simple\nlinear time-invariant approximation of the whole-body dynamics is able to\nexecute basic locomotion tasks on complex legged robots. The formulation\nrequires no online nonlinear dynamics evaluations or matrix inversions. We\ndemonstrate walking, disturbance rejection, and even navigation to a goal\nposition without a separate footstep planner on a quadrupedal robot. In\naddition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with\nsignificant limb inertia, complex actuator dynamics, and large sim-to-real gap.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u4f7f\u7528\u7ebf\u6027\u65f6\u4e0d\u53d8\u8fd1\u4f3c\u7684\u5168\u8eab\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5728\u590d\u6742\u817f\u5f0f\u673a\u5668\u4eba\u4e0a\u6267\u884c\u57fa\u672c\u8fd0\u52a8\u4efb\u52a1\uff0c\u65e0\u9700\u5728\u7ebf\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8bc4\u4f30\u6216\u77e9\u9635\u6c42\u9006\u3002", "motivation": "\u7814\u7a76\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u8fd0\u52a8\u63a7\u5236\u5668\u9700\u8981\u8003\u8651\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u63a2\u7d22\u7b80\u5316\u63a7\u5236\u65b9\u6cd5\u5728\u590d\u6742\u673a\u5668\u4eba\u4e0a\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u65f6\u4e0d\u53d8\u8fd1\u4f3c\u6765\u5efa\u6a21\u5168\u8eab\u52a8\u529b\u5b66\uff0c\u5f00\u53d1\u4e86\u65e0\u9700\u5728\u7ebf\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8bc4\u4f30\u6216\u77e9\u9635\u6c42\u9006\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u884c\u8d70\u3001\u6270\u52a8\u6291\u5236\u548c\u65e0\u9700\u5355\u72ec\u6b65\u6001\u89c4\u5212\u5668\u7684\u76ee\u6807\u4f4d\u7f6e\u5bfc\u822a\uff0c\u5e76\u5728\u6db2\u538b\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u52a8\u6001\u884c\u8d70\u80fd\u529b\u3002", "conclusion": "\u5373\u4f7f\u4f7f\u7528\u7b80\u5316\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u4e5f\u80fd\u5728\u5177\u6709\u663e\u8457\u80a2\u4f53\u60ef\u6027\u3001\u590d\u6742\u6267\u884c\u5668\u52a8\u529b\u5b66\u548c\u5927\u4eff\u771f-\u73b0\u5b9e\u5dee\u8ddd\u7684\u590d\u6742\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u6709\u6548\u7684\u8fd0\u52a8\u63a7\u5236\u3002"}}
{"id": "2509.17940", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17940", "abs": "https://arxiv.org/abs/2509.17940", "authors": ["Shuyao Shang", "Yuntao Chen", "Yuqi Wang", "Yingyan Li", "Zhaoxiang Zhang"], "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving", "comment": "NeurIPS 2025", "summary": "End-to-end autonomous driving has substantially progressed by directly\npredicting future trajectories from raw perception inputs, which bypasses\ntraditional modular pipelines. However, mainstream methods trained via\nimitation learning suffer from critical safety limitations, as they fail to\ndistinguish between trajectories that appear human-like but are potentially\nunsafe. Some recent approaches attempt to address this by regressing multiple\nrule-driven scores but decoupling supervision from policy optimization,\nresulting in suboptimal performance. To tackle these challenges, we propose\nDriveDPO, a Safety Direct Preference Optimization Policy Learning framework.\nFirst, we distill a unified policy distribution from human imitation similarity\nand rule-based safety scores for direct policy optimization. Further, we\nintroduce an iterative Direct Preference Optimization stage formulated as\ntrajectory-level preference alignment. Extensive experiments on the NAVSIM\nbenchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of\n90.0. Furthermore, qualitative results across diverse challenging scenarios\nhighlight DriveDPO's ability to produce safer and more reliable driving\nbehaviors.", "AI": {"tldr": "DriveDPO\u662f\u4e00\u4e2a\u5b89\u5168\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u6a21\u4eff\u76f8\u4f3c\u6027\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5b89\u5168\u8bc4\u5206\u6765\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b58\u5728\u5173\u952e\u7684\u5b89\u5168\u9650\u5236\uff0c\u65e0\u6cd5\u533a\u5206\u770b\u4f3c\u4eba\u7c7b\u9a7e\u9a76\u4f46\u6f5c\u5728\u4e0d\u5b89\u5168\u7684\u8f68\u8ff9\u3002\u4e00\u4e9b\u6700\u8fd1\u7684\u65b9\u6cd5\u5c1d\u8bd5\u901a\u8fc7\u56de\u5f52\u591a\u4e2a\u89c4\u5219\u9a71\u52a8\u8bc4\u5206\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u76d1\u7763\u4e0e\u7b56\u7565\u4f18\u5316\u8131\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u9996\u5148\u4ece\u4eba\u7c7b\u6a21\u4eff\u76f8\u4f3c\u6027\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5b89\u5168\u8bc4\u5206\u4e2d\u63d0\u70bc\u7edf\u4e00\u7684\u7b56\u7565\u5206\u5e03\u7528\u4e8e\u76f4\u63a5\u7b56\u7565\u4f18\u5316\u3002\u7136\u540e\u5f15\u5165\u8fed\u4ee3\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u9636\u6bb5\uff0c\u5c06\u5176\u5236\u5b9a\u4e3a\u8f68\u8ff9\u7ea7\u522b\u7684\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDriveDPO\u5b9e\u73b0\u4e8690.0\u7684PDMS\u65b0\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DriveDPO\u80fd\u591f\u4ea7\u751f\u66f4\u5b89\u5168\u548c\u66f4\u53ef\u9760\u7684\u9a7e\u9a76\u884c\u4e3a\uff0c\u5728\u591a\u6837\u5316\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.17941", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17941", "abs": "https://arxiv.org/abs/2509.17941", "authors": ["Zichao Hu", "Chen Tang", "Michael J. Munje", "Yifeng Zhu", "Alex Liu", "Shuijing Liu", "Garrett Warnell", "Peter Stone", "Joydeep Biswas"], "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion", "comment": "Conference on Robot Learning (CoRL) 2025 Project site:\n  https://amrl.cs.utexas.edu/ComposableNav/", "summary": "This paper considers the problem of enabling robots to navigate dynamic\nenvironments while following instructions. The challenge lies in the\ncombinatorial nature of instruction specifications: each instruction can\ninclude multiple specifications, and the number of possible specification\ncombinations grows exponentially as the robot's skill set expands. For example,\n\"overtake the pedestrian while staying on the right side of the road\" consists\nof two specifications: \"overtake the pedestrian\" and \"walk on the right side of\nthe road.\" To tackle this challenge, we propose ComposableNav, based on the\nintuition that following an instruction involves independently satisfying its\nconstituent specifications, each corresponding to a distinct motion primitive.\nUsing diffusion models, ComposableNav learns each primitive separately, then\ncomposes them in parallel at deployment time to satisfy novel combinations of\nspecifications unseen in training. Additionally, to avoid the onerous need for\ndemonstrations of individual motion primitives, we propose a two-stage training\nprocedure: (1) supervised pre-training to learn a base diffusion model for\ndynamic navigation, and (2) reinforcement learning fine-tuning that molds the\nbase model into different motion primitives. Through simulation and real-world\nexperiments, we show that ComposableNav enables robots to follow instructions\nby generating trajectories that satisfy diverse and unseen combinations of\nspecifications, significantly outperforming both non-compositional VLM-based\npolicies and costmap composing baselines. Videos and additional materials can\nbe found on the project page: https://amrl.cs.utexas.edu/ComposableNav/", "AI": {"tldr": "ComposableNav\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\uff0c\u80fd\u591f\u7ec4\u5408\u4e0d\u540c\u7684\u8fd0\u52a8\u57fa\u5143\u6765\u6ee1\u8db3\u6307\u4ee4\u4e2d\u7684\u591a\u4e2a\u89c4\u8303\u7ec4\u5408\uff0c\u5373\u4f7f\u8fd9\u4e9b\u7ec4\u5408\u5728\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u9075\u5faa\u5305\u542b\u591a\u4e2a\u89c4\u8303\u7ec4\u5408\u7684\u6307\u4ee4\u7684\u6311\u6218\uff0c\u907f\u514d\u89c4\u8303\u7ec4\u5408\u6570\u91cf\u968f\u6280\u80fd\u6269\u5c55\u800c\u6307\u6570\u589e\u957f\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5206\u522b\u5b66\u4e60\u6bcf\u4e2a\u8fd0\u52a8\u57fa\u5143\uff0c\u7136\u540e\u5728\u90e8\u7f72\u65f6\u5e76\u884c\u7ec4\u5408\u5b83\u4eec\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u9884\u8bad\u7ec3\u5b66\u4e60\u57fa\u7840\u6269\u6563\u6a21\u578b\uff0c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5c06\u5176\u5851\u9020\u6210\u4e0d\u540c\u7684\u8fd0\u52a8\u57fa\u5143\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cComposableNav\u80fd\u591f\u751f\u6210\u6ee1\u8db3\u591a\u6837\u4e14\u672a\u89c1\u8fc7\u7684\u89c4\u8303\u7ec4\u5408\u7684\u8f68\u8ff9\uff0c\u663e\u8457\u4f18\u4e8e\u975e\u7ec4\u5408\u7684VLM\u7b56\u7565\u548c\u6210\u672c\u56fe\u7ec4\u5408\u57fa\u7ebf\u3002", "conclusion": "ComposableNav\u901a\u8fc7\u7ec4\u5408\u5b66\u4e60\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u89c4\u8303\u7ec4\u5408\u7684\u6307\u6570\u589e\u957f\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17952", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17952", "abs": "https://arxiv.org/abs/2509.17952", "authors": ["Mahdi Nobar", "J\u00fcrg Keller", "Alessandro Forino", "John Lygeros", "Alisa Rupenyan"], "title": "Guided Multi-Fidelity Bayesian Optimization for Data-driven Controller Tuning with Digital Twins", "comment": "This preprint is intended for submission to IEEE Robotics and\n  Automation Letters (RA-L)", "summary": "We propose a \\textit{guided multi-fidelity Bayesian optimization} framework\nfor data-efficient controller tuning that integrates corrected digital twin\n(DT) simulations with real-world measurements. The method targets closed-loop\nsystems with limited-fidelity simulations or inexpensive approximations. To\naddress model mismatch, we build a multi-fidelity surrogate with a learned\ncorrection model that refines DT estimates from real data. An adaptive\ncost-aware acquisition function balances expected improvement, fidelity, and\nsampling cost. Our method ensures adaptability as new measurements arrive. The\naccuracy of DTs is re-estimated, dynamically adapting both cross-source\ncorrelations and the acquisition function. This ensures that accurate DTs are\nused more frequently, while inaccurate DTs are appropriately downweighted.\nExperiments on robotic drive hardware and supporting numerical studies\ndemonstrate that our method enhances tuning efficiency compared to standard\nBayesian optimization (BO) and multi-fidelity methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f15\u5bfc\u5f0f\u591a\u4fdd\u771f\u5ea6\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6570\u636e\u9ad8\u6548\u7684\u63a7\u5236\u5668\u8c03\u8c10\uff0c\u901a\u8fc7\u6574\u5408\u6821\u6b63\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u91cf\u6765\u5e94\u5bf9\u6709\u9650\u4fdd\u771f\u5ea6\u6a21\u62df\u7cfb\u7edf\u3002", "motivation": "\u9488\u5bf9\u5177\u6709\u6709\u9650\u4fdd\u771f\u5ea6\u6a21\u62df\u6216\u5ec9\u4ef7\u8fd1\u4f3c\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u89e3\u51b3\u6a21\u578b\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u9ad8\u63a7\u5236\u5668\u8c03\u8c10\u7684\u6548\u7387\u3002", "method": "\u6784\u5efa\u591a\u4fdd\u771f\u5ea6\u4ee3\u7406\u6a21\u578b\uff0c\u5305\u542b\u5b66\u4e60\u6821\u6b63\u6a21\u578b\u6765\u6539\u8fdb\u6570\u5b57\u5b6a\u751f\u4f30\u8ba1\uff1b\u4f7f\u7528\u81ea\u9002\u5e94\u6210\u672c\u611f\u77e5\u91c7\u96c6\u51fd\u6570\u5e73\u8861\u9884\u671f\u6539\u8fdb\u3001\u4fdd\u771f\u5ea6\u548c\u91c7\u6837\u6210\u672c\uff1b\u52a8\u6001\u8c03\u6574\u8de8\u6e90\u76f8\u5173\u6027\u548c\u91c7\u96c6\u51fd\u6570\u3002", "result": "\u5728\u673a\u5668\u4eba\u9a71\u52a8\u786c\u4ef6\u548c\u652f\u6301\u6027\u6570\u503c\u7814\u7a76\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u591a\u4fdd\u771f\u5ea6\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8c03\u8c10\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u786e\u4fdd\u9002\u5e94\u6027\uff0c\u968f\u7740\u65b0\u6d4b\u91cf\u6570\u636e\u7684\u5230\u6765\u52a8\u6001\u8c03\u6574\u6570\u5b57\u5b6a\u751f\u7684\u51c6\u786e\u6027\uff0c\u4f7f\u51c6\u786e\u6570\u5b57\u5b6a\u751f\u66f4\u9891\u7e41\u4f7f\u7528\uff0c\u4e0d\u51c6\u786e\u7684\u9002\u5f53\u964d\u6743\u3002"}}
{"id": "2509.18005", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18005", "abs": "https://arxiv.org/abs/2509.18005", "authors": ["Yanxin Zhang", "Liang He", "Zeyi Kang", "Zuheng Ming", "Kaixing Zhao"], "title": "M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer", "comment": "8 pages", "summary": "In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86M3ET\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7Mamba\u6a21\u5757\u548c\u8bed\u4e49\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u7279\u5f81\u878d\u5408\u548c\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u548c\u51cf\u5c11\u53c2\u6570\u91cf", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u5229\u7528\u6587\u672c\u6a21\u6001\uff0c\u4f9d\u8d56\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u65e0\u76d1\u7763\u673a\u5668\u4eba\u73af\u5883\u4e2d\u8bed\u4e49\u63d0\u53d6\u53d7\u9650\uff0c\u4e14\u8ba1\u7b97\u5bc6\u96c6\u5bfc\u81f4\u8d44\u6e90\u6d88\u8017\u9ad8", "method": "M3ET\u6a21\u578b\u7ed3\u5408Mamba\u6a21\u5757\u548c\u8bed\u4e49\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f18\u5316\u7279\u5f81\u878d\u5408\u3001\u5bf9\u9f50\u548c\u6a21\u6001\u91cd\u5efa", "result": "M3ET\u63d0\u5347\u8de8\u4efb\u52a1\u6027\u80fd\uff0c\u9884\u8bad\u7ec3\u63a8\u7406\u901f\u5ea6\u63d0\u53472.3\u500d\uff0cVQA\u4efb\u52a1\u51c6\u786e\u7387\u4fdd\u63010.74\uff0c\u53c2\u6570\u91cf\u51cf\u5c1167%", "conclusion": "\u5c3d\u7ba1EQA\u4efb\u52a1\u6027\u80fd\u6709\u9650\uff0c\u4f46M3ET\u7684\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u4f7f\u5176\u7279\u522b\u9002\u5408\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a"}}
{"id": "2509.18043", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18043", "abs": "https://arxiv.org/abs/2509.18043", "authors": ["Yinlong Dai", "Andre Keyser", "Dylan P. Losey"], "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States", "comment": null, "summary": "Imitation learning (IL) has proven effective across a wide range of\nmanipulation tasks. However, IL policies often struggle when faced with\nout-of-distribution observations; for instance, when the target object is in a\npreviously unseen position or occluded by other objects. In these cases,\nextensive demonstrations are needed for current IL methods to reach robust and\ngeneralizable behaviors. But when humans are faced with these sorts of atypical\ninitial states, we often rearrange the environment for more favorable task\nexecution. For example, a person might rotate a coffee cup so that it is easier\nto grasp the handle, or push a box out of the way so they can directly grasp\ntheir target object. In this work we seek to equip robot learners with the same\ncapability: enabling robots to prepare the environment before executing their\ngiven policy. We propose ReSET, an algorithm that takes initial states -- which\nare outside the policy's distribution -- and autonomously modifies object poses\nso that the restructured scene is similar to training data. Theoretically, we\nshow that this two step process (rearranging the environment before rolling out\nthe given policy) reduces the generalization gap. Practically, our ReSET\nalgorithm combines action-agnostic human videos with task-agnostic\nteleoperation data to i) decide when to modify the scene, ii) predict what\nsimplifying actions a human would take, and iii) map those predictions into\nrobot action primitives. Comparisons with diffusion policies, VLAs, and other\nbaselines show that using ReSET to prepare the environment enables more robust\ntask execution with equal amounts of total training data. See videos at our\nproject website: https://reset2025paper.github.io/", "AI": {"tldr": "ReSET\u7b97\u6cd5\u901a\u8fc7\u8ba9\u673a\u5668\u4eba\u5148\u8c03\u6574\u73af\u5883\u5e03\u5c40\u4f7f\u5176\u66f4\u63a5\u8fd1\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u7136\u540e\u518d\u6267\u884c\u4efb\u52a1\u7b56\u7565\uff0c\u4ece\u800c\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u5728\u9762\u5bf9\u5206\u5e03\u5916\u89c2\u5bdf\u65f6\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u9762\u5bf9\u76ee\u6807\u7269\u4f53\u4f4d\u7f6e\u53d8\u5316\u6216\u88ab\u906e\u6321\u7b49\u5206\u5e03\u5916\u89c2\u5bdf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5927\u91cf\u6f14\u793a\u6570\u636e\u624d\u80fd\u8fbe\u5230\u9c81\u68d2\u6027\u3002\u4eba\u7c7b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u4f1a\u5148\u8c03\u6574\u73af\u5883\u5e03\u5c40\u6765\u7b80\u5316\u4efb\u52a1\u6267\u884c\uff0c\u672c\u6587\u5e0c\u671b\u8d4b\u4e88\u673a\u5668\u4eba\u540c\u6837\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faReSET\u7b97\u6cd5\uff0c\u7ed3\u5408\u52a8\u4f5c\u65e0\u5173\u7684\u4eba\u7c7b\u89c6\u9891\u548c\u4efb\u52a1\u65e0\u5173\u7684\u9065\u64cd\u4f5c\u6570\u636e\uff0c\u6765\u51b3\u5b9a\u4f55\u65f6\u4fee\u6539\u573a\u666f\u3001\u9884\u6d4b\u4eba\u7c7b\u4f1a\u91c7\u53d6\u7684\u7b80\u5316\u52a8\u4f5c\uff0c\u5e76\u5c06\u8fd9\u4e9b\u9884\u6d4b\u6620\u5c04\u5230\u673a\u5668\u4eba\u52a8\u4f5c\u57fa\u5143\u4e2d\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8fd9\u79cd\u4e24\u6b65\u8fc7\u7a0b\uff08\u5148\u91cd\u65b0\u5b89\u6392\u73af\u5883\u518d\u6267\u884c\u7b56\u7565\uff09\u53ef\u4ee5\u51cf\u5c11\u6cdb\u5316\u5dee\u8ddd\u3002\u4e0e\u6269\u6563\u7b56\u7565\u3001VLAs\u7b49\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4f7f\u7528ReSET\u51c6\u5907\u73af\u5883\u53ef\u4ee5\u5728\u76f8\u540c\u8bad\u7ec3\u6570\u636e\u91cf\u4e0b\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "ReSET\u7b97\u6cd5\u901a\u8fc7\u8ba9\u673a\u5668\u4eba\u4e3b\u52a8\u8c03\u6574\u73af\u5883\u5e03\u5c40\u6765\u6539\u5584\u6a21\u4eff\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5904\u7406\u5206\u5e03\u5916\u89c2\u5bdf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18046", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18046", "abs": "https://arxiv.org/abs/2509.18046", "authors": ["Yinuo Wang", "Yuanyang Qi", "Jinzhao Zhou", "Gavin Tao"], "title": "HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba", "comment": "10 pages", "summary": "End-to-end reinforcement learning (RL) for humanoid locomotion is appealing\nfor its compact perception-action mapping, yet practical policies often suffer\nfrom training instability, inefficient feature fusion, and high actuation cost.\nWe present HuMam, a state-centric end-to-end RL framework that employs a\nsingle-layer Mamba encoder to fuse robot-centric states with oriented footstep\ntargets and a continuous phase clock. The policy outputs joint position targets\ntracked by a low-level PD loop and is optimized with PPO. A concise six-term\nreward balances contact quality, swing smoothness, foot placement, posture, and\nbody stability while implicitly promoting energy saving. On the JVRC-1 humanoid\nin mc-mujoco, HuMam consistently improves learning efficiency, training\nstability, and overall task performance over a strong feedforward baseline,\nwhile reducing power consumption and torque peaks. To our knowledge, this is\nthe first end-to-end humanoid RL controller that adopts Mamba as the fusion\nbackbone, demonstrating tangible gains in efficiency, stability, and control\neconomy.", "AI": {"tldr": "HuMam\u662f\u4e00\u4e2a\u57fa\u4e8eMamba\u7f16\u7801\u5668\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\uff0c\u901a\u8fc7\u878d\u5408\u673a\u5668\u4eba\u72b6\u6001\u3001\u8db3\u90e8\u76ee\u6807\u4f4d\u59ff\u548c\u8fde\u7eed\u76f8\u4f4d\u65f6\u949f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u8fd0\u52a8\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e2d\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u7279\u5f81\u878d\u5408\u6548\u7387\u4f4e\u548c\u9a71\u52a8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5355\u5c42Mamba\u7f16\u7801\u5668\u878d\u5408\u673a\u5668\u4eba\u4e2d\u5fc3\u72b6\u6001\u3001\u5b9a\u5411\u8db3\u90e8\u76ee\u6807\u548c\u8fde\u7eed\u76f8\u4f4d\u65f6\u949f\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u4f18\u5316\u7b56\u7565\uff0c\u8f93\u51fa\u5173\u8282\u4f4d\u7f6e\u76ee\u6807\u5e76\u901a\u8fc7PD\u63a7\u5236\u5668\u8ddf\u8e2a\uff0c\u8bbe\u8ba1\u516d\u9879\u5956\u52b1\u51fd\u6570\u5e73\u8861\u63a5\u89e6\u8d28\u91cf\u3001\u6446\u52a8\u5e73\u6ed1\u5ea6\u7b49\u8981\u7d20\u3002", "result": "\u5728JVRC-1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\uff0cHuMam\u76f8\u6bd4\u524d\u9988\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u529f\u8017\u548c\u626d\u77e9\u5cf0\u503c\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u91c7\u7528Mamba\u4f5c\u4e3a\u878d\u5408\u9aa8\u5e72\u7684\u7aef\u5230\u7aef\u4eba\u5f62\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u5728\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u63a7\u5236\u7ecf\u6d4e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u8fdb\u5c55\u3002"}}
{"id": "2509.18053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18053", "abs": "https://arxiv.org/abs/2509.18053", "authors": ["Hsu-kuang Chiu", "Ryo Hachiuma", "Chien-Yi Wang", "Yu-Chiang Frank Wang", "Min-Hung Chen", "Stephen F. Smith"], "title": "V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts", "comment": null, "summary": "Current state-of-the-art autonomous vehicles could face safety-critical\nsituations when their local sensors are occluded by large nearby objects on the\nroad. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed\nas a means of addressing this problem, and one recently introduced framework\nfor cooperative autonomous driving has further adopted an approach that\nincorporates a Multimodal Large Language Model (MLLM) to integrate cooperative\nperception and planning processes. However, despite the potential benefit of\napplying graph-of-thoughts reasoning to the MLLM, this idea has not been\nconsidered by previous cooperative autonomous driving research. In this paper,\nwe propose a novel graph-of-thoughts framework specifically designed for\nMLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our\nproposed novel ideas of occlusion-aware perception and planning-aware\nprediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for\ntraining and testing the cooperative driving graph-of-thoughts. Our\nexperimental results show that our method outperforms other baselines in\ncooperative perception, prediction, and planning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u601d\u7ef4\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u534f\u4f5c\u81ea\u52a8\u9a7e\u9a76\uff0c\u901a\u8fc7\u906e\u6321\u611f\u77e5\u611f\u77e5\u548c\u89c4\u5212\u611f\u77e5\u9884\u6d4b\u6765\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5c40\u90e8\u4f20\u611f\u5668\u88ab\u5927\u578b\u7269\u4f53\u906e\u6321\u65f6\u53ef\u80fd\u9762\u4e34\u5b89\u5168\u5173\u952e\u60c5\u51b5\uff0c\u800c\u73b0\u6709\u7684V2V\u534f\u4f5c\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u5c1a\u672a\u8003\u8651\u5c06\u56fe\u601d\u7ef4\u63a8\u7406\u5e94\u7528\u4e8eMLLM\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u56fe\u601d\u7ef4\u6846\u67b6\uff0c\u5305\u542b\u906e\u6321\u611f\u77e5\u611f\u77e5\u548c\u89c4\u5212\u611f\u77e5\u9884\u6d4b\u7b49\u65b0\u601d\u60f3\uff0c\u5e76\u5f00\u53d1\u4e86V2V-GoT-QA\u6570\u636e\u96c6\u548cV2V-GoT\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u534f\u4f5c\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u56fe\u601d\u7ef4\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u534f\u4f5c\u81ea\u52a8\u9a7e\u9a76\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f20\u611f\u5668\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2509.18068", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18068", "abs": "https://arxiv.org/abs/2509.18068", "authors": ["Bin Zhao", "Nakul Garg"], "title": "RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds", "comment": null, "summary": "Millimeter-wave radar provides perception robust to fog, smoke, dust, and low\nlight, making it attractive for size, weight, and power constrained robotic\nplatforms. Current radar imaging methods, however, rely on synthetic aperture\nor multi-frame aggregation to improve resolution, which is impractical for\nsmall aerial, inspection, or wearable systems. We present RadarSFD, a\nconditional latent diffusion framework that reconstructs dense LiDAR-like point\nclouds from a single radar frame without motion or SAR. Our approach transfers\ngeometric priors from a pretrained monocular depth estimator into the diffusion\nbackbone, anchors them to radar inputs via channel-wise latent concatenation,\nand regularizes outputs with a dual-space objective combining latent and\npixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer\nDistance and 28 cm Modified Hausdorff Distance, improving over the single-frame\nRadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame\nmethods using 5-41 frames. Qualitative results show recovery of fine walls and\nnarrow gaps, and experiments across new environments confirm strong\ngeneralization. Ablation studies highlight the importance of pretrained\ninitialization, radar BEV conditioning, and the dual-space loss. Together,\nthese results establish the first practical single-frame, no-SAR mmWave radar\npipeline for dense point cloud perception in compact robotic systems.", "AI": {"tldr": "RadarSFD\u662f\u4e00\u4e2a\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5e27\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u91cd\u5efa\u5bc6\u96c6\u7684LiDAR\u5f0f\u70b9\u4e91\uff0c\u65e0\u9700\u8fd0\u52a8\u6216\u591a\u5e27\u805a\u5408\uff0c\u9002\u7528\u4e8e\u5c0f\u578b\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "motivation": "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5728\u96fe\u3001\u70df\u3001\u7070\u5c18\u548c\u4f4e\u5149\u6761\u4ef6\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u9002\u5408\u5c3a\u5bf8\u3001\u91cd\u91cf\u548c\u529f\u7387\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002\u4f46\u73b0\u6709\u96f7\u8fbe\u6210\u50cf\u65b9\u6cd5\u9700\u8981\u5408\u6210\u5b54\u5f84\u6216\u591a\u5e27\u805a\u5408\u6765\u63d0\u9ad8\u5206\u8fa8\u7387\uff0c\u8fd9\u5bf9\u5c0f\u578b\u7cfb\u7edf\u4e0d\u5b9e\u7528\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u4ece\u9884\u8bad\u7ec3\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u8f6c\u79fb\u51e0\u4f55\u5148\u9a8c\uff0c\u901a\u8fc7\u901a\u9053\u7ea7\u6f5c\u5728\u8fde\u63a5\u5c06\u5176\u951a\u5b9a\u5230\u96f7\u8fbe\u8f93\u5165\uff0c\u5e76\u4f7f\u7528\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u548c\u50cf\u7d20\u7a7a\u95f4\u635f\u5931\u7684\u53cc\u7a7a\u95f4\u76ee\u6807\u8fdb\u884c\u6b63\u5219\u5316\u3002", "result": "\u5728RadarHD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRadarSFD\u8fbe\u523035\u5398\u7c73Chamfer\u8ddd\u79bb\u548c28\u5398\u7c73\u6539\u8fdbHausdorff\u8ddd\u79bb\uff0c\u4f18\u4e8e\u5355\u5e27\u57fa\u7ebf\uff0856\u5398\u7c73\uff0c45\u5398\u7c73\uff09\uff0c\u5e76\u4e0e\u4f7f\u75285-41\u5e27\u7684\u591a\u5e27\u65b9\u6cd5\u7ade\u4e89\u3002\u5b9a\u6027\u7ed3\u679c\u663e\u793a\u80fd\u591f\u6062\u590d\u7cbe\u7ec6\u5899\u58c1\u548c\u72ed\u7a84\u95f4\u9699\u3002", "conclusion": "RadarSFD\u5efa\u7acb\u4e86\u9996\u4e2a\u5b9e\u7528\u7684\u5355\u5e27\u3001\u65e0SAR\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7ba1\u9053\uff0c\u4e3a\u7d27\u51d1\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u5bc6\u96c6\u70b9\u4e91\u611f\u77e5\u80fd\u529b\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.18084", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18084", "abs": "https://arxiv.org/abs/2509.18084", "authors": ["Jiawen Tian", "Liqun Huang", "Zhongren Cui", "Jingchao Qiao", "Jiafeng Xu", "Xiao Ma", "Zeyu Ren"], "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces", "comment": "Tech Report.13 pages, 9 figures. Project page:\n  https://bytewrist.github.io/", "summary": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic\nparallel wrist for robotic manipulation. ByteWrist addresses the critical\nlimitations of existing serial and parallel wrists in narrow-space operations\nthrough a compact three-stage parallel drive mechanism integrated with\narc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)\nmotion while maintaining exceptional compactness, making it particularly\nsuitable for complex unstructured environments such as home services, medical\nassistance, and precision assembly. The key innovations include: (1) a nested\nthree-stage motor-driven linkages that minimize volume while enabling\nindependent multi-DOF control, (2) arc-shaped end linkages that optimize force\ntransmission and expand motion range, and (3) a central supporting ball\nfunctioning as a spherical joint that enhances structural stiffness without\ncompromising flexibility. Meanwhile, we present comprehensive kinematic\nmodeling including forward / inverse kinematics and a numerical Jacobian\nsolution for precise control. Empirically, we observe ByteWrist demonstrates\nstrong performance in narrow-space maneuverability and dual-arm cooperative\nmanipulation tasks, outperforming Kinova-based systems. Results indicate\nsignificant improvements in compactness, efficiency, and stiffness compared to\ntraditional designs, establishing ByteWrist as a promising solution for\nnext-generation robotic manipulation in constrained environments.", "AI": {"tldr": "ByteWrist\u662f\u4e00\u79cd\u65b0\u578b\u9ad8\u67d4\u6027\u4eff\u4eba\u5e76\u8054\u624b\u8155\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u4e09\u7ea7\u5e76\u8054\u9a71\u52a8\u673a\u5236\u548c\u5f27\u5f62\u672b\u7aef\u8fde\u6746\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e32\u8054\u548c\u5e76\u8054\u624b\u8155\u5728\u72ed\u7a84\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u7684\u5173\u952e\u9650\u5236\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e32\u8054\u548c\u5e76\u8054\u624b\u8155\u5728\u72ed\u7a84\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\uff08\u5982\u5bb6\u5ead\u670d\u52a1\u3001\u533b\u7597\u8f85\u52a9\u548c\u7cbe\u5bc6\u88c5\u914d\uff09\u63d0\u4f9b\u66f4\u7d27\u51d1\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5d4c\u5957\u4e09\u7ea7\u7535\u673a\u9a71\u52a8\u8fde\u6746\u8bbe\u8ba1\u51cf\u5c0f\u4f53\u79ef\uff0c\u5f27\u5f62\u672b\u7aef\u8fde\u6746\u4f18\u5316\u529b\u4f20\u9012\u548c\u6269\u5c55\u8fd0\u52a8\u8303\u56f4\uff0c\u4e2d\u592e\u652f\u6491\u7403\u4f5c\u4e3a\u7403\u5173\u8282\u589e\u5f3a\u7ed3\u6784\u521a\u5ea6\u3002\u540c\u65f6\u63d0\u4f9b\u5b8c\u6574\u7684\u8fd0\u52a8\u5b66\u5efa\u6a21\uff0c\u5305\u62ec\u6b63\u5411/\u9006\u5411\u8fd0\u52a8\u5b66\u548c\u6570\u503c\u96c5\u53ef\u6bd4\u89e3\u3002", "result": "ByteWrist\u5728\u72ed\u7a84\u7a7a\u95f4\u673a\u52a8\u6027\u548c\u53cc\u81c2\u534f\u540c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8eKinova\u7cfb\u7edf\uff0c\u5728\u7d27\u51d1\u6027\u3001\u6548\u7387\u548c\u521a\u5ea6\u65b9\u9762\u76f8\u6bd4\u4f20\u7edf\u8bbe\u8ba1\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "ByteWrist\u4e3a\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4e0b\u4e00\u4ee3\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7d27\u51d1\u6027\u3001\u7075\u6d3b\u6027\u548c\u6027\u80fd\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
