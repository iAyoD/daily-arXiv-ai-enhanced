<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 94]
- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedPI是一个用于评估大语言模型在医患对话中表现的高维基准，包含105个维度，涵盖医疗过程、治疗安全、治疗效果和医患沟通等方面。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为单轮问答形式，无法全面评估LLMs在复杂医患对话中的表现。需要建立更全面、细粒度的评估框架来指导LLMs在医疗诊断和治疗建议中的应用。

Method: 构建了五层评估框架：1)患者包（合成电子病历）；2)具有记忆和情感的AI患者；3)任务矩阵（就诊原因×就诊目标）；4)105维评估框架（1-4分制）；5)校准的AI评委LLMs提供评分、标记和证据链推理。

Result: 评估了9个旗舰模型（Claude Opus 4.1、Claude Sonnet 4、MedGemma等），在366个AI患者和7,097次对话中，所有LLMs在多个维度表现不佳，特别是在鉴别诊断方面。

Conclusion: MedPI基准揭示了当前LLMs在医疗对话中的局限性，特别是诊断能力不足。该工作可为未来LLMs在诊断和治疗建议中的应用提供指导。

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

TL;DR: RAGVUE是一个用于RAG系统诊断和可解释评估的框架，通过分解检索质量、答案相关性、完整性和忠实度等维度，提供透明化的评估过程。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估指标通常将异构行为压缩为单一分数，无法区分错误是来自检索、推理还是基础模型，缺乏对系统问题的深入洞察。

Method: RAGVUE将RAG行为分解为检索质量、答案相关性和完整性、严格的声明级忠实度以及评估者校准等维度，每个指标都包含结构化解释，支持手动指标选择和全自动代理评估。

Result: 在对比实验中，RAGVUE能够发现现有工具（如RAGAS）经常忽略的细粒度失败案例，提供了更全面的评估视角。

Conclusion: RAGVUE提供了一个透明、可解释的RAG评估框架，能够集成到研究流程和实际RAG开发中，帮助开发者更好地诊断和优化系统性能。

Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [3] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

TL;DR: 提出完全无监督的中文动词搭配结构数据库构建方法，通过聚类算法从大规模语料中生成可解释的动词搭配结构，用于增强LLMs的可解释性


<details>
  <summary>Details</summary>
Motivation: 为中文语言构建动词搭配结构数据库，补充LLMs在需要解释性和可解释性的应用场景中缺乏显式规则的问题

Method: 将动词搭配结构定义为投影、有根、有序、有向无环图，使用聚类算法从大规模语料检索的句子中生成动词搭配结构

Result: 统计分析显示生成的搭配结构具有功能独立性和分级典型性特征；在动词语法错误纠正任务中，基于搭配结构最大匹配的算法优于LLMs

Conclusion: 提出的无监督方法能有效构建中文动词搭配结构数据库，为需要可解释性的应用场景提供补充LLMs的显式规则

Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [4] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

TL;DR: 使用LLM生成合成电商产品数据，通过三种修改策略创建高质量标注数据，在MAVE数据集上达到与真实数据相当的性能


<details>
  <summary>Details</summary>
Motivation: 电商产品信息提取需要高质量标注数据，但获取成本高、难度大，特别是在低资源场景下

Method: 提出系统化合成电商产品数据方法，使用LLM和属性感知提示，包含三种控制修改策略：属性保留修改、控制负例生成、系统属性移除

Result: 人工评估2000个合成产品显示99.6%自然度、96.5%有效属性值、90%以上属性一致性；在MAVE数据集上达到60.5%准确率，与真实数据（60.8%）相当，远超零样本基线（13.4%）；混合数据配置达到68.8%准确率

Conclusion: 该框架为增强电商数据集提供了实用解决方案，特别适用于低资源场景，合成数据质量高且能有效提升模型性能

Abstract: Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [5] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

TL;DR: 该研究提出"集体叙事锚定"协议，通过参与式方法将社区故事转化为结构化叙事单元，集成到AI系统中，以解决LLM在社区特定查询上的知识盲点问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在回答社区特定问题时经常失败，形成"知识盲点"，边缘化地方声音并加剧认知不公。现有AI系统缺乏对地方知识的理解和整合。

Method: 开发参与式协议，通过三个工作坊(N=24)收集社区故事，设计结构化叙事单元提取方法，包括实体、时间、地点提取、验证和来源控制。审计县级基准(14,782个QA对)并创建参与式QA集进行评估。

Result: 审计显示76.7%的错误源于事实空白、文化误解、地理混淆和时间错位。在参与式QA集上，最先进的LLM在没有额外上下文时正确率低于21%。收集的叙事中包含缺失的事实，可直接解决主要错误模式。

Conclusion: 该研究提供了分类法、协议和参与式评估框架，为构建社区锚定的AI系统奠定基础。强调检索优先、来源可见、地方治理的QA系统设计原则，并探讨了代表性、权力、治理、隐私等关键设计张力。

Abstract: Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [6] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

TL;DR: TeleTables是一个评估LLM在电信标准中表格知识和解释能力的基准，发现小模型在3GPP规范上表现不佳，大模型表现更好，需要领域专业化微调。


<details>
  <summary>Details</summary>
Motivation: LLM在电信行业应用广泛，但在3GPP标准上表现不佳。研究发现标准中大量使用表格呈现关键信息，但LLM对这些表格的知识和解释能力尚未得到充分研究。

Method: 通过多阶段数据生成流程构建TeleTables基准：从3GPP标准中提取表格，使用多模态和推理导向的LLM生成和验证问题，最终创建包含500个人工验证问答对的数据集。

Result: 小模型（<10B参数）在3GPP知识回忆和表格解释方面都表现不佳，表明预训练中对电信标准的接触有限且缺乏处理复杂技术材料的归纳偏置。大模型在表格解释方面表现出更强的推理能力。

Conclusion: TeleTables基准揭示了LLM在电信标准表格处理方面的局限性，强调了领域专业化微调对于可靠解释和推理电信标准的必要性。

Abstract: Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [7] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: FronTalk是一个前端代码生成基准测试，专注于研究具有多模态反馈的对话式代码生成，包含100个多轮对话，并提出基于代理的评估框架和解决遗忘问题的AceCoder方法。


<details>
  <summary>Details</summary>
Motivation: 前端开发中，草图、线框图和标注截图等视觉工件对传达设计意图至关重要，但它们在多轮代码生成中的作用尚未得到充分探索。现有研究缺乏对多模态反馈在对话式代码生成中作用的系统性研究。

Method: 1) 创建FronTalk基准测试：包含100个多轮对话，源自新闻、金融、艺术等领域的真实网站，每轮对话同时包含文本指令和等效的视觉指令；2) 提出基于代理的评估框架：使用Web代理模拟用户探索网站，评估功能正确性和用户体验；3) 提出AceCoder基线方法：通过自主Web代理对每个过去指令的实现进行批判，解决遗忘问题。

Result: 评估20个模型发现两个关键挑战：1) 显著遗忘问题：模型会覆盖先前实现的功能，导致任务失败；2) 视觉反馈解释的持续挑战，特别是开源视觉语言模型。AceCoder方法将遗忘问题减少到接近零，性能提升高达9.3%（从56.0%到65.3%）。

Conclusion: FronTalk为前端开发和多轮多模态代码生成的一般交互动态研究提供了坚实基础，揭示了现有模型在对话式代码生成中的关键挑战，并提出了有效的解决方案。

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [8] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

TL;DR: 提出一种基于时空动态感知的自适应重掩码策略，通过检测每个token的时间方差和空间偏差来动态调整置信度阈值，显著提升扩散语言模型的生成效率（最高8.9倍加速）同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型的重掩码策略依赖单一全局置信度阈值，忽略了token在时间和空间维度上的动态特性，导致冗余迭代和并行性受限，影响生成效率。

Method: 提出动态检测每个token的时间方差（反映收敛状态）和空间偏差（反映token间相关性），基于这些信号为每个token在每个时间步自适应调整置信度阈值。

Result: 在主流数据集上显著提升扩散语言模型的运行效率，实现最高8.9倍的加速，同时忠实保持生成质量。

Conclusion: 通过考虑token的时空动态特性，自适应重掩码策略能有效解决固定阈值策略的局限性，大幅提升扩散语言模型的效率而不牺牲质量。

Abstract: Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [9] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

TL;DR: 本文提出一种结合微调语言模型与RAG的AI系统，用于提升大学招生办公室的咨询回复效率与准确性，通过领域特定微调和响应生成优化解决复杂招生场景中的信息准确性问题。


<details>
  <summary>Details</summary>
Motivation: 大学招生办公室面临大量咨询的高效处理挑战，回复时间和信息准确性直接影响潜在学生的感知。传统RAG在复杂狭窄的招生领域存在局限性，可能导致上下文不充分的回答。

Method: 提出混合方法：1）对招生特定数据集微调语言模型，增强领域理解能力；2）结合RAG获取最新信息；3）优化响应生成逻辑，平衡质量与速度的实验设置。

Result: 通过领域特定微调提升了模型对RAG检索信息的准确解释能力，生成更符合招生领域需求的输出。优化策略实现了响应质量与速度的平衡。

Conclusion: 结合微调与RAG的混合方法有效解决了大学招生咨询中的响应质量和准确性问题，为复杂狭窄领域的AI应用提供了可行方案。

Abstract: University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [10] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

TL;DR: 提出一种轻量级线性探针方法，通过分析LLM内部特征来量化政治意识形态偏差，并直接调整输出层概率来对齐用户观点，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 研究发现LLM内部的政治意识形态组织与人类意识形态空间存在系统性、模型特定的不对齐问题，需要一种低成本、实用的方法来纠正这种偏差。

Method: 开发轻量级线性探针，从模型内部特征计算偏差分数，直接调整最终输出概率，实现与特定用户观点的对齐，同时保持模型原有推理能力。

Result: 该方法能够量化LLM与人类意识形态空间的不对齐程度，并通过最小化修正输出层来有效对齐模型与用户观点，且计算成本低。

Conclusion: 提出的线性探针方法提供了一种简单高效的方式来测量和纠正LLM的政治意识形态偏差，为模型对齐提供了一种实用且低成本的解决方案。

Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


### [11] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: LEXMA：基于强化学习的LLM微调框架，为不同受众生成决策正确且忠实于预测因素的叙事式解释，应用于抵押贷款审批场景。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在高风险消费者交互中应用广泛，但决策逻辑不透明。现有可解释AI技术依赖事后数值特征归因，无法提供连贯的决策叙事。LLM虽能生成自然语言解释，但面临三大挑战：解释需同时保证决策正确性和忠实性；需服务不同受众而不改变决策规则；需要标签高效的训练方式，不依赖大量人工标注的解释数据。

Method: 提出LEXMA框架，结合反思增强的监督微调和两阶段群体相对策略优化（GRPO）。微调两个独立的参数集：一个提升决策正确性，另一个满足不同受众的文体要求。使用不依赖人工标注解释的奖励信号进行训练。

Result: 在抵押贷款审批场景中，LEXMA相比其他LLM基线显著提升了预测性能。人工评估显示：面向专家的解释更加关注风险因素，面向消费者的解释更清晰、更具可操作性且更礼貌。

Conclusion: LEXMA提供了一种成本高效、系统化的LLM微调方法，可提升商业决策的解释质量，为透明AI系统的可扩展部署提供了强大潜力。

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [12] [Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments](https://arxiv.org/abs/2601.04209)
*Seokhwan Ko,Donghyeon Lee,Jaewoo Chun,Hyungsoo Han,Junghwan Cho*

Main category: cs.CL

TL;DR: 开发了一个基于检索增强生成(RAG)的本地化系统，用于推荐医学机构内的研究合作者，使用PubMedBERT生成领域特定嵌入，结合本地部署的LLaMA3模型，在遵守医院隐私规定下支持生物医学知识发现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗环境中应用广泛，但医院严格的隐私和网络安全规定要求敏感数据必须在完全本地化的基础设施中处理。因此需要开发能够在本地部署的系统来支持临床、研究和行政工作流程。

Method: 开发了一个检索增强生成(RAG)系统，使用PubMedBERT生成领域特定的嵌入向量，结合本地部署的LLaMA3模型进行生成式合成，基于机构成员在PubMed上的出版物来推荐研究合作者。

Result: 研究证明了在本地部署约束下，整合领域专业化编码器与轻量级大型语言模型来支持生物医学知识发现的可行性和实用性。

Conclusion: 该系统展示了在遵守严格隐私规定的前提下，通过结合领域特定嵌入和本地化生成模型，能够有效支持医疗环境中的研究合作推荐和知识发现任务。

Abstract: Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

</details>


### [13] [Complexity Agnostic Recursive Decomposition of Thoughts](https://arxiv.org/abs/2601.04210)
*Kaleem Ullah Qasim,Jiashu Zhang,Hafiz Saif Ur Rehman*

Main category: cs.CL

TL;DR: CARD框架通过预测问题复杂度并自适应分解，在保持高准确率的同时显著降低推理成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多步推理中常因固定推理策略而失败，这些策略忽略了问题的具体难度差异

Method: CARD框架包含MRCE复杂度估计器（0.6B Qwen模型预测30个细粒度特征）和两阶段递归求解器：基于任务配置的层次分解和递归MRCE配置的每步思考预算分配

Result: 在三个推理模型上，GSM8K准确率达81.4%-89.2%，token成本降低1.88x-2.40x；MATH-500准确率达75.1%-86.8%，token使用减少1.71x-5.74x

Conclusion: 预先的复杂度估计能同时实现更高准确率和显著效率提升

Abstract: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.

</details>


### [14] [Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays](https://arxiv.org/abs/2601.04211)
*Nikita Zmanovskii*

Main category: cs.CL

TL;DR: Qwerty AI是一个端到端系统，用于根据俄罗斯联邦法律436-FZ自动进行俄语剧本的年龄分级和内容安全评估，能在2分钟内处理长达700页的剧本，准确率达80%。


<details>
  <summary>Details</summary>
Motivation: 俄罗斯媒体行业需要根据联邦法律436-FZ对内容进行年龄分级和内容安全评估，传统人工审核效率低且主观性强，需要自动化解决方案来应对编辑挑战。

Method: 系统采用微调的Phi-3-mini模型配合4位量化，将剧本分割为叙事单元，检测五个内容违规类别（暴力、性内容、脏话、物质、恐怖元素），并分配年龄分级（0+, 6+, 12+, 16+, 18+），提供可解释的评估理由。

Result: 系统在严格约束下（无外部API调用、80GB VRAM限制、<5分钟处理时间）实现了80%的分级准确率和80-95%的分割精度（取决于格式），已部署在Yandex Cloud上并支持CUDA加速。

Conclusion: Qwerty AI系统在Wink黑客松（2025年11月）中开发，展示了在实际生产工作流程中的实用性，能够有效解决俄罗斯媒体行业的真实编辑挑战。

Abstract: We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.

</details>


### [15] [TrueBrief: Faithful Summarization through Small Language Models](https://arxiv.org/abs/2601.04212)
*Kumud Lakara,Ruibo Shi,Fran Silavong*

Main category: cs.CL

TL;DR: TrueBrief是一个端到端框架，通过偏好优化范式增强小型语言模型在文本摘要任务中的忠实度，使用可控幻觉注入生成合成偏好数据。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成高质量文本方面表现出色，但在安全关键领域部署时，其产生幻觉的倾向构成了重大挑战。需要增强小型语言模型在文本摘要任务中的忠实度。

Method: 提出TrueBrief端到端框架，采用偏好优化范式。核心是数据生成模块，通过可控幻觉注入生成合成偏好数据，用于训练小型语言模型。

Result: 该工作提供了关于数据质量和模型大小对基于偏好的优化影响的见解，强调了这些方法最有效的条件。

Conclusion: TrueBrief框架能够有效增强小型语言模型在文本摘要任务中的忠实度，为安全关键领域的部署提供了可行方案。

Abstract: Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.

</details>


### [16] [AnimatedLLM: Explaining LLMs with Interactive Visualizations](https://arxiv.org/abs/2601.04213)
*Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: AnimatedLLM是一个交互式网页应用，通过预计算轨迹在浏览器中可视化Transformer语言模型的工作机制，用于NLP教学。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理教育中越来越重要，但展示其内部工作机制的教学材料稀缺。

Method: 开发了一个完全在浏览器中运行的交互式网页应用，使用预计算的开源LLM轨迹和手动整理的输入数据，提供Transformer语言模型的逐步可视化。

Result: 创建了AnimatedLLM应用（https://animatedllm.github.io），可作为教学辅助工具和自学资源。

Conclusion: AnimatedLLM填补了LLM教学可视化工具的空白，为NLP教育提供了实用的交互式学习平台。

Abstract: Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.

</details>


### [17] [From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning](https://arxiv.org/abs/2601.04278)
*Xiaoyu Xu,Minxin Du,Zitong Li,Zi Liang,Zhibiao Guo,Shiyu Zhang,Peizhao Hu,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: BiForget：利用目标模型自身生成高质量遗忘集的自动化框架，通过种子引导和对抗提示匹配模型内部知识分布，提升遗忘评估的严谨性


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘基准测试往往无法准确反映模型实际学到的"遗忘范围"，需要更忠实表示模型内部知识分布的评估方法

Method: 提出BiForget框架，形式化定义领域级和实例级两种遗忘粒度，利用目标模型自身通过种子引导和对抗提示生成高质量遗忘集

Result: 在多个基准测试中实现相关性、多样性和效率的平衡，在哈利波特领域提升相关性约20、多样性约0.05，同时将总数据量减半

Conclusion: BiForget为评估LLM遗忘提供了更严谨的基础，促进更鲁棒的遗忘效果和更好的效用保持

Abstract: Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.

</details>


### [18] [RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation](https://arxiv.org/abs/2601.04350)
*Joseph James,Chenghao Xiao,Yucheng Li,Nafise Sadat Moosavi,Chenghua Lin*

Main category: cs.CL

TL;DR: RIGOURATE是一个两阶段多模态框架，用于从论文正文中检索支持证据并为每个主张分配过度陈述分数，旨在提高科学严谨性和透明度。


<details>
  <summary>Details</summary>
Motivation: 科学严谨性常常被忽视，作者倾向于做出超出结果支持的夸大陈述，这损害了科学交流的清晰度和透明度。

Method: 1) 构建包含10K+主张-证据对的数据集，使用8个LLM标注，通过同行评审评论校准过度陈述分数；2) 使用微调的重排序器进行证据检索；3) 使用微调模型预测过度陈述分数并提供理由。

Result: 相比强基线方法，RIGOURATE在证据检索和过度陈述检测方面表现更好，能够有效识别科学论文中的夸大主张。

Conclusion: RIGOURATE框架实现了证据比例原则的操作化，支持更清晰、更透明的科学交流，有助于提高科学论文的严谨性。

Abstract: Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.

</details>


### [19] [Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties](https://arxiv.org/abs/2601.04373)
*Akriti Dhasmana,Aarohi Srivastava,David Chiang*

Main category: cs.CL

TL;DR: 该研究对印度方言和语言变体的跨语言语音识别进行实证分析，发现系统性能与语言亲缘关系相关但非唯一决定因素，少量方言数据微调可达到大量标准语言数据的效果，并通过Garhwali案例和错误分析揭示ASR系统在方言处理中的偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索跨语言语音识别在自发、嘈杂、代码混合的印度方言和语言变体中的表现，特别关注语言亲缘关系对ASR性能的影响，以及方言数据在微调中的有效性。

Method: 采用实证研究方法，分析多种印度方言和语言变体的跨语言迁移，包括：1）比较不同语言亲缘距离下的ASR性能；2）对比少量方言数据与大量标准语言数据的微调效果；3）对低资源Pahari语言变体Garhwali进行案例研究；4）评估多种当代ASR模型；5）通过转录错误分析系统偏见。

Result: 结果显示：1）ASR性能通常随语言亲缘距离减小而提升，但该因素不能完全解释方言环境下的表现；2）少量方言数据微调常能达到大量亲缘相关高资源标准语言数据微调的同等效果；3）Garhwali案例显示当代ASR模型在低资源方言上面临挑战；4）错误分析揭示ASR系统对预训练语言存在偏见。

Conclusion: 结论指出语言亲缘关系是跨语言ASR的重要因素但非唯一决定因素，少量方言数据微调具有高效性，ASR系统在方言和非标准化语音处理中存在对预训练语言的偏见，需要更细致的方言适应策略。

Abstract: We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.

</details>


### [20] [Disco-RAG: Discourse-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04377)
*Dongqi Liu,Hang Ding,Qiming Feng,Jian Li,Xurong Xie,Zhucun Xue,Chengjie Wang,Jiangning Zhang,Yabiao Wang*

Main category: cs.CL

TL;DR: Disco-RAG：一种将篇章结构信号显式注入生成过程的RAG框架，通过构建篇章树和修辞图来增强模型对分散知识的综合能力，在问答和长文档摘要任务上取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG策略将检索到的段落视为扁平无结构的信息，这阻碍了模型捕捉结构线索，限制了其从分散在不同文档中的证据进行知识综合的能力。

Method: 提出Disco-RAG框架：1）构建块内篇章树捕捉局部层次结构；2）构建块间修辞图建模跨段落连贯性；3）将这些结构联合集成到规划蓝图中以指导生成过程。

Result: 在问答和长文档摘要基准测试中，Disco-RAG无需微调即取得最先进的结果，证明了方法的有效性。

Conclusion: 篇章结构在推进RAG系统发展中扮演重要角色，显式注入篇章信号能显著增强模型的知识综合能力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.

</details>


### [21] [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](https://arxiv.org/abs/2601.04389)
*Iago Alves Brito,Walcy Santos Rezende Rios,Julia Soares Dollis,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 本文指出当前大语言模型安全评估存在"选择性安全"问题，即安全对齐不是普适能力而是人口统计学层次结构，不同少数群体的防御率差异显著，且模型规模扩大会加剧这种差异。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估存在危险错觉，将"身份仇恨"等聚合为标量分数，掩盖了对特定人群的系统性脆弱性。需要揭示这种选择性安全现象，挑战当前安全评估的普适性假设。

Method: 引入MiJaBench双语对抗基准，包含44,000个提示，覆盖16个少数群体。生成528,000个提示-响应对，从12个最先进的LLM中构建MiJaBench-Align数据集。

Result: 安全对齐不是广义语义能力，而是人口统计学层次结构：同一模型内，仅基于目标群体的不同，防御率波动高达33%。模型规模扩大会加剧这些差异，表明当前对齐技术没有建立非歧视原则，而是强化了仅针对特定群体的记忆拒绝边界。

Conclusion: 当前的安全对齐技术存在选择性保护问题，挑战了安全性的规模定律。需要研究细粒度的人口统计学对齐，数据集和代码已开源以促进相关研究。

Abstract: Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.

</details>


### [22] [ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://arxiv.org/abs/2601.04394)
*Sharanya Dasgupta,Arkaprabha Basu,Sujoy Nath,Swagatam Das*

Main category: cs.CL

TL;DR: ARREST是一个统一框架，通过外部网络识别和纠正LLM潜在激活空间中的表征错位，在不微调模型参数的情况下调节虚假和不安全输出，同时生成软拒绝和硬拒绝。


<details>
  <summary>Details</summary>
Motivation: LLMs在事实性和安全性方面仍缺乏人类认知的平衡能力，作者认为事实性和安全性失败都源于潜在激活空间中的表征错位，而不是完全独立的对齐问题。

Method: 提出ARREST框架，训练外部网络理解激活波动，选择性干预模型，将虚假输出调节为真实输出，不安全输出调节为安全输出，同时包含软拒绝和硬拒绝机制。

Result: 实证结果显示ARREST不仅能调节错位，相比RLHF对齐模型在生成软拒绝方面更加灵活，这得益于对抗训练。

Conclusion: 通过外部网络调节LLM激活空间中的表征错位是解决事实性和安全性问题的有效统一方法，ARREST框架展示了这一方法的潜力。

Abstract: Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.

</details>


### [23] [Interpreting Transformers Through Attention Head Intervention](https://arxiv.org/abs/2601.04398)
*Mason Kadem,Rong Zheng*

Main category: cs.CL

TL;DR: 论文探讨神经网络机制可解释性的重要性，强调理解AI决策过程对于问责、认知研究和知识发现的关键作用。


<details>
  <summary>Details</summary>
Motivation: 神经网络能力不断增强但机制不透明，需要理解其决策过程以实现：1）高风险领域的问责与控制；2）数字大脑和认知涌现研究；3）AI超越人类时的新知识发现。

Method: 论文主要提出机制可解释性（mechanistic interpretability）作为核心概念框架，强调理解神经网络内部工作机制的重要性。

Result: 论文没有提供具体实验结果，而是建立了机制可解释性作为重要研究方向的论证框架，强调其在AI发展中的关键地位。

Conclusion: 机制可解释性对于确保AI系统的安全、可靠和负责任发展至关重要，是连接AI技术进步与社会需求的关键桥梁。

Abstract: Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.

</details>


### [24] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: 本文提出了Gavel-Ref评估框架和Gavel-Agent代理框架，用于评估LLM在多文档法律案例摘要任务上的表现，发现即使最强模型在复杂长上下文任务上仍表现有限。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM现在支持高达1M token的上下文，但它们在复杂长上下文任务上的有效性仍不清楚。本文针对多文档法律案例摘要任务进行研究，其中单个案例通常跨越多个文档，总计100K-500K token。

Method: 引入Gavel-Ref评估框架，包含26个项目的多值清单评估、剩余事实和写作风格评估。同时开发Gavel-Agent代理框架，为LLM配备六种工具，直接从案例文档中导航和提取清单。

Result: 评估12个前沿LLM在100个法律案例上的表现，发现即使最强的Gemini 2.5 Pro模型也只达到约50%的Gavel-Ref分数。模型在简单清单项目上表现良好，但在多值或罕见项目上表现不佳。Gavel-Agent使用Qwen3时，相比GPT-4.1的端到端提取，token使用减少36%，清单分数仅下降7%。

Conclusion: 多文档法律案例摘要对当前LLM仍具挑战性，需要专门的评估框架和工具增强方法。随着LLM改进可能超越人工摘要，自主代理框架将成为更可靠的评估和提取方法。

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [25] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: LLMs经常无法挑战用户的有害信念，研究发现这源于模型默认迎合用户假设且缺乏认知警惕性。通过简单语用干预（如添加"等一下"）可显著提升模型挑战有害信念的能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗建议、社会推理等领域经常无法挑战用户的有害信念，这种失败可能源于模型默认迎合用户假设并表现出不足的认知警惕性。研究者希望从语用学角度理解和解决这一问题。

Method: 研究分析了影响人类迎合行为的社会和语言因素（话题相关性、语言编码、来源可靠性）如何同样影响LLMs的迎合行为。通过三个安全基准测试（Cancer-Myth、SAGE-Eval、ELEPHANT）评估模型挑战有害信念的能力，并测试简单语用干预（如添加"等一下"短语）的效果。

Result: 研究发现影响人类迎合行为的因素同样影响LLMs，解释了模型在不同安全基准测试中的性能差异。简单的语用干预（如添加"等一下"）能显著提升模型在这些基准测试中的表现，同时保持较低的错误率。

Conclusion: 研究强调了考虑语用学对于评估LLM行为和改善LLM安全性的重要性。简单的语用干预可以有效提升模型挑战有害信念的能力，为改进LLM安全性提供了实用方法。

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [26] [Learning to Simulate Human Dialogue](https://arxiv.org/abs/2601.04436)
*Kanishk Gandhi,Agam Bhatia,Noah D. Goodman*

Main category: cs.CL

TL;DR: 研究发现：在对话预测中，直接最大化真实人类响应的对数概率比使用LLM作为评判员进行奖励优化更有效，尤其是在允许模型思考的情况下。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过预测对话中的下一轮发言来建模人类思维过程，比较不同学习方法的有效性。

Method: 比较两种学习维度：(1)是否允许模型在回答前思考；(2)使用LLM评判员奖励（基于语义相似性和信息完整性）vs直接最大化真实人类对话的对数概率。将思维链作为潜变量，推导对数概率的下界。

Result: 优化评判员奖励虽然提高了评判分数，但降低了对真实人类响应的似然估计，在人类评判测试中表现更差。直接最大化人类响应对数概率在所有评估中都表现更好，尤其是在允许思考的情况下。

Conclusion: 思维帮助主要在与真实人类对话分布匹配的目标训练下有效，将此方法扩展到更广泛的对话数据可能产生对人类行为有更细致理解的模型。

Abstract: To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.

</details>


### [27] [Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models](https://arxiv.org/abs/2601.04448)
*San Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: MB-Defense：一种针对指令调优大语言模型的后门攻击防御框架，通过防御性毒化和权重恢复两阶段训练，有效降低攻击成功率同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 指令调优的大语言模型虽然实现了广泛的任务泛化能力，但依赖大规模数据集使其容易受到后门攻击。目前针对指令调优模型的防御研究不足，需要开发有效的防御机制。

Method: MB-Defense包含两个阶段：1）防御性毒化：将攻击者触发器和防御触发器合并为统一的后门表示；2）权重恢复：通过额外训练打破这种表示，恢复模型的干净行为。

Result: 在多个大语言模型上的广泛实验表明，MB-Defense显著降低了攻击成功率，同时保持了指令跟随能力。该方法提供了通用且数据高效的防御策略。

Conclusion: MB-Defense为指令调优大语言模型提供了一种有效的后门攻击防御方案，能够提高模型对未见后门攻击的鲁棒性，具有通用性和数据效率优势。

Abstract: Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.

</details>


### [28] [Users Mispredict Their Own Preferences for AI Writing Assistance](https://arxiv.org/abs/2601.04461)
*Vivian Lai,Zana Buçinca,Nil-Jana Akpinar,Mo Houtti,Hyeonsu B. Kang,Kevin Chian,Namjoon Suh,Alex C. Williams*

Main category: cs.CL

TL;DR: 用户对AI写作助手帮助偏好的自我报告与实际行为存在显著差异，尤其是紧急性的重要性被高估，导致基于用户自述设计的系统性能不佳。


<details>
  <summary>Details</summary>
Motivation: 主动式AI写作助手需要预测用户何时需要写作帮助，但目前缺乏对用户偏好驱动因素的实证理解，特别是用户自我报告与实际行为之间可能存在的不一致。

Method: 通过因子式情境研究，让50名参与者进行750对配对比较，分析用户对AI写作助手帮助偏好的驱动因素，比较用户自我报告与实际行为模式。

Result: 研究发现：1) 写作努力程度是主要驱动因素(ρ=0.597)，紧急性无预测力(ρ≈0)；2) 用户存在显著的感知-行为差距，自我报告中紧急性排名第一但实际行为中最弱；3) 基于用户自述设计的系统准确率仅57.7%，低于基于行为模式的61.3%(p<0.05)。

Conclusion: 依赖用户内省进行系统设计会误导优化，主动式自然语言生成系统应基于实际行为模式而非用户自我报告来设计。

Abstract: Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.

</details>


### [29] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: ProMem提出主动记忆提取方法，通过自问自答的反馈循环解决传统摘要式记忆提取的两个主要问题：前瞻性不足和一次性提取导致信息丢失。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆管理研究主要关注如何组织和使用记忆摘要，但忽视了初始记忆提取阶段。基于递归处理理论，现有摘要式方法存在两大局限：1) 摘要是"前瞻性"的，作为盲目的"前馈"过程会遗漏重要细节；2) 提取通常是"一次性"的，缺乏验证事实的反馈循环，导致信息损失累积。

Method: 提出ProMem（主动记忆提取），将提取视为迭代认知过程。引入递归反馈循环，代理通过自问自答主动探索对话历史，能够恢复缺失信息和纠正错误。

Result: ProMem显著提高了提取记忆的完整性和问答准确性，在提取质量和token成本之间实现了优越的权衡。

Conclusion: 主动记忆提取方法通过迭代反馈循环解决了传统摘要式记忆提取的局限性，为LLM代理的长时交互和个性化提供了更有效的记忆管理方案。

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [30] [Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions](https://arxiv.org/abs/2601.04465)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: Concept Tokens是一种轻量级方法，通过在预训练LLM中添加特殊token并仅从其定义学习嵌入，来引导冻结模型的行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要微调整个模型或依赖上下文学习，这可能导致计算成本高或指令遵循能力下降。研究者希望找到一种更轻量、更精确的方式来控制LLM对特定概念的理解和行为。

Method: 在预训练LLM中添加新的特殊token，仅从该概念的多个自然语言定义中学习其嵌入表示（LLM参数保持冻结）。训练时用新token替换定义中的概念词，使用标准语言建模目标优化嵌入。

Result: 1) 在HotpotQA闭卷问答中减少幻觉：否定token主要增加弃答，肯定token增加幻觉并降低精度；2) 在二语教学反馈中观察到类似方向性效应；3) 相比上下文提供完整定义，Concept Tokens能更好地保持对其他指令的遵循；4) 定性研究展示了嵌入捕获的信息及其局限性。

Conclusion: Concept Tokens提供了一种从定义学习的紧凑控制信号，能够有效引导冻结LLM的行为，在减少幻觉、教学应用等方面表现出方向性控制能力，且比上下文学习更好地保持指令遵循。

Abstract: We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.

</details>


### [31] [SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers](https://arxiv.org/abs/2601.04469)
*Iaroslav Chelombitko,Ekaterina Chelombitko,Aleksey Komissarov*

Main category: cs.CL

TL;DR: SampoNLP：面向乌拉尔语系形态丰富语言的语料库无关形态词典构建工具包，通过自参考原子性评分生成高质量词典，并基于此系统评估BPE分词器，提出综合性能评分IPS确定最佳词汇量


<details>
  <summary>Details</summary>
Motivation: 乌拉尔语系形态丰富语言缺乏干净的形态词典，阻碍了分词器评估。需要一种在低资源环境下创建高质量形态词典的方法，以系统评估BPE分词器在这些语言上的表现。

Method: 1. 开发SampoNLP工具包，使用MDL启发的自参考原子性评分，通过内部结构线索过滤复合形式，无需语料库即可创建形态词典；2. 为芬兰语、匈牙利语和爱沙尼亚语生成高纯度词典；3. 系统评估不同词汇量（8k-256k）的BPE分词器；4. 提出综合性能评分IPS平衡形态覆盖和过度切分

Result: 1. 成功为三种语言生成高质量形态词典；2. 通过IPS曲线分析发现"拐点"，首次为这些语言提供基于实证的最佳词汇量推荐；3. 定量证明了标准BPE对高度黏着语言的局限性

Conclusion: SampoNLP为低资源形态丰富语言提供了有效的词典构建方案，IPS指标帮助确定BPE分词器的最佳词汇量，揭示了标准BPE在黏着语言上的不足，为相关研究提供了实用指导和资源

Abstract: The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.
  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.
  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP

</details>


### [32] [WESR: Scaling and Evaluating Word-level Event-Speech Recognition](https://arxiv.org/abs/2601.04508)
*Chenchen Yang,Kexin Huang,Liwei Fan,Qian Tu,Botian Jiang,Dong Zhang,Linqi Yin,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 论文提出WESR-Bench，一个用于非语言声音事件检测的评估基准，包含21种声音事件的精细分类、900+专家标注语料，以及位置感知评估协议，解决了现有方法类别覆盖不足和评估标准缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 语音不仅传递语言信息，还包含丰富的非语言声音事件（如笑声、哭声）。虽然语义转录研究较多，但非语言事件的精确定位仍然是一个关键但未被充分探索的挑战。现有方法存在任务定义不足（类别覆盖有限、时间粒度模糊）和缺乏标准化评估框架的问题，阻碍了下游应用的发展。

Method: 1. 开发了包含21种声音事件的精细分类法，分为离散型（独立）和连续型（与语音混合）两类；2. 引入WESR-Bench专家标注评估集（900+语料），采用新的位置感知协议，将ASR错误与事件检测解耦；3. 构建了1700+小时语料库，训练专门模型。

Result: 构建的专门模型在非语言声音事件检测方面超越了开源音频语言模型和商业API，同时保持了ASR质量。WESR-Bench为离散和连续事件提供了精确的定位测量能力。

Conclusion: WESR将作为未来建模丰富真实世界听觉场景研究的基础资源，解决了非语言声音事件检测的关键挑战，为领域提供了标准化的评估框架和强大的基线模型。

Abstract: Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.

</details>


### [33] [LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation](https://arxiv.org/abs/2601.04516)
*Yuxiao Ye,Yiming Zhang,Yiran Ma,Huiyuan Xie,Huining Zhu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出LinguaGame框架，通过博弈论建模多智能体对话，提升语言沟通效率


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统主要关注架构设计（如角色分配、工作流编排），而忽视了交互过程本身。本文旨在通过改进智能体之间的语言沟通效率，帮助它们更有效地传达意图。

Method: 提出LinguaGame框架，将对话建模为基于沟通意图和策略的信号博弈，使用无需训练的均衡近似算法进行推理时决策调整。该框架基于语言学原理，最小化任务特定耦合，将对话视为有意图和策略的沟通过程。

Result: 在模拟法庭程序和辩论场景中评估，通过人类专家评估显示在沟通效率方面取得显著提升。

Conclusion: LinguaGame框架通过博弈论方法有效提升了多智能体系统的语言沟通效率，为对话生成提供了新的语言学基础范式。

Abstract: Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.

</details>


### [34] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: GRACE是一个强化学习框架，通过证据充分性评估、关键证据提取和选择性弃答，统一解决RAG系统中无证据正确回答和证据不足时编造答案两大问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统存在两个关键缺陷：1）在没有明确证据支持的情况下提供正确答案；2）当检索上下文不足时产生编造的响应。现有研究独立处理这些问题，缺乏一个统一的框架来整合基于证据的接地和可靠弃答。

Method: 提出GRACE强化学习框架：1）使用异构检索器生成多样化训练样本的数据构建方法，无需人工标注；2）采用多阶段门控奖励函数训练模型评估证据充分性、提取关键支持证据，并提供答案或明确弃答。

Result: 在两个基准测试上，GRACE实现了最先进的整体准确性，在准确响应和拒绝之间取得了良好平衡，同时仅需要先前方法10%的标注成本。

Conclusion: GRACE提供了一个统一的强化学习框架，能同时解决RAG系统中的两个关键缺陷，通过自动化数据构建和多阶段奖励机制，在保持高性能的同时显著降低了标注成本。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [35] [BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534)
*Amit Bin Tariqul,A N M Zahid Hossain Milkan,Sahab-Al-Chowdhury,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文首次系统评估了孟加拉语LLM文本生成中的水印方法，发现现有方法在跨语言往返翻译攻击下检测准确率崩溃至9-13%，提出分层水印策略将准确率提升25-35%，达到40-50%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在文本生成中的广泛应用，水印技术对于作者归属、知识产权保护和滥用检测变得至关重要。然而现有水印方法在高资源语言中表现良好，但在低资源语言（如孟加拉语）中的鲁棒性尚未得到充分探索。

Method: 1. 系统评估KGW、指数采样(EXP)和Waterfall三种最先进文本水印方法在孟加拉语LLM文本生成中的表现；2. 提出分层水印策略，结合嵌入时水印和生成后水印；3. 在跨语言往返翻译攻击下测试水印鲁棒性。

Result: 在良性条件下，KGW和EXP检测准确率>88%，困惑度和ROUGE指标下降可忽略。但往返翻译攻击使检测准确率崩溃至9-13%。分层水印将攻击后检测准确率提升25-35%，达到40-50%，相对单层方法提升3-4倍，代价是可控的语义质量下降。

Conclusion: 分层水印是低资源语言（如孟加拉语）中实用、无需训练的水印解决方案，量化了多语言水印中鲁棒性与质量之间的权衡关系，为低资源语言水印提供了有效方法。

Abstract: As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.

</details>


### [36] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: NeuronLLM：基于功能拮抗原理的任务级LLM理解框架，通过对比学习识别促进和抑制任务完成的神经元，解决现有方法只关注支持性神经元、忽略抑制性角色和偶然行为的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM神经元识别方法存在三个主要问题：1）针对特定能力的方法不适用于需要多种能力协调的任务场景；2）只关注与任务正相关的支持性神经元，忽略抑制性神经元；3）LLM的偶然正确行为会导致神经元归因错误。需要一种更全面的任务级理解框架。

Method: NeuronLLM采用生物学功能拮抗原理，认为任务表现由两类对立神经元共同决定：促进任务完成的"好神经元"和抑制任务完成的"坏神经元"。通过对比学习同时建模这两类神经元，并利用增强问题集来缓解LLM的偶然行为。

Result: 在不同规模和家族的LLM上进行的综合实验表明，NeuronLLM在四个NLP任务上优于现有方法，为LLM的功能组织提供了新见解。

Conclusion: NeuronLLM通过功能拮抗原理和对比学习，实现了对LLM神经元的全面建模，不仅识别支持性神经元，还能发现抑制性神经元，有效缓解偶然行为带来的归因错误，为理解和引导LLM提供了更可靠的框架。

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [37] [FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback](https://arxiv.org/abs/2601.04574)
*Seongyeub Chu,Jongwoo Kim,Munyong Yi*

Main category: cs.CL

TL;DR: 本文提出FeedEval框架，用于评估LLM生成的作文反馈质量，通过三个教学维度（具体性、帮助性、有效性）筛选高质量反馈，提升作文评分模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动作文评分研究不仅关注分数预测，更强调生成高质量反馈。但现有方法常使用未经质量验证的LLM生成反馈，导致噪声传播到下游应用。需要一种系统方法来评估和筛选高质量的LLM生成反馈。

Method: 提出FeedEval框架，包含三个维度专业化的LLM评估器（具体性、帮助性、有效性），使用本研究构建的数据集训练，评估多个反馈候选并选择高质量反馈用于下游任务。

Result: 在ASAP++基准测试中，FeedEval与人类专家判断高度一致；使用FeedEval筛选的高质量反馈训练的作文评分模型获得更优的评分性能；小型LLM的修订实验显示，FeedEval识别的高质量反馈能带来更有效的作文修订。

Conclusion: FeedEval框架能有效评估LLM生成作文反馈的质量，筛选高质量反馈可提升下游作文评分和修订任务性能，为自动化作文评估提供了可靠的反馈质量控制方法。

Abstract: Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.

</details>


### [38] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: RL-Text2Vis是首个基于强化学习的文本到可视化生成框架，使用GRPO和多目标奖励机制，在图表质量上比GPT-4o提升22%，代码执行成功率从78%提升到97%。


<details>
  <summary>Details</summary>
Motivation: 当前Text2Vis系统存在两个主要问题：闭源LLM生成的图表语义对齐和清晰度不足，开源模型则经常产生不可执行或视觉效果差的输出。传统的监督微调虽然能提升代码可执行性，但无法通过执行后反馈来改善可视化质量。

Method: 提出RL-Text2Vis框架，基于Group Relative Policy Optimization (GRPO)，采用新颖的多目标奖励机制，联合优化文本准确性、代码有效性和可视化质量，利用执行后反馈进行训练。

Result: 在Text2Vis基准测试中，图表质量相比GPT-4o相对提升22%，代码执行成功率从零样本基线的78%提升到97%。在Qwen2.5模型（7B和14B）上训练，显著优于零样本和监督基线，并在VIS-Eval和NVBench等域外数据集上表现出良好的泛化能力。

Conclusion: GRPO被证明是可视化生成中结构化多模态推理的有效策略，RL-Text2Vis框架能显著提升文本到可视化系统的性能，特别是在图表质量和代码可执行性方面。

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [39] [THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report](https://arxiv.org/abs/2601.04597)
*KBTG Labs,:,Anuruth Lertpiya,Danupat Khamnuansin,Kantapong Sucharitpongpan,Pornchanan Balee,Tawunrat Chalothorn,Thadpong Pongthawornkamol,Monchai Lertsutthiwong*

Main category: cs.CL

TL;DR: 通过模型合并技术，将Qwen-8B与泰国专用模型ThaiLLM-8B和金融模型THaLLE-CFA-8B合并，创建具备多领域能力的高性能泰语大语言模型，解决单一模型训练成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: 在银行和金融领域，大语言模型能自动化复杂任务并提升决策效率，但由于隐私、安全和监管要求，企业偏好本地部署。泰国LLM计划旨在提升开源LLM的泰语能力，但企业面临部署多个专业模型与训练单一多功能模型成本过高之间的权衡。

Method: 采用模型合并作为资源高效的替代方案。进行了两个关键实验：1) 合并Qwen-8B与ThaiLLM-8B；2) 合并Qwen-8B与ThaiLLM-8B和THaLLE-CFA-8B，创建具备通用和金融领域能力的多功能模型。

Result: 实验一显示ThaiLLM-8B显著提升了Qwen-8B的泰语通用能力，在M3和M6 O-NET考试中表现更好。实验二进一步提升了通用和金融领域的性能，在M3和M6 O-NET、Flare-CFA和Thai-IC基准测试中均有提升。

Conclusion: 模型合并是高效创建多功能大语言模型的可行方法，能够平衡性能与成本，为泰国企业提供本地化、多能力的语言模型解决方案。

Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.

</details>


### [40] [On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions](https://arxiv.org/abs/2601.04600)
*Zhiyuan He,Binghan Chen,Tianxiang Xiong,Ziyang Sun,Mozhao Zhu,Xi Chen*

Main category: cs.CL

TL;DR: ROME知识编辑方法在单跳事实更新上高效，但在多跳推理任务中存在"跳跃过晚"、泛化能力下降和过拟合问题，提出冗余编辑策略改善多跳推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法（特别是ROME）在单跳事实更新上表现出色，但在需要知识链的多跳推理任务中面临显著挑战，需要解决这些局限性。

Method: 通过分析ROME在不同层深度编辑的效果，识别出三个关键失败模式，并提出冗余编辑策略来缓解"跳跃过晚"和泛化衰减问题。

Result: 冗余编辑方法可将2跳问题的准确率提高至少15.5个百分点，相比之前的单编辑策略提升96%，但需要在特异性和语言自然性上做出权衡。

Conclusion: 冗余编辑策略能有效改善知识编辑方法在多跳推理任务中的性能，为解决知识编辑在多跳推理中的局限性提供了有前景的方向。

Abstract: Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.

</details>


### [41] [When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation](https://arxiv.org/abs/2601.04609)
*Rhea Kapur,Robert Hawkins,Elisa Kreiss*

Main category: cs.CL

TL;DR: 论文提出将描述特异性与长度解耦，构建了控制长度但变化信息内容的数据集，验证了人们偏好特异性而非冗长，支持评估方法应优先考虑特异性而非冗长。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型生成的描述中，特异性常与长度混淆。作者认为这两个概念需要解耦：描述可以简洁但信息密集，也可以冗长但空洞。需要建立更准确的评估框架来区分描述的信息质量。

Method: 1) 定义特异性相对于对比集：描述越能区分目标图像与其他可能图像，特异性越高；2) 构建控制长度但变化信息内容的数据集；3) 通过人类偏好实验验证特异性与长度的关系；4) 分析长度预算分配对特异性的影响。

Result: 1) 人类偏好实验显示，无论长度如何，人们都更偏好特异性高的描述；2) 仅控制长度不能解释特异性差异，长度预算的分配方式也很重要；3) 支持直接优先考虑特异性而非冗长的评估方法。

Conclusion: 描述特异性应与长度解耦，评估视觉语言模型生成的描述时应优先考虑特异性而非冗长。仅控制长度不足以保证描述质量，需要更精细的评估方法关注信息内容的有效性。

Abstract: Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.

</details>


### [42] [Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR](https://arxiv.org/abs/2601.04611)
*Yihong Tang,Kehai Chen,Xuefeng Bai,Benyou Wang,Zeming Liu,Haifeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: Character-R1框架通过认知聚焦奖励、参考引导奖励和角色条件奖励归一化，为角色扮演代理提供可验证的奖励信号，解决现有方法在复杂情境中角色不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 当前角色扮演代理通常模仿表面行为，缺乏内部认知一致性，在复杂情境中容易出现角色不符的错误。现有研究缺乏有效的角色感知推理奖励信号。

Method: 提出Character-R1框架，包含三个核心设计：1) 认知聚焦奖励：基于10个角色元素（如世界观）进行显式标签分析，结构化内部认知；2) 参考引导奖励：使用基于重叠度的指标，以参考响应作为优化锚点，增强探索和性能；3) 角色条件奖励归一化：根据角色类别调整奖励分布，确保在异质角色间的稳健优化。

Result: 大量实验表明，Character-R1在知识、记忆等多个方面显著优于现有方法。

Conclusion: Character-R1通过提供全面的可验证奖励信号，有效解决了角色扮演代理的认知一致性问题，提升了在复杂情境中的角色表现。

Abstract: Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.

</details>


### [43] [From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset](https://arxiv.org/abs/2601.04632)
*Haneul Yoo,Won Ik Cho,Geunhye Kim,Jiyoon Han*

Main category: cs.CL

TL;DR: 本文提出CuCu框架，利用国家社会课程自动生成文化特定的开放问答对，构建韩国文化问答数据集KCaQA，以解决LLM文化对齐问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多任务上表现优异，但其进步在不同语言和文化间不均衡，通常反映英语中心训练数据中隐含的价值观。需要实用的文化对齐方法来解决这一问题

Method: 提出CuCu框架：一个自动化的多智能体LLM框架，将国家教科书课程转化为开放式的、文化特定的问答对。应用于韩国国家社会课程，构建KCaQA数据集

Result: 构建了包含34.1k个开放式问答对的KCaQA数据集。定量和定性分析表明，KCaQA覆盖文化特定主题，并产生基于当地社会文化背景的响应

Conclusion: 利用国家社会课程作为文化感知监督的基础，提供了一种可扩展的文化对齐方法，有助于解决LLM在不同文化间的表现不均衡问题

Abstract: Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.

</details>


### [44] [MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark](https://arxiv.org/abs/2601.04633)
*Anyang Song,Ying Cheng,Yiqian Xu,Rui Feng*

Main category: cs.CL

TL;DR: 提出MAGA方法，通过增强生成文本的对齐性来提升检测器的泛化能力，同时测试现有检测器的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 机器生成文本越来越难以与人类写作区分，加剧了假新闻和网络欺诈等滥用问题。现有微调检测器的泛化能力高度依赖数据集质量，仅扩大机器生成文本来源不足，需要进一步增强生成过程

Method: 提出MAGA方法，通过从提示构建到推理过程的全面对齐，其中关键组件是系统提出的强化学习从检测器反馈（RLDF）

Result: 在RoBERTa检测器上，使用MAGA训练集微调后，泛化检测AUC平均提升4.60%。MAGA数据集导致所选检测器的AUC平均下降8.13%

Conclusion: MAGA方法通过增强生成文本的对齐性，既能攻击现有检测器测试其鲁棒性，又能提升检测器的泛化能力，为未来检测器泛化检测能力研究提供指示意义

Abstract: Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.

</details>


### [45] [SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation](https://arxiv.org/abs/2601.04638)
*Sirry Chen,Jieyi Wang,Wei Chen,Zhongyu Wei*

Main category: cs.CL

TL;DR: SpeechMedAssist：一种通过两阶段训练范式（文本知识注入+有限语音数据模态对齐）实现语音医疗咨询的SpeechLM模型，仅需1万合成语音样本即可有效工作


<details>
  <summary>Details</summary>
Motivation: 医疗咨询本质上是语音中心的，但现有研究多基于长文本交互，不够自然且对患者不友好。虽然语音语言模型（SpeechLMs）能实现更自然的语音交互，但医疗语音数据稀缺和直接微调效率低下阻碍了SpeechLMs在医疗咨询中的应用

Method: 提出SpeechMedAssist，利用SpeechLMs的架构特性，将传统单阶段训练解耦为两阶段范式：1）通过文本进行知识和能力注入；2）用有限语音数据进行模态重新对齐。仅需1万个合成语音样本，大大降低了对医疗语音数据的需求

Result: 实验结果表明，该模型在单轮问答和多轮模拟交互的医疗咨询场景中，在大多数评估设置下都优于所有基线模型，在有效性和鲁棒性方面表现更佳

Conclusion: SpeechMedAssist通过创新的两阶段训练范式，成功解决了医疗语音数据稀缺问题，使SpeechLMs能够有效应用于医疗咨询场景，为语音驱动的医疗助手提供了可行的解决方案

Abstract: Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.

</details>


### [46] [CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2601.04664)
*Yifan Le,Yunliang Li*

Main category: cs.CL

TL;DR: CRANE：基于相关性的神经元分析框架，通过神经元级干预重新定义语言特异性，比基于激活的方法更精确地识别语言特定神经元


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在不同语言上表现强劲，但神经元层面的语言能力组织方式仍不清楚。现有基于激活的启发式方法混淆了语言偏好与功能重要性，需要更精确的分析方法。

Method: 提出CRANE框架，通过有针对性的神经元级干预来识别语言特定神经元，基于神经元对语言条件预测的贡献而非激活幅度来表征神经元专业化

Result: 神经元级干预揭示了一致的不对称模式：掩蔽目标语言相关神经元会选择性降低该语言性能，同时很大程度上保留其他语言性能，表明神经元具有语言选择性但非排他性专业化。在英语、中文和越南语的多个基准测试中，CRANE比基于激活的方法更精确地分离语言特定组件

Conclusion: CRANE框架通过功能必要性重新定义语言特异性，提供了更精确的神经元级语言能力分析工具，揭示了多语言LLM中语言选择性但非排他性的神经元专业化模式

Abstract: Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.

</details>


### [47] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: ToolGate：一个为LLM工具调用提供逻辑安全保证和可验证状态演化的前向执行框架，通过形式化工具合约和运行时验证确保状态演化的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具调用框架主要依赖自然语言推理来确定何时调用工具以及是否提交结果，缺乏逻辑安全性和可验证性的形式化保证。需要建立一个能够提供可靠安全保证和可调试性的框架。

Method: ToolGate维护一个显式的符号状态空间作为类型化的键值映射，表示可信的世界信息。每个工具被形式化为Hoare风格的合约，包含前置条件和后置条件。前置条件通过检查当前状态是否满足要求来门控工具调用，后置条件通过运行时验证确定工具结果是否可以提交来更新状态。

Result: 实验验证表明，ToolGate显著提高了工具增强LLM系统的可靠性和可验证性，同时在复杂的多步推理任务上保持了有竞争力的性能。

Conclusion: ToolGate为构建更可信赖和可调试的AI系统奠定了基础，这些系统将语言模型与外部工具集成，通过形式化验证确保逻辑安全性和状态演化的可靠性。

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [48] [See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation](https://arxiv.org/abs/2601.04692)
*Naquee Rizwan,Subhankar Swain,Paramananda Bhaskar,Gagan Aryan,Shehryaar Shah Khan,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 提出基于生成式AI的多模态框架，在有限数据条件下实现仇恨表情包检测、解释和干预三合一


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将仇恨表情包的检测、解释和干预分开研究，不符合实际应用场景；同时构建大规模标注数据集成本过高，需要能在有限数据条件下工作的通用解决方案

Method: 利用任务特定的生成式多模态智能体和大型多模态模型的少样本适应能力，构建统一框架处理不同类型表情包

Result: 这是首个专注于有限数据条件下通用仇恨表情包内容审核的工作，具有在实际生产场景中部署的潜力

Conclusion: 提出的生成式AI框架能够同时处理仇恨表情包的检测、解释和干预，解决了现有研究的分离问题，并在数据有限条件下展现出实际应用价值

Abstract: In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.

</details>


### [49] [Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding](https://arxiv.org/abs/2601.04693)
*Sungmok Jung,Yeonkyoung So,Joonhak Lee,Sangho Kim,Yelim Ahn,Jaejin Lee*

Main category: cs.CL

TL;DR: 论文提出了针对韩语否定理解的基准测试Thunder-KoNUBench，评估了47个LLM，发现模型在否定理解上表现不佳，但通过在该基准上微调可以提升韩语否定理解和更广泛的语境理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管否定对大型语言模型具有挑战性，但评估否定理解的基准测试（特别是针对韩语）非常稀缺。作者通过语料库分析发现LLM在否定理解上表现下降，因此需要建立一个反映韩语否定现象经验分布的基准测试。

Method: 1) 进行韩语否定的语料库分析；2) 开发Thunder-KoNUBench基准测试，这是一个句子级基准，反映韩语否定现象的经验分布；3) 评估47个LLM，分析模型规模和指令微调的影响；4) 在Thunder-KoNUBench上进行微调实验。

Result: LLM在韩语否定理解上表现不佳，性能下降。通过Thunder-KoNUBench微调可以显著改善韩语否定理解能力，同时还能提升更广泛的韩语语境理解能力。模型规模和指令微调对否定理解有影响。

Conclusion: Thunder-KoNUBench填补了韩语否定理解基准测试的空白，证明了LLM在韩语否定理解上的局限性，同时展示了通过针对性微调可以有效提升模型在韩语否定和语境理解方面的能力。

Abstract: Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

</details>


### [50] [PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards](https://arxiv.org/abs/2601.04700)
*Mukesh Ghimire,Aosong Feng,Liwen You,Youzhi Luo,Fang Liu,Xuan Zhu*

Main category: cs.CL

TL;DR: PRISM：一种结合过程奖励模型和模型内部置信度的无监督训练框架，用于提升LLM在数学推理和代码生成等任务上的性能，解决现有内部一致性信号不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的后训练方法依赖昂贵的人工监督或外部验证器，但随着LLM解决问题能力的提升，进一步改进需要难以获得的高质量解决方案。无监督学习变得日益重要，但现有的基于模型内部一致性（如熵或自确定性）的方法在大规模和长期训练中不可靠。

Method: 提出PRISM统一训练框架，使用过程奖励模型（PRM）在缺乏真实标签的情况下指导学习，并结合模型内部置信度。有效结合PRM和自确定性可以实现稳定训练和更好的测试性能。

Result: PRISM框架能够实现稳定训练和更好的测试时性能，同时保持模型内部置信度的可控性。

Conclusion: 通过结合过程奖励模型和模型内部置信度，PRISM解决了无监督学习中内部一致性信号不可靠的问题，为LLM在数学推理和代码生成等任务上的持续改进提供了有效途径。

Abstract: Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.

</details>


### [51] [Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.04710)
*Feihu Jin,Shipeng Cen,Ying Tan*

Main category: cs.CL

TL;DR: 提出一种基于先验信息扰动的零阶优化方法，通过动态计算引导向量改进梯度估计，显著加速收敛并提升性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调中反向传播的内存开销巨大，零阶优化虽能避免反向传播但存在梯度估计方差高、收敛慢的问题

Method: 提出即插即用方法，引入先验信息扰动改进梯度估计，动态计算引导向量指导扰动方向，并研究贪婪扰动策略

Result: 在多种规模和架构的LLM上验证，方法能无缝集成现有优化器，实现更快收敛和更优性能，在OPT-13B模型上超越传统零阶优化和梯度基线

Conclusion: 提出的先验信息扰动方法显著提升零阶优化的效率和准确性，为大规模模型微调提供了内存高效且性能优越的解决方案

Abstract: Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.

</details>


### [52] [DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs](https://arxiv.org/abs/2601.04711)
*Anh Thi-Hoang Nguyen,Khanh Quoc Tran,Tin Van Huynh,Phuoc Tan-Hoang Nguyen,Cam Tan Nguyen,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: DSC2025 ViHallu Challenge是首个针对越南语大语言模型幻觉检测的大规模共享任务，发布了包含10,000个标注样本的ViHallu数据集，111个团队参与，最佳系统达到84.80%的宏F1分数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生产环境中的可靠性受到幻觉问题的严重制约，而越南语等中低资源语言缺乏标准化的幻觉检测评估框架，需要建立专门的基准来推动越南语AI系统的可信度研究。

Method: 构建ViHallu数据集，包含10,000个标注的（上下文、提示、响应）三元组，分为无幻觉、内在幻觉和外在幻觉三类，采用事实性、噪声和对抗性三种提示类型。组织大规模共享任务，邀请团队开发检测方法。

Result: 111个团队参与，最佳系统达到84.80%的宏F1分数，相比基线编码器模型的32.83%有显著提升。指令调优的大语言模型结合结构化提示和集成策略表现最佳，但内在幻觉检测仍然最具挑战性。

Conclusion: 越南语幻觉检测仍然是一个具有挑战性的问题，特别是内在幻觉检测。这项工作为越南语AI系统的可信度和可靠性研究奠定了基础，建立了首个严格的基准并探索了多种检测方法。

Abstract: The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations -- fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types -- factual, noisy, and adversarial -- to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.

</details>


### [53] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: 论文提出"角色身份"概念，将角色分为参数身份（预训练知识）和属性身份（行为特性），发现名人角色初始优势会快速消失，而负面社会属性是角色扮演代理保真度的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的角色扮演代理缺乏对角色身份的结构化定义，通常将角色视为任意文本输入。需要系统性地理解角色身份的不同维度及其对角色扮演质量的影响。

Method: 提出角色身份的多维结构概念，分为参数身份和属性身份。构建统一的角色档案模式，在相同结构约束下生成名人和合成角色。通过单轮和多轮交互评估这些层次。

Result: 发现两个关键现象：1)"名气消退"：名人角色在初始回合因参数知识有优势，但随着对话上下文积累，这种优势迅速消失；2)"本性难移"：模型能稳健地表现一般人格特质，但角色扮演代理性能对道德和人际关系的效价高度敏感。

Conclusion: 负面社会属性是角色扮演代理保真度的主要瓶颈，为未来角色构建和评估提供了指导方向。角色身份的结构化分析有助于提升角色扮演质量。

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [54] [Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking](https://arxiv.org/abs/2601.04720)
*Mingxin Li,Yanzhao Zhang,Dingkun Long,Keqin Chen,Sibo Song,Shuai Bai,Zhibo Yang,Pengjun Xie,An Yang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-VL-Embedding和Qwen3-VL-Reranker是基于Qwen3-VL的多模态检索模型系列，支持文本、图像、文档图像和视频的统一表示，在MMEB-V2等基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个端到端的高精度多模态搜索管道，将不同模态（文本、图像、文档图像、视频）映射到统一的表示空间，支持多语言和多尺寸部署。

Method: 采用多阶段训练范式：大规模对比预训练 → 重排序模型蒸馏。支持Matryoshka表示学习实现灵活嵌入维度，处理32k token输入。重排序模型使用交叉编码器架构和交叉注意力机制进行细粒度相关性估计。

Result: Qwen3-VL-Embedding-8B在MMEB-V2基准上获得77.8分，排名第一（截至2025年1月8日）。模型系列在多模态嵌入评估基准上达到最先进水平，支持30多种语言，提供2B和8B参数版本。

Conclusion: Qwen3-VL-Embedding和Qwen3-VL-Reranker系列为多模态检索任务提供了有效的解决方案，在图像-文本检索、视觉问答、视频-文本匹配等任务上表现出色，具有实际部署价值。

Abstract: In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.

</details>


### [55] [Automatic Classifiers Underdetect Emotions Expressed by Men](https://arxiv.org/abs/2601.04730)
*Ivan Smirnov,Segun T. Aroyehun,Paul Plener,David Garcia*

Main category: cs.CL

TL;DR: 研究发现情感分析模型在男性和女性作者文本上存在系统性偏见，男性作者文本的错误率普遍更高，表明当前机器学习工具在性别公平性方面仍需改进。


<details>
  <summary>Details</summary>
Motivation: 自动情感和情绪分类器广泛使用，但其可靠性评估通常依赖第三方标注而非情绪体验者本人，可能掩盖系统性偏见。需要研究这些工具在不同人群中的表现差异。

Method: 使用超过100万条自我标注的帖子数据集，采用预先注册的研究设计，分析414种模型和情绪类别的组合，评估性别偏见在情感检测中的表现。

Result: 研究发现，在不同类型的自动分类器和各种基础情绪中，男性作者文本的错误率始终高于女性作者文本。这种偏见可能影响下游应用结果。

Conclusion: 情感分析尚未完全解决，特别是在确保跨人口群体的公平模型行为方面。当样本的性别构成未知或变化时，包括大型语言模型在内的当前机器学习工具应谨慎使用。

Abstract: The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.

</details>


### [56] [AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs](https://arxiv.org/abs/2601.04736)
*Han Zhu,Jiale Chen,Chengkun Cai,Shengjie Sun,Haoran Li,Yujin Zhou,Chi-Min Chan,Pengcheng Wen,Lei Li,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 提出InterSafe-V多模态对话安全数据集和AM³Safety框架，通过冷启动拒绝阶段和基于对话轮次的GRPO微调，显著降低多轮多模态对话中的攻击成功率，同时提升模型的安全性和帮助性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在交互应用中部署增多，但在多轮多模态场景下安全漏洞显著：有害意图可在多轮对话中逐步重构，安全协议随对话进展被遗忘。现有RLHF对齐方法主要针对单轮VQA任务，需要昂贵的人工偏好标注，限制了其在对话场景中的有效性和可扩展性。

Method: 1) 构建InterSafe-V开源多模态对话数据集，包含11,270个对话和500个专门设计的拒绝VQA样本，通过模型间交互构建以更准确反映真实场景；2) 提出AM³Safety框架，结合冷启动拒绝阶段和基于对话轮次的双目标奖励的Group Relative Policy Optimization微调。

Result: 在Qwen2.5-VL-7B-Instruct和LLaVA-NeXT-7B模型上，攻击成功率降低超过10%，无害性维度提升至少8%，帮助性维度提升超过13%，同时保持模型的通用能力。

Conclusion: 提出的InterSafe-V数据集和AM³Safety框架有效解决了多轮多模态对话中的安全问题，显著提升了模型的安全性和帮助性，为多模态对话安全对齐提供了实用解决方案。

Abstract: Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.

</details>


### [57] [RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation](https://arxiv.org/abs/2601.04740)
*Huawei Zheng,Xinqi Jiang,Sen Yang,Shouling Ji,Yingcai Wu,Dazhen Deng*

Main category: cs.CL

TL;DR: 提出一个端到端框架，通过知识图谱引导生成领域相关有害提示，再通过双重路径混淆改写将显式有害提示转换为隐式变体，以解决专业领域LLM安全测试中隐式有害提示数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在金融、医疗等专业领域的应用引入了独特的安全风险，但领域特定的有害提示数据集稀缺且主要依赖人工构建。现有公共数据集主要关注显式有害提示，而现代LLM防御系统通常能检测并拒绝这些提示。相比之下，通过间接领域知识表达的隐式有害提示更难检测，更能反映现实世界威胁。

Method: 提出端到端框架：1) 知识图谱引导的有害提示生成 - 系统化生成领域相关提示；2) 双重路径混淆改写 - 通过直接改写和上下文增强改写将显式有害提示转换为隐式变体。

Result: 该框架能够生成高质量数据集，结合了强大的领域相关性和隐式性，支持更现实的红队测试并推进LLM安全研究。代码和数据集已在GitHub发布。

Conclusion: 提出的框架解决了将领域知识转化为可操作约束和增加生成有害提示隐式性的两大挑战，为专业领域LLM安全测试提供了更有效的工具和数据集。

Abstract: Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.

</details>


### [58] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: Tool-MAD：一个多智能体辩论框架，通过为每个智能体分配不同的外部工具（如搜索API或RAG模块）来增强事实核查能力，采用自适应查询机制和量化评估方法，在事实核查基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在幻觉和事实不准确问题，特别是在复杂推理和事实核查任务中。现有的多智能体辩论框架主要依赖内部知识或静态文档，容易产生幻觉。虽然MADKE引入了外部证据，但其一次性检索机制限制了在辩论过程中对新论点或新信息的适应性。

Method: 1) 多智能体辩论框架：每个智能体使用异构的外部工具（如搜索API、RAG模块），鼓励多样化视角；2) 自适应查询机制：基于辩论流程迭代优化证据检索；3) 集成忠实度和答案相关性评分：让法官智能体能够定量评估每个响应的连贯性和问题对齐度，有效检测幻觉。

Result: 在四个事实核查基准测试中，Tool-MAD始终优于最先进的多智能体辩论框架，实现了高达5.5%的准确率提升。在医学专业领域，Tool-MAD在各种工具配置和领域条件下表现出强大的鲁棒性和适应性。

Conclusion: Tool-MAD通过整合异构外部工具、自适应检索机制和量化评估方法，有效解决了多智能体辩论中的幻觉问题，在事实核查任务中表现出优越性能，具有广泛的现实世界应用潜力。

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [59] [PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks](https://arxiv.org/abs/2601.04758)
*Yehoon Jang,Chaewon Lee,Hyun-seok Min,Sungchul Choi*

Main category: cs.CL

TL;DR: PILOT-Bench是首个针对PTAB专利上诉决策的基准测试，包含三个IRAC对齐的分类任务，用于评估LLMs在专利领域法律推理能力，发现闭源模型显著优于开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在专利和法律实践中的应用仅限于轻量级任务，缺乏系统评估其在专利领域结构化法律推理能力的方法。PTAB每年处理大量涉及技术和法律融合的上诉案件，需要建立专门的评估基准。

Method: 构建PILOT-Bench基准，将PTAB决策与USPTO专利数据在案例层面对齐，并形式化三个IRAC对齐的分类任务：问题类型、委员会权威依据和子决策。评估了多种闭源和开源LLMs，从输入变化设置、模型家族和错误倾向等多个角度进行分析。

Result: 在问题类型任务上，闭源模型的Micro-F1分数持续超过0.75，而最强的开源模型（Qwen-8B）性能约为0.56，显示出显著的推理能力差距。闭源模型在专利领域法律推理方面表现明显更优。

Conclusion: PILOT-Bench为系统评估专利领域法律推理能力奠定了基础，指出了通过数据集设计和模型对齐改进LLMs的未来方向。所有数据、代码和基准资源均已开源。

Abstract: The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.

</details>


### [60] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: 研究发现LLM内部表征中语法和语义信息是线性编码的，通过计算句子表征的语法和语义"质心"可以显著影响句子相似度，且这两种信息在不同层有不同编码模式


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（特别是DeepSeek-V3）内部表征如何编码语法和语义信息，探索这些信息是否以线性方式编码，以及语法和语义信息在模型中的编码差异

Method: 通过平均具有相同语法结构或语义的句子的隐藏表征向量，获得语法和语义"质心"向量，然后从句子向量中减去这些质心来分析对相似度的影响

Result: 语法和语义信息在LLM表征中至少部分线性编码，减去语法/语义质心会显著影响与语法/语义匹配句子的相似度；语法和语义在不同层的编码模式不同，且这两种信号可以在一定程度上解耦

Conclusion: 大型语言模型的内部表征中，语法和语义信息以不同的方式编码，且都至少部分采用线性编码方式，这表明模型对这两种语言信息有差异化的处理机制

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [61] [Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence](https://arxiv.org/abs/2601.04766)
*Shengyin Sun,Yiming Li,Renxi Liu,Weizhe Lin,Hui-Ling Zhen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 提出了一种基于KL散度的无训练验证机制，替代需要昂贵监督训练的Judge Decoding，在推理和编码任务中表现优异


<details>
  <summary>Details</summary>
Motivation: Judge Decoding虽然能加速LLM推理，但通常依赖昂贵且嘈杂的监督训练。本文从第一性原理出发，发现监督学习得到的"关键性"评分本质上已编码在草稿-目标分布差异中

Method: 通过理论证明学习到的线性判断器与KL散度之间的结构对应关系，提出基于KL散度的简单无训练验证机制，无需监督训练

Result: 在推理和编码基准测试中，该方法匹配或优于复杂训练判断器（如AutoJudge），对领域转移具有更强的鲁棒性，完全消除了监督瓶颈

Conclusion: 基于KL散度的无训练验证机制为Judge Decoding提供了一种简单有效的替代方案，无需昂贵监督训练，具有更好的鲁棒性和实用性

Abstract: Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.

</details>


### [62] [LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal](https://arxiv.org/abs/2601.04768)
*Dongjun Kim,Jeongho Yoon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 提出LANGSAE EDITING方法，通过稀疏自编码器去除多语言嵌入中的语言身份信号，提升跨语言检索质量


<details>
  <summary>Details</summary>
Motivation: 多语言检索中，嵌入向量同时编码语义和语言身份信息，导致同语言对相似度虚高，挤占了其他语言相关证据的空间

Method: 使用池化嵌入训练后处理稀疏自编码器，通过跨语言激活统计识别语言相关潜在单元，在推理时抑制这些单元，并在原始维度重建嵌入

Result: 在多种语言上实验显示排序质量和跨语言覆盖度一致提升，对文字差异大的语言效果尤其显著

Conclusion: 该方法无需重新训练基础编码器或重新编码原始文本，即可与现有向量数据库兼容，有效去除语言身份信号，改善多语言检索性能

Abstract: Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.

</details>


### [63] [NC2C: Automated Convexification of Generic Non-Convex Optimization Problems](https://arxiv.org/abs/2601.04789)
*Xinyue Peng,Yanming Liu,Yihan Cang,Yuwei Zhang,Xinyi Wang,Songhang Deng,Jiannan Cao*

Main category: cs.CL

TL;DR: NC2C是一个基于大语言模型的端到端自动化框架，能够将通用的非凸优化问题转化为可解的凸形式，显著减少对专家知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 非凸优化问题在数学规划、工程设计和科学计算中普遍存在，传统求解器因其复杂的目标函数和约束条件而面临困难。手动凸化效率低下且过度依赖专家知识，需要自动化解决方案。

Method: NC2C利用LLM的数学推理能力，自主检测非凸成分，选择最优凸化策略，并生成严格的凸等价形式。框架集成了符号推理、自适应变换技术和迭代验证，配备错误校正循环和可行域校正机制以确保鲁棒性。

Result: 在100个通用非凸问题的多样化数据集上，NC2C实现了89.3%的执行率和76%的成功率，能够产生可行、高质量的凸变换，显著优于基线方法。

Conclusion: NC2C能够有效利用LLM实现非凸到凸的自动化转换，减少专家依赖，使凸求解器能够高效处理先前难以解决的优化任务。

Abstract: Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

</details>


### [64] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: 本文首次系统分析了多智能体系统中基于角色的权威偏见，发现专家型和参照型权威角色比法定型权威角色更具影响力，权威偏见源于权威角色坚持立场而普通智能体灵活调整，且需要明确立场声明才能产生偏见。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统常使用权威角色提升性能，但权威偏见对智能体交互的影响尚未得到充分研究。本文旨在填补这一空白，系统分析角色型权威偏见在自由形式多智能体评估中的影响。

Method: 采用French和Raven的权力理论，将权威角色分为法定型、参照型和专家型三类。使用ChatEval平台，在12轮对话中分析这些权威角色的影响力。实验使用GPT-4o和DeepSeek R1模型进行。

Result: 实验显示：1) 专家型和参照型权威角色比法定型权威角色更具影响力；2) 权威偏见不是通过普通智能体的主动顺从产生，而是权威角色坚持立场而普通智能体表现出灵活性；3) 权威影响力需要明确的立场声明，中性回应无法产生偏见。

Conclusion: 研究结果为设计具有不对称交互模式的多智能体框架提供了关键见解，揭示了权威偏见的形成机制和影响因素，有助于更有效地设计多智能体系统中的角色分配和交互模式。

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [65] [When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection](https://arxiv.org/abs/2601.04833)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 该论文发现AI生成文本存在"晚期波动衰减"现象，并提出基于晚期统计特征的检测方法，在基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本AI文本检测方法通常在整个序列上聚合词元级统计信息，忽略了自回归生成过程中的时间动态特性。作者旨在探索AI生成文本与人类写作在时间维度上的差异。

Method: 分析了超过12万个文本样本，发现AI生成文本存在"晚期波动衰减"现象：随着生成过程推进，对数概率波动迅速稳定，而人类写作保持较高变异性。基于此提出两个简单特征：导数离散度和局部波动率，仅使用晚期统计信息计算。

Result: 在序列后半段，AI生成文本的波动性比人类写作低24-32%。提出的方法无需扰动采样或额外模型访问，在EvoBench和MAGE基准测试中达到最先进性能，并与现有全局方法有强互补性。

Conclusion: 时间动态特性是区分AI生成文本与人类写作的重要线索，晚期波动衰减现象为开发更有效的检测方法提供了新视角。仅使用晚期统计特征的简单方法就能实现优异性能。

Abstract: Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

</details>


### [66] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: RAAR是一个检索增强的代理推理框架，用于跨领域虚假信息检测，通过多视角证据检索和多智能体协作推理，显著提升了跨领域检测能力。


<details>
  <summary>Details</summary>
Motivation: 跨领域虚假信息检测面临挑战，因为虚假信息在不同领域出现，知识和话语存在显著差异。现有方法通常依赖单一视角线索，难以泛化到具有挑战性或代表性不足的领域，而推理大语言模型虽然对复杂任务有效，但仅限于同分布数据。

Method: RAAR框架包含：1) 检索与目标样本语义、情感和写作风格对齐的多视角源领域证据；2) 通过专门的多智能体协作构建可验证的多步推理路径，其中特定视角智能体产生互补分析，总结智能体在验证器指导下整合它们；3) 应用监督微调和强化学习训练单一多任务验证器以增强验证和推理能力。

Result: 基于RAAR训练了RAAR-8b和RAAR-14b模型。在三个跨领域虚假信息检测任务上的评估显示，RAAR显著增强了基础模型的能力，并优于其他跨领域方法、先进LLM和基于LLM的适应方法。

Conclusion: RAAR是首个检索增强的代理推理框架，通过多视角证据检索和多智能体协作推理，有效解决了跨领域虚假信息检测中的领域差异和单一视角限制问题，显著提升了检测性能。

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [67] [Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics](https://arxiv.org/abs/2601.04854)
*Oshri Naparstek*

Main category: cs.CL

TL;DR: 提出连续自回归语言生成模型，将词元表示为连续向量，在离散化前通过多步更新"成熟"，实现确定性解码生成连贯文本，无需词元级采样。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型在离散词元序列上操作，每一步必须选择特定词元，这种早期离散化迫使不确定性通过词元级采样解决，导致生成不稳定、重复和对解码启发式敏感。

Method: 引入连续自回归语言生成框架，将词元表示为连续向量，通过确定性动态过程演化这些表示，仅在表示充分收敛时才离散化。使用硬解码恢复离散文本，不确定性在连续空间中保持和解决。

Result: 仅通过成熟过程就足以使用确定性解码（argmax）生成连贯多样的文本，无需依赖词元级采样、扩散式去噪或辅助稳定机制。这是首个通过演化连续词元表示到收敛再离散化的自回归语言模型。

Conclusion: 连续自回归语言生成模型通过延迟离散化，在连续空间中演化词元表示，实现了稳定生成而无需词元级采样，为语言生成提供了新的范式。

Abstract: Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

</details>


### [68] [MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News](https://arxiv.org/abs/2601.04857)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Baojie Qu,Runteng Guo,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MisSpans是首个多领域、人工标注的span级虚假信息检测与分析基准，包含三个互补任务：识别句子中的虚假片段、分类虚假片段类型、提供基于片段的解释，用于细粒度定位和可解释分析。


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测方法通常在整句或段落级别使用粗糙的二元标签进行评估，掩盖了真假细节在单句中并存的现象，且全局解释无法识别具体误导片段或区分虚假类型（如扭曲vs捏造）。

Method: 构建包含真实和虚假新闻故事配对的MisSpans基准，定义三个任务：MisSpansIdentity（识别虚假片段）、MisSpansType（分类虚假类型）、MisSpansExplanation（提供基于片段的解释）。通过专家标注、标准化指南和一致性检查确保高质量标注。

Result: 评估了15个代表性LLM（包括推理增强和非推理变体）在零样本和单样本设置下的表现，结果显示细粒度虚假信息识别与分析具有挑战性，性能受模型大小、推理能力及领域特定文本特征等多因素交互影响。

Conclusion: MisSpans基准填补了细粒度虚假信息检测的空白，支持更精确的定位、超越真假的细致表征和可操作解释，揭示了当前LLM在细粒度虚假信息分析方面的局限性，为未来研究提供了重要基准。

Abstract: Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.

</details>


### [69] [A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs](https://arxiv.org/abs/2601.04859)
*Maxime Delmas,Lei Xu,André Freitas*

Main category: cs.CL

TL;DR: ToPG是一个新颖的RAG框架，通过构建命题-实体-段落的异构图，结合建议-选择循环进行图遍历，在简单和复杂QA任务上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法存在局限性：基于分块的RAG擅长简单事实检索但无法处理复杂多跳查询；检索与推理交错的方法缺乏全局语料库意识；基于知识图谱的RAG在复杂任务上表现好但在单跳事实查询上表现差。需要一种能兼顾两者的解决方案。

Method: 提出ToPG框架：1) 将知识库建模为命题、实体和段落的异构图；2) 使用迭代的建议-选择循环进行图遍历：建议阶段实现查询感知的图遍历，选择阶段利用LLM反馈修剪无关命题并为下一轮迭代提供种子。

Result: 在三个不同的QA任务（简单、复杂和抽象QA）上评估，ToPG在准确性和质量指标上都表现出强大性能。证明查询感知的图遍历结合事实粒度是高效结构化RAG系统的关键组件。

Conclusion: ToPG通过异构图建模和迭代的图遍历机制，成功弥合了不同RAG方法之间的差距，在简单和复杂查询任务上都实现了优异性能。该框架已开源可用。

Abstract: Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.

</details>


### [70] [EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis](https://arxiv.org/abs/2601.04875)
*Xuanguang Pan,Chongyang Tao,Jiayuan Bai,Jianling Gao,Zhengwei Tao,Xiansheng Zhou,Gavin Cheung,Shuai Ma*

Main category: cs.CL

TL;DR: EvolSQL是一个结构感知的数据合成框架，通过从种子数据演化SQL查询来生成高质量、结构多样的Text-to-SQL训练数据，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL模型面临高质量、多样化、结构复杂数据集稀缺的问题。现有方法要么依赖有限的人工标注数据，要么直接使用LLM生成数据但缺乏对SQL结构的显式控制，导致结构多样性和复杂性有限。

Method: EvolSQL框架包含：1) 探索性查询-SQL扩展以增加问题多样性和模式覆盖；2) 使用基于SQL抽象语法树的六个原子转换算子的自适应定向演化策略，逐步增加关系、谓词、聚合和嵌套维度的查询复杂度；3) 执行基础的SQL精炼模块和模式感知去重确保高质量、结构多样的映射对。

Result: 实验结果表明，使用EvolSQL数据微调的7B模型，仅用1/18的数据量就超越了在更大的SynSQL数据集上训练的模型。

Conclusion: EvolSQL通过结构感知的数据合成方法，能够有效生成高质量、结构多样的Text-to-SQL训练数据，显著提升模型性能，为解决Text-to-SQL数据稀缺问题提供了有效方案。

Abstract: Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

</details>


### [71] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: Mind2Report是一个模仿商业分析师认知过程的深度研究智能体，通过动态记忆增强LLM，在商业报告合成任务上超越现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体生成的商业报告在质量、可靠性和覆盖范围方面仍有局限，无法满足高风险商业决策的需求。

Method: 设计无训练的智能体工作流，模拟商业分析师认知过程：1) 探测细粒度意图；2) 搜索网络源并实时记录提炼信息；3) 迭代合成报告；使用动态记忆增强通用LLM支持长格式认知过程。

Result: 在QRC-Eval数据集（200个真实商业任务）上，Mind2Report在报告质量、可靠性和覆盖范围方面均优于OpenAI和Gemini等领先基线模型。

Conclusion: 虽然这是初步研究，但Mind2Report为未来商业深度研究智能体的设计提供了基础，展示了模拟人类分析师认知过程的有效性。

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [72] [CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters](https://arxiv.org/abs/2601.04885)
*Ao Sun,Xiaoyu Wang,Zhe Tan,Yu Li,Jiachen Zhu,Shu Su,Yuheng Jia*

Main category: cs.CL

TL;DR: CuMA框架通过文化混合适配器解决LLM对齐中的均值崩溃问题，使用人口感知路由分离冲突梯度，在文化多样性基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务全球用户时，对齐需要从强制普遍共识转向尊重文化多元主义。研究发现密集模型在拟合冲突价值分布时会出现"均值崩溃"，收敛到无法代表不同群体的通用平均值。

Method: 提出CuMA（文化混合适配器）框架，将对齐视为条件容量分离问题。通过人口感知路由，CuMA内化潜在文化拓扑，将冲突梯度显式解耦到专门的专家子空间中。

Result: 在WorldValuesBench、Community Alignment和PRISM等基准上的广泛评估显示，CuMA实现了最先进的性能，显著优于密集基线和仅语义的MoE方法。分析证实CuMA有效缓解了均值崩溃，保持了文化多样性。

Conclusion: CuMA框架通过文化混合适配器成功解决了LLM对齐中的文化多样性问题，为全球服务的大语言模型提供了尊重文化多元主义的有效对齐方法。

Abstract: As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.

</details>


### [73] [Faithful Summarisation under Disagreement via Belief-Level Aggregation](https://arxiv.org/abs/2601.04889)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 提出一个分歧感知的摘要生成流程，将信念级聚合与语言生成分离，以更好地处理观点冲突


<details>
  <summary>Details</summary>
Motivation: 现有方法（特别是基于LLM的系统）在处理观点冲突时倾向于平滑分歧、过度代表多数意见，限制了摘要的忠实性

Method: 引入分歧感知合成流程：1）将文档表示为结构化信念集；2）使用基于距离的信念合并算子进行显式冲突建模；3）仅用LLM将聚合后的信念生成为自然语言摘要

Result: 足够大的模型在生成时处理聚合可以匹配信念级聚合，但这种行为在不同架构或能力下不稳定；信念级聚合结合简单提示能在各模型中保持稳定强大的分歧感知性能

Conclusion: 信念级聚合与语言生成分离的方法能产生更忠实、流畅且基于事实的摘要，在处理观点冲突时表现更稳定可靠

Abstract: Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.

</details>


### [74] [V-FAT: Benchmarking Visual Fidelity Against Text-bias](https://arxiv.org/abs/2601.04897)
*Ziteng Wang,Yujie He,Guanliang Li,Siqi Yang,Jiaqi Xiong,Songxiang Liu*

Main category: cs.CL

TL;DR: 本文提出V-FAT基准测试来诊断多模态大语言模型中的文本偏见问题，发现当前模型在视觉-文本冲突场景下表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在标准视觉推理基准上表现优异，但存在过度依赖语言捷径而非真正视觉基础的问题（称为文本偏见）。作者旨在探究视觉感知与语言先验之间的根本张力。

Method: 将文本偏见解耦为两个维度：内部语料偏见（来自预训练中的统计相关性）和外部指令偏见（来自对齐导致的迎合倾向）。提出V-FAT诊断基准，包含4,026个VQA实例，采用三级评估框架：L1（非典型图像）、L2（误导指令）、L3（两者协同）。引入视觉鲁棒性评分来惩罚"幸运"的语言猜测并奖励真正的视觉保真度。

Result: 评估12个前沿MLLM发现，虽然模型在现有基准上表现出色，但在高语言主导性下经历了显著的视觉崩溃。

Conclusion: 多模态大语言模型存在严重的文本偏见问题，当前评估方法未能充分测试模型的真实视觉理解能力，需要更严格的基准来评估视觉保真度。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.

</details>


### [75] [Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences](https://arxiv.org/abs/2601.04925)
*Arkadiusz Modzelewski,Paweł Golik,Anna Kołos,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: LLM生成的劝说性文本检测研究：研究发现明显劝说性LLM文本比人类文本更容易检测，但微妙劝说性LLM文本会降低自动检测性能，并首次提供了全面的语言分析对比。


<details>
  <summary>Details</summary>
Motivation: LLM能够生成高度说服力的文本，引发了对其被滥用于宣传、操纵等有害目的的担忧。核心问题是：LLM生成的劝说性文本是否比人类撰写的更难自动检测？

Method: 1. 对LLM生成劝说性内容的可控生成方法进行分类；2. 引入Persuaficial多语言基准数据集，涵盖英语、德语、波兰语、意大利语、法语和俄语六种语言；3. 进行广泛的实证评估，比较人类撰写和LLM生成的劝说性文本；4. 提供首个全面的语言分析，对比人类和LLM生成的劝说性文本。

Result: 1. 明显的LLM生成的劝说性文本比人类撰写的更容易检测；2. 微妙的LLM生成的劝说性文本会持续降低自动检测性能；3. 通过语言分析提供了对比洞察，可能指导开发更可解释和鲁棒的检测工具。

Conclusion: LLM生成的劝说性文本检测具有复杂性：明显劝说性内容相对容易检测，但微妙劝说性内容对现有检测方法构成挑战。研究为开发更有效的检测工具提供了重要见解和基准资源。

Abstract: Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.

</details>


### [76] [GenProve: Learning to Generate Text with Fine-Grained Provenance](https://arxiv.org/abs/2601.04932)
*Jingxuan Wei,Xingyue Wang,Yanghaoyu Liao,Jie Dong,Yuchen Liu,Caijun Jia,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: 论文提出GenProve框架，通过细粒度溯源三元组解决LLM幻觉问题，在ReFInE数据集上结合SFT和GRPO优化，显著提升答案保真度和溯源正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM引用方法通常粒度较粗，无法区分直接引用和复杂推理，用户难以验证引用来源如何支持生成主张，需要更细粒度的可追溯性解决方案。

Method: 提出生成时细粒度溯源任务，创建ReFInE数据集（包含专家验证的引用、压缩、推理标注），开发GenProve框架结合监督微调（SFT）和组相对策略优化（GRPO），优化答案保真度和溯源正确性的复合奖励。

Result: GenProve在联合评估中显著优于14个强大的LLM，分析发现模型在表面引用方面表现良好，但在基于推理的溯源方面存在显著困难，表明可验证推理仍是与表面引用不同的前沿挑战。

Conclusion: 细粒度溯源是解决LLM幻觉问题的有效方法，但模型在推理溯源方面仍存在明显差距，需要进一步研究可验证推理能力。

Abstract: Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.

</details>


### [77] [A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction](https://arxiv.org/abs/2601.04960)
*Qing Wang,Zehan Li,Yaodong Song,Hongjie Chen,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: 提出统一的语音情感智能模型，采用IEAT数据构建策略将用户情感状态及原因注入模型推理过程，通过两阶段训练实现情感感知推理，在HumDial基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音对话系统缺乏情感智能，无法理解用户情感状态及其原因。传统方法将情感作为显式监督信号，而非内部推理过程，限制了模型的情感理解能力。

Method: 1. 提出IEAT数据构建策略，将用户情感状态及原因注入模型内部推理过程；2. 采用两阶段渐进训练：第一阶段通过自蒸馏进行语音-文本对齐和情感属性建模，第二阶段进行端到端跨模态联合优化，确保文本和语音情感表达的一致性。

Result: 在HumDial情感智能基准测试中，该方法在情感轨迹建模、情感推理和共情回应生成方面均取得顶级性能，在LLM评估和人工评估中都表现优异。

Conclusion: IEAT策略能够有效将情感感知推理内化到统一语音语言模型中，两阶段训练方法确保了跨模态情感表达的一致性，为构建情感智能语音对话系统提供了有效解决方案。

Abstract: This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

</details>


### [78] [Text as a Universal Interface for Transferable Personalization](https://arxiv.org/abs/2601.04963)
*Yuting Liu,Jian Guan,Jia-Nan Li,Wei Wu,Jiang-Ming Yang,Jianzhe Zhao,Guibing Guo*

Main category: cs.CL

TL;DR: 提出使用自然语言作为通用偏好表示接口，开发AlignXplore+模型生成文本偏好摘要，在9个基准测试中SOTA表现超越更大开源模型


<details>
  <summary>Details</summary>
Motivation: 现有工作将用户偏好表示为隐式、模型特定的向量或参数，产生难以解释和跨模型/任务迁移的"黑盒"配置文件。需要一种通用、可解释且可重用的偏好表示方法

Method: 采用自然语言作为模型和任务无关的偏好表示接口，提出两阶段训练框架：1) 在高质量合成数据上进行监督微调；2) 使用强化学习优化长期效用和跨任务可迁移性。基于此开发AlignXplore+通用偏好推理模型

Result: 在9个基准测试中，8B参数的AlignXplore+模型达到最先进性能，显著优于更大的开源模型，同时在任务、模型家族和交互格式间展现出强大的可迁移性

Conclusion: 自然语言作为偏好表示接口能够产生可解释、可重用且支持持续演化的偏好描述，为LLM个性化提供了更透明和通用的解决方案

Abstract: We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.

</details>


### [79] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: 论文提出在监督微调中同时使用正负思维链轨迹，并开发GLOW损失加权方法，显著提升大语言模型的OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调只使用正确最终答案的思维链轨迹（正样本），而忽略错误答案的轨迹（负样本）。作者认为这种做法浪费了大量监督信号，加剧过拟合，限制了模型的域外泛化能力。

Method: 1. 在监督微调中同时使用正负思维链轨迹；2. 提出GLOW（Gain-based LOss Weighting）方法，基于样本在训练过程中的进展动态调整损失权重；3. 系统分析了负样本中的22种常见模式及其对训练动态的影响。

Result: 1. 使用负样本训练相比仅用正样本带来显著的域外泛化提升；2. GLOW方法在Qwen2.5-7B上实现5.51%的OOD增益；3. 作为强化学习初始化，将MMLU分数从72.82%提升到76.47%；4. 负样本使推理时策略熵提升35.67%，促进探索。

Conclusion: 负思维链轨迹包含有价值的监督信号，通过GLOW方法有效利用这些信号可以显著提升大语言模型的泛化能力，为监督微调提供了新的优化方向。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [80] [Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei](https://arxiv.org/abs/2601.05004)
*Peng Wang,Xilin Tao,Siyi Yao,Jiageng Wu,Yuntao Zou,Zhuotao Tian,Libo Qin,Dagang Li*

Main category: cs.CL

TL;DR: 提出SAS多智能体框架，通过自动检索和亚文化对齐解决LLM在亚文化群体自毁行为检测中的知识滞后和语义错配问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 亚文化群体的自毁行为因其独特表达方式而难以识别，现有LLM方法面临知识滞后（亚文化俚语更新快于LLM训练周期）和语义错配（难以理解亚文化特有表达）两大挑战。

Method: 提出Subcultural Alignment Solver (SAS)多智能体框架，结合自动检索和亚文化对齐机制，增强LLM在亚文化自毁行为检测中的能力。

Result: SAS在实验中优于当前先进的多智能体框架OWL，性能可与微调LLM相媲美。

Conclusion: SAS框架能有效提升LLM在亚文化自毁行为检测中的表现，为未来研究提供宝贵资源，推动该领域发展。

Abstract: Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.

</details>


### [81] [Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models](https://arxiv.org/abs/2601.05019)
*Yueqing Hu,Xinyang Peng,Shuting Peng,Hanqi Wang,Tianhong Wang*

Main category: cs.CL

TL;DR: 推理蒸馏中的SFT训练导致学生模型仅模仿推理的语言形式而非认知结构，造成功能对齐崩溃和负迁移现象


<details>
  <summary>Details</summary>
Motivation: 研究推理蒸馏中监督微调(SFT)方法是否能够传递教师模型与人类认知成本的自然对齐特性，验证"邯郸学步"（表面模仿）假说

Method: 在14个模型上测试推理蒸馏效果，比较教师模型与学生模型在人类难度缩放方面的对齐程度，分析SFT训练对认知结构传递的影响

Result: 教师模型与人类难度缩放高度相关(r=0.64)，而蒸馏后的学生模型对齐显著下降(r=0.34)，甚至出现负迁移现象，表现低于蒸馏前基线

Conclusion: SFT训练导致"货物崇拜"效应，学生仅仪式性地复制推理的语言形式而非内部化教师的动态资源分配策略，人类认知特性是强化学习的涌现属性而非被动模仿的结果

Abstract: Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.

</details>


### [82] [ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG](https://arxiv.org/abs/2601.05038)
*Jianbo Li,Yi Jiang,Sendong Zhao,Bairui Hu,Haochun Wang,Bing Qin*

Main category: cs.CL

TL;DR: ArcAligner是一种轻量级模块，通过自适应门控系统帮助LLM更好地利用高度压缩的上下文表示，在保持速度的同时提升压缩文档的理解能力。


<details>
  <summary>Details</summary>
Motivation: RAG虽然能提高LLM准确性，但长文档输入会导致模型变慢且昂贵。现有上下文压缩方法（如摘要和嵌入压缩）存在一个矛盾：压缩程度越高，LLM越难理解内容。

Method: 提出ArcAligner（自适应递归上下文对齐器），这是一个集成到语言模型层的轻量级模块。它使用自适应门控系统，只在信息复杂时增加额外处理能力，从而保持系统速度。

Result: 在知识密集型QA基准测试中，ArcAligner在可比压缩率下持续优于压缩基线，特别是在多跳和长尾场景中表现突出。

Conclusion: ArcAligner通过自适应门控机制有效解决了高度压缩上下文表示的理解难题，在保持效率的同时提升了RAG系统的性能。

Abstract: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.

</details>


### [83] [Compositional Steering of Large Language Models with Steering Tokens](https://arxiv.org/abs/2601.05062)
*Gorjan Radevski,Kiril Gashteovski,Giwon Hong,Carolin Lawrence,Goran Glavaš*

Main category: cs.CL

TL;DR: 提出组合引导令牌方法，通过自蒸馏将自然语言指令嵌入专用令牌，实现LLM的多行为组合控制，优于现有方法且能泛化到未见过的行为组合。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中部署LLM需要满足多个期望的可控输出。现有工作主要关注单一行为的LLM引导，而组合引导（同时引导LLM朝向多个行为）仍是一个未充分探索的问题。

Method: 提出组合引导令牌方法：1）通过自蒸馏将自然语言指令表达的个体行为嵌入专用令牌；2）训练专用组合令牌来捕捉组合概念，使其能泛化到未见过的行为组合，包括未见过的行为和未见过的行为数量。

Result: 在不同LLM架构上的实验表明，引导令牌在多行为控制方面优于竞争方法（指令、激活引导和LoRA合并）。组合引导令牌能有效泛化到未见过的行为组合，并且与自然语言指令互补，结合使用时能获得进一步增益。

Conclusion: 组合引导令牌为LLM的多行为控制提供了一种有效方法，在输入令牌空间操作使其具有更好的零样本组合能力，并能泛化到未见过的组合，为实际应用中的复杂行为控制提供了新途径。

Abstract: Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.

</details>


### [84] [SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment](https://arxiv.org/abs/2601.05075)
*Ziyang Chen,Zhenxuan Huang,Yile Wang,Weiqin Wang,Lu Yin,Hui Huang*

Main category: cs.CL

TL;DR: SemPA是一种通过语义偏好对齐增强句子表示同时保持LLMs生成能力的新方法，使用句子级DPO在释义生成任务上优化模型，在语义相似度任务和生成基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统句子嵌入方法在非生成预训练模型上使用token级对比学习。基于生成式LLMs的嵌入方法要么依赖固定提示模板（性能有限），要么修改模型架构（损害生成能力）。需要一种既能提升句子表示又不牺牲LLMs生成能力的方法。

Method: 提出SemPA方法，通过语义偏好对齐增强句子表示。使用句子级直接偏好优化（DPO）在释义生成任务上高效优化LLMs，让模型学习区分语义等价的句子，同时保持内在生成能力。在Plackett-Luce模型框架下建立了DPO与对比学习的理论联系。

Result: 在语义文本相似性任务和各种LLMs基准测试中，SemPA实现了更好的语义表示，同时没有牺牲LLMs固有的生成能力。

Conclusion: SemPA是一种有效的方法，能够在保持生成式大语言模型原有生成能力的同时，显著提升其句子表示质量，为LLMs的嵌入学习提供了新的优化路径。

Abstract: Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

</details>


### [85] [Code-Mix Sentiment Analysis on Hinglish Tweets](https://arxiv.org/abs/2601.05091)
*Aashi Garg,Aneshya Das,Arshi Arya,Anushka Goyal,Aditi*

Main category: cs.CL

TL;DR: 提出針對印度Hinglish推文的高性能情感分類框架，通過微調mBERT並使用子詞分詞來處理代碼混合語言，提升品牌監測準確性。


<details>
  <summary>Details</summary>
Motivation: 印度品牌監測面臨Hinglish（印地語-英語混合語）的挑戰，傳統NLP模型無法處理這種代碼混合語言的複雜性，導致情感分析不準確和市場洞察誤導。

Method: 微調多語言BERT（mBERT），利用其多語言能力理解印度社交媒體的語言多樣性，並採用子詞分詞技術有效處理羅馬化Hinglish中的拼寫變體、俚語和未登錄詞。

Result: 開發出可用於生產的品牌情感追蹤AI解決方案，為低資源代碼混合環境的多語言NLP建立了強有力的基準。

Conclusion: 該研究成功解決了Hinglish情感分類的挑戰，為印度社交媒體的品牌監測提供了有效的技術方案，並推動了多語言NLP在代碼混合環境中的發展。

Abstract: The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.

</details>


### [86] [How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness](https://arxiv.org/abs/2601.05104)
*Florence Bernays,Marco Henriques Pereira,Jochen Menges*

Main category: cs.CL

TL;DR: 研究发现人类在与ChatGPT互动时的情感语调会影响AI的回答质量，愤怒和赞扬能提升AI表现，而责备则无效；同时这些情感还会延续到后续的人际交流中。


<details>
  <summary>Details</summary>
Motivation: 探索人类在与AI互动时的情感表达如何影响AI的行为表现，以及这种情感互动是否会延续到后续的人际交流中，理解人机交互的情感动态及其社会影响。

Method: 采用组间实验设计，让参与者在与ChatGPT（GPT-4.0）完成两项任务（撰写公开回应和解决伦理困境）时表达特定情感（赞扬、愤怒、责备或保持中性），然后分析ChatGPT的回答质量和内容变化，并观察参与者后续人际交流中的语言使用。

Result: 赞扬ChatGPT能最大程度提升其回答质量；愤怒也能带来较小提升；责备则无效。在伦理困境中，愤怒使ChatGPT减少对企业利益的关注，责备则增加对公共利益的重视。责备互动后，人们在人际交流中使用更多负面、敌对和失望的表达。

Conclusion: 人机互动中的情感语调不仅显著影响AI的输出质量，还会"溢出"到后续的人际交流中，形成情感传递效应，这对设计AI系统和理解人机交互的社会影响具有重要意义。

Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.

</details>


### [87] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 本文首次全面综述了AI评估从LLM-as-a-Judge到Agent-as-a-Judge的范式转变，建立了发展分类法，并提供了下一代智能体评估的路线图。


<details>
  <summary>Details</summary>
Motivation: 随着评估对象变得越来越复杂、专业和多步骤，LLM-as-a-Judge方法因固有偏见、浅层单次推理以及无法验证现实观察而受限，这促使了向Agent-as-a-Judge的范式转变。尽管智能体评估系统迅速涌现，但该领域缺乏统一的框架来导航这一变化格局。

Method: 本文通过识别范式转变的关键维度，建立发展分类法，组织核心方法论，并调查通用和专业领域的应用，提供了首个全面的综述框架。

Result: 建立了从LLM-as-a-Judge到Agent-as-a-Judge演进的统一框架，提出了发展分类法，系统整理了核心方法，并分析了各领域的应用案例。

Conclusion: 本文填补了智能体评估领域的框架空白，通过提供全面的综述、分类法和路线图，为下一代更稳健、可验证和细致的AI评估系统指明了发展方向。

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [88] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: DocDancer：一个端到端训练的开源文档问答代理，通过工具驱动框架解决文档探索与理解问题，使用探索-合成数据生成方法解决训练数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 现有文档问答代理缺乏有效的工具利用能力，且主要依赖闭源模型，需要开发开源、工具驱动的端到端文档问答代理

Method: 将文档问答建模为信息检索问题，提出工具驱动的代理框架，明确建模文档探索和理解过程；引入探索-合成数据合成管道解决高质量训练数据稀缺问题

Result: 在MMLongBench-Doc和DocBench两个长上下文文档理解基准测试上训练模型，证明了方法的有效性；进一步分析为代理工具设计和合成数据提供了有价值的见解

Conclusion: DocDancer是一个有效的端到端开源文档问答代理，通过工具驱动框架和数据合成方法解决了现有方法的局限性，为文档理解任务提供了新的解决方案

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [89] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: RelayLLM是一个通过令牌级协作解码实现高效推理的框架，让小语言模型(SLM)作为主动控制器，只在关键令牌时调用大语言模型(LLM)，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理计算成本高、延迟大，而SLM推理能力不足。现有的协作方法（如级联或路由）在粗粒度上操作，将整个查询卸载给LLM，当SLM能够处理大部分推理步骤时会造成显著的计算浪费。

Method: 提出RelayLLM框架，采用令牌级协作解码。SLM作为主动控制器，通过特殊命令动态调用LLM处理关键令牌，实现"接力"生成。采用两阶段训练框架：预热阶段和组相对策略优化(GRPO)，教导模型平衡独立性与策略性求助。

Result: 在六个基准测试中，RelayLLM平均准确率达到49.52%，有效弥合了两个模型之间的性能差距。仅调用LLM处理总生成令牌的1.07%，相比性能匹配的随机路由器提供98.2%的成本降低。

Conclusion: RelayLLM通过细粒度的令牌级协作，在保持高性能的同时大幅降低计算成本，为高效推理提供了有前景的解决方案。

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [90] [Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference](https://arxiv.org/abs/2601.05170)
*Rasmus Blanck,Bill Noble,Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 本文分析了自然语言推理（NLI）任务的逻辑属性，提出了三种可能的标签集解读，并通过分析SNLI数据集来评估模型在元推理一致性上的表现，以理解数据集编码的逻辑关系。


<details>
  <summary>Details</summary>
Motivation: 自然语言推理（NLI）是评估语言模型自然语言理解能力的重要任务，但其逻辑属性常被误解和错误描述。理解NLI捕捉的推理概念对于解释模型在该任务上的表现至关重要。

Method: 1. 提出三种可能的NLI标签集解读方式；2. 对每种解读进行全面的元推理属性分析；3. 聚焦SNLI数据集，利用（1）共享前提的NLI项目和（2）LLM生成的项目来评估在SNLI上训练的模型的元推理一致性。

Result: 通过分析模型在元推理一致性上的表现，揭示了SNLI数据集编码的逻辑关系类型，帮助理解NLI任务实际捕捉的推理概念。

Conclusion: 该研究为理解NLI任务的逻辑基础提供了系统框架，有助于更准确地解释语言模型在NLI任务上的表现，并揭示了数据集设计对模型学习逻辑关系的影响。

Abstract: Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.

</details>


### [91] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: Inside Out框架使用PersonaTree进行长期用户画像，通过轻量级MemListener进行结构化记忆操作，在保持一致性的同时实现记忆压缩，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有长期个性化对话系统在处理无限交互流与有限上下文约束时存在记忆噪声累积、推理退化、人设不一致等问题，需要更有效的长期用户画像管理方法。

Method: 提出Inside Out框架：1) 使用全局维护的PersonaTree作为长期用户画像载体，通过初始模式约束主干并更新枝叶实现可控增长；2) 训练轻量级MemListener通过强化学习生成结构化操作(ADD/UPDATE/DELETE/NO_OP)；3) 响应生成时直接利用PersonaTree，需要细节时触发代理模式按需引入细节。

Result: PersonaTree在抑制上下文噪声和保持人设一致性方面优于全文拼接和各种个性化记忆系统；轻量级MemListener在记忆操作决策性能上达到甚至超过DeepSeek-R1-0528和Gemini-3-Pro等强大推理模型。

Conclusion: Inside Out框架通过PersonaTree和MemListener的组合，有效解决了长期个性化对话中的记忆管理问题，实现了可控增长、记忆压缩和一致性保持，为实际应用提供了高效解决方案。

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [92] [LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation](https://arxiv.org/abs/2601.05192)
*Samy Haffoudhi,Fabian M. Suchanek,Nils Holzenberger*

Main category: cs.CL

TL;DR: LELA是一种模块化的从粗到精的实体链接方法，利用大语言模型能力，无需微调即可适应不同领域、知识库和LLM，性能接近微调方法并显著优于非微调方法。


<details>
  <summary>Details</summary>
Motivation: 实体链接是知识图谱构建、问答和信息提取等任务的基础步骤。现有方法通常需要针对特定领域或知识库进行微调，缺乏通用性和灵活性。需要一种无需微调就能适应不同设置的方法。

Method: LELA采用模块化的从粗到精方法：1) 利用LLM能力进行实体链接；2) 模块化设计使其能够适应不同目标领域、知识库和LLM；3) 完全无需微调阶段，通过提示工程和上下文学习实现。

Result: 在各种实体链接设置下的实验表明：1) LELA与微调方法相比具有高度竞争力；2) 显著优于非微调方法；3) 展示了在不同领域、知识库和LLM间的良好适应性。

Conclusion: LELA提供了一种有效且灵活的实体链接解决方案，无需微调即可达到接近微调方法的性能，为跨领域实体链接任务提供了实用工具。

Abstract: Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

</details>


### [93] [Measuring and Fostering Peace through Machine Learning and Artificial Intelligence](https://arxiv.org/abs/2601.05232)
*P. Gilda,P. Dungarwal,A. Thongkham,E. T. Ajayi,S. Choudhary,T. M. Terol,C. Lam,J. P. Araujo,M. McFadyen-Mungalln,L. S. Liebovitch,P. T. Coleman,H. West,K. Sieck,S. Carter*

Main category: cs.CL

TL;DR: 使用机器学习从新闻和社交媒体测量和平水平，并开发MirrorMirror浏览器扩展实时反馈媒体和平度，旨在促进更尊重、细致的信息传播。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体内容创作者倾向于使用情绪激活（特别是愤怒）来增加点击率，而71%的20-40岁年轻人主要通过短视频获取新闻，这种媒体消费模式可能影响社会和平。需要超越简单的参与度指标，促进更尊重、细致和富有信息量的沟通。

Method: 1) 对新闻媒体：使用神经网络从在线新闻源的文本嵌入中测量和平水平，模型在一个新闻数据集上训练并在不同数据集上验证；2) 对社交媒体（如YouTube）：使用词级（GoEmotions）和上下文级（大语言模型）方法测量与和平相关的重要社会维度；3) 开发MirrorMirror Chrome扩展，为YouTube观众提供实时媒体和平度反馈。

Result: 新闻媒体模型在不同数据集上表现出高准确性；开发了测量社交媒体和平度的模型；创建了MirrorMirror Chrome扩展并进行了测试；长期目标是将其发展为开源工具，供内容创作者、记者、研究人员、平台和个人用户使用。

Conclusion: 通过机器学习测量媒体和平度并开发实时反馈工具，有助于用户更好地理解自己的媒体消费习惯，鼓励内容创作者减少情绪操纵，促进更和平、尊重和富有信息量的沟通，最终支持社会和平建设。

Abstract: We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.

</details>


### [94] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 论文提出GDPO方法，解决GRPO在多奖励强化学习中因归一化导致奖励信号模糊的问题，通过解耦各奖励的归一化来提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能力增强，用户期望它们不仅能提供准确回答，还能在不同场景下符合多样的人类偏好。现有的多奖励强化学习通常直接使用GRPO，但作者发现这种直接应用会导致不同奖励组合在归一化后优势值相同，降低训练信号分辨率，导致次优收敛甚至训练失败。

Method: 提出Group reward-Decoupled Normalization Policy Optimization (GDPO)方法，核心思想是解耦各个奖励的归一化处理，更忠实地保留奖励间的相对差异，从而实现更准确的多奖励优化和显著提升的训练稳定性。

Result: 在工具调用、数学推理和代码推理三个任务上对比GDPO和GRPO，评估正确性指标（准确率、错误率）和约束遵循指标（格式、长度）。在所有设置中，GDPO都一致优于GRPO，证明了其有效性和在多奖励强化学习优化中的泛化能力。

Conclusion: GDPO通过解耦奖励归一化，有效解决了GRPO在多奖励设置中的局限性，提供了更稳定、更有效的多奖励强化学习优化方法，能够更好地平衡不同的人类偏好需求。

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [95] [Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization](https://arxiv.org/abs/2601.04334)
*Amit Jain,Richard Linares*

Main category: cs.RO

TL;DR: 本文提出了一种结合大型语言模型（LLM）与群相对策略优化（GRPO）的学习型制导控制方法，通过两阶段训练在多种动态系统中实现稳定控制策略。


<details>
  <summary>Details</summary>
Motivation: 传统控制方法在处理复杂非线性系统时面临挑战，需要开发能够同时提供有效控制和人机可解释决策过程的智能控制系统。

Method: 采用两阶段训练：1）监督微调（SFT）学习格式化和控制原语；2）GRPO进行交互驱动的策略改进。将具备显式推理能力的LLM与GRPO结合，为每个环境训练控制器。

Result: 在从线性系统到非线性振荡动力学，再到具有陀螺耦合和推力约束的三维航天器姿态控制等四个控制问题上验证了框架有效性。结果显示，通过GRPO优化的具备显式推理的LLM能够在一致训练设置下合成可行的稳定策略。

Conclusion: 该工作为将基于GRPO的推理应用于自主控制系统奠定了基础，在航空航天和其他安全关键领域具有潜在应用价值。两阶段训练方法使模型能够生成控制序列，同时提供人类可读的决策过程解释。

Abstract: This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.

</details>


### [96] [UNIC: Learning Unified Multimodal Extrinsic Contact Estimation](https://arxiv.org/abs/2601.04356)
*Zhengtong Xu,Yuki Shirai*

Main category: cs.RO

TL;DR: UNIC是一个无需先验知识或相机标定的统一多模态框架，用于估计抓取物体与环境之间的外部接触，通过场景可供性地图的统一接触表示和多模态融合机制实现鲁棒学习。


<details>
  <summary>Details</summary>
Motivation: 接触丰富的操作需要可靠估计外部接触（抓取物体与环境之间的交互），但现有方法依赖预定义接触类型、固定抓取配置或相机标定等限制性假设，阻碍了在新物体和非结构化环境中的泛化能力。

Method: UNIC直接编码相机坐标系中的视觉观测，并与本体感觉和触觉模态以完全数据驱动方式集成；引入基于场景可供性地图的统一接触表示来捕捉多样接触形态；采用带随机掩码的多模态融合机制进行鲁棒的多模态表示学习。

Result: 在未见接触位置上的平均Chamfer距离误差为9.6毫米，在未见物体上表现良好，对缺失模态具有鲁棒性，并能适应动态相机视角，证明了外部接触估计在接触丰富操作中的实用性和多功能性。

Conclusion: UNIC框架将外部接触估计确立为接触丰富操作中实用且多功能的能力，无需先验知识或相机标定即可在各种条件下可靠工作。

Abstract: Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.

</details>


### [97] [Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces](https://arxiv.org/abs/2601.04401)
*Arsyi Aziz,Peng Wei*

Main category: cs.RO

TL;DR: 论文提出一种基于多智能体强化学习的飞机间隔保障方法，使用相对极坐标状态空间和Transformer编码器，在结构化与非结构化空域中实现自适应、可扩展的分散式冲突解决。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的计量方法依赖预先计算的时间表，缺乏应对高级空中交通随机性所需的灵活性。现有MARL方法容易过拟合特定空域结构，难以适应新配置。

Method: 将MARL问题重新表述为相对极坐标状态空间，使用Transformer编码器模型在不同交通模式和交叉角度下训练，为飞机提供速度建议以解决冲突并保持巡航速度。

Result: 单层编码器配置在结构化与非结构化空域中均优于深层变体，实现接近零的空中接近碰撞率，分离违规时间更短，且优于纯注意力基线模型。

Conclusion: 新的状态表示、神经网络架构设计和训练策略为飞机间隔保障提供了适应性强、可扩展的分散式解决方案，适用于各种空域配置。

Abstract: Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.

</details>


### [98] [Fast Continuum Robot Shape and External Load State Estimation on SE(3)](https://arxiv.org/abs/2601.04493)
*James M. Ferguson,Alan Kuntz,Tucker Hermans*

Main category: cs.RO

TL;DR: 提出一个连续体机器人时空状态估计通用框架，结合驱动、外力、测量不确定性，通过因子图实现实时运动学、力感知和负载估计


<details>
  <summary>Details</summary>
Motivation: 现有基于流形的连续体机器人状态估计方法通常采用简化的Cosserat杆模型，无法直接考虑驱动输入或外部载荷，需要更全面的框架来整合各种不确定性因素

Method: 提出通用框架，整合驱动不确定性（如肌腱张力）、作用力/力矩、过程噪声、边界条件和任意主干测量；通过添加时间先验实现空间（弧长）和时间域的联合估计；将弧长域离散化为因子图表示，利用SLAM风格的稀疏非线性优化

Result: 在仿真中展示肌腱驱动机器人的实时运动学（含不确定性）、基于位置反馈的末端力感知、基于主干应变的分布式负载估计；在实验中验证手术同心管机器人的精确运动学和末端力估计，展示手术触诊潜力

Conclusion: 该框架适用于广泛的连续体机器人类型，通过整合时空不确定性实现全面的状态估计，为实时运动学、力感知和负载估计提供了通用解决方案，在手术机器人等领域具有应用潜力

Abstract: Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads. We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements. By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \textit{spacetime} state estimation. Discretizing the arclength domain yields a factor graph representation of the continuum robot model, which can be exploited for fast batch sparse nonlinear optimization in the style of SLAM. The framework is general and applies to a broad class of continuum robots; as illustrative cases, we show (i) tendon-driven robots in simulation, where we demonstrate real-time kinematics with uncertainty, tip force sensing from position feedback, and distributed load estimation from backbone strain, and (ii) a surgical concentric tube robot in experiment, where we validate accurate kinematics and tip force estimation, highlighting potential for surgical palpation.

</details>


### [99] [Multiagent Reinforcement Learning with Neighbor Action Estimation](https://arxiv.org/abs/2601.04511)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu*

Main category: cs.RO

TL;DR: 提出一种增强型多智能体强化学习框架，通过动作估计神经网络推断邻居行为，无需显式动作交换，提升了现实工程环境中的部署可行性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法依赖智能体间的显式动作交换，但在现实工程环境中常因通信限制、延迟、能耗和可靠性要求而不切实际，需要一种能在信息受限环境下有效协作的方法。

Method: 提出增强型多智能体强化学习框架，集成轻量级动作估计模块，每个智能体仅使用本地可观测信息推断邻居行为，实现无需显式动作共享的协作策略学习，与标准TD3算法完全兼容且可扩展。

Result: 在双臂机器人操作任务（协作搬运物体）中验证了该框架，实验结果表明该方法显著提升了现实机器人系统的鲁棒性和部署可行性，同时减少了对信息基础设施的依赖。

Conclusion: 该研究推动了去中心化多智能体人工智能系统的发展，使AI能在动态、信息受限的现实环境中有效运行，为工程应用提供了实用的解决方案。

Abstract: Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.

</details>


### [100] [Design and Development of Modular Limbs for Reconfigurable Robots on the Moon](https://arxiv.org/abs/2601.04541)
*Gustavo H. Diaz,A. Sejal Jain,Matteo Brugnera,Elian Neppel,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发了名为Moonbots的4自由度模块化机器人肢体，可与轮式模块组合适应不同环境和任务，主要用于月球探索和建设任务，展示了9种不同功能配置。


<details>
  <summary>Details</summary>
Motivation: 为空间探索和月球建设任务开发灵活、多功能的模块化机器人系统，以应对资源受限的太空任务环境，提高机器人的适应性和多功能性。

Method: 设计了具有高扭矩-速度比的统一执行器驱动的4自由度机器人肢体模块，可与轮式模块连接组合；开发了硬件实现、机械设计和软件架构；评估了执行器在不同负载下的控制性能。

Result: 成功实现了模块化机器人系统，展示了9种功能配置：4自由度肢体、8自由度肢体、车辆、龙形、最小化、四足、货物、最小货物和自行车配置，验证了系统的适应性和多功能性。

Conclusion: Moonbots模块化机器人系统为可重构机器人研究提供了实用基础，其统一执行器设计简化了开发和维护，系统能够适应不同环境和任务需求，特别适合资源受限的太空任务。

Abstract: In this paper, we present the development of 4-DOF robot limbs, which we call Moonbots, designed to connect in various configurations with each other and wheel modules, enabling adaptation to different environments and tasks. These modular components are intended primarily for robotic systems in space exploration and construction on the Moon in our Moonshot project. Such modular robots add flexibility and versatility for space missions where resources are constrained. Each module is driven by a common actuator characterized by a high torque-to-speed ratio, supporting both precise control and dynamic motion when required. This unified actuator design simplifies development and maintenance across the different module types. The paper describes the hardware implementation, the mechanical design of the modules, and the overall software architecture used to control and coordinate them. Additionally, we evaluate the control performance of the actuator under various load conditions to characterize its suitability for modular robot applications. To demonstrate the adaptability of the system, we introduce nine functional configurations assembled from the same set of modules: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. These configurations reflect different locomotion strategies and task-specific behaviors, offering a practical foundation for further research in reconfigurable robotic systems.

</details>


### [101] [Data-Driven Terramechanics Approach Towards a Realistic Real-Time Simulator for Lunar Rovers](https://arxiv.org/abs/2601.04547)
*Jakob M. Kern,James M. Hurrell,Shreya Santra,Keisuke Takehana,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出一种结合高视觉保真度和物理准确性的月球表面模拟器，采用数据驱动的回归模型预测车轮-土壤相互作用，实现实时物理可信的地形响应和高视觉质量


<details>
  <summary>Details</summary>
Motivation: 当前月球表面模拟器要么侧重视觉真实性，要么侧重物理准确性，无法全面复现月球条件，限制了漫游车操作测试和任务规划的能力

Method: 采用数据驱动方法，基于全车和单轮实验与模拟数据，建立滑移和下沉的回归模型；改进地形变形和车轮轨迹可视化以增强真实感

Result: 回归式地形力学模型能准确复现平坦地形和20度坡度上的稳态和动态滑移以及下沉行为，经现场测试结果验证；支持实时应用

Conclusion: 该方法成功填补了高视觉保真度和物理准确性之间的空白，为月球漫游车操作提供了更全面的模拟环境，支持实时物理可信的地形响应

Abstract: High-fidelity simulators for the lunar surface provide a digital environment for extensive testing of rover operations and mission planning. However, current simulators focus on either visual realism or physical accuracy, which limits their capability to replicate lunar conditions comprehensively. This work addresses that gap by combining high visual fidelity with realistic terrain interaction for a realistic representation of rovers on the lunar surface. Because direct simulation of wheel-soil interactions is computationally expensive, a data-driven approach was adopted, using regression models for slip and sinkage from data collected in both full-rover and single-wheel experiments and simulations. The resulting regression-based terramechanics model accurately reproduced steady-state and dynamic slip, as well as sinkage behavior, on flat terrain and slopes up to 20 degrees, with validation against field test results. Additionally, improvements were made to enhance the realism of terrain deformation and wheel trace visualization. This method supports real-time applications that require physically plausible terrain response alongside high visual fidelity.

</details>


### [102] [Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain](https://arxiv.org/abs/2601.04551)
*Riku Suzuki,Ayumi Umemura,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出基于离散傅里叶变换的点云地图压缩方法，通过去除高频分量来压缩地形数据，适用于行星、沙漠等平缓地形


<details>
  <summary>Details</summary>
Motivation: SLAM技术对机器人探索任务至关重要，但点云数据量大，而机载计算能力和通信带宽有限，需要有效的压缩方法

Method: 使用离散傅里叶变换将数字高程模型转换为频域2D图像，去除高频分量，专注于平缓地形（如行星、沙漠）的探索

Result: 在两种不同高程剖面的地形上使用相机序列评估了压缩率和精度

Conclusion: 该方法能有效压缩数据大小而不会显著降低点云质量，特别适用于平缓地形环境

Abstract: Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions. While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods. To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT). The proposed technique converts the Digital Elevation Model (DEM) to the frequency-domain 2D image and omits its high-frequency components, focusing on the exploration of gradual terrains such as planets and deserts. Unlike terrains with detailed structures such as artificial environments, high-frequency components contribute little to the representation of gradual terrains. Thus, this method is effective in compressing data size without significant degradation of the point cloud. We evaluated the method in terms of compression rate and accuracy using camera sequences of two terrains with different elevation profiles.

</details>


### [103] [UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation](https://arxiv.org/abs/2601.04629)
*Zhongxuan Li,Zeliang Guo,Jun Hu,David Navarro-Alarcon,Jia Pan,Hongmin Wu,Peng Zhou*

Main category: cs.RO

TL;DR: UniBiDex是一个统一的机器人双手灵巧操作遥操作框架，支持VR和领导者-跟随者两种输入模式，通过零空间控制优化双手配置，在厨房整理任务中表现出更高的成功率、更平滑的轨迹和更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人双手灵巧操作遥操作框架通常只支持单一输入模式，缺乏统一的控制架构。需要一种能够整合异构输入设备、提供一致运动学处理和安全性保证的框架，以收集高质量的人类演示数据并加速机器人学习进展。

Method: UniBiDex框架将异构输入设备集成到共享控制堆栈中，采用零空间控制来优化双手配置，确保跨任务的平滑、无碰撞和奇异性感知运动。框架提供一致的运动学处理和安全性保证。

Result: 在包含五个顺序操作子任务的长期厨房整理任务中，UniBiDex相比强基线表现出更高的任务成功率、更平滑的轨迹和更好的鲁棒性。所有硬件和软件组件都已开源发布。

Conclusion: UniBiDex提供了一个统一的双手灵巧操作遥操作框架，能够降低收集大规模高质量人类演示数据的门槛，加速机器人学习的进展。开源发布有助于社区采用和进一步发展。

Abstract: We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees The framework employs nullspace control to optimize bimanual configurations ensuring smooth collisionfree and singularityaware motion across tasks We validate UniBiDex on a longhorizon kitchentidying task involving five sequential manipulation subtasks demonstrating higher task success rates smoother trajectories and improved robustness compared to strong baselines By releasing all hardware and software components as opensource we aim to lower the barrier to collecting largescale highquality human demonstration datasets and accelerate progress in robot learning.

</details>


### [104] [Model of Spatial Human-Agent Interaction with Consideration for Others](https://arxiv.org/abs/2601.04657)
*Takafumi Sakamoto,Yugo Takeuchi*

Main category: cs.RO

TL;DR: 构建了一个考虑他人通信意愿的计算空间交互模型，通过VR实验验证模型有效性


<details>
  <summary>Details</summary>
Motivation: 通信机器人在公共空间需要主动与人交流，但又不应该打扰行人，因此需要基于他人行为估计其通信意愿并相应调整自身行为

Method: 构建了考虑他人的计算空间交互模型，将"考虑"量化为参数：根据估计的他人内部状态调整自身内部状态的程度。在VR环境中进行人与虚拟机器人的交互实验来验证模型

Result: 当参与者向目标移动时，低考虑值的虚拟机器人会抑制参与者的移动，而高考虑值的机器人则不会。当参与者接近机器人时，无论考虑值高低，机器人都会表现出接近行为，从而减少参与者的移动

Conclusion: 实验结果验证了所提模型能够阐明考虑他人的交互行为，为机器人社交交互设计提供了理论基础

Abstract: Communication robots often need to initiate conversations with people in public spaces. At the same time, such robots must not disturb pedestrians. To handle these two requirements, an agent needs to estimate the communication desires of others based on their behavior and then adjust its own communication activities accordingly. In this study, we construct a computational spatial interaction model that considers others. Consideration is expressed as a quantitative parameter: the amount of adjustment of one's internal state to the estimated internal state of the other. To validate the model, we experimented with a human and a virtual robot interacting in a VR environment. The results show that when the participant moves to the target, a virtual robot with a low consideration value inhibits the participant's movement, while a robot with a higher consideration value did not inhibit the participant's movement. When the participant approached the robot, the robot also exhibited approaching behavior, regardless of the consideration value, thus decreasing the participant's movement. These results appear to verify the proposed model's ability to clarify interactions with consideration for others.

</details>


### [105] [Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture](https://arxiv.org/abs/2601.04668)
*Laukik Patade,Rohan Rane,Sandeep Pillai*

Main category: cs.RO

TL;DR: 该研究使用深度强化学习在连续动作空间中优化农业无人地面车辆的路径规划，通过DDPG和TD3算法在动态环境中实现95%的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统网格化方法（如A*和Dijkstra算法）在动态农业环境中存在局限性，需要自适应学习策略来应对复杂多变的农业场景。

Method: 研究采用深度强化学习技术，从DQN及其改进方法开始，然后重点研究连续动作空间模型DDPG和TD3，并在ROS和Gazebo构建的三维环境中进行实验验证。

Result: 在三维动态农业环境中，预训练的TD3智能体实现了95%的成功率，能够有效处理移动障碍物，同时确保作物和机器人的安全。

Conclusion: 连续动作空间的深度强化学习方法在农业无人地面车辆路径规划中表现出色，特别是TD3算法在动态环境中具有鲁棒性和高成功率，为精准农业提供了有效的自适应导航解决方案。

Abstract: This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra's algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.

</details>


### [106] [SeqWalker: Sequential-Horizon Vision-and-Language Navigation with Hierarchical Planning](https://arxiv.org/abs/2601.04699)
*Zebin Han,Xudong Wang,Baichen Liu,Qi Lyu,Zhenduo Shang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.RO

TL;DR: SeqWalker：基于分层规划框架的SH-VLN导航模型，通过高层规划器动态选择子指令和低层规划器探索验证策略，解决多任务导航中的信息过载问题


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言导航模型在处理复杂的多任务长时程指令时性能显著下降，信息过载损害了代理关注观察相关细节的能力

Method: 提出SeqWalker分层规划框架：1) 高层规划器基于当前视觉观察动态选择上下文相关的子指令；2) 低层规划器采用探索验证策略，利用指令的内在逻辑结构进行轨迹纠错

Result: 扩展IVLN数据集建立新基准，大量实验证明SeqWalker的优越性

Conclusion: SeqWalker通过分层规划和探索验证策略有效解决了SH-VLN中的信息过载问题，提升了多任务导航性能

Abstract: Sequential-Horizon Vision-and-Language Navigation (SH-VLN) presents a challenging scenario where agents should sequentially execute multi-task navigation guided by complex, long-horizon language instructions. Current vision-and-language navigation models exhibit significant performance degradation with such multi-task instructions, as information overload impairs the agent's ability to attend to observationally relevant details. To address this problem, we propose SeqWalker, a navigation model built on a hierarchical planning framework. Our SeqWalker features: i) A High-Level Planner that dynamically selects global instructions into contextually relevant sub-instructions based on the agent's current visual observations, thus reducing cognitive load; ii) A Low-Level Planner incorporating an Exploration-Verification strategy that leverages the inherent logical structure of instructions for trajectory error correction. To evaluate SH-VLN performance, we also extend the IVLN dataset and establish a new benchmark. Extensive experiments are performed to demonstrate the superiority of the proposed SeqWalker.

</details>


### [107] [Zero Wrench Control via Wrench Disturbance Observer for Learning-free Peg-in-hole Assembly](https://arxiv.org/abs/2601.04881)
*Kiyoung Choi,Juwon Jeong,Sehoon Oh*

Main category: cs.RO

TL;DR: 提出动态力矩扰动观测器(DW-DOB)，通过将任务空间惯性嵌入观测器名义模型，实现接触丰富操作中的高灵敏度零力矩控制。


<details>
  <summary>Details</summary>
Motivation: 传统观测器无法补偿惯性效应，导致在接触丰富任务中无法区分固有动态反应与真实外部力矩，限制了零力矩控制的精度和灵敏度。

Method: 设计动态力矩扰动观测器，将任务空间惯性嵌入观测器名义模型中，干净地分离固有动态反应与真实外部力矩，并通过基于无源性的分析确保动态条件下的稳定交互。

Result: 在工业公差(H7/h6)的孔轴配合实验中验证了方法有效性，实现了更深、更柔顺的插入，残余力矩最小，性能优于传统力矩扰动观测器和PD基线。

Conclusion: DW-DOB为接触丰富任务中的高精度零力矩控制提供了一种实用的无学习解决方案，在保持对小力/力矩敏感性的同时确保接触力矩的鲁棒调节。

Abstract: This paper proposes a Dynamic Wrench Disturbance Observer (DW-DOB) designed to achieve highly sensitive zero-wrench control in contact-rich manipulation. By embedding task-space inertia into the observer nominal model, DW-DOB cleanly separates intrinsic dynamic reactions from true external wrenches. This preserves sensitivity to small forces and moments while ensuring robust regulation of contact wrenches. A passivity-based analysis further demonstrates that DW-DOB guarantees stable interactions under dynamic conditions, addressing the shortcomings of conventional observers that fail to compensate for inertial effects. Peg-in-hole experiments at industrial tolerances (H7/h6) validate the approach, yielding deeper and more compliant insertions with minimal residual wrenches and outperforming a conventional wrench disturbance observer and a PD baseline. These results highlight DW-DOB as a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks.

</details>


### [108] [SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles](https://arxiv.org/abs/2601.04948)
*Junchi Gu,Feiyang Yuan,Weize Shi,Tianchen Huang,Haopeng Zhang,Xiaohu Zhang,Yu Wang,Wei Gao,Shiwu Zhang*

Main category: cs.RO

TL;DR: 提出配备被动轮的人形机器人进行轮滑运动，通过深度强化学习控制框架实现滑行步态，相比传统双足行走显著降低了冲击强度和运输成本


<details>
  <summary>Details</summary>
Motivation: 传统人形机器人行走和跑步时频繁的脚部撞击会产生高瞬时冲击力，导致关节磨损加剧和能量利用率差。轮滑作为一种具有重要生物力学价值的运动，能够通过合理利用身体惯性实现快速连续滑动，动能损失最小

Method: 提出一种新型人形机器人，每只脚配备一排四个被动轮用于轮滑。开发基于深度强化学习的控制框架，用于滑行步态，奖励函数设计基于轮滑的内在特性

Result: 学习到的策略首先在仿真中分析，然后部署到物理机器人上。与传统双足行走步态相比，滑行步态在冲击强度和运输成本方面表现出更好的平滑性和效率。这两个指标分别降低了75.86%和63.34%

Conclusion: 轮滑作为一种优越的运动模式，能够显著提高能量效率和延长关节寿命，为人形机器人提供了更高效、更平滑的运动方式

Abstract: Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\%$ and $63.34\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.

</details>


### [109] [When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics](https://arxiv.org/abs/2601.04982)
*Johannes A. Gaus,Winfried Ilg,Daniel Haeufle*

Main category: cs.RO

TL;DR: 提出基于校准概率的安全关键触发框架，用于日常生活活动的多模态下一动作预测，通过后处理校准使预测置信度与经验可靠性对齐，并设计ACT/HOLD规则确保仅在可靠性高时提供辅助


<details>
  <summary>Details</summary>
Motivation: 辅助设备需要在提供支持前确定用户的意图和预测的可靠性。原始模型置信度往往无法反映真实正确性，存在安全风险，需要一种安全关键的触发机制

Method: 引入基于校准概率的安全关键触发框架：1) 使用后处理校准使预测置信度与经验可靠性对齐，将误校准降低约一个数量级；2) 基于校准置信度设计简单的ACT/HOLD规则，仅在可靠性高时采取行动，否则保持等待；3) 将置信度阈值转化为辅助动作的定量安全参数

Result: 后处理校准显著减少了误校准（约一个数量级），同时不影响准确性。校准置信度驱动的ACT/HOLD规则能够实现可验证的行为，为辅助控制循环提供安全保证

Conclusion: 该框架将置信度阈值转化为定量安全参数，使辅助设备能够在可靠性高时安全地提供支持，在可靠性不足时保持等待，实现了辅助控制循环中的可验证安全行为

Abstract: Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.

</details>


### [110] [The RoboSense Challenge: Sense Anything, Navigate Anywhere, Adapt Across Platforms](https://arxiv.org/abs/2601.05014)
*Lingdong Kong,Shaoyuan Xie,Zeying Gong,Ye Li,Meng Chu,Ao Liang,Yuhao Dong,Tianshuai Hu,Ronghe Qiu,Rong Li,Hanjiang Hu,Dongyue Lu,Wei Yin,Wenhao Ding,Linfeng Li,Hang Song,Wenwei Zhang,Yuexin Ma,Junwei Liang,Zhedong Zheng,Lai Xing Ng,Benoit R. Cottereau,Wei Tsang Ooi,Ziwei Liu,Zhanpeng Zhang,Weichao Qiu,Wei Zhang,Ji Ao,Jiangpeng Zheng,Siyu Wang,Guang Yang,Zihao Zhang,Yu Zhong,Enzhu Gao,Xinhan Zheng,Xueting Wang,Shouming Li,Yunkai Gao,Siming Lan,Mingfei Han,Xing Hu,Dusan Malic,Christian Fruhwirth-Reisinger,Alexander Prutsch,Wei Lin,Samuel Schulter,Horst Possegger,Linfeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li,Youngseok Kim,Sihwan Hwang,Hyeonjun Jeong,Aodi Wu,Xubo Luo,Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao,Shuangzhi Li,Junlong Shen,Xingyu Li,Hao Ruan,Jinliang Lin,Zhiming Luo,Yu Zang,Cheng Wang,Hanshi Wang,Xijie Gong,Yixiang Yang,Qianli Ma,Zhipeng Zhang,Wenxiang Shi,Jingmeng Zhou,Weijun Zeng,Kexin Xu,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Xiyan Feng,Wenbo Zhang,Lu Zhang,Yunzhi Zhuge,Huchuan Lu,You He,Seungjun Yu,Junsung Park,Youngsun Lim,Hyunjung Shim,Faduo Liang,Zihang Wang,Yiming Peng,Guanyu Zong,Xu Li,Binghao Wang,Hao Wei,Yongxin Ma,Yunke Shi,Shuaipeng Liu,Dong Kong,Yongchun Lin,Huitong Yang,Liang Lei,Haoang Li,Xinliang Zhang,Zhiyong Wang,Xiaofeng Wang,Yuxia Fu,Yadan Luo,Djamahl Etchegaray,Yang Li,Congfei Li,Yuxiang Sun,Wenkai Zhu,Wang Xu,Linru Li,Longjie Liao,Jun Yan,Benwu Wang,Xueliang Ren,Xiaoyu Yue,Jixian Zheng,Jinfeng Wu,Shurui Qin,Wei Cong,Yao He*

Main category: cs.RO

TL;DR: RoboSense 2025挑战赛旨在推进机器人感知的鲁棒性和适应性，包含五个互补研究赛道，吸引了全球广泛参与，通过分析获胜方案揭示了方法趋势和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 自主系统在开放动态环境中部署时，感知模型需要在传感器噪声、环境变化和平台差异下保持可靠。现有方法在未见条件下性能下降，需要更鲁棒和可泛化的机器人感知。

Method: 挑战赛设计了五个互补研究赛道：语言引导决策、社会合规导航、传感器配置泛化、跨视角跨模态对应、跨平台3D感知。提供标准化数据集、基线模型和统一评估协议。

Result: 吸引了来自16个国家85个机构的143支团队参与，反映了广泛的社区参与。通过分析23个获胜方案，揭示了新兴方法趋势、共享设计原则和开放挑战。

Conclusion: RoboSense 2025为评估真实世界感知可靠性提供了全面基准，推动了构建能够在真实环境中可靠感知、鲁棒行动和跨平台适应的机器人。

Abstract: Autonomous systems are increasingly deployed in open and dynamic environments -- from city streets to aerial and indoor spaces -- where perception models must remain reliable under sensor noise, environmental variation, and platform shifts. However, even state-of-the-art methods often degrade under unseen conditions, highlighting the need for robust and generalizable robot sensing. The RoboSense 2025 Challenge is designed to advance robustness and adaptability in robot perception across diverse sensing scenarios. It unifies five complementary research tracks spanning language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view and cross-modal correspondence, and cross-platform 3D perception. Together, these tasks form a comprehensive benchmark for evaluating real-world sensing reliability under domain shifts, sensor failures, and platform discrepancies. RoboSense 2025 provides standardized datasets, baseline models, and unified evaluation protocols, enabling large-scale and reproducible comparison of robust perception methods. The challenge attracted 143 teams from 85 institutions across 16 countries, reflecting broad community engagement. By consolidating insights from 23 winning solutions, this report highlights emerging methodological trends, shared design principles, and open challenges across all tracks, marking a step toward building robots that can sense reliably, act robustly, and adapt across platforms in real-world environments.

</details>


### [111] [Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses](https://arxiv.org/abs/2601.05074)
*Julian Kulozik,Nathanaël Jarrassé*

Main category: cs.RO

TL;DR: 提出了一种基于躯干运动的补偿效应放大控制(CEAC)方法，用于上肢假肢肘关节的连续速度控制，通过放大躯干与假肢的自然耦合关系实现直观控制。


<details>
  <summary>Details</summary>
Motivation: 尽管上肢假肢设计有所进步，但实现中间关节（如手腕和肘部）的直观控制仍然具有挑战性，特别是在连续和速度调制的运动方面。需要一种能够利用用户自然补偿运动的新控制范式。

Method: 提出补偿效应放大控制(CEAC)方法，利用躯干的屈伸作为输入来控制假肢肘关节速度。该方法放大躯干与假肢之间的自然耦合，同时引入受控延迟，允许用户同时调节假肢关节的位置和速度。

Result: 在12名健全参与者使用具有主动肘关节的超肢假肢进行的通用绘图任务中，以及10名参与者的多目标到达任务中，结果显示任务性能与自然手臂运动相当，即使手势速度或绘图尺寸变化时也能保持符合人体工学的躯干姿势。

Conclusion: CEAC有效恢复了关节协调动作，在躯干和肘部之间分配运动努力，实现了直观的轨迹控制而无需极端补偿运动。为需要连续精确协调的任务中的上肢假肢中间关节控制提供了有前景的策略。

Abstract: Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.

</details>


### [112] [Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration](https://arxiv.org/abs/2601.05243)
*Xingyi He,Adhitya Polavaram,Yunhao Cao,Om Deshmukh,Tianrui Wang,Xiaowei Zhou,Kuan Fang*

Main category: cs.RO

TL;DR: CorDex框架从单个人类演示生成合成数据，学习灵巧机械手的功能性抓取，通过对应关系数据引擎和多模态预测网络实现对新物体的泛化。


<details>
  <summary>Details</summary>
Motivation: 灵巧机械手的功能性抓取是实现工具使用和复杂操作的关键能力，但受到两个瓶颈限制：大规模数据集的稀缺性，以及学习模型中缺乏语义和几何推理的整合。

Method: 1. 基于对应关系的数据引擎：从单个人类演示生成多样化的合成训练数据，通过对应关系估计将专家抓取转移到生成物体上，并通过优化调整抓取。2. 多模态预测网络：整合视觉和几何信息，设计局部-全局融合模块和重要性感知采样机制，实现稳健且计算高效的功能性灵巧抓取预测。

Result: 通过跨多个物体类别的广泛实验，CorDex在未见过的物体实例上表现出良好的泛化能力，并显著优于最先进的基线方法。

Conclusion: CorDex框架通过从单个人类演示生成合成数据，有效解决了灵巧功能性抓取的数据稀缺问题，并整合了语义和几何推理，实现了对新物体的稳健抓取泛化。

Abstract: Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.

</details>


### [113] [LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model](https://arxiv.org/abs/2601.05248)
*Zhuoyang Liu,Jiaming Liu,Hao Chen,Ziyu Guo,Chengkai Hou,Chenyang Gu,Jiale Yu,Xiangju Mi,Renrui Zhang,Zhengping Che,Jian Tang,Pheng-Ann Heng,Shanghang Zhang*

Main category: cs.RO

TL;DR: LaST₀提出了一种通过潜在时空思维链进行高效推理的VLA框架，解决了现有方法推理延迟高和语言表示瓶颈的问题，在机器人操作任务中实现了更高的成功率和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法通过显式生成语言推理轨迹或未来视觉观察来提高动作准确性，但存在两个主要问题：1）显式推理带来不可忽视的推理延迟，限制了机器人操作所需的时间分辨率；2）推理局限于语言空间，形成表示瓶颈，难以准确捕捉难以言表的物理属性。

Method: 提出LaST₀框架，通过潜在时空思维链进行高效推理。具体包括：1）引入令牌高效的潜在CoT空间，建模未来视觉动态、3D结构信息和机器人本体感受状态，并跨时间扩展这些表示以支持时间一致的隐式推理轨迹；2）采用混合Transformer的双系统架构，推理专家进行低频潜在推理，动作专家基于机器人导向的潜在表示生成高频动作；3）通过异构操作频率训练，实现推理和动作推理速率之间的自适应切换。

Result: 在10个模拟和6个真实世界操作任务中，LaST₀相比先前VLA方法分别提高了8%和13%的平均成功率，同时实现了显著更快的推理速度。

Conclusion: LaST₀通过潜在时空思维链解决了VLA模型中的推理延迟和表示瓶颈问题，在保持高效推理的同时显著提升了机器人操作性能，为机器人学习提供了更有效的推理-动作协调框架。

Abstract: Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0

</details>
