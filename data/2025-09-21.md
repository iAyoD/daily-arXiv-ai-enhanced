<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 48]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [AEGIS: Automated Error Generation and Identification for Multi-Agent Systems](https://arxiv.org/abs/2509.14295)
*Fanqi Kong,Ruijie Zhang,Huaxiao Yin,Guibin Zhang,Xiaofei Zhang,Ziang Chen,Zhaowei Zhang,Xiaoyuan Zhang,Song-Chun Zhu,Xue Feng*

Main category: cs.RO

TL;DR: AEGIS是一个自动化错误生成和识别框架，通过向成功的多智能体系统轨迹中注入可控错误来创建大规模错误数据集，支持监督微调、强化学习和对比学习三种范式，显著提升错误识别性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统日益自主和复杂，但缺乏大规模、多样化的错误标注数据集，严重阻碍了系统可靠性和安全性研究。

Method: 使用基于LLM的上下文感知自适应操纵器，通过提示注入和响应破坏等复杂攻击方式，在成功轨迹中系统性地注入可控和可追踪的错误。

Result: 在三种学习范式下，使用AEGIS数据训练的模型都取得了显著改进，部分微调模型的性能可与大一个数量级的专有系统相媲美甚至更优。

Conclusion: AEGIS自动化数据生成框架是开发更鲁棒和可解释多智能体系统的关键资源，验证了自动化错误生成方法的有效性。

Abstract: As Multi-Agent Systems (MAS) become increasingly autonomous and complex,
understanding their error modes is critical for ensuring their reliability and
safety. However, research in this area has been severely hampered by the lack
of large-scale, diverse datasets with precise, ground-truth error labels. To
address this bottleneck, we introduce \textbf{AEGIS}, a novel framework for
\textbf{A}utomated \textbf{E}rror \textbf{G}eneration and
\textbf{I}dentification for Multi-Agent \textbf{S}ystems. By systematically
injecting controllable and traceable errors into initially successful
trajectories, we create a rich dataset of realistic failures. This is achieved
using a context-aware, LLM-based adaptive manipulator that performs
sophisticated attacks like prompt injection and response corruption to induce
specific, predefined error modes. We demonstrate the value of our dataset by
exploring three distinct learning paradigms for the error identification task:
Supervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our
comprehensive experiments show that models trained on AEGIS data achieve
substantial improvements across all three learning paradigms. Notably, several
of our fine-tuned models demonstrate performance competitive with or superior
to proprietary systems an order of magnitude larger, validating our automated
data generation framework as a crucial resource for developing more robust and
interpretable multi-agent systems. Our project website is available at
https://kfq20.github.io/AEGIS-Website.

</details>


### [2] [FlowDrive: Energy Flow Field for End-to-End Autonomous Driving](https://arxiv.org/abs/2509.14303)
*Hao Jiang,Zhipeng Zhang,Yu Gao,Zhigang Sun,Yiru Wang,Yuwen Heng,Shuo Wang,Jinhao Chai,Zhuo Chen,Hao Zhao,Hao Sun,Xi Zhang,Anqing Jiang,Chuan Hu*

Main category: cs.RO

TL;DR: FlowDrive是一个端到端自动驾驶框架，通过引入物理可解释的能量场（风险势场和车道吸引场）来增强BEV表示，实现安全可解释的运动规划


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶框架缺乏对风险和引导先验的显式建模，无法同时处理几何障碍物硬约束和无显式几何的规则语义软约束

Method: 提出基于物理可解释的能量流场（风险势场和车道吸引场）来编码语义先验和安全提示；采用条件扩散规划器通过特征级门控将运动意图预测与轨迹去噪解耦

Result: 在NAVSIM v2基准测试中达到86.3的EPDMS分数，在安全性和规划质量方面均超越先前基线方法

Conclusion: FlowDrive通过引入可解释的流场特征和任务解耦机制，显著提升了自动驾驶运动规划的性能和可解释性

Abstract: Recent advances in end-to-end autonomous driving leverage multi-view images
to construct BEV representations for motion planning. In motion planning,
autonomous vehicles need considering both hard constraints imposed by
geometrically occupied obstacles (e.g., vehicles, pedestrians) and soft,
rule-based semantics with no explicit geometry (e.g., lane boundaries, traffic
priors). However, existing end-to-end frameworks typically rely on BEV features
learned in an implicit manner, lacking explicit modeling of risk and guidance
priors for safe and interpretable planning. To address this, we propose
FlowDrive, a novel framework that introduces physically interpretable
energy-based flow fields-including risk potential and lane attraction fields-to
encode semantic priors and safety cues into the BEV space. These flow-aware
features enable adaptive refinement of anchor trajectories and serve as
interpretable guidance for trajectory generation. Moreover, FlowDrive decouples
motion intent prediction from trajectory denoising via a conditional diffusion
planner with feature-level gating, alleviating task interference and enhancing
multimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that
FlowDrive achieves state-of-the-art performance with an EPDMS of 86.3,
surpassing prior baselines in both safety and planning quality. The project is
available at https://astrixdrive.github.io/FlowDrive.github.io/.

</details>


### [3] [Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move](https://arxiv.org/abs/2509.14342)
*Bikram Pandit,Aayam Kumar Shrestha,Alan Fern*

Main category: cs.RO

TL;DR: 本文提出了一种去中心化多足机器人协同运输方法，通过分层策略架构和星座奖励设计，使独立机器人仅通过接触力协调运输不可抓取物体，无需通信或集中控制。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖机械刚性连接的局限性，探索在无通信和集中控制条件下，独立机器人仅通过接触力协调运输不可抓取物体的挑战性问题。

Method: 采用分层策略架构分离基座移动和手臂控制，提出星座奖励公式统一位置和方向跟踪以强制执行刚性接触行为，通过精心设计的奖励和训练课程使机器人表现得如同刚性连接。

Result: 在2-10个机器人规模上展示了鲁棒的运输性能，适用于不同几何形状和质量的物体，并实现了从仿真到轻量级物体的实际转移。

Conclusion: 该方法通过共享策略参数和隐式同步线索实现协调，可扩展到任意团队规模而无需重新训练，为去中心化协同运输提供了有效解决方案。

Abstract: We study decentralized cooperative transport using teams of N-quadruped
robots with arm that must pinch, lift, and move ungraspable objects through
physical contact alone. Unlike prior work that relies on rigid mechanical
coupling between robots and objects, we address the more challenging setting
where mechanically independent robots must coordinate through contact forces
alone without any communication or centralized control. To this end, we employ
a hierarchical policy architecture that separates base locomotion from arm
control, and propose a constellation reward formulation that unifies position
and orientation tracking to enforce rigid contact behavior. The key insight is
encouraging robots to behave as if rigidly connected to the object through
careful reward design and training curriculum rather than explicit mechanical
constraints. Our approach enables coordination through shared policy parameters
and implicit synchronization cues - scaling to arbitrary team sizes without
retraining. We show extensive simulation experiments to demonstrate robust
transport across 2-10 robots on diverse object geometries and masses, along
with sim2real transfer results on lightweight objects.

</details>


### [4] [LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation](https://arxiv.org/abs/2509.14349)
*Zhengyang Kris Weng,Matthew L. Elwin,Han Liu*

Main category: cs.RO

TL;DR: LeVR是一个模块化软件框架，通过VR遥操作解决机器人模仿学习中的数据收集难题，并与LeRobot框架原生集成，提供从数据采集到策略部署的端到端工作流。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模仿学习中两个关键问题：1）为配备灵巧手的机械臂提供稳健直观的VR遥操作数据收集；2）与现有模仿学习框架的无缝集成，简化演示数据收集流程。

Method: 开发LeVR模块化框架，提供VR遥操作接口，与LeRobot模仿学习框架原生集成。发布LeFranX开源实现，支持Franka FER机械臂和RobotEra XHand灵巧手，实现端到端工作流。

Result: 成功收集了包含100个专家演示的公开数据集，并用其微调了最先进的视觉运动策略，验证了系统的有效性。

Conclusion: LeVR框架有效解决了机器人模仿学习中的数据收集挑战，通过开源框架、实现和数据集加速机器人社区的模仿学习研究。

Abstract: We introduce LeVR, a modular software framework designed to bridge two
critical gaps in robotic imitation learning. First, it provides robust and
intuitive virtual reality (VR) teleoperation for data collection using robot
arms paired with dexterous hands, addressing a common limitation in existing
systems. Second, it natively integrates with the powerful LeRobot imitation
learning (IL) framework, enabling the use of VR-based teleoperation data and
streamlining the demonstration collection process. To demonstrate LeVR, we
release LeFranX, an open-source implementation for the Franka FER arm and
RobotEra XHand, two widely used research platforms. LeFranX delivers a
seamless, end-to-end workflow from data collection to real-world policy
deployment. We validate our system by collecting a public dataset of 100 expert
demonstrations and use it to successfully fine-tune state-of-the-art visuomotor
policies. We provide our open-source framework, implementation, and dataset to
accelerate IL research for the robotics community.

</details>


### [5] [DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion](https://arxiv.org/abs/2509.14353)
*Dvij Kalaria,Sudarshan S Harithas,Pushkal Katara,Sangkyung Kwak,Sarthak Bhagat,Shankar Sastry,Srinath Sridhar,Sai Vemprala,Ashish Kapoor,Jonathan Chung-Kuan Huang*

Main category: cs.RO

TL;DR: DreamControl结合扩散模型和强化学习，利用人类运动数据训练的扩散先验来指导RL策略，实现人形机器人的全身自主技能学习


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人同时控制上下半身和物体交互的挑战性任务，通过人类运动先验指导RL策略，发现直接RL无法获得的解决方案

Method: 使用人类运动数据训练扩散先验模型，然后在仿真环境中用该先验指导强化学习策略完成特定任务

Result: 在Unitree G1机器人上验证了方法的有效性，能够完成涉及上下半身同时控制和物体交互的多样化挑战性任务

Conclusion: 人类运动信息的扩散先验使RL能够发现直接RL无法获得的解决方案，扩散模型固有地促进自然动作，有助于仿真到现实的迁移

Abstract: We introduce DreamControl, a novel methodology for learning autonomous
whole-body humanoid skills. DreamControl leverages the strengths of diffusion
models and Reinforcement Learning (RL): our core innovation is the use of a
diffusion prior trained on human motion data, which subsequently guides an RL
policy in simulation to complete specific tasks of interest (e.g., opening a
drawer or picking up an object). We demonstrate that this human motion-informed
prior allows RL to discover solutions unattainable by direct RL, and that
diffusion models inherently promote natural looking motions, aiding in
sim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1
robot across a diverse set of challenging tasks involving simultaneous lower
and upper body control and object interaction.

</details>


### [6] [CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks](https://arxiv.org/abs/2509.14380)
*Seoyeon Choi,Kanghyun Ryu,Jonghoon Ock,Negar Mehr*

Main category: cs.RO

TL;DR: CRAFT是一个利用基础模型作为"教练"的多机器人协调框架，通过LLM自动分解长时程任务为子任务序列，使用LLM生成奖励函数，并通过VLM引导的奖励精炼循环来训练复杂协调行为。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习在机器人应用中面临高维连续动作空间、复杂奖励设计和非平稳转换等挑战，而人类通过分阶段课程学习复杂协调行为，这启发了利用基础模型作为教练的框架设计。

Method: 使用大型语言模型(LLM)的规划能力自动分解长时程协调任务为子任务序列，通过LLM生成每个子任务的奖励函数，并利用视觉语言模型(VLM)引导的奖励精炼循环进行优化训练。

Result: 在多四足机器人导航和双手操作任务上验证了CRAFT学习复杂协调行为的能力，并在真实硬件实验中验证了多四足机器人导航策略的有效性。

Conclusion: CRAFT框架成功展示了利用基础模型的推理能力作为多机器人协调"教练"的可行性，能够自动分解任务、生成奖励并精炼训练过程，为复杂多机器人协调任务提供了有效的解决方案。

Abstract: Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for
learning coordination in multi-agent systems. However, applying MARL to
robotics still remains challenging due to high-dimensional continuous joint
action spaces, complex reward design, and non-stationary transitions inherent
to decentralized settings. On the other hand, humans learn complex coordination
through staged curricula, where long-horizon behaviors are progressively built
upon simpler skills. Motivated by this, we propose CRAFT: Coaching
Reinforcement learning Autonomously using Foundation models for multi-robot
coordination Tasks, a framework that leverages the reasoning capabilities of
foundation models to act as a "coach" for multi-robot coordination. CRAFT
automatically decomposes long-horizon coordination tasks into sequences of
subtasks using the planning capability of Large Language Models (LLMs). In what
follows, CRAFT trains each subtask using reward functions generated by LLM, and
refines them through a Vision Language Model (VLM)-guided reward-refinement
loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation
tasks, demonstrating its capability to learn complex coordination behaviors. In
addition, we validate the multi-quadruped navigation policy in real hardware
experiments.

</details>


### [7] [RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings](https://arxiv.org/abs/2509.14383)
*Yuhong Lu*

Main category: cs.RO

TL;DR: RLBind是一个两阶段对抗不变性跨模态对齐框架，通过在干净-对抗样本对上进行无监督微调来强化视觉编码器，并利用跨模态对应关系最小化特征与文本锚点的差异，实现鲁棒的统一多模态嵌入。


<details>
  <summary>Details</summary>
Motivation: 机器人部署中视觉分支面临对抗性和自然损坏，现有防御方法通常只在CLIP风格编码器中对齐干净和对抗特征，忽视了更广泛的跨模态对应关系，导致效果有限且可能损害零样本迁移能力。

Method: 两阶段框架：第一阶段在干净-对抗样本对上进行无监督微调强化视觉编码器；第二阶段利用跨模态对应关系，最小化干净/对抗特征与文本锚点的差异，同时强制跨模态的类间分布对齐。

Result: 在图像、音频、热成像和视频数据上的广泛实验表明，RLBind在干净准确率和有界对抗鲁棒性方面持续优于LanguageBind主干和标准微调基线。

Conclusion: RLBind在不牺牲泛化能力的情况下提高了鲁棒性，为具身机器人在导航、操作等自主场景中提供了更安全的多传感器感知堆栈实用路径。

Abstract: Unified multi-modal encoders that bind vision, audio, and other sensors into
a shared embedding space are attractive building blocks for robot perception
and decision-making. However, on-robot deployment exposes the vision branch to
adversarial and natural corruptions, making robustness a prerequisite for
safety. Prior defenses typically align clean and adversarial features within
CLIP-style encoders and overlook broader cross-modal correspondence, yielding
modest gains and often degrading zero-shot transfer. We introduce RLBind, a
two-stage adversarial-invariant cross-modal alignment framework for robust
unified embeddings. Stage 1 performs unsupervised fine-tuning on
clean-adversarial pairs to harden the visual encoder. Stage 2 leverages
cross-modal correspondence by minimizing the discrepancy between
clean/adversarial features and a text anchor, while enforcing class-wise
distributional alignment across modalities. Extensive experiments on Image,
Audio, Thermal, and Video data show that RLBind consistently outperforms the
LanguageBind backbone and standard fine-tuning baselines in both clean accuracy
and norm-bounded adversarial robustness. By improving resilience without
sacrificing generalization, RLBind provides a practical path toward safer
multi-sensor perception stacks for embodied robots in navigation, manipulation,
and other autonomy settings.

</details>


### [8] [GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot](https://arxiv.org/abs/2509.14412)
*Artem Lykov,Oleg Kobzarev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: GestOS是一个基于手势的操作系统，通过语义理解手势意图并利用大语言模型动态分配任务给异构机器人团队，实现无需明确指定目标的自适应控制。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常将手势映射为固定命令或单智能体动作，缺乏对异构机器人团队的语义理解和动态任务分配能力，需要更智能的手势交互系统来实现灵活协作。

Method: 结合轻量级视觉感知和LLM推理：将手部姿态转换为结构化文本描述，LLM推断意图并生成机器人特定命令，实时匹配最适合的机器人执行任务。

Result: 实现了上下文感知的自适应控制，无需用户明确指定目标或命令，支持在动态环境中与机器人系统进行可扩展、灵活和用户友好的协作。

Conclusion: GestOS将手势交互从识别推进到智能编排，为异构机器人团队提供了语义理解和动态任务分配的新范式，提升了人机协作的效率和自然性。

Abstract: We present GestOS, a gesture-based operating system for high-level control of
heterogeneous robot teams. Unlike prior systems that map gestures to fixed
commands or single-agent actions, GestOS interprets hand gestures semantically
and dynamically distributes tasks across multiple robots based on their
capabilities, current state, and supported instruction sets. The system
combines lightweight visual perception with large language model (LLM)
reasoning: hand poses are converted into structured textual descriptions, which
the LLM uses to infer intent and generate robot-specific commands. A robot
selection module ensures that each gesture-triggered task is matched to the
most suitable agent in real time. This architecture enables context-aware,
adaptive control without requiring explicit user specification of targets or
commands. By advancing gesture interaction from recognition to intelligent
orchestration, GestOS supports scalable, flexible, and user-friendly
collaboration with robotic systems in dynamic environments.

</details>


### [9] [Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting](https://arxiv.org/abs/2509.14421)
*Dario Tscholl,Yashwanth Nakka,Brian Gunter*

Main category: cs.RO

TL;DR: 提出基于3D高斯泼溅的感知驱动安全滤波器，将每个3DGS转换为闭式前向碰撞锥，生成一阶控制屏障函数，实现高效、主动的避障规划


<details>
  <summary>Details</summary>
Motivation: 传统基于距离的CBF方法反应滞后，只在障碍物接近时才激活，导致避障动作不够平滑且计算成本高。需要一种能够提前预警、计算高效的安全规划方法

Method: 利用3D高斯泼溅的解析几何特性，将每个splat转换为闭式前向碰撞锥，构建一阶CBF并嵌入二次规划问题中。通过Minkowski和膨胀处理机器人物理尺寸

Result: 在包含17万个splat的大规模合成场景中，规划时间减少3倍，轨迹抖动显著降低，同时保持相同安全水平

Conclusion: 该方法完全解析、无需高阶CBF扩展，计算高效且能主动避障，适用于空间机器人、卫星系统等实时导航场景

Abstract: We present a perception-driven safety filter that converts each 3D Gaussian
Splat (3DGS) into a closed-form forward collision cone, which in turn yields a
first-order control barrier function (CBF) embedded within a quadratic program
(QP). By exploiting the analytic geometry of splats, our formulation provides a
continuous, closed-form representation of collision constraints that is both
simple and computationally efficient. Unlike distance-based CBFs, which tend to
activate reactively only when an obstacle is already close, our collision-cone
CBF activates proactively, allowing the robot to adjust earlier and thereby
produce smoother and safer avoidance maneuvers at lower computational cost. We
validate the method on a large synthetic scene with approximately 170k splats,
where our filter reduces planning time by a factor of 3 and significantly
decreased trajectory jerk compared to a state-of-the-art 3DGS planner, while
maintaining the same level of safety. The approach is entirely analytic,
requires no high-order CBF extensions (HOCBFs), and generalizes naturally to
robots with physical extent through a principled Minkowski-sum inflation of the
splats. These properties make the method broadly applicable to real-time
navigation in cluttered, perception-derived extreme environments, including
space robotics and satellite systems.

</details>


### [10] [Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control](https://arxiv.org/abs/2509.14431)
*Keqin Wang,Tao Zhong,David Chang,Christine Allen-Blanchette*

Main category: cs.RO

TL;DR: 提出了LEGO框架，通过图神经网络实现置换等变性和E(n)-等变性，解决多智能体强化学习中的训练稳定性、对抗性条件下鲁棒性和泛化性问题。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习在竞争性场景中同时适应导致训练不稳定、非动能对抗措施在不利条件下失效、以及策略在不同智能体数量环境中泛化能力差的问题。

Method: 提出Local-Canonicalization Equivariant Graph Neural Networks (LEGO)框架，集成图神经网络捕捉置换等变性和泛化能力，使用规范化实现E(n)-等变性，采用异构表示编码角色特定的归纳偏置。

Result: 在合作和竞争性群体基准测试中，LEGO优于强基线并提高了泛化能力。在真实世界实验中，LEGO展现出对不同团队规模和智能体故障的鲁棒性。

Conclusion: LEGO框架有效解决了MARL中的关键挑战，提供了更好的训练稳定性、鲁棒性和泛化性能，适用于复杂决策场景中的智能体协调。

Abstract: Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm
for coordinating swarms of agents in complex decision-making, yet major
challenges remain. In competitive settings such as pursuer-evader tasks,
simultaneous adaptation can destabilize training; non-kinetic countermeasures
often fail under adverse conditions; and policies trained in one configuration
rarely generalize to environments with a different number of agents. To address
these issues, we propose the Local-Canonicalization Equivariant Graph Neural
Networks (LEGO) framework, which integrates seamlessly with popular MARL
algorithms such as MAPPO. LEGO employs graph neural networks to capture
permutation equivariance and generalization to different agent numbers,
canonicalization to enforce E(n)-equivariance, and heterogeneous
representations to encode role-specific inductive biases. Experiments on
cooperative and competitive swarm benchmarks show that LEGO outperforms strong
baselines and improves generalization. In real-world experiments, LEGO
demonstrates robustness to varying team sizes and agent failure.

</details>


### [11] [Online Learning of Deceptive Policies under Intermittent Observation](https://arxiv.org/abs/2509.14453)
*Gokul Puthumanaillam,Ram Padmanabhan,Jose Fuentes,Nicole Cruz,Paulo Padrao,Ruben Hernandez,Hao Jiang,William Schafer,Leonardo Bobadilla,Melkior Ornik*

Main category: cs.RO

TL;DR: 本文研究在间歇性监控环境下，智能体如何通过心智理论实现欺骗性行为，在追求私人目标的同时保持对监督者参考策略的表面合规性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的监督控制往往不是连续监控，而是在已知边界内的零星观察。受人类监督者行为的启发，研究如何在这样的环境下实现欺骗性合规行为。

Method: 利用心智理论建模监督者的期望，从中提取一个标量指标——预期偏差证据，该指标结合了参考策略与当前动作分布的差异程度以及智能体对即将发生观察的信念。将此标量作为状态依赖权重注入到在线强化学习的KL正则化策略改进步骤中。

Result: 在海洋(ASV)和空中(UAV)导航的真实硬件实验中，ToM引导的强化学习方法能够在线运行，实现高回报和成功率，同时观察轨迹证据与监督者期望保持校准。

Conclusion: 心智理论可以重新用于指导在线强化学习实现欺骗性行为，通过单一校准标量平滑权衡自身利益与合规性，避免了手工或启发式策略的设计。

Abstract: In supervisory control settings, autonomous systems are not monitored
continuously. Instead, monitoring often occurs at sporadic intervals within
known bounds. We study the problem of deception, where an agent pursues a
private objective while remaining plausibly compliant with a supervisor's
reference policy when observations occur. Motivated by the behavior of real,
human supervisors, we situate the problem within Theory of Mind: the
representation of what an observer believes and expects to see. We show that
Theory of Mind can be repurposed to steer online reinforcement learning (RL)
toward such deceptive behavior. We model the supervisor's expectations and
distill from them a single, calibrated scalar -- the expected evidence of
deviation if an observation were to happen now. This scalar combines how unlike
the reference and current action distributions appear, with the agent's belief
that an observation is imminent. Injected as a state-dependent weight into a
KL-regularized policy improvement step within an online RL loop, this scalar
informs a closed-form update that smoothly trades off self-interest and
compliance, thus sidestepping hand-crafted or heuristic policies. In
real-world, real-time hardware experiments on marine (ASV) and aerial (UAV)
navigation, our ToM-guided RL runs online, achieves high return and success
with observed-trace evidence calibrated to the supervisor's expectations.

</details>


### [12] [Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring](https://arxiv.org/abs/2509.14460)
*Abhiroop Ajith,Constantinos Chamzas*

Main category: cs.RO

TL;DR: 提出了一种从视觉数据自动发现离散图结构抽象的方法，用于机器人重排任务规划，结合结构约束和注意力引导的视觉距离来识别有意义的抽象表示。


<details>
  <summary>Details</summary>
Motivation: 人类能够自然地使用抽象层次进行推理，而机器人领域的抽象通常需要人工设计，限制了可扩展性和实际应用。需要从视觉数据自动发现有用抽象来提升规划框架的扩展性和实用性。

Method: 结合结构约束和注意力引导的视觉距离，利用重排问题固有的二分图结构，将结构约束和视觉嵌入整合到统一框架中，从纯视觉输入自主发现抽象表示。

Result: 在仿真环境中的两个重排任务上评估，该方法能够一致地识别出有意义的抽象表示，支持高效的高层规划，性能优于现有方法。

Conclusion: 该方法成功实现了从视觉数据自动发现离散图结构抽象，为机器人重排任务提供了有效的抽象表示支持高层规划，展示了在复杂环境中自动化抽象发现的可行性。

Abstract: Learning abstractions directly from data is a core challenge in robotics.
Humans naturally operate at an abstract level, reasoning over high-level
subgoals while delegating execution to low-level motor skills -- an ability
that enables efficient problem solving in complex environments. In robotics,
abstractions and hierarchical reasoning have long been central to planning, yet
they are typically hand-engineered, demanding significant human effort and
limiting scalability. Automating the discovery of useful abstractions directly
from visual data would make planning frameworks more scalable and more
applicable to real-world robotic domains. In this work, we focus on
rearrangement tasks where the state is represented with raw images, and propose
a method to induce discrete, graph-structured abstractions by combining
structural constraints with an attention-guided visual distance. Our approach
leverages the inherent bipartite structure of rearrangement problems,
integrating structural constraints and visual embeddings into a unified
framework. This enables the autonomous discovery of abstractions from vision
alone, which can subsequently support high-level planning. We evaluate our
method on two rearrangement tasks in simulation and show that it consistently
identifies meaningful abstractions that facilitate effective planning and
outperform existing approaches.

</details>


### [13] [Object Recognition and Force Estimation with the GelSight Baby Fin Ray](https://arxiv.org/abs/2509.14510)
*Sandra Q. Liu,Yuxiang Ma,Edward H. Adelson*

Main category: cs.RO

TL;DR: 本文介绍了一种基于相机触觉传感的软体机器人手GelSight Baby Fin Ray，通过机器学习方法实现了坚果壳纹理识别、力与位置估计，并比较了不同神经网络结构的性能。


<details>
  <summary>Details</summary>
Motivation: 软体机器人手和触觉传感技术的进步使其能够执行更复杂的任务，GelSight Baby Fin Ray结合了相机触觉传感和软体Fin Ray结构，能够捕获丰富的接触信息，需要进一步探索其潜力。

Method: 使用机器学习方法，包括ResNet50、GoogLeNet以及3层和5层卷积神经网络结构，进行纹理识别、力与位置估计的消融研究。

Result: 研究表明机器学习能够从高分辨率触觉图像中有效提取有用信息，不同神经网络结构在性能上有所差异。

Conclusion: 机器学习是从触觉图像中提取信息的有前景技术，能够增强软体机器人对环境的理解和交互能力。

Abstract: Recent advances in soft robotic hands and tactile sensing have enabled both
to perform an increasing number of complex tasks with the aid of machine
learning. In particular, we presented the GelSight Baby Fin Ray in our previous
work, which integrates a camera with a soft, compliant Fin Ray structure.
Camera-based tactile sensing gives the GelSight Baby Fin Ray the ability to
capture rich contact information like forces, object geometries, and textures.
Moreover, our previous work showed that the GelSight Baby Fin Ray can dig
through clutter, and classify in-shell nuts. To further examine the potential
of the GelSight Baby Fin Ray, we leverage learning to distinguish nut-in-shell
textures and to perform force and position estimation. We implement ablation
studies with popular neural network structures, including ResNet50, GoogLeNet,
and 3- and 5-layer convolutional neural network (CNN) structures. We conclude
that machine learning is a promising technique to extract useful information
from high-resolution tactile images and empower soft robotics to better
understand and interact with the environments.

</details>


### [14] [Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods](https://arxiv.org/abs/2509.14516)
*Adam D. Hines,Alejandro Fontan,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: Event-LAB是一个统一框架，用于在多个数据集上运行事件相机定位方法，解决了该领域代码依赖和数据格式多样化的挑战。


<details>
  <summary>Details</summary>
Motivation: 事件相机定位研究快速增长，但代码依赖和数据格式多样化使得方法比较困难且实现复杂，需要一个统一框架来简化研究流程。

Method: 使用Pixi包和依赖管理器实现Event-LAB框架，支持单命令行安装和调用，实现了视觉位置识别(VPR)和同步定位与建图(SLAM)两种常见管道。

Result: 框架能够系统可视化分析多个方法和数据集的结果，揭示了事件收集计数和帧生成窗口大小等参数对性能的重要影响。

Conclusion: Event-LAB为研究社区提供了公平比较方法的能力，通过一致的参数设置简化了多条件实验设置流程。

Abstract: Event-based localization research and datasets are a rapidly growing area of
interest, with a tenfold increase in the cumulative total number of published
papers on this topic over the past 10 years. Whilst the rapid expansion in the
field is exciting, it brings with it an associated challenge: a growth in the
variety of required code and package dependencies as well as data formats,
making comparisons difficult and cumbersome for researchers to implement
reliably. To address this challenge, we present Event-LAB: a new and unified
framework for running several event-based localization methodologies across
multiple datasets. Event-LAB is implemented using the Pixi package and
dependency manager, that enables a single command-line installation and
invocation for combinations of localization methods and datasets. To
demonstrate the capabilities of the framework, we implement two common
event-based localization pipelines: Visual Place Recognition (VPR) and
Simultaneous Localization and Mapping (SLAM). We demonstrate the ability of the
framework to systematically visualize and analyze the results of multiple
methods and datasets, revealing key insights such as the association of
parameters that control event collection counts and window sizes for frame
generation to large variations in performance. The results and analysis
demonstrate the importance of fairly comparing methodologies with consistent
event image generation parameters. Our Event-LAB framework provides this
ability for the research community, by contributing a streamlined workflow for
easily setting up multiple conditions.

</details>


### [15] [Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking](https://arxiv.org/abs/2509.14530)
*Zhenghao Fei,Wenwu Lu,Linsheng Hou,Chen Peng*

Main category: cs.RO

TL;DR: 提出了一种基于人类演示学习的草莓采摘机器人系统，使用4自由度SCARA机械臂和改进的End Pose Assisted Action Chunking Transformer算法，在遮挡场景下显著优于直接ACT实现。


<details>
  <summary>Details</summary>
Motivation: 草莓自然生长在簇群中，经常被叶片、茎秆和其他果实遮挡，传统感知-规划-控制系统难以在杂乱环境中采摘被遮挡的草莓，需要灵巧操作来绕过或轻柔移动周围柔软物体。

Method: 采用4自由度SCARA机械臂配合人类遥操作接口进行高效数据收集，利用改进的End Pose Assisted Action Chunking Transformer (ACT)算法开发精细的视觉运动采摘策略。

Result: 在各种遮挡场景下的实验表明，改进方法显著优于直接ACT实现，证明了其在遮挡草莓采摘实际应用中的潜力。

Conclusion: 基于人类演示学习的机器人系统能够有效解决草莓采摘中的遮挡问题，改进的ACT算法在复杂环境中表现出优越性能，具有实际应用价值。

Abstract: Strawberries naturally grow in clusters, interwoven with leaves, stems, and
other fruits, which frequently leads to occlusion. This inherent growth habit
presents a significant challenge for robotic picking, as traditional
percept-plan-control systems struggle to reach fruits amid the clutter.
Effectively picking an occluded strawberry demands dexterous manipulation to
carefully bypass or gently move the surrounding soft objects and precisely
access the ideal picking point located at the stem just above the calyx. To
address this challenge, we introduce a strawberry-picking robotic system that
learns from human demonstrations. Our system features a 4-DoF SCARA arm paired
with a human teleoperation interface for efficient data collection and
leverages an End Pose Assisted Action Chunking Transformer (ACT) to develop a
fine-grained visuomotor picking policy. Experiments under various occlusion
scenarios demonstrate that our modified approach significantly outperforms the
direct implementation of ACT, underscoring its potential for practical
application in occluded strawberry picking.

</details>


### [16] [Dual-Arm Hierarchical Planning for Laboratory Automation: Vibratory Sieve Shaker Operations](https://arxiv.org/abs/2509.14531)
*Haoran Xiao,Xue Wang,Huimin Lu,Zhiwen Zeng,Zirui Guo,Ziqi Ni,Yicong Ye,Wei Dai*

Main category: cs.RO

TL;DR: 提出分层规划框架解决振动筛自动化操作中的狭窄空间双臂操作、双手交接和受限容器递送等挑战，显著提升规划效率和路径质量


<details>
  <summary>Details</summary>
Motivation: 解决材料实验室振动筛自动化操作中的三个关键挑战：狭窄空间的双臂盖操作、重叠工作空间的双手交接、以及有方向约束的受阻粉末容器递送

Method: 采用分层规划框架，结合先验引导路径规划和多步轨迹优化。前者使用有限高斯混合模型提高狭窄通道采样效率，后者通过路径缩短、简化、关节约束和B样条平滑来优化路径

Result: 规划时间减少80.4%，路径点减少89.4%，并在物理实验中成功完成完整的振动筛操作流程

Conclusion: 该框架有效解决了复杂实验室自动化中的操作挑战，具有实际应用价值

Abstract: This paper addresses the challenges of automating vibratory sieve shaker
operations in a materials laboratory, focusing on three critical tasks: 1)
dual-arm lid manipulation in 3 cm clearance spaces, 2) bimanual handover in
overlapping workspaces, and 3) obstructed powder sample container delivery with
orientation constraints. These tasks present significant challenges, including
inefficient sampling in narrow passages, the need for smooth trajectories to
prevent spillage, and suboptimal paths generated by conventional methods. To
overcome these challenges, we propose a hierarchical planning framework
combining Prior-Guided Path Planning and Multi-Step Trajectory Optimization.
The former uses a finite Gaussian mixture model to improve sampling efficiency
in narrow passages, while the latter refines paths by shortening, simplifying,
imposing joint constraints, and B-spline smoothing. Experimental results
demonstrate the framework's effectiveness: planning time is reduced by up to
80.4%, and waypoints are decreased by 89.4%. Furthermore, the system completes
the full vibratory sieve shaker operation workflow in a physical experiment,
validating its practical applicability for complex laboratory automation.

</details>


### [17] [SimCoachCorpus: A naturalistic dataset with language and trajectories for embodied teaching](https://arxiv.org/abs/2509.14548)
*Emily Sumner,Deepak E. Gopinath,Laporsha Dees,Patricio Reyes Gomez,Xiongyi Cui,Andrew Silva,Jean Costa,Allison Morgan,Mariah Schrum,Tiffany L. Chen,Avinash Balachandran,Guy Rosman*

Main category: cs.RO

TL;DR: SimCoachCorpus是一个独特的赛车模拟驾驶数据集，包含专业教练的一对一指导和无指导驾驶数据，用于研究语言和物理动作结合的技能获取过程。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏捕捉人们通过语言指导获得具身技能的数据集，特别是在语言和物理动作深度结合的领域。

Method: 收集29名人类在赛车模拟器中约90分钟的驾驶数据，其中15人接受专业教练的一对一指导，14人无指导。数据集包含车辆状态、地图信息、教练语音反馈等多种同步数据。

Result: 数据集包含超过20,000条实时反馈语句、400多条最终反馈语句和40多小时的驾驶数据，并提供了教练类别标注、学生遵守情况评分以及参与者的认知负荷和情绪状态。

Conclusion: 该自然主义数据集可用于研究运动学习动态、探索语言现象和训练教学计算模型，已在上下文学习、模仿学习和主题建模等应用中展示价值。

Abstract: Curated datasets are essential for training and evaluating AI approaches, but
are often lacking in domains where language and physical action are deeply
intertwined. In particular, few datasets capture how people acquire embodied
skills through verbal instruction over time. To address this gap, we introduce
SimCoachCorpus: a unique dataset of race car simulator driving that allows for
the investigation of rich interactive phenomena during guided and unguided
motor skill acquisition. In this dataset, 29 humans were asked to drive in a
simulator around a race track for approximately ninety minutes. Fifteen
participants were given personalized one-on-one instruction from a professional
performance driving coach, and 14 participants drove without coaching. \name\
includes embodied features such as vehicle state and inputs, map (track
boundaries and raceline), and cone landmarks. These are synchronized with
concurrent verbal coaching from a professional coach and additional feedback at
the end of each lap. We further provide annotations of coaching categories for
each concurrent feedback utterance, ratings on students' compliance with
coaching advice, and self-reported cognitive load and emotional state of
participants (gathered from surveys during the study). The dataset includes
over 20,000 concurrent feedback utterances, over 400 terminal feedback
utterances, and over 40 hours of vehicle driving data. Our naturalistic dataset
can be used for investigating motor learning dynamics, exploring linguistic
phenomena, and training computational models of teaching. We demonstrate
applications of this dataset for in-context learning, imitation learning, and
topic modeling. The dataset introduced in this work will be released publicly
upon publication of the peer-reviewed version of this paper. Researchers
interested in early access may register at
https://tinyurl.com/SimCoachCorpusForm.

</details>


### [18] [Hierarchical Planning and Scheduling for Reconfigurable Multi-Robot Disassembly Systems under Structural Constraints](https://arxiv.org/abs/2509.14564)
*Takuya Kiyokawa,Tomoki Ishikura,Shingo Hamada,Genichiro Matsuda,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一种用于可重构机器人自动无损拆解约束结构的系统集成方法，通过分层优化和多目标遗传算法解决大规模复杂搜索空间问题


<details>
  <summary>Details</summary>
Motivation: 可重构机器人系统需要适应目标结构进行配置和协调，但庞大复杂的搜索空间容易陷入局部最优，需要开发有效的规划方法

Method: 集成多机械臂和旋转台的可重构系统，采用分层优化方法：使用多目标遗传算法进行序列和任务规划（含运动评估），然后用约束规划进行调度，特别针对序列规划设计了染色体初始化方法

Result: 仿真结果表明该方法能有效解决可重构机器人拆解中的复杂问题

Conclusion: 该方法能够在现实时间范围内生成满足多个优选条件和强制要求的计划，成功解决了可重构机器人拆解约束结构的规划问题

Abstract: This study presents a system integration approach for planning schedules,
sequences, tasks, and motions for reconfigurable robots to automatically
disassemble constrained structures in a non-destructive manner. Such systems
must adapt their configuration and coordination to the target structure, but
the large and complex search space makes them prone to local optima. To address
this, we integrate multiple robot arms equipped with different types of tools,
together with a rotary stage, into a reconfigurable setup. This flexible system
is based on a hierarchical optimization method that generates plans meeting
multiple preferred conditions under mandatory requirements within a realistic
timeframe. The approach employs two many-objective genetic algorithms for
sequence and task planning with motion evaluations, followed by constraint
programming for scheduling. Because sequence planning has a much larger search
space, we introduce a chromosome initialization method tailored to constrained
structures to mitigate the risk of local optima. Simulation results demonstrate
that the proposed method effectively solves complex problems in reconfigurable
robotic disassembly.

</details>


### [19] [Toward Embodiment Equivariant Vision-Language-Action Policy](https://arxiv.org/abs/2509.14630)
*Anzhe Chen,Yifei Yang,Zhenjie Zhu,Kechun Xu,Zhongxiang Zhou,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于等变性理论的视觉-语言-动作策略框架，通过设计对机器人配置变换具有等变性的动作空间和策略，解决了跨具身预训练中的配置泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作策略在大规模预训练中虽然能够学习跨任务、环境和具身的操作技能，但对新颖机器人配置的泛化能力有限。大多数方法过于关注模型大小、数据集规模和多样性，而忽视了动作空间的设计，导致需要昂贵的适配成本。

Method: 提出了一个基于等变性理论的框架：(1)建立了动作空间和策略设计的具身等变性理论；(2)引入了强制配置等变性的动作解码器；(3)结合了几何感知网络架构来增强具身无关的空间推理能力。

Result: 在仿真和真实环境中的大量实验表明，该方法提高了预训练的有效性，并能够在新颖机器人具身上实现高效的微调。

Conclusion: 通过等变性理论指导的动作空间和策略设计，能够有效解决跨具身预训练中的配置泛化问题，为机器人学习提供了更强大的泛化能力。

Abstract: Vision-language-action policies learn manipulation skills across tasks,
environments and embodiments through large-scale pre-training. However, their
ability to generalize to novel robot configurations remains limited. Most
approaches emphasize model size, dataset scale and diversity while paying less
attention to the design of action spaces. This leads to the configuration
generalization problem, which requires costly adaptation. We address this
challenge by formulating cross-embodiment pre-training as designing policies
equivariant to embodiment configuration transformations. Building on this
principle, we propose a framework that (i) establishes a embodiment
equivariance theory for action space and policy design, (ii) introduces an
action decoder that enforces configuration equivariance, and (iii) incorporates
a geometry-aware network architecture to enhance embodiment-agnostic spatial
reasoning. Extensive experiments in both simulation and real-world settings
demonstrate that our approach improves pre-training effectiveness and enables
efficient fine-tuning on novel robot embodiments. Our code is available at
https://github.com/hhcaz/e2vla

</details>


### [20] [BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots](https://arxiv.org/abs/2509.14636)
*Yufei Wei,Wangtao Lu,Sha Lu,Chenxiao Hu,Fuzhang Han,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: BEV-ODOM2是一个改进的单目视觉里程计框架，通过密集BEV光流监督和PV-BEV融合来解决现有BEV方法的稀疏监督和信息丢失问题，在多个数据集上实现了40%的相对轨迹误差改进。


<details>
  <summary>Details</summary>
Motivation: 现有的BEV方法在单目视觉里程计中存在稀疏监督信号和透视到BEV投影过程中的信息丢失问题，需要在不增加额外标注的情况下解决这些限制。

Method: 提出密集BEV光流监督（从3-DoF位姿真值构建像素级指导）和PV-BEV融合（在投影前计算相关体积以保留6-DoF运动线索同时保持尺度一致性），采用三个监督层次和增强的旋转采样。

Result: 在KITTI、NCLT、Oxford和新收集的ZJH-VO多尺度数据集上进行了广泛评估，相比之前的BEV方法实现了40%的相对轨迹误差改进，达到最先进的性能。

Conclusion: BEV-ODOM2框架有效解决了BEV方法的监督稀疏和信息丢失问题，提出的ZJH-VO数据集涵盖了从地下停车场到室外广场的多样化地面车辆场景，为未来研究提供了支持。

Abstract: Bird's-Eye-View (BEV) representation offers a metric-scaled planar workspace,
facilitating the simplification of 6-DoF ego-motion to a more robust 3-DoF
model for monocular visual odometry (MVO) in intelligent transportation
systems. However, existing BEV methods suffer from sparse supervision signals
and information loss during perspective-to-BEV projection. We present
BEV-ODOM2, an enhanced framework addressing both limitations without additional
annotations. Our approach introduces: (1) dense BEV optical flow supervision
constructed from 3-DoF pose ground truth for pixel-level guidance; (2) PV-BEV
fusion that computes correlation volumes before projection to preserve 6-DoF
motion cues while maintaining scale consistency. The framework employs three
supervision levels derived solely from pose data: dense BEV flow, 5-DoF for the
PV branch, and final 3-DoF output. Enhanced rotation sampling further balances
diverse motion patterns in training. Extensive evaluation on KITTI, NCLT,
Oxford, and our newly collected ZJH-VO multi-scale dataset demonstrates
state-of-the-art performance, achieving 40 improvement in RTE compared to
previous BEV methods. The ZJH-VO dataset, covering diverse ground vehicle
scenarios from underground parking to outdoor plazas, is publicly available to
facilitate future research.

</details>


### [21] [Efficient 3D Perception on Embedded Systems via Interpolation-Free Tri-Plane Lifting and Volume Fusion](https://arxiv.org/abs/2509.14641)
*Sibaek Lee,Jiung Yeon,Hyeonwoo Yu*

Main category: cs.RO

TL;DR: 提出了一种新颖的无插值三平面提升和体积融合框架，通过将3D体素直接投影到平面特征并通过广播和求和重建特征体积，显著降低了计算复杂度，适合嵌入式实时3D感知。


<details>
  <summary>Details</summary>
Motivation: 现有的三平面方法依赖2D图像特征插值、逐点查询和隐式MLP，计算量大且不适合嵌入式3D推理。需要一种更高效的3D卷积替代方案。

Method: 提出插值自由的三平面提升和体积融合框架，将非线性转移到2D卷积，添加低分辨率体积分支通过轻量级集成层融合提升特征，实现端到端GPU加速。

Result: 分类和补全任务保持或提升精度，分割和检测任务以轻微精度下降换取显著计算节省，在NVIDIA Jetson Orin nano上实现稳健实时吞吐量。

Conclusion: 该方法在效率和精度之间取得良好平衡，适用于嵌入式机器人感知系统，证明了在保持性能的同时实现实时处理的能力。

Abstract: Dense 3D convolutions provide high accuracy for perception but are too
computationally expensive for real-time robotic systems. Existing tri-plane
methods rely on 2D image features with interpolation, point-wise queries, and
implicit MLPs, which makes them computationally heavy and unsuitable for
embedded 3D inference. As an alternative, we propose a novel interpolation-free
tri-plane lifting and volumetric fusion framework, that directly projects 3D
voxels into plane features and reconstructs a feature volume through broadcast
and summation. This shifts nonlinearity to 2D convolutions, reducing complexity
while remaining fully parallelizable. To capture global context, we add a
low-resolution volumetric branch fused with the lifted features through a
lightweight integration layer, yielding a design that is both efficient and
end-to-end GPU-accelerated. To validate the effectiveness of the proposed
method, we conduct experiments on classification, completion, segmentation, and
detection, and we map the trade-off between efficiency and accuracy across
tasks. Results show that classification and completion retain or improve
accuracy, while segmentation and detection trade modest drops in accuracy for
significant computational savings. On-device benchmarks on an NVIDIA Jetson
Orin nano confirm robust real-time throughput, demonstrating the suitability of
the approach for embedded robotic perception.

</details>


### [22] [RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI](https://arxiv.org/abs/2509.14687)
*Cong Tai,Zhaoyu Zheng,Haixu Long,Hansheng Wu,Haodong Xiang,Zhengbin Long,Jun Xiong,Rong Shi,Shizhuang Zhang,Gang Qiu,He Wang,Ruifeng Li,Jun Huang,Bin Chang,Shuai Feng,Tao Shen*

Main category: cs.RO

TL;DR: RealMirror是一个开源的视觉-语言-动作(VLA)平台，通过低成本数据收集、标准化基准测试和Sim2Real零样本迁移技术，解决了人形机器人VLA研究中的数据获取成本高、缺乏标准化基准和仿真-现实差距等核心挑战。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人视觉-语言-动作(VLA)研究面临的三大核心挑战：数据采集成本高昂、缺乏标准化基准测试、以及仿真环境与真实世界之间的显著差距。

Method: 构建了一个端到端的VLA研究平台，包括高效低成本的数据收集系统、模型训练和推理系统；引入了专门的人形机器人VLA基准测试；通过生成模型和3D高斯泼溅技术重建真实环境和机器人模型，实现零样本Sim2Real迁移。

Result: 成功实现了无需微调的零样本Sim2Real迁移，仅在仿真数据上训练的模型能够无缝地在真实机器人上执行任务；提供了包含多场景、大量轨迹和各种VLA模型的标准化基准。

Conclusion: RealMirror通过整合关键组件，为人形机器人VLA模型的发展提供了一个强大的框架，显著加速了该领域的研究进展。

Abstract: The emerging field of Vision-Language-Action (VLA) for humanoid robots faces
several fundamental challenges, including the high cost of data acquisition,
the lack of a standardized benchmark, and the significant gap between
simulation and the real world. To overcome these obstacles, we propose
RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror
builds an efficient, low-cost data collection, model training, and inference
system that enables end-to-end VLA research without requiring a real robot. To
facilitate model evolution and fair comparison, we also introduce a dedicated
VLA benchmark for humanoid robots, featuring multiple scenarios, extensive
trajectories, and various VLA models. Furthermore, by integrating generative
models and 3D Gaussian Splatting to reconstruct realistic environments and
robot models, we successfully demonstrate zero-shot Sim2Real transfer, where
models trained exclusively on simulation data can perform tasks on a real robot
seamlessly, without any fine-tuning. In conclusion, with the unification of
these critical components, RealMirror provides a robust framework that
significantly accelerates the development of VLA models for humanoid robots.
Project page: https://terminators2025.github.io/RealMirror.github.io

</details>


### [23] [exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation](https://arxiv.org/abs/2509.14688)
*Yue Xu,Litao Wei,Pengyu An,Qingyu Zhang,Yong-Lu Li*

Main category: cs.RO

TL;DR: 提出了exUMI触觉数据收集设备和TPP触觉预测预训练框架，通过硬件算法协同设计解决触觉机器人学习中的数据稀缺和稀疏性问题


<details>
  <summary>Details</summary>
Motivation: 解决触觉机器人学习中数据收集困难、数据可用性低、缺乏力反馈以及触觉数据稀疏等关键挑战

Method: 开发exUMI可扩展数据收集设备（增强版UMI，包含AR运动捕捉、旋转编码器、模块化视觉触觉传感和自动校准），并提出TPP触觉预测预训练框架，通过动作感知的时序触觉预测进行表示学习

Result: 实现了100%数据可用性，收集了超过100万帧触觉数据，真实世界实验显示TPP优于传统触觉模仿学习方法

Conclusion: 通过硬件算法协同设计，填补了人类触觉直觉与机器人学习之间的鸿沟，为接触丰富的操作研究提供了开源资源

Abstract: Tactile-aware robot learning faces critical challenges in data collection and
representation due to data scarcity and sparsity, and the absence of force
feedback in existing systems. To address these limitations, we introduce a
tactile robot learning system with both hardware and algorithm innovations. We
present exUMI, an extensible data collection device that enhances the vanilla
UMI with robust proprioception (via AR MoCap and rotary encoder), modular
visuo-tactile sensing, and automated calibration, achieving 100% data
usability. Building on an efficient collection of over 1 M tactile frames, we
propose Tactile Prediction Pretraining (TPP), a representation learning
framework through action-aware temporal tactile prediction, capturing contact
dynamics and mitigating tactile sparsity. Real-world experiments show that TPP
outperforms traditional tactile imitation learning. Our work bridges the gap
between human tactile intuition and robot learning through co-designed hardware
and algorithms, offering open-source resources to advance contact-rich
manipulation research. Project page: https://silicx.github.io/exUMI.

</details>


### [24] [Wohlhart's Three-Loop Mechanism: An Overconstrained and Shaky Linkage](https://arxiv.org/abs/2509.14698)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文重新分析了一个三环空间连杆机构，该机构在参考构型下具有有限自由度3（过约束）但微分自由度为5，研究表明其构型空间是局部光滑流形且微分自由度局部恒定，因此该机构是摇动的而非奇异。


<details>
  <summary>Details</summary>
Motivation: 重新审视Wohlhart在ARK 2004提出的三环空间连杆机构（基于Baker 1980年的两环机构扩展），该机构在Diez-Martinez等人ARK 2006论文中已有分析，但需要进一步研究其局部运动学特性。

Method: 采用局部分析方法，计算运动学切锥和构型空间的局部近似，分析机构的有限自由度、微分自由度以及构型空间的几何特性。

Result: 发现该连杆机构在参考构型下具有有限自由度3（过约束）但微分自由度为5，构型空间是局部光滑流形，微分自由度局部恒定，表明机构是摇动的而非处于奇异位置。

Conclusion: 该三环空间连杆机构具有独特的运动学特性：虽然是过约束机构，但其参考构型不是奇异位置，而是摇动机构，这通过高阶局部分析和运动学切锥计算得到验证。

Abstract: This paper revisits a three-loop spatial linkage that was proposed in an ARK
2004 paper by Karl Wohlhart (as extension of a two-loop linkage proposed by
Eddie Baker in 1980) and later analyzed in an ARK 2006 paper by Diez-Martinez
et. al. A local analysis shows that this linkage has a finite degree of freedom
(DOF) 3 (and is thus overconstrained) while in its reference configuration the
differential DOF is 5. It is shown that its configuration space is locally a
smooth manifold so that the reference configuration is not a c-space
singularity. It is shown that the differential DOF is locally constant, which
makes this linkage shaky (so that the reference configuration is not a
singularity). The higher-order local analysis is facilitated by the computation
of the kinematic tangent cone as well as a local approximation of the c-space.

</details>


### [25] [Rethinking Reference Trajectories in Agile Drone Racing: A Unified Reference-Free Model-Based Controller via MPPI](https://arxiv.org/abs/2509.14726)
*Fangguo Zhao,Xin Guan,Shuo Li*

Main category: cs.RO

TL;DR: 提出了一种基于MPPI的无参考轨迹方法，通过直接优化门进度目标来实现时间最优的无人机竞速，性能优于或媲美传统基于参考轨迹的方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的控制器依赖预计算参考轨迹，但轨迹跟踪和轮廓控制都需要参考，而强化学习研究表明这些方法优化的是替代目标而非直接最大化门进度的主要竞速目标。

Method: 将RL奖励塑造中得到的门进度目标直接整合到MPPI框架中，利用MPPI的采样特性实时优化不连续不可微的目标函数，并与轨迹跟踪、轮廓控制进行系统对比。

Result: 提出的无参考方法实现了有竞争力的竞速性能，达到或超过了基于参考的方法，MPPI在优化不连续目标方面优于传统梯度求解器。

Conclusion: 参考自由的MPPI方法能够有效优化不连续的门进度目标，为无人机竞速提供了新的高性能控制方案，无需依赖预计算参考轨迹。

Abstract: While model-based controllers have demonstrated remarkable performance in
autonomous drone racing, their performance is often constrained by the reliance
on pre-computed reference trajectories. Conventional approaches, such as
trajectory tracking, demand a dynamically feasible, full-state reference,
whereas contouring control relaxes this requirement to a geometric path but
still necessitates a reference. Recent advancements in reinforcement learning
(RL) have revealed that many model-based controllers optimize surrogate
objectives, such as trajectory tracking, rather than the primary racing goal of
directly maximizing progress through gates. Inspired by these findings, this
work introduces a reference-free method for time-optimal racing by
incorporating this gate progress objective, derived from RL reward shaping,
directly into the Model Predictive Path Integral (MPPI) formulation. The
sampling-based nature of MPPI makes it uniquely capable of optimizing the
discontinuous and non-differentiable objective in real-time. We also establish
a unified framework that leverages MPPI to systematically and fairly compare
three distinct objective functions with a consistent dynamics model and
parameter set: classical trajectory tracking, contouring control, and the
proposed gate progress objective. We compare the performance of these three
objectives when solved via both MPPI and a traditional gradient-based solver.
Our results demonstrate that the proposed reference-free approach achieves
competitive racing performance, rivaling or exceeding reference-based methods.
Videos are available at https://zhaofangguo.github.io/racing_mppi/

</details>


### [26] [Investigating the Effect of LED Signals and Emotional Displays in Human-Robot Shared Workspaces](https://arxiv.org/abs/2509.14748)
*Maria Ibrahim,Alap Kshirsagar,Dorothea Koert,Jan Peters*

Main category: cs.RO

TL;DR: 本研究探讨了非语言沟通（LED灯光信号和情感显示）在人类-机器人协作中的影响，发现情感显示虽然能提升交互感知，但对碰撞预警和任务效率的改善有限。


<details>
  <summary>Details</summary>
Motivation: 在共享工作空间中，有效沟通对安全和效率至关重要，需要研究非语言沟通方式如何改善人机交互。

Method: 在Franka Emika Panda机器人上集成LED灯带和平板情感显示，通过18名参与者的协作实验评估三种条件：仅LED信号、LED加反应式情感显示、LED加预判式情感显示。

Result: 情感显示增加了机器人的交互感知性，但在碰撞预警、沟通清晰度和任务效率方面相比仅使用LED信号没有显著改善。

Conclusion: 情感线索可以增强用户参与度，但在共享工作空间的任务性能影响有限，LED信号本身已足够有效。

Abstract: Effective communication is essential for safety and efficiency in human-robot
collaboration, particularly in shared workspaces. This paper investigates the
impact of nonverbal communication on human-robot interaction (HRI) by
integrating reactive light signals and emotional displays into a robotic
system. We equipped a Franka Emika Panda robot with an LED strip on its end
effector and an animated facial display on a tablet to convey movement intent
through colour-coded signals and facial expressions. We conducted a human-robot
collaboration experiment with 18 participants, evaluating three conditions: LED
signals alone, LED signals with reactive emotional displays, and LED signals
with pre-emptive emotional displays. We collected data through questionnaires
and position tracking to assess anticipation of potential collisions, perceived
clarity of communication, and task performance. The results indicate that while
emotional displays increased the perceived interactivity of the robot, they did
not significantly improve collision anticipation, communication clarity, or
task efficiency compared to LED signals alone. These findings suggest that
while emotional cues can enhance user engagement, their impact on task
performance in shared workspaces is limited.

</details>


### [27] [Designing Latent Safety Filters using Pre-Trained Vision Models](https://arxiv.org/abs/2509.14758)
*Ihab Tabbara,Yuxuan Yang,Ahmad Hamzeh,Maxwell Astafyev,Hussein Sibai*

Main category: cs.RO

TL;DR: 本文探讨预训练视觉模型在视觉安全滤波器中的应用效果，比较不同训练策略和模型架构的性能差异。


<details>
  <summary>Details</summary>
Motivation: 确保视觉控制系统的安全性是部署在关键环境中的主要挑战，现有安全滤波器在视觉控制应用有限，需要研究预训练视觉模型在安全滤波器中的有效性。

Method: 使用预训练视觉模型作为分类器、Hamilton-Jacobi可达性安全滤波器和潜在世界模型的骨干网络，比较从头训练、微调和冻结三种策略，评估不同模型在安全策略切换决策中的表现。

Result: 评估了不同预训练视觉模型在各种任务中的性能表现，比较了学习世界模型和Q函数在安全策略切换决策中的优劣，并讨论了在资源受限设备上的实际部署考虑。

Conclusion: 预训练视觉模型可作为有效的视觉安全滤波器骨干，但需要根据具体任务选择适当的训练策略和模型架构，同时考虑实际部署的硬件约束。

Abstract: Ensuring safety of vision-based control systems remains a major challenge
hindering their deployment in critical settings. Safety filters have gained
increased interest as effective tools for ensuring the safety of classical
control systems, but their applications in vision-based control settings have
so far been limited. Pre-trained vision models (PVRs) have been shown to be
effective perception backbones for control in various robotics domains. In this
paper, we are interested in examining their effectiveness when used for
designing vision-based safety filters. We use them as backbones for classifiers
defining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety
filters, and for latent world models. We discuss the trade-offs between
training from scratch, fine-tuning, and freezing the PVRs when training the
models they are backbones for. We also evaluate whether one of the PVRs is
superior across all tasks, evaluate whether learned world models or Q-functions
are better for switching decisions to safe policies, and discuss practical
considerations for deploying these PVRs on resource-constrained devices.

</details>


### [28] [COMPASS: Confined-space Manipulation Planning with Active Sensing Strategy](https://arxiv.org/abs/2509.14787)
*Qixuan Li,Chen Le,Dongyue Huang,Jincheng Yu,Xinlei Chen*

Main category: cs.RO

TL;DR: COMPASS是一个多阶段探索和操作框架，用于受限杂乱环境中的机器人操作，通过操作感知的采样规划器和多目标效用函数提高操作成功率24.25%。


<details>
  <summary>Details</summary>
Motivation: 受限杂乱环境中的操作面临部分可观测性和复杂配置空间的挑战，需要智能探索策略来安全理解场景并搜索目标。

Method: 提出COMPASS框架：1）近场感知扫描构建局部碰撞地图；2）多目标效用函数寻找信息丰富且利于后续操作的视点；3）约束操作优化策略生成尊重障碍物约束的操作位姿。

Result: 在模拟中比仅考虑信息增益的探索方法提高操作成功率24.25%，真实世界实验证明在受限环境中具有主动感知和操作能力。

Conclusion: COMPASS框架通过多阶段探索和操作感知规划，有效解决了受限杂乱环境中的机器人操作挑战，在模拟和真实环境中均表现出色。

Abstract: Manipulation in confined and cluttered environments remains a significant
challenge due to partial observability and complex configuration spaces.
Effective manipulation in such environments requires an intelligent exploration
strategy to safely understand the scene and search the target. In this paper,
we propose COMPASS, a multi-stage exploration and manipulation framework
featuring a manipulation-aware sampling-based planner. First, we reduce
collision risks with a near-field awareness scan to build a local collision
map. Additionally, we employ a multi-objective utility function to find
viewpoints that are both informative and conducive to subsequent manipulation.
Moreover, we perform a constrained manipulation optimization strategy to
generate manipulation poses that respect obstacle constraints. To
systematically evaluate method's performance under these difficulties, we
propose a benchmark of confined-space exploration and manipulation containing
four level challenging scenarios. Compared to exploration methods designed for
other robots and only considering information gain, our framework increases
manipulation success rate by 24.25% in simulations. Real-world experiments
demonstrate our method's capability for active sensing and manipulation in
confined environments.

</details>


### [29] [Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution](https://arxiv.org/abs/2509.14816)
*Humphrey Munn,Brendan Tidd,Peter Böhm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: GCR-PPO是一种改进的actor-critic优化方法，通过多目标梯度分解和冲突解决机制，解决了强化学习中多目标奖励的冲突问题，在机器人控制任务中表现出比传统PPO更好的可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习将多个任务目标聚合为单一标量奖励，导致需要精细的奖励调优、容易陷入局部最优，且随着目标数量增加，调优成本和次优性问题加剧。多目标方法在机器人RL中应用较少，主要因为计算成本和优化难度。

Method: 提出GCR-PPO方法，修改actor-critic优化：使用多头critic将actor更新分解为各目标的梯度，基于目标优先级解决梯度冲突。在IsaacLab操作和运动基准测试以及多目标修改任务上进行评估。

Result: 相比并行PPO具有更好的可扩展性(p=0.04)，无显著计算开销。在高冲突任务中表现更好，平均改进9.5%，冲突越大的任务改进越大。

Conclusion: GCR-PPO通过显式处理多目标梯度冲突，有效解决了传统标量化方法的局限性，为机器人强化学习中的多目标优化提供了有效的解决方案。

Abstract: Reinforcement Learning (RL) robot controllers usually aggregate many task
objectives into one scalar reward. While large-scale proximal policy
optimisation (PPO) has enabled impressive results such as robust robot
locomotion in the real world, many tasks still require careful reward tuning
and are brittle to local optima. Tuning cost and sub-optimality grow with the
number of objectives, limiting scalability. Modelling reward vectors and their
trade-offs can address these issues; however, multi-objective methods remain
underused in RL for robotics because of computational cost and optimisation
difficulty. In this work, we investigate the conflict between gradient
contributions for each objective that emerge from scalarising the task
objectives. In particular, we explicitly address the conflict between
task-based rewards and terms that regularise the policy towards realistic
behaviour. We propose GCR-PPO, a modification to actor-critic optimisation that
decomposes the actor update into objective-wise gradients using a multi-headed
critic and resolves conflicts based on the objective priority. Our methodology,
GCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion
benchmarks and additional multi-objective modifications on two related tasks.
We show superior scalability compared to parallel PPO (p = 0.04), without
significant computational overhead. We also show higher performance with more
conflicting tasks. GCR-PPO improves on large-scale PPO with an average
improvement of 9.5%, with high-conflict tasks observing a greater improvement.
The code is available at https://github.com/humphreymunn/GCR-PPO.

</details>


### [30] [CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human](https://arxiv.org/abs/2509.14889)
*Nan Sun,Yongchang Li,Chenxu Wang,Huiying Li,Huaping Liu*

Main category: cs.RO

TL;DR: CollabVLA是一个自反思的视觉-语言-动作框架，通过混合专家设计将标准视觉运动策略转变为协作助手，解决了现有VLA方法的领域过拟合、不可解释推理和高延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉-语言-动作(VLA)系统的关键局限性，包括领域过拟合、不可解释的推理过程，以及辅助生成模型的高延迟问题，将VLA从黑盒控制器转变为真正能与人类协作的智能助手。

Method: 采用混合专家设计，整合基于VLM的反思推理和基于扩散的动作生成，通过两阶段训练方法（动作接地和反思调优）实现显式自我反思，并在不确定或重复失败时主动寻求人类指导。

Result: 相比生成式智能体，将标准化时间减少约2倍，Dream计数减少约4倍，实现了更高的成功率、改进的可解释性，以及与现有方法相比平衡的低延迟性能。

Conclusion: 这项工作是向将VLA从黑盒控制器转变为真正能够推理、行动并与人类协作的辅助智能体迈出的开创性一步。

Abstract: In this work, we present CollabVLA, a self-reflective vision-language-action
framework that transforms a standard visuomotor policy into a collaborative
assistant. CollabVLA tackles key limitations of prior VLAs, including domain
overfitting, non-interpretable reasoning, and the high latency of auxiliary
generative models, by integrating VLM-based reflective reasoning with
diffusion-based action generation under a mixture-of-experts design. Through a
two-stage training recipe of action grounding and reflection tuning, it
supports explicit self-reflection and proactively solicits human guidance when
confronted with uncertainty or repeated failure. It cuts normalized Time by ~2x
and Dream counts by ~4x vs. generative agents, achieving higher success rates,
improved interpretability, and balanced low latency compared with existing
methods. This work takes a pioneering step toward shifting VLAs from opaque
controllers to genuinely assistive agents capable of reasoning, acting, and
collaborating with humans.

</details>


### [31] [PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in Spherical Robots](https://arxiv.org/abs/2509.14915)
*Shenghai Yuan,Jason Wai Hao Yee,Weixiang Guo,Zhongyuan Liu,Thien-Minh Nguyen,Lihua Xie*

Main category: cs.RO

TL;DR: PERAL是一个用于球形机器人的感知感知运动控制框架，通过内部差速驱动实现被动LiDAR激励，无需额外硬件即可增强垂直扫描多样性，提高导航性能。


<details>
  <summary>Details</summary>
Motivation: 水平安装的LiDAR（如MID360）在近地面捕获的点云较少，限制了地形感知能力，在特征稀缺环境中性能下降。现有解决方案要么牺牲水平感知，要么增加执行器、成本和功耗。

Method: 通过建模内部差速驱动与传感器姿态的耦合关系，在名义目标或轨迹跟踪命令上叠加有界的非周期性振荡，丰富垂直扫描多样性同时保持导航精度。

Result: 在实验室、走廊和战术环境中验证，实现高达96%的地图完整性，轨迹跟踪误差减少27%，并实现鲁棒的近地面人体检测，相比基准方案重量、功耗和成本更低。

Conclusion: PERAL框架成功实现了无需专用硬件的被动LiDAR激励，显著提升了球形机器人在特征稀缺环境中的感知和导航性能，具有实际应用价值。

Abstract: Autonomous mobile robots increasingly rely on LiDAR-IMU odometry for
navigation and mapping, yet horizontally mounted LiDARs such as the MID360
capture few near-ground returns, limiting terrain awareness and degrading
performance in feature-scarce environments. Prior solutions - static tilt,
active rotation, or high-density sensors - either sacrifice horizontal
perception or incur added actuators, cost, and power. We introduce PERAL, a
perception-aware motion control framework for spherical robots that achieves
passive LiDAR excitation without dedicated hardware. By modeling the coupling
between internal differential-drive actuation and sensor attitude, PERAL
superimposes bounded, non-periodic oscillations onto nominal goal- or
trajectory-tracking commands, enriching vertical scan diversity while
preserving navigation accuracy. Implemented on a compact spherical robot, PERAL
is validated across laboratory, corridor, and tactical environments.
Experiments demonstrate up to 96 percent map completeness, a 27 percent
reduction in trajectory tracking error, and robust near-ground human detection,
all at lower weight, power, and cost compared with static tilt, active
rotation, and fixed horizontal baselines. The design and code will be
open-sourced upon acceptance.

</details>


### [32] [Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale](https://arxiv.org/abs/2509.14932)
*Tobias Jülg,Pierre Krack,Seongjin Bien,Yannik Blei,Khaled Gamal,Ken Nakahara,Johannes Hechtl,Roberto Calandra,Wolfram Burgard,Florian Walter*

Main category: cs.RO

TL;DR: 提出了Robot Control Stack (RCS) - 一个轻量级的机器人学习生态系统，专门为支持大规模通用策略研究而设计，解决了传统机器人软件框架在VLA模型训练中的瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 传统机器人软件框架在大规模视觉-语言-动作模型训练中成为瓶颈，仿真环境对真实世界实验的转换支持有限，需要一个新的软件栈来支持机器人学习研究

Method: 设计了模块化、可扩展的分层架构，提供统一的仿真和物理机器人接口，支持sim-to-real迁移，具有最小依赖但完整功能集

Result: 评估了RCS在VLA和RL策略开发周期中的可用性和性能，对Octo、OpenVLA和Pi Zero在多个机器人上进行了广泛评估，展示了仿真数据如何提升真实世界策略性能

Conclusion: RCS成功填补了机器人学习研究中的软件基础设施空白，为大规模通用策略的训练和部署提供了高效支持，促进了sim-to-real迁移的研究

Abstract: Vision-Language-Action models (VLAs) mark a major shift in robot learning.
They replace specialized architectures and task-tailored components of expert
policies with large-scale data collection and setup-specific fine-tuning. In
this machine learning-focused workflow that is centered around models and
scalable training, traditional robotics software frameworks become a
bottleneck, while robot simulations offer only limited support for
transitioning from and to real-world experiments. In this work, we close this
gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from
the ground up to support research in robot learning with large-scale generalist
policies. At its core, RCS features a modular and easily extensible layered
architecture with a unified interface for simulated and physical robots,
facilitating sim-to-real transfer. Despite its minimal footprint and
dependencies, it offers a complete feature set, enabling both real-world
experiments and large-scale training in simulation. Our contribution is
twofold: First, we introduce the architecture of RCS and explain its design
principles. Second, we evaluate its usability and performance along the
development cycle of VLA and RL policies. Our experiments also provide an
extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed
light on how simulation data can improve real-world policy performance. Our
code, datasets, weights, and videos are available at:
https://robotcontrolstack.github.io/

</details>


### [33] [CAD-Driven Co-Design for Flight-Ready Jet-Powered Humanoids](https://arxiv.org/abs/2509.14935)
*Punith Reddy Vanteddu,Davide Gorbani,Giuseppe L'Erario,Hosameldin Awadalla Omer Mohamed,Fabio Bergonti,Daniele Pucci*

Main category: cs.RO

TL;DR: CAD驱动的协同设计框架，用于优化喷气动力空中人形机器人执行动态约束轨迹，通过设计实验生成5000个几何变体，使用聚类和MPC控制进行多目标优化


<details>
  <summary>Details</summary>
Motivation: 开发一个系统化的方法来优化喷气动力空中人形机器人的设计和控制参数，使其能够有效执行动态约束的飞行轨迹

Method: 采用设计实验方法生成几何变体设计，使用K-means聚类选择代表性模型，基于最小加加速度轨迹评估性能，使用NSGA-II算法进行多目标优化

Result: 框架输出一组飞行就绪的人形机器人配置和经过验证的控制参数

Conclusion: 提供了一个结构化的方法来选择和实施可行的空中人形机器人设计，实现了设计与控制的协同优化

Abstract: This paper presents a CAD-driven co-design framework for optimizing
jet-powered aerial humanoid robots to execute dynamically constrained
trajectories. Starting from the iRonCub-Mk3 model, a Design of Experiments
(DoE) approach is used to generate 5,000 geometrically varied and mechanically
feasible designs by modifying limb dimensions, jet interface geometry (e.g.,
angle and offset), and overall mass distribution. Each model is constructed
through CAD assemblies to ensure structural validity and compatibility with
simulation tools. To reduce computational cost and enable parameter sensitivity
analysis, the models are clustered using K-means, with representative centroids
selected for evaluation. A minimum-jerk trajectory is used to assess flight
performance, providing position and velocity references for a momentum-based
linearized Model Predictive Control (MPC) strategy. A multi-objective
optimization is then conducted using the NSGA-II algorithm, jointly exploring
the space of design centroids and MPC gain parameters. The objectives are to
minimize trajectory tracking error and mechanical energy expenditure. The
framework outputs a set of flight-ready humanoid configurations with validated
control parameters, offering a structured method for selecting and implementing
feasible aerial humanoid designs.

</details>


### [34] [A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for Generalizable Manipulation of Articulated Objects](https://arxiv.org/abs/2509.14939)
*Hao Zhang,Zhen Kan,Weiwei Shang,Yongduan Song*

Main category: cs.RO

TL;DR: DART是一个新颖的框架，通过结合扩散策略、affordance学习和线性时序逻辑(LTL)表示，提高了铰接式灵巧操作的学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决铰接物体操作和跨类别泛化的挑战，现有方法在这些方面存在显著困难。

Method: 利用LTL理解任务语义，affordance学习识别最佳交互点，扩散策略泛化跨类别交互，并基于交互数据的优化方法精炼动作。

Result: 实验结果表明DART在操作能力、泛化性能、迁移推理和鲁棒性方面优于大多数现有方法。

Conclusion: DART框架通过结合语义理解、affordance学习和扩散策略，有效提升了铰接式灵巧操作的性能和泛化能力。

Abstract: Despite recent advances in dexterous manipulations, the manipulation of
articulated objects and generalization across different categories remain
significant challenges. To address these issues, we introduce DART, a novel
framework that enhances a diffusion-based policy with affordance learning and
linear temporal logic (LTL) representations to improve the learning efficiency
and generalizability of articulated dexterous manipulation. Specifically, DART
leverages LTL to understand task semantics and affordance learning to identify
optimal interaction points. The {diffusion-based policy} then generalizes these
interactions across various categories. Additionally, we exploit an
optimization method based on interaction data to refine actions, overcoming the
limitations of traditional diffusion policies that typically rely on offline
reinforcement learning or learning from demonstrations. Experimental results
demonstrate that DART outperforms most existing methods in manipulation
ability, generalization performance, transfer reasoning, and robustness. For
more information, visit our project website at:
https://sites.google.com/view/dart0257/.

</details>


### [35] [Multi-CAP: A Multi-Robot Connectivity-Aware Hierarchical Coverage Path Planning Algorithm for Unknown Environments](https://arxiv.org/abs/2509.14941)
*Zongyuan Shen,Burhanuddin Shirose,Prasanna Sriganesh,Bhaskar Vundurthy,Howie Choset,Matthew Travers*

Main category: cs.RO

TL;DR: 提出Multi-CAP算法，通过连接感知的层次化覆盖路径规划，解决多机器人协同覆盖未知大环境的问题，显著减少路径长度和冲突。


<details>
  <summary>Details</summary>
Motivation: 多机器人在未知大环境中进行协同覆盖时，需要最小化总覆盖路径长度并减少机器人间的冲突，这是一个重要挑战。

Method: 采用层次化覆盖路径规划算法，构建并动态维护表示环境连接子区域的邻接图，将子区域分配问题建模为车辆路径问题(VRP)，为每个机器人计算不相交的路径，各机器人在子区域内独立执行覆盖策略。

Result: 通过仿真和多机器人硬件实验证明，Multi-CAP在覆盖时间、总路径长度和路径重叠率等关键指标上显著优于现有最先进方法。

Conclusion: 连接感知图和全局路径规划器在实现性能提升中起到关键作用，该方法有效解决了多机器人协同覆盖的挑战。

Abstract: Efficient coordination of multiple robots for coverage of large, unknown
environments is a significant challenge that involves minimizing the total
coverage path length while reducing inter-robot conflicts. In this paper, we
introduce a Multi-robot Connectivity-Aware Planner (Multi-CAP), a hierarchical
coverage path planning algorithm that facilitates multi-robot coordination
through a novel connectivity-aware approach. The algorithm constructs and
dynamically maintains an adjacency graph that represents the environment as a
set of connected subareas. Critically, we make the assumption that the
environment, while unknown, is bounded. This allows for incremental refinement
of the adjacency graph online to ensure its structure represents the physical
layout of the space, both in observed and unobserved areas of the map as robots
explore the environment. We frame the task of assigning subareas to robots as a
Vehicle Routing Problem (VRP), a well-studied problem for finding optimal
routes for a fleet of vehicles. This is used to compute disjoint tours that
minimize redundant travel, assigning each robot a unique, non-conflicting set
of subareas. Each robot then executes its assigned tour, independently adapting
its coverage strategy within each subarea to minimize path length based on
real-time sensor observations of the subarea. We demonstrate through
simulations and multi-robot hardware experiments that Multi-CAP significantly
outperforms state-of-the-art methods in key metrics, including coverage time,
total path length, and path overlap ratio. Ablation studies further validate
the critical role of our connectivity-aware graph and the global tour planner
in achieving these performance gains.

</details>


### [36] [Human Interaction for Collaborative Semantic SLAM using Extended Reality](https://arxiv.org/abs/2509.14949)
*Laura Ribeiro,Muhammad Shaheer,Miguel Fernandez-Cortizas,Ali Tourani,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: HICS-SLAM是一个人在环的语义SLAM框架，通过扩展现实环境实现人机实时协作，提升复杂环境下的语义建图效果


<details>
  <summary>Details</summary>
Motivation: 传统语义SLAM系统在遮挡、数据不完整或几何模糊的真实场景中表现不佳，无法充分利用人类的高层次空间和语义知识

Method: 提出基于共享扩展现实环境的人机协作框架，允许操作人员直接与机器人3D场景图交互并添加高层语义概念，采用基于图的语义融合方法整合人类干预与机器人感知

Result: 在真实建筑工地数据集上的实验评估显示，相比自动化基线方法，在房间检测精度、地图精度和语义完整性方面均有显著提升

Conclusion: 该方法证明了人机协作在语义SLAM中的有效性，并具有未来扩展的潜力

Abstract: Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robot
maps with structural and semantic information, enabling robots to operate more
effectively in complex environments. However, these systems struggle in
real-world scenarios with occlusions, incomplete data, or ambiguous geometries,
as they cannot fully leverage the higher-level spatial and semantic knowledge
humans naturally apply. We introduce HICS-SLAM, a Human-in-the-Loop semantic
SLAM framework that uses a shared extended reality environment for real-time
collaboration. The system allows human operators to directly interact with and
visualize the robot's 3D scene graph, and add high-level semantic concepts
(e.g., rooms or structural entities) into the mapping process. We propose a
graph-based semantic fusion methodology that integrates these human
interventions with robot perception, enabling scalable collaboration for
enhanced situational awareness. Experimental evaluations on real-world
construction site datasets demonstrate improvements in room detection accuracy,
map precision, and semantic completeness compared to automated baselines,
demonstrating both the effectiveness of the approach and its potential for
future extensions.

</details>


### [37] [Exploratory Movement Strategies for Texture Discrimination with a Neuromorphic Tactile Sensor](https://arxiv.org/abs/2509.14954)
*Xingchen Xu,Ao Li,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 提出基于人类探索策略的神经形态触觉感知框架，通过NeuroTac传感器采集数据，测试六种运动模式后发现滑动+旋转组合在复杂条件下达到87.33%准确率且功耗仅8.04mW


<details>
  <summary>Details</summary>
Motivation: 受人类探索策略启发，开发机器人纹理分类的神经形态触觉感知系统，旨在提高机器人与环境的交互能力

Method: 使用NeuroTac传感器采集神经形态触觉数据，测试六种探索运动（滑动、旋转、敲击及其组合），在固定环境和变化条件下评估性能

Result: 滑动+旋转组合在变化接触深度和速度条件下达到最高87.33%准确率，功耗极低仅8.04mW

Conclusion: 滑动+旋转是神经形态触觉感知在纹理分类任务中的最优探索策略，对增强机器人环境交互具有重要前景

Abstract: We propose a neuromorphic tactile sensing framework for robotic texture
classification that is inspired by human exploratory strategies. Our system
utilizes the NeuroTac sensor to capture neuromorphic tactile data during a
series of exploratory motions. We first tested six distinct motions for texture
classification under fixed environment: sliding, rotating, tapping, as well as
the combined motions: sliding+rotating, tapping+rotating, and tapping+sliding.
We chose sliding and sliding+rotating as the best motions based on final
accuracy and the sample timing length needed to reach converged accuracy. In
the second experiment designed to simulate complex real-world conditions, these
two motions were further evaluated under varying contact depth and speeds.
Under these conditions, our framework attained the highest accuracy of 87.33\%
with sliding+rotating while maintaining an extremely low power consumption of
only 8.04 mW. These results suggest that the sliding+rotating motion is the
optimal exploratory strategy for neuromorphic tactile sensing deployment in
texture classification tasks and holds significant promise for enhancing
robotic environmental interaction.

</details>


### [38] [Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery](https://arxiv.org/abs/2509.14967)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 提出一个手术机器人框架，通过视觉上下文和工具能力知识库来消解外科医生口头指令的歧义，使用符合预测方法确保安全性，在胆囊切除术数据集上达到60%的消歧率


<details>
  <summary>Details</summary>
Motivation: 解决手术中人机协作中口头通信的固有歧义性问题，提高手术机器人的指令理解准确性和患者安全性

Method: 采用两级基于affordance的推理过程：使用多模态视觉语言模型分析手术场景，结合工具能力知识库进行指令推理；使用双集符合预测方法提供统计严格的置信度度量

Result: 在胆囊切除术视频的歧义性手术请求数据集上评估，实现了60%的总体消歧率

Conclusion: 该框架为手术室中更安全的人机交互提供了一种方法，能够有效识别和标记歧义指令，提高手术机器人的可靠性和安全性

Abstract: Effective human-robot collaboration in surgery is affected by the inherent
ambiguity of verbal communication. This paper presents a framework for a
robotic surgical assistant that interprets and disambiguates verbal
instructions from a surgeon by grounding them in the visual context of the
operating field. The system employs a two-level affordance-based reasoning
process that first analyzes the surgical scene using a multimodal
vision-language model and then reasons about the instruction using a knowledge
base of tool capabilities. To ensure patient safety, a dual-set conformal
prediction method is used to provide a statistically rigorous confidence
measure for robot decisions, allowing it to identify and flag ambiguous
commands. We evaluated our framework on a curated dataset of ambiguous surgical
requests from cholecystectomy videos, demonstrating a general disambiguation
rate of 60% and presenting a method for safer human-robot interaction in the
operating room.

</details>


### [39] [PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments](https://arxiv.org/abs/2509.14978)
*Yifan Zhai,Rudolf Reiter,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 提出了感知感知的MPPI（PA-MPPI）方法，通过在目标被遮挡时引入感知成本来引导轨迹探索未知区域，解决了传统MPPI在未知环境中无法探索的问题，在硬件实验中性能提升达100%。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机在未知环境导航中的三个关键挑战：障碍物导致的非凸自由空间、四旋翼特定动力学和目标、需要探索未知区域寻找路径。传统MPPI方法虽然能处理前两个挑战，但在未知环境中性能受限，无法探索被大型障碍物遮挡的区域。

Method: 提出感知感知MPPI（PA-MPPI）方法，当目标被遮挡时，通过感知成本函数在线调整轨迹，使轨迹偏向能够感知未知区域的路径，从而扩展可通行空间的映射并增加找到替代路径的可能性。

Result: 硬件实验表明，PA-MPPI以50Hz频率运行，配合高效的感知和建图模块，在具有挑战性的设置中性能比基线提升高达100%，且能作为导航基础模型的安全鲁棒动作策略。

Conclusion: PA-MPPI有效解决了传统MPPI在未知环境中的探索局限性，通过感知感知机制实现了更好的导航性能，为四旋翼无人机在搜索救援等实际任务中的未知环境导航提供了有效解决方案。

Abstract: Quadrotor navigation in unknown environments is critical for practical
missions such as search-and-rescue. Solving it requires addressing three key
challenges: the non-convexity of free space due to obstacles,
quadrotor-specific dynamics and objectives, and the need for exploration of
unknown regions to find a path to the goal. Recently, the Model Predictive Path
Integral (MPPI) method has emerged as a promising solution that solves the
first two challenges. By leveraging sampling-based optimization, it can
effectively handle non-convex free space while directly optimizing over the
full quadrotor dynamics, enabling the inclusion of quadrotor-specific costs
such as energy consumption. However, its performance in unknown environments is
limited, as it lacks the ability to explore unknown regions when blocked by
large obstacles. To solve this issue, we introduce Perception-Aware MPPI
(PA-MPPI). Here, perception-awareness is defined as adapting the trajectory
online based on perception objectives. Specifically, when the goal is occluded,
PA-MPPI's perception cost biases trajectories that can perceive unknown
regions. This expands the mapped traversable space and increases the likelihood
of finding alternative paths to the goal. Through hardware experiments, we
demonstrate that PA-MPPI, running at 50 Hz with our efficient perception and
mapping module, performs up to 100% better than the baseline in our challenging
settings where the state-of-the-art MPPI fails. In addition, we demonstrate
that PA-MPPI can be used as a safe and robust action policy for navigation
foundation models, which often provide goal poses that are not directly
reachable.

</details>


### [40] [M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation](https://arxiv.org/abs/2509.14980)
*Ju Dong,Lei Zhang,Liding Zhang,Yao Ling,Yu Fu,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: M4Diffuser是一个用于移动操作的混合框架，结合多视角扩散策略和新型优化控制器，在非结构化环境中显著提高成功率和减少碰撞


<details>
  <summary>Details</summary>
Motivation: 解决单视角方法在非结构化环境中视野受限、探索能力不足的问题，以及传统控制器在奇异点附近效率低下的挑战

Method: 整合多视角扩散策略（利用本体感知状态和互补相机视角生成任务相关末端执行器目标）和ReM-QP控制器（消除松弛变量提高计算效率，加入可操作性感知偏好增强鲁棒性）

Result: 在仿真和真实环境实验中，相比基线方法成功率提高7-56%，碰撞减少3-31%，展现出强大的泛化能力和平滑的全身协调性能

Conclusion: M4Diffuser为在非结构化环境中实现可靠的移动操作提供了有效途径，通过混合框架解决了视野限制和控制器效率问题

Abstract: Mobile manipulation requires the coordinated control of a mobile base and a
robotic arm while simultaneously perceiving both global scene context and
fine-grained object details. Existing single-view approaches often fail in
unstructured environments due to limited fields of view, exploration, and
generalization abilities. Moreover, classical controllers, although stable,
struggle with efficiency and manipulability near singularities. To address
these challenges, we propose M4Diffuser, a hybrid framework that integrates a
Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP
(ReM-QP) controller for mobile manipulation. The diffusion policy leverages
proprioceptive states and complementary camera perspectives with both
close-range object details and global scene context to generate task-relevant
end-effector goals in the world frame. These high-level goals are then executed
by the ReM-QP controller, which eliminates slack variables for computational
efficiency and incorporates manipulability-aware preferences for robustness
near singularities. Comprehensive experiments in simulation and real-world
environments show that M4Diffuser achieves 7 to 56 percent higher success rates
and reduces collisions by 3 to 31 percent over baselines. Our approach
demonstrates robust performance for smooth whole-body coordination, and strong
generalization to unseen tasks, paving the way for reliable mobile manipulation
in unstructured environments. Details of the demo and supplemental material are
available on our project website https://sites.google.com/view/m4diffuser.

</details>


### [41] [The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation](https://arxiv.org/abs/2509.14984)
*João Damião Almeida,Egidio Falotico,Cecilia Laschi,José Santos-Victor*

Main category: cs.RO

TL;DR: 本文研究机器人手部不同区域触觉反馈对物体重定向任务的影响，分析触觉传感器配置如何影响深度强化学习控制策略的鲁棒性和操作效率


<details>
  <summary>Details</summary>
Motivation: 虽然指尖是常见的传感器放置位置，但手部其他区域的触觉信息贡献经常被忽视，需要研究最优触觉传感器配置来提升仿人机器人手的操作能力

Method: 通过分析手部不同区域（手指和手掌）的触觉反馈，研究其对深度强化学习控制策略鲁棒性的影响，并探讨物体特性与最优传感器配置之间的关系

Result: 识别出能够提高操作效率和准确性的触觉传感配置，为增强操作能力的仿人末端执行器设计提供有价值的见解

Conclusion: 手部多个区域的触觉反馈对精确控制至关重要，最优传感器配置取决于物体特性，研究结果为仿人机器人手的触觉传感设计提供了重要指导

Abstract: In-hand manipulation tasks, particularly in human-inspired robotic systems,
must rely on distributed tactile sensing to achieve precise control across a
wide variety of tasks. However, the optimal configuration of this network of
sensors is a complex problem, and while the fingertips are a common choice for
placing sensors, the contribution of tactile information from other regions of
the hand is often overlooked. This work investigates the impact of tactile
feedback from various regions of the fingers and palm in performing in-hand
object reorientation tasks. We analyze how sensory feedback from different
parts of the hand influences the robustness of deep reinforcement learning
control policies and investigate the relationship between object
characteristics and optimal sensor placement. We identify which tactile sensing
configurations contribute to improving the efficiency and accuracy of
manipulation. Our results provide valuable insights for the design and use of
anthropomorphic end-effectors with enhanced manipulation capabilities.

</details>


### [42] [ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task Pretraining and Fine-Tuning](https://arxiv.org/abs/2509.14992)
*Yifan Zhai,Lorenzo Terenzi,Patrick Frey,Diego Garcia Soto,Pascal Egli,Marco Hutter*

Main category: cs.RO

TL;DR: ExT是一个用于挖掘机自主操作的开源框架，通过大规模演示数据预训练和多任务策略微调，实现了从仿真到真实机器的高精度迁移和快速适应新任务的能力。


<details>
  <summary>Details</summary>
Motivation: 解决自主挖掘机部署中的挑战，传统方法需要针对每个新场景进行大量手动调优，而大规模预训练模型在挖掘领域的应用尚未充分探索。

Method: 开发ExT统一框架，包括大规模演示数据收集、预训练和多任务策略微调（监督微调和强化学习微调），支持快速适应新任务和操作条件。

Result: 预训练的ExT策略能够以厘米级精度执行完整挖掘循环，仿真到真实机器的迁移性能与专用单任务控制器相当，在仿真中展示了对新任务和机器配置的快速适应能力。

Conclusion: ExT展示了作为可扩展和通用自主挖掘基础框架的潜力，能够有效处理未见过的工地条件和新的硬件配置。

Abstract: Scaling up the deployment of autonomous excavators is of great economic and
societal importance. Yet it remains a challenging problem, as effective systems
must robustly handle unseen worksite conditions and new hardware
configurations. Current state-of-the-art approaches rely on highly engineered,
task-specific controllers, which require extensive manual tuning for each new
scenario. In contrast, recent advances in large-scale pretrained models have
shown remarkable adaptability across tasks and embodiments in domains such as
manipulation and navigation, but their applicability to heavy construction
machinery remains largely unexplored. In this work, we introduce ExT, a unified
open-source framework for large-scale demonstration collection, pretraining,
and fine-tuning of multitask excavation policies. ExT policies are first
trained on large-scale demonstrations collected from a mix of experts, then
fine-tuned either with supervised fine-tuning (SFT) or reinforcement learning
fine-tuning (RLFT) to specialize to new tasks or operating conditions. Through
both simulation and real-world experiments, we show that pretrained ExT
policies can execute complete excavation cycles with centimeter-level accuracy,
successfully transferring from simulation to real machine with performance
comparable to specialized single-task controllers. Furthermore, in simulation,
we demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new
tasks, out-of-distribution conditions, and machine configurations, while
maintaining strong performance on previously learned tasks. These results
highlight the potential of ExT to serve as a foundation for scalable and
generalizable autonomous excavation.

</details>


### [43] [Semantic-LiDAR-Inertial-Wheel Odometry Fusion for Robust Localization in Large-Scale Dynamic Environments](https://arxiv.org/abs/2509.14999)
*Haoxuan Jiang,Peicong Qian,Yusen Xie,Linwei Zheng,Xiaocong Li,Ming Liu,Jun Ma*

Main category: cs.RO

TL;DR: 提出了一种紧耦合的语义-激光雷达-惯性-轮式里程计融合框架，用于大规模动态环境中的高精度状态估计和鲁棒定位，通过语义体素地图和改进的扫描匹配算法显著减少长期轨迹漂移。


<details>
  <summary>Details</summary>
Motivation: 解决大规模动态环境中可靠、无漂移的全局定位难题，这对于自动驾驶导航至关重要。

Method: 使用紧耦合多传感器融合迭代误差状态卡尔曼滤波器(iESKF)，结合语义体素地图表示和改进的扫描匹配算法，并引入3D自适应缩放策略来调整轮式里程计测量权重。

Result: 在百万平方米自动化港口进行了3575小时的真实世界实验，结果显示系统在大规模动态环境中优于最先进的激光雷达定位方法。

Conclusion: 该框架在大规模动态环境中表现出卓越的可靠性和实用价值，为自动驾驶导航提供了有效的解决方案。

Abstract: Reliable, drift-free global localization presents significant challenges yet
remains crucial for autonomous navigation in large-scale dynamic environments.
In this paper, we introduce a tightly-coupled Semantic-LiDAR-Inertial-Wheel
Odometry fusion framework, which is specifically designed to provide
high-precision state estimation and robust localization in large-scale dynamic
environments. Our framework leverages an efficient semantic-voxel map
representation and employs an improved scan matching algorithm, which utilizes
global semantic information to significantly reduce long-term trajectory drift.
Furthermore, it seamlessly fuses data from LiDAR, IMU, and wheel odometry using
a tightly-coupled multi-sensor fusion Iterative Error-State Kalman Filter
(iESKF). This ensures reliable localization without experiencing abnormal
drift. Moreover, to tackle the challenges posed by terrain variations and
dynamic movements, we introduce a 3D adaptive scaling strategy that allows for
flexible adjustments to wheel odometry measurement weights, thereby enhancing
localization precision. This study presents extensive real-world experiments
conducted in a one-million-square-meter automated port, encompassing 3,575
hours of operational data from 35 Intelligent Guided Vehicles (IGVs). The
results consistently demonstrate that our system outperforms state-of-the-art
LiDAR-based localization methods in large-scale dynamic environments,
highlighting the framework's reliability and practical value.

</details>


### [44] [Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships](https://arxiv.org/abs/2509.15052)
*Walker Gosrich,Saurav Agarwal,Kashish Garg,Siddharth Mayya,Matthew Malencia,Mark Yim,Vijay Kumar*

Main category: cs.RO

TL;DR: 提出了一种新的多机器人任务分配方法，通过任务图建模复杂任务关系，使用网络流算法高效近似求解，并开发在线重分配算法提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统多机器人任务分配方法难以处理复杂任务优先级关系、机器人联盟协作以及任务间协调问题，需要新的建模和求解框架。

Method: 使用任务图表示任务及其关系，奖励函数建模联盟规模和前置任务效果；提出基于网络流的近似算法和在线迭代重分配算法。

Result: 在随机任务和奖励函数测试中优于混合整数求解器和贪心启发式算法，在高级仿真器中验证了基于真实物理现象的有效性。

Conclusion: 该方法能有效建模复杂任务关系，高效生成高质量任务计划，并利用任务关系提升性能，具有实际应用价值。

Abstract: We propose a new formulation for the multi-robot task allocation problem that
incorporates (a) complex precedence relationships between tasks, (b) efficient
intra-task coordination, and (c) cooperation through the formation of robot
coalitions. A task graph specifies the tasks and their relationships, and a set
of reward functions models the effects of coalition size and preceding task
performance. Maximizing task rewards is NP-hard; hence, we propose network
flow-based algorithms to approximate solutions efficiently. A novel online
algorithm performs iterative re-allocation, providing robustness to task
failures and model inaccuracies to achieve higher performance than offline
approaches. We comprehensively evaluate the algorithms in a testbed with random
missions and reward functions and compare them to a mixed-integer solver and a
greedy heuristic. Additionally, we validate the overall approach in an advanced
simulator, modeling reward functions based on realistic physical phenomena and
executing the tasks with realistic robot dynamics. Results establish efficacy
in modeling complex missions and efficiency in generating high-fidelity task
plans while leveraging task relationships.

</details>


### [45] [Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue](https://arxiv.org/abs/2509.15061)
*Xingyao Lin,Xinghao Zhu,Tianyi Lu,Sicheng Xie,Hui Zhang,Xipeng Qiu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: 提出了Ask-to-Clarify框架，通过多轮对话解决模糊指令问题，然后端到端生成低级动作，在8个真实世界任务中优于现有VLA方法


<details>
  <summary>Details</summary>
Motivation: 当前基于VLA的具身代理大多采用单向模式，接收指令后直接执行而不寻求反馈，这在指令模糊的真实场景中会失败。需要创建能够与人类沟通、协调并根据反馈调整动作的协作代理

Method: Ask-to-Clarify框架包含两个组件：用于协作的VLM和用于动作的扩散模型，通过连接模块生成扩散条件。采用两阶段知识隔离训练策略，先微调协作组件处理模糊性，然后集成动作组件

Result: 在8个真实世界任务评估中，该框架优于现有最先进的VLA方法

Conclusion: 提出的框架和训练策略为实现协作式具身代理提供了一条可行路径

Abstract: The ultimate goal of embodied agents is to create collaborators that can
interact with humans, not mere executors that passively follow instructions.
This requires agents to communicate, coordinate, and adapt their actions based
on human feedback. Recently, advances in VLAs have offered a path toward this
goal. However, most current VLA-based embodied agents operate in a one-way
mode: they receive an instruction and execute it without feedback. This
approach fails in real-world scenarios where instructions are often ambiguous.
In this paper, we address this problem with the Ask-to-Clarify framework. Our
framework first resolves ambiguous instructions by asking questions in a
multi-turn dialogue. Then it generates low-level actions end-to-end.
Specifically, the Ask-to-Clarify framework consists of two components, one VLM
for collaboration and one diffusion for action. We also introduce a connection
module that generates conditions for the diffusion based on the output of the
VLM. This module adjusts the observation by instructions to create reliable
conditions. We train our framework with a two-stage knowledge-insulation
strategy. First, we fine-tune the collaboration component using
ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the
action component while freezing the collaboration one. This preserves the
interaction abilities while fine-tuning the diffusion to generate actions. The
training strategy guarantees our framework can first ask questions, then
generate actions. During inference, a signal detector functions as a router
that helps our framework switch between asking questions and taking actions. We
evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it
outperforms existing state-of-the-art VLAs. The results suggest that our
proposed framework, along with the training strategy, provides a path toward
collaborative embodied agents.

</details>


### [46] [Energy-Constrained Navigation for Planetary Rovers under Hybrid RTG-Solar Power](https://arxiv.org/abs/2509.15062)
*Tianxin Hu,Weixiang Guo,Ruimeng Liu,Xinhang Xu,Rui Qian,Jinyu Chen,Shenghai Yuan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一个能量约束轨迹规划框架，将RTG-太阳能混合电源的累积能量预算和瞬时功率约束整合到SE(2)多项式轨迹优化中，确保轨迹平滑、动态可行且功率合规


<details>
  <summary>Details</summary>
Motivation: 未来行星探测车需要在混合电源（RTG稳定输出+太阳能可变输入）下长期运行，现有规划方法大多忽视能量可行性和瞬时功率约束，专注于地形可通行性或几何平滑度

Method: 基于物理的平移、旋转和阻力功率模型，结合子系统基础负载，在SE(2)多项式轨迹优化中同时考虑累积能量预算和瞬时功率约束

Result: 在月球类地形模拟中，该方法生成的轨迹峰值功率仅超出规定限制0.55%，而现有方法超出限制超过17%

Conclusion: 该方法为长期行星任务提供了一种原理性和实用性的能量感知自主规划方法，确保轨迹同时满足平滑性、动态可行性和功率合规性

Abstract: Future planetary exploration rovers must operate for extended durations on
hybrid power inputs that combine steady radioisotope thermoelectric generator
(RTG) output with variable solar photovoltaic (PV) availability. While
energy-aware planning has been studied for aerial and underwater robots under
battery limits, few works for ground rovers explicitly model power flow or
enforce instantaneous power constraints. Classical terrain-aware planners
emphasize slope or traversability, and trajectory optimization methods
typically focus on geometric smoothness and dynamic feasibility, neglecting
energy feasibility. We present an energy-constrained trajectory planning
framework that explicitly integrates physics-based models of translational,
rotational, and resistive power with baseline subsystem loads, under hybrid
RTG-solar input. By incorporating both cumulative energy budgets and
instantaneous power constraints into SE(2)-based polynomial trajectory
optimization, the method ensures trajectories that are simultaneously smooth,
dynamically feasible, and power-compliant. Simulation results on lunar-like
terrain show that our planner generates trajectories with peak power within
0.55 percent of the prescribed limit, while existing methods exceed limits by
over 17 percent. This demonstrates a principled and practical approach to
energy-aware autonomy for long-duration planetary missions.

</details>


### [47] [AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use](https://arxiv.org/abs/2509.15153)
*Yating Lin,Zixuan Huang,Fan Yang,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了基于扩散模型的AnoF-Diff方法，用于从时间序列数据中提取力-扭矩特征并检测异常，在嘈杂数据集上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现实世界中的流式传感器数据通常噪声大、非平稳且因任务和工具而异，现有多元时间序列异常检测方法难以直接应用于强力工具使用任务

Method: 基于扩散模型提取力-扭矩特征，采用并行异常评分评估方法，支持一步扩散的在线异常检测

Result: 在四个强力工具使用任务上，F1分数和AUROC指标均优于现有最先进方法，对噪声数据集更具鲁棒性

Conclusion: AnoF-Diff方法在强力工具使用场景中表现出优异的异常检测性能，特别适合处理现实世界中的噪声和非平稳数据

Abstract: Multivariate time-series anomaly detection, which is critical for identifying
unexpected events, has been explored in the field of machine learning for
several decades. However, directly applying these methods to data from forceful
tool use tasks is challenging because streaming sensor data in the real world
tends to be inherently noisy, exhibits non-stationary behavior, and varies
across different tasks and tools. To address these challenges, we propose a
method, AnoF-Diff, based on the diffusion model to extract force-torque
features from time-series data and use force-torque features to detect
anomalies. We compare our method with other state-of-the-art methods in terms
of F1-score and Area Under the Receiver Operating Characteristic curve (AUROC)
on four forceful tool-use tasks, demonstrating that our method has better
performance and is more robust to a noisy dataset. We also propose the method
of parallel anomaly score evaluation based on one-step diffusion and
demonstrate how our method can be used for online anomaly detection in several
forceful tool use experiments.

</details>


### [48] [Parallel Simulation of Contact and Actuation for Soft Growing Robots](https://arxiv.org/abs/2509.15180)
*Yitian Gao,Lucas Chen,Priyanka Bhovad,Sicheng Wang,Zachary Kingston,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: 本文提出了一个统一的建模框架，用于软生长机器人的生长、弯曲、驱动和障碍物接触的集成建模，开发了快速并行仿真框架，并通过设计优化和实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 软生长机器人（藤蔓机器人）在非结构化和动态环境中表现出卓越的安全交互能力，但现有研究主要关注被动变形机器人的接触规划，需要为更复杂环境中的导航增加主动转向能力。

Method: 扩展梁矩模型以包含驱动对生长过程中运动学的影响，开发快速并行仿真框架，集成藤蔓机器人的生长、弯曲、驱动和障碍物接触建模。

Result: 模型和仿真器通过真实机器人实验得到验证，设计优化任务找到了在杂乱环境中导航的藤蔓机器人设计，通过利用环境接触最小化所需执行器数量，设计对环境不确定性和制造不确定性具有鲁棒性。

Conclusion: 优化的设计在障碍物丰富的环境中成功部署，证明了所提出框架在软生长机器人设计和导航中的有效性和实用性。

Abstract: Soft growing robots, commonly referred to as vine robots, have demonstrated
remarkable ability to interact safely and robustly with unstructured and
dynamic environments. It is therefore natural to exploit contact with the
environment for planning and design optimization tasks. Previous research has
focused on planning under contact for passively deforming robots with
pre-formed bends. However, adding active steering to these soft growing robots
is necessary for successful navigation in more complex environments. To this
end, we develop a unified modeling framework that integrates vine robot growth,
bending, actuation, and obstacle contact. We extend the beam moment model to
include the effects of actuation on kinematics under growth and then use these
models to develop a fast parallel simulation framework. We validate our model
and simulator with real robot experiments. To showcase the capabilities of our
framework, we apply our model in a design optimization task to find designs for
vine robots navigating through cluttered environments, identifying designs that
minimize the number of required actuators by exploiting environmental contacts.
We show the robustness of the designs to environmental and manufacturing
uncertainties. Finally, we fabricate an optimized design and successfully
deploy it in an obstacle-rich environment.

</details>
