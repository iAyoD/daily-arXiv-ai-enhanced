{"id": "2510.02464", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.02464", "abs": "https://arxiv.org/abs/2510.02464", "authors": ["Isaac Ngui", "Courtney McBeth", "Andr\u00e9 Santos", "Grace He", "Katherine J. Mimnaugh", "James D. Motes", "Luciano Soares", "Marco Morales", "Nancy M. Amato"], "title": "ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality", "comment": null, "summary": "We propose the Extended Reality Universal Planning Toolkit (ERUPT), an\nextended reality (XR) system for interactive motion planning. Our system allows\nusers to create and dynamically reconfigure environments while they plan robot\npaths. In immersive three-dimensional XR environments, users gain a greater\nspatial understanding. XR also unlocks a broader range of natural interaction\ncapabilities, allowing users to grab and adjust objects in the environment\nsimilarly to the real world, rather than using a mouse and keyboard with the\nscene projected onto a two-dimensional computer screen. Our system integrates\nwith MoveIt, a manipulation planning framework, allowing users to send motion\nplanning requests and visualize the resulting robot paths in virtual or\naugmented reality. We provide a broad range of interaction modalities, allowing\nusers to modify objects in the environment and interact with a virtual robot.\nOur system allows operators to visualize robot motions, ensuring desired\nbehavior as it moves throughout the environment, without risk of collisions\nwithin a virtual space, and to then deploy planned paths on physical robots in\nthe real world.", "AI": {"tldr": "ERUPT\u662f\u4e00\u4e2a\u6269\u5c55\u73b0\u5b9e(XR)\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ea4\u4e92\u5f0f\u8fd0\u52a8\u89c4\u5212\uff0c\u5141\u8bb8\u7528\u6237\u5728\u89c4\u5212\u673a\u5668\u4eba\u8def\u5f84\u65f6\u521b\u5efa\u548c\u52a8\u6001\u91cd\u65b0\u914d\u7f6e\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4f7f\u7528\u9f20\u6807\u952e\u76d8\u5728\u4e8c\u7ef4\u5c4f\u5e55\u4e0a\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u7406\u89e3\u548c\u81ea\u7136\u4ea4\u4e92\u80fd\u529b\u3002XR\u6280\u672f\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u7a7a\u95f4\u611f\u77e5\u548c\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u7cfb\u7edf\u96c6\u6210\u4e86MoveIt\u64cd\u7eb5\u89c4\u5212\u6846\u67b6\uff0c\u63d0\u4f9b\u591a\u79cd\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5141\u8bb8\u7528\u6237\u5728\u865a\u62df\u6216\u589e\u5f3a\u73b0\u5b9e\u4e2d\u4fee\u6539\u73af\u5883\u5bf9\u8c61\u3001\u4e0e\u865a\u62df\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u5e76\u53ef\u89c6\u5316\u673a\u5668\u4eba\u8fd0\u52a8\u8def\u5f84\u3002", "result": "\u7528\u6237\u53ef\u4ee5\u5728\u865a\u62df\u7a7a\u95f4\u4e2d\u5b89\u5168\u5730\u53ef\u89c6\u5316\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u786e\u4fdd\u671f\u671b\u884c\u4e3a\u5e76\u907f\u514d\u78b0\u649e\uff0c\u7136\u540e\u5c06\u89c4\u5212\u597d\u7684\u8def\u5f84\u90e8\u7f72\u5230\u771f\u5b9e\u4e16\u754c\u7684\u7269\u7406\u673a\u5668\u4eba\u4e0a\u3002", "conclusion": "ERUPT\u7cfb\u7edf\u901a\u8fc7XR\u6280\u672f\u663e\u8457\u6539\u5584\u4e86\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u3001\u5b89\u5168\u7684\u89c4\u5212\u73af\u5883\uff0c\u5e76\u5b9e\u73b0\u4e86\u865a\u62df\u89c4\u5212\u5230\u7269\u7406\u90e8\u7f72\u7684\u65e0\u7f1d\u8854\u63a5\u3002"}}
{"id": "2510.02469", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02469", "abs": "https://arxiv.org/abs/2510.02469", "authors": ["Sung-Yeon Park", "Adam Lee", "Juanwu Lu", "Can Cui", "Luyang Jiang", "Rohit Gupta", "Kyungtae Han", "Ahmadreza Moradipari", "Ziran Wang"], "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting", "comment": null, "summary": "Driving scene manipulation with sensor data is emerging as a promising\nalternative to traditional virtual driving simulators. However, existing\nframeworks struggle to generate realistic scenarios efficiently due to limited\nediting capabilities. To address these challenges, we present SIMSplat, a\npredictive driving scene editor with language-aligned Gaussian splatting. As a\nlanguage-controlled editor, SIMSplat enables intuitive manipulation using\nnatural language prompts. By aligning language with Gaussian-reconstructed\nscenes, it further supports direct querying of road objects, allowing precise\nand flexible editing. Our method provides detailed object-level editing,\nincluding adding new objects and modifying the trajectories of both vehicles\nand pedestrians, while also incorporating predictive path refinement through\nmulti-agent motion prediction to generate realistic interactions among all\nagents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's\nextensive editing capabilities and adaptability across a wide range of\nscenarios. Project page: https://sungyeonparkk.github.io/simsplat/", "AI": {"tldr": "SIMSplat\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u5bf9\u9f50\u9ad8\u65af\u6cfc\u6e85\u7684\u9884\u6d4b\u6027\u9a7e\u9a76\u573a\u666f\u7f16\u8f91\u5668\uff0c\u652f\u6301\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u76f4\u89c2\u5730\u64cd\u4f5c\u9a7e\u9a76\u573a\u666f\uff0c\u80fd\u591f\u6dfb\u52a0\u65b0\u7269\u4f53\u548c\u4fee\u6539\u8f66\u8f86\u3001\u884c\u4eba\u8f68\u8ff9\uff0c\u5e76\u6574\u5408\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u751f\u6210\u903c\u771f\u7684\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u7684\u9a7e\u9a76\u573a\u666f\u7f16\u8f91\u6846\u67b6\u7531\u4e8e\u7f16\u8f91\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u9ad8\u6548\u751f\u6210\u903c\u771f\u573a\u666f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u76f4\u89c2\u64cd\u4f5c\u4e14\u652f\u6301\u7cbe\u786e\u7075\u6d3b\u7f16\u8f91\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u5bf9\u9f50\u7684\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u91cd\u5efa\u9a7e\u9a76\u573a\u666f\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u64cd\u4f5c\uff0c\u63d0\u4f9b\u8be6\u7ec6\u7684\u5bf9\u8c61\u7ea7\u7f16\u8f91\u529f\u80fd\uff0c\u5305\u62ec\u6dfb\u52a0\u65b0\u7269\u4f53\u548c\u4fee\u6539\u8f68\u8ff9\uff0c\u5e76\u6574\u5408\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u8fdb\u884c\u8def\u5f84\u4f18\u5316\u3002", "result": "\u5728Waymo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSIMSplat\u5177\u6709\u5e7f\u6cdb\u7684\u7f16\u8f91\u80fd\u529b\u548c\u8de8\u573a\u666f\u9002\u5e94\u6027\u3002", "conclusion": "SIMSplat\u901a\u8fc7\u8bed\u8a00\u5bf9\u9f50\u7684\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5b9e\u73b0\u4e86\u76f4\u89c2\u3001\u7cbe\u786e\u7684\u9a7e\u9a76\u573a\u666f\u7f16\u8f91\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u4ea4\u4e92\u573a\u666f\uff0c\u4e3a\u9a7e\u9a76\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02526", "abs": "https://arxiv.org/abs/2510.02526", "authors": ["Anamika J H", "Anujith Muraleedharan"], "title": "U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation", "comment": "8 pages, 5 figures. Accepted to the IROS 2025 Workshop on Perception\n  and Planning for Mobile Manipulation in Changing Environments", "summary": "Robots manipulating in changing environments must act on percepts that are\nlate, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer\nthat leaves the low-level controller unchanged while re-aiming task goals\n(pre-contact, contact, post) as new observations arrive. Unlike motion\nretargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming\nas a first-class, pluggable module between perception and control. Our main\ntechnical contribution is UAR-PF, an uncertainty-aware retargeter that\nmaintains a distribution over object pose under sensing lag and selects goals\nthat maximize expected progress. We instantiate a reproducible Shift x Lag\nstress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion,\nwhere the object undergoes abrupt in-plane shifts while synthetic perception\nlag is injected during approach. Across 0-10 cm shifts and 0-400 ms lags,\nUAR-PF and ICP degrade gracefully relative to a no-retarget baseline, achieving\nhigher success with modest end-effector travel and fewer aborts; simple\noperational safeguards further improve stability. Contributions: (1) UAR-PF for\nlag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting\ninterface; and (3) a reproducible Shift x Lag benchmark with evaluation on\npick, push, stacking, and peg insertion.", "AI": {"tldr": "U-LAG\u662f\u4e00\u4e2a\u4e2d\u6267\u884c\u76ee\u6807\u91cd\u5b9a\u5411\u5c42\uff0c\u80fd\u591f\u5728\u611f\u77e5\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u91cd\u65b0\u8c03\u6574\u4efb\u52a1\u76ee\u6807\uff0c\u4fdd\u6301\u5e95\u5c42\u63a7\u5236\u5668\u4e0d\u53d8\u3002\u901a\u8fc7UAR-PF\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u91cd\u5b9a\u5411\u5668\u5904\u7406\u611f\u77e5\u6ede\u540e\u4e0b\u7684\u7269\u4f53\u59ff\u6001\u5206\u5e03\uff0c\u5728Shift x Lag\u538b\u529b\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u53d8\u5316\u73af\u5883\u4e2d\u64cd\u4f5c\u65f6\uff0c\u5fc5\u987b\u5904\u7406\u5ef6\u8fdf\u3001\u566a\u58f0\u6216\u8fc7\u65f6\u7684\u611f\u77e5\u6570\u636e\uff0c\u9700\u8981\u80fd\u591f\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u9002\u5e94\u73af\u5883\u53d8\u5316\u7684\u76ee\u6807\u91cd\u5b9a\u5411\u65b9\u6cd5\u3002", "method": "\u63d0\u51faU-LAG\u6846\u67b6\uff0c\u5305\u62ecUAR-PF\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u91cd\u5b9a\u5411\u5668\uff0c\u7ef4\u62a4\u7269\u4f53\u59ff\u6001\u5206\u5e03\u5e76\u9009\u62e9\u6700\u5927\u5316\u9884\u671f\u8fdb\u5ea6\u7684\u76ee\u6807\u3002\u4f7f\u7528\u53ef\u63d2\u62d4\u7684\u91cd\u5b9a\u5411\u63a5\u53e3\uff0c\u5728PyBullet/PandaGym\u4e2d\u8fdb\u884c\u53ef\u590d\u73b0\u7684Shift x Lag\u538b\u529b\u6d4b\u8bd5\u3002", "result": "\u57280-10\u5398\u7c73\u504f\u79fb\u548c0-400\u6beb\u79d2\u5ef6\u8fdf\u6761\u4ef6\u4e0b\uff0cUAR-PF\u548cICP\u76f8\u6bd4\u65e0\u91cd\u5b9a\u5411\u57fa\u7ebf\u8868\u73b0\u66f4\u4f18\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6210\u529f\u7387\u3001\u66f4\u5c11\u672b\u7aef\u6267\u884c\u5668\u79fb\u52a8\u548c\u66f4\u5c11\u4e2d\u6b62\uff0c\u64cd\u4f5c\u5b89\u5168\u63aa\u65bd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u3002", "conclusion": "U-LAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u76ee\u6807\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u80fd\u591f\u4f18\u96c5\u5730\u5904\u7406\u611f\u77e5\u5ef6\u8fdf\u548c\u73af\u5883\u53d8\u5316\uff0c\u5728\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2510.02538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02538", "abs": "https://arxiv.org/abs/2510.02538", "authors": ["Yilin Wang", "Shangzhe Li", "Haoyi Niu", "Zhiao Huang", "Weitong Zhang", "Hao Su"], "title": "A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models", "comment": null, "summary": "We are interested in solving the problem of imitation learning with a limited\namount of real-world expert data. Existing offline imitation methods often\nstruggle with poor data coverage and severe performance degradation. We propose\na solution that leverages robot simulators to achieve online imitation\nlearning. Our sim-to-real framework is based on world models and combines\nonline imitation pretraining with offline finetuning. By leveraging online\ninteractions, our approach alleviates the data coverage limitations of offline\nmethods, leading to improved robustness and reduced performance degradation\nduring finetuning. It also enhances generalization during domain transfer. Our\nempirical results demonstrate its effectiveness, improving success rates by at\nleast 31.7% in sim-to-sim transfer and 23.3% in sim-to-real transfer over\nexisting offline imitation learning baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u673a\u5668\u4eba\u6a21\u62df\u5668\u8fdb\u884c\u5728\u7ebf\u6a21\u4eff\u5b66\u4e60\u7684sim-to-real\u6846\u67b6\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u7ed3\u5408\u5728\u7ebf\u9884\u8bad\u7ec3\u548c\u79bb\u7ebf\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u4e2d\u6570\u636e\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u4e13\u5bb6\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u9762\u4e34\u6570\u636e\u8986\u76d6\u4e0d\u8db3\u548c\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684sim-to-real\u6846\u67b6\uff0c\u7ed3\u5408\u5728\u7ebf\u6a21\u4eff\u9884\u8bad\u7ec3\u548c\u79bb\u7ebf\u5fae\u8c03\uff0c\u5229\u7528\u5728\u7ebf\u4ea4\u4e92\u7f13\u89e3\u6570\u636e\u8986\u76d6\u9650\u5236\u3002", "result": "\u5728sim-to-sim\u8f6c\u79fb\u4e2d\u6210\u529f\u7387\u63d0\u9ad8\u81f3\u5c1131.7%\uff0c\u5728sim-to-real\u8f6c\u79fb\u4e2d\u63d0\u9ad8\u81f3\u5c1123.3%\uff0c\u4f18\u4e8e\u73b0\u6709\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u6709\u6548\u7f13\u89e3\u4e86\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u7684\u6570\u636e\u8986\u76d6\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u9886\u57df\u8f6c\u79fb\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.02584", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.02584", "abs": "https://arxiv.org/abs/2510.02584", "authors": ["Mohammad Abtahi", "Navid Mojahed", "Shima Nazari"], "title": "Efficient Optimal Path Planning in Dynamic Environments Using Koopman MPC", "comment": "This work has been submitted to the ACC2026 conference", "summary": "This paper presents a data-driven model predictive control framework for\nmobile robots navigating in dynamic environments, leveraging Koopman operator\ntheory. Unlike the conventional Koopman-based approaches that focus on the\nlinearization of system dynamics only, our work focuses on finding a global\nlinear representation for the optimal path planning problem that includes both\nthe nonlinear robot dynamics and collision-avoidance constraints. We deploy\nextended dynamic mode decomposition to identify linear and bilinear Koopman\nrealizations from input-state data. Our open-loop analysis demonstrates that\nonly the bilinear Koopman model can accurately capture nonlinear state-input\ncouplings and quadratic terms essential for collision avoidance, whereas linear\nrealizations fail to do so. We formulate a quadratic program for the robot path\nplanning in the presence of moving obstacles in the lifted space and determine\nthe optimal robot action in an MPC framework. Our approach is capable of\nfinding the safe optimal action 320 times faster than a nonlinear MPC\ncounterpart that solves the path planning problem in the original state space.\nOur work highlights the potential of bilinear Koopman realizations for\nlinearization of highly nonlinear optimal control problems subject to nonlinear\nstate and input constraints to achieve computational efficiency similar to\nlinear problems.", "AI": {"tldr": "\u57fa\u4e8eKoopman\u7b97\u5b50\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\uff0c\u901a\u8fc7\u53cc\u7ebf\u6027Koopman\u6a21\u578b\u5b9e\u73b0\u975e\u7ebf\u6027\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u9ad8\u6548\u7ebf\u6027\u5316\u3002", "motivation": "\u4f20\u7edfKoopman\u65b9\u6cd5\u4ec5\u5173\u6ce8\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u7ebf\u6027\u5316\uff0c\u800c\u672c\u6587\u65e8\u5728\u4e3a\u5305\u542b\u975e\u7ebf\u6027\u673a\u5668\u4eba\u52a8\u529b\u5b66\u548c\u907f\u969c\u7ea6\u675f\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u95ee\u9898\u5bfb\u627e\u5168\u5c40\u7ebf\u6027\u8868\u793a\u3002", "method": "\u4f7f\u7528\u6269\u5c55\u52a8\u6001\u6a21\u6001\u5206\u89e3\u4ece\u8f93\u5165\u72b6\u6001\u6570\u636e\u4e2d\u8bc6\u522b\u7ebf\u6027\u548c\u53cc\u7ebf\u6027Koopman\u5b9e\u73b0\uff0c\u5728\u63d0\u5347\u7a7a\u95f4\u4e2d\u5236\u5b9a\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5728MPC\u6846\u67b6\u4e2d\u786e\u5b9a\u6700\u4f18\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u53cc\u7ebf\u6027Koopman\u6a21\u578b\u80fd\u51c6\u786e\u6355\u6349\u975e\u7ebf\u6027\u72b6\u6001-\u8f93\u5165\u8026\u5408\u548c\u907f\u969c\u6240\u9700\u7684\u4e8c\u6b21\u9879\uff0c\u800c\u7ebf\u6027\u5b9e\u73b0\u65e0\u6cd5\u505a\u5230\u3002\u8be5\u65b9\u6cd5\u6bd4\u5728\u539f\u59cb\u72b6\u6001\u7a7a\u95f4\u6c42\u89e3\u7684\u975e\u7ebf\u6027MPC\u5feb320\u500d\u3002", "conclusion": "\u53cc\u7ebf\u6027Koopman\u5b9e\u73b0\u5177\u6709\u5c06\u9ad8\u5ea6\u975e\u7ebf\u6027\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7ebf\u6027\u5316\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u7ebf\u6027\u95ee\u9898\u76f8\u4f3c\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.02594", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02594", "abs": "https://arxiv.org/abs/2510.02594", "authors": ["Ruo Chen", "David Blow", "Adnan Abdullah", "Md Jahidul Islam"], "title": "SubSense: VR-Haptic and Motor Feedback for Immersive Control in Subsea Telerobotics", "comment": "Presented at the OCEANS 2025 Great Lakes Conference", "summary": "This paper investigates the integration of haptic feedback and virtual\nreality (VR) control interfaces to enhance teleoperation and telemanipulation\nof underwater ROVs (remotely operated vehicles). Traditional ROV teleoperation\nrelies on low-resolution 2D camera feeds and lacks immersive and sensory\nfeedback, which diminishes situational awareness in complex subsea\nenvironments. We propose SubSense -- a novel VR-Haptic framework incorporating\na non-invasive feedback interface to an otherwise 1-DOF (degree of freedom)\nmanipulator, which is paired with the teleoperator's glove to provide haptic\nfeedback and grasp status. Additionally, our framework integrates end-to-end\nsoftware for managing control inputs and displaying immersive camera views\nthrough a VR platform. We validate the system through comprehensive experiments\nand user studies, demonstrating its effectiveness over conventional\nteleoperation interfaces, particularly for delicate manipulation tasks. Our\nresults highlight the potential of multisensory feedback in immersive virtual\nenvironments to significantly improve remote situational awareness and mission\nperformance, offering more intuitive and accessible ROV operations in the\nfield.", "AI": {"tldr": "\u63d0\u51faSubSense\u6846\u67b6\uff0c\u7ed3\u5408VR\u89e6\u89c9\u53cd\u9988\u589e\u5f3a\u6c34\u4e0bROV\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u901a\u8fc7\u975e\u4fb5\u5165\u5f0f\u53cd\u9988\u63a5\u53e3\u548cVR\u5e73\u53f0\u63d0\u5347\u64cd\u4f5c\u8005\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u60c5\u5883\u611f\u77e5\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfROV\u8fdc\u7a0b\u64cd\u4f5c\u4f9d\u8d56\u4f4e\u5206\u8fa8\u73872D\u6444\u50cf\u5934\uff0c\u7f3a\u4e4f\u6c89\u6d78\u611f\u548c\u611f\u5b98\u53cd\u9988\uff0c\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u964d\u4f4e\u4e86\u60c5\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5f00\u53d1SubSense VR-\u89e6\u89c9\u6846\u67b6\uff0c\u5305\u542b\u4e0e\u64cd\u4f5c\u8005\u624b\u5957\u914d\u5bf9\u7684\u975e\u4fb5\u5165\u5f0f\u53cd\u9988\u63a5\u53e3\uff0c\u63d0\u4f9b\u89e6\u89c9\u53cd\u9988\u548c\u6293\u53d6\u72b6\u6001\uff0c\u5e76\u96c6\u6210\u7aef\u5230\u7aef\u8f6f\u4ef6\u7ba1\u7406\u63a7\u5236\u8f93\u5165\u548c\u901a\u8fc7VR\u5e73\u53f0\u663e\u793a\u6c89\u6d78\u5f0f\u6444\u50cf\u5934\u89c6\u56fe\u3002", "result": "\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u7cfb\u7edf\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u8fdc\u7a0b\u64cd\u4f5c\u754c\u9762\uff0c\u5728\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u591a\u611f\u5b98\u53cd\u9988\u5728\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u63d0\u5347\u8fdc\u7a0b\u60c5\u5883\u611f\u77e5\u548c\u4efb\u52a1\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u4e3a\u73b0\u573aROV\u64cd\u4f5c\u63d0\u4f9b\u66f4\u76f4\u89c2\u548c\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02614", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02614", "abs": "https://arxiv.org/abs/2510.02614", "authors": ["Harsh Gupta", "Xiaofeng Guo", "Huy Ha", "Chuer Pan", "Muqing Cao", "Dongjae Lee", "Sebastian Sherer", "Shuran Song", "Guanya Shi"], "title": "UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies", "comment": "Result videos can be found at umi-on-air.github.io", "summary": "We introduce UMI-on-Air, a framework for embodiment-aware deployment of\nembodiment-agnostic manipulation policies. Our approach leverages diverse,\nunconstrained human demonstrations collected with a handheld gripper (UMI) to\ntrain generalizable visuomotor policies. A central challenge in transferring\nthese policies to constrained robotic embodiments-such as aerial\nmanipulators-is the mismatch in control and robot dynamics, which often leads\nto out-of-distribution behaviors and poor execution. To address this, we\npropose Embodiment-Aware Diffusion Policy (EADP), which couples a high-level\nUMI policy with a low-level embodiment-specific controller at inference time.\nBy integrating gradient feedback from the controller's tracking cost into the\ndiffusion sampling process, our method steers trajectory generation towards\ndynamically feasible modes tailored to the deployment embodiment. This enables\nplug-and-play, embodiment-aware trajectory adaptation at test time. We validate\nour approach on multiple long-horizon and high-precision aerial manipulation\ntasks, showing improved success rates, efficiency, and robustness under\ndisturbances compared to unguided diffusion baselines. Finally, we demonstrate\ndeployment in previously unseen environments, using UMI demonstrations\ncollected in the wild, highlighting a practical pathway for scaling\ngeneralizable manipulation skills across diverse-and even highly\nconstrained-embodiments. All code, data, and checkpoints will be publicly\nreleased after acceptance. Result videos can be found at umi-on-air.github.io.", "AI": {"tldr": "UMI-on-Air\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u624b\u6301\u5939\u722a\u6536\u96c6\u7684\u4eba\u7c7b\u6f14\u793a\u7b56\u7565\u90e8\u7f72\u5230\u53d7\u7ea6\u675f\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff08\u5982\u7a7a\u4e2d\u673a\u68b0\u81c2\uff09\u3002\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u7b56\u7565\u548c\u4f4e\u5c42\u7279\u5b9a\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4ece\u65e0\u7ea6\u675f\u4eba\u7c7b\u6f14\u793a\u5230\u53d7\u7ea6\u675f\u673a\u5668\u4eba\u5e73\u53f0\u90e8\u7f72\u65f6\u7684\u63a7\u5236\u548c\u52a8\u6001\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u907f\u514d\u5206\u5e03\u5916\u884c\u4e3a\u548c\u6267\u884c\u5931\u8d25\u3002", "method": "\u63d0\u51faEmbodiment-Aware Diffusion Policy (EADP)\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u9ad8\u5c42UMI\u7b56\u7565\u4e0e\u4f4e\u5c42\u7279\u5b9a\u63a7\u5236\u5668\u8026\u5408\uff0c\u901a\u8fc7\u63a7\u5236\u5668\u8ddf\u8e2a\u6210\u672c\u7684\u68af\u5ea6\u53cd\u9988\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u957f\u89c6\u91ce\u548c\u9ad8\u7cbe\u5ea6\u7a7a\u4e2d\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u65e0\u5f15\u5bfc\u6269\u6563\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u6548\u7387\u548c\u6297\u5e72\u6270\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u4e0d\u540c\uff08\u5373\u4f7f\u662f\u9ad8\u5ea6\u53d7\u7ea6\u675f\u7684\uff09\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6269\u5c55\u901a\u7528\u64cd\u4f5c\u6280\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2510.02616", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02616", "abs": "https://arxiv.org/abs/2510.02616", "authors": ["Mobin Habibpour", "Alireza Nemati", "Ali Meghdari", "Alireza Taheri", "Shima Nazari"], "title": "RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments", "comment": "Proceedings of SAI Intelligent Systems Conference 2023", "summary": "Simultaneous Localization and Mapping (SLAM) plays an important role in many\nrobotics fields, including social robots. Many of the available visual SLAM\nmethods are based on the assumption of a static world and struggle in dynamic\nenvironments. In the current study, we introduce a real-time semantic RGBD SLAM\napproach designed specifically for dynamic environments. Our proposed system\ncan effectively detect moving objects and maintain a static map to ensure\nrobust camera tracking. The key innovation of our approach is the incorporation\nof deep learning-based semantic information into SLAM systems to mitigate the\nimpact of dynamic objects. Additionally, we enhance the semantic segmentation\nprocess by integrating an Extended Kalman filter to identify dynamic objects\nthat may be temporarily idle. We have also implemented a generative network to\nfill in the missing regions of input images belonging to dynamic objects. This\nhighly modular framework has been implemented on the ROS platform and can\nachieve around 22 fps on a GTX1080. Benchmarking the developed pipeline on\ndynamic sequences from the TUM dataset suggests that the proposed approach\ndelivers competitive localization error in comparison with the state-of-the-art\nmethods, all while operating in near real-time. The source code is publicly\navailable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u52a8\u6001\u73af\u5883\u7684\u5b9e\u65f6\u8bed\u4e49RGBD SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u4fe1\u606f\u68c0\u6d4b\u79fb\u52a8\u7269\u4f53\uff0c\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u8bc6\u522b\u4e34\u65f6\u9759\u6b62\u7684\u52a8\u6001\u7269\u4f53\uff0c\u5e76\u751f\u6210\u7f51\u7edc\u586b\u8865\u52a8\u6001\u7269\u4f53\u533a\u57df\uff0c\u5728TUM\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u5b9e\u65f6\u7684\u7ade\u4e89\u6027\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9SLAM\u65b9\u6cd5\u5927\u591a\u57fa\u4e8e\u9759\u6001\u4e16\u754c\u5047\u8bbe\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u7269\u4f53\u7684SLAM\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u673a\u5668\u4eba\u7b49\u5e94\u7528\u573a\u666f\u4e2d\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u4fe1\u606f\u5230SLAM\u7cfb\u7edf\uff0c\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u589e\u5f3a\u8bed\u4e49\u5206\u5272\u4ee5\u8bc6\u522b\u4e34\u65f6\u9759\u6b62\u7684\u52a8\u6001\u7269\u4f53\uff0c\u5b9e\u73b0\u751f\u6210\u7f51\u7edc\u586b\u8865\u52a8\u6001\u7269\u4f53\u533a\u57df\uff0c\u6784\u5efa\u9ad8\u5ea6\u6a21\u5757\u5316\u7684ROS\u5e73\u53f0\u6846\u67b6\u3002", "result": "\u5728GTX1080\u4e0a\u8fbe\u5230\u7ea622fps\uff0c\u5728TUM\u6570\u636e\u96c6\u52a8\u6001\u5e8f\u5217\u4e0a\u7684\u5b9a\u4f4d\u8bef\u5dee\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u5b9e\u65f6\u7684\u8fd0\u884c\u901f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u73af\u5883\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u51c6\u786e\u7684\u5b9a\u4f4d\uff0c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.02623", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02623", "abs": "https://arxiv.org/abs/2510.02623", "authors": ["Taha Shafa", "Yiming Meng", "Melkior Ornik"], "title": "Reachable Predictive Control: A Novel Control Algorithm for Nonlinear Systems with Unknown Dynamics and its Practical Applications", "comment": null, "summary": "This paper proposes an algorithm capable of driving a system to follow a\npiecewise linear trajectory without prior knowledge of the system dynamics.\nMotivated by a critical failure scenario in which a system can experience an\nabrupt change in its dynamics, we demonstrate that it is possible to follow a\nset of waypoints comprised of states analytically proven to be reachable\ndespite not knowing the system dynamics. The proposed algorithm first applies\nsmall perturbations to locally learn the system dynamics around the current\nstate, then computes the set of states that are provably reachable using the\nlocally learned dynamics and their corresponding maximum growth-rate bounds,\nand finally synthesizes a control action that navigates the system to a\nguaranteed reachable state.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u7cfb\u7edf\u52a8\u529b\u5b66\u5148\u9a8c\u77e5\u8bc6\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u9a71\u52a8\u7cfb\u7edf\u8ddf\u8e2a\u5206\u6bb5\u7ebf\u6027\u8f68\u8ff9\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7cfb\u7edf\u52a8\u529b\u5b66\u53d1\u751f\u7a81\u53d8\u7684\u5173\u952e\u6545\u969c\u573a\u666f\u3002", "motivation": "\u9488\u5bf9\u7cfb\u7edf\u52a8\u529b\u5b66\u53ef\u80fd\u53d1\u751f\u7a81\u53d8\u7684\u5173\u952e\u6545\u969c\u573a\u666f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u4e0d\u77e5\u9053\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u8bc1\u8ddf\u8e2a\u8f68\u8ff9\u7684\u7b97\u6cd5\u3002", "method": "\u7b97\u6cd5\u9996\u5148\u65bd\u52a0\u5c0f\u6270\u52a8\u5c40\u90e8\u5b66\u4e60\u5f53\u524d\u72b6\u6001\u9644\u8fd1\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u7136\u540e\u8ba1\u7b97\u53ef\u8bc1\u660e\u53ef\u8fbe\u7684\u72b6\u6001\u96c6\u5408\u53ca\u5176\u5bf9\u5e94\u7684\u6700\u5927\u589e\u957f\u7387\u8fb9\u754c\uff0c\u6700\u540e\u5408\u6210\u63a7\u5236\u52a8\u4f5c\u5c06\u7cfb\u7edf\u5bfc\u822a\u5230\u4fdd\u8bc1\u53ef\u8fbe\u7684\u72b6\u6001\u3002", "result": "\u8bc1\u660e\u4e86\u5373\u4f7f\u4e0d\u77e5\u9053\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u4e5f\u80fd\u8ddf\u8e2a\u7531\u5206\u6790\u8bc1\u660e\u53ef\u8fbe\u7684\u72b6\u6001\u7ec4\u6210\u7684\u4e00\u7cfb\u5217\u8def\u5f84\u70b9\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u591f\u5728\u7cfb\u7edf\u52a8\u529b\u5b66\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5c40\u90e8\u5b66\u4e60\u548c\u53ef\u8fbe\u6027\u5206\u6790\uff0c\u5b9e\u73b0\u5206\u6bb5\u7ebf\u6027\u8f68\u8ff9\u7684\u53ef\u9760\u8ddf\u8e2a\u3002"}}
{"id": "2510.02624", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02624", "abs": "https://arxiv.org/abs/2510.02624", "authors": ["Qun Yang", "Soung Chang Liew"], "title": "Multi-robot Rigid Formation Navigation via Synchronous Motion and Discrete-time Communication-Control Optimization", "comment": null, "summary": "Rigid-formation navigation of multiple robots is essential for applications\nsuch as cooperative transportation. This process involves a team of\ncollaborative robots maintaining a predefined geometric configuration, such as\na square, while in motion. For untethered collaborative motion, inter-robot\ncommunication must be conducted through a wireless network. Notably, few\nexisting works offer a comprehensive solution for multi-robot formation\nnavigation executable on microprocessor platforms via wireless networks,\nparticularly for formations that must traverse complex curvilinear paths. To\naddress this gap, we introduce a novel \"hold-and-hit\" communication-control\nframework designed to work seamlessly with the widely-used Robotic Operating\nSystem (ROS) platform. The hold-and-hit framework synchronizes robot movements\nin a manner robust against wireless network delays and packet loss. It operates\nover discrete-time communication-control cycles, making it suitable for\nimplementation on contemporary microprocessors. Complementary to hold-and-hit,\nwe propose an intra-cycle optimization approach that enables rigid formations\nto closely follow desired curvilinear paths, even under the nonholonomic\nmovement constraints inherent to most vehicular robots. The combination of\nhold-and-hit and intra-cycle optimization ensures precise and reliable\nnavigation even in challenging scenarios. Simulations in a virtual environment\ndemonstrate the superiority of our method in maintaining a four-robot square\nformation along an S-shaped path, outperforming two existing approaches.\nFurthermore, real-world experiments validate the effectiveness of our\nframework: the robots maintained an inter-distance error within $\\pm 0.069m$\nand an inter-angular orientation error within $\\pm19.15^{\\circ}$ while\nnavigating along an S-shaped path at a fixed linear velocity of $0.1 m/s$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\"hold-and-hit\"\u901a\u4fe1\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408ROS\u5e73\u53f0\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u521a\u6027\u7f16\u961f\u5bfc\u822a\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u65e0\u7ebf\u7f51\u7edc\u5ef6\u8fdf\u548c\u6570\u636e\u5305\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u80fd\u591f\u5728\u5fae\u5904\u7406\u5668\u5e73\u53f0\u4e0a\u901a\u8fc7\u65e0\u7ebf\u7f51\u7edc\u6267\u884c\u590d\u6742\u66f2\u7ebf\u8def\u5f84\u591a\u673a\u5668\u4eba\u7f16\u961f\u5bfc\u822a\u7684\u5168\u9762\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\"hold-and-hit\"\u901a\u4fe1\u63a7\u5236\u6846\u67b6\u4e0eROS\u5e73\u53f0\u96c6\u6210\uff0c\u7ed3\u5408\u5468\u671f\u5185\u4f18\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u7f16\u961f\u5728\u975e\u5b8c\u6574\u8fd0\u52a8\u7ea6\u675f\u4e0b\u7cbe\u786e\u8ddf\u8e2a\u66f2\u7ebf\u8def\u5f84\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff1a\u56db\u673a\u5668\u4eba\u6b63\u65b9\u5f62\u7f16\u961f\u6cbfS\u5f62\u8def\u5f84\u5bfc\u822a\u65f6\uff0c\u95f4\u8ddd\u8bef\u5dee\u5728\u00b10.069m\u5185\uff0c\u89d2\u5ea6\u8bef\u5dee\u5728\u00b119.15\u00b0\u5185\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u5b9e\u73b0\u7cbe\u786e\u53ef\u9760\u7684\u7f16\u961f\u5bfc\u822a\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.02627", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02627", "abs": "https://arxiv.org/abs/2510.02627", "authors": ["Ruining Yang", "Yi Xu", "Yixiao Chen", "Yun Fu", "Lili Su"], "title": "A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios", "comment": null, "summary": "Accurate trajectory prediction is fundamental to autonomous driving, as it\nunderpins safe motion planning and collision avoidance in complex environments.\nHowever, existing benchmark datasets suffer from a pronounced long-tail\ndistribution problem, with most samples drawn from low-density scenarios and\nsimple straight-driving behaviors. This underrepresentation of high-density\nscenarios and safety critical maneuvers such as lane changes, overtaking and\nturning is an obstacle to model generalization and leads to overly optimistic\nevaluations. To address these challenges, we propose a novel trajectory\ngeneration framework that simultaneously enhances scenarios density and\nenriches behavioral diversity. Specifically, our approach converts continuous\nroad environments into a structured grid representation that supports\nfine-grained path planning, explicit conflict detection, and multi-agent\ncoordination. Built upon this representation, we introduce behavior-aware\ngeneration mechanisms that combine rule-based decision triggers with\nFrenet-based trajectory smoothing and dynamic feasibility constraints. This\ndesign allows us to synthesize realistic high-density scenarios and rare\nbehaviors with complex interactions that are often missing in real data.\nExtensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets\ndemonstrate that our method significantly improves both agent density and\nbehavior diversity, while preserving motion realism and scenario-level safety.\nOur synthetic data also benefits downstream trajectory prediction models and\nenhances performance in challenging high-density scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f68\u8ff9\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7f51\u683c\u8868\u793a\u548c\u57fa\u4e8e\u884c\u4e3a\u7684\u751f\u6210\u673a\u5236\uff0c\u589e\u5f3a\u573a\u666f\u5bc6\u5ea6\u548c\u884c\u4e3a\u591a\u6837\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u9884\u6d4b\u6570\u636e\u96c6\u5b58\u5728\u660e\u663e\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5927\u591a\u6570\u6837\u672c\u6765\u81ea\u4f4e\u5bc6\u5ea6\u573a\u666f\u548c\u7b80\u5355\u76f4\u7ebf\u9a7e\u9a76\u884c\u4e3a\uff0c\u7f3a\u4e4f\u9ad8\u5bc6\u5ea6\u573a\u666f\u548c\u5b89\u5168\u5173\u952e\u64cd\u4f5c\uff08\u5982\u53d8\u9053\u3001\u8d85\u8f66\u3001\u8f6c\u5f2f\uff09\u7684\u5145\u5206\u8868\u793a\uff0c\u8fd9\u963b\u788d\u4e86\u6a21\u578b\u6cdb\u5316\u5e76\u5bfc\u81f4\u8fc7\u4e8e\u4e50\u89c2\u7684\u8bc4\u4f30\u3002", "method": "\u5c06\u8fde\u7eed\u9053\u8def\u73af\u5883\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7f51\u683c\u8868\u793a\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u8def\u5f84\u89c4\u5212\u3001\u663e\u5f0f\u51b2\u7a81\u68c0\u6d4b\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3002\u57fa\u4e8e\u6b64\u8868\u793a\uff0c\u5f15\u5165\u884c\u4e3a\u611f\u77e5\u751f\u6210\u673a\u5236\uff0c\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u51b3\u7b56\u89e6\u53d1\u3001Frenet-based\u8f68\u8ff9\u5e73\u6ed1\u548c\u52a8\u6001\u53ef\u884c\u6027\u7ea6\u675f\u3002", "result": "\u5728Argoverse 1\u548cArgoverse 2\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u5bc6\u5ea6\u548c\u884c\u4e3a\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8fd0\u52a8\u771f\u5b9e\u6027\u548c\u573a\u666f\u7ea7\u5b89\u5168\u6027\u3002\u5408\u6210\u6570\u636e\u8fd8\u4f7f\u4e0b\u6e38\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u53d7\u76ca\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f68\u8ff9\u751f\u6210\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8f68\u8ff9\u9884\u6d4b\u6570\u636e\u96c6\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u80fd\u591f\u5408\u6210\u771f\u5b9e\u7684\u9ad8\u5bc6\u5ea6\u573a\u666f\u548c\u590d\u6742\u4ea4\u4e92\u7684\u7f55\u89c1\u884c\u4e3a\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u8bc4\u4f30\u548c\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2510.02716", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02716", "abs": "https://arxiv.org/abs/2510.02716", "authors": ["Junlin Zeng", "Xin Zhang", "Xiang Zhao", "Yan Pan"], "title": "A $1000\\times$ Faster LLM-enhanced Algorithm For Path Planning in Large-scale Grid Maps", "comment": null, "summary": "Path planning in grid maps, arising from various applications, has garnered\nsignificant attention. Existing methods, such as A*, Dijkstra, and their\nvariants, work well for small-scale maps but fail to address large-scale ones\ndue to high search time and memory consumption. Recently, Large Language Models\n(LLMs) have shown remarkable performance in path planning but still suffer from\nspatial illusion and poor planning performance. Among all the works, LLM-A*\n\\cite{meng2024llm} leverages LLM to generate a series of waypoints and then\nuses A* to plan the paths between the neighboring waypoints. In this way, the\ncomplete path is constructed. However, LLM-A* still suffers from high\ncomputational time for large-scale maps. To fill this gap, we conducted a deep\ninvestigation into LLM-A* and found its bottleneck, resulting in limited\nperformance. Accordingly, we design an innovative LLM-enhanced algorithm, abbr.\nas iLLM-A*. iLLM-A* includes 3 carefully designed mechanisms, including the\noptimization of A*, an incremental learning method for LLM to generate\nhigh-quality waypoints, and the selection of the appropriate waypoints for A*\nfor path planning. Finally, a comprehensive evaluation on various grid maps\nshows that, compared with LLM-A*, iLLM-A* \\textbf{1) achieves more than\n$1000\\times$ speedup on average, and up to $2349.5\\times$ speedup in the\nextreme case, 2) saves up to $58.6\\%$ of the memory cost, 3) achieves both\nobviously shorter path length and lower path length standard deviation.}", "AI": {"tldr": "\u672c\u6587\u63d0\u51faiLLM-A*\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316A*\u641c\u7d22\u3001\u589e\u91cf\u5b66\u4e60\u751f\u6210\u9ad8\u8d28\u91cf\u8def\u5f84\u70b9\u3001\u667a\u80fd\u9009\u62e9\u8def\u5f84\u70b9\u7b49\u673a\u5236\uff0c\u76f8\u6bd4LLM-A*\u5b9e\u73b0\u4e861000\u500d\u4ee5\u4e0a\u7684\u52a0\u901f\u3001\u6700\u9ad858.6%\u5185\u5b58\u8282\u7701\uff0c\u5e76\u83b7\u5f97\u66f4\u77ed\u66f4\u7a33\u5b9a\u7684\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5982A*\u3001Dijkstra\u5728\u5927\u89c4\u6a21\u5730\u56fe\u4e2d\u641c\u7d22\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u8fc7\u9ad8\uff0c\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u9519\u89c9\u548c\u89c4\u5212\u6027\u80fd\u5dee\u7684\u95ee\u9898\u3002LLM-A*\u867d\u7136\u7ed3\u5408\u4e86LLM\u548cA*\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u5730\u56fe\u4e2d\u8ba1\u7b97\u65f6\u95f4\u4ecd\u7136\u5f88\u9ad8\u3002", "method": "\u8bbe\u8ba1iLLM-A*\u7b97\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) A*\u641c\u7d22\u4f18\u5316 2) \u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u8ba9LLM\u751f\u6210\u9ad8\u8d28\u91cf\u8def\u5f84\u70b9 3) \u667a\u80fd\u9009\u62e9\u9002\u5408A*\u89c4\u5212\u7684\u8def\u5f84\u70b9", "result": "\u76f8\u6bd4LLM-A*\uff1a1) \u5e73\u5747\u52a0\u901f\u8d85\u8fc71000\u500d\uff0c\u6781\u7aef\u60c5\u51b5\u4e0b\u8fbe2349.5\u500d 2) \u6700\u9ad8\u8282\u770158.6%\u5185\u5b58 3) \u8def\u5f84\u957f\u5ea6\u66f4\u77ed\u4e14\u6807\u51c6\u5dee\u66f4\u4f4e", "conclusion": "iLLM-A*\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM-A*\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5728\u5927\u89c4\u6a21\u7f51\u683c\u5730\u56fe\u8def\u5f84\u89c4\u5212\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.02728", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02728", "abs": "https://arxiv.org/abs/2510.02728", "authors": ["Lingfeng Zhang", "Erjia Xiao", "Yuchen Zhang", "Haoxiang Fu", "Ruibin Hu", "Yanbiao Ma", "Wenbo Ding", "Long Chen", "Hangjun Ye", "Xiaoshuai Hao"], "title": "Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4", "comment": null, "summary": "Cross-modal drone navigation remains a challenging task in robotics,\nrequiring efficient retrieval of relevant images from large-scale databases\nbased on natural language descriptions. The RoboSense 2025 Track 4 challenge\naddresses this challenge, focusing on robust, natural language-guided\ncross-view image retrieval across multiple platforms (drones, satellites, and\nground cameras). Current baseline methods, while effective for initial\nretrieval, often struggle to achieve fine-grained semantic matching between\ntext queries and visual content, especially in complex aerial scenes. To\naddress this challenge, we propose a two-stage retrieval refinement method:\nCaption-Guided Retrieval System (CGRS) that enhances the baseline coarse\nranking through intelligent reranking. Our method first leverages a baseline\nmodel to obtain an initial coarse ranking of the top 20 most relevant images\nfor each query. We then use Vision-Language-Model (VLM) to generate detailed\ncaptions for these candidate images, capturing rich semantic descriptions of\ntheir visual content. These generated captions are then used in a multimodal\nsimilarity computation framework to perform fine-grained reranking of the\noriginal text query, effectively building a semantic bridge between the visual\ncontent and natural language descriptions. Our approach significantly improves\nupon the baseline, achieving a consistent 5\\% improvement across all key\nmetrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the\nchallenge, demonstrating the practical value of our semantic refinement\nstrategy in real-world robotic navigation scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u68c0\u7d22\u4f18\u5316\u65b9\u6cd5CGRS\uff0c\u901a\u8fc7\u667a\u80fd\u91cd\u6392\u5e8f\u589e\u5f3a\u57fa\u7ebf\u7c97\u6392\u7ed3\u679c\uff0c\u5728\u8de8\u6a21\u6001\u65e0\u4eba\u673a\u5bfc\u822a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u83b7\u5f97\u6311\u6218\u8d5b\u7b2c\u4e8c\u540d\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u57fa\u7ebf\u65b9\u6cd5\u5728\u590d\u6742\u7a7a\u4e2d\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u73b0\u6587\u672c\u67e5\u8be2\u4e0e\u89c6\u89c9\u5185\u5bb9\u4e4b\u95f4\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5339\u914d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u7684\u8de8\u89c6\u89d2\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u3002", "method": "\u9996\u5148\u4f7f\u7528\u57fa\u7ebf\u6a21\u578b\u83b7\u53d6\u524d20\u4e2a\u6700\u76f8\u5173\u56fe\u50cf\u7684\u7c97\u6392\u7ed3\u679c\uff0c\u7136\u540e\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u5019\u9009\u56fe\u50cf\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0\uff0c\u6700\u540e\u901a\u8fc7\u591a\u6a21\u6001\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6846\u67b6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\u3002", "result": "\u5728\u6240\u6709\u5173\u952e\u6307\u6807\uff08Recall@1\u3001Recall@5\u3001Recall@10\uff09\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e00\u81f4\u76845%\u63d0\u5347\uff0c\u5728RoboSense 2025 Track 4\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e8c\u540d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u7ec6\u5316\u7b56\u7565\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5bfc\u822a\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u8bc1\u660e\u4e86\u5728\u89c6\u89c9\u5185\u5bb9\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u6784\u5efa\u8bed\u4e49\u6865\u6881\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.02738", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02738", "abs": "https://arxiv.org/abs/2510.02738", "authors": ["Tianyu Li", "Yihan Li", "Zizhe Zhang", "Nadia Figueroa"], "title": "Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data", "comment": null, "summary": "While visuomotor policy has made advancements in recent years, contact-rich\ntasks still remain a challenge. Robotic manipulation tasks that require\ncontinuous contact demand explicit handling of compliance and force. However,\nmost visuomotor policies ignore compliance, overlooking the importance of\nphysical interaction with the real world, often leading to excessive contact\nforces or fragile behavior under uncertainty. Introducing force information\ninto vision-based imitation learning could help improve awareness of contacts,\nbut could also require a lot of data to perform well. One remedy for data\nscarcity is to generate data in simulation, yet computationally taxing\nprocesses are required to generate data good enough not to suffer from the\nSim2Real gap. In this work, we introduce a framework for generating\nforce-informed data in simulation, instantiated by a single human\ndemonstration, and show how coupling with a compliant policy improves the\nperformance of a visuomotor policy learned from synthetic data. We validate our\napproach on real-robot tasks, including non-prehensile block flipping and a\nbi-manual object moving, where the learned policy exhibits reliable contact\nmaintenance and adaptation to novel conditions. Project Website:\nhttps://flow-with-the-force-field.github.io/webpage/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u751f\u6210\u529b\u4fe1\u606f\u4eff\u771f\u6570\u636e\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u987a\u5e94\u6027\u7b56\u7565\u63d0\u5347\u4ece\u5408\u6210\u6570\u636e\u5b66\u4e60\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u9760\u63a5\u89e6\u4fdd\u6301\u548c\u65b0\u6761\u4ef6\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u663e\u5f0f\u5904\u7406\u987a\u5e94\u6027\u548c\u529b\uff0c\u4f46\u5927\u591a\u6570\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5ffd\u7565\u987a\u5e94\u6027\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u63a5\u89e6\u529b\u6216\u8106\u5f31\u884c\u4e3a\u3002\u5f15\u5165\u529b\u4fe1\u606f\u53ef\u6539\u5584\u63a5\u89e6\u611f\u77e5\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6570\u636e\u3002", "method": "\u901a\u8fc7\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u751f\u6210\u529b\u4fe1\u606f\u4eff\u771f\u6570\u636e\uff0c\u7ed3\u5408\u987a\u5e94\u6027\u7b56\u7565\u6765\u8bad\u7ec3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\uff08\u975e\u6293\u53d6\u5757\u7ffb\u8f6c\u548c\u53cc\u624b\u7269\u4f53\u79fb\u52a8\uff09\u4e2d\u9a8c\u8bc1\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u8868\u73b0\u51fa\u53ef\u9760\u7684\u63a5\u89e6\u4fdd\u6301\u548c\u65b0\u6761\u4ef6\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u4eff\u771f\u751f\u6210\u7684\u6570\u636e\u63d0\u5347\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u5927\u91cf\u771f\u5b9e\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.02803", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02803", "abs": "https://arxiv.org/abs/2510.02803", "authors": ["Yifan Liao", "Zhen Sun", "Xiaoyun Qiu", "Zixiao Zhao", "Wenbing Tang", "Xinlei He", "Xinhu Zheng", "Tianwei Zhang", "Xinyi Huang", "Xingshuo Han"], "title": "Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving", "comment": "13 pages,5 figures", "summary": "Visual Language Models (VLMs), with powerful multimodal reasoning\ncapabilities, are gradually integrated into autonomous driving by several\nautomobile manufacturers to enhance planning capability in challenging\nenvironments. However, the trajectory planning capability of VLMs in work\nzones, which often include irregular layouts, temporary traffic control, and\ndynamically changing geometric structures, is still unexplored. To bridge this\ngap, we conduct the \\textit{first} systematic study of VLMs for work zone\ntrajectory planning, revealing that mainstream VLMs fail to generate correct\ntrajectories in $68.0%$ of cases. To better understand these failures, we first\nidentify candidate patterns via subgraph mining and clustering analysis, and\nthen confirm the validity of $8$ common failure patterns through human\nverification. Building on these findings, we propose REACT-Drive, a trajectory\nplanning framework that integrates VLMs with Retrieval-Augmented Generation\n(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases\ninto constraint rules and executable trajectory planning code, while RAG\nretrieves similar patterns in new scenarios to guide trajectory generation.\nExperimental results on the ROADWork dataset show that REACT-Drive yields a\nreduction of around $3\\times$ in average displacement error relative to VLM\nbaselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the\nlowest inference time ($0.58$s) compared with other methods such as fine-tuning\n($17.90$s). We further conduct experiments using a real vehicle in 15 work zone\nscenarios in the physical world, demonstrating the strong practicality of\nREACT-Drive.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4f5c\u533a\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e3b\u6d41VLMs\u572868%\u60c5\u51b5\u4e0b\u751f\u6210\u9519\u8bef\u8f68\u8ff9\uff0c\u5e76\u63d0\u51faREACT-Drive\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u663e\u8457\u63d0\u5347\u89c4\u5212\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u5de5\u4f5c\u533a\u5177\u6709\u4e0d\u89c4\u5219\u5e03\u5c40\u3001\u4e34\u65f6\u4ea4\u901a\u63a7\u5236\u548c\u52a8\u6001\u53d8\u5316\u7684\u51e0\u4f55\u7ed3\u6784\u7b49\u590d\u6742\u7279\u5f81\uff0c\u800c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u7c7b\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u7f3a\u9677\u3002", "method": "\u63d0\u51faREACT-Drive\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff1a\u9996\u5148\u901a\u8fc7\u5b50\u56fe\u6316\u6398\u548c\u805a\u7c7b\u5206\u6790\u8bc6\u522b\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\uff0c\u7136\u540e\u5229\u7528VLMs\u5c06\u5148\u9a8c\u5931\u8d25\u6848\u4f8b\u8f6c\u5316\u4e3a\u7ea6\u675f\u89c4\u5219\u548c\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u6700\u540e\u901a\u8fc7RAG\u68c0\u7d22\u76f8\u4f3c\u6a21\u5f0f\u6307\u5bfc\u65b0\u573a\u666f\u7684\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u5728ROADWork\u6570\u636e\u96c6\u4e0a\uff0cREACT-Drive\u76f8\u6bd4VLM\u57fa\u7ebf\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\u51cf\u5c11\u7ea63\u500d\uff0c\u63a8\u7406\u65f6\u95f4\u4ec5\u4e3a0.58\u79d2\uff0c\u8fdc\u4f4e\u4e8e\u5fae\u8c03\u65b9\u6cd5\u768417.90\u79d2\u3002\u572815\u4e2a\u771f\u5b9e\u5de5\u4f5c\u533a\u573a\u666f\u7684\u7269\u7406\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5f3a\u5b9e\u7528\u6027\u3002", "conclusion": "REACT-Drive\u6709\u6548\u89e3\u51b3\u4e86VLMs\u5728\u5de5\u4f5c\u533a\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u5de5\u4f5c\u533a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.02808", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.02808", "abs": "https://arxiv.org/abs/2510.02808", "authors": ["Andreas Christou", "Elliot Lister", "Georgia Andreopoulou", "Don Mahad", "Sethu Vijayakumar"], "title": "Assist-as-needed Control for FES in Foot Drop Management", "comment": null, "summary": "Foot drop is commonly managed using Functional Electrical Stimulation (FES),\ntypically delivered via open-loop controllers with fixed stimulation\nintensities. While users may manually adjust the intensity through external\ncontrols, this approach risks overstimulation, leading to muscle fatigue and\ndiscomfort, or understimulation, which compromises dorsiflexion and increases\nfall risk. In this study, we propose a novel closed-loop FES controller that\ndynamically adjusts the stimulation intensity based on real-time toe clearance,\nproviding \"assistance as needed\". We evaluate this system by inducing foot drop\nin healthy participants and comparing the effects of the closed-loop controller\nwith a traditional open-loop controller across various walking conditions,\nincluding different speeds and surface inclinations. Kinematic data reveal that\nour closed-loop controller maintains adequate toe clearance without\nsignificantly affecting the joint angles of the hips, the knees, and the\nankles, and while using significantly lower stimulation intensities compared to\nthe open-loop controller. These findings suggest that the proposed method not\nonly matches the effectiveness of existing systems but also offers the\npotential for reduced muscle fatigue and improved long-term user comfort and\nadherence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u811a\u8dbe\u95f4\u9699\u7684\u95ed\u73afFES\u63a7\u5236\u5668\uff0c\u80fd\u52a8\u6001\u8c03\u6574\u523a\u6fc0\u5f3a\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u5f00\u73af\u63a7\u5236\u5668\u5728\u7ef4\u6301\u8db3\u591f\u811a\u8dbe\u95f4\u9699\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u523a\u6fc0\u5f3a\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5f00\u73afFES\u63a7\u5236\u5668\u91c7\u7528\u56fa\u5b9a\u523a\u6fc0\u5f3a\u5ea6\uff0c\u7528\u6237\u624b\u52a8\u8c03\u6574\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u523a\u6fc0\uff08\u808c\u8089\u75b2\u52b3\u548c\u4e0d\u9002\uff09\u6216\u523a\u6fc0\u4e0d\u8db3\uff08\u80cc\u5c48\u4e0d\u8db3\u548c\u8dcc\u5012\u98ce\u9669\u589e\u52a0\uff09\u3002", "method": "\u5f00\u53d1\u95ed\u73afFES\u63a7\u5236\u5668\uff0c\u6839\u636e\u5b9e\u65f6\u811a\u8dbe\u95f4\u9699\u52a8\u6001\u8c03\u6574\u523a\u6fc0\u5f3a\u5ea6\uff0c\u5728\u5065\u5eb7\u53c2\u4e0e\u8005\u4e2d\u8bf1\u5bfc\u8db3\u4e0b\u5782\uff0c\u6bd4\u8f83\u95ed\u73af\u4e0e\u5f00\u73af\u63a7\u5236\u5668\u5728\u4e0d\u540c\u6b65\u884c\u6761\u4ef6\uff08\u901f\u5ea6\u548c\u5730\u9762\u503e\u659c\u5ea6\uff09\u4e0b\u7684\u6548\u679c\u3002", "result": "\u95ed\u73af\u63a7\u5236\u5668\u5728\u7ef4\u6301\u8db3\u591f\u811a\u8dbe\u95f4\u9699\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u523a\u6fc0\u5f3a\u5ea6\uff0c\u4e14\u4e0d\u5f71\u54cd\u9acb\u3001\u819d\u3001\u8e1d\u5173\u8282\u89d2\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5339\u914d\u73b0\u6709\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u8fd8\u5177\u5907\u51cf\u5c11\u808c\u8089\u75b2\u52b3\u3001\u63d0\u9ad8\u957f\u671f\u7528\u6237\u8212\u9002\u5ea6\u548c\u4f9d\u4ece\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.02851", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02851", "abs": "https://arxiv.org/abs/2510.02851", "authors": ["Jeyoung Park", "Yeonsub Lim", "Seungeun Oh", "Jihong Park", "Jinho Choi", "Seong-Lyun Kim"], "title": "Action Deviation-Aware Inference for Low-Latency Wireless Robots", "comment": null, "summary": "To support latency-sensitive AI applications ranging from autonomous driving\nto industrial robot manipulation, 6G envisions distributed ML, connecting\ndistributed computational resources in edge and cloud over hyper-reliable\nlow-latency communication (HRLLC). In this setting, speculative decoding can\nfacilitate collaborative inference of models distributively deployed: an\non-device draft model locally generates drafts and a remote server-based target\nmodel verifies and corrects them, resulting lower latency. However, unlike\nautoregressive text generation, behavior cloning policies, typically used for\nembodied AI applications like robot manipulation and autonomous driving, cannot\nparallelize verification and correction for multiple drafts as each action\ndepends on observation which needs to be updated by a previous action. To this\nend, we propose Action Deviation-Aware Hybrid Inference, wherein the draft\nmodel estimates an action's need for verification and correction by the target\nmodel and selectively skips communication and computation for server\noperations. Action deviation shows a strong correlation with action's rejection\nprobability by the target model, enabling selective skipping. We derive the\npath deviation threshold that balances the transmission rate and the inference\nperformance, and we empirically show that action deviation-aware hybrid\ninference reduces uplink transmission and server operation by 40%, while\nlowering end-to-end latency by 33.32% relative to hybrid inference without\nskipping and achieving task success rate up to 97.03% of that of target model\nonly inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u4f5c\u504f\u5dee\u611f\u77e5\u7684\u6df7\u5408\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u8df3\u8fc7\u670d\u52a1\u5668\u9a8c\u8bc1\u6765\u964d\u4f4e\u5ef6\u8fdf\u548c\u4f20\u8f93\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "6G\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u9700\u8981\u652f\u6301\u5ef6\u8fdf\u654f\u611f\u7684AI\u5e94\u7528\uff0c\u4f46\u884c\u4e3a\u514b\u9686\u7b56\u7565\u65e0\u6cd5\u50cf\u6587\u672c\u751f\u6210\u90a3\u6837\u5e76\u884c\u9a8c\u8bc1\u591a\u4e2a\u8349\u6848\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u4f9d\u8d56\u4e8e\u524d\u4e00\u4e2a\u52a8\u4f5c\u66f4\u65b0\u540e\u7684\u89c2\u5bdf\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u52a8\u4f5c\u504f\u5dee\u611f\u77e5\u6df7\u5408\u63a8\u7406\uff0c\u8349\u6848\u6a21\u578b\u4f30\u8ba1\u52a8\u4f5c\u9700\u8981\u76ee\u6807\u6a21\u578b\u9a8c\u8bc1\u548c\u4fee\u6b63\u7684\u7a0b\u5ea6\uff0c\u9009\u62e9\u6027\u8df3\u8fc7\u670d\u52a1\u5668\u64cd\u4f5c\u3002\u63a8\u5bfc\u8def\u5f84\u504f\u5dee\u9608\u503c\u6765\u5e73\u8861\u4f20\u8f93\u7387\u548c\u63a8\u7406\u6027\u80fd\u3002", "result": "\u51cf\u5c11\u4e0a\u884c\u4f20\u8f93\u548c\u670d\u52a1\u5668\u64cd\u4f5c40%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e33.32%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u53ef\u8fbe\u76ee\u6807\u6a21\u578b\u5355\u72ec\u63a8\u7406\u768497.03%\u3002", "conclusion": "\u52a8\u4f5c\u504f\u5dee\u611f\u77e5\u6df7\u5408\u63a8\u7406\u80fd\u6709\u6548\u964d\u4f4e\u5206\u5e03\u5f0f\u63a8\u7406\u7684\u5ef6\u8fdf\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u76ee\u6807\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.02874", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02874", "abs": "https://arxiv.org/abs/2510.02874", "authors": ["Charith Premachandra", "U-Xuan Tan"], "title": "Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping", "comment": "Accepted and presented at the 15th International Conference on Indoor\n  Positioning and Indoor Navigation (IPIN) 2025, see\n  https://ipin-conference.org/2025/", "summary": "Traditional exteroceptive sensors in mobile robots, such as LiDARs and\ncameras often struggle to perceive the environment in poor visibility\nconditions. Recently, radar technologies, such as ultra-wideband (UWB) have\nemerged as potential alternatives due to their ability to see through adverse\nenvironmental conditions (e.g. dust, smoke and rain). However, due to the small\napertures with low directivity, the UWB radars cannot reconstruct a detailed\nimage of its field of view (FOV) using a single scan. Hence, a virtual large\naperture is synthesized by moving the radar along a mobile robot path. The\nresulting synthetic aperture radar (SAR) image is a high-definition\nrepresentation of the surrounding environment. Hence, this paper proposes a\npipeline for mobile robots to incorporate UWB radar-based SAR imaging to map an\nunknown environment. Finally, we evaluated the performance of classical feature\ndetectors: SIFT, SURF, BRISK, AKAZE and ORB to identify loop closures using UWB\nSAR images. The experiments were conducted emulating adverse environmental\nconditions. The results demonstrate the viability and effectiveness of UWB SAR\nimaging for high-resolution environmental mapping and loop closure detection\ntoward more robust and reliable robotic perception systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79fb\u52a8\u673a\u5668\u4eba\u4f7f\u7528UWB\u96f7\u8fbe\u5408\u6210\u5b54\u5f84\u6210\u50cf\u6765\u6620\u5c04\u672a\u77e5\u73af\u5883\u7684\u6d41\u7a0b\uff0c\u5e76\u5728\u6076\u52a3\u73af\u5883\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4e86\u591a\u79cd\u7ecf\u5178\u7279\u5f81\u68c0\u6d4b\u5668\u7528\u4e8e\u95ed\u73af\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u5916\u611f\u77e5\u4f20\u611f\u5668\uff08\u5982LiDAR\u548c\u76f8\u673a\uff09\u5728\u80fd\u89c1\u5ea6\u5dee\u7684\u73af\u5883\u4e2d\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u800cUWB\u96f7\u8fbe\u80fd\u591f\u7a7f\u900f\u6076\u52a3\u73af\u5883\u6761\u4ef6\uff0c\u4f46\u5355\u4e2a\u626b\u63cf\u65e0\u6cd5\u91cd\u5efa\u8be6\u7ec6\u56fe\u50cf\u3002", "method": "\u901a\u8fc7\u79fb\u52a8\u96f7\u8fbe\u6cbf\u673a\u5668\u4eba\u8def\u5f84\u5408\u6210\u865a\u62df\u5927\u5b54\u5f84\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387SAR\u56fe\u50cf\uff0c\u5e76\u8bc4\u4f30SIFT\u3001SURF\u3001BRISK\u3001AKAZE\u548cORB\u7b49\u7279\u5f81\u68c0\u6d4b\u5668\u5728UWB SAR\u56fe\u50cf\u4e0a\u7684\u95ed\u73af\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eUWB SAR\u6210\u50cf\u5728\u9ad8\u5206\u8fa8\u7387\u73af\u5883\u6620\u5c04\u548c\u95ed\u73af\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "UWB SAR\u6210\u50cf\u80fd\u591f\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02885", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02885", "abs": "https://arxiv.org/abs/2510.02885", "authors": ["Faduo Liang", "Yunfeng Yang", "Shi-Lu Dai"], "title": "Point Cloud-Based Control Barrier Functions for Model Predictive Control in Safety-Critical Navigation of Autonomous Mobile Robots", "comment": "8 pages, 8 figures, accepted to IROS2025", "summary": "In this work, we propose a novel motion planning algorithm to facilitate\nsafety-critical navigation for autonomous mobile robots. The proposed algorithm\nintegrates a real-time dynamic obstacle tracking and mapping system that\ncategorizes point clouds into dynamic and static components. For dynamic point\nclouds, the Kalman filter is employed to estimate and predict their motion\nstates. Based on these predictions, we extrapolate the future states of dynamic\npoint clouds, which are subsequently merged with static point clouds to\nconstruct the forward-time-domain (FTD) map. By combining control barrier\nfunctions (CBFs) with nonlinear model predictive control, the proposed\nalgorithm enables the robot to effectively avoid both static and dynamic\nobstacles. The CBF constraints are formulated based on risk points identified\nthrough collision detection between the predicted future states and the FTD\nmap. Experimental results from both simulated and real-world scenarios\ndemonstrate the efficacy of the proposed algorithm in complex environments. In\nsimulation experiments, the proposed algorithm is compared with two baseline\napproaches, showing superior performance in terms of safety and robustness in\nobstacle avoidance. The source code is released for the reference of the\nrobotics community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u969c\u788d\u7269\u8ffd\u8e2a\u548c\u9884\u6d4b\u7684\u5b89\u5168\u5173\u952e\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u524d\u5411\u65f6\u95f4\u57df\u5730\u56fe\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u5b89\u5168\u5bfc\u822a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b89\u5168\u907f\u969c\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9700\u8981\u540c\u65f6\u5904\u7406\u9759\u6001\u548c\u52a8\u6001\u969c\u788d\u7269\u7684\u6311\u6218\u3002", "method": "\u96c6\u6210\u5b9e\u65f6\u52a8\u6001\u969c\u788d\u7269\u8ffd\u8e2a\u4e0e\u5efa\u56fe\u7cfb\u7edf\uff0c\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u9884\u6d4b\u52a8\u6001\u70b9\u4e91\u8fd0\u52a8\u72b6\u6001\uff0c\u6784\u5efa\u524d\u5411\u65f6\u95f4\u57df\u5730\u56fe\uff0c\u5e76\u7ed3\u5408\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u4e0e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u4e24\u79cd\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u548c\u9759\u6001\u969c\u788d\u7269\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u673a\u5668\u4eba\u793e\u533a\u53c2\u8003\u3002"}}
{"id": "2510.02941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02941", "abs": "https://arxiv.org/abs/2510.02941", "authors": ["Stefano Trepella", "Mauro Martini", "No\u00e9 P\u00e9rez-Higueras", "Andrea Ostuni", "Fernando Caballero", "Luis Merino", "Marcello Chiaberge"], "title": "Metrics vs Surveys: Can Quantitative Measures Replace Human Surveys in Social Robot Navigation? A Correlation Analysis", "comment": null, "summary": "Social, also called human-aware, navigation is a key challenge for the\nintegration of mobile robots into human environments. The evaluation of such\nsystems is complex, as factors such as comfort, safety, and legibility must be\nconsidered. Human-centered assessments, typically conducted through surveys,\nprovide reliable insights but are costly, resource-intensive, and difficult to\nreproduce or compare across systems. Alternatively, numerical social navigation\nmetrics are easy to compute and facilitate comparisons, yet the community lacks\nconsensus on a standard set of metrics.\n  This work explores the relationship between numerical metrics and\nhuman-centered evaluations to identify potential correlations. If specific\nquantitative measures align with human perceptions, they could serve as\nstandardized evaluation tools, reducing the dependency on surveys. Our results\nindicate that while current metrics capture some aspects of robot navigation\nbehavior, important subjective factors remain insufficiently represented and\nnew metrics are necessary.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u793e\u4ea4\u5bfc\u822a\u4e2d\u6570\u503c\u6307\u6807\u4e0e\u4eba\u7c7b\u4e2d\u5fc3\u8bc4\u4f30\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u65e8\u5728\u627e\u5230\u80fd\u591f\u66ff\u4ee3\u6602\u8d35\u4eba\u5de5\u8c03\u67e5\u7684\u6807\u51c6\u5316\u5b9a\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5bfc\u822a\u8bc4\u4f30\u590d\u6742\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u4eba\u7c7b\u4e2d\u5fc3\u8bc4\u4f30\u53ef\u9760\u4f46\u8d44\u6e90\u5bc6\u96c6\uff0c\u6570\u503c\u6307\u6807\u6613\u4e8e\u8ba1\u7b97\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u3002\u7814\u7a76\u65e8\u5728\u627e\u5230\u4e24\u8005\u76f8\u5173\u6027\u4ee5\u964d\u4f4e\u5bf9\u4eba\u5de5\u8c03\u67e5\u7684\u4f9d\u8d56\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6570\u503c\u6307\u6807\u4e0e\u4eba\u7c7b\u4e2d\u5fc3\u8bc4\u4f30\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63a2\u7d22\u5b9a\u91cf\u6d4b\u91cf\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e4b\u95f4\u7684\u6f5c\u5728\u76f8\u5173\u6027\u3002", "result": "\u5f53\u524d\u6570\u503c\u6307\u6807\u80fd\u591f\u6355\u6349\u673a\u5668\u4eba\u5bfc\u822a\u884c\u4e3a\u7684\u67d0\u4e9b\u65b9\u9762\uff0c\u4f46\u91cd\u8981\u7684\u4e3b\u89c2\u56e0\u7d20\u4ecd\u7136\u6ca1\u6709\u5f97\u5230\u5145\u5206\u4f53\u73b0\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6307\u6807\u3002", "conclusion": "\u867d\u7136\u73b0\u6709\u6307\u6807\u90e8\u5206\u6709\u6548\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6570\u503c\u6307\u6807\u6765\u66f4\u597d\u5730\u53cd\u6620\u4eba\u7c7b\u5bf9\u793e\u4ea4\u5bfc\u822a\u7684\u4e3b\u89c2\u8bc4\u4ef7\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u3002"}}
{"id": "2510.02946", "categories": ["cs.RO", "cs.SY", "eess.SY", "93D25", "I.6.5; F.2.1"], "pdf": "https://arxiv.org/pdf/2510.02946", "abs": "https://arxiv.org/abs/2510.02946", "authors": ["Juraj Lieskovsk\u00fd", "Hijiri Akahane", "Aoto Osawa", "Jaroslav Bu\u0161ek", "Ikuo Mizuuchi", "Tom\u00e1\u0161 Vyhl\u00eddal"], "title": "Single-Rod Brachiation Robot: Mechatronic Control Design and Validation of Prejump Phases", "comment": "11 pages, 13 figures, 1 table, Accepted 27 July 2025, Available\n  online 16 Sept 2025, Version of Record 28 Sept 2025", "summary": "A complete mechatronic design of a minimal configuration brachiation robot is\npresented. The robot consists of a single rigid rod with gripper mechanisms\nattached to both ends. The grippers are used to hang the robot on a horizontal\nbar on which it swings or rotates. The motion is imposed by repositioning the\nrobot's center of mass, which is performed using a crank-slide mechanism. Based\non a non-linear model, an optimal control strategy is proposed, for\nrepositioning the center of mass in a bang-bang manner. Consequently, utilizing\nthe concept of input-output linearization, a continuous control strategy is\nproposed that takes into account the limited torque of the crank-slide\nmechanism and its geometry. An increased attention is paid to energy\naccumulation towards the subsequent jump stage of the brachiation. These two\nstrategies are validated and compared in simulations. The continuous control\nstrategy is then also implemented within a low-cost STM32-based control system,\nand both the swing and rotation stages of the brachiation motion are\nexperimentally validated.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u5c0f\u914d\u7f6e\u6446\u8361\u673a\u5668\u4eba\u7684\u5b8c\u6574\u673a\u7535\u8bbe\u8ba1\uff0c\u91c7\u7528\u5355\u521a\u6027\u6746\u548c\u4e24\u7aef\u6293\u53d6\u5668\uff0c\u901a\u8fc7\u66f2\u67c4\u6ed1\u5757\u673a\u6784\u79fb\u52a8\u8d28\u5fc3\u6765\u63a7\u5236\u8fd0\u52a8\uff0c\u5f00\u53d1\u4e86\u6700\u4f18\u548c\u8fde\u7eed\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u4e86\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u7b80\u5355\u53ef\u9760\u7684\u6446\u8361\u673a\u5668\u4eba\uff0c\u80fd\u591f\u901a\u8fc7\u63a7\u5236\u8d28\u5fc3\u4f4d\u7f6e\u5b9e\u73b0\u6446\u52a8\u548c\u65cb\u8f6c\u8fd0\u52a8\uff0c\u4e3a\u540e\u7eed\u8df3\u8dc3\u9636\u6bb5\u79ef\u7d2f\u80fd\u91cf\u3002", "method": "\u4f7f\u7528\u5355\u521a\u6027\u6746\u548c\u4e24\u7aef\u6293\u53d6\u5668\uff0c\u901a\u8fc7\u66f2\u67c4\u6ed1\u5757\u673a\u6784\u91cd\u65b0\u5b9a\u4f4d\u8d28\u5fc3\uff0c\u57fa\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u63d0\u51fabang-bang\u6700\u4f18\u63a7\u5236\u548c\u8003\u8651\u626d\u77e9\u9650\u5236\u7684\u8fde\u7eed\u63a7\u5236\u7b56\u7565\u3002", "result": "\u4e24\u79cd\u63a7\u5236\u7b56\u7565\u5728\u4eff\u771f\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u8fde\u7eed\u63a7\u5236\u7b56\u7565\u5728STM32\u63a7\u5236\u7cfb\u7edf\u4e0a\u5b9e\u73b0\uff0c\u6446\u52a8\u548c\u65cb\u8f6c\u8fd0\u52a8\u5747\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u6700\u5c0f\u914d\u7f6e\u6446\u8361\u673a\u5668\u4eba\u8bbe\u8ba1\u53ef\u884c\uff0c\u8fde\u7eed\u63a7\u5236\u7b56\u7565\u80fd\u591f\u6709\u6548\u5904\u7406\u673a\u6784\u9650\u5236\uff0c\u4e3a\u540e\u7eed\u8df3\u8dc3\u9636\u6bb5\u6210\u529f\u79ef\u7d2f\u80fd\u91cf\u3002"}}
{"id": "2510.02968", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02968", "abs": "https://arxiv.org/abs/2510.02968", "authors": ["Amir Habel", "Fawad Mehboob", "Jeffrin Sam", "Clement Fortin", "Dzmitry Tsetserukou"], "title": "YawSitter: Modeling and Controlling a Tail-Sitter UAV with Enhanced Yaw Control", "comment": null, "summary": "Achieving precise lateral motion modeling and decoupled control in hover\nremains a significant challenge for tail-sitter Unmanned Aerial Vehicles\n(UAVs), primarily due to complex aerodynamic couplings and the absence of\nwelldefined lateral dynamics. This paper presents a novel modeling and control\nstrategy that enhances yaw authority and lateral motion by introducing a\nsideslip force model derived from differential propeller slipstream effects\nacting on the fuselage under differential thrust. The resulting lateral force\nalong the body y-axis enables yaw-based lateral position control without\ninducing roll coupling. The control framework employs a YXZ Euler rotation\nformulation to accurately represent attitude and incorporate gravitational\ncomponents while directly controlling yaw in the yaxis, thereby improving\nlateral dynamic behavior and avoiding singularities. The proposed approach is\nvalidated through trajectory-tracking simulations conducted in a Unity-based\nenvironment. Tests on both rectangular and circular paths in hover mode\ndemonstrate stable performance, with low mean absolute position errors and yaw\ndeviations constrained within 5.688 degrees. These results confirm the\neffectiveness of the proposed lateral force generation model and provide a\nfoundation for the development of agile, hover-capable tail-sitter UAVs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5c3e\u5ea7\u5f0f\u65e0\u4eba\u673a\u4fa7\u5411\u8fd0\u52a8\u5efa\u6a21\u4e0e\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5dee\u52a8\u63a8\u529b\u4ea7\u751f\u7684\u4fa7\u6ed1\u529b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e0\u6eda\u8f6c\u8026\u5408\u7684\u504f\u822a\u63a7\u5236\u4fa7\u5411\u4f4d\u7f6e\u63a7\u5236\u3002", "motivation": "\u5c3e\u5ea7\u5f0f\u65e0\u4eba\u673a\u5728\u60ac\u505c\u72b6\u6001\u4e0b\u7531\u4e8e\u590d\u6742\u7684\u6c14\u52a8\u8026\u5408\u548c\u7f3a\u4e4f\u660e\u786e\u7684\u4fa7\u5411\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u4fa7\u5411\u8fd0\u52a8\u5efa\u6a21\u548c\u89e3\u8026\u63a7\u5236\u3002", "method": "\u5229\u7528\u5dee\u52a8\u87ba\u65cb\u6868\u6ed1\u6d41\u6548\u5e94\u5728\u673a\u8eab\u4ea7\u751f\u7684\u4fa7\u6ed1\u529b\u6a21\u578b\uff0c\u91c7\u7528YXZ\u6b27\u62c9\u65cb\u8f6c\u516c\u5f0f\u8868\u793a\u59ff\u6001\u5e76\u76f4\u63a5\u63a7\u5236\u504f\u822a\u8f74\uff0c\u6539\u5584\u4fa7\u5411\u52a8\u6001\u884c\u4e3a\u5e76\u907f\u514d\u5947\u70b9\u3002", "result": "\u5728Unity\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u8f68\u8ff9\u8ddf\u8e2a\u4eff\u771f\u663e\u793a\uff0c\u5728\u77e9\u5f62\u548c\u5706\u5f62\u8def\u5f84\u4e0a\u5747\u8868\u73b0\u7a33\u5b9a\uff0c\u5e73\u5747\u7edd\u5bf9\u4f4d\u7f6e\u8bef\u5dee\u4f4e\uff0c\u504f\u822a\u504f\u5dee\u63a7\u5236\u57285.688\u5ea6\u4ee5\u5185\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4fa7\u5411\u529b\u751f\u6210\u6a21\u578b\u6709\u6548\uff0c\u4e3a\u5f00\u53d1\u654f\u6377\u7684\u60ac\u505c\u80fd\u529b\u5c3e\u5ea7\u5f0f\u65e0\u4eba\u673a\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.02975", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02975", "abs": "https://arxiv.org/abs/2510.02975", "authors": ["Amir Hossein Barjini", "Jouni Mattila"], "title": "AI-Enhanced Kinematic Modeling of Flexible Manipulators Using Multi-IMU Sensor Fusion", "comment": null, "summary": "This paper presents a novel framework for estimating the position and\norientation of flexible manipulators undergoing vertical motion using multiple\ninertial measurement units (IMUs), optimized and calibrated with ground truth\ndata. The flexible links are modeled as a series of rigid segments, with joint\nangles estimated from accelerometer and gyroscope measurements acquired by\ncost-effective IMUs. A complementary filter is employed to fuse the\nmeasurements, with its parameters optimized through particle swarm optimization\n(PSO) to mitigate noise and delay. To further improve estimation accuracy,\nresidual errors in position and orientation are compensated using radial basis\nfunction neural networks (RBFNN). Experimental results validate the\neffectiveness of the proposed intelligent multi-IMU kinematic estimation\nmethod, achieving root mean square errors (RMSE) of 0.00021~m, 0.00041~m, and\n0.00024~rad for $y$, $z$, and $\\theta$, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u591a\u4e2aIMU\u4f30\u8ba1\u67d4\u6027\u673a\u68b0\u81c2\u4f4d\u7f6e\u548c\u59ff\u6001\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7PSO\u4f18\u5316\u4e92\u8865\u6ee4\u6ce2\u5668\u53c2\u6570\uff0c\u5e76\u7528RBFNN\u8865\u507f\u6b8b\u5dee\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8fd0\u52a8\u4f30\u8ba1\u3002", "motivation": "\u67d4\u6027\u673a\u68b0\u81c2\u5728\u5782\u76f4\u8fd0\u52a8\u4e2d\u7684\u7cbe\u786e\u4f4d\u7f6e\u548c\u59ff\u6001\u4f30\u8ba1\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u89e3\u51b3\u4f4e\u6210\u672cIMU\u7684\u566a\u58f0\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u9ad8\u8fd0\u52a8\u63a7\u5236\u7684\u51c6\u786e\u6027\u3002", "method": "\u5c06\u67d4\u6027\u8fde\u6746\u5efa\u6a21\u4e3a\u521a\u6027\u6bb5\uff0c\u4f7f\u7528\u4e92\u8865\u6ee4\u6ce2\u5668\u878d\u5408\u52a0\u901f\u5ea6\u8ba1\u548c\u9640\u87ba\u4eea\u6570\u636e\uff0cPSO\u4f18\u5316\u6ee4\u6ce2\u5668\u53c2\u6570\uff0cRBFNN\u8865\u507f\u4f4d\u7f6e\u548c\u59ff\u6001\u7684\u6b8b\u5dee\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728y\u3001z\u548c\u03b8\u65b9\u5411\u4e0a\u7684RMSE\u5206\u522b\u4e3a0.00021m\u30010.00041m\u548c0.00024rad\uff0c\u8fbe\u5230\u4e86\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u591aIMU\u8fd0\u52a8\u4f30\u8ba1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u67d4\u6027\u673a\u68b0\u81c2\u7684\u4f4d\u7f6e\u548c\u59ff\u6001\uff0c\u4e3a\u67d4\u6027\u673a\u68b0\u81c2\u7684\u7cbe\u786e\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.02976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02976", "abs": "https://arxiv.org/abs/2510.02976", "authors": ["Alvaro Paz", "Pauli Mustalahti", "Mohammad Dastranj", "Jouni Mattila"], "title": "Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks", "comment": null, "summary": "This paper presents a framework for real-time optimal controlling of a\nheavy-duty skid-steered mobile platform for trajectory tracking. The importance\nof accurate real-time performance of the controller lies in safety\nconsiderations of situations where the dynamic system under control is affected\nby uncertainties and disturbances, and the controller should compensate for\nsuch phenomena in order to provide stable performance. A multiple-shooting\nnonlinear model-predictive control framework is proposed in this paper. This\nframework benefits from suitable algorithm along with readings from various\nsensors for genuine real-time performance with extremely high accuracy. The\ncontroller is then tested for tracking different trajectories where it\ndemonstrates highly desirable performance in terms of both speed and accuracy.\nThis controller shows remarkable improvement when compared to existing\nnonlinear model-predictive controllers in the literature that were implemented\non skid-steered mobile platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91cd\u578b\u6ed1\u79fb\u8f6c\u5411\u79fb\u52a8\u5e73\u53f0\u8f68\u8ff9\u8ddf\u8e2a\u7684\u5b9e\u65f6\u6700\u4f18\u63a7\u5236\u6846\u67b6\uff0c\u91c7\u7528\u591a\u5c04\u51fb\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u52a8\u6001\u7cfb\u7edf\u53d7\u5230\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\u5f71\u54cd\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u51c6\u786e\u5b9e\u65f6\u7684\u63a7\u5236\u5668\u6765\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u5c04\u51fb\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u5408\u9002\u7684\u7b97\u6cd5\u548c\u5404\u79cd\u4f20\u611f\u5668\u8bfb\u6570\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u5b9e\u65f6\u9ad8\u6027\u80fd\u63a7\u5236\u3002", "result": "\u63a7\u5236\u5668\u5728\u8ddf\u8e2a\u4e0d\u540c\u8f68\u8ff9\u65f6\u8868\u73b0\u51fa\u9ad8\u5ea6\u7406\u60f3\u7684\u901f\u5ea6\u548c\u7cbe\u5ea6\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u6587\u732e\u4e2d\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u91cd\u578b\u6ed1\u79fb\u8f6c\u5411\u79fb\u52a8\u5e73\u53f0\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u5b9e\u65f6\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03011", "abs": "https://arxiv.org/abs/2510.03011", "authors": ["Chenyuan Chen", "Haoran Ding", "Ran Ding", "Tianyu Liu", "Zewen He", "Anqing Duan", "Dezhen Song", "Xiaodan Liang", "Yoshihiko Nakamura"], "title": "3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning", "comment": null, "summary": "Diffusion models, as a class of deep generative models, have recently emerged\nas powerful tools for robot skills by enabling stable training with reliable\nconvergence. In this paper, we present an end-to-end framework for generating\nlong, smooth trajectories that explicitly target high surface coverage across\nvarious industrial tasks, including polishing, robotic painting, and spray\ncoating. The conventional methods are always fundamentally constrained by their\npredefined functional forms, which limit the shapes of the trajectories they\ncan represent and make it difficult to handle complex and diverse tasks.\nMoreover, their generalization is poor, often requiring manual redesign or\nextensive parameter tuning when applied to new scenarios. These limitations\nhighlight the need for more expressive generative models, making\ndiffusion-based approaches a compelling choice for trajectory generation. By\niteratively denoising trajectories with carefully learned noise schedules and\nconditioning mechanisms, diffusion models not only ensure smooth and consistent\nmotion but also flexibly adapt to the task context. In experiments, our method\nimproves trajectory continuity, maintains high coverage, and generalizes to\nunseen shapes, paving the way for unified end-to-end trajectory learning across\nindustrial surface-processing tasks without category-specific models. On\naverage, our approach improves Point-wise Chamfer Distance by 98.2\\% and\nsmoothness by 97.0\\%, while increasing surface coverage by 61\\% compared to\nprior methods. The link to our code can be found\n\\href{https://anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md}{here}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u8f68\u8ff9\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u8868\u9762\u5904\u7406\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u8fde\u7eed\u6027\u3001\u8986\u76d6\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u51fd\u6570\u5f62\u5f0f\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u4efb\u52a1\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u8981\u4e3a\u4e0d\u540c\u573a\u666f\u624b\u52a8\u91cd\u65b0\u8bbe\u8ba1\u6216\u8c03\u53c2\u3002\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u8f68\u8ff9\uff0c\u7ed3\u5408\u7cbe\u5fc3\u5b66\u4e60\u7684\u566a\u58f0\u8c03\u5ea6\u548c\u6761\u4ef6\u673a\u5236\uff0c\u786e\u4fdd\u5e73\u6ed1\u4e00\u81f4\u7684\u8fd0\u52a8\u5e76\u7075\u6d3b\u9002\u5e94\u4efb\u52a1\u4e0a\u4e0b\u6587\u3002", "result": "\u5e73\u5747\u6539\u8fdb\u70b9\u5411Chamfer\u8ddd\u79bb98.2%\uff0c\u5e73\u6ed1\u5ea697.0%\uff0c\u8868\u9762\u8986\u76d6\u7387\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad861%\uff0c\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5f62\u72b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u8868\u9762\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u8f68\u8ff9\u5b66\u4e60\u65b9\u6848\uff0c\u65e0\u9700\u7c7b\u522b\u7279\u5b9a\u6a21\u578b\u3002"}}
{"id": "2510.03022", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03022", "abs": "https://arxiv.org/abs/2510.03022", "authors": ["Rui Zhong", "Yizhe Sun", "Junjie Wen", "Jinming Li", "Chuang Cheng", "Wei Dai", "Zhiwen Zeng", "Huimin Lu", "Yichen Zhu", "Yi Xu"], "title": "HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton", "comment": null, "summary": "A significant bottleneck in humanoid policy learning is the acquisition of\nlarge-scale, diverse datasets, as collecting reliable real-world data remains\nboth difficult and cost-prohibitive. To address this limitation, we introduce\nHumanoidExo, a novel system that transfers human motion to whole-body humanoid\ndata. HumanoidExo offers a high-efficiency solution that minimizes the\nembodiment gap between the human demonstrator and the robot, thereby tackling\nthe scarcity of whole-body humanoid data. By facilitating the collection of\nmore voluminous and diverse datasets, our approach significantly enhances the\nperformance of humanoid robots in dynamic, real-world scenarios. We evaluated\nour method across three challenging real-world tasks: table-top manipulation,\nmanipulation integrated with stand-squat motions, and whole-body manipulation.\nOur results empirically demonstrate that HumanoidExo is a crucial addition to\nreal-robot data, as it enables the humanoid policy to generalize to novel\nenvironments, learn complex whole-body control from only five real-robot\ndemonstrations, and even acquire new skills (i.e., walking) solely from\nHumanoidExo data.", "AI": {"tldr": "HumanoidExo\u7cfb\u7edf\u901a\u8fc7\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8f6c\u6362\u4e3a\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u9762\u4e34\u7684\u4e3b\u8981\u74f6\u9888\u662f\u83b7\u53d6\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u56e0\u4e3a\u6536\u96c6\u53ef\u9760\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u65e2\u56f0\u96be\u53c8\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5f00\u53d1\u4e86HumanoidExo\u7cfb\u7edf\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4eba\u7c7b\u6f14\u793a\u8005\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4f53\u73b0\u5dee\u8ddd\uff0c\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8f6c\u6362\u4e3a\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff1a\u684c\u9762\u64cd\u4f5c\u3001\u7ed3\u5408\u7ad9\u7acb-\u8e72\u4e0b\u52a8\u4f5c\u7684\u64cd\u4f5c\u4ee5\u53ca\u5168\u8eab\u64cd\u4f5c\u3002\u7ed3\u679c\u8868\u660eHumanoidExo\u80fd\u591f\u4f7f\u4eba\u5f62\u7b56\u7565\u6cdb\u5316\u5230\u65b0\u73af\u5883\uff0c\u4ec5\u75285\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u6f14\u793a\u5c31\u80fd\u5b66\u4e60\u590d\u6742\u7684\u5168\u8eab\u63a7\u5236\uff0c\u751a\u81f3\u4ec5\u4eceHumanoidExo\u6570\u636e\u5c31\u80fd\u5b66\u4e60\u65b0\u6280\u80fd\uff08\u5982\u884c\u8d70\uff09\u3002", "conclusion": "HumanoidExo\u662f\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u7684\u91cd\u8981\u8865\u5145\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03031", "abs": "https://arxiv.org/abs/2510.03031", "authors": ["Yufei Zhu", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics", "comment": "IEEE Robotics and Automation Letters", "summary": "Long-term human motion prediction (LHMP) is important for the safe and\nefficient operation of autonomous robots and vehicles in environments shared\nwith humans. Accurate predictions are important for applications including\nmotion planning, tracking, human-robot interaction, and safety monitoring. In\nthis paper, we exploit Maps of Dynamics (MoDs), which encode spatial or\nspatio-temporal motion patterns as environment features, to achieve LHMP for\nhorizons of up to 60 seconds. We propose an MoD-informed LHMP framework that\nsupports various types of MoDs and includes a ranking method to output the most\nlikely predicted trajectory, improving practical utility in robotics. Further,\na time-conditioned MoD is introduced to capture motion patterns that vary\nacross different times of day. We evaluate MoD-LHMP instantiated with three\ntypes of MoDs. Experiments on two real-world datasets show that MoD-informed\nmethod outperforms learning-based ones, with up to 50\\% improvement in average\ndisplacement error, and the time-conditioned variant achieves the highest\naccuracy overall. Project code is available at\nhttps://github.com/test-bai-cpu/LHMP-with-MoDs.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u5730\u56fe(MoDs)\u7684\u957f\u671f\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u73af\u5883\u52a8\u6001\u7279\u5f81\u6765\u9884\u6d4b\u957f\u8fbe60\u79d2\u7684\u4eba\u4f53\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u6761\u4ef6MoD\u6765\u6355\u6349\u4e0d\u540c\u65f6\u6bb5\u7684\u8fd0\u52a8\u6a21\u5f0f\u53d8\u5316\u3002", "motivation": "\u957f\u671f\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u5bf9\u4e8e\u81ea\u4e3b\u673a\u5668\u4eba\u548c\u8f66\u8f86\u5728\u4e0e\u4eba\u7c7b\u5171\u4eab\u73af\u5883\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u51c6\u786e\u7684\u9884\u6d4b\u5bf9\u8fd0\u52a8\u89c4\u5212\u3001\u8ddf\u8e2a\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u5b89\u5168\u76d1\u63a7\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u63d0\u51faMoD-informed LHMP\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u7c7b\u578b\u7684\u52a8\u6001\u5730\u56fe\uff0c\u5305\u542b\u6392\u5e8f\u65b9\u6cd5\u8f93\u51fa\u6700\u53ef\u80fd\u7684\u9884\u6d4b\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u6761\u4ef6MoD\u6765\u6355\u6349\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u8fd0\u52a8\u6a21\u5f0f\u53d8\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoD-informed\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\u63d0\u5347\u9ad8\u8fbe50%\uff0c\u65f6\u95f4\u6761\u4ef6\u53d8\u4f53\u5728\u6240\u6709\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u52a8\u6001\u5730\u56fe\u80fd\u591f\u6709\u6548\u63d0\u5347\u957f\u671f\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u65f6\u95f4\u6761\u4ef6MoD\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03081", "abs": "https://arxiv.org/abs/2510.03081", "authors": ["Guiliang Liu", "Bo Yue", "Yi Jin Kim", "Kui Jia"], "title": "Embracing Evolution: A Call for Body-Control Co-Design in Embodied Humanoid Robot", "comment": null, "summary": "Humanoid robots, as general-purpose physical agents, must integrate both\nintelligent control and adaptive morphology to operate effectively in diverse\nreal-world environments. While recent research has focused primarily on\noptimizing control policies for fixed robot structures, this position paper\nargues for evolving both control strategies and humanoid robots' physical\nstructure under a co-design mechanism. Inspired by biological evolution, this\napproach enables robots to iteratively adapt both their form and behavior to\noptimize performance within task-specific and resource-constrained contexts.\nDespite its promise, co-design in humanoid robotics remains a relatively\nunderexplored domain, raising fundamental questions about its feasibility and\nnecessity in achieving true embodied intelligence. To address these challenges,\nwe propose practical co-design methodologies grounded in strategic exploration,\nSim2Real transfer, and meta-policy learning. We further argue for the essential\nrole of co-design by analyzing it from methodological, application-driven, and\ncommunity-oriented perspectives. Striving to guide and inspire future studies,\nwe present open research questions, spanning from short-term innovations to\nlong-term goals. This work positions co-design as a cornerstone for developing\nthe next generation of intelligent and adaptable humanoid agents.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5728\u4eba\u5f62\u673a\u5668\u4eba\u4e2d\u91c7\u7528\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316\u63a7\u5236\u7b56\u7565\u548c\u7269\u7406\u7ed3\u6784\uff0c\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u5177\u8eab\u667a\u80fd\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u56fa\u5b9a\u673a\u5668\u4eba\u7ed3\u6784\u7684\u63a7\u5236\u7b56\u7565\u4f18\u5316\uff0c\u4f46\u4eba\u5f62\u673a\u5668\u4eba\u4f5c\u4e3a\u901a\u7528\u7269\u7406\u4ee3\u7406\uff0c\u9700\u8981\u540c\u65f6\u5177\u5907\u667a\u80fd\u63a7\u5236\u548c\u9002\u5e94\u6027\u5f62\u6001\u624d\u80fd\u5728\u591a\u6837\u5316\u73b0\u5b9e\u73af\u5883\u4e2d\u6709\u6548\u8fd0\u4f5c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6218\u7565\u63a2\u7d22\u3001Sim2Real\u8fc1\u79fb\u548c\u5143\u7b56\u7565\u5b66\u4e60\u7684\u5b9e\u7528\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4ece\u65b9\u6cd5\u8bba\u3001\u5e94\u7528\u9a71\u52a8\u548c\u793e\u533a\u5bfc\u5411\u4e09\u4e2a\u89d2\u5ea6\u5206\u6790\u534f\u540c\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002", "result": "\u786e\u7acb\u4e86\u534f\u540c\u8bbe\u8ba1\u4f5c\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u667a\u80fd\u548c\u9002\u5e94\u6027\u4eba\u5f62\u4ee3\u7406\u7684\u57fa\u77f3\u5730\u4f4d\uff0c\u5e76\u63d0\u51fa\u4e86\u4ece\u77ed\u671f\u521b\u65b0\u5230\u957f\u671f\u76ee\u6807\u7684\u5f00\u653e\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u534f\u540c\u8bbe\u8ba1\u662f\u5b9e\u73b0\u771f\u6b63\u5177\u8eab\u667a\u80fd\u7684\u5173\u952e\u8def\u5f84\uff0c\u9700\u8981\u540c\u65f6\u8fdb\u5316\u673a\u5668\u4eba\u7684\u5f62\u6001\u548c\u884c\u4e3a\uff0c\u4ee5\u5728\u4efb\u52a1\u7279\u5b9a\u548c\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2510.03119", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03119", "abs": "https://arxiv.org/abs/2510.03119", "authors": ["Chaoxiang Ye", "Guido de Croon", "Salua Hamaza"], "title": "Whisker-based Tactile Flight for Tiny Drones", "comment": null, "summary": "Tiny flying robots hold great potential for search-and-rescue, safety\ninspections, and environmental monitoring, but their small size limits\nconventional sensing-especially with poor-lighting, smoke, dust or reflective\nobstacles. Inspired by nature, we propose a lightweight, 3.2-gram,\nwhisker-based tactile sensing apparatus for tiny drones, enabling them to\nnavigate and explore through gentle physical interaction. Just as rats and\nmoles use whiskers to perceive surroundings, our system equips drones with\ntactile perception in flight, allowing obstacle sensing even in pitch-dark\nconditions. The apparatus uses barometer-based whisker sensors to detect\nobstacle locations while minimising destabilisation. To address sensor noise\nand drift, we develop a tactile depth estimation method achieving sub-6 mm\naccuracy. This enables drones to navigate, contour obstacles, and explore\nconfined spaces solely through touch-even in total darkness along both soft and\nrigid surfaces. Running fully onboard a 192-KB RAM microcontroller, the system\nsupports autonomous tactile flight and is validated in both simulation and\nreal-world tests. Our bio-inspired approach redefines vision-free navigation,\nopening new possibilities for micro aerial vehicles in extreme environments.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4ec5\u91cd3.2\u514b\u7684\u80e1\u987b\u5f0f\u89e6\u89c9\u4f20\u611f\u88c5\u7f6e\uff0c\u4f7f\u5fae\u578b\u65e0\u4eba\u673a\u80fd\u591f\u5728\u5b8c\u5168\u9ed1\u6697\u7b49\u6076\u52a3\u73af\u5883\u4e2d\u901a\u8fc7\u89e6\u89c9\u611f\u77e5\u8fdb\u884c\u5bfc\u822a\u548c\u63a2\u7d22\u3002", "motivation": "\u5fae\u578b\u98de\u884c\u673a\u5668\u4eba\u5728\u641c\u6551\u3001\u5b89\u5168\u68c0\u67e5\u548c\u73af\u5883\u76d1\u6d4b\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5c0f\u5c3a\u5bf8\u9650\u5236\u4e86\u4f20\u7edf\u4f20\u611f\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5149\u7ebf\u5dee\u3001\u70df\u96fe\u3001\u7070\u5c18\u6216\u53cd\u5149\u969c\u788d\u7269\u73af\u5883\u4e2d\u3002\u53d7\u81ea\u7136\u754c\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u89e6\u89c9\u611f\u77e5\u7cfb\u7edf\u6765\u514b\u670d\u89c6\u89c9\u9650\u5236\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6c14\u538b\u8ba1\u7684\u80e1\u987b\u4f20\u611f\u5668\u68c0\u6d4b\u969c\u788d\u7269\u4f4d\u7f6e\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u98de\u884c\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002\u5f00\u53d1\u4e86\u89e6\u89c9\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5904\u7406\u4f20\u611f\u5668\u566a\u58f0\u548c\u6f02\u79fb\u95ee\u9898\u3002\u7cfb\u7edf\u5728192KB RAM\u5fae\u63a7\u5236\u5668\u4e0a\u5b8c\u5168\u81ea\u4e3b\u8fd0\u884c\u3002", "result": "\u89e6\u89c9\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u8fbe\u5230\u4e9a6\u6beb\u7c73\uff0c\u4f7f\u65e0\u4eba\u673a\u80fd\u591f\u4ec5\u901a\u8fc7\u89e6\u89c9\u8fdb\u884c\u5bfc\u822a\u3001\u6cbf\u969c\u788d\u7269\u8f6e\u5ed3\u98de\u884c\u548c\u63a2\u7d22\u53d7\u9650\u7a7a\u95f4\uff0c\u5373\u4f7f\u5728\u5b8c\u5168\u9ed1\u6697\u4e2d\u4e5f\u80fd\u6cbf\u8f6f\u786c\u8868\u9762\u98de\u884c\u3002\u7cfb\u7edf\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u8fd9\u79cd\u4eff\u751f\u65b9\u6cd5\u91cd\u65b0\u5b9a\u4e49\u4e86\u65e0\u89c6\u89c9\u5bfc\u822a\uff0c\u4e3a\u5fae\u578b\u98de\u884c\u5668\u5728\u6781\u7aef\u73af\u5883\u4e2d\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.03123", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03123", "abs": "https://arxiv.org/abs/2510.03123", "authors": ["Zhe Shen"], "title": "Learning Stability Certificate for Robotics in Real-World Environments", "comment": null, "summary": "Stability certificates play a critical role in ensuring the safety and\nreliability of robotic systems. However, deriving these certificates for\ncomplex, unknown systems has traditionally required explicit knowledge of\nsystem dynamics, often making it a daunting task. This work introduces a novel\nframework that learns a Lyapunov function directly from trajectory data,\nenabling the certification of stability for autonomous systems without needing\ndetailed system models. By parameterizing the Lyapunov candidate using a neural\nnetwork and ensuring positive definiteness through Cholesky factorization, our\napproach automatically identifies whether the system is stable under the given\ntrajectory. To address the challenges posed by noisy, real-world data, we allow\nfor controlled violations of the stability condition, focusing on maintaining\nhigh confidence in the stability certification process. Our results demonstrate\nthat this framework can provide data-driven stability guarantees, offering a\nrobust method for certifying the safety of robotic systems in dynamic,\nreal-world environments. This approach works without access to the internal\ncontrol algorithms, making it applicable even in situations where system\nbehavior is opaque or proprietary. The tool for learning the stability proof is\nopen-sourced by this research: https://github.com/HansOersted/stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u8f68\u8ff9\u6570\u636e\u76f4\u63a5\u5b66\u4e60\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u7684\u65b0\u6846\u67b6\uff0c\u65e0\u9700\u7cfb\u7edf\u52a8\u529b\u5b66\u6a21\u578b\u5373\u53ef\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u7a33\u5b9a\u6027\u8ba4\u8bc1", "motivation": "\u4f20\u7edf\u7a33\u5b9a\u6027\u8bc1\u4e66\u63a8\u5bfc\u9700\u8981\u660e\u786e\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\u77e5\u8bc6\uff0c\u5bf9\u4e8e\u590d\u6742\u672a\u77e5\u7cfb\u7edf\u6765\u8bf4\u5f88\u56f0\u96be\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u6570\u636e\u76f4\u63a5\u5b66\u4e60\u7a33\u5b9a\u6027\u8bc1\u4e66\u7684\u65b9\u6cd5", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u674e\u96c5\u666e\u8bfa\u592b\u5019\u9009\u51fd\u6570\uff0c\u901a\u8fc7Cholesky\u5206\u89e3\u786e\u4fdd\u6b63\u5b9a\u6027\uff0c\u5141\u8bb8\u7a33\u5b9a\u6027\u6761\u4ef6\u7684\u53d7\u63a7\u8fdd\u53cd\u4ee5\u5904\u7406\u566a\u58f0\u6570\u636e", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5728\u52a8\u6001\u771f\u5b9e\u73af\u5883\u4e2d\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u9c81\u68d2\u7684\u7a33\u5b9a\u6027\u8ba4\u8bc1", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bbf\u95ee\u5185\u90e8\u63a7\u5236\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u7cfb\u7edf\u884c\u4e3a\u4e0d\u900f\u660e\u6216\u4e13\u6709\u7684\u60c5\u51b5\uff0c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7a33\u5b9a\u6027\u8ba4\u8bc1\u5de5\u5177"}}
{"id": "2510.03142", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03142", "abs": "https://arxiv.org/abs/2510.03142", "authors": ["Tianyu Xu", "Jiawei Chen", "Jiazhao Zhang", "Wenyao Zhang", "Zekun Qi", "Minghan Li", "Zhizheng Zhang", "He Wang"], "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning", "comment": "Project page: https://pku-epic.github.io/MM-Nav-Web/", "summary": "Visual navigation policy is widely regarded as a promising direction, as it\nmimics humans by using egocentric visual observations for navigation. However,\noptical information of visual observations is difficult to be explicitly\nmodeled like LiDAR point clouds or depth maps, which subsequently requires\nintelligent models and large-scale data. To this end, we propose to leverage\nthe intelligence of the Vision-Language-Action (VLA) model to learn diverse\nnavigation capabilities from synthetic expert data in a teacher-student manner.\nSpecifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360\nobservations) based on pretrained large language models and visual foundation\nmodels. For large-scale navigation data, we collect expert data from three\nreinforcement learning (RL) experts trained with privileged depth information\nin three challenging tailor-made environments for different navigation\ncapabilities: reaching, squeezing, and avoiding. We iteratively train our VLA\nmodel using data collected online from RL experts, where the training ratio is\ndynamically balanced based on performance on individual capabilities. Through\nextensive experiments in synthetic environments, we demonstrate that our model\nachieves strong generalization capability. Moreover, we find that our student\nVLA model outperforms the RL teachers, demonstrating the synergistic effect of\nintegrating multiple capabilities. Extensive real-world experiments further\nconfirm the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51faMM-Nav\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u65b9\u5f0f\u4ece\u5408\u6210\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u591a\u6837\u5316\u5bfc\u822a\u80fd\u529b\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\u96be\u4ee5\u50cf\u6fc0\u5149\u96f7\u8fbe\u6216\u6df1\u5ea6\u56fe\u90a3\u6837\u663e\u5f0f\u5efa\u6a21\uff0c\u9700\u8981\u667a\u80fd\u6a21\u578b\u548c\u5927\u89c4\u6a21\u6570\u636e\uff0c\u56e0\u6b64\u5229\u7528VLA\u6a21\u578b\u7684\u667a\u80fd\u4ece\u5408\u6210\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u591a\u89c6\u56feVLA\u6a21\u578bMM-Nav\uff0c\u4ece\u4e09\u4e2a\u5177\u6709\u7279\u6743\u6df1\u5ea6\u4fe1\u606f\u7684RL\u4e13\u5bb6\u6536\u96c6\u6570\u636e\uff0c\u5728\u4e09\u4e2a\u5b9a\u5236\u73af\u5883\u4e2d\u5206\u522b\u8bad\u7ec3\u5230\u8fbe\u3001\u6324\u538b\u548c\u907f\u969c\u80fd\u529b\uff0c\u91c7\u7528\u52a8\u6001\u5e73\u8861\u8bad\u7ec3\u6bd4\u4f8b\u7684\u5728\u7ebf\u6570\u636e\u8fed\u4ee3\u8bad\u7ec3\u3002", "result": "\u5728\u5408\u6210\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b66\u751fVLA\u6a21\u578b\u751a\u81f3\u8d85\u8d8a\u4e86RL\u6559\u5e08\u6a21\u578b\uff0c\u4f53\u73b0\u4e86\u591a\u79cd\u80fd\u529b\u6574\u5408\u7684\u534f\u540c\u6548\u5e94\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7VLA\u6a21\u578b\u548c\u5408\u6210\u4e13\u5bb6\u6570\u636e\u7684\u6559\u5e08-\u5b66\u751f\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u5bfc\u822a\u80fd\u529b\u7684\u6709\u6548\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u5bfc\u822a\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.03169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03169", "abs": "https://arxiv.org/abs/2510.03169", "authors": ["Duanjiao Li", "Yun Chen", "Ying Zhang", "Junwen Yao", "Dongyue Huang", "Jianguo Zhang", "Ning Ding"], "title": "Optimal Smooth Coverage Trajectory Planning for Quadrotors in Cluttered Environment", "comment": "This paper has been accepted for publication in the 44th Chinese\n  Control Conference, 2025. Please cite the paper using appropriate formats", "summary": "For typical applications of UAVs in power grid scenarios, we construct the\nproblem as planning UAV trajectories for coverage in cluttered environments. In\nthis paper, we propose an optimal smooth coverage trajectory planning\nalgorithm. The algorithm consists of two stages. In the front-end, a Genetic\nAlgorithm (GA) is employed to solve the Traveling Salesman Problem (TSP) for\nPoints of Interest (POIs), generating an initial sequence of optimized visiting\npoints. In the back-end, the sequence is further optimized by considering\ntrajectory smoothness, time consumption, and obstacle avoidance. This is\nformulated as a nonlinear least squares problem and solved to produce a smooth\ncoverage trajectory that satisfies these constraints. Numerical simulations\nvalidate the effectiveness of the proposed algorithm, ensuring UAVs can\nsmoothly cover all POIs in cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u5e73\u6ed1\u8986\u76d6\u8f68\u8ff9\u89c4\u5212\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u7b97\u6cd5\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u89e3\u51b3TSP\u95ee\u9898\u548c\u8f68\u8ff9\u5e73\u6ed1\u4f18\u5316\u3002", "motivation": "\u9488\u5bf9\u65e0\u4eba\u673a\u5728\u7535\u7f51\u7b49\u590d\u6742\u73af\u5883\u4e2d\u7684\u8986\u76d6\u5e94\u7528\u9700\u6c42\uff0c\u9700\u8981\u89c4\u5212\u65e2\u80fd\u9ad8\u6548\u8bbf\u95ee\u6240\u6709\u5174\u8da3\u70b9\u53c8\u6ee1\u8db3\u5e73\u6ed1\u6027\u3001\u907f\u969c\u7b49\u7ea6\u675f\u7684\u8f68\u8ff9\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u524d\u7aef\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u6c42\u89e3TSP\u95ee\u9898\u751f\u6210\u521d\u59cb\u8bbf\u95ee\u5e8f\u5217\uff0c\u540e\u7aef\u901a\u8fc7\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u4f18\u5316\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u3001\u65f6\u95f4\u548c\u907f\u969c\u7ea6\u675f\u3002", "result": "\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5e73\u6ed1\u8986\u76d6\u6240\u6709\u5174\u8da3\u70b9\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e73\u6ed1\u8986\u76d6\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2510.03182", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.03182", "abs": "https://arxiv.org/abs/2510.03182", "authors": ["Yilun Hao", "Yongchao Chen", "Chuchu Fan", "Yang Zhang"], "title": "Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning", "comment": "30 pages, 5 figures, 5 tables", "summary": "Vision Language Models (VLMs) show strong potential for visual planning but\nstruggle with precise spatial and long-horizon reasoning. In contrast, Planning\nDomain Definition Language (PDDL) planners excel at long-horizon formal\nplanning, but cannot interpret visual inputs. Recent works combine these\ncomplementary advantages by enabling VLMs to turn visual planning problems into\nPDDL files for formal planning. However, while VLMs can generate PDDL problem\nfiles satisfactorily, they struggle to accurately generate the PDDL domain\nfiles, which describe all the planning rules. As a result, prior methods rely\non human experts to predefine domain files or on constant environment access\nfor refinement. We propose VLMFP, a Dual-VLM-guided framework that can\nautonomously generate both PDDL problem and domain files for formal visual\nplanning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A\nSimVLM that simulates action consequences based on input rule descriptions, and\na GenVLM that generates and iteratively refines PDDL files by comparing the\nPDDL and SimVLM execution results. VLMFP unleashes multiple levels of\ngeneralizability: The same generated PDDL domain file works for all the\ndifferent instances under the same problem, and VLMs generalize to different\nproblems with varied appearances and rules. We evaluate VLMFP with 6 grid-world\ndomains and test its generalization to unseen instances, appearance, and game\nrules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,\nsimulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal\nreaching for seen and unseen appearances, respectively. With the guidance of\nSimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for\nunseen instances in seen and unseen appearances, respectively. Project page:\nhttps://sites.google.com/view/vlmfp.", "AI": {"tldr": "VLMFP\u662f\u4e00\u4e2a\u53ccVLM\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u4e3b\u751f\u6210PDDL\u95ee\u9898\u6587\u4ef6\u548c\u9886\u57df\u6587\u4ef6\uff0c\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u89c4\u5212\u4e2d\u7a7a\u95f4\u548c\u957f\u65f6\u63a8\u7406\u7684\u4e0d\u8db3\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u89c4\u5212\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u7cbe\u786e\u7a7a\u95f4\u548c\u957f\u65f6\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff1b\u800cPDDL\u89c4\u5212\u5668\u64c5\u957f\u5f62\u5f0f\u5316\u89c4\u5212\u4f46\u65e0\u6cd5\u5904\u7406\u89c6\u89c9\u8f93\u5165\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u9884\u5b9a\u4e49\u9886\u57df\u6587\u4ef6\u6216\u6301\u7eed\u73af\u5883\u8bbf\u95ee\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u6027\u3002", "method": "\u63d0\u51faVLMFP\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2aVLM\uff1aSimVLM\u57fa\u4e8e\u89c4\u5219\u63cf\u8ff0\u6a21\u62df\u52a8\u4f5c\u540e\u679c\uff0cGenVLM\u901a\u8fc7\u6bd4\u8f83PDDL\u548cSimVLM\u6267\u884c\u7ed3\u679c\u6765\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316PDDL\u6587\u4ef6\u3002", "result": "\u57286\u4e2a\u7f51\u683c\u4e16\u754c\u9886\u57df\u6d4b\u8bd5\u4e2d\uff0cSimVLM\u5728\u53ef\u89c1\u548c\u672a\u89c1\u5916\u89c2\u4e0b\u5206\u522b\u51c6\u786e\u63cf\u8ff095.5%/82.6%\u573a\u666f\uff0c\u6a21\u62df85.5%/87.8%\u52a8\u4f5c\u5e8f\u5217\uff0c\u5224\u65ad82.4%/85.6%\u76ee\u6807\u8fbe\u6210\u3002VLMFP\u5728\u53ef\u89c1\u548c\u672a\u89c1\u5916\u89c2\u4e0b\u5bf9\u672a\u89c1\u5b9e\u4f8b\u5206\u522b\u751f\u621070.0%/54.1%\u7684\u6709\u6548\u89c4\u5212\u3002", "conclusion": "VLMFP\u5b9e\u73b0\u4e86\u591a\u5c42\u6b21\u7684\u6cdb\u5316\u80fd\u529b\uff1a\u751f\u6210\u7684PDDL\u9886\u57df\u6587\u4ef6\u9002\u7528\u4e8e\u540c\u4e00\u95ee\u9898\u4e0b\u7684\u6240\u6709\u5b9e\u4f8b\uff0cVLM\u80fd\u591f\u6cdb\u5316\u5230\u5177\u6709\u4e0d\u540c\u5916\u89c2\u548c\u89c4\u5219\u7684\u95ee\u9898\u4e2d\u3002"}}
