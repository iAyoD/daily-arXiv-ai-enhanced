<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: RIMRULE：一种基于动态规则注入的神经符号方法，通过从失败轨迹中提炼紧凑、可解释的规则来提升LLM在特定领域工具使用中的性能，无需修改模型权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定领域工具使用时面临挑战，因为API可能具有特殊性、文档不足或针对私有工作流定制，需要有效的任务特定工具适应方法。

Method: 提出RIMRULE神经符号方法：1）从失败轨迹中提炼紧凑可解释规则；2）使用最小描述长度目标优化规则通用性和简洁性；3）规则以自然语言和结构化符号形式存储；4）推理时动态注入规则到提示中。

Result: 在工具使用基准测试中，该方法提高了已见和未见工具的准确性，优于基于提示的适应方法，并能与微调互补。规则可在不同LLM间重用，包括长推理LLM，展示了符号知识的跨架构可移植性。

Conclusion: RIMRULE通过动态规则注入有效提升了LLM在特定领域工具使用中的性能，提供了一种无需修改模型权重的适应方法，且规则具有跨模型的可移植性。

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [2] [Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning](https://arxiv.org/abs/2601.00095)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CL

TL;DR: MetaJuLS：一种元强化学习方法，通过自适应约束传播实现跨语言和任务的通用结构化推理，相比GPU优化基线提速1.5-2倍，仅需5-10梯度步即可适应新任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要结构化推理（如JSON模式强制执行、多语言解析），输出必须满足复杂约束。现有方法需要针对每个任务进行专门训练，效率低下且难以跨域适应。

Method: 提出MetaJuLS元强化学习方法，将结构化推理建模为自适应约束传播问题。使用图注意力网络和元学习训练通用约束传播策略，无需任务特定重训练即可应用于不同语言和任务。

Result: 在Universal Dependencies（10种语言）和LLM约束生成（LogicBench、GSM8K-Constrained）上，MetaJuLS相比GPU优化基线获得1.5-2倍加速，精度仅比最先进解析器低0.2%。仅需5-10梯度步（5-15秒）即可适应新语言/任务，而非数小时训练。

Conclusion: MetaJuLS通过减少LLM部署中的传播步骤，直接降低推理碳足迹，促进绿色AI。策略分析发现其学习到类似人类的解析策略（易优先）和新颖的非直观启发式方法。

Abstract: Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\times$ speedups over GPU-optimized baselines while maintaining within 0.2\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.

</details>


### [3] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

TL;DR: Pat-DEVAL：首个专为专利说明书设计的多维评估框架，通过法律约束推理机制评估长文本结构连贯性和法定合规性，显著优于现有评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有专利自动撰写评估方法无法评估长文本结构连贯性和法定合规性（如充分公开和书面描述要求），需要专门针对专利说明书的法律技术特点设计评估框架。

Method: 提出Pat-DEVAL框架，采用LLM-as-a-judge范式，引入Chain-of-Legal-Thought（CoLT）法律约束推理机制，强制执行顺序性专利法特定分析。

Result: 在Pap2Pat-EvalGold数据集上，Pat-DEVAL达到0.69的皮尔逊相关系数，显著优于基线指标和现有LLM评估器；在法律专业合规性方面达到0.73的优越相关性。

Conclusion: 通过明确注入法定约束条件，Pat-DEVAL为自动专利撰写系统的实际部署提供了稳健的方法基础，建立了确保技术合理性和法律合规性的新标准。

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [4] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 该论文通过系统分析IEMOCAP数据集，填补了情感对话识别中的两个关键空白：识别架构选择的有效性评估，以及识别与生成之间的语言学连接分析。


<details>
  <summary>Details</summary>
Motivation: 当前情感对话识别(ERC)虽然取得了高准确率，但存在两个关键空白：1) 缺乏对哪些架构选择真正重要的理解；2) 缺乏将识别与生成连接起来的语言学分析。论文旨在通过系统分析填补这些空白。

Method: 采用系统分析方法，对IEMOCAP数据集进行：1) 识别方面：进行严格的消融研究，使用10种随机种子评估，分析对话上下文、分层句子表示和外部情感词典的影响；2) 语言学分析：分析5,286个话语标记出现情况，研究情感与标记位置之间的关联。

Result: 识别方面：1) 对话上下文至关重要，90%的性能增益来自最近10-30轮对话；2) 分层句子表示在话语层面有帮助，但提供对话上下文后此优势消失；3) 外部情感词典无增益。使用简单架构达到82.69%(4类)和67.07%(6类)加权F1，优于现有方法。语言学分析：发现情感与话语标记位置显著相关(p<.0001)，悲伤话语的左边缘标记使用率(21.9%)低于其他情感(28-32%)。

Conclusion: 对话上下文是情感识别的关键因素，简单的因果上下文架构就能取得优异性能。悲伤话语缺乏显性语用信号，最需要对话历史进行消歧，这解释了为什么悲伤情感从上下文中获益最大(+22%)。研究为情感识别提供了架构指导，并建立了识别与生成之间的语言学联系。

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [5] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

TL;DR: 提出针对时序知识图谱推理的蒸馏框架，利用大语言模型作为教师模型，将结构和时序推理能力转移到轻量级学生模型，在保持高效的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理模型参数大、计算密集，硬件成本和能耗高，难以部署在资源受限的实时推理平台。现有压缩和蒸馏技术主要针对静态知识图谱，无法充分捕捉时序依赖关系，导致推理性能下降。

Method: 提出专门针对时序知识图谱推理的蒸馏框架，利用大语言模型作为教师模型指导蒸馏过程，有效转移结构和时序推理能力到轻量级学生模型。通过整合大规模公共知识和任务特定时序信息，增强学生模型对时序动态的建模能力，同时保持紧凑高效的架构。

Result: 在多个公开基准数据集上的广泛实验表明，该方法持续优于强基线，在推理准确性、计算效率和实际可部署性之间实现了有利的平衡。

Conclusion: 提出的蒸馏框架有效解决了时序知识图谱推理中的效率和部署问题，通过大语言模型指导的蒸馏过程，实现了轻量级模型的高性能时序推理，为资源受限平台的实时推理应用提供了可行方案。

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [6] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

TL;DR: 该研究提出了一种将循证医学原则融入图检索增强生成的方法，通过PICO框架对齐和贝叶斯重排序提升医学问答质量，并在运动康复领域验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的检索增强生成方法主要关注性能提升，但忽视了循证医学原则。存在两个关键问题：1) 查询与检索证据之间缺乏PICO框架对齐；2) 重排序过程中未考虑证据等级层次。

Method: 提出了一种通用的循证医学适应策略，将PICO框架整合到知识图谱构建和检索中，并设计了一个贝叶斯启发的重排序算法，在不引入预定义权重的情况下根据证据等级校准排序分数。

Result: 在运动康复领域验证了该框架，构建了包含357,844个节点和371,226条边的知识图谱，创建了1,637个QA对的可复用基准。系统在多个指标上表现优异：nugget覆盖度0.830、答案忠实度0.819、语义相似度0.882、PICOT匹配准确率0.788。五位临床专家在5点李克特量表上给出了4.66-4.84的高评分。

Conclusion: 该研究提出的循证医学适应策略不仅提高了检索和答案质量，而且可转移到其他临床领域。发布的资源有助于解决运动康复领域RAG数据集稀缺的问题。

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [7] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: JP-TL-Bench是一个轻量级开源基准测试，用于指导日英翻译系统的迭代开发，专注于区分"哪个翻译更好"而非"翻译是否可接受"，通过成对LLM比较和Bradley-Terry模型提供稳定评分。


<details>
  <summary>Details</summary>
Motivation: 日英翻译中，礼貌、隐含意义、省略和语域等细微选择对自然度影响很大，现有评估往往只能判断翻译是否可接受，而无法区分"哪个好翻译更好"。需要专门针对日英翻译特点的评估基准。

Method: 使用无参考的成对LLM比较方法，将候选模型与固定的版本化锚点集进行对比。通过Bradley-Terry模型聚合结果，计算胜率并转换为0-10分的"LT"评分（基于逻辑变换的对数强度）。

Result: JP-TL-Bench提供了结构稳定的评分系统，由于每个候选模型都针对相同的冻结锚点集进行评估，在相同基础集、评判标准和聚合代码下，评分具有结构稳定性。

Conclusion: JP-TL-Bench是一个可靠且经济的评估协议，专门针对日英翻译的细微差别设计，能够有效指导翻译系统的迭代开发，解决了"哪个好翻译更好"这一核心挑战。

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [8] [Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback](https://arxiv.org/abs/2601.00224)
*Yan Sun,Ming Cai,Stanley Kok*

Main category: cs.CL

TL;DR: 论文提出两种互补的验证技术(Q*和Feedback+)来提高LLM助手在对话式商业分析中的可靠性，通过生成器-判别器框架将验证责任从用户转移到系统。


<details>
  <summary>Details</summary>
Motivation: 当前对话式商业分析系统缺乏内置验证机制，用户需要手动验证可能错误的输出，这影响了企业工作流程中LLM助手生成准确、语义对齐且可执行输出的能力。

Method: 提出两种验证技术：1) Q*：通过反向翻译和代码与用户意图的语义匹配进行验证；2) Feedback+：通过执行反馈指导代码优化。这些技术嵌入在生成器-判别器框架中。

Result: 在Spider、Bird和GSM8K三个基准数据集上的评估显示，Q*和Feedback+都能降低错误率和任务完成时间。反向翻译被识别为关键瓶颈。

Conclusion: 本研究为构建更可靠、企业级的生成式AI系统提供了一个设计导向的框架，能够提供可信的决策支持，同时指出了反向翻译作为未来改进的关键机会。

Abstract: As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.

</details>


### [9] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 该研究全面评估了LLMs生成多语言反事实的能力，发现翻译生成的反事实有效性更高但修改更多，多语言反事实数据增强对低资源语言效果更好，但生成质量限制了性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在生成英语反事实方面表现出色且具备多语言能力，但其在多语言反事实生成方面的有效性尚不明确，需要系统研究来评估LLMs在多语言环境下的反事实生成能力。

Method: 1) 对六种语言进行自动评估，比较直接生成和通过英语翻译生成的反事实；2) 分析高资源欧洲语言反事实的编辑模式；3) 识别跨语言反事实生成中的错误类型；4) 评估多语言反事实数据增强与跨语言数据增强的效果。

Result: 1) 翻译生成的反事实有效性更高但需要更多修改，且质量仍不及原始英语反事实；2) 高资源欧洲语言的编辑模式相似，表明跨语言扰动遵循共同策略；3) 识别出四种跨语言一致的错误类型；4) 多语言数据增强比跨语言增强效果更好，尤其对低资源语言，但生成不完善限制了性能提升。

Conclusion: LLMs在多语言反事实生成方面存在局限性，翻译方法能提高有效性但代价是更多修改，多语言数据增强有潜力但受限于生成质量，需要改进多语言反事实生成技术以充分发挥其解释和数据增强价值。

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [10] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: WildAGTEval是一个评估LLM代理在真实API复杂性下函数调用能力的基准，考虑了API规范和API执行两个维度的现实世界复杂性，包含约32K测试配置，发现无关信息复杂性对LLM性能影响最大。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法假设理想化的API系统，忽略了现实世界因素如噪声API输出，无法准确评估LLM代理在实际复杂环境中的函数调用能力。

Method: 创建WildAGTEval基准，包含60个不同复杂性场景，可组合成约32K测试配置，考虑API规范（详细文档和使用约束）和API执行（运行时挑战）两个维度的复杂性。

Result: 大多数场景具有挑战性，无关信息复杂性对LLM性能影响最大，使强LLM性能下降27.3%；定性分析发现LLM有时会扭曲用户意图以声称任务完成，严重影响用户满意度。

Conclusion: WildAGTEval提供了一个更真实的评估框架，揭示了LLM代理在实际API复杂性下的局限性，特别是处理无关信息和意图扭曲的问题，对提升LLM代理的实用性和用户满意度具有重要意义。

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [11] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 量化对LLM自解释能力有负面影响，但程度相对较小，不影响量化作为模型压缩技术的有效性


<details>
  <summary>Details</summary>
Motivation: 量化被广泛用于加速大型语言模型推理和部署，但其对自解释能力的影响尚未被研究。自解释是LLM为自身输出提供解释的能力，在高风险应用中越来越依赖这种透明度，因此理解量化是否以及如何降低自解释质量和忠实度至关重要。

Method: 研究两种自解释类型：自然语言解释和反事实示例，使用三种常见量化技术在多个比特宽度下对LLM进行量化。通过用户研究评估量化对自解释连贯性和可信度的影响。

Result: 量化通常导致自解释质量（最多下降4.4%）和忠实度（最多下降2.38%）的适度下降。用户研究表明量化降低了自解释的连贯性和可信度（最多8.5%）。与较小模型相比，较大模型在自解释质量方面对量化的抵抗力有限，但能更好地保持忠实度。没有一种量化技术能在任务准确性、自解释质量和忠实度方面始终表现优异。

Conclusion: 量化对自解释的影响因上下文而异，建议针对具体用例验证自解释质量，特别是对更敏感的自然语言解释。然而，自解释质量和忠实度的相对较小恶化并不影响量化作为模型压缩技术的有效性。

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [12] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: DepFlow：通过三阶段抑郁条件文本转语音框架缓解抑郁症检测中的语义偏见，构建伪装抑郁增强数据集提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症数据集（如DAIC-WOZ）中语言情感与诊断标签强耦合，导致模型学习语义捷径，在真实场景（如伪装抑郁）中鲁棒性不足。需要解决语义偏见问题，提升模型在语言内容与抑郁状态不匹配情况下的检测能力。

Method: 提出DepFlow三阶段框架：1）抑郁声学编码器通过对抗训练学习说话人和内容不变的抑郁嵌入；2）带FiLM调制的流匹配TTS模型注入抑郁嵌入，控制抑郁严重程度同时保持内容和说话人身份；3）基于原型的严重程度映射机制提供平滑可解释的抑郁连续体操控。利用该框架构建伪装抑郁增强数据集（CDoA）。

Result: 抑郁声学编码器ROC-AUC达0.693，有效解耦同时保持抑郁区分性。CDoA数据集在三种抑郁症检测架构上分别提升macro-F1 9%、12%和5%，优于传统增强策略。框架还提供可控合成平台用于对话系统和仿真评估。

Conclusion: DepFlow成功缓解抑郁症检测中的语义偏见，通过构建声学-语义不匹配数据增强模型鲁棒性，为临床数据有限的场景提供可控合成解决方案，具有实际应用价值。

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [13] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: 提出一种针对大语言模型幻觉问题的鲁棒不确定性量化方法，通过构建包含虚假名称的陷阱问题集来检测模型在生成多事实内容时的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉问题严重影响了AI生成内容的可靠性，现有不确定性量化方法在常规问答中有效，但在面对非规范或对抗性提问时表现不足，这限制了LLM在需要强大批判性思维能力的实际应用中的可靠性。

Method: 1) 构建包含虚假名称的陷阱问题集来测试模型在多事实生成任务中的表现；2) 提出一种新颖的鲁棒不确定性量化方法(RU)，专门针对非规范或对抗性提问场景设计。

Result: 1) 构建的陷阱问题集表现优异；2) 在四个不同模型上与基线方法相比，提出的RU方法在ROCAUC值上平均提升了0.1-0.2，显著优于现有最佳基线方法。

Conclusion: 该研究填补了传统不确定性量化方法在面对非规范或对抗性提问时的性能差距，提出的鲁棒不确定性量化方法为缓解大语言模型幻觉问题提供了新的视角和方法，增强了LLM在实际应用中的可靠性。

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [14] [The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2601.00364)
*Jiandong Shao,Raphael Tang,Crystina Zhang,Karin Sevegnani,Pontus Stenetorp,Jianfei Yang,Yao Lu*

Main category: cs.CL

TL;DR: 移除双语数据（仅占语料库2%）使翻译性能下降56%，但对跨语言QA和推理任务影响不大；平行数据（14%）能恢复91%翻译性能，而语码转换数据（72%）贡献很小


<details>
  <summary>Details</summary>
Motivation: 研究双语数据在多语言大语言模型跨语言能力中的作用细节，虽然双语数据在预训练语料中占比很小，但其具体贡献机制尚不清楚

Method: 在受控条件下从头训练模型，比较标准网络语料库与移除所有多语言文档的纯单语版本；将双语数据分类为平行数据（14%）、语码转换数据（72%）和其他文档（14%），通过逐步重新引入不同类型双语数据进行细粒度消融实验

Result: 移除双语数据导致翻译性能下降56%，但跨语言QA和推理任务表现稳定；平行数据几乎完全恢复翻译性能（达到未过滤基线的91%），而语码转换数据贡献很小；其他跨语言任务基本不受双语数据类型影响

Conclusion: 翻译严重依赖平行数据提供的系统性词级对齐，而跨语言理解和推理能力即使没有双语数据也能实现；双语数据中只有平行数据对翻译性能至关重要

Abstract: Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.

</details>


### [15] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: BERT-JEPA (BEPA) 通过将 JEPA 训练目标添加到 BERT 风格模型中，解决了 [CLS] 嵌入空间坍缩问题，将其转变为语言无关空间，从而在多语言基准测试中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决 BERT 风格模型中 [CLS] 嵌入空间坍缩的问题，并创建语言无关的表示空间，以提升多语言任务性能。

Method: 在 BERT 风格模型中添加 JEPA（联合嵌入预测架构）训练目标，通过预测性架构来改进 [CLS] 嵌入空间，使其成为语言无关的表示。

Result: BERT-JEPA 在多语言基准测试中表现出性能提升，证明了该方法在创建语言无关表示空间方面的有效性。

Conclusion: 将 JEPA 训练目标整合到 BERT 风格模型中是一种有效的策略，可以解决 [CLS] 嵌入空间坍缩问题，创建语言无关的表示，并提升多语言任务的性能。

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [16] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

TL;DR: Geo-R是一个免检索的图像地理定位框架，通过基于规则的层次推理和强化学习实现精确、可解释的地理定位，无需依赖合成标注或外部图像检索。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图像地理定位中通常依赖合成推理标注或外部图像检索，这限制了可解释性和泛化能力。需要一种免检索的方法，直接从真实坐标中挖掘结构化推理路径。

Method: 提出Chain of Region：基于规则的层次推理范式，将GPS坐标映射到地理实体（国家、省份、城市等），生成精确可解释的监督信号。采用轻量级强化学习策略，基于Haversine距离设计坐标对齐奖励，通过空间有意义的反馈优化预测。

Result: 在多个基准测试中验证了Geo-R的有效性，实现了更高的定位精度、更强的泛化能力和更透明的推理过程，建立了免检索图像地理定位的新范式。

Conclusion: Geo-R通过结构化地理推理与直接空间监督的结合，为可扩展、可解释的图像地理定位提供了新的解决方案，模型和代码将开源以促进进一步研究。

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [17] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: judgeWEL：一个用于卢森堡语命名实体识别的数据集，通过LLM自动标注和验证，规模比现有数据集大5倍


<details>
  <summary>Details</summary>
Motivation: 为低资源语言构建NER数据集面临资源稀缺、标注成本高且不一致的挑战，卢森堡语作为代表性低资源语言需要更好的数据集支持

Method: 利用Wikipedia和Wikidata作为弱监督源，通过文章内部链接推断实体类型，然后使用多个LLM进行噪声过滤和质量验证

Result: 构建的数据集规模是现有卢森堡语NER数据集的5倍，实体类别覆盖更广更平衡，为多语言和低资源NER研究提供了重要资源

Conclusion: 提出的基于Wikipedia/Wikidata和LLM验证的管道能有效构建低资源语言NER数据集，judgeWEL数据集显著推进了卢森堡语NLP研究

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [18] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: 该论文提出了一种新的超关系时序知识广义超图（HTKGHs）结构，用于更有效地表示复杂的地缘政治事件，并基于POLECAT数据库构建了htkgh-polecat数据集，最后评估了LLMs在关系预测任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的超关系时序知识图（HTKGs）虽然能表示简单的时序关系，但表达能力有限，无法有效表示包含两个以上主要实体的复杂事实，这在真实世界的地缘政治事件中很常见。

Method: 1. 提出HTKGHs作为HTKGs的泛化形式，支持两种常见的地缘政治事件复杂事实类型；2. 基于POLECAT全球事件数据库构建htkgh-polecat数据集；3. 在关系预测任务上对主流LLMs进行基准测试和分析。

Result: 建立了HTKGHs的形式化定义，展示了其向后兼容性；创建了htkgh-polecat数据集；评估了LLMs在复杂预测场景中的适应性和能力。

Conclusion: HTKGHs能够更有效地表示复杂的地缘政治事件，为LLMs在复杂时序关系预测任务上的性能评估提供了新的数据集和基准。

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [19] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

TL;DR: 比较三种轻量级Transformer模型（DistilBERT、MiniLM、ALBERT）在多领域文本自动化任务中的性能与效率，发现各有优势：ALBERT准确率最高，MiniLM推理速度最快，DistilBERT最均衡。


<details>
  <summary>Details</summary>
Motivation: 企业NLP应用对高效轻量级模型处理多领域文本自动化任务的需求日益增长，需要比较不同轻量级Transformer模型在性能与效率方面的表现。

Method: 使用IMDB、AG News和Measuring Hate Speech三个数据集，在客户情感分类、新闻主题分类、毒性及仇恨言论检测三个领域，对比DistilBERT、MiniLM和ALBERT三种模型。评估指标包括准确率、精确率、召回率、F1分数等性能指标，以及模型大小、推理时间、吞吐量和内存使用等效率指标。

Result: 没有单一模型在所有维度上表现最佳：ALBERT在多个领域获得最高任务特定准确率；MiniLM在推理速度和吞吐量方面表现最优；DistilBERT在任务间准确率最一致，同时保持有竞争力的效率。

Conclusion: 研究揭示了准确率与效率之间的权衡关系，建议：延迟敏感的企业应用选择MiniLM，需要平衡性能的选择DistilBERT，资源受限环境选择ALBERT。

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [20] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: 该论文对比了语言的社会建构主义（语言游戏）和数学导向的语义场理论，认为两者是互补而非竞争的关系，LLMs的成功支持语言具有底层数学结构的观点，但其在语用推理上的局限则体现了社会基础的重要性。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型（LLMs）这一新的实证环境，检验长期存在的语言意义理论，特别是对比社会建构主义（语言游戏）和数学导向的语义场理论这两种广泛方法。

Method: 基于作者先前工作，将词汇场（Lexfelder）和语言场（Lingofelder）形式化为连续语义空间中的交互结构，分析Transformer架构的核心特性（分布式表示、注意力机制、嵌入空间的几何规律性）与这些概念的关系。

Result: LLMs在捕捉语义规律性方面的成功支持了语言具有底层数学结构的观点，而其在语用推理和上下文敏感性方面的持续局限则与哲学语言使用理论强调的社会基础重要性相一致。

Conclusion: 数学结构和语言游戏可以理解为互补而非竞争的观点，这一框架澄清了纯统计语言模型的范围和局限，并为理论指导的AI架构提供了新的方向。

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [21] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 提出Defensive M2S训练范式，通过多轮到单轮对话压缩技术，将护栏模型的训练成本从O(n²)降至O(n)，推理token减少94.6%，同时攻击检测召回率提升38.9个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有护栏模型处理完整多轮对话历史计算成本高，需要更高效的训练和推理方法来实现大规模安全部署。

Method: 提出Defensive M2S训练范式，在多轮到单轮压缩对话上微调护栏模型，使用三种压缩模板（hyphenize、numberize、pythonize），在三个护栏模型家族上进行评估。

Result: 训练token减少93倍（从1570万降至16.9万），推理token减少94.6%（从3231降至173），最佳配置攻击检测召回率达93.8%，比基线提升38.9个百分点。

Conclusion: M2S压缩可作为护栏部署的有效效率技术，实现长多轮对话的可扩展安全筛查，显著降低训练和推理成本同时保持高性能。

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [22] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: 提出一种针对职业教育培训领域历史文档的噪声感知命名实体识别方法，通过合成OCR错误、迁移学习和多阶段微调来提高在噪声条件下的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 职业教育培训领域的历史数字化文档存在OCR噪声问题，传统NER方法在这种噪声条件下性能下降，需要专门针对该领域且能处理OCR噪声的NER解决方案。

Method: 采用噪声感知训练（NAT），通过合成注入OCR错误，结合迁移学习和多阶段微调。系统比较了三种互补策略：在噪声数据、干净数据和人工合成数据上的训练。

Result: 实验结果表明，领域特定和噪声感知的微调显著提高了在噪声条件下的鲁棒性和准确性。该方法首次在VET文档中识别多种实体类型，并提供了公开可用的代码。

Conclusion: 提出的方法有效解决了VET领域历史文档的NER问题，具有语言可迁移性，为领域特定上下文中的噪声感知NER提供了可复现的解决方案。

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [23] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

TL;DR: 该研究分析了复杂句子结构（如关系从句、状语从句、并列结构等）如何影响基于规则的原子句子提取性能，发现规则方法具有中等至高准确性但对句法复杂度敏感。


<details>
  <summary>Details</summary>
Motivation: 现有原子句子提取方法缺乏可解释性，无法揭示哪些具体语言结构导致提取失败。需要系统分析复杂句子结构对提取性能的影响。

Method: 使用WikiSplit数据集，在spaCy中实现基于依存关系的提取规则，生成100个黄金标准原子句子集，使用ROUGE和BERTScore评估性能。

Result: 系统达到ROUGE-1 F1=0.6714，ROUGE-2 F1=0.478，ROUGE-L F1=0.650，BERTScore F1=0.5898，显示中等至高的词汇、结构和语义对齐。挑战性结构包括关系从句、同位语、并列谓语、状语从句和被动结构。

Conclusion: 基于规则的原子句子提取具有合理准确性，但对句法复杂度敏感，需要进一步改进以处理复杂语言结构。

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [24] [Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends](https://arxiv.org/abs/2601.00536)
*Yuelyu Ji,Zhuochun Li,Rui Meng,Daqing He*

Main category: cs.CL

TL;DR: 本文提出一个四轴框架来分析多跳问答系统的执行过程，将执行过程作为分析单元，涵盖整体执行计划、索引结构、下一步控制和停止/继续标准，并总结了现有系统的权衡和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答系统中，检索-推理过程通常隐含在模型内部，使得不同模型家族的程序选择难以比较。需要系统化的框架来分析这些执行过程。

Method: 提出四轴分析框架：(A)整体执行计划，(B)索引结构，(C)下一步控制策略和触发机制，(D)停止/继续标准。使用该框架对代表性多跳QA系统进行映射分析。

Result: 在标准基准测试上综合了报告的消融实验和趋势，突出了有效性、效率和证据忠实度之间的反复权衡。

Conclusion: 提出了检索-推理代理的开放挑战，包括结构感知规划、可迁移的控制策略以及在分布偏移下的鲁棒停止机制。

Abstract: Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

</details>


### [25] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: ECR框架通过语义锚点保持紧凑模型嵌入空间的结构一致性，无需教师输出或修改解码架构


<details>
  <summary>Details</summary>
Motivation: 紧凑模型在容量受限或多语言场景下容易丢失嵌入空间结构，导致语义漂移和下游任务性能下降，现有压缩方法只关注表层输出对齐而忽略了底层流形结构

Method: 提出嵌入一致性调节(ECR)框架：从教师嵌入中离线计算语义锚点，让紧凑模型学习在这些锚点周围保持一致的几何结构，无需匹配logits或内部特征，推理时仅添加小型投影步骤

Result: 在10万条多语言语料实验中，ECR稳定训练并跨任务和语言保持语义结构，产生更紧凑且任务对齐的表示空间，使低容量模型学习到比传统基线更清晰的流形

Conclusion: ECR帮助紧凑模型更好地遵循任务要求，使其在严格效率或隐私限制下更容易部署，无需教师输出且与蒸馏兼容但独立

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [26] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

TL;DR: 提出HLoRA框架，基于CTC架构实现轻量级、语言无关的多语言ASR系统，通过分层LoRA-MoE设计实现单次解码，无需语言标签，显著提升低资源场景下的解码效率。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多语言ASR模型（如Whisper）计算和延迟成本高，难以部署到资源受限的边缘设备。需要开发轻量级、语言无关且高效的多语言ASR系统。

Method: 提出语言无关的分层LoRA-MoE（HLoRA）框架，集成到mHuBERT-CTC模型中。包含：1）多语言共享LoRA学习语言不变声学表示；2）语言特定LoRA专家建模语言相关特征；3）基于LID后验的LoRA路由机制，实现无需语言标签的单次解码。

Result: 在MSR-86K和MLC-SLM 2025 Challenge数据集上实验，HLoRA仅通过单次解码就达到与最先进两阶段推理方法相当的性能，显著提升了低资源多语言ASR应用的解码效率。

Conclusion: HLoRA框架成功实现了轻量级、语言无关的多语言ASR系统，通过分层LoRA-MoE设计和智能路由机制，在保持性能的同时大幅降低计算成本，适合资源受限的边缘设备部署。

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [27] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: InfoSynth是一个基于信息论原则自动生成和评估推理基准的框架，使用KL散度和熵量化基准新颖性和多样性，通过遗传算法和迭代代码反馈生成Python编程问题，实现97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基准创建依赖人工，成本高且耗时，现有基准常污染LLM训练数据，需要新颖多样的基准来准确评估LLM的真实能力。

Method: 提出基于KL散度和熵的指标量化基准新颖性和多样性，无需昂贵模型评估；开发端到端流水线，使用遗传算法和迭代代码反馈从种子数据集合成稳健的Python编程问题。

Result: 方法在97%的情况下能准确生成新问题的测试用例和解决方案；合成的基准相比种子数据集始终表现出更高的新颖性和多样性；算法能控制生成问题的新颖性/多样性和难度。

Conclusion: InfoSynth为LLM提供了可扩展、自验证的高质量基准构建流水线，能生成新颖多样的基准来准确评估模型能力。

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [28] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

TL;DR: CSSBench是一个专门针对中文特定对抗模式的安全基准测试，用于评估轻量级大语言模型在中文环境下的安全性，填补了现有基准测试主要关注英文的空白。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地部署在成本敏感和边缘设备场景中，但现有的安全防护主要针对英文。中文恶意查询通常通过同音字、拼音、符号分割等中文特定模式隐藏意图，这些对抗模式在现有基准测试中未被充分捕捉，特别是对于可能更脆弱的轻量级模型。

Method: 提出了中文特定安全基准测试(CSSBench)，强调中文对抗模式，覆盖六个真实中文场景中的常见领域：非法活动与合规、隐私泄露、健康与医疗错误信息、欺诈与仇恨、成人内容、公共与政治安全，并将查询组织成多种任务类型。

Result: 评估了一系列流行的轻量级大语言模型，测量过度拒绝行为以评估安全性导致的性能下降。结果显示，中文特定对抗模式对轻量级大语言模型构成关键挑战。

Conclusion: CSSBench为中文环境下的大语言模型安全性提供了全面评估，有助于实际部署中的鲁棒性提升，填补了现有基准测试的空白。

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [29] [Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence](https://arxiv.org/abs/2601.00596)
*Sumanth Balaji,Piyush Mishra,Aashraya Sachdeva,Suraj Agrawal*

Main category: cs.CL

TL;DR: JourneyBench是一个用于评估客户支持场景中策略感知AI代理的新基准，通过图表示生成多样化支持场景，并提出用户旅程覆盖率分数来衡量策略遵循能力。


<details>
  <summary>Details</summary>
Motivation: 传统客户支持系统（如IVR）依赖刚性脚本，缺乏处理复杂策略驱动任务的灵活性。虽然LLM代理提供了有前景的替代方案，但评估它们遵循业务规则和真实支持工作流程的能力仍然是一个开放挑战。现有基准主要关注工具使用或任务完成，忽视了代理遵循多步骤策略、导航任务依赖关系以及对不可预测用户行为保持鲁棒性的能力。

Method: 引入JourneyBench基准，利用图表示生成多样化、真实的支持场景。提出用户旅程覆盖率分数（User Journey Coverage Score）来衡量策略遵循能力。评估了两种代理设计：静态提示代理（SPA）和动态提示代理（DPA），后者明确建模策略控制。

Result: 在三个领域的703个对话中，DPA显著提高了策略遵循能力，甚至允许较小的模型如GPT-4o-mini在策略遵循方面胜过更强大的模型如GPT-4o。结果表明结构化编排的重要性。

Conclusion: JourneyBench作为一个关键资源，可以推动AI驱动的客户支持超越IVR时代的限制，展示了结构化编排在确保策略遵循方面的重要性。

Abstract: Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.

</details>


### [30] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: 提出一个模型无关的框架，通过重复采样和多数投票机制为减少LLM幻觉提供概率保证


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在确定性自动化工作流中经常产生上下文幻觉，这些错误在输入固定且正确性明确的情况下尤为严重，需要一种简单可靠的方法来降低幻觉概率

Method: 通过独立上下文窗口重复采样相同提示，利用LLM作为评判者识别正确答案，并通过多数投票机制增强不完美评判者的可靠性

Result: 实验显示管道失败概率随重复次数指数下降，幻觉选择概率随评判者数量指数下降，理论预测与实验结果完全匹配

Conclusion: 该方法提供了一种轻量级、模块化且理论可靠的方法，可在不修改模型权重、解码策略或提示工程的情况下，将固定输入工作流中的幻觉概率降至任意低水平

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [31] [Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations](https://arxiv.org/abs/2601.00647)
*QiWei Meng*

Main category: cs.CL

TL;DR: Physio-DPO：一种基于物理信息的对齐框架，通过考虑物理能量景观的连续结构来减少蛋白质语言模型的结构幻觉，提高生成蛋白质的热力学稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型蛋白质语言模型在生成蛋白质设计方面表现出强大潜力，但经常产生结构幻觉，生成的语言可能性高但热力学不稳定的序列。现有的对齐方法（如DPO）将偏好建模为二元标签，忽略了物理能量景观的连续结构。

Method: 提出Physio-DPO框架，引入幅度感知目标函数，根据天然结构与物理扰动硬负样本之间的能量差距来缩放优化更新，将蛋白质语言模型基于热力学稳定性进行对齐。

Result: Physio-DPO在实验中始终优于SFT、PPO和标准DPO等基线方法，将自一致性RMSD降低到1.28 Å，并将可折叠性提高到92.8%。定性分析显示Physio-DPO通过恢复疏水核心堆积和氢键网络等生物物理相互作用，有效缓解结构幻觉。

Conclusion: Physio-DPO通过将物理能量景观的连续结构纳入对齐过程，显著提高了蛋白质语言模型生成的热力学稳定性，为减少结构幻觉提供了有效解决方案。

Abstract: Large Protein Language Models have shown strong potential for generative protein design, yet they frequently produce structural hallucinations, generating sequences with high linguistic likelihood that fold into thermodynamically unstable conformations. Existing alignment approaches such as Direct Preference Optimization are limited in this setting, as they model preferences as binary labels and ignore the continuous structure of the physical energy landscape. We propose Physio-DPO, a physics informed alignment framework that grounds protein language models in thermodynamic stability. Physio-DPO introduces a magnitude aware objective that scales optimization updates according to the energy gap between native structures and physics perturbed hard negatives. Experiments show that Physio-DPO consistently outperforms strong baselines including SFT, PPO, and standard DPO, reducing self consistency RMSD to 1.28 Å and increasing foldability to 92.8%. Qualitative analysis further demonstrates that Physio-DPO effectively mitigates structural hallucinations by recovering biophysical interactions such as hydrophobic core packing and hydrogen bond networks.

</details>


### [32] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: FwPKM 是一种新型架构，将静态的产品键记忆转换为动态的快速权重情景记忆，解决了序列建模中存储容量与计算效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型中的序列建模层面临存储容量与计算效率的权衡：Softmax注意力提供无限存储但计算成本高，线性变体效率高但存储有限。需要一种既能高效计算又能动态存储的解决方案。

Method: 提出快速权重产品键记忆（FwPKM），将稀疏的产品键记忆从静态模块转变为动态的"快速权重"情景记忆。通过局部块级梯度下降在训练和推理时动态更新参数，使模型能够快速记忆和检索输入序列中的新键值对。

Result: FwPKM 作为有效的情景记忆补充了标准模块的语义记忆，在长上下文数据集上显著降低了困惑度。在"大海捞针"评估中，尽管仅在4K标记序列上训练，却能泛化到128K标记的上下文。

Conclusion: FwPKM 成功解决了序列建模中存储与效率的权衡问题，提供了一种动态、高效的记忆机制，能够有效处理长上下文序列，展示了在有限训练数据下对更长上下文的泛化能力。

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [33] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: 提出Sigmoid Head方法解决语言模型概率不可靠的质量估计问题，通过sigmoid激活和负采样策略改进质量评估


<details>
  <summary>Details</summary>
Motivation: 语言模型的概率分布不是可靠的质量估计器，因为自然语言具有歧义性。当多个输出选项都有效时，模型的概率分布会分散在这些选项上，从而误导性地表明输出质量较低。这主要由两个原因造成：1) LM的最终输出激活使用softmax，不允许多个正确选项同时获得高概率；2) LM的训练数据是单一、one-hot编码的参考，暗示每个输出步骤只有一个正确选项。

Method: 提出在预训练语言模型之上训练质量估计模块。该模块称为Sigmoid Head，是一个额外的具有sigmoid激活的解嵌入头，用于解决第一个限制。为解决第二个限制，在训练Sigmoid Head的负采样过程中，使用启发式方法避免选择可能替代的正确标记。Sigmoid Head在训练和推理过程中计算效率高。

Result: Sigmoid Head的概率相比原始softmax头是显著更好的质量信号。由于Sigmoid Head不依赖人工标注的质量数据，相比监督式QE，它在域外设置中更加鲁棒。

Conclusion: Sigmoid Head方法有效解决了语言模型概率分布作为质量估计器的局限性，通过sigmoid激活和负采样策略提供了更可靠的质量评估，特别是在处理自然语言歧义性和域外场景时表现更优。

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [34] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: 评估大型语言模型在文本跨度识别任务中的表现，特别是在情感分析、攻击性语言识别和声明验证等主观性任务中


<details>
  <summary>Details</summary>
Motivation: 当前大多数跨度识别方法依赖BERT等较小预训练模型，而大型语言模型在主观性跨度识别任务（如基于方面的情感分析）中研究不足，需要填补这一空白

Method: 评估多种LLM在三个流行任务（情感分析、攻击性语言识别、声明验证）中的文本跨度识别性能，探索指令调优、上下文学习和思维链等策略

Result: 结果表明文本内部的潜在关系有助于LLM识别精确的文本跨度

Conclusion: LLM在主观性文本跨度识别任务中具有潜力，文本内部关系对精确识别至关重要

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [35] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

TL;DR: 跨省评估BCCRTron和GatorTron模型在加拿大癌症登记中的适应性，通过集成方法显著减少漏诊癌症，实现隐私保护的工作流程


<details>
  <summary>Details</summary>
Motivation: 基于人群的癌症登记依赖病理报告，但手动提取资源密集且导致数据延迟。现有NLP系统在不同司法管辖区间的泛化能力尚不清楚，需要评估跨省适应性和建立可互操作的癌症监测基础设施

Method: 使用纽芬兰与拉布拉多癌症登记的约104,000份（Tier 1）和22,000份（Tier 2）去标识化病理报告，对BCCRTron和GatorTron进行微调。采用互补的摘要式和诊断导向的报告部分输入管道，并通过保守的OR集成方法结合两个模型

Result: 集成模型在Tier 1任务中召回率达到0.99，漏诊癌症减少至24例（单独模型为48和54例）；Tier 2任务中召回率0.99，漏诊可报告癌症减少至33例（单独模型为54和46例）。证明跨省适应可行且集成方法显著改善性能

Conclusion: 在一个司法管辖区预训练的transformer模型可以通过适度微调适应另一个管辖区。集成互补文本表示能大幅减少漏诊癌症并提高错误覆盖。隐私保护工作流程（仅共享模型权重）支持可互操作的NLP基础设施，为未来泛加拿大癌症病理基础模型奠定基础

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [36] [Reinforcement learning with timed constraints for robotics motion planning](https://arxiv.org/abs/2601.00087)
*Zhaoan Wang,Junchao Li,Mahdi Mohammad,Shaoping Xiao*

Main category: cs.RO

TL;DR: 提出基于自动机的强化学习框架，用于在MDP和POMDP中合成满足MITL时序逻辑约束的策略


<details>
  <summary>Details</summary>
Motivation: 动态不确定环境中的机器人系统需要满足复杂时序约束的规划器，但MITL与强化学习的结合面临随机动态和部分可观测性的挑战

Method: 将MITL公式转换为Timed-LDGBA自动机，与决策过程同步构建产品时序模型，采用简单而富有表达力的奖励结构，适用于Q-learning

Result: 在三个仿真研究中验证：5×5网格世界MDP、10×10网格世界POMDP和办公室服务机器人场景，框架能学习满足严格时间约束的策略，可扩展到更大状态空间，在部分可观测环境中有效

Conclusion: 该框架为时间关键和不确定环境中的可靠机器人规划提供了潜力，能够处理随机转移和部分可观测性，满足严格的时间约束要求

Abstract: Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \times 5$ grid-world formulated as an MDP, a $10 \times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.

</details>


### [37] [Compositional Diffusion with Guided search for Long-Horizon Planning](https://arxiv.org/abs/2601.00126)
*Utkarsh A Mishra,David He,Yongxin Chen,Danfei Xu*

Main category: cs.RO

TL;DR: CDGS提出了一种组合扩散引导搜索方法，通过将搜索嵌入扩散去噪过程来解决组合生成模型中的模式平均问题，在机器人操作、全景图像和长视频生成等任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 组合生成模型在建模长时程任务分布方面很有前景，但当局部分布是多模态时，现有组合方法会平均不兼容的模式，导致生成的计划既不可行也不连贯。

Method: CDGS将搜索直接嵌入扩散去噪过程：通过基于种群的采样探索局部模式的多样组合，使用基于似然的过滤剪枝不可行候选，通过重叠段之间的迭代重采样强制全局一致性。

Result: 在7个机器人操作任务上匹配了oracle性能，优于缺乏组合性或需要长时程训练数据的基线方法，并能泛化到文本引导的全景图像和长视频生成任务。

Conclusion: CDGS有效解决了组合生成模型中的模式平均问题，通过局部到全局的消息传递实现了跨领域的泛化能力，为长时程规划提供了有效的组合生成框架。

Abstract: Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this \emph{mode averaging} problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/

</details>


### [38] [SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication](https://arxiv.org/abs/2601.00163)
*Junfeng Chen,Yuxiao Zhu,Xintong Zhang,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: SLEI3D框架解决异构机器人群体在未知环境中同时进行探索、检查和实时通信的问题，通过多层多速率规划机制协调机器人子组，支持间歇性和主动性通信协议。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，感兴趣区域是未知的，需要在探索过程中在线识别。异构机器人配备不同传感器（如长距离激光雷达用于快速探索，近距离相机用于详细检查），且环境通常缺乏全局通信，只能通过ad-hoc无线网络在近距离无障碍时通信。

Method: 提出SLEI3D规划协调框架，集成在线协作3D探索、自适应检查和及时通信策略。针对特征数量和位置的不确定性，开发多层多速率规划机制，用于机器人子组间和子组内的主动会合和本地计划协调。

Result: 通过大规模高保真仿真验证（最多48个机器人，384千立方米），并进行了7个机器人的硬件实验。项目网站已公开。

Conclusion: SLEI3D框架有效解决了异构机器人在未知环境中的协同探索、检查和通信问题，通过创新的规划机制实现了高效的任务执行。

Abstract: Robotic fleets such as unmanned aerial and ground vehicles have been widely used for routine inspections of static environments, where the areas of interest are known and planned in advance. However, in many applications, such areas of interest are unknown and should be identified online during exploration. Thus, this paper considers the problem of simultaneous exploration, inspection of unknown environments and then real-time communication to a mobile ground control station to report the findings. The heterogeneous robots are equipped with different sensors, e.g., long-range lidars for fast exploration and close-range cameras for detailed inspection. Furthermore, global communication is often unavailable in such environments, where the robots can only communicate with each other via ad-hoc wireless networks when they are in close proximity and free of obstruction. This work proposes a novel planning and coordination framework (SLEI3D) that integrates the online strategies for collaborative 3D exploration, adaptive inspection and timely communication (via the intermit-tent or proactive protocols). To account for uncertainties w.r.t. the number and location of features, a multi-layer and multi-rate planning mechanism is developed for inter-and-intra robot subgroups, to actively meet and coordinate their local plans. The proposed framework is validated extensively via high-fidelity simulations of numerous large-scale missions with up to 48 robots and 384 thousand cubic meters. Hardware experiments of 7 robots are also conducted. Project website is available at https://junfengchen-robotics.github.io/SLEI3D/.

</details>


### [39] [SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks](https://arxiv.org/abs/2601.00238)
*Julia Di,Kenneth A. W. Hoffmann,Tony G. Chen,Tian-Ao Ren,Mark R. Cutkosky*

Main category: cs.RO

TL;DR: SLAP系统为中型无人机提供了一种温和的垂直树干栖息方法，具有栖息失败检测和恢复能力，在室内实验中达到75%的栖息成功率。


<details>
  <summary>Details</summary>
Motivation: 现有垂直表面栖息方法主要针对轻型无人机，采用高速激进的着陆方式，不适合携带敏感电子设备的中型调查无人机。需要一种温和、安全的栖息方案，并具备失败恢复能力。

Method: 开发了SLAP系统，包含：视觉栖息点检测器、IMU栖息失败检测器、姿态控制器、光学近距离检测系统，以及使用市售slapbands制成的快速主动弹性抓取器（带微刺）。在1.2kg商用四旋翼无人机上进行验证。

Result: 室内自主飞行实验在真实橡树段上进行了20次飞行，达到75%的栖息成功率。在2次诱导失败飞行中，实现了100%的栖息失败恢复率。

Conclusion: SLAP系统为中型无人机提供了一种可行的垂直树干栖息方案，实现了温和的栖息过程和可靠的失败恢复能力，展示了系统级集成的有效性。

Abstract: Perching allows unmanned aerial vehicles (UAVs) to reduce energy consumption, remain anchored for surface sampling operations, or stably survey their surroundings. Previous efforts for perching on vertical surfaces have predominantly focused on lightweight mechanical design solutions with relatively scant system-level integration. Furthermore, perching strategies for vertical surfaces commonly require high-speed, aggressive landing operations that are dangerous for a surveyor drone with sensitive electronics onboard. This work presents the preliminary investigation of a perching approach suitable for larger drones that both gently perches on vertical tree trunks and reacts and recovers from perch failures. The system in this work, called SLAP, consists of vision-based perch site detector, an IMU (inertial-measurement-unit)-based perch failure detector, an attitude controller for soft perching, an optical close-range detection system, and a fast active elastic gripper with microspines made from commercially-available slapbands. We validated this approach on a modified 1.2 kg commercial quadrotor with component and system analysis. Initial human-in-the-loop autonomous indoor flight experiments achieved a 75% perch success rate on a real oak tree segment across 20 flights, and 100% perch failure recovery across 2 flights with induced failures.

</details>


### [40] [Vehicle Painting Robot Path Planning Using Hierarchical Optimization](https://arxiv.org/abs/2601.00271)
*Yuya Nagai,Hiromitsu Nakamura,Narito Shinmachi,Yuta Higashizono,Satoshi Ono*

Main category: cs.RO

TL;DR: 本文提出了一种分层优化方法，用于自动化车辆喷漆过程中多机械臂的路径规划，将问题分解为上层VRP式分配和下层详细路径规划，能够满足喷漆工艺的特殊约束。


<details>
  <summary>Details</summary>
Motivation: 在汽车生产工厂中，车辆喷漆过程使用多个机械臂同时对传送线上的车身进行喷漆。目前喷漆路径设计仍是工程师耗时的手动任务，需要自动化和减少设计时间。传统机器人路径规划技术（如焊接）无法直接应用于喷漆过程的独特约束。

Method: 将喷漆路径设计构建为分层优化问题：上层子问题类似于车辆路径问题（VRP），负责将车身区域分配给机械臂；下层子问题涉及详细路径规划。通过设计变量表示、约束、修复算子和初始化过程，灵活处理喷漆过程的特定约束。

Result: 在三种商用车型上的实验表明，所提方法能够自动设计出满足所有喷漆约束的路径，其质量与工程师手动设计的路径相当。

Conclusion: 分层优化方法成功解决了车辆喷漆路径规划的自动化问题，通过将复杂问题分解为可管理的子问题，并结合喷漆工艺的特殊约束，实现了与人工设计相当的质量。

Abstract: In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers.

</details>


### [41] [Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors](https://arxiv.org/abs/2601.00275)
*Dusan Nemec,Gal Versano,Itai Savin,Vojtech Simak,Juraj Kekelak,Itzik Klein*

Main category: cs.RO

TL;DR: WiCHINS：一种结合轮载和车体惯性传感器的轮式底盘惯性导航系统，用于在GNSS受限或光照条件差的环境中实现精确的纯惯性导航


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号受限或光照条件恶劣的实际场景中，自动驾驶车辆和轮式机器人只能依赖惯性传感器进行导航，但惯性测量误差会导致随时间漂移。需要开发一种能够在这些挑战性环境中实现稳健导航的纯惯性导航方法。

Method: 提出WiCHINS系统，结合轮载惯性传感器和车体惯性传感器。开发了一个三阶段框架，每个阶段使用专用的扩展卡尔曼滤波器，在估计过程中充分利用轮子和车体不同位置的传感器优势。

Result: 使用5个惯性测量单元，总记录时间228.6分钟的数据集进行评估。与四个其他惯性基线方法比较，使用两个轮子和一个车体惯性测量单元时，平均位置误差为11.4米，占平均行驶距离的2.4%。

Conclusion: WiCHINS方法能够在挑战性环境中实现稳健导航，有助于缩小纯惯性导航的性能差距，为自动驾驶车辆和轮式机器人在GNSS受限环境中的导航提供了有效解决方案。

Abstract: Autonomous vehicles and wheeled robots are widely used in many applications in both indoor and outdoor settings. In practical situations with limited GNSS signals or degraded lighting conditions, the navigation solution may rely only on inertial sensors and as result drift in time due to errors in the inertial measurement. In this work, we propose WiCHINS, a wheeled and chassis inertial navigation system by combining wheel-mounted-inertial sensors with a chassis-mounted inertial sensor for accurate pure inertial navigation. To that end, we derive a three-stage framework, each with a dedicated extended Kalman filter. This framework utilizes the benefits of each location (wheel/body) during the estimation process. To evaluate our proposed approach, we employed a dataset with five inertial measurement units with a total recording time of 228.6 minutes. We compare our approach with four other inertial baselines and demonstrate an average position error of 11.4m, which is $2.4\%$ of the average traveled distance, using two wheels and one body inertial measurement units. As a consequence, our proposed method enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap.

</details>


### [42] [Replaceable Bit-based Gripper for Picking Cluttered Food Items](https://arxiv.org/abs/2601.00305)
*Prashant Kumar,Yukiyasu Domae,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出一种可更换夹爪头的机器人夹爪系统，用于处理杂乱食品的重量分拣，针对不同形状食品设计专用夹爪头，实现快速切换和精确重量控制。


<details>
  <summary>Details</summary>
Motivation: 食品包装行业面临处理各种形状、重量食品的挑战，特别是杂乱、柔性、粘性食品（如鱼子酱和意大利面）的精确重量分拣问题。

Method: 设计可更换夹爪头系统，配备专用食品夹爪头和皮带更换系统，针对不同食品形状（鱼子酱和意大利面）设计专用夹爪头，实现快速切换和精确重量控制。

Result: 夹爪成功抓取鱼子酱和意大利面，重量分拣准确率分别超过95%和80%，系统展示快速切换不同夹爪头的能力，可处理多种食品。

Conclusion: 可更换夹爪头系统能有效处理杂乱食品的重量分拣，实现快速切换和精确控制，为食品包装自动化提供实用解决方案。

Abstract: The food packaging industry goes through changes in food items and their weights quite rapidly. These items range from easy-to-pick, single-piece food items to flexible, long and cluttered ones. We propose a replaceable bit-based gripper system to tackle the challenge of weight-based handling of cluttered food items. The gripper features specialized food attachments(bits) that enhance its grasping capabilities, and a belt replacement system allows switching between different food items during packaging operations. It offers a wide range of control options, enabling it to grasp and drop specific weights of granular, cluttered, and entangled foods. We specifically designed bits for two flexible food items that differ in shape: ikura(salmon roe) and spaghetti. They represent the challenging categories of sticky, granular food and long, sticky, cluttered food, respectively. The gripper successfully picked up both spaghetti and ikura and demonstrated weight-specific dropping of these items with an accuracy over 80% and 95% respectively. The gripper system also exhibited quick switching between different bits, leading to the handling of a large range of food items.

</details>


### [43] [Space Debris Removal using Nano-Satellites controlled by Low-Power Autonomous Agents](https://arxiv.org/abs/2601.00465)
*Dennis Christmann,Juan F. Gutierrez,Sthiti Padhi,Patrick Plörer,Aditya Takur,Simona Silvestri,Andres Gomez*

Main category: cs.RO

TL;DR: 本文提出了一种基于自主智能纳米卫星群的空间碎片清理方法，通过无线微控制器实现自主代理软件，并在专用测试平台上验证了方法的可行性和能效。


<details>
  <summary>Details</summary>
Motivation: 空间碎片日益增多，对卫星安全和太空旅行构成威胁。现有的老旧航天器和碎片需要被安全处理，而小型纳米卫星群有望解决这一问题。

Method: 基于资源受限平台上的自主代理技术进展，实现了一种简化的智能自主纳米卫星群方法。将自主代理软件部署在无线微控制器上，并在专用测试平台上进行实验验证。

Result: 实验证明了该方法的可行性和整体能效，展示了智能自主纳米卫星群实现空间碎片安全离轨的潜力。

Conclusion: 该工作为利用自主智能纳米卫星群清理空间碎片提供了初步简化方案，验证了在资源受限平台上实现此类系统的可行性，为未来空间碎片管理提供了新思路。

Abstract: Space debris is an ever-increasing problem in space travel. There are already many old, no longer functional spacecraft and debris orbiting the earth, which endanger both the safe operation of satellites and space travel. Small nano-satellite swarms can address this problem by autonomously de-orbiting debris safely into the Earth's atmosphere. This work builds on the recent advances of autonomous agents deployed in resource-constrained platforms and shows a first simplified approach how such intelligent and autonomous nano-satellite swarms can be realized. We implement our autonomous agent software on wireless microcontrollers and perform experiments on a specialized test-bed to show the feasibility and overall energy efficiency of our approach.

</details>


### [44] [Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation](https://arxiv.org/abs/2601.00545)
*Varun Agrawal,Frank Dellaert*

Main category: cs.RO

TL;DR: 提出了一种高效的混合因子图框架和变量消除算法，用于处理机器人中同时包含连续和离散变量的混合问题，实现精确的最大后验估计和边缘化。


<details>
  <summary>Details</summary>
Motivation: 机器人中的许多混合问题同时包含连续和离散组件，现有方法基于近似求解，缺乏精确的混合估计框架。

Method: 开发了混合高斯因子和混合条件表示，推导了条件线性高斯方案下的混合变量消除过程，使用树结构因子表示和剪枝策略控制离散假设数量。

Result: 在具有模糊测量的SLAM数据集上验证了框架的有效性，展示了准确性、通用性和简单性。

Conclusion: 提出的混合因子图框架能够精确处理混合估计问题，通过变量消除生成混合贝叶斯网络，为机器人中的混合问题提供了有效的解决方案。

Abstract: Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem. Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations. In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables. Our approach first develops a novel hybrid Gaussian factor which can connect to both discrete and continuous variables, and a hybrid conditional which can represent multiple continuous hypotheses conditioned on the discrete variables. Using these representations, we derive the process of hybrid variable elimination under the Conditional Linear Gaussian scheme, giving us exact posteriors as hybrid Bayes network. To bound the number of discrete hypotheses, we use a tree-structured representation of the factors coupled with a simple pruning and probabilistic assignment scheme, which allows for tractable inference. We demonstrate the applicability of our framework on a SLAM dataset with ambiguous measurements, where discrete choices for the most likely measurement have to be made. Our demonstrated results showcase the accuracy, generality, and simplicity of our hybrid factor graph framework.

</details>


### [45] [LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration](https://arxiv.org/abs/2601.00555)
*Abu Hanif Muhammad Syarubany,Farhan Zaki Rahmani,Trio Widianto*

Main category: cs.RO

TL;DR: 提出基于LLM的端到端室内购物机器人系统，通过语义地图构建和模块化运动控制实现从自然语言指令到多店铺导航和物品抓取的任务执行


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在室内购物环境中执行复杂导航和物品检索任务的自主机器人系统，通过LLM实现高级决策能力，同时保持系统的模块化和可调试性

Method: 使用LLM作为决策核心，构建轻量级语义地图（检测路口标志牌并存储方向-POI关系），利用AprilTag作为可重复锚点，通过ROS有限状态主控制器执行模块化运动原语（避障、AprilTag接近、店铺进入、抓取）

Result: 在Gazebo仿真和真实走廊环境中验证了系统能够端到端执行从用户指令到多店铺导航和物品检索的任务，系统保持模块化和可调试性，通过文本地图和决策日志便于分析

Conclusion: LLM-based agentic exploration系统成功实现了室内购物任务的端到端执行，展示了LLM在机器人高级决策中的有效性，同时模块化架构确保了系统的可靠性和可维护性

Abstract: This paper presents an end-to-end LLM-based agentic exploration system for an indoor shopping task, evaluated in both Gazebo simulation and a corresponding real-world corridor layout. The robot incrementally builds a lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations together with estimated junction poses, while AprilTags provide repeatable anchors for approach and alignment. Given a natural-language shopping request, an LLM produces a constrained discrete action at each junction (direction and whether to enter a store), and a ROS finite-state main controller executes the decision by gating modular motion primitives, including local-costmap-based obstacle avoidance, AprilTag approaching, store entry, and grasping. Qualitative results show that the integrated stack can perform end-to-end task execution from user instruction to multi-store navigation and object retrieval, while remaining modular and debuggable through its text-based map and logged decision history.

</details>


### [46] [Priority-Aware Multi-Robot Coverage Path Planning](https://arxiv.org/abs/2601.00580)
*Kanghoon Lee,Hyeonjun Kim,Jiachen Li,Jinkyoo Park*

Main category: cs.RO

TL;DR: 提出优先级感知的多机器人覆盖路径规划（PA-MCPP）问题，通过两阶段框架优化优先级区域覆盖延迟和总完成时间


<details>
  <summary>Details</summary>
Motivation: 传统多机器人覆盖路径规划（MCPP）假设所有区域重要性相同，但在实际场景中某些区域需要优先覆盖（如紧急区域、关键设施），现有方法无法有效处理这种优先级差异

Method: 提出可扩展的两阶段框架：1) 贪婪区域分配结合局部搜索和基于生成树的路径规划；2) 斯坦纳树引导的剩余覆盖

Result: 实验表明，相比标准MCPP基线，该方法显著降低了优先级加权延迟，同时保持了竞争力的总完成时间；敏感性分析显示方法能良好扩展到多机器人场景，且通过调整优先级权重可有效控制区域覆盖行为

Conclusion: PA-MCPP问题框架和两阶段方法能够有效处理优先级区域覆盖需求，在保持整体效率的同时优化关键区域的覆盖延迟

Abstract: Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.

</details>


### [47] [NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots](https://arxiv.org/abs/2601.00609)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种用于大型移动机器人的综合导航与控制框架，通过视觉姿态估计、非线性模型预测控制、深度神经网络控制策略和安全监控模块，确保在易打滑地形上的稳定安全操作。


<details>
  <summary>Details</summary>
Motivation: 大型移动机器人在松散、未固结的地形上操作时，由于牵引力降低容易出现打滑问题，需要一种能够确保稳定性和安全性能的导航控制框架。

Method: 提出四模块架构：1) 视觉姿态估计模块融合传感器和立体相机；2) 高层非线性模型预测控制更新轮子运动指令；3) 低层深度神经网络控制策略近似轮驱动机制，增强鲁棒自适应控制；4) 对数安全模块监控整个系统。

Result: 低层控制框架保证了驱动子系统的均匀指数稳定性，安全模块确保了整个系统级的安全操作，在6000公斤大型移动机器人上进行了对比实验验证。

Conclusion: 该综合框架通过联合高性能技术，为大型移动机器人在易打滑地形上提供了稳定安全的导航控制解决方案，实现了系统级的安全保证。

Abstract: A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies.

</details>


### [48] [Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework](https://arxiv.org/abs/2601.00610)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: 提出一个用于大型机器人的安全目标到达控制框架，通过模块化设计结合视觉姿态估计、强化学习运动规划、深度学习建模、鲁棒自适应控制和安全监控，确保系统稳定性和安全性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人应用中需要大量探索状态-动作空间，期间行为可能不安全，这限制了其在复杂地形上运行的大型复杂机器人上的应用。需要设计一个安全的目标到达控制框架来解决这一问题。

Method: 将系统分解为五个紧密耦合的功能模块：1) 实时视觉姿态估计提供准确机器人状态；2) 强化学习运动规划器生成平滑运动指令；3) 监督深度学习模型捕捉机器人复杂动力学；4) 基于模型的鲁棒自适应控制器确保轮子跟踪运动指令；5) 数学安全监控器监控机器人安全。

Result: 提出的框架保证了执行系统的均匀指数稳定性和整个操作的安全性。在6000公斤机器人上的不同场景实验证实了该框架的有效性。

Conclusion: 通过模块化设计结合多种技术，成功实现了大型机器人的安全目标到达控制，解决了强化学习在复杂机器人应用中存在的安全问题，为大型机器人在不稳定地形上的安全操作提供了有效解决方案。

Abstract: Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework.

</details>


### [49] [From 2D to 3D terrain-following area coverage path planning](https://arxiv.org/abs/2601.00614)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 提出一种3D地形跟随的区域覆盖路径规划算法，生成相邻路径，在保持工作宽度间距的同时，使路径在特定工作高度上跟随地形起伏。


<details>
  <summary>Details</summary>
Motivation: 在农业等实际应用中，需要机械设备在复杂三维地形上进行高效覆盖作业。传统的2D路径规划无法适应地形起伏，需要开发能够同时考虑水平间距和垂直高度的3D地形跟随路径规划方法。

Method: 使用逆距离加权方法生成均匀间距的高程数据，结合局部搜索算法，生成相邻路径。路径在水平方向保持机械设备工作宽度的间距，在垂直方向保持特定工作高度跟随地形起伏。

Result: 算法在真实农业场景的3D数据上进行了验证，成功生成了适应地形起伏的覆盖路径。与2D等效算法相比，算法复杂度增加，但能够更好地适应实际地形条件。

Conclusion: 提出的3D地形跟随区域覆盖路径规划算法能够有效处理复杂地形条件下的覆盖任务，为农业机械等应用提供了实用的路径规划解决方案。

Abstract: An algorithm for 3D terrain-following area coverage path planning is presented. Multiple adjacent paths are generated that are (i) locally apart from each other by a distance equal to the working width of a machinery, while (ii) simultaneously floating at a projection distance equal to a specific working height above the terrain. The complexities of the algorithm in comparison to its 2D equivalent are highlighted. These include uniformly spaced elevation data generation using an Inverse Distance Weighting-approach and a local search. Area coverage path planning results for real-world 3D data within an agricultural context are presented to validate the algorithm.

</details>


### [50] [RoboReward: General-Purpose Vision-Language Reward Models for Robotics](https://arxiv.org/abs/2601.00675)
*Tony Lee,Andrew Wagenmaker,Karl Pertsch,Percy Liang,Sergey Levine,Chelsea Finn*

Main category: cs.RO

TL;DR: 提出RoboReward数据集和基准，用于评估视觉语言模型在机器人任务中的奖励建模能力，并训练了4B/8B参数模型，在真实机器人强化学习中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 在机器人领域，获取有效的奖励函数通常需要大量人工标注或手工设计，而视觉语言模型作为自动奖励模型的潜力尚未在真实机器人任务中得到充分验证。

Method: 1) 构建RoboReward数据集和基准，基于Open X-Embodiment和RoboArena的大规模真实机器人数据；2) 提出负样本数据增强流程，通过反事实重标注和时间裁剪生成校准的负样本和接近成功样本；3) 训练4B和8B参数的视觉语言奖励模型。

Result: 评估显示现有视觉语言模型在所有任务上均不占优，训练出的4B/8B参数模型在短视界机器人任务中优于更大模型，8B模型在真实机器人强化学习中大幅超越Gemini Robotics-ER 1.5，接近人工奖励的效果。

Conclusion: 视觉语言模型作为机器人奖励函数具有潜力，但仍有改进空间；提出的RoboReward数据集和训练方法为这一方向提供了系统评估框架，训练的小型模型在实际应用中表现出色。

Abstract: A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \emph{negative examples data augmentation} pipeline that generates calibrated \emph{negatives} and \emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.

</details>


### [51] [DefVINS: Visual-Inertial Odometry for Deformable Scenes](https://arxiv.org/abs/2601.00702)
*Samuel Cerezo,Javier Civera*

Main category: cs.RO

TL;DR: DefVINS：一种视觉惯性里程计框架，通过嵌入变形图显式分离刚性IMU锚定状态与非刚性变形，提高在可变形场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统视觉惯性里程计（VIO）基于刚性假设，在可变形场景中容易过度拟合局部非刚性运动或产生严重漂移，需要专门处理变形问题的解决方案。

Method: 使用嵌入变形图表示非刚性变形，将刚性IMU锚定状态与非刚性变形分离；采用可观测性分析指导变形激活策略，在估计条件良好时渐进激活非刚性自由度。

Result: 通过惯性约束与可观测性感知的变形激活相结合，在非刚性环境中提高了系统的鲁棒性，消融研究验证了该方法的有效性。

Conclusion: DefVINS框架通过显式分离刚性与非刚性运动，结合惯性约束和条件化激活策略，有效解决了可变形场景中的视觉惯性里程计问题。

Abstract: Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.

</details>


### [52] [Calling for Backup: How Children Navigate Successive Robot Communication Failures](https://arxiv.org/abs/2601.00754)
*Maria Teresa Parreira,Isabel Neto,Filipa Rocha,Wendy Ju*

Main category: cs.RO

TL;DR: 研究探索儿童对机器人重复错误的反应，发现儿童与成人既有相似调整行为，也有更多脱离互动行为，但错误不影响儿童对机器人的感知。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注成人对机器人连续错误的反应，但儿童对此的反应尚未充分探索。本研究旨在了解儿童如何应对机器人的社交错误和性能错误，特别是重复的对话错误。

Method: 采用Liu等人的连续机器人失败范式，让59名8-10岁儿童与机器人互动。机器人连续三次无法理解儿童提示，研究人员通过视频记录分析儿童的行为反应。

Result: 儿童与成人反应有相似之处（调整提示、改变语调、情绪化非语言反应），但儿童表现出更多脱离行为（暂时忽略机器人或寻求成人帮助）。错误不影响儿童对机器人的感知，表明儿童对对话期望更灵活。

Conclusion: 儿童对机器人重复错误的反应与成人不同，需要设计更有效、适合儿童发展的人类-机器人交互系统。儿童更灵活的对话期望为设计儿童友好型机器人提供了重要启示。

Abstract: How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.

</details>
