{"id": "2510.06339", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06339", "abs": "https://arxiv.org/abs/2510.06339", "authors": ["Leiyao Cui", "Zihang Zhao", "Sirui Xie", "Wenhuan Zhang", "Zhi Han", "Yixin Zhu"], "title": "Vi-TacMan: Articulated Object Manipulation via Vision and Touch", "comment": null, "summary": "Autonomous manipulation of articulated objects remains a fundamental\nchallenge for robots in human environments. Vision-based methods can infer\nhidden kinematics but can yield imprecise estimates on unfamiliar objects.\nTactile approaches achieve robust control through contact feedback but require\naccurate initialization. This suggests a natural synergy: vision for global\nguidance, touch for local precision. Yet no framework systematically exploits\nthis complementarity for generalized articulated manipulation. Here we present\nVi-TacMan, which uses vision to propose grasps and coarse directions that seed\na tactile controller for precise execution. By incorporating surface normals as\ngeometric priors and modeling directions via von Mises-Fisher distributions,\nour approach achieves significant gains over baselines (all p<0.0001).\nCritically, manipulation succeeds without explicit kinematic models -- the\ntactile controller refines coarse visual estimates through real-time contact\nregulation. Tests on more than 50,000 simulated and diverse real-world objects\nconfirm robust cross-category generalization. This work establishes that coarse\nvisual cues suffice for reliable manipulation when coupled with tactile\nfeedback, offering a scalable paradigm for autonomous systems in unstructured\nenvironments.", "AI": {"tldr": "Vi-TacMan\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff0c\u4f7f\u7528\u89c6\u89c9\u63d0\u4f9b\u5168\u5c40\u6293\u53d6\u5efa\u8bae\u548c\u7c97\u7565\u65b9\u5411\uff0c\u7136\u540e\u901a\u8fc7\u89e6\u89c9\u63a7\u5236\u5668\u8fdb\u884c\u7cbe\u786e\u6267\u884c\uff0c\u65e0\u9700\u663e\u5f0f\u8fd0\u52a8\u5b66\u6a21\u578b\u3002", "motivation": "\u81ea\u4e3b\u64cd\u4f5c\u94f0\u63a5\u7269\u4f53\u662f\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u57fa\u672c\u6311\u6218\u3002\u89c6\u89c9\u65b9\u6cd5\u53ef\u4ee5\u63a8\u65ad\u9690\u85cf\u8fd0\u52a8\u5b66\u4f46\u4e0d\u7cbe\u786e\uff0c\u89e6\u89c9\u65b9\u6cd5\u901a\u8fc7\u63a5\u89e6\u53cd\u9988\u5b9e\u73b0\u9c81\u68d2\u63a7\u5236\u4f46\u9700\u8981\u51c6\u786e\u521d\u59cb\u5316\u3002\u4e24\u8005\u5177\u6709\u5929\u7136\u4e92\u8865\u6027\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u63d0\u51fa\u6293\u53d6\u548c\u7c97\u7565\u65b9\u5411\uff0c\u4f5c\u4e3a\u89e6\u89c9\u63a7\u5236\u5668\u7684\u79cd\u5b50\u3002\u901a\u8fc7\u8868\u9762\u6cd5\u7ebf\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u4f7f\u7528\u51af\u00b7\u7c73\u585e\u65af-\u8d39\u5e0c\u5c14\u5206\u5e03\u5efa\u6a21\u65b9\u5411\u3002\u89e6\u89c9\u63a7\u5236\u5668\u901a\u8fc7\u5b9e\u65f6\u63a5\u89e6\u8c03\u8282\u6765\u7ec6\u5316\u7c97\u7565\u7684\u89c6\u89c9\u4f30\u8ba1\u3002", "result": "\u5728\u8d85\u8fc750,000\u4e2a\u6a21\u62df\u548c\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u4e0a\u7684\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u8de8\u7c7b\u522b\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff08\u6240\u6709p<0.0001\uff09\u3002", "conclusion": "\u7c97\u89c6\u89c9\u7ebf\u7d22\u7ed3\u5408\u89e6\u89c9\u53cd\u9988\u8db3\u4ee5\u5b9e\u73b0\u53ef\u9760\u64cd\u4f5c\uff0c\u4e3a\u65e0\u7ed3\u6784\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002"}}
{"id": "2510.06351", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06351", "abs": "https://arxiv.org/abs/2510.06351", "authors": ["Kaleb Ben Naveed", "Devansh R. Agrawal", "Dimitra Panagou"], "title": "A Formal gatekeeper Framework for Safe Dual Control with Active Exploration", "comment": "Submitted to American Control Conference (ACC) 2026", "summary": "Planning safe trajectories under model uncertainty is a fundamental\nchallenge. Robust planning ensures safety by considering worst-case\nrealizations, yet ignores uncertainty reduction and leads to overly\nconservative behavior. Actively reducing uncertainty on-the-fly during a\nnominal mission defines the dual control problem. Most approaches address this\nby adding a weighted exploration term to the cost, tuned to trade off the\nnominal objective and uncertainty reduction, but without formal consideration\nof when exploration is beneficial. Moreover, safety is enforced in some methods\nbut not in others. We propose a framework that integrates robust planning with\nactive exploration under formal guarantees as follows: The key innovation and\ncontribution is that exploration is pursued only when it provides a verifiable\nimprovement without compromising safety. To achieve this, we utilize our\nearlier work on gatekeeper as an architecture for safety verification, and\nextend it so that it generates both safe and informative trajectories that\nreduce uncertainty and the cost of the mission, or keep it within a\nuser-defined budget. The methodology is evaluated via simulation case studies\non the online dual control of a quadrotor under parametric uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u9c81\u68d2\u89c4\u5212\u4e0e\u4e3b\u52a8\u63a2\u7d22\u7684\u6846\u67b6\uff0c\u4ec5\u5728\u63a2\u7d22\u80fd\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u6539\u8fdb\u4e14\u4e0d\u635f\u5bb3\u5b89\u5168\u6027\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u4f20\u7edf\u9c81\u68d2\u89c4\u5212\u8fc7\u4e8e\u4fdd\u5b88\u4e14\u5ffd\u7565\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u63a2\u7d22\u65f6\u673a\u548c\u5b89\u5168\u6027\u7684\u6b63\u5f0f\u8003\u8651\u3002", "method": "\u5229\u7528gatekeeper\u67b6\u6784\u8fdb\u884c\u5b89\u5168\u9a8c\u8bc1\uff0c\u6269\u5c55\u5176\u751f\u6210\u65e2\u5b89\u5168\u53c8\u4fe1\u606f\u4e30\u5bcc\u7684\u8f68\u8ff9\uff0c\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u548c\u4efb\u52a1\u6210\u672c\uff0c\u6216\u4fdd\u6301\u5728\u7528\u6237\u5b9a\u4e49\u7684\u9884\u7b97\u5185\u3002", "result": "\u901a\u8fc7\u56db\u65cb\u7ffc\u98de\u884c\u5668\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7684\u5728\u7ebf\u53cc\u63a7\u5236\u4eff\u771f\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u524d\u63d0\u4e0b\uff0c\u667a\u80fd\u5730\u9009\u62e9\u65f6\u673a\u8fdb\u884c\u63a2\u7d22\uff0c\u6709\u6548\u5e73\u8861\u4efb\u52a1\u6267\u884c\u4e0e\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u7684\u9700\u6c42\u3002"}}
{"id": "2510.06357", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06357", "abs": "https://arxiv.org/abs/2510.06357", "authors": ["Grayson Byrd", "Corban Rivera", "Bethany Kemp", "Meghan Booker", "Aurora Schmidt", "Celso M de Melo", "Lalithkumar Seenivasan", "Mathias Unberath"], "title": "Constrained Natural Language Action Planning for Resilient Embodied Systems", "comment": null, "summary": "Replicating human-level intelligence in the execution of embodied tasks\nremains challenging due to the unconstrained nature of real-world environments.\nNovel use of large language models (LLMs) for task planning seeks to address\nthe previously intractable state/action space of complex planning tasks, but\nhallucinations limit their reliability, and thus, viability beyond a research\ncontext. Additionally, the prompt engineering required to achieve adequate\nsystem performance lacks transparency, and thus, repeatability. In contrast to\nLLM planning, symbolic planning methods offer strong reliability and\nrepeatability guarantees, but struggle to scale to the complexity and ambiguity\nof real-world tasks. We introduce a new robotic planning method that augments\nLLM planners with symbolic planning oversight to improve reliability and\nrepeatability, and provide a transparent approach to defining hard constraints\nwith considerably stronger clarity than traditional prompt engineering.\nImportantly, these augmentations preserve the reasoning capabilities of LLMs\nand retain impressive generalization in open-world environments. We demonstrate\nour approach in simulated and real-world environments. On the ALFWorld planning\nbenchmark, our approach outperforms current state-of-the-art methods, achieving\na near-perfect 99% success rate. Deployment of our method to a real-world\nquadruped robot resulted in 100% task success compared to 50% and 30% for pure\nLLM and symbolic planners across embodied pick and place tasks. Our approach\npresents an effective strategy to enhance the reliability, repeatability and\ntransparency of LLM-based robot planners while retaining their key strengths:\nflexibility and generalizability to complex real-world environments. We hope\nthat this work will contribute to the broad goal of building resilient embodied\nintelligent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7b26\u53f7\u89c4\u5212\u7684\u65b0\u673a\u5668\u4eba\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b26\u53f7\u89c4\u5212\u76d1\u7763\u63d0\u9ad8LLM\u89c4\u5212\u5668\u7684\u53ef\u9760\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u7eaf\u7b26\u53f7\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u5230\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u7684\u5c40\u9650\u6027\u3002", "method": "\u5c06LLM\u89c4\u5212\u5668\u4e0e\u7b26\u53f7\u89c4\u5212\u76d1\u7763\u76f8\u7ed3\u5408\uff0c\u7528\u7b26\u53f7\u65b9\u6cd5\u63d0\u4f9b\u786c\u7ea6\u675f\u5b9a\u4e49\uff0c\u4fdd\u7559LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\u3002", "result": "\u5728ALFWorld\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523099%\u7684\u6210\u529f\u7387\uff1b\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\uff0c\u76f8\u6bd4\u7eafLLM\u89c4\u5212\u5668\uff0850%\uff09\u548c\u7eaf\u7b26\u53f7\u89c4\u5212\u5668\uff0830%\uff09\uff0c\u5b9e\u73b0\u4e86100%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u89c4\u5212\u5668\u7684\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u900f\u660e\u5ea6\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u4f18\u52bf\u3002"}}
{"id": "2510.06481", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06481", "abs": "https://arxiv.org/abs/2510.06481", "authors": ["Amirhossein Mollaei Khass", "Guangyi Liu", "Vivek Pandey", "Wen Jiang", "Boshu Lei", "Kostas Daniilidis", "Nader Motee"], "title": "Active Next-Best-View Optimization for Risk-Averse Path Planning", "comment": null, "summary": "Safe navigation in uncertain environments requires planning methods that\nintegrate risk aversion with active perception. In this work, we present a\nunified framework that refines a coarse reference path by constructing\ntail-sensitive risk maps from Average Value-at-Risk statistics on an\nonline-updated 3D Gaussian-splat Radiance Field. These maps enable the\ngeneration of locally safe and feasible trajectories. In parallel, we formulate\nNext-Best-View (NBV) selection as an optimization problem on the SE(3) pose\nmanifold, where Riemannian gradient descent maximizes an expected information\ngain objective to reduce uncertainty most critical for imminent motion. Our\napproach advances the state-of-the-art by coupling risk-averse path refinement\nwith NBV planning, while introducing scalable gradient decompositions that\nsupport efficient online updates in complex environments. We demonstrate the\neffectiveness of the proposed framework through extensive computational\nstudies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u5e73\u5747\u98ce\u9669\u4ef7\u503c\u7684\u5c3e\u90e8\u654f\u611f\u98ce\u9669\u5730\u56fe\u6765\u7ec6\u5316\u53c2\u8003\u8def\u5f84\uff0c\u540c\u65f6\u5728SE(3)\u59ff\u6001\u6d41\u5f62\u4e0a\u4f18\u5316\u6700\u4f73\u89c6\u70b9\u9009\u62e9\uff0c\u8026\u5408\u98ce\u9669\u89c4\u907f\u8def\u5f84\u4f18\u5316\u4e0e\u4e3b\u52a8\u611f\u77e5\u3002", "motivation": "\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\u9700\u8981\u5c06\u98ce\u9669\u89c4\u907f\u4e0e\u4e3b\u52a8\u611f\u77e5\u76f8\u7ed3\u5408\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e24\u65b9\u9762\u7684\u96c6\u6210\u4e0d\u591f\u5145\u5206\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u66f4\u65b0\u76843D\u9ad8\u65af\u6cfc\u6e85\u8f90\u5c04\u573a\u6784\u5efa\u5c3e\u90e8\u654f\u611f\u98ce\u9669\u5730\u56fe\uff0c\u5728SE(3)\u59ff\u6001\u6d41\u5f62\u4e0a\u901a\u8fc7\u9ece\u66fc\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u6700\u4f73\u89c6\u70b9\u9009\u62e9\uff0c\u6700\u5927\u5316\u671f\u671b\u4fe1\u606f\u589e\u76ca\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u8ba1\u7b97\u7814\u7a76\u8bc1\u660e\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u5c40\u90e8\u5b89\u5168\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u53ef\u6269\u5c55\u7684\u68af\u5ea6\u5206\u89e3\u652f\u6301\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u5728\u7ebf\u66f4\u65b0\uff0c\u63a8\u8fdb\u4e86\u98ce\u9669\u89c4\u907f\u8def\u5f84\u4f18\u5316\u4e0e\u6700\u4f73\u89c6\u70b9\u89c4\u5212\u7684\u8026\u5408\u3002"}}
{"id": "2510.06492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06492", "abs": "https://arxiv.org/abs/2510.06492", "authors": ["Matthew Kim", "Kensuke Nakamura", "Andrea Bajcsy"], "title": "What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?", "comment": "8 tables 6 figures", "summary": "Safe control techniques, such as Hamilton-Jacobi reachability, provide\nprincipled methods for synthesizing safety-preserving robot policies but\ntypically assume hand-designed state spaces and full observability. Recent work\nhas relaxed these assumptions via latent-space safe control, where state\nrepresentations and dynamics are learned jointly through world models that\nreconstruct future high-dimensional observations (e.g., RGB images) from\ncurrent observations and actions. This enables safety constraints that are\ndifficult to specify analytically (e.g., spilling) to be framed as\nclassification problems in latent space, allowing controllers to operate\ndirectly from raw observations. However, these methods assume that\nsafety-critical features are observable in the learned latent state. We ask:\nwhen are latent state spaces sufficient for safe control? To study this, we\nexamine temperature-based failures, comparable to overheating in cooking or\nmanufacturing tasks, and find that RGB-only observations can produce myopic\nsafety behaviors, e.g., avoiding seeing failure states rather than preventing\nfailure itself. To predict such behaviors, we introduce a mutual\ninformation-based measure that identifies when observations fail to capture\nsafety-relevant features. Finally, we propose a multimodal-supervised training\nstrategy that shapes the latent state with additional sensory inputs during\ntraining, but requires no extra modalities at deployment, and validate our\napproach in simulation and on hardware with a Franka Research 3 manipulator\npreventing a pot of wax from overheating.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8eRGB\u89c2\u6d4b\u7684\u6f5c\u5728\u7a7a\u95f4\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e92\u4fe1\u606f\u5ea6\u91cf\u6765\u8bc6\u522b\u89c2\u6d4b\u4e2d\u7f3a\u5931\u7684\u5b89\u5168\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u6765\u6539\u8fdb\u6f5c\u5728\u72b6\u6001\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u6f5c\u5728\u7a7a\u95f4\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\u5047\u8bbe\u5b89\u5168\u5173\u952e\u7279\u5f81\u53ef\u4ee5\u5728\u5b66\u4e60\u7684\u6f5c\u5728\u72b6\u6001\u4e2d\u89c2\u5bdf\u5230\uff0c\u4f46\u5b9e\u9645\u4e2dRGB\u89c2\u6d4b\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u67d0\u4e9b\u5b89\u5168\u76f8\u5173\u7279\u5f81\uff08\u5982\u6e29\u5ea6\uff09\uff0c\u5bfc\u81f4\u77ed\u89c6\u7684\u5b89\u5168\u884c\u4e3a\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u5ea6\u91cf\u6765\u8bc6\u522b\u89c2\u6d4b\u4e2d\u7f3a\u5931\u7684\u5b89\u5168\u76f8\u5173\u7279\u5f81\uff1b\u63d0\u51fa\u591a\u6a21\u6001\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u989d\u5916\u4f20\u611f\u5668\u8f93\u5165\u5851\u9020\u6f5c\u5728\u72b6\u6001\uff0c\u4f46\u90e8\u7f72\u65f6\u4ec5\u9700RGB\u89c2\u6d4b\u3002", "result": "\u5728\u4eff\u771f\u548cFranka Research 3\u673a\u68b0\u81c2\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u6210\u529f\u9632\u6b62\u8721\u9505\u8fc7\u70ed\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u76d1\u7763\u8bad\u7ec3\u53ef\u4ee5\u6539\u5584\u6f5c\u5728\u72b6\u6001\u8868\u793a\uff0c\u4f7f\u57fa\u4e8eRGB\u89c2\u6d4b\u7684\u5b89\u5168\u63a7\u5236\u5668\u80fd\u591f\u66f4\u597d\u5730\u9884\u6d4b\u548c\u9632\u6b62\u6e29\u5ea6\u76f8\u5173\u7684\u6545\u969c\u3002"}}
{"id": "2510.06518", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.06518", "abs": "https://arxiv.org/abs/2510.06518", "authors": ["Malakhi Hopkins", "Varun Murali", "Vijay Kumar", "Camillo J Taylor"], "title": "Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots", "comment": "8 pages, 8 figures, submitted to ICRA 2026", "summary": "Autonomous aerial robots are increasingly being deployed in real-world\nscenarios, where transparent obstacles present significant challenges to\nreliable navigation and mapping. These materials pose a unique problem for\ntraditional perception systems because they lack discernible features and can\ncause conventional depth sensors to fail, leading to inaccurate maps and\npotential collisions. To ensure safe navigation, robots must be able to\naccurately detect and map these transparent obstacles. Existing methods often\nrely on large, expensive sensors or algorithms that impose high computational\nburdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots.\nIn this work, we propose a novel and computationally efficient framework for\ndetecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our\nmethod fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor\nwith a custom, lightweight 2D convolution model. This specialized approach\naccurately detects specular reflections and propagates their depth into\ncorresponding empty regions of the depth map, effectively rendering transparent\nobstacles visible. The entire pipeline operates in real-time, utilizing only a\nsmall fraction of a CPU core on an embedded processor. We validate our system\nthrough a series of experiments in both controlled and real-world environments,\ndemonstrating the utility of our method through experiments where the robot\nmaps indoor environments containing glass. Our work is, to our knowledge, the\nfirst of its kind to demonstrate a real-time, onboard transparent obstacle\nmapping system on a low-SWaP quadrotor using only the CPU.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c0f\u578b\u65e0\u4eba\u673a\u5b9e\u65f6\u68c0\u6d4b\u548c\u6620\u5c04\u900f\u660e\u969c\u788d\u7269\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u878d\u5408ToF\u76f8\u673a\u548c\u8d85\u58f0\u6ce2\u4f20\u611f\u5668\u6570\u636e\uff0c\u4f7f\u7528\u5b9a\u52362D\u5377\u79ef\u6a21\u578b\u5728\u5d4c\u5165\u5f0fCPU\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "motivation": "\u900f\u660e\u969c\u788d\u7269\u5bf9\u4f20\u7edf\u611f\u77e5\u7cfb\u7edf\u6784\u6210\u6311\u6218\uff0c\u7f3a\u4e4f\u53ef\u8bc6\u522b\u7279\u5f81\u5bfc\u81f4\u6df1\u5ea6\u4f20\u611f\u5668\u5931\u6548\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5927\u578b\u6602\u8d35\u4f20\u611f\u5668\u6216\u9ad8\u8ba1\u7b97\u91cf\u7b97\u6cd5\uff0c\u4e0d\u9002\u5408\u4f4eSWaP\u65e0\u4eba\u673a\u3002", "method": "\u878d\u5408ToF\u76f8\u673a\u548c\u8d85\u58f0\u6ce2\u4f20\u611f\u5668\u6570\u636e\uff0c\u91c7\u7528\u5b9a\u5236\u8f7b\u91cf\u7ea72D\u5377\u79ef\u6a21\u578b\u68c0\u6d4b\u955c\u9762\u53cd\u5c04\u5e76\u5c06\u6df1\u5ea6\u4f20\u64ad\u5230\u6df1\u5ea6\u56fe\u7684\u7a7a\u533a\u57df\uff0c\u4f7f\u900f\u660e\u969c\u788d\u7269\u53ef\u89c1\u3002", "result": "\u5728\u53d7\u63a7\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6709\u6548\u6027\uff0c\u65e0\u4eba\u673a\u80fd\u591f\u6210\u529f\u6620\u5c04\u5305\u542b\u73bb\u7483\u7684\u5ba4\u5185\u73af\u5883\uff0c\u6574\u4e2a\u7ba1\u9053\u5728\u5d4c\u5165\u5f0f\u5904\u7406\u5668\u4e0a\u4ec5\u4f7f\u7528\u5c11\u91cfCPU\u6838\u5fc3\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u4f4eSWaP\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u4ec5\u4f7f\u7528CPU\u5b9e\u73b0\u5b9e\u65f6\u3001\u673a\u8f7d\u900f\u660e\u969c\u788d\u7269\u6620\u5c04\u7684\u7cfb\u7edf\u3002"}}
{"id": "2510.06546", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06546", "abs": "https://arxiv.org/abs/2510.06546", "authors": ["Mohammad Nazeri", "Sheldon Mei", "Jeffrey Watchorn", "Alex Zhang", "Erin Ng", "Tao Wen", "Abhijoy Mandal", "Kevin Golovin", "Alan Aspuru-Guzik", "Frank Gu"], "title": "RAISE: A self-driving laboratory for interfacial property formulation discovery", "comment": "Mohammad Nazeri, Sheldon Mei, and Jeffrey Watchorn contributed\n  equally to this work. *Corresponding author: Frank Gu (f.gu@utoronto.ca)", "summary": "Surface wettability is a critical design parameter for biomedical devices,\ncoatings, and textiles. Contact angle measurements quantify liquid-surface\ninteractions, which depend strongly on liquid formulation. Herein, we present\nthe Robotic Autonomous Imaging Surface Evaluator (RAISE), a closed-loop,\nself-driving laboratory that is capable of linking liquid formulation\noptimization with surface wettability assessment. RAISE comprises a full\nexperimental orchestrator with the ability of mixing liquid ingredients to\ncreate varying formulation cocktails, transferring droplets of prepared\nformulations to a high-throughput stage, and using a pick-and-place camera tool\nfor automated droplet image capture. The system also includes an automated\nimage processing pipeline to measure contact angles. This closed loop\nexperiment orchestrator is integrated with a Bayesian Optimization (BO) client,\nwhich enables iterative exploration of new formulations based on previous\ncontact angle measurements to meet user-defined objectives. The system operates\nin a high-throughput manner and can achieve a measurement rate of approximately\n1 contact angle measurement per minute. Here we demonstrate RAISE can be used\nto explore surfactant wettability and how surfactant combinations create\ntunable formulations that compensate for purity-related variations.\nFurthermore, multi-objective BO demonstrates how precise and optimal\nformulations can be reached based on application-specific goals. The\noptimization is guided by a desirability score, which prioritizes formulations\nthat are within target contact angle ranges, minimize surfactant usage and\nreduce cost. This work demonstrates the capabilities of RAISE to autonomously\nlink liquid formulations to contact angle measurements in a closed-loop system,\nusing multi-objective BO to efficiently identify optimal formulations aligned\nwith researcher-defined criteria.", "AI": {"tldr": "RAISE\u662f\u4e00\u4e2a\u95ed\u73af\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u7cfb\u7edf\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u81ea\u52a8\u8fde\u63a5\u6db2\u4f53\u914d\u65b9\u4f18\u5316\u4e0e\u8868\u9762\u6da6\u6e7f\u6027\u8bc4\u4f30\uff0c\u5b9e\u73b0\u9ad8\u901a\u91cf\u63a5\u89e6\u89d2\u6d4b\u91cf\u548c\u914d\u65b9\u4f18\u5316\u3002", "motivation": "\u8868\u9762\u6da6\u6e7f\u6027\u662f\u751f\u7269\u533b\u5b66\u8bbe\u5907\u3001\u6d82\u5c42\u548c\u7eba\u7ec7\u54c1\u7684\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\uff0c\u4f46\u6db2\u4f53\u914d\u65b9\u5bf9\u63a5\u89e6\u89d2\u6d4b\u91cf\u5f71\u54cd\u5f88\u5927\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u4f18\u5316\u914d\u65b9\u4e0e\u6da6\u6e7f\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "RAISE\u7cfb\u7edf\u5305\u542b\u5b9e\u9a8c\u534f\u8c03\u5668\uff0c\u80fd\u591f\u6df7\u5408\u6db2\u4f53\u6210\u5206\u3001\u8f6c\u79fb\u6db2\u6ef4\u3001\u81ea\u52a8\u91c7\u96c6\u56fe\u50cf\uff0c\u5e76\u96c6\u6210\u8d1d\u53f6\u65af\u4f18\u5316\u5ba2\u6237\u7aef\u8fdb\u884c\u8fed\u4ee3\u914d\u65b9\u63a2\u7d22\u3002", "result": "\u7cfb\u7edf\u6d4b\u91cf\u901f\u7387\u7ea6\u4e3a\u6bcf\u5206\u949f1\u4e2a\u63a5\u89e6\u89d2\u6d4b\u91cf\uff0c\u80fd\u591f\u63a2\u7d22\u8868\u9762\u6d3b\u6027\u5242\u6da6\u6e7f\u6027\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\u627e\u5230\u7b26\u5408\u7279\u5b9a\u5e94\u7528\u76ee\u6807\u7684\u6700\u4f73\u914d\u65b9\u3002", "conclusion": "RAISE\u5c55\u793a\u4e86\u5728\u95ed\u73af\u7cfb\u7edf\u4e2d\u81ea\u4e3b\u8fde\u63a5\u6db2\u4f53\u914d\u65b9\u4e0e\u63a5\u89e6\u89d2\u6d4b\u91cf\u7684\u80fd\u529b\uff0c\u4f7f\u7528\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\u9ad8\u6548\u8bc6\u522b\u7b26\u5408\u7814\u7a76\u4eba\u5458\u5b9a\u4e49\u6807\u51c6\u7684\u6700\u4f73\u914d\u65b9\u3002"}}
{"id": "2510.06566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06566", "abs": "https://arxiv.org/abs/2510.06566", "authors": ["Vincent Lam", "Robin Chhabra"], "title": "Safe Obstacle-Free Guidance of Space Manipulators in Debris Removal Missions via Deep Reinforcement Learning", "comment": null, "summary": "The objective of this study is to develop a model-free workspace trajectory\nplanner for space manipulators using a Twin Delayed Deep Deterministic Policy\nGradient (TD3) agent to enable safe and reliable debris capture. A local\ncontrol strategy with singularity avoidance and manipulability enhancement is\nemployed to ensure stable execution. The manipulator must simultaneously track\na capture point on a non-cooperative target, avoid self-collisions, and prevent\nunintended contact with the target. To address these challenges, we propose a\ncurriculum-based multi-critic network where one critic emphasizes accurate\ntracking and the other enforces collision avoidance. A prioritized experience\nreplay buffer is also used to accelerate convergence and improve policy\nrobustness. The framework is evaluated on a simulated seven-degree-of-freedom\nKUKA LBR iiwa mounted on a free-floating base in Matlab/Simulink, demonstrating\nsafe and adaptive trajectory generation for debris removal missions.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eTD3\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u6a21\u578b\u7a7a\u95f4\u673a\u68b0\u81c2\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u591a\u6279\u8bc4\u5668\u7f51\u7edc\u548c\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u592a\u7a7a\u788e\u7247\u6355\u83b7", "motivation": "\u89e3\u51b3\u7a7a\u95f4\u673a\u68b0\u81c2\u5728\u6355\u83b7\u975e\u5408\u4f5c\u76ee\u6807\u65f6\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u7cbe\u786e\u8ddf\u8e2a\u3001\u907f\u514d\u81ea\u78b0\u649e\u548c\u9632\u6b62\u610f\u5916\u63a5\u89e6\u7684\u590d\u6742\u6311\u6218", "method": "\u4f7f\u7528TD3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u63a7\u5236\u7b56\u7565\u8fdb\u884c\u5947\u5f02\u6027\u907f\u514d\u548c\u53ef\u64cd\u4f5c\u6027\u589e\u5f3a\uff0c\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u7684\u591a\u6279\u8bc4\u5668\u7f51\u7edc\u548c\u4f18\u5148\u7ecf\u9a8c\u56de\u653e", "result": "\u5728Matlab/Simulink\u4e2d\u6a21\u62df\u7684\u4e03\u81ea\u7531\u5ea6KUKA LBR iiwa\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u5b89\u5168\u81ea\u9002\u5e94\u7684\u788e\u7247\u6e05\u9664\u8f68\u8ff9", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u4e3a\u7a7a\u95f4\u788e\u7247\u6e05\u9664\u4efb\u52a1\u63d0\u4f9b\u5b89\u5168\u53ef\u9760\u7684\u8f68\u8ff9\u89c4\u5212\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.06633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06633", "abs": "https://arxiv.org/abs/2510.06633", "authors": ["Kruthika Gangaraju", "Tanmayi Inaparthy", "Jiaqi Yang", "Yihao Zheng", "Fengpei Yuan"], "title": "Assist-As-Needed: Adaptive Multimodal Robotic Assistance for Medication Management in Dementia Care", "comment": null, "summary": "People living with dementia (PLWDs) face progressively declining abilities in\nmedication management-from simple forgetfulness to complete task breakdown-yet\nmost assistive technologies fail to adapt to these changing needs. This\none-size-fits-all approach undermines autonomy, accelerates dependence, and\nincreases caregiver burden. Occupational therapy principles emphasize matching\nassistance levels to individual capabilities: minimal reminders for those who\nmerely forget, spatial guidance for those who misplace items, and comprehensive\nmultimodal support for those requiring step-by-step instruction. However,\nexisting robotic systems lack this adaptive, graduated response framework\nessential for maintaining PLWD independence. We present an adaptive multimodal\nrobotic framework using the Pepper robot that dynamically adjusts assistance\nbased on real-time assessment of user needs. Our system implements a\nhierarchical intervention model progressing from (1) simple verbal reminders,\nto (2) verbal + gestural cues, to (3) full multimodal guidance combining\nphysical navigation to medication locations with step-by-step verbal and\ngestural instructions. Powered by LLM-driven interaction strategies and\nmultimodal sensing, the system continuously evaluates task states to provide\njust-enough assistance-preserving autonomy while ensuring medication adherence.\nWe conducted a preliminary study with healthy adults and dementia care\nstakeholders in a controlled lab setting, evaluating the system's usability,\ncomprehensibility, and appropriateness of adaptive feedback mechanisms. This\nwork contributes: (1) a theoretically grounded adaptive assistance framework\ntranslating occupational therapy principles into HRI design, (2) a multimodal\nrobotic implementation that preserves PLWD dignity through graduated support,\nand (3) empirical insights into stakeholder perceptions of adaptive robotic\ncare.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePepper\u673a\u5668\u4eba\u7684\u81ea\u9002\u5e94\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u75f4\u5446\u75c7\u60a3\u8005\u63d0\u4f9b\u52a8\u6001\u8c03\u6574\u7684\u7528\u836f\u7ba1\u7406\u8f85\u52a9\uff0c\u4ece\u7b80\u5355\u63d0\u9192\u5230\u5168\u9762\u6307\u5bfc\u7684\u5206\u7ea7\u5e72\u9884\u3002", "motivation": "\u73b0\u6709\u8f85\u52a9\u6280\u672f\u65e0\u6cd5\u9002\u5e94\u75f4\u5446\u75c7\u60a3\u8005\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\uff0c\u7edf\u4e00\u7684\u8f85\u52a9\u65b9\u5f0f\u4f1a\u524a\u5f31\u60a3\u8005\u81ea\u4e3b\u6027\u3001\u52a0\u901f\u4f9d\u8d56\u5e76\u589e\u52a0\u62a4\u7406\u8d1f\u62c5\u3002\u9700\u8981\u57fa\u4e8e\u804c\u4e1a\u6cbb\u7597\u539f\u5219\u7684\u9002\u5e94\u6027\u8f85\u52a9\u6846\u67b6\u3002", "method": "\u4f7f\u7528Pepper\u673a\u5668\u4eba\u5b9e\u73b0\u5206\u5c42\u5e72\u9884\u6a21\u578b\uff1a\u4ece\u7b80\u5355\u8bed\u8a00\u63d0\u9192\uff0c\u5230\u8bed\u8a00+\u624b\u52bf\u63d0\u793a\uff0c\u518d\u5230\u7ed3\u5408\u7269\u7406\u5bfc\u822a\u548c\u9010\u6b65\u6307\u5bfc\u7684\u5b8c\u6574\u591a\u6a21\u6001\u5f15\u5bfc\u3002\u7cfb\u7edf\u901a\u8fc7LLM\u9a71\u52a8\u7684\u4ea4\u4e92\u7b56\u7565\u548c\u591a\u6a21\u6001\u611f\u77e5\u5b9e\u65f6\u8bc4\u4f30\u4efb\u52a1\u72b6\u6001\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u5bf9\u5065\u5eb7\u6210\u4eba\u548c\u75f4\u5446\u75c7\u62a4\u7406\u5229\u76ca\u76f8\u5173\u8005\u8fdb\u884c\u4e86\u521d\u6b65\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u9002\u5e94\u6027\u53cd\u9988\u673a\u5236\u7684\u9002\u5f53\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8d21\u732e\u5305\u62ec\uff1a\u57fa\u4e8e\u804c\u4e1a\u6cbb\u7597\u539f\u5219\u7684\u81ea\u9002\u5e94\u8f85\u52a9\u6846\u67b6\u3001\u4fdd\u62a4\u60a3\u8005\u5c0a\u4e25\u7684\u591a\u6a21\u6001\u673a\u5668\u4eba\u5b9e\u73b0\uff0c\u4ee5\u53ca\u5bf9\u9002\u5e94\u6027\u673a\u5668\u4eba\u62a4\u7406\u5229\u76ca\u76f8\u5173\u8005\u770b\u6cd5\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002"}}
{"id": "2510.06710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06710", "abs": "https://arxiv.org/abs/2510.06710", "authors": ["Hongzhi Zang", "Mingjie Wei", "Si Xu", "Yongji Wu", "Zhen Guo", "Yuanqing Wang", "Hao Lin", "Liangzhi Shi", "Yuqing Xie", "Zhexuan Xu", "Zhihao Liu", "Kang Chen", "Wenhao Tang", "Quanlu Zhang", "Weinan Zhang", "Chao Yu", "Yu Wang"], "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training", "comment": "This is the technical report of the RLinf Team, focusing on the\n  algorithm side. For the system-level design, please refer to\n  arXiv:2509.15965. The open-sourced code link: https://github.com/RLinf/RLinf", "summary": "Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwith supervised fine-tuning (SFT), which struggles to generalize under\ndistribution shifts due to error accumulation. Reinforcement learning (RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexible resource allocation design that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, for GPU-parallelized simulators, RLinf-VLA implements a novel\nhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface, RLinf-VLA seamlessly supports diverse\nVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,\nPPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25\nManiSkill tasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-world Franka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envision RLinf-VLA as a\nfoundation to accelerate and standardize research on embodied intelligence.", "AI": {"tldr": "RLinf-VLA\u662f\u4e00\u4e2a\u7edf\u4e00\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u62df\u5668\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5927\u591a\u4f7f\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u8bad\u7ec3\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u5f3a\u5316\u5b66\u4e60(RL)\u867d\u7136\u80fd\u76f4\u63a5\u4f18\u5316\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5e73\u53f0\u8fdb\u884c\u516c\u5e73\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "\u5f00\u53d1\u4e86RLinf-VLA\u6846\u67b6\uff0c\u91c7\u7528\u7075\u6d3b\u7684\u8d44\u6e90\u914d\u7f6e\u8bbe\u8ba1\uff0c\u652f\u6301GPU\u5e76\u884c\u6a21\u62df\u5668\uff0c\u5b9e\u73b0\u4e86\u6df7\u5408\u7ec6\u7c92\u5ea6\u6d41\u6c34\u7ebf\u5206\u914d\u6a21\u5f0f\uff0c\u652f\u6301\u591a\u79cdVLA\u67b6\u6784\u3001RL\u7b97\u6cd5\u548c\u6a21\u62df\u5668\u3002", "result": "\u5728130\u4e2aLIBERO\u4efb\u52a1\u4e0a\u8fbe\u523098.11%\u6210\u529f\u7387\uff0c\u572825\u4e2aManiSkill\u4efb\u52a1\u4e0a\u8fbe\u523097.66%\u6210\u529f\u7387\u3002\u5728\u771f\u5b9eFranka\u673a\u5668\u4eba\u4e0a\uff0cRL\u8bad\u7ec3\u7684\u7b56\u7565\u6bd4SFT\u8bad\u7ec3\u7684\u7b56\u7565\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RLinf-VLA\u4e3a\u5177\u8eab\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u52a0\u901f\u548c\u6807\u51c6\u5316\u7684\u57fa\u7840\u6846\u67b6\uff0c\u5e76\u603b\u7ed3\u4e86\u4e00\u5957\u5c06RL\u5e94\u7528\u4e8eVLA\u8bad\u7ec3\u7684\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2510.06717", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06717", "abs": "https://arxiv.org/abs/2510.06717", "authors": ["Yuanfei Lin", "Sebastian Illing", "Matthias Althoff"], "title": "SanDRA: Safe Large-Language-Model-Based Decision Making for Automated Vehicles Using Reachability Analysis", "comment": "@2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Large language models have been widely applied to knowledge-driven\ndecision-making for automated vehicles due to their strong generalization and\nreasoning capabilities. However, the safety of the resulting decisions cannot\nbe ensured due to possible hallucinations and the lack of integrated vehicle\ndynamics. To address this issue, we propose SanDRA, the first safe\nlarge-language-model-based decision making framework for automated vehicles\nusing reachability analysis. Our approach starts with a comprehensive\ndescription of the driving scenario to prompt large language models to generate\nand rank feasible driving actions. These actions are translated into temporal\nlogic formulas that incorporate formalized traffic rules, and are subsequently\nintegrated into reachability analysis to eliminate unsafe actions. We validate\nour approach in both open-loop and closed-loop driving environments using\noff-the-shelf and finetuned large language models, showing that it can provide\nprovably safe and, where possible, legally compliant driving actions, even\nunder high-density traffic conditions. To ensure transparency and facilitate\nfuture research, all code and experimental setups are publicly available at\ngithub.com/CommonRoad/SanDRA.", "AI": {"tldr": "SanDRA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8fbe\u6027\u5206\u6790\u786e\u4fdd\u51b3\u7b56\u5b89\u5168\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u5e7b\u89c9\u548c\u7f3a\u4e4f\u8f66\u8f86\u52a8\u529b\u5b66\u96c6\u6210\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u7531\u4e8e\u53ef\u80fd\u4ea7\u751f\u5e7b\u89c9\u548c\u7f3a\u4e4f\u8f66\u8f86\u52a8\u529b\u5b66\u96c6\u6210\uff0c\u5176\u51b3\u7b56\u5b89\u5168\u6027\u65e0\u6cd5\u4fdd\u8bc1\u3002", "method": "\u9996\u5148\u5168\u9762\u63cf\u8ff0\u9a7e\u9a76\u573a\u666f\uff0c\u63d0\u793a\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e76\u6392\u5e8f\u53ef\u884c\u9a7e\u9a76\u52a8\u4f5c\uff1b\u7136\u540e\u5c06\u8fd9\u4e9b\u52a8\u4f5c\u8f6c\u5316\u4e3a\u5305\u542b\u5f62\u5f0f\u5316\u4ea4\u901a\u89c4\u5219\u7684\u65f6\u95f4\u903b\u8f91\u516c\u5f0f\uff1b\u6700\u540e\u901a\u8fc7\u53ef\u8fbe\u6027\u5206\u6790\u6d88\u9664\u4e0d\u5b89\u5168\u52a8\u4f5c\u3002", "result": "\u5728\u5f00\u73af\u548c\u95ed\u73af\u9a7e\u9a76\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u9a7e\u9a76\u52a8\u4f5c\uff0c\u5e76\u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u786e\u4fdd\u5408\u6cd5\u5408\u89c4\uff0c\u5373\u4f7f\u5728\u9ad8\u5bc6\u5ea6\u4ea4\u901a\u6761\u4ef6\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "conclusion": "SanDRA\u6846\u67b6\u80fd\u591f\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u5b89\u5168\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6240\u6709\u4ee3\u7801\u548c\u5b9e\u9a8c\u8bbe\u7f6e\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.06754", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06754", "abs": "https://arxiv.org/abs/2510.06754", "authors": ["Christian Maurer", "Snehal Jauhri", "Sophie Lueth", "Georgia Chalvatzaki"], "title": "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene", "comment": "Project website: https://sites.google.com/view/uniffield", "summary": "Comprehensive visual, geometric, and semantic understanding of a 3D scene is\ncrucial for successful execution of robotic tasks, especially in unstructured\nand complex environments. Additionally, to make robust decisions, it is\nnecessary for the robot to evaluate the reliability of perceived information.\nWhile recent advances in 3D neural feature fields have enabled robots to\nleverage features from pretrained foundation models for tasks such as\nlanguage-guided manipulation and navigation, existing methods suffer from two\ncritical limitations: (i) they are typically scene-specific, and (ii) they lack\nthe ability to model uncertainty in their predictions. We present UniFField, a\nunified uncertainty-aware neural feature field that combines visual, semantic,\nand geometric features in a single generalizable representation while also\npredicting uncertainty in each modality. Our approach, which can be applied\nzero shot to any new environment, incrementally integrates RGB-D images into\nour voxel-based feature representation as the robot explores the scene,\nsimultaneously updating uncertainty estimation. We evaluate our uncertainty\nestimations to accurately describe the model prediction errors in scene\nreconstruction and semantic feature prediction. Furthermore, we successfully\nleverage our feature predictions and their respective uncertainty for an active\nobject search task using a mobile manipulator robot, demonstrating the\ncapability for robust decision-making.", "AI": {"tldr": "UniFField\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u795e\u7ecf\u7279\u5f81\u573a\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u540c\u65f6\u9884\u6d4b\u6bcf\u4e2a\u6a21\u6001\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u96f6\u6837\u672c\u5e94\u7528\u4e8e\u65b0\u73af\u5883\u3002", "motivation": "\u673a\u5668\u4eba\u9700\u8981\u5168\u9762\u7406\u89e33D\u573a\u666f\u5e76\u8bc4\u4f30\u611f\u77e5\u4fe1\u606f\u7684\u53ef\u9760\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u573a\u666f\u7279\u5b9a\u6027\u548c\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u901a\u7528\u8868\u793a\uff0c\u589e\u91cf\u6574\u5408RGB-D\u56fe\u50cf\uff0c\u540c\u65f6\u66f4\u65b0\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u53ef\u96f6\u6837\u672c\u5e94\u7528\u4e8e\u65b0\u73af\u5883\u3002", "result": "\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u51c6\u786e\u63cf\u8ff0\u573a\u666f\u91cd\u5efa\u548c\u8bed\u4e49\u7279\u5f81\u9884\u6d4b\u4e2d\u7684\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u4e3b\u52a8\u7269\u4f53\u641c\u7d22\u4efb\u52a1\u4e2d\u6210\u529f\u5e94\u7528\u3002", "conclusion": "UniFField\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u51b3\u7b56\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u590d\u6742\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2510.06836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06836", "abs": "https://arxiv.org/abs/2510.06836", "authors": ["Jes\u00fas Bautista", "H\u00e9ctor Garc\u00eda de Marina"], "title": "Distributed 3D Source Seeking via SO(3) Geometric Control of Robot Swarms", "comment": "7 pages, 3 figures. Submitted for presentation at the IFAC World\n  Congress 2026", "summary": "This paper presents a geometric control framework on the Lie group SO(3) for\n3D source-seeking by robots with first-order attitude dynamics and constant\ntranslational speed. By working directly on SO(3), the approach avoids\nEuler-angle singularities and quaternion ambiguities, providing a unique,\nintrinsic representation of orientation. We design a proportional feed-forward\ncontroller that ensures exponential alignment of each agent to an estimated\nascending direction toward a 3D scalar field source. The controller adapts to\nbounded unknown variations and preserves well-posed swarm formations. Numerical\nsimulations demonstrate the effectiveness of the method, with all code provided\nopen source for reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728SO(3)\u674e\u7fa4\u4e0a\u7684\u51e0\u4f55\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5177\u6709\u4e00\u9636\u59ff\u6001\u52a8\u529b\u5b66\u548c\u6052\u5b9a\u5e73\u79fb\u901f\u5ea6\u7684\u673a\u5668\u4eba\u8fdb\u884c3D\u6e90\u8ffd\u8e2a\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u6b27\u62c9\u89d2\u5947\u5f02\u6027\u548c\u56db\u5143\u6570\u6b67\u4e49\u6027\uff0c\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u5185\u5728\u65b9\u5411\u8868\u793a\u3002", "motivation": "\u4f20\u7edf\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u5982\u6b27\u62c9\u89d2\u548c\u56db\u5143\u6570\u5b58\u5728\u5947\u5f02\u6027\u548c\u6b67\u4e49\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u5728SO(3)\u674e\u7fa4\u4e0a\u7684\u5185\u5728\u51e0\u4f55\u63a7\u5236\u6846\u67b6\u6765\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u7a33\u5b9a\u76843D\u6e90\u8ffd\u8e2a\u3002", "method": "\u8bbe\u8ba1\u4e86\u6bd4\u4f8b\u524d\u9988\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e0e\u4f30\u8ba1\u7684\u671d\u54113D\u6807\u91cf\u573a\u6e90\u7684\u4e0a\u5347\u65b9\u5411\u6307\u6570\u5bf9\u9f50\u3002\u63a7\u5236\u5668\u9002\u5e94\u6709\u754c\u672a\u77e5\u53d8\u5316\u5e76\u4fdd\u6301\u826f\u597d\u5f62\u6210\u7684\u7fa4\u4f53\u7f16\u961f\u3002", "result": "\u6570\u503c\u4eff\u771f\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6240\u6709\u4ee3\u7801\u5f00\u6e90\u63d0\u4f9b\u4ee5\u786e\u4fdd\u53ef\u91cd\u73b0\u6027\u3002", "conclusion": "\u5728SO(3)\u674e\u7fa4\u4e0a\u7684\u51e0\u4f55\u63a7\u5236\u6846\u67b6\u4e3a3D\u6e90\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u65e0\u5947\u5f02\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u59ff\u6001\u63a7\u5236\u95ee\u9898\u5e76\u4fdd\u6301\u7fa4\u4f53\u7f16\u961f\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.07027", "categories": ["cs.RO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2510.07027", "abs": "https://arxiv.org/abs/2510.07027", "authors": ["Saravana Prashanth Murali Babu", "Aida Parvaresh", "Ahmad Rafsanjani"], "title": "Tailoring materials into kirigami robots", "comment": null, "summary": "Kirigami, the traditional paper-cutting craft, holds immense potential for\nrevolutionizing robotics by providing multifunctional, lightweight, and\nadaptable solutions. Kirigami structures, characterized by their\nbending-dominated deformation, offer resilience to tensile forces and\nfacilitate shape morphing under small actuation forces. Kirigami components\nsuch as actuators, sensors, batteries, controllers, and body structures can be\ntailored to specific robotic applications by optimizing cut patterns. Actuators\nbased on kirigami principles exhibit complex motions programmable through\nvarious energy sources, while kirigami sensors bridge the gap between\nelectrical conductivity and compliance. Kirigami-integrated batteries enable\nenergy storage directly within robot structures, enhancing flexibility and\ncompactness. Kirigami-controlled mechanisms mimic mechanical computations,\nenabling advanced functionalities such as shape morphing and memory functions.\nApplications of kirigami-enabled robots include grasping, locomotion, and\nwearables, showcasing their adaptability to diverse environments and tasks.\nDespite promising opportunities, challenges remain in the design of cut\npatterns for a given function and streamlining fabrication techniques.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u526a\u7eb8\u6280\u672f\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5305\u62ec\u57fa\u4e8e\u526a\u7eb8\u539f\u7406\u7684\u9a71\u52a8\u5668\u3001\u4f20\u611f\u5668\u3001\u7535\u6c60\u3001\u63a7\u5236\u5668\u548c\u7ed3\u6784\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6293\u53d6\u3001\u79fb\u52a8\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u7b49\u9886\u57df\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u526a\u7eb8\u6280\u672f\u5177\u6709\u591a\u529f\u80fd\u3001\u8f7b\u91cf\u548c\u53ef\u9002\u5e94\u7684\u7279\u70b9\uff0c\u6709\u671b\u9769\u65b0\u673a\u5668\u4eba\u6280\u672f\uff0c\u901a\u8fc7\u4f18\u5316\u5207\u5272\u56fe\u6848\u4e3a\u7279\u5b9a\u673a\u5668\u4eba\u5e94\u7528\u5b9a\u5236\u7ec4\u4ef6\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u526a\u7eb8\u56fe\u6848\u8bbe\u8ba1\u9a71\u52a8\u5668\u3001\u4f20\u611f\u5668\u3001\u7535\u6c60\u548c\u63a7\u5236\u5668\uff0c\u5229\u7528\u526a\u7eb8\u7ed3\u6784\u7684\u5f2f\u66f2\u4e3b\u5bfc\u53d8\u5f62\u7279\u6027\u5b9e\u73b0\u590d\u6742\u8fd0\u52a8\u3001\u4f20\u611f\u548c\u80fd\u91cf\u5b58\u50a8\u529f\u80fd\u3002", "result": "\u526a\u7eb8\u9a71\u52a8\u5668\u53ef\u7f16\u7a0b\u590d\u6742\u8fd0\u52a8\uff0c\u4f20\u611f\u5668\u5b9e\u73b0\u5bfc\u7535\u6027\u4e0e\u67d4\u987a\u6027\u7684\u7ed3\u5408\uff0c\u96c6\u6210\u7535\u6c60\u589e\u5f3a\u7ed3\u6784\u7075\u6d3b\u6027\uff0c\u63a7\u5236\u673a\u5236\u6a21\u62df\u673a\u68b0\u8ba1\u7b97\uff0c\u5728\u6293\u53d6\u3001\u79fb\u52a8\u548c\u53ef\u7a7f\u6234\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u9002\u5e94\u6027\u3002", "conclusion": "\u526a\u7eb8\u6280\u672f\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728\u56fe\u6848\u8bbe\u8ba1\u548c\u5236\u9020\u5de5\u827a\u4f18\u5316\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2510.07028", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07028", "abs": "https://arxiv.org/abs/2510.07028", "authors": ["Sicong Pan", "Xuying Huang", "Maren Bennewitz"], "title": "Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction", "comment": "Accepted to the Active Perception Workshop at IROS 2025", "summary": "Periodic 3D reconstruction is essential for crop monitoring, but costly when\neach cycle restarts from scratch, wasting resources and ignoring information\nfrom previous captures. We propose temporal-prior-guided view planning for\nperiodic plant reconstruction, in which a previously reconstructed model of the\nsame plant is non-rigidly aligned to a new partial observation to form an\napproximation of the current geometry. To accommodate plant growth, we inflate\nthis approximation and solve a set covering optimization problem to compute a\nminimal set of views. We integrated this method into a complete pipeline that\nacquires one additional next-best view before registration for robustness and\nthen plans a globally shortest path to connect the planned set of views and\noutputs the best view sequence. Experiments on maize and tomato under\nhemisphere and sphere view spaces show that our system maintains or improves\nsurface coverage while requiring fewer views and comparable movement cost\ncompared to state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u5e8f\u5148\u9a8c\u5f15\u5bfc\u7684\u89c6\u70b9\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u5468\u671f\u6027\u690d\u72693D\u91cd\u5efa\uff0c\u5229\u7528\u5148\u524d\u91cd\u5efa\u6a21\u578b\u6307\u5bfc\u65b0\u5468\u671f\u7684\u9ad8\u6548\u6570\u636e\u91c7\u96c6\u3002", "motivation": "\u5468\u671f\u6027\u690d\u72693D\u91cd\u5efa\u6bcf\u6b21\u4ece\u5934\u5f00\u59cb\u6210\u672c\u9ad8\u6602\uff0c\u6d6a\u8d39\u8d44\u6e90\u4e14\u5ffd\u7565\u4e86\u5148\u524d\u6355\u83b7\u7684\u4fe1\u606f\u3002", "method": "\u5c06\u5148\u524d\u91cd\u5efa\u6a21\u578b\u975e\u521a\u6027\u5bf9\u9f50\u5230\u65b0\u7684\u90e8\u5206\u89c2\u6d4b\uff0c\u5f62\u6210\u5f53\u524d\u51e0\u4f55\u8fd1\u4f3c\uff1b\u4e3a\u9002\u5e94\u690d\u7269\u751f\u957f\u81a8\u80c0\u8be5\u8fd1\u4f3c\uff1b\u901a\u8fc7\u96c6\u5408\u8986\u76d6\u4f18\u5316\u8ba1\u7b97\u6700\u5c0f\u89c6\u70b9\u96c6\uff1b\u96c6\u6210\u5b8c\u6574\u6d41\u7a0b\uff0c\u5305\u542b\u989d\u5916\u6700\u4f18\u89c6\u70b9\u91c7\u96c6\u548c\u5168\u5c40\u6700\u77ed\u8def\u5f84\u89c4\u5212\u3002", "result": "\u5728\u7389\u7c73\u548c\u756a\u8304\u4e0a\u7684\u534a\u7403\u548c\u7403\u9762\u89c6\u70b9\u7a7a\u95f4\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u8868\u9762\u8986\u76d6\u7387\u7684\u540c\u65f6\uff0c\u9700\u8981\u66f4\u5c11\u7684\u89c6\u70b9\u548c\u53ef\u6bd4\u8f83\u7684\u79fb\u52a8\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u65f6\u5e8f\u4fe1\u606f\uff0c\u63d0\u9ad8\u5468\u671f\u6027\u690d\u7269\u91cd\u5efa\u7684\u6548\u7387\u3002"}}
{"id": "2510.07030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07030", "abs": "https://arxiv.org/abs/2510.07030", "authors": ["Abhinav Kumar", "Fan Yang", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "Diffusing Trajectory Optimization Problems for Recovery During Multi-Finger Manipulation", "comment": null, "summary": "Multi-fingered hands are emerging as powerful platforms for performing fine\nmanipulation tasks, including tool use. However, environmental perturbations or\nexecution errors can impede task performance, motivating the use of recovery\nbehaviors that enable normal task execution to resume. In this work, we take\nadvantage of recent advances in diffusion models to construct a framework that\nautonomously identifies when recovery is necessary and optimizes contact-rich\ntrajectories to recover. We use a diffusion model trained on the task to\nestimate when states are not conducive to task execution, framed as an\nout-of-distribution detection problem. We then use diffusion sampling to\nproject these states in-distribution and use trajectory optimization to plan\ncontact-rich recovery trajectories. We also propose a novel diffusion-based\napproach that distills this process to efficiently diffuse the full\nparameterization, including constraints, goal state, and initialization, of the\nrecovery trajectory optimization problem, saving time during online execution.\nWe compare our method to a reinforcement learning baseline and other methods\nthat do not explicitly plan contact interactions, including on a hardware\nscrewdriver-turning task where we show that recovering using our method\nimproves task performance by 96% and that ours is the only method evaluated\nthat can attempt recovery without causing catastrophic task failure. Videos can\nbe found at https://dtourrecovery.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6307\u624b\u6062\u590d\u6846\u67b6\uff0c\u80fd\u81ea\u52a8\u68c0\u6d4b\u9700\u8981\u6062\u590d\u7684\u72b6\u6001\u5e76\u4f18\u5316\u63a5\u89e6\u4e30\u5bcc\u7684\u6062\u590d\u8f68\u8ff9\uff0c\u5728\u87ba\u4e1d\u5200\u8f6c\u52a8\u4efb\u52a1\u4e2d\u63d0\u534796%\u6027\u80fd\u3002", "motivation": "\u591a\u6307\u624b\u5728\u6267\u884c\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u65f6\uff0c\u73af\u5883\u6270\u52a8\u6216\u6267\u884c\u9519\u8bef\u4f1a\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\uff0c\u9700\u8981\u6062\u590d\u884c\u4e3a\u6765\u6062\u590d\u6b63\u5e38\u4efb\u52a1\u6267\u884c\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u68c0\u6d4b\u4e0d\u9002\u5408\u4efb\u52a1\u6267\u884c\u7684\u72b6\u6001\uff08\u4f5c\u4e3a\u5206\u5e03\u5916\u68c0\u6d4b\u95ee\u9898\uff09\uff0c\u901a\u8fc7\u6269\u6563\u91c7\u6837\u5c06\u72b6\u6001\u6295\u5f71\u56de\u5206\u5e03\u5185\uff0c\u5e76\u7528\u8f68\u8ff9\u4f18\u5316\u89c4\u5212\u63a5\u89e6\u4e30\u5bcc\u7684\u6062\u590d\u8f68\u8ff9\u3002\u8fd8\u63d0\u51fa\u65b0\u65b9\u6cd5\u5c06\u6574\u4e2a\u8fc7\u7a0b\u84b8\u998f\u4ee5\u9ad8\u6548\u6269\u6563\u6062\u590d\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u7684\u5b8c\u6574\u53c2\u6570\u5316\u3002", "result": "\u4e0e\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u548c\u5176\u4ed6\u4e0d\u663e\u5f0f\u89c4\u5212\u63a5\u89e6\u4ea4\u4e92\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u786c\u4ef6\u87ba\u4e1d\u5200\u8f6c\u52a8\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u6062\u590d\u4f7f\u4efb\u52a1\u6027\u80fd\u63d0\u534796%\uff0c\u4e14\u662f\u552f\u4e00\u80fd\u5728\u4e0d\u5bfc\u81f4\u707e\u96be\u6027\u4efb\u52a1\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u5c1d\u8bd5\u6062\u590d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6062\u590d\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u591a\u6307\u624b\u64cd\u4f5c\u4e2d\u7684\u6270\u52a8\u548c\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u6062\u590d\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07067", "abs": "https://arxiv.org/abs/2510.07067", "authors": ["Daria Pugacheva", "Andrey Moskalenko", "Denis Shepelev", "Andrey Kuznetsov", "Vlad Shakhuro", "Elena Tutubalina"], "title": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models", "comment": null, "summary": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling\nrobots to interpret and execute language instructions. However, their\nrobustness to natural language variability in real-world scenarios has not been\nthoroughly investigated. In this work, we present a novel systematic study of\nthe robustness of state-of-the-art VLA models under linguistic perturbations.\nSpecifically, we evaluate model performance under two types of instruction\nnoise: (1) human-generated paraphrasing and (2) the addition of irrelevant\ncontext. We further categorize irrelevant contexts into two groups according to\ntheir length and their semantic and lexical proximity to robot commands. In\nthis study, we observe consistent performance degradation as context size\nexpands. We also demonstrate that the model can exhibit relative robustness to\nrandom context, with a performance drop within 10%, while semantically and\nlexically similar context of the same length can trigger a quality decline of\naround 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To\nmitigate this, we propose an LLM-based filtering framework that extracts core\ncommands from noisy inputs. Incorporating our filtering step allows models to\nrecover up to 98.5% of their original performance under noisy conditions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c(VLA)\u6a21\u578b\u5728\u8bed\u8a00\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u8bed\u4e49\u548c\u8bcd\u6c47\u76f8\u4f3c\u7684\u65e0\u5173\u4e0a\u4e0b\u6587\u654f\u611f\uff0c\u6027\u80fd\u4e0b\u964d\u53ef\u8fbe50%\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u8fc7\u6ee4\u6846\u67b6\u6765\u6062\u590d\u6027\u80fd\u3002", "motivation": "VLA\u6a21\u578b\u5728\u5177\u8eabAI\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5bf9\u81ea\u7136\u8bed\u8a00\u53d8\u5316\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u8bed\u8a00\u6270\u52a8\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u8bc4\u4f30VLA\u6a21\u578b\u5728\u4e24\u79cd\u6307\u4ee4\u566a\u58f0\u4e0b\u7684\u6027\u80fd\uff1a(1)\u4eba\u5de5\u751f\u6210\u7684\u91ca\u4e49\uff1b(2)\u6dfb\u52a0\u65e0\u5173\u4e0a\u4e0b\u6587\u3002\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u8fc7\u6ee4\u6846\u67b6\u4ece\u566a\u58f0\u8f93\u5165\u4e2d\u63d0\u53d6\u6838\u5fc3\u6307\u4ee4\u3002", "result": "\u6a21\u578b\u6027\u80fd\u968f\u4e0a\u4e0b\u6587\u6269\u5c55\u800c\u4e0b\u964d\uff1b\u5bf9\u968f\u673a\u4e0a\u4e0b\u6587\u76f8\u5bf9\u9c81\u68d2(\u6027\u80fd\u4e0b\u964d<10%)\uff0c\u4f46\u5bf9\u8bed\u4e49\u8bcd\u6c47\u76f8\u4f3c\u7684\u4e0a\u4e0b\u6587\u654f\u611f(\u6027\u80fd\u4e0b\u964d\u7ea650%)\uff1b\u4eba\u5de5\u91ca\u4e49\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u8fd120%\uff1b\u4f7f\u7528\u8fc7\u6ee4\u6846\u67b6\u53ef\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u6062\u590d98.5%\u7684\u539f\u59cb\u6027\u80fd\u3002", "conclusion": "VLA\u6a21\u578b\u5bf9\u8bed\u8a00\u6270\u52a8\u654f\u611f\uff0c\u7279\u522b\u662f\u8bed\u4e49\u76f8\u5173\u7684\u65e0\u5173\u4e0a\u4e0b\u6587\uff1b\u63d0\u51fa\u7684LLM\u8fc7\u6ee4\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.07077", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07077", "abs": "https://arxiv.org/abs/2510.07077", "authors": ["Kento Kawaharazuka", "Jihoon Oh", "Jun Yamada", "Ingmar Posner", "Yuke Zhu"], "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "comment": "Accepted to IEEE Access, website: https://vla-survey.github.io", "summary": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io .", "AI": {"tldr": "\u672c\u6587\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u5176\u67b6\u6784\u3001\u5b66\u4e60\u65b9\u6cd5\u3001\u673a\u5668\u4eba\u5e73\u53f0\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u65e8\u5728\u4e3a\u673a\u5668\u4eba\u793e\u533a\u63d0\u4f9b\u5b9e\u9645\u5e94\u7528\u6307\u5bfc\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0cVLA\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u6570\u636e\uff0c\u65e8\u5728\u5b66\u4e60\u80fd\u591f\u6cdb\u5316\u5230\u591a\u6837\u5316\u4efb\u52a1\u3001\u5bf9\u8c61\u3001\u4f53\u73b0\u548c\u73af\u5883\u7684\u7b56\u7565\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u90e8\u7f72\u3002", "method": "\u672c\u6587\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684VLA\u7efc\u8ff0\uff0c\u5305\u62ec\u7b56\u7565\u548c\u67b6\u6784\u8f6c\u53d8\u3001\u67b6\u6784\u4e0e\u6784\u5efa\u6a21\u5757\u3001\u6a21\u6001\u7279\u5b9a\u5904\u7406\u6280\u672f\u3001\u5b66\u4e60\u8303\u5f0f\uff0c\u4ee5\u53ca\u673a\u5668\u4eba\u5e73\u53f0\u3001\u6570\u636e\u6536\u96c6\u7b56\u7565\u3001\u6570\u636e\u96c6\u3001\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u548c\u8bc4\u4f30\u57fa\u51c6\u7684\u8be6\u7ec6\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6574\u5408\u8f6f\u4ef6\u548c\u786c\u4ef6\u7ec4\u4ef6\uff0c\u672c\u6587\u4e3aVLA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u6808\u89c6\u89d2\uff0c\u5e76\u6574\u7406\u4e86\u6309\u8bad\u7ec3\u65b9\u6cd5\u3001\u8bc4\u4f30\u65b9\u6cd5\u3001\u6a21\u6001\u548c\u6570\u636e\u96c6\u5206\u7c7b\u7684\u53c2\u8003\u8d44\u6599\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u673a\u5668\u4eba\u793e\u533a\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u4fc3\u8fdbVLA\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u6240\u6709\u53c2\u8003\u8d44\u6599\u53ef\u5728\u9879\u76ee\u7f51\u7ad9\u4e0a\u83b7\u53d6\u3002"}}
{"id": "2510.07094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07094", "abs": "https://arxiv.org/abs/2510.07094", "authors": ["David Rytz", "Kim Tien Ly", "Ioannis Havoutis"], "title": "Sampling Strategies for Robust Universal Quadrupedal Locomotion Policies", "comment": null, "summary": "This work focuses on sampling strategies of configuration variations for\ngenerating robust universal locomotion policies for quadrupedal robots. We\ninvestigate the effects of sampling physical robot parameters and joint\nproportional-derivative gains to enable training a single reinforcement\nlearning policy that generalizes to multiple parameter configurations. Three\nfundamental joint gain sampling strategies are compared: parameter sampling\nwith (1) linear and polynomial function mappings of mass-to-gains, (2)\nperformance-based adaptive filtering, and (3) uniform random sampling. We\nimprove the robustness of the policy by biasing the configurations using\nnominal priors and reference models. All training was conducted on RaiSim,\ntested in simulation on a range of diverse quadrupeds, and zero-shot deployed\nonto hardware using the ANYmal quadruped robot. Compared to multiple baseline\nimplementations, our results demonstrate the need for significant joint\ncontroller gains randomization for robust closing of the sim-to-real gap.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u5173\u8282\u589e\u76ca\u91c7\u6837\u7b56\u7565\uff0c\u7528\u4e8e\u8bad\u7ec3\u80fd\u6cdb\u5316\u5230\u591a\u79cd\u53c2\u6570\u914d\u7f6e\u7684\u901a\u7528\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\uff0c\u901a\u8fc7\u5728RaiSim\u4e2d\u8bad\u7ec3\u5e76\u5728ANYmal\u673a\u5668\u4eba\u4e0a\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u8bc1\u660e\u4e86\u5173\u8282\u63a7\u5236\u5668\u589e\u76ca\u968f\u673a\u5316\u5bf9\u7f29\u5c0f\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u591a\u79cd\u7269\u7406\u53c2\u6570\u914d\u7f6e\u7684\u9c81\u68d2\u901a\u7528\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\uff0c\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u5173\u8282\u589e\u76ca\u91c7\u6837\u7b56\u7565\uff1a(1) \u8d28\u91cf\u548c\u589e\u76ca\u7684\u7ebf\u6027/\u591a\u9879\u5f0f\u51fd\u6570\u6620\u5c04\uff1b(2) \u57fa\u4e8e\u6027\u80fd\u7684\u81ea\u9002\u5e94\u6ee4\u6ce2\uff1b(3) \u5747\u5300\u968f\u673a\u91c7\u6837\u3002\u4f7f\u7528\u540d\u4e49\u5148\u9a8c\u548c\u53c2\u8003\u6a21\u578b\u504f\u7f6e\u914d\u7f6e\uff0c\u5728RaiSim\u4e2d\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u6240\u6709\u7b56\u7565\u5728\u591a\u79cd\u56db\u8db3\u673a\u5668\u4eba\u4eff\u771f\u4e2d\u6d4b\u8bd5\uff0c\u5e76\u5728ANYmal\u673a\u5668\u4eba\u4e0a\u96f6\u6837\u672c\u90e8\u7f72\u3002\u7ed3\u679c\u8868\u660e\uff0c\u663e\u8457\u7684\u5173\u8282\u63a7\u5236\u5668\u589e\u76ca\u968f\u673a\u5316\u5bf9\u4e8e\u9c81\u68d2\u5730\u7f29\u5c0f\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u5173\u8282\u63a7\u5236\u5668\u589e\u76ca\u7684\u663e\u8457\u968f\u673a\u5316\u662f\u5b9e\u73b0\u9c81\u68d2\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7684\u5173\u952e\u56e0\u7d20\uff0c\u76f8\u6bd4\u591a\u4e2a\u57fa\u7ebf\u5b9e\u73b0\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.07133", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07133", "abs": "https://arxiv.org/abs/2510.07133", "authors": ["Tony Zhang", "Burak Kantarci", "Umair Siddique"], "title": "A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model", "comment": null, "summary": "Ensuring the safety of self-driving cars remains a major challenge due to the\ncomplexity and unpredictability of real-world driving environments. Traditional\ntesting methods face significant limitations, such as the oracle problem, which\nmakes it difficult to determine whether a system's behavior is correct, and the\ninability to cover the full range of scenarios an autonomous vehicle may\nencounter. In this paper, we introduce a digital twin-driven metamorphic\ntesting framework that addresses these challenges by creating a virtual replica\nof the self-driving system and its operating environment. By combining digital\ntwin technology with AI-based image generative models such as Stable Diffusion,\nour approach enables the systematic generation of realistic and diverse driving\nscenes. This includes variations in weather, road topology, and environmental\nfeatures, all while maintaining the core semantics of the original scenario.\nThe digital twin provides a synchronized simulation environment where changes\ncan be tested in a controlled and repeatable manner. Within this environment,\nwe define three metamorphic relations inspired by real-world traffic rules and\nvehicle behavior. We validate our framework in the Udacity self-driving\nsimulator and demonstrate that it significantly enhances test coverage and\neffectiveness. Our method achieves the highest true positive rate (0.719), F1\nscore (0.689), and precision (0.662) compared to baseline approaches. This\npaper highlights the value of integrating digital twins with AI-powered\nscenario generation to create a scalable, automated, and high-fidelity testing\nsolution for autonomous vehicle safety.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u8715\u53d8\u6d4b\u8bd5\u6846\u67b6\uff0c\u7ed3\u5408AI\u56fe\u50cf\u751f\u6210\u6280\u672f\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u521b\u5efa\u865a\u62df\u6d4b\u8bd5\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u771f\u5b9e\u9a7e\u9a76\u73af\u5883\u4e2d\u6d4b\u8bd5\u7684\u6311\u6218\uff0c\u5305\u62ec\u9884\u8a00\u673a\u95ee\u9898\u548c\u573a\u666f\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6280\u672f\u548cAI\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\uff0c\u521b\u5efa\u865a\u62df\u9a7e\u9a76\u573a\u666f\u526f\u672c\uff0c\u5b9a\u4e49\u57fa\u4e8e\u4ea4\u901a\u89c4\u5219\u7684\u4e09\u79cd\u8715\u53d8\u5173\u7cfb\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728Udacity\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\uff0c\u83b7\u5f97\u6700\u9ad8\u771f\u9633\u6027\u7387\uff080.719\uff09\u3001F1\u5206\u6570\uff080.689\uff09\u548c\u7cbe\u786e\u7387\uff080.662\uff09\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u4e0eAI\u573a\u666f\u751f\u6210\u76f8\u7ed3\u5408\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u4e14\u9ad8\u4fdd\u771f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07134", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07134", "abs": "https://arxiv.org/abs/2510.07134", "authors": ["Jiahang Liu", "Yunpeng Qi", "Jiazhao Zhang", "Minghan Li", "Shaoan Wang", "Kui Wu", "Hanjing Ye", "Hong Zhang", "Zhibo Chen", "Fangwei Zhong", "Zhizheng Zhang", "He Wang"], "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking", "comment": "Project page: https://pku-epic.github.io/TrackVLA-plus-plus-Web/", "summary": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins\npractical applications, such as companion robots, guidance robots and service\nassistants, where continuously following moving targets is essential. Recent\nadvances have enabled language-guided tracking in complex and unstructured\nscenes. However, existing approaches lack explicit spatial reasoning and\neffective temporal memory, causing failures under severe occlusions or in the\npresence of similar-looking distractors. To address these challenges, we\npresent TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances\nembodied visual tracking with two key modules, a spatial reasoning mechanism\nand a Target Identification Memory (TIM). The reasoning module introduces a\nChain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative\nposition and encodes it as a compact polar-coordinate token for action\nprediction. Guided by these spatial priors, the TIM employs a gated update\nstrategy to preserve long-horizon target memory, ensuring spatiotemporal\nconsistency and mitigating target loss during extended occlusions. Extensive\nexperiments show that TrackVLA++ achieves state-of-the-art performance on\npublic benchmarks across both egocentric and multi-camera settings. On the\nchallenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading\napproach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong\nzero-shot generalization, enabling robust real-world tracking in dynamic and\noccluded scenarios.", "AI": {"tldr": "TrackVLA++\u662f\u4e00\u4e2a\u589e\u5f3a\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u673a\u5236\u548c\u76ee\u6807\u8bc6\u522b\u8bb0\u5fc6\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e25\u91cd\u906e\u6321\u548c\u76f8\u4f3c\u5e72\u6270\u7269\u60c5\u51b5\u4e0b\u7684\u8ddf\u8e2a\u5931\u8d25\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u6709\u6548\u7684\u65f6\u95f4\u8bb0\u5fc6\uff0c\u5bfc\u81f4\u5728\u4e25\u91cd\u906e\u6321\u6216\u5b58\u5728\u76f8\u4f3c\u5e72\u6270\u7269\u65f6\u8ddf\u8e2a\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u7a7a\u95f4\u63a8\u7406\u673a\u5236\uff08Polar-CoT\uff09\u548c\u76ee\u6807\u8bc6\u522b\u8bb0\u5fc6\uff08TIM\uff09\u3002Polar-CoT\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63a8\u65ad\u76ee\u6807\u76f8\u5bf9\u4f4d\u7f6e\u5e76\u7f16\u7801\u4e3a\u6781\u5750\u6807\u4ee4\u724c\uff1bTIM\u91c7\u7528\u95e8\u63a7\u66f4\u65b0\u7b56\u7565\u4fdd\u6301\u957f\u671f\u76ee\u6807\u8bb0\u5fc6\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728EVT-Bench DT\u5206\u5272\u4e0a\u5206\u522b\u8d85\u8fc7\u5148\u524d\u9886\u5148\u65b9\u6cd55.1\u548c12\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TrackVLA++\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u548c\u65f6\u95f4\u8bb0\u5fc6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u52a8\u6001\u548c\u906e\u6321\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2510.07152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07152", "abs": "https://arxiv.org/abs/2510.07152", "authors": ["Jingkai Sun", "Gang Han", "Pihai Sun", "Wen Zhao", "Jiahang Cao", "Jiaxu Wang", "Yijie Guo", "Qiang Zhang"], "title": "DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction", "comment": null, "summary": "Recent advancements in legged robot perceptive locomotion have shown\npromising progress. However, terrain-aware humanoid locomotion remains largely\nconstrained to two paradigms: depth image-based end-to-end learning and\nelevation map-based methods. The former suffers from limited training\nefficiency and a significant sim-to-real gap in depth perception, while the\nlatter depends heavily on multiple vision sensors and localization systems,\nresulting in latency and reduced robustness. To overcome these challenges, we\npropose a novel framework that tightly integrates three key components: (1)\nTerrain-Aware Locomotion Policy with a Blind Backbone, which leverages\npre-trained elevation map-based perception to guide reinforcement learning with\nminimal visual input; (2) Multi-Modality Cross-Attention Transformer, which\nreconstructs structured terrain representations from noisy depth images; (3)\nRealistic Depth Images Synthetic Method, which employs self-occlusion-aware ray\ncasting and noise-aware modeling to synthesize realistic depth observations,\nachieving over 30\\% reduction in terrain reconstruction error. This combination\nenables efficient policy training with limited data and hardware resources,\nwhile preserving critical terrain features essential for generalization. We\nvalidate our framework on a full-sized humanoid robot, demonstrating agile and\nadaptive locomotion across diverse and challenging terrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4eba\u5f62\u673a\u5668\u4eba\u5730\u5f62\u611f\u77e5\u8fd0\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u76f2\u8fd0\u52a8\u7b56\u7565\u3001\u591a\u6a21\u6001\u6ce8\u610f\u529b\u53d8\u6362\u5668\u548c\u771f\u5b9e\u6df1\u5ea6\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8bad\u7ec3\u6548\u7387\u4f4e\u3001sim-to-real\u5dee\u8ddd\u5927\u3001\u5ef6\u8fdf\u9ad8\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5730\u5f62\u611f\u77e5\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u6548\u7387\u4f4e\u4e14sim-to-real\u5dee\u8ddd\u5927\uff1b\u57fa\u4e8e\u9ad8\u7a0b\u56fe\u7684\u65b9\u6cd5\u4f9d\u8d56\u591a\u4f20\u611f\u5668\u548c\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u548c\u9c81\u68d2\u6027\u964d\u4f4e\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) \u5730\u5f62\u611f\u77e5\u8fd0\u52a8\u7b56\u7565\uff08\u76f2\u9aa8\u5e72\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u9ad8\u7a0b\u56fe\u611f\u77e5\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\uff1b(2) \u591a\u6a21\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u53d8\u6362\u5668\uff0c\u4ece\u566a\u58f0\u6df1\u5ea6\u56fe\u50cf\u91cd\u5efa\u7ed3\u6784\u5316\u5730\u5f62\u8868\u793a\uff1b(3) \u771f\u5b9e\u6df1\u5ea6\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u91c7\u7528\u81ea\u906e\u6321\u611f\u77e5\u5149\u7ebf\u6295\u5c04\u548c\u566a\u58f0\u611f\u77e5\u5efa\u6a21\u3002", "result": "\u5b9e\u73b0\u4e86\u8d85\u8fc730%\u7684\u5730\u5f62\u91cd\u5efa\u8bef\u5dee\u51cf\u5c11\uff0c\u5728\u5b8c\u6574\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6837\u6311\u6218\u6027\u5730\u5f62\u4e0a\u7684\u654f\u6377\u81ea\u9002\u5e94\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u548c\u786c\u4ef6\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u7684\u5173\u952e\u5730\u5f62\u7279\u5f81\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5730\u5f62\u611f\u77e5\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07160", "abs": "https://arxiv.org/abs/2510.07160", "authors": ["Fengze Xie", "Xiaozhou Fan", "Jacob Schuster", "Yisong Yue", "Morteza Gharib"], "title": "A Narwhal-Inspired Sensing-to-Control Framework for Small Fixed-Wing Aircraft", "comment": null, "summary": "Fixed-wing unmanned aerial vehicles (UAVs) offer endurance and efficiency but\nlack low-speed agility due to highly coupled dynamics. We present an end-to-end\nsensing-to-control pipeline that combines bio-inspired hardware,\nphysics-informed dynamics learning, and convex control allocation. Measuring\nairflow on a small airframe is difficult because near-body aerodynamics,\npropeller slipstream, control-surface actuation, and ambient gusts distort\npressure signals. Inspired by the narwhal's protruding tusk, we mount in-house\nmulti-hole probes far upstream and complement them with sparse, carefully\nplaced wing pressure sensors for local flow measurement. A data-driven\ncalibration maps probe pressures to airspeed and flow angles. We then learn a\ncontrol-affine dynamics model using the estimated airspeed/angles and sparse\nsensors. A soft left/right symmetry regularizer improves identifiability under\npartial observability and limits confounding between wing pressures and\nflaperon inputs. Desired wrenches (forces and moments) are realized by a\nregularized least-squares allocator that yields smooth, trimmed actuation.\nWind-tunnel studies across a wide operating range show that adding wing\npressures reduces force-estimation error by 25-30%, the proposed model degrades\nless under distribution shift (about 12% versus 44% for an unstructured\nbaseline), and force tracking improves with smoother inputs, including a 27%\nreduction in normal-force RMSE versus a plain affine model and 34% versus an\nunstructured baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7684\u7aef\u5230\u7aef\u611f\u77e5-\u63a7\u5236\u7ba1\u9053\uff0c\u7ed3\u5408\u4eff\u751f\u786c\u4ef6\u3001\u7269\u7406\u4fe1\u606f\u52a8\u529b\u5b66\u5b66\u4e60\u548c\u51f8\u63a7\u5236\u5206\u914d\uff0c\u89e3\u51b3\u4e86\u4f4e\u7a7a\u901f\u654f\u6377\u6027\u95ee\u9898\u3002", "motivation": "\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u5177\u6709\u7eed\u822a\u548c\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u9ad8\u5ea6\u8026\u5408\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u7f3a\u4e4f\u4f4e\u7a7a\u901f\u654f\u6377\u6027\u3002\u6d4b\u91cf\u5c0f\u578b\u673a\u67b6\u4e0a\u7684\u6c14\u6d41\u5f88\u56f0\u96be\uff0c\u56e0\u4e3a\u8fd1\u4f53\u7a7a\u6c14\u52a8\u529b\u5b66\u3001\u87ba\u65cb\u6868\u6ed1\u6d41\u3001\u63a7\u5236\u9762\u9a71\u52a8\u548c\u73af\u5883\u9635\u98ce\u4f1a\u626d\u66f2\u538b\u529b\u4fe1\u53f7\u3002", "method": "1. \u4eff\u751f\u786c\u4ef6\u8bbe\u8ba1\uff1a\u57fa\u4e8e\u72ec\u89d2\u9cb8\u7a81\u51fa\u7684\u957f\u7259\u6982\u5ff5\uff0c\u5728\u4e0a\u6e38\u5b89\u88c5\u81ea\u5236\u591a\u5b54\u63a2\u5934\uff0c\u5e76\u8f85\u4ee5\u7a00\u758f\u5e03\u7f6e\u7684\u673a\u7ffc\u538b\u529b\u4f20\u611f\u5668\u8fdb\u884c\u5c40\u90e8\u6d41\u91cf\u6d4b\u91cf\uff1b2. \u6570\u636e\u9a71\u52a8\u6821\u51c6\uff1a\u5c06\u63a2\u5934\u538b\u529b\u6620\u5c04\u5230\u7a7a\u901f\u548c\u6d41\u52a8\u89d2\u5ea6\uff1b3. \u5b66\u4e60\u63a7\u5236\u4eff\u5c04\u52a8\u529b\u5b66\u6a21\u578b\uff1a\u4f7f\u7528\u4f30\u8ba1\u7684\u7a7a\u901f/\u89d2\u5ea6\u548c\u7a00\u758f\u4f20\u611f\u5668\uff1b4. \u8f6f\u5de6\u53f3\u5bf9\u79f0\u6b63\u5219\u5316\u5668\u63d0\u9ad8\u53ef\u8bc6\u522b\u6027\uff1b5. \u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u5206\u914d\u5668\u5b9e\u73b0\u5e73\u6ed1\u3001\u4fee\u6574\u7684\u9a71\u52a8\u3002", "result": "\u98ce\u6d1e\u7814\u7a76\u8868\u660e\uff1a\u6dfb\u52a0\u673a\u7ffc\u538b\u529b\u53ef\u4f7f\u529b\u4f30\u8ba1\u8bef\u5dee\u51cf\u5c1125-30%\uff1b\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\u8f83\u5c11\uff08\u7ea612%\uff0c\u800c\u672a\u7ecf\u7ed3\u6784\u5316\u7684\u57fa\u7ebf\u4e3a44%\uff09\uff1b\u529b\u8ddf\u8e2a\u901a\u8fc7\u66f4\u5e73\u6ed1\u7684\u8f93\u5165\u5f97\u5230\u6539\u5584\uff0c\u5305\u62ec\u4e0e\u666e\u901a\u4eff\u5c04\u6a21\u578b\u76f8\u6bd4\u6cd5\u5411\u529bRMSE\u51cf\u5c1127%\uff0c\u4e0e\u672a\u7ecf\u7ed3\u6784\u5316\u7684\u57fa\u7ebf\u76f8\u6bd4\u51cf\u5c1134%\u3002", "conclusion": "\u8be5\u7aef\u5230\u7aef\u65b9\u6cd5\u901a\u8fc7\u4eff\u751f\u4f20\u611f\u3001\u7269\u7406\u4fe1\u606f\u5efa\u6a21\u548c\u51f8\u4f18\u5316\u63a7\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u5728\u4f4e\u7a7a\u901f\u4e0b\u7684\u654f\u6377\u6027\u548c\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2510.07181", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07181", "abs": "https://arxiv.org/abs/2510.07181", "authors": ["Yi Han", "Cheng Chi", "Enshen Zhou", "Shanyu Rong", "Jingkun An", "Pengwei Wang", "Zhongyuan Wang", "Lu Sheng", "Shanghang Zhang"], "title": "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics", "comment": "9 pages, 6 figures", "summary": "Vision-Language Models (VLMs) have shown remarkable capabilities in spatial\nreasoning, yet they remain fundamentally limited to qualitative precision and\nlack the computational precision required for real-world robotics. Current\napproaches fail to leverage metric cues from depth sensors and camera\ncalibration, instead reducing geometric problems to pattern recognition tasks\nthat cannot deliver the centimeter-level accuracy essential for robotic\nmanipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel\nframework that transforms VLMs from perceptual estimators to geometric\ncomputers by enabling them to generate and execute precise geometric\ncomputations through external tools. Rather than attempting to internalize\ncomplex geometric operations within neural networks, TIGeR empowers models to\nrecognize geometric reasoning requirements, synthesize appropriate\ncomputational code, and invoke specialized libraries for exact calculations. To\nsupport this paradigm, we introduce TIGeR-300K, a comprehensive\ntool-invocation-oriented dataset covering point transformations, pose\nestimation, trajectory generation, and spatial compatibility verification,\ncomplete with tool invocation sequences and intermediate computations. Through\na two-stage training pipeline combining supervised fine-tuning (SFT) and\nreinforcement fine-tuning (RFT) with our proposed hierarchical reward design,\nTIGeR achieves SOTA performance on geometric reasoning benchmarks while\ndemonstrating centimeter-level precision in real-world robotic manipulation\ntasks.", "AI": {"tldr": "TIGeR\u662f\u4e00\u4e2a\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u611f\u77e5\u4f30\u8ba1\u5668\u8f6c\u53d8\u4e3a\u51e0\u4f55\u8ba1\u7b97\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u5b9e\u73b0\u7cbe\u786e\u51e0\u4f55\u8ba1\u7b97\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8fbe\u5230\u5398\u7c73\u7ea7\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ec5\u9650\u4e8e\u5b9a\u6027\u7cbe\u5ea6\uff0c\u7f3a\u4e4f\u673a\u5668\u4eba\u5e94\u7528\u6240\u9700\u7684\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u65e0\u6cd5\u5229\u7528\u6df1\u5ea6\u4f20\u611f\u5668\u548c\u76f8\u673a\u6807\u5b9a\u7684\u5ea6\u91cf\u7ebf\u7d22\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\uff0c\u7ed3\u5408\u5206\u5c42\u5956\u52b1\u8bbe\u8ba1\uff0c\u8ba9\u6a21\u578b\u8bc6\u522b\u51e0\u4f55\u63a8\u7406\u9700\u6c42\u3001\u751f\u6210\u8ba1\u7b97\u4ee3\u7801\u5e76\u8c03\u7528\u4e13\u4e1a\u5e93\u8fdb\u884c\u7cbe\u786e\u8ba1\u7b97\u3002", "result": "\u5728\u51e0\u4f55\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u5398\u7c73\u7ea7\u7cbe\u5ea6\u3002", "conclusion": "TIGeR\u6846\u67b6\u6210\u529f\u5c06VLMs\u8f6c\u53d8\u4e3a\u51e0\u4f55\u8ba1\u7b97\u673a\uff0c\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u6240\u9700\u7684\u7cbe\u786e\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.07197", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07197", "abs": "https://arxiv.org/abs/2510.07197", "authors": ["Aman Singh", "Deepak Kapa", "Suryank Joshi", "Shishir Kolathaya"], "title": "COMPAct: Computational Optimization and Automated Modular design of Planetary Actuators", "comment": "8 pages, 9 Figures, 2 tables, first two authors contributed equally", "summary": "The optimal design of robotic actuators is a critical area of research, yet\nlimited attention has been given to optimizing gearbox parameters and\nautomating actuator CAD. This paper introduces COMPAct: Computational\nOptimization and Automated Modular Design of Planetary Actuators, a framework\nthat systematically identifies optimal gearbox parameters for a given motor\nacross four gearbox types, single-stage planetary gearbox (SSPG), compound\nplanetary gearbox (CPG), Wolfrom planetary gearbox (WPG), and double-stage\nplanetary gearbox (DSPG). The framework minimizes mass and actuator width while\nmaximizing efficiency, and further automates actuator CAD generation to enable\ndirect 3D printing without manual redesign. Using this framework, optimal\ngearbox designs are explored over a wide range of gear ratios, providing\ninsights into the suitability of different gearbox types across various gear\nratio ranges. In addition, the framework is used to generate CAD models of all\nfour gearbox types with varying gear ratios and motors. Two actuator types are\nfabricated and experimentally evaluated through power efficiency, no-load\nbacklash, and transmission stiffness tests. Experimental results indicate that\nthe SSPG actuator achieves a mechanical efficiency of 60-80 %, a no-load\nbacklash of 0.59 deg, and a transmission stiffness of 242.7 Nm/rad, while the\nCPG actuator demonstrates 60 % efficiency, 2.6 deg backlash, and a stiffness of\n201.6 Nm/rad. Code available at:\nhttps://anonymous.4open.science/r/COMPAct-SubNum-3408 Video:\nhttps://youtu.be/99zOKgxsDho", "AI": {"tldr": "COMPAct\u6846\u67b6\u901a\u8fc7\u8ba1\u7b97\u4f18\u5316\u548c\u81ea\u52a8\u5316\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u7cfb\u7edf\u5730\u4e3a\u56db\u79cd\u884c\u661f\u9f7f\u8f6e\u7bb1\u7c7b\u578b\u4f18\u5316\u53c2\u6570\uff0c\u6700\u5c0f\u5316\u8d28\u91cf\u548c\u5bbd\u5ea6\u540c\u65f6\u6700\u5927\u5316\u6548\u7387\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u53ef\u76f4\u63a53D\u6253\u5370\u7684CAD\u6a21\u578b\u3002", "motivation": "\u673a\u5668\u4eba\u6267\u884c\u5668\u8bbe\u8ba1\u4e2d\uff0c\u9f7f\u8f6e\u7bb1\u53c2\u6570\u4f18\u5316\u548cCAD\u81ea\u52a8\u5316\u8bbe\u8ba1\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u4f18\u5316\u65b9\u6cd5\u548c\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u5347\u8bbe\u8ba1\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u5f00\u53d1COMPAct\u6846\u67b6\uff0c\u5bf9\u56db\u79cd\u884c\u661f\u9f7f\u8f6e\u7bb1\u7c7b\u578b\uff08SSPG\u3001CPG\u3001WPG\u3001DSPG\uff09\u8fdb\u884c\u53c2\u6570\u4f18\u5316\uff0c\u6700\u5c0f\u5316\u8d28\u91cf\u548c\u5bbd\u5ea6\uff0c\u6700\u5927\u5316\u6548\u7387\uff0c\u5e76\u81ea\u52a8\u5316\u751f\u6210CAD\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSSPG\u6267\u884c\u5668\u673a\u68b0\u6548\u738760-80%\uff0c\u7a7a\u8f7d\u56de\u5dee0.59\u5ea6\uff0c\u4f20\u52a8\u521a\u5ea6242.7 Nm/rad\uff1bCPG\u6267\u884c\u5668\u6548\u738760%\uff0c\u56de\u5dee2.6\u5ea6\uff0c\u521a\u5ea6201.6 Nm/rad\u3002", "conclusion": "COMPAct\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9f7f\u8f6e\u7bb1\u53c2\u6570\u7684\u4f18\u5316\u8bbe\u8ba1\u548cCAD\u81ea\u52a8\u5316\u751f\u6210\uff0c\u4e3a\u4e0d\u540c\u4f20\u52a8\u6bd4\u8303\u56f4\u63d0\u4f9b\u4e86\u5408\u9002\u7684\u9f7f\u8f6e\u7bb1\u7c7b\u578b\u9009\u62e9\u6307\u5bfc\u3002"}}
{"id": "2510.07210", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07210", "abs": "https://arxiv.org/abs/2510.07210", "authors": ["Donald Pfaffmann", "Matthias Klusch", "Marcel Steinmetz"], "title": "HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving", "comment": null, "summary": "We present a novel hybrid learning-assisted planning method, named HyPlan,\nfor solving the collision-free navigation problem for self-driving cars in\npartially observable traffic environments. HyPlan combines methods for\nmulti-agent behavior prediction, deep reinforcement learning with proximal\npolicy optimization and approximated online POMDP planning with heuristic\nconfidence-based vertical pruning to reduce its execution time without\ncompromising safety of driving. Our experimental performance analysis on the\nCARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed\nthat HyPlan may navigate safer than selected relevant baselines and perform\nsignificantly faster than considered alternative online POMDP planners.", "AI": {"tldr": "HyPlan\u662f\u4e00\u79cd\u6df7\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u9884\u6d4b\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebfPOMDP\u89c4\u5212\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u65e0\u78b0\u649e\u5bfc\u822a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u4ea4\u901a\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6709\u884c\u4eba\u7684\u5173\u952e\u4ea4\u901a\u573a\u666f\u4e0b\u3002", "method": "\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u9884\u6d4b\u3001\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u53ca\u57fa\u4e8e\u542f\u53d1\u5f0f\u7f6e\u4fe1\u5ea6\u5782\u76f4\u526a\u679d\u7684\u8fd1\u4f3c\u5728\u7ebfPOMDP\u89c4\u5212\u3002", "result": "\u5728CARLA-CTS2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHyPlan\u6bd4\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u5bfc\u822a\u66f4\u5b89\u5168\uff0c\u4e14\u6bd4\u66ff\u4ee3\u5728\u7ebfPOMDP\u89c4\u5212\u5668\u6267\u884c\u901f\u5ea6\u663e\u8457\u66f4\u5feb\u3002", "conclusion": "HyPlan\u65b9\u6cd5\u5728\u4e0d\u5f71\u54cd\u9a7e\u9a76\u5b89\u5168\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u5782\u76f4\u526a\u679d\u6280\u672f\u6709\u6548\u51cf\u5c11\u4e86\u6267\u884c\u65f6\u95f4\uff0c\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u9a7e\u9a76\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
