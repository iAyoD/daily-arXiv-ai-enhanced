<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: FLEET是一个混合分散式框架，将自然语言指令转化为优化的多机器人调度，结合LLM前端和形式化后端来解决异构机器人团队的协调问题。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人团队从自然语言指令进行协调的挑战，语言规划器难以处理长期协调和幻觉问题，而纯形式化方法需要封闭世界模型。

Method: 使用LLM前端生成任务图和机器人-任务适应度矩阵，形式化后端解决最小化完工时间问题，机器人执行具有自主闭环控制的子任务。

Result: 在多个语言引导的自主协调基准测试中，FLEET在异构任务的两智能体团队上比最先进的生成规划器提高了成功率。

Conclusion: 混合整数线性规划主要改善时间结构，LLM衍生的适应度对能力耦合任务至关重要，两者结合提供最高整体性能，并在真实硬件试验中得到验证。

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [2] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: 提出基于门控循环单元的轻量级编码器-解码器模型，用于在信息稀缺条件下预测高性能车辆的未来状态，最大平均相对误差低于2.6%。


<details>
  <summary>Details</summary>
Motivation: 高性能车辆的动态建模需要详细的系统结构信息，但这些信息通常对非设计者不可得，这是自动驾驶应用中的典型问题。

Method: 使用基于门控循环单元的编码器-解码器模型，通过车辆历史状态和驾驶员控制动作来关联未来状态。

Result: 在极端动态条件下最大平均相对误差低于2.6%，对噪声输入具有良好的鲁棒性，输出信号具有物理一致性。

Conclusion: 该数据驱动模型在信息稀缺条件下能够准确预测车辆动态，且无需物理约束即可保持物理一致性。

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [3] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: HJCD-IK是一种GPU加速的逆运动学求解器，结合了方向感知的贪婪坐标下降初始化方案和基于雅可比矩阵的优化程序，在精度和速度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统解析求解器受限于低自由度和特定拓扑结构，而数值优化方法计算成本高且容易陷入局部最优。需要结合采样和优化来提升逆运动学求解的精度和速度。

Method: 提出HJCD-IK方法，使用GPU加速的采样方法，结合方向感知的贪婪坐标下降初始化方案和雅可比矩阵优化程序。

Result: 相比现有最优方法，HJCD-IK在收敛速度和整体精度方面都有提升，在精度-延迟帕累托前沿上始终找到解决方案，通常实现数量级增益。

Conclusion: HJCD-IK是一个高效的GPU加速逆运动学求解器，能够产生高质量样本分布，代码已开源。

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [4] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了摊销价值优化(AVO)方法，通过引入学习价值函数来预测未来任务性能，指导轨迹优化器朝向有利于后续子任务的状态，从而解决灵巧操作任务中独立优化子任务导致的性能限制和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作任务需要在不同接触模式间切换，传统方法将任务分解为独立优化的子任务存在两个问题：1) 缺乏未来子任务信息可能导致系统陷入难以继续后续任务的状态；2) 优化过程计算成本高昂。

Method: 引入学习价值函数预测总未来任务性能，将该价值函数纳入轨迹优化的成本函数中，通过价值函数梯度指导优化器朝向最小化未来子任务成本的状态。

Result: 在螺丝刀抓取和转动任务的仿真和真实实验中验证了AVO的有效性，即使计算预算减少50%，性能仍优于没有价值函数的轨迹优化方法。

Conclusion: AVO方法能够桥接独立优化的子任务，减少在线计算需求，在灵巧操作任务中表现出优越性能。

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [5] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: 提出了IPIM原语计算，使基于采样的检测规划器能够完全使用神经SDF表示，在保持检测轨迹质量的同时显著减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 基础设施老化复杂化使得高效检测规划愈发重要。现有基于采样的检测规划器速度快但内存消耗大，特别是对于大型复杂结构。虽然隐式模型（如神经SDF）能高效表示此类结构，但现有规划器原语计算主要针对显式环境模型设计。

Method: 提出了一组名为IPIM的原语计算，使基于采样的检测规划器能够在整个规划过程中完全使用神经SDF表示，无需在隐式和显式环境模型之间频繁转换。

Result: 在三个场景（包括具有9200万个三角网格面的复杂真实结构）的评估表明，即使使用基本的基于采样规划器配合IPIM，也能生成与最先进规划器质量相当的检测轨迹，同时内存使用量减少高达70倍。

Conclusion: IPIM原语计算成功解决了基于采样检测规划器在隐式模型环境中的内存效率问题，为大型复杂基础设施的高效检测规划提供了可行方案。

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [6] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: GATO是一个开源的GPU加速批量轨迹优化求解器，专门为中等批量规模（数十到数百个求解）的实时MPC应用设计，相比CPU基线实现18-21倍加速，相比GPU基线实现1.4-16倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速方法要么并行化单个求解以满足实时截止时间，要么以低于实时速率扩展到非常大的批量，或者通过限制模型通用性来实现速度。这为需要实时批量求解的现代MPC应用留下了性能差距。

Method: GATO采用算法、软件和计算硬件的协同设计，利用块级、warp级和线程级并行性在求解内部和求解之间实现超高性能。

Result: 模拟基准测试显示，随着批量大小增加，相比CPU基线实现18-21倍加速，相比GPU基线实现1.4-16倍加速。案例研究展示了改进的扰动抑制和收敛行为，并在工业机械臂上进行了硬件验证。

Conclusion: GATO填补了中等批量规模实时MPC应用的求解器性能空白，通过开源支持可重现性和采用。

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [7] [Differentiable Particle Optimization for Fast Sequential Manipulation](https://arxiv.org/abs/2510.07674)
*Lucas Chen,Shrutheesh Raman Iyer,Zachary Kingston*

Main category: cs.RO

TL;DR: SPaSM是一个完全GPU并行化的框架，通过将约束评估、采样和梯度优化编译为优化的CUDA内核，实现端到端的轨迹优化，无需CPU协调，在具有挑战性的基准测试中实现了毫秒级的求解时间。


<details>
  <summary>Details</summary>
Motivation: 解决序列机器人操作任务中在潜在高维配置空间中寻找满足多个物体交互几何约束的无碰撞轨迹的问题，现有方法由于CPU-GPU数据传输开销和复杂逻辑导致性能有限。

Method: 采用两阶段粒子优化策略：首先通过大规模并行采样解决放置约束，然后在关节空间中提升解到完整轨迹优化。将约束评估、采样和梯度优化编译为优化的CUDA内核。

Result: 在具有挑战性的基准测试中实现了毫秒级的求解时间，成功率100%，相比现有方法实现了4000倍的加速。

Conclusion: SPaSM通过完全GPU并行化和端到端优化，显著提高了序列操作任务的求解效率，能够实时处理复杂约束场景。

Abstract: Sequential robot manipulation tasks require finding collision-free
trajectories that satisfy geometric constraints across multiple object
interactions in potentially high-dimensional configuration spaces. Solving
these problems in real-time and at large scales has remained out of reach due
to computational requirements. Recently, GPU-based acceleration has shown
promising results, but prior methods achieve limited performance due to CPU-GPU
data transfer overhead and complex logic that prevents full hardware
utilization. To this end, we present SPaSM (Sampling Particle optimization for
Sequential Manipulation), a fully GPU-parallelized framework that compiles
constraint evaluation, sampling, and gradient-based optimization into optimized
CUDA kernels for end-to-end trajectory optimization without CPU coordination.
The method consists of a two-stage particle optimization strategy: first
solving placement constraints through massively parallel sampling, then lifting
solutions to full trajectory optimization in joint space. Unlike hierarchical
approaches, SPaSM jointly optimizes object placements and robot trajectories to
handle scenarios where motion feasibility constrains placement options.
Experimental evaluation on challenging benchmarks demonstrates solution times
in the realm of $\textbf{milliseconds}$ with a 100% success rate; a
$4000\times$ speedup compared to existing approaches.

</details>


### [8] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: 提出EB-MBD方法，通过引入渐进式障碍约束解决模型扩散中的约束问题，避免性能下降和计算昂贵的投影操作


<details>
  <summary>Details</summary>
Motivation: 模型扩散中的约束会导致灾难性性能下降，即使在简单的2D系统中，由于蒙特卡洛近似得分函数的样本效率低下

Method: 引入新兴障碍函数，使用渐进式障碍约束，分析每轮迭代的采样活跃度来指导障碍参数调度

Result: 在2D碰撞避免和3D水下机械臂系统中，比模型扩散获得更低成本解，比基于投影的方法计算时间少几个数量级

Conclusion: EB-MBD方法通过渐进障碍约束有效解决了模型扩散中的约束问题，显著提升解质量且无需昂贵投影操作

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [9] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一种用于双足机器人在粗糙地形上安全导航的概率安全规划与控制框架，结合高斯过程地形估计和保形预测，确保动态可行性和质心鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在不确定地形上安全导航的挑战，需要同时考虑动态可行性、质心稳定性和地形不确定性。

Method: 采用高斯过程回归估计地形高程，利用保形预测构建置信区间，设计基于收缩的MPC导航框架和飞轮扭矩控制律。

Result: 在MuJoCo中对Digit双足机器人进行物理仿真验证，证明了框架在不确定地形上的安全导航能力。

Conclusion: 该框架为双足机器人提供了概率安全性和目标可达性保证，在不确定地形上实现了鲁棒导航。

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [10] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: 提出了一种可配置、组件无关的幻觉注入框架，用于研究自动驾驶车辆感知失败对安全性的影响，通过模拟验证了六种幻觉类型对碰撞和险情的影响。


<details>
  <summary>Details</summary>
Motivation: 现有故障注入研究通常针对单一传感器或感知模块，导致难以推广或集成到统一仿真环境中的孤立框架，需要一种更通用的方法来分析感知失败对安全性的影响。

Method: 将感知失败重新定义为幻觉，提出可配置的组件无关幻觉注入框架，在开源模拟器中注入六种合理的幻觉类型，执行超过18,350次模拟测试。

Result: 统计验证了框架有效性，量化了每种幻觉类型对碰撞和险情的影响，某些幻觉（如感知延迟和漂移）显著增加了测试场景中的碰撞风险。

Conclusion: 该框架提供了一个可扩展、统计验证、组件无关且完全互操作的工具集，简化并加速了自动驾驶安全验证，为容错和弹性自动驾驶设计奠定基础。

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [11] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: TrajSkill是一个从人类演示视频直接学习机器人操作技能的框架，通过将人体运动表示为稀疏光流轨迹来实现跨形态技能迁移。


<details>
  <summary>Details</summary>
Motivation: 解决人类与机器人之间形态差异导致的技能迁移困难，现有方法依赖配对数据集或手工设计奖励，限制了可扩展性和泛化能力。

Method: 将人体运动表示为稀疏光流轨迹作为形态无关的运动线索，结合视觉和文本输入，联合合成时间一致的机器人操作视频并转换为可执行动作。

Result: 在MetaWorld模拟实验中，FVD降低39.6%，KVD降低36.6%，跨形态成功率提升高达16.7%。真实机器人厨房操作任务验证了方法的有效性。

Conclusion: TrajSkill实现了从人类演示视频到机器人的跨形态技能迁移，在模拟和真实环境中都表现出色。

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [12] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: IntentionVLA是一个具有课程训练范式和高效推理机制的视觉-语言-动作模型框架，通过推理密集型预训练和推理引导的操作，显著提升了在复杂人机交互中的隐式人类意图推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的VLA模型主要在相关性有限的多模态任务上进行预训练，缺乏推理密集型预训练和推理引导的操作，无法执行复杂真实世界交互所需的隐式人类意图推理。

Method: 首先利用精心设计的推理数据（结合意图推理、空间定位和紧凑的具身推理）进行预训练，赋予模型推理和感知能力；然后在微调阶段使用紧凑推理输出作为动作生成的上下文指导，实现间接指令下的快速推理。

Result: IntentionVLA显著优于π0，直接指令下成功率提高18%，意图指令下比ECoT提高28%；在分布外意图任务上，成功率是基线的两倍以上，并实现零样本人机交互40%的成功率。

Conclusion: IntentionVLA代表了下一代人机交互系统的有前景范式，通过推理引导的方法显著提升了模型在复杂交互场景中的表现。

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [13] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: 提出了GM3（广义微移动模型），这是一个基于轮胎刷表示的统一物理模型，能够捕捉各种微移动车辆的动力学特性，包括轮胎滑移、负载转移和骑手/车辆倾斜。


<details>
  <summary>Details</summary>
Motivation: 现有的微移动车辆动力学模型主要依赖运动学自行车模型或其变体，忽略了轮胎滑移、负载转移和骑手/车辆倾斜等重要物理效应，缺乏统一的物理基础模型来覆盖各种常见的微移动车辆和车轮布局。

Method: 开发了基于轮胎刷表示的轮胎级公式GM3，支持任意车轮配置（单/双轨和多轮平台）。构建了交互式模型无关仿真框架，使用固定步长RK4积分、人在回路和脚本控制，以及实时轨迹跟踪和日志记录。

Result: 在斯坦福无人机数据集的死亡圆环（环形交叉口）场景中，对骑行者、滑板者和手推车类别进行了GM3的实证验证。

Conclusion: GM3提供了一个统一的物理基础模型，能够准确捕捉各种微移动车辆的动力学特性，弥补了现有模型的不足。

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [14] [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DM1提出了一种结合分散正则化的流匹配框架，解决现有基于流的策略中的表示崩溃问题，在保持一步推理效率的同时显著提升机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的生成模型虽然采样效率高，但存在表示崩溃问题，无法区分相似的视觉表示，导致在精确操作任务中失败。

Method: 将分散正则化集成到MeanFlow中，在不同中间嵌入层使用多种分散正则化变体，鼓励训练批次间的多样化表示，无需额外网络模块或专门训练过程。

Result: 在RoboMimic基准测试中，推理速度提升20-40倍（0.07s vs 2-3.5s），成功率提高10-20个百分点，Lift任务达到99%成功率（基线为85%）。真实机器人部署验证了从仿真到物理世界的有效迁移。

Conclusion: 这是首个利用表示正则化使基于流的策略在机器人操作中实现强性能的工作，为高效鲁棒的操作提供了一种简单而强大的方法。

Abstract: The ability to learn multi-modal action distributions is indispensable for
robotic manipulation policies to perform precise and robust control. Flow-based
generative models have recently emerged as a promising solution to learning
distributions of actions, offering one-step action generation and thus
achieving much higher sampling efficiency compared to diffusion-based methods.
However, existing flow-based policies suffer from representation collapse, the
inability to distinguish similar visual representations, leading to failures in
precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive
Regularization for One-Step Robotic Manipulation), a novel flow matching
framework that integrates dispersive regularization into MeanFlow to prevent
collapse while maintaining one-step efficiency. DM1 employs multiple dispersive
regularization variants across different intermediate embedding layers,
encouraging diverse representations across training batches without introducing
additional network modules or specialized training procedures. Experiments on
RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s
vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the
Lift task reaching 99% success over 85% of the baseline. Real-robot deployment
on a Franka Panda further validates that DM1 transfers effectively from
simulation to the physical world. To the best of our knowledge, this is the
first work to leverage representation regularization to enable flow-based
policies to achieve strong performance in robotic manipulation, establishing a
simple yet powerful approach for efficient and robust manipulation.

</details>


### [15] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: 提出了USIM水下机器人多任务视觉-语言-动作数据集和U0通用模型，通过多模态融合和感知增强模块，在多种水下任务中实现了80%的成功率，比基线方法在移动操作任务中减少21.2%的目标距离。


<details>
  <summary>Details</summary>
Motivation: 水下环境对机器人操作具有独特挑战，如复杂流体动力学、有限能见度和受限通信。虽然数据驱动方法在陆地机器人中取得了进展，但由于缺乏大规模高质量水下数据集，开发能够自主执行多任务的水下智能仍然极具挑战性。

Method: 构建USIM模拟数据集，包含56.1万帧数据、1852条轨迹，覆盖20个任务和9种场景。提出U0视觉-语言-动作模型，通过多模态融合整合双目视觉和其他传感器，并引入卷积注意力感知增强模块来提升空间理解和移动操作能力。

Result: 在检查、避障、扫描和动态跟踪等任务中达到80%的成功率，在移动操作任务中比基线方法减少21.2%的目标距离，证明了方法的有效性。

Conclusion: USIM和U0表明视觉-语言-动作模型可以有效地应用于水下机器人应用，为可扩展数据集构建、改进任务自主性和实现智能通用水下机器人提供了基础。

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [16] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文介绍了参加IROS 2025 RoboSense挑战赛社会导航赛道的技术方案，通过在Falcon模型基础上增加主动风险感知模块，提升了在动态人群环境中的社会合规导航性能，在16支参赛队伍中获得第2名。


<details>
  <summary>Details</summary>
Motivation: 开发能够在动态人群环境中安全、高效且符合社会规范的自主导航系统，解决仅使用车载传感器（RGB-D和里程计）且无法访问全局地图的挑战。

Method: 在Falcon模型基础上引入主动风险感知模块，学习预测周围人类基于距离的碰撞风险分数，增强空间感知能力和主动避碰行为。

Result: 在Social-HM3D基准测试中，该方法提高了代理在拥挤室内场景中保持个人空间合规性的能力，在挑战赛中获得了第2名。

Conclusion: 主动风险感知模块有效提升了社会导航性能，使自主代理在动态人群环境中能够更好地遵守社会规范并安全导航。

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [17] [Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots](https://arxiv.org/abs/2510.07882)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 提出了DualTHOR双臂人形机器人模拟器和Proprio-MLLM模型，通过集成本体感知信息显著提升了在长视野任务中的规划性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在双臂人形机器人长视野任务中效果有限，主要由于缺乏系统性评估平台和模型缺乏本体感知能力。

Method: 开发了DualTHOR模拟器，并提出了Proprio-MLLM模型，通过运动位置嵌入和跨空间编码器整合本体感知信息。

Result: 在现有MLLMs表现不佳的环境中，Proprio-MLLM实现了19.75%的平均规划性能提升。

Conclusion: 该工作为人形机器人具身智能提供了关键模拟平台和有效模型，推动了该领域的发展。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
the ability to serve as high-level planners, enabling robots to follow complex
human instructions. However, their effectiveness, especially in long-horizon
tasks involving dual-arm humanoid robots, remains limited. This limitation
arises from two main challenges: (i) the absence of simulation platforms that
systematically support task evaluation and data collection for humanoid robots,
and (ii) the insufficient embodiment awareness of current MLLMs, which hinders
reasoning about dual-arm selection logic and body positions during planning. To
address these issues, we present DualTHOR, a new dual-arm humanoid simulator,
with continuous transition and a contingency mechanism. Building on this
platform, we propose Proprio-MLLM, a model that enhances embodiment awareness
by incorporating proprioceptive information with motion-based position
embedding and a cross-spatial encoder. Experiments show that, while existing
MLLMs struggle in this environment, Proprio-MLLM achieves an average
improvement of 19.75% in planning performance. Our work provides both an
essential simulation platform and an effective model to advance embodied
intelligence in humanoid robotics. The code is available at
https://anonymous.4open.science/r/DualTHOR-5F3B.

</details>


### [18] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: GRACE框架通过可执行分析概念(EAC)弥合视觉语言模型语义推理与机器人物理执行之间的鸿沟，实现精确和通用的机器人操作


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型(VLMs)在语义推理和任务规划方面的能力与机器人实际物理执行之间的差距，即"语义到物理"的鸿沟

Method: 引入可执行分析概念(EAC)作为数学定义的蓝图，编码物体可供性、几何约束和操作语义，通过结构化策略脚手架管道将自然语言指令和视觉信息转化为实例化的EAC，从中推导抓取姿态、力方向和物理可行的运动轨迹

Result: 在模拟和真实环境中对多种铰接物体实现了强大的零样本泛化能力，无需任务特定训练

Conclusion: GRACE提供了一个统一且可解释的接口，通过语义-物理接地有效实现了精确和可泛化的操作

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [19] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: 提出了基于角度-轴空间的方向表示方法，通过加权平均机制在SO(3)流形上融合多条轨迹，实现多个局部约束的同时整合。


<details>
  <summary>Details</summary>
Motivation: SO(3)旋转群是黎曼流形，其非欧几何特性导致局部约束整合困难，特别是同时整合多个局部约束时存在扭曲问题。

Method: 使用角度-轴表示方法，在不同基点考虑不同局部约束生成多条轨迹，然后通过提出的加权平均机制融合这些轨迹生成平滑轨迹。

Result: 能够适应任意期望路径点的方向，处理角加速度约束，同时整合多个局部约束以获得额外收益，如实现更小的加速度成本。

Conclusion: 该方法解决了SO(3)流形上的扭曲问题，使现成的欧几里得学习算法在非欧空间中重新适用，验证了同时整合多个局部约束的有效性。

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [20] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: FastUMI-100K是一个大规模UMI风格的多模态演示数据集，包含超过10万条演示轨迹，涵盖54个任务和数百种物体类型，用于解决机器人操作学习中的数据限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类遥操作机器人收集的数据集在可扩展性、轨迹平滑性和不同机器人实体间的适用性方面存在限制，无法满足现实世界复杂操作任务的需求。

Method: 通过FastUMI机器人系统收集数据，该系统采用模块化、硬件解耦的机械设计和集成轻量级跟踪系统，在代表性家庭环境中收集多模态数据流，包括末端执行器状态、多视角腕戴鱼眼图像和文本注释。

Result: 实验结果表明，FastUMI-100K能够在各种基线算法上实现高策略成功率，证明了其鲁棒性、适应性和现实世界适用性。

Conclusion: FastUMI-100K提供了一个更可扩展、灵活和适应性强的解决方案，能够满足现实世界机器人演示数据的多样化需求，为解决复杂动态操作挑战提供了有效的数据支持。

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [21] [Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation](https://arxiv.org/abs/2510.08044)
*Shiyuan Yin,Chenjia Bai,Zihao Zhang,Junwei Jin,Xinxin Zhang,Chi Zhang,Xuelong Li*

Main category: cs.RO

TL;DR: CURE方法将LLM规划中的不确定性分解为认知不确定性和内在不确定性，通过随机网络蒸馏和MLP回归头进行估计，在机器人任务中提供更可靠的执行结果预测。


<details>
  <summary>Details</summary>
Motivation: LLM在机器人规划中存在幻觉问题，导致过度自信但可能不准确或不安全的计划。现有研究未能充分区分认知和内在不确定性，限制了不确定性估计的有效性。

Method: 提出CURE方法，将不确定性分解为认知不确定性（细分为任务清晰度和任务熟悉度）和内在不确定性，使用随机网络蒸馏和基于LLM特征的多层感知机回归头进行估计。

Result: 在厨房操作和桌面重排实验中验证，相比现有方法，CURE的不确定性估计与真实执行结果更一致。

Conclusion: CURE通过分解和单独估计不同类型的不确定性，显著提高了LLM规划中不确定性估计的可靠性。

Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities,
enabling robots to understand natural language instructions and generate
high-level plans with appropriate grounding. However, LLM hallucinations
present a significant challenge, often leading to overconfident yet potentially
misaligned or unsafe plans. While researchers have explored uncertainty
estimation to improve the reliability of LLM-based planning, existing studies
have not sufficiently differentiated between epistemic and intrinsic
uncertainty, limiting the effectiveness of uncertainty estimation. In this
paper, we present Combined Uncertainty estimation for Reliable Embodied
planning (CURE), which decomposes the uncertainty into epistemic and intrinsic
uncertainty, each estimated separately. Furthermore, epistemic uncertainty is
subdivided into task clarity and task familiarity for more accurate evaluation.
The overall uncertainty assessments are obtained using random network
distillation and multi-layer perceptron regression heads driven by LLM
features. We validated our approach in two distinct experimental settings:
kitchen manipulation and tabletop rearrangement experiments. The results show
that, compared to existing methods, our approach yields uncertainty estimates
that are more closely aligned with the actual execution outcomes.

</details>


### [22] [Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography](https://arxiv.org/abs/2510.08106)
*Zihan Li,Yixiao Xu,Lei Zhang,Taiyu Han,Xinshan Yang,Yingni Wang,Mingxuan Liu,Shenghai Xin,Linxun Liu,Hongen Liao,Guochen Ning*

Main category: cs.RO

TL;DR: 开发了一个自主轻量级超声机器人系统，能够在资源有限地区自动获取专家级标准肝脏超声平面并检测病理，包括在高海拔城市和野外环境中有效工作。


<details>
  <summary>Details</summary>
Motivation: 肝脏疾病是全球主要健康负担，超声是首选诊断工具，但肝脏超声检查需要专业技能，而专家超声医师在资源有限地区严重短缺。

Method: 系统包含一个AI代理，集成多模态感知和记忆注意力机制来定位不可见目标结构，以及一个588克6自由度缆线驱动机器人，通过安装在腹部增强运动鲁棒性。

Result: 机器人能够自主获取专家级标准肝脏超声平面，在患者中检测病理，包括在海拔2261米的西宁等医疗资源有限地区，在快速运动个体和野外环境中表现有效。

Conclusion: 这是首个在多种挑战性场景中实现自主超声检查的演示，有望改变服务不足地区获得专家级诊断的机会。

Abstract: Liver disease is a major global health burden. While ultrasound is the
first-line diagnostic tool, liver sonography requires locating multiple
non-continuous planes from positions where target structures are often not
visible, for biometric assessment and lesion detection, requiring significant
expertise. However, expert sonographers are severely scarce in resource-limited
regions. Here, we develop an autonomous lightweight ultrasound robot comprising
an AI agent that integrates multi-modal perception with memory attention for
localization of unseen target structures, and a 588-gram 6-degrees-of-freedom
cable-driven robot. By mounting on the abdomen, the system enhances robustness
against motion. Our robot can autonomously acquire expert-level standard liver
ultrasound planes and detect pathology in patients, including two from Xining,
a 2261-meter-altitude city with limited medical resources. Our system performs
effectively on rapid-motion individuals and in wilderness environments. This
work represents the first demonstration of autonomous sonography across
multiple challenging scenarios, potentially transforming access to expert-level
diagnostics in underserved regions.

</details>


### [23] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: 提出了一种基于聚类的技术，用于从UI日志中提取常规日志，特别针对存在噪声（人类执行不一致性）的场景，相比现有技术能提取更准确的常规日志。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多不直接关注模型发现，仅提取常规中的动作集合，且未在存在噪声（人类执行不一致性）的场景下进行评估。

Method: 采用基于聚类的技术来提取常规日志，在九个UI日志上进行实验，注入不同水平的噪声。

Result: 与现有技术相比，该技术能提取更准确的常规日志，特别是在存在噪声的情况下，通过标准评估指标验证了其优越性。

Conclusion: 该方法在存在噪声的场景下能有效提取常规日志，为机器人流程自动化提供了更好的支持。

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [24] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: 提出了NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对，用于评估导航代理的空间智能。评估了22个导航代理，并提出了新的SNav模型，在基准测试和真实机器人测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注语义理解，但忽视了系统评估导航代理的空间感知和推理能力。

Method: 引入NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对，用于探测导航代理的空间智能。提出SNav模型作为新的空间智能导航模型。

Result: 评估了22个导航代理，包括最先进的导航模型和多模态大语言模型。SNav在NavSpace和真实机器人测试中优于现有导航代理。

Conclusion: NavSpace基准揭示了具身导航中的空间智能问题，SNav为未来工作建立了强有力的基准。

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


### [25] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: 该研究比较了经典PID控制器与现代强化学习算法（DDPG、PPO、TRPO）在欠约束电缆驱动并联机器人控制中的性能，发现TRPO在多种轨迹下均表现最佳，具有最低的RMS误差和对更大控制更新间隔的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估经典和现代控制方法在真实世界电缆驱动并联机器人中的性能，特别关注欠约束系统和有限时间离散化的情况。

Method: 对经典PID控制器和现代强化学习算法（包括DDPG、PPO和TRPO）进行对比分析。

Result: TRPO在所有方法中表现最佳，在各种轨迹下实现最低的均方根误差，并且对更大的控制更新间隔具有鲁棒性。TRPO能够平衡探索和利用，在嘈杂的真实环境中实现稳定控制。

Conclusion: TRPO作为复杂机器人控制任务的鲁棒解决方案具有潜力，对动态环境和未来在传感器融合或混合控制策略中的应用具有重要意义。

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [26] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: Airy是一个艺术装置，通过两个强化学习训练的机械臂竞争抖床单来展示复杂AI系统的意图，使观众能够直观理解机器决策。


<details>
  <summary>Details</summary>
Motivation: 随着工业机器人进入人类共享空间，其不透明的决策过程威胁安全、信任和公共监督。该项目旨在探索如何使复杂多智能体AI变得直观可理解。

Method: 基于三个设计原则：竞争作为明确指标（谁抖得更高）、具身熟悉性（观众熟悉抖床单动作）、传感器到感官映射（通过森林和天气投影显示机器人合作或竞争），构建艺术装置。

Result: 在五个国际展览中的观察表明，观众能够实时解读机器人的策略、冲突和合作，情绪反应与系统内部状态一致。

Conclusion: 该项目展示了感官隐喻如何将黑盒系统转变为公共界面，使复杂AI决策变得可感知和理解。

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [27] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: 本文提出了一种基于单级重构的逆最优控制方法，相比传统双层方法计算时间减少15倍，且对噪声具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有逆最优控制方法要么基于缓慢的双层过程，要么基于快速但对噪声敏感的优化条件违反最小化方法，需要一种既快速又鲁棒的新方法。

Method: 采用等式约束最优控制模型，通过单级重构将传统双层IOC方法转化为单级优化问题。

Result: 在平面到达任务的数值实验中，该方法对极大噪声水平表现出弹性，计算时间比经典双层实现减少15倍。

Conclusion: 单级重构方法在保持结果等效性的同时，显著提高了逆最优控制的计算效率和噪声鲁棒性。

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [28] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出了一种使用CAD软件API验证6-6 Stewart-Gough平台机械臂最大无碰撞球体尺寸的方法，通过自动化更新移动平台位置并检查腿部碰撞来验证CFS安全性。


<details>
  <summary>Details</summary>
Motivation: 需要验证并行机械臂在给定移动平台方向下的最大无碰撞工作空间，确保操作安全性。

Method: 利用CAD软件API自动化更新移动平台位置，在CFS表面壳层内采样，检查每对腿部之间的碰撞情况。

Result: 该方法能够验证预计算的CFS安全性，并可估计任何空间并行机械臂的无碰撞工作空间。

Conclusion: 所提出的基于CAD API的自动化方法有效验证了Stewart-Gough平台机械臂的无碰撞球体尺寸，为并行机械臂的安全操作提供了可靠验证手段。

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


### [29] [Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/abs/2510.08464)
*Jason Jabbour,Dong-Ki Kim,Max Smith,Jay Patrikar,Radhika Ghosal,Youhui Wang,Ali Agha,Vijay Janapa Reddi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: GLUESTICK是一种后剪枝恢复方法，通过权重空间插值在剪枝后的VLA模型中恢复功能，无需额外训练即可在保持稀疏性的同时大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作(VLA)模型在资源受限硬件上部署困难，现有剪枝方法会导致性能急剧下降和安全违规增加。

Method: 在密集模型和剪枝模型之间进行一次性权重空间插值计算修正项，推理时每个剪枝层使用该修正项恢复丢失的能力。

Result: 在多种VLA架构和操作导航任务中，GLUESTICK实现了竞争性的内存效率，同时显著恢复成功率并减少安全违规。

Conclusion: GLUESTICK提供了一种无需训练、与剪枝算法无关的高效解决方案，通过单一超参数控制效率与精度权衡。

Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but
remain challenging to deploy on resource-limited hardware. Pruning has enabled
efficient compression of large language models (LLMs), yet it is largely
understudied in robotics. Surprisingly, we observe that pruning VLA models
leads to drastic degradation and increased safety violations. We introduce
GLUESTICK, a post-pruning recovery method that restores much of the original
model's functionality while retaining sparsity benefits. Our method performs a
one-time interpolation between the dense and pruned models in weight-space to
compute a corrective term. This correction is used during inference by each
pruned layer to recover lost capabilities with minimal overhead. GLUESTICK
requires no additional training, is agnostic to the pruning algorithm, and
introduces a single hyperparameter that controls the tradeoff between
efficiency and accuracy. Across diverse VLA architectures and tasks in
manipulation and navigation, GLUESTICK achieves competitive memory efficiency
while substantially recovering success rates and reducing safety violations.
Additional material can be found at: https://gluestick-vla.github.io/.

</details>


### [30] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan是一个自动化框架，可将人类视觉演示转换为类人机器人的双手灵巧操作技能，无需相机标定、深度传感器、3D物体模型或真实手部物体运动标注。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常只考虑简化的浮动手模型，需要大量手动数据收集和昂贵的运动捕捉设备。DexMan旨在直接从第三人称视频中学习灵巧操作技能，降低数据收集成本。

Method: 直接处理人类操作刚性物体的第三人称视频，利用基于接触的奖励来改进从噪声手部物体姿态估计中的策略学习，直接控制类人机器人。

Result: 在TACO基准测试中实现最先进的物体姿态估计性能（ADD-S和VSD分别提升0.08和0.12），在OakInk-v2上成功率比先前方法提高19%，可从真实和合成视频生成技能。

Conclusion: DexMan能够无需手动数据收集和昂贵运动捕捉，从野外视频中学习灵巧操作技能，为训练通用灵巧操作创建大规模多样化数据集提供了可能。

Abstract: We present DexMan, an automated framework that converts human visual
demonstrations into bimanual dexterous manipulation skills for humanoid robots
in simulation. Operating directly on third-person videos of humans manipulating
rigid objects, DexMan eliminates the need for camera calibration, depth
sensors, scanned 3D object assets, or ground-truth hand and object motion
annotations. Unlike prior approaches that consider only simplified floating
hands, it directly controls a humanoid robot and leverages novel contact-based
rewards to improve policy learning from noisy hand-object poses estimated from
in-the-wild videos.
  DexMan achieves state-of-the-art performance in object pose estimation on the
TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.
Meanwhile, its reinforcement learning policy surpasses previous methods by 19%
in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both
real and synthetic videos, without the need for manual data collection and
costly motion capture, and enabling the creation of large-scale, diverse
datasets for training generalist dexterous manipulation.

</details>


### [31] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出了R2RGen框架，直接从真实世界的点云观测-动作对生成数据，无需模拟器和渲染，实现了高效即插即用的真实到真实3D数据增强。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中的空间泛化问题，传统方法需要大量人工演示来覆盖不同空间配置，现有数据生成方法存在显著的模拟到真实差距，且受限于固定基座场景和预定义相机视角。

Method: 基于单源演示，通过细粒度场景和轨迹解析标注机制、分组增强策略处理多对象组合和任务约束、相机感知处理对齐生成数据与真实3D传感器分布。

Result: R2RGen在大量实验中显著提高了数据效率，并展示了在移动操作中扩展和应用的强大潜力。

Conclusion: R2RGen框架通过真实到真实的3D数据生成，有效解决了机器人操作的空间泛化问题，为移动操作提供了可行的解决方案。

Abstract: Towards the aim of generalized robotic manipulation, spatial generalization
is the most fundamental capability that requires the policy to work robustly
under different spatial distribution of objects, environment and agent itself.
To achieve this, substantial human demonstrations need to be collected to cover
different spatial configurations for training a generalized visuomotor policy
via imitation learning. Prior works explore a promising direction that
leverages data generation to acquire abundant spatially diverse data from
minimal source demonstrations. However, most approaches face significant
sim-to-real gap and are often limited to constrained settings, such as
fixed-base scenarios and predefined camera viewpoints. In this paper, we
propose a real-to-real 3D data generation framework (R2RGen) that directly
augments the pointcloud observation-action pairs to generate real-world data.
R2RGen is simulator- and rendering-free, thus being efficient and
plug-and-play. Specifically, given a single source demonstration, we introduce
an annotation mechanism for fine-grained parsing of scene and trajectory. A
group-wise augmentation strategy is proposed to handle complex multi-object
compositions and diverse task constraints. We further present camera-aware
processing to align the distribution of generated data with real-world 3D
sensor. Empirically, R2RGen substantially enhances data efficiency on extensive
experiments and demonstrates strong potential for scaling and application on
mobile manipulation.

</details>


### [32] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 提出一种解决机器人手内物体旋转sim-to-real挑战的新框架，通过联合动态模型和自主数据收集策略，实现单一策略在真实世界中泛化到各种物体和条件。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中复杂的接触动力学导致的sim-to-real差距问题，克服现有方法在物体几何、尺寸、手腕姿势等方面的限制。

Method: 使用联合动态模型学习拟合真实世界数据并调整模拟策略动作，通过关节因子化动态、压缩系统影响为低维变量，配合自主数据收集策略。

Result: 单一策略成功旋转复杂形状物体（如动物模型）、高宽比物体（达5.33）和小尺寸物体，处理多样化手腕方向和旋转轴。

Conclusion: 该方法在真实世界评估和复杂任务遥操作应用中验证了有效性和鲁棒性，实现了前所未有的泛化能力。

Abstract: Achieving generalized in-hand object rotation remains a significant challenge
in robotics, largely due to the difficulty of transferring policies from
simulation to the real world. The complex, contact-rich dynamics of dexterous
manipulation create a "reality gap" that has limited prior work to constrained
scenarios involving simple geometries, limited object sizes and aspect ratios,
constrained wrist poses, or customized hands. We address this sim-to-real
challenge with a novel framework that enables a single policy, trained in
simulation, to generalize to a wide variety of objects and conditions in the
real world. The core of our method is a joint-wise dynamics model that learns
to bridge the reality gap by effectively fitting limited amount of real-world
collected data and then adapting the sim policy's actions accordingly. The
model is highly data-efficient and generalizable across different whole-hand
interaction distributions by factorizing dynamics across joints, compressing
system-wide influences into low-dimensional variables, and learning each
joint's evolution from its own dynamic profile, implicitly capturing these net
effects. We pair this with a fully autonomous data collection strategy that
gathers diverse, real-world interaction data with minimal human intervention.
Our complete pipeline demonstrates unprecedented generality: a single policy
successfully rotates challenging objects with complex shapes (e.g., animals),
high aspect ratios (up to 5.33), and small sizes, all while handling diverse
wrist orientations and rotation axes. Comprehensive real-world evaluations and
a teleoperation application for complex tasks validate the effectiveness and
robustness of our approach. Website: https://meowuu7.github.io/DexNDM/

</details>


### [33] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本机器人操作框架，能够将任务描述转换为可执行计划，无需演示数据，支持刚性、关节和可变形物体的跨平台操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设任务分布内或需要特定平台数据微调，限制了跨平台迁移能力。本文旨在实现无需演示的零样本操作，支持不同机器人平台。

Method: 使用视频生成模型将任务描述合成视频，通过感知模块提取3D对象流，为刚性物体计算相对位姿并转换为机器人动作，对可变形物体使用基于粒子的动力学模型进行规划。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节和可变形物体操作任务，实现了有效的零样本执行，无需演示或特定平台训练。

Conclusion: NovaFlow通过将任务理解与底层控制解耦，实现了跨平台的自然迁移，为机器人零样本操作提供了有效解决方案。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.

</details>


### [34] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 本文重新评估了自动驾驶系统中离线与在线性能评估之间的相关性，发现两者相关性比之前研究报道的更差，并提出了一种基于认知不确定性的离线指标，显著改善了相关性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的真实世界评估通常通过离线方式进行，但离线模型性能与在线设置之间的关联性研究不足，特别是对于复杂城市机动场景。

Method: 通过广泛的仿真实验分析离线与在线评估的相关性，并研究基于认知不确定性的离线指标，以捕捉可能导致闭环设置中错误的事件。

Result: 发现离线与在线设置的相关性比先前研究报道的更差，提出的基于认知不确定性的离线指标相比先前指标实现了超过13%的相关性改进。

Conclusion: 当前驾驶策略评估实践和指标的有效性值得怀疑，基于认知不确定性的离线指标能更好地桥接离线与在线评估之间的差距。

Abstract: Real-World evaluation of perception-based planning models for robotic
systems, such as autonomous vehicles, can be safely and inexpensively conducted
offline, i.e., by computing model prediction error over a pre-collected
validation dataset with ground-truth annotations. However, extrapolating from
offline model performance to online settings remains a challenge. In these
settings, seemingly minor errors can compound and result in test-time
infractions or collisions. This relationship is understudied, particularly
across diverse closed-loop metrics and complex urban maneuvers. In this work,
we revisit this undervalued question in policy evaluation through an extensive
set of experiments across diverse conditions and metrics. Based on analysis in
simulation, we find an even worse correlation between offline and online
settings than reported by prior studies, casting doubts on the validity of
current evaluation practices and metrics for driving policies. Next, we bridge
the gap between offline and online evaluation. We investigate an offline metric
based on epistemic uncertainty, which aims to capture events that are likely to
cause errors in closed-loop settings. The resulting metric achieves over 13%
improvement in correlation compared to previous offline metrics. We further
validate the generalization of our findings beyond the simulation environment
in real-world settings, where even greater gains are observed.

</details>


### [35] [BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation](https://arxiv.org/abs/2510.08572)
*Rocktim Jyoti Das,Harsh Singh,Diana Turmakhan,Muhammad Abdullah Sohail,Mingfei Han,Preslav Nakov,Fabio Pizzati,Ivan Laptev*

Main category: cs.RO

TL;DR: BLAZER是一个从自动生成数据学习机器人操作策略的框架，利用LLM规划器的零样本能力在仿真中自动生成多样化操作任务的演示，成功案例用于微调LLM以提升规划能力，无需人工监督。


<details>
  <summary>Details</summary>
Motivation: 机器人领域缺乏互联网规模的多样化任务演示数据，现有数据集规模受限，需要手动收集和整理。

Method: 基于LLM规划器的零样本能力，在仿真中自动生成多样化操作任务的演示，用成功案例微调LLM，无需人工监督。

Result: BLAZER显著提升了仿真和真实环境中的零样本操作能力，在训练集外任务上也有改进，并能缩小LLM模型规模。

Conclusion: BLAZER框架通过自动生成训练数据有效解决了机器人领域数据稀缺问题，提升了策略的泛化能力和鲁棒性。

Abstract: Scaling data and models has played a pivotal role in the remarkable progress
of computer vision and language. Inspired by these domains, recent efforts in
robotics have similarly focused on scaling both data and model size to develop
more generalizable and robust policies. However, unlike vision and language,
robotics lacks access to internet-scale demonstrations across diverse robotic
tasks and environments. As a result, the scale of existing datasets typically
suffers from the need for manual data collection and curation. To address this
problem, here we propose BLAZER, a framework that learns manipulation policies
from automatically generated training data. We build on the zero-shot
capabilities of LLM planners and automatically generate demonstrations for
diverse manipulation tasks in simulation. Successful examples are then used to
finetune an LLM and to improve its planning capabilities without human
supervision. Notably, while BLAZER training requires access to the simulator's
state, we demonstrate direct transfer of acquired skills to sensor-based
manipulation. Through extensive experiments, we show BLAZER to significantly
improve zero-shot manipulation in both simulated and real environments.
Moreover, BLAZER improves on tasks outside of its training pool and enables
downscaling of LLM models. Our code and data will be made publicly available on
the project page.

</details>
