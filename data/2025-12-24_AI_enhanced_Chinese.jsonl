{"id": "2512.19855", "categories": ["cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.19855", "abs": "https://arxiv.org/abs/2512.19855", "authors": ["Andrew Stirling", "Mykola Lukashchuk", "Dmitry Bagaev", "Wouter Kouw", "James R. Forbes"], "title": "Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study", "comment": null, "summary": "This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.", "AI": {"tldr": "\u5c06ESGVI\u7b97\u6cd5\u6269\u5c55\u5230\u77e9\u9635\u674e\u7fa4\u4ee5\u5904\u7406\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u56e0\u5b50\u5904\u7406\u91cd\u5c3e\u548c\u504f\u659c\u566a\u58f0\uff0c\u5e94\u7528\u4e8eUWB\u5b9a\u4f4d\u4e2d\u7684NLOS\u573a\u666f\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u5e76\u4fdd\u6301\u4e86\u7a00\u758f\u6027\u3002", "motivation": "\u4f20\u7edf\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u59ff\u6001\u5206\u91cf\u7684\u72b6\u6001\u65f6\u65e0\u6cd5\u4fdd\u6301\u5e95\u5c42\u7fa4\u7ed3\u6784\uff0c\u4e14\u5728UWB\u5b9a\u4f4d\u4e2d\u5e38\u89c1\u7684NLOS\u548c\u591a\u5f84\u6548\u5e94\u4f1a\u4ea7\u751f\u91cd\u5c3e\u548c\u504f\u659c\u566a\u58f0\u5206\u5e03\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "1) \u5c06ESGVI\u6269\u5c55\u5230\u77e9\u9635\u674e\u7fa4\u4e0a\uff0c\u4f7f\u7b97\u6cd5\u80fd\u5904\u7406\u59ff\u6001\u4f30\u8ba1\u5e76\u4fdd\u6301\u7fa4\u7ed3\u6784\uff1b2) \u5f15\u5165\u56e0\u5b50\u5904\u7406\u91cd\u5c3e\u548c\u504f\u659c\u566a\u58f0\u5206\u5e03\uff1b3) \u5728\u56e0\u5b50\u56fe\u6846\u67b6\u4e2d\u5b9e\u73b0\uff0c\u4fdd\u6301\u7a00\u758f\u548c\u65e0\u5bfc\u6570\u7ed3\u6784\u3002", "result": "\u5728NLOS\u4e30\u5bcc\u7684UWB\u5b9a\u4f4d\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u76f8\u5f53\u7684\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b97\u6cd5\u7684\u7a00\u758f\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6210\u529f\u5c06ESGVI\u6269\u5c55\u5230\u77e9\u9635\u674e\u7fa4\u548c\u91cd\u5c3e\u566a\u58f0\u573a\u666f\uff0c\u4e3aUWB\u5b9a\u4f4d\u7b49\u5b9e\u9645\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u4f30\u8ba1\u6846\u67b6\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u4e86\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u5e94\u7528\u3002"}}
{"id": "2512.19914", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.19914", "abs": "https://arxiv.org/abs/2512.19914", "authors": ["Sujan Warnakulasooriya", "Andreas Willig", "Xiaobing Wu"], "title": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones", "comment": "35 pages", "summary": "Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4f18\u5148\u7ea7\u8c03\u5ea6\u7684\u65e0\u4eba\u673a\u7f16\u961f\u521d\u59cb\u5f62\u6210\u7b97\u6cd5\uff0c\u901a\u8fc7\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u548c\u5ef6\u8fdf\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u65e0\u78b0\u649e\u8f68\u8ff9\u89c4\u5212", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u96c6\u7fa4\u7f16\u961f\u7b97\u6cd5\u5728\u521d\u59cb\u5f62\u6210\u9636\u6bb5\u5b58\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u6f5c\u5728\u78b0\u649e\u8feb\u4f7f\u65e0\u4eba\u673a\u91c7\u53d6\u6b21\u4f18\u8f68\u8ff9\u65f6\u8868\u73b0\u4e0d\u4f73", "method": "\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u8c03\u5ea6\u7b97\u6cd5\uff1a\u6839\u636e\u65e0\u4eba\u673a\u7684\u6f5c\u5728\u78b0\u649e\u6570\u91cf\u548c\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u800c\u4e0d\u6c38\u4e45\u963b\u788d\u5176\u4ed6\u65e0\u4eba\u673a\u7684\u80fd\u529b\u5206\u914d\u4f18\u5148\u7ea7\uff0c\u7136\u540e\u8ba1\u7b97\u9002\u5f53\u5ef6\u8fdf\u4ee5\u786e\u4fdd\u65e0\u78b0\u649e\u8def\u5f84", "result": "\u6210\u529f\u4e3a\u591a\u8fbe5000\u67b6\u65e0\u4eba\u673a\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8e\u8026\u5408\u5ea6\u7684\u542f\u53d1\u5f0f\u4f18\u5148\u7ea7\u89c4\u5212\u65b9\u6cd5\uff08CDH-PP\uff09", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5148\u7ea7\u8c03\u5ea6\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u96c6\u7fa4\u521d\u59cb\u7f16\u961f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7f16\u961f\u5f62\u6210\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2512.20014", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20014", "abs": "https://arxiv.org/abs/2512.20014", "authors": ["Sangoh Lee", "Sangwoo Mo", "Wook-Shin Han"], "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting", "comment": null, "summary": "While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as \"bring my cup\", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.", "AI": {"tldr": "VAP\uff08\u89c6\u89c9\u6ce8\u610f\u529b\u63d0\u793a\uff09\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u611f\u77e5\u9002\u914d\u5668\uff0c\u901a\u8fc7\u89c6\u89c9\u8bb0\u5fc6\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0c\u8ba9\u51bb\u7ed3\u7684VLA\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u548c\u64cd\u4f5c\u4e2a\u6027\u5316\u5bf9\u8c61\uff0c\u5982\"\u62ff\u6211\u7684\u676f\u5b50\"\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u901a\u7528\u6307\u4ee4\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u4e2a\u6027\u5316\u547d\u4ee4\uff08\u5982\"\u62ff\u6211\u7684\u676f\u5b50\"\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u9700\u8981\u4ece\u89c6\u89c9\u76f8\u4f3c\u7684\u5bf9\u8c61\u4e2d\u8bc6\u522b\u7279\u5b9a\u5b9e\u4f8b\u3002", "method": "\u63d0\u51faVAP\uff08Visual Attentive Prompting\uff09\uff0c\u5c06\u53c2\u8003\u56fe\u50cf\u4f5c\u4e3a\u975e\u53c2\u6570\u89c6\u89c9\u8bb0\u5fc6\uff0c\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u548c\u5d4c\u5165\u5339\u914d\u5b9a\u4f4d\u4e2a\u6027\u5316\u5bf9\u8c61\uff0c\u7136\u540e\u901a\u8fc7\u9ad8\u4eae\u5bf9\u8c61\u548c\u91cd\u5199\u6307\u4ee4\u5c06\u5176\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\uff08Personalized-SIMPLER\u548cPersonalized-VLABench\uff09\u548c\u771f\u5b9e\u4e16\u754c\u684c\u9762\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVAP\u5728\u6210\u529f\u7387\u548c\u6b63\u786e\u5bf9\u8c61\u64cd\u4f5c\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u901a\u7528\u7b56\u7565\u548ctoken\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "VAP\u6709\u52a9\u4e8e\u5f25\u5408\u8bed\u4e49\u7406\u89e3\u548c\u5b9e\u4f8b\u7ea7\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u51bb\u7ed3\u7684VLA\u6a21\u578b\u80fd\u591f\u6709\u6548\u5904\u7406\u4e2a\u6027\u5316\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2512.20166", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20166", "abs": "https://arxiv.org/abs/2512.20166", "authors": ["Xiaofan Wang", "Xingyu Gao", "Jianlong Fu", "Zuolei Li", "Dean Fortier", "Galen Mullins", "Andrey Kolobov", "Baining Guo"], "title": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation", "comment": null, "summary": "The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable \"embodiment-anchored\" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.", "AI": {"tldr": "LoLA\u6846\u67b6\u901a\u8fc7\u6574\u5408\u957f\u671f\u591a\u89c6\u89d2\u89c2\u5bdf\u548c\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\uff0c\u5b9e\u73b0\u591a\u6b65\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u5728\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5f80\u5f80\u5ffd\u89c6\u5386\u53f2\u4fe1\u606f\u548c\u8fde\u8d2f\u52a8\u4f5c\u5e8f\u5217\u751f\u6210\u80fd\u529b\uff0c\u96be\u4ee5\u5b8c\u6210\u957f\u65f6\u7a0b\u3001\u8bed\u8a00\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u5386\u53f2\u5e8f\u5217\u548c\u591a\u89c6\u89d2\u89c2\u5bdf\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u5f15\u5165\u72b6\u6001\u611f\u77e5\u6f5c\u5728\u91cd\u8868\u793a\u6a21\u5757\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u548c\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u7a7a\u95f4\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\"\u5177\u8eab\u951a\u5b9a\"\u6f5c\u5728\u7a7a\u95f4\u5c06\u8868\u793a\u4e0e\u7269\u7406\u5c3a\u5ea6\u5bf9\u9f50", "result": "\u5728\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cLoLA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa", "conclusion": "LoLA\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5386\u53f2\u4fe1\u606f\u3001\u591a\u89c6\u89d2\u89c2\u5bdf\u548c\u673a\u5668\u4eba\u72b6\u6001\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u8bed\u8a00\u5f15\u5bfc\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2512.19864", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19864", "abs": "https://arxiv.org/abs/2512.19864", "authors": ["Shashi Kant Gupta", "Arijeet Pramanik", "Jerrin John Thomas", "Regina Schwind", "Lauren Wiener", "Avi Raju", "Jeremy Kornbluth", "Yanshan Wang", "Zhaohui Su", "Hrituraj Singh"], "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data", "comment": "39 Pages, Supplementary Included", "summary": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u5927\u89c4\u6a21\u63d0\u53d6\u7ed3\u6784\u5316\u80bf\u7624\u5b66\u6570\u636e\uff0c\u572840\u4e07\u4efd\u4e34\u5e8a\u7b14\u8bb0\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u63d0\u53d6\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u80bf\u7624\u6cbb\u7597\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u6587\u6863\u7ea7\u63d0\u53d6\u6216\u7279\u5b9a\u4e34\u5e8a\u53d8\u91cf\uff0c\u65e0\u6cd5\u5904\u7406\u5927\u91cf\u4e34\u5e8a\u6587\u6863\u4e2d\u7684\u77db\u76fe\u4fe1\u606f\u3002\u4eba\u5de5\u6807\u6ce8\u867d\u7136\u51c6\u786e\u4f46\u6210\u672c\u8fc7\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684\u80bf\u7624\u6570\u636e\u63d0\u53d6\u4efb\u52a1\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u3001\u81ea\u9002\u5e94\u7684\u5b50\u4efb\u52a1\u3002\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u63a8\u7406\u4ee3\u7406\uff0c\u914d\u5907\u4e0a\u4e0b\u6587\u654f\u611f\u68c0\u7d22\u548c\u8fed\u4ee3\u5408\u6210\u80fd\u529b\uff0c\u4ece\u771f\u5b9e\u4e16\u754c\u7684\u80bf\u7624\u5b66\u7b14\u8bb0\u4e2d\u5168\u9762\u63d0\u53d6\u7ed3\u6784\u5316\u4e34\u5e8a\u53d8\u91cf\u3002", "result": "\u5728\u5305\u542b2250\u540d\u764c\u75c7\u60a3\u8005\u768440\u591a\u4e07\u4efd\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u548cPDF\u62a5\u544a\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5e73\u5747F1\u5206\u6570\u8fbe\u52300.93\uff0c103\u4e2a\u80bf\u7624\u7279\u5f02\u6027\u4e34\u5e8a\u53d8\u91cf\u4e2d\u6709100\u4e2a\u8d85\u8fc70.85\uff0c\u5173\u952e\u53d8\u91cf\uff08\u5982\u751f\u7269\u6807\u5fd7\u7269\u548c\u836f\u7269\uff09\u8d85\u8fc70.95\u3002\u96c6\u6210\u5230\u6570\u636e\u6574\u7406\u5de5\u4f5c\u6d41\u4e2d\u5b9e\u73b0\u4e860.94\u7684\u76f4\u63a5\u4eba\u5de5\u6279\u51c6\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u7aef\u5230\u7aef\u3001\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u80bf\u7624\u6570\u636e\u63d0\u53d6\u5e94\u7528\uff0c\u80fd\u591f\u4ece\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u6587\u6863\u4e2d\u5168\u9762\u3001\u51c6\u786e\u5730\u63d0\u53d6\u80bf\u7624\u5b66\u4fe1\u606f\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20188", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20188", "abs": "https://arxiv.org/abs/2512.20188", "authors": ["Teqiang Zou", "Hongliang Zeng", "Yuxuan Nong", "Yifan Li", "Kehui Liu", "Haotian Yang", "Xinyang Ling", "Xin Li", "Lianyang Ma"], "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation", "comment": null, "summary": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.", "AI": {"tldr": "\u63d0\u51fa\u5f02\u6b65\u5feb\u6162VLA\u6846\u67b6DuoCore-FS\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u9891\u52a8\u4f5c\u751f\u6210\u548c\u6162\u901fVLM\u63a8\u7406\uff0c\u89e3\u51b3\u4f20\u7edfVLA\u7cfb\u7edf\u540c\u6b65\u6267\u884c\u5bfc\u81f4\u7684\u6027\u80fd\u9650\u5236\uff0c\u5b9e\u73b030Hz\u5168\u8eab\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u4f20\u7edfVLA\u7cfb\u7edf\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u52a8\u4f5c\u4e13\u5bb6\u7edf\u4e00\u5728\u5355\u4e00\u9891\u7387\u4e0b\u8fd0\u884c\uff0c\u53d7\u9650\u4e8e\u5927\u578bVLM\u7684\u4f4e\u63a8\u7406\u901f\u5ea6\uff0c\u5bfc\u81f4\u63a7\u5236\u7a33\u5b9a\u6027\u548c\u5b9e\u65f6\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u66f4\u591a\u5173\u8282\u3001\u66f4\u5927\u8fd0\u52a8\u7a7a\u95f4\u548c\u52a8\u6001\u89c6\u89d2\u53d8\u5316\u7684\u5168\u8eab\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u95ee\u9898\u66f4\u7a81\u51fa\u3002", "method": "\u63d0\u51fa\u5f02\u6b65\u5feb\u6162VLA\u6846\u67b6(DuoCore-FS)\uff0c\u5305\u542b\uff1a1) \u5feb\u901f\u8def\u5f84\u7528\u4e8e\u9ad8\u9891\u52a8\u4f5c\u751f\u6210\uff1b2) \u6162\u901f\u8def\u5f84\u7528\u4e8e\u4e30\u5bcc\u7684VLM\u63a8\u7406\uff1b3) \u6f5c\u5728\u8868\u793a\u7f13\u51b2\u533a\u6865\u63a5\u4e24\u4e2a\u7cfb\u7edf\uff0c\u5b58\u50a8\u6307\u4ee4\u8bed\u4e49\u548c\u4e0e\u573a\u666f\u6307\u4ee4\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7684\u52a8\u4f5c\u63a8\u7406\u8868\u793a\uff1b4) \u5168\u8eab\u52a8\u4f5c\u5206\u8bcd\u5668\u63d0\u4f9b\u7d27\u51d1\u7edf\u4e00\u7684\u5168\u8eab\u52a8\u4f5c\u8868\u793a\u3002VLM\u548c\u52a8\u4f5c\u4e13\u5bb6\u4ecd\u4fdd\u6301\u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3\u3002", "result": "DuoCore-FS\u652f\u630130\u4ebf\u53c2\u6570VLM\u7684\u540c\u65f6\u5b9e\u73b030Hz\u5168\u8eab\u52a8\u4f5c\u5757\u751f\u6210\uff0c\u6bd4\u540c\u7c7b\u89c4\u6a21VLA\u6a21\u578b\u5feb\u7ea63\u500d\u3002\u771f\u5b9e\u4e16\u754c\u5168\u8eab\u64cd\u4f5c\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u540c\u6b65\u5feb\u6162VLA\u57fa\u7ebf\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e14\u54cd\u5e94\u6027\u663e\u8457\u589e\u5f3a\u3002", "conclusion": "DuoCore-FS\u901a\u8fc7\u5f02\u6b65\u67b6\u6784\u89e3\u51b3\u4e86VLA\u7cfb\u7edf\u5728\u5168\u8eab\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5b9e\u65f6\u6027\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u7aef\u5230\u7aef\u7b56\u7565\u5b66\u4e60\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\u548c\u54cd\u5e94\u901f\u5ea6\uff0c\u5df2\u4f5c\u4e3aAstribot\u673a\u5668\u4eba\u5e73\u53f0\u7684\u4e00\u90e8\u5206\u63d0\u4f9b\u7ed9\u5546\u4e1a\u7528\u6237\u3002"}}
{"id": "2512.19903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19903", "abs": "https://arxiv.org/abs/2512.19903", "authors": ["Kirk Vanacore", "Rene F. Kizilcec"], "title": "How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse", "comment": null, "summary": "Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.", "AI": {"tldr": "LLMs\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5bf9\u771f\u5b9e\u8bfe\u5802\u6307\u4ee4\u5206\u7c7b\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u901a\u8fc7\u5c11\u6837\u672c\u63d0\u793a\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6700\u4f73\u914d\u7f6e\u8fbe\u5230Cohen's Kappa=0.58\uff0c\u4f46\u4ecd\u5b58\u5728\u53ef\u9760\u6027\u9650\u5236\u3002", "motivation": "\u968f\u7740LLMs\u5728\u6559\u80b2\u6280\u672f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e86\u89e3\u5176\u5728\u672a\u7ecf\u5b9a\u5236\u7684\u60c5\u51b5\u4e0b\u5bf9\u771f\u5b9e\u6559\u80b2\u573a\u666f\u7684\u89e3\u8bfb\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u6709\u52a9\u4e8e\u8bbe\u5b9a\u5408\u7406\u9884\u671f\u548c\u5efa\u7acb\u57fa\u51c6\u3002", "method": "\u6bd4\u8f83\u4e86\u516d\u4e2aLLM\u5728\u771f\u5b9e\u8bfe\u5802\u8f6c\u5f55\u672c\u4e2d\u5206\u7c7b\u6559\u5b66\u6307\u4ee4\u7684\u57fa\u7840\u6027\u80fd\uff0c\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u3001\u5355\u6837\u672c\u548c\u5c11\u6837\u672c\u4e09\u79cd\u5178\u578b\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u96f6\u6837\u672c\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5c11\u6837\u672c\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6700\u4f73\u914d\u7f6e\u8fbe\u5230Cohen's Kappa=0.58\uff1b\u6027\u80fd\u56e0\u6559\u5b66\u6307\u4ee4\u7c7b\u578b\u800c\u5f02\uff0c\u9ad8\u53ec\u56de\u7387\u5e38\u4ee5\u589e\u52a0\u8bef\u62a5\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u89e3\u8bfb\u6559\u5b66\u8bdd\u8bed\u65b9\u9762\u8868\u73b0\u51fa\u6709\u610f\u4e49\u4f46\u6709\u9650\u7684\u80fd\u529b\uff0c\u63d0\u793a\u8bbe\u8ba1\u6709\u52a9\u4e8e\u6316\u6398\u6f5c\u529b\u4f46\u4e0d\u80fd\u6d88\u9664\u6839\u672c\u7684\u53ef\u9760\u6027\u9650\u5236\u3002"}}
{"id": "2512.20224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20224", "abs": "https://arxiv.org/abs/2512.20224", "authors": ["Qijun Qin", "Ziqi Zhang", "Yihan Zhong", "Feng Huang", "Xikun Liu", "Runzhi Hu", "Hang Chen", "Wei Hu", "Dongzhe Su", "Jun Zhang", "Hoi-Fung Ng", "Weisong Wen"], "title": "UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas", "comment": "8 pages, 9 figures, IEEE ITSC 2025", "summary": "Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.", "AI": {"tldr": "UrbanV2X\u662f\u4e00\u4e2a\u7528\u4e8e\u8f66\u8f86-\u57fa\u7840\u8bbe\u65bd\u534f\u540c\u5bfc\u822a\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\uff0c\u5305\u542b\u8f66\u8f86\u548c\u8def\u8fb9\u57fa\u7840\u8bbe\u65bd\u91c7\u96c6\u7684\u540c\u6b65\u6570\u636e\uff0c\u652f\u6301\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u7814\u7a76\u3002", "motivation": "\u7531\u4e8e\u5355\u4e2a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5c40\u9650\u6027\uff0cC-V2X\u6280\u672f\u901a\u8fc7\u4f20\u611f\u5668\u4fe1\u606f\u5171\u4eab\u4e3a\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u7136\u800c\uff0c\u652f\u6301\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u8f66\u8f86-\u57fa\u7840\u8bbe\u65bd\u534f\u540c\u5bfc\u822a\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4ecd\u7136\u7a00\u7f3a\u3002", "method": "\u5728\u9999\u6e2fC-V2X\u6d4b\u8bd5\u573a\u6536\u96c6\u8f66\u8f86\u548c\u8def\u8fb9\u57fa\u7840\u8bbe\u65bd\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\u3002\u8f66\u8f7d\u5e73\u53f0\u63d0\u4f9b\u591a\u4e2a\u5de5\u4e1a\u76f8\u673a\u3001LiDAR\u30014D\u96f7\u8fbe\u3001UWB\u3001IMU\u548c\u9ad8\u7cbe\u5ea6GNSS-RTK/INS\u5bfc\u822a\u7cfb\u7edf\u7684\u540c\u6b65\u6570\u636e\u3002\u8def\u8fb9\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9bLiDAR\u3001GNSS\u548cUWB\u6d4b\u91cf\u3002\u6574\u4e2a\u7cfb\u7edf\u4f7f\u7528\u7cbe\u786e\u65f6\u95f4\u534f\u8bae(PTP)\u8fdb\u884c\u540c\u6b65\uff0c\u5e76\u63d0\u4f9b\u4f20\u611f\u5668\u6807\u5b9a\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86UrbanV2X\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\uff0c\u652f\u6301\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u7814\u7a76\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u5404\u79cd\u5bfc\u822a\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "UrbanV2X\u586b\u8865\u4e86\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u8f66\u8f86-\u57fa\u7840\u8bbe\u65bd\u534f\u540c\u5bfc\u822a\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3aC-V2X\u548c\u667a\u80fd\u4ea4\u901a\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.19908", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.19908", "abs": "https://arxiv.org/abs/2512.19908", "authors": ["Jingyi Qiu", "Hong Chen", "Zongyi Li"], "title": "Counterfactual LLM-based Framework for Measuring Rhetorical Style", "comment": null, "summary": "The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u53cd\u4e8b\u5b9e\u6846\u67b6\u91cf\u5316\u673a\u5668\u5b66\u4e60\u8bba\u6587\u4e2d\u7684\u4fee\u8f9e\u98ce\u683c\uff0c\u53d1\u73b0\u613f\u666f\u5f0f\u4fee\u8f9e\u80fd\u663e\u8457\u9884\u6d4b\u5f15\u7528\u548c\u5a92\u4f53\u5173\u6ce8\u5ea6\uff0c\u4e142023\u5e74\u540e\u4fee\u8f9e\u5f3a\u5ea6\u56e0LLM\u5199\u4f5c\u8f85\u52a9\u800c\u6025\u5267\u4e0a\u5347\u3002", "motivation": "AI\u9886\u57df\u5bf9\u673a\u5668\u5b66\u4e60\u8bba\u6587\u4e2d\u7684\"\u7092\u4f5c\"\u73b0\u8c61\u65e5\u76ca\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5c06\u4fee\u8f9e\u98ce\u683c\u4e0e\u5b9e\u8d28\u5185\u5bb9\u5206\u79bb\u3002\u7531\u4e8e\u5927\u80c6\u8bed\u8a00\u53ef\u80fd\u6e90\u4e8e\u5f3a\u5b9e\u8bc1\u7ed3\u679c\u6216\u4ec5\u662f\u4fee\u8f9e\u98ce\u683c\uff0c\u533a\u5206\u4e24\u8005\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u7684LLM\u6846\u67b6\uff1a\u591a\u4e2aLLM\u4fee\u8f9e\u89d2\u8272\u4ece\u76f8\u540c\u5b9e\u8d28\u5185\u5bb9\u751f\u6210\u53cd\u4e8b\u5b9e\u5199\u4f5c\uff0cLLM\u8bc4\u59d4\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83\u8bc4\u4f30\uff0c\u4f7f\u7528Bradley-Terry\u6a21\u578b\u805a\u5408\u7ed3\u679c\u3002\u5e94\u7528\u4e8e2017-2025\u5e748,485\u7bc7ICLR\u6295\u7a3f\uff0c\u751f\u6210\u8d85\u8fc725\u4e07\u7bc7\u53cd\u4e8b\u5b9e\u5199\u4f5c\u3002", "result": "\u613f\u666f\u5f0f\u4fee\u8f9e\u80fd\u663e\u8457\u9884\u6d4b\u4e0b\u6e38\u5173\u6ce8\u5ea6\uff08\u5305\u62ec\u5f15\u7528\u548c\u5a92\u4f53\u5173\u6ce8\uff09\uff0c\u5373\u4f7f\u63a7\u5236\u540c\u884c\u8bc4\u5ba1\u8bc4\u4ef7\u540e\u4f9d\u7136\u6709\u6548\u3002\u89c2\u5bdf\u52302023\u5e74\u540e\u4fee\u8f9e\u5f3a\u5ea6\u6025\u5267\u4e0a\u5347\uff0c\u5b9e\u8bc1\u8bc1\u636e\u8868\u660e\u8fd9\u4e3b\u8981\u7531LLM\u5199\u4f5c\u8f85\u52a9\u9a71\u52a8\u3002\u6846\u67b6\u53ef\u9760\u6027\u901a\u8fc7\u89d2\u8272\u9009\u62e9\u9c81\u68d2\u6027\u548cLLM\u5224\u65ad\u4e0e\u4eba\u5de5\u6807\u6ce8\u9ad8\u76f8\u5173\u6027\u9a8c\u8bc1\u3002", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u6d4b\u91cf\u548c\u6539\u8fdb\u79d1\u5b66\u8bc4\u4f30\u7684\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u91cf\u5316\u4fee\u8f9e\u98ce\u683c\u4e0e\u5b9e\u8d28\u5185\u5bb9\u7684\u5206\u79bb\uff0c\u4e3a\u79d1\u5b66\u5199\u4f5c\u8bc4\u4f30\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.20299", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20299", "abs": "https://arxiv.org/abs/2512.20299", "authors": ["Zhongyu Xia", "Wenhao Chen", "Yongtao Wang", "Ming-Hsuan Yang"], "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System", "comment": null, "summary": "Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.", "AI": {"tldr": "KnowVal\uff1a\u901a\u8fc7\u5f00\u653e\u4e16\u754c\u611f\u77e5\u4e0e\u77e5\u8bc6\u68c0\u7d22\u534f\u540c\u6574\u5408\uff0c\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u89c4\u5212\u6027\u80fd\u5e76\u4fdd\u6301\u4e0e\u73b0\u6709\u67b6\u6784\u517c\u5bb9", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u96be\u4ee5\u901a\u8fc7\u6a21\u4eff\u6216\u6709\u9650\u7684\u5f3a\u5316\u5956\u52b1\u6355\u6349\u51b3\u7b56\u80cc\u540e\u7684\u590d\u6742\u903b\u8f91\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u3001\u9a7e\u9a76\u77e5\u8bc6\u548c\u4ef7\u503c\u5bf9\u9f50\u95ee\u9898", "method": "\u6784\u5efa\u5305\u542b\u4ea4\u901a\u6cd5\u89c4\u3001\u9632\u5fa1\u6027\u9a7e\u9a76\u539f\u5219\u548c\u9053\u5fb7\u89c4\u8303\u7684\u5168\u9762\u9a7e\u9a76\u77e5\u8bc6\u56fe\u8c31\uff0c\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u9ad8\u6548\u68c0\u7d22\u673a\u5236\uff0c\u521b\u5efa\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u4ef7\u503c\u6a21\u578b\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u4ef7\u503c\u5bf9\u9f50\u8f68\u8ff9\u8bc4\u4f30", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f4e\u78b0\u649e\u7387\uff0c\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u89c4\u5212\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u67b6\u6784\u517c\u5bb9", "conclusion": "KnowVal\u901a\u8fc7\u6574\u5408\u5f00\u653e\u4e16\u754c\u611f\u77e5\u3001\u77e5\u8bc6\u68c0\u7d22\u548c\u4ef7\u503c\u5bf9\u9f50\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2512.19933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19933", "abs": "https://arxiv.org/abs/2512.19933", "authors": ["Zhixiang Lu", "Xueyuan Deng", "Yiran Liu", "Yulong Li", "Qiang Yan", "Imran Razzak", "Jionglong Su"], "title": "PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation", "comment": null, "summary": "Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u7ed3\u5408\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u548c\u4e2a\u6027\u6761\u4ef6\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u610f\u89c1\u52a8\u6001\uff0c\u901a\u8fc7MBTI\u4e2a\u6027\u5206\u914d\u63d0\u9ad8\u6a21\u578b\u771f\u5b9e\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4ee3\u7406\u7684\u610f\u89c1\u52a8\u6001\u6a21\u578b\u56e0\u5047\u8bbe\u540c\u8d28\u6027\u800c\u65e0\u6cd5\u6355\u6349\u5728\u7ebf\u6781\u5316\u7684\u5fc3\u7406\u5f02\u8d28\u6027\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9\u610f\u8bc6\u5f62\u6001\u5206\u6b67\u653e\u5927\u673a\u5236\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51faPRISM\u6df7\u5408\u6846\u67b6\uff1a\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6a21\u62df\u8fde\u7eed\u60c5\u611f\u6f14\u5316\uff0c\u7ed3\u5408\u4e2a\u6027\u6761\u4ef6\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5904\u7406\u79bb\u6563\u51b3\u7b56\u3002\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5206\u914d\u57fa\u4e8eMBTI\u7684\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u521d\u59cb\u5316\u3002", "result": "PRISM\u5728\u4e2a\u6027\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u540c\u8d28\u6a21\u578b\u548cBig Five\u57fa\u51c6\uff0c\u4e0e\u4eba\u7c7b\u771f\u5b9e\u6570\u636e\u66f4\u543b\u5408\u3002\u8be5\u6846\u67b6\u80fd\u6709\u6548\u590d\u5236\u7406\u6027\u6291\u5236\u548c\u60c5\u611f\u5171\u9e23\u7b49\u6d8c\u73b0\u73b0\u8c61\u3002", "conclusion": "PRISM\u4e3a\u5206\u6790\u590d\u6742\u793e\u4ea4\u5a92\u4f53\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u901a\u8fc7\u6574\u5408\u5fc3\u7406\u5f02\u8d28\u6027\u66f4\u597d\u5730\u6a21\u62df\u610f\u89c1\u52a8\u6001\u548c\u6781\u5316\u73b0\u8c61\u3002"}}
{"id": "2512.20322", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20322", "abs": "https://arxiv.org/abs/2512.20322", "authors": ["Katsu Uchiyama", "Ryuma Niiyama"], "title": "Pneumatic bladder links with wide range of motion joints for articulated inflatable robots", "comment": "Accepted at IROS2024 (IEEE/RSJ International Conference on Intelligent Robots and Systems)", "summary": "Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\\pm 150 ^{\\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u91c7\u7528Hillberry\u5173\u8282\u7684\u591a\u6bb5\u6c14\u52a8\u8180\u80f1\u8fde\u6746\u53ef\u5145\u6c14\u673a\u5668\u4eba\uff0c\u5177\u6709\u5927\u8fd0\u52a8\u8303\u56f4\u548c\u9ad8\u8d1f\u8f7d\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u673a\u68b0\u81c2\u548c\u817f\u5f0f\u8fd0\u52a8\u3002", "motivation": "\u63a2\u7d22\u53ef\u5145\u6c14\u673a\u5668\u4eba\u7684\u591a\u6837\u5316\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u5927\u8fd0\u52a8\u8303\u56f4\u548c\u9ad8\u8d1f\u8f7d\u80fd\u529b\u7684\u5173\u8282\u7ed3\u6784\uff0c\u4ee5\u6269\u5c55\u53ef\u5145\u6c14\u673a\u5668\u4eba\u7684\u529f\u80fd\u8fb9\u754c\u3002", "method": "\u8bbe\u8ba1\u91c7\u7528Hillberry\u6eda\u52a8\u63a5\u89e6\u5173\u8282\u7684\u591a\u6bb5\u6c14\u52a8\u8180\u80f1\u8fde\u6746\u7ed3\u6784\uff0c\u8180\u80f1\u8fde\u6746\u91c7\u7528\u5e06\u5e03\u548c\u805a\u6c28\u916f\u53cc\u5c42\u7ed3\u6784\u5b9e\u73b0\u6c14\u5bc6\u6027\u548c\u5f62\u72b6\u7075\u6d3b\u6027\uff0c\u6784\u5efa3\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c\u817f\u5f0f\u7ed3\u6784\u3002", "result": "Hillberry\u5173\u8282\u5b9e\u73b0\u00b1150\u00b0\u5927\u8fd0\u52a8\u8303\u56f4\uff1b3\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u53ef\u642c\u8fd0500g\u8d1f\u8f7d\uff1b2\u81ea\u7531\u5ea6\u548c1\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u5206\u522b\u53ef\u4e3e\u8d773.4kg\u548c5kg\u8d1f\u8f7d\uff1b3\u81ea\u7531\u5ea6\u5145\u6c14\u817f\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u6210\u529f\u5b9e\u73b0\u817f\u5f0f\u8fd0\u52a8\u3002", "conclusion": "\u63d0\u51fa\u7684Hillberry\u5173\u8282\u96c6\u6210\u65b9\u6848\u4e3a\u53ef\u5145\u6c14\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u8fd0\u52a8\u8303\u56f4\u548c\u8d1f\u8f7d\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5728\u673a\u68b0\u81c2\u548c\u817f\u5f0f\u8fd0\u52a8\u7b49\u591a\u6837\u5316\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.19950", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19950", "abs": "https://arxiv.org/abs/2512.19950", "authors": ["Heet Bodara", "Md Masum Mushfiq", "Isma Farah Siddiqui"], "title": "Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems", "comment": null, "summary": "Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u8bed\u8c03\u504f\u89c1\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u63a7\u5bf9\u8bdd\u5408\u6210\u548c\u8bed\u8c03\u5206\u7c7b\u6a21\u578b\uff0c\u53d1\u73b0\u5373\u4f7f\u4e2d\u6027\u63d0\u793a\u751f\u6210\u7684\u5bf9\u8bdd\u4e5f\u5b58\u5728\u7cfb\u7edf\u6027\u8bed\u8c03\u504f\u5dee\uff0c\u5e76\u5f00\u53d1\u4e86\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57\u4e2a\u4eba\u52a9\u7406\u7b49\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u867d\u7136\u56de\u7b54\u6d41\u7545\u81ea\u7136\uff0c\u4f46\u5e38\u5e38\u5e26\u6709\u5fae\u5999\u7684\u8bed\u8c03\u504f\u89c1\uff08\u5982\u8fc7\u5ea6\u793c\u8c8c\u3001\u6b22\u5feb\u6216\u8c28\u614e\uff09\uff0c\u8fd9\u4e9b\u504f\u89c1\u4f1a\u5f71\u54cd\u7528\u6237\u5bf9\u4fe1\u4efb\u3001\u540c\u7406\u5fc3\u548c\u516c\u5e73\u6027\u7684\u611f\u77e5\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68c0\u6d4b\u548c\u7406\u89e3\u3002", "method": "1. \u521b\u5efa\u4e24\u4e2a\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u96c6\uff1a\u4e00\u4e2a\u7531\u4e2d\u6027\u63d0\u793a\u751f\u6210\uff0c\u53e6\u4e00\u4e2a\u660e\u786e\u5f15\u5bfc\u751f\u6210\u79ef\u6781\u6216\u6d88\u6781\u8bed\u8c03\uff1b2. \u4f7f\u7528\u9884\u8bad\u7ec3\u7684DistilBERT\u6a21\u578b\u8fdb\u884c\u5f31\u76d1\u7763\u6807\u6ce8\uff1b3. \u8bad\u7ec3\u591a\u4e2a\u5206\u7c7b\u5668\u68c0\u6d4b\u8bed\u8c03\u6a21\u5f0f\uff1b4. \u91c7\u7528\u96c6\u6210\u6a21\u578b\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\u3002", "result": "1. \u5373\u4f7f\u4e2d\u6027\u63d0\u793a\u751f\u6210\u7684\u5bf9\u8bdd\u96c6\u4e5f\u663e\u793a\u51fa\u4e00\u81f4\u7684\u8bed\u8c03\u504f\u659c\uff0c\u8868\u660e\u504f\u89c1\u6e90\u4e8e\u6a21\u578b\u5e95\u5c42\u7684\u5bf9\u8bdd\u98ce\u683c\uff1b2. \u96c6\u6210\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u8fbe0.92\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u8bc1\u660e\u8bed\u8c03\u504f\u89c1\u662f\u7cfb\u7edf\u6027\u3001\u53ef\u6d4b\u91cf\u7684\uff1b3. \u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5bf9\u8bdd\u4e2d\u7684\u8bed\u8c03\u504f\u89c1\u6a21\u5f0f\u3002", "conclusion": "\u8bed\u8c03\u504f\u89c1\u662f\u5927\u8bed\u8a00\u6a21\u578b\u9690\u85cf\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u901a\u8fc7\u53ef\u63a7\u5bf9\u8bdd\u5408\u6210\u4e0e\u8bed\u8c03\u5206\u7c7b\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u7a33\u5065\u4e14\u5408\u4e4e\u4f26\u7406\u5730\u8bc6\u522b\u4e2a\u4eba\u52a9\u7406\u4ea4\u4e92\u4e2d\u7684\u60c5\u611f\u503e\u5411\uff0c\u8fd9\u5bf9\u4e8e\u8bbe\u8ba1\u516c\u5e73\u53ef\u4fe1\u7684\u5bf9\u8bddAI\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.20355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20355", "abs": "https://arxiv.org/abs/2512.20355", "authors": ["Hao Wei", "Peiji Wang", "Qianhao Wang", "Tong Qin", "Fei Gao", "Yulin Si"], "title": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration", "comment": null, "summary": "Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.", "AI": {"tldr": "FAR-AVIO\u662f\u4e00\u4e2a\u57fa\u4e8e\u8212\u5c14\u8865\u7684\u6c34\u4e0b\u58f0\u5b66-\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7EKF\u5d4c\u5165\u8212\u5c14\u8865\u5b9e\u73b0\u8054\u5408\u4f4d\u59ff-\u5730\u6807\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u6052\u5b9a\u65f6\u95f4\u66f4\u65b0\uff0c\u5e76\u5305\u542b\u81ea\u9002\u5e94\u4f20\u611f\u5668\u6743\u91cd\u8c03\u6574\u548c\u5728\u7ebf\u6807\u5b9a\u529f\u80fd\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u5bf9\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u5e26\u6765\u4e25\u5cfb\u6311\u6218\uff0c\u5305\u62ec\u5f3a\u5149\u8870\u51cf\u3001\u6d77\u6d0b\u96ea\u82b1\u548c\u6d4a\u5ea6\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u60ef\u6027\u53ef\u89c2\u6d4b\u6027\u964d\u4f4e\u548c\u9891\u7e41\u8ddf\u8e2a\u5931\u8d25\u3002\u867d\u7136\u7d27\u5bc6\u8026\u5408\u7684\u58f0\u5b66-\u89c6\u89c9-\u60ef\u6027\u878d\u5408\u53ef\u4ee5\u63d0\u4f9b\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u4f46\u57fa\u4e8e\u56fe\u7684\u4f18\u5316\u8ba1\u7b97\u91cf\u8fc7\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "1) \u5c06\u8212\u5c14\u8865\u516c\u5f0f\u5d4c\u5165\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(EKF)\uff0c\u5b9e\u73b0\u8054\u5408\u4f4d\u59ff-\u5730\u6807\u4f18\u5316\uff0c\u540c\u65f6\u901a\u8fc7\u9ad8\u6548\u8fb9\u7f18\u5316\u5730\u6807\u72b6\u6001\u4fdd\u6301\u6052\u5b9a\u65f6\u95f4\u66f4\u65b0\uff1b2) \u5f15\u5165\u81ea\u9002\u5e94\u6743\u91cd\u8c03\u6574\u548c\u53ef\u9760\u6027\u8bc4\u4f30(AWARE)\u6a21\u5757\uff0c\u5728\u7ebf\u8bc4\u4f30\u89c6\u89c9\u3001\u60ef\u6027\u548cDVL\u6d4b\u91cf\u7684\u53ef\u9760\u6027\u5e76\u81ea\u9002\u5e94\u8c03\u8282\u5176\u6743\u91cd\uff1b3) \u5f00\u53d1\u9ad8\u6548\u7684\u5728\u7ebf\u6807\u5b9a\u65b9\u6848\uff0c\u8054\u5408\u4f30\u8ba1DVL-IMU\u5916\u53c2\uff0c\u65e0\u9700\u4e13\u7528\u6807\u5b9a\u52a8\u4f5c\u3002", "result": "\u6570\u503c\u6a21\u62df\u548c\u771f\u5b9e\u6c34\u4e0b\u5b9e\u9a8c\u4e00\u81f4\u8868\u660e\uff0cFAR-AVIO\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6c34\u4e0bSLAM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9c81\u68d2\u8fd0\u884c\u3002", "conclusion": "FAR-AVIO\u901a\u8fc7\u8212\u5c14\u8865EKF\u6846\u67b6\u3001\u81ea\u9002\u5e94\u4f20\u611f\u5668\u6743\u91cd\u8c03\u6574\u548c\u5728\u7ebf\u6807\u5b9a\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u58f0\u5b66-\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u5e73\u53f0\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b9e\u65f6\u72b6\u6001\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.19995", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19995", "abs": "https://arxiv.org/abs/2512.19995", "authors": ["Ming Li", "Chenrui Fan", "Yize Cheng", "Soheil Feizi", "Tianyi Zhou"], "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "comment": null, "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "AI": {"tldr": "\u63d0\u51faThinkARM\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8f68\u8ff9\u62bd\u8c61\u4e3a\u529f\u80fd\u6027\u63a8\u7406\u6b65\u9aa4\uff08\u5982\u5206\u6790\u3001\u63a2\u7d22\u3001\u5b9e\u73b0\u3001\u9a8c\u8bc1\u7b49\uff09\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u601d\u7ef4\u52a8\u6001\u548c\u7ed3\u6784\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5c55\u793a\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u5176\u5e95\u5c42\u7684\u8ba4\u77e5\u7ed3\u6784\u548c\u6b65\u9aa4\u96be\u4ee5\u901a\u8fc7\u8868\u9762\u7edf\u8ba1\u6765\u8bc6\u522b\u548c\u5206\u6790\u3002\u9700\u8981\u4e00\u79cd\u4e2d\u95f4\u5c3a\u5ea6\u7684\u6846\u67b6\u6765\u660e\u786e\u62bd\u8c61\u63a8\u7406\u6b65\u9aa4\uff0c\u4ee5\u4fbf\u7cfb\u7edf\u5206\u6790\u63a8\u7406\u7684\u7ed3\u6784\u548c\u52a8\u6001\u3002", "method": "\u91c7\u7528Schoenfeld\u7684Episode Theory\u4f5c\u4e3a\u5f52\u7eb3\u6027\u4e2d\u95f4\u5c3a\u5ea6\u89c6\u89d2\uff0c\u5f15\u5165ThinkARM\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u663e\u5f0f\u62bd\u8c61\u4e3a\u529f\u80fd\u6027\u63a8\u7406\u6b65\u9aa4\u3002\u5e94\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u8bca\u65ad\u6848\u4f8b\u7814\u7a76\u5206\u6790\u63a2\u7d22\u6b65\u9aa4\u7684\u5173\u952e\u4f5c\u7528\u548c\u6548\u7387\u5bfc\u5411\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u62bd\u8c61\u63ed\u793a\u4e86\u53ef\u91cd\u590d\u7684\u601d\u7ef4\u52a8\u6001\u548c\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u7684\u7ed3\u6784\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5728token\u7ea7\u89c6\u56fe\u4e2d\u4e0d\u660e\u663e\u3002\u63a2\u7d22\u6b65\u9aa4\u4f5c\u4e3a\u4e0e\u6b63\u786e\u6027\u76f8\u5173\u7684\u5173\u952e\u5206\u652f\u6b65\u9aa4\uff0c\u800c\u6548\u7387\u5bfc\u5411\u65b9\u6cd5\u9009\u62e9\u6027\u5730\u6291\u5236\u8bc4\u4f30\u53cd\u9988\u6b65\u9aa4\u800c\u975e\u5747\u5300\u7f29\u77ed\u54cd\u5e94\u3002", "conclusion": "Episode\u7ea7\u8868\u793a\u4f7f\u63a8\u7406\u6b65\u9aa4\u663e\u5f0f\u5316\uff0c\u80fd\u591f\u7cfb\u7edf\u5206\u6790\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u4e2d\u63a8\u7406\u7684\u7ed3\u6784\u3001\u7a33\u5b9a\u6027\u548c\u53d8\u5316\u65b9\u5f0f\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2512.20475", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20475", "abs": "https://arxiv.org/abs/2512.20475", "authors": ["Maulana Bisyir Azhari", "Donghun Han", "Je In You", "Sungjun Park", "David Hyunchul Shim"], "title": "Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing", "comment": null, "summary": "The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3a\u963f\u5e03\u624e\u6bd4\u81ea\u4e3b\u8d5b\u8f66\u8054\u76df\u65e0\u4eba\u673a\u51a0\u519b\u8054\u8d5b\u5f00\u53d1\u7684\u5355\u76ee\u89c6\u89c9\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\u7cfb\u7edf\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e0e\u57fa\u4e8eYOLO\u7684\u95e8\u6846\u68c0\u6d4b\u5668\uff0c\u7ed3\u5408\u611f\u77e5\u611f\u77e5\u89c4\u5212\u5668\uff0c\u5728\u6bd4\u8d5b\u4e2d\u83b7\u5f97\u591a\u9879\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u6bd4\u8d5b\u8981\u6c42\u4ec5\u4f7f\u7528\u5355\u6444\u50cf\u5934\u548c\u4f4e\u8d28\u91cfIMU\u8fdb\u884c\u9ad8\u901f\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\uff0c\u8fd9\u79cd\u6700\u5c0f\u4f20\u611f\u5668\u914d\u7f6e\u5bb9\u6613\u5728\u9ad8\u901f\u98de\u884c\u548c\u6fc0\u8fdb\u673a\u52a8\u65f6\u4ea7\u751f\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u6f02\u79fb\uff0c\u9700\u8981\u89e3\u51b3\u5b9a\u4f4d\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u878d\u5408\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u8f93\u51fa\u4e0e\u57fa\u4e8eYOLO\u7684\u95e8\u6846\u68c0\u6d4b\u5668\u63d0\u4f9b\u7684\u5168\u5c40\u4f4d\u7f6e\u6d4b\u91cf\uff0c\u540c\u65f6\u8bbe\u8ba1\u611f\u77e5\u611f\u77e5\u89c4\u5212\u5668\u751f\u6210\u5e73\u8861\u901f\u5ea6\u4e0e\u95e8\u6846\u53ef\u89c1\u6027\u7684\u8f68\u8ff9\u3002", "result": "\u7cfb\u7edf\u5728\u6bd4\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aAI Grand Challenge\u7b2c\u4e09\u540d\uff08\u6700\u9ad8\u901f\u5ea643.2 km/h\uff09\uff0cAI Drag Race\u7b2c\u4e8c\u540d\uff08\u901f\u5ea6\u8d85\u8fc759 km/h\uff09\uff0cAI Multi-Drone Race\u7b2c\u4e8c\u540d\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u7cfb\u7edf\u67b6\u6784\u548c\u57fa\u4e8e\u6bd4\u8d5b\u6570\u636e\u7684\u6027\u80fd\u5206\u6790\uff0c\u4e3a\u57fa\u4e8e\u5355\u76ee\u89c6\u89c9\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u98de\u884c\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.20092", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20092", "abs": "https://arxiv.org/abs/2512.20092", "authors": ["Yiming Du", "Baojun Wang", "Yifan Xiang", "Zhaowei Wang", "Wenyu Huang", "Boyang Xue", "Bin Liang", "Xingshan Zeng", "Fei Mi", "Haoli Bai", "Lifeng Shang", "Jeff Z. Pan", "Yuxin Jiang", "Kam-Fai Wong"], "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents", "comment": null, "summary": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/", "AI": {"tldr": "Memory-T1\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65f6\u95f4\u611f\u77e5\u8bb0\u5fc6\u9009\u62e9\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u5bf9\u8bdd\u5386\u53f2\u4e2d\u7684\u65f6\u5e8f\u63a8\u7406\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7b56\u7565\u548c\u4e09\u7ea7\u5956\u52b1\u51fd\u6570\u63d0\u5347\u65f6\u5e8f\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u5728\u5904\u7406\u957f\u5bf9\u8bdd\u5386\u53f2\u65f6\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u65f6\u5e8f\u76f8\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u63a8\u7406\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u968f\u7740\u5bf9\u8bdd\u5386\u53f2\u589e\u957f\u548c\u566a\u58f0\u7d2f\u79ef\uff0c\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u5bf9\u8bdd\u667a\u80fd\u4f53\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u63d0\u51faMemory-T1\u6846\u67b6\uff1a1\uff09\u7c97\u7b5b\u9009\uff1a\u4f7f\u7528\u65f6\u5e8f\u548c\u76f8\u5173\u6027\u8fc7\u6ee4\u5668\u5c06\u5bf9\u8bdd\u5386\u53f2\u526a\u679d\u4e3a\u5019\u9009\u96c6\uff1b2\uff09\u7ec6\u9009\u62e9\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u7cbe\u786e\u9009\u62e9\u8bc1\u636e\u4f1a\u8bdd\uff1b3\uff09\u4e09\u7ea7\u5956\u52b1\u51fd\u6570\uff1a\u4f18\u5316\u7b54\u6848\u51c6\u786e\u6027\u3001\u8bc1\u636e\u57fa\u7840\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u5176\u4e2d\u65f6\u5e8f\u4e00\u81f4\u6027\u5956\u52b1\u63d0\u4f9b\u5bc6\u96c6\u4fe1\u53f7\uff0c\u8bc4\u4f30\u4f1a\u8bdd\u7ea7\uff08\u65f6\u5e8f\u90bb\u8fd1\u6027\uff09\u548c\u8bdd\u8bed\u7ea7\uff08\u65f6\u5e8f\u4fdd\u771f\u5ea6\uff09\u7684\u5bf9\u9f50\u3002", "result": "\u5728Time-Dialog\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemory-T1\u5c067B\u6a21\u578b\u63d0\u5347\u81f367.0%\u7684\u603b\u4f53\u5f97\u5206\uff0c\u521b\u4e0b\u5f00\u6e90\u6a21\u578b\u65b0SOTA\uff0c\u6bd414B\u57fa\u7ebf\u9ad8\u51fa10.2%\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u8bc1\u636e\u57fa\u7840\u5956\u52b1\u5171\u540c\u8d21\u732e15.0%\u7684\u6027\u80fd\u63d0\u5347\u3002\u6846\u67b6\u5728128k\u4ee4\u724c\u957f\u5ea6\u4e0b\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u800c\u57fa\u7ebf\u6a21\u578b\u5df2\u5931\u6548\u3002", "conclusion": "Memory-T1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7684\u65f6\u95f4\u611f\u77e5\u8bb0\u5fc6\u9009\u62e9\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5bf9\u8bdd\u5386\u53f2\u4e2d\u7684\u65f6\u5e8f\u63a8\u7406\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u63a8\u7406\u51c6\u786e\u6027\uff0c\u4e3a\u5bf9\u8bdd\u667a\u80fd\u4f53\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20591", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20591", "abs": "https://arxiv.org/abs/2512.20591", "authors": ["Changyi Lin", "Boda Huo", "Mingyang Yu", "Emily Ruppel", "Bingqing Chen", "Jonathan Francis", "Ding Zhao"], "title": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing", "comment": null, "summary": "Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.", "AI": {"tldr": "LightTact\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9\u89e6\u89c9\u6307\u5c16\u4f20\u611f\u5668\uff0c\u91c7\u7528\u5149\u5b66\u539f\u7406\u76f4\u63a5\u53ef\u89c6\u5316\u63a5\u89e6\uff0c\u65e0\u9700\u4f9d\u8d56\u8868\u9762\u53d8\u5f62\uff0c\u80fd\u591f\u611f\u77e5\u6db2\u4f53\u3001\u534a\u6db2\u4f53\u548c\u8d85\u8f6f\u6750\u6599\u7684\u6781\u8f7b\u5fae\u63a5\u89e6\u3002", "motivation": "\u73b0\u6709\u89e6\u89c9\u4f20\u611f\u5668\u5927\u591a\u4f9d\u8d56\u8868\u9762\u53d8\u5f62\u6765\u63a8\u65ad\u63a5\u89e6\uff0c\u96be\u4ee5\u53ef\u9760\u611f\u77e5\u4e0e\u6db2\u4f53\u3001\u534a\u6db2\u4f53\u6216\u8d85\u8f6f\u6750\u6599\u7b49\u4e0d\u53d1\u751f\u5b8f\u89c2\u53d8\u5f62\u7684\u63a5\u89e6\u3002\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u53d8\u5f62\u7684\u63a5\u89e6\u611f\u77e5\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u73af\u5883\u5149\u963b\u6321\u5149\u5b66\u914d\u7f6e\uff0c\u6291\u5236\u975e\u63a5\u89e6\u533a\u57df\u7684\u5916\u90e8\u5149\u548c\u5185\u90e8\u7167\u660e\uff0c\u4ec5\u4f20\u8f93\u771f\u5b9e\u63a5\u89e6\u4ea7\u751f\u7684\u6f2b\u5c04\u5149\u3002\u4ea7\u751f\u9ad8\u5bf9\u6bd4\u5ea6\u539f\u59cb\u56fe\u50cf\uff0c\u975e\u63a5\u89e6\u50cf\u7d20\u63a5\u8fd1\u9ed1\u8272\uff0c\u63a5\u89e6\u50cf\u7d20\u4fdd\u7559\u63a5\u89e6\u8868\u9762\u7684\u81ea\u7136\u5916\u89c2\u3002", "result": "\u5b9e\u73b0\u7cbe\u786e\u7684\u50cf\u7d20\u7ea7\u63a5\u89e6\u5206\u5272\uff0c\u5bf9\u6750\u6599\u7279\u6027\u3001\u63a5\u89e6\u529b\u3001\u8868\u9762\u5916\u89c2\u548c\u73af\u5883\u5149\u7167\u5177\u6709\u9c81\u68d2\u6027\u3002\u5728\u673a\u68b0\u81c2\u4e0a\u6f14\u793a\u4e86\u6c34\u6269\u6563\u3001\u9762\u971c\u8638\u53d6\u548c\u8584\u819c\u4ea4\u4e92\u7b49\u6781\u8f7b\u5fae\u63a5\u89e6\u9a71\u52a8\u64cd\u4f5c\u3002\u89c6\u89c9\u89e6\u89c9\u56fe\u50cf\u53ef\u76f4\u63a5\u7531\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\uff0c\u5b9e\u73b0\u7535\u963b\u503c\u63a8\u7406\u7528\u4e8e\u673a\u5668\u4eba\u5206\u62e3\u3002", "conclusion": "LightTact\u63d0\u4f9b\u4e86\u4e00\u79cd\u53d8\u5f62\u65e0\u5173\u7684\u5149\u5b66\u63a5\u89e6\u611f\u77e5\u65b9\u6cd5\uff0c\u80fd\u591f\u53ef\u9760\u68c0\u6d4b\u6781\u8f7b\u5fae\u63a5\u89e6\uff0c\u4e3a\u4e0e\u8f6f\u6750\u6599\u548c\u6db2\u4f53\u4ea4\u4e92\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\uff0c\u5176\u89c6\u89c9\u89e6\u89c9\u56fe\u50cf\u4e0e\u73b0\u6709AI\u6a21\u578b\u517c\u5bb9\u3002"}}
{"id": "2512.20097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20097", "abs": "https://arxiv.org/abs/2512.20097", "authors": ["Zuo Wang", "Ye Yuan"], "title": "A Novel Graph-Sequence Learning Model for Inductive Text Classification", "comment": null, "summary": "Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.", "AI": {"tldr": "TextGSL\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u5e8f\u5217\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u8fb9\u7c7b\u578b\u6587\u672c\u56fe\u5e76\u96c6\u6210Transformer\u5c42\uff0c\u89e3\u51b3\u73b0\u6709GNN\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u4e2d\u7ed3\u6784\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u548c\u5e8f\u5217\u4fe1\u606f\u7f3a\u5931\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a1\uff09\u672a\u80fd\u5145\u5206\u8003\u8651\u8bcd\u5bf9\u4e4b\u95f4\u7684\u591a\u6837\u5316\u7ed3\u6784\u4fe1\u606f\uff08\u5982\u5171\u73b0\u3001\u53e5\u6cd5\u3001\u8bed\u4e49\u5173\u7cfb\uff09\uff1b2\uff09\u5728\u6587\u672c\u56fe\u7ed3\u6784\u5b66\u4e60\u6a21\u5757\u4e2d\u5ffd\u7565\u4e86\u5e8f\u5217\u4fe1\u606f\uff0c\u65e0\u6cd5\u5904\u7406\u5305\u542b\u65b0\u8bcd\u548c\u65b0\u5173\u7cfb\u7684\u6587\u672c\u3002", "method": "\u63d0\u51faTextGSL\u6a21\u578b\uff1a1\uff09\u4e3a\u6bcf\u4e2a\u6587\u672c\u6784\u5efa\u5355\u6587\u672c\u7ea7\u56fe\uff0c\u57fa\u4e8e\u8bcd\u5bf9\u95f4\u7684\u591a\u6837\u5316\u5173\u7cfb\u5efa\u7acb\u4e0d\u540c\u7684\u8fb9\u7c7b\u578b\uff1b2\uff09\u8bbe\u8ba1\u81ea\u9002\u5e94\u591a\u8fb9\u6d88\u606f\u4f20\u9012\u8303\u5f0f\u6765\u805a\u5408\u8bcd\u5bf9\u95f4\u7684\u591a\u6837\u5316\u7ed3\u6784\u4fe1\u606f\uff1b3\uff09\u901a\u8fc7\u96c6\u6210Transformer\u5c42\u6765\u6355\u6349\u6587\u672c\u6570\u636e\u7684\u5e8f\u5217\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTextGSL\u5728\u51c6\u786e\u7387\u65b9\u9762\u4f18\u4e8e\u591a\u4e2a\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5b66\u4e60\u66f4\u5177\u5224\u522b\u6027\u6587\u672c\u8868\u793a\u7684\u80fd\u529b\u3002", "conclusion": "TextGSL\u901a\u8fc7\u7ed3\u5408\u591a\u8fb9\u56fe\u7ed3\u6784\u548c\u5e8f\u5217\u4fe1\u606f\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709GNN\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.20111", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20111", "abs": "https://arxiv.org/abs/2512.20111", "authors": ["Aly Lidayan", "Jakob Bjorner", "Satvik Golechha", "Kartik Goyal", "Alane Suhr"], "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language", "comment": null, "summary": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.", "AI": {"tldr": "ABBEL\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u8868\u8fbe\u7684\u4fe1\u5ff5\u74f6\u9888\u6765\u538b\u7f29\u591a\u6b65\u51b3\u7b56\u4efb\u52a1\u7684\u5386\u53f2\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u751f\u6210\u548c\u57fa\u4e8e\u4fe1\u5ff5\u884c\u52a8\uff0c\u5728\u4fdd\u6301\u8fd1\u6052\u5b9a\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\u8d85\u8d8a\u5b8c\u6574\u4e0a\u4e0b\u6587\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u957f\u5ea6\u589e\u52a0\uff0c\u5728\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u5b8c\u6574\u4ea4\u4e92\u5386\u53f2\u53d8\u5f97\u8ba1\u7b97\u4e0a\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u538b\u7f29\u5386\u53f2\u53c8\u80fd\u4fdd\u6301\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faABBEL\u6846\u67b6\uff1a\u7528\u81ea\u7136\u8bed\u8a00\u4fe1\u5ff5\u72b6\u6001\uff08\u4efb\u52a1\u76f8\u5173\u672a\u77e5\u4fe1\u606f\u7684\u6458\u8981\uff09\u66ff\u4ee3\u957f\u4ea4\u4e92\u5386\u53f2\uff1b\u6bcf\u6b65\u5148\u66f4\u65b0\u5148\u9a8c\u4fe1\u5ff5\u5f62\u6210\u540e\u9a8c\u4fe1\u5ff5\uff0c\u7136\u540e\u4ec5\u57fa\u4e8e\u540e\u9a8c\u9009\u62e9\u884c\u52a8\uff1b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u751f\u6210\u548c\u57fa\u4e8e\u4fe1\u5ff5\u884c\u52a8\uff0c\u5305\u62ec\u4fe1\u5ff5\u8d28\u91cf\u8bc4\u5206\u548c\u957f\u5ea6\u60e9\u7f5a\u3002", "result": "ABBEL\u652f\u6301\u751f\u6210\u53ef\u89e3\u91ca\u4fe1\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u8fd1\u6052\u5b9a\u7684\u5185\u5b58\u4f7f\u7528\uff1b\u4f46\u74f6\u9888\u65b9\u6cd5\u6613\u53d7\u9519\u8bef\u4f20\u64ad\u5f71\u54cd\uff0c\u521d\u59cb\u6027\u80fd\u4f4e\u4e8e\u5b8c\u6574\u4e0a\u4e0b\u6587\u8bbe\u7f6e\uff1b\u901a\u8fc7RL\u8bad\u7ec3\u540e\uff0cABBEL\u6027\u80fd\u8d85\u8d8a\u5b8c\u6574\u4e0a\u4e0b\u6587\u8bbe\u7f6e\uff0c\u4e14\u5185\u5b58\u4f7f\u7528\u5c11\u4e8e\u540c\u671f\u65b9\u6cd5\u3002", "conclusion": "ABBEL\u6846\u67b6\u901a\u8fc7\u4fe1\u5ff5\u74f6\u9888\u6709\u6548\u538b\u7f29\u591a\u6b65\u4ea4\u4e92\u5386\u53f2\uff0c\u7ed3\u5408RL\u8bad\u7ec3\u80fd\u591f\u8d85\u8d8a\u5b8c\u6574\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u4e3a\u957f\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u4e86\u5185\u5b58\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20136", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20136", "abs": "https://arxiv.org/abs/2512.20136", "authors": ["Hyeongcheol Park", "Jiyoung Seo", "Jaewon Mun", "Hogun Park", "Wonmin Byeon", "Sung June Kim", "Hyeonsoo Im", "JeungSub Lee", "Sangpil Kim"], "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.", "AI": {"tldr": "\u63d0\u51faM\u00b3KG-RAG\uff0c\u4e00\u79cd\u591a\u8df3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u8df3MMKG\u548cGRASP\u673a\u5236\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u7684\u63a8\u7406\u6df1\u5ea6\u548c\u7b54\u6848\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001RAG\u5728\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u73b0\u6709\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff08MMKGs\uff09\u7684\u6a21\u6001\u8986\u76d6\u6709\u9650\u4e14\u591a\u8df3\u8fde\u63a5\u4e0d\u8db3\uff1b2\uff09\u4ec5\u57fa\u4e8e\u5171\u4eab\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u76f8\u4f3c\u6027\u68c0\u7d22\uff0c\u65e0\u6cd5\u8fc7\u6ee4\u65e0\u5173\u6216\u5197\u4f59\u77e5\u8bc6\u3002", "method": "1\uff09\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u6784\u5efa\u591a\u8df3MMKG\uff08M\u00b3KG\uff09\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u5b9e\u4f53\u4e09\u5143\u7ec4\uff1b2\uff09\u63d0\u51faGRASP\uff08Grounded Retrieval And Selective Pruning\uff09\u673a\u5236\uff0c\u786e\u4fdd\u5b9e\u4f53\u4e0e\u67e5\u8be2\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u8bc4\u4f30\u7b54\u6848\u652f\u6301\u76f8\u5173\u6027\uff0c\u5e76\u4fee\u526a\u5197\u4f59\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cM\u00b3KG-RAG\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u548c\u57fa\u7840\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "M\u00b3KG-RAG\u901a\u8fc7\u6784\u5efa\u591a\u8df3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u548c\u5f15\u5165GRASP\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u591a\u6a21\u6001RAG\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u77e5\u8bc6\u68c0\u7d22\u8d28\u91cf\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2512.20144", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20144", "abs": "https://arxiv.org/abs/2512.20144", "authors": ["Yuxin Wang", "Shicheng Fang", "Bo Wang", "Qi Luo", "Xuanjing Huang", "Yining Zheng", "Xipeng Qiu"], "title": "Multi-hop Reasoning via Early Knowledge Alignment", "comment": "16 pages", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.", "AI": {"tldr": "EKA\u901a\u8fc7\u65e9\u671f\u77e5\u8bc6\u5bf9\u9f50\u6a21\u5757\uff0c\u5728\u8fed\u4ee3RAG\u7cfb\u7edf\u4e2d\u8ba9LLM\u5728\u89c4\u5212\u524d\u5148\u4e0e\u68c0\u7d22\u96c6\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u3001\u51cf\u5c11\u7ea7\u8054\u9519\u8bef\uff0c\u63d0\u9ad8\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3RAG\u7cfb\u7edf\u5728\u89c4\u5212\u95ee\u9898\u5206\u89e3\u65f6\u672a\u5145\u5206\u5229\u7528\u68c0\u7d22\u8bed\u6599\u5e93\u4fe1\u606f\uff0c\u5bfc\u81f4\u68c0\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u63a8\u7406\u94fe\u7ea7\u8054\u9519\u8bef\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u63d0\u51faEarly Knowledge Alignment (EKA)\u6a21\u5757\uff0c\u5728\u8fed\u4ee3RAG\u7cfb\u7edf\u4e2d\u8ba9LLM\u5728\u89c4\u5212\u524d\u5148\u4e0e\u68c0\u7d22\u96c6\u5bf9\u9f50\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u68c0\u7d22\u77e5\u8bc6\u5efa\u7acb\u66f4\u5f3a\u63a8\u7406\u57fa\u7840\u3002", "result": "\u5728\u516d\u4e2a\u6807\u51c6RAG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEKA\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u3001\u51cf\u5c11\u7ea7\u8054\u9519\u8bef\uff0c\u63d0\u9ad8\u6027\u80fd\u4e0e\u6548\u7387\u3002\u4ece\u71b5\u89d2\u5ea6\u5206\u6790\u663e\u793a\u65e9\u671f\u77e5\u8bc6\u51cf\u5c11\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u5fc5\u8981\u63a2\u7d22\u3002", "conclusion": "EKA\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u529f\u80fd\u63a8\u7406\u7b56\u7565\uff0c\u53ef\u65e0\u7f1d\u6269\u5c55\u5230\u5927\u578b\u6a21\u578b\uff0c\u5728\u8fed\u4ee3RAG\u7cfb\u7edf\u4e2d\u63a8\u8fdb\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u9610\u660e\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u4e0e\u9ad8\u6548\u63a2\u7d22\u4e4b\u95f4\u7684\u5173\u952e\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2512.20145", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20145", "abs": "https://arxiv.org/abs/2512.20145", "authors": ["Xiang Chen", "Yixin Ou", "Quan Feng", "Lei Li", "Piji Li", "Haibo Ye", "Sheng-Jun Huang", "Shuofei Qiao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models", "comment": "IEEE/ACM Transactions on Audio, Speech and Language Processing", "summary": "The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.", "AI": {"tldr": "RetroPrompt\u662f\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u673a\u5236\u4ece\u77e5\u8bc6\u5e93\u4e2d\u83b7\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5728\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u63d0\u5347PFM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4ecd\u9075\u5faa\u53c2\u6570\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u8bb0\u5fc6\u548c\u6b7b\u8bb0\u786c\u80cc\u7684\u6cdb\u5316\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u5145\u5206\u5229\u7528\u975e\u5178\u578b\u5b9e\u4f8b\u5e76\u907f\u514d\u5728\u6709\u9650\u6570\u636e\u4e0b\u5bf9\u6d45\u5c42\u6a21\u5f0f\u7684\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51faRetroPrompt\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u8bad\u7ec3\u6570\u636e\u751f\u6210\u7684\u516c\u5f00\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5728\u8f93\u5165\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u878d\u5165\u68c0\u7d22\u673a\u5236\uff0c\u4ece\u800c\u5c06\u77e5\u8bc6\u4ece\u5355\u7eaf\u8bb0\u5fc6\u4e2d\u89e3\u8026\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u591a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\uff0c\u8bc1\u660eRetroPrompt\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u5bf9\u6b7b\u8bb0\u786c\u80cc\u7684\u4f9d\u8d56\u5e76\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RetroPrompt\u901a\u8fc7\u68c0\u7d22\u673a\u5236\u5e73\u8861\u8bb0\u5fc6\u4e0e\u6cdb\u5316\uff0c\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u63d0\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u8303\u5f0f\u3002"}}
{"id": "2512.20156", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.20156", "abs": "https://arxiv.org/abs/2512.20156", "authors": ["Qian Chen", "Luyao Cheng", "Chong Deng", "Xiangang Li", "Jiaqing Liu", "Chao-Hong Tan", "Wen Wang", "Junhao Xu", "Jieping Ye", "Qinglin Zhang", "Qiquan Zhang", "Jingren Zhou"], "title": "Fun-Audio-Chat Technical Report", "comment": "21 pages, https://github.com/FunAudioLLM/Fun-Audio-Chat", "summary": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.", "AI": {"tldr": "Fun-Audio-Chat\u662f\u4e00\u4e2a\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\u548c\u6838\u5fc3\u9e21\u5c3e\u9152\u8bad\u7ec3\u89e3\u51b3\u4e86\u8bed\u97f3-\u6587\u672c\u6a21\u578b\u4e2d\u7684\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u8bed\u97f3\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u8054\u5408\u8bed\u97f3-\u6587\u672c\u6a21\u578b\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u8bed\u97f3\u6807\u8bb0\uff0825Hz\uff09\u548c\u6587\u672c\u6807\u8bb0\uff08~3Hz\uff09\u4e4b\u95f4\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u4f1a\u7a00\u91ca\u8bed\u4e49\u4fe1\u606f\uff1b2\uff09\u9ad8\u8ba1\u7b97\u6210\u672c\uff1b3\uff09\u5bfc\u81f4\u6587\u672cLLM\u77e5\u8bc6\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002\u8fd9\u4e9b\u9650\u5236\u4e86\u8bed\u97f3\u4ea4\u4e92\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\uff1a1\uff09\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\uff08DRSR\uff09\uff1a\u5171\u4eabLLM\u4ee5\u9ad8\u6548\u76845Hz\u5904\u7406\u97f3\u9891\uff08\u901a\u8fc7\u6807\u8bb0\u5206\u7ec4\uff09\uff0c\u800c\u8bed\u97f3\u7cbe\u70bc\u5934\u4ee525Hz\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u8bb0\uff0c\u5e73\u8861\u6548\u7387\u548c\u8d28\u91cf\uff1b2\uff09\u6838\u5fc3\u9e21\u5c3e\u9152\u8bad\u7ec3\uff1a\u4e24\u9636\u6bb5\u5fae\u8c03\u4e0e\u4e2d\u95f4\u5408\u5e76\uff0c\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u3002\u7136\u540e\u5e94\u7528\u591a\u4efb\u52a1DPO\u8bad\u7ec3\u589e\u5f3a\u9c81\u68d2\u6027\u3001\u97f3\u9891\u7406\u89e3\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u8bed\u97f3\u5171\u60c5\u80fd\u529b\u3002", "result": "Fun-Audio-Chat 8B\u548cMoE 30B-A3B\u5728\u8bed\u97f3\u5230\u6587\u672c\u548c\u8bed\u97f3\u5230\u8bed\u97f3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u53e3\u8bedQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5728\u76f8\u4f3c\u89c4\u6a21\u6a21\u578b\u4e2d\u6392\u540d\u524d\u5217\u3002\u5728\u97f3\u9891\u7406\u89e3\u3001\u8bed\u97f3\u529f\u80fd\u8c03\u7528\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u8bed\u97f3\u5171\u60c5\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u751a\u81f3\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u8fd8\u5f00\u53d1\u4e86\u5168\u53cc\u5de5\u53d8\u4f53Fun-Audio-Chat-Duplex\u3002", "conclusion": "Fun-Audio-Chat\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5206\u8fa8\u7387\u67b6\u6784\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8bed\u97f3-\u6587\u672c\u6a21\u578b\u7684\u5173\u952e\u9650\u5236\uff0c\u5728\u4fdd\u6301\u6587\u672cLLM\u77e5\u8bc6\u7684\u540c\u65f6\u83b7\u5f97\u4e86\u5f3a\u5927\u7684\u97f3\u9891\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u97f3\u9891-\u6587\u672c\u9884\u8bad\u7ec3\u3002"}}
{"id": "2512.20164", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20164", "abs": "https://arxiv.org/abs/2512.20164", "authors": ["Honglin Mu", "Jinghao Liu", "Kaiyang Wan", "Rui Xing", "Xiuying Chen", "Timothy Baldwin", "Wanxiang Che"], "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications", "comment": null, "summary": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.", "AI": {"tldr": "LLMs\u5728\u7b80\u5386\u7b5b\u9009\u4e2d\u5b58\u5728\u5bf9\u6297\u6307\u4ee4\u653b\u51fb\u6f0f\u6d1e\uff0c\u653b\u51fb\u6210\u529f\u7387\u8d8580%\u3002\u672c\u6587\u63d0\u51faFIDS\u9632\u5fa1\u65b9\u6cd5\uff0c\u7ed3\u5408LoRA\u9002\u5e94\uff0c\u76f8\u6bd4\u63d0\u793a\u9632\u5fa1\u6548\u679c\u66f4\u597d\uff0c\u7efc\u5408\u9632\u5fa1\u53ef\u964d\u4f4e26.3%\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "LLMs\u5728\u4ee3\u7801\u5ba1\u67e5\u3001\u5185\u5bb9\u5ba1\u6838\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u9690\u85cf\u5728\u8f93\u5165\u6570\u636e\uff08\u5982\u7b80\u5386\u3001\u4ee3\u7801\uff09\u4e2d\u7684\"\u5bf9\u6297\u6307\u4ee4\"\u64cd\u7eb5\uff0c\u5bfc\u81f4\u504f\u79bb\u9884\u671f\u4efb\u52a1\u3002\u5728\u7b80\u5386\u7b5b\u9009\u7b49\u5e94\u7528\u4e2d\uff0c\u9488\u5bf9\u8fd9\u79cd\u6f0f\u6d1e\u7684\u9632\u5fa1\u63aa\u65bd\u5f80\u5f80\u7f3a\u5931\u3002", "method": "1) \u5efa\u7acb\u8bc4\u4f30\u7b80\u5386\u7b5b\u9009\u6f0f\u6d1e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1b2) \u8bc4\u4f30\u4e24\u79cd\u9632\u5fa1\u673a\u5236\uff1a\u57fa\u4e8e\u63d0\u793a\u7684\u9632\u5fa1\u548c\u63d0\u51fa\u7684FIDS\uff08\u901a\u8fc7\u5206\u79bb\u68c0\u6d4b\u5916\u6765\u6307\u4ee4\uff09\u65b9\u6cd5\uff0c\u540e\u8005\u4f7f\u7528LoRA\u9002\u5e94\u6280\u672f\uff1b3) \u7ed3\u5408\u4e24\u79cd\u9632\u5fa1\u65b9\u6cd5\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "1) \u67d0\u4e9b\u653b\u51fb\u7c7b\u578b\u7684\u6210\u529f\u7387\u8d85\u8fc780%\uff1b2) \u63d0\u793a\u9632\u5fa1\u964d\u4f4e10.1%\u653b\u51fb\u7387\uff0c\u4f46\u589e\u52a012.5%\u8bef\u62d2\u7387\uff1b3) FIDS\u65b9\u6cd5\u964d\u4f4e15.4%\u653b\u51fb\u7387\uff0c\u589e\u52a010.4%\u8bef\u62d2\u7387\uff1b4) \u7ed3\u5408\u65b9\u6cd5\u964d\u4f4e26.3%\u653b\u51fb\u7387\uff0c\u8bad\u7ec3\u65f6\u9632\u5fa1\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4fdd\u62a4\u65b9\u9762\u4f18\u4e8e\u63a8\u7406\u65f6\u7f13\u89e3\u3002", "conclusion": "LLMs\u5728\u7b80\u5386\u7b5b\u9009\u7b49\u5e94\u7528\u4e2d\u5b58\u5728\u5bf9\u6297\u6307\u4ee4\u653b\u51fb\u7684\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u8981\u4e13\u95e8\u7684\u9632\u5fa1\u673a\u5236\u3002FIDS\u65b9\u6cd5\u7ed3\u5408LoRA\u9002\u5e94\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u9632\u5fa1\u6b64\u7c7b\u653b\u51fb\uff0c\u8bad\u7ec3\u65f6\u9632\u5fa1\u7b56\u7565\u6bd4\u63a8\u7406\u65f6\u7f13\u89e3\u63aa\u65bd\u66f4\u6709\u6548\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2512.20182", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20182", "abs": "https://arxiv.org/abs/2512.20182", "authors": ["Shuzheng Si", "Qingyi Wang", "Haozhe Zhao", "Yuzhuo Bai", "Guanqiao Chen", "Kangyang Luo", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination", "comment": null, "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.", "AI": {"tldr": "FaithLens\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u5fe0\u5b9e\u6027\u5e7b\u89c9\u7684\u9ad8\u6548\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u4f9b\u4e8c\u5143\u9884\u6d4b\u548c\u89e3\u91ca\uff0c\u572812\u4e2a\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u6458\u8981\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7ecf\u5e38\u51fa\u73b0\u5fe0\u5b9e\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u63d0\u9ad8\u53ef\u4fe1\u5ea6\u3002", "method": "1) \u4f7f\u7528\u5148\u8fdbLLM\u5408\u6210\u5e26\u89e3\u91ca\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u8fc7\u6ee4\u7b56\u7565\u786e\u4fdd\u6807\u7b7e\u6b63\u786e\u6027\u3001\u89e3\u91ca\u8d28\u91cf\u548c\u6570\u636e\u591a\u6837\u6027\uff1b2) \u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u4f5c\u4e3a\u51b7\u542f\u52a8\uff1b3) \u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5956\u52b1\u9884\u6d4b\u6b63\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "result": "\u572812\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\uff0c8B\u53c2\u6570\u7684FaithLens\u8d85\u8d8a\u4e86GPT-4.1\u548co3\u7b49\u5148\u8fdb\u6a21\u578b\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u89e3\u91ca\uff0c\u5728\u53ef\u4fe1\u5ea6\u3001\u6548\u7387\u548c\u6709\u6548\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u72ec\u7279\u7684\u5e73\u8861\u3002", "conclusion": "FaithLens\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u8f93\u51fa\u4e2d\u7684\u5fe0\u5b9e\u6027\u5e7b\u89c9\uff0c\u540c\u65f6\u63d0\u4f9b\u89e3\u91ca\u4ee5\u63d0\u9ad8\u53ef\u4fe1\u5ea6\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.20204", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20204", "abs": "https://arxiv.org/abs/2512.20204", "authors": ["Marko \u010cechovi\u010d", "Nat\u00e1lia Komorn\u00edkov\u00e1", "Dominik Mach\u00e1\u010dek", "Ond\u0159ej Bojar"], "title": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings", "comment": "12 pages, 2 figures, 6 tables, published as a conference paper in Text, Speech, and Dialogue 28th International Conference, TSD 2025, Erlangen, Germany, August 25-28, 2025, Proceedings, Part II. This version published here on arXiv.org is before review comments and seedings of the TSD conference staff", "summary": "Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u8de8\u8bed\u8a00\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u5728\u65e0\u5171\u540c\u8bed\u8a00\u8005\u4f1a\u8bae\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bef\u89e3\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u771f\u5b9e\u3001\u591a\u529f\u80fd\u7684\u8bc4\u4f30\u8bed\u6599\u5e93\u6765\u8bc4\u4f30\u81ea\u52a8\u7cfb\u7edf\u5728\u65e0\u5171\u540c\u8bed\u8a00\u8005\u4f1a\u8bae\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u7814\u7a76\u8de8\u8bed\u8a00\u4ea4\u6d41\u4e2d\u7684\u8bef\u89e3\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u521b\u5efa\u5305\u542b12\u79cd\u8bed\u8a00\u76845\u5c0f\u65f6\u8de8\u8bed\u8a00\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u5305\u542b\u8bed\u97f3\u5f55\u97f3\u3001ASR\u8f6c\u5f55\u3001\u4eba\u5de5\u6821\u5bf9\u7ffb\u8bd1\u548c\u4f1a\u8bae\u7eaa\u8981\u3002\u540c\u65f6\u624b\u52a8\u6807\u6ce8\u8bef\u89e3\uff0c\u5e76\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Gemini\uff09\u7684\u81ea\u52a8\u8bef\u89e3\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u591a\u8bed\u8a00\u5bf9\u8bdd\u7684\u5b9e\u7528\u8bed\u6599\u5e93\u3002\u5728\u8bef\u89e3\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cGemini\u6a21\u578b\u80fd\u591f\u4ee577%\u7684\u53ec\u56de\u7387\u548c47%\u7684\u7cbe\u786e\u5ea6\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u8bef\u89e3\u7247\u6bb5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8de8\u8bed\u8a00\u5bf9\u8bdd\u8bc4\u4f30\u8d44\u6e90\uff0c\u5e76\u8bc1\u660e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u8de8\u8bed\u8a00\u4ea4\u6d41\u8bef\u89e3\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c3d\u7ba1\u7cbe\u786e\u5ea6\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2512.20292", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.20292", "abs": "https://arxiv.org/abs/2512.20292", "authors": ["Wenzheng Zeng", "Mingyu Ouyang", "Langyuan Cui", "Hwee Tou Ng"], "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers", "comment": "AAAI 2026 (with appendix)", "summary": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.", "AI": {"tldr": "SlideTailor\uff1a\u57fa\u4e8e\u7528\u6237\u504f\u597d\u7684\u8bba\u6587\u8f6c\u5e7b\u706f\u7247\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u793a\u4f8b\u5bf9\u548c\u89c6\u89c9\u6a21\u677f\u9690\u5f0f\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u5e7b\u706f\u7247\u751f\u6210", "motivation": "\u73b0\u6709\u81ea\u52a8\u5e7b\u706f\u7247\u751f\u6210\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u56e0\u4e3a\u7528\u6237\u504f\u597d\u591a\u6837\u4e14\u96be\u4ee5\u660e\u786e\u63cf\u8ff0\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0e\u7528\u6237\u671f\u671b\u4e0d\u7b26", "method": "\u63d0\u51faSlideTailor\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u8bba\u6587-\u5e7b\u706f\u7247\u793a\u4f8b\u5bf9\u548c\u89c6\u89c9\u6a21\u677f\u9690\u5f0f\u5b66\u4e60\u7528\u6237\u504f\u597d\uff1b2\uff09\u91c7\u7528\u94fe\u5f0f\u8bed\u97f3\u673a\u5236\u4f7f\u5e7b\u706f\u7247\u5185\u5bb9\u4e0e\u53e3\u5934\u53d9\u8ff0\u5bf9\u9f50\uff1b3\uff09\u6784\u5efa\u5305\u542b\u591a\u6837\u5316\u7528\u6237\u504f\u597d\u7684\u57fa\u51c6\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u4ece\u9690\u5f0f\u8f93\u5165\u4e2d\u63d0\u53d6\u548c\u6cdb\u5316\u7528\u6237\u504f\u597d\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u5e7b\u706f\u7247\u8d28\u91cf\uff0c\u5e76\u652f\u6301\u89c6\u9891\u6f14\u793a\u7b49\u4e0b\u6e38\u5e94\u7528", "conclusion": "SlideTailor\u901a\u8fc7\u81ea\u7136\u6613\u5f97\u7684\u8f93\u5165\uff08\u793a\u4f8b\u5bf9\u548c\u6a21\u677f\uff09\u6709\u6548\u6355\u6349\u7528\u6237\u504f\u597d\uff0c\u5b9e\u73b0\u4e86\u66f4\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u4e2a\u6027\u5316\u5e7b\u706f\u7247\u751f\u6210\uff0c\u4e3a\u81ea\u52a8\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2512.20293", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20293", "abs": "https://arxiv.org/abs/2512.20293", "authors": ["Jaykumar Kasundra", "Anjaneya Praharaj", "Sourabh Surana", "Lakshmi Sirisha Chodisetty", "Sourav Sharma", "Abhigya Verma", "Abhishek Bhardwaj", "Debasish Kanhar", "Aakash Bhagat", "Khalil Slimi", "Seganrasan Subramanian", "Sathwik Tejaswi Madhusudhan", "Ranga Prasad Chenna", "Srinivas Sunkara"], "title": "AprielGuard", "comment": null, "summary": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.", "AI": {"tldr": "AprielGuard\u662f\u4e00\u4e2a8B\u53c2\u6570\u7684\u5b89\u5168\u9632\u62a4\u6a21\u578b\uff0c\u7edf\u4e00\u5904\u7406LLM\u7684\u5b89\u5168\u98ce\u9669\u548c\u5bf9\u6297\u5a01\u80c1\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u9632\u62a4\u65b9\u6848\u3002", "motivation": "\u73b0\u6709LLM\u9632\u62a4\u5de5\u5177\u901a\u5e38\u5c06\u5b89\u5168\u98ce\u9669\uff08\u5982\u6bd2\u6027\u3001\u504f\u89c1\uff09\u548c\u5bf9\u6297\u5a01\u80c1\uff08\u5982\u63d0\u793a\u6ce8\u5165\u3001\u8d8a\u72f1\uff09\u89c6\u4e3a\u72ec\u7acb\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a8B\u53c2\u6570\u7684\u5b89\u5168\u9632\u62a4\u6a21\u578b\uff0c\u91c7\u7528\u7edf\u4e00\u5206\u7c7b\u548c\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u591a\u6837\u5316\u7684\u5f00\u653e\u548c\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u6db5\u76d6\u72ec\u7acb\u63d0\u793a\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u548c\u4e13\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAprielGuard\u5728\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u548c\u5bf9\u6297\u64cd\u4f5c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u9aa4\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\uff0c\u8d85\u8d8a\u4e86Llama-Guard\u548cGranite Guardian\u7b49\u73b0\u6709\u5f00\u6e90\u9632\u62a4\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u8be5\u6a21\u578b\uff0c\u65e8\u5728\u63a8\u8fdb\u5173\u4e8eLLM\u53ef\u9760\u9632\u62a4\u7684\u900f\u660e\u548c\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u4e3aLLM\u5728\u5bf9\u8bdd\u548c\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u4fdd\u969c\u3002"}}
{"id": "2512.20298", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20298", "abs": "https://arxiv.org/abs/2512.20298", "authors": ["Karolina Dro\u017cd\u017c", "Kacper Dudzic", "Anna Sterna", "Marcin Moskalewicz"], "title": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives", "comment": null, "summary": "Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.", "AI": {"tldr": "LLM\u5728\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u4e2d\u6574\u4f53\u51c6\u786e\u7387\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb621.91\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u5728\u81ea\u604b\u578b\u4eba\u683c\u969c\u788d\u8bca\u65ad\u4e0a\u8868\u73b0\u6781\u5dee\uff0c\u5b58\u5728\u4ef7\u503c\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u5728\u7cbe\u795e\u79d1\u81ea\u6211\u8bc4\u4f30\u4e2d\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u89e3\u91ca\u5b9a\u6027\u60a3\u8005\u53d9\u8ff0\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4eba\u683c\u969c\u788d\u8bca\u65ad\u65b9\u9762\u3002", "method": "\u4f7f\u7528\u6ce2\u5170\u8bed\u7b2c\u4e00\u4eba\u79f0\u81ea\u4f20\u4f53\u53d9\u8ff0\uff0c\u76f4\u63a5\u6bd4\u8f83\u6700\u5148\u8fdb\u7684LLM\u4e0e\u7cbe\u795e\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u5728\u8fb9\u7f18\u578b\u4eba\u683c\u969c\u788d\u548c\u81ea\u604b\u578b\u4eba\u683c\u969c\u788d\u8bca\u65ad\u4e0a\u7684\u8868\u73b0\u3002", "result": "Gemini Pro\u6a21\u578b\u6574\u4f53\u8bca\u65ad\u51c6\u786e\u738765.48%\uff0c\u6bd4\u4eba\u7c7b\u4e13\u5bb643.57%\u9ad821.91\u4e2a\u767e\u5206\u70b9\u3002\u5728\u8fb9\u7f18\u578b\u4eba\u683c\u969c\u788d\u8bca\u65ad\u4e0a\u8868\u73b0\u4f18\u5f02\uff08F1=83.4 vs 80.0\uff09\uff0c\u4f46\u5728\u81ea\u604b\u578b\u4eba\u683c\u969c\u788d\u8bca\u65ad\u4e0a\u4e25\u91cd\u4e0d\u8db3\uff08F1=6.7 vs 50.0\uff09\uff0c\u6a21\u578b\u5bf9\"\u81ea\u604b\"\u8fd9\u4e00\u4ef7\u503c\u8d1f\u8f7d\u672f\u8bed\u8868\u73b0\u51fa\u6297\u62d2\u3002", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u89e3\u91ca\u590d\u6742\u7684\u7b2c\u4e00\u4eba\u79f0\u4e34\u5e8a\u6570\u636e\uff0c\u4f46\u4ecd\u5b58\u5728\u4e25\u91cd\u7684\u53ef\u9760\u6027\u548c\u504f\u89c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4ef7\u503c\u5224\u65ad\u7684\u8bca\u65ad\u4e0a\u3002"}}
{"id": "2512.20308", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.20308", "abs": "https://arxiv.org/abs/2512.20308", "authors": ["Maxime Poli", "Mahi Luthra", "Youssef Benchekroun", "Yosuke Higuchi", "Martin Gleize", "Jiayi Shen", "Robin Algayres", "Yu-An Chung", "Mido Assran", "Juan Pino", "Emmanuel Dupoux"], "title": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision", "comment": "30 pages, 16 figures", "summary": "The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.", "AI": {"tldr": "SpidR\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u3001\u81ea\u84b8\u998f\u548c\u5728\u7ebf\u805a\u7c7b\u8bad\u7ec3\uff0c\u80fd\u9ad8\u6548\u5b66\u4e60\u5305\u542b\u4e30\u5bcc\u8bed\u97f3\u4fe1\u606f\u7684\u8868\u793a\uff0c\u7279\u522b\u9002\u5408\u65e0\u6587\u672c\u8bed\u97f3\u8bed\u8a00\u5efa\u6a21\uff0c\u5728\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u4e14\u8bad\u7ec3\u65f6\u95f4\u5927\u5e45\u7f29\u77ed\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u5efa\u6a21\u548c\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u7684\u5e76\u884c\u53d1\u5c55\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u76f4\u63a5\u4ece\u8bed\u97f3\u4e2d\u5b66\u4e60\u8bed\u8a00\u800c\u65e0\u9700\u6587\u672c\u4e2d\u95f4\u8868\u793a\uff0c\u8fd9\u9700\u8981\u4ece\u8bed\u97f3\u4e2d\u76f4\u63a5\u63d0\u53d6\u8bed\u4e49\u8868\u793a\u3002", "method": "\u63d0\u51faSpidR\u6a21\u578b\uff0c\u5728\u539f\u59cb\u6ce2\u5f62\u4e0a\u4f7f\u7528\u63a9\u7801\u9884\u6d4b\u76ee\u6807\uff0c\u7ed3\u5408\u81ea\u84b8\u998f\u548c\u5728\u7ebf\u805a\u7c7b\u8fdb\u884c\u8bad\u7ec3\u3002\u5b66\u751f\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u5b66\u4e60\u9884\u6d4b\u6765\u81ea\u6559\u5e08\u6a21\u578b\u4e2d\u95f4\u5c42\u7684\u5206\u914d\uff0c\u8fd9\u79cd\u5b66\u4e60\u76ee\u6807\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u7a33\u5b9a\u4e86\u5728\u7ebf\u805a\u7c7b\u8fc7\u7a0b\uff0c\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u7801\u672c\u3002", "result": "SpidR\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5(sWUGGY, sBLIMP, tSC)\u4e0a\u8d85\u8d8a\u4e86wav2vec 2.0\u3001HuBERT\u3001WavLM\u548cDinoSR\u3002\u7cfb\u7edf\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u8bed\u97f3\u5355\u5143\u8d28\u91cf\u6307\u6807(ABX, PNMI)\u4e0e\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002\u76f8\u6bd4HuBERT\u9700\u8981\u4e00\u5468\u8bad\u7ec3\uff0cSpidR\u4ec5\u970016\u4e2aGPU\u8bad\u7ec3\u4e00\u5929\uff0c\u5927\u5e45\u7f29\u77ed\u9884\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "SpidR\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u8868\u793a\u5b66\u4e60\uff0c\u4e3a\u65e0\u6587\u672c\u8bed\u97f3\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8bad\u7ec3\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.20324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20324", "abs": "https://arxiv.org/abs/2512.20324", "authors": ["Nurul Labib Sayeedi", "Md. Faiyaz Abdullah Sayeedi", "Khushnur Binte Jahangir", "Swakkhar Shatabda", "Sarah Masud Preum"], "title": "Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles", "comment": null, "summary": "Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BanglaRiddleEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,244\u4e2a\u5b5f\u52a0\u62c9\u8bed\u8c1c\u8bed\uff0c\u8bc4\u4f30LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6bd4\u55bb\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u8fdc\u672a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6NLP\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6bd4\u55bb\u6027\u3001\u6587\u5316\u76f8\u5173\u6027\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u5b5f\u52a0\u62c9\u8bed\u8fd9\u6837\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u6d41\u6c34\u7ebf\u751f\u6210\u601d\u7ef4\u94fe\u89e3\u91ca\u3001\u8bed\u4e49\u8fde\u8d2f\u7684\u5e72\u6270\u9879\u548c\u7ec6\u7c92\u5ea6\u6b67\u4e49\u6807\u6ce8\uff0c\u6784\u5efa\u5305\u542b4,976\u4e2a\u8c1c\u8bed\u4efb\u52a1\u5de5\u4ef6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u751f\u6210\u5f0f\u95ee\u7b54\u4e2d\u8fbe\u5230\u4e2d\u7b49\u8bed\u4e49\u91cd\u53e0\u4f46\u6b63\u786e\u7387\u4f4e\uff0c\u591a\u9879\u9009\u62e9\u9898\u51c6\u786e\u7387\u6700\u9ad8\u7ea656%\uff08\u4eba\u7c7b\u57fa\u7ebf83%\uff09\uff0c\u6b67\u4e49\u89e3\u51b3\u7387\u572826%-68%\u4e4b\u95f4\uff0c\u9ad8\u8d28\u91cf\u89e3\u91ca\u4ec5\u9650\u4e8e\u6700\u5f3a\u6a21\u578b\u3002", "conclusion": "\u5f53\u524dLLM\u80fd\u591f\u6355\u6349\u5b5f\u52a0\u62c9\u8bed\u8c1c\u8bed\u63a8\u7406\u7684\u4e00\u4e9b\u7ebf\u7d22\uff0c\u4f46\u8ddd\u79bb\u4eba\u7c7b\u6c34\u5e73\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\uff0cBanglaRiddleEval\u6210\u4e3a\u4f4e\u8d44\u6e90\u6bd4\u55bb\u63a8\u7406\u7684\u6311\u6218\u6027\u65b0\u57fa\u51c6\u3002"}}
{"id": "2512.20352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20352", "abs": "https://arxiv.org/abs/2512.20352", "authors": ["Nilesh Jain", "Seyi Adeyinka", "Leor Roseman", "Aza Allsop"], "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation", "comment": "11 pages, 1 figure, 3 tables", "summary": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($\u03ba$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($\u03ba= 0.907$, cosine=95.3%), followed by GPT-4o ($\u03ba= 0.853$, cosine=92.6%) and Claude ($\u03ba= 0.842$, cosine=92.1%). All three models achieve a high agreement ($\u03ba> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u500b\u591a\u8996\u89d2\u9a57\u8b49\u6846\u67b6\uff0c\u7d50\u5408\u96c6\u6210\u9a57\u8b49\u8207\u96d9\u91cd\u53ef\u9760\u6027\u6307\u6a19\uff08Cohen's Kappa\u548c\u9918\u5f26\u76f8\u4f3c\u5ea6\uff09\uff0c\u7528\u65bcLLM\u4e3b\u984c\u5206\u6790\uff0c\u5be6\u73fe\u9ad8\u53ef\u9760\u6027\u7684AI\u8f14\u52a9\u8cea\u6027\u7814\u7a76\u3002", "motivation": "\u50b3\u7d71\u8cea\u6027\u7814\u7a76\u7684\u7de8\u78bc\u8005\u9593\u4e00\u81f4\u6027\u65b9\u6cd5\u9700\u8981\u591a\u4eba\u53c3\u8207\u3001\u8017\u6642\u4e14\u4e00\u81f4\u6027\u901a\u5e38\u4e0d\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u500b\u66f4\u9ad8\u6548\u53ef\u9760\u7684AI\u8f14\u52a9\u8cea\u6027\u7814\u7a76\u65b9\u6cd5\u8ad6\u3002", "method": "\u958b\u767c\u591a\u8996\u89d2\u9a57\u8b49\u6846\u67b6\uff0c\u7d50\u5408\u96c6\u6210\u9a57\u8b49\u8207\u96d9\u91cd\u53ef\u9760\u6027\u6307\u6a19\uff1aCohen's Kappa\u7528\u65bc\u7de8\u78bc\u8005\u9593\u4e00\u81f4\u6027\uff0c\u9918\u5f26\u76f8\u4f3c\u5ea6\u7528\u65bc\u8a9e\u7fa9\u4e00\u81f4\u6027\u3002\u6846\u67b6\u652f\u6301\u53ef\u914d\u7f6e\u5206\u6790\u53c3\u6578\uff081-6\u7a2e\u5b50\uff0c\u6eab\u5ea60.0-2.0\uff09\u3001\u81ea\u5b9a\u7fa9\u63d0\u793a\u7d50\u69cb\u548c\u8b8a\u91cf\u66ff\u63db\uff0c\u4e26\u63d0\u4f9b\u8de8JSON\u683c\u5f0f\u7684\u5171\u8b58\u4e3b\u984c\u63d0\u53d6\u3002", "result": "\u5728\u8ff7\u5e7b\u85dd\u8853\u6cbb\u7642\u8a2a\u8ac7\u6587\u672c\u4e0a\u8a55\u4f30\u4e09\u500bLLM\uff08Gemini 2.5 Pro\u3001GPT-4o\u3001Claude 3.5 Sonnet\uff09\uff0c\u6bcf\u500b\u6a21\u578b\u9032\u884c\u516d\u6b21\u7368\u7acb\u904b\u884c\u3002Gemini\u7372\u5f97\u6700\u9ad8\u53ef\u9760\u6027\uff08\u03ba=0.907\uff0c\u9918\u5f26\u76f8\u4f3c\u5ea695.3%\uff09\uff0c\u6240\u6709\u6a21\u578b\u5747\u9054\u5230\u9ad8\u4e00\u81f4\u6027\uff08\u03ba>0.80\uff09\u3002\u6846\u67b6\u6210\u529f\u63d0\u53d6\u8de8\u904b\u884c\u7684\u5171\u8b58\u4e3b\u984c\uff0cGemini\u8b58\u52256\u500b\u4e3b\u984c\uff0cGPT-4o\u8b58\u52255\u500b\uff0cClaude\u8b58\u52254\u500b\u3002", "conclusion": "\u8a72\u6846\u67b6\u70ba\u53ef\u9760\u7684AI\u8f14\u52a9\u8cea\u6027\u7814\u7a76\u5efa\u7acb\u4e86\u65b9\u6cd5\u8ad6\u57fa\u790e\uff0c\u63d0\u4f9b\u900f\u660e\u53ef\u9760\u6027\u6307\u6a19\u3001\u9748\u6d3b\u914d\u7f6e\u548c\u7d50\u69cb\u7121\u95dc\u7684\u5171\u8b58\u63d0\u53d6\uff0c\u958b\u6e90\u5be6\u73fe\u652f\u6301\u7814\u7a76\u4eba\u54e1\u9032\u884c\u9ad8\u53ef\u9760\u6027\u7684\u8cea\u6027\u5206\u6790\u3002"}}
{"id": "2512.20404", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20404", "abs": "https://arxiv.org/abs/2512.20404", "authors": ["Junyi Liu", "Stanley Kok"], "title": "Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining", "comment": "WITS 2025 (Workshop on Information Technologies and Systems 2025)", "summary": "With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u60c5\u611f\u611f\u77e5\u7684\u6587\u672c\u6458\u8981\u6846\u67b6\uff0c\u5c06\u60c5\u611f\u4fe1\u53f7\u6574\u5408\u5230\u62bd\u53d6\u5f0f\uff08TextRank\uff09\u548c\u751f\u6210\u5f0f\uff08UniLM\uff09\u65b9\u6cd5\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u7b49\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u60c5\u611f\u4e30\u5bcc\u5185\u5bb9\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u5a92\u4f53\u3001\u8bc4\u8bba\u548c\u8bba\u575b\u7b49\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\uff0c\u6587\u672c\u6316\u6398\u5728\u4fe1\u606f\u7cfb\u7edf\uff08IS\uff09\u4e2d\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6458\u8981\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7ed3\u6784\u5316\u7684\u65b0\u95fb\u5185\u5bb9\u4f18\u5316\uff0c\u96be\u4ee5\u5904\u7406\u5608\u6742\u3001\u975e\u6b63\u5f0f\u7684\u7528\u6237\u751f\u6210\u6587\u672c\u3002\u60c5\u611f\u7ebf\u7d22\u5bf9\u4e8e\u54c1\u724c\u76d1\u63a7\u548c\u5e02\u573a\u5206\u6790\u7b49IS\u4efb\u52a1\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5c06\u60c5\u611f\u5efa\u6a21\u6574\u5408\u5230\u77ed\u6587\u672c\u6458\u8981\u4e2d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u60c5\u611f\u611f\u77e5\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u62bd\u53d6\u5f0f\uff08TextRank\uff09\u548c\u751f\u6210\u5f0f\uff08UniLM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u60c5\u611f\u4fe1\u53f7\u5d4c\u5165\u5230\u6392\u540d\u548c\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002\u8fd9\u79cd\u53cc\u91cd\u8bbe\u8ba1\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u60c5\u611f\u7ec6\u5fae\u5dee\u522b\u548c\u4e3b\u9898\u76f8\u5173\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u7b80\u6d01\u3001\u60c5\u611f\u4e30\u5bcc\u7684\u6458\u8981\uff0c\u589e\u5f3a\u4e86\u60c5\u611f\u7ec6\u5fae\u5dee\u522b\u548c\u4e3b\u9898\u76f8\u5173\u6027\u7684\u6355\u6349\uff0c\u4ece\u800c\u6539\u5584\u52a8\u6001\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u53ca\u65f6\u5e72\u9884\u548c\u6218\u7565\u51b3\u7b56\u3002", "conclusion": "\u60c5\u611f\u611f\u77e5\u7684\u6458\u8981\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u7ed3\u6784\u5316\u3001\u60c5\u611f\u4e30\u5bcc\u7684\u7528\u6237\u751f\u6210\u6587\u672c\uff0c\u4e3a\u4fe1\u606f\u7cfb\u7edf\u4e2d\u7684\u54c1\u724c\u76d1\u63a7\u3001\u5e02\u573a\u5206\u6790\u548c\u6218\u7565\u51b3\u7b56\u63d0\u4f9b\u66f4\u597d\u7684\u652f\u6301\u3002"}}
{"id": "2512.20491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20491", "abs": "https://arxiv.org/abs/2512.20491", "authors": ["Chen Hu", "Haikuo Du", "Heng Wang", "Lin Lin", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Tianchi Yue", "Wang You", "Wei Ji", "Wei Yuan", "Wenjin Deng", "Xiaojian Yuan", "Xiaoyun Zhang", "Xiangyu Liu", "Xikai Liu", "Yanming Xu", "Yicheng Cao", "Yifei Zhang", "Yongyao Wang", "Yubo Shu", "Yurong Zhang", "Yuxiang Zhang", "Zheng Gong", "Zhichao Chang", "Binyan Li", "Dan Ma", "Furong Jia", "Hongyuan Wang", "Jiayu Liu", "Jing Bai", "Junlan Liu", "Manjiao Liu", "Na Wang", "Qiuping Wu", "Qinxin Du", "Shiwei Li", "Wen Sun", "Yifeng Gong", "Yonglin Chen", "Yuling Zhao", "Yuxuan Lin", "Ziqi Ren", "Zixuan Wang", "Aihu Zhang", "Brian Li", "Buyun Ma", "Kang An", "Li Xie", "Mingliang Li", "Pan Li", "Shidong Yang", "Xi Chen", "Xiaojia Liu", "Yuchu Luo", "Yuan Song", "YuanHao Ding", "Yuanwei Liang", "Zexi Li", "Zhaoning Zhang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Yibo Zhu"], "title": "Step-DeepResearch Technical Report", "comment": null, "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.", "AI": {"tldr": "Step-DeepResearch\u662f\u4e00\u4e2a\u7528\u4e8e\u6df1\u5ea6\u7814\u7a76\u7684\u7aef\u5230\u7aef\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u539f\u5b50\u80fd\u529b\u6570\u636e\u5408\u6210\u7b56\u7565\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u8def\u5f84\uff0c\u5728\u4e2d\u6587\u9886\u57dfADR-Bench\u4e0a\u8868\u73b0\u51fa\u8272\uff0c32B\u6a21\u578b\u5728Scale AI Research Rubrics\u4e0a\u8fbe\u523061.4%\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u5b66\u672f\u57fa\u51c6\uff08\u5982BrowseComp\uff09\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u5bf9\u5f00\u653e\u5f0f\u6df1\u5ea6\u7814\u7a76\u7684\u9700\u6c42\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u610f\u56fe\u8bc6\u522b\u3001\u957f\u89c6\u91ce\u51b3\u7b56\u548c\u8de8\u6e90\u9a8c\u8bc1\u80fd\u529b\u3002\u540c\u65f6\u4e2d\u6587\u9886\u57df\u7f3a\u4e4f\u5408\u9002\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u539f\u5b50\u80fd\u529b\u7684\u6570\u636e\u5408\u6210\u7b56\u7565\u6765\u589e\u5f3a\u89c4\u5212\u548c\u62a5\u544a\u64b0\u5199\u80fd\u529b\uff1b2. \u91c7\u7528\u4ece\u667a\u80fd\u4f53\u4e2d\u671f\u8bad\u7ec3\u5230SFT\u548cRL\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u8def\u5f84\uff1b3. \u5f15\u5165\u6e05\u5355\u5f0f\u8bc4\u5224\u5668\u63d0\u9ad8\u9c81\u68d2\u6027\uff1b4. \u5efa\u7acb\u4e2d\u6587\u9886\u57dfADR-Bench\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "Step-DeepResearch\uff0832B\uff09\u5728Scale AI Research Rubrics\u4e0a\u83b7\u5f9761.4%\u5206\u6570\u3002\u5728ADR-Bench\u4e0a\u663e\u8457\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\uff0c\u5e76\u4e0eOpenAI\u3001Gemini DeepResearch\u7b49\u95ed\u6e90SOTA\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u7cbe\u7ec6\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\u4f7f\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u80fd\u591f\u4ee5\u884c\u4e1a\u9886\u5148\u7684\u6210\u672c\u6548\u76ca\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.20569", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20569", "abs": "https://arxiv.org/abs/2512.20569", "authors": ["Yanhong Li", "Songlin Yang", "Shawn Tan", "Mayank Mishra", "Rameswar Panda", "Jiawei Zhou", "Yoon Kim"], "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection", "comment": null, "summary": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c42\u91cd\u8981\u6027\u5206\u6570\u7684\u7b80\u5355\u9ad8\u6548\u5c42\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u9884\u8bad\u7ec3softmax\u6ce8\u610f\u529bTransformer\u84b8\u998f\u4e3a\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u4e86RADLADS\u84b8\u998f\u6d41\u7a0b\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c06\u9884\u8bad\u7ec3\u7684softmax\u6ce8\u610f\u529bTransformer\u84b8\u998f\u4e3a\u6df7\u5408\u67b6\u6784\uff08\u4ea4\u66ff\u4f7f\u7528softmax\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u5c42\uff09\u662f\u63d0\u9ad8LLM\u63a8\u7406\u6548\u7387\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4f46\u5173\u952e\u6311\u6218\u5728\u4e8e\u5982\u4f55\u9009\u62e9\u54ea\u4e9b\u5c42\u8f6c\u6362\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u53d8\u4f53\u3002", "method": "1. \u4f7f\u7528\u5c11\u91cf\u901a\u7528\u6587\u672c\u6570\u636e\u8bad\u7ec3\u5f97\u5230\u5c42\u91cd\u8981\u6027\u5206\u6570\uff1b2. \u57fa\u4e8e\u8fd9\u4e9b\u5206\u6570\u8fdb\u884c\u5c42\u9009\u62e9\uff1b3. \u91c7\u7528RADLADS\u84b8\u998f\u6d41\u7a0b\uff1a\u6ce8\u610f\u529b\u6743\u91cd\u8f6c\u79fb\u3001\u9690\u85cf\u72b6\u6001\u5bf9\u9f50\u3001KL\u5206\u5e03\u5339\u914d\uff0c\u6700\u540e\u8fdb\u884c\u5c11\u91cf\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u5c42\u9009\u62e9\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5305\u62ec\u57fa\u4e8e\u56fa\u5b9a\u6bd4\u4f8b\u5747\u5300\u4ea4\u66ff\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4f9d\u8d56\u4e13\u95e8\u8bca\u65ad\u6570\u636e\u96c6\u7684\u590d\u6742\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u7684\u5c42\u91cd\u8981\u6027\u8bc4\u5206\u548cRADLADS\u84b8\u998f\u6d41\u7a0b\uff0c\u53ef\u4ee5\u9ad8\u6548\u5730\u5c06\u9884\u8bad\u7ec3Transformer\u8f6c\u6362\u4e3a\u6df7\u5408\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2512.20578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20578", "abs": "https://arxiv.org/abs/2512.20578", "authors": ["Amirhosein Ghasemabadi", "Di Niu"], "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits", "comment": null, "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.", "AI": {"tldr": "Gnosis\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u81ea\u611f\u77e5\u673a\u5236\uff0c\u901a\u8fc7\u89e3\u7801LLM\u5185\u90e8\u72b6\u6001\u6765\u9884\u6d4b\u81ea\u8eab\u9519\u8bef\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\uff0c\u4ec5\u589e\u52a0\u7ea6500\u4e07\u53c2\u6570", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u6d41\u7545\u590d\u6742\u7684\u8f93\u51fa\uff0c\u4f46\u5e38\u5e38\u65e0\u6cd5\u8bc6\u522b\u81ea\u5df1\u7684\u9519\u8bef\u548c\u5e7b\u89c9\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8bc4\u5224\u5668\u3001\u591a\u6837\u672c\u4e00\u81f4\u6027\u6216\u57fa\u4e8e\u6587\u672c\u7684\u81ea\u6211\u6279\u8bc4\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u8981\u4e48\u4e0e\u771f\u5b9e\u6b63\u786e\u6027\u76f8\u5173\u6027\u8f83\u5f31\u3002\u4f5c\u8005\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5185\u90e8\u72b6\u6001\u6765\u9884\u6d4bLLM\u81ea\u8eab\u7684\u5931\u8d25\u3002", "method": "\u63d0\u51faGnosis\u673a\u5236\uff1a\u88ab\u52a8\u89c2\u5bdfLLM\u7684\u5185\u90e8\u8f68\u8ff9\uff08\u9690\u85cf\u72b6\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\uff0c\u5c06\u5176\u538b\u7f29\u4e3a\u56fa\u5b9a\u9884\u7b97\u7684\u63cf\u8ff0\u7b26\uff0c\u7136\u540e\u9884\u6d4b\u6b63\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u4ec5\u589e\u52a0\u7ea6500\u4e07\u53c2\u6570\uff0c\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\uff0c\u63a8\u7406\u6210\u672c\u53ef\u5ffd\u7565\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u5f00\u653e\u57df\u95ee\u7b54\u548c\u5b66\u672f\u77e5\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57281.7B\u523020B\u53c2\u6570\u7684\u51bb\u7ed3\u9aa8\u5e72\u6a21\u578b\u4e0a\uff0cGnosis\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u5185\u90e8\u57fa\u7ebf\u548c\u5927\u578b\u5916\u90e8\u8bc4\u5224\u5668\u3002\u6b64\u5916\uff0c\u5b83\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u90e8\u5206\u751f\u6210\uff0c\u5b9e\u73b0\u65e9\u671f\u5931\u8d25\u8f68\u8ff9\u68c0\u6d4b\u548c\u8ba1\u7b97\u611f\u77e5\u63a7\u5236\u3002", "conclusion": "\u53ef\u9760\u7684\u6b63\u786e\u6027\u7ebf\u7d22\u5185\u5728\u4e8e\u751f\u6210\u8fc7\u7a0b\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u9ad8\u6548\u63d0\u53d6\u3002Gnosis\u5c55\u793a\u4e86LLM\u901a\u8fc7\u5185\u90e8\u72b6\u6001\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\u7684\u6f5c\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.20595", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20595", "abs": "https://arxiv.org/abs/2512.20595", "authors": ["Dhruv Anand", "Ehsan Shareghi"], "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs", "comment": "27 pages, 5 figures, 9 tables. Cube available at https://github.com/dana-23/cube-bench", "summary": "We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.", "AI": {"tldr": "Cube Bench\u662f\u4e00\u4e2a\u57fa\u4e8e\u9b54\u65b9\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u7a7a\u95f4\u548c\u5e8f\u5217\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b\u4e94\u4e2a\u6280\u80fd\u7ef4\u5ea6\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u6a21\u578b\u6027\u80fd\u968f\u9b54\u65b9\u590d\u6742\u5ea6\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff0c\u95ed\u6e90\u6a21\u578b\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u548c\u5e8f\u5217\u63a8\u7406\u65b9\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u7a7a\u95f4\u4efb\u52a1\u4e2d\u591a\u6b65\u9aa4\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u521b\u5efaCube Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u9b54\u65b9\u89e3\u8c1c\u4efb\u52a1\u5206\u89e3\u4e3a\u4e94\u4e2a\u6280\u80fd\u7ef4\u5ea6\uff1a\u56fe\u50cf\u548c\u6587\u672c\u91cd\u5efa\u9b54\u65b9\u9762\u3001\u9009\u62e9\u6700\u4f18\u4e0b\u4e00\u6b65\u3001\u9884\u6d4b\u5019\u9009\u79fb\u52a8\u7ed3\u679c\u3001\u6267\u884c\u591a\u6b65\u9aa4\u8ba1\u5212\u5e76\u7ea0\u9519\u3001\u68c0\u6d4b\u548c\u4fee\u6b63\u81ea\u8eab\u9519\u8bef\u3002\u4f7f\u7528\u7edf\u4e00\u7684\u9b54\u65b9\u72b6\u6001\u3001\u63d0\u793a\u8bcd\u548c\u89e3\u6790\u5668\uff0c\u4ee5\u53ca\u5355\u4e00\u7684\u8ddd\u79bb\u89e3\u51b3\u5ea6\u91cf\u6807\u51c6\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4e03\u4e2a\u6d4b\u8bd5\u7684MLLM\u6a21\u578b\u51c6\u786e\u7387\u968f\u9b54\u65b9\u590d\u6742\u5ea6\u6df1\u5ea6\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff1b\u6a21\u578b\u4e00\u65e6\u8f68\u8ff9\u505c\u6ede\u6216\u53d1\u6563\u5c31\u5f88\u5c11\u80fd\u6062\u590d\uff1b\u9ad8\u9762\u91cd\u5efa\u51c6\u786e\u7387\u4e0d\u80fd\u4fdd\u8bc1\u826f\u597d\u7684\u52a8\u4f5c\u9009\u62e9\u6216\u591a\u6b65\u9aa4\u6267\u884c\u80fd\u529b\uff1b\u95ed\u6e90\u6a21\u578b\u5728\u5355\u6b65\u611f\u77e5\u548c\u591a\u6b65\u63a7\u5236\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff1b\u7b80\u5355\u7684\u81ea\u6211\u6821\u6b63\u65b9\u6cd5\u80fd\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\u4f46\u4e5f\u53ef\u80fd\u5f15\u5165\u8fc7\u5ea6\u601d\u8003\u3002", "conclusion": "Cube Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30MLLM\u7684\u5e8f\u5217\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5f00\u6e90\u6a21\u578b\u4e0e\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2512.20604", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20604", "abs": "https://arxiv.org/abs/2512.20604", "authors": ["Alexandros Christoforos", "Chadbourne Davis"], "title": "MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts", "comment": "Under submission", "summary": "We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.", "AI": {"tldr": "MoE-DiffuSeq\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u63d0\u5347\u957f\u6587\u6863\u751f\u6210\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u751f\u6210\u6a21\u578b\uff08\u5982DiffuSeq\uff09\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5f00\u9500\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u957f\u6587\u6863\u751f\u6210\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "1. \u96c6\u6210\u7a00\u758f\u6ce8\u610f\u529b\u4e0e\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u957f\u5e8f\u5217\u5efa\u6a21\uff1b2. \u8bbe\u8ba1\u5b9a\u5236\u5316\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u6587\u672c\u8d28\u91cf\u548c\u8fde\u8d2f\u6027\uff1b3. \u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u8f6f\u5438\u6536\u72b6\u6001\uff0c\u52a0\u901f\u5e8f\u5217\u91cd\u5efa\u5e76\u63d0\u9ad8\u751f\u6210\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoE-DiffuSeq\u76f8\u6bd4\u73b0\u6709\u6269\u6563\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u91c7\u6837\u901f\u5ea6\uff0c\u5728\u79d1\u5b66\u6587\u7ae0\u751f\u6210\u3001\u4ee3\u7801\u5e93\u5efa\u6a21\u548c\u957f\u5bf9\u8bdd\u751f\u6210\u7b49\u957f\u6587\u6863\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u6548\u7387\u3001\u901f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u8868\u8fbe\u80fd\u529b\u65b9\u9762\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "MoE-DiffuSeq\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u9ad8\u8d28\u91cf\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
