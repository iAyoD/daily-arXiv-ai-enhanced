<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.RO](#cs.RO) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data](https://arxiv.org/abs/2512.19864)
*Shashi Kant Gupta,Arijeet Pramanik,Jerrin John Thomas,Regina Schwind,Lauren Wiener,Avi Raju,Jeremy Kornbluth,Yanshan Wang,Zhaohui Su,Hrituraj Singh*

Main category: cs.CL

TL;DR: 提出基于LLM的智能代理框架，从电子健康记录中大规模提取结构化肿瘤学数据，在40万份临床笔记上实现高精度提取，显著降低人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的非结构化临床笔记包含丰富的肿瘤治疗信息，但现有自动化方法存在局限性：要么使用合成数据集，要么只关注文档级提取或特定临床变量，无法处理大量临床文档中的矛盾信息。人工标注虽然准确但成本过高且不可扩展。

Method: 提出智能代理框架，将复杂的肿瘤数据提取任务分解为模块化、自适应的子任务。使用大型语言模型作为推理代理，配备上下文敏感检索和迭代合成能力，从真实世界的肿瘤学笔记中全面提取结构化临床变量。

Result: 在包含2250名癌症患者的40多万份非结构化临床笔记和PDF报告的大规模数据集上评估，平均F1分数达到0.93，103个肿瘤特异性临床变量中有100个超过0.85，关键变量（如生物标志物和药物）超过0.95。集成到数据整理工作流中实现了0.94的直接人工批准率，显著降低了标注成本。

Conclusion: 这是首个基于LLM代理的端到端、大规模结构化肿瘤数据提取应用，能够从真实世界的临床文档中全面、准确地提取肿瘤学信息，为临床决策和研究提供了可扩展的解决方案。

Abstract: Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale

</details>


### [2] [How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse](https://arxiv.org/abs/2512.19903)
*Kirk Vanacore,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: LLMs在零样本设置下对真实课堂指令分类表现中等，但通过少样本提示可显著提升性能，最佳配置达到Cohen's Kappa=0.58，但仍存在可靠性限制。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在教育技术中的广泛应用，了解其在未经定制的情况下对真实教育场景的解读能力至关重要，这有助于设定合理预期和建立基准。

Method: 比较了六个LLM在真实课堂转录本中分类教学指令的基础性能，评估了零样本、单样本和少样本三种典型提示方法。

Result: 零样本表现中等，但少样本提示显著提升了最先进模型的性能，最佳配置达到Cohen's Kappa=0.58；性能因教学指令类型而异，高召回率常以增加误报为代价。

Conclusion: 基础模型在解读教学话语方面表现出有意义但有限的能力，提示设计有助于挖掘潜力但不能消除根本的可靠性限制。

Abstract: Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.

</details>


### [3] [Counterfactual LLM-based Framework for Measuring Rhetorical Style](https://arxiv.org/abs/2512.19908)
*Jingyi Qiu,Hong Chen,Zongyi Li*

Main category: cs.CL

TL;DR: 论文提出基于LLM的反事实框架量化机器学习论文中的修辞风格，发现愿景式修辞能显著预测引用和媒体关注度，且2023年后修辞强度因LLM写作辅助而急剧上升。


<details>
  <summary>Details</summary>
Motivation: AI领域对机器学习论文中的"炒作"现象日益关注，但现有方法难以将修辞风格与实质内容分离。由于大胆语言可能源于强实证结果或仅是修辞风格，区分两者具有挑战性。

Method: 提出反事实的LLM框架：多个LLM修辞角色从相同实质内容生成反事实写作，LLM评委通过成对比较评估，使用Bradley-Terry模型聚合结果。应用于2017-2025年8,485篇ICLR投稿，生成超过25万篇反事实写作。

Result: 愿景式修辞能显著预测下游关注度（包括引用和媒体关注），即使控制同行评审评价后依然有效。观察到2023年后修辞强度急剧上升，实证证据表明这主要由LLM写作辅助驱动。框架可靠性通过角色选择鲁棒性和LLM判断与人工标注高相关性验证。

Conclusion: LLM可作为测量和改进科学评估的工具，能够有效量化修辞风格与实质内容的分离，为科学写作评估提供新方法。

Abstract: The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.

</details>


### [4] [PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation](https://arxiv.org/abs/2512.19933)
*Zhixiang Lu,Xueyuan Deng,Yiran Liu,Yulong Li,Qiang Yan,Imran Razzak,Jionglong Su*

Main category: cs.CL

TL;DR: PRISM是一个结合随机微分方程和个性条件部分可观察马尔可夫决策过程的混合框架，用于模拟社交媒体中的意见动态，通过MBTI个性分配提高模型真实性。


<details>
  <summary>Details</summary>
Motivation: 传统基于代理的意见动态模型因假设同质性而无法捕捉在线极化的心理异质性，这阻碍了对意识形态分歧放大机制的理解。

Method: 提出PRISM混合框架：使用随机微分方程模拟连续情感演化，结合个性条件部分可观察马尔可夫决策过程处理离散决策。为多模态大语言模型代理分配基于MBTI的认知策略，并通过大规模社交媒体数据集进行数据驱动的初始化。

Result: PRISM在个性一致性方面显著优于标准同质模型和Big Five基准，与人类真实数据更吻合。该框架能有效复制理性抑制和情感共鸣等涌现现象。

Conclusion: PRISM为分析复杂社交媒体生态系统提供了强大工具，通过整合心理异质性更好地模拟意见动态和极化现象。

Abstract: Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.

</details>


### [5] [Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems](https://arxiv.org/abs/2512.19950)
*Heet Bodara,Md Masum Mushfiq,Isma Farah Siddiqui*

Main category: cs.CL

TL;DR: 该研究探索大语言模型在对话系统中的语调偏见问题，通过可控对话合成和语调分类模型，发现即使中性提示生成的对话也存在系统性语调偏差，并开发了有效的检测方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数字个人助理等对话系统中广泛应用，虽然回答流畅自然，但常常带有微妙的语调偏见（如过度礼貌、欢快或谨慎），这些偏见会影响用户对信任、同理心和公平性的感知，需要系统性地检测和理解。

Method: 1. 创建两个合成对话数据集：一个由中性提示生成，另一个明确引导生成积极或消极语调；2. 使用预训练的DistilBERT模型进行弱监督标注；3. 训练多个分类器检测语调模式；4. 采用集成模型提高检测效果。

Result: 1. 即使中性提示生成的对话集也显示出一致的语调偏斜，表明偏见源于模型底层的对话风格；2. 集成模型实现了高达0.92的宏观F1分数，证明语调偏见是系统性、可测量的；3. 方法能够有效检测对话中的语调偏见模式。

Conclusion: 语调偏见是大语言模型隐藏的行为特征，通过可控对话合成与语调分类模型的结合，可以稳健且合乎伦理地识别个人助理交互中的情感倾向，这对于设计公平可信的对话AI具有重要意义。

Abstract: Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.

</details>


### [6] [Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/abs/2512.19995)
*Ming Li,Chenrui Fan,Yize Cheng,Soheil Feizi,Tianyi Zhou*

Main category: cs.CL

TL;DR: 提出ThinkARM框架，将大语言模型的推理轨迹抽象为功能性推理步骤（如分析、探索、实现、验证等），用于系统分析数学问题解决中的思维动态和结构差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型越来越多地展示推理轨迹，但其底层的认知结构和步骤难以通过表面统计来识别和分析。需要一种中间尺度的框架来明确抽象推理步骤，以便系统分析推理的结构和动态。

Method: 采用Schoenfeld的Episode Theory作为归纳性中间尺度视角，引入ThinkARM框架，将推理轨迹显式抽象为功能性推理步骤。应用于不同模型在数学问题解决中的表现，并通过两个诊断案例研究分析探索步骤的关键作用和效率导向方法的影响。

Result: 该抽象揭示了可重复的思维动态和推理模型与非推理模型之间的结构差异，这些差异在token级视图中不明显。探索步骤作为与正确性相关的关键分支步骤，而效率导向方法选择性地抑制评估反馈步骤而非均匀缩短响应。

Conclusion: Episode级表示使推理步骤显式化，能够系统分析现代语言模型中推理的结构、稳定性和变化方式，为理解模型推理过程提供了新的分析工具。

Abstract: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.

</details>


### [7] [Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents](https://arxiv.org/abs/2512.20092)
*Yiming Du,Baojun Wang,Yifan Xiang,Zhaowei Wang,Wenyu Huang,Boyang Xue,Bin Liang,Xingshan Zeng,Fei Mi,Haoli Bai,Lifeng Shang,Jeff Z. Pan,Yuxin Jiang,Kam-Fai Wong*

Main category: cs.CL

TL;DR: Memory-T1：基于强化学习的时间感知记忆选择框架，用于长对话历史中的时序推理，通过粗到细策略和三级奖励函数提升时序一致性


<details>
  <summary>Details</summary>
Motivation: 现有长上下文模型在处理长对话历史时难以准确识别时序相关信息，导致推理性能显著下降。随着对话历史增长和噪声累积，时序推理能力成为对话智能体的关键瓶颈。

Method: 提出Memory-T1框架：1）粗筛选：使用时序和相关性过滤器将对话历史剪枝为候选集；2）细选择：基于强化学习的智能体精确选择证据会话；3）三级奖励函数：优化答案准确性、证据基础和时序一致性，其中时序一致性奖励提供密集信号，评估会话级（时序邻近性）和话语级（时序保真度）的对齐。

Result: 在Time-Dialog基准测试中，Memory-T1将7B模型提升至67.0%的总体得分，创下开源模型新SOTA，比14B基线高出10.2%。消融研究表明时序一致性和证据基础奖励共同贡献15.0%的性能提升。框架在128k令牌长度下仍保持鲁棒性，而基线模型已失效。

Conclusion: Memory-T1通过强化学习的时间感知记忆选择策略，有效解决了长对话历史中的时序推理挑战，显著提升了模型在噪声环境下的时序一致性和推理准确性，为对话智能体的长上下文处理提供了有效解决方案。

Abstract: Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/

</details>


### [8] [A Novel Graph-Sequence Learning Model for Inductive Text Classification](https://arxiv.org/abs/2512.20097)
*Zuo Wang,Ye Yuan*

Main category: cs.CL

TL;DR: TextGSL：一种新颖的图序列学习模型，通过构建多边类型文本图并集成Transformer层，解决现有GNN文本分类方法中结构信息利用不足和序列信息缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的文本分类方法存在两个主要局限：1）未能充分考虑词对之间的多样化结构信息（如共现、句法、语义关系）；2）在文本图结构学习模块中忽略了序列信息，无法处理包含新词和新关系的文本。

Method: 提出TextGSL模型：1）为每个文本构建单文本级图，基于词对间的多样化关系建立不同的边类型；2）设计自适应多边消息传递范式来聚合词对间的多样化结构信息；3）通过集成Transformer层来捕捉文本数据的序列信息。

Result: 在多个基准数据集上的实验结果表明，TextGSL在准确率方面优于多个强基线模型，证明了其学习更具判别性文本表示的能力。

Conclusion: TextGSL通过结合多边图结构和序列信息学习，有效解决了现有GNN文本分类方法的局限性，在文本分类任务中取得了更好的性能。

Abstract: Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.

</details>


### [9] [ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language](https://arxiv.org/abs/2512.20111)
*Aly Lidayan,Jakob Bjorner,Satvik Golechha,Kartik Goyal,Alane Suhr*

Main category: cs.CL

TL;DR: ABBEL框架通过语言表达的信念瓶颈来压缩多步决策任务的历史，使用强化学习训练LLM生成和基于信念行动，在保持近恒定内存使用的同时超越完整上下文性能。


<details>
  <summary>Details</summary>
Motivation: 随着序列决策任务长度增加，在上下文中保持完整交互历史变得计算上不切实际，需要一种既能压缩历史又能保持性能的方法。

Method: 提出ABBEL框架：用自然语言信念状态（任务相关未知信息的摘要）替代长交互历史；每步先更新先验信念形成后验信念，然后仅基于后验选择行动；使用强化学习训练LLM生成和基于信念行动，包括信念质量评分和长度惩罚。

Result: ABBEL支持生成可解释信念，同时保持近恒定的内存使用；但瓶颈方法易受错误传播影响，初始性能低于完整上下文设置；通过RL训练后，ABBEL性能超越完整上下文设置，且内存使用少于同期方法。

Conclusion: ABBEL框架通过信念瓶颈有效压缩多步交互历史，结合RL训练能够超越完整上下文性能，为长序列决策任务提供了内存高效且性能优越的解决方案。

Abstract: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.

</details>


### [10] [M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2512.20136)
*Hyeongcheol Park,Jiyoung Seo,Jaewon Mun,Hogun Park,Wonmin Byeon,Sung June Kim,Hyeonsoo Im,JeungSub Lee,Sangpil Kim*

Main category: cs.CL

TL;DR: 提出M³KG-RAG，一种多跳多模态知识图谱增强的检索增强生成方法，通过构建多跳MMKG和GRASP机制，提升多模态大语言模型在音频-视觉领域的推理深度和答案忠实度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态RAG在音频-视觉领域面临两个主要挑战：1）现有多模态知识图谱（MMKGs）的模态覆盖有限且多跳连接不足；2）仅基于共享多模态嵌入空间的相似性检索，无法过滤无关或冗余知识。

Method: 1）设计轻量级多智能体流水线构建多跳MMKG（M³KG），包含上下文丰富的多模态实体三元组；2）提出GRASP（Grounded Retrieval And Selective Pruning）机制，确保实体与查询的精确对齐，评估答案支持相关性，并修剪冗余上下文。

Result: 在多个多模态基准测试上的广泛实验表明，M³KG-RAG显著提升了多模态大语言模型的多模态推理和基础能力，优于现有方法。

Conclusion: M³KG-RAG通过构建多跳多模态知识图谱和引入GRASP机制，有效解决了音频-视觉领域多模态RAG的局限性，提升了模型的知识检索质量和推理能力。

Abstract: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.

</details>


### [11] [Multi-hop Reasoning via Early Knowledge Alignment](https://arxiv.org/abs/2512.20144)
*Yuxin Wang,Shicheng Fang,Bo Wang,Qi Luo,Xuanjing Huang,Yining Zheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: EKA通过早期知识对齐模块，在迭代RAG系统中让LLM在规划前先与检索集对齐，显著提升检索精度、减少级联错误，提高性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有迭代RAG系统在规划问题分解时未充分利用检索语料库信息，导致检索效率低下和推理链级联错误，影响性能。

Method: 提出Early Knowledge Alignment (EKA)模块，在迭代RAG系统中让LLM在规划前先与检索集对齐，利用上下文相关的检索知识建立更强推理基础。

Result: 在六个标准RAG数据集上的实验表明，EKA显著提升检索精度、减少级联错误，提高性能与效率。从熵角度分析显示早期知识减少了推理过程中的不必要探索。

Conclusion: EKA作为无需训练的多功能推理策略，可无缝扩展到大型模型，在迭代RAG系统中推进了最先进水平，阐明了结构化推理与高效探索之间的关键相互作用。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.

</details>


### [12] [Retrieval-augmented Prompt Learning for Pre-trained Foundation Models](https://arxiv.org/abs/2512.20145)
*Xiang Chen,Yixin Ou,Quan Feng,Lei Li,Piji Li,Haibo Ye,Sheng-Jun Huang,Shuofei Qiao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: RetroPrompt是一种新的提示学习方法，通过检索机制从知识库中获取上下文信息，在记忆与泛化之间取得平衡，提升PFM在零样本和少样本场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统提示学习方法仍遵循参数化学习范式，在记忆和死记硬背的泛化稳定性方面存在不足，难以充分利用非典型实例并避免在有限数据下对浅层模式的过拟合。

Method: 提出RetroPrompt方法，通过从训练数据生成的公开知识库中检索相关上下文信息，在输入、训练和推理阶段都融入检索机制，从而将知识从单纯记忆中解耦。

Result: 在自然语言处理和计算机视觉任务的多数据集上进行了全面实验，证明RetroPrompt在零样本和少样本场景下具有优越性能，有效减少对死记硬背的依赖并增强泛化能力。

Conclusion: RetroPrompt通过检索机制平衡记忆与泛化，显著提升预训练基础模型在少样本学习中的性能，为提示学习提供了新的有效范式。

Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.

</details>


### [13] [Fun-Audio-Chat Technical Report](https://arxiv.org/abs/2512.20156)
*Qian Chen,Luyao Cheng,Chong Deng,Xiangang Li,Jiaqing Liu,Chao-Hong Tan,Wen Wang,Junhao Xu,Jieping Ye,Qinglin Zhang,Qiquan Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: Fun-Audio-Chat是一个大型音频语言模型，通过双分辨率语音表示和核心鸡尾酒训练解决了语音-文本模型中的分辨率不匹配和灾难性遗忘问题，在语音理解和生成任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有联合语音-文本模型面临三个关键挑战：1）语音标记（25Hz）和文本标记（~3Hz）之间的时间分辨率不匹配会稀释语义信息；2）高计算成本；3）导致文本LLM知识的灾难性遗忘。这些限制了语音交互的质量和效率。

Method: 采用两种创新方法：1）双分辨率语音表示（DRSR）：共享LLM以高效的5Hz处理音频（通过标记分组），而语音精炼头以25Hz生成高质量标记，平衡效率和质量；2）核心鸡尾酒训练：两阶段微调与中间合并，减轻灾难性遗忘。然后应用多任务DPO训练增强鲁棒性、音频理解、指令跟随和语音共情能力。

Result: Fun-Audio-Chat 8B和MoE 30B-A3B在语音到文本和语音到语音任务上表现优异，在口语QA基准测试中在相似规模模型中排名前列。在音频理解、语音功能调用、指令跟随和语音共情方面也表现出竞争性甚至优越的性能。还开发了全双工变体Fun-Audio-Chat-Duplex。

Conclusion: Fun-Audio-Chat通过创新的双分辨率架构和多阶段训练策略，有效解决了现有语音-文本模型的关键限制，在保持文本LLM知识的同时获得了强大的音频理解、推理和生成能力，无需大规模音频-文本预训练。

Abstract: Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.

</details>


### [14] [AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications](https://arxiv.org/abs/2512.20164)
*Honglin Mu,Jinghao Liu,Kaiyang Wan,Rui Xing,Xiuying Chen,Timothy Baldwin,Wanxiang Che*

Main category: cs.CL

TL;DR: LLMs在简历筛选中存在对抗指令攻击漏洞，攻击成功率超80%。本文提出FIDS防御方法，结合LoRA适应，相比提示防御效果更好，综合防御可降低26.3%攻击成功率。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码审查、内容审核等任务中表现出色，但研究发现它们容易受到隐藏在输入数据（如简历、代码）中的"对抗指令"操纵，导致偏离预期任务。在简历筛选等应用中，针对这种漏洞的防御措施往往缺失。

Method: 1) 建立评估简历筛选漏洞的基准测试；2) 评估两种防御机制：基于提示的防御和提出的FIDS（通过分离检测外来指令）方法，后者使用LoRA适应技术；3) 结合两种防御方法进行综合评估。

Result: 1) 某些攻击类型的成功率超过80%；2) 提示防御降低10.1%攻击率，但增加12.5%误拒率；3) FIDS方法降低15.4%攻击率，增加10.4%误拒率；4) 结合方法降低26.3%攻击率，训练时防御在安全性和实用性保护方面优于推理时缓解。

Conclusion: LLMs在简历筛选等应用中存在对抗指令攻击的严重漏洞，需要专门的防御机制。FIDS方法结合LoRA适应能有效检测和防御此类攻击，训练时防御策略比推理时缓解措施更有效，为实际应用提供了更好的安全保障。

Abstract: Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by "adversarial instructions" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.

</details>


### [15] [FaithLens: Detecting and Explaining Faithfulness Hallucination](https://arxiv.org/abs/2512.20182)
*Shuzheng Si,Qingyi Wang,Haozhe Zhao,Yuzhuo Bai,Guanqiao Chen,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: FaithLens是一个用于检测大语言模型输出中忠实性幻觉的高效模型，能够同时提供二元预测和解释，在12个任务上超越GPT-4等先进模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在检索增强生成和摘要等实际应用中经常出现忠实性幻觉问题，需要高效可靠的检测方法来提高可信度。

Method: 1) 使用先进LLM合成带解释的训练数据，并通过数据过滤策略确保标签正确性、解释质量和数据多样性；2) 在精心策划的训练数据上进行微调作为冷启动；3) 使用基于规则的强化学习进一步优化，奖励预测正确性和解释质量。

Result: 在12个多样化任务上，8B参数的FaithLens超越了GPT-4.1和o3等先进模型，能够产生高质量的解释，在可信度、效率和有效性之间实现了独特的平衡。

Conclusion: FaithLens提供了一个成本效益高且有效的解决方案，用于检测LLM输出中的忠实性幻觉，同时提供解释以提高可信度，在实际应用中具有重要价值。

Abstract: Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.

</details>


### [16] [Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings](https://arxiv.org/abs/2512.20204)
*Marko Čechovič,Natália Komorníková,Dominik Macháček,Ondřej Bojar*

Main category: cs.CL

TL;DR: 该研究创建了一个跨语言对话语料库，用于评估自动语音翻译系统在无共同语言者会议中的应用，并提出了基于大语言模型的误解自动检测方法。


<details>
  <summary>Details</summary>
Motivation: 需要构建一个真实、多功能的评估语料库来评估自动系统在无共同语言者会议中的表现，同时研究跨语言交流中的误解检测问题。

Method: 创建包含12种语言的5小时跨语言对话语料库，包含语音录音、ASR转录、人工校对翻译和会议纪要。同时手动标注误解，并测试大语言模型（如Gemini）的自动误解检测能力。

Result: 构建了包含多语言对话的实用语料库。在误解检测任务中，Gemini模型能够以77%的召回率和47%的精确度识别文本中的误解片段。

Conclusion: 该研究提供了一个有价值的跨语言对话评估资源，并证明大语言模型在检测跨语言交流误解方面具有潜力，尽管精确度仍有提升空间。

Abstract: Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.
  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.

</details>


### [17] [SlideTailor: Personalized Presentation Slide Generation for Scientific Papers](https://arxiv.org/abs/2512.20292)
*Wenzheng Zeng,Mingyu Ouyang,Langyuan Cui,Hwee Tou Ng*

Main category: cs.CL

TL;DR: SlideTailor：基于用户偏好的论文转幻灯片生成框架，通过示例对和视觉模板隐式学习用户偏好，实现个性化幻灯片生成


<details>
  <summary>Details</summary>
Motivation: 现有自动幻灯片生成方法往往无法满足用户个性化需求，因为用户偏好多样且难以明确描述，导致生成结果与用户期望不符

Method: 提出SlideTailor框架：1）通过论文-幻灯片示例对和视觉模板隐式学习用户偏好；2）采用链式语音机制使幻灯片内容与口头叙述对齐；3）构建包含多样化用户偏好的基准数据集

Result: 实验证明该框架能有效从隐式输入中提取和泛化用户偏好，显著提升生成幻灯片质量，并支持视频演示等下游应用

Conclusion: SlideTailor通过自然易得的输入（示例对和模板）有效捕捉用户偏好，实现了更符合用户需求的个性化幻灯片生成，为自动内容创作提供了新思路

Abstract: Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.

</details>


### [18] [AprielGuard](https://arxiv.org/abs/2512.20293)
*Jaykumar Kasundra,Anjaneya Praharaj,Sourabh Surana,Lakshmi Sirisha Chodisetty,Sourav Sharma,Abhigya Verma,Abhishek Bhardwaj,Debasish Kanhar,Aakash Bhagat,Khalil Slimi,Seganrasan Subramanian,Sathwik Tejaswi Madhusudhan,Ranga Prasad Chenna,Srinivas Sunkara*

Main category: cs.CL

TL;DR: AprielGuard是一个8B参数的安全防护模型，统一处理LLM的安全风险和对抗威胁，在多种基准测试中表现优于现有开源防护方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防护工具通常将安全风险（如毒性、偏见）和对抗威胁（如提示注入、越狱）视为独立问题，这限制了其鲁棒性和泛化能力。

Method: 开发了一个8B参数的安全防护模型，采用统一分类和学习框架，在多样化的开放和合成数据上进行训练，涵盖独立提示、多轮对话和代理工作流，并通过结构化推理轨迹增强可解释性。

Result: 在多个公共和专有基准测试中，AprielGuard在检测有害内容和对抗操作方面表现出色，特别是在多步骤和推理密集型场景中，超越了Llama-Guard和Granite Guardian等现有开源防护方案。

Conclusion: 通过发布该模型，旨在推进关于LLM可靠防护的透明和可重复研究，为LLM在对话和代理场景中的安全部署提供更强大的保障。

Abstract: Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.

</details>


### [19] [Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives](https://arxiv.org/abs/2512.20298)
*Karolina Drożdż,Kacper Dudzic,Anna Sterna,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: LLM在精神疾病诊断中整体准确率超过人类专家21.91个百分点，但在自恋型人格障碍诊断上表现极差，存在价值偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在精神科自我评估中的使用增加，需要评估其解释定性患者叙述的能力，特别是在人格障碍诊断方面。

Method: 使用波兰语第一人称自传体叙述，直接比较最先进的LLM与精神健康专业人士在边缘型人格障碍和自恋型人格障碍诊断上的表现。

Result: Gemini Pro模型整体诊断准确率65.48%，比人类专家43.57%高21.91个百分点。在边缘型人格障碍诊断上表现优异（F1=83.4 vs 80.0），但在自恋型人格障碍诊断上严重不足（F1=6.7 vs 50.0），模型对"自恋"这一价值负载术语表现出抗拒。

Conclusion: LLM能够有效解释复杂的第一人称临床数据，但仍存在严重的可靠性和偏见问题，特别是在涉及价值判断的诊断上。

Abstract: Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term "narcissism." Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.

</details>


### [20] [SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision](https://arxiv.org/abs/2512.20308)
*Maxime Poli,Mahi Luthra,Youssef Benchekroun,Yosuke Higuchi,Martin Gleize,Jiayi Shen,Robin Algayres,Yu-An Chung,Mido Assran,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: SpidR是一个自监督语音表示模型，通过掩码预测、自蒸馏和在线聚类训练，能高效学习包含丰富语音信息的表示，特别适合无文本语音语言建模，在多项下游任务上超越现有模型，且训练时间大幅缩短。


<details>
  <summary>Details</summary>
Motivation: 随着语言建模和语音表示学习的并行发展，研究者希望直接从语音中学习语言而无需文本中间表示，这需要从语音中直接提取语义表示。

Method: 提出SpidR模型，在原始波形上使用掩码预测目标，结合自蒸馏和在线聚类进行训练。学生模型的中间层学习预测来自教师模型中间层的分配，这种学习目标相比先前方法稳定了在线聚类过程，产生更高质量的码本。

Result: SpidR在语言建模基准测试(sWUGGY, sBLIMP, tSC)上超越了wav2vec 2.0、HuBERT、WavLM和DinoSR。系统评估验证了语音单元质量指标(ABX, PNMI)与语言建模性能的相关性。相比HuBERT需要一周训练，SpidR仅需16个GPU训练一天，大幅缩短预训练时间。

Conclusion: SpidR通过创新的训练方法实现了高质量的语音表示学习，为无文本语音语言建模提供了高效解决方案，训练代码和模型已开源。

Abstract: The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.

</details>


### [21] [Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://arxiv.org/abs/2512.20324)
*Nurul Labib Sayeedi,Md. Faiyaz Abdullah Sayeedi,Khushnur Binte Jahangir,Swakkhar Shatabda,Sarah Masud Preum*

Main category: cs.CL

TL;DR: 论文介绍了BanglaRiddleEval基准测试，包含1,244个孟加拉语谜语，评估LLM在低资源语言和比喻推理方面的能力，结果显示当前模型远未达到人类水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在标准NLP基准上表现优异，但在比喻性、文化相关性和低资源语言环境中的推理能力尚未充分探索，特别是在孟加拉语这样的低资源语言中。

Method: 使用基于LLM的流水线生成思维链解释、语义连贯的干扰项和细粒度歧义标注，构建包含4,976个谜语任务工件的基准测试，评估开源和闭源模型在不同提示策略下的表现。

Result: 模型在生成式问答中达到中等语义重叠但正确率低，多项选择题准确率最高约56%（人类基线83%），歧义解决率在26%-68%之间，高质量解释仅限于最强模型。

Conclusion: 当前LLM能够捕捉孟加拉语谜语推理的一些线索，但距离人类水平仍有很大差距，BanglaRiddleEval成为低资源比喻推理的挑战性新基准。

Abstract: Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.

</details>


### [22] [Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation](https://arxiv.org/abs/2512.20352)
*Nilesh Jain,Seyi Adeyinka,Leor Roseman,Aza Allsop*

Main category: cs.CL

TL;DR: 提出一個多視角驗證框架，結合集成驗證與雙重可靠性指標（Cohen's Kappa和餘弦相似度），用於LLM主題分析，實現高可靠性的AI輔助質性研究。


<details>
  <summary>Details</summary>
Motivation: 傳統質性研究的編碼者間一致性方法需要多人參與、耗時且一致性通常不高，因此需要一個更高效可靠的AI輔助質性研究方法論。

Method: 開發多視角驗證框架，結合集成驗證與雙重可靠性指標：Cohen's Kappa用於編碼者間一致性，餘弦相似度用於語義一致性。框架支持可配置分析參數（1-6種子，溫度0.0-2.0）、自定義提示結構和變量替換，並提供跨JSON格式的共識主題提取。

Result: 在迷幻藝術治療訪談文本上評估三個LLM（Gemini 2.5 Pro、GPT-4o、Claude 3.5 Sonnet），每個模型進行六次獨立運行。Gemini獲得最高可靠性（κ=0.907，餘弦相似度95.3%），所有模型均達到高一致性（κ>0.80）。框架成功提取跨運行的共識主題，Gemini識別6個主題，GPT-4o識別5個，Claude識別4個。

Conclusion: 該框架為可靠的AI輔助質性研究建立了方法論基礎，提供透明可靠性指標、靈活配置和結構無關的共識提取，開源實現支持研究人員進行高可靠性的質性分析。

Abstract: Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.

</details>


### [23] [Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining](https://arxiv.org/abs/2512.20404)
*Junyi Liu,Stanley Kok*

Main category: cs.CL

TL;DR: 提出一个情感感知的文本摘要框架，将情感信号整合到抽取式（TextRank）和生成式（UniLM）方法中，以更好地处理社交媒体等非结构化文本的情感丰富内容。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体、评论和论坛等非结构化数据的快速增长，文本挖掘在信息系统（IS）中变得至关重要。现有摘要方法主要针对结构化的新闻内容优化，难以处理嘈杂、非正式的用户生成文本。情感线索对于品牌监控和市场分析等IS任务非常重要，但很少有研究将情感建模整合到短文本摘要中。

Method: 提出一个情感感知框架，扩展了抽取式（TextRank）和生成式（UniLM）方法，通过将情感信号嵌入到排名和生成过程中。这种双重设计能够更好地捕捉情感细微差别和主题相关性。

Result: 该框架能够生成简洁、情感丰富的摘要，增强了情感细微差别和主题相关性的捕捉，从而改善动态在线环境中的及时干预和战略决策。

Conclusion: 情感感知的摘要框架能够有效处理非结构化、情感丰富的用户生成文本，为信息系统中的品牌监控、市场分析和战略决策提供更好的支持。

Abstract: With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.

</details>


### [24] [Step-DeepResearch Technical Report](https://arxiv.org/abs/2512.20491)
*Chen Hu,Haikuo Du,Heng Wang,Lin Lin,Mingrui Chen,Peng Liu,Ruihang Miao,Tianchi Yue,Wang You,Wei Ji,Wei Yuan,Wenjin Deng,Xiaojian Yuan,Xiaoyun Zhang,Xiangyu Liu,Xikai Liu,Yanming Xu,Yicheng Cao,Yifei Zhang,Yongyao Wang,Yubo Shu,Yurong Zhang,Yuxiang Zhang,Zheng Gong,Zhichao Chang,Binyan Li,Dan Ma,Furong Jia,Hongyuan Wang,Jiayu Liu,Jing Bai,Junlan Liu,Manjiao Liu,Na Wang,Qiuping Wu,Qinxin Du,Shiwei Li,Wen Sun,Yifeng Gong,Yonglin Chen,Yuling Zhao,Yuxuan Lin,Ziqi Ren,Zixuan Wang,Aihu Zhang,Brian Li,Buyun Ma,Kang An,Li Xie,Mingliang Li,Pan Li,Shidong Yang,Xi Chen,Xiaojia Liu,Yuchu Luo,Yuan Song,YuanHao Ding,Yuanwei Liang,Zexi Li,Zhaoning Zhang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: Step-DeepResearch是一个用于深度研究的端到端智能体，通过原子能力数据合成策略和渐进式训练路径，在中文领域ADR-Bench上表现出色，32B模型在Scale AI Research Rubrics上达到61.4%分数。


<details>
  <summary>Details</summary>
Motivation: 现有学术基准（如BrowseComp）无法满足现实世界对开放式深度研究的需求，需要更强的意图识别、长视野决策和跨源验证能力。同时中文领域缺乏合适的评估基准。

Method: 1. 提出基于原子能力的数据合成策略来增强规划和报告撰写能力；2. 采用从智能体中期训练到SFT和RL的渐进式训练路径；3. 引入清单式评判器提高鲁棒性；4. 建立中文领域ADR-Bench评估基准。

Result: Step-DeepResearch（32B）在Scale AI Research Rubrics上获得61.4%分数。在ADR-Bench上显著优于同类模型，并与OpenAI、Gemini DeepResearch等闭源SOTA模型相媲美。

Conclusion: 精细化的训练方法使中等规模模型能够以行业领先的成本效益实现专家级能力，证明了深度研究智能体的可行性。

Abstract: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.

</details>


### [25] [Distilling to Hybrid Attention Models via KL-Guided Layer Selection](https://arxiv.org/abs/2512.20569)
*Yanhong Li,Songlin Yang,Shawn Tan,Mayank Mishra,Rameswar Panda,Jiawei Zhou,Yoon Kim*

Main category: cs.CL

TL;DR: 提出一种基于层重要性分数的简单高效层选择方法，用于将预训练softmax注意力Transformer蒸馏为混合架构，结合了RADLADS蒸馏流程，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将预训练的softmax注意力Transformer蒸馏为混合架构（交替使用softmax和线性注意力层）是提高LLM推理效率的有前景方法，但关键挑战在于如何选择哪些层转换为线性注意力变体。

Method: 1. 使用少量通用文本数据训练得到层重要性分数；2. 基于这些分数进行层选择；3. 采用RADLADS蒸馏流程：注意力权重转移、隐藏状态对齐、KL分布匹配，最后进行少量微调。

Result: 该方法比现有层选择方法更有效，包括基于固定比例均匀交替线性注意力的启发式方法，以及依赖专门诊断数据集的复杂方法。

Conclusion: 通过简单的层重要性评分和RADLADS蒸馏流程，可以高效地将预训练Transformer转换为混合架构，在保持性能的同时提高推理效率。

Abstract: Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.

</details>


### [26] [Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits](https://arxiv.org/abs/2512.20578)
*Amirhosein Ghasemabadi,Di Niu*

Main category: cs.CL

TL;DR: Gnosis：一种轻量级自感知机制，通过解码LLM内部状态来预测自身错误，无需外部监督，仅增加约500万参数


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能生成流畅复杂的输出，但常常无法识别自己的错误和幻觉。现有方法依赖外部评判器、多样本一致性或基于文本的自我批评，这些方法要么增加计算成本，要么与真实正确性相关性较弱。作者探索是否可以通过检查推理过程中的内部状态来预测LLM自身的失败。

Method: 提出Gnosis机制：被动观察LLM的内部轨迹（隐藏状态和注意力模式），将其压缩为固定预算的描述符，然后预测正确性。该方法仅增加约500万参数，与序列长度无关，推理成本可忽略。

Result: 在数学推理、开放域问答和学术知识基准测试中，在1.7B到20B参数的冻结骨干模型上，Gnosis在准确性和校准方面持续优于强大的内部基线和大型外部评判器。此外，它能零样本泛化到部分生成，实现早期失败轨迹检测和计算感知控制。

Conclusion: 可靠的正确性线索内在于生成过程，无需外部监督即可高效提取。Gnosis展示了LLM通过内部状态进行自我验证的潜力，为构建更可靠的AI系统提供了新方向。

Abstract: Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.

</details>


### [27] [Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs](https://arxiv.org/abs/2512.20595)
*Dhruv Anand,Ehsan Shareghi*

Main category: cs.CL

TL;DR: Cube Bench是一个基于魔方的多模态大语言模型基准测试，用于评估空间和序列推理能力，包含五个技能维度，测试结果显示模型性能随魔方复杂度增加而急剧下降，闭源模型优于开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在空间和序列推理方面的评估工具不足，需要一种能够系统评估模型在复杂空间任务中多步骤推理能力的基准测试。

Method: 创建Cube Bench基准测试，将魔方解谜任务分解为五个技能维度：图像和文本重建魔方面、选择最优下一步、预测候选移动结果、执行多步骤计划并纠错、检测和修正自身错误。使用统一的魔方状态、提示词和解析器，以及单一的距离解决度量标准，比较不同模型在不同复杂度下的表现。

Result: 七个测试的MLLM模型准确率随魔方复杂度深度增加而急剧下降；模型一旦轨迹停滞或发散就很少能恢复；高面重建准确率不能保证良好的动作选择或多步骤执行能力；闭源模型在单步感知和多步控制任务上都优于开源模型；简单的自我校正方法能带来适度改进但也可能引入过度思考。

Conclusion: Cube Bench提供了一个紧凑、可复现的基准测试来评估MLLM的序列空间推理能力，揭示了当前模型在复杂空间推理任务中的局限性，特别是开源模型与闭源模型之间的性能差距。

Abstract: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.

</details>


### [28] [MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts](https://arxiv.org/abs/2512.20604)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: MoE-DiffuSeq是一个基于专家混合的扩散模型框架，专门用于提升长文档生成的效率和性能，通过稀疏注意力机制和专家混合架构解决现有扩散模型在长序列生成中的计算成本和内存开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本生成模型（如DiffuSeq）在处理长序列时面临高计算成本和内存开销的挑战，限制了它们在长文档生成场景中的实际应用。

Method: 1. 集成稀疏注意力与专家混合架构，实现高效可扩展的长序列建模；2. 设计定制化的稀疏注意力机制，在降低计算复杂度的同时保持文本质量和连贯性；3. 在扩散过程中引入软吸收状态，加速序列重建并提高生成精度。

Result: 实验表明，MoE-DiffuSeq相比现有扩散模型显著提升了训练效率和采样速度，在科学文章生成、代码库建模和长对话生成等长文档场景中表现优异，在效率、速度、准确性和表达能力方面均有改进。

Conclusion: MoE-DiffuSeq通过专家混合架构和稀疏注意力机制，有效解决了扩散模型在长文本生成中的计算瓶颈，推动了扩散模型在高质量长文本生成中的实际应用。

Abstract: We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [29] [Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study](https://arxiv.org/abs/2512.19855)
*Andrew Stirling,Mykola Lukashchuk,Dmitry Bagaev,Wouter Kouw,James R. Forbes*

Main category: cs.RO

TL;DR: 将ESGVI算法扩展到矩阵李群以处理姿态估计，并引入因子处理重尾和偏斜噪声，应用于UWB定位中的NLOS场景，提高了精度并保持了稀疏性。


<details>
  <summary>Details</summary>
Motivation: 传统状态估计方法在处理具有姿态分量的状态时无法保持底层群结构，且在UWB定位中常见的NLOS和多径效应会产生重尾和偏斜噪声分布，需要更鲁棒的估计方法。

Method: 1) 将ESGVI扩展到矩阵李群上，使算法能处理姿态估计并保持群结构；2) 引入因子处理重尾和偏斜噪声分布；3) 在因子图框架中实现，保持稀疏和无导数结构。

Result: 在NLOS丰富的UWB定位实验中验证了所提方法，相比传统方法显示出更高的精度和相当的稳定性，同时保持了算法的稀疏性和计算效率。

Conclusion: 成功将ESGVI扩展到矩阵李群和重尾噪声场景，为UWB定位等实际问题提供了更鲁棒的估计框架，开源实现促进了更广泛的研究应用。

Abstract: This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.

</details>


### [30] [A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones](https://arxiv.org/abs/2512.19914)
*Sujan Warnakulasooriya,Andreas Willig,Xiaobing Wu*

Main category: cs.RO

TL;DR: 提出基于优先级调度的无人机编队初始形成算法，通过碰撞风险评估和延迟计算实现高效无碰撞轨迹规划


<details>
  <summary>Details</summary>
Motivation: 现有无人机集群编队算法在初始形成阶段存在效率和可扩展性问题，特别是当潜在碰撞迫使无人机采取次优轨迹时表现不佳

Method: 基于优先级的调度算法：根据无人机的潜在碰撞数量和到达目标位置而不永久阻碍其他无人机的能力分配优先级，然后计算适当延迟以确保无碰撞路径

Result: 成功为多达5000架无人机生成无碰撞轨迹，在性能和计算效率上均优于基于耦合度的启发式优先级规划方法（CDH-PP）

Conclusion: 提出的优先级调度算法能够高效解决大规模无人机集群初始编队问题，显著提升编队形成过程的效率和可扩展性

Abstract: Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.

</details>


### [31] [Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting](https://arxiv.org/abs/2512.20014)
*Sangoh Lee,Sangwoo Mo,Wook-Shin Han*

Main category: cs.RO

TL;DR: VAP（视觉注意力提示）是一种无需训练的感知适配器，通过视觉记忆和开放词汇检测，让冻结的VLA模型能够识别和操作个性化对象，如"拿我的杯子"。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型在通用指令上表现良好，但在处理个性化命令（如"拿我的杯子"）时表现不佳，因为需要从视觉相似的对象中识别特定实例。

Method: 提出VAP（Visual Attentive Prompting），将参考图像作为非参数视觉记忆，通过开放词汇检测和嵌入匹配定位个性化对象，然后通过高亮对象和重写指令将其作为视觉提示注入。

Result: 在模拟基准（Personalized-SIMPLER和Personalized-VLABench）和真实世界桌面基准上的实验表明，VAP在成功率和正确对象操作方面始终优于通用策略和token学习基线。

Conclusion: VAP有助于弥合语义理解和实例级控制之间的差距，使冻结的VLA模型能够有效处理个性化对象操作任务。

Abstract: While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as "bring my cup", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.

</details>


### [32] [LoLA: Long Horizon Latent Action Learning for General Robot Manipulation](https://arxiv.org/abs/2512.20166)
*Xiaofan Wang,Xingyu Gao,Jianlong Fu,Zuolei Li,Dean Fortier,Galen Mullins,Andrey Kolobov,Baining Guo*

Main category: cs.RO

TL;DR: LoLA框架通过整合长期多视角观察和机器人本体感知，实现多步推理和动作生成，在长时程操作任务中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型往往忽视历史信息和连贯动作序列生成能力，难以完成长时程、语言引导的机器人操作任务

Method: 使用视觉语言模型编码历史序列和多视角观察的上下文特征，引入状态感知潜在重表示模块，将视觉输入和语言指令转换为可执行的机器人运动空间，通过可学习的"具身锚定"潜在空间将表示与物理尺度对齐

Result: 在仿真基准测试和真实机器人任务中，LoLA显著优于现有最先进方法，特别是在长时程操作任务上表现突出

Conclusion: LoLA框架通过整合历史信息、多视角观察和机器人状态，成功解决了长时程语言引导机器人操作的关键挑战

Abstract: The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable "embodiment-anchored" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.

</details>


### [33] [Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation](https://arxiv.org/abs/2512.20188)
*Teqiang Zou,Hongliang Zeng,Yuxuan Nong,Yifan Li,Kehui Liu,Haotian Yang,Xinyang Ling,Xin Li,Lianyang Ma*

Main category: cs.RO

TL;DR: 提出异步快慢VLA框架DuoCore-FS，通过分离高频动作生成和慢速VLM推理，解决传统VLA系统同步执行导致的性能限制，实现30Hz全身动作生成。


<details>
  <summary>Details</summary>
Motivation: 传统VLA系统将视觉语言模型和动作专家统一在单一频率下运行，受限于大型VLM的低推理速度，导致控制稳定性和实时性不足，尤其在涉及更多关节、更大运动空间和动态视角变化的全身机器人操作中问题更突出。

Method: 提出异步快慢VLA框架(DuoCore-FS)，包含：1) 快速路径用于高频动作生成；2) 慢速路径用于丰富的VLM推理；3) 潜在表示缓冲区桥接两个系统，存储指令语义和与场景指令上下文对齐的动作推理表示；4) 全身动作分词器提供紧凑统一的全身动作表示。VLM和动作专家仍保持端到端联合训练。

Result: DuoCore-FS支持30亿参数VLM的同时实现30Hz全身动作块生成，比同类规模VLA模型快约3倍。真实世界全身操作实验显示相比同步快慢VLA基线，任务成功率提高且响应性显著增强。

Conclusion: DuoCore-FS通过异步架构解决了VLA系统在全身机器人操作中的实时性瓶颈，在保持端到端策略学习的同时显著提升了控制性能和响应速度，已作为Astribot机器人平台的一部分提供给商业用户。

Abstract: Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.

</details>


### [34] [UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas](https://arxiv.org/abs/2512.20224)
*Qijun Qin,Ziqi Zhang,Yihan Zhong,Feng Huang,Xikun Liu,Runzhi Hu,Hang Chen,Wei Hu,Dongzhe Su,Jun Zhang,Hoi-Fung Ng,Weisong Wen*

Main category: cs.RO

TL;DR: UrbanV2X是一个用于车辆-基础设施协同导航的多传感器数据集，包含车辆和路边基础设施采集的同步数据，支持密集城市环境下的智能交通应用研究。


<details>
  <summary>Details</summary>
Motivation: 由于单个自动驾驶车辆的局限性，C-V2X技术通过传感器信息共享为实现完全自动驾驶提供了新途径。然而，支持复杂城市环境中车辆-基础设施协同导航的真实世界数据集仍然稀缺。

Method: 在香港C-V2X测试场收集车辆和路边基础设施的多传感器数据。车载平台提供多个工业相机、LiDAR、4D雷达、UWB、IMU和高精度GNSS-RTK/INS导航系统的同步数据。路边基础设施提供LiDAR、GNSS和UWB测量。整个系统使用精确时间协议(PTP)进行同步，并提供传感器标定数据。

Result: 创建了UrbanV2X数据集，这是一个全面的多传感器数据集，支持密集城市环境下的智能交通应用研究。数据集已公开可用，并提供了各种导航算法的基准测试。

Conclusion: UrbanV2X填补了复杂城市环境中车辆-基础设施协同导航数据集的空白，为C-V2X和智能交通研究提供了宝贵资源，有助于推动完全自动驾驶技术的发展。

Abstract: Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.

</details>


### [35] [KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System](https://arxiv.org/abs/2512.20299)
*Zhongyu Xia,Wenhao Chen,Yongtao Wang,Ming-Hsuan Yang*

Main category: cs.RO

TL;DR: KnowVal：通过开放世界感知与知识检索协同整合，实现视觉语言推理的自动驾驶系统，显著提升规划性能并保持与现有架构兼容


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶方法主要依赖数据驱动学习，难以通过模仿或有限的强化奖励捕捉决策背后的复杂逻辑，需要解决视觉语言推理、驾驶知识和价值对齐问题

Method: 构建包含交通法规、防御性驾驶原则和道德规范的全面驾驶知识图谱，开发基于LLM的高效检索机制，创建人类偏好数据集并训练价值模型进行可解释的价值对齐轨迹评估

Result: 在nuScenes数据集上实现最低碰撞率，在Bench2Drive基准测试中达到最先进水平，显著提升规划性能同时保持与现有架构兼容

Conclusion: KnowVal通过整合开放世界感知、知识检索和价值对齐，为自动驾驶系统提供了更可靠的视觉语言推理能力，解决了传统数据驱动方法的局限性

Abstract: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.

</details>


### [36] [Pneumatic bladder links with wide range of motion joints for articulated inflatable robots](https://arxiv.org/abs/2512.20322)
*Katsu Uchiyama,Ryuma Niiyama*

Main category: cs.RO

TL;DR: 提出一种采用Hillberry关节的多段气动膀胱连杆可充气机器人，具有大运动范围和高负载能力，适用于机械臂和腿式运动。


<details>
  <summary>Details</summary>
Motivation: 探索可充气机器人的多样化应用，需要开发具有大运动范围和高负载能力的关节结构，以扩展可充气机器人的功能边界。

Method: 设计采用Hillberry滚动接触关节的多段气动膀胱连杆结构，膀胱连杆采用帆布和聚氨酯双层结构实现气密性和形状灵活性，构建3自由度机械臂和腿式结构。

Result: Hillberry关节实现±150°大运动范围；3自由度机械臂可搬运500g负载；2自由度和1自由度机械臂分别可举起3.4kg和5kg负载；3自由度充气腿在移动平台上成功实现腿式运动。

Conclusion: 提出的Hillberry关节集成方案为可充气机器人提供了前所未有的运动范围和负载能力，展示了在机械臂和腿式运动等多样化应用中的潜力。

Abstract: Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\pm 150 ^{\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.

</details>


### [37] [FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration](https://arxiv.org/abs/2512.20355)
*Hao Wei,Peiji Wang,Qianhao Wang,Tong Qin,Fei Gao,Yulin Si*

Main category: cs.RO

TL;DR: FAR-AVIO是一个基于舒尔补的水下声学-视觉-惯性里程计框架，通过EKF嵌入舒尔补实现联合位姿-地标优化，同时保持恒定时间更新，并包含自适应传感器权重调整和在线标定功能。


<details>
  <summary>Details</summary>
Motivation: 水下环境对视觉惯性里程计系统带来严峻挑战，包括强光衰减、海洋雪花和浊度等问题，导致惯性可观测性降低和频繁跟踪失败。虽然紧密耦合的声学-视觉-惯性融合可以提供准确的状态估计，但基于图的优化计算量过大，难以在资源受限平台上实时部署。

Method: 1) 将舒尔补公式嵌入扩展卡尔曼滤波器(EKF)，实现联合位姿-地标优化，同时通过高效边缘化地标状态保持恒定时间更新；2) 引入自适应权重调整和可靠性评估(AWARE)模块，在线评估视觉、惯性和DVL测量的可靠性并自适应调节其权重；3) 开发高效的在线标定方案，联合估计DVL-IMU外参，无需专用标定动作。

Result: 数值模拟和真实水下实验一致表明，FAR-AVIO在定位精度和计算效率方面均优于最先进的水下SLAM基线方法，能够在低功耗嵌入式平台上实现鲁棒运行。

Conclusion: FAR-AVIO通过舒尔补EKF框架、自适应传感器权重调整和在线标定，解决了水下声学-视觉-惯性里程计的计算效率和鲁棒性问题，为资源受限的水下机器人平台提供了实用的实时状态估计解决方案。

Abstract: Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.

</details>


### [38] [Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing](https://arxiv.org/abs/2512.20475)
*Maulana Bisyir Azhari,Donghun Han,Je In You,Sungjun Park,David Hyunchul Shim*

Main category: cs.RO

TL;DR: 本文介绍了为阿布扎比自主赛车联盟无人机冠军联赛开发的单目视觉自主无人机竞速系统，通过融合视觉惯性里程计与基于YOLO的门框检测器，结合感知感知规划器，在比赛中获得多项优异成绩。


<details>
  <summary>Details</summary>
Motivation: 比赛要求仅使用单摄像头和低质量IMU进行高速自主无人机竞速，这种最小传感器配置容易在高速飞行和激进机动时产生视觉惯性里程计漂移，需要解决定位精度问题。

Method: 采用卡尔曼滤波器融合视觉惯性里程计输出与基于YOLO的门框检测器提供的全局位置测量，同时设计感知感知规划器生成平衡速度与门框可见性的轨迹。

Result: 系统在比赛中表现优异：AI Grand Challenge第三名（最高速度43.2 km/h），AI Drag Race第二名（速度超过59 km/h），AI Multi-Drone Race第二名。

Conclusion: 本文展示了完整的系统架构和基于比赛数据的性能分析，为基于单目视觉的自主无人机飞行系统开发提供了有价值的见解。

Abstract: The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.

</details>


### [39] [LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing](https://arxiv.org/abs/2512.20591)
*Changyi Lin,Boda Huo,Mingyang Yu,Emily Ruppel,Bingqing Chen,Jonathan Francis,Ding Zhao*

Main category: cs.RO

TL;DR: LightTact是一种新型视觉触觉指尖传感器，采用光学原理直接可视化接触，无需依赖表面变形，能够感知液体、半液体和超软材料的极轻微接触。


<details>
  <summary>Details</summary>
Motivation: 现有触觉传感器大多依赖表面变形来推断接触，难以可靠感知与液体、半液体或超软材料等不发生宏观变形的接触。需要一种不依赖变形的接触感知方法。

Method: 采用环境光阻挡光学配置，抑制非接触区域的外部光和内部照明，仅传输真实接触产生的漫射光。产生高对比度原始图像，非接触像素接近黑色，接触像素保留接触表面的自然外观。

Result: 实现精确的像素级接触分割，对材料特性、接触力、表面外观和环境光照具有鲁棒性。在机械臂上演示了水扩散、面霜蘸取和薄膜交互等极轻微接触驱动操作。视觉触觉图像可直接由现有视觉语言模型解释，实现电阻值推理用于机器人分拣。

Conclusion: LightTact提供了一种变形无关的光学接触感知方法，能够可靠检测极轻微接触，为与软材料和液体交互的机器人操作开辟了新可能性，其视觉触觉图像与现有AI模型兼容。

Abstract: Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.

</details>
