{"id": "2511.15886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15886", "abs": "https://arxiv.org/abs/2511.15886", "authors": ["Jeremias Ferrao", "Ezgi Basar", "Khondoker Ittehadul Islam", "Mahrokh Hassani"], "title": "What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning", "comment": "Received the Best Student Project Award at RuG's Advanced-NLP course", "summary": "This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e24\u79cd\u5f52\u56e0\u65b9\u6cd5\u5206\u6790\u591a\u8bed\u8a00LLM\u4e2d\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5f52\u56e0\u6a21\u5f0f\uff0c\u53d1\u73b0CoT\u63d0\u793a\u5728\u51c6\u786e\u6027\u3001\u591a\u8bed\u8a00\u9c81\u68d2\u6027\u548c\u89e3\u91ca\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u8bc4\u4f30\u591a\u8bed\u8a00LLM\u4e2d\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528ContextCite\u8fdb\u884c\u6b65\u9aa4\u7ea7\u5f52\u56e0\u548cInseq\u8fdb\u884c\u4ee4\u724c\u7ea7\u5f52\u56e0\uff0c\u5728Qwen2.5 1.5B-Instruct\u6a21\u578b\u4e0a\u5e94\u7528MGSM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7\u5426\u5b9a\u548c\u5e72\u6270\u53e5\u8fdb\u884c\u53d7\u63a7\u6270\u52a8\u3002", "result": "\u5f52\u56e0\u5206\u6570\u8fc7\u5ea6\u5f3a\u8c03\u6700\u7ec8\u63a8\u7406\u6b65\u9aa4\uff08\u7279\u522b\u662f\u9519\u8bef\u751f\u6210\u65f6\uff09\uff1b\u7ed3\u6784\u5316CoT\u63d0\u793a\u4e3b\u8981\u6539\u5584\u9ad8\u8d44\u6e90\u62c9\u4e01\u8bed\u7cfb\u8bed\u8a00\u7684\u51c6\u786e\u6027\uff1b\u53d7\u63a7\u6270\u52a8\u4f1a\u964d\u4f4e\u6a21\u578b\u51c6\u786e\u6027\u548c\u5f52\u56e0\u4e00\u81f4\u6027\u3002", "conclusion": "\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u591a\u8bed\u8a00\u9c81\u68d2\u6027\u548c\u89e3\u91ca\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u589e\u5f3a\u5176\u53ef\u9760\u6027\u548c\u8de8\u8bed\u8a00\u9002\u7528\u6027\u3002"}}
{"id": "2511.15887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15887", "abs": "https://arxiv.org/abs/2511.15887", "authors": ["Seungbeen Lee", "Jinhong Jeong", "Donghyun Kim", "Yejin Son", "Youngjae Yu"], "title": "Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language", "comment": null, "summary": "Our ability to interpret others' mental states through nonverbal cues (NVCs) is fundamental to our survival and social cohesion. While existing Theory of Mind (ToM) benchmarks have primarily focused on false-belief tasks and reasoning with asymmetric information, they overlook other mental states beyond belief and the rich tapestry of human nonverbal communication. We present Motion2Mind, a framework for evaluating the ToM capabilities of machines in interpreting NVCs. Leveraging an expert-curated body-language reference as a proxy knowledge base, we build Motion2Mind, a carefully curated video dataset with fine-grained nonverbal cue annotations paired with manually verified psychological interpretations. It encompasses 222 types of nonverbal cues and 397 mind states. Our evaluation reveals that current AI systems struggle significantly with NVC interpretation, exhibiting not only a substantial performance gap in Detection, as well as patterns of over-interpretation in Explanation compared to human annotators.", "AI": {"tldr": "Motion2Mind\u662f\u4e00\u4e2a\u8bc4\u4f30\u673a\u5668\u901a\u8fc7\u975e\u8bed\u8a00\u7ebf\u7d22\u7406\u89e3\u4ed6\u4eba\u5fc3\u7406\u72b6\u6001\u80fd\u529b\u7684\u6846\u67b6\uff0c\u5305\u542b\u7cbe\u5fc3\u7b56\u5212\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u6db5\u76d6222\u79cd\u975e\u8bed\u8a00\u7ebf\u7d22\u548c397\u79cd\u5fc3\u7406\u72b6\u6001\u3002", "motivation": "\u73b0\u6709\u5fc3\u7406\u7406\u8bba\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9519\u8bef\u4fe1\u5ff5\u4efb\u52a1\u548c\u4e0d\u5bf9\u79f0\u4fe1\u606f\u63a8\u7406\uff0c\u5ffd\u89c6\u4e86\u4fe1\u5ff5\u4e4b\u5916\u7684\u5176\u4ed6\u5fc3\u7406\u72b6\u6001\u548c\u4e30\u5bcc\u7684\u975e\u8bed\u8a00\u4ea4\u6d41\u3002", "method": "\u5229\u7528\u4e13\u5bb6\u7b56\u5212\u7684\u8eab\u4f53\u8bed\u8a00\u53c2\u8003\u4f5c\u4e3a\u77e5\u8bc6\u5e93\uff0c\u6784\u5efa\u5305\u542b\u7ec6\u7c92\u5ea6\u975e\u8bed\u8a00\u7ebf\u7d22\u6807\u6ce8\u548c\u4eba\u5de5\u9a8c\u8bc1\u5fc3\u7406\u89e3\u91ca\u7684\u89c6\u9891\u6570\u636e\u96c6\u3002", "result": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u975e\u8bed\u8a00\u7ebf\u7d22\u89e3\u91ca\u65b9\u9762\u8868\u73b0\u663e\u8457\u4e0d\u8db3\uff0c\u5728\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u5de8\u5927\u6027\u80fd\u5dee\u8ddd\uff0c\u5728\u89e3\u91ca\u65b9\u9762\u76f8\u6bd4\u4eba\u7c7b\u6807\u6ce8\u8005\u8868\u73b0\u51fa\u8fc7\u5ea6\u89e3\u91ca\u6a21\u5f0f\u3002", "conclusion": "\u673a\u5668\u5728\u7406\u89e3\u975e\u8bed\u8a00\u7ebf\u7d22\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u63d0\u5347\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2511.15976", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15976", "abs": "https://arxiv.org/abs/2511.15976", "authors": ["Sarik Ghazarian", "Abhinav Gullapalli", "Swair Shah", "Anurag Beniwal", "Nanyun Peng", "Narayanan Sadagopan", "Zhou Yu"], "title": "TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues", "comment": null, "summary": "In real-world task-oriented dialogue (TOD) settings, agents are required to strictly adhere to complex instructions while conducting multi-turn conversations with customers. These instructions are typically presented in natural language format and include general guidelines and step-by-step procedures with complex constraints. Existing TOD benchmarks often oversimplify the complex nature of these instructions by reducing them to simple schemas composed of intents, slots, and API call configurations. To address this gap and systematically benchmark LLMs' instruction-following capabilities, we propose TOD-ProcBench, a challenging benchmark featuring complex process instructions with intricate, fine-grained constraints that evaluates various LLMs' abilities to understand and follow instructions in multi-turn TODs. Our benchmark dataset comprises instruction documents derived from the high-quality ABCD dataset with corresponding conversations under human quality control. We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements. We design three tasks to comprehensively benchmark LLMs' complex instruction-following capabilities in multi-turn TODs. Task 1 evaluates how LLMs retrieve the most relevant statement from a complex instruction and predict the corresponding next action. In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions, and then we analyze how effectively LLMs can identify instruction-violating responses. Task 3 investigates LLMs' abilities in conditional generation of instruction-following responses based on the original complex instructions. Additionally, we conduct studies on the impact of multilingual settings and different instruction text formats on compliance performance. We release our benchmark under the Llama 3.3 Community License Agreement.", "AI": {"tldr": "\u63d0\u51fa\u4e86TOD-ProcBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u591a\u8f6e\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u9075\u5faa\u590d\u6742\u8fc7\u7a0b\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u5305\u542b\u4e09\u4e2a\u4efb\u52a1\u6765\u5168\u9762\u6d4b\u8bd5\u6307\u4ee4\u7406\u89e3\u3001\u8fdd\u89c4\u68c0\u6d4b\u548c\u6761\u4ef6\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709TOD\u57fa\u51c6\u8fc7\u5ea6\u7b80\u5316\u590d\u6742\u6307\u4ee4\u4e3a\u7b80\u5355\u6a21\u5f0f\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u4e2d\u9700\u8981\u4e25\u683c\u9075\u5b88\u590d\u6742\u7ea6\u675f\u7684\u5bf9\u8bdd\u573a\u666f\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30LLMs\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u9ad8\u8d28\u91cfABCD\u6570\u636e\u96c6\u6784\u5efa\u5305\u542b\u590d\u6742\u8fc7\u7a0b\u6307\u4ee4\u7684\u57fa\u51c6\uff0c\u5c06\u7ec6\u7c92\u5ea6\u7ea6\u675f\u548c\u52a8\u4f5c\u7a0b\u5e8f\u8868\u8ff0\u4e3a\u591a\u7ea7\u6761\u4ef6-\u52a8\u4f5c\u6307\u4ee4\u8bed\u53e5\uff0c\u8bbe\u8ba1\u4e09\u4e2a\u4efb\u52a1\uff1a\u76f8\u5173\u8bed\u53e5\u68c0\u7d22\u4e0e\u52a8\u4f5c\u9884\u6d4b\u3001\u6307\u4ee4\u8fdd\u89c4\u54cd\u5e94\u8bc6\u522b\u3001\u6761\u4ef6\u751f\u6210\u6307\u4ee4\u9075\u5faa\u54cd\u5e94\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u590d\u6742\u7ea6\u675f\u7684\u6307\u4ee4\u6587\u6863\u548c\u5bf9\u5e94\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cdLLMs\u5728\u591a\u8f6eTOD\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u8bbe\u7f6e\u548c\u6307\u4ee4\u683c\u5f0f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "TOD-ProcBench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u4e3a\u8bc4\u4f30LLMs\u5728\u590d\u6742\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u5bf9\u8bdd\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2511.16035", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16035", "abs": "https://arxiv.org/abs/2511.16035", "authors": ["Kieron Kretschmar", "Walter Laurito", "Sharan Maiya", "Samuel Marks"], "title": "Liars' Bench: Evaluating Lie Detectors for Language Models", "comment": "*Kieron Kretschmar and Walter Laurito contributed equally to this work. 10 pages, 2 figures; plus appendix. Code at https://github.com/Cadenza-Labs/liars-bench and datasets at https://huggingface.co/datasets/Cadenza-Labs/liars-bench Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "summary": "Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.", "AI": {"tldr": "LIARS' BENCH\u662f\u4e00\u4e2a\u5305\u542b72,863\u4e2a\u8c0e\u8a00\u548c\u8bda\u5b9e\u56de\u7b54\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8c0e\u8a00\u68c0\u6d4b\u6280\u672f\uff0c\u53d1\u73b0\u73b0\u6709\u6280\u672f\u5728\u68c0\u6d4b\u67d0\u4e9b\u7c7b\u578b\u7684\u8c0e\u8a00\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8c0e\u8a00\u68c0\u6d4b\u6280\u672f\u901a\u5e38\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u53ef\u80fd\u751f\u6210\u7684\u5404\u79cd\u8c0e\u8a00\u7c7b\u578b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u5e73\u53f0\u6765\u8bc4\u4f30\u8fd9\u4e9b\u6280\u672f\u3002", "method": "\u521b\u5efaLIARS' BENCH\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b4\u4e2a\u5f00\u6e90\u6a21\u578b\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u751f\u6210\u768472,863\u4e2a\u8c0e\u8a00\u548c\u8bda\u5b9e\u56de\u7b54\u793a\u4f8b\uff0c\u6db5\u76d6\u4e0d\u540c\u8c0e\u8a00\u7c7b\u578b\uff0c\u6309\u8bf4\u8c0e\u539f\u56e0\u548c\u4fe1\u5ff5\u76ee\u6807\u4e24\u4e2a\u7ef4\u5ea6\u53d8\u5316\u3002", "result": "\u8bc4\u4f30\u4e09\u79cd\u9ed1\u767d\u76d2\u8c0e\u8a00\u68c0\u6d4b\u6280\u672f\u540e\u53d1\u73b0\uff0c\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u68c0\u6d4b\u67d0\u4e9b\u7c7b\u578b\u7684\u8c0e\u8a00\uff0c\u7279\u522b\u662f\u5728\u4ec5\u51ed\u5bf9\u8bdd\u8bb0\u5f55\u65e0\u6cd5\u5224\u65ad\u6a21\u578b\u662f\u5426\u8bf4\u8c0e\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "LIARS' BENCH\u63ed\u793a\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8c0e\u8a00\u68c0\u6d4b\u6280\u672f\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2511.15909", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15909", "abs": "https://arxiv.org/abs/2511.15909", "authors": ["J. Cristobal", "A. Z. Zain Aldeen", "M. Izadi", "R. Faieghi"], "title": "Gimballed Rotor Mechanism for Omnidirectional Quadrotors", "comment": "6 pages, 7 figures, CASE 2025", "summary": "This paper presents the design of a gimballed rotor mechanism as a modular and efficient solution for constructing omnidirectional quadrotors. Unlike conventional quadrotors, which are underactuated, this class of quadrotors achieves full actuation, enabling independent motion in all six degrees of freedom. While existing omnidirectional quadrotor designs often require significant structural modifications, the proposed gimballed rotor system maintains a lightweight and easy-to-integrate design by incorporating servo motors within the rotor platforms, allowing independent tilting of each rotor without major alterations to the central structure of a quadrotor. To accommodate this unconventional design, we develop a new control allocation scheme in PX4 Autopilot and present successful flight tests, validating the effectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528\u4e07\u5411\u8282\u8f6c\u5b50\u673a\u5236\u7684\u6a21\u5757\u5316\u5168\u5411\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u8bbe\u8ba1\uff0c\u901a\u8fc7\u72ec\u7acb\u503e\u659c\u6bcf\u4e2a\u8f6c\u5b50\u5b9e\u73b0\u516d\u81ea\u7531\u5ea6\u5168\u9a71\u52a8\u63a7\u5236", "motivation": "\u4f20\u7edf\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u662f\u6b20\u9a71\u52a8\u7684\uff0c\u65e0\u6cd5\u5b9e\u73b0\u516d\u81ea\u7531\u5ea6\u72ec\u7acb\u63a7\u5236\u3002\u73b0\u6709\u5168\u5411\u56db\u65cb\u7ffc\u8bbe\u8ba1\u901a\u5e38\u9700\u8981\u5927\u5e45\u7ed3\u6784\u4fee\u6539\uff0c\u672c\u8bbe\u8ba1\u65e8\u5728\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5728\u8f6c\u5b50\u5e73\u53f0\u5185\u96c6\u6210\u4f3a\u670d\u7535\u673a\uff0c\u5b9e\u73b0\u6bcf\u4e2a\u8f6c\u5b50\u7684\u72ec\u7acb\u503e\u659c\uff0c\u65e0\u9700\u5bf9\u56db\u65cb\u7ffc\u4e2d\u5fc3\u7ed3\u6784\u8fdb\u884c\u91cd\u5927\u6539\u52a8\u3002\u5728PX4\u81ea\u52a8\u9a7e\u9a76\u4eea\u4e2d\u5f00\u53d1\u4e86\u65b0\u7684\u63a7\u5236\u5206\u914d\u65b9\u6848", "result": "\u6210\u529f\u8fdb\u884c\u4e86\u98de\u884c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u4e07\u5411\u8282\u8f6c\u5b50\u673a\u5236\u4e3a\u6784\u5efa\u5168\u5411\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u5168\u9a71\u52a8\u63a7\u5236\u80fd\u529b"}}
{"id": "2511.16054", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16054", "abs": "https://arxiv.org/abs/2511.16054", "authors": ["Gwen Yidou-Weng", "Ian Li", "Anji Liu", "Oliver Broadrick", "Guy Van den Broeck", "Benjie Wang"], "title": "Learning Tractable Distributions Of Language Model Continuations", "comment": null, "summary": "Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.", "AI": {"tldr": "\u63d0\u51faLTLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u548c\u56fa\u5b9a\u53ef\u5904\u7406\u4ee3\u7406\u6a21\u578b\uff0c\u89e3\u51b3\u53d7\u63a7\u6587\u672c\u751f\u6210\u4e2d\u672a\u6765\u4f9d\u8d56\u7ea6\u675f\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u6761\u4ef6\u4f3c\u7136\u548c\u7ea6\u675f\u6ee1\u8db3\u5ea6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u53ef\u5904\u7406\u4ee3\u7406\u6a21\u578b\uff08\u5982HMM\uff09\u8fd1\u4f3c\u7ee7\u7eed\u6982\u7387\uff0c\u4f46\u8fd9\u4e9b\u4ee3\u7406\u6a21\u578b\u901a\u5e38\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u5f31\uff0c\u5f71\u54cd\u67e5\u8be2\u8d28\u91cf", "method": "LTLA\u5c06\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u4e30\u5bcc\u524d\u7f00\u7f16\u7801\uff0c\u4e0e\u56fa\u5b9a\u53ef\u5904\u7406\u4ee3\u7406\u6a21\u578b\u914d\u5bf9\u8ba1\u7b97\u7cbe\u786e\u7ee7\u7eed\u6982\u7387\uff0c\u901a\u8fc7\u5355\u6b21\u6279\u91cfHMM\u66f4\u65b0\u5904\u7406\u6240\u6709\u5019\u9009\u6807\u8bb0\uff0c\u4fdd\u6301\u4ee3\u7406\u89e3\u7801\u5668\u56fa\u5b9a\u4ee5\u5b9e\u73b0\u8ba1\u7b97\u590d\u7528", "result": "LTLA\u83b7\u5f97\u6bd4\u65e0\u6761\u4ef6HMM\u66f4\u9ad8\u7684\u6761\u4ef6\u4f3c\u7136\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u8fd1\u4f3c\u7ee7\u7eed\u5206\u5e03\uff0c\u5728\u53d7\u63a7\u751f\u6210\u4efb\u52a1\u4e2d\u63d0\u9ad8\u7ea6\u675f\u6ee1\u8db3\u5ea6\u4e14\u4fdd\u6301\u76f8\u4f3c\u6d41\u7545\u6027\uff0c\u63a8\u7406\u5f00\u9500\u6700\u5c0f", "conclusion": "LTLA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u63a7\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u672a\u6765\u4f9d\u8d56\u7ea6\u675f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf"}}
{"id": "2511.15914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15914", "abs": "https://arxiv.org/abs/2511.15914", "authors": ["Debasmita Ghose", "Oz Gitelson", "Ryan Jin", "Grace Abawe", "Marynel Vazquez", "Brian Scassellati"], "title": "I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration", "comment": "Accepted to RA-L", "summary": "For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u68c0\u6d4b\u4eba\u7c7b\u76ee\u6807\u53d8\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\u5e8f\u5217\u5e76\u9a8c\u8bc1\u5176\u5408\u7406\u6027\uff0c\u5f53\u68c0\u6d4b\u5230\u76ee\u6807\u53d8\u5316\u65f6\uff0c\u673a\u5668\u4eba\u91cd\u65b0\u8bc4\u4f30\u76f8\u5173\u5386\u53f2\u52a8\u4f5c\u5e76\u6784\u5efa\u89c4\u5212\u6811\u6765\u9009\u62e9\u534f\u52a9\u4eba\u7c7b\u7684\u52a8\u4f5c\u3002", "motivation": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u4eba\u7c7b\u7ecf\u5e38\u5728\u4efb\u52a1\u4e2d\u9014\u6539\u53d8\u76ee\u6807\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u56fa\u5b9a\u76ee\u6807\uff0c\u8fd9\u4f7f\u5f97\u673a\u5668\u4eba\u96be\u4ee5\u9002\u5e94\u53d8\u5316\u800c\u65e0\u9700\u663e\u5f0f\u6c9f\u901a\u3002", "method": "\u8ddf\u8e2a\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\u5e8f\u5217\u5e76\u9a8c\u8bc1\u5176\u5408\u7406\u6027\uff0c\u68c0\u6d4b\u5230\u76ee\u6807\u53d8\u5316\u540e\u91cd\u65b0\u8bc4\u4f30\u5386\u53f2\u52a8\u4f5c\uff0c\u6784\u5efaReceding Horizon Planning\u6811\u6765\u9009\u62e9\u534f\u52a9\u52a8\u4f5c\u5e76\u9f13\u52b1\u533a\u5206\u6027\u52a8\u4f5c\u4ee5\u63ed\u793a\u66f4\u65b0\u540e\u7684\u76ee\u6807\u3002", "result": "\u5728\u5305\u542b30\u79cd\u72ec\u7279\u98df\u8c31\u7684\u534f\u4f5c\u70f9\u996a\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4e09\u79cd\u57fa\u7ebf\u7b97\u6cd5\uff0c\u5728\u76ee\u6807\u5207\u6362\u540e\u5feb\u901f\u6536\u655b\u5230\u6b63\u786e\u76ee\u6807\uff0c\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u5e76\u63d0\u9ad8\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u4eba\u7c7b\u76ee\u6807\u53d8\u5316\u5e76\u5feb\u901f\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u6548\u7387\u3002"}}
{"id": "2511.16072", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16072", "abs": "https://arxiv.org/abs/2511.16072", "authors": ["S\u00e9bastien Bubeck", "Christian Coester", "Ronen Eldan", "Timothy Gowers", "Yin Tat Lee", "Alexandru Lupsasca", "Mehtaab Sawhney", "Robert Scherrer", "Mark Sellke", "Brian K. Spears", "Derya Unutmaz", "Kevin Weil", "Steven Yin", "Nikita Zhivotovskiy"], "title": "Early science acceleration experiments with GPT-5", "comment": "89 pages", "summary": "AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86GPT-5\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\uff08\u6570\u5b66\u3001\u7269\u7406\u3001\u5929\u6587\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u751f\u7269\u5b66\u3001\u6750\u6599\u79d1\u5b66\uff09\u4e2d\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u53d6\u5f97\u65b0\u8fdb\u5c55\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5305\u62ec\u56db\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6570\u5b66\u65b0\u7ed3\u679c\u3002", "motivation": "\u8bb8\u591a\u79d1\u5b66\u5bb6\u5bf9\u524d\u6cbfAI\u7684\u80fd\u529b\u4e86\u89e3\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5177\u4f53\u6848\u4f8b\u5c55\u793aGPT-5\u5982\u4f55\u52a0\u901f\u79d1\u5b66\u7814\u7a76\uff0c\u5e76\u9610\u660e\u4eba\u673a\u534f\u4f5c\u7684\u6709\u6548\u6a21\u5f0f\u3002", "method": "\u6536\u96c6\u4e86\u591a\u4e2a\u9886\u57df\u7684\u77ed\u6848\u4f8b\u7814\u7a76\uff0c\u8bb0\u5f55\u4e86\u7814\u7a76\u4eba\u5458\u4e0eGPT-5\u7684\u4e92\u52a8\u8fc7\u7a0b\uff0c\u5c55\u793a\u4e86AI\u5982\u4f55\u5e2e\u52a9\u89e3\u51b3\u5177\u4f53\u7814\u7a76\u95ee\u9898\u3002", "result": "GPT-5\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\u4ea7\u751f\u4e86\u65b0\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u9886\u57df\u5e2e\u52a9\u89e3\u51b3\u4e86\u56db\u4e2a\u5148\u524d\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u7ed3\u679c\u90fd\u7ecf\u8fc7\u4e86\u4eba\u7c7b\u4f5c\u8005\u7684\u4ed4\u7ec6\u9a8c\u8bc1\u3002", "conclusion": "\u867d\u7136GPT-5\u7684\u8d21\u732e\u5728\u8303\u56f4\u4e0a\u76f8\u5bf9\u6709\u9650\uff0c\u4f46\u5176\u5728\u52a0\u901f\u79d1\u5b66\u7814\u7a76\u65b9\u9762\u7684\u6f5c\u529b\u662f\u6df1\u8fdc\u7684\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u524d\u6cbfAI\u7684\u5feb\u901f\u53d1\u5c55\u901f\u5ea6\u3002"}}
{"id": "2511.15956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15956", "abs": "https://arxiv.org/abs/2511.15956", "authors": ["Aliyah Smith", "Monroe Kennedy"], "title": "The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces", "comment": "9 pages, 6 figures", "summary": "As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u58f0\u97f3\u5bf9\u4eba\u7c7b\u611f\u77e5\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u64cd\u4f5c\u58f0\u97f3\u4e0d\u4f1a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u4fa7\u5411\u7a7a\u95f4\u5b9a\u4f4d\u51c6\u786e\u4f46\u6b63\u5411\u5b9a\u4f4d\u4e0b\u964d\uff0c\u7a7a\u95f4\u58f0\u97f3\u80fd\u540c\u65f6\u4f20\u9012\u4efb\u52a1\u4fe1\u606f\u5e76\u63d0\u5347\u6e29\u6696\u611f\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u65e5\u76ca\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u7406\u89e3\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u58f0\u97f3\u4e0e\u4eba\u7c7b\u6c9f\u901a\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u58f0\u97f3\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4ea4\u4e92\u6e20\u9053\u3002", "method": "\u901a\u8fc7\u5b9a\u4f4d\u548c\u4ea4\u63a5\u4efb\u52a1\uff0c\u63a2\u7d22\u4e86Kinova Gen3\u673a\u68b0\u81c2\u7684\u56e0\u679c\u6027\u548c\u529f\u80fd\u6027\u58f0\u97f3\u6548\u679c\uff0c\u5305\u62ec\u7a7a\u95f4\u58f0\u97f3\u7684\u65b0\u9896\u7814\u7a76\u3002", "result": "\u673a\u68b0\u81c2\u7684\u64cd\u4f5c\u58f0\u97f3\u672a\u5bf9\u611f\u77e5\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u4fa7\u5411\u7a7a\u95f4\u5b9a\u4f4d\u9ad8\u5ea6\u51c6\u786e\u4f46\u6b63\u5411\u5b9a\u4f4d\u4e0b\u964d\uff0c\u7a7a\u95f4\u58f0\u97f3\u80fd\u540c\u65f6\u4f20\u8fbe\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u5e76\u4fc3\u8fdb\u6e29\u6696\u611f\u3001\u51cf\u5c11\u4e0d\u9002\u3002", "conclusion": "\u529f\u80fd\u6027\u548c\u53d8\u9769\u6027\u542c\u89c9\u8bbe\u8ba1\u6709\u6f5c\u529b\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u58f0\u97f3\u7684\u4ea4\u4e92\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.16122", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16122", "abs": "https://arxiv.org/abs/2511.16122", "authors": ["Qing Zhang", "Bing Xu", "Xudong Zhang", "Yifan Shi", "Yang Li", "Chen Zhang", "Yik Chung Wu", "Ngai Wong", "Yijie Chen", "Hong Dai", "Xiansen Chen", "Mian Zhang"], "title": "ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models", "comment": null, "summary": "The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u96c6\u6210\u5b66\u4e60\u7684\u63d0\u793a\u4f18\u5316\u6846\u67b6ELPO\uff0c\u901a\u8fc7\u6295\u7968\u673a\u5236\u3001\u5171\u4eab\u751f\u6210\u7b56\u7565\u548c\u4e0d\u540c\u641c\u7d22\u65b9\u6cd5\u63d0\u5347\u63d0\u793a\u4f18\u5316\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5355\u4e00\u6a21\u578b\u6216\u7b97\u6cd5\uff0c\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "ELPO\u6846\u67b6\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u601d\u60f3\uff0c\u7ed3\u5408\u6295\u7968\u673a\u5236\u3001\u5171\u4eab\u751f\u6210\u7b56\u7565\u548c\u591a\u79cd\u641c\u7d22\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u63d0\u793a\u751f\u6210\u548c\u641c\u7d22\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eELPO\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5982\u5728ArSarcasm\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u63d0\u9ad8\u4e867.6\u5206\u3002", "conclusion": "ELPO\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u63d0\u793a\u4f18\u5316\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.15995", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15995", "abs": "https://arxiv.org/abs/2511.15995", "authors": ["Zili Tang", "Ying Zhang", "Meng Guo"], "title": "PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization", "comment": "20 pages, 24 figures. Accepted to IEEE Transactions on Robotics (T-RO), 2025", "summary": "Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u534f\u4f5c\u63a8\u52a8\u4efb\u610f\u5f62\u72b6\u7269\u4f53\u5230\u76ee\u6807\u4f4d\u7f6e\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u6df7\u5408\u6267\u884c\u7b56\u7565\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5904\u7406\u63a5\u89e6\u4e30\u5bcc\u7684\u63a8\u52a8\u4efb\u52a1\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u4eba\u6ca1\u6709\u673a\u68b0\u81c2\uff0c\u8bb8\u591a\u7269\u4f53\u4e0d\u9002\u5408\u6293\u53d6\u64cd\u4f5c\uff08\u5982\u5927\u7bb1\u5b50\u548c\u5706\u67f1\u4f53\uff09\u3002\u63a8\u52a8\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u975e\u6293\u53d6\u6280\u80fd\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5047\u8bbe\u9884\u5b9a\u4e49\u7684\u63a8\u52a8\u6a21\u5f0f\u548c\u56fa\u5b9a\u5f62\u72b6\u7269\u4f53\u3002\u672c\u6587\u89e3\u51b3\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u63a7\u5236\u673a\u5668\u4eba\u8230\u961f\u534f\u4f5c\u63a8\u52a8\u591a\u4e2a\u4efb\u610f\u5f62\u72b6\u7269\u4f53\u5230\u5404\u81ea\u76ee\u7684\u5730\u7684\u901a\u7528\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u7ec4\u5408\u6df7\u5408\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a(I) \u63a8\u52a8\u5b50\u4efb\u52a1\u7684\u5206\u89e3\u3001\u6392\u5e8f\u548c\u6eda\u52a8\u5206\u914d\u7ed9\u673a\u5668\u4eba\u5b50\u7ec4\uff1b(II) \u5173\u952e\u5e27\u5f15\u5bfc\u7684\u6df7\u5408\u641c\u7d22\u4f18\u5316\u6bcf\u4e2a\u5b50\u4efb\u52a1\u7684\u53c2\u6570\u5316\u63a8\u52a8\u6a21\u5f0f\u5e8f\u5217\uff1b(III) \u6df7\u5408\u63a7\u5236\u6267\u884c\u8fd9\u4e9b\u6a21\u5f0f\u5e76\u5728\u5b83\u4eec\u4e4b\u95f4\u8f6c\u6362\u3002\u8fd8\u91c7\u7528\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u52a0\u901f\u5668\u6765\u9884\u6d4b\u5173\u952e\u5e27\u548c\u63a8\u52a8\u6a21\u5f0f\u3002", "result": "\u8be5\u6846\u67b6\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u662f\u5b8c\u5907\u7684\u3002\u5728\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u5e7f\u6cdb\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u673a\u5668\u4eba\u6570\u91cf\u548c\u4e00\u822c\u5f62\u72b6\u7269\u4f53\u4e0b\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u5e76\u63a8\u5e7f\u5230\u5f02\u6784\u673a\u5668\u4eba\u3001\u5e73\u9762\u88c5\u914d\u548c6D\u63a8\u52a8\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5728\u7ebf\u4efb\u52a1\u534f\u8c03\u3001\u63a5\u89e6\u6a21\u5f0f\u5207\u6362\u548c\u6b20\u9a71\u52a8\u7b49\u6311\u6218\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u975e\u6293\u53d6\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16147", "abs": "https://arxiv.org/abs/2511.16147", "authors": ["Dabiao Ma", "Ziming Dai", "Zhimin Xin", "Shu Wang", "Ye Wang", "Haojun Fei"], "title": "TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating", "comment": "11 pages, 3 figures", "summary": "In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Token-Selective PEFT\uff08TS-PEFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5bf9\u4f4d\u7f6e\u7d22\u5f15\u5b50\u96c6\u5e94\u7528PEFT\u4fee\u6539\uff0c\u6311\u6218\u4e86\u4f20\u7edfPEFT\u5bf9\u6240\u6709\u4f4d\u7f6e\u7d22\u5f15\u8fdb\u884c\u4fee\u6539\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u4f20\u7edfPEFT\u65b9\u6cd5\u5bf9\u6240\u6709\u4f4d\u7f6e\u7d22\u5f15\u5e94\u7528\u4fee\u6539\uff0c\u4f46\u4f5c\u8005\u8d28\u7591\u8fd9\u79cd\u5168\u9762\u4fee\u6539\u7684\u5fc5\u8981\u6027\uff0c\u8ba4\u4e3a\u53ef\u80fd\u65e2\u591a\u4f59\u53c8\u9002\u5f97\u5176\u53cd\u3002", "method": "\u5f15\u5165TS-PEFT\u8303\u5f0f\uff0c\u4f7f\u7528\u51fd\u6570S\u9009\u62e9\u6027\u5730\u5bf9\u4f4d\u7f6e\u7d22\u5f15\u5b50\u96c6\u5e94\u7528PEFT\u4fee\u6539\uff0c\u800c\u975e\u5bf9\u6240\u6709\u7d22\u5f15\u8fdb\u884c\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6240\u6709\u7d22\u5f15\u4e0d\u52a0\u9009\u62e9\u5730\u5e94\u7528PEFT\u4e0d\u4ec5\u591a\u4f59\uff0c\u800c\u4e14\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002", "conclusion": "\u4e3aPEFT\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4e3b\u5f20\u91c7\u7528\u66f4\u6709\u9488\u5bf9\u6027\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765\u4f18\u5316\u5927\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2511.16048", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.16048", "abs": "https://arxiv.org/abs/2511.16048", "authors": ["Qing Zhang", "Jing Huang", "Mingyang Xu", "Jun Rekimoto"], "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud", "comment": "NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems", "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\"\u4f4e\u7cbe\u5ea6\"\u673a\u5668\u4eba\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u548c\u53d9\u4e8b\u601d\u7ef4\u521b\u5efa\u5177\u6709\u4e2a\u6027\u7684\u4e0d\u5b8c\u7f8e\u673a\u5668\u4eba\u4f34\u4fa3\uff0c\u800c\u975e\u8ffd\u6c42\u4f20\u7edf\u673a\u5668\u4eba\u7684\u7cbe\u786e\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u6280\u672f\u7684\u521b\u9020\u6027\u6f5c\u529b\uff0c\u6311\u6218\u4e3b\u6d41\u673a\u5668\u4eba\u8ffd\u6c42\u7cbe\u786e\u6027\u548c\u5b8c\u7f8e\u6027\u80fd\u7684\u4f20\u7edf\u89c2\u5ff5\uff0c\u901a\u8fc7\"\u7269\u7406\u6545\u969c\"\u548c\u8bed\u4e49\u5bfc\u822a\u521b\u9020\u5177\u6709\u4e2a\u6027\u7684\u673a\u5668\u4eba\u827a\u672f\u88c5\u7f6e\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u4e3b\u7ba1\u9053\uff0c\u6452\u5f03LiDAR\u548cSLAM\u7b49\u4f20\u7edf\u4f20\u611f\u5668\uff0c\u4ec5\u4f9d\u8d56\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u8fdb\u884c\u5bfc\u822a\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e3a\u673a\u5668\u4eba\u8d4b\u4e88\u751f\u7269\u542f\u53d1\u7684\u4e2a\u6027\u3002", "result": "13\u5206\u949f\u81ea\u4e3b\u98de\u884c\u65e5\u5fd7\u548c\u540e\u7eed\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u521b\u5efa\u53ef\u91cf\u5316\u7684\u4e0d\u540c\u4e2a\u6027\uff0c\u5e76\u6d8c\u73b0\u51fa\u57fa\u4e8e\u5730\u6807\u7684\u5bfc\u822a\u548c\"\u8ba1\u5212\u5230\u6267\u884c\"\u5dee\u8ddd\u7b49\u884c\u4e3a\u3002", "conclusion": "\u5c55\u793a\u4e86\u4e00\u4e2a\u4f4e\u7cbe\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u5efa\u4e0d\u5b8c\u7f8e\u7684\u673a\u5668\u4eba\u4f34\u4fa3\uff0c\u5176\u6210\u529f\u6807\u51c6\u5728\u4e8e\u4e2a\u6027\u7279\u5f81\u800c\u975e\u6548\u7387\uff0c\u4f53\u73b0\u4e86\u673a\u5668\u4eba\u6280\u672f\u7684\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.16198", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2511.16198", "abs": "https://arxiv.org/abs/2511.16198", "authors": ["Sebastian Haan"], "title": "SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning", "comment": "21 pages, 4 figures", "summary": "Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.", "AI": {"tldr": "SemanticCite\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5168\u6587\u5206\u6790\u9a8c\u8bc1\u5f15\u6587\u51c6\u786e\u6027\uff0c\u63d0\u4f9b\u8be6\u7ec6\u63a8\u7406\u548c\u76f8\u5173\u6587\u672c\u7247\u6bb5\u6765\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u5b66\u672f\u6587\u732e\u9762\u4e34\u8bed\u4e49\u5f15\u7528\u9519\u8bef\u3001AI\u751f\u6210\u5e7b\u89c9\u5f15\u7528\u4ee5\u53ca\u4f20\u7edf\u5f15\u7528\u683c\u5f0f\u65e0\u6cd5\u6307\u660e\u5177\u4f53\u652f\u6301\u6bb5\u843d\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u79d1\u5b66\u4ea4\u6d41\u7684\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u591a\u79cd\u68c0\u7d22\u65b9\u6cd5\u548c\u56db\u7c7b\u5206\u7c7b\u7cfb\u7edf\uff08\u652f\u6301\u3001\u90e8\u5206\u652f\u6301\u3001\u4e0d\u652f\u6301\u3001\u4e0d\u786e\u5b9a\uff09\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f15\u7528\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u6027\u80fd\u4e0e\u5927\u578b\u5546\u4e1a\u7cfb\u7edf\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u663e\u8457\u964d\u4f4e\uff0c\u4f7f\u5927\u89c4\u6a21\u5f15\u7528\u9a8c\u8bc1\u53d8\u5f97\u53ef\u884c\u3002", "conclusion": "SemanticCite\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u5f15\u7528\u9a8c\u8bc1\u3001\u7b80\u5316\u540c\u884c\u8bc4\u5ba1\u548cAI\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u63a7\u5236\uff0c\u4e3a\u7ef4\u62a4\u5927\u89c4\u6a21\u5f15\u7528\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u5f00\u6e90\u57fa\u7840\u3002"}}
{"id": "2511.16050", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16050", "abs": "https://arxiv.org/abs/2511.16050", "authors": ["Takeru Tsunoori", "Masato Kobayashi", "Yuki Uranishi"], "title": "Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers", "comment": null, "summary": "Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua", "AI": {"tldr": "Bi-AQUA\u662f\u9996\u4e2a\u57fa\u4e8e\u53cc\u8fb9\u63a7\u5236\u7684\u6c34\u4e0b\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u7ea7\u5149\u7167\u9002\u5e94\u673a\u5236\u89e3\u51b3\u6c34\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5149\u7167\u53d8\u5316\u6311\u6218\uff0c\u5728\u771f\u5b9e\u6c34\u4e0b\u62fe\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u65e0\u5149\u7167\u5efa\u6a21\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u9762\u4e34\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u8272\u5f69\u5931\u771f\u548c\u80fd\u89c1\u5ea6\u964d\u4f4e\u7b49\u6839\u672c\u6027\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u8fd9\u4e9b\u73af\u5883\u6761\u4ef6\u7684\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u4e09\u7ea7\u5149\u7167\u9002\u5e94\u673a\u5236\uff1a1\uff09\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u7167\u660e\u7f16\u7801\u5668\u4eceRGB\u56fe\u50cf\u63d0\u53d6\u5149\u7167\u8868\u793a\uff1b2\uff09\u901a\u8fc7FiLM\u8c03\u5236\u89c6\u89c9\u4e3b\u5e72\u7279\u5f81\u8fdb\u884c\u81ea\u9002\u5e94\u5149\u7167\u611f\u77e5\u7279\u5f81\u63d0\u53d6\uff1b3\uff09\u5728transformer\u7f16\u7801\u5668\u8f93\u5165\u4e2d\u6dfb\u52a0\u663e\u5f0f\u5149\u7167token\u8fdb\u884c\u4efb\u52a1\u611f\u77e5\u6761\u4ef6\u5316\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6c34\u4e0b\u62fe\u53d6\u4efb\u52a1\u4e2d\uff0cBi-AQUA\u5728\u4e0d\u540c\u9759\u6001\u548c\u52a8\u6001\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u65e0\u5149\u7167\u5efa\u6a21\u7684\u53cc\u8fb9\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u6240\u6709\u4e09\u4e2a\u5149\u7167\u611f\u77e5\u7ec4\u4ef6\u90fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c06\u9646\u5730\u53cc\u8fb9\u63a7\u5236\u6a21\u4eff\u5b66\u4e60\u4e0e\u6c34\u4e0b\u64cd\u4f5c\u8fde\u63a5\u8d77\u6765\uff0c\u5b9e\u73b0\u4e86\u5728\u6311\u6218\u6027\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u529b\u654f\u611f\u81ea\u4e3b\u64cd\u4f5c\u3002"}}
{"id": "2511.16275", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16275", "abs": "https://arxiv.org/abs/2511.16275", "authors": ["Xingtao Zhao", "Hao Peng", "Dingli Su", "Xianghua Zeng", "Chunyang Liu", "Jinzhi Liao", "Philip S. Yu"], "title": "SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs", "comment": "14 pages of main text and 10 pages of appendices", "summary": "Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.", "AI": {"tldr": "\u63d0\u51faSemantic Structural Entropy (SeSE)\u6846\u67b6\uff0c\u4ece\u7ed3\u6784\u4fe1\u606f\u89d2\u5ea6\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u4e8e\u5e7b\u89c9\u68c0\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u4e49\u6982\u7387\u5206\u5e03\u6216\u6210\u5bf9\u8ddd\u79bb\uff0c\u5ffd\u7565\u4e86\u6f5c\u5728\u7684\u8bed\u4e49\u7ed3\u6784\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0d\u591f\u7cbe\u786e\u3002\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\uff0c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8e\u907f\u514d\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u81ea\u9002\u5e94\u7a00\u758f\u5316\u6709\u5411\u8bed\u4e49\u56fe\u6784\u5efa\u7b97\u6cd5\uff0c\u6355\u83b7\u65b9\u5411\u6027\u8bed\u4e49\u4f9d\u8d56\u5e76\u81ea\u52a8\u4fee\u526a\u5f15\u5165\u8d1f\u9762\u5e72\u6270\u7684\u8fde\u63a5\uff1b\u901a\u8fc7\u5c42\u6b21\u62bd\u8c61\u5229\u7528\u6f5c\u5728\u8bed\u4e49\u7ed3\u6784\u4fe1\u606f\uff0c\u5c06SeSE\u5b9a\u4e49\u4e3a\u6700\u4f18\u8bed\u4e49\u7f16\u7801\u6811\u7684\u7ed3\u6784\u71b5\uff0c\u5f62\u5f0f\u5316\u8bed\u4e49\u7a7a\u95f4\u5728\u6700\u4f18\u538b\u7f29\u540e\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u572829\u4e2a\u6a21\u578b-\u6570\u636e\u96c6\u7ec4\u5408\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSeSE\u663e\u8457\u4f18\u4e8e\u5148\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u57fa\u7ebf\uff0c\u5305\u62ec\u5f3a\u76d1\u7763\u65b9\u6cd5\u548c\u6700\u8fd1\u63d0\u51fa\u7684KLE\u65b9\u6cd5\u3002", "conclusion": "SeSE\u6846\u67b6\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u7ed3\u6784\u4fe1\u606f\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u6709\u6548\u68c0\u6d4b\u5e7b\u89c9\u751f\u6210\uff0c\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2511.16158", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16158", "abs": "https://arxiv.org/abs/2511.16158", "authors": ["Lara Bergmann", "Cedric Grothues", "Klaus Neumann"], "title": "MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics", "comment": null, "summary": "Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MagBotSim\uff0c\u4e00\u4e2a\u7528\u4e8e\u78c1\u60ac\u6d6e\u7cfb\u7edf\u7684\u7269\u7406\u4eff\u771f\u5e73\u53f0\uff0c\u5c06\u78c1\u60ac\u6d6e\u7cfb\u7edf\u89c6\u4e3a\u673a\u5668\u4eba\u96c6\u7fa4\uff0c\u65e8\u5728\u63a8\u52a8\u4e0b\u4e00\u4ee3\u5236\u9020\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "motivation": "\u78c1\u60ac\u6d6e\u7cfb\u7edf\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u4e0d\u4ec5\u80fd\u5b9e\u73b0\u52a8\u6001\u8fd0\u8f93\uff0c\u8fd8\u5177\u5907\u672a\u88ab\u5f00\u53d1\u7684\u64cd\u7eb5\u80fd\u529b\u3002\u901a\u8fc7\u5c06\u8fd0\u8f93\u548c\u64cd\u7eb5\u529f\u80fd\u6574\u5408\u5230\u534f\u8c03\u7684\u78c1\u673a\u5668\u4eba\u96c6\u7fa4\u4e2d\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5236\u9020\u7cfb\u7edf\u7684\u6548\u7387\u3001\u9002\u5e94\u6027\u548c\u7d27\u51d1\u6027\u3002", "method": "\u5f00\u53d1\u4e86MagBotSim\uff08\u78c1\u673a\u5668\u4eba\u4eff\u771f\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u78c1\u60ac\u6d6e\u7cfb\u7edf\u4eff\u771f\u5e73\u53f0\uff0c\u5c06\u78c1\u60ac\u6d6e\u7cfb\u7edf\u6846\u67b6\u5316\u4e3a\u673a\u5668\u4eba\u96c6\u7fa4\uff0c\u5e76\u63d0\u4f9b\u4e13\u7528\u4eff\u771f\u5de5\u5177\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86MagBotSim\u4eff\u771f\u5e73\u53f0\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6587\u6863\u3001\u89c6\u9891\u3001\u5b9e\u9a8c\u548c\u4ee3\u7801\u8d44\u6e90\uff0c\u4e3a\u78c1\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u5c06\u78c1\u60ac\u6d6e\u7cfb\u7edf\u89c6\u4e3a\u673a\u5668\u4eba\u96c6\u7fa4\u5e76\u63d0\u4f9b\u4e13\u7528\u4eff\u771f\u5de5\u5177\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u7531\u78c1\u673a\u5668\u4eba\u9a71\u52a8\u7684\u5236\u9020\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u5236\u9020\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2511.16324", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16324", "abs": "https://arxiv.org/abs/2511.16324", "authors": ["Wei Xia", "Zhi-Hong Deng"], "title": "SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.", "AI": {"tldr": "SDA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u5206\u5e03\u6a21\u578b\u8f93\u51fa\u6982\u7387\u6765\u589e\u5f3aLLM\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u5bf9\u9f50\uff0c\u57283H\u7ef4\u5ea6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u786e\u4fdd\u5176\u54cd\u5e94\u4e0e\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\u6210\u4e3a\u57fa\u7840\u6027\u6311\u6218\uff0c\u9700\u8981\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u91cd\u8bad\u7ec3\u6216\u5927\u91cf\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u6709\u6548\u4e14\u9ad8\u6548\u5730\u5bf9\u9f50\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u63d0\u51faSDA\uff08Steering-Driven Distribution Alignment\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u7684\u5bf9\u9f50\u6307\u4ee4\u52a8\u6001\u91cd\u5206\u5e03\u6a21\u578b\u8f93\u51fa\u6982\u7387\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u589e\u5f3a\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u8f7b\u91cf\u7ea7\u3001\u8d44\u6e90\u9ad8\u6548\uff0c\u517c\u5bb9\u591a\u79cd\u5f00\u6e90LLM\u3002", "result": "\u57288\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u6765\u6e90\u7684\u5f00\u6e90LLM\u4e0a\u8bc4\u4f30\uff0cSDA\u5728\u5e2e\u52a9\u6027\u3001\u65e0\u5bb3\u6027\u548c\u8bda\u5b9e\u6027\u4e09\u4e2a\u5173\u952e\u5bf9\u9f50\u7ef4\u5ea6\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1a\u5e2e\u52a9\u6027\u5e73\u5747\u63d0\u534764.4%\uff0c\u8bda\u5b9e\u6027\u63d0\u534730%\uff0c\u65e0\u5bb3\u6027\u63d0\u534711.5%\u3002", "conclusion": "SDA\u662f\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u80fd\u591f\u8de8\u4e0d\u540c\u6a21\u578b\u548c\u5e94\u7528\u573a\u666f\u4e00\u81f4\u63d0\u5347\u5bf9\u9f50\u6027\u80fd\uff0c\u652f\u6301\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50\uff0c\u5e76\u53ef\u72ec\u7acb\u4f7f\u7528\u6216\u4e0e\u57fa\u4e8e\u8bad\u7ec3\u7684\u5bf9\u9f50\u7b56\u7565\u96c6\u6210\u3002"}}
{"id": "2511.16200", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16200", "abs": "https://arxiv.org/abs/2511.16200", "authors": ["Kewei Chen", "Yayu Long", "Mingsheng Shang"], "title": "PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks", "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Multi-robot systems in complex physical collaborations face a \"shared brain dilemma\": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace \"raw data communication\" with \"semantic communication\" by performing \"semantic distillation\" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the \"shared brain dilemma\" in resource-constrained multi-robot systems.", "AI": {"tldr": "PIPHEN\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u84b8\u998f\u5c06\u9ad8\u7ef4\u611f\u77e5\u6570\u636e\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u7269\u7406\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\"\u5171\u4eab\u5927\u8111\u56f0\u5883\"\uff0c\u663e\u8457\u964d\u4f4e\u5e26\u5bbd\u9700\u6c42\u548c\u51b3\u7b56\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u590d\u6742\u7269\u7406\u534f\u4f5c\u4e2d\u9762\u4e34\u7684\u9ad8\u7ef4\u591a\u5a92\u4f53\u6570\u636e\u4f20\u8f93\u5e26\u6765\u7684\u5e26\u5bbd\u74f6\u9888\u548c\u51b3\u7b56\u5ef6\u8fdf\u95ee\u9898\uff0c\u5373\"\u5171\u4eab\u5927\u8111\u56f0\u5883\"\u3002", "method": "\u63d0\u51faPIPHEN\u5206\u5e03\u5f0f\u7269\u7406\u8ba4\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u5305\u542b\u7269\u7406\u4ea4\u4e92\u9884\u6d4b\u7f51\u7edc(PIPN)\u8fdb\u884c\u8bed\u4e49\u84b8\u998f\uff0c\u4ee5\u53ca\u57fa\u4e8e\u80fd\u91cf\u5b88\u6052\u7684\u54c8\u5bc6\u987f\u80fd\u91cf\u7f51\u7edc(HEN)\u63a7\u5236\u5668\u3002", "result": "\u5c06\u4fe1\u606f\u8868\u793a\u538b\u7f29\u81f3\u539f\u59cb\u6570\u636e\u91cf\u76845%\u4ee5\u4e0b\uff0c\u534f\u4f5c\u51b3\u7b56\u5ef6\u8fdf\u4ece315ms\u964d\u81f376ms\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u89e3\u51b3\"\u5171\u4eab\u5927\u8111\u56f0\u5883\"\u63d0\u4f9b\u4e86\u6839\u672c\u6027\u7684\u9ad8\u6548\u8303\u5f0f\u3002"}}
{"id": "2511.16331", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16331", "abs": "https://arxiv.org/abs/2511.16331", "authors": ["Jiashu Yao", "Heyan Huang", "Shuang Zeng", "Chuwei Luo", "WangJie You", "Jie Tang", "Qingsong Liu", "Yuhang Guo", "Yangyang Kang"], "title": "Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement", "comment": "Accepted to AAAI 2026", "summary": "Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only \"simple\" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u6211\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u91cd\u5199\u81ea\u8eab\u63a8\u7406\u6587\u672c\u5e76\u4ece\u4e2d\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff0c\u5728\u4fdd\u6301RL\u7b97\u6cd5\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u51c6\u786e\u6027\u548c\u63a8\u7406\u957f\u5ea6\u5e73\u8861\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4ec5\u5173\u6ce8\u6700\u7ec8\u6b63\u786e\u6027\u7684\u5956\u52b1\u673a\u5236\u65e0\u6cd5\u5bf9\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u8be6\u7ec6\u76d1\u7763\uff0c\u5bfc\u81f4\u63a8\u7406\u8d28\u91cf\u4e0d\u4f73\uff0c\u51fa\u73b0\u8fc7\u5ea6\u601d\u8003\u3001\u601d\u8003\u4e0d\u8db3\u3001\u5197\u4f59\u601d\u8003\u548c\u601d\u7ef4\u6df7\u4e71\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9009\u62e9\u6027\u91cd\u5199\u65b9\u6cd5\uff0c\u4ec5\u5bf9\u6a21\u578b\u6301\u7eed\u6b63\u786e\u7684\"\u7b80\u5355\"\u6837\u672c\u8fdb\u884c\u91cd\u5199\uff0c\u5c06\u91cd\u5199\u548c\u539f\u59cb\u751f\u6210\u7f16\u8bd1\u5728\u5355\u4e2a\u6279\u6b21\u4e2d\uff0c\u4fdd\u6301RL\u7b97\u6cd5\u53ef\u6269\u5c55\u6027\u4e14\u4ec5\u5f15\u5165\u7ea610%\u5f00\u9500\u3002", "result": "\u5728\u51c6\u786e\u7387-\u957f\u5ea6\u6743\u8861\u65b9\u9762\uff0c\u81ea\u6211\u91cd\u5199\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u63d0\u53470.6%\u7684\u540c\u65f6\u663e\u8457\u7f29\u77ed\u63a8\u7406\u957f\u5ea646%\uff0c\u5728\u5185\u90e8\u63a8\u7406\u8d28\u91cf\u65b9\u9762\uff0cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u5206\u663e\u8457\u63d0\u9ad87.2\u5206\u3002", "conclusion": "\u81ea\u6211\u91cd\u5199\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u5185\u90e8\u63a8\u7406\u7f3a\u9677\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2511.16223", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16223", "abs": "https://arxiv.org/abs/2511.16223", "authors": ["Vincenzo Pomponi", "Paolo Franceschi", "Stefano Baraldo", "Loris Roveda", "Oliver Avram", "Luca Maria Gambardella", "Anna Valente"], "title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks", "comment": null, "summary": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.", "AI": {"tldr": "DynaMimicGen (D-MG) \u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\u5c31\u80fd\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\uff0c\u7279\u522b\u652f\u6301\u52a8\u6001\u4efb\u52a1\u8bbe\u7f6e\u3002\u5b83\u4f7f\u7528\u52a8\u6001\u8fd0\u52a8\u57fa\u5143(DMPs)\u5c06\u6f14\u793a\u884c\u4e3a\u9002\u5e94\u5230\u65b0\u73af\u5883\u548c\u52a8\u6001\u53d8\u5316\u4e2d\uff0c\u751f\u6210\u5e73\u6ed1\u3001\u771f\u5b9e\u7684\u7b1b\u5361\u5c14\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u6765\u5b66\u4e60\u9c81\u68d2\u7684\u64cd\u4f5c\u7b56\u7565\uff0c\u4f46\u6570\u636e\u6536\u96c6\u8017\u65f6\u8d39\u529b\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e0d\u5b9e\u7528\u3002\u9700\u8981\u4e00\u79cd\u4ece\u5c11\u91cf\u4eba\u7c7b\u76d1\u7763\u4e2d\u8bad\u7ec3\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u52a8\u6001\u4efb\u52a1\u8bbe\u7f6e\u3002", "method": "\u7ed9\u5b9a\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\uff0cD-MG\u9996\u5148\u5c06\u6f14\u793a\u5206\u5272\u6210\u6709\u610f\u4e49\u7684\u5b50\u4efb\u52a1\uff0c\u7136\u540e\u5229\u7528\u52a8\u6001\u8fd0\u52a8\u57fa\u5143(DMPs)\u6765\u9002\u5e94\u548c\u6cdb\u5316\u6f14\u793a\u884c\u4e3a\u5230\u65b0\u7684\u52a8\u6001\u53d8\u5316\u73af\u5883\u3002\u76f8\u6bd4\u4f9d\u8d56\u9759\u6001\u5047\u8bbe\u6216\u7b80\u5355\u8f68\u8ff9\u63d2\u503c\u7684\u65b9\u6cd5\uff0cD-MG\u751f\u6210\u5e73\u6ed1\u3001\u771f\u5b9e\u4e14\u4efb\u52a1\u4e00\u81f4\u7684\u7b1b\u5361\u5c14\u8f68\u8ff9\u3002", "result": "\u5728D-MG\u751f\u6210\u6570\u636e\u4e0a\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u4ee3\u7406\u5728\u957f\u671f\u89c6\u91ce\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u5806\u53e0\u7acb\u65b9\u4f53\u548c\u5c06\u676f\u5b50\u653e\u5165\u62bd\u5c49\u7b49\u4efb\u52a1\uff0c\u5373\u4f7f\u5728\u4e0d\u53ef\u9884\u6d4b\u7684\u73af\u5883\u53d8\u5316\u4e0b\u4e5f\u80fd\u826f\u597d\u6267\u884c\u3002", "conclusion": "D-MG\u901a\u8fc7\u6d88\u9664\u5bf9\u5927\u91cf\u4eba\u7c7b\u6f14\u793a\u7684\u9700\u6c42\u5e76\u652f\u6301\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6cdb\u5316\uff0c\u4e3a\u624b\u52a8\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u5f3a\u5927\u800c\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.16345", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16345", "abs": "https://arxiv.org/abs/2511.16345", "authors": ["Blake Matheny", "Phuong Minh Nguyen", "Minh Le Nguyen", "Stephanie Reynolds"], "title": "NLP Datasets for Idiom and Figurative Language Tasks", "comment": "32 pages, 10 figures", "summary": "Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.", "AI": {"tldr": "\u672c\u6587\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u4e60\u8bed\u548c\u6bd4\u55bb\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4e60\u8bed\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u548c\u8bc4\u4f30\u5c55\u793a\u4e86\u8fd9\u4e9b\u6570\u636e\u96c6\u5728\u63d0\u5347\u6a21\u578b\u5904\u7406\u6bd4\u55bb\u8bed\u8a00\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e60\u8bed\u548c\u6bd4\u55bb\u8bed\u8a00\u5728\u53e3\u8bed\u548c\u4e66\u9762\u8bed\u4e2d\u5360\u5f88\u5927\u6bd4\u4f8b\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8fd9\u7c7b\u8bed\u8a00\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u867d\u7136\u5fae\u8c03\u65b9\u6cd5\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u4f46\u66f4\u597d\u66f4\u5927\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u73b0\u6709\u4e60\u8bed\u548c\u6bd4\u55bb\u8bed\u8a00\u6570\u636e\u96c6\u83b7\u53d6\u7efc\u5408\u4e60\u8bed\u5217\u8868\uff0c\u4ece\u5927\u578b\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u4e0a\u4e0b\u6587\u5e8f\u5217\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6f5c\u5728\u4e60\u8bed\u548c\u6bd4\u55bb\u8bed\u8a00\u8868\u8fbe\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e24\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u786e\u5207\u4e60\u8bed\u548c\u6bd4\u55bb\u8bed\u8a00\u8868\u8fbe\u6570\u636e\u96c6\u3002", "result": "\u521b\u5efa\u7684\u6570\u636e\u96c6\u7ecf\u8fc7\u540e\u5904\u7406\u4ee5\u5b9e\u73b0\u6a21\u578b\u65e0\u5173\u7684\u8bad\u7ec3\u517c\u5bb9\u6027\uff0c\u5e76\u5728\u69fd\u4f4d\u6807\u6ce8\u548c\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u4e3a\u6784\u5efa\u65b0\u6a21\u578b\u548c\u5f00\u53d1\u65b0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u7684\u7c7b\u522b\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6bd4\u55bb\u8bed\u8a00\u7684\u80fd\u529b\u3002"}}
{"id": "2511.16233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16233", "abs": "https://arxiv.org/abs/2511.16233", "authors": ["Kewei Chen", "Yayu Long", "Shuai Li", "Mingsheng Shang"], "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models", "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.", "AI": {"tldr": "FT-NCFM\u662f\u4e00\u4e2a\u6570\u636e\u4e2d\u5fc3\u7684\u751f\u6210\u5f0f\u6570\u636e\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u5b9e\u8ffd\u8e2a\u5f15\u64ce\u8bc4\u4f30\u6837\u672c\u5185\u5728\u4ef7\u503c\uff0c\u7136\u540e\u5408\u6210\u6a21\u578b\u65e0\u5173\u3001\u4fe1\u606f\u5bc6\u96c6\u7684\u53ef\u91cd\u7528\u6570\u636e\u8d44\u4ea7\uff0c\u4ec5\u97005%\u7684\u84b8\u998f\u6838\u5fc3\u96c6\u5c31\u80fd\u8fbe\u5230\u5b8c\u6574\u6570\u636e\u96c685-90%\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u3001\u5197\u4f59\u4e14\u4ef7\u503c\u4e0d\u5747\u6570\u636e\u96c6\u7684\u8fc7\u5ea6\u4f9d\u8d56\u95ee\u9898\uff0c\u73b0\u6709\u6a21\u578b\u4e2d\u5fc3\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u6570\u636e\u5c42\u9762\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5305\u542b\u56e0\u679c\u5f52\u56e0\u548c\u7a0b\u5e8f\u5316\u5bf9\u6bd4\u9a8c\u8bc1\u7684\u4e8b\u5b9e\u8ffd\u8e2a\u5f15\u64ce\u8bc4\u4f30\u6837\u672c\u4ef7\u503c\uff0c\u7136\u540e\u901a\u8fc7\u5bf9\u6297\u6027NCFM\u8fc7\u7a0b\u5408\u6210\u6a21\u578b\u65e0\u5173\u7684\u4fe1\u606f\u5bc6\u96c6\u6570\u636e\u8d44\u4ea7\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41VLA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u75285%\u7684\u84b8\u998f\u6838\u5fc3\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u5c31\u80fd\u8fbe\u5230\u5b8c\u6574\u6570\u636e\u96c685-90%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u51cf\u5c1180%\u4ee5\u4e0a\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u667a\u80fd\u6570\u636e\u84b8\u998f\u662f\u6784\u5efa\u9ad8\u6548\u9ad8\u6027\u80fdVLA\u6a21\u578b\u7684\u6781\u5177\u524d\u666f\u7684\u65b0\u8def\u5f84\u3002"}}
{"id": "2511.16353", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16353", "abs": "https://arxiv.org/abs/2511.16353", "authors": ["Jonathan Kamp", "Lisa Beinborn", "Antske Fokkens"], "title": "Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies", "comment": "Long paper accepted to the main conference of AACL 2025. Please cite the conference proceedings when available", "summary": "Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff08rationales\uff09\u7684\u5145\u5206\u6027\u6307\u6807\uff0c\u53d1\u73b0\u5176\u4e0d\u80fd\u6709\u6548\u8861\u91cf\u6a21\u578b\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u63a2\u8ba8\u4e86rationales\u5728token\u5206\u7c7b\u548c\u6ce8\u610f\u529b\u6b63\u5219\u5316\u4e2d\u7684\u4e0d\u540c\u4f5c\u7528\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528rationales\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u57fa\u4e8e\u6b63\u786e\u539f\u56e0\u5b66\u4e60\u65f6\uff0c\u5145\u5206\u6027\u6307\u6807\u63d0\u4f9b\u7684\u4fe1\u606f\u6709\u9650\uff0c\u9700\u8981\u66f4\u6df1\u5165\u5730\u7406\u89e3rationales\u4fe1\u606f\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5c06\u5145\u5206\u6027\u4e0e\u4e24\u79cd\u5efa\u6a21\u8303\u5f0f\u5173\u8054\uff1atoken\u5206\u7c7b\u80fd\u529b\uff08\u8bc6\u522brationales\u4e2d\u7684token\uff09\u548c\u6ce8\u610f\u529b\u6b63\u5219\u5316\uff08\u5728\u8f93\u5165\u4e2d\u878d\u5165rationales\uff09\uff0c\u5206\u6790rationales\u7684\u5b9e\u9645\u6548\u679c\u3002", "result": "\u53d1\u73b0\u9ad8\u5ea6\u4fe1\u606f\u5316\u7684rationales\u4e0d\u4e00\u5b9a\u6709\u52a9\u4e8e\u6b63\u786e\u5206\u7c7b\uff1b\u5145\u5206\u6027\u5b9e\u9645\u4e0a\u6355\u6349\u7684\u662f\u975erationales\u4e0a\u4e0b\u6587\u7684\u5206\u7c7b\u5f71\u54cd\uff1b\u878d\u5165rationales\u4fe1\u606f\u53ef\u4ee5\u63d0\u5347\u8de8\u57df\u5206\u7c7b\u4f46\u7ed3\u679c\u4e0d\u4e00\u81f4\uff1b\u5145\u5206\u6027\u4e0etoken\u5206\u7c7b\u65e0\u5173\u3002", "conclusion": "rationales\u5177\u6709\u590d\u6742\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7cfb\u7edf\u6355\u6349\u8fd9\u7c7b\u4fe1\u606f\u7684\u6307\u6807\u8fdb\u884c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.16262", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16262", "abs": "https://arxiv.org/abs/2511.16262", "authors": ["Oliver Bimber", "Karl Dietrich von Ellenrieder", "Michael Haller", "Rakesh John Amala Arokia Nathan", "Gianni Lunardi", "Marco Camurri", "Mohamed Youssef", "Santos Miguel Orozco Soto", "Jeremy E. Niven"], "title": "How Robot Dogs See the Unseeable", "comment": null, "summary": "Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u62df\u52a8\u7269\u4fa7\u5411\u6446\u52a8\u7684\u51dd\u89c6\u884c\u4e3a\uff0c\u673a\u5668\u4eba\u53ef\u4ee5\u5b9e\u73b0\u5408\u6210\u5b54\u5f84\u611f\u77e5\uff0c\u6709\u6548\u6d88\u9664\u906e\u6321\u7269\u5e72\u6270\uff0c\u6062\u590d\u88ab\u906e\u6321\u7684\u80cc\u666f\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u89c6\u89c9\u4e2d\u56e0\u5c0f\u5149\u5708\u548c\u5927\u666f\u6df1\u5bfc\u81f4\u7684\u906e\u6321\u95ee\u9898\uff0c\u4f7f\u524d\u666f\u969c\u788d\u7269\u548c\u80cc\u666f\u7269\u4f53\u90fd\u80fd\u6e05\u6670\u6210\u50cf\uff0c\u5bfc\u81f4\u5173\u952e\u573a\u666f\u4fe1\u606f\u88ab\u906e\u6321\u3002", "method": "\u8ba9\u673a\u5668\u4eba\u6267\u884c\u4fa7\u5411\u6446\u52a8\u8fd0\u52a8\uff0c\u76f8\u673a\u5f62\u6210\u5bbd\u5408\u6210\u5b54\u5f84\uff0c\u901a\u8fc7\u8ba1\u7b97\u6574\u5408\u6355\u83b7\u7684\u56fe\u50cf\u5408\u6210\u6781\u6d45\u666f\u6df1\u56fe\u50cf\uff0c\u6a21\u7cca\u906e\u6321\u5143\u7d20\u540c\u65f6\u4f7f\u80cc\u666f\u6e05\u6670\u805a\u7126\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u611f\u77e5\uff0c\u6062\u590d\u57fa\u672c\u573a\u666f\u7406\u89e3\uff0c\u5e76\u652f\u6301\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u9ad8\u7ea7\u89c6\u89c9\u63a8\u7406\uff0c\u5bf9\u4f20\u7edf\u906e\u6321\u56fe\u50cf\u5931\u6548\u7684\u60c5\u51b5\u6709\u6548\u3002", "conclusion": "\u901a\u8fc7\u51dd\u89c6\u8fd0\u52a8\u5b9e\u73b0\u5408\u6210\u5b54\u5f84\u611f\u77e5\u662f\u590d\u6742\u6742\u4e71\u73af\u5883\u4e2d\u9ad8\u7ea7\u573a\u666f\u7406\u89e3\u7684\u5173\u952e\uff0c\u5c06\u52a8\u7269\u884c\u4e3a\u4e0e\u673a\u5668\u4eba\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u7acb\u5373\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16397", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16397", "abs": "https://arxiv.org/abs/2511.16397", "authors": ["Ren Ma", "Jiantao Qiu", "Chao Xu", "Pei Chu", "Kaiwen Liu", "Pengli Ren", "Yuan Qu", "Jiahui Peng", "Linfeng Hou", "Mengjie Liu", "Lindong Lu", "Wenchang Ning", "Jia Yu", "Rui Min", "Jin Shi", "Haojiong Chen", "Peng Zhang", "Wenjian Zhang", "Qian Jiang", "Zengjie Hu", "Guoqiang Yang", "Zhenxiang Li", "Fukai Shang", "Zhongying Tu", "Wentao Zhang", "Dahua Lin", "Conghui He"], "title": "AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser", "comment": null, "summary": "While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.", "AI": {"tldr": "MinerU-HTML\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684HTML\u5230\u6587\u672c\u63d0\u53d6\u7ba1\u9053\uff0c\u901a\u8fc7\u5e8f\u5217\u6807\u6ce8\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u7f51\u9875\u5185\u5bb9\u63d0\u53d6\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u4fdd\u7559\u7ed3\u6784\u5316\u5143\u7d20\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7f51\u9875\u8bed\u6599\u5e93\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u63d0\u53d6\u5668\uff08\u5982Trafilatura\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u6587\u6863\u7ed3\u6784\u548c\u7ed3\u6784\u5316\u5143\u7d20\uff08\u516c\u5f0f\u3001\u4ee3\u7801\u3001\u8868\u683c\u7b49\uff09\uff0c\u9650\u5236\u4e86\u6570\u636e\u8d28\u91cf\u3002\u4f5c\u8005\u8ba4\u4e3a\u63d0\u5347\u63d0\u53d6\u8d28\u91cf\u5bf9\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u53ef\u80fd\u4e0d\u4e9a\u4e8e\u8fc7\u6ee4\u7b56\u7565\u3002", "method": "\u5c06\u5185\u5bb9\u63d0\u53d6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e8f\u5217\u6807\u6ce8\u95ee\u9898\uff0c\u4f7f\u75280.6B\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u683c\u5f0f\u5316\u7ba1\u9053\uff1a\u9996\u5148\u5bf9\u8bed\u4e49\u5143\u7d20\u8fdb\u884c\u663e\u5f0f\u5206\u7c7b\uff0c\u7136\u540e\u8f6c\u6362\u4e3aMarkdown\u683c\u5f0f\u3002", "result": "\u5728MainWebBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMinerU-HTML\u8fbe\u523081.8% ROUGE-N F1\uff0c\u663e\u8457\u4f18\u4e8eTrafilatura\u768463.6%\u3002\u5728\u7ed3\u6784\u5316\u5143\u7d20\u4fdd\u7559\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff1a\u4ee3\u7801\u575790.9%\uff0c\u516c\u5f0f94.0%\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u6784\u5efa\u7684AICC\u8bed\u6599\u5e93\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523050.8%\uff0c\u6bd4Trafilatura\u63d0\u53d6\u7684\u8bed\u6599\u5e93\u9ad81.08\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "HTML\u63d0\u53d6\u662f\u7f51\u9875\u8bed\u6599\u5e93\u6784\u5efa\u4e2d\u5173\u952e\u4f46\u5e38\u88ab\u4f4e\u4f30\u7684\u7ec4\u4ef6\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u63d0\u53d6\u65b9\u6cd5\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6539\u8fdb\u8def\u5f84\u3002"}}
{"id": "2511.16265", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16265", "abs": "https://arxiv.org/abs/2511.16265", "authors": ["Haru Fukatsu", "Ryoji Yasuda", "Yuki Funabora", "Shinji Doki"], "title": "Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist", "comment": "8 pages, 8 figures. This work has been submitted to the IEEE for possible publication", "summary": "This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86Funabot-Upper\u89e6\u89c9\u5957\u88c5\uff0c\u901a\u8fc7\u72ec\u7acb\u523a\u6fc0\u5173\u8282\u548c\u808c\u8089\uff0c\u5728\u8eaf\u5e72\u3001\u80a9\u90e8\u3001\u8098\u90e8\u548c\u624b\u8155\u7b4914\u4e2a\u4e0a\u534a\u8eab\u52a8\u4f5c\u4e2d\u8bf1\u5bfc\u52a8\u89c9\u611f\u77e5\uff0c\u76f8\u6bd4\u524d\u4ee3\u8bbe\u8ba1\u663e\u8457\u51cf\u5c11\u4e86\u611f\u77e5\u6df7\u6dc6\uff0c\u8bc6\u522b\u51c6\u786e\u7387\u4ece68.8%\u63d0\u5347\u81f394.6%\u3002", "motivation": "\u73b0\u6709\u53ef\u7a7f\u6234\u89e6\u89c9\u8bbe\u5907\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u8eab\u4f53\u90e8\u4f4d\u7684\u9a8c\u8bc1\uff0c\u96be\u4ee5\u5728\u591a\u4e2a\u8eab\u4f53\u90e8\u4f4d\u5e94\u7528\u76f8\u540c\u65b9\u6cd5\u3002\u524d\u4ee3Funabot-Suit\u5728\u80a9\u90e8\u548c\u8098\u90e8\u4e4b\u95f4\u51fa\u73b0\u4e86\u611f\u77e5\u6df7\u6dc6\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u8bbe\u8ba1\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u4e86\u65b0\u7684\u7b80\u5316\u8bbe\u8ba1\u7b56\u7565\uff0c\u901a\u8fc7\u72ec\u7acb\u523a\u6fc0\u5173\u8282\u548c\u808c\u8089\u6765\u8bf1\u5bfc\u52a8\u89c9\u611f\u77e5\u3002\u5229\u7528\u4eba\u5de5\u808c\u8089\u6536\u7f29\u4f7f\u670d\u88c5\u4e09\u7ef4\u53d8\u5f62\u7684\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u80fd\u72ec\u7acb\u523a\u6fc0\u8eaf\u5e72\u3001\u80a9\u90e8\u3001\u8098\u90e8\u548c\u624b\u8155\u7684\u65b0\u578b\u89e6\u89c9\u5957\u88c5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eFunabot-Upper\u6210\u529f\u5728\u591a\u4e2a\u5173\u8282\u8bf1\u5bfc\u52a8\u89c9\u611f\u77e5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u524d\u4ee3\u8bbe\u8ba1\u4e2d\u89c2\u5bdf\u5230\u7684\u611f\u77e5\u6df7\u6dc6\u3002\u65b0\u5957\u88c5\u76f8\u6bd4\u524d\u4ee3Funabot-Suit\uff0c\u8bc6\u522b\u51c6\u786e\u7387\u4ece68.8%\u663e\u8457\u63d0\u5347\u81f394.6%\u3002", "conclusion": "\u65b0\u8bbe\u8ba1\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u808c\u8089\u523a\u6fc0\u5bfc\u81f4\u7684\u611f\u77e5\u6df7\u6dc6\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8be5\u89e6\u89c9\u5957\u88c5\u5728\u591a\u4e2a\u4e0a\u534a\u8eab\u5173\u8282\u8bf1\u5bfc\u52a8\u89c9\u611f\u77e5\u7684\u4f18\u8d8a\u6027\u548c\u672a\u6765\u89e6\u89c9\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.16416", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16416", "abs": "https://arxiv.org/abs/2511.16416", "authors": ["Connor McElroy", "Thiago E. A. de Oliveira", "Chris Brogly"], "title": "Classification of worldwide news articles by perceived quality, 2018-2024", "comment": null, "summary": "This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533a\u5206\u611f\u77e5\u4f4e\u8d28\u91cf\u4e0e\u9ad8\u8d28\u91cf\u65b0\u95fb\u6587\u7ae0\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528141\u4e07\u7bc7\u65b0\u95fb\u6587\u7ae0\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fModernBERT-large\uff09\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u662f\u5426\u80fd\u591f\u6709\u6548\u533a\u5206\u611f\u77e5\u4f4e\u8d28\u91cf\u548c\u9ad8\u54c1\u8d28\u7684\u65b0\u95fb\u6587\u7ae0\uff0c\u4ee5\u5e2e\u52a9\u81ea\u52a8\u8bc6\u522b\u65b0\u95fb\u8d28\u91cf\u3002", "method": "\u4f7f\u75283\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548c3\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e141\u4e07\u7bc7\u82f1\u8bed\u65b0\u95fb\u6587\u7ae0\u6570\u636e\u96c6\uff0c\u6bcf\u7bc7\u6587\u7ae0\u6709194\u4e2a\u8bed\u8a00\u7279\u5f81\uff0c\u6309\u4e13\u5bb6\u8bc4\u5206\u5c06\u65b0\u95fb\u6e90\u5206\u4e3a\u4f4e\u8d28\u91cf\u548c\u9ad8\u54c1\u8d28\u4e24\u7c7b\u3002", "result": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff08\u5982\u968f\u673a\u68ee\u6797\uff09\u8868\u73b0\u826f\u597d\uff08\u51c6\u786e\u73870.7355\uff0cROC AUC 0.8131\uff09\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0cModernBERT-large\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff08\u51c6\u786e\u73870.8744\uff0cROC-AUC 0.9593\uff0cF1 0.8739\uff09\u3002", "conclusion": "\u4f20\u7edfCPU\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u90fd\u80fd\u6709\u6548\u533a\u5206\u5168\u7403\u65b0\u95fb\u6587\u7ae0\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u5176\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2511.16306", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16306", "abs": "https://arxiv.org/abs/2511.16306", "authors": ["Lasse Hohmeyer", "Mihaela Popescu", "Ivan Bergonzani", "Dennis Mronga", "Frank Kirchner"], "title": "InEKFormer: A Hybrid State Estimator for Humanoid Robots", "comment": "Accepted at The 22nd International Conference on Advanced Robotics (ICAR 2025)", "summary": "Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot's floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. Due to recent advances in the field of machine learning, deep learning methods are increasingly used for state estimation tasks. In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network. We compare our method with the InEKF and the KalmanNet approaches on datasets obtained from the humanoid robot RH5. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548cTransformer\u7f51\u7edc\u7684\u65b0\u578b\u6df7\u5408\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5InEKFormer\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u5e76\u5728RH5\u4eba\u5f62\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u53cc\u8db3\u8fd0\u52a8\u4ecd\u7136\u9762\u4e34\u7a33\u5b9a\u6027\u548c\u52a8\u6001\u6027\u6311\u6218\uff0c\u72b6\u6001\u4f30\u8ba1\u5bf9\u63d0\u4f9b\u5feb\u901f\u51c6\u786e\u7684\u673a\u5668\u4eba\u6d6e\u52a8\u57fa\u5ea7\u72b6\u6001\u53cd\u9988\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\u8c03\u6574\u566a\u58f0\u53c2\u6570\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e3a\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u4e86InEKFormer\u6df7\u5408\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5c06\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e0eTransformer\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u5728RH5\u4eba\u5f62\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u4e0eInEKF\u548cKalmanNet\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660eTransformer\u5728\u4eba\u5f62\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u540c\u65f6\u4e5f\u51f8\u663e\u4e86\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\u9700\u8981\u9c81\u68d2\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\u3002", "conclusion": "InEKFormer\u5c55\u793a\u4e86Transformer\u7f51\u7edc\u5728\u4eba\u5f62\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u524d\u666f\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u81ea\u56de\u5f52\u8bad\u7ec3\u7b56\u7565\u6765\u5904\u7406\u9ad8\u7ef4\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\u3002"}}
{"id": "2511.16438", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16438", "abs": "https://arxiv.org/abs/2511.16438", "authors": ["Sherine George", "Nithish Saji"], "title": "ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports", "comment": "Workshop paper accepted at AI4DF 2025 (part of ACM ICAIF 2025). 3 pages including tables and figures", "summary": "We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.", "AI": {"tldr": "ESGBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u53ef\u89e3\u91caESG\u95ee\u7b54\u7cfb\u7edf\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u4f01\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\u7684\u95ee\u9898\u3001\u4eba\u5de5\u6807\u6ce8\u7684\u7b54\u6848\u548c\u8bc1\u636e\u652f\u6301\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30ESG\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u63a8\u52a8\u900f\u660e\u548c\u8d1f\u8d23\u4efb\u7684ESG\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5305\u542b\u591aESG\u4e3b\u9898\u9886\u57df\u95ee\u9898\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u914d\u5907\u4eba\u5de5\u6807\u6ce8\u7684\u7b54\u6848\u548c\u8bc1\u636e\u652f\u6301\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5206\u6790\u4e86\u6700\u5148\u8fdbLLM\u5728ESGBench\u4e0a\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u4e86\u5728\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u9886\u57df\u5bf9\u9f50\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "ESGBench\u65e8\u5728\u52a0\u901f\u900f\u660e\u548c\u8d1f\u8d23\u4efbESG\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u4e3a\u8bc4\u4f30\u53ef\u89e3\u91caESG\u95ee\u7b54\u63d0\u4f9b\u6807\u51c6\u5316\u6846\u67b6\u3002"}}
{"id": "2511.16330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16330", "abs": "https://arxiv.org/abs/2511.16330", "authors": ["Shreyas Kumar", "Ravi Prakash"], "title": "Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86C-GMS\u6846\u67b6\uff0c\u7ed3\u5408DMP\u548cVIC\u7b56\u7565\u5b66\u4e60\uff0c\u901a\u8fc7\u6570\u5b66\u5b9a\u4e49\u7684\u7a33\u5b9a\u589e\u76ca\u8c03\u5ea6\u6d41\u5f62\u4fdd\u8bc1Lyapunov\u7a33\u5b9a\u6027\u548c\u6267\u884c\u5668\u53ef\u884c\u6027\uff0c\u65e0\u9700\u5956\u52b1\u60e9\u7f5a\u6216\u540e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u534f\u4f5c\u6280\u80fd\u5b66\u4e60\u4e2d\u5b58\u5728\u4e0d\u7a33\u5b9a\u548c\u4e0d\u5b89\u5168\u63a2\u7d22\u98ce\u9669\uff0c\u7279\u522b\u662f\u963b\u6297\u589e\u76ca\u65f6\u53d8\u7279\u6027\u5e26\u6765\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u8ba4\u8bc1\u9ad8\u65af\u6d41\u5f62\u91c7\u6837(C-GMS)\u6846\u67b6\uff0c\u5c06\u7b56\u7565\u63a2\u7d22\u91cd\u6784\u4e3a\u4ece\u6570\u5b66\u5b9a\u4e49\u7684\u7a33\u5b9a\u589e\u76ca\u8c03\u5ea6\u6d41\u5f62\u4e2d\u91c7\u6837\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7b56\u7565rollout\u90fd\u7a33\u5b9a\u4e14\u7269\u7406\u53ef\u5b9e\u73b0\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u6709\u754c\u6a21\u578b\u8bef\u5dee\u548c\u90e8\u7f72\u65f6\u4e0d\u786e\u5b9a\u6027\u4e0b\u4e5f\u80fd\u4fdd\u8bc1\u6709\u754c\u8ddf\u8e2a\u8bef\u5dee\u3002", "conclusion": "C-GMS\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u9760\u81ea\u4e3b\u4ea4\u4e92\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u7a33\u5b9a\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2511.16467", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16467", "abs": "https://arxiv.org/abs/2511.16467", "authors": ["Andrew Gomes"], "title": "Anatomy of an Idiom: Tracing Non-Compositionality in Language Models", "comment": null, "summary": "We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u4e60\u8bed\u8868\u8fbe\uff0c\u53d1\u73b0\u4e86\u4e60\u8bed\u5904\u7406\u7684\u72ec\u7279\u8ba1\u7b97\u6a21\u5f0f\uff0c\u5305\u62ec\"\u4e60\u8bed\u5934\"\u548c\"\u589e\u5f3a\u63a5\u6536\"\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3Transformer\u6a21\u578b\u5982\u4f55\u5904\u7406\u975e\u7ec4\u5408\u6027\u8bed\u8a00\uff08\u5982\u4e60\u8bed\uff09\uff0c\u63a2\u7d22\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u673a\u5236\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684\u8def\u5f84\u4fee\u8865\u7b97\u6cd5\u8fdb\u884c\u7535\u8def\u53d1\u73b0\u548c\u5206\u6790\uff0c\u8bc6\u522b\u51fa\"\u4e60\u8bed\u5934\"\uff08\u5728\u591a\u4e2a\u4e60\u8bed\u4e2d\u9891\u7e41\u6fc0\u6d3b\u7684\u6ce8\u610f\u529b\u5934\uff09\u548c\"\u589e\u5f3a\u63a5\u6536\"\uff08\u4e60\u8bedtoken\u95f4\u56e0\u65e9\u671f\u5904\u7406\u800c\u589e\u5f3a\u7684\u6ce8\u610f\u529b\uff09\u73b0\u8c61\u3002", "result": "\u53d1\u73b0\u4e60\u8bed\u5904\u7406\u5177\u6709\u72ec\u7279\u7684\u8ba1\u7b97\u6a21\u5f0f\uff0c\u8bc6\u522b\u51fa\u7279\u5b9a\u7684\u4e60\u8bed\u5904\u7406\u7535\u8def\uff0c\u8fd9\u4e9b\u7535\u8def\u5c55\u793a\u4e86Transformer\u5728\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u673a\u5236\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7406\u89e3Transformer\u5982\u4f55\u5904\u7406\u975e\u7ec4\u5408\u6027\u8bed\u8a00\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u4e3a\u7406\u89e3\u66f4\u590d\u6742\u8bed\u6cd5\u7ed3\u6784\u7684\u5904\u7406\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2511.16372", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16372", "abs": "https://arxiv.org/abs/2511.16372", "authors": ["Bowen Xu", "Zexuan Yan", "Minghao Lu", "Xiyu Fan", "Yi Luo", "Youshen Lin", "Zhiqiang Chen", "Yeke Chen", "Qiyuan Qiao", "Peng Lu"], "title": "Flow-Aided Flight Through Dynamic Clutters From Point To Motion", "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L), November, 2025", "summary": "Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u98de\u884c\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u6fc0\u5149\u96f7\u8fbe\u611f\u77e5\u5b9e\u73b0\u4ece\u70b9\u5230\u8fd0\u52a8\u7684\u76f4\u63a5\u63a7\u5236\uff0c\u65e0\u9700\u76ee\u6807\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u9884\u6d4b\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u907f\u969c\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u5efa\u6a21\u969c\u788d\u7269\u8fd0\u52a8\uff0c\u4f46\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u906e\u6321\u573a\u666f\u4e2d\u8017\u65f6\u4e14\u4e0d\u53ef\u9760\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u611f\u77e5\u8ddd\u79bb\u56fe\u548c\u70b9\u6d41\u4f5c\u4e3a\u73af\u5883\u53d8\u5316\u611f\u77e5\u8868\u793a\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\uff0c\u901a\u8fc7\u76f8\u5bf9\u8fd0\u52a8\u8c03\u5236\u7684\u8ddd\u79bb\u573a\u9a71\u52a8\u907f\u969c\u884c\u4e3a\u3002", "result": "\u7cfb\u7edf\u5728\u6a21\u62df\u5668\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u6210\u529f\u7387\u548c\u9002\u5e94\u6027\uff0c\u4e14\u80fd\u5728\u771f\u5b9e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u5b9e\u73b0\u5b89\u5168\u673a\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u73af\u5883\u8868\u793a\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u6742\u4e71\u73af\u5883\u4e2d\u9ad8\u6548\u53ef\u9760\u7684\u81ea\u4e3b\u98de\u884c\uff0c\u65e0\u9700\u590d\u6742\u7684\u8fd0\u52a8\u5efa\u6a21\u3002"}}
{"id": "2511.16470", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16470", "abs": "https://arxiv.org/abs/2511.16470", "authors": ["Mateusz Chili\u0144ski", "Julita O\u0142tusek", "Wojciech Ja\u015bkowski"], "title": "Arctic-Extract Technical Report", "comment": null, "summary": "Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.", "AI": {"tldr": "Arctic-Extract\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u6027\u80fd\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u4ece\u626b\u63cf\u6216\u6570\u5b57\u5546\u52a1\u6587\u6863\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\uff0c\u53ef\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u90e8\u7f72\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u5b9e\u73b0\u5148\u8fdb\u6027\u80fd\u53c8\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u6587\u6863\u7406\u89e3\u6a21\u578b\uff0c\u89e3\u51b3\u957f\u6587\u6863\u5904\u7406\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7279\u5b9a\u7684\u8bad\u7ec3\u534f\u8bae\uff0c\u4f18\u5316\u6a21\u578b\u5927\u5c0f\u548c\u6027\u80fd\u5e73\u8861\uff0c\u4f7f\u6a21\u578b\u4ec5\u91cd6.6 GiB\uff0c\u53ef\u5728A10 GPU\u7b49\u6709\u9650\u8d44\u6e90\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002", "result": "\u6a21\u578b\u5728A10 GPU\u4e0a\u53ef\u5904\u7406\u591a\u8fbe125\u9875A4\u6587\u6863\uff0c\u5728\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Arctic-Extract\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u5148\u8fdb\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6587\u6863\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16390", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16390", "abs": "https://arxiv.org/abs/2511.16390", "authors": ["Ajith Anil Meera", "Poppy Collis", "Polina Arbuzova", "Abi\u00e1n Torres", "Paul F Kinghorn", "Ricardo Sanz", "Pablo Lanillos"], "title": "Robot Metacognition: Decision Making with Confidence for Tool Invention", "comment": "under review", "summary": "Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u67b6\u6784\uff0c\u7528\u4e8e\u8bc4\u4f30\u51b3\u7b56\u53ef\u9760\u6027\uff0c\u5e76\u5728\u81ea\u4e3b\u5de5\u5177\u53d1\u660e\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u667a\u80fd\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5373\u5bf9\u81ea\u8eab\u8ba4\u77e5\u8fc7\u7a0b\u548c\u51b3\u7b56\u7684\u53cd\u601d\u80fd\u529b\u3002\u8fd9\u79cd\u81ea\u6211\u76d1\u63a7\u80fd\u529b\u5bf9\u4e8e\u5b66\u4e60\u3001\u51b3\u7b56\u548c\u95ee\u9898\u89e3\u51b3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\uff0c\u63d0\u51fa\u4ee5\u7f6e\u4fe1\u5ea6\u4e3a\u4e2d\u5fc3\u7684\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u67b6\u6784\uff0c\u5c06\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u673a\u5668\u4eba\u51b3\u7b56\u65b9\u6848\u4e2d\u7684\u5143\u8ba4\u77e5\u5ea6\u91cf\uff0c\u5f3a\u8c03\u5177\u8eab\u884c\u52a8\u76d1\u63a7\u4ee5\u5b9e\u73b0\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002", "result": "\u7f6e\u4fe1\u5ea6\u544a\u77e5\u7684\u673a\u5668\u4eba\u80fd\u591f\u8bc4\u4f30\u5176\u51b3\u7b56\u7684\u53ef\u9760\u6027\uff0c\u5728\u5b9e\u9645\u7269\u7406\u90e8\u7f72\u4e2d\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5728\u81ea\u4e3b\u5de5\u5177\u53d1\u660e\u7528\u4f8b\u4e2d\u6210\u529f\u6f14\u793a\u3002", "conclusion": "\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u67b6\u6784\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u8d28\u91cf\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u7684\u672a\u6765\u5e94\u7528\u548c\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2511.16528", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16528", "abs": "https://arxiv.org/abs/2511.16528", "authors": ["\u00d6zay Ezerceli", "Mahmoud El Hussieni", "Selva Ta\u015f", "Reyhan Bayraktar", "Fatma Bet\u00fcl Terzio\u011flu", "Yusuf \u00c7elebi", "Ya\u011f\u0131z Asker"], "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval", "comment": null, "summary": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5$\\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\\times$ faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.", "AI": {"tldr": "TurkColBERT\u662f\u9996\u4e2a\u9488\u5bf9\u571f\u8033\u5176\u8bed\u68c0\u7d22\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u5bc6\u96c6\u7f16\u7801\u5668\u548c\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u3002\u7ed3\u679c\u663e\u793a\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u5728\u53c2\u6570\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5c0f3-5\u500d\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5bc6\u96c6\u7f16\u7801\u5668\uff0c\u4e14\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u68c0\u7d22\u3002", "motivation": "\u795e\u7ecf\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f62\u6001\u4e30\u5bcc\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u571f\u8033\u5176\u8bed\u4e2d\u7814\u7a76\u4e0d\u8db3\u3002\u5bc6\u96c6\u53cc\u7f16\u7801\u5668\u76ee\u524d\u5728\u571f\u8033\u5176IR\u4e2d\u5360\u4e3b\u5bfc\uff0c\u4f46\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9002\u5e94\u6d41\u7a0b\uff1a\u5148\u5728\u571f\u8033\u5176NLI/STS\u4efb\u52a1\u4e0a\u5fae\u8c03\u82f1\u8bed\u548c\u591a\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u7136\u540e\u4f7f\u7528\u5728MS MARCO-TR\u4e0a\u8bad\u7ec3\u7684PyLate\u5c06\u5176\u8f6c\u6362\u4e3aColBERT\u98ce\u683c\u7684\u68c0\u7d22\u5668\u3002\u8bc4\u4f30\u4e8610\u4e2a\u6a21\u578b\u57285\u4e2a\u571f\u8033\u5176BEIR\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u663e\u793a\u51fa\u5f3a\u5927\u7684\u53c2\u6570\u6548\u7387\uff1a1.0M\u53c2\u6570\u7684colbert-hash-nano-tr\u6bd4600M\u53c2\u6570\u7684turkish-e5-large\u5c0f600\u500d\uff0c\u4f46\u4fdd\u7559\u4e86\u517671%\u4ee5\u4e0a\u7684\u5e73\u5747mAP\u3002\u5c0f3-5\u500d\u7684\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5bc6\u96c6\u7f16\u7801\u5668\uff0cColmmBERT-base-TR\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e0amAP\u63d0\u5347\u8fbe+13.8%\u3002", "conclusion": "TurkColBERT\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u5728\u571f\u8033\u5176\u8bed\u68c0\u7d22\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u53c2\u6570\u6548\u7387\u548c\u7279\u5b9a\u9886\u57df\u6027\u80fd\u65b9\u9762\u3002MUVERA+Rerank\u7d22\u5f15\u7b97\u6cd5\u6bd4PLAID\u5feb3.33\u500d\uff0c\u5e76\u63d0\u4f9b\u4e86+1.7%\u7684\u76f8\u5bf9mAP\u589e\u76ca\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u68c0\u7d22\u3002"}}
{"id": "2511.16406", "categories": ["cs.RO", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2511.16406", "abs": "https://arxiv.org/abs/2511.16406", "authors": ["Luis Luna", "Isaac Chairez", "Andrey Polyakov"], "title": "Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators", "comment": null, "summary": "Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u79fb\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u7684\u540c\u8d28PID\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u540c\u8d28\u63a7\u5236\u7406\u8bba\u589e\u5f3a\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\uff0c\u5728\u5b58\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u5916\u90e8\u5e72\u6270\u65f6\u5b9e\u73b0\u9c81\u68d2\u534f\u8c03\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u7531\u4e8e\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3001\u6b20\u9a71\u52a8\u7279\u6027\u4ee5\u53ca\u57fa\u5ea7\u4e0e\u673a\u68b0\u81c2\u5b50\u7cfb\u7edf\u4e4b\u95f4\u7684\u8026\u5408\uff0c\u5b58\u5728\u663e\u8457\u7684\u63a7\u5236\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u6027\u7684\u9c81\u68d2\u63a7\u5236\u7b56\u7565\u3002", "method": "\u8bbe\u8ba1\u4e86\u540c\u8d28PID\u63a7\u5236\u5668\u7ed3\u6784\uff0c\u5229\u7528\u540c\u8d28\u63a7\u5236\u7406\u8bba\u5c06\u4f20\u7edfPID\u589e\u76ca\u63a8\u5e7f\u4e3a\u975e\u7ebf\u6027\u3001\u72b6\u6001\u76f8\u5173\u7684\u51fd\u6570\uff0c\u901a\u8fc7\u5206\u7ea7\u540c\u8d28\u65b9\u6cd5\u6539\u5584\u8ddf\u8e2a\u8bef\u5dee\u7684\u6536\u655b\u6027\uff0c\u5e76\u4f7f\u7528Lyapunov\u65b9\u6cd5\u8fdb\u884c\u7a33\u5b9a\u6027\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ehPID\u63a7\u5236\u5668\u5728\u79fb\u52a8\u57fa\u5ea7\u548c\u673a\u68b0\u81c2\u7684\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7ebf\u6027PID\u63a7\u5236\u5668\uff0c\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u7a33\u6001\u8bef\u5dee\u548c\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u589e\u5f3a\u4e0b\u4e00\u4ee3\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u5728\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u5206\u6790\u57fa\u7840\u7684\u63a7\u5236\u6846\u67b6\u3002"}}
{"id": "2511.16540", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16540", "abs": "https://arxiv.org/abs/2511.16540", "authors": ["\u00c9lo\u00efse Benito-Rodriguez", "Einar Urdshals", "Jasmina Nasufi", "Nicky Pochinkov"], "title": "Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks", "comment": "13 pages, 5 figures", "summary": "Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.", "AI": {"tldr": "\u57fa\u4e8eLLM\u6fc0\u6d3b\u72b6\u6001\u9884\u6d4b\u6587\u672c\u7c7b\u578b\uff0c\u4f7f\u7528Mistral-7B\u548c\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u901a\u8fc7scikit-learn\u5206\u7c7b\u5668\u5b9e\u73b0\u9ad8\u8fbe98%\u548c71%\u7684F1\u5206\u6570\u3002", "motivation": "\u7406\u89e3LLM\u5bf9\u4e8e\u786e\u4fdd\u5176\u5b89\u5168\u6709\u76ca\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8eLLM\u7ed3\u6784\u96be\u4ee5\u89e3\u91ca\u4e14\u65e0\u6cd5\u5bf9\u6240\u6709\u8f93\u51fa\u8fdb\u884c\u4eba\u5de5\u8bc4\u4f30\uff0c\u9700\u8981\u5f00\u53d1\u9884\u6d4b\u6027\u6846\u67b6\u3002", "method": "\u4f7f\u7528Mistral-7B\u6a21\u578b\uff0c\u57fa\u4e8e\u5176\u6fc0\u6d3b\u72b6\u6001\u9884\u6d4b\u6587\u672c\u7c7b\u578b\uff0c\u91c7\u7528scikit-learn\u5206\u7c7b\u5668\u8fdb\u884c\u6d45\u5c42\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098%\u548c71%\u7684F1\u5206\u6570\uff0c\u7ed3\u679c\u59cb\u7ec8\u4f18\u4e8e\u5bf9\u7167\u4efb\u52a1\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6587\u672c\u7c7b\u578b\u53ef\u4ee5\u4eceLLM\u4e2d\u901a\u8fc7\u6d45\u5c42\u5b66\u4e60\u6a21\u578b\u63a8\u65ad\u51fa\u6765\uff0c\u4e3a\u7406\u89e3LLM\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\u3002"}}
{"id": "2511.16407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16407", "abs": "https://arxiv.org/abs/2511.16407", "authors": ["Xizhou Bu", "Jiexi Lyu", "Fulei Sun", "Ruichen Yang", "Zhiqiang Ma", "Wei Li"], "title": "LAOF: Robust Latent Action Learning with Optical Flow Constraints", "comment": "Code can be found at https://github.com/XizoB/LAOF", "summary": "Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.", "AI": {"tldr": "LAOF\u662f\u4e00\u4e2a\u5229\u7528\u5149\u6d41\u4f5c\u4e3a\u52a8\u4f5c\u9a71\u52a8\u4fe1\u53f7\u7684\u4f2a\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u5bf9\u5e72\u6270\u7269\u5177\u6709\u9c81\u68d2\u6027\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u5927\u89c4\u6a21\u89c6\u9891\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u65f6\u5bb9\u6613\u53d7\u5230\u52a8\u4f5c\u65e0\u5173\u5e72\u6270\u7269\u7684\u5f71\u54cd\uff0c\u800c\u52a8\u4f5c\u76d1\u7763\u7684\u6709\u6548\u6027\u53d7\u5230\u53ef\u7528\u52a8\u4f5c\u6807\u7b7e\u7a00\u7f3a\u7684\u9650\u5236\u3002\u5149\u6d41\u80fd\u591f\u81ea\u7136\u6291\u5236\u80cc\u666f\u5143\u7d20\u5e76\u5f3a\u8c03\u8fd0\u52a8\u7269\u4f53\uff0c\u56e0\u6b64\u88ab\u7528\u4f5c\u52a8\u4f5c\u9a71\u52a8\u4fe1\u53f7\u3002", "method": "\u63d0\u51faLAOF\u6846\u67b6\uff0c\u5229\u7528\u667a\u80fd\u4f53\u7684\u5149\u6d41\u4f5c\u4e3a\u52a8\u4f5c\u9a71\u52a8\u4fe1\u53f7\uff0c\u901a\u8fc7\u5149\u6d41\u7ea6\u675f\u6765\u5b66\u4e60\u5bf9\u5e72\u6270\u7269\u5177\u6709\u9c81\u68d2\u6027\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\u3002\u8fd9\u662f\u4e00\u4e2a\u4f2a\u76d1\u7763\u6846\u67b6\uff0c\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u7279\u522b\u6709\u6548\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLAOF\u5b66\u4e60\u7684\u6f5c\u5728\u8868\u793a\u5728\u4e0b\u6e38\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5149\u6d41\u7ea6\u675f\u663e\u8457\u7a33\u5b9a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5728\u6807\u7b7e\u6781\u5ea6\u7a00\u7f3a\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u6f5c\u5728\u8868\u793a\u7684\u8d28\u91cf\uff0c\u4e14\u5728\u52a8\u4f5c\u6807\u7b7e\u6bd4\u4f8b\u589e\u52a0\u523010%\u65f6\u4ecd\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "\u5373\u4f7f\u6ca1\u6709\u52a8\u4f5c\u76d1\u7763\uff0cLAOF\u4e5f\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4f7f\u75281%\u52a8\u4f5c\u6807\u7b7e\u8bad\u7ec3\u7684\u52a8\u4f5c\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5149\u6d41\u7ea6\u675f\u5728\u6f5c\u5728\u52a8\u4f5c\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.16544", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16544", "abs": "https://arxiv.org/abs/2511.16544", "authors": ["Zachary Ellis", "Jared Joselowitz", "Yash Deo", "Yajie He", "Anna Kalygina", "Aisling Higham", "Mana Rahimzadeh", "Yan Jia", "Ibrahim Habli", "Ernest Lim"], "title": "WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue", "comment": null, "summary": "As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $\u03ba$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86ASR\u8bc4\u4f30\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u8bcd\u9519\u8bef\u7387(WER)\u7684\u505a\u6cd5\uff0c\u53d1\u73b0WER\u7b49\u73b0\u6709\u6307\u6807\u4e0e\u4e34\u5e8a\u5f71\u54cd\u76f8\u5173\u6027\u5dee\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u66ff\u4ee3\u4e13\u5bb6\u4e34\u5e8a\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524dASR\u5728\u4e34\u5e8a\u5bf9\u8bdd\u4e2d\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u6807\u51c6\u8bc4\u4f30\u4ecd\u4e3b\u8981\u4f9d\u8d56WER\uff0c\u9700\u8981\u9a8c\u8bc1WER\u662f\u5426\u4e0e\u8f6c\u5f55\u9519\u8bef\u7684\u4e34\u5e8a\u5f71\u54cd\u76f8\u5173\u3002", "method": "\u5efa\u7acb\u4e13\u5bb6\u4e34\u5e8a\u8bc4\u4f30\u57fa\u51c6\uff0c\u6bd4\u8f83\u771f\u5b9e\u8bdd\u8bed\u4e0eASR\u8f6c\u5f55\u7248\u672c\uff0c\u6807\u6ce8\u4e34\u5e8a\u5f71\u54cd\uff1b\u5f15\u5165\u57fa\u4e8eGEPA\u4f18\u5316\u7684LLM\u8bc4\u4f30\u5668\u6765\u590d\u5236\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "WER\u548c\u73b0\u6709\u6307\u6807\u4e0e\u4e34\u5e8a\u98ce\u9669\u6807\u7b7e\u76f8\u5173\u6027\u5dee\uff1b\u4f18\u5316\u7684LLM\u8bc4\u4f30\u5668(Gemini-2.5-Pro)\u8fbe\u523090%\u51c6\u786e\u7387\u548c0.816\u7684Cohen's \u03ba\uff0c\u6027\u80fd\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5c06ASR\u8bc4\u4f30\u4ece\u7b80\u5355\u7684\u6587\u672c\u4fdd\u771f\u5ea6\u8f6c\u5411\u5fc5\u8981\u7684\u3001\u53ef\u6269\u5c55\u7684\u4e34\u5e8a\u5bf9\u8bdd\u5b89\u5168\u6027\u8bc4\u4f30\u3002"}}
{"id": "2511.16434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16434", "abs": "https://arxiv.org/abs/2511.16434", "authors": ["Chenming Wu", "Xiaofan Li", "Chengkai Dai"], "title": "From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization", "comment": "Technical report (7 pages)", "summary": "The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\\textit{\\underline{S}upport-\\underline{E}ffective \\underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.", "AI": {"tldr": "SEG\u6846\u67b6\u901a\u8fc7\u5c06\u652f\u6301\u7ed3\u6784\u4f18\u5316\u96c6\u6210\u52303D\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u76f4\u63a5\u4f18\u5316\u6a21\u578b\u4ee5\u51cf\u5c11\u652f\u6491\u6750\u6599\u4f7f\u7528\uff0c\u663e\u8457\u964d\u4f4e3D\u6253\u5370\u7684\u6750\u6599\u6d6a\u8d39\u548c\u751f\u4ea7\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d3D\u5207\u7247\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u540e\u5904\u7406\u4f18\u5316\uff0c\u800c\u975e\u5728\u8bbe\u8ba1\u9636\u6bb5\u5c31\u8003\u8651\u652f\u6491\u6548\u7387\u30023D\u6253\u5370\u4e2d\u652f\u6491\u7ed3\u6784\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u4f1a\u9020\u6210\u6750\u6599\u6d6a\u8d39\u548c\u989d\u5916\u751f\u4ea7\u65f6\u95f4\u3002", "method": "SEG\u6846\u67b6\u5c06\u5e26\u504f\u79fb\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316(ODPO)\u96c6\u6210\u52303D\u751f\u6210\u6d41\u7a0b\u4e2d\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a0\u5165\u652f\u6491\u7ed3\u6784\u6a21\u62df\uff0c\u9f13\u52b1\u751f\u6210\u9700\u8981\u8f83\u5c11\u652f\u6491\u7684\u51e0\u4f55\u5f62\u72b6\u3002", "result": "\u5728Thingi10k-Val\u548cGPT-3DP-Val\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEG\u5728\u652f\u6491\u4f53\u79ef\u51cf\u5c11\u548c\u53ef\u6253\u5370\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8eTRELLIS\u3001DPO\u548cDRO\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SEG\u901a\u8fc7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u4f18\u5316\u6a21\u578b\uff0c\u6709\u6f5c\u529b\u6539\u53d83D\u6253\u5370\u5b9e\u8df5\uff0c\u4e3a\u66f4\u53ef\u6301\u7eed\u548c\u9ad8\u6548\u7684\u6570\u5b57\u5236\u9020\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.16577", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16577", "abs": "https://arxiv.org/abs/2511.16577", "authors": ["Kexin Zhao", "Ken Forbus"], "title": "Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation", "comment": "16 pages", "summary": "Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7684\u8bcd\u4e49\u6d88\u6b67\u65b9\u6cd5\uff0c\u4f7f\u7528\u7edf\u8ba1\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6d88\u6b67\u5668\uff0c\u5c06\u7b26\u53f7NLU\u7cfb\u7edf\u751f\u6210\u7684\u5019\u9009\u542b\u4e49\u8f6c\u6362\u4e3a\u53ef\u533a\u5206\u7684\u81ea\u7136\u8bed\u8a00\u66ff\u4ee3\u9879\uff0c\u901a\u8fc7LLM\u9009\u62e9\u9002\u5f53\u89e3\u91ca\u3002", "motivation": "\u5f53\u524d\u8bcd\u4e49\u6d88\u6b67\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7c97\u7c92\u5ea6\u8868\u793a\uff08\u5982WordNet\u540c\u4e49\u8bcd\u96c6\uff09\uff0c\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0c\u96be\u4ee5\u81ea\u52a8\u6d88\u6b67\u66f4\u4e30\u5bcc\u7684\u8868\u793a\uff08\u5982\u57fa\u4e8eOpenCyc\uff09\uff0c\u9650\u5236\u4e86\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06\u7b26\u53f7NLU\u7cfb\u7edf\u751f\u6210\u7684\u591a\u4e2a\u5019\u9009\u542b\u4e49\u8f6c\u6362\u4e3a\u53ef\u533a\u5206\u7684\u81ea\u7136\u8bed\u8a00\u66ff\u4ee3\u9879\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u67e5\u8be2\u5668\uff0c\u6839\u636e\u8bed\u8a00\u4e0a\u4e0b\u6587\u9009\u62e9\u9002\u5f53\u89e3\u91ca\uff0c\u5e76\u5c06\u9009\u5b9a\u542b\u4e49\u4f20\u64ad\u56de\u7b26\u53f7NLU\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u9ec4\u91d1\u7b54\u6848\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7684\u8bcd\u4e49\u6d88\u6b67\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u8868\u793a\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2511.16518", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16518", "abs": "https://arxiv.org/abs/2511.16518", "authors": ["Xiaoshuai Hao", "Lei Zhou", "Zhijian Huang", "Zhiwen Hou", "Yingbo Tang", "Lingfeng Zhang", "Guang Li", "Zheng Lu", "Shuhuai Ren", "Xianhui Meng", "Yuchen Zhang", "Jing Wu", "Jinghui Lu", "Chenxu Dang", "Jiayi Guan", "Jianhua Wu", "Zhiyi Hou", "Hanbing Li", "Shumeng Xia", "Mingliang Zhou", "Yinan Zheng", "Zihao Yue", "Shuhao Gu", "Hao Tian", "Yuannan Shen", "Jianwei Cui", "Wen Zhang", "Shaoqing Xu", "Bing Wang", "Haiyang Sun", "Zeyu Zhu", "Yuncheng Jiang", "Zibin Guo", "Chuhong Gong", "Chaofan Zhang", "Wenbo Ding", "Kun Ma", "Guang Chen", "Rui Cai", "Diyun Xiang", "Heng Qu", "Fuli Luo", "Hangjun Ye", "Long Chen"], "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report", "comment": "Code: https://github.com/XiaomiMiMo/MiMo-Embodied Model: https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B", "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.", "AI": {"tldr": "MiMo-Embodied\u662f\u9996\u4e2a\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eabAI\u9886\u57df\u90fd\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u7684\u8de8\u5177\u8eab\u57fa\u7840\u6a21\u578b\uff0c\u572829\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u65b0\u8bb0\u5f55\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eabAI\u4efb\u52a1\u7684\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u63a2\u7d22\u8fd9\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u6b63\u5411\u8fc1\u79fb\u548c\u76f8\u4e92\u589e\u5f3a\u6548\u5e94\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u5b66\u4e60\u3001\u7cbe\u5fc3\u6784\u5efa\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u601d\u7ef4\u94fe/\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\uff0c\u5c06\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eabAI\u4efb\u52a1\u6574\u5408\u5230\u5355\u4e00\u6a21\u578b\u4e2d\u3002", "result": "\u572817\u4e2a\u5177\u8eabAI\u57fa\u51c6\u6d4b\u8bd5\uff08\u4efb\u52a1\u89c4\u5212\u3001\u529f\u80fd\u9884\u6d4b\u3001\u7a7a\u95f4\u7406\u89e3\uff09\u548c12\u4e2a\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\uff08\u73af\u5883\u611f\u77e5\u3001\u72b6\u6001\u9884\u6d4b\u3001\u9a7e\u9a76\u89c4\u5212\uff09\u4e2d\u5747\u521b\u4e0b\u65b0\u8bb0\u5f55\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u3001\u95ed\u6e90\u548c\u4e13\u7528\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u591a\u9636\u6bb5\u5b66\u4e60\u3001\u6570\u636e\u6784\u5efa\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eabAI\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u6b63\u5411\u8fc1\u79fb\u6548\u5e94\uff0c\u80fd\u591f\u76f8\u4e92\u589e\u5f3a\uff0c\u4e3a\u8de8\u9886\u57df\u57fa\u7840\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.16654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16654", "abs": "https://arxiv.org/abs/2511.16654", "authors": ["Elias Lumer", "Alex Cardenas", "Matt Melich", "Myles Mason", "Sara Dieter", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "Roberto Hernandez"], "title": "Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems", "comment": null, "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u6a21\u6001RAG\u7cfb\u7edf\u4e2d\u4e24\u79cd\u68c0\u7d22\u65b9\u6cd5\uff1a\u57fa\u4e8e\u6587\u672c\u5206\u5757\u68c0\u7d22\uff08\u56fe\u50cf\u5148\u88abLLM\u6458\u8981\u6210\u6587\u672c\uff09\u548c\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\uff08\u56fe\u50cf\u4ee5\u539f\u751f\u5f62\u5f0f\u5b58\u50a8\u5728\u5411\u91cf\u7a7a\u95f4\u4e2d\uff09\u3002\u5b9e\u9a8c\u8868\u660e\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u6458\u8981\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u4f9d\u8d56LLM\u5c06\u56fe\u50cf\u6458\u8981\u6210\u6587\u672c\uff0c\u5bfc\u81f4\u89c6\u89c9\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e22\u5931\uff0c\u5f71\u54cd\u4e0b\u6e38\u68c0\u7d22\u548c\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002", "method": "\u5728\u91d1\u878d\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e866\u4e2aLLM\u6a21\u578b\u548c2\u4e2a\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u6bd4\u8f83\u6587\u672c\u5206\u5757\u68c0\u7d22\u548c\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\u5728mAP@5\u4e0a\u7edd\u5bf9\u63d0\u534713%\uff08\u76f8\u5bf9\u63d0\u534732%\uff09\uff0c\u5728nDCG@5\u4e0a\u7edd\u5bf9\u63d0\u534711%\uff08\u76f8\u5bf9\u63d0\u534720%\uff09\uff0c\u4e14\u4ea7\u751f\u66f4\u51c6\u786e\u3001\u4e8b\u5b9e\u4e00\u81f4\u7684\u56de\u7b54\u3002", "conclusion": "LLM\u6458\u8981\u4f1a\u5728\u9884\u5904\u7406\u9636\u6bb5\u5f15\u5165\u4fe1\u606f\u635f\u5931\uff0c\u800c\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u80fd\u4fdd\u7559\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u4e3a\u68c0\u7d22\u548c\u63a8\u7406\u63d0\u4f9b\u66f4\u597d\u7684\u652f\u6301\u3002"}}
{"id": "2511.16651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16651", "abs": "https://arxiv.org/abs/2511.16651", "authors": ["Yang Tian", "Yuyin Yang", "Yiman Xie", "Zetao Cai", "Xu Shi", "Ning Gao", "Hangxu Liu", "Xuekun Jiang", "Zherui Qiu", "Feng Yuan", "Yaping Li", "Ping Wang", "Junhao Cai", "Jia Zeng", "Hao Dong", "Jiangmiao Pang"], "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy", "comment": null, "summary": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $\u03c0$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $\u03c0_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $\u03c0_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u8bc1\u660e\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u5c31\u80fd\u8fbe\u5230\u6700\u5f3a\u771f\u5b9e\u6570\u636e\u96c6\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u4eff\u771f\u7684\u5de8\u5927\u4ef7\u503c\u3002", "motivation": "\u63a2\u7d22\u771f\u5b9e\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u5bf9VLA\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5f53\u524dVLA\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u800c\u5408\u6210\u6570\u636e\u5c1a\u672a\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u5c55\u73b0\u540c\u7b49\u80fd\u529b\u3002", "method": "\u521b\u5efa\u5305\u542b63\u4e07\u6761\u8f68\u8ff9\u30017433\u5c0f\u65f6\u7684\u5408\u6210\u6570\u636e\u96c6InternData-A1\uff0c\u6db5\u76d64\u79cd\u673a\u5668\u4eba\u5f62\u6001\u300118\u79cd\u6280\u80fd\u300170\u4e2a\u4efb\u52a1\u548c227\u4e2a\u573a\u666f\uff0c\u901a\u8fc7\u9ad8\u5ea6\u81ea\u4e3b\u3001\u5b8c\u5168\u89e3\u8026\u7684\u7ec4\u5408\u5f0f\u4eff\u771f\u6d41\u6c34\u7ebf\u751f\u6210\u3002", "result": "\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u572849\u4e2a\u4eff\u771f\u4efb\u52a1\u30015\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u548c4\u4e2a\u957f\u65f6\u7a0b\u7075\u5de7\u4efb\u52a1\u4e0a\u4e0e\u5b98\u65b9\u03c0_0\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u5e76\u5c55\u73b0\u51fa\u4ee4\u4eba\u60ca\u8bb6\u7684\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u80fd\u591f\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u7528\u4e8eVLA\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u4e3a\u5177\u8eabAI\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u521b\u5efa\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u83b7\u53d6\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u7684\u95e8\u69db\u3002"}}
{"id": "2511.16664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16664", "abs": "https://arxiv.org/abs/2511.16664", "authors": ["Ali Taghibakhshi", "Sharath Turuvekere Sreenivas", "Saurav Muralidharan", "Ruisi Cai", "Marcin Chochowski", "Ameya Sunil Mahabaleshwarkar", "Yoshi Suhara", "Oluwatobi Olabiyi", "Daniel Korzekwa", "Mostofa Patwary", "Mohammad Shoeybi", "Jan Kautz", "Bryan Catanzaro", "Ashwath Aithal", "Nima Tajbakhsh", "Pavlo Molchanov"], "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs", "comment": null, "summary": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.", "AI": {"tldr": "Nemotron Elastic\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u63a8\u7406\u5bfc\u5411LLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u7236\u6a21\u578b\u4e2d\u5d4c\u5165\u591a\u4e2a\u5d4c\u5957\u5b50\u6a21\u578b\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u90e8\u7f72\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u6210\u672c\u8fc7\u9ad8\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u4ecd\u9700\u5927\u91cf\u8bad\u7ec3token\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6848\u6765\u6784\u5efa\u591a\u5c3a\u5ea6\u63a8\u7406\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6df7\u5408Mamba-Attention\u67b6\u6784\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u8def\u7531\u5668\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u7ed3\u5408\u7ec4\u611f\u77e5SSM\u5f39\u6027\u5316\u3001\u5f02\u6784MLP\u5f39\u6027\u5316\u3001\u5f52\u4e00\u5316MSE\u5c42\u91cd\u8981\u6027\u8bc4\u4f30\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u3002", "result": "\u5728Nemotron Nano V2 12B\u6a21\u578b\u4e0a\u5e94\u7528\uff0c\u4ec5\u7528110B\u8bad\u7ec3token\u540c\u65f6\u751f\u62109B\u548c6B\u6a21\u578b\uff0c\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\u6210\u672c\u964d\u4f4e360\u500d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u538b\u7f29\u6280\u672f\u964d\u4f4e7\u500d\uff0c\u5404\u5d4c\u5957\u6a21\u578b\u7cbe\u5ea6\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u591a\u5408\u4e00\u63a8\u7406\u6a21\u578b\uff0c\u90e8\u7f72\u5185\u5b58\u4e0e\u6a21\u578b\u5bb6\u65cf\u6570\u91cf\u65e0\u5173\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u591a\u5c3a\u5ea6\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16661", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16661", "abs": "https://arxiv.org/abs/2511.16661", "authors": ["Irmak Guzey", "Haozhi Qi", "Julen Urain", "Changhao Wang", "Jessica Yin", "Krishna Bodduluri", "Mike Lambeta", "Lerrel Pinto", "Akshara Rai", "Jitendra Malik", "Tingfan Wu", "Akash Sharma", "Homanga Bharadhwaj"], "title": "Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations", "comment": null, "summary": "Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.", "AI": {"tldr": "AINA\u6846\u67b6\u901a\u8fc7Aria Gen 2\u773c\u955c\u6536\u96c6\u4eba\u7c7b\u65e5\u5e38\u4efb\u52a1\u89c6\u9891\u6570\u636e\uff0c\u65e0\u9700\u673a\u5668\u4eba\u6570\u636e\u5373\u53ef\u5b66\u4e60\u591a\u6307\u673a\u5668\u4eba\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4eba\u673a\u4f53\u73b0\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u76ee\u6807\u662f\u4ece\u4eba\u7c7b\u5728\u81ea\u7136\u73af\u5883\u4e2d\u6267\u884c\u65e5\u5e38\u4efb\u52a1\u7684\u89c6\u9891\u4e2d\u5b66\u4e60\u591a\u6307\u673a\u5668\u4eba\u7b56\u7565\uff0c\u51cf\u5c11\u5bf9\u52b3\u52a8\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u8f7b\u4fbf\u7684Aria Gen 2\u773c\u955c\u6536\u96c6\u6570\u636e\uff0c\u8be5\u8bbe\u5907\u914d\u5907\u9ad8\u5206\u8fa8\u7387RGB\u76f8\u673a\u3001\u7cbe\u786e\u76843D\u5934\u90e8\u548c\u624b\u90e8\u59ff\u6001\u8ddf\u8e2a\u4ee5\u53ca\u7acb\u4f53\u89c6\u89c9\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5b66\u4e60\u57fa\u4e8e3D\u70b9\u7684\u591a\u6307\u624b\u7b56\u7565\u3002", "result": "\u5728\u4e5d\u4e2a\u65e5\u5e38\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u7ed3\u679c\uff0c\u7b56\u7565\u5bf9\u80cc\u666f\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u673a\u5668\u4eba\u6570\u636e\uff08\u5305\u62ec\u5728\u7ebf\u4fee\u6b63\u3001\u5f3a\u5316\u5b66\u4e60\u6216\u4eff\u771f\uff09\u5373\u53ef\u76f4\u63a5\u90e8\u7f72\u3002", "conclusion": "AINA\u6846\u67b6\u4f7f\u4ece\u4eba\u7c7b\u89c6\u9891\u5b66\u4e60\u591a\u6307\u673a\u5668\u4eba\u7b56\u7565\u7684\u76ee\u6807\u524d\u8fdb\u4e86\u4e00\u5927\u6b65\uff0c\u4efb\u4f55\u4eba\u3001\u4efb\u4f55\u5730\u70b9\u3001\u4efb\u4f55\u73af\u5883\u90fd\u80fd\u6536\u96c6\u6570\u636e\u7528\u4e8e\u7b56\u7565\u5b66\u4e60\u3002"}}
