<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 本文综述了通过增加推理时计算来提高预训练大语言模型预测准确性的方法，重点关注问题分解方式和子问题组织结构，统一了多种推理增强技术。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，如何在推理阶段通过额外计算资源来提升模型性能成为一个重要研究方向。现有方法如思维链、分支解决合并等虽然有效但缺乏统一的理论框架。

Method: 采用分类学方法，从问题分解和子问题组织结构（顺序、并行、树状）两个维度系统分析推理时扩展技术，将不同方法置于统一框架下进行比较。

Result: 建立了统一的分类框架，揭示了不同推理增强方法的内在联系和差异，为理解现有技术提供了系统性视角。

Conclusion: 通过统一视角整合了多种推理时扩展方法，为未来研究指明了方向，强调需要更深入地理解问题分解策略与计算效率之间的权衡。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [2] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型在推理过程中很早就对最终答案做出内部决策，即使需要更长的输出才能得出明确答案。


<details>
  <summary>Details</summary>
Motivation: 探索链式思维推理中，语言模型在何时内部确定最终结果，以及这种早期承诺对模型推理过程的影响。

Method: 通过在推理过程的前t个token后的隐藏状态上训练线性分类器，预测模型的最终正确性。

Result: 发现模型在仅经过几个token后就能高度预测最终正确性；对于难题，预测准确率下降揭示了选择偏差：难题在长链式思维中比例过高。

Conclusion: 推理模型的内部成功自我评估在仅几个token后就出现，这对可解释性和推理时控制具有重要意义。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [3] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: LiveCLKTBench是一个自动化生成管道，专门用于隔离和测量大型语言模型的跨语言知识迁移能力，通过时间敏感的知识实体来评估真实的知识迁移而非预训练暴露。


<details>
  <summary>Details</summary>
Motivation: 评估跨语言知识迁移具有挑战性，因为目标语言中的正确答案可能来自真正的知识迁移，也可能来自预训练期间的先验暴露。需要一种能够隔离和测量真实知识迁移的方法。

Method: 构建LiveCLKTBench管道：识别真实领域中的自包含、时间敏感知识实体，基于时间发生进行过滤，验证模型知识，生成事实性问题并翻译成多种语言来评估跨语言迁移能力。

Result: 评估多个LLM在五种语言上的表现，发现跨语言迁移受语言距离强烈影响，且在不同语言方向往往不对称。较大模型能改善迁移，但收益随规模增长而递减，且在不同领域表现各异。

Conclusion: LiveCLKTBench为多语言迁移研究提供了新的见解，并证明其作为未来研究可靠基准的价值。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [4] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASS是一个轻量级控制框架，通过基于模型的反馈回路在解码过程中动态调节注意力头，以减少LLM的上下文幻觉问题


<details>
  <summary>Details</summary>
Motivation: LLM经常生成流畅但事实错误的陈述，这是由于它们在上下文知识和参数知识之间分配注意力的方式存在问题。理解和引导这种内部行为对于可信部署和模型机制的科学可解释性至关重要

Method: 引入COMPASS框架，通过上下文依赖分数(CRS)量化上下文依赖度，使用PID控制器动态调节注意力头，无需重新训练或多轮解码

Result: 在多个基准测试中，COMPASS持续减少上下文幻觉率(绝对降低2.8%到5.8%)，并揭示了不同注意力头对证据对齐的贡献

Conclusion: 反馈驱动的可解释性为科学理解LLM行为提供了一条途径

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [5] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 评估手动和自动韵律分割标注对巴西葡萄牙语语音合成质量的影响，发现韵律分割训练能产生更清晰自然的语音，手动分割引入更大变异性有助于更自然的韵律。


<details>
  <summary>Details</summary>
Motivation: 自发语音合成面临捕捉对话自然流程的挑战，包括话轮转换、停顿和不流畅性。现有系统主要通过隐式建模韵律特征，但显式韵律分割数据集及其对自发语音合成的影响尚未充分探索。

Method: 使用非自回归模型FastSpeech 2，在巴西葡萄牙语数据集上评估手动和自动韵律分割标注的效果，分析其对语音合成质量的影响。

Result: 实验结果显示，使用韵律分割训练能产生略微更清晰和声学自然的语音。自动分割产生更规则的片段，手动分割引入更大变异性，有助于更自然的韵律。两种方法都能重现预期的核重音模式，但韵律模型更接近自然的预核轮廓。

Conclusion: 韵律分割标注能提升自发语音合成的质量，手动分割在韵律自然性方面表现更好。所有数据集、源代码和训练模型已公开，支持可重复性和未来研究。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [6] [Human or LLM as Standardized Patients? A Comparative Study for Medical Education](https://arxiv.org/abs/2511.14783)
*Bingquan Zhang,Xiaoxiao Liu,Yuchi Wang,Lei Zhou,Qianqian Xie,Benyou Wang*

Main category: cs.CL

TL;DR: EasyMED是一个多智能体框架，用于模拟标准化患者，通过三个智能体实现真实对话、事实一致性和可操作反馈，在SPBench基准测试中达到与人类标准化患者相当的学习效果，同时提供更好的灵活性、心理安全性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 标准化患者在临床技能培训中不可或缺，但存在成本高、灵活性差和难以扩展的问题。现有的基于大语言模型的模拟器虽然成本较低，但行为不一致且缺乏与人类标准化患者的严格比较。

Method: 提出了EasyMED多智能体框架，包含患者智能体（实现真实对话）、辅助智能体（确保事实一致性）和评估智能体（提供可操作反馈）。同时引入了SPBench基准，包含14个专科的真实SP-医生交互数据和8个专家定义的评估标准。

Result: 实验表明，EasyMED能够匹配人类标准化患者的学习效果，对基础较差的学生产生更大的技能提升，同时提供更好的灵活性、心理安全性和成本效益。

Conclusion: EasyMED框架在临床技能培训中能够有效替代人类标准化患者，提供更灵活、安全和经济的训练解决方案。

Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.

</details>


### [7] [Opinion Mining and Analysis Using Hybrid Deep Neural Networks](https://arxiv.org/abs/2511.14796)
*Adel Hidri,Suleiman Ali Alsaif,Muteeb Alahmari,Eman AlShehri,Minyar Sassi Hidri*

Main category: cs.CL

TL;DR: 提出了一种混合双向门控循环单元和长短期记忆网络（HBGRU-LSTM）模型，用于改进情感分析，在IMDB电影评论和亚马逊产品评价数据集上达到95%的测试准确率，优于传统深度学习框架。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体和电子商务的影响力日益增长，理解客户态度变得至关重要。现有基于词典的方法和传统机器学习技术在处理上下文细微差别和可扩展性方面不足，而深度学习在语义关系捕捉方面表现出改进潜力。

Method: 引入混合深度神经网络模型，结合双向门控循环单元（BGRU）和长短期记忆（LSTM）层，以解决上下文细微差别、可扩展性和类别不平衡等挑战。

Result: HBGRU-LSTM架构达到95%测试准确率，优于LSTM（93.06%）、CNN+LSTM（93.31%）和GRU+LSTM（92.20%）。在平衡数据集上，负面情感召回率从86%提升至96%，误分类损失从20.24%降至13.3%。

Conclusion: 所提出的混合模型在情感分析任务中表现出卓越性能，特别是在处理类别不平衡和上下文细微差别方面，确保了更公平公正的情感分类。

Abstract: Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.

</details>


### [8] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: HTP通过分层令牌预置解决LLM中因果注意力机制导致的信息流限制问题，在块级预置摘要令牌并采用均值池化，显著提升长文档嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的因果注意力机制限制了从后到前的信息流动，降低了表示质量。现有方法通过预置单个摘要令牌来解决，但会过度压缩信息，在长文档上表现不佳。

Method: 提出分层令牌预置(HTP)：1) 将输入分块并在后续块预置块级摘要令牌，创建多条后向信息流路径；2) 用均值池化替代最后令牌池化，缓解读取级过度压缩问题。

Result: 在11个检索数据集和30个通用嵌入基准测试中取得一致性能提升，尤其在长上下文设置下表现突出。

Conclusion: HTP作为一种简单、架构无关的方法，增强了零样本和微调模型，为获得更优的长文档嵌入提供了可扩展路径。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [9] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 提出了一个数学基础框架来理解、测量和缓解大语言模型的幻觉问题，结合概率建模、信息论等方法分析错误传播，并提出改进的不确定性度量和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但仍然容易产生幻觉——听起来合理但实际上不正确或缺乏事实支持的输出。需要系统性地解决这一问题。

Method: 使用概率建模、信息论、三角信号分析和贝叶斯不确定性估计来分析错误的自回归传播，提出改进的不确定性度量（包括语义和相位感知变体），并开发缓解策略如对比解码、检索增强接地、事实对齐和弃权机制。

Result: 建立了一个统一框架，将校准、检索和对齐方面的最新进展联系起来，支持更安全可靠的大语言模型。

Conclusion: 该数学基础框架为理解和缓解大语言模型幻觉问题提供了系统方法，有助于开发更安全可靠的模型。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [10] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: TASA是一个学生感知的数学辅导框架，通过整合学生画像、记忆和遗忘动态来实现个性化学习，相比现有方法能更好地适应学生的知识演变过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅导系统大多未能捕捉学生知识随熟练度、概念差距和遗忘模式的动态演变，特别是在需要精细支架的数学辅导中尤为突出。

Method: TASA维护结构化学生画像（掌握情况）和事件记忆（学习交互记录），结合连续遗忘曲线和知识追踪，动态更新学生掌握状态并生成难度校准的问题和解释。

Result: 实证结果显示TASA相比代表性基线方法实现了更优的学习成果和更自适应的辅导行为。

Conclusion: 在基于LLM的辅导系统中建模时间遗忘和学习者画像至关重要，TASA框架验证了这种整合的有效性。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [11] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 提出了一个评估多语言视觉语言模型在印度语言中表现的可扩展框架，并创建了HinTel-AlignBench基准测试，发现模型在印度语言上的表现相比英语平均下降8.3分（印地语）和5.5分（泰卢固语）。


<details>
  <summary>Details</summary>
Motivation: 解决当前多语言VLM评估的四个主要局限：依赖未验证的自动翻译、任务/领域覆盖范围窄、样本量有限、缺乏文化和本地来源的问答数据。

Method: 开发半自动数据集创建框架，结合回译、过滤和人工验证；创建包含英语对齐样本的印地语和泰卢固语基准测试，包括适应英语数据集和本地印度数据集。

Result: 在5个任务中，4个任务在印度语言上的表现相比英语出现退化，平均退化幅度为印地语8.3分、泰卢固语5.5分。

Conclusion: 多语言多模态理解需要改进，特别是在印度语言上的表现存在显著差距，需要更好的评估方法和模型优化。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [12] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文通过跨编码器分析、语言特征和稀疏自编码器，首次全面研究了内在维度与可解释文本属性之间的关系，发现ID与基于熵的指标互补，在不同文本类型中呈现稳定分层，并识别出影响ID的因果特征。


<details>
  <summary>Details</summary>
Motivation: 内在维度是现代LLM分析中的重要工具，但其文本决定因素尚未得到充分探索。本文旨在通过可解释的文本属性来理解ID，填补这一研究空白。

Method: 采用跨编码器分析、语言特征提取和稀疏自编码器（SAEs）方法，结合文本长度控制、类型分层分析和因果特征识别。

Result: 发现ID与熵指标不相关；不同文本类型呈现稳定ID分层（科学文本约8，百科内容约9，创意/观点写作约10.5）；通过SAEs识别出科学信号降低ID，人性化信号增加ID的因果特征。

Conclusion: 当代模型认为科学写作相对"简单"，而小说、观点和情感内容增加了表示自由度。研究为ID的正确使用和基于ID结果的合理解释提供了实践指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [13] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: OEMA是一个零样本临床命名实体识别框架，通过多智能体协作解决传统方法对标注数据的依赖问题，在MTSamples和VAERS数据集上达到最先进的精确匹配性能。


<details>
  <summary>Details</summary>
Motivation: 临床命名实体识别需要从电子健康记录中提取信息，但监督模型如CRF和BioClinicalBERT需要昂贵的标注数据。零样本NER虽然减少这种依赖，但在示例选择粒度和提示与自我改进集成方面存在困难。

Method: 提出OEMA框架，包含三个组件：自标注器生成示例、鉴别器通过SNOMED CT过滤示例、预测器使用实体描述进行准确推理，通过本体引导推理和多智能体协作实现。

Result: 在MTSamples和VAERS数据集上，OEMA实现了最先进的精确匹配性能。在相关匹配下，与监督的BioClinicalBERT相当并超过CRF。

Conclusion: OEMA通过本体引导推理和多智能体协作解决了零样本NER的关键挑战，实现了接近监督学习的性能，在临床NLP应用中显示出前景。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [14] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: 提出C3上下文级联压缩方法，使用大小不同的两个LLM级联处理文本压缩和解码任务，在20倍压缩比下达到98%解码准确率，40倍压缩比下保持93%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决百万级token长上下文任务对LLMs带来的计算和内存挑战，探索文本压缩的上限。

Method: 级联两个不同大小的LLM：小型LLM作为第一阶段将长上下文压缩为一组潜在token（32或64长度），大型LLM作为第二阶段在压缩上下文上执行解码任务。

Result: 20倍压缩比下达到98%解码准确率（DeepSeek-OCR约60%），40倍压缩比下保持约93%准确率。

Conclusion: C3压缩在上下文压缩领域表现出优越性能和可行性，为光学字符压缩、OCR等相关领域的压缩比上限提供了参考。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [15] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: TeamNRC在BHASHA-Task 1语法错误修正共享任务中，使用零/少样本提示不同规模语言模型的方法，在泰卢固语和印地语中分别获得第4和第2名。研究扩展到泰米尔语、马拉雅拉姆语和孟加拉语，重点关注数据质量和评估指标问题。


<details>
  <summary>Details</summary>
Motivation: 探索小规模语言模型在印度语言语法错误修正任务中的潜力，同时关注高质量数据集构建和适合印度语言文字的评估指标问题。

Method: 使用零/少样本提示方法，测试了从4B参数到大型专有模型的不同规模语言模型。

Result: 在泰卢固语获得GLEU分数83.78（第4名），在印地语获得84.31（第2名），并扩展到其他三种印度语言进行实验。

Conclusion: 小规模语言模型在印度语言语法错误修正任务中具有潜力，但需要关注数据集质量和评估指标的适当性。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [16] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 本文使用SetFit框架对阿拉伯方言酒店评论进行情感分析，在AHaSIS共享任务中取得73%的F1分数，排名第12位。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言情感分析面临语言多样性和标注数据稀缺的挑战，特别是在酒店评论等专业领域。

Method: 采用SetFit（句子Transformer微调）框架，这是一种数据高效的少样本学习技术。

Result: 在官方评估集上获得73%的F1分数，在26个参与者中排名第12位。

Conclusion: 这项工作突显了少样本学习在处理专业领域（如酒店评论）中微妙阿拉伯方言文本时应对数据稀缺问题的潜力。

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [17] [Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304)
*Piercosma Bisconti,Matteo Prandi,Federico Pierucci,Francesco Giarrusso,Marcantonio Bracale,Marcello Galisai,Vincenzo Suriani,Olga Sorokoletova,Federico Sartore,Daniele Nardi*

Main category: cs.CL

TL;DR: 研究发现诗歌形式的对抗性提示可作为通用单轮越狱技术，在25个前沿LLM中攻击成功率高达90%，诗歌转换比散文基线攻击成功率提升18倍，揭示了当前安全机制的局限性。


<details>
  <summary>Details</summary>
Motivation: 探索风格变异是否能绕过LLM的安全机制，测试当前对齐方法的根本局限性。

Method: 将1200个有害提示转换为诗歌形式，使用标准化元提示生成对抗性诗歌，通过开放权重评判模型和人工验证评估攻击成功率。

Result: 手工制作诗歌平均越狱成功率62%，元提示转换约43%，显著优于非诗歌基线，显示该漏洞跨越不同模型家族和安全训练方法。

Conclusion: 仅通过风格变异就能规避当代安全机制，表明当前对齐方法和评估协议存在根本性局限。

Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.

</details>


### [18] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2是西班牙语/英语医疗多选推理数据集的扩展版本，包含超过12,000个问题，用于评估LLM在医疗推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 响应高质量数据集的需求，以捕捉医疗推理的语言和概念复杂性。

Method: 扩展数据集至12,000+问题，基于西班牙专业考试；使用提示、RAG和基于概率的答案选择对开源LLM进行基准测试；提供多语言版本支持未来研究。

Result: 性能主要由模型规模和内在推理能力驱动，复杂推理策略带来的增益有限。

Conclusion: HEAD-QA v2是推进生物医学推理和模型改进研究的可靠资源。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [19] [The Empowerment of Science of Science by Large Language Models: New Tools and Methods](https://arxiv.org/abs/2511.15370)
*Guoqiang Liang,Jingqian Gong,Mengxuan Li,Gege Lin,Shuo Zhang*

Main category: cs.CL

TL;DR: 本文全面回顾了支持大语言模型的核心技术，并探讨了LLMs在科学计量学领域的潜在应用，包括基于AI代理的科学评估模型、新研究前沿检测和知识图谱构建方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在自然语言理解、图像识别和多模态任务中展现出卓越能力，它们正朝着AGI方向发展，成为全球技术竞争的核心问题。本文旨在从用户角度系统梳理支撑LLMs的核心技术，并探索其在科学计量学领域的应用前景。

Method: 采用文献综述方法，系统梳理了提示工程、知识增强的检索增强生成、微调、预训练和工具学习等核心技术，同时追溯了科学计量学的发展历史，并提出了LLMs在该领域的应用框架。

Result: 提出了基于AI代理的科学评估模型，开发了使用LLMs进行新研究前沿检测和知识图谱构建的新方法，为科学计量学领域提供了创新的技术路径。

Conclusion: 大语言模型在科学计量学领域具有广阔的应用前景，能够推动科学评估、研究前沿发现和知识组织等关键任务的智能化发展，为科学研究提供新的技术支撑和方法论创新。

Abstract: Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.

</details>


### [20] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: 开发了一个合规的检索系统，通过LLM重排序和语义搜索技术，在航空维修环境中显著减少技术员查找手册的时间，从6-15分钟降至18秒，同时保持监管合规性。


<details>
  <summary>Details</summary>
Motivation: 航空维修技术人员花费高达30%的工作时间查找手册，这是MRO操作中的效率瓶颈，且所有程序必须可追溯到认证来源。

Method: 构建版本鲁棒的嵌入向量，使用视觉语言解析结构化认证内容，结合LLM重排序和语义搜索技术，与现有认证查看器协同工作而非替代。

Result: 在49k合成查询上达到>90%检索准确率，双语对照研究显示90.9%的top-10成功率，查找时间减少95%（从6-15分钟降至18秒）。

Conclusion: 语义检索可以在严格监管约束下运行，并显著减少现实世界多语言MRO工作流程中的操作负担。

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [21] [DEPO: Dual-Efficiency Preference Optimization for LLM Agents](https://arxiv.org/abs/2511.15392)
*Sirui Chen,Mengshi Zhao,Lei Xu,Yuying Zhao,Beier Zhu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: DEPO是一种双效率偏好优化方法，通过联合奖励简洁响应和更少行动步骤，显著减少大语言模型代理的token使用和步骤数，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理能力提升往往导致思维链变长，影响实际场景中的交互效率，但目前缺乏对LLM代理效率的系统定义。

Method: 提出双效率定义（步骤级效率和轨迹级效率），并开发DEPO方法，通过偏好优化联合奖励简洁响应和更少行动步骤。

Result: 在WebShop和BabyAI上，DEPO减少token使用达60.9%，步骤数减少达26.9%，性能提升达29.3%。在数学基准测试中也能泛化，仅用25%数据训练仍保持效率增益。

Conclusion: DEPO有效解决了LLM代理效率问题，在减少资源消耗的同时提升性能，具有很好的泛化能力和数据效率。

Abstract: Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.

</details>


### [22] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: 提出了NAMEGEn框架，通过多代理优化解决中文起名任务中的多目标灵活性和解释复杂性挑战，在无训练的情况下超越多个基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决创意自然语言生成中的两大挑战：(1)多目标灵活性 - 用户需求个性化、细粒度且多元；(2)解释复杂性 - 需要理解隐含意义以增强用户感知。这些挑战限制了短文本生成中的创意和洞察力。

Method: NAMEGEn多代理优化框架，迭代交替进行目标提取、名称生成和评估，以满足多样化需求并生成准确解释。构建了17k+古典诗歌语料库增强美学，并引入CBNames基准。

Result: 实验表明NAMEGEn能有效生成满足多样化个性化需求的创意名称，并提供有意义的解释，在无训练的情况下超越了六个基于不同LLM骨干的基线方法。

Conclusion: NAMEGEn框架成功解决了创意自然语言生成中的多目标灵活性和解释复杂性挑战，特别是在中文起名这一代表性短文本生成任务中表现出色。

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


### [23] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: SPRING Lab开发了用于ASRU MADASR 2.0挑战的多语言ASR系统，采用多解码器架构和音素通用标签集作为中间表示，在Track 2中在3种语言上超越了基线WER/CER，并获得了最高的语言和方言识别准确率。


<details>
  <summary>Details</summary>
Motivation: 改进ASR系统以在8种语言的33种方言中准确预测语言和方言，参与Track 1和Track 2的限制数据使用和从头构建多语言系统的挑战。

Method: 使用多解码器架构，以音素通用标签集作为中间表示进行训练，并讨论了将音素空间增益转换回对应字形表示的各种方法。

Result: 在CLS空间中超越了基线性能，在Track 2中3种语言的WER/CER优于基线，在所有参赛团队中获得了最高的语言ID和方言ID准确率。

Conclusion: 提出的多解码器架构和音素CLS方法有效提升了多语言ASR系统的性能，特别是在语言和方言识别方面表现出色。

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [24] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster是一个完全基于LLM的文本聚类框架，通过动态内存和双提示策略解决传统LLM聚类缺乏状态记忆和难以控制聚类粒度的问题，实现了无需调优的端到端聚类。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在文本聚类中存在两个主要限制：缺乏迭代优化的状态记忆能力，以及难以管理聚类粒度。这导致现有方法需要依赖复杂的外部模块流水线，无法实现真正的端到端方法。

Method: 提出LLM-MemCluster框架，将聚类重新概念化为完全LLM原生的任务。该框架利用动态内存赋予状态感知能力，采用双提示策略使模型能够推理并确定聚类数量。

Result: 在多个基准数据集上的评估表明，这个无需调优的框架显著且持续地优于强基线方法。

Conclusion: LLM-MemCluster为基于LLM的文本聚类提供了一个有效、可解释且真正端到端的范式。

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [25] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 本文提出了LPDS数据结构和pelican nlp工具包，旨在标准化语言数据处理流程，提高研究可重复性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型等AI技术发展带来了语言数据处理方法的演进，但缺乏标准化数据组织和可重复处理方法成为重要挑战。

Method: 1) 提出LPDS数据结构，受神经科学BIDS标准启发；2) 开发pelican nlp Python包，支持模块化语言处理流程；3) 使用单一配置文件指定完整处理流程。

Result: 构建了端到端的语言数据处理管道，支持从数据清洗到复杂语言和声学特征提取的标准化处理。

Conclusion: LPDS和pelican nlp共同提供了确保方法透明度和增强可重复性的语言数据处理解决方案。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [26] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: Mera Multi是一个针对俄语的多模态评估框架，包含文本、图像、音频和视频四种模态的18个评估任务，为俄语多模态大语言模型提供首个基准测试。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对俄语的多模态基准测试，多模态大语言模型的智能、局限性和风险尚未得到充分理解，特别是在考虑俄语文化和语言特性的情况下。

Method: 构建了包含18个全新评估任务的多模态基准，采用指令式评估方法，涵盖通用模型和模态特定架构，并设计了防止基准泄露的方法论。

Result: 为闭源和开源模型提供了基线结果，建立了统一的多模态能力分类体系和评估标准。

Conclusion: 该基准不仅填补了俄语多模态评估的空白，其方法论也可复用于构建其他斯拉夫语系语言的类似基准。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [27] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: HSKBenchmark是首个用于中文二语习得的分阶段建模和写作评估基准，涵盖HSK 3-6级，包含真实教材、合成指令样本和基于语言学的评估系统，通过课程调优框架模拟人类学习轨迹。


<details>
  <summary>Details</summary>
Motivation: 语言习得研究对揭示人类语言智能本质至关重要，但控制人类学习者语言输入的实验在伦理和实践上不可行，需要可验证和可扩展的建模方法，特别是在中文二语习得领域。

Method: 构建HSKBenchmark基准，包含真实教材、合成指令样本和测试主题；采用课程调优框架从初级到高级训练模型；创建基于语言学的评估系统，考察语法覆盖、写作错误、词汇句法复杂度和整体评分。

Result: 实验结果表明HSKBenchmark能有效建模中文二语习得，并作为可靠的动态写作评估基准。微调后的大语言模型写作表现与高级人类学习者相当，并展现出类似人类的习得特征。

Conclusion: HSKBenchmark、HSKAgent和检查点作为基础工具和资源，有望为未来语言习得建模和大语言模型可解释性研究铺平道路。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


### [28] [Tokenisation over Bounded Alphabets is Hard](https://arxiv.org/abs/2511.15709)
*Violeta Kastreva,Philip Whittington,Dennis Komm,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文证明了即使在有界字母表（包括二进制和一元字母表）上，tokenization 仍然是 NP 完全问题，且不存在多项式时间近似方案，解释了为什么实际算法如 BPE 和 UnigramLM 都是启发式的。


<details>
  <summary>Details</summary>
Motivation: 先前研究假设 tokenization 应用于无限大字母表，但实际中 tokenizer 操作于固定大小的字母表（如字节或 Unicode 字符），需要填补这一理论空白。

Method: 分析有界 n 元字母表上的 tokenization，考虑两种变体：自底向上 tokenization 和直接 tokenization，分别选择合并操作序列或词汇表来最优压缩数据集。

Result: 证明即使在二进制字母表上，两种变体都是 NP 完全的，且不存在多项式时间近似方案；直接 tokenization 在一元字母表上也是 NP 完全的。

Conclusion: tokenization 的计算难处理性不是大字母表或复杂构造的产物，而是根本性障碍，表明近似算法是未来 tokenization 研究的重要方向。

Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [29] [Z-Merge: Multi-Agent Reinforcement Learning for On-Ramp Merging with Zone-Specific V2X Traffic Information](https://arxiv.org/abs/2511.14910)
*Yassine Ibork,Myounggyu Won,Lokesh Das*

Main category: cs.RO

TL;DR: 提出了一种基于V2X通信的多智能体强化学习框架，用于解决自动驾驶车辆在混合交通环境中的匝道汇入问题，通过结合局部和全局信息协调车道变换和车间距调整策略。


<details>
  <summary>Details</summary>
Motivation: 现有的匝道汇入方法通常只依赖局部或邻近信息，仅采用车道变换或车间距创建策略，导致在安全性和交通效率方面表现不佳。

Method: 将匝道汇入控制问题建模为多智能体部分可观测马尔可夫决策过程，利用V2X通信获取路侧单元提供的全局信息，设计混合动作空间并采用参数化深度Q学习方法。

Result: 通过集成SUMO交通模拟器和MOSAIC V2X模拟器进行广泛仿真，证明该框架在不同交通场景下显著提高了汇入成功率、交通效率和道路安全性。

Conclusion: V2X辅助的多智能体强化学习框架能够有效协调匝道汇入过程中的复杂交互，为自动驾驶车辆在混合交通环境中的安全高效汇入提供了可行解决方案。

Abstract: Ramp merging is a critical and challenging task for autonomous vehicles (AVs), particularly in mixed traffic environments with human-driven vehicles (HVs). Existing approaches typically rely on either lane-changing or inter-vehicle gap creation strategies based solely on local or neighboring information, often leading to suboptimal performance in terms of safety and traffic efficiency. In this paper, we present a V2X (vehicle-to-everything communication)-assisted Multiagent Reinforcement Learning (MARL) framework for on-ramp merging that effectively coordinates the complex interplay between lane-changing and inter-vehicle gap adaptation strategies by utilizing zone-specific global information available from a roadside unit (RSU). The merging control problem is formulated as a Multiagent Partially Observable Markov Decision Process (MA-POMDP), where agents leverage both local and global observations through V2X communication. To support both discrete and continuous control decisions, we design a hybrid action space and adopt a parameterized deep Q-learning approach. Extensive simulations, integrating the SUMO traffic simulator and the MOSAIC V2X simulator, demonstrate that our framework significantly improves merging success rate, traffic efficiency, and road safety across diverse traffic scenarios.

</details>


### [30] [A visual study of ICP variants for Lidar Odometry](https://arxiv.org/abs/2511.14919)
*Sebastian Dingler,Hannes Burrichter*

Main category: cs.RO

TL;DR: 本文研究激光雷达里程计中的ICP算法，分析其受动态物体、非重叠区域和传感器噪声影响的问题，并提出动态物体过滤和解决自车盲区问题的新方法。


<details>
  <summary>Details</summary>
Motivation: 激光雷达里程计中使用的ICP算法在实际应用中受到动态物体、非重叠区域和传感器噪声的影响，导致精度下降，需要研究这些影响并提出改进方法。

Method: 基于最近提出的可视化ICP多维目标函数的方法，研究不同ICP变体在激光雷达里程计中的应用，并提出动态物体过滤和解决自车盲区问题的新方法。

Result: 通过可视化方法可以清晰观察到ICP算法在实际应用中的问题，提出的新方法能够有效过滤动态物体并解决自车盲区问题。

Conclusion: 可视化方法有助于深入理解ICP算法在激光雷达里程计中的局限性，提出的新方法能够提升里程计的精度和鲁棒性。

Abstract: Odometry with lidar sensors is a state-of-the-art method to estimate the ego pose of a moving vehicle. Many implementations of lidar odometry use variants of the Iterative Closest Point (ICP) algorithm. Real-world effects such as dynamic objects, non-overlapping areas, and sensor noise diminish the accuracy of ICP. We build on a recently proposed method that makes these effects visible by visualizing the multidimensional objective function of ICP in two dimensions. We use this method to study different ICP variants in the context of lidar odometry. In addition, we propose a novel method to filter out dynamic objects and to address the ego blind spot problem.

</details>


### [31] [SVBRD-LLM: Self-Verifying Behavioral Rule Discovery for Autonomous Vehicle Identification](https://arxiv.org/abs/2511.14977)
*Xiangyu Li,Zhaomiao Guo*

Main category: cs.RO

TL;DR: SVBRD-LLM框架通过零样本提示工程从真实交通视频中自动发现、验证和应用可解释的行为规则，用于分析自动驾驶车辆与人类驾驶车辆的差异。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在公共道路上运行增多，理解其真实世界行为对交通安全分析、政策制定和公众接受度至关重要。

Method: 使用YOLOv8和ByteTrack提取车辆轨迹，计算运动学特征，通过GPT-5零样本提示比较自动驾驶与人类驾驶车辆，生成35个结构化行为规则假设，并在验证集上测试和迭代优化。

Result: 在超过1500小时真实交通视频上的实验显示，该框架在自动驾驶车辆识别任务中达到90.0%准确率和93.3% F1分数，发现的规则揭示了自动驾驶车辆在速度控制平滑性、变道保守性和加速度稳定性方面的明显特征。

Conclusion: 该框架能够有效发现和验证自动驾驶车辆的可解释行为规则，为交通安全分析和政策制定提供可靠依据。

Abstract: As more autonomous vehicles operate on public roads, understanding real-world behavior of autonomous vehicles is critical to analyzing traffic safety, making policies, and public acceptance. This paper proposes SVBRD-LLM, a framework that automatically discovers, verifies, and applies interpretable behavioral rules from real traffic videos through zero-shot prompt engineering. The framework extracts vehicle trajectories using YOLOv8 and ByteTrack, computes kinematic features, and employs GPT-5 zero-shot prompting to compare autonomous and human-driven vehicles, generating 35 structured behavioral rule hypotheses. These rules are tested on a validation set, iteratively refined based on failure cases to filter spurious correlations, and compiled into a high-confidence rule library. The framework is evaluated on an independent test set for speed change prediction, lane change prediction, and autonomous vehicle identification tasks. Experiments on over 1500 hours of real traffic videos show that the framework achieves 90.0% accuracy and 93.3% F1-score in autonomous vehicle identification. The discovered rules clearly reveal distinctive characteristics of autonomous vehicles in speed control smoothness, lane change conservativeness, and acceleration stability, with each rule accompanied by semantic description, applicable context, and validation confidence.

</details>


### [32] [An Alignment-Based Approach to Learning Motions from Demonstrations](https://arxiv.org/abs/2511.14988)
*Alex Cuellar,Christopher K Fourie,Julie A Shah*

Main category: cs.RO

TL;DR: CALM是一个基于演示学习的框架，通过将运动轨迹与代表性"平均"轨迹对齐，而不是纯粹依赖时间或状态，来避免时间依赖和时间无关方法的缺点。


<details>
  <summary>Details</summary>
Motivation: 现有的LfD方法分为时间依赖和时间无关两类，各有优缺点：时间无关方法无法学习重叠轨迹，而时间依赖方法在扰动下会产生不良行为。需要一种新方法来解决这些问题。

Method: 提出CALM框架，使用轨迹对齐技术，基于演示聚类的代表性平均轨迹进行对齐，能够处理扰动下的对齐偏移，并生成多模态行为。

Result: 在2D数据集上验证了CALM能够减轻时间依赖和时间无关技术的缺点，并在7自由度机器人上成功实现了三个领域的任务学习。

Conclusion: CALM通过轨迹对齐方法有效结合了时间依赖和时间无关方法的优点，同时避免了它们各自的缺点，为机器人运动学习提供了更鲁棒的解决方案。

Abstract: Learning from Demonstration (LfD) has shown to provide robots with fundamental motion skills for a variety of domains. Various branches of LfD research (e.g., learned dynamical systems and movement primitives) can generally be classified into ''time-dependent'' or ''time-independent'' systems. Each provides fundamental benefits and drawbacks -- time-independent methods cannot learn overlapping trajectories, while time-dependence can result in undesirable behavior under perturbation. This paper introduces Cluster Alignment for Learned Motions (CALM), an LfD framework dependent upon an alignment with a representative ''mean" trajectory of demonstrated motions rather than pure time- or state-dependence. We discuss the convergence properties of CALM, introduce an alignment technique able to handle the shifts in alignment possible under perturbation, and utilize demonstration clustering to generate multi-modal behavior. We show how CALM mitigates the drawbacks of time-dependent and time-independent techniques on 2D datasets and implement our system on a 7-DoF robot learning tasks in three domains.

</details>


### [33] [Communication-Aware Asynchronous Distributed Trajectory Optimization for UAV Swarm](https://arxiv.org/abs/2511.14994)
*Yue Yu,Xiaobo Zheng,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一个通信感知的异步分布式轨迹优化框架，用于无人机群在通信受限环境下的轨迹规划，通过两层级架构减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决分布式优化在通信受限环境中的部署挑战，包括不可靠链路和有限数据交换问题，使无人机群能在实际通信不可靠场景中运行。

Method: 采用两层级架构：使用参数化差分动态规划进行单个无人机的局部轨迹优化，结合异步交替方向乘子法进行群级协调。

Result: 实现了完全分布式优化并显著降低通信开销，特别擅长处理非线性动力学和时空耦合问题。

Conclusion: 该框架适用于通信不可靠的实际场景，为无人机群在通信受限环境中的轨迹规划提供了有效解决方案。

Abstract: Distributed optimization offers a promising paradigm for trajectory planning in Unmanned Aerial Vehicle (UAV) swarms, yet its deployment in communication-constrained environments remains challenging due to unreliable links and limited data exchange. This paper addresses this issue via a two-tier architecture explicitly designed for operation under communication constraints. We develop a Communication-Aware Asynchronous Distributed Trajectory Optimization (CA-ADTO) framework that integrates Parameterized Differential Dynamic Programming (PDDP) for local trajectory optimization of individual UAVs with an asynchronous Alternating Direction Method of Multipliers (async-ADMM) for swarm-level coordination. The proposed architecture enables fully distributed optimization while substantially reducing communication overhead, making it suitable for real-world scenarios in which reliable connectivity cannot be guaranteed. The method is particularly effective in handling nonlinear dynamics and spatio-temporal coupling under communication constraints.

</details>


### [34] [Lie Group Control Architectures for UAVs: a Comparison of SE2(3)-Based Approaches in Simulation and Hardware](https://arxiv.org/abs/2511.15023)
*Dimitria Silveria,Kleber Cabral,Peter Jardine,Sidney Givigi*

Main category: cs.RO

TL;DR: 本文提出了一种基于李群的四旋翼先进控制策略，结合SE2(3)模型预测控制器(MPC)的预测能力和约束处理与李群几何特性，在Quanser QDrone平台上验证了其优越的轨迹跟踪性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于李群的四旋翼控制理论已有发展，但需要在实际平台上验证其有效性，并与现有工业标准控制器进行对比。

Method: 开发了新颖的SE2(3)模型预测控制器，结合最优控制的预测能力和约束处理与李群几何特性，并与SE2(3) LQR方法和工业标准控制架构进行比较。

Result: SE2(3) MPC在仿真中表现与SE2(3) LQR相当，但在实际部署中展现出更优越的轨迹跟踪性能和鲁棒性。

Conclusion: 这项工作证明了基于李群的控制器在实际应用中的有效性，并提供了关于其对系统行为和实时性能影响的比较见解。

Abstract: This paper presents the integration and experimental validation of advanced control strategies for quadcopters based on Lie groups. We build upon recent theoretical developments on SE2(3)-based controllers and introduce a novel SE2(3) model predictive controller (MPC) that combines the predictive capabilities and constraint-handling of optimal control with the geometric properties of Lie group formulations. We evaluated this MPC against a state-of-the-art SE2(3)-based LQR approach and obtained comparable performance in simulation. Both controllers where also deployed on the Quanser QDrone platform and compared to each other and an industry standard control architecture. Results show that the SE_2(3) MPC achieves superior trajectory tracking performance and robustness across a range of scenarios. This work demonstrates the practical effectiveness of Lie group-based controllers and offers comparative insights into their impact on system behaviour and real-time performance

</details>


### [35] [Painted Heart Beats](https://arxiv.org/abs/2511.15105)
*Angshu Adhya,Cindy Yang,Emily Wu,Rishad Hasan,Abhishek Narula,Patrícia Alves-Oliveira*

Main category: cs.RO

TL;DR: AURA框架实现人机协同绘画，通过心率传感器感知艺术家情绪状态，机器人据此调整与画布的距离，实现流畅的艺术交互。


<details>
  <summary>Details</summary>
Motivation: 结合人类艺术家的生物特征来指导流畅的艺术互动，探索人机协同创作的新模式。

Method: 开发机器人手臂与人类艺术家协作绘画，使用EmotiBit传感器检测艺术家心率，根据心率变化（兴奋或平静）决定机器人靠近或远离画布区域。

Result: 实现了基于艺术家生理状态的机器人行为调整，当检测到较高心率时机器人远离画布，心率正常时继续全画布绘画。

Conclusion: 该工作成功将人类生物特征融入人机艺术交互，为协同创作提供了新的技术途径。

Abstract: In this work we present AURA, a framework for synergistic human-artist painting. We developed a robot arm that collaboratively paints with a human artist. The robot has an awareness of the artist's heartbeat through the EmotiBit sensor, which provides the arousal levels of the painter. Given the heartbeat detected, the robot decides to increase proximity to the artist's workspace or retract. If a higher heartbeat is detected, which is associated with increased arousal in human artists, the robot will move away from that area of the canvas. If the artist's heart rate is detected as neutral, indicating the human artist's baseline state, the robot will continue its painting actions across the entire canvas. We also demonstrate and propose alternative robot-artist interactions using natural language and physical touch. This work combines the biometrics of a human artist to inform fluent artistic interactions.

</details>


### [36] [Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization](https://arxiv.org/abs/2511.15194)
*Jian Deng,Yuandong Wang,Yangfu Zhu,Tao Feng,Tianyu Wo,Zhenzhou Shao*

Main category: cs.RO

TL;DR: Eq.Bot是一个基于SE(2)群等变理论的通用规范化框架，用于机器人操作学习，通过将观察转换到规范空间并应用现有策略，赋予模型空间等变性而无需架构修改。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习框架缺乏几何一致性保证，难以处理旋转和平移等空间变换，而现有等变方法实现复杂、计算成本高且可移植性差。

Method: 提出Eq.Bot框架，将观察转换到规范空间，应用现有策略，然后将结果动作映射回原始空间，作为模型无关的解决方案。

Result: 在基于CNN和Transformer的架构上，Eq.Bot在各种机器人操作任务上优于现有方法，最大改进可达50.0%。

Conclusion: Eq.Bot是一种有效的通用框架，能够为机器人操作学习模型提供空间等变性，无需修改模型架构。

Abstract: Robotic manipulation systems are increasingly deployed across diverse domains. Yet existing multi-modal learning frameworks lack inherent guarantees of geometric consistency, struggling to handle spatial transformations such as rotations and translations. While recent works attempt to introduce equivariance through bespoke architectural modifications, these methods suffer from high implementation complexity, computational cost, and poor portability. Inspired by human cognitive processes in spatial reasoning, we propose Eq.Bot, a universal canonicalization framework grounded in SE(2) group equivariant theory for robotic manipulation learning. Our framework transforms observations into a canonical space, applies an existing policy, and maps the resulting actions back to the original space. As a model-agnostic solution, Eq.Bot aims to endow models with spatial equivariance without requiring architectural modifications. Extensive experiments demonstrate the superiority of Eq.Bot under both CNN-based (e.g., CLIPort) and Transformer-based (e.g., OpenVLA-OFT) architectures over existing methods on various robotic manipulation tasks, where the most significant improvement can reach 50.0%.

</details>


### [37] [VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation](https://arxiv.org/abs/2511.15200)
*Tairan He,Zi Wang,Haoru Xue,Qingwei Ben,Zhengyi Luo,Wenli Xiao,Ye Yuan,Xingye Da,Fernando Castañeda,Shankar Sastry,Changliu Liu,Guanya Shi,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: VIRAL是一个视觉模拟到现实的框架，通过大规模模拟训练人形机器人的移动操作技能，无需真实世界微调即可部署到硬件上。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在真实世界中缺乏自主移动操作能力的关键障碍。

Method: 采用师生设计：特权RL教师基于完整状态学习长时程移动操作，然后通过大规模模拟和tiled渲染从教师策略中蒸馏出基于视觉的学生策略，结合在线DAgger和行为克隆训练。

Result: 在Unitree G1人形机器人上，基于RGB的策略能够执行长达54个周期的连续移动操作，适应多样的空间和外观变化，性能接近专家级遥操作水平。

Conclusion: 计算规模对成功至关重要，大规模视觉领域随机化和真实到模拟的对齐是实现零样本部署的关键。

Abstract: A key barrier to the real-world deployment of humanoid robots is the lack of autonomous loco-manipulation skills. We introduce VIRAL, a visual sim-to-real framework that learns humanoid loco-manipulation entirely in simulation and deploys it zero-shot to real hardware. VIRAL follows a teacher-student design: a privileged RL teacher, operating on full state, learns long-horizon loco-manipulation using a delta action space and reference state initialization. A vision-based student policy is then distilled from the teacher via large-scale simulation with tiled rendering, trained with a mixture of online DAgger and behavior cloning. We find that compute scale is critical: scaling simulation to tens of GPUs (up to 64) makes both teacher and student training reliable, while low-compute regimes often fail. To bridge the sim-to-real gap, VIRAL combines large-scale visual domain randomization over lighting, materials, camera parameters, image quality, and sensor delays--with real-to-sim alignment of the dexterous hands and cameras. Deployed on a Unitree G1 humanoid, the resulting RGB-based policy performs continuous loco-manipulation for up to 54 cycles, generalizing to diverse spatial and appearance variations without any real-world fine-tuning, and approaching expert-level teleoperation performance. Extensive ablations dissect the key design choices required to make RGB-based humanoid loco-manipulation work in practice.

</details>


### [38] [A Class of Dual-Frame Passively-Tilting Fully-Actuated Hexacopter](https://arxiv.org/abs/2511.15225)
*Jiajun Liu,Yimin Zhu,Xiaorui Liu,Mingye Cao,Mingchao Li,Lixian Zhang*

Main category: cs.RO

TL;DR: 提出了一种新型全驱动六旋翼无人机，采用双框架被动倾斜结构，用最少的执行器实现平移运动和姿态的独立控制，相比现有全驱动无人机消除了内力抵消，在同等载荷下具有更高飞行效率和续航能力。


<details>
  <summary>Details</summary>
Motivation: 现有全驱动无人机存在内力抵消问题，导致飞行效率低下和续航能力不足。本文旨在通过创新的结构设计解决这些问题，实现更高效的全驱动飞行控制。

Method: 采用双框架被动倾斜结构设计，基于全驱动六旋翼动力学模型设计全驱动控制器，通过仿真验证系统性能。

Result: 仿真结果表明，该全驱动六旋翼具有优越的全驱动运动能力，所提出的控制策略有效实现了高效稳定的控制。

Conclusion: 提出的新型全驱动六旋翼无人机结构设计成功解决了内力抵消问题，在保持全驱动能力的同时显著提升了飞行效率和续航性能。

Abstract: This paper proposed a novel fully-actuated hexacopter. It features a dual-frame passive tilting structure and achieves independent control of translational motion and attitude with minimal actuators. Compared to previous fully-actuated UAVs, it liminates internal force cancellation, resulting in higher flight efficiency and endurance under equivalent payload conditions. Based on the dynamic model of fully-actuated hexacopter, a full-actuation controller is designed to achieve efficient and stable control. Finally, simulation is conducted, validating the superior fully-actuated motion capability of fully-actuated hexacopter and the effectiveness of the proposed control strategy.

</details>


### [39] [Modelling and Model-Checking a ROS2 Multi-Robot System using Timed Rebeca](https://arxiv.org/abs/2511.15227)
*Hiep Hong Trinh,Marjan Sirjani,Federico Ciccozzi,Abu Naser Masud,Mikael Sjödin*

Main category: cs.RO

TL;DR: 本文展示了如何使用Timed Rebeca建模语言和模型检查技术来设计和验证多机器人系统，包括离散化连续系统、状态空间优化以及模型与实现之间的双向工程流程。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统具有复杂的异步交互和并发特性，需要形式化验证方法来确保设计意图的正确性。模型驱动开发能够实现快速原型设计和早期验证。

Method: 使用Timed Rebeca建模语言对ROS2节点拓扑、周期性物理信号和运动基元进行建模，开发不同的离散化策略来抽象复杂信息，应用优化技术提升计算效率。

Result: 成功建立了多机器人系统的模型，实现了从离散模型到连续系统的桥梁，并通过状态空间压缩保持了模型准确性。发布了Rebeca和ROS2代码作为多自主机器人系统建模的基础。

Conclusion: 证明了使用模型设计和验证多机器人系统的可行性，展示了离散建模连续系统进行高效模型检查的方法，以及模型与实现之间的双向工程流程。

Abstract: Model-based development enables quicker prototyping, earlier experimentation and validation of design intents. For a multi-agent system with complex asynchronous interactions and concurrency, formal verification, model-checking in particular, offers an automated mechanism for verifying desired properties. Timed Rebeca is an actor-based modelling language supporting reactive, concurrent and time semantics, accompanied with a model-checking compiler. These capabilities allow using Timed Rebeca to correctly model ROS2 node topographies, recurring physical signals, motion primitives and other timed and time-convertible behaviors. The biggest challenges in modelling and verifying a multi-robot system lie in abstracting complex information, bridging the gap between a discrete model and a continuous system and compacting the state space, while maintaining the model's accuracy. We develop different discretization strategies for different kinds of information, identifying the 'enough' thresholds of abstraction, and applying efficient optimization techniques to boost computations. With this work we demonstrate how to use models to design and verify a multi-robot system, how to discretely model a continuous system to do model-checking efficiently, and the round-trip engineering flow between the model and the implementation. The released Rebeca and ROS2 codes can serve as a foundation for modelling multiple autonomous robots systems.

</details>


### [40] [Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy](https://arxiv.org/abs/2511.15239)
*Tomoki Nakao,Kazumi Kasaura,Tadashi Kozuno*

Main category: cs.RO

TL;DR: 提出一种基于缠绕数拓扑不变量的分层导航方法，通过强化学习学习对称性破缺策略，解决多智能体导航中的死锁问题。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体导航中由于对称性导致的死锁问题，当多个智能体交互时，自主打破通过彼此的决策对称性具有固有困难。

Method: 分层策略：学习型规划器通过强化学习生成缠绕数表示的拓扑合作策略和动态权重，模型型控制器基于这些参数执行无碰撞高效运动。

Result: 在仿真和真实机器人实验中，该方法在密集环境中优于现有基线，有效避免碰撞和死锁，实现更优的导航性能。

Conclusion: 分层结构结合了学习型方法的灵活决策能力和模型型方法的可靠性，成功解决了对称性导致的死锁问题。

Abstract: We address the fundamental challenge of resolving symmetry-induced deadlocks in distributed multi-agent navigation by proposing a new hierarchical navigation method. When multiple agents interact, it is inherently difficult for them to autonomously break the symmetry of deciding how to pass each other. To tackle this problem, we introduce an approach that quantifies cooperative symmetry-breaking strategies using a topological invariant called the winding number, and learns the strategies themselves through reinforcement learning. Our method features a hierarchical policy consisting of a learning-based Planner, which plans topological cooperative strategies, and a model-based Controller, which executes them. Through reinforcement learning, the Planner learns to produce two types of parameters for the Controller: one is the topological cooperative strategy represented by winding numbers, and the other is a set of dynamic weights that determine which agent interaction to prioritize in dense scenarios where multiple agents cross simultaneously. The Controller then generates collision-free and efficient motions based on the strategy and weights provided by the Planner. This hierarchical structure combines the flexible decision-making ability of learning-based methods with the reliability of model-based approaches. Simulation and real-world robot experiments demonstrate that our method outperforms existing baselines, particularly in dense environments, by efficiently avoiding collisions and deadlocks while achieving superior navigation performance. The code for the experiments is available at https://github.com/omron-sinicx/WNumMPC.

</details>


### [41] [Behavior Trees vs Executable Ontologies: a Comparative Analysis of Robot Control Paradigms](https://arxiv.org/abs/2511.15274)
*Alexander Boldachev*

Main category: cs.RO

TL;DR: 比较两种机器人行为建模方法：命令式行为树(BT)和声明式可执行本体(EO)，发现EO通过事件驱动状态传播实现与BT相当的响应性和模块性，但采用完全不同的语义域建模架构。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器人控制中的语义-过程差距，探索从过程式编程转向语义域建模的替代框架。

Method: 通过boldsea框架实现两种方法：BT使用分层控制流，EO使用基于事件的语义图和数据流规则。在移动操作任务中进行实际比较。

Result: EO实现了与BT相当的响应性和模块性，同时支持运行时模型修改、完整时间可追溯性以及数据、逻辑和接口的统一表示，这些在BT中难以实现。BT在稳定可预测场景中表现优异。

Conclusion: EO提供了一种替代框架，通过语义域建模解决机器人控制中的语义-过程差距，两种方法在动态演化机器人系统中各有优势。

Abstract: This paper compares two distinct approaches to modeling robotic behavior: imperative Behavior Trees (BTs) and declarative Executable Ontologies (EO), implemented through the boldsea framework. BTs structure behavior hierarchically using control-flow, whereas EO represents the domain as a temporal, event-based semantic graph driven by dataflow rules. We demonstrate that EO achieves comparable reactivity and modularity to BTs through a fundamentally different architecture: replacing polling-based tick execution with event-driven state propagation. We propose that EO offers an alternative framework, moving from procedural programming to semantic domain modeling, to address the semantic-process gap in traditional robotic control. EO supports runtime model modification, full temporal traceability, and a unified representation of data, logic, and interface - features that are difficult or sometimes impossible to achieve with BTs, although BTs excel in established, predictable scenarios. The comparison is grounded in a practical mobile manipulation task. This comparison highlights the respective operational strengths of each approach in dynamic, evolving robotic systems.

</details>


### [42] [Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception](https://arxiv.org/abs/2511.15279)
*Jiashu Yang,Yifan Han,Yucheng Xie,Ning Guo,Wenzhao Lian*

Main category: cs.RO

TL;DR: EyeVLA是一个主动视觉感知机器人眼球系统，通过指令驱动的旋转和缩放动作，在像素和空间预算约束下主动获取更精确的视觉信息，实现宽区域覆盖与细粒度细节采集的统一。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型和固定RGB-D相机系统无法同时实现宽区域覆盖和细粒度细节采集，限制了在开放世界机器人应用中的效能。需要开发能够主动获取信息、基于指令采取行动的视觉感知系统。

Method: 将动作行为离散化为动作token，与具有强开放世界理解能力的视觉语言模型集成，在单一自回归序列中联合建模视觉、语言和动作。使用2D边界框坐标指导推理链，应用强化学习优化视点选择策略，仅需少量真实世界数据即可将VLM的开放世界场景理解能力迁移到VLA策略。

Result: 实验表明，系统在真实世界环境中高效执行指令场景，通过旋转和缩放动作主动获取更准确的视觉信息，实现了强大的环境感知能力。

Conclusion: EyeVLA引入了一种新颖的机器人视觉系统，利用详细且空间丰富的大规模具身数据，为下游具身任务主动获取高信息量的视觉观察。

Abstract: In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.

</details>


### [43] [Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2511.15284)
*Jonas De Maeyer,Hossein Yarahmadi,Moharram Challenger*

Main category: cs.RO

TL;DR: 提出了一种基于区域感知的强化学习框架，用于动态环境中的路径规划，利用环境变化的局部性特点，通过分层分解和分布式智能体实现本地适应。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的路径规划面临环境变化不确定性和现有方法可扩展性差的挑战，需要能够持续适应局部变化的解决方案。

Method: 采用分层环境分解和分布式RL智能体，提出基于子环境成功率的重训练机制，探索单智能体Q学习和多智能体联邦Q学习两种训练范式。

Result: 联邦变体始终优于单智能体版本，接近A* Oracle性能，同时保持更短的适应时间和强大的可扩展性。

Conclusion: 该去中心化框架无需全局规划器，为未来使用深度RL和灵活环境分解的改进奠定了基础，尽管在大型环境中初始训练仍较耗时。

Abstract: Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.

</details>


### [44] [Optimizing Robot Positioning Against Placement Inaccuracies: A Study on the Fanuc CRX10iA/L](https://arxiv.org/abs/2511.15290)
*Nicolas Gautier,Yves Guillermit,Mathieu Porez,David Lemoine,Damien Chablat*

Main category: cs.RO

TL;DR: 提出一种基于粒子群优化和α-shape算法的方法，用于确定协作机器人在工业任务轨迹下的最佳基座位置，并评估位置容错性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人基座位置不确定性问题，特别是在移动基座部署时，为机器人放置提供鲁棒性标准。

Method: 使用粒子群优化算法搜索可行位置，α-shape算法绘制可行区域边界，Voronoi图计算最大内接圆，结合逆运动学模型和雅可比矩阵评估轨迹执行。

Result: 开发的方法能够有效确定机器人基座的最佳位置，并量化位置容错能力，考虑了多达16个逆运动学解和各种约束条件。

Conclusion: 该方法为机器人部署提供了系统化的基座位置优化方案，特别适用于需要位置容错性的移动基座应用场景。

Abstract: This study presents a methodology for determining the optimal base placement of a Fanuc CRX10iA/L collaborative robot for a desired trajectory corresponding to an industrial task. The proposed method uses a particle swarm optimization algorithm that explores the search space to find positions for performing the trajectory. An $α$-shape algorithm is then used to draw the borders of the feasibility areas, and the largest circle inscribed is calculated from the Voronoi diagrams. The aim of this approach is to provide a robustness criterion in the context of robot placement inaccuracies that may be encountered, for example, if the robot is placed on a mobile base when the system is deployed by an operator. The approach developed uses an inverse kinematics model to evaluate all initial configurations, then moves the robot end-effector along the reference trajectory using the Jacobian matrix and assigns a score to the attempt. For the Fanuc CRX10iA/L robot, there can be up to 16 solutions to the inverse kinematics model. The calculation of these solutions is not trivial and requires a specific study that planning tools such as MoveIt cannot fully take into account. Additionally, the optimization process must consider constraints such as joint limits, singularities, and workspace limitations to ensure feasible and efficient trajectory execution.

</details>


### [45] [MSA - Technique for Stiffness Modeling of Manipulators with Complex and Hybrid Structures](https://arxiv.org/abs/2511.15294)
*Alexandr Klimchik,Anatol Pashkevich,Damien Chablat*

Main category: cs.RO

TL;DR: 提出了一种基于矩阵结构分析的复杂混合结构机械臂刚度建模系统方法，适用于包含闭环、柔性连杆、刚性连接、被动和弹性关节等多种架构的机械臂。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理混合架构机械臂的刚度建模，特别是包含闭环、柔性连杆、弹性关节等复杂结构的情况，需要一种更通用的建模方法。

Method: 使用矩阵结构分析方法，将机械臂刚度模型表示为描述连杆弹性的常规方程和描述连杆间连接的约束方程集合，避免了传统扩展刚度矩阵中的列/行合并过程。

Result: 该方法能够以半解析方式生成笛卡尔刚度矩阵，并通过NaVaRo机械臂的刚度分析验证了其优势。

Conclusion: 所提出的方法为复杂混合结构机械臂的刚度建模提供了一种系统且直接的解决方案，特别适用于包含多种架构元素的机械臂系统。

Abstract: The paper presents a systematic approach for stiffness modeling of manipulators with complex and hybrid structures using matrix structural analysis. In contrast to previous results, it is suitable for mixed architectures containing closed-loops, flexible links, rigid connections, passive and elastic joints with external loadings and preloadings. The proposed approach produces the Cartesian stiffness matrices in a semi-analytical manner. It presents the manipulator stiffness model as a set of conventional equations describing the link elasticities that are supplemented by a set of constraints describing connections between links. Its allows user straightforward aggregation of stiffness model equations avoiding traditional column/row merging procedures in the extended stiffness matrix. Advantages of this approach are illustrated by stiffness analysis of NaVaRo manipulator.

</details>


### [46] [C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models](https://arxiv.org/abs/2511.15333)
*Nayoung Oh,Dohyun Kim,Junhyeong Bang,Rohan Paul,Daehyung Park*

Main category: cs.RO

TL;DR: C2F-Space是一个新颖的从粗到精的空间定位框架，通过视觉语言模型进行粗略区域估计，然后通过超像素化进行精细调整，显著提升了空间定位性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理复杂的空间推理（如距离、几何关系和物体间关系），而视觉语言模型虽然推理能力强，但无法输出细粒度的区域结果。

Method: 采用两阶段方法：1）使用基于网格的视觉定位提示和提出-验证策略，通过VLM估计近似但空间一致的区域；2）通过超像素化将区域与局部环境对齐进行细化。

Result: C2F-Space在成功率和交并比指标上显著优于五种最先进的基线方法，消融研究证实了各模块的有效性及其协同效应。

Conclusion: 该框架成功解决了空间定位中的复杂推理问题，并在模拟机器人拾放任务中展示了应用潜力。

Abstract: Space grounding refers to localizing a set of spatial references described in natural language instructions. Traditional methods often fail to account for complex reasoning -- such as distance, geometry, and inter-object relationships -- while vision-language models (VLMs), despite strong reasoning abilities, struggle to produce a fine-grained region of outputs. To overcome these limitations, we propose C2F-Space, a novel coarse-to-fine space-grounding framework that (i) estimates an approximated yet spatially consistent region using a VLM, then (ii) refines the region to align with the local environment through superpixelization. For the coarse estimation, we design a grid-based visual-grounding prompt with a propose-validate strategy, maximizing VLM's spatial understanding and yielding physically and semantically valid canonical region (i.e., ellipses). For the refinement, we locally adapt the region to surrounding environment without over-relaxed to free space. We construct a new space-grounding benchmark and compare C2F-Space with five state-of-the-art baselines using success rate and intersection-over-union. Our C2F-Space significantly outperforms all baselines. Our ablation study confirms the effectiveness of each module in the two-step process and their synergistic effect of the combined framework. We finally demonstrate the applicability of C2F-Space to simulated robotic pick-and-place tasks.

</details>


### [47] [Platform-Agnostic Reinforcement Learning Framework for Safe Exploration of Cluttered Environments with Graph Attention](https://arxiv.org/abs/2511.15358)
*Gabriele Calzolari,Vidya Sumathy,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出了一种平台无关的强化学习框架，结合图神经网络策略和安全过滤器，实现障碍物密集环境中的高效安全自主探索。


<details>
  <summary>Details</summary>
Motivation: 在障碍物密集环境中进行自主探索需要既保证效率又确保安全性的策略，现有方法在部署学习策略到真实机器人平台时面临安全挑战。

Method: 使用PPO算法训练图神经网络策略进行下一个航点选择，结合安全过滤器确保移动安全，并设计了基于势场的奖励函数考虑未探索区域接近度和预期信息增益。

Result: 在仿真和实验室环境中的广泛评估表明，该方法在杂乱空间中实现了高效且安全的探索。

Conclusion: 该框架将基于强化学习的探索策略的适应性与显式安全机制的可靠性相结合，为在真实环境中部署学习策略提供了关键保障。

Abstract: Autonomous exploration of obstacle-rich spaces requires strategies that ensure efficiency while guaranteeing safety against collisions with obstacles. This paper investigates a novel platform-agnostic reinforcement learning framework that integrates a graph neural network-based policy for next-waypoint selection, with a safety filter ensuring safe mobility. Specifically, the neural network is trained using reinforcement learning through the Proximal Policy Optimization (PPO) algorithm to maximize exploration efficiency while minimizing safety filter interventions. Henceforth, when the policy proposes an infeasible action, the safety filter overrides it with the closest feasible alternative, ensuring consistent system behavior. In addition, this paper introduces a reward function shaped by a potential field that accounts for both the agent's proximity to unexplored regions and the expected information gain from reaching them. The proposed framework combines the adaptability of reinforcement learning-based exploration policies with the reliability provided by explicit safety mechanisms. This feature plays a key role in enabling the deployment of learning-based policies on robotic platforms operating in real-world environments. Extensive evaluations in both simulations and experiments performed in a lab environment demonstrate that the approach achieves efficient and safe exploration in cluttered spaces.

</details>


### [48] [RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer](https://arxiv.org/abs/2511.15414)
*Mingyang Feng,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: 提出RRT*former算法，将RRT*与Transformer网络结合，利用环境信息和先前样本信息来指导采样过程，在路径最优性和采样效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有采样路径规划算法忽略了环境信息和先前样本信息，而这些信息对于指导采样过程非常有用。

Method: 将标准RRT*算法与Transformer网络集成，Transformer用于提取环境特征并利用先前样本信息来更好地指导采样过程。

Result: 相比RRT*、Neural RRT*及其变体，在路径最优性和采样效率方面取得显著改进。

Conclusion: RRT*former算法通过整合环境信息和历史样本信息，有效提升了采样路径规划的性能。

Abstract: We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.

</details>


### [49] [Discovering Optimal Natural Gaits of Dissipative Systems via Virtual Energy Injection](https://arxiv.org/abs/2511.15513)
*Korbinian Griesbauer,Davide Calzolari,Maximilian Raff,C. David Remy,Alin Albu-Schäffer*

Main category: cs.RO

TL;DR: 提出了一种多阶段框架，通过利用自然动力学设计能量最优控制输入，提高弹性腿式机器人的能量效率。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人在非结构化环境中具有优势，但能效通常不如轮式机器人。利用自然动力学和弹性元件可以改善其能量经济性。

Method: 采用多阶段框架：首先通过新颖的能量注入技术识别被动运动模式，然后使用延续方法推导完全驱动耗散系统的能量最优控制输入。

Result: 在单腿和多腿机器人系统的仿真模型中验证了方法的适用性，展示了通过利用自然系统动力学提高效率的潜力。

Conclusion: 该方法为弹性腿式机器人的设计和操作提供了有价值的见解，通过利用自然系统动力学为提高效率和适应性提供了途径。

Abstract: Legged robots offer several advantages when navigating unstructured environments, but they often fall short of the efficiency achieved by wheeled robots. One promising strategy to improve their energy economy is to leverage their natural (unactuated) dynamics using elastic elements. This work explores that concept by designing energy-optimal control inputs through a unified, multi-stage framework. It starts with a novel energy injection technique to identify passive motion patterns by harnessing the system's natural dynamics. This enables the discovery of passive solutions even in systems with energy dissipation caused by factors such as friction or plastic collisions. Building on these passive solutions, we then employ a continuation approach to derive energy-optimal control inputs for the fully actuated, dissipative robotic system. The method is tested on simulated models to demonstrate its applicability in both single- and multi-legged robotic systems. This analysis provides valuable insights into the design and operation of elastic legged robots, offering pathways to improve their efficiency and adaptability by exploiting the natural system dynamics.

</details>


### [50] [Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies](https://arxiv.org/abs/2511.15520)
*Gabriel Lauzier,Alexandre Girard,François Ferland*

Main category: cs.RO

TL;DR: 本文研究了在扩散策略中部分执行去噪过程的可能性，允许机器人系统动态与计算机上的反向扩散过程并行运行，以提高实时性。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作任务中表现出色，但其计算昂贵的反向时间扩散过程难以满足实时应用的快速决策需求。

Method: 通过理论分析研究当系统动态与去噪动态耦合时，使用扩散策略的闭环系统稳定性边界。

Result: 提出了一个更快的模仿学习框架和基于演示方差的控制器稳定性度量指标。

Conclusion: 该工作为实时应用中的扩散策略提供了理论支撑和实用框架。

Abstract: Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.

</details>


### [51] [Decentralized Gaussian Process Classification and an Application in Subsea Robotics](https://arxiv.org/abs/2511.15529)
*Yifei Gao,Hans J. He,Daniel J. Stilwell,James McMahon*

Main category: cs.RO

TL;DR: 提出了一种用于AUV团队实时构建通信成功概率地图的数据共享策略，通过选择性地共享通信测量数据来应对水下声学通信的不确定性。


<details>
  <summary>Details</summary>
Motivation: 水下自主航行器团队依赖声学通信进行协调，但该通信媒介受限于有限范围、多径效应和低带宽。为应对声学通信的不确定性，需要实时学习通信环境。

Method: 将问题建模为分散式分类问题，AUV共享部分通信测量数据来构建通信成功概率地图。主要贡献是严格推导的数据共享策略，用于选择在AUV之间共享的测量数据。

Result: 使用弗吉尼亚理工大学690 AUV团队收集的真实声学通信数据进行了实验验证，证明该方法在水下环境中的有效性。

Conclusion: 提出的数据共享策略能够有效帮助AUV团队实时构建通信成功概率地图，应对水下声学通信的不确定性。

Abstract: Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.

</details>


### [52] [NMPC-based Motion Planning with Adaptive Weighting for Dynamic Object Interception](https://arxiv.org/abs/2511.15532)
*Chen Cai,Saksham Kohli,Steven Liu*

Main category: cs.RO

TL;DR: 提出一种自适应终端非线性MPC方法，用于双协作机械臂的动态物体拦截，相比原始终端方法显著降低控制能耗并提高运动质量


<details>
  <summary>Details</summary>
Motivation: 解决协作机械臂系统在闭环约束下拦截快速移动物体时的协调挑战，特别是传统方法容易违反执行器功率限制的问题

Method: 采用自适应终端非线性MPC，结合成本整形技术，与依赖终端惩罚的原始终端方法形成对比

Result: 实验显示AT方法有效缓解执行器功率限制违规，轨迹质量显著提升，平均规划周期仅19ms（小于40ms系统采样时间）

Conclusion: AT方法在计算开销最小的情况下显著改善运动质量和鲁棒性，非常适合动态协作拦截任务

Abstract: Catching fast-moving objects serves as a benchmark for robotic agility, posing significant coordination challenges for cooperative manipulator systems holding a catcher, particularly due to inherent closed-chain constraints. This paper presents a nonlinear model predictive control (MPC)-based motion planner that bridges high-level interception planning with real-time joint space control, enabling dynamic object interception for systems comprising two cooperating arms. We introduce an Adaptive- Terminal (AT) MPC formulation featuring cost shaping, which contrasts with a simpler Primitive-Terminal (PT) approach relying heavily on terminal penalties for rapid convergence. The proposed AT formulation is shown to effectively mitigate issues related to actuator power limit violations frequently encountered with the PT strategy, yielding trajectories and significantly reduced control effort. Experimental results on a robotic platform with two cooperative arms, demonstrating excellent real time performance, with an average planner cycle computation time of approximately 19 ms-less than half the 40 ms system sampling time. These results indicate that the AT formulation achieves significantly improved motion quality and robustness with minimal computational overhead compared to the PT baseline, making it well-suited for dynamic, cooperative interception tasks.

</details>


### [53] [UltraDP: Generalizable Carotid Ultrasound Scanning with Force-Aware Diffusion Policy](https://arxiv.org/abs/2511.15550)
*Ruoqu Chen,Xiangjie Yan,Kangchen Lv,Gao Huang,Zheng Li,Xiang Li*

Main category: cs.RO

TL;DR: UltraDP是一种基于扩散策略的自主超声扫描方法，通过多感官输入生成适合多模态动作分布的动作，专门用于颈动脉超声扫描，在未见过的受试者上达到95%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有超声扫描机器人在泛化性和数据利用效率方面存在局限，患者解剖结构变化和复杂的人机交互给自主机器人扫描带来挑战。

Method: 提出UltraDP方法，接收多感官输入（超声图像、腕部相机图像、接触力、探头姿态），使用扩散策略生成动作；设计专门引导模块使策略输出将动脉置于超声图像中心的动作；采用混合力-阻抗控制器确保稳定接触和安全交互；构建包含21名志愿者210次扫描的大规模训练数据集。

Result: 在横向扫描未见过的受试者时，UltraDP达到95%的成功率，展示了其有效性。

Conclusion: UltraDP通过探索引导模块和扩散策略的强大泛化能力，在自主颈动脉超声扫描中表现出色，为克服现有机器人扫描限制提供了有效解决方案。

Abstract: Ultrasound scanning is a critical imaging technique for real-time, non-invasive diagnostics. However, variations in patient anatomy and complex human-in-the-loop interactions pose significant challenges for autonomous robotic scanning. Existing ultrasound scanning robots are commonly limited to relatively low generalization and inefficient data utilization. To overcome these limitations, we present UltraDP, a Diffusion-Policy-based method that receives multi-sensory inputs (ultrasound images, wrist camera images, contact wrench, and probe pose) and generates actions that are fit for multi-modal action distributions in autonomous ultrasound scanning of carotid artery. We propose a specialized guidance module to enable the policy to output actions that center the artery in ultrasound images. To ensure stable contact and safe interaction between the robot and the human subject, a hybrid force-impedance controller is utilized to drive the robot to track such trajectories. Also, we have built a large-scale training dataset for carotid scanning comprising 210 scans with 460k sample pairs from 21 volunteers of both genders. By exploring our guidance module and DP's strong generalization ability, UltraDP achieves a 95% success rate in transverse scanning on previously unseen subjects, demonstrating its effectiveness.

</details>


### [54] [SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models](https://arxiv.org/abs/2511.15605)
*Senyu Fei,Siyin Wang,Li Ji,Ao Li,Shiduo Zhang,Liming Liu,Jinlong Hou,Jingjing Gong,Xianzhong Zhao,Xipeng Qiu*

Main category: cs.RO

TL;DR: SRPO是一个新颖的VLA-RL框架，通过利用模型自身成功轨迹作为自参考，消除了对外部演示或手动奖励工程的需求，显著提高了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型严重依赖专家演示，导致演示偏差和性能限制。强化学习作为后训练策略面临严重的奖励稀疏性问题，仅依赖二元成功指标浪费了失败轨迹中的有价值信息。

Method: 提出自参考策略优化(SRPO)，利用当前训练批次中生成的成功轨迹作为自参考，为失败尝试分配渐进式奖励。核心创新是使用世界模型的潜在空间表示来稳健地测量行为进展。

Result: 在LIBERO基准测试中，从48.9%的成功率基线出发，仅用200个RL步骤就达到了99.2%的新最先进成功率，相对改进103%。在LIBERO-Plus基准上实现了167%的性能提升。

Conclusion: SRPO通过自参考和潜在世界表示，有效解决了VLA-RL中的奖励稀疏性问题，实现了高效训练和卓越性能，无需额外监督。

Abstract: Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.

</details>


### [55] [Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography](https://arxiv.org/abs/2511.15614)
*Sai Puppala,Ismail Hossain,Jahangir Alam,Sajedul Talukder*

Main category: cs.RO

TL;DR: Optimus-Q机器人系统在核电站中自主监测空气质量和检测污染，采用自适应学习和安全量子通信技术，通过红外传感器实时监测有害气体，利用联邦学习提升预测能力，并通过量子密钥分发确保数据传输安全。


<details>
  <summary>Details</summary>
Motivation: 核电站等高风险环境需要更安全、高效的监测系统，传统方法在实时性、数据安全和适应性方面存在局限，因此需要整合机器人技术、机器学习和量子通信来提升环境监测能力。

Method: 开发Optimus-Q机器人系统，配备先进红外传感器进行实时环境监测，采用联邦学习方法在多个核电站间协作学习而不泄露数据隐私，结合量子密钥分发(QKD)确保安全数据传输，使用系统化导航模式和机器学习算法优化区域覆盖效率。

Result: 通过仿真和真实实验验证，Optimus-Q机器人能够有效提升核电站的运行安全性和响应能力，成功监测CO2、CO和CH4等有害气体排放，并实现了安全的数据传输和协作学习。

Conclusion: 该研究表明将机器人技术、机器学习和量子技术整合到危险环境监测系统中具有巨大潜力，能够显著改善核电站等高风险设施的安全性和运行效率。

Abstract: The integration of advanced robotics in nuclear power plants (NPPs) presents a transformative opportunity to enhance safety, efficiency, and environmental monitoring in high-stakes environments. Our paper introduces the Optimus-Q robot, a sophisticated system designed to autonomously monitor air quality and detect contamination while leveraging adaptive learning techniques and secure quantum communication. Equipped with advanced infrared sensors, the Optimus-Q robot continuously streams real-time environmental data to predict hazardous gas emissions, including carbon dioxide (CO$_2$), carbon monoxide (CO), and methane (CH$_4$). Utilizing a federated learning approach, the robot collaborates with other systems across various NPPs to improve its predictive capabilities without compromising data privacy. Additionally, the implementation of Quantum Key Distribution (QKD) ensures secure data transmission, safeguarding sensitive operational information. Our methodology combines systematic navigation patterns with machine learning algorithms to facilitate efficient coverage of designated areas, thereby optimizing contamination monitoring processes. Through simulations and real-world experiments, we demonstrate the effectiveness of the Optimus-Q robot in enhancing operational safety and responsiveness in nuclear facilities. This research underscores the potential of integrating robotics, machine learning, and quantum technologies to revolutionize monitoring systems in hazardous environments.

</details>


### [56] [Real-time Point Cloud Data Transmission via L4S for 5G-Edge-Assisted Robotics](https://arxiv.org/abs/2511.15677)
*Gerasimos Damigos,Achilleas Santi Seisa,Nikolaos Stathoulopoulos,Sara Sandberg,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出了一种基于L4S技术结合Draco压缩算法的实时LiDAR数据传输框架，能够在5G网络环境下实现低延迟、低损耗的3D点云数据流式传输，支持机器人应用的实时卸载处理。


<details>
  <summary>Details</summary>
Motivation: 机器人应用需要实时传输高比特率的3D LiDAR数据到云端进行处理，但现有技术难以在公共网络环境中同时保证低延迟和低损耗的数据传输质量。

Method: 扩展L4S-enabled SCReAM v2传输框架，集成Draco几何压缩算法，根据感知的信道容量和网络负载动态压缩LiDAR数据，实现自适应数据传输。

Result: 在公共5G网络的多公里城市环境中进行实验验证，系统能够保持低延迟和低损耗要求，成功支持3D SLAM算法的实时卸载和评估。

Conclusion: 该框架为需要实时LiDAR数据传输的机器人应用提供了一种有效的解决方案，在保持数据精度的同时实现了高效的网络传输性能。

Abstract: This article presents a novel framework for real-time Light Detection and Ranging (LiDAR) data transmission that leverages rate-adaptive technologies and point cloud encoding methods to ensure low-latency, and low-loss data streaming. The proposed framework is intended for, but not limited to, robotic applications that require real-time data transmission over the internet for offloaded processing. Specifically, the Low Latency, Low Loss, Scalable Throughput L4S-enabled SCReAM v2 transmission framework is extended to incorporate the Draco geometry compression algorithm, enabling dynamic compression of high-bitrate 3D LiDAR data according to the sensed channel capacity and network load. The low-latency 3D LiDAR streaming system is designed to maintain minimal end-to-end delay while constraining encoding errors to meet the accuracy requirements of robotic applications. We demonstrate the effectiveness of the proposed method through real-world experiments conducted over a public 5G network across multi-kilometer urban environments. The low-latency and low-loss requirements are preserved, while real-time offloading and evaluation of 3D SLAM algorithms are used to validate the framework's performance in practical use cases.

</details>


### [57] [In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data](https://arxiv.org/abs/2511.15704)
*Xiongyi Cai,Ri-Zhao Qiu,Geng Chen,Lai Wei,Isabella Liu,Tianshu Huang,Xuxin Cheng,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出了一种收集和使用自我中心视频数据的可扩展方法，将人类数据分为野外数据和任务数据，并构建了PHSD数据集。通过领域适应技术，开发了Human0策略，实现了仅从人类数据中学习语言指令跟随、少样本学习和鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 自我中心视频是学习操作策略的宝贵数据源，但由于数据异构性，现有方法未能充分发挥其潜力。本文旨在通过系统化的人类数据分类和使用方法，解锁自我中心数据的全部潜力。

Method: 将人类数据分为野外数据和任务数据，构建包含1000+小时野外数据和20+小时任务数据的PHSD数据集。使用领域适应技术训练大型自我中心语言条件流匹配策略Human0，缩小人类与人形机器人之间的差距。

Result: Human0实现了从人类数据中学习语言指令跟随、少样本学习能力，并通过任务数据提高了鲁棒性。

Conclusion: 通过系统化的人类数据分类和领域适应技术，可以有效地利用自我中心视频数据学习操作策略，实现语言指令跟随、少样本学习和鲁棒性等新特性。

Abstract: Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/

</details>
