<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 62]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience](https://arxiv.org/abs/2511.00026)
*Chaitanya Shinde,Divya Garikapati*

Main category: cs.RO

TL;DR: 本文综述了生成式AI在汽车行业的应用现状，重点分析了GAN和VAE等使能技术，探讨了在自动驾驶验证、零部件设计优化和人机交互等方面的机遇，同时指出了计算需求、偏见、知识产权和对抗鲁棒性等挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在成为汽车行业的变革力量，但现有综述多集中于感知或制造领域，本文旨在填补生成式AI在语音人机交互方面的研究空白，连接安全性和用户体验视角。

Method: 通过全面文献回顾和案例分析（梅赛德斯-奔驰MBUX虚拟助手），系统梳理生成式AI在汽车行业的应用现状、技术基础和发展前景。

Result: 研究发现生成式AI能够通过合成数据生成加速自动驾驶验证、优化零部件设计，并实现比传统基于规则的助手更自然、主动和个性化的车内交互体验。

Conclusion: 生成式AI在汽车领域具有巨大潜力，但需要解决技术、伦理和安全挑战，未来研究应致力于实现更安全、高效和以用户为中心的移动出行。

Abstract: Generative Artificial Intelligence is emerging as a transformative force in
the automotive industry, enabling novel applications across vehicle design,
manufacturing, autonomous driving, predictive maintenance, and in vehicle user
experience. This paper provides a comprehensive review of the current state of
GenAI in automotive, highlighting enabling technologies such as Generative
Adversarial Networks and Variational Autoencoders. Key opportunities include
accelerating autonomous driving validation through synthetic data generation,
optimizing component design, and enhancing human machine interaction via
personalized and adaptive interfaces. At the same time, the paper identifies
significant technical, ethical, and safety challenges, including computational
demands, bias, intellectual property concerns, and adversarial robustness, that
must be addressed for responsible deployment. A case study on Mercedes Benzs
MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more
natural, proactive, and personalized in car interactions compared to legacy
rule based assistants. Through this review and case study, the paper outlines
both the promise and limitations of GenAI integration in the automotive sector
and presents directions for future research and development aimed at achieving
safer, more efficient, and user centric mobility. Unlike prior reviews that
focus solely on perception or manufacturing, this paper emphasizes generative
AI in voice based HMI, bridging safety and user experience perspectives.

</details>


### [2] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: STRIDER框架通过整合空间布局先验和动态任务反馈，系统优化智能体在零样本视觉语言导航任务中的决策空间，显著提升了导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在零样本视觉语言导航中缺乏结构化决策和先前动作反馈的充分整合，导致导航鲁棒性不足，需要解决动作与空间结构和任务意图的长期对齐问题。

Method: 提出STRIDER框架，包含结构化路径点生成器（通过空间结构约束动作空间）和任务对齐调节器（基于任务进度调整行为），确保导航过程中的语义对齐。

Result: 在R2R-CE和RxR-CE基准测试中显著优于现有SOTA方法，成功率从29%提升至35%，相对增益达20.7%。

Conclusion: 空间约束决策和反馈引导执行对于提升零样本VLN-CE导航保真度至关重要。

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [3] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: BiBo系统利用现成的视觉语言模型控制人形智能体，通过指令编译器和运动执行器实现开放环境中的多样化交互，无需大量数据收集。


<details>
  <summary>Details</summary>
Motivation: 解决人形智能体在开放环境中处理灵活多样化交互的困难，避免收集海量数据的高昂成本，利用现成VLM的强泛化能力。

Method: 包含两个核心组件：1) 具身指令编译器 - 让VLM感知环境并将高级指令转换为带控制参数的低级原始命令；2) 基于扩散的运动执行器 - 从命令生成类人运动并动态适应环境物理反馈。

Result: 在开放环境中实现90.2%的交互任务成功率，文本引导运动执行精度比现有方法提高16.3%。

Conclusion: BiBo系统成功展示了利用现成VLM控制人形智能体的可行性，能够处理基础交互和复杂运动，为减少数据依赖提供了有效方案。

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [4] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: AR1是一个视觉-语言-动作模型，通过因果链推理与轨迹规划相结合，在复杂驾驶场景中提升决策能力，在规划准确性和安全性方面相比纯轨迹基线有显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决端到端模仿学习在安全关键的长尾场景中性能脆弱的问题，这些场景中监督稀疏且因果理解有限。

Method: 1) 构建因果链数据集；2) 模块化VLA架构结合视觉语言模型和扩散轨迹解码器；3) 多阶段训练策略包括监督微调和强化学习。

Result: 在挑战性案例中规划准确性提升12%，脱轨率降低35%，近距离遭遇率降低25%；RL后训练使推理质量提升45%，推理-动作一致性提升37%。

Conclusion: 通过将可解释推理与精确控制相结合，AR1展示了实现L4级自动驾驶的可行路径。

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [5] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: 提出基于数字孪生技术的机器人控制器自主动态重构框架，通过虚拟环境模拟优化运动轨迹，实现物理机器人在动态环境中的快速自适应。


<details>
  <summary>Details</summary>
Motivation: 传统控制系统在动态环境（如智慧城市、精准农业）中难以快速适应不断变化的地形和环境条件，导致效率低下或操作失败。

Method: 利用数字孪生技术创建机器人操作环境的虚拟副本，模拟和优化运动轨迹，根据现实世界变化重新计算路径和控制参数，并将更新后的代码部署到物理机器人。

Result: 实现了无需人工干预的快速可靠自适应，提高了机器人在动态环境中的操作效率和可靠性。

Conclusion: 该工作推进了数字孪生在机器人技术中的集成，为智能动态环境中增强自主性提供了可扩展的解决方案。

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [6] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: Real-DRL框架通过DRL-Student、PHY-Teacher和Trigger三个交互组件，在真实物理系统中实现安全优先的深度强化学习，解决了未知风险和Sim2Real差距问题。


<details>
  <summary>Details</summary>
Motivation: 解决安全关键自主系统中深度强化学习的实时安全性挑战，特别是应对未知风险和仿真到现实的差距问题。

Method: 采用三组件交互框架：DRL-Student负责双重自学习与教学学习范式，PHY-Teacher基于物理模型提供安全关键功能，Trigger管理两者交互。

Result: 在真实四足机器人、NVIDIA Isaac Gym四足机器人和倒立摆系统上的实验验证了框架的有效性，实现了安全保证和自动层次学习。

Conclusion: Real-DRL框架能够有效确保安全关键自主系统的安全性，同时实现高性能学习，解决了传统DRL方法的安全挑战。

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [7] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: 提出了一种共享自主框架，通过VR遥操作和自主手部控制相结合的方式，高效收集高质量的手臂-手部协调演示数据，并训练端到端VLA策略，在多样化物体上达到90%的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中高质量训练数据稀缺的问题，现有方法存在人工遥操作负担重、自动化规划动作不自然等限制。

Method: 采用共享自主框架，人类操作员通过VR控制手臂宏观运动，自主DexGrasp-VLA策略基于触觉和视觉反馈控制手部精细动作；训练端到端VLA策略并加入手臂-手部特征增强模块；通过纠正遥操作实现持续改进。

Result: 以最少人力生成高质量数据，在多样化物体（包括未见实例）上达到90%的成功率。

Conclusion: 该框架有效解决了灵巧操作中的数据收集挑战，显著提升了机器人的操作能力。

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [8] [EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations](https://arxiv.org/abs/2511.00153)
*Justin Yu,Yide Shentu,Di Wu,Pieter Abbeel,Ken Goldberg,Philipp Wu*

Main category: cs.RO

TL;DR: 提出了EgoMI框架，通过捕捉同步的末端执行器和主动头部轨迹来解决模仿学习中的人机体现差异问题，使用记忆增强策略处理快速视角变化。


<details>
  <summary>Details</summary>
Motivation: 人类演示的模仿学习面临体现差异挑战，人类在操作时主动协调头部和手部运动，产生动态的头部运动，而静态机器人感知系统无法复制这些行为，导致策略性能下降。

Method: 开发EgoMI框架捕捉同步的末端执行器和主动头部轨迹，引入记忆增强策略选择性整合历史观察，以处理快速和广泛的头部视角变化。

Result: 在配备驱动相机头的双手机器人上评估，显示具有显式头部运动建模的策略始终优于基线方法。

Conclusion: EgoMI通过协调的手眼学习有效弥合了人机体现差异，为半人形机器人的鲁棒模仿学习提供了解决方案。

Abstract: Imitation learning from human demonstrations offers a promising approach for
robot skill acquisition, but egocentric human data introduces fundamental
challenges due to the embodiment gap. During manipulation, humans actively
coordinate head and hand movements, continuously reposition their viewpoint and
use pre-action visual fixation search strategies to locate relevant objects.
These behaviors create dynamic, task-driven head motions that static robot
sensing systems cannot replicate, leading to a significant distribution shift
that degrades policy performance. We present EgoMI (Egocentric Manipulation
Interface), a framework that captures synchronized end-effector and active head
trajectories during manipulation tasks, resulting in data that can be
retargeted to compatible semi-humanoid robot embodiments. To handle rapid and
wide-spanning head viewpoint changes, we introduce a memory-augmented policy
that selectively incorporates historical observations. We evaluate our approach
on a bimanual robot equipped with an actuated camera head and find that
policies with explicit head-motion modeling consistently outperform baseline
methods. Results suggest that coordinated hand-eye learning with EgoMI
effectively bridges the human-robot embodiment gap for robust imitation
learning on semi-humanoid embodiments. Project page:
https://egocentric-manipulation-interface.github.io

</details>


### [9] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: 使用时间序列基础模型（特别是Chronos）预测未记录的伸手试验，可在仅使用8次实际试验的情况下达到与24-28次完整试验相当的可靠性，显著缩短Kinarm视觉引导伸手评估时间。


<details>
  <summary>Details</summary>
Motivation: Kinarm机器人的视觉引导伸手评估需要40-64次伸手试验，造成时间和疲劳负担。研究旨在评估时间序列基础模型是否能用早期试验子集替代未记录试验，同时保持标准参数的可靠性。

Method: 分析461名中风和599名对照参与者的VGR速度信号，仅保留前8或16次试验，使用ARIMA、MOMENT和Chronos模型预测合成试验，重新计算四个运动学特征并与完整试验参考比较。

Result: Chronos预测使所有参数ICC≥0.90，仅需8次实际试验加预测试验即可达到24-28次实际试验的可靠性。MOMENT有中等改善，ARIMA改善最小。

Conclusion: 基础模型预测可大幅缩短Kinarm VGR评估时间，对最严重中风患者，评估时间从4-5分钟降至约1分钟，同时保持运动学精度，为中风后运动障碍评估提供高效机器人评估范式。

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [10] [Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial](https://arxiv.org/abs/2511.00259)
*Andria J. Farrens,Luis Garcia-Fernandez,Raymond Diaz Rojas,Jillian Obeso Estrada,Dylan Reinsdorf,Vicky Chan,Disha Gupta,Joel Perry,Eric Wolbrecht,An Do,Steven C. Cramer,David J. Reinkensmeyer*

Main category: cs.RO

TL;DR: 针对有本体感觉缺陷的中风患者，两种本体感觉定制训练方法（Propriopixel训练和虚拟辅助训练）比标准训练更能改善手部功能，且本体感觉改善与手功能提升相关，神经敏感性也得到增强。


<details>
  <summary>Details</summary>
Motivation: 测试本体感觉定制的机器人训练是否能改善中风患者的手部功能和神经处理，探索精准康复的途径。

Method: 使用机器人手指外骨骼，比较三种训练方法：标准训练、Propriopixel训练（通过游戏化运动增强本体感觉处理）和虚拟辅助训练（减少机器人辅助以增加对自我生成反馈的依赖）。46名慢性中风患者随机分组完成9次2小时训练。

Result: 在有本体感觉缺陷的参与者中，Propriopixel训练（Box and Block Test: 7±4.2, p=0.002）和虚拟辅助训练（4.5±4.4, p=0.068）比标准训练（0.8±2.3积木）带来更大的手功能改善。本体感觉改善与手功能提升相关，定制训练增强了神经对本体感觉线索的敏感性。

Conclusion: 本体感觉定制训练是精准神经康复的有效途径，能改善手部功能并增强神经处理能力。

Abstract: Precision rehabilitation aims to tailor movement training to improve
outcomes. We tested whether proprioceptively-tailored robotic training improves
hand function and neural processing in stroke survivors. Using a robotic finger
exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel
Training, which uses robot-facilitated, gamified movements to enhance
proprioceptive processing, and Virtual Assistance Training, which reduces
robotic aid to increase reliance on self-generated feedback. In a randomized
controlled trial, forty-six chronic stroke survivors completed nine 2-hour
sessions of Standard, Propriopixel or Virtual training. Among participants with
proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)
and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand
function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with
improvements in hand function. Tailored training enhanced neural sensitivity to
proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive
Contingent Negative Variation. These findings support proprioceptively-tailored
training as a pathway to precision neurorehabilitation.

</details>


### [11] [FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications](https://arxiv.org/abs/2511.00306)
*Baoshan Song,Ruijie Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本文建立了滑动窗口因子图优化(SW-FGO)与卡尔曼滤波器变体(KFV)之间的理论联系，提出了递归FGO(Re-FGO)框架，在特定条件下Re-FGO可精确重现EKF/IEKF/REKF/RIEKF，同时阐明了SW-FGO在非线性、非高斯场景中的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然SW-FGO在导航研究中因对非高斯噪声和非线性测量模型的鲁棒性而受到关注，但其与扩展卡尔曼滤波器(EKF)等卡尔曼滤波器变体之间的理论关系仍不明确，需要建立两者之间的理论连接。

Method: 提出了递归FGO(Re-FGO)框架，在马尔可夫假设、高斯噪声和L2损失函数以及单状态窗口的明确条件下，将KFV表示为SW-FGO形式。

Result: 在特定条件下，Re-FGO能够精确重现EKF/IEKF/REKF/RIEKF，同时SW-FGO在非线性、非高斯场景中展现出可预测计算成本下的可测量优势。

Conclusion: 澄清了SW-FGO与KFV之间的关系，突出了SW-FGO在实际应用中的独特优势，特别是在数值估计和深度学习集成方面。

Abstract: Sliding window-factor graph optimization (SW-FGO) has gained more and more
attention in navigation research due to its robust approximation to
non-Gaussian noises and nonlinearity of measuring models. There are lots of
works focusing on its application performance compared to extended Kalman
filter (EKF) but there is still a myth at the theoretical relationship between
the SW-FGO and EKF. In this paper, we find the necessarily fair condition to
connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF
(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the
conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV
under SW-FGO formulation. Under explicit conditions (Markov assumption,
Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates
exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in
nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after
clarifying the connection between them, we highlight the unique advantages of
SW-FGO in practical phases, especially on numerical estimation and deep
learning integration. The code and data used in this work is open sourced at
https://github.com/Baoshan-Song/KFV-FGO-Comparison.

</details>


### [12] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: SonarSweep是一个新颖的端到端深度学习框架，通过将平面扫描算法适配于声纳和视觉数据的跨模态融合，解决了水下视觉退化环境中的3D重建难题。


<details>
  <summary>Details</summary>
Motivation: 在视觉退化的水下环境中进行准确的3D重建具有挑战性。单模态方法不足：基于视觉的方法因能见度差和几何约束而失败，而声纳则因固有的高程模糊性和低分辨率而受限。现有的融合技术依赖启发式方法和有缺陷的几何假设，导致显著伪影且无法建模复杂场景。

Method: SonarSweep通过将原理性的平面扫描算法适配于声纳和视觉数据的跨模态融合，构建了一个端到端的深度学习框架。该方法在高保真模拟和真实世界环境中进行了广泛实验。

Result: SonarSweep能够持续生成密集且准确的深度图，在各种挑战性条件下（特别是在高浊度环境中）显著优于最先进的方法。

Conclusion: SonarSweep克服了现有方法的局限性，为水下3D重建提供了有效的解决方案。作者将公开代码和首个包含同步立体相机和声纳数据的新型数据集，以促进进一步研究。

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [13] [Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory](https://arxiv.org/abs/2511.00412)
*John A. Christian,Michael R. Walker II,Wyatt Bridgman,Michael J. Sparapany*

Main category: cs.RO

TL;DR: 提出了一种基于经典Runge-Kutta积分方法的新型圆锥补偿算法，该算法能够简化到最流行的圆锥算法，并提供了生成高阶算法的清晰流程。


<details>
  <summary>Details</summary>
Motivation: 现代车辆通常使用捷联系统，陀螺仪积分需要考虑传感器在积分期间的旋转，因此需要圆锥补偿。现有多种圆锥补偿算法，但需要更系统的方法来生成高阶算法。

Method: 从经典的Runge-Kutta积分方法直接构建新型圆锥校正算法，通过简化情况可退化为最流行的圆锥算法，并提供了生成高阶算法的明确步骤。

Result: 成功开发了一类新的圆锥补偿算法，该算法基于Runge-Kutta方法，能够系统性地生成不同阶数的补偿方案。

Conclusion: 提出的基于Runge-Kutta的新型圆锥补偿算法为导航系统提供了一种系统化的高阶补偿方法，简化了算法开发过程。

Abstract: The integration of gyroscope measurements is an essential task for most
navigation systems. Modern vehicles typically use strapdown systems, such that
gyro integration requires coning compensation to account for the sensor's
rotation during the integration. Many coning compensation algorithms have been
developed and a few are reviewed. This work introduces a new class of coning
correction algorithm built directly from the classical Runge-Kutta integration
routines. A simple case is shown to collapse to one of the most popular coning
algorithms and a clear procedure for generating higher-order algorithms is
presented.

</details>


### [14] [Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU](https://arxiv.org/abs/2511.00492)
*Simon Giel,James Hurrell,Shreya Santra,Ashutosh Mishra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发用于月球机器人系统MoonBot的铲斗滚筒，通过3D打印原型测试，实现连续挖掘率777.54 kg/h和批量挖掘率172.02 kg/h，能耗分别为0.022 Wh/kg和0.86 Wh/kg。


<details>
  <summary>Details</summary>
Motivation: 月球原位资源利用是实现可持续月球探索的关键技术，挖掘月壤是获取月球资源的第一步。

Method: 为日本Moonshot计划的模块化机器人系统MoonBot开发铲斗滚筒，使用PLA材料3D打印原型，通过沙盒测试评估效率。

Result: 工具重4.8kg，体积14.06L，连续挖掘率777.54 kg/h，能耗0.022 Wh/kg；批量挖掘率172.02 kg/h，能耗0.86 Wh/kg。

Conclusion: 概念成功实现，主要优势是与模块化MoonBot平台的兼容性，未来可集成传感器和自主控制系统改进挖掘过程。

Abstract: In-Situ Resource Utilization (ISRU) is one of the key technologies for
enabling sustainable access to the Moon. The ability to excavate lunar regolith
is the first step in making lunar resources accessible and usable. This work
presents the development of a bucket drum for the modular robotic system
MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made
of PLA was manufactured to evaluate its efficiency through a series of sandbox
tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is
capable of continuous excavation at a rate of 777.54 kg/h with a normalized
energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is
172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of
excavated material. The obtained results demonstrate the successful
implementation of the concept. A key advantage of the developed tool is its
compatibility with the modular MoonBot robotic platform, which enables flexible
and efficient mission planning. Further improvements may include the
integration of sensors and an autonomous control system to enhance the
excavation process.

</details>


### [15] [Descriptive Model-based Learning and Control for Bipedal Locomotion](https://arxiv.org/abs/2511.00512)
*Suraj Kumar,Andy Ruina*

Main category: cs.RO

TL;DR: 提出了一种新型的双足平衡控制方法，避免将低维模型强加于完整模型，而是使用描述性模型来维持平衡，让剩余自由度在高维空间中自由演化，从而实现高效的人形步态和更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统双足机器人平衡控制方法依赖低维模型进行运动规划和反应控制，这限制了完整机器人的行为，导致效率低下的行走模式（如弯曲膝盖）。但双足平衡本质上是低维的，可以用简单的状态和动作描述符在低维状态空间中有效描述。

Method: 提出控制框架使用具有维持平衡所需最小自由度的描述性模型，允许剩余自由度在高维空间中自由演化，不预设低维模型到完整模型。

Result: 该方法产生了高效的人形行走步态，并提高了鲁棒性。

Conclusion: 通过仅约束低维状态空间中的投影，让机器人的运动在高维状态空间中自由演化，可以实现更自然高效的双足平衡控制。

Abstract: Bipedal balance is challenging due to its multi-phase, hybrid nature and
high-dimensional state space. Traditional balance control approaches for
bipedal robots rely on low-dimensional models for locomotion planning and
reactive control, constraining the full robot to behave like these simplified
models. This involves tracking preset reference paths for the Center of Mass
and upper body obtained through low-dimensional models, often resulting in
inefficient walking patterns with bent knees. However, we observe that bipedal
balance is inherently low-dimensional and can be effectively described with
simple state and action descriptors in a low-dimensional state space. This
allows the robot's motion to evolve freely in its high-dimensional state space,
only constraining its projection in the low-dimensional state space. In this
work, we propose a novel control approach that avoids prescribing a
low-dimensional model to the full model. Instead, our control framework uses a
descriptive model with the minimum degrees of freedom necessary to maintain
balance, allowing the remaining degrees of freedom to evolve freely in the
high-dimensional space. This results in an efficient human-like walking gait
and improved robustness.

</details>


### [16] [Adaptive and Multi-object Grasping via Deformable Origami Modules](https://arxiv.org/abs/2511.00516)
*Peiyi Wang,Paul A. M. Lefeuvre,Shangwei Zou,Zhenwei Ni,Daniela Rus,Cecilia Laschi*

Main category: cs.RO

TL;DR: 提出了一种基于折纸结构的混合夹爪，具有被动变形能力和恒定力输出，可实现多物体同时抓取，无需主动传感或反馈控制。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人夹爪通常依赖笨重的执行器、复杂控制策略或先进触觉传感来实现稳定抓取，需要更简单高效的解决方案。

Method: 采用多指混合夹爪设计，每个手指由并行折纸模块组成，通过单自由度执行器驱动，实现被动形状适应和稳定抓取力。

Result: 展示了同时多物体抓取能力，能够抓取不同形状和大小的堆叠物体，并在不同状态下独立放置，显著提高操作效率。

Conclusion: 折纸基柔性结构作为可扩展模块，在家庭和工业拾取场景中具有实现自适应、稳定和高效多物体操作的潜力。

Abstract: Soft robotics gripper have shown great promise in handling fragile and
geometrically complex objects. However, most existing solutions rely on bulky
actuators, complex control strategies, or advanced tactile sensing to achieve
stable and reliable grasping performance. In this work, we present a
multi-finger hybrid gripper featuring passively deformable origami modules that
generate constant force and torque output. Each finger composed of parallel
origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape
adaptability and stable grasping force without active sensing or feedback
control. More importantly, we demonstrate an interesting capability in
simultaneous multi-object grasping, which allows stacked objects of varied
shape and size to be picked, transported and placed independently at different
states, significantly improving manipulation efficiency compared to
single-object grasping. These results highlight the potential of origami-based
compliant structures as scalable modules for adaptive, stable and efficient
multi-object manipulation in domestic and industrial pick-and-place scenarios.

</details>


### [17] [Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy](https://arxiv.org/abs/2511.00555)
*Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 提出D3P算法，通过双分支架构解耦不同感官模态，结合深度Koopman算子增强视觉表示学习，在机器人操作任务中显著优于现有扩散策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的策略在机器人模仿学习中难以捕捉跨多步的强时间依赖性，特别是当结合本体感知输入时容易过拟合，导致任务失败。

Method: 采用双分支架构：视觉分支编码视觉观察指示任务进展，融合分支整合视觉和本体感知输入进行精确操作；引入深度Koopman算子模块捕捉视觉输入的结构化时间动态；使用生成模型的测试时损失作为置信信号指导动作块聚合。

Result: 在6个RLBench桌面任务仿真实验中平均性能提升14.6%；在3个真实世界机器人操作任务中实现15.0%的改进。

Conclusion: D3P算法通过解耦感官模态和增强视觉表示学习，有效解决了现有扩散策略在机器人模仿学习中的局限性，显著提升了策略执行的可靠性。

Abstract: Integrating generative models with action chunking has shown significant
promise in imitation learning for robotic manipulation. However, the existing
diffusion-based paradigm often struggles to capture strong temporal
dependencies across multiple steps, particularly when incorporating
proprioceptive input. This limitation can lead to task failures, where the
policy overfits to proprioceptive cues at the expense of capturing the visually
derived features of the task. To overcome this challenge, we propose the Deep
Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a
dual-branch architecture to decouple the roles of different sensory modality
combinations. The visual branch encodes the visual observations to indicate
task progression, while the fused branch integrates both visual and
proprioceptive inputs for precise manipulation. Within this architecture, when
the robot fails to accomplish intermediate goals, such as grasping a drawer
handle, the policy can dynamically switch to execute action chunks generated by
the visual branch, allowing recovery to previously observed states and
facilitating retrial of the task. To further enhance visual representation
learning, we incorporate a Deep Koopman Operator module that captures
structured temporal dynamics from visual inputs. During inference, we use the
test-time loss of the generative model as a confidence signal to guide the
aggregation of the temporally overlapping predicted action chunks, thereby
enhancing the reliability of policy execution. In simulation experiments across
six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion
policy by an average of 14.6\%. On three real-world robotic manipulation tasks,
it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.

</details>


### [18] [Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles](https://arxiv.org/abs/2511.00635)
*Hyungtae Lim,Daebeom Kim,Hyun Myung*

Main category: cs.RO

TL;DR: 提出Multi-Mapcher框架，通过大规模地图到地图配准实现多会话SLAM的初始对齐，替代传统依赖闭环检测的方法，提高了异质LiDAR传感器的建图性能。


<details>
  <summary>Details</summary>
Motivation: 现有MSS方法过度依赖闭环检测，但在异质LiDAR传感器（不同密度和视场角）下性能会下降，需要新的初始对齐方法。

Method: 使用大规模地图到地图配准进行会话间初始对齐，然后基于半径搜索找到闭环，最后采用锚节点优化的位姿图优化构建全局一致地图。

Result: 实验表明该方法在各种LiDAR传感器上性能显著优于现有方法，且速度更快。

Conclusion: Multi-Mapcher框架通过地图级配准有效解决了异质LiDAR多会话SLAM的初始对齐问题，提供了更鲁棒和高效的解决方案。

Abstract: As various 3D light detection and ranging (LiDAR) sensors have been
introduced to the market, research on multi-session simultaneous localization
and mapping (MSS) using heterogeneous LiDAR sensors has been actively
conducted. Existing MSS methods mostly rely on loop closure detection for
inter-session alignment; however, the performance of loop closure detection can
be potentially degraded owing to the differences in the density and field of
view (FoV) of the sensors used in different sessions. In this study, we
challenge the existing paradigm that relies heavily on loop detection modules
and propose a novel MSS framework, called Multi-Mapcher, that employs
large-scale map-to-map registration to perform inter-session initial alignment,
which is commonly assumed to be infeasible, by leveraging outlier-robust 3D
point cloud registration. Next, after finding inter-session loops by radius
search based on the assumption that the inter-session initial alignment is
sufficiently precise, anchor node-based robust pose graph optimization is
employed to build a consistent global map. As demonstrated in our experiments,
our approach shows substantially better MSS performance for various LiDAR
sensors used to capture the sessions and is faster than state-of-the-art
approaches. Our code is available at
https://github.com/url-kaist/multi-mapcher.

</details>


### [19] [When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage](https://arxiv.org/abs/2511.00783)
*Jingzehua Xu,Weihang Zhang,Yangyang Li,Hongmiaoyi Zhang,Guanwen Xie,Jiwei Tang,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: 提出了一种语义引导的模糊控制框架，将大语言模型与可解释控制和轻量级协调相结合，解决水下多机器人协同覆盖的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决水下多机器人协同覆盖面临的局部可观测性、有限通信、环境不确定性以及缺乏全局定位等问题。

Method: 使用LLM将原始多模态观测压缩为紧凑的人类可解释语义标记，通过模糊推理系统映射为平滑稳定的转向和步态命令，并引入语义通信实现多机器人协调。

Result: 在未知珊瑚礁环境中的广泛模拟显示，该框架在有限感知和通信条件下实现了稳健的OOI导向导航和协同覆盖，提高了效率和适应性。

Conclusion: 该框架缩小了语义认知与分布式水下控制在GPS拒绝、无地图条件下的差距，为水下多机器人系统提供了有效的解决方案。

Abstract: Underwater multi-robot cooperative coverage remains challenging due to
partial observability, limited communication, environmental uncertainty, and
the lack of access to global localization. To address these issues, this paper
presents a semantics-guided fuzzy control framework that couples Large Language
Models (LLMs) with interpretable control and lightweight coordination. Raw
multimodal observations are compressed by the LLM into compact,
human-interpretable semantic tokens that summarize obstacles, unexplored
regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy
inference system with pre-defined membership functions then maps these tokens
into smooth and stable steering and gait commands, enabling reliable navigation
without relying on global positioning. Then, we further coordinate multiple
robots by introducing semantic communication that shares intent and local
context in linguistic form, enabling agreement on who explores where while
avoiding redundant revisits. Extensive simulations in unknown reef-like
environments show that, under limited sensing and communication, the proposed
framework achieves robust OOI-oriented navigation and cooperative coverage with
improved efficiency and adaptability, narrowing the gap between semantic
cognition and distributed underwater control in GPS-denied, map-free
conditions.

</details>


### [20] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: 提出了一种基于Hankel动态模态分解的在线框架，用于实时学习和预测其他智能体的非线性运动模型，通过滑动窗口处理部分噪声数据，实现去噪和短期预测。


<details>
  <summary>Details</summary>
Motivation: 自主系统需要从部分噪声数据中预测附近智能体的运动，但现有方法难以在实时条件下学习非线性预测模型。

Method: 使用改进的滑动窗口Hankel-DMD，通过Hankel矩阵嵌入部分噪声测量，利用Page矩阵进行奇异值硬阈值估计有效秩，Cadzow投影确保结构化低秩一致性，构建时变Hankel-DMD提升线性预测器进行多步预测。

Result: 在仿真和动态起重机实验平台上验证，方法在Gaussian和重尾噪声下实现稳定的方差感知去噪和短期预测，适合集成到实时控制框架中。

Conclusion: 该方法能够实时学习非线性运动预测模型，为下游估计器和风险感知规划提供方差跟踪信号，适合自主系统的实时控制需求。

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [21] [Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches](https://arxiv.org/abs/2511.00840)
*William Suliman,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: 提出了一种结合启发式步态规划的基于学习的两足机器人行走框架，通过期望躯干速度跟踪实现精确的环境交互，在非结构化环境中表现出优越的鲁棒性和能效。


<details>
  <summary>Details</summary>
Motivation: 传统基于完整或简化动力学的步态规划方法需要复杂的步态规划器和分析模型，限制了机器人在非结构化环境中的适应能力。

Method: 采用启发式步态规划策略，结合Raibert型控制器根据期望与实际躯干速度误差调节足部位置，避免使用复杂的分析模型。

Result: 与基于线性倒立摆模型的控制器相比，该方法在目标速度跟踪精度上达到80%的准确率，在崎岖地形上的鲁棒性提高超过50%，并具有更好的能效。

Conclusion: 在训练架构中引入复杂的分析模型可能不是实现稳定鲁棒两足行走的必要条件，即使在非结构化环境中也是如此。

Abstract: This work presents an extended framework for learning-based bipedal
locomotion that incorporates a heuristic step-planning strategy guided by
desired torso velocity tracking. The framework enables precise interaction
between a humanoid robot and its environment, supporting tasks such as crossing
gaps and accurately approaching target objects. Unlike approaches based on full
or simplified dynamics, the proposed method avoids complex step planners and
analytical models. Step planning is primarily driven by heuristic commands,
while a Raibert-type controller modulates the foot placement length based on
the error between desired and actual torso velocity. We compare our method with
a model-based step-planning approach -- the Linear Inverted Pendulum Model
(LIPM) controller. Experimental results demonstrate that our approach attains
comparable or superior accuracy in maintaining target velocity (up to 80%),
significantly greater robustness on uneven terrain (over 50% improvement), and
improved energy efficiency. These results suggest that incorporating complex
analytical, model-based components into the training architecture may be
unnecessary for achieving stable and robust bipedal walking, even in
unstructured environments.

</details>


### [22] [Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots](https://arxiv.org/abs/2511.00917)
*Junyao Shi,Rujia Yang,Kaitian Chao,Selina Bingqing Wan,Yifei Shao,Jiahui Lei,Jianing Qian,Long Le,Pratik Chaudhari,Kostas Daniilidis,Chuan Wen,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: Maestro是一个基于视觉语言模型(VLM)的机器人系统，通过动态组合感知、规划和控制模块来构建通用策略，超越了现有VLA模型的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 当前通用机器人研究主要依赖大规模数据集训练端到端模型，本文探索了另一条路径：围绕VLM构建通用策略，利用其通用能力与特定机器人模块相结合。

Method: 使用VLM编码代理动态组合感知、规划和控制模块为程序化策略，具有简化的闭环接口和多样化工具库。

Result: 在挑战性操作技能上显著超越现有VLA模型的零样本性能，易于扩展新模块，适应新形态，并能通过少量真实世界经验进行本地代码编辑适应。

Conclusion: Maestro展示了基于VLM构建通用机器人策略的可行性，提供了一种可扩展、可编辑且适应性强的替代方案。

Abstract: Today's best-explored routes towards generalist robots center on collecting
ever larger "observations-in actions-out" robotics datasets to train large
end-to-end models, copying a recipe that has worked for vision-language models
(VLMs). We pursue a road less traveled: building generalist policies directly
around VLMs by augmenting their general capabilities with specific robot
capabilities encapsulated in a carefully curated set of perception, planning,
and control modules. In Maestro, a VLM coding agent dynamically composes these
modules into a programmatic policy for the current task and scenario. Maestro's
architecture benefits from a streamlined closed-loop interface without many
manually imposed structural constraints, and a comprehensive and diverse tool
repertoire. As a result, it largely surpasses today's VLA models for zero-shot
performance on challenging manipulation skills. Further, Maestro is easily
extensible to incorporate new modules, easily editable to suit new embodiments
such as a quadruped-mounted arm, and even easily adapts from minimal real-world
experiences through local code edits.

</details>


### [23] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: Fast-SmartWay是一个端到端的零样本视觉语言导航框架，仅使用三个前视RGB-D图像和自然语言指令，无需全景视图和路径点预测器，显著降低了延迟并提高了现实世界适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLN-CE方法依赖全景观测和两阶段流水线，导致显著延迟并限制现实世界应用。需要消除这些限制以实现更实用的零样本导航。

Method: 使用三个前视RGB-D图像结合自然语言指令，让MLLM直接预测动作。引入不确定性感知推理模块，包括消歧模块和未来-过去双向推理机制，以增强决策鲁棒性。

Result: 在模拟和真实机器人环境中的实验表明，该方法显著降低了每步延迟，同时达到或优于全景视图基线的性能。

Conclusion: Fast-SmartWay证明了在现实世界零样本具身导航中的实用性和有效性，为实时应用提供了可行的解决方案。

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [24] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: URDF-Anything是一个基于3D多模态大语言模型的端到端自动重建框架，用于构建关节物体的数字孪生，通过联合优化几何分割和运动学参数预测，显著提升了分割精度、参数预测准确性和物理可执行性。


<details>
  <summary>Details</summary>
Motivation: 构建准确的关节物体数字孪生对机器人仿真训练和具身AI世界模型构建至关重要，但传统方法需要繁琐的手动建模或多阶段流程，因此需要开发端到端的自动重建解决方案。

Method: 采用基于点云和文本多模态输入的自回归预测框架，实现几何分割和运动学参数预测的联合优化，并设计了专门的[SEG]标记机制与点云特征直接交互，确保细粒度部件级分割与运动学参数预测的一致性。

Result: 在模拟和真实数据集上的实验表明，该方法在几何分割（mIoU提升17%）、运动学参数预测（平均误差减少29%）和物理可执行性（超越基线50%）方面显著优于现有方法，且在训练集外物体上表现出优秀的泛化能力。

Conclusion: 该工作为机器人仿真构建数字孪生提供了高效解决方案，显著增强了从仿真到现实的迁移能力。

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [25] [Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing](https://arxiv.org/abs/2511.00983)
*Yizhao Qian,Yujie Zhu,Jiayuan Luo,Li Liu,Yixuan Yuan,Guochen Ning,Hongen Liao*

Main category: cs.RO

TL;DR: 提出了一种用于机器人超声系统的协同感知控制框架，通过解耦双流感知网络和单步流策略，实现了超过60Hz的闭环控制频率，在动态目标跟踪中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决机器人超声系统中大规模高频干扰下动态目标实时跟踪的关键挑战，主要由于现有系统的端到端延迟问题，需要感知与控制的协同设计。

Method: 包含两个紧密耦合的贡献：(1) 解耦双流感知网络，从2D图像高频估计3D平移状态；(2) 单步流策略，在一次推理中生成完整动作序列，绕过传统策略的迭代瓶颈。

Result: 在动态体模上，系统跟踪复杂3D轨迹的平均误差低于6.5mm，能从超过170mm位移中稳健重获目标，能以102mm/s速度跟踪目标，终端误差低于1.7mm。人体志愿者体内实验验证了框架的有效性和鲁棒性。

Conclusion: 该工作提出了一个整体架构的机器人超声系统，统一了高带宽跟踪与大规模重新定位，是实现动态临床环境中稳健自主性的关键一步。

Abstract: Real-time tracking of dynamic targets amidst large-scale, high-frequency
disturbances remains a critical unsolved challenge in Robotic Ultrasound
Systems (RUSS), primarily due to the end-to-end latency of existing systems.
This paper argues that breaking this latency barrier requires a fundamental
shift towards the synergistic co-design of perception and control. We realize
it in a novel framework with two tightly-coupled contributions: (1) a Decoupled
Dual-Stream Perception Network that robustly estimates 3D translational state
from 2D images at high frequency, and (2) a Single-Step Flow Policy that
generates entire action sequences in one inference pass, bypassing the
iterative bottleneck of conventional policies. This synergy enables a
closed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system
not only tracks complex 3D trajectories with a mean error below 6.5mm but also
demonstrates robust re-acquisition from over 170mm displacement. Furthermore,
it can track targets at speeds of 102mm/s, achieving a terminal error below
1.7mm. Moreover, in-vivo experiments on a human volunteer validate the
framework's effectiveness and robustness in a realistic clinical setting. Our
work presents a RUSS holistically architected to unify high-bandwidth tracking
with large-scale repositioning, a critical step towards robust autonomy in
dynamic clinical environments.

</details>


### [26] [GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies](https://arxiv.org/abs/2511.00998)
*Ziye Wang,Li Kang,Yiran Qin,Jiahua Ma,Zhanglin Peng,Lei Bai,Ruimao Zhang*

Main category: cs.RO

TL;DR: 提出GauDP方法，通过高斯-图像协同表示实现多智能体协作系统中的可扩展感知感知模仿学习，在保持个体视角的同时实现全局一致的行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡细粒度局部控制与全局场景理解，导致可扩展性有限和协作质量下降。需要一种能在保持个体视角的同时实现全局协调的解决方案。

Method: 构建全局一致的3D高斯场，然后动态地将3D高斯属性重新分配到每个智能体的局部视角，使所有智能体能够从共享场景表示中自适应查询任务关键特征。

Result: 在RoboFactory基准测试中表现优于现有基于图像的方法，接近基于点云方法的有效性，同时在智能体数量增加时保持强可扩展性。

Conclusion: GauDP方法无需额外传感模态即可实现细粒度控制和全局一致行为，为多智能体协作系统提供了有效的解决方案。

Abstract: Recently, effective coordination in embodied multi-agent systems has remained
a fundamental challenge, particularly in scenarios where agents must balance
individual perspectives with global environmental awareness. Existing
approaches often struggle to balance fine-grained local control with
comprehensive scene understanding, resulting in limited scalability and
compromised collaboration quality. In this paper, we present GauDP, a novel
Gaussian-image synergistic representation that facilitates scalable,
perception-aware imitation learning in multi-agent collaborative systems.
Specifically, GauDP constructs a globally consistent 3D Gaussian field from
decentralized RGB observations, then dynamically redistributes 3D Gaussian
attributes to each agent's local perspective. This enables all agents to
adaptively query task-critical features from the shared scene representation
while maintaining their individual viewpoints. This design facilitates both
fine-grained control and globally coherent behavior without requiring
additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the
RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our
method achieves superior performance over existing image-based methods and
approaches the effectiveness of point-cloud-driven methods, while maintaining
strong scalability as the number of agents increases.

</details>


### [27] [AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models](https://arxiv.org/abs/2511.01031)
*Mathieu Dubied,Paolo Tiso,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出基于张量参数降阶模型的优化算法，用于高效优化受复杂非线性力作用的软体机器人结构，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 软体结构在复杂非线性力作用下的高效优化是机器人领域的关键挑战，传统有限元模拟计算成本高昂。

Method: 使用张量参数降阶模型，结合维度缩减和近似求解技术，在特定降阶基上使用解析梯度进行非线性约束优化。

Result: 成功应用于软体游泳机器人形状优化，能够处理内外非线性流体动力，实现快速准确计算。

Conclusion: 该方法不仅降低了计算复杂度，还为软体机器人中复杂非线性系统的优化开辟了新途径，推动更高效的设计和控制。

Abstract: The efficient optimization of actuated soft structures, particularly under
complex nonlinear forces, remains a critical challenge in advancing robotics.
Simulations of nonlinear structures, such as soft-bodied robots modeled using
the finite element method (FEM), often demand substantial computational
resources, especially during optimization. To address this challenge, we
propose a novel optimization algorithm based on a tensorial parametric reduced
order model (PROM). Our algorithm leverages dimensionality reduction and
solution approximation techniques to facilitate efficient solving of nonlinear
constrained optimization problems. The well-structured tensorial approach
enables the use of analytical gradients within a specifically chosen reduced
order basis (ROB), significantly enhancing computational efficiency. To
showcase the performance of our method, we apply it to optimizing soft robotic
swimmer shapes. These actuated soft robots experience hydrodynamic forces,
subjecting them to both internal and external nonlinear forces, which are
incorporated into our optimization process using a data-free ROB for fast and
accurate computations. This approach not only reduces computational complexity
but also unlocks new opportunities to optimize complex nonlinear systems in
soft robotics, paving the way for more efficient design and control.

</details>


### [28] [Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment](https://arxiv.org/abs/2511.01083)
*Zihan Wang,Jianwen Li,Li-Fan Wu,Nina Mahmoudian*

Main category: cs.RO

TL;DR: SPAR-H是一个用于无人机河流跟踪的人类在环学习方法，通过融合直接偏好优化和基于奖励的路径，在有限的人类干预下实现高效策略适应。


<details>
  <summary>Details</summary>
Motivation: 无人机在河流环境监测中面临模拟训练策略的分布偏移和安全风险，需要从有限的人类干预中实现高效适应。

Method: 提出SPAR-H方法，结合直接偏好优化和基于奖励的路径，训练即时奖励估计器，并使用信任域代理更新策略。

Result: 使用5次人类在环测试，SPAR-H在测试方法中获得最高的最终回合奖励和最低的初始条件方差，学习到的奖励模型与人类偏好一致。

Conclusion: 双重状态偏好为河流导航中的数据高效在线适应提供了实用路径。

Abstract: Rivers are critical corridors for environmental monitoring and disaster
response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven
policies can provide fast, low-cost coverage. However, deployment exposes
simulation-trained policies with distribution shift and safety risks and
requires efficient adaptation from limited human interventions. We study
human-in-the-loop (HITL) learning with a conservative overseer who vetoes
unsafe or inefficient actions and provides statewise preferences by comparing
the agent's proposal with a corrective override. We introduce Statewise Hybrid
Preference Alignment for Robotics (SPAR-H), which fuses direct preference
optimization on policy logits with a reward-based pathway that trains an
immediate-reward estimator from the same preferences and updates the policy
using a trust-region surrogate. With five HITL rollouts collected from a fixed
novice policy, SPAR-H achieves the highest final episodic reward and the lowest
variance across initial conditions among tested methods. The learned reward
model aligns with human-preferred actions and elevates nearby non-intervened
choices, supporting stable propagation of improvements. We benchmark SPAR-H
against imitation learning (IL), direct preference variants, and evaluative
reinforcement learning (RL) in the HITL setting, and demonstrate real-world
feasibility of continual preference alignment for UAV river following. Overall,
dual statewise preferences empirically provide a practical route to
data-efficient online adaptation in riverine navigation.

</details>


### [29] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: SLAP方法通过结合任务与运动规划(TAMP)和模型无关强化学习(RL)，自动发现新的抽象动作选项，从而解决长时程决策问题。


<details>
  <summary>Details</summary>
Motivation: 传统TAMP方法依赖人工定义的抽象动作选项，限制了智能体只能执行人类工程师已知的行为。需要一种能自动发现新选项的方法来提升决策能力。

Method: 利用现有TAMP选项，在抽象规划图中使用模型无关强化学习来学习捷径，自动发现新的抽象动作选项。

Result: 在四个模拟机器人环境中，SLAP显著缩短了规划长度(减少50%以上)，提高了任务成功率，并发现了动态物理即兴动作(如拍打、摆动、擦拭)。

Conclusion: SLAP方法能够自动发现超越人工设计的新行为选项，在长时程决策任务中优于纯规划和分层强化学习方法。

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [30] [An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs](https://arxiv.org/abs/2511.01165)
*Dong Heon Han,Mayank Mehta,Runze Zuo,Zachary Wanger,Daniel Bruder*

Main category: cs.RO

TL;DR: 提出一种增强的软机器人本体感知方法，仅使用现成传感器实现精确形状估计，通过IMU和弯曲传感器融合来减轻IMU漂移，在45分钟连续操作中实现16.96毫米均方根误差。


<details>
  <summary>Details</summary>
Motivation: 开发成本效益高且易于应用的软机器人形状估计方法，解决IMU传感器漂移问题，实现可靠的长期本体感知。

Method: 集成惯性测量单元和互补弯曲传感器，使用卡尔曼滤波器融合两种传感器的段端方向数据，采用分段恒定曲率模型从融合方向数据估计尖端位置并重建机器人变形。

Result: 在无负载、外力和被动障碍物交互条件下，45分钟连续操作中均方根误差为16.96毫米（总长度的2.91%），相比仅使用IMU的方法误差减少56%。

Conclusion: 该方法不仅实现了软机器人的长期本体感知，而且在各种条件下保持了高精度和鲁棒性。

Abstract: This study presents an enhanced proprioceptive method for accurate shape
estimation of soft robots using only off-the-shelf sensors, ensuring
cost-effectiveness and easy applicability. By integrating inertial measurement
units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling
reliable long-term proprioception. A Kalman filter fuses segment tip
orientations from both sensors in a mutually compensatory manner, improving
shape estimation over single-sensor methods. A piecewise constant curvature
model estimates the tip location from the fused orientation data and
reconstructs the robot's deformation. Experiments under no loading, external
forces, and passive obstacle interactions during 45 minutes of continuous
operation showed a root mean square error of 16.96 mm (2.91% of total length),
a 56% reduction compared to IMU-only benchmarks. These results demonstrate that
our approach not only enables long-duration proprioception in soft robots but
also maintains high accuracy and robustness across these diverse conditions.

</details>


### [31] [Scaling Cross-Embodiment World Models for Dexterous Manipulation](https://arxiv.org/abs/2511.01177)
*Zihao He,Bo Ai,Tongzhou Mu,Yulin Liu,Weikang Wan,Jiawei Fu,Yilun Du,Henrik I. Christensen,Hao Su*

Main category: cs.RO

TL;DR: 该论文提出了一种跨具身学习方法，通过将不同形态的机器人表示为3D粒子集，并定义粒子位移作为动作，构建了基于图的世界模型来捕捉环境动态，从而实现了在不同机器人形态间的策略迁移。


<details>
  <summary>Details</summary>
Motivation: 解决不同机器人形态间由于动作空间和运动学差异导致的数据共享和策略迁移困难问题，探索是否存在跨具身的动作迁移不变性。

Method: 将不同机器人形态表示为3D粒子集，定义粒子位移作为共享动作表示，训练基于图的世界模型来捕捉环境动态，并结合基于模型的规划方法。

Result: 实验表明：(1)增加训练形态数量能提高对未见形态的泛化能力；(2)同时使用模拟和真实数据训练优于单独使用；(3)学习到的模型能在不同自由度的机器人上实现有效控制。

Conclusion: 世界模型为跨具身灵巧操作提供了一个有前景的统一接口。

Abstract: Cross-embodiment learning seeks to build generalist robots that operate
across diverse morphologies, but differences in action spaces and kinematics
hinder data sharing and policy transfer. This raises a central question: Is
there any invariance that allows actions to transfer across embodiments? We
conjecture that environment dynamics are embodiment-invariant, and that world
models capturing these dynamics can provide a unified interface across
embodiments. To learn such a unified world model, the crucial step is to design
state and action representations that abstract away embodiment-specific details
while preserving control relevance. To this end, we represent different
embodiments (e.g., human hands and robot hands) as sets of 3D particles and
define actions as particle displacements, creating a shared representation for
heterogeneous data and control problems. A graph-based world model is then
trained on exploration data from diverse simulated robot hands and real human
hands, and integrated with model-based planning for deployment on novel
hardware. Experiments on rigid and deformable manipulation tasks reveal three
findings: (i) scaling to more training embodiments improves generalization to
unseen ones, (ii) co-training on both simulated and real data outperforms
training on either alone, and (iii) the learned models enable effective control
on robots with varied degrees of freedom. These results establish world models
as a promising interface for cross-embodiment dexterous manipulation.

</details>


### [32] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: 提出LiDAR-VGGT框架，通过两阶段融合将LiDAR惯性里程计与VGGT模型紧密耦合，实现大规模彩色点云重建，解决了VGGT在大环境中尺度缺失和LIVO对外参标定敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LIVO方法对外参标定高度敏感，而3D视觉基础模型VGGT在大规模环境中扩展性有限且缺乏度量尺度，需要克服这些限制来实现更好的大规模彩色点云重建。

Method: 采用两阶段粗到精融合管道：预融合模块通过鲁棒初始化细化估计VGGT位姿和粗尺度点云；后融合模块增强跨模态3D相似变换，使用基于边界框的正则化减少LiDAR与相机FOV不一致导致的尺度失真。

Result: 在多个数据集上的广泛实验表明，LiDAR-VGGT实现了密集、全局一致的彩色点云，优于VGGT方法和LIVO基线方法。

Conclusion: LiDAR-VGGT成功解决了VGGT的尺度缺失和LIVO的标定敏感问题，实现了高质量的大规模彩色点云重建，并发布了开源的颜色点云评估工具包。

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [33] [Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures](https://arxiv.org/abs/2511.01199)
*Max McCandless,Jonathan Hamid,Sammy Elmariah,Nathaniel Langer,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 本文提出了一种可操纵球囊式心脏镜，通过单一输入（球囊充气压力）独立控制球囊直径和弯曲角度，实现心脏内可视化与工具输送，特别适用于主动脉瓣叶切开等经导管手术。


<details>
  <summary>Details</summary>
Motivation: 为了从开胸手术转向更安全的经导管手术，需要改进成像技术和机器人解决方案，以简化、精确地导航工具。传统成像方式（如荧光透视和超声）存在局限性，可通过心脏内直接光学可视化来克服。

Method: 设计了一种可操纵球囊式心脏镜，球囊可收缩通过血管系统，在心脏内充气进行可视化，并通过集成工作通道输送工具。通过精心设计球囊壁厚度，使用单一输入（充气压力）独立控制球囊直径（对应视野直径）和弯曲角度（实现工作通道精确定位）。

Result: 该球囊技术可调整以适应各种心内任务，展示了用于主动脉瓣叶切开的特定球囊设计，并实现了基于图像的弯曲角度闭环控制，在工具插入和取出期间保持稳定方向控制。

Conclusion: 可操纵球囊式心脏镜提供了一种新颖的解决方案，通过单一输入控制多个输出参数，实现了心脏内精确可视化与工具操作，为经导管手术提供了更安全有效的替代方案。

Abstract: To move away from open-heart surgery towards safer transcatheter procedures,
there is a growing need for improved imaging techniques and robotic solutions
to enable simple, accurate tool navigation. Common imaging modalities, such as
fluoroscopy and ultrasound, have limitations that can be overcome using
cardioscopy, i.e., direct optical visualization inside the beating heart. We
present a cardioscope designed as a steerable balloon. As a balloon, it can be
collapsed to pass through the vasculature and subsequently inflated inside the
heart for visualization and tool delivery through an integrated working
channel. Through careful design of balloon wall thickness, a single input,
balloon inflation pressure, is used to independently control two outputs,
balloon diameter (corresponding to field of view diameter) and balloon bending
angle (enabling precise working channel positioning). This balloon technology
can be tuned to produce cardioscopes designed for a range of intracardiac
tasks. To illustrate this approach, a balloon design is presented for the
specific task of aortic leaflet laceration. Image-based closed-loop control of
bending angle is also demonstrated as a means of enabling stable orientation
control during tool insertion and removal.

</details>


### [34] [Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference](https://arxiv.org/abs/2511.01219)
*Muhua Zhang,Lei Ma,Ying Wu,Kai Shen,Deqing Huang,Henry Leung*

Main category: cs.RO

TL;DR: 提出一个被动2D全局重定位框架，解决机器人绑架问题，通过单次LiDAR扫描和占据栅格地图高效可靠地估计全局位姿，使用多假设方案平衡完整性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人绑架问题(KRP)，即在已知地图中重新定位机器人而无需先验位姿估计，这对于SLAM初始化和定位丢失恢复至关重要，旨在提升移动机器人的长期自主性。

Method: 采用多假设方案，使用RRT在可通行约束下生成稀疏均匀的位置假设，通过SMAD指标初步排序假设并实现早期终止，提出TAM指标进行可靠的方向选择和最终位姿评估。

Result: 在资源受限的移动机器人上的真实世界实验表明，该框架在全局重定位成功率和计算效率方面均优于现有方法。

Conclusion: 提出的被动2D全局重定位框架能够高效可靠地解决机器人绑架问题，通过多假设方案和优化的指标设计，在非全景LiDAR扫描和资源受限条件下表现出色。

Abstract: This paper addresses the Kidnapped Robot Problem (KRP), a core localization
challenge of relocalizing a robot in a known map without prior pose estimate
when localization loss or at SLAM initialization. For this purpose, a passive
2-D global relocalization framework is proposed. It estimates the global pose
efficiently and reliably from a single LiDAR scan and an occupancy grid map
while the robot remains stationary, thereby enhancing the long-term autonomy of
mobile robots. The proposed framework casts global relocalization as a
non-convex problem and solves it via the multi-hypothesis scheme with batched
multi-stage inference and early termination, balancing completeness and
efficiency. The Rapidly-exploring Random Tree (RRT), under traversability
constraints, asymptotically covers the reachable space to generate sparse,
uniformly distributed feasible positional hypotheses, fundamentally reducing
the sampling space. The hypotheses are preliminarily ordered by the proposed
Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that
facilitates the early termination by prioritizing high-likelihood candidates.
The SMAD computation is optimized for non-panoramic scans. And the
Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for
reliable orientation selection at hypothesized positions and accurate final
pose evaluation to mitigate degradation in conventional likelihood-field
metrics under translational uncertainty induced by sparse hypotheses, as well
as non-panoramic LiDAR scan and environmental changes. Real-world experiments
on a resource-constrained mobile robot with non-panoramic LiDAR scan
demonstrate that the proposed framework outperforms existing methods in both
global relocalization success rate and computational efficiency.

</details>


### [35] [Embodiment Transfer Learning for Vision-Language-Action Models](https://arxiv.org/abs/2511.01224)
*Chengmeng Li,Yaxin Peng*

Main category: cs.RO

TL;DR: 提出了ET-VLA框架，通过合成持续预训练和具身思维图技术，实现预训练视觉语言动作模型向多机器人系统的有效迁移，在双手机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视觉语言动作模型在多机器人协作方面存在困难，需要高效且成本低廉的跨具身迁移学习方法。

Method: ET-VLA框架包含合成持续预训练（使用合成数据预热模型）和具身思维图技术（将子任务建模为节点以区分不同具身的功能角色）。

Result: 在三个不同双手机器人具身上验证，在六个真实世界任务中比OpenVLA性能提升超过53.2%。

Conclusion: ET-VLA为多机器人协作提供了一种高效的预训练模型迁移方法，显著提升了多具身任务的性能表现。

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
learning, enabling training on large-scale, cross-embodiment data and
fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs
struggle with multi-robot collaboration. We introduce embodiment transfer
learning, denoted as ET-VLA, a novel framework for efficient and effective
transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic
Continued Pretraining (SCP), which uses synthetically generated data to warm up
the model for the new embodiment, bypassing the need for real human
demonstrations and reducing data collection costs. SCP enables the model to
learn correct actions and precise action token numbers. Following SCP, the
model is fine-tuned on target embodiment data. To further enhance the model
performance on multi-embodiment, we present the Embodied Graph-of-Thought
technique, a novel approach that formulates each sub-task as a node, that
allows the VLA model to distinguish the functionalities and roles of each
embodiment during task execution. Our work considers bimanual robots, a simple
version of multi-robot to verify our approaches. We validate the effectiveness
of our method on both simulation benchmarks and real robots covering three
different bimanual embodiments. In particular, our proposed ET-VLA \space can
outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all
codes to support the community in advancing VLA models for robot learning.

</details>


### [36] [High-Precision Surgical Robotic System for Intraocular Procedures](https://arxiv.org/abs/2511.01232)
*Yu-Ting Lai,Jacob Rosen,Yasamin Foroutani,Ji Ma,Wen-Cheng Wu,Jean-Pierre Hubschman,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 开发了一种新型机器人系统，用于提高眼科手术中的工具尖端精度、跟踪性能和工具交换机制，在OCT引导下实现了高精度的白内障晶状体提取手术。


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统在眼科手术中精度、自由度和工具交换机制方面存在不足，需要开发更精确、更灵活的机器人系统来改善手术效果。

Method: 设计制造了新型机器人系统，通过机器人校准和精确坐标配准，使用光学相干断层扫描系统评估工具尖端精度和远程运动中心机制。

Result: 工具尖端定位精度达到0.053±0.031毫米，成功在OCT引导下完成了基于深度学习术前解剖建模和实时监督的自动化白内障晶状体提取手术。

Conclusion: 该机器人系统显著提高了眼科手术的精度和自动化水平，为复杂眼科手术提供了可靠的技术支持。

Abstract: Despite the extensive demonstration of robotic systems for both cataract and
vitreoretinal procedures, existing technologies or mechanisms still possess
insufficient accuracy, precision, and degrees of freedom for instrument
manipulation or potentially automated tool exchange during surgical procedures.
A new robotic system that focuses on improving tooltip accuracy, tracking
performance, and smooth instrument exchange mechanism is therefore designed and
manufactured. Its tooltip accuracy, precision, and mechanical capability of
maintaining small incision through remote center of motion were externally
evaluated using an optical coherence tomography (OCT) system. Through robot
calibration and precise coordinate registration, the accuracy of tooltip
positioning was measured to be 0.053$\pm$0.031 mm, and the overall performance
was demonstrated on an OCT-guided automated cataract lens extraction procedure
with deep learning-based pre-operative anatomical modeling and real-time
supervision.

</details>


### [37] [Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments](https://arxiv.org/abs/2511.01236)
*Junwen Zhang,Changyue Liu,Pengqi Fu,Xiang Guo,Ye Shi,Xudong Liang,Zhijian Wang,Hanzhi Ma*

Main category: cs.RO

TL;DR: SATPlanner是一个基于大语言模型的球面张拉整体机器人路径规划器，通过语义推理在未知环境中实现高效可靠的路径规划，显著减少搜索空间并保持路径质量。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划器将环境视为几何网格，在复杂场景中容易失败且缺乏语义理解。球面张拉整体机器人在未知环境中的路径规划需要平衡探索效率和规划鲁棒性。

Method: 提出SATPlanner，利用LLM进行高层次环境理解。核心是自适应观测窗口机制，根据环境复杂度动态调整感知范围：在开阔空间缩小窗口快速穿越，在复杂障碍配置时扩大窗口进行推理。

Result: 在1000次仿真试验中达到100%成功率，搜索空间比A*算法减少37.2%，同时保持接近最优的路径长度。在物理原型机器人上验证了可行性。

Conclusion: 将路径规划重构为语义推理任务，SATPlanner通过语义环境理解实现了高效可靠的规划，搜索空间仅随路径长度线性增长(O(L))，为混合软硬机器人提供了有效的路径规划解决方案。

Abstract: Endowed with inherent dynamical properties that grant them remarkable
ruggedness and adaptability, spherical tensegrity robots stand as prototypical
examples of hybrid softrigid designs and excellent mobile platforms. However,
path planning for these robots in unknown environments presents a significant
challenge, requiring a delicate balance between efficient exploration and
robust planning. Traditional path planners, which treat the environment as a
geometric grid, often suffer from redundant searches and are prone to failure
in complex scenarios due to their lack of semantic understanding. To overcome
these limitations, we reframe path planning in unknown environments as a
semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots
(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages
high-level environmental comprehension to generate efficient and reliable
planning strategies.At the core of SATPlanner is an Adaptive Observation Window
mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This
mechanism dynamically adjusts the perceptual field of the agent: it narrows for
rapid traversal of open spaces and expands to reason about complex obstacle
configurations. This allows the agent to construct a semantic belief of the
environment, enabling the search space to grow only linearly with the path
length (O(L)) while maintaining path quality. We extensively evaluate
SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,
outperforming other real-time planning algorithms. Critically, SATPlanner
reduces the search space by 37.2% compared to the A* algorithm while achieving
comparable, near-optimal path lengths. Finally, the practical feasibility of
SATPlanner is validated on a physical spherical tensegrity robot prototype.

</details>


### [38] [Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control](https://arxiv.org/abs/2511.01256)
*Yasamin Foroutani,Yasamin Mousavi-Motlagh,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 提出一种迭代学习控制策略，用于机器人手术中工具的精确旋转插入，相比直线插入提高了穿透效果和安全性


<details>
  <summary>Details</summary>
Motivation: 机器人工具路径的精确控制面临系统不对准、未建模动态和驱动误差等挑战，特别是在视网膜下注射等精密手术中

Method: 使用4自由度机器人操纵器，通过前向运动学校准提高精度，然后基于OCT体积扫描的误差测量进行迭代学习控制，逐步调整关节指令

Result: 在离体猪眼视网膜下注射实验中，优化轨迹相比直线插入在组织穿透和视网膜下注射方面获得更高成功率

Conclusion: ILC方法有效克服了对准挑战，为其他需要受控插入的高精度机器人任务提供了潜在应用

Abstract: Achieving precise control of robotic tool paths is often challenged by
inherent system misalignments, unmodeled dynamics, and actuation inaccuracies.
This work introduces an Iterative Learning Control (ILC) strategy to enable
precise rotational insertion of a tool during robotic surgery, improving
penetration efficacy and safety compared to straight insertion tested in
subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,
where misalignment of the fourth joint complicates the simple application of
needle rotation, motivating an ILC approach that iteratively adjusts joint
commands based on positional feedback. The process begins with calibrating the
forward kinematics for the chosen surgical tool to achieve higher accuracy,
followed by successive ILC iterations guided by Optical Coherence Tomography
(OCT) volume scans to measure the error and refine control inputs. Experimental
results, tested on subretinal injection tasks on ex vivo pig eyes, show that
the optimized trajectory resulted in higher success rates in tissue penetration
and subretinal injection compared to straight insertion, demonstrating the
effectiveness of ILC in overcoming misalignment challenges. This approach
offers potential applications for other high precision robot tasks requiring
controlled insertions as well.

</details>


### [39] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: 提出了一种将折纸结构与针织面料结合的新方法，通过编程针法和材料图案来制造可穿戴软机器人，实现了结构可重构性和舒适性的平衡。


<details>
  <summary>Details</summary>
Motivation: 软机器人在可穿戴设备中具有舒适安全的优势，但如何同时保持结构完整性和顺应性是一个重大挑战。

Method: 结合折纸结构的优势与针织面料的材料可编程性，通过选择性加入热熔纱线在柔性折痕周围创建刚性面板，编程针法和材料图案将折纸图案转化为针织设计。

Result: 成功复制了复杂的折纸镶嵌图案（Miura-ori、Yoshimura、Kresling），并开发出能够运动的可穿戴针织万花筒循环机器人，实验量化了折叠力矩。

Conclusion: 针织折纸结合了结构可重构性、材料可编程性和制造可扩展性，是下一代可穿戴机器人的有前景平台。

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [40] [Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation](https://arxiv.org/abs/2511.01276)
*Yiyao Ma,Kai Chen,Kexin Zheng,Qi Dou*

Main category: cs.RO

TL;DR: 提出基于条件扩散模型的灵巧抓取生成框架，通过将高质量抓取从形状模板转移到同类新物体，解决了抓取稳定性和任务适应性的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有分析方法能保证抓取稳定性但效率低且缺乏任务适应性，生成方法效率高但泛化能力差。需要一种能兼顾抓取质量、生成效率和泛化性能的方法。

Method: 使用条件扩散模型，将抓取转移问题重新定义为物体接触图生成，引入双映射机制处理复杂形状变化，并联合转移接触图、部件图和方向图三个地图，最后通过鲁棒抓取恢复机制优化抓取配置。

Result: 实验证明该方法在多种任务中有效平衡了抓取质量、生成效率和泛化性能，表现出优越性。

Conclusion: 提出的基于扩散模型的转移框架成功解决了灵巧抓取生成中的关键挑战，为机器人抓取提供了高效且适应性强的解决方案。

Abstract: Dexterous grasp generation is a fundamental challenge in robotics, requiring
both grasp stability and adaptability across diverse objects and tasks.
Analytical methods ensure stable grasps but are inefficient and lack task
adaptability, while generative approaches improve efficiency and task
integration but generalize poorly to unseen objects and tasks due to data
limitations. In this paper, we propose a transfer-based framework for dexterous
grasp generation, leveraging a conditional diffusion model to transfer
high-quality grasps from shape templates to novel objects within the same
category. Specifically, we reformulate the grasp transfer problem as the
generation of an object contact map, incorporating object shape similarity and
task specifications into the diffusion process. To handle complex shape
variations, we introduce a dual mapping mechanism, capturing intricate
geometric relationship between shape templates and novel objects. Beyond the
contact map, we derive two additional object-centric maps, the part map and
direction map, to encode finer contact details for more stable grasps. We then
develop a cascaded conditional diffusion model framework to jointly transfer
these three maps, ensuring their intra-consistency. Finally, we introduce a
robust grasp recovery mechanism, identifying reliable contact points and
optimizing grasp configurations efficiently. Extensive experiments demonstrate
the superiority of our proposed method. Our approach effectively balances grasp
quality, generation efficiency, and generalization performance across various
tasks. Project homepage: https://cmtdiffusion.github.io/

</details>


### [41] [A High-Speed Capable Spherical Robot](https://arxiv.org/abs/2511.01288)
*Bixuan Zhang,Fengqi Zhang,Haojie Chen,You Wang,Jie Hao,Zhiyuan Luo,Guang Li*

Main category: cs.RO

TL;DR: 设计了一种新型球形机器人结构，能够实现高达10米/秒的高速运动，通过引入与次级摆对齐的动量轮，显著提升了运动性能和地形适应性。


<details>
  <summary>Details</summary>
Motivation: 基于单摆驱动球形机器人的局限性，旨在开发能够实现稳定高速运动的新型球形机器人结构，解决原有结构无法达到高速稳定运动的问题。

Method: 在单摆驱动球形机器人基础上，引入与次级摆轴线对齐的动量轮，构建新型球形机器人结构，并通过解耦控制实现稳定高速运动。

Result: 物理样机实验证明，该新型球形机器人能够通过简单的解耦控制实现稳定高速运动，同时显著提升了越障性能和地形鲁棒性。

Conclusion: 所设计的新型球形机器人结构成功实现了高速运动能力，不仅提高了运动速度，还大幅改善了越障性能和地形适应性，为球形机器人的应用拓展了新的可能性。

Abstract: This paper designs a new spherical robot structure capable of supporting
high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven
spherical robot, the design incorporates a momentum wheel with an axis aligned
with the secondary pendulum, creating a novel spherical robot structure.
Practical experiments with the physical prototype have demonstrated that this
new spherical robot can achieve stable high-speed motion through simple
decoupled control, which was unattainable with the original structure. The
spherical robot designed for high-speed motion not only increases speed but
also significantly enhances obstacle-crossing performance and terrain
robustness.

</details>


### [42] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: Kinematify是一个从RGB图像或文本提示自动合成关节物体的框架，解决了高自由度物体运动学拓扑推断和静态几何关节参数估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 关节物体建模对于机器人操作和自身建模至关重要，但现有方法依赖运动序列或手动标注数据集，难以扩展。

Method: 结合MCTS搜索进行结构推断和几何驱动优化进行关节推理，生成物理一致且功能有效的描述。

Result: 在合成和真实环境中的多样化输入上评估，在配准和运动学拓扑准确性方面优于先前工作。

Conclusion: Kinematify能够从任意RGB图像或文本提示自动合成关节物体，为复杂系统的建模提供了可扩展的解决方案。

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [43] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: RobustVLA是一种轻量级在线强化学习后训练方法，通过雅可比正则化和平滑正则化增强视觉-语言-动作模型在环境扰动下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在分布外部署时对观测噪声、传感器误差和执行扰动等环境不确定性缺乏鲁棒性，而现有的RL后训练方法主要关注奖励最大化，忽视了环境不确定性的鲁棒性。

Method: 提出RobustVLA方法，通过系统鲁棒性分析识别出两个关键正则化：雅可比正则化（减轻观测噪声敏感性）和平滑正则化（在动作扰动下稳定策略）。

Result: 在多样化机器人环境中的广泛实验表明，RobustVLA在鲁棒性和可靠性方面显著优于现有最先进方法。

Conclusion: 原则性的鲁棒性感知RL后训练是提高VLA模型可靠性和鲁棒性的关键步骤。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [44] [Embodied Cognition Augmented End2End Autonomous Driving](https://arxiv.org/abs/2511.01334)
*Ling Niu,Xiaoji Zheng,Han Wang,Chen Zheng,Ziyuan Yang,Bokui Chen,Jiangtao Gong*

Main category: cs.RO

TL;DR: 提出E³AD新范式，通过视觉特征提取网络与EEG大模型的对比学习来学习人类驾驶认知，以增强端到端自动驾驶规划性能


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法依赖标签监督训练的视觉特征提取网络，这种有限监督框架限制了驾驶模型的通用性和适用性

Method: 收集认知数据集进行对比学习，研究用人类驾驶认知增强端到端规划的方法和机制，在公开自动驾驶数据集上使用流行驾驶模型作为基线

Result: E³AD范式显著提升了基线模型的端到端规划性能，消融研究验证了驾驶认知的贡献和对比学习过程的有效性

Conclusion: 这是首个将人类驾驶认知整合到端到端自动驾驶规划的工作，为未来脑启发自动驾驶系统提供了有价值的见解

Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a
new paradigm. However, popular end-to-end approaches typically rely on visual
feature extraction networks trained under label supervision. This limited
supervision framework restricts the generality and applicability of driving
models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which
advocates for comparative learning between visual feature extraction networks
and the general EEG large model, in order to learn latent human driving
cognition for enhancing end-to-end planning. In this work, we collected a
cognitive dataset for the mentioned contrastive learning process. Subsequently,
we investigated the methods and potential mechanisms for enhancing end-to-end
planning with human driving cognition, using popular driving models as
baselines on publicly available autonomous driving datasets. Both open-loop and
closed-loop tests are conducted for a comprehensive evaluation of planning
performance. Experimental results demonstrate that the $E^{3}AD$ paradigm
significantly enhances the end-to-end planning performance of baseline models.
Ablation studies further validate the contribution of driving cognition and the
effectiveness of comparative learning process. To the best of our knowledge,
this is the first work to integrate human driving cognition for improving
end-to-end autonomous driving planning. It represents an initial attempt to
incorporate embodied cognitive data into end-to-end autonomous driving,
providing valuable insights for future brain-inspired autonomous driving
systems. Our code will be made available at Github

</details>


### [45] [Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers](https://arxiv.org/abs/2511.01346)
*Shun Yoshida,Qingchuan Song,Bastian E. Rapp,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 开发了首个能够自主闭合和重新张开的人造捕蝇草系统，使用热响应形状记忆材料实现双向运动。


<details>
  <summary>Details</summary>
Motivation: 虽然捕蝇草的快速闭合运动已被广泛研究，但将其闭合和重新张开两种运动都转化为自主的植物启发软机器尚未实现。

Method: 使用新型热响应UV固化形状记忆材料构建软机器人系统，通过形状记忆聚合物构建双曲陷阱叶片，在38°C时闭合，使用形状记忆弹性体条作为拮抗驱动器在45°C时重新张开。

Result: 成功制造出与真实捕蝇草大小相当的热响应人造捕蝇草，能够在自然温度范围内触发闭合和重新张开运动。

Conclusion: 这是首个展示热响应闭合和重新张开功能的人造捕蝇草系统，实现了对温度升高的程序化顺序运动，标志着自主双向移动软机器发展的新进展。

Abstract: Despite their often perceived static and slow nature, some plants can move
faster than the blink of an eye. The rapid snap closure motion of the Venus
flytrap (Dionaea muscipula) has long captivated the interest of researchers and
engineers alike, serving as a model for plant-inspired soft machines and
robots. The translation of the fast snapping closure has inspired the
development of various artificial Venus flytrap (AVF) systems. However,
translating both the closing and reopening motion of D. muscipula into an
autonomous plant inspired soft machine has yet to be achieved. In this study,
we present an AVF that autonomously closes and reopens, utilizing novel
thermo-responsive UV-curable shape memory materials for soft robotic systems.
The life-sized thermo-responsive AVF exhibits closing and reopening motions
triggered in a naturally occurring temperature range. The doubly curved trap
lobes, built from shape memory polymers, close at 38{\deg}C, while reopening
initiates around 45{\deg}C, employing shape memory elastomer strips as
antagonistic actuators to facilitate lobe reopening. This work represents the
first demonstration of thermo-responsive closing and reopening in an AVF with
programmed sequential motion in response to increasing temperature. This
approach marks the next step toward autonomously bidirectional moving soft
machines/robots.

</details>


### [46] [Design and development of an electronics-free earthworm robot](https://arxiv.org/abs/2511.01347)
*Riddhi Das,Joscha Teichmann,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 提出了一种无电子元件的仿蚯蚓气动软体机器人，采用改进的气动逻辑门设计实现蠕动运动，无需外部电子控制单元。


<details>
  <summary>Details</summary>
Motivation: 现有仿蚯蚓机器人主要依赖气动驱动，但需要笨重、高功耗的电子控制单元，限制了实际应用。本研究旨在开发无电子控制的气动软体机器人系统。

Method: 将预配置的气动逻辑门单元与波纹管执行器集成，构建即插即用式模块化系统，通过改进的PLG设计实现蠕动运动控制。

Result: 改进的PLG控制系统有效产生蠕动波传播，实现自主运动且偏差最小，系统复杂度降低的同时保持了高效的驱动性能。

Conclusion: 该研究为开发无电子元件的蠕动软体机器人提供了概念验证，在危险环境中具有应用潜力，未来将优化设计并探索使用机载压缩空气源的无线操作。

Abstract: Soft robotic systems have gained widespread attention due to their inherent
flexibility, adaptability, and safety, making them well-suited for varied
applications. Among bioinspired designs, earthworm locomotion has been
extensively studied for its efficient peristaltic motion, enabling movement in
confined and unstructured environments. Existing earthworm-inspired robots
primarily utilize pneumatic actuation due to its high force-to-weight ratio and
ease of implementation. However, these systems often rely on bulky,
power-intensive electronic control units, limiting their practicality. In this
work, we present an electronics-free, earthworm-inspired pneumatic robot
utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating
preconfigured PLG units with bellow actuators, we achieved a plug-and-play
style modular system capable of peristaltic locomotion without external
electronic components. The proposed design reduces system complexity while
maintaining efficient actuation. We characterize the bellow actuators under
different operating conditions and evaluate the robots locomotion performance.
Our findings demonstrate that the modified PLG-based control system effectively
generates peristaltic wave propagation, achieving autonomous motion with
minimal deviation. This study serves as a proof of concept for the development
of electronics-free, peristaltic soft robots. The proposed system has potential
for applications in hazardous environments, where untethered, adaptable
locomotion is critical. Future work will focus on further optimizing the robot
design and exploring untethered operation using onboard compressed air sources.

</details>


### [47] [Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator](https://arxiv.org/abs/2511.01350)
*Maartje H. M. Wermelink,Renate Sachse,Sebastian Kruppert,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 该研究分析了捕蝇草的快速闭合机制，并基于其几何特征和双稳态特性设计了两种3D打印的双稳态执行器模型。


<details>
  <summary>Details</summary>
Motivation: 深入理解捕蝇草运动力学，并将其原理应用于人工双稳态叶片执行器的设计，开发可作为软快速抓取器的人工捕蝇草。

Method: 识别捕蝇草叶片的几何特征（尺寸比例、厚度梯度），并将这些特征转移到两种3D打印的双稳态执行器模型中：一种模拟捕蝇草叶片几何形状，另一种使用CAD设计的叶片模型。

Result: 两种模型都表现出凹-凸双稳态特性并能快速闭合，这是开发人工捕蝇草的第一步。

Conclusion: 成功将捕蝇草的双稳态机制应用于人工执行器设计，为开发能够模拟生物模型机械行为的人工捕蝇草奠定了基础。

Abstract: The Venus flytrap (Dionaea muscipula) does not only serve as the textbook
model for a carnivorous plant, but also has long intrigued both botanists and
engineers with its rapidly closing leaf trap. The trap closure is triggered by
two consecutive touches of a potential prey, after which the lobes rapidly
switch from their concave open-state to their convex close-state and catch the
prey within 100-500 ms after being triggered. This transformation from concave
to convex is initiated by changes in turgor pressure and the release of stored
elastic energy from prestresses in the concave state, which accelerate this
movement, leading to inversion of the lobes bi-axial curvature. Possessing two
low-energy states, the leaves can be characterized as bistable systems. With
our research, we seek to deepen the understanding of Venus flytrap motion
mechanics and apply its principles to the design of an artificial bistable lobe
actuator. We identified geometrical characteristics, such as dimensional ratios
and the thickness gradient in the lobe, and transferred these to two 3D-printed
bistable actuator models. One actuator parallels the simulated geometry of a
Venus flytrap leaf, the other is a lobe model designed with CAD. Both models
display concave-convex bi-stability and snap close. These demonstrators are the
first step in the development of an artificial Venus flytrap that mimics the
mechanical behavior of the biological model and can be used as a soft fast
gripper.

</details>


### [48] [Lateral Velocity Model for Vehicle Parking Applications](https://arxiv.org/abs/2511.01369)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 提出了一种改进的侧向速度模型，用于解决自动泊车中侧向速度估计的挑战，该模型仅需两个参数，更适合消费级车辆应用。


<details>
  <summary>Details</summary>
Motivation: 自动泊车需要精确的定位，但消费级车辆缺乏专用传感器来测量侧向速度。现有方法依赖零滑移假设，但在低速驾驶时该假设不成立，导致估计不准确。

Method: 分析真实泊车场景数据，识别零滑移假设的系统性偏差，提出一个仅需两个参数的改进侧向速度模型来更好地捕捉车辆在泊车时的侧向动力学。

Result: 新模型提高了侧向速度估计的准确性，同时保持了简单性，适合集成到消费级应用中。

Conclusion: 提出的侧向速度模型在泊车场景中比传统零滑移模型更准确，且参数少，易于在消费级车辆中实现。

Abstract: Automated parking requires accurate localization for quick and precise
maneuvering in tight spaces. While the longitudinal velocity can be measured
using wheel encoders, the estimation of the lateral velocity remains a key
challenge due to the absence of dedicated sensors in consumer-grade vehicles.
Existing approaches often rely on simplified vehicle models, such as the
zero-slip model, which assumes no lateral velocity at the rear axle. It is well
established that this assumption does not hold during low-speed driving and
researchers thus introduce additional heuristics to account for differences. In
this work, we analyze real-world data from parking scenarios and identify a
systematic deviation from the zero-slip assumption. We provide explanations for
the observed effects and then propose a lateral velocity model that better
captures the lateral dynamics of the vehicle during parking. The model improves
estimation accuracy, while relying on only two parameters, making it
well-suited for integration into consumer-grade applications.

</details>


### [49] [CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels](https://arxiv.org/abs/2511.01379)
*Kun Hu,Menggang Li,Zhiwen Jin,Chaoquan Tang,Eryi Hu,Gongbo Zhou*

Main category: cs.RO

TL;DR: 提出CM-LIUW-Odometry多模态SLAM框架，融合LiDAR-IMU-UWB-轮式里程计，用于GPS缺失的复杂地下煤矿环境定位与建图


<details>
  <summary>Details</summary>
Motivation: 解决地下煤矿环境中GPS不可用、地形不平导致轮式里程计精度下降、长隧道特征稀少降低LiDAR效果等挑战

Method: 基于IESKF的多模态融合框架：LiDAR-IMU里程计与UWB绝对定位紧耦合；轮式里程计紧耦合集成，辅以非完整约束和车辆杠杆臂补偿；自适应运动模式切换机制

Result: 在真实地下煤矿场景中验证了方法的优越精度和鲁棒性，优于现有最先进方法

Conclusion: 提出的多模态SLAM框架在地下煤矿环境中表现出色，代码已开源

Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and
GPS-denied underground coal mine environments presents significant challenges.
Sensors must contend with abnormal operating conditions: GPS unavailability
impedes scene reconstruction and absolute geographic referencing, uneven or
slippery terrain degrades wheel odometer accuracy, and long, feature-poor
tunnels reduce LiDAR effectiveness. To address these issues, we propose
CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM
framework based on the Iterated Error-State Kalman Filter (IESKF). First,
LiDAR-inertial odometry is tightly fused with UWB absolute positioning
constraints to align the SLAM system with a global coordinate. Next, wheel
odometer is integrated through tight coupling, enhanced by nonholonomic
constraints (NHC) and vehicle lever arm compensation, to address performance
degradation in areas beyond UWB measurement range. Finally, an adaptive motion
mode switching mechanism dynamically adjusts the robot's motion mode based on
UWB measurement range and environmental degradation levels. Experimental
results validate that our method achieves superior accuracy and robustness in
real-world underground coal mine scenarios, outperforming state-of-the-art
approaches. We open source our code of this work on Github to benefit the
robotics community.

</details>


### [50] [CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation](https://arxiv.org/abs/2511.01383)
*Landson Guo,Andres M. Diaz Aguilar,William Talbot,Turcan Tuna,Marco Hutter,Cesar Cadena*

Main category: cs.RO

TL;DR: 提出名为CaRLi-V的多传感器融合管道，通过结合RADAR、LiDAR和相机数据，实现密集点云的3D速度估计，特别适用于机器人与非刚性动态物体（如人类）的交互。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，准确的3D点速度估计对机器人的路径规划、碰撞避免和物体操作至关重要，特别是在与非刚性动态代理（如人类）交互时。

Method: 使用原始RADAR测量创建速度立方体表示径向速度，结合光流估计切向速度，通过LiDAR获取点云距离测量，采用闭式解融合这些信息。

Result: CaRLi-V作为开源ROS2包，在自定义数据集上测试显示相对于真实值具有较低的速度误差指标。

Conclusion: 该方法能够为密集点云生成3D速度估计，为机器人应用提供有效的点速度估计能力。

Abstract: Accurate point-wise velocity estimation in 3D is crucial for robot
interaction with non-rigid, dynamic agents, such as humans, enabling robust
performance in path planning, collision avoidance, and object manipulation in
dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,
and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.
This pipeline leverages raw RADAR measurements to create a novel RADAR
representation, the velocity cube, which densely represents radial velocities
within the RADAR's field-of-view. By combining the velocity cube for radial
velocity extraction, optical flow for tangential velocity estimation, and LiDAR
for point-wise range measurements through a closed-form solution, our approach
can produce 3D velocity estimates for a dense array of points. Developed as an
open-source ROS2 package, CaRLi-V has been field-tested against a custom
dataset and proven to produce low velocity error metrics relative to ground
truth, enabling point-wise velocity estimation for robotic applications.

</details>


### [51] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: FoldPath是一种基于神经场的端到端对象中心运动生成方法，通过连续函数学习机器人运动，无需后处理步骤，在仅有70个专家样本的情况下实现了工业环境下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前对象中心运动生成技术要么基于临时启发式方法，要么依赖敏感后处理步骤的学习流程，需要更稳健的算法来生成复杂3D几何上的扩展、对象感知轨迹。

Method: 提出FoldPath方法，使用神经场将机器人运动建模为连续函数，隐式编码平滑输出路径，避免了离散路径点预测和拼接排序的后处理步骤。

Result: FoldPath在预测性能上优于最近提出的学习方法，在真实工业环境中仅用70个专家样本就展现出泛化能力，通过仿真环境验证和新的长时程机器人路径评估指标证明了其有效性。

Conclusion: FoldPath通过连续函数学习范式消除了脆弱的后处理步骤，推进了对象中心运动生成任务向实际应用的成熟发展。

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [52] [Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots](https://arxiv.org/abs/2511.01437)
*Elian Neppel,Shamistan Karimov,Ashutosh Mishra,Gustavo Hernan Diaz Huenupan,Hazal Gozbasi,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: MoonBot平台是一个模块化空间机器人系统，采用分布式异构组件架构，通过ROS2和Zenoh实现数据导向通信，支持动态重构和去中心化控制，已在实地部署中验证。


<details>
  <summary>Details</summary>
Motivation: 解决模块化机器人系统在集成和维护方面的重大挑战，将模块化概念从物理重构扩展到软件、通信和编排层面，构建可扩展且鲁棒的系统架构。

Method: 采用组件化设计、基于ROS2和Zenoh的数据导向通信模型，以及能够管理复杂多模块组件的部署编排器，实现分布式异构模块化。

Result: 开发了开源Motion Stack软件，经过数月的实地部署验证，支持机器人自组装、机器人间协作和远程操作，显著降低了集成和维护开销。

Conclusion: 虽然以太空应用为目标，但提出了可推广的机器人系统设计模式，适用于需要跨时间、硬件、团队和操作环境扩展的系统。

Abstract: This paper presents the software architecture and deployment strategy behind
the MoonBot platform: a modular space robotic system composed of heterogeneous
components distributed across multiple computers, networks and ultimately
celestial bodies. We introduce a principled approach to distributed,
heterogeneous modularity, extending modular robotics beyond physical
reconfiguration to software, communication and orchestration. We detail the
architecture of our system that integrates component-based design, a
data-oriented communication model using ROS2 and Zenoh, and a deployment
orchestrator capable of managing complex multi-module assemblies. These
abstractions enable dynamic reconfiguration, decentralized control, and
seamless collaboration between numerous operators and modules. At the heart of
this system lies our open-source Motion Stack software, validated by months of
field deployment with self-assembling robots, inter-robot cooperation, and
remote operation. Our architecture tackles the significant hurdles of modular
robotics by significantly reducing integration and maintenance overhead, while
remaining scalable and robust. Although tested with space in mind, we propose
generalizable patterns for designing robotic systems that must scale across
time, hardware, teams and operational environments.

</details>


### [53] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: AERMANI-VLM是一个将预训练视觉语言模型适配到空中机械臂控制的框架，通过分离高层推理和低层控制，无需任务特定微调即可实现安全可靠的操作。


<details>
  <summary>Details</summary>
Motivation: 直接部署视觉语言模型驱动的策略到空中机械臂存在不安全、不可靠的问题，因为生成的动作往往不一致、容易产生幻觉，且飞行动力学不可行。

Method: 将自然语言指令、任务上下文和安全约束编码到结构化提示中，引导模型生成逐步推理轨迹，然后从预定义的安全技能库中选择离散动作，实现符号推理与物理动作的解耦。

Result: 在仿真和硬件实验中验证了框架在多样化多步骤拾取放置任务中的有效性，展示了对未见指令、物体和环境的强泛化能力。

Conclusion: 通过分离推理和控制，AERMANI-VLM能够减轻幻觉命令并防止不安全行为，实现鲁棒的任务完成。

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [54] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: MO-SeGMan是一个多目标顺序引导操作规划器，用于解决高度受限的重排问题，通过最小化重新规划和机器人移动距离，在复杂场景中生成可行的运动规划。


<details>
  <summary>Details</summary>
Motivation: 解决高度受限、非单调的重排规划问题，需要在保持关键依赖结构的同时优化多个目标。

Method: 采用选择性引导前向搜索(SGFS)和自适应子目标选择的精炼方法，通过惰性评估生成对象放置序列。

Result: 在9个基准重排任务中，MO-SeGMan在所有情况下都生成了可行的运动规划，相比基线方法获得了更快的求解时间和更优的解质量。

Conclusion: MO-SeGMan框架在处理复杂重排规划问题时表现出鲁棒性和可扩展性。

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [55] [Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues](https://arxiv.org/abs/2511.01493)
*Wei Huang,Jiaxin Li,Zang Wan,Huijun Di,Wei Liang,Zhu Yang*

Main category: cs.RO

TL;DR: 提出GlocDiff扩散策略，结合楼层平面图的全局路径规划和RGB观测的局部深度特征，解决室内导航中视觉与空间信息融合的挑战，并通过噪声扰动增强对位姿估计误差的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决室内导航中两个关键挑战：1）第一人称RGB观测与楼层平面图之间的模态差距阻碍视觉和空间信息的融合；2）在未见环境中由于RGB输入与楼层平面图缺乏显式几何对齐，导致精确定位困难。

Method: 提出GlocDiff扩散策略，集成楼层平面图的全局路径规划和RGB观测的局部深度感知特征。楼层平面图提供显式全局引导，深度特征提供隐式几何线索。在训练中引入噪声扰动增强对位姿估计误差的鲁棒性，在推理时结合相对稳定的VO模块。

Result: 在FloNa基准测试上的广泛实验证明GlocDiff在实现优越导航性能方面的效率和有效性，真实世界部署的成功也突显了其广泛实际应用的潜力。

Conclusion: GlocDiff通过结合全局路径规划和局部深度特征，有效解决了室内导航中的模态差距和定位挑战，并通过噪声扰动和VO模块的结合显著提升了导航性能，具有广泛的实际应用前景。

Abstract: Guiding an agent to a specific target in indoor environments based solely on
RGB inputs and a floor plan is a promising yet challenging problem. Although
existing methods have made significant progress, two challenges remain
unresolved. First, the modality gap between egocentric RGB observations and the
floor plan hinders the integration of visual and spatial information for both
local obstacle avoidance and global planning. Second, accurate localization is
critical for navigation performance, but remains challenging at deployment in
unseen environments due to the lack of explicit geometric alignment between RGB
inputs and floor plans. We propose a novel diffusion-based policy, denoted as
GlocDiff, which integrates global path planning from the floor plan with local
depth-aware features derived from RGB observations. The floor plan offers
explicit global guidance, while the depth features provide implicit geometric
cues, collectively enabling precise prediction of optimal navigation directions
and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation
during training to enhance robustness against pose estimation errors, and we
find that combining this with a relatively stable VO module during inference
results in significantly improved navigation performance. Extensive experiments
on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in
achieving superior navigation performance, and the success of real-world
deployments also highlights its potential for widespread practical
applications.

</details>


### [56] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 提出Phy-Tac方法，通过物理条件触觉技术实现力最优稳定抓取，结合姿态选择、触觉预测和力调节，使机器人像人类一样用最小必要力抓取物体。


<details>
  <summary>Details</summary>
Motivation: 人类能自然使用最小必要力稳定抓取物体，而机器人通常依赖刚性、过度挤压的控制方式，需要缩小这一差距。

Method: 1) 基于物理的姿态选择器识别最优力分布的可行接触区域；2) 物理条件潜在扩散模型预测目标触觉印记；3) 潜在空间LQR控制器驱动夹爪以最小驱动力达到目标触觉印记。

Result: Phy-LDM在触觉预测精度上表现优异，Phy-Tac在抓取稳定性和力效率方面优于固定力和GraspNet基线方法。

Conclusion: 实验证明该方法实现了力高效和自适应操作，缩小了机器人与人类抓取之间的差距。

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>


### [57] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: MARS是一个基于多模态大语言模型的多智能体机器人系统，专为智能家居机器人设计，用于为残障人士提供辅助服务。系统包含视觉感知、风险评估、规划和评估四个智能体，通过多模态感知和分层决策实现自适应、风险感知和个性化的室内环境辅助。


<details>
  <summary>Details</summary>
Motivation: 现有系统在风险感知规划、用户个性化和将语言计划转化为可执行技能方面存在困难，特别是在杂乱的家庭环境中。需要开发能够提供智能辅助的系统来支持残障人士。

Method: 集成四个智能体：视觉感知智能体从环境图像中提取语义和空间特征，风险评估智能体识别和优先处理危险，规划智能体生成可执行动作序列，评估智能体进行迭代优化。结合多模态感知和分层多智能体决策。

Result: 在多个数据集上的实验表明，该系统在风险感知规划和协调多智能体执行方面优于最先进的多模态模型。

Conclusion: 该方法展示了协作AI在实际辅助场景中的潜力，并为在现实环境中部署基于MLLM的多智能体系统提供了可推广的方法论。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [58] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: 提出了一种统一的视觉-语言-动作模型，通过联合扩散过程同步优化图像生成和动作预测，实现理解、生成和行动的协同作用


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型要么依赖外部专家进行模态统一，要么将图像生成和动作预测作为独立过程处理，限制了任务间的直接协同效益

Method: 提出统一扩散VLA和联合离散去噪扩散过程(JD3P)，通过单一去噪轨迹整合多模态，构建统一标记空间和混合注意力机制，采用两阶段训练流程和推理优化技术

Result: 在CALVIN、LIBERO和SimplerEnv基准测试中达到最先进性能，推理速度比自回归方法快4倍

Conclusion: 该方法通过同步去噪过程实现了理解、生成和行动的内在协同，在性能和效率方面均表现出色

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


### [59] [Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping](https://arxiv.org/abs/2511.01770)
*Liudi Yang,Yang Bai,Yuhao Wang,Ibrahim Alsarraj,Gitta Kutyniok,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 提出了一种轻量级的驱动空间学习框架，利用流匹配模型从确定性演示中学习分布控制表示，实现软体机器人抓取，仅需30次演示即可在完整工作空间达到97.5%的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统刚性机器人手依赖复杂的基于模型和重反馈控制器来处理不确定接触，而软体机器人具有机械智能特性，其欠驱动结构和全身被动柔性能自然适应不确定接触。

Method: 使用流匹配模型从确定性演示中学习驱动空间的分布控制表示，无需密集传感或重控制循环，仅需少量演示数据。

Result: 仅用30次演示（不到可达工作空间的8%）就实现了97.5%的抓取成功率，能泛化到±33%的抓取物体尺寸变化，且在20%-200%的执行时间缩放下保持稳定性能。

Conclusion: 驱动空间学习通过利用软体机器人的被动冗余自由度和柔韧性，将身体力学转化为功能控制智能，显著减轻了中央控制器在这种不确定任务中的负担。

Abstract: Robotic grasping under uncertainty remains a fundamental challenge due to its
uncertain and contact-rich nature. Traditional rigid robotic hands, with
limited degrees of freedom and compliance, rely on complex model-based and
heavy feedback controllers to manage such interactions. Soft robots, by
contrast, exhibit embodied mechanical intelligence: their underactuated
structures and passive flexibility of their whole body, naturally accommodate
uncertain contacts and enable adaptive behaviors. To harness this capability,
we propose a lightweight actuation-space learning framework that infers
distributional control representations for whole-body soft robotic grasping,
directly from deterministic demonstrations using a flow matching model
(Rectified Flow),without requiring dense sensing or heavy control loops. Using
only 30 demonstrations (less than 8% of the reachable workspace), the learned
policy achieves a 97.5% grasp success rate across the whole workspace,
generalizes to grasped-object size variations of +-33%, and maintains stable
performance when the robot's dynamic response is directly adjusted by scaling
the execution time from 20% to 200%. These results demonstrate that
actuation-space learning, by leveraging its passive redundant DOFs and
flexibility, converts the body's mechanics into functional control intelligence
and substantially reduces the burden on central controllers for this
uncertain-rich task.

</details>


### [60] [MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll](https://arxiv.org/abs/2511.01774)
*Alexander Schperberg,Yusuke Tanaka,Stefano Di Cairano,Dennis Hong*

Main category: cs.RO

TL;DR: MOBIUS是一个多模态双足城市侦察机器人，能够行走、爬行、攀爬和滚动，通过混合控制架构和高级规划实现不同运动模式间的平滑转换


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂城市环境中执行多种运动模式（行走、爬行、攀爬、滚动）的机器人，扩展机器人的交互能力、工作空间和穿越能力

Method: 采用四肢体设计（两个6自由度手臂和两个4自由度腿部），结合强化学习运动控制、基于模型的预测和导纳控制，使用MIQCP规划器自主选择运动模式

Result: 硬件实验展示了稳健的步态转换、动态攀爬和通过夹持抓握实现的全身体重支撑

Conclusion: MOBIUS证明了形态学、高级规划和控制的紧密集成对于实现移动定位操作和抓取的重要性，显著扩展了机器人的交互能力

Abstract: This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot
(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features
four limbs--two 6-DoF arms with two-finger grippers for manipulation and
climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across
diverse terrains without reconfiguration. A hybrid control architecture
combines reinforcement learning-based locomotion with model-based predictive
and admittance control enhanced for safety by a Reference Governor toward
compliant contact interactions. A high-level MIQCP planner autonomously selects
locomotion modes to balance stability and energy efficiency. Hardware
experiments demonstrate robust gait transitions, dynamic climbing, and
full-body load support via pinch grasp. Overall, MOBIUS demonstrates the
importance of tight integration between morphology, high-level planning, and
control to enable mobile loco-manipulation and grasping, substantially
expanding its interaction capabilities, workspace, and traversability.

</details>


### [61] [GenDexHand: Generative Simulation for Dexterous Hands](https://arxiv.org/abs/2511.01791)
*Feng Chen,Zhuxiu Xu,Tianzhe Chu,Xunzhe Zhou,Li Sun,Zewen Wu,Shenghua Gao,Zhongyu Li,Yanchao Yang,Yi Ma*

Main category: cs.RO

TL;DR: GenDexHand是一个生成式仿真流水线，能够自主生成多样化的灵巧手操作任务和环境，通过VLM反馈的闭环精炼过程提高环境质量，并将任务分解为子任务以支持顺序强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中数据稀缺的根本瓶颈问题，现有基于LLM的方法在灵巧操作中迁移效果差，而灵巧操作由于自由度更高而更困难，大规模生成可行且可训练的灵巧手任务仍是一个开放挑战。

Method: 引入闭环精炼过程，基于视觉语言模型的反馈调整物体放置和尺度；将每个任务分解为子任务以支持顺序强化学习；提供生成式仿真流水线自动生成多样化任务。

Result: 显著提高了生成环境的平均质量；减少了训练时间并提高了成功率；为灵巧手行为的大规模训练提供了可行路径。

Conclusion: 通过基于仿真的合成数据生成解决方案，为具身智能中多样化灵巧手行为的可扩展训练提供了可行路径。

Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence.
Existing approaches use large language models (LLMs) to automate gripper-based
simulation generation, but they transfer poorly to dexterous manipulation,
which demands more specialized environment design. Meanwhile, dexterous
manipulation tasks are inherently more difficult due to their higher degrees of
freedom. Massively generating feasible and trainable dexterous hand tasks
remains an open challenge. To this end, we present GenDexHand, a generative
simulation pipeline that autonomously produces diverse robotic tasks and
environments for dexterous manipulation. GenDexHand introduces a closed-loop
refinement process that adjusts object placements and scales based on
vision-language model (VLM) feedback, substantially improving the average
quality of generated environments. Each task is further decomposed into
sub-tasks to enable sequential reinforcement learning, reducing training time
and increasing success rates. Our work provides a viable path toward scalable
training of diverse dexterous hand behaviors in embodied intelligence by
offering a simulation-based solution to synthetic data generation. Our website:
https://winniechen2002.github.io/GenDexHand/.

</details>


### [62] [Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator](https://arxiv.org/abs/2511.01797)
*Javier Ballesteros-Jerez,Jesus Martínez-Gómez,Ismael García-Varea,Luis Orozco-Barbosa,Manuel Castillo-Cara*

Main category: cs.RO

TL;DR: 提出了一种使用大规模MIMO系统CSI数据的混合神经网络模型，用于移动机器人位置推断。该方法将CNN与MLP结合形成HyNN，通过TINTO工具将CSI数据转换为合成图像来估计2D机器人位置。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在复杂室内环境中的精确定位和导航问题，利用大规模MIMO系统的CSI数据提供更准确的位置估计。

Method: 使用混合神经网络(HyNN)模型，结合CNN和MLP，将CSI数据通过TINTO工具转换为合成图像进行位置估计，并与机器人仿真器和ROS系统集成。

Result: 实现了移动机器人在复杂室内环境中的精确定位和导航，展示了HyNN模型在位置估计方面的潜力。

Conclusion: 该方法提供了一个可推广的通用流程，适用于不同场景和数据集，为移动机器人室内定位提供了有效的解决方案。

Abstract: We present a hybrid neural network model for inferring the position of mobile
robots using Channel State Information (CSI) data from a Massive MIMO system.
By leveraging an existing CSI dataset, our approach integrates a Convolutional
Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural
Network (HyNN) that estimates 2D robot positions. CSI readings are converted
into synthetic images using the TINTO tool. The localisation solution is
integrated with a robotics simulator, and the Robot Operating System (ROS),
which facilitates its evaluation through heterogeneous test cases, and the
adoption of state estimators like Kalman filters. Our contributions illustrate
the potential of our HyNN model in achieving precise indoor localisation and
navigation for mobile robots in complex environments. The study follows, and
proposes, a generalisable procedure applicable beyond the specific use case
studied, making it adaptable to different scenarios and datasets.

</details>
