<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Vi-TacMan: Articulated Object Manipulation via Vision and Touch](https://arxiv.org/abs/2510.06339)
*Leiyao Cui,Zihang Zhao,Sirui Xie,Wenhuan Zhang,Zhi Han,Yixin Zhu*

Main category: cs.RO

TL;DR: Vi-TacMan是一个结合视觉和触觉的机器人操作系统，使用视觉提供全局抓取建议和粗略方向，然后通过触觉控制器进行精确执行，无需显式运动学模型。


<details>
  <summary>Details</summary>
Motivation: 自主操作铰接物体是机器人在人类环境中的基本挑战。视觉方法可以推断隐藏运动学但不精确，触觉方法通过接触反馈实现鲁棒控制但需要准确初始化。两者具有天然互补性。

Method: 使用视觉提出抓取和粗略方向，作为触觉控制器的种子。通过表面法线作为几何先验，使用冯·米塞斯-费希尔分布建模方向。触觉控制器通过实时接触调节来细化粗略的视觉估计。

Result: 在超过50,000个模拟和多样化真实世界物体上的测试证实了跨类别的鲁棒泛化能力，相比基线方法取得了显著提升（所有p<0.0001）。

Conclusion: 粗视觉线索结合触觉反馈足以实现可靠操作，为无结构环境中的自主系统提供了可扩展的范式。

Abstract: Autonomous manipulation of articulated objects remains a fundamental
challenge for robots in human environments. Vision-based methods can infer
hidden kinematics but can yield imprecise estimates on unfamiliar objects.
Tactile approaches achieve robust control through contact feedback but require
accurate initialization. This suggests a natural synergy: vision for global
guidance, touch for local precision. Yet no framework systematically exploits
this complementarity for generalized articulated manipulation. Here we present
Vi-TacMan, which uses vision to propose grasps and coarse directions that seed
a tactile controller for precise execution. By incorporating surface normals as
geometric priors and modeling directions via von Mises-Fisher distributions,
our approach achieves significant gains over baselines (all p<0.0001).
Critically, manipulation succeeds without explicit kinematic models -- the
tactile controller refines coarse visual estimates through real-time contact
regulation. Tests on more than 50,000 simulated and diverse real-world objects
confirm robust cross-category generalization. This work establishes that coarse
visual cues suffice for reliable manipulation when coupled with tactile
feedback, offering a scalable paradigm for autonomous systems in unstructured
environments.

</details>


### [2] [A Formal gatekeeper Framework for Safe Dual Control with Active Exploration](https://arxiv.org/abs/2510.06351)
*Kaleb Ben Naveed,Devansh R. Agrawal,Dimitra Panagou*

Main category: cs.RO

TL;DR: 提出一个集成鲁棒规划与主动探索的框架，仅在探索能提供可验证改进且不损害安全性的情况下进行探索。


<details>
  <summary>Details</summary>
Motivation: 解决模型不确定性下的安全轨迹规划问题，传统鲁棒规划过于保守且忽略不确定性减少，而现有方法缺乏对探索时机和安全性的正式考虑。

Method: 利用gatekeeper架构进行安全验证，扩展其生成既安全又信息丰富的轨迹，减少不确定性和任务成本，或保持在用户定义的预算内。

Result: 通过四旋翼飞行器参数不确定性的在线双控制仿真案例研究验证了方法的有效性。

Conclusion: 该框架能够在保证安全的前提下，智能地选择时机进行探索，有效平衡任务执行与不确定性减少的需求。

Abstract: Planning safe trajectories under model uncertainty is a fundamental
challenge. Robust planning ensures safety by considering worst-case
realizations, yet ignores uncertainty reduction and leads to overly
conservative behavior. Actively reducing uncertainty on-the-fly during a
nominal mission defines the dual control problem. Most approaches address this
by adding a weighted exploration term to the cost, tuned to trade off the
nominal objective and uncertainty reduction, but without formal consideration
of when exploration is beneficial. Moreover, safety is enforced in some methods
but not in others. We propose a framework that integrates robust planning with
active exploration under formal guarantees as follows: The key innovation and
contribution is that exploration is pursued only when it provides a verifiable
improvement without compromising safety. To achieve this, we utilize our
earlier work on gatekeeper as an architecture for safety verification, and
extend it so that it generates both safe and informative trajectories that
reduce uncertainty and the cost of the mission, or keep it within a
user-defined budget. The methodology is evaluated via simulation case studies
on the online dual control of a quadrotor under parametric uncertainty.

</details>


### [3] [Constrained Natural Language Action Planning for Resilient Embodied Systems](https://arxiv.org/abs/2510.06357)
*Grayson Byrd,Corban Rivera,Bethany Kemp,Meghan Booker,Aurora Schmidt,Celso M de Melo,Lalithkumar Seenivasan,Mathias Unberath*

Main category: cs.RO

TL;DR: 提出了一种结合大语言模型和符号规划的新机器人规划方法，通过符号规划监督提高LLM规划器的可靠性和可重复性，在模拟和真实环境中都取得了接近完美的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在机器人任务规划中的幻觉问题和缺乏透明度的问题，同时克服纯符号规划方法难以扩展到复杂现实世界任务的局限性。

Method: 将LLM规划器与符号规划监督相结合，用符号方法提供硬约束定义，保留LLM的推理能力，同时提高系统的可靠性和透明度。

Result: 在ALFWorld规划基准测试中达到99%的成功率；在真实四足机器人部署中，相比纯LLM规划器（50%）和纯符号规划器（30%），实现了100%的任务成功率。

Conclusion: 该方法有效增强了基于LLM的机器人规划器的可靠性、可重复性和透明度，同时保留了其在复杂现实环境中灵活性和泛化能力的关键优势。

Abstract: Replicating human-level intelligence in the execution of embodied tasks
remains challenging due to the unconstrained nature of real-world environments.
Novel use of large language models (LLMs) for task planning seeks to address
the previously intractable state/action space of complex planning tasks, but
hallucinations limit their reliability, and thus, viability beyond a research
context. Additionally, the prompt engineering required to achieve adequate
system performance lacks transparency, and thus, repeatability. In contrast to
LLM planning, symbolic planning methods offer strong reliability and
repeatability guarantees, but struggle to scale to the complexity and ambiguity
of real-world tasks. We introduce a new robotic planning method that augments
LLM planners with symbolic planning oversight to improve reliability and
repeatability, and provide a transparent approach to defining hard constraints
with considerably stronger clarity than traditional prompt engineering.
Importantly, these augmentations preserve the reasoning capabilities of LLMs
and retain impressive generalization in open-world environments. We demonstrate
our approach in simulated and real-world environments. On the ALFWorld planning
benchmark, our approach outperforms current state-of-the-art methods, achieving
a near-perfect 99% success rate. Deployment of our method to a real-world
quadruped robot resulted in 100% task success compared to 50% and 30% for pure
LLM and symbolic planners across embodied pick and place tasks. Our approach
presents an effective strategy to enhance the reliability, repeatability and
transparency of LLM-based robot planners while retaining their key strengths:
flexibility and generalizability to complex real-world environments. We hope
that this work will contribute to the broad goal of building resilient embodied
intelligent systems.

</details>


### [4] [Active Next-Best-View Optimization for Risk-Averse Path Planning](https://arxiv.org/abs/2510.06481)
*Amirhossein Mollaei Khass,Guangyi Liu,Vivek Pandey,Wen Jiang,Boshu Lei,Kostas Daniilidis,Nader Motee*

Main category: cs.RO

TL;DR: 提出一个统一框架，通过构建基于平均风险价值的尾部敏感风险地图来细化参考路径，同时在SE(3)姿态流形上优化最佳视点选择，耦合风险规避路径优化与主动感知。


<details>
  <summary>Details</summary>
Motivation: 在不确定环境中实现安全导航需要将风险规避与主动感知相结合，现有方法在这两方面的集成不够充分。

Method: 使用在线更新的3D高斯泼溅辐射场构建尾部敏感风险地图，在SE(3)姿态流形上通过黎曼梯度下降优化最佳视点选择，最大化期望信息增益。

Result: 通过广泛的计算研究证明了所提框架的有效性，能够生成局部安全可行的轨迹。

Conclusion: 该框架通过引入可扩展的梯度分解支持复杂环境中的高效在线更新，推进了风险规避路径优化与最佳视点规划的耦合。

Abstract: Safe navigation in uncertain environments requires planning methods that
integrate risk aversion with active perception. In this work, we present a
unified framework that refines a coarse reference path by constructing
tail-sensitive risk maps from Average Value-at-Risk statistics on an
online-updated 3D Gaussian-splat Radiance Field. These maps enable the
generation of locally safe and feasible trajectories. In parallel, we formulate
Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose
manifold, where Riemannian gradient descent maximizes an expected information
gain objective to reduce uncertainty most critical for imminent motion. Our
approach advances the state-of-the-art by coupling risk-averse path refinement
with NBV planning, while introducing scalable gradient decompositions that
support efficient online updates in complex environments. We demonstrate the
effectiveness of the proposed framework through extensive computational
studies.

</details>


### [5] [What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?](https://arxiv.org/abs/2510.06492)
*Matthew Kim,Kensuke Nakamura,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 该论文研究了基于RGB观测的潜在空间安全控制方法的局限性，提出了一个互信息度量来识别观测中缺失的安全相关特征，并开发了一种多模态监督训练策略来改进潜在状态表示。


<details>
  <summary>Details</summary>
Motivation: 现有潜在空间安全控制方法假设安全关键特征可以在学习的潜在状态中观察到，但实际中RGB观测可能无法捕捉某些安全相关特征（如温度），导致短视的安全行为。

Method: 引入基于互信息的度量来识别观测中缺失的安全相关特征；提出多模态监督训练策略，在训练时使用额外传感器输入塑造潜在状态，但部署时仅需RGB观测。

Result: 在仿真和Franka Research 3机械臂硬件实验中验证了该方法，成功防止蜡锅过热，证明了方法的有效性。

Conclusion: 多模态监督训练可以改善潜在状态表示，使基于RGB观测的安全控制器能够更好地预测和防止温度相关的故障。

Abstract: Safe control techniques, such as Hamilton-Jacobi reachability, provide
principled methods for synthesizing safety-preserving robot policies but
typically assume hand-designed state spaces and full observability. Recent work
has relaxed these assumptions via latent-space safe control, where state
representations and dynamics are learned jointly through world models that
reconstruct future high-dimensional observations (e.g., RGB images) from
current observations and actions. This enables safety constraints that are
difficult to specify analytically (e.g., spilling) to be framed as
classification problems in latent space, allowing controllers to operate
directly from raw observations. However, these methods assume that
safety-critical features are observable in the learned latent state. We ask:
when are latent state spaces sufficient for safe control? To study this, we
examine temperature-based failures, comparable to overheating in cooking or
manufacturing tasks, and find that RGB-only observations can produce myopic
safety behaviors, e.g., avoiding seeing failure states rather than preventing
failure itself. To predict such behaviors, we introduce a mutual
information-based measure that identifies when observations fail to capture
safety-relevant features. Finally, we propose a multimodal-supervised training
strategy that shapes the latent state with additional sensory inputs during
training, but requires no extra modalities at deployment, and validate our
approach in simulation and on hardware with a Franka Research 3 manipulator
preventing a pot of wax from overheating.

</details>


### [6] [Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots](https://arxiv.org/abs/2510.06518)
*Malakhi Hopkins,Varun Murali,Vijay Kumar,Camillo J Taylor*

Main category: cs.RO

TL;DR: 提出了一种用于小型无人机实时检测和映射透明障碍物的轻量级框架，融合ToF相机和超声波传感器数据，使用定制2D卷积模型在嵌入式CPU上实时运行。


<details>
  <summary>Details</summary>
Motivation: 透明障碍物对传统感知系统构成挑战，缺乏可识别特征导致深度传感器失效，现有方法通常依赖大型昂贵传感器或高计算量算法，不适合低SWaP无人机。

Method: 融合ToF相机和超声波传感器数据，采用定制轻量级2D卷积模型检测镜面反射并将深度传播到深度图的空区域，使透明障碍物可见。

Result: 在受控和真实环境实验中验证了系统有效性，无人机能够成功映射包含玻璃的室内环境，整个管道在嵌入式处理器上仅使用少量CPU核心实时运行。

Conclusion: 这是首个在低SWaP四旋翼无人机上仅使用CPU实现实时、机载透明障碍物映射的系统。

Abstract: Autonomous aerial robots are increasingly being deployed in real-world
scenarios, where transparent obstacles present significant challenges to
reliable navigation and mapping. These materials pose a unique problem for
traditional perception systems because they lack discernible features and can
cause conventional depth sensors to fail, leading to inaccurate maps and
potential collisions. To ensure safe navigation, robots must be able to
accurately detect and map these transparent obstacles. Existing methods often
rely on large, expensive sensors or algorithms that impose high computational
burdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots.
In this work, we propose a novel and computationally efficient framework for
detecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our
method fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor
with a custom, lightweight 2D convolution model. This specialized approach
accurately detects specular reflections and propagates their depth into
corresponding empty regions of the depth map, effectively rendering transparent
obstacles visible. The entire pipeline operates in real-time, utilizing only a
small fraction of a CPU core on an embedded processor. We validate our system
through a series of experiments in both controlled and real-world environments,
demonstrating the utility of our method through experiments where the robot
maps indoor environments containing glass. Our work is, to our knowledge, the
first of its kind to demonstrate a real-time, onboard transparent obstacle
mapping system on a low-SWaP quadrotor using only the CPU.

</details>


### [7] [RAISE: A self-driving laboratory for interfacial property formulation discovery](https://arxiv.org/abs/2510.06546)
*Mohammad Nazeri,Sheldon Mei,Jeffrey Watchorn,Alex Zhang,Erin Ng,Tao Wen,Abhijoy Mandal,Kevin Golovin,Alan Aspuru-Guzik,Frank Gu*

Main category: cs.RO

TL;DR: RAISE是一个闭环自主实验室系统，通过贝叶斯优化自动连接液体配方优化与表面润湿性评估，实现高通量接触角测量和配方优化。


<details>
  <summary>Details</summary>
Motivation: 表面润湿性是生物医学设备、涂层和纺织品的关键设计参数，但液体配方对接触角测量影响很大，需要自动化系统来优化配方与润湿性之间的关系。

Method: RAISE系统包含实验协调器，能够混合液体成分、转移液滴、自动采集图像，并集成贝叶斯优化客户端进行迭代配方探索。

Result: 系统测量速率约为每分钟1个接触角测量，能够探索表面活性剂润湿性，通过多目标贝叶斯优化找到符合特定应用目标的最佳配方。

Conclusion: RAISE展示了在闭环系统中自主连接液体配方与接触角测量的能力，使用多目标贝叶斯优化高效识别符合研究人员定义标准的最佳配方。

Abstract: Surface wettability is a critical design parameter for biomedical devices,
coatings, and textiles. Contact angle measurements quantify liquid-surface
interactions, which depend strongly on liquid formulation. Herein, we present
the Robotic Autonomous Imaging Surface Evaluator (RAISE), a closed-loop,
self-driving laboratory that is capable of linking liquid formulation
optimization with surface wettability assessment. RAISE comprises a full
experimental orchestrator with the ability of mixing liquid ingredients to
create varying formulation cocktails, transferring droplets of prepared
formulations to a high-throughput stage, and using a pick-and-place camera tool
for automated droplet image capture. The system also includes an automated
image processing pipeline to measure contact angles. This closed loop
experiment orchestrator is integrated with a Bayesian Optimization (BO) client,
which enables iterative exploration of new formulations based on previous
contact angle measurements to meet user-defined objectives. The system operates
in a high-throughput manner and can achieve a measurement rate of approximately
1 contact angle measurement per minute. Here we demonstrate RAISE can be used
to explore surfactant wettability and how surfactant combinations create
tunable formulations that compensate for purity-related variations.
Furthermore, multi-objective BO demonstrates how precise and optimal
formulations can be reached based on application-specific goals. The
optimization is guided by a desirability score, which prioritizes formulations
that are within target contact angle ranges, minimize surfactant usage and
reduce cost. This work demonstrates the capabilities of RAISE to autonomously
link liquid formulations to contact angle measurements in a closed-loop system,
using multi-objective BO to efficiently identify optimal formulations aligned
with researcher-defined criteria.

</details>


### [8] [Safe Obstacle-Free Guidance of Space Manipulators in Debris Removal Missions via Deep Reinforcement Learning](https://arxiv.org/abs/2510.06566)
*Vincent Lam,Robin Chhabra*

Main category: cs.RO

TL;DR: 开发基于TD3强化学习的无模型空间机械臂轨迹规划器，通过多批评器网络和课程学习实现安全可靠的太空碎片捕获


<details>
  <summary>Details</summary>
Motivation: 解决空间机械臂在捕获非合作目标时需要同时满足精确跟踪、避免自碰撞和防止意外接触的复杂挑战

Method: 使用TD3强化学习算法，结合局部控制策略进行奇异性避免和可操作性增强，采用基于课程的多批评器网络和优先经验回放

Result: 在Matlab/Simulink中模拟的七自由度KUKA LBR iiwa机械臂上验证了框架的有效性，能够生成安全自适应的碎片清除轨迹

Conclusion: 提出的框架能够为空间碎片清除任务提供安全可靠的轨迹规划解决方案

Abstract: The objective of this study is to develop a model-free workspace trajectory
planner for space manipulators using a Twin Delayed Deep Deterministic Policy
Gradient (TD3) agent to enable safe and reliable debris capture. A local
control strategy with singularity avoidance and manipulability enhancement is
employed to ensure stable execution. The manipulator must simultaneously track
a capture point on a non-cooperative target, avoid self-collisions, and prevent
unintended contact with the target. To address these challenges, we propose a
curriculum-based multi-critic network where one critic emphasizes accurate
tracking and the other enforces collision avoidance. A prioritized experience
replay buffer is also used to accelerate convergence and improve policy
robustness. The framework is evaluated on a simulated seven-degree-of-freedom
KUKA LBR iiwa mounted on a free-floating base in Matlab/Simulink, demonstrating
safe and adaptive trajectory generation for debris removal missions.

</details>


### [9] [Assist-As-Needed: Adaptive Multimodal Robotic Assistance for Medication Management in Dementia Care](https://arxiv.org/abs/2510.06633)
*Kruthika Gangaraju,Tanmayi Inaparthy,Jiaqi Yang,Yihao Zheng,Fengpei Yuan*

Main category: cs.RO

TL;DR: 提出了一种基于Pepper机器人的自适应多模态框架，用于为痴呆症患者提供动态调整的用药管理辅助，从简单提醒到全面指导的分级干预。


<details>
  <summary>Details</summary>
Motivation: 现有辅助技术无法适应痴呆症患者不断变化的需求，统一的辅助方式会削弱患者自主性、加速依赖并增加护理负担。需要基于职业治疗原则的适应性辅助框架。

Method: 使用Pepper机器人实现分层干预模型：从简单语言提醒，到语言+手势提示，再到结合物理导航和逐步指导的完整多模态引导。系统通过LLM驱动的交互策略和多模态感知实时评估任务状态。

Result: 在实验室环境中对健康成人和痴呆症护理利益相关者进行了初步研究，评估了系统的可用性、可理解性和适应性反馈机制的适当性。

Conclusion: 该工作贡献包括：基于职业治疗原则的自适应辅助框架、保护患者尊严的多模态机器人实现，以及对适应性机器人护理利益相关者看法的实证见解。

Abstract: People living with dementia (PLWDs) face progressively declining abilities in
medication management-from simple forgetfulness to complete task breakdown-yet
most assistive technologies fail to adapt to these changing needs. This
one-size-fits-all approach undermines autonomy, accelerates dependence, and
increases caregiver burden. Occupational therapy principles emphasize matching
assistance levels to individual capabilities: minimal reminders for those who
merely forget, spatial guidance for those who misplace items, and comprehensive
multimodal support for those requiring step-by-step instruction. However,
existing robotic systems lack this adaptive, graduated response framework
essential for maintaining PLWD independence. We present an adaptive multimodal
robotic framework using the Pepper robot that dynamically adjusts assistance
based on real-time assessment of user needs. Our system implements a
hierarchical intervention model progressing from (1) simple verbal reminders,
to (2) verbal + gestural cues, to (3) full multimodal guidance combining
physical navigation to medication locations with step-by-step verbal and
gestural instructions. Powered by LLM-driven interaction strategies and
multimodal sensing, the system continuously evaluates task states to provide
just-enough assistance-preserving autonomy while ensuring medication adherence.
We conducted a preliminary study with healthy adults and dementia care
stakeholders in a controlled lab setting, evaluating the system's usability,
comprehensibility, and appropriateness of adaptive feedback mechanisms. This
work contributes: (1) a theoretically grounded adaptive assistance framework
translating occupational therapy principles into HRI design, (2) a multimodal
robotic implementation that preserves PLWD dignity through graduated support,
and (3) empirical insights into stakeholder perceptions of adaptive robotic
care.

</details>


### [10] [RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training](https://arxiv.org/abs/2510.06710)
*Hongzhi Zang,Mingjie Wei,Si Xu,Yongji Wu,Zhen Guo,Yuanqing Wang,Hao Lin,Liangzhi Shi,Yuqing Xie,Zhexuan Xu,Zhihao Liu,Kang Chen,Wenhao Tang,Quanlu Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: RLinf-VLA是一个统一高效的强化学习训练框架，专门用于视觉-语言-动作(VLA)模型，解决了现有监督微调方法的泛化问题，并在多个模拟器和真实机器人上展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型大多使用监督微调(SFT)训练，在分布偏移下泛化能力不足。强化学习(RL)虽然能直接优化任务性能，但缺乏统一的平台进行公平系统比较。

Method: 开发了RLinf-VLA框架，采用灵活的资源配置设计，支持GPU并行模拟器，实现了混合细粒度流水线分配模式，支持多种VLA架构、RL算法和模拟器。

Result: 在130个LIBERO任务上达到98.11%成功率，在25个ManiSkill任务上达到97.66%成功率。在真实Franka机器人上，RL训练的策略比SFT训练的策略具有更强的泛化能力。

Conclusion: RLinf-VLA为具身智能研究提供了加速和标准化的基础框架，并总结了一套将RL应用于VLA训练的最佳实践。

Abstract: Recent progress in vision and language foundation models has significantly
advanced multimodal understanding, reasoning, and generation, inspiring a surge
of interest in extending such capabilities to embodied settings through
vision-language-action (VLA) models. Yet, most VLA models are still trained
with supervised fine-tuning (SFT), which struggles to generalize under
distribution shifts due to error accumulation. Reinforcement learning (RL)
offers a promising alternative by directly optimizing task performance through
interaction, but existing attempts remain fragmented and lack a unified
platform for fair and systematic comparison across model architectures and
algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and
efficient framework for scalable RL training of VLA models. The system adopts a
highly flexible resource allocation design that addresses the challenge of
integrating rendering, training, and inference in RL+VLA training. In
particular, for GPU-parallelized simulators, RLinf-VLA implements a novel
hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup
in training. Through a unified interface, RLinf-VLA seamlessly supports diverse
VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,
PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a
unified model achieves 98.11\% across 130 LIBERO tasks and 97.66\% across 25
ManiSkill tasks. Beyond empirical performance, our study distills a set of best
practices for applying RL to VLA training and sheds light on emerging patterns
in this integration. Furthermore, we present preliminary deployment on a
real-world Franka robot, where RL-trained policies exhibit stronger
generalization than those trained with SFT. We envision RLinf-VLA as a
foundation to accelerate and standardize research on embodied intelligence.

</details>


### [11] [SanDRA: Safe Large-Language-Model-Based Decision Making for Automated Vehicles Using Reachability Analysis](https://arxiv.org/abs/2510.06717)
*Yuanfei Lin,Sebastian Illing,Matthias Althoff*

Main category: cs.RO

TL;DR: SanDRA是一个基于大语言模型的安全自动驾驶决策框架，通过可达性分析确保决策安全，解决了大语言模型可能产生幻觉和缺乏车辆动力学集成的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动驾驶决策中应用广泛，但由于可能产生幻觉和缺乏车辆动力学集成，其决策安全性无法保证。

Method: 首先全面描述驾驶场景，提示大语言模型生成并排序可行驾驶动作；然后将这些动作转化为包含形式化交通规则的时间逻辑公式；最后通过可达性分析消除不安全动作。

Result: 在开环和闭环驾驶环境中验证了该方法，能够提供可证明的安全驾驶动作，并在可能的情况下确保合法合规，即使在高密度交通条件下也能有效工作。

Conclusion: SanDRA框架能够为自动驾驶提供安全可靠的决策支持，解决了大语言模型在安全关键应用中的局限性，所有代码和实验设置已公开。

Abstract: Large language models have been widely applied to knowledge-driven
decision-making for automated vehicles due to their strong generalization and
reasoning capabilities. However, the safety of the resulting decisions cannot
be ensured due to possible hallucinations and the lack of integrated vehicle
dynamics. To address this issue, we propose SanDRA, the first safe
large-language-model-based decision making framework for automated vehicles
using reachability analysis. Our approach starts with a comprehensive
description of the driving scenario to prompt large language models to generate
and rank feasible driving actions. These actions are translated into temporal
logic formulas that incorporate formalized traffic rules, and are subsequently
integrated into reachability analysis to eliminate unsafe actions. We validate
our approach in both open-loop and closed-loop driving environments using
off-the-shelf and finetuned large language models, showing that it can provide
provably safe and, where possible, legally compliant driving actions, even
under high-density traffic conditions. To ensure transparency and facilitate
future research, all code and experimental setups are publicly available at
github.com/CommonRoad/SanDRA.

</details>


### [12] [UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene](https://arxiv.org/abs/2510.06754)
*Christian Maurer,Snehal Jauhri,Sophie Lueth,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: UniFField是一个统一的不确定性感知神经特征场，结合了视觉、语义和几何特征，同时预测每个模态的不确定性，可零样本应用于新环境。


<details>
  <summary>Details</summary>
Motivation: 机器人需要全面理解3D场景并评估感知信息的可靠性，但现有方法存在场景特定性和缺乏不确定性建模的局限性。

Method: 使用基于体素的通用表示，增量整合RGB-D图像，同时更新不确定性估计，可零样本应用于新环境。

Result: 不确定性估计能准确描述场景重建和语义特征预测中的模型预测误差，并在移动机械臂的主动物体搜索任务中成功应用。

Conclusion: UniFField能够实现鲁棒决策，为机器人在非结构化复杂环境中的任务执行提供了可靠支持。

Abstract: Comprehensive visual, geometric, and semantic understanding of a 3D scene is
crucial for successful execution of robotic tasks, especially in unstructured
and complex environments. Additionally, to make robust decisions, it is
necessary for the robot to evaluate the reliability of perceived information.
While recent advances in 3D neural feature fields have enabled robots to
leverage features from pretrained foundation models for tasks such as
language-guided manipulation and navigation, existing methods suffer from two
critical limitations: (i) they are typically scene-specific, and (ii) they lack
the ability to model uncertainty in their predictions. We present UniFField, a
unified uncertainty-aware neural feature field that combines visual, semantic,
and geometric features in a single generalizable representation while also
predicting uncertainty in each modality. Our approach, which can be applied
zero shot to any new environment, incrementally integrates RGB-D images into
our voxel-based feature representation as the robot explores the scene,
simultaneously updating uncertainty estimation. We evaluate our uncertainty
estimations to accurately describe the model prediction errors in scene
reconstruction and semantic feature prediction. Furthermore, we successfully
leverage our feature predictions and their respective uncertainty for an active
object search task using a mobile manipulator robot, demonstrating the
capability for robust decision-making.

</details>


### [13] [Distributed 3D Source Seeking via SO(3) Geometric Control of Robot Swarms](https://arxiv.org/abs/2510.06836)
*Jesús Bautista,Héctor García de Marina*

Main category: cs.RO

TL;DR: 本文提出了一种在SO(3)李群上的几何控制框架，用于具有一阶姿态动力学和恒定平移速度的机器人进行3D源追踪。该方法避免了欧拉角奇异性和四元数歧义性，提供了独特的内在方向表示。


<details>
  <summary>Details</summary>
Motivation: 传统方向表示方法如欧拉角和四元数存在奇异性和歧义性问题，需要一种在SO(3)李群上的内在几何控制框架来避免这些问题，同时实现稳定的3D源追踪。

Method: 设计了比例前馈控制器，确保每个智能体与估计的朝向3D标量场源的上升方向指数对齐。控制器适应有界未知变化并保持良好形成的群体编队。

Result: 数值仿真证明了该方法的有效性，所有代码开源提供以确保可重现性。

Conclusion: 在SO(3)李群上的几何控制框架为3D源追踪提供了鲁棒且无奇异性的解决方案，能够有效处理姿态控制问题并保持群体编队稳定性。

Abstract: This paper presents a geometric control framework on the Lie group SO(3) for
3D source-seeking by robots with first-order attitude dynamics and constant
translational speed. By working directly on SO(3), the approach avoids
Euler-angle singularities and quaternion ambiguities, providing a unique,
intrinsic representation of orientation. We design a proportional feed-forward
controller that ensures exponential alignment of each agent to an estimated
ascending direction toward a 3D scalar field source. The controller adapts to
bounded unknown variations and preserves well-posed swarm formations. Numerical
simulations demonstrate the effectiveness of the method, with all code provided
open source for reproducibility.

</details>


### [14] [Tailoring materials into kirigami robots](https://arxiv.org/abs/2510.07027)
*Saravana Prashanth Murali Babu,Aida Parvaresh,Ahmad Rafsanjani*

Main category: cs.RO

TL;DR: 本文综述了剪纸技术在机器人领域的应用潜力，包括基于剪纸原理的驱动器、传感器、电池、控制器和结构，展示了其在抓取、移动和可穿戴设备等领域的适应性。


<details>
  <summary>Details</summary>
Motivation: 剪纸技术具有多功能、轻量和可适应的特点，有望革新机器人技术，通过优化切割图案为特定机器人应用定制组件。

Method: 通过优化剪纸图案设计驱动器、传感器、电池和控制器，利用剪纸结构的弯曲主导变形特性实现复杂运动、传感和能量存储功能。

Result: 剪纸驱动器可编程复杂运动，传感器实现导电性与柔顺性的结合，集成电池增强结构灵活性，控制机制模拟机械计算，在抓取、移动和可穿戴应用中表现出良好适应性。

Conclusion: 剪纸技术为机器人提供了有前景的解决方案，但在图案设计和制造工艺优化方面仍面临挑战。

Abstract: Kirigami, the traditional paper-cutting craft, holds immense potential for
revolutionizing robotics by providing multifunctional, lightweight, and
adaptable solutions. Kirigami structures, characterized by their
bending-dominated deformation, offer resilience to tensile forces and
facilitate shape morphing under small actuation forces. Kirigami components
such as actuators, sensors, batteries, controllers, and body structures can be
tailored to specific robotic applications by optimizing cut patterns. Actuators
based on kirigami principles exhibit complex motions programmable through
various energy sources, while kirigami sensors bridge the gap between
electrical conductivity and compliance. Kirigami-integrated batteries enable
energy storage directly within robot structures, enhancing flexibility and
compactness. Kirigami-controlled mechanisms mimic mechanical computations,
enabling advanced functionalities such as shape morphing and memory functions.
Applications of kirigami-enabled robots include grasping, locomotion, and
wearables, showcasing their adaptability to diverse environments and tasks.
Despite promising opportunities, challenges remain in the design of cut
patterns for a given function and streamlining fabrication techniques.

</details>


### [15] [Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction](https://arxiv.org/abs/2510.07028)
*Sicong Pan,Xuying Huang,Maren Bennewitz*

Main category: cs.RO

TL;DR: 提出了一种时序先验引导的视点规划方法，用于周期性植物3D重建，利用先前重建模型指导新周期的高效数据采集。


<details>
  <summary>Details</summary>
Motivation: 周期性植物3D重建每次从头开始成本高昂，浪费资源且忽略了先前捕获的信息。

Method: 将先前重建模型非刚性对齐到新的部分观测，形成当前几何近似；为适应植物生长膨胀该近似；通过集合覆盖优化计算最小视点集；集成完整流程，包含额外最优视点采集和全局最短路径规划。

Result: 在玉米和番茄上的半球和球面视点空间实验表明，系统在保持或提高表面覆盖率的同时，需要更少的视点和可比较的移动成本。

Conclusion: 该方法能够有效利用时序信息，提高周期性植物重建的效率。

Abstract: Periodic 3D reconstruction is essential for crop monitoring, but costly when
each cycle restarts from scratch, wasting resources and ignoring information
from previous captures. We propose temporal-prior-guided view planning for
periodic plant reconstruction, in which a previously reconstructed model of the
same plant is non-rigidly aligned to a new partial observation to form an
approximation of the current geometry. To accommodate plant growth, we inflate
this approximation and solve a set covering optimization problem to compute a
minimal set of views. We integrated this method into a complete pipeline that
acquires one additional next-best view before registration for robustness and
then plans a globally shortest path to connect the planned set of views and
outputs the best view sequence. Experiments on maize and tomato under
hemisphere and sphere view spaces show that our system maintains or improves
surface coverage while requiring fewer views and comparable movement cost
compared to state-of-the-art baselines.

</details>


### [16] [Diffusing Trajectory Optimization Problems for Recovery During Multi-Finger Manipulation](https://arxiv.org/abs/2510.07030)
*Abhinav Kumar,Fan Yang,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的多指手恢复框架，能自动检测需要恢复的状态并优化接触丰富的恢复轨迹，在螺丝刀转动任务中提升96%性能。


<details>
  <summary>Details</summary>
Motivation: 多指手在执行精细操作任务时，环境扰动或执行错误会影响任务性能，需要恢复行为来恢复正常任务执行。

Method: 使用扩散模型检测不适合任务执行的状态（作为分布外检测问题），通过扩散采样将状态投影回分布内，并用轨迹优化规划接触丰富的恢复轨迹。还提出新方法将整个过程蒸馏以高效扩散恢复轨迹优化问题的完整参数化。

Result: 与强化学习基线和其他不显式规划接触交互的方法相比，在硬件螺丝刀转动任务中，使用该方法恢复使任务性能提升96%，且是唯一能在不导致灾难性任务失败的情况下尝试恢复的方法。

Conclusion: 基于扩散模型的恢复框架能有效处理多指手操作中的扰动和错误，显著提升任务性能，为接触丰富的恢复行为提供了有效解决方案。

Abstract: Multi-fingered hands are emerging as powerful platforms for performing fine
manipulation tasks, including tool use. However, environmental perturbations or
execution errors can impede task performance, motivating the use of recovery
behaviors that enable normal task execution to resume. In this work, we take
advantage of recent advances in diffusion models to construct a framework that
autonomously identifies when recovery is necessary and optimizes contact-rich
trajectories to recover. We use a diffusion model trained on the task to
estimate when states are not conducive to task execution, framed as an
out-of-distribution detection problem. We then use diffusion sampling to
project these states in-distribution and use trajectory optimization to plan
contact-rich recovery trajectories. We also propose a novel diffusion-based
approach that distills this process to efficiently diffuse the full
parameterization, including constraints, goal state, and initialization, of the
recovery trajectory optimization problem, saving time during online execution.
We compare our method to a reinforcement learning baseline and other methods
that do not explicitly plan contact interactions, including on a hardware
screwdriver-turning task where we show that recovering using our method
improves task performance by 96% and that ours is the only method evaluated
that can attempt recovery without causing catastrophic task failure. Videos can
be found at https://dtourrecovery.github.io/.

</details>


### [17] [Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models](https://arxiv.org/abs/2510.07067)
*Daria Pugacheva,Andrey Moskalenko,Denis Shepelev,Andrey Kuznetsov,Vlad Shakhuro,Elena Tutubalina*

Main category: cs.RO

TL;DR: 该论文系统研究了视觉语言动作(VLA)模型在语言扰动下的鲁棒性，发现模型对语义和词汇相似的无关上下文敏感，性能下降可达50%，并提出基于LLM的过滤框架来恢复性能。


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身AI中广泛应用，但其在真实场景中对自然语言变化的鲁棒性尚未得到充分研究，需要系统评估模型在语言扰动下的表现。

Method: 评估VLA模型在两种指令噪声下的性能：(1)人工生成的释义；(2)添加无关上下文。提出基于LLM的过滤框架从噪声输入中提取核心指令。

Result: 模型性能随上下文扩展而下降；对随机上下文相对鲁棒(性能下降<10%)，但对语义词汇相似的上下文敏感(性能下降约50%)；人工释义导致性能下降近20%；使用过滤框架可在噪声条件下恢复98.5%的原始性能。

Conclusion: VLA模型对语言扰动敏感，特别是语义相关的无关上下文；提出的LLM过滤框架能有效缓解这一问题，显著提升模型在噪声环境下的鲁棒性。

Abstract: Vision Language Action (VLA) models are widely used in Embodied AI, enabling
robots to interpret and execute language instructions. However, their
robustness to natural language variability in real-world scenarios has not been
thoroughly investigated. In this work, we present a novel systematic study of
the robustness of state-of-the-art VLA models under linguistic perturbations.
Specifically, we evaluate model performance under two types of instruction
noise: (1) human-generated paraphrasing and (2) the addition of irrelevant
context. We further categorize irrelevant contexts into two groups according to
their length and their semantic and lexical proximity to robot commands. In
this study, we observe consistent performance degradation as context size
expands. We also demonstrate that the model can exhibit relative robustness to
random context, with a performance drop within 10%, while semantically and
lexically similar context of the same length can trigger a quality decline of
around 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To
mitigate this, we propose an LLM-based filtering framework that extracts core
commands from noisy inputs. Incorporating our filtering step allows models to
recover up to 98.5% of their original performance under noisy conditions.

</details>


### [18] [Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications](https://arxiv.org/abs/2510.07077)
*Kento Kawaharazuka,Jihoon Oh,Jun Yamada,Ingmar Posner,Yuke Zhu*

Main category: cs.RO

TL;DR: 本文对视觉-语言-动作（VLA）模型进行了全面综述，涵盖其架构、学习方法、机器人平台、数据集和评估基准，旨在为机器人社区提供实际应用指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型在机器人领域的应用日益增多，VLA模型通过统一视觉、语言和动作数据，旨在学习能够泛化到多样化任务、对象、体现和环境的策略，实现更灵活和可扩展的机器人部署。

Method: 本文提供了系统性的VLA综述，包括策略和架构转变、架构与构建模块、模态特定处理技术、学习范式，以及机器人平台、数据收集策略、数据集、数据增强方法和评估基准的详细分析。

Result: 通过整合软件和硬件组件，本文为VLA系统提供了全栈视角，并整理了按训练方法、评估方法、模态和数据集分类的参考资料。

Conclusion: 本综述旨在为机器人社区提供实用指导，促进VLA模型在真实世界机器人系统中的应用，所有参考资料可在项目网站上获取。

Abstract: Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .

</details>


### [19] [Sampling Strategies for Robust Universal Quadrupedal Locomotion Policies](https://arxiv.org/abs/2510.07094)
*David Rytz,Kim Tien Ly,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 该研究比较了三种关节增益采样策略，用于训练能泛化到多种参数配置的通用四足机器人运动策略，通过在RaiSim中训练并在ANYmal机器人上零样本部署，证明了关节控制器增益随机化对缩小仿真到现实差距的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发能够适应多种物理参数配置的鲁棒通用四足机器人运动策略，解决仿真到现实迁移中的泛化问题。

Method: 比较了三种关节增益采样策略：(1) 质量和增益的线性/多项式函数映射；(2) 基于性能的自适应滤波；(3) 均匀随机采样。使用名义先验和参考模型偏置配置，在RaiSim中训练强化学习策略。

Result: 所有策略在多种四足机器人仿真中测试，并在ANYmal机器人上零样本部署。结果表明，显著的关节控制器增益随机化对于鲁棒地缩小仿真到现实差距至关重要。

Conclusion: 关节控制器增益的显著随机化是实现鲁棒仿真到现实迁移的关键因素，相比多个基线实现，该方法表现出更好的性能。

Abstract: This work focuses on sampling strategies of configuration variations for
generating robust universal locomotion policies for quadrupedal robots. We
investigate the effects of sampling physical robot parameters and joint
proportional-derivative gains to enable training a single reinforcement
learning policy that generalizes to multiple parameter configurations. Three
fundamental joint gain sampling strategies are compared: parameter sampling
with (1) linear and polynomial function mappings of mass-to-gains, (2)
performance-based adaptive filtering, and (3) uniform random sampling. We
improve the robustness of the policy by biasing the configurations using
nominal priors and reference models. All training was conducted on RaiSim,
tested in simulation on a range of diverse quadrupeds, and zero-shot deployed
onto hardware using the ANYmal quadruped robot. Compared to multiple baseline
implementations, our results demonstrate the need for significant joint
controller gains randomization for robust closing of the sim-to-real gap.

</details>


### [20] [A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model](https://arxiv.org/abs/2510.07133)
*Tony Zhang,Burak Kantarci,Umair Siddique*

Main category: cs.RO

TL;DR: 提出基于数字孪生的蜕变测试框架，结合AI图像生成技术，为自动驾驶系统创建虚拟测试环境，显著提升测试覆盖率和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统在复杂真实驾驶环境中测试的挑战，包括预言机问题和场景覆盖不足的问题。

Method: 结合数字孪生技术和AI图像生成模型（如Stable Diffusion），创建虚拟驾驶场景副本，定义基于交通规则的三种蜕变关系进行测试。

Result: 在Udacity自动驾驶模拟器中验证，获得最高真阳性率（0.719）、F1分数（0.689）和精确率（0.662），优于基线方法。

Conclusion: 数字孪生与AI场景生成相结合，为自动驾驶安全测试提供了可扩展、自动化且高保真的解决方案。

Abstract: Ensuring the safety of self-driving cars remains a major challenge due to the
complexity and unpredictability of real-world driving environments. Traditional
testing methods face significant limitations, such as the oracle problem, which
makes it difficult to determine whether a system's behavior is correct, and the
inability to cover the full range of scenarios an autonomous vehicle may
encounter. In this paper, we introduce a digital twin-driven metamorphic
testing framework that addresses these challenges by creating a virtual replica
of the self-driving system and its operating environment. By combining digital
twin technology with AI-based image generative models such as Stable Diffusion,
our approach enables the systematic generation of realistic and diverse driving
scenes. This includes variations in weather, road topology, and environmental
features, all while maintaining the core semantics of the original scenario.
The digital twin provides a synchronized simulation environment where changes
can be tested in a controlled and repeatable manner. Within this environment,
we define three metamorphic relations inspired by real-world traffic rules and
vehicle behavior. We validate our framework in the Udacity self-driving
simulator and demonstrate that it significantly enhances test coverage and
effectiveness. Our method achieves the highest true positive rate (0.719), F1
score (0.689), and precision (0.662) compared to baseline approaches. This
paper highlights the value of integrating digital twins with AI-powered
scenario generation to create a scalable, automated, and high-fidelity testing
solution for autonomous vehicle safety.

</details>


### [21] [TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking](https://arxiv.org/abs/2510.07134)
*Jiahang Liu,Yunpeng Qi,Jiazhao Zhang,Minghan Li,Shaoan Wang,Kui Wu,Hanjing Ye,Hong Zhang,Zhibo Chen,Fangwei Zhong,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: TrackVLA++是一个增强型视觉-语言-动作模型，通过空间推理机制和目标识别记忆模块，解决了现有方法在严重遮挡和相似干扰物情况下的跟踪失败问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏明确的空间推理和有效的时间记忆，导致在严重遮挡或存在相似干扰物时跟踪失败。

Method: 提出两个关键模块：空间推理机制（Polar-CoT）和目标识别记忆（TIM）。Polar-CoT通过链式思维推断目标相对位置并编码为极坐标令牌；TIM采用门控更新策略保持长期目标记忆。

Result: 在公共基准测试中达到最先进性能，在EVT-Bench DT分割上分别超过先前领先方法5.1和12个百分点，并表现出强大的零样本泛化能力。

Conclusion: TrackVLA++通过空间推理和时间记忆机制，显著提升了在动态和遮挡场景下的鲁棒跟踪性能。

Abstract: Embodied Visual Tracking (EVT) is a fundamental ability that underpins
practical applications, such as companion robots, guidance robots and service
assistants, where continuously following moving targets is essential. Recent
advances have enabled language-guided tracking in complex and unstructured
scenes. However, existing approaches lack explicit spatial reasoning and
effective temporal memory, causing failures under severe occlusions or in the
presence of similar-looking distractors. To address these challenges, we
present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances
embodied visual tracking with two key modules, a spatial reasoning mechanism
and a Target Identification Memory (TIM). The reasoning module introduces a
Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative
position and encodes it as a compact polar-coordinate token for action
prediction. Guided by these spatial priors, the TIM employs a gated update
strategy to preserve long-horizon target memory, ensuring spatiotemporal
consistency and mitigating target loss during extended occlusions. Extensive
experiments show that TrackVLA++ achieves state-of-the-art performance on
public benchmarks across both egocentric and multi-camera settings. On the
challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading
approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong
zero-shot generalization, enabling robust real-world tracking in dynamic and
occluded scenarios.

</details>


### [22] [DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction](https://arxiv.org/abs/2510.07152)
*Jingkai Sun,Gang Han,Pihai Sun,Wen Zhao,Jiahang Cao,Jiaxu Wang,Yijie Guo,Qiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种新型人形机器人地形感知运动框架，通过结合盲运动策略、多模态注意力变换器和真实深度图像合成方法，解决了现有方法的训练效率低、sim-to-real差距大、延迟高等问题。


<details>
  <summary>Details</summary>
Motivation: 现有地形感知人形机器人运动方法存在两大局限：基于深度图像的端到端学习方法训练效率低且sim-to-real差距大；基于高程图的方法依赖多传感器和定位系统，导致延迟和鲁棒性降低。

Method: 框架包含三个关键组件：(1) 地形感知运动策略（盲骨干），利用预训练的高程图感知指导强化学习；(2) 多模态交叉注意力变换器，从噪声深度图像重建结构化地形表示；(3) 真实深度图像合成方法，采用自遮挡感知光线投射和噪声感知建模。

Result: 实现了超过30%的地形重建误差减少，在完整尺寸人形机器人上验证了框架，展示了在多样挑战性地形上的敏捷自适应运动能力。

Conclusion: 该框架能够在有限数据和硬件资源下实现高效策略训练，同时保留对泛化至关重要的关键地形特征，为人形机器人的地形感知运动提供了有效解决方案。

Abstract: Recent advancements in legged robot perceptive locomotion have shown
promising progress. However, terrain-aware humanoid locomotion remains largely
constrained to two paradigms: depth image-based end-to-end learning and
elevation map-based methods. The former suffers from limited training
efficiency and a significant sim-to-real gap in depth perception, while the
latter depends heavily on multiple vision sensors and localization systems,
resulting in latency and reduced robustness. To overcome these challenges, we
propose a novel framework that tightly integrates three key components: (1)
Terrain-Aware Locomotion Policy with a Blind Backbone, which leverages
pre-trained elevation map-based perception to guide reinforcement learning with
minimal visual input; (2) Multi-Modality Cross-Attention Transformer, which
reconstructs structured terrain representations from noisy depth images; (3)
Realistic Depth Images Synthetic Method, which employs self-occlusion-aware ray
casting and noise-aware modeling to synthesize realistic depth observations,
achieving over 30\% reduction in terrain reconstruction error. This combination
enables efficient policy training with limited data and hardware resources,
while preserving critical terrain features essential for generalization. We
validate our framework on a full-sized humanoid robot, demonstrating agile and
adaptive locomotion across diverse and challenging terrains.

</details>


### [23] [A Narwhal-Inspired Sensing-to-Control Framework for Small Fixed-Wing Aircraft](https://arxiv.org/abs/2510.07160)
*Fengze Xie,Xiaozhou Fan,Jacob Schuster,Yisong Yue,Morteza Gharib*

Main category: cs.RO

TL;DR: 提出了一种用于固定翼无人机的端到端感知-控制管道，结合仿生硬件、物理信息动力学学习和凸控制分配，解决了低空速敏捷性问题。


<details>
  <summary>Details</summary>
Motivation: 固定翼无人机具有续航和效率优势，但由于高度耦合的动力学特性，缺乏低空速敏捷性。测量小型机架上的气流很困难，因为近体空气动力学、螺旋桨滑流、控制面驱动和环境阵风会扭曲压力信号。

Method: 1. 仿生硬件设计：基于独角鲸突出的长牙概念，在上游安装自制多孔探头，并辅以稀疏布置的机翼压力传感器进行局部流量测量；2. 数据驱动校准：将探头压力映射到空速和流动角度；3. 学习控制仿射动力学模型：使用估计的空速/角度和稀疏传感器；4. 软左右对称正则化器提高可识别性；5. 正则化最小二乘分配器实现平滑、修整的驱动。

Result: 风洞研究表明：添加机翼压力可使力估计误差减少25-30%；所提出的模型在分布偏移下性能下降较少（约12%，而未经结构化的基线为44%）；力跟踪通过更平滑的输入得到改善，包括与普通仿射模型相比法向力RMSE减少27%，与未经结构化的基线相比减少34%。

Conclusion: 该端到端方法通过仿生传感、物理信息建模和凸优化控制，显著提高了固定翼无人机在低空速下的敏捷性和控制性能。

Abstract: Fixed-wing unmanned aerial vehicles (UAVs) offer endurance and efficiency but
lack low-speed agility due to highly coupled dynamics. We present an end-to-end
sensing-to-control pipeline that combines bio-inspired hardware,
physics-informed dynamics learning, and convex control allocation. Measuring
airflow on a small airframe is difficult because near-body aerodynamics,
propeller slipstream, control-surface actuation, and ambient gusts distort
pressure signals. Inspired by the narwhal's protruding tusk, we mount in-house
multi-hole probes far upstream and complement them with sparse, carefully
placed wing pressure sensors for local flow measurement. A data-driven
calibration maps probe pressures to airspeed and flow angles. We then learn a
control-affine dynamics model using the estimated airspeed/angles and sparse
sensors. A soft left/right symmetry regularizer improves identifiability under
partial observability and limits confounding between wing pressures and
flaperon inputs. Desired wrenches (forces and moments) are realized by a
regularized least-squares allocator that yields smooth, trimmed actuation.
Wind-tunnel studies across a wide operating range show that adding wing
pressures reduces force-estimation error by 25-30%, the proposed model degrades
less under distribution shift (about 12% versus 44% for an unstructured
baseline), and force tracking improves with smoother inputs, including a 27%
reduction in normal-force RMSE versus a plain affine model and 34% versus an
unstructured baseline.

</details>


### [24] [TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2510.07181)
*Yi Han,Cheng Chi,Enshen Zhou,Shanyu Rong,Jingkun An,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: TIGeR是一个将视觉语言模型从感知估计器转变为几何计算器的框架，通过外部工具实现精确几何计算，在机器人操作中达到厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理方面仅限于定性精度，缺乏机器人应用所需的计算精度，无法利用深度传感器和相机标定的度量线索。

Method: 采用两阶段训练流程：监督微调（SFT）和强化微调（RFT），结合分层奖励设计，让模型识别几何推理需求、生成计算代码并调用专业库进行精确计算。

Result: 在几何推理基准测试中达到SOTA性能，在真实世界机器人操作任务中展示厘米级精度。

Conclusion: TIGeR框架成功将VLMs转变为几何计算机，通过工具集成实现了机器人操作所需的精确几何推理能力。

Abstract: Vision-Language Models (VLMs) have shown remarkable capabilities in spatial
reasoning, yet they remain fundamentally limited to qualitative precision and
lack the computational precision required for real-world robotics. Current
approaches fail to leverage metric cues from depth sensors and camera
calibration, instead reducing geometric problems to pattern recognition tasks
that cannot deliver the centimeter-level accuracy essential for robotic
manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools. Rather than attempting to internalize
complex geometric operations within neural networks, TIGeR empowers models to
recognize geometric reasoning requirements, synthesize appropriate
computational code, and invoke specialized libraries for exact calculations. To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations. Through
a two-stage training pipeline combining supervised fine-tuning (SFT) and
reinforcement fine-tuning (RFT) with our proposed hierarchical reward design,
TIGeR achieves SOTA performance on geometric reasoning benchmarks while
demonstrating centimeter-level precision in real-world robotic manipulation
tasks.

</details>


### [25] [COMPAct: Computational Optimization and Automated Modular design of Planetary Actuators](https://arxiv.org/abs/2510.07197)
*Aman Singh,Deepak Kapa,Suryank Joshi,Shishir Kolathaya*

Main category: cs.RO

TL;DR: COMPAct框架通过计算优化和自动化模块化设计，系统地为四种行星齿轮箱类型优化参数，最小化质量和宽度同时最大化效率，并自动生成可直接3D打印的CAD模型。


<details>
  <summary>Details</summary>
Motivation: 机器人执行器设计中，齿轮箱参数优化和CAD自动化设计研究不足，需要系统化的优化方法和自动化工具来提升设计效率和质量。

Method: 开发COMPAct框架，对四种行星齿轮箱类型（SSPG、CPG、WPG、DSPG）进行参数优化，最小化质量和宽度，最大化效率，并自动化生成CAD模型。

Result: 实验显示SSPG执行器机械效率60-80%，空载回差0.59度，传动刚度242.7 Nm/rad；CPG执行器效率60%，回差2.6度，刚度201.6 Nm/rad。

Conclusion: COMPAct框架成功实现了齿轮箱参数的优化设计和CAD自动化生成，为不同传动比范围提供了合适的齿轮箱类型选择指导。

Abstract: The optimal design of robotic actuators is a critical area of research, yet
limited attention has been given to optimizing gearbox parameters and
automating actuator CAD. This paper introduces COMPAct: Computational
Optimization and Automated Modular Design of Planetary Actuators, a framework
that systematically identifies optimal gearbox parameters for a given motor
across four gearbox types, single-stage planetary gearbox (SSPG), compound
planetary gearbox (CPG), Wolfrom planetary gearbox (WPG), and double-stage
planetary gearbox (DSPG). The framework minimizes mass and actuator width while
maximizing efficiency, and further automates actuator CAD generation to enable
direct 3D printing without manual redesign. Using this framework, optimal
gearbox designs are explored over a wide range of gear ratios, providing
insights into the suitability of different gearbox types across various gear
ratio ranges. In addition, the framework is used to generate CAD models of all
four gearbox types with varying gear ratios and motors. Two actuator types are
fabricated and experimentally evaluated through power efficiency, no-load
backlash, and transmission stiffness tests. Experimental results indicate that
the SSPG actuator achieves a mechanical efficiency of 60-80 %, a no-load
backlash of 0.59 deg, and a transmission stiffness of 242.7 Nm/rad, while the
CPG actuator demonstrates 60 % efficiency, 2.6 deg backlash, and a stiffness of
201.6 Nm/rad. Code available at:
https://anonymous.4open.science/r/COMPAct-SubNum-3408 Video:
https://youtu.be/99zOKgxsDho

</details>


### [26] [HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving](https://arxiv.org/abs/2510.07210)
*Donald Pfaffmann,Matthias Klusch,Marcel Steinmetz*

Main category: cs.RO

TL;DR: HyPlan是一种混合学习方法，结合了多智能体行为预测、深度强化学习和在线POMDP规划，用于解决自动驾驶在部分可观测交通环境中的无碰撞导航问题。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在部分可观测交通环境中安全导航的挑战，特别是在有行人的关键交通场景下。

Method: 结合多智能体行为预测、近端策略优化的深度强化学习，以及基于启发式置信度垂直剪枝的近似在线POMDP规划。

Result: 在CARLA-CTS2基准测试中，HyPlan比相关基线方法导航更安全，且比替代在线POMDP规划器执行速度显著更快。

Conclusion: HyPlan方法在不影响驾驶安全的前提下，通过垂直剪枝技术有效减少了执行时间，为部分可观测环境下的自动驾驶导航提供了有效解决方案。

Abstract: We present a novel hybrid learning-assisted planning method, named HyPlan,
for solving the collision-free navigation problem for self-driving cars in
partially observable traffic environments. HyPlan combines methods for
multi-agent behavior prediction, deep reinforcement learning with proximal
policy optimization and approximated online POMDP planning with heuristic
confidence-based vertical pruning to reduce its execution time without
compromising safety of driving. Our experimental performance analysis on the
CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed
that HyPlan may navigate safer than selected relevant baselines and perform
significantly faster than considered alternative online POMDP planners.

</details>
