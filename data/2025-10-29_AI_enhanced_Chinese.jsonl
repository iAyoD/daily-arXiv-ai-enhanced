{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoboOmni\u6846\u67b6\uff0c\u57fa\u4e8e\u5168\u6a21\u6001LLM\uff0c\u901a\u8fc7\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u610f\u56fe\u8bc6\u522b\uff0c\u652f\u6301\u4e3b\u52a8\u4ea4\u4e92\u548c\u52a8\u4f5c\u6267\u884c\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u548cASR\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u4eba\u7c7b\u5f88\u5c11\u76f4\u63a5\u53d1\u51fa\u6307\u4ee4\uff0c\u800c\u662f\u901a\u8fc7\u5bf9\u8bdd\u3001\u73af\u5883\u58f0\u97f3\u548c\u89c6\u89c9\u7ebf\u7d22\u8868\u8fbe\u610f\u56fe\u3002\u73b0\u6709VLA\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u6307\u4ee4\uff0c\u9700\u8981\u673a\u5668\u4eba\u80fd\u591f\u4e3b\u52a8\u63a8\u65ad\u7528\u6237\u610f\u56fe\u3002", "method": "\u63d0\u51faPerceiver-Thinker-Talker-Executor\u6846\u67b6\uff0c\u57fa\u4e8e\u7aef\u5230\u7aef\u5168\u6a21\u6001LLM\uff0c\u7edf\u4e00\u610f\u56fe\u8bc6\u522b\u3001\u4ea4\u4e92\u786e\u8ba4\u548c\u52a8\u4f5c\u6267\u884c\u3002\u65f6\u7a7a\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\uff0c\u652f\u6301\u76f4\u63a5\u8bed\u97f3\u4ea4\u4e92\u3002\u6784\u5efa\u4e86\u5305\u542b14\u4e07\u6761\u6570\u636e\u7684OmniAction\u6570\u636e\u96c6\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cRoboOmni\u5728\u6210\u529f\u7387\u3001\u63a8\u7406\u901f\u5ea6\u3001\u610f\u56fe\u8bc6\u522b\u548c\u4e3b\u52a8\u534f\u52a9\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u548cASR\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RoboOmni\u901a\u8fc7\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u6307\u4ee4\u548c\u5168\u6a21\u6001\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u4e3a\u4e3b\u52a8\u610f\u56fe\u8bc6\u522b\u548c\u534f\u4f5c\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23860", "abs": "https://arxiv.org/abs/2510.23860", "authors": ["Hyung Chan Cho", "Go-Eum Cha", "Yanfu Liu", "Sooyeon Jeong"], "title": "Motivating Students' Self-study with Goal Reminder and Emotional Support", "comment": "RO-MAN 2025 accepted paper", "summary": "While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement.", "AI": {"tldr": "\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u540c\u4f34\u5b66\u4e60\u4f34\u4fa3\uff0c\u901a\u8fc7\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u5e2e\u52a9\u5927\u5b66\u751f\u81ea\u4e3b\u5b66\u4e60\uff0c\u76f8\u6bd4\u4ec5\u63d0\u4f9b\u7269\u7406\u5b58\u5728\u7684\u63a7\u5236\u7ec4\uff0c\u80fd\u63d0\u9ad8\u4e13\u6ce8\u5ea6\u3001\u751f\u4ea7\u529b\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u867d\u7136\u793e\u4ea4\u673a\u5668\u4eba\u5728\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u652f\u6301\u6548\u679c\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5b83\u4eec\u5728\u81ea\u4e3b\u5b66\u4e60\u60c5\u5883\u4e2d\u5bf9\u5b66\u751f\u7684\u6f5c\u5728\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u6027\u7684Wizard-of-Oz\u7814\u7a76\uff0c\u6bd4\u8f83\u76ee\u6807\u63d0\u9192\u3001\u60c5\u611f\u652f\u6301\u548c\u4ec5\u7269\u7406\u5b58\u5728\uff08\u63a7\u5236\u7ec4\uff09\u4e09\u79cd\u6761\u4ef6\u4e0b\u673a\u5668\u4eba\u5bf9\u5b66\u751f\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u6761\u4ef6\u4e0b\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u66f4\u9ad8\u7684\u6613\u7528\u6027\uff0c\u76ee\u6807\u63d0\u9192\u7ec4\u8fd8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u672a\u6765\u4f7f\u7528\u610f\u613f\u3002\u53c2\u4e0e\u8005\u5bf9\u673a\u5668\u4eba\u7684\u6ee1\u610f\u5ea6\u4e0e\u5176\u5c06\u673a\u5668\u4eba\u89c6\u4e3a\u793e\u4f1a\u5b9e\u4f53\u7684\u611f\u77e5\u76f8\u5173\uff0c\u8fd9\u79cd\u611f\u77e5\u80fd\u9884\u6d4b\u5b66\u4e60\u76ee\u6807\u7684\u8fbe\u6210\u7a0b\u5ea6\u3002", "conclusion": "\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u901a\u8fc7\u529f\u80fd\u548c\u60c5\u611f\u53c2\u4e0e\uff0c\u5728\u81ea\u4e3b\u5b66\u4e60\u652f\u6301\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.23902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23902", "abs": "https://arxiv.org/abs/2510.23902", "authors": ["Jans Solano", "Diego Quiroz"], "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped", "comment": "Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots", "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4f4e\u6210\u672c\u8f6e\u5f0f\u56db\u8db3\u673a\u5668\u4eba\u7684\u6062\u590d\u611f\u77e5\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df1\u5ea6\u76f8\u673a\u89c6\u89c9\u611f\u77e5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u548c\u8dcc\u5012\u81ea\u4e3b\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u4f9d\u8d56\u6602\u8d35\u7684\u6267\u884c\u5668\u548c\u4f20\u611f\u5668\uff0c\u4e14\u5f88\u5c11\u96c6\u6210\u8dcc\u5012\u6062\u590d\u529f\u80fd\uff0c\u7279\u522b\u662f\u5728\u8f6e\u817f\u6df7\u5408\u5f62\u6001\u4e2d\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u8fdb\u884c\u89c6\u89c9\u611f\u77e5\uff0c\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u548c\u81ea\u4e3b\u6062\u590d\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u5728\u975e\u89c4\u5219\u5730\u5f62\u4e0a\u5177\u6709\u654f\u6377\u79fb\u52a8\u6027\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u4ece\u5916\u90e8\u6270\u52a8\u548c\u81ea\u81f4\u6545\u969c\u4e2d\u6062\u590d\uff0c\u5728\u7ed3\u6784\u5316\u5ba4\u5185\u7a7a\u95f4\u5b9e\u73b0\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5728\u9884\u7b97\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u81ea\u4e3b\u5bfc\u822a\u548c\u9c81\u68d2\u8fd0\u52a8\u7b56\u7565\u7684\u95e8\u69db\u3002"}}
{"id": "2510.23928", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23928", "abs": "https://arxiv.org/abs/2510.23928", "authors": ["Raman Jha", "Yang Zhou", "Giuseppe Loianno"], "title": "Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments", "comment": "Under Review for ROBOVIS 2026", "summary": "In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u57fa\u4e8e\u8bef\u5dee\u7684\u9009\u62e9\u6a21\u5757\u548c\u57fa\u4e8e\u52a8\u91cf\u7684\u66f4\u65b0\u6a21\u5757\uff0c\u52a8\u6001\u8c03\u6574\u5173\u952e\u5e27\u9009\u62e9\u9608\u503c\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u53473D\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u611f\u77e5\u7684\u5173\u952e\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u4ece\u538b\u7f29\u6570\u636e\u6d41\u4e2d\u521b\u5efa\u9ad8\u8d28\u91cf\u76843D\u4e16\u754c\u8868\u793a\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u673a\u5668\u4eba\u5b66\u4e60\u548c\u90e8\u7f72\u63d0\u4f9b\u5173\u952e\u6b65\u9aa4\u3002", "method": "\u96c6\u6210\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff1a\u57fa\u4e8e\u5149\u5ea6\u8bef\u5dee\u548c\u7ed3\u6784\u76f8\u4f3c\u6027(SSIM)\u8bef\u5dee\u7684\u9009\u62e9\u6a21\u5757\uff0c\u4ee5\u53ca\u57fa\u4e8e\u52a8\u91cf\u7684\u66f4\u65b0\u6a21\u5757\uff0c\u52a8\u6001\u8c03\u6574\u5173\u952e\u5e27\u9009\u62e9\u9608\u503c\u4ee5\u9002\u5e94\u573a\u666f\u8fd0\u52a8\u52a8\u6001\u3002", "result": "\u5728Spann3r\u548cCUT3R\u4e24\u4e2a\u6700\u5148\u8fdb\u76843D\u91cd\u5efa\u7f51\u7edc\u4e0a\u8bc4\u4f30\uff0c\u89c2\u5bdf\u5230\u91cd\u5efa\u8d28\u91cf\u7684\u4e00\u81f4\u6539\u8fdb\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u9759\u6001\u5173\u952e\u5e27\u9009\u62e9\u7b56\u7565\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5404\u7ec4\u4ef6\u5bf9\u6574\u4f53\u6027\u80fd\u589e\u76ca\u7684\u6709\u6548\u8d21\u732e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee3\u8868\u4e86\u81ea\u9002\u5e94\u611f\u77e5\u7cfb\u7edf\u7684\u6709\u610f\u4e49\u7684\u8fdb\u6b65\uff0c\u80fd\u591f\u52a8\u6001\u54cd\u5e94\u590d\u6742\u548c\u6f14\u53d8\u7684\u89c6\u89c9\u573a\u666f\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23954", "abs": "https://arxiv.org/abs/2510.23954", "authors": ["Pejman Kheradmand", "Behnam Moradkhani", "Raghavasimhan Sankaranarayanan", "Kent K. Yamamoto", "Tanner J. Zachem", "Patrick J. Codd", "Yash Chitalia", "Pierre E. Dupont"], "title": "A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons", "comment": null, "summary": "Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eCosserat\u6746\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21n\u4e2a\u540c\u5fc3\u7ba1\u3001\u6bcf\u4e2a\u7ba1\u7531m_i\u4e2a\u808c\u8171\u9a71\u52a8\u7684\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u6784\uff0c\u5b9e\u73b0\u4e86\u5c0f\u4e8e4%\u603b\u957f\u5ea6\u7684\u5c16\u7aef\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u6784\u7ed3\u5408\u4e86\u808c\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u548c\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u4f18\u70b9\uff0c\u4f46\u7f3a\u4e4f\u5b8c\u6574\u901a\u7528\u7684\u529b\u5b66\u6a21\u578b\u3002", "method": "\u4f7f\u7528Cosserat\u6746\u7406\u8bba\u6846\u67b6\uff0c\u5141\u8bb8\u6bcf\u4e2a\u7ba1\u626d\u8f6c\u548c\u4f38\u957f\uff0c\u540c\u65f6\u5f3a\u5236\u5f2f\u66f2\u5171\u4eab\u4e2d\u5fc3\u7ebf\u3002", "result": "\u5728\u53cc\u7ba1\u548c\u4e09\u7ba1\u7ec4\u4ef6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5c16\u7aef\u9884\u6d4b\u8bef\u5dee\u5c0f\u4e8e\u673a\u5668\u4eba\u603b\u957f\u5ea6\u76844%\uff1b\u5e94\u7528\u4e8e\u73b0\u6709\u673a\u5668\u4eba\u65f6\uff0c\u6700\u5927\u5c16\u7aef\u504f\u5dee\u7ea6\u4e3a\u603b\u957f\u5ea6\u76845%\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5148\u8fdb\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u7cbe\u786e\u5f62\u72b6\u4f30\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23963", "abs": "https://arxiv.org/abs/2510.23963", "authors": ["Hiroki Ishikawa", "Kyosuke Ishibashi", "Ko Yamamoto"], "title": "Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping", "comment": null, "summary": "This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u80fd\u529b\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u6307\uff0c\u80fd\u591f\u901a\u8fc7\u5305\u88f9\u65b9\u5f0f\u6293\u53d6\u7269\u4f53\u3002\u8be5\u624b\u6307\u91c7\u7528\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u901a\u8fc7\u5355\u4e00\u9a71\u52a8\u6e90\u5b9e\u73b0\u6df1\u5ea6\u63d2\u5165\u548c\u7a33\u5b9a\u6293\u53d6\u529f\u80fd\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u8f6f\u4f53\u624b\u80fd\u591f\u5728\u5bc6\u96c6\u5806\u653e\u7684\u591a\u4e2a\u7269\u4f53\u4e2d\u6293\u53d6\u5355\u4e2a\u7269\u4f53\uff0c\u8f6f\u4f53\u624b\u6307\u9700\u8981\u5177\u5907\u5e73\u9762\u5185\u548c\u5e73\u9762\u5916\u7684\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u529f\u80fd\uff0c\u4ee5\u4fbf\u6df1\u5165\u7269\u4f53\u95f4\u7684\u72ed\u7a84\u95f4\u9699\u5e76\u4fdd\u6301\u626d\u8f6c\u53d8\u5f62\u72b6\u6001\u3002", "method": "\u63d0\u51fa\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u901a\u8fc7\u538b\u529b\u53d8\u5316\u81ea\u9002\u5e94\u8c03\u6574\u521a\u5ea6\uff1b\u8fdb\u884c\u6709\u9650\u5143\u5206\u6790\u786e\u5b9a\u8bbe\u8ba1\u53c2\u6570\uff1b\u5f00\u53d1\u8f6f\u4f53\u624b\u6307\u5e76\u8fdb\u884c\u57fa\u7840\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u6709\u9650\u5143\u5206\u6790\u4f18\u5316\u4e86\u8bbe\u8ba1\u53c2\u6570\uff0c\u5f00\u53d1\u7684\u8f6f\u4f53\u624b\u6307\u80fd\u591f\u6210\u529f\u6293\u53d6\u5404\u79cd\u7269\u4f53\uff0c\u9a8c\u8bc1\u4e86\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u548c\u5305\u88f9\u6293\u53d6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53ef\u53d8\u521a\u5ea6\u8f6f\u4f53\u624b\u6307\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5728\u5bc6\u96c6\u7269\u4f53\u4e2d\u6293\u53d6\u5355\u4e2a\u7269\u4f53\u7684\u6311\u6218\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u6293\u53d6\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2510.23988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23988", "abs": "https://arxiv.org/abs/2510.23988", "authors": ["Phuc Nguyen Xuan", "Thanh Nguyen Canh", "Huu-Hung Nguyen", "Nak Young Chong", "Xiem HoangVan"], "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting", "comment": null, "summary": "This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity rendering, ideal for robotics. However,\nits use in multi-robot systems introduces significant challenges in maintaining\nglobal consistency, managing communication, and fusing data from heterogeneous\nsources. We systematically categorize approaches by their architecture --\ncentralized, distributed -- and analyze core components like multi-agent\nconsistency and alignment, communication-efficient, Gaussian representation,\nsemantic distillation, fusion and pose optimization, and real-time scalability.\nIn addition, a summary of critical datasets and evaluation metrics is provided\nto contextualize performance. Finally, we identify key open challenges and\nchart future research directions, including lifelong mapping, semantic\nassociation and mapping, multi-model for robustness, and bridging the Sim2Real\ngap.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u591a\u673a\u5668\u4eba\u534f\u540cSLAM\u9886\u57df\uff0c\u5206\u6790\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u663e\u5f0f\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u975e\u5e38\u9002\u5408\u673a\u5668\u4eba\u5e94\u7528\u3002\u4f46\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u7ef4\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3001\u7ba1\u7406\u901a\u4fe1\u548c\u878d\u5408\u5f02\u6784\u6570\u636e\u6e90\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u6309\u67b6\u6784\uff08\u96c6\u4e2d\u5f0f\u3001\u5206\u5e03\u5f0f\uff09\u5206\u7c7b\u65b9\u6cd5\uff0c\u5206\u6790\u591a\u667a\u80fd\u4f53\u4e00\u81f4\u6027\u5bf9\u9f50\u3001\u901a\u4fe1\u6548\u7387\u3001\u9ad8\u65af\u8868\u793a\u3001\u8bed\u4e49\u84b8\u998f\u3001\u878d\u5408\u4e0e\u4f4d\u59ff\u4f18\u5316\u3001\u5b9e\u65f6\u53ef\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u7ec4\u4ef6\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u952e\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u7684\u603b\u7ed3\uff0c\u4e3a\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u80cc\u666f\u3002\u8bc6\u522b\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u786e\u5b9a\u4e86\u5173\u952e\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u7ec8\u8eab\u5efa\u56fe\u3001\u8bed\u4e49\u5173\u8054\u4e0e\u5efa\u56fe\u3001\u591a\u6a21\u578b\u9c81\u68d2\u6027\u4ee5\u53ca\u5f25\u5408Sim2Real\u5dee\u8ddd\u3002"}}
{"id": "2510.23997", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.23997", "abs": "https://arxiv.org/abs/2510.23997", "authors": ["Stanley Wu", "Mohamad H. Danesh", "Simon Li", "Hanna Yurchyk", "Amin Abyaneh", "Anas El Houssaini", "David Meger", "Hsiu-Chin Lin"], "title": "VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion", "comment": "Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures", "summary": "Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy", "AI": {"tldr": "VOCALoco\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u8f93\u5165\u52a8\u6001\u8c03\u6574\u56db\u8db3\u673a\u5668\u4eba\u7684\u6b65\u6001\u7b56\u7565\uff0c\u5728\u697c\u68af\u4efb\u52a1\u4e2d\u6bd4\u7aef\u5230\u7aefDRL\u65b9\u6cd5\u66f4\u5b89\u5168\u3001\u66f4\u8282\u80fd\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6cdb\u5316\u5230\u65b0\u5730\u5f62\u65f6\u5b58\u5728\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u9650\u5236\uff0c\u9700\u8981\u66f4\u5b89\u5168\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6b65\u6001\u7b56\u7565\u96c6\uff0c\u901a\u8fc7\u9884\u6d4b\u6267\u884c\u5b89\u5168\u6027\u548c\u80fd\u8017\u6765\u8bc4\u4f30\u7b56\u7565\u53ef\u884c\u6027\uff0c\u9009\u62e9\u65e2\u5b89\u5168\u53c8\u8282\u80fd\u7684\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u697c\u68af\u4e0a\u4e0b\u4efb\u52a1\u4e2d\uff0cVOCALoco\u76f8\u6bd4\u4f20\u7edf\u7aef\u5230\u7aefDRL\u7b56\u7565\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "VOCALoco\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6709\u6548\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u5b89\u5168\u6027\u548c\u80fd\u6548\u3002"}}
{"id": "2510.24029", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\u878d\u5165\u8fb9\u754c\u5411\u91cf\u7ec6\u80de(BVC)\u6846\u67b6\u76843D\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf2D BVC\u6a21\u578b\u5728\u6c34\u5e73\u5bf9\u79f0\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u7a7a\u95f4\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u7269\u542f\u53d1\u673a\u5668\u4eba\u6a21\u578b\u7684\u7a7a\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8fb9\u754c\u5411\u91cf\u7ec6\u80de(BVC)\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u4e8c\u7ef4\u73af\u5883\uff0c\u5728\u5b58\u5728\u6c34\u5e73\u5bf9\u79f0\u6027\u7684\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u7a7a\u95f4\u6a21\u7cca\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u5c06\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\u7eb3\u5165BVC\u6846\u67b6\uff0c\u5b9e\u73b0\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u9c81\u68d2\u8fb9\u754c\u68c0\u6d4b\u3002", "method": "\u901a\u8fc7\u5904\u7406LiDAR\u6570\u636e\u6355\u6349\u5782\u76f4\u8f6e\u5ed3\uff0c\u5c06\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\u878d\u5165BVC\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u4e09\u7ef4\u8fb9\u754c\u68c0\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u533a\u5206\u5728\u7eaf2D\u8868\u793a\u4e0b\u65e0\u6cd5\u533a\u5206\u7684\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u5728\u5782\u76f4\u53d8\u5316\u6700\u5c0f\u7684\u73af\u5883\u4e2d\uff0c3D\u6a21\u578b\u6027\u80fd\u4e0e2D\u57fa\u7ebf\u76f8\u5f53\uff1b\u968f\u77403D\u590d\u6742\u6027\u589e\u52a0\uff0c\u8be5\u6a21\u578b\u4ea7\u751f\u66f4\u591a\u72ec\u7279\u7684\u573a\u6240\u573a\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u7a7a\u95f4\u6df7\u53e0\u3002", "conclusion": "\u5728BVC\u5b9a\u4f4d\u4e2d\u6dfb\u52a0\u5782\u76f4\u7ef4\u5ea6\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u771f\u5b9e\u4e16\u754c3D\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u548c\u6620\u5c04\u80fd\u529b\uff0c\u540c\u65f6\u5728\u7b80\u5355\u7684\u8fd1\u5e73\u9762\u573a\u666f\u4e2d\u4fdd\u6301\u6027\u80fd\u4e00\u81f4\u3002"}}
{"id": "2510.24052", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24052", "abs": "https://arxiv.org/abs/2510.24052", "authors": ["Jongsuk Kim", "Jaeyoung Lee", "Gyojin Han", "Dongjae Lee", "Minki Jeong", "Junmo Kim"], "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration", "comment": null, "summary": "Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models.", "AI": {"tldr": "SynAD\u662f\u9996\u4e2a\u5229\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u667a\u80fd\u4f53\u5408\u6210\u573a\u666f\u4e2d\u6307\u5b9a\u5177\u6709\u6700\u5168\u9762\u9a7e\u9a76\u4fe1\u606f\u7684\u667a\u80fd\u4f53\u4f5c\u4e3a\u81ea\u8f66\uff0c\u5c06\u8def\u5f84\u7ea7\u573a\u666f\u6295\u5f71\u5230\u5730\u56fe\u4e0a\uff0c\u5e76\u4f7f\u7528\u65b0\u5f00\u53d1\u7684Map-to-BEV\u7f51\u7edc\u83b7\u53d6\u9e1f\u77b0\u56fe\u7279\u5f81\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u9a7e\u9a76\u6570\u636e\u9650\u5236\u4e86\u8bad\u7ec3\u573a\u666f\u7684\u591a\u6837\u6027\uff0c\u800c\u5408\u6210\u573a\u666f\u751f\u6210\u867d\u7136\u80fd\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u6307\u5b9a\u7684\u81ea\u8f66\u548c\u76f8\u5173\u4f20\u611f\u5668\u8f93\u5165\u3002", "method": "\u5728\u591a\u667a\u80fd\u4f53\u5408\u6210\u573a\u666f\u4e2d\u6307\u5b9a\u5177\u6709\u6700\u5168\u9762\u9a7e\u9a76\u4fe1\u606f\u7684\u667a\u80fd\u4f53\u4f5c\u4e3a\u81ea\u8f66\uff1b\u5c06\u8def\u5f84\u7ea7\u573a\u666f\u6295\u5f71\u5230\u5730\u56fe\u4e0a\uff1b\u4f7f\u7528\u65b0\u5f00\u53d1\u7684Map-to-BEV\u7f51\u7edc\u83b7\u53d6\u9e1f\u77b0\u56fe\u7279\u5f81\uff1b\u8bbe\u8ba1\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u6574\u5408\u57fa\u4e8e\u5730\u56fe\u7684\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSynAD\u6709\u6548\u6574\u5408\u4e86\u6240\u6709\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "SynAD\u901a\u8fc7\u6865\u63a5\u5408\u6210\u573a\u666f\u751f\u6210\u548c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u4e3a\u66f4\u5168\u9762\u548c\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.24055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24055", "abs": "https://arxiv.org/abs/2510.24055", "authors": ["Xiucheng Zhang", "Yang Jiang", "Hongwei Qing", "Jiashuo Bai"], "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation", "comment": "8 pages", "summary": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation", "AI": {"tldr": "\u63d0\u51faLCVR\u548cLMoE-DP\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u57fa\u7840\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u9762\u4e34\u7684\u611f\u77e5\u6a21\u7cca\u548c\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u4efb\u52a1\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u5b58\u5728\u7684\u611f\u77e5\u6a21\u7cca\uff08\u89c6\u89c9\u76f8\u4f3c\u4efb\u52a1\u96be\u4ee5\u533a\u5206\uff09\u548c\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u89c6\u89c9\u8868\u793a(LCVR)\u6a21\u5757\u548c\u8bed\u8a00\u6761\u4ef6\u6df7\u5408\u4e13\u5bb6\u5bc6\u5ea6\u7b56\u7565(LMoE-DP)\u3002LCVR\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u5bf9\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u57fa\u7840\uff0c\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u4efb\u52a1\uff1bLMoE-DP\u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u67b6\u6784\u4e13\u95e8\u5904\u7406\u4e0d\u540c\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u8c03\u5236\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLCVR\u5c06Action Chunking with Transformers (ACT)\u548cDiffusion Policy (DP)\u7684\u6210\u529f\u7387\u5206\u522b\u63d0\u534733.75%\u548c25%\u3002\u5b8c\u6574\u6846\u67b6\u8fbe\u523079%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u6bd4\u5148\u8fdb\u57fa\u7ebf\u63d0\u534721%\u3002", "conclusion": "\u7ed3\u5408\u8bed\u4e49\u57fa\u7840\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u3001\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u64cd\u4f5c\uff0c\u4e3a\u591a\u4efb\u52a1\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24067", "abs": "https://arxiv.org/abs/2510.24067", "authors": ["Tianyi Ding", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "title": "Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition", "comment": null, "summary": "This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u81ea\u4e3b\u5728\u7ebf\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\u5b9e\u73b0\u5e73\u8861\u7684\u533a\u57df\u5212\u5206\u548c\u4efb\u52a1\u5206\u914d\uff0c\u5728\u969c\u788d\u5bc6\u96c6\u7684\u975e\u51f8\u73af\u5883\u4e2d\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u969c\u788d\u5bc6\u96c6\u975e\u51f8\u73af\u5883\u4e2d\u7684\u534f\u540c\u63a2\u7d22\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u5206\u5e03\u5f0f\u63a2\u7d22\u89c4\u5212\u4e2d\u7684\u52a8\u6001\u5e73\u8861\u533a\u57df\u5212\u5206\u548c\u4efb\u52a1\u5206\u914d\u6311\u6218\u3002", "method": "\u4f7f\u7528\u589e\u91cf\u66f4\u65b0\u7684\u62d3\u6251\u56fe\u7ed3\u6784\u8868\u5f81\u7a7a\u95f4\u8fde\u901a\u6027\u548c\u5168\u5c40\u63a2\u7d22\u5b8c\u6574\u6027\uff1b\u63d0\u51fa\u5206\u5e03\u5f0f\u52a0\u6743\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\u5b9e\u73b0\u5e73\u8861\u56fe\u7a7a\u95f4\u5212\u5206\uff1b\u5c40\u90e8\u89c4\u5212\u5668\u4f18\u5316\u63a2\u7d22\u76ee\u6807\u8bbf\u95ee\u5e8f\u5217\u5e76\u751f\u6210\u5b89\u5168\u5e73\u6ed1\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u63a2\u7d22\u6548\u7387\u3001\u5b8c\u6574\u6027\u548c\u673a\u5668\u4eba\u56e2\u961f\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u534f\u540c\u63a2\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5e73\u8861\u7684\u5206\u5e03\u5f0f\u81ea\u4e3b\u63a2\u7d22\u3002"}}
{"id": "2510.24069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f4d\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u8db3\u5f0f\u673a\u5668\u4eba\u5728\u6574\u4e2a\u8f68\u8ff9\u4e2d\u6ee1\u8db3\u5e73\u79fb\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u751f\u6210\u53ef\u9760\u8fd0\u52a8\u9700\u8981\u540c\u65f6\u8ba1\u7b97\u8def\u5f84\u548c\u63a5\u89e6\u5e8f\u5217\uff0c\u5e76\u51c6\u786e\u8003\u8651\u52a8\u529b\u5b66\u95ee\u9898\u3002", "method": "\u5229\u7528\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7684\u53e0\u52a0\u7279\u6027\u89e3\u8026\u5404\u63a5\u89e6\u70b9\u7684\u5e73\u79fb\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u8d1d\u585e\u5c14\u591a\u9879\u5f0f\u7684\u5fae\u5206\u77e9\u9635\u5efa\u7acb\u4f4d\u7f6e\u4e0e\u529b\u7684\u89e3\u6790\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u8d1d\u585e\u5c14\u591a\u9879\u5f0f\u7684\u51f8\u5305\u7279\u6027\u786e\u4fdd\u6469\u64e6\u9525\u7ea6\u675f\u3002", "result": "\u8be5\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u80fd\u591f\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u751f\u6210\u5177\u6709\u5404\u79cd\u6b65\u6001\u5e8f\u5217\u7684\u52a8\u6001\u53ef\u9760\u8fd0\u52a8\u3002", "conclusion": "\u901a\u8fc7\u56db\u8db3\u673a\u5668\u4eba\u6a21\u578b\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u52a8\u529b\u5b66\u53ef\u884c\u6027\u548c\u8fd0\u52a8\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24108", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.", "AI": {"tldr": "ZTRS\u662f\u9996\u4e2a\u5b8c\u5168\u6d88\u9664\u6a21\u4eff\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u4ec5\u901a\u8fc7\u5956\u52b1\u5b66\u4e60\u76f4\u63a5\u5904\u7406\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\uff0c\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u63d0\u51fa\u7684\u8be6\u5c3d\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u4f46\u53d7\u9650\u4e8e\u6b21\u4f18\u4e13\u5bb6\u6f14\u793a\u548c\u90e8\u7f72\u65f6\u7684\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\u3002\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u5229\u7528\u6a21\u62df\u73af\u5883\u6269\u5c55\uff0c\u4f46\u901a\u5e38\u5c40\u9650\u4e8e\u4f4e\u7ef4\u7b26\u53f7\u8f93\u5165\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "method": "\u63d0\u51faZTRS\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u8be6\u5c3d\u7b56\u7565\u4f18\u5316(EPO)\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u53ef\u679a\u4e3e\u52a8\u4f5c\u548c\u5956\u52b1\u7684\u7b56\u7565\u68af\u5ea6\u53d8\u4f53\uff0c\u5b8c\u5168\u6d88\u9664\u6a21\u4eff\u5b66\u4e60\uff0c\u4ec5\u4ece\u5956\u52b1\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aNavtest\uff08\u901a\u7528\u73b0\u5b9e\u4e16\u754c\u5f00\u73af\u89c4\u5212\uff09\u3001Navhard\uff08\u6311\u6218\u6027\u73b0\u5b9e\u4e16\u754c\u548c\u5408\u6210\u573a\u666f\u5f00\u73af\u89c4\u5212\uff09\u548cHUGSIM\uff08\u6a21\u62df\u95ed\u73af\u9a7e\u9a76\uff09\u3002\u5728Navhard\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728HUGSIM\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ZTRS\u6210\u529f\u5c55\u793a\u4e86\u5b8c\u5168\u6d88\u9664\u6a21\u4eff\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u7ed3\u5408\u4f20\u611f\u5668\u8f93\u5165\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u3002"}}
{"id": "2510.24109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24109", "abs": "https://arxiv.org/abs/2510.24109", "authors": ["Wenbin Ding", "Jun Chen", "Mingjia Chen", "Fei Xie", "Qi Mao", "Philip Dames"], "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u673a\u5668\u4eba\u5177\u8eab\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u3001\u89c6\u89c9\u8bed\u8a00\u667a\u80fd\u4f53\u548c\u52a8\u4f5c\u6267\u884c\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u5728\u7ebf\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u5bf9\u673a\u5668\u4eba\u7684\u667a\u80fd\u6c34\u5e73\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3001\u590d\u6742\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u65b9\u9762\u3002\u73b0\u6709\u7684LLM\u5177\u8eab\u667a\u80fd\u4f53\u5f80\u5f80\u7f3a\u4e4f\u5728\u7ebf\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u6a21\u5757\u3001\u89c6\u89c9\u8bed\u8a00\u667a\u80fd\u4f53\u6a21\u5757\u548c\u52a8\u4f5c\u6267\u884c\u6a21\u5757\u7684\u673a\u5668\u4eba\u5177\u8eab\u667a\u80fd\u4f53\u6846\u67b6\u3002\u89c6\u89c9\u8bed\u8a00\u667a\u80fd\u4f53\u5305\u62ec\u57fa\u4e8e\u89c6\u89c9\u7684\u4efb\u52a1\u89c4\u5212\u5668\u3001\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u5668\u548c\u4efb\u52a1\u6267\u884c\u53cd\u9988\u8bc4\u4f30\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u8be5\u667a\u80fd\u4f53\u7684\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u4ec5\u4f7f\u7528LLM+CLIP\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8628%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u673a\u5668\u4eba\u64cd\u4f5c\u667a\u80fd\u4f53\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2510.24118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo", "AI": {"tldr": "LagMemo\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u7684\u89c6\u89c9\u5bfc\u822a\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u548c\u591a\u76ee\u6807\u5bfc\u822a\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e003D\u8bed\u8a00\u8bb0\u5fc6\u548c\u5c40\u90e8\u611f\u77e5\u9a8c\u8bc1\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u76ee\u6807\u3001\u5355\u6a21\u6001\u548c\u5c01\u95ed\u96c6\u76ee\u6807\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u548c\u591a\u76ee\u6807\u5bfc\u822a\u7684\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u5728\u63a2\u7d22\u9636\u6bb5\u6784\u5efa\u7edf\u4e003D\u8bed\u8a00\u8bb0\u5fc6\uff0c\u901a\u8fc7\u67e5\u8be2\u8bb0\u5fc6\u9884\u6d4b\u5019\u9009\u76ee\u6807\u4f4d\u7f6e\uff0c\u5e76\u96c6\u6210\u5c40\u90e8\u611f\u77e5\u9a8c\u8bc1\u673a\u5236\u5728\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5339\u914d\u548c\u9a8c\u8bc1\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLagMemo\u7684\u8bb0\u5fc6\u6a21\u5757\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5b9a\u4f4d\uff0c\u5728\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LagMemo\u901a\u8fc7\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u548c\u52a8\u6001\u9a8c\u8bc1\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u7684\u6311\u6218\u3002"}}
{"id": "2510.24194", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24194", "abs": "https://arxiv.org/abs/2510.24194", "authors": ["Ev Zisselman", "Mirco Mutti", "Shelly Francis-Meretzki", "Elisei Shafer", "Aviv Tamar"], "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames", "comment": null, "summary": "Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home", "AI": {"tldr": "\u63d0\u51fa\"\u8499\u773c\u4e13\u5bb6\"\u65b9\u6cd5\uff0c\u5728\u884c\u4e3a\u514b\u9686\u4e2d\u5411\u6f14\u793a\u8005\u9690\u85cf\u90e8\u5206\u4efb\u52a1\u4fe1\u606f\uff0c\u8feb\u4f7f\u6f14\u793a\u8005\u8fdb\u884c\u975e\u5e73\u51e1\u63a2\u7d22\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u884c\u4e3a\u514b\u9686\u9700\u8981\u5927\u91cf\u4efb\u52a1\u6f14\u793a\u6765\u83b7\u5f97\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u4eba\u7c7b\u4e13\u5bb6\u901a\u5e38\u62e5\u6709\u5b8c\u6574\u4efb\u52a1\u4fe1\u606f\uff0c\u6f14\u793a\u7684\u662f\u8fd1\u4e4e\u6700\u4f18\u884c\u4e3a\u3002\u672c\u6587\u63a2\u7d22\u901a\u8fc7\u9650\u5236\u6f14\u793a\u8005\u4fe1\u606f\u6765\u6539\u8fdb\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u5411\u6f14\u793a\u8005\u9690\u85cf\u90e8\u5206\u4efb\u52a1\u4fe1\u606f\uff0c\u521b\u5efa\"\u8499\u773c\u4e13\u5bb6\"\uff0c\u8feb\u4f7f\u4ed6\u4eec\u8fdb\u884c\u63a2\u7d22\u6027\u884c\u4e3a\u3002\u514b\u9686\u8fd9\u4e9b\u8499\u773c\u4e13\u5bb6\u7684\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u5b8c\u5168\u77e5\u60c5\u4e13\u5bb6\u7684\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u514b\u9686\u8499\u773c\u4e13\u5bb6\u6bd4\u514b\u9686\u5b8c\u5168\u77e5\u60c5\u4e13\u5bb6\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u6cdb\u5316\u5f97\u66f4\u597d\u3002\u5728\u771f\u5b9e\u673a\u5668\u4eba\u63d2\u5b54\u4efb\u52a1\u548cProcgen\u57fa\u51c6\u89c6\u9891\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u6548\u679c\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u90fd\u8868\u660e\uff0c\u514b\u9686\u8499\u773c\u4e13\u5bb6\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u6f14\u793a\u4efb\u52a1\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\uff0c\u6cdb\u5316\u8bef\u5dee\u4e0e\u6f14\u793a\u8005\u53ef\u83b7\u5f97\u7684\u4efb\u52a1\u4fe1\u606f\u91cf\u5e73\u65b9\u6839\u6210\u6b63\u6bd4\u3002"}}
{"id": "2510.24257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24257", "abs": "https://arxiv.org/abs/2510.24257", "authors": ["Ziqi Ma", "Changda Tian", "Yue Gao"], "title": "Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors", "comment": null, "summary": "In recent years, there has been growing interest in developing robots and\nautonomous systems that can interact with human in a more natural and intuitive\nway. One of the key challenges in achieving this goal is to enable these\nsystems to manipulate objects and tools in a manner that is similar to that of\nhumans. In this paper, we propose a novel approach for learning human-style\nmanipulation skills by using adversarial motion priors, which we name HMAMP.\nThe approach leverages adversarial networks to model the complex dynamics of\ntool and object manipulation, as well as the aim of the manipulation task. The\ndiscriminator is trained using a combination of real-world data and simulation\ndata executed by the agent, which is designed to train a policy that generates\nrealistic motion trajectories that match the statistical properties of human\nmotion. We evaluated HMAMP on one challenging manipulation task: hammering, and\nthe results indicate that HMAMP is capable of learning human-style manipulation\nskills that outperform current baseline methods. Additionally, we demonstrate\nthat HMAMP has potential for real-world applications by performing real robot\narm hammering tasks. In general, HMAMP represents a significant step towards\ndeveloping robots and autonomous systems that can interact with humans in a\nmore natural and intuitive way, by learning to manipulate tools and objects in\na manner similar to how humans do.", "AI": {"tldr": "\u63d0\u51faHMAMP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u5b66\u4e60\u4eba\u7c7b\u98ce\u683c\u7684\u64cd\u7eb5\u6280\u80fd\uff0c\u5728\u9524\u51fb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u4ee5\u66f4\u81ea\u7136\u76f4\u89c2\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u4e92\u52a8\u7684\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\uff0c\u5173\u952e\u6311\u6218\u662f\u8ba9\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u591f\u4ee5\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65b9\u5f0f\u64cd\u7eb5\u7269\u4f53\u548c\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u7f51\u7edc\u5efa\u6a21\u5de5\u5177\u548c\u7269\u4f53\u64cd\u7eb5\u7684\u590d\u6742\u52a8\u529b\u5b66\uff0c\u5224\u522b\u5668\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u6570\u636e\u548c\u667a\u80fd\u4f53\u6267\u884c\u7684\u6a21\u62df\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u8fd0\u52a8\u7edf\u8ba1\u7279\u6027\u7684\u771f\u5b9e\u8fd0\u52a8\u8f68\u8ff9\u7684\u7b56\u7565\u3002", "result": "\u5728\u9524\u51fb\u4efb\u52a1\u4e2d\uff0cHMAMP\u80fd\u591f\u5b66\u4e60\u4eba\u7c7b\u98ce\u683c\u7684\u64cd\u7eb5\u6280\u80fd\uff0c\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u624b\u81c2\u9524\u51fb\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "HMAMP\u4ee3\u8868\u4e86\u671d\u7740\u5f00\u53d1\u80fd\u591f\u4ee5\u66f4\u81ea\u7136\u76f4\u89c2\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u4e92\u52a8\u673a\u5668\u4eba\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u901a\u8fc7\u5b66\u4e60\u4ee5\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65b9\u5f0f\u64cd\u7eb5\u5de5\u5177\u548c\u7269\u4f53\u3002"}}
{"id": "2510.24261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24261", "abs": "https://arxiv.org/abs/2510.24261", "authors": ["Jingyi Tian", "Le Wang", "Sanping Zhou", "Sen Wang", "Jiayi Li", "Gang Hua"], "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation", "comment": "Accepted to NeurIPS 2025", "summary": "Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.", "AI": {"tldr": "DynaRend\u662f\u4e00\u4e2a\u901a\u8fc7\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u5b66\u4e603D\u611f\u77e5\u548c\u52a8\u6001\u611f\u77e5\u7684\u4e09\u5e73\u9762\u7279\u5f81\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u56e0\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u800c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d562D\u89c6\u89c9\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u8981\u4e48\u4f7f\u7528\u5927\u89c4\u6a21\u89c6\u9891\u9884\u6d4b\u6a21\u578b\uff0c\u4f46\u90fd\u672a\u80fd\u8054\u5408\u5b66\u4e60\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u52a8\u6001\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u63a9\u7801\u91cd\u5efa\u548c\u672a\u6765\u9884\u6d4b\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u5b66\u4e603D\u611f\u77e5\u548c\u52a8\u6001\u611f\u77e5\u7684\u4e09\u5e73\u9762\u7279\u5f81\u3002\u5728\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8054\u5408\u6355\u83b7\u7a7a\u95f4\u51e0\u4f55\u3001\u672a\u6765\u52a8\u6001\u548c\u4efb\u52a1\u8bed\u4e49\u3002", "result": "\u5728RLBench\u548cColosseum\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cDynaRend\u5728\u7b56\u7565\u6210\u529f\u7387\u3001\u5bf9\u73af\u5883\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u548c\u771f\u5b9e\u4e16\u754c\u9002\u7528\u6027\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DynaRend\u80fd\u591f\u6709\u6548\u5b66\u4e60\u7edf\u4e00\u76843D\u611f\u77e5\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24315", "abs": "https://arxiv.org/abs/2510.24315", "authors": ["Baozhe Zhang", "Xinwei Chen", "Qingcheng Chen", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation", "comment": null, "summary": "CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios.", "AI": {"tldr": "\u63d0\u51faCoNi-OA\u7b97\u6cd5\uff0c\u4e3a\u65e0\u4eba\u673a-\u65e0\u4eba\u8f66\u534f\u540c\u573a\u666f\u8bbe\u8ba1\uff0c\u4ec5\u4f7f\u7528\u5355\u5e27LiDAR\u6570\u636e\u751f\u6210\u8c03\u5236\u77e9\u9635\u6765\u8c03\u6574\u65e0\u4eba\u673a\u901f\u5ea6\u5b9e\u73b0\u907f\u969c\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\u3002", "motivation": "CoNi-MPC\u6846\u67b6\u5728\u7a7a\u5730\u534f\u540c\u4efb\u52a1\u4e2d\u4ec5\u4f9d\u8d56\u76f8\u5bf9\u72b6\u6001\uff0c\u4f46\u7f3a\u4e4f\u73af\u5883\u4fe1\u606f\u5bfc\u81f4\u907f\u969c\u56f0\u96be\u3002\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\u7684\u907f\u969c\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u65e0\u4eba\u673a\u5355\u5e27\u539f\u59cbLiDAR\u6570\u636e\u751f\u6210\u8c03\u5236\u77e9\u9635\uff0c\u76f4\u63a5\u8c03\u6574\u56db\u65cb\u7ffc\u901f\u5ea6\u5b9e\u73b0\u907f\u969c\u3002\u57fa\u4e8e\u8c03\u5236\u7684\u65b9\u6cd5\u5728\u65e0\u4eba\u8f66\u975e\u60ef\u6027\u7cfb\u4e2d\u5b9e\u65f6\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "result": "\u8ba1\u7b97\u9700\u6c42\u663e\u8457\u964d\u4f4e\uff08\u6bcf\u6b21\u8fed\u4ee3\u5c0f\u4e8e5\u6beb\u79d2\uff09\uff0c\u5728\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u4fdd\u6301\u5b89\u5168\u6027\uff0c\u9002\u5e94\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u3002", "conclusion": "CoNi-OA\u7b97\u6cd5\u4e3a\u65e0\u4eba\u673a-\u65e0\u4eba\u8f66\u534f\u540c\u63d0\u4f9b\u9ad8\u6548\u907f\u969c\u65b9\u6848\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u6216\u969c\u788d\u7269\u9884\u6d4b\uff0c\u5728\u7279\u5f81\u7f3a\u5931\u6216\u672a\u77e5\u573a\u666f\u4e2d\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\u3002"}}
{"id": "2510.24335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24335", "abs": "https://arxiv.org/abs/2510.24335", "authors": ["Mingyu Jeong", "Eunsung Kim", "Sehun Park", "Andrew Jaeyong Choi"], "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation", "comment": "9 pages, 10 figures", "summary": "We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8", "AI": {"tldr": "NVSim\u662f\u4e00\u4e2a\u4ece\u666e\u901a\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u53ef\u5bfc\u822a\u5ba4\u5185\u6a21\u62df\u5668\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3D\u626b\u63cf\u7684\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002", "motivation": "\u4f20\u7edf3D\u626b\u63cf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u673a\u5668\u4eba\u904d\u5386\u6570\u636e\u4e2d\u7a00\u758f\u89c2\u6d4b\u5730\u677f\u5bfc\u81f4\u7684\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5f15\u5165\u5730\u677f\u611f\u77e5\u9ad8\u65af\u6cfc\u6e85\u786e\u4fdd\u6e05\u6d01\u53ef\u5bfc\u822a\u7684\u5730\u5e73\u9762\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u65e0\u7f51\u683c\u53ef\u904d\u5386\u6027\u68c0\u67e5\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5206\u6790\u6e32\u67d3\u89c6\u56fe\u6784\u5efa\u62d3\u6251\u56fe\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4ece\u771f\u5b9e\u4e16\u754c\u6570\u636e\u751f\u6210\u6709\u6548\u7684\u5927\u89c4\u6a21\u5bfc\u822a\u56fe\u3002", "conclusion": "NVSim\u6846\u67b6\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u666e\u901a\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u53ef\u5bfc\u822a\u5ba4\u5185\u6a21\u62df\u5668\u3002"}}
{"id": "2510.24457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24457", "abs": "https://arxiv.org/abs/2510.24457", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance", "comment": "8 pages, 11 figures", "summary": "This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u6027\u76843D\u6865\u5f0f\u8d77\u91cd\u673a\u6700\u4f18\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u5904\u7406\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\u7b49\u590d\u6742\u7ea6\u675f\uff0c\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u7684\u8d77\u91cd\u673a\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u8d77\u91cd\u673a\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\u7b49\u590d\u6742\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u6267\u884c\u5668\u9971\u548c\u548c\u78b0\u649e\u98ce\u9669\uff0c\u5f71\u54cd\u8d77\u91cd\u673a\u8fd0\u52a8\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u5229\u7528\u5fae\u5206\u5e73\u5766\u6027\u6846\u67b6\uff0c\u76f4\u63a5\u7eb3\u5165\u975e\u7ebf\u6027\u6469\u64e6\u548c\u8d1f\u8f7d\u7ef3\u7d22\u78b0\u649e\u907f\u514d\u7b49\u590d\u6742\u7269\u7406\u52a8\u6001\u7ea6\u675f\uff0c\u4ec5\u5728\u7ec8\u70b9\u7ea6\u675f\u8d1f\u8f7d\u6446\u52a8\u4ee5\u5b9e\u73b0\u6fc0\u8fdb\u8fd0\u52a8\u3002", "result": "\u5bf9\u6bd4\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u5ffd\u7565\u5e72\u6469\u64e6\u4f1a\u5bfc\u81f4\u6267\u884c\u5668\u9971\u548c\u548c\u78b0\u649e\uff0c\u800c\u6469\u64e6\u5efa\u6a21\u662f\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u8d77\u91cd\u673a\u8f68\u8ff9\u7684\u57fa\u672c\u8981\u6c42\u3002", "conclusion": "\u6469\u64e6\u5efa\u6a21\u5bf9\u4e8e\u8d77\u91cd\u673a\u5feb\u901f\u5b89\u5168\u8f68\u8ff9\u751f\u6210\u81f3\u5173\u91cd\u8981\uff0c\u6240\u63d0\u51fa\u7684\u5fae\u5206\u5e73\u5766\u6027\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u7ea6\u675f\uff0c\u63d0\u5347\u8d77\u91cd\u673a\u8fd0\u52a8\u6027\u80fd\u3002"}}
{"id": "2510.24508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24508", "abs": "https://arxiv.org/abs/2510.24508", "authors": ["Haoying Li", "Yifan Peng", "Junfeng Wu"], "title": "Supervisory Measurement-Guided Noise Covariance Estimation", "comment": null, "summary": "Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u89c6\u89d2\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u95ee\u9898\u5206\u89e3\u4e3a\u91cc\u7a0b\u8ba1\u548c\u76d1\u63a7\u6d4b\u91cf\u7684\u8054\u5408\u4f3c\u7136\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u4fe1\u606f\u5229\u7528\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u51c6\u786e\u786e\u5b9a\uff0c\u53d7\u73af\u5883\u53d8\u5316\u3001\u524d\u7aef\u5904\u7406\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u800c\u53ef\u9760\u7684\u59ff\u6001\u4f30\u8ba1\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u566a\u58f0\u534f\u65b9\u5dee\u89c4\u8303\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff1a\u4e0b\u5c42\u4f7f\u7528\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u8f68\u8ff9\u4f30\u8ba1\uff0c\u4e0a\u5c42\u901a\u8fc7\u68af\u5ea6\u66f4\u65b0\u4f18\u5316\u534f\u65b9\u5dee\uff1b\u901a\u8fc7\u56e0\u5b50\u5206\u89e3\u5c06\u5d4c\u5957\u8d1d\u53f6\u65af\u4f9d\u8d56\u8f6c\u6362\u4e3a\u94fe\u5f0f\u7ed3\u6784\uff0c\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.24515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24515", "abs": "https://arxiv.org/abs/2510.24515", "authors": ["Malintha Fernando", "Petter \u00d6gren", "Silun Zhang"], "title": "Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems", "comment": "Submitted to IEEE Robotics and Automation Letters", "summary": "The Team Orienteering Problem (TOP) generalizes many real-world multi-robot\nscheduling and routing tasks that occur in autonomous mobility, aerial\nlogistics, and surveillance applications. While many flavors of the TOP exist\nfor planning in multi-robot systems, they assume that all the robots cooperate\ntoward a single objective; thus, they do not extend to settings where the\nrobots compete in reward-scarce environments. We propose Stochastic\nPrize-Collecting Games (SPCG) as an extension of the TOP to plan in the\npresence of self-interested robots operating on a graph, under energy\nconstraints and stochastic transitions. A theoretical study on complete and\nstar graphs establishes that there is a unique pure Nash equilibrium in SPCGs\nthat coincides with the optimal routing solution of an equivalent TOP given a\nrank-based conflict resolution rule. This work proposes two algorithms: Ordinal\nRank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in\ntemporarily-formed local neighborhoods during the games' stages, and Fictitious\nOrdinal Response Learning (FORL) to obtain best-response policies against one's\nsenior-rank opponents. Empirical evaluations conducted on road networks and\nsynthetic graphs under both dynamic and stationary prize distributions show\nthat 1) the state-aliasing induced by OR-conditioning enables learning policies\nthat scale more efficiently to large team sizes than those trained with the\nglobal index, and 2) Policies trained with FORL generalize better to imbalanced\nprize distributions than those with other multi-agent training methods.\nFinally, the learned policies in the SPCG achieved between 87% and 95%\noptimality compared to an equivalent TOP solution obtained by mixed-integer\nlinear programming.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u968f\u673a\u5956\u52b1\u6536\u96c6\u6e38\u620f(SPCG)\u4f5c\u4e3a\u56e2\u961f\u5b9a\u5411\u95ee\u9898(TOP)\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u5728\u81ea\u5229\u673a\u5668\u4eba\u3001\u80fd\u91cf\u7ea6\u675f\u548c\u968f\u673a\u8f6c\u79fb\u6761\u4ef6\u4e0b\u8fdb\u884c\u89c4\u5212\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b58\u5728\u7eaf\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u7b97\u6cd5\u6765\u5b66\u4e60\u6709\u6548\u7684\u7ade\u4e89\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u56e2\u961f\u5b9a\u5411\u95ee\u9898(TOP)\u53d8\u4f53\u5047\u8bbe\u6240\u6709\u673a\u5668\u4eba\u5408\u4f5c\u5b9e\u73b0\u5355\u4e00\u76ee\u6807\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u5956\u52b1\u7a00\u7f3a\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7ade\u4e89\u7684\u573a\u666f\u3002\u9700\u8981\u89e3\u51b3\u81ea\u5229\u673a\u5668\u4eba\u5728\u56fe\u7ed3\u6784\u4e0a\u7ade\u4e89\u6027\u89c4\u5212\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u5e8f\u6570\u6392\u540d\u641c\u7d22(ORS)\u7528\u4e8e\u83b7\u53d6\u6e38\u620f\u9636\u6bb5\u4e2d\u4e34\u65f6\u5f62\u6210\u7684\u5c40\u90e8\u90bb\u57df\u4e2d\u7684\u6709\u6548\u6392\u540d\uff0c\u4ee5\u53ca\u865a\u6784\u5e8f\u6570\u54cd\u5e94\u5b66\u4e60(FORL)\u7528\u4e8e\u9488\u5bf9\u9ad8\u6392\u540d\u5bf9\u624b\u5b66\u4e60\u6700\u4f73\u54cd\u5e94\u7b56\u7565\u3002", "result": "\u5728\u9053\u8def\u7f51\u7edc\u548c\u5408\u6210\u56fe\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff1a1) ORS\u7684\u72b6\u6001\u6df7\u53e0\u4f7f\u5b66\u4e60\u7b56\u7565\u6bd4\u4f7f\u7528\u5168\u5c40\u7d22\u5f15\u7684\u7b56\u7565\u66f4\u80fd\u6269\u5c55\u5230\u5927\u89c4\u6a21\u56e2\u961f\uff1b2) FORL\u8bad\u7ec3\u7684\u7b56\u7565\u6bd4\u5176\u4ed6\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\u5728\u975e\u5e73\u8861\u5956\u52b1\u5206\u5e03\u4e0b\u6cdb\u5316\u80fd\u529b\u66f4\u597d\uff1b3) \u5b66\u4e60\u7b56\u7565\u8fbe\u5230\u4e8687%-95%\u7684\u6700\u4f18\u6027\u3002", "conclusion": "SPCG\u6846\u67b6\u6210\u529f\u6269\u5c55\u4e86TOP\u4ee5\u5904\u7406\u7ade\u4e89\u6027\u591a\u673a\u5668\u4eba\u89c4\u5212\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5b66\u4e60\u7b56\u7565\u63a5\u8fd1\u6700\u4f18\u89e3\u3002"}}
{"id": "2510.24533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24533", "abs": "https://arxiv.org/abs/2510.24533", "authors": ["Yuan Shen", "Yuze Hong", "Guangyang Zeng", "Tengfei Zhang", "Pui Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots", "comment": null, "summary": "Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods.", "AI": {"tldr": "GeVI-SLAM\u662f\u4e00\u79cd\u91cd\u529b\u589e\u5f3a\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u7acb\u4f53\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u548c\u91cd\u529b\u521d\u59cb\u5316\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u673a\u5668\u4ebaSLAM\u4e2d\u7684\u89c6\u89c9\u9000\u5316\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4ebaVI SLAM\u9762\u4e34\u9891\u7e41\u89c6\u89c9\u9000\u5316\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u548c\u5efa\u56fe\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u7acb\u4f53\u76f8\u673a\u76f4\u63a5\u6df1\u5ea6\u4f30\u8ba1\u6d88\u9664\u5c3a\u5ea6\u4f30\u8ba1\u9700\u6c42\uff1b\u901a\u8fc7\u7cbe\u786e\u91cd\u529b\u521d\u59cb\u5316\u89e3\u8026\u4fef\u4ef0\u548c\u6eda\u8f6c\uff0c\u91c7\u75284\u81ea\u7531\u5ea6PnP\u95ee\u9898\u6c42\u89e3\uff1b\u63d0\u51fa\u504f\u5dee\u6d88\u9664\u76844-DOF PnP\u4f30\u8ba1\u5668\uff1b\u52a8\u6001\u8fd0\u52a8\u65f6\u8054\u5408\u4f30\u8ba1IMU\u534f\u65b9\u5dee\u8fdb\u884c6-DOF\u4f4d\u59ff\u4f18\u5316\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGeVI-SLAM\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "GeVI-SLAM\u901a\u8fc7\u91cd\u529b\u589e\u5f3a\u548c\u521b\u65b0\u76844-DOF PnP\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0bVI SLAM\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24554", "abs": "https://arxiv.org/abs/2510.24554", "authors": ["Vignesh Kottayam Viswanathan", "Yifan Bai", "Scott Fredriksson", "Sumeet Satpute", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments", "comment": "Submitted for ICRA 2026", "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u6846\u67b6\u89e3\u51b3\u673a\u5668\u4eba\u5de1\u68c0\u4e2d\u7684\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u5168\u5c40\u89c4\u5212\u4e0e\u5c40\u90e8\u91cd\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u8986\u76d6\u76ee\u6807\u7684\u540c\u65f6\u9002\u5e94\u5c40\u90e8\u5730\u5f62\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5df2\u77e5\u73af\u5883\u6a21\u578b\u89c4\u5212\u5de1\u68c0\u8def\u7ebf\uff0c\u4f46\u5b9e\u9645\u73af\u5883\u4e0e\u6a21\u578b\u5b58\u5728\u5dee\u5f02\uff08\u81ea\u7136\u6216\u4eba\u4e3a\u56e0\u7d20\u5bfc\u81f4\u5730\u5f62\u53d8\u5316\u6216\u8def\u5f84\u963b\u585e\uff09\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u5c42\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u5386\u53f2\u5730\u56fe\u751f\u6210\u5168\u5c40\u89c6\u70b9\u89c4\u5212\uff1b2\uff09\u6839\u636e\u5f53\u524d\u573a\u666f\u5f62\u6001\u8fdb\u884c\u5c40\u90e8\u89c6\u70b9\u91cd\u89c4\u5212\u3002\u8be5\u5c42\u6b21\u7ed3\u6784\u4fdd\u6301\u5168\u5c40\u8986\u76d6\u76ee\u6807\uff0c\u540c\u65f6\u5b9e\u73b0\u5bf9\u5c40\u90e8\u8868\u9762\u5f62\u6001\u7684\u9002\u5e94\u6027\u8c03\u6574\u3002", "result": "\u5728\u771f\u5b9e\u5730\u4e0b\u77ff\u4e95\u73af\u5883\u4e2d\u4f7f\u7528\u56db\u8db3\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u90e8\u7f72\u9a8c\u8bc1\uff0c\u8bc1\u660e\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u5e76\u5b8c\u6210\u5de1\u68c0\u4efb\u52a1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5c42\u6846\u67b6\u80fd\u591f\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u4f7f\u5c40\u90e8\u81ea\u4e3b\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u73af\u5883\u53d8\u5316\u5e76\u6210\u529f\u5b8c\u6210\u5de1\u68c0\u4efb\u52a1\u3002"}}
{"id": "2510.24571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24571", "abs": "https://arxiv.org/abs/2510.24571", "authors": ["Hongxu Zhao", "Guangyang Zeng", "Yunling Shao", "Tengfei Zhang", "Junfeng Wu"], "title": "Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots", "comment": null, "summary": "The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u8fed\u4ee3\u6821\u51c6(UIC)\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u7684\u5916\u53c2\u548c\u65f6\u95f4\u504f\u79fb\u8054\u5408\u4f30\u8ba1\uff0c\u7279\u522b\u9488\u5bf9\u6c34\u4e0bSLAM\u4e2d\u7684DVL\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709DVL\u6821\u51c6\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u8981\u4e48\u4f9d\u8d56\u8fc7\u5ea6\u7b80\u5316\u7684\u5047\u8bbe\uff0c\u4e14\u6ca1\u6709\u540c\u65f6\u4f30\u8ba1\u5e73\u79fb\u5916\u53c2\u548c\u65f6\u95f4\u504f\u79fb\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6700\u5927\u540e\u9a8c\u6982\u7387\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u8fd0\u52a8\u5148\u9a8c\u8fdb\u884c\u9ad8\u4fdd\u771f\u8fd0\u52a8\u63d2\u503c\uff0c\u901a\u8fc7\u4ea4\u66ff\u6267\u884cGP\u8fd0\u52a8\u72b6\u6001\u66f4\u65b0\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u6821\u51c6\u53d8\u91cf\u66f4\u65b0\u3002", "result": "\u5f00\u53d1\u4e86\u5f00\u6e90\u7684DVL-\u76f8\u673a\u6821\u51c6\u5de5\u5177\u7bb1\uff0c\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "UIC\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u6c34\u4e0b\u5e94\u7528\uff0c\u5176GP\u5148\u9a8c\u96c6\u6210\u548c\u53ef\u9760\u521d\u59cb\u5316\u7a0b\u5e8f\u7684\u8bbe\u8ba1\u4e5f\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u591a\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\u3002"}}
{"id": "2510.24584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24584", "abs": "https://arxiv.org/abs/2510.24584", "authors": ["J\u00f8rgen Anker Olsen", "Lars R\u00f8nhaug Pettersen", "Kostas Alexis"], "title": "Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning", "comment": "8 pages", "summary": "This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u673a\u5668\u4ebaOlympus\u5b9e\u73b0\u7cbe\u786e\u9ad8\u6027\u80fd\u8df3\u8dc3\uff0c\u5305\u62ec\u5782\u76f4\u548c\u6c34\u5e73\u8df3\u8dc3\u7b56\u7565\uff0c\u7ed3\u5408\u884c\u8d70\u7b56\u7565\u5b9e\u73b0\u591a\u529f\u80fd\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8df3\u8dc3\u4efb\u52a1\u4e2d\u5956\u52b1\u7a00\u758f\u548c\u52a8\u6001\u884c\u4e3a\u63a2\u7d22\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u65e0\u9700\u53c2\u8003\u8f68\u8ff9\u7684\u9ad8\u6027\u80fd\u8df3\u8dc3\u7b56\u7565\uff0c\u5b9e\u73b0Sim2Real\u7684\u6709\u6548\u8fc1\u79fb\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f39\u9053\u8fd0\u52a8\u5b9a\u5f8b\u7a20\u5bc6\u5316\u7a00\u758f\u5956\u52b1\uff0c\u91c7\u7528\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\u65b9\u6848\u52a0\u901f\u52a8\u6001\u8df3\u8dc3\u884c\u4e3a\u63a2\u7d22\uff0c\u7ed3\u5408\u884c\u8d70\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u591a\u6837\u5316\u5730\u5f62\u4e0a\u7684\u884c\u8d70\u80fd\u529b\uff0c\u6c34\u5e73\u8df3\u8dc3\u8fbe1.25\u7c73(\u5398\u7c73\u7ea7\u7cbe\u5ea6)\uff0c\u5782\u76f4\u8df3\u8dc3\u8fbe1.0\u7c73\uff0c\u6027\u80fd\u8d85\u8d8a\u5148\u524d\u5de5\u4f5c\uff0c\u6210\u529f\u8de8\u8d8aSim2Real\u5dee\u8ddd\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u9ad8\u6027\u80fd\u8df3\u8dc3\u7b56\u7565\uff0c\u4ec5\u9700\u5c11\u91cf\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u5168\u5411\u8df3\u8dc3\uff0c\u4e3a\u673a\u5668\u4eba\u52a8\u6001\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24623", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24623", "abs": "https://arxiv.org/abs/2510.24623", "authors": ["Nicolai Steinke", "Daniel Goehring"], "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization", "comment": null, "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.", "AI": {"tldr": "GroundLoc\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528LiDAR\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7\u9e1f\u77b0\u56fe\u6295\u5f71\u548c\u5173\u952e\u70b9\u8bc6\u522b\u6280\u672f\uff0c\u5728\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u79fb\u52a8\u673a\u5668\u4eba\u7684\u7cbe\u786e\u5b9a\u4f4d\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684LiDAR\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5927\u578b\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u5b9e\u65f6\u5b9a\u4f4d\uff0c\u540c\u65f6\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u5e76\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u3002", "method": "\u4f7f\u7528\u9e1f\u77b0\u56fe\u6295\u5f71\u805a\u7126\u5730\u9762\u533a\u57df\uff0c\u91c7\u7528R2D2\u7f51\u7edc\u6216SIFT\u65b9\u6cd5\u8bc6\u522b\u5173\u952e\u70b9\u8fdb\u884c\u5730\u56fe\u914d\u51c6\uff0c\u5c06\u5148\u9a8c\u5730\u56fe\u5b58\u50a8\u4e3a2D\u6805\u683c\u56fe\u50cf\u3002", "result": "\u5728SemanticKITTI\u548cHeLiPR\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u591a\u4f1a\u8bdd\u5b9a\u4f4d\u7684\u5e73\u5747\u8f68\u8ff9\u8bef\u5dee\u4f4e\u4e8e50\u5398\u7c73\uff0c\u6ee1\u8db3\u5728\u7ebf\u8fd0\u884c\u8981\u6c42\uff0c\u6bcf\u5e73\u65b9\u516c\u91cc\u4ec5\u97004MB\u5b58\u50a8\u3002", "conclusion": "GroundLoc\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7684LiDAR\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7cbe\u5ea6\u3001\u5b58\u50a8\u6548\u7387\u548c\u4f20\u611f\u5668\u517c\u5bb9\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.24671", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24671", "abs": "https://arxiv.org/abs/2510.24671", "authors": ["Li Li", "Tobias Brinkmann", "Till Temmen", "Markus Eisenbarth", "Jakob Andert"], "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder", "comment": null, "summary": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u589e\u5f3a\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE-T)\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u73af\u5c9b\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u573a\u666f\uff0c\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u539f\u59cb\u573a\u666f\u5e76\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u5408\u6210\u573a\u666f\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u5728\u91cf\u4ea7\u8f66\u4e2d\u7684\u96c6\u6210\u5ea6\u63d0\u9ad8\uff0c\u786e\u4fdd\u5176\u529f\u80fd\u6027\u548c\u9c81\u68d2\u6027\u9762\u4e34\u66f4\u5927\u6311\u6218\u3002\u76f8\u6bd4\u4f20\u7edf\u9053\u8def\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u573a\u666f\u7684\u865a\u62df\u6d4b\u8bd5\u5728\u65f6\u95f4\u6210\u672c\u6548\u7387\u3001\u53ef\u91cd\u590d\u6027\u548c\u8fb9\u7f18\u6848\u4f8b\u63a2\u7d22\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "method": "\u4f7f\u7528Transformer\u589e\u5f3a\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE-T)\u6a21\u578b\uff0c\u9488\u5bf9\u5177\u6709\u9ad8\u8f66\u8f86\u52a8\u6001\u548c\u590d\u6742\u5e03\u5c40\u7684\u73af\u5c9b\u4ea4\u901a\u573a\u666f\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u573a\u666f\u751f\u6210\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u539f\u59cb\u573a\u666f\u5e76\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u5408\u6210\u573a\u666f\u3002\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u663e\u793a\u90e8\u5206\u89e3\u7f20\uff0c\u591a\u4e2a\u6f5c\u5728\u7ef4\u5ea6\u5bf9\u573a\u666f\u5c5e\u6027\uff08\u5982\u8f66\u8f86\u8fdb\u5165\u65f6\u95f4\u3001\u9000\u51fa\u65f6\u95f4\u548c\u901f\u5ea6\u66f2\u7ebf\uff09\u5177\u6709\u660e\u663e\u4e14\u53ef\u89e3\u91ca\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u4e3a\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u9a8c\u8bc1\u751f\u6210\u573a\u666f\uff0c\u540c\u65f6\u4e3a\u8fd9\u4e9b\u529f\u80fd\u7684\u5f00\u53d1\u548c\u8fed\u4ee3\u6539\u8fdb\u63d0\u4f9b\u6570\u636e\u589e\u5f3a\u3002"}}
{"id": "2510.24676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24676", "abs": "https://arxiv.org/abs/2510.24676", "authors": ["Jiaxuan Zhang", "Yuquan Leng", "Yixuan Guo", "Chenglong Fu"], "title": "Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis", "comment": "6 pages, conference", "summary": "For amputees with powered transfemoral prosthetics, navigating obstacles or\ncomplex terrain remains challenging. This study addresses this issue by using\nan inertial sensor on the sound ankle to guide obstacle-crossing movements. A\ngenetic algorithm computes the optimal neural network structure to predict the\nrequired angles of the thigh and knee joints. A gait progression prediction\nalgorithm determines the actuation angle index for the prosthetic knee motor,\nultimately defining the necessary thigh and knee angles and gait progression.\nResults show that when the standard deviation of Gaussian noise added to the\nthigh angle data is less than 1, the method can effectively eliminate noise\ninterference, achieving 100\\% accuracy in gait phase estimation under 150 Hz,\nwith thigh angle prediction error being 8.71\\% and knee angle prediction error\nbeing 6.78\\%. These findings demonstrate the method's ability to accurately\npredict gait progression and joint angles, offering significant practical value\nfor obstacle negotiation in powered transfemoral prosthetics.", "AI": {"tldr": "\u4f7f\u7528\u5065\u5eb7\u811a\u8e1d\u4e0a\u7684\u60ef\u6027\u4f20\u611f\u5668\u6307\u5bfc\u622a\u80a2\u8005\u8de8\u8d8a\u969c\u788d\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u6765\u9884\u6d4b\u5927\u817f\u548c\u819d\u5173\u8282\u89d2\u5ea6\uff0c\u5b9e\u73b0100%\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u622a\u80a2\u8005\u7a7f\u6234\u52a8\u529b\u578b\u7ecf\u80a1\u9aa8\u5047\u80a2\u65f6\u96be\u4ee5\u8de8\u8d8a\u969c\u788d\u548c\u590d\u6742\u5730\u5f62\u7684\u95ee\u9898\u3002", "method": "\u5728\u5065\u5eb7\u811a\u8e1d\u653e\u7f6e\u60ef\u6027\u4f20\u611f\u5668\uff0c\u7528\u9057\u4f20\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u9884\u6d4b\u5173\u8282\u89d2\u5ea6\uff0c\u7ed3\u5408\u6b65\u6001\u8fdb\u5c55\u9884\u6d4b\u7b97\u6cd5\u786e\u5b9a\u819d\u5173\u8282\u7535\u673a\u9a71\u52a8\u89d2\u5ea6\u3002", "result": "\u5f53\u5927\u817f\u89d2\u5ea6\u6570\u636e\u6dfb\u52a0\u7684\u9ad8\u65af\u566a\u58f0\u6807\u51c6\u5dee\u5c0f\u4e8e1\u65f6\uff0c\u80fd\u6709\u6548\u6d88\u9664\u566a\u58f0\u5e72\u6270\uff0c\u5728150Hz\u4e0b\u5b9e\u73b0100%\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u51c6\u786e\u7387\uff0c\u5927\u817f\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee8.71%\uff0c\u819d\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee6.78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u9884\u6d4b\u6b65\u6001\u8fdb\u5c55\u548c\u5173\u8282\u89d2\u5ea6\uff0c\u5bf9\u52a8\u529b\u578b\u7ecf\u80a1\u9aa8\u5047\u80a2\u7684\u969c\u788d\u8de8\u8d8a\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24680", "abs": "https://arxiv.org/abs/2510.24680", "authors": ["Zishuo Wang", "Joel Loo", "David Hsu"], "title": "Fare: Failure Resilience in Learned Visual Navigation Control", "comment": null, "summary": "While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments.", "AI": {"tldr": "Fare\u6846\u67b6\u6784\u5efa\u5177\u6709\u6545\u969c\u6062\u590d\u80fd\u529b\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7OOD\u68c0\u6d4b\u548c\u8bc6\u522b\u5b9e\u73b0\u81ea\u52a8\u6545\u969c\u6062\u590d\uff0c\u65e0\u9700\u663e\u5f0f\u6545\u969c\u6570\u636e\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u5bb9\u6613\u53d1\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u6545\u969c\uff0c\u9700\u8981\u80fd\u591f\u68c0\u6d4b\u5e76\u81ea\u52a8\u4ece\u6545\u969c\u4e2d\u6062\u590d\u7684\u5f39\u6027\u7b56\u7565\u3002", "method": "\u5728\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u4e2d\u5d4c\u5165OOD\u68c0\u6d4b\u548c\u8bc6\u522b\u673a\u5236\uff0c\u65e0\u9700\u4f7f\u7528\u663e\u5f0f\u6545\u969c\u6570\u636e\uff0c\u5e76\u914d\u5bf9\u6062\u590d\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660eFare\u80fd\u591f\u5728\u4e24\u79cd\u4e0d\u540c\u7b56\u7565\u67b6\u6784\u4e0a\u5b9e\u73b0\u6545\u969c\u6062\u590d\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u8ddd\u79bb\u5bfc\u822a\u3002", "conclusion": "Fare\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u6545\u969c\u5f39\u6027\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u6545\u969c\u8bc6\u522b\u548c\u6062\u590d\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24683", "abs": "https://arxiv.org/abs/2510.24683", "authors": ["Caleb Escobedo", "Nataliya Nechyporenko", "Shreyas Kadekodi", "Alessandro Roncone"], "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers", "comment": null, "summary": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u66f2\u7ebf\u548c\u865a\u62df\u7ea6\u675f\u4e09\u4e2a\u8bbe\u8ba1\u8981\u7d20\uff0c\u5e76\u901a\u8fc7\u57fa\u672c\u673a\u5668\u4eba-\u969c\u788d\u7269\u5b9e\u9a8c\u573a\u666f\u9a8c\u8bc1\u673a\u5668\u4eba\u884c\u4e3a\u3002", "motivation": "\u5b9e\u65f6\u63a7\u5236\u662f\u673a\u5668\u4eba\u5728\u52a8\u6001\u7269\u4f53\u73af\u5883\u4e2d\u5b89\u5168\u64cd\u4f5c\u7684\u5173\u952e\u65b9\u9762\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9884\u6d4b\u548c\u907f\u514d\u78b0\u649e\u7684\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u3002", "method": "\u5efa\u7acb\u5206\u6790\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u66f2\u7ebf\u548c\u865a\u62df\u7ea6\u675f\u4e09\u4e2a\u8bbe\u8ba1\u8981\u7d20\uff0c\u4f7f\u7528\u57fa\u672c\u673a\u5668\u4eba-\u969c\u788d\u7269\u5b9e\u9a8c\u573a\u666f\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u6bd4\u8f83\u4e09\u4e2a\u4ee3\u8868\u6027\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u3002", "result": "\u5206\u6790\u53d1\u73b0\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u7684\u8bbe\u8ba1\u5f80\u5f80\u7f3a\u4e4f\u8fd0\u52a8\u5b66\u8003\u8651\u3001\u63a7\u5236\u70b9\u8fde\u7eed\u6027\u548c\u8fd0\u52a8\u66f2\u7ebf\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u7528\u4e8e\u672a\u6765\u8bbe\u8ba1\u3001\u6bd4\u8f83\u548c\u57fa\u51c6\u6d4b\u8bd5\u907f\u969c\u65b9\u6cd5\u3002"}}
{"id": "2510.24692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24692", "abs": "https://arxiv.org/abs/2510.24692", "authors": ["Jun Wang", "Ziyang Zhou", "Ardalan Kahak", "Suyi Li"], "title": "Embodying Physical Computing into Soft Robots", "comment": null, "summary": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5c06\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u8ba8\u8bba\u4e86\u4e09\u79cd\u72ec\u7279\u7b56\u7565\uff1a\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u50a8\u5907\u6c60\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\uff0c\u4f7f\u8f6f\u4f53\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u884c\u4e3a\u800c\u65e0\u9700\u4f20\u7edf\u7535\u5b50\u5143\u4ef6\u3002", "motivation": "\u8f6f\u5316\u548c\u96c6\u6210\u8ba1\u7b97\u673a\u4e0e\u63a7\u5236\u5668\u662f\u8f6f\u4f53\u673a\u5668\u4eba\u5b9e\u73b0\u65e5\u5e38\u4f7f\u7528\u9c81\u68d2\u6027\u548c\u667a\u80fd\u5316\u7684\u5173\u952e\u524d\u6cbf\u9886\u57df\uff0c\u7269\u7406\u8ba1\u7b97\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u5c06\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4e09\u79cd\u5177\u4f53\u65b9\u6cd5\uff1a\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u50a8\u5907\u6c60\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u673a\u68b0\u8ba1\u7b97\u5185\u6838\u7684\u5185\u90e8\u76f8\u4e92\u4f5c\u7528\u6765\u5904\u7406\u8f93\u5165\u8f93\u51fa\u3002", "result": "\u8fd9\u4e9b\u5d4c\u5165\u5f0f\u8ba1\u7b97\u673a\u4f7f\u8f6f\u4f53\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u884c\u4e3a\uff0c\u5305\u62ec\u5e26\u907f\u969c\u7684\u534f\u8c03\u8fd0\u52a8\u3001\u6709\u6548\u8f7d\u8377\u91cd\u91cf\u548c\u65b9\u5411\u5206\u7c7b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u903b\u8f91\u89c4\u5219\u7684\u53ef\u7f16\u7a0b\u64cd\u4f5c\uff0c\u65e0\u9700\u4f9d\u8d56CMOS\u7535\u5b50\u5143\u4ef6\u3002", "conclusion": "\u672c\u6587\u8be6\u7ec6\u9610\u8ff0\u4e86\u5d4c\u5165\u5f0f\u7269\u7406\u8ba1\u7b97\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u7efc\u8ff0\u4e86\u5f53\u524d\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u7269\u7406\u8ba1\u7b97\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
