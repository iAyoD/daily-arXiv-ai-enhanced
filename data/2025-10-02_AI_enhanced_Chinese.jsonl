{"id": "2510.00154", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00154", "abs": "https://arxiv.org/abs/2510.00154", "authors": ["Xinyi Liu", "Mohammadreza Fani Sani", "Zewei Zhou", "Julius Wirbel", "Bahram Zarrin", "Roberto Galeazzi"], "title": "RoboPilot: Generalizable Dynamic Robotic Manipulation with Dual-thinking Modes", "comment": null, "summary": "Despite rapid progress in autonomous robotics, executing complex or\nlong-horizon tasks remains a fundamental challenge. Most current approaches\nfollow an open-loop paradigm with limited reasoning and no feedback, resulting\nin poor robustness to environmental changes and severe error accumulation. We\npresent RoboPilot, a dual-thinking closed-loop framework for robotic\nmanipulation that supports adaptive reasoning for complex tasks in real-world\ndynamic environments. RoboPilot leverages primitive actions for structured task\nplanning and flexible action generation, while introducing feedback to enable\nreplanning from dynamic changes and execution errors. Chain-of-Thought\nreasoning further enhances high-level task planning and guides low-level action\ngeneration. The system dynamically switches between fast and slow thinking to\nbalance efficiency and accuracy. To systematically evaluate the robustness of\nRoboPilot in diverse robot manipulation scenarios, we introduce\nRoboPilot-Bench, a benchmark spanning 21 tasks across 10 categories, including\ninfeasible-task recognition and failure recovery. Experiments show that\nRoboPilot outperforms state-of-the-art baselines by 25.9\\% in task success\nrate, and the real-world deployment on an industrial robot further demonstrates\nits robustness in real-world settings.", "AI": {"tldr": "RoboPilot\u662f\u4e00\u4e2a\u53cc\u601d\u7ef4\u95ed\u73af\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5feb\u901f\u548c\u6162\u901f\u601d\u7ef4\u7684\u52a8\u6001\u5207\u6362\u6765\u5e73\u8861\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u590d\u6742\u4efb\u52a1\u7684\u9002\u5e94\u6027\u63a8\u7406\u548c\u6267\u884c\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u5927\u591a\u91c7\u7528\u5f00\u73af\u8303\u5f0f\uff0c\u7f3a\u4e4f\u63a8\u7406\u548c\u53cd\u9988\u673a\u5236\uff0c\u5bfc\u81f4\u5bf9\u73af\u5883\u53d8\u5316\u7684\u9c81\u68d2\u6027\u5dee\u548c\u9519\u8bef\u7d2f\u79ef\u4e25\u91cd\u3002", "method": "RoboPilot\u5229\u7528\u539f\u59cb\u52a8\u4f5c\u8fdb\u884c\u7ed3\u6784\u5316\u4efb\u52a1\u89c4\u5212\u548c\u7075\u6d3b\u52a8\u4f5c\u751f\u6210\uff0c\u5f15\u5165\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u52a8\u6001\u53d8\u5316\u548c\u6267\u884c\u9519\u8bef\u7684\u91cd\u65b0\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u589e\u5f3a\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\u548c\u6307\u5bfc\u4f4e\u5c42\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRoboPilot\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa25.9%\uff0c\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u7684\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "RoboPilot\u6846\u67b6\u901a\u8fc7\u53cc\u601d\u7ef4\u95ed\u73af\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00182", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00182", "abs": "https://arxiv.org/abs/2510.00182", "authors": ["Jorge Mendez-Mendez"], "title": "A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream", "comment": null, "summary": "Using large language models (LLMs) to solve complex robotics problems\nrequires understanding their planning capabilities. Yet while we know that LLMs\ncan plan on some problems, the extent to which these planning capabilities\ncover the space of robotics tasks is unclear. One promising direction is to\nintegrate the semantic knowledge of LLMs with the formal reasoning of task and\nmotion planning (TAMP). However, the myriad of choices for how to integrate\nLLMs within TAMP complicates the design of such systems. We develop 16\nalgorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our\nzero-shot experiments across 4,950 problems and three domains reveal that the\nGemini-based planners exhibit lower success rates and higher planning times\nthan their engineered counterparts. We show that providing geometric details\nincreases the number of task-planning errors compared to pure PDDL\ndescriptions, and that (faster) non-reasoning LLM variants outperform (slower)\nreasoning variants in most cases, since the TAMP system can direct the LLM to\ncorrect its mistakes.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4f7f\u7528Gemini 2.5 Flash\u66ff\u4ee3TAMP\u5173\u952e\u7ec4\u4ef6\u768416\u79cd\u7b97\u6cd5\uff0c\u53d1\u73b0\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\uff0cLLM\u7684\u6210\u529f\u7387\u8f83\u4f4e\u3001\u89c4\u5212\u65f6\u95f4\u8f83\u957f\uff0c\u4e14\u51e0\u4f55\u7ec6\u8282\u4f1a\u589e\u52a0\u4efb\u52a1\u89c4\u5212\u9519\u8bef\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u673a\u5668\u4eba\u590d\u6742\u95ee\u9898\u4e2d\u7684\u89c4\u5212\u80fd\u529b\uff0c\u7279\u522b\u662f\u5982\u4f55\u5c06LLM\u7684\u8bed\u4e49\u77e5\u8bc6\u4e0eTAMP\u7684\u5f62\u5f0f\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u4ee5\u8986\u76d6\u66f4\u5e7f\u6cdb\u7684\u673a\u5668\u4eba\u4efb\u52a1\u7a7a\u95f4\u3002", "method": "\u5f00\u53d1\u4e8616\u79cd\u4f7f\u7528Gemini 2.5 Flash\u66ff\u4ee3TAMP\u5173\u952e\u7ec4\u4ef6\u7684\u7b97\u6cd5\uff0c\u57284,950\u4e2a\u95ee\u9898\u548c\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u96f6\u6837\u672c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540cLLM\u53d8\u4f53\u7684\u6027\u80fd\u3002", "result": "\u57fa\u4e8eGemini\u7684\u89c4\u5212\u5668\u6bd4\u5de5\u7a0b\u5316\u5bf9\u5e94\u65b9\u6848\u6210\u529f\u7387\u66f4\u4f4e\u3001\u89c4\u5212\u65f6\u95f4\u66f4\u957f\uff1b\u63d0\u4f9b\u51e0\u4f55\u7ec6\u8282\u4f1a\u589e\u52a0\u4efb\u52a1\u89c4\u5212\u9519\u8bef\uff1b\u975e\u63a8\u7406LLM\u53d8\u4f53\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u63a8\u7406\u53d8\u4f53\u3002", "conclusion": "LLM\u5728TAMP\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\u9700\u8981\u8c28\u614e\u8bbe\u8ba1\uff0c\u975e\u63a8\u7406LLM\u53d8\u4f53\u53ef\u80fd\u66f4\u6709\u6548\uff0c\u56e0\u4e3aTAMP\u7cfb\u7edf\u53ef\u4ee5\u6307\u5bfcLLM\u7ea0\u6b63\u9519\u8bef\u3002"}}
{"id": "2510.00188", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00188", "abs": "https://arxiv.org/abs/2510.00188", "authors": ["Alireza Aliyari", "Gholamreza Vossoughi"], "title": "A Novel Robust Control Method Combining DNN-Based NMPC Approximation and PI Control: Application to Exoskeleton Squat Movements", "comment": null, "summary": "Nonlinear Model Predictive Control (NMPC) is a precise controller, but its\nheavy computational load often prevents application in robotic systems. Some\nstudies have attempted to approximate NMPC using deep neural networks\n(NMPC-DNN). However, in the presence of unexpected disturbances or when\noperating conditions differ from training data, this approach lacks robustness,\nleading to large tracking errors. To address this issue, for the first time,\nthe NMPC-DNN output is combined with a PI controller (Hybrid NMPC-DNN-PI). The\nproposed controller is validated by applying it to an exoskeleton robot during\nsquat movement, which has a complex dynamic model and has received limited\nattention regarding robust nonlinear control design. A human-robot dynamic\nmodel with three active joints (ankle, knee, hip) is developed, and more than\n5.3 million training samples are used to train the DNN. The results show that,\nunder unseen conditions for the DNN, the tracking error in Hybrid NMPC-DNN-PI\nis significantly lower compared to NMPC-DNN. Moreover, human joint torques are\ngreatly reduced with the use of the exoskeleton, with RMS values for the\nstudied case reduced by 30.9%, 41.8%, and 29.7% at the ankle, knee, and hip,\nrespectively. In addition, the computational cost of Hybrid NMPC-DNN-PI is\n99.93% lower than that of NMPC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408NMPC-DNN-PI\u63a7\u5236\u5668\uff0c\u5c06\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684DNN\u8fd1\u4f3c\u4e0ePI\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u5e94\u7528\u4e8e\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u4e0b\u8e72\u8fd0\u52a8\u63a7\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edfNMPC\u8ba1\u7b97\u8d1f\u8f7d\u91cd\uff0c\u800c\u7eafDNN\u8fd1\u4f3c\u65b9\u6cd5\u5728\u9047\u5230\u8bad\u7ec3\u6570\u636e\u672a\u8986\u76d6\u7684\u6270\u52a8\u6216\u5de5\u51b5\u65f6\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u5bfc\u81f4\u8ddf\u8e2a\u8bef\u5dee\u589e\u5927\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u4e09\u4e2a\u4e3b\u52a8\u5173\u8282\uff08\u8e1d\u3001\u819d\u3001\u9acb\uff09\u7684\u4eba\u673a\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4f7f\u7528530\u591a\u4e07\u8bad\u7ec3\u6837\u672c\u8bad\u7ec3DNN\uff0c\u5e76\u5c06NMPC-DNN\u8f93\u51fa\u4e0ePI\u63a7\u5236\u5668\u7ed3\u5408\u5f62\u6210\u6df7\u5408\u63a7\u5236\u5668\u3002", "result": "\u5728DNN\u672a\u89c1\u6761\u4ef6\u4e0b\uff0c\u6df7\u5408\u63a7\u5236\u5668\u7684\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e\u7eafNMPC-DNN\uff1b\u5916\u9aa8\u9abc\u4f7f\u7528\u4f7f\u4eba\u4f53\u5173\u8282\u626d\u77e9\u5927\u5e45\u964d\u4f4e\uff08\u8e1d30.9%\u3001\u819d41.8%\u3001\u9acb29.7%\uff09\uff1b\u8ba1\u7b97\u6210\u672c\u6bd4NMPC\u964d\u4f4e99.93%\u3002", "conclusion": "\u6df7\u5408NMPC-DNN-PI\u63a7\u5236\u5668\u5728\u4fdd\u6301NMPC\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u590d\u6742\u7684\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u63a7\u5236\u3002"}}
{"id": "2510.00225", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.00225", "abs": "https://arxiv.org/abs/2510.00225", "authors": ["Yue Meng", "Fei Chen", "Chuchu Fan"], "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks", "comment": null, "summary": "Learning control policies for complex, long-horizon tasks is a central\nchallenge in robotics and autonomous systems. Signal Temporal Logic (STL)\noffers a powerful and expressive language for specifying such tasks, but its\nnon-Markovian nature and inherent sparse reward make it difficult to be solved\nvia standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus\nonly on limited STL fragments or use STL robustness scores as sparse terminal\nrewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization,\nto solve general STL tasks. TGPO decomposes STL into timed subgoals and\ninvariant constraints and provides a hierarchical framework to tackle the\nproblem. The high-level component of TGPO proposes concrete time allocations\nfor these subgoals, and the low-level time-conditioned policy learns to achieve\nthe sequenced subgoals using a dense, stage-wise reward signal. During\ninference, we sample various time allocations and select the most promising\nassignment for the policy network to rollout the solution trajectory. To foster\nefficient policy learning for complex STL with multiple subgoals, we leverage\nthe learned critic to guide the high-level temporal search via\nMetropolis-Hastings sampling, focusing exploration on temporally feasible\nsolutions. We conduct experiments on five environments, ranging from\nlow-dimensional navigation to manipulation, drone, and quadrupedal locomotion.\nUnder a wide range of STL tasks, TGPO significantly outperforms\nstate-of-the-art baselines (especially for high-dimensional and long-horizon\ncases), with an average of 31.6% improvement in task success rate compared to\nthe best baseline. The code will be available at\nhttps://github.com/mengyuest/TGPO", "AI": {"tldr": "TGPO\u662f\u4e00\u79cd\u89e3\u51b3\u901a\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u4efb\u52a1\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06STL\u5206\u89e3\u4e3a\u5b9a\u65f6\u5b50\u76ee\u6807\u548c\u4e0d\u53d8\u7ea6\u675f\uff0c\u4f7f\u7528\u9ad8\u5c42\u65f6\u95f4\u5206\u914d\u548c\u4f4e\u5c42\u7b56\u7565\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u7b56\u7565\u5b66\u4e60\u6311\u6218\uff0c\u7279\u522b\u662f\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u548c\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u6709\u9650\u7684STL\u7247\u6bb5\u6216\u4f7f\u7528\u7a00\u758f\u5956\u52b1\u3002", "method": "\u63d0\u51faTGPO\u5206\u5c42\u6846\u67b6\uff1a\u9ad8\u5c42\u7ec4\u4ef6\u5206\u914d\u5b50\u76ee\u6807\u65f6\u95f4\uff0c\u4f4e\u5c42\u65f6\u95f4\u6761\u4ef6\u7b56\u7565\u4f7f\u7528\u5bc6\u96c6\u7684\u9636\u6bb5\u5956\u52b1\u5b66\u4e60\uff1b\u5229\u7528\u5b66\u4e60\u5230\u7684critic\u901a\u8fc7Metropolis-Hastings\u91c7\u6837\u6307\u5bfc\u9ad8\u5c42\u65f6\u95f4\u641c\u7d22\u3002", "result": "\u57285\u4e2a\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cTGPO\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u6bd4\u6700\u4f73\u57fa\u7ebf\u5e73\u5747\u63d0\u534731.6%\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u548c\u957f\u65f6\u7a0b\u60c5\u51b5\u4e0b\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TGPO\u80fd\u591f\u6709\u6548\u89e3\u51b3\u901a\u7528STL\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u548c\u5bc6\u96c6\u5956\u52b1\u673a\u5236\u514b\u670d\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u5728\u590d\u6742\u65f6\u5e8f\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.00272", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00272", "abs": "https://arxiv.org/abs/2510.00272", "authors": ["Odichimnma Ezeji", "Michael Ziegltrum", "Giulio Turrisi", "Tommaso Belvedere", "Valerio Modugno"], "title": "BC-MPPI: A Probabilistic Constraint Layer for Safe Model-Predictive Path-Integral Control", "comment": null, "summary": "Model Predictive Path Integral (MPPI) control has recently emerged as a fast,\ngradient-free alternative to model-predictive control in highly non-linear\nrobotic tasks, yet it offers no hard guarantees on constraint satisfaction. We\nintroduce Bayesian-Constraints MPPI (BC-MPPI), a lightweight safety layer that\nattaches a probabilistic surrogate to every state and input constraint. At each\nre-planning step the surrogate returns the probability that a candidate\ntrajectory is feasible; this joint probability scales the weight given to a\ncandidate, automatically down-weighting rollouts likely to collide or exceed\nlimits and pushing the sampling distribution toward the safe subset; no\nhand-tuned penalty costs or explicit sample rejection required. We train the\nsurrogate from 1000 offline simulations and deploy the controller on a\nquadrotor in MuJoCo with both static and moving obstacles. Across K in\n[100,1500] rollouts BC-MPPI preserves safety margins while satisfying the\nprescribed probability of violation. Because the surrogate is a stand-alone,\nversion-controlled artefact and the runtime safety score is a single scalar,\nthe approach integrates naturally with verification-and-validation pipelines\nfor certifiable autonomous systems.", "AI": {"tldr": "BC-MPPI\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5b89\u5168\u5c42\uff0c\u4e3aMPPI\u63a7\u5236\u6dfb\u52a0\u6982\u7387\u7ea6\u675f\u4fdd\u8bc1\uff0c\u901a\u8fc7\u6982\u7387\u4ee3\u7406\u8bc4\u4f30\u8f68\u8ff9\u53ef\u884c\u6027\uff0c\u81ea\u52a8\u964d\u4f4e\u53ef\u80fd\u8fdd\u53cd\u7ea6\u675f\u7684\u8f68\u8ff9\u6743\u91cd\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u60e9\u7f5a\u6210\u672c\u3002", "motivation": "MPPI\u63a7\u5236\u867d\u7136\u5feb\u901f\u4e14\u65e0\u9700\u68af\u5ea6\uff0c\u4f46\u7f3a\u4e4f\u786c\u6027\u7ea6\u675f\u4fdd\u8bc1\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u6765\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u4e3a\u6bcf\u4e2a\u72b6\u6001\u548c\u8f93\u5165\u7ea6\u675f\u9644\u52a0\u6982\u7387\u4ee3\u7406\uff0c\u5728\u6bcf\u6b21\u91cd\u89c4\u5212\u65f6\u8bc4\u4f30\u5019\u9009\u8f68\u8ff9\u7684\u53ef\u884c\u6027\u6982\u7387\uff0c\u8be5\u6982\u7387\u7f29\u653e\u8f68\u8ff9\u6743\u91cd\uff0c\u81ea\u52a8\u5c06\u91c7\u6837\u5206\u5e03\u63a8\u5411\u5b89\u5168\u5b50\u96c6\u3002", "result": "\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u6d4b\u8bd5\uff0cBC-MPPI\u5728\u4fdd\u6301\u5b89\u5168\u88d5\u5ea6\u7684\u540c\u65f6\u6ee1\u8db3\u89c4\u5b9a\u7684\u8fdd\u53cd\u6982\u7387\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u969c\u788d\u7269\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u72ec\u7acb\u3001\u7248\u672c\u53ef\u63a7\u7684\u5b89\u5168\u4ee3\u7406\uff0c\u8fd0\u884c\u65f6\u5b89\u5168\u8bc4\u5206\u662f\u5355\u4e00\u6807\u91cf\uff0c\u81ea\u7136\u96c6\u6210\u5230\u53ef\u8ba4\u8bc1\u81ea\u4e3b\u7cfb\u7edf\u7684\u9a8c\u8bc1-\u786e\u8ba4\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2510.00329", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00329", "abs": "https://arxiv.org/abs/2510.00329", "authors": ["Sarmad Mehrdad", "Maxime Sabbah", "Vincent Bonnet", "Ludovic Righetti"], "title": "Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning", "comment": "8 pages, 4 figures", "summary": "This paper investigates the application of Minimal Observation Inverse\nReinforcement Learning (MO-IRL) to model and predict human arm-reaching\nmovements with time-varying cost weights. Using a planar two-link biomechanical\nmodel and high-resolution motion-capture data from subjects performing a\npointing task, we segment each trajectory into multiple phases and learn\nphase-specific combinations of seven candidate cost functions. MO-IRL\niteratively refines cost weights by scaling observed and generated trajectories\nin the maximum entropy IRL formulation, greatly reducing the number of required\ndemonstrations and convergence time compared to classical IRL approaches.\nTraining on ten trials per posture yields average joint-angle Root Mean Squared\nErrors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight\ndivisions, respectively, versus 10.4 deg using a single static weight.\nCross-validation on remaining trials and, for the first time, inter-subject\nvalidation on an unseen subject's 20 trials, demonstrates comparable predictive\naccuracy, around 8 deg RMSE, indicating robust generalization. Learned weights\nemphasize joint acceleration minimization during movement onset and\ntermination, aligning with smoothness principles observed in biological motion.\nThese results suggest that MO-IRL can efficiently uncover dynamic,\nsubject-independent cost structures underlying human motor control, with\npotential applications for humanoid robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5e94\u7528\u6700\u5c0f\u89c2\u6d4b\u9006\u5f3a\u5316\u5b66\u4e60(MO-IRL)\u5efa\u6a21\u4eba\u7c7b\u624b\u81c2\u4f38\u5c55\u8fd0\u52a8\uff0c\u901a\u8fc7\u5206\u6bb5\u5b66\u4e60\u65f6\u53d8\u6210\u672c\u6743\u91cd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u6f14\u793a\u6570\u636e\u548c\u6536\u655b\u65f6\u95f4\uff0c\u5728\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u4e0a\u53d6\u5f97\u4e866.4-5.6\u5ea6\u7684RMSE\uff0c\u4f18\u4e8e\u9759\u6001\u6743\u91cd\u65b9\u6cd5\u768410.4\u5ea6\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u4e2d\u7684\u52a8\u6001\u6210\u672c\u7ed3\u6784\uff0c\u4f20\u7edfIRL\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6f14\u793a\u6570\u636e\u4e14\u6536\u655b\u6162\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u63ed\u793a\u65f6\u53d8\u6210\u672c\u6743\u91cd\u3002", "method": "\u4f7f\u7528\u5e73\u9762\u53cc\u8fde\u6746\u751f\u7269\u529b\u5b66\u6a21\u578b\u548c\u9ad8\u5206\u8fa8\u7387\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff0c\u5c06\u8f68\u8ff9\u5206\u6bb5\u4e3a\u591a\u4e2a\u9636\u6bb5\uff0c\u5b66\u4e60\u4e03\u4e2a\u5019\u9009\u6210\u672c\u51fd\u6570\u7684\u9636\u6bb5\u7279\u5b9a\u7ec4\u5408\uff0c\u901a\u8fc7MO-IRL\u8fed\u4ee3\u4f18\u5316\u6210\u672c\u6743\u91cd\u3002", "result": "\u6bcf\u4e2a\u59ff\u52bf10\u6b21\u8bd5\u9a8c\u8bad\u7ec3\uff0c\u516d\u6bb5\u548c\u516b\u6bb5\u6743\u91cd\u5212\u5206\u7684\u5e73\u5747\u5173\u8282\u89d2\u5ea6RMSE\u5206\u522b\u4e3a6.4\u5ea6\u548c5.6\u5ea6\uff0c\u4f18\u4e8e\u9759\u6001\u6743\u91cd\u768410.4\u5ea6\uff1b\u4ea4\u53c9\u9a8c\u8bc1\u548c\u8de8\u88ab\u8bd5\u9a8c\u8bc1\u5747\u663e\u793a\u7ea68\u5ea6RMSE\u7684\u7a33\u5065\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MO-IRL\u80fd\u6709\u6548\u63ed\u793a\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u4e2d\u52a8\u6001\u3001\u88ab\u8bd5\u72ec\u7acb\u7684\u6210\u672c\u7ed3\u6784\uff0c\u5b66\u4e60\u5230\u7684\u6743\u91cd\u5728\u8fd0\u52a8\u8d77\u59cb\u548c\u7ec8\u6b62\u9636\u6bb5\u5f3a\u8c03\u5173\u8282\u52a0\u901f\u5ea6\u6700\u5c0f\u5316\uff0c\u7b26\u5408\u751f\u7269\u8fd0\u52a8\u7684\u5e73\u6ed1\u6027\u539f\u5219\uff0c\u5177\u6709\u4eba\u5f62\u673a\u5668\u4eba\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.00358", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00358", "abs": "https://arxiv.org/abs/2510.00358", "authors": ["Linjin He", "Xinda Qi", "Dong Chen", "Zhaojian Li", "Xiaobo Tan"], "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts", "comment": null, "summary": "Soft snake robots offer remarkable flexibility and adaptability in complex\nenvironments, yet their control remains challenging due to highly nonlinear\ndynamics. Existing model-based and bio-inspired controllers rely on simplified\nassumptions that limit performance. Deep reinforcement learning (DRL) has\nrecently emerged as a promising alternative, but online training is often\nimpractical because of costly and potentially damaging real-world interactions.\nOffline RL provides a safer option by leveraging pre-collected datasets, but it\nsuffers from distribution shift, which degrades generalization to unseen\nscenarios. To overcome this challenge, we propose DiSA-IQL\n(Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that\nincorporates robustness modulation by penalizing unreliable state-action pairs\nto mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks\nacross two settings: in-distribution and out-of-distribution evaluation.\nSimulation results show that DiSA-IQL consistently outperforms baseline models,\nincluding Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla\nIQL, achieving higher success rates, smoother trajectories, and improved\nrobustness. The codes are open-sourced to support reproducibility and to\nfacilitate further research in offline RL for soft robot control.", "AI": {"tldr": "\u63d0\u51faDiSA-IQL\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u4e0d\u53ef\u9760\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u6765\u7f13\u89e3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u8f6f\u86c7\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8f6f\u86c7\u673a\u5668\u4eba\u63a7\u5236\u9762\u4e34\u9ad8\u5ea6\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5316\u5047\u8bbe\u9650\u5236\u6027\u80fd\u3002\u5728\u7ebfDRL\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u79bb\u7ebfRL\u5b58\u5728\u5206\u5e03\u504f\u79fb\u95ee\u9898\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6269\u5c55IQL\u65b9\u6cd5\uff0c\u5f15\u5165\u5206\u5e03\u504f\u79fb\u611f\u77e5\u673a\u5236\uff0c\u901a\u8fc7\u9c81\u68d2\u6027\u8c03\u5236\u60e9\u7f5a\u4e0d\u53ef\u9760\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u6765\u7f13\u89e3\u5206\u5e03\u504f\u79fb\u3002", "result": "\u5728\u76ee\u6807\u5230\u8fbe\u4efb\u52a1\u4e2d\uff0cDiSA-IQL\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8eBC\u3001CQL\u548c\u6807\u51c6IQL\uff0c\u83b7\u5f97\u66f4\u9ad8\u6210\u529f\u7387\u3001\u66f4\u5e73\u6ed1\u8f68\u8ff9\u548c\u66f4\u597d\u9c81\u68d2\u6027\u3002", "conclusion": "DiSA-IQL\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u5728\u8f6f\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4ee3\u7801\u5f00\u6e90\u652f\u6301\u53ef\u590d\u73b0\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.00401", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.00401", "abs": "https://arxiv.org/abs/2510.00401", "authors": ["Shounak Sural", "Charles Kekeh", "Wenliang Liu", "Federico Pecora", "Mouhacine Benosman"], "title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting", "comment": null, "summary": "Long-horizon motion forecasting for multiple autonomous robots is challenging\ndue to non-linear agent interactions, compounding prediction errors, and\ncontinuous-time evolution of dynamics. Learned dynamics of such a system can be\nuseful in various applications such as travel time prediction,\nprediction-guided planning and generative simulation. In this work, we aim to\ndevelop an efficient trajectory forecasting model conditioned on multi-agent\ngoals. Motivated by the recent success of physics-guided deep learning for\npartially known dynamical systems, we develop a model based on neural\nControlled Differential Equations (CDEs) for long-horizon motion forecasting.\nUnlike discrete-time methods such as RNNs and transformers, neural CDEs operate\nin continuous time, allowing us to combine physics-informed constraints and\nbiases to jointly model multi-robot dynamics. Our approach, named PINCoDE\n(Physics-Informed Neural Controlled Differential Equations), learns\ndifferential equation parameters that can be used to predict the trajectories\nof a multi-agent system starting from an initial condition. PINCoDE is\nconditioned on future goals and enforces physics constraints for robot motion\nover extended periods of time. We adopt a strategy that scales our model from\n10 robots to 100 robots without the need for additional model parameters, while\nproducing predictions with an average ADE below 0.5 m for a 1-minute horizon.\nFurthermore, progressive training with curriculum learning for our PINCoDE\nmodel results in a 2.7X reduction of forecasted pose error over 4 minute\nhorizons compared to analytical models.", "AI": {"tldr": "\u63d0\u51fa\u4e86PINCoDE\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u7684\u7269\u7406\u4fe1\u606f\u591a\u673a\u5668\u4eba\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u8fde\u7eed\u65f6\u95f4\u5185\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u8fdb\u884c\u957f\u65f6\u7a0b\u8fd0\u52a8\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u591a\u81ea\u4e3b\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u8fd0\u52a8\u9884\u6d4b\u7684\u6311\u6218\uff0c\u5305\u62ec\u975e\u7ebf\u6027\u4ea4\u4e92\u3001\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u548c\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u6f14\u5316\uff0c\u4e3a\u65c5\u884c\u65f6\u95f4\u9884\u6d4b\u3001\u89c4\u5212\u5f15\u5bfc\u548c\u751f\u6210\u4eff\u771f\u7b49\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff08CDEs\uff09\u5728\u8fde\u7eed\u65f6\u95f4\u5185\u5efa\u6a21\u591a\u673a\u5668\u4eba\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u7ea6\u675f\u548c\u504f\u5dee\uff0c\u6a21\u578b\u53ef\u57fa\u4e8e\u672a\u6765\u76ee\u6807\u8fdb\u884c\u6761\u4ef6\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u5373\u53ef\u4ece10\u4e2a\u673a\u5668\u4eba\u6269\u5c55\u5230100\u4e2a\u673a\u5668\u4eba\u3002", "result": "\u57281\u5206\u949f\u9884\u6d4b\u65f6\u57df\u5185\u5e73\u5747ADE\u4f4e\u4e8e0.5\u7c73\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7684\u6e10\u8fdb\u8bad\u7ec3\u57284\u5206\u949f\u65f6\u57df\u5185\u6bd4\u89e3\u6790\u6a21\u578b\u51cf\u5c112.7\u500d\u7684\u4f4d\u59ff\u8bef\u5dee\u3002", "conclusion": "PINCoDE\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u5efa\u6a21\u548c\u7269\u7406\u7ea6\u675f\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2510.00406", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.00406", "abs": "https://arxiv.org/abs/2510.00406", "authors": ["Hengtao Li", "Pengxiang Ding", "Runze Suo", "Yihao Wang", "Zirui Ge", "Dongyuan Zang", "Kexian Yu", "Mingyang Sun", "Hongyin Zhang", "Donglin Wang", "Weihua Su"], "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators", "comment": null, "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely\nheavily on imitation learning, leading to compounding errors and poor\nrobustness under distribution shift. Reinforcement learning (RL) can mitigate\nthese issues yet typically demands costly real-world interactions or suffers\nfrom sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning\nframework that leverages a data-driven world model as a controllable simulator.\nTrained from real interaction data, the simulator predicts future visual\nobservations conditioned on actions, allowing policy rollouts with dense,\ntrajectory-level rewards derived from goal-achieving references. This design\ndelivers an efficient and action-aligned learning signal, drastically lowering\nsample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses\nstrong supervised baselines and achieves greater efficiency than\nsimulator-based RL. Moreover, it exhibits strong robustness under perturbed\nconditions, sustaining stable task execution. Our results establish\nworld-model-based RFT as a practical post-training paradigm to enhance the\ngeneralization and robustness of VLA models. For more details, please refer to\nhttps://vla-rft.github.io/.", "AI": {"tldr": "VLA-RFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u53ef\u63a7\u6a21\u62df\u5668\u6765\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4ec5\u9700\u4e0d\u5230400\u6b65\u5fae\u8c03\u5c31\u80fd\u8d85\u8d8a\u76d1\u7763\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u5bb9\u6613\u4ea7\u751f\u7d2f\u79ef\u9519\u8bef\u4e14\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u9c81\u68d2\u6027\u5dee\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6216\u53d7\u9650\u4e8e\u4eff\u771f\u5230\u771f\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u5229\u7528\u4ece\u771f\u5b9e\u4ea4\u4e92\u6570\u636e\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u53ef\u63a7\u6a21\u62df\u5668\uff0c\u9884\u6d4b\u57fa\u4e8e\u52a8\u4f5c\u7684\u672a\u6765\u89c6\u89c9\u89c2\u5bdf\uff0c\u4ece\u800c\u5728\u7b56\u7565rollout\u4e2d\u83b7\u5f97\u5bc6\u96c6\u7684\u8f68\u8ff9\u7ea7\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u4ec5\u9700\u4e0d\u5230400\u6b65\u5fae\u8c03\u5c31\u8d85\u8d8a\u4e86\u5f3a\u76d1\u7763\u57fa\u7ebf\uff0c\u6bd4\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u66f4\u9ad8\u6548\uff0c\u5728\u6270\u52a8\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5fae\u8c03\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3aVLA\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.00441", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00441", "abs": "https://arxiv.org/abs/2510.00441", "authors": ["Yiyuan Pan", "Yunzhe Xu", "Zhe Liu", "Hesheng Wang"], "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation", "comment": null, "summary": "Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents.", "AI": {"tldr": "NeuRO\u662f\u4e00\u4e2a\u96c6\u6210\u5b66\u4e60\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u7f51\u7edc\u4e0e\u4e0b\u6e38\u4efb\u52a1\u7ea7\u9c81\u68d2\u4f18\u5316\u7d27\u5bc6\u7ed3\u5408\uff0c\u901a\u8fc7\u90e8\u5206\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\u548c\u4fdd\u5f62\u6821\u51c6\u5c06\u89c6\u89c9\u9884\u6d4b\u8f6c\u5316\u4e3a\u51f8\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u4e0b\u7684\u89c6\u89c9\u5bfc\u822a\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u5bfc\u822a\u662f\u5177\u8eabAI\u7684\u57fa\u7840\u95ee\u9898\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9700\u8981\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\u6765\u5904\u7406\u591a\u76ee\u6807\u4efb\u52a1\u3002\u4e3b\u8981\u74f6\u9888\u662f\u6570\u636e\u7a00\u7f3a\uff1a\u4ece\u6709\u9650\u6570\u636e\u5b66\u4e60\u7684\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u65e0\u6cd5\u6cdb\u5316\u5230\u5206\u5e03\u5916\u573a\u666f\u3002\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u901a\u5e38\u589e\u52a0\u67b6\u6784\u590d\u6742\u5ea6\uff0c\u4f46\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e2d\u9002\u5f97\u5176\u53cd\u3002", "method": "NeuRO\u6846\u67b6\uff1a(i) \u4f7f\u7528\u90e8\u5206\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc(PICNNs)\u548c\u4fdd\u5f62\u6821\u51c6\u5c06\u566a\u58f0\u89c6\u89c9\u9884\u6d4b\u8f6c\u5316\u4e3a\u51f8\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\uff0c\u76f4\u63a5\u53c2\u6570\u5316\u4f18\u5316\u7ea6\u675f\uff1b(ii) \u5c06\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u7684\u89c4\u5212\u91cd\u65b0\u8868\u8ff0\u4e3a\u9c81\u68d2\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u73af\u5883\u8f6c\u79fb\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7b56\u7565\u3002", "result": "\u5728\u65e0\u5e8f\u548c\u987a\u5e8f\u591a\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cNeuRO\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u73af\u5883\u7684\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2510.00466", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00466", "abs": "https://arxiv.org/abs/2510.00466", "authors": ["Run Su", "Hao Fu", "Shuai Zhou", "Yingao Fu"], "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation", "comment": null, "summary": "Offline reinforcement learning (RL) has emerged as a promising framework for\naddressing robot social navigation challenges. However, inherent uncertainties\nin pedestrian behavior and limited environmental interaction during training\noften lead to suboptimal exploration and distributional shifts between offline\ntraining and online deployment. To overcome these limitations, this paper\nproposes a novel offline-to-online fine-tuning RL algorithm for robot social\nnavigation by integrating Return-to-Go (RTG) prediction into a causal\nTransformer architecture. Our algorithm features a spatiotem-poral fusion model\ndesigned to precisely estimate RTG values in real-time by jointly encoding\ntemporal pedestrian motion patterns and spatial crowd dynamics. This RTG\nprediction framework mitigates distribution shift by aligning offline policy\ntraining with online environmental interactions. Furthermore, a hybrid\noffline-online experience sampling mechanism is built to stabilize policy\nupdates during fine-tuning, ensuring balanced integration of pre-trained\nknowledge and real-time adaptation. Extensive experiments in simulated social\nnavigation environments demonstrate that our method achieves a higher success\nrate and lower collision rate compared to state-of-the-art baselines. These\nresults underscore the efficacy of our algorithm in enhancing navigation policy\nrobustness and adaptability. This work paves the way for more reliable and\nadaptive robotic navigation systems in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u5fae\u8c03\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06Return-to-Go\u9884\u6d4b\u96c6\u6210\u5230\u56e0\u679cTransformer\u67b6\u6784\u4e2d\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u793e\u4ea4\u5bfc\u822a\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u793e\u4ea4\u5bfc\u822a\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u884c\u4eba\u884c\u4e3a\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u8bad\u7ec3\u671f\u95f4\u73af\u5883\u4ea4\u4e92\u6709\u9650\uff0c\u5bfc\u81f4\u6b21\u4f18\u63a2\u7d22\u548c\u79bb\u7ebf\u8bad\u7ec3\u4e0e\u5728\u7ebf\u90e8\u7f72\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u3002", "method": "\u91c7\u7528\u65f6\u7a7a\u878d\u5408\u6a21\u578b\u5b9e\u65f6\u7cbe\u786e\u4f30\u8ba1RTG\u503c\uff0c\u8054\u5408\u7f16\u7801\u65f6\u95f4\u884c\u4eba\u8fd0\u52a8\u6a21\u5f0f\u548c\u7a7a\u95f4\u4eba\u7fa4\u52a8\u6001\uff1b\u6784\u5efa\u6df7\u5408\u79bb\u7ebf-\u5728\u7ebf\u7ecf\u9a8c\u91c7\u6837\u673a\u5236\uff0c\u7a33\u5b9a\u5fae\u8c03\u671f\u95f4\u7684\u653f\u7b56\u66f4\u65b0\u3002", "result": "\u5728\u6a21\u62df\u793e\u4ea4\u5bfc\u822a\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u4f4e\u7684\u78b0\u649e\u7387\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u589e\u5f3a\u5bfc\u822a\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u66f4\u53ef\u9760\u548c\u81ea\u9002\u5e94\u7684\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.00491", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00491", "abs": "https://arxiv.org/abs/2510.00491", "authors": ["Han Zhou", "Jinjin Cao", "Liyuan Ma", "Xueji Fang", "Guo-jun Qi"], "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment", "comment": null, "summary": "Learning diverse manipulation skills for real-world robots is severely\nbottlenecked by the reliance on costly and hard-to-scale teleoperated\ndemonstrations. While human videos offer a scalable alternative, effectively\ntransferring manipulation knowledge is fundamentally hindered by the\nsignificant morphological gap between human and robotic embodiments. To address\nthis challenge and facilitate skill transfer from human to robot, we introduce\nTraj2Action,a novel framework that bridges this embodiment gap by using the 3D\ntrajectory of the operational endpoint as a unified intermediate\nrepresentation, and then transfers the manipulation knowledge embedded in this\ntrajectory to the robot's actions. Our policy first learns to generate a coarse\ntrajectory, which forms an high-level motion plan by leveraging both human and\nrobot data. This plan then conditions the synthesis of precise, robot-specific\nactions (e.g., orientation and gripper state) within a co-denoising framework.\nExtensive real-world experiments on a Franka robot demonstrate that Traj2Action\nboosts the performance by up to 27% and 22.25% over $\\pi_0$ baseline on short-\nand long-horizon real-world tasks, and achieves significant gains as human data\nscales in robot policy learning. Our project website, featuring code and video\ndemonstrations, is available at\nhttps://anonymous.4open.science/w/Traj2Action-4A45/.", "AI": {"tldr": "Traj2Action\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u64cd\u4f5c\u7aef\u70b9\u76843D\u8f68\u8ff9\u4f5c\u4e3a\u7edf\u4e00\u4e2d\u95f4\u8868\u793a\uff0c\u5f25\u5408\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u5f62\u6001\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4ece\u4eba\u7c7b\u89c6\u9891\u5230\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\u7684\u8fc1\u79fb\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u591a\u6837\u5316\u64cd\u4f5c\u6280\u80fd\u5b66\u4e60\u4f9d\u8d56\u6602\u8d35\u9065\u64cd\u4f5c\u6f14\u793a\u7684\u95ee\u9898\uff0c\u5229\u7528\u53ef\u6269\u5c55\u7684\u4eba\u7c7b\u89c6\u9891\u4f5c\u4e3a\u66ff\u4ee3\uff0c\u4f46\u9762\u4e34\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5f62\u6001\u5dee\u5f02\u7684\u6311\u6218\u3002", "method": "\u4f7f\u75283D\u8f68\u8ff9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u9996\u5148\u751f\u6210\u7c97\u7565\u8f68\u8ff9\u4f5c\u4e3a\u9ad8\u5c42\u8fd0\u52a8\u89c4\u5212\uff0c\u7136\u540e\u5728\u534f\u540c\u53bb\u566a\u6846\u67b6\u4e2d\u5408\u6210\u7cbe\u786e\u7684\u673a\u5668\u4eba\u7279\u5b9a\u52a8\u4f5c\u3002", "result": "\u5728Franka\u673a\u5668\u4eba\u4e0a\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793a\uff0cTraj2Action\u5728\u77ed\u65f6\u548c\u957f\u65f6\u4efb\u52a1\u4e0a\u5206\u522b\u6bd4\u57fa\u7ebf\u63d0\u534727%\u548c22.25%\uff0c\u4e14\u968f\u7740\u4eba\u7c7b\u6570\u636e\u89c4\u6a21\u6269\u5927\uff0c\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u6548\u679c\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Traj2Action\u901a\u8fc7\u8f68\u8ff9\u4e2d\u95f4\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u7c7b-\u673a\u5668\u4eba\u5f62\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u4ece\u4eba\u7c7b\u89c6\u9891\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.00524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00524", "abs": "https://arxiv.org/abs/2510.00524", "authors": ["Baoshan Song", "Penggao Yan", "Xiao Xia", "Yihan Zhong", "Weisong Wen", "Li-Ta Hsu"], "title": "Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK/INS/odometer fusion", "comment": null, "summary": "Reliable GNSS positioning in complex environments remains a critical\nchallenge due to non-line-of-sight (NLOS) propagation, multipath effects, and\nfrequent signal blockages. These effects can easily introduce large outliers\ninto the raw pseudo-range measurements, which significantly degrade the\nperformance of global navigation satellite system (GNSS) real-time kinematic\n(RTK) positioning and limit the effectiveness of tightly coupled GNSS-based\nintegrated navigation system. To address this issue, we propose a two-stage\noutlier detection method and apply the method in a tightly coupled GNSS-RTK,\ninertial navigation system (INS), and odometer integration based on factor\ngraph optimization (FGO). In the first stage, Doppler measurements are employed\nto detect pseudo-range outliers in a GNSS-only manner, since Doppler is less\nsensitive to multipath and NLOS effects compared with pseudo-range, making it a\nmore stable reference for detecting sudden inconsistencies. In the second\nstage, pre-integrated inertial measurement units (IMU) and odometer constraints\nare used to generate predicted double-difference pseudo-range measurements,\nwhich enable a more refined identification and rejection of remaining outliers.\nBy combining these two complementary stages, the system achieves improved\nrobustness against both gross pseudo-range errors and degraded satellite\nmeasuring quality. The experimental results demonstrate that the two-stage\ndetection framework significantly reduces the impact of pseudo-range outliers,\nand leads to improved positioning accuracy and consistency compared with\nrepresentative baseline approaches. In the deep urban canyon test, the outlier\nmitigation method has limits the RMSE of GNSS-RTK/INS/odometer fusion from 0.52\nm to 0.30 m, with 42.3% improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5f02\u5e38\u503c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8eGNSS-RTK/INS/\u91cc\u7a0b\u8ba1\u7d27\u8026\u5408\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u666e\u52d2\u6d4b\u91cf\u548c\u9884\u79ef\u5206\u7ea6\u675f\u6765\u68c0\u6d4b\u548c\u6d88\u9664\u4f2a\u8ddd\u5f02\u5e38\u503c\uff0c\u63d0\u9ad8\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u590d\u6742\u73af\u5883\u4e2d\u7684GNSS\u5b9a\u4f4d\u9762\u4e34\u975e\u89c6\u8ddd\u4f20\u64ad\u3001\u591a\u5f84\u6548\u5e94\u548c\u4fe1\u53f7\u906e\u6321\u7b49\u6311\u6218\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f1a\u5bfc\u81f4\u4f2a\u8ddd\u6d4b\u91cf\u51fa\u73b0\u5927\u5f02\u5e38\u503c\uff0c\u4e25\u91cd\u5f71\u54cdRTK\u5b9a\u4f4d\u6027\u80fd\u548c\u7d27\u8026\u5408\u5bfc\u822a\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5f02\u5e38\u503c\u68c0\u6d4b\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u591a\u666e\u52d2\u6d4b\u91cf\u8fdb\u884cGNSS-only\u5f02\u5e38\u68c0\u6d4b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u9884\u79ef\u5206IMU\u548c\u91cc\u7a0b\u8ba1\u7ea6\u675f\u751f\u6210\u9884\u6d4b\u7684\u53cc\u5dee\u4f2a\u8ddd\u6d4b\u91cf\uff0c\u8fdb\u884c\u66f4\u7cbe\u7ec6\u7684\u5f02\u5e38\u8bc6\u522b\u548c\u5254\u9664\u3002\u57fa\u4e8e\u56e0\u5b50\u56fe\u4f18\u5316\u7684\u7d27\u8026\u5408GNSS-RTK/INS/\u91cc\u7a0b\u8ba1\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e24\u9636\u6bb5\u68c0\u6d4b\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u4f2a\u8ddd\u5f02\u5e38\u503c\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002\u5728\u6df1\u5ea6\u57ce\u5e02\u5ce1\u8c37\u6d4b\u8bd5\u4e2d\uff0c\u5f02\u5e38\u503c\u6291\u5236\u65b9\u6cd5\u5c06\u878d\u5408\u7cfb\u7edf\u7684RMSE\u4ece0.52\u7c73\u964d\u4f4e\u52300.30\u7c73\uff0c\u63d0\u5347\u4e8642.3%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u7684\u68c0\u6d4b\u9636\u6bb5\uff0c\u7cfb\u7edf\u5bf9\u7c97\u5927\u4f2a\u8ddd\u8bef\u5dee\u548c\u536b\u661f\u6d4b\u91cf\u8d28\u91cf\u4e0b\u964d\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684GNSS\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2510.00573", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00573", "abs": "https://arxiv.org/abs/2510.00573", "authors": ["Yen-Ling Tai", "Yi-Ru Yang", "Kuan-Ting Yu", "Yu-Wei Chao", "Yi-Ting Chen"], "title": "GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks", "comment": null, "summary": "Robotic food scooping is a critical manipulation skill for food preparation\nand service robots. However, existing robot learning algorithms, especially\nlearn-from-demonstration methods, still struggle to handle diverse and dynamic\nfood states, which often results in spillage and reduced reliability. In this\nwork, we introduce GRITS: A Spillage-Aware Guided Diffusion Policy for Robot\nFood Scooping Tasks. This framework leverages guided diffusion policy to\nminimize food spillage during scooping and to ensure reliable transfer of food\nitems from the initial to the target location. Specifically, we design a\nspillage predictor that estimates the probability of spillage given current\nobservation and action rollout. The predictor is trained on a simulated dataset\nwith food spillage scenarios, constructed from four primitive shapes (spheres,\ncubes, cones, and cylinders) with varied physical properties such as mass,\nfriction, and particle size. At inference time, the predictor serves as a\ndifferentiable guidance signal, steering the diffusion sampling process toward\nsafer trajectories while preserving task success. We validate GRITS on a\nreal-world robotic food scooping platform. GRITS is trained on six food\ncategories and evaluated on ten unseen categories with different shapes and\nquantities. GRITS achieves an 82% task success rate and a 4% spillage rate,\nreducing spillage by over 40% compared to baselines without guidance, thereby\ndemonstrating its effectiveness.", "AI": {"tldr": "GRITS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f15\u5bfc\u6269\u6563\u7b56\u7565\u7684\u673a\u5668\u4eba\u98df\u7269\u8200\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6ea2\u51fa\u6982\u7387\u6765\u6307\u5bfc\u52a8\u4f5c\u751f\u6210\uff0c\u663e\u8457\u51cf\u5c11\u98df\u7269\u6ea2\u51fa\u5e76\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\u5728\u5904\u7406\u591a\u6837\u5316\u548c\u52a8\u6001\u53d8\u5316\u7684\u98df\u7269\u72b6\u6001\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u5bfc\u81f4\u98df\u7269\u6ea2\u51fa\u548c\u53ef\u9760\u6027\u964d\u4f4e\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u98df\u7269\u8200\u53d6\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u6ea2\u51fa\u9884\u6d4b\u5668\u4f30\u8ba1\u5f53\u524d\u89c2\u5bdf\u548c\u52a8\u4f5c\u5e8f\u5217\u4e0b\u7684\u6ea2\u51fa\u6982\u7387\uff0c\u5728\u6a21\u62df\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4f5c\u4e3a\u53ef\u5fae\u5206\u5f15\u5bfc\u4fe1\u53f7\u6307\u5bfc\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u751f\u6210\u66f4\u5b89\u5168\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u57286\u79cd\u98df\u7269\u7c7b\u522b\u4e0a\u8bad\u7ec3\uff0c\u572810\u79cd\u672a\u89c1\u7c7b\u522b\u4e0a\u8bc4\u4f30\uff0c\u8fbe\u523082%\u4efb\u52a1\u6210\u529f\u7387\u548c4%\u6ea2\u51fa\u7387\uff0c\u76f8\u6bd4\u65e0\u5f15\u5bfc\u57fa\u7ebf\u6ea2\u51fa\u7387\u964d\u4f4e40%\u4ee5\u4e0a\u3002", "conclusion": "GRITS\u6846\u67b6\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u7b56\u7565\u6709\u6548\u51cf\u5c11\u4e86\u98df\u7269\u8200\u53d6\u8fc7\u7a0b\u4e2d\u7684\u6ea2\u51fa\u95ee\u9898\uff0c\u5728\u591a\u6837\u98df\u7269\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.00600", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00600", "abs": "https://arxiv.org/abs/2510.00600", "authors": ["Pietro Mazzaglia", "Cansu Sancaktar", "Markus Peschl", "Daniel Dijkman"], "title": "Hybrid Training for Vision-Language-Action Models", "comment": null, "summary": "Using Large Language Models to produce intermediate thoughts, a.k.a.\nChain-of-thought (CoT), before providing an answer has been a successful recipe\nfor solving complex language tasks. In robotics, similar embodied CoT\nstrategies, generating thoughts before actions, have also been shown to lead to\nimproved performance when using Vision-Language-Action models (VLAs). As these\ntechniques increase the length of the model's generated outputs to include the\nthoughts, the inference time is negatively affected. Delaying an agent's\nactions in real-world executions, as in robotic manipulation settings, strongly\naffects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for\nachieving performance improvements? In this work, we explore the idea of Hybrid\nTraining (HyT), a framework that enables VLAs to learn from thoughts and\nbenefit from the associated performance gains, while enabling the possibility\nto leave out CoT generation during inference. Furthermore, by learning to\nconditionally predict a diverse set of outputs, HyT supports flexibility at\ninference time, enabling the model to either predict actions directly, generate\nthoughts or follow instructions. We evaluate the proposed method in a series of\nsimulated benchmarks and real-world experiments.", "AI": {"tldr": "\u63d0\u51faHybrid Training (HyT)\u6846\u67b6\uff0c\u8ba9\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u5b66\u4e60\u601d\u7ef4\u94fe\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u53ef\u4ee5\u9009\u62e9\u8df3\u8fc7\u601d\u7ef4\u751f\u6210\u76f4\u63a5\u8f93\u51fa\u52a8\u4f5c\uff0c\u4ece\u800c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u601d\u7ef4\u94fe(CoT)\u6280\u672f\u867d\u7136\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u4f1a\u663e\u8457\u589e\u52a0\u63a8\u7406\u65f6\u95f4\uff0c\u8fd9\u5728\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\u4e2d\u4e25\u91cd\u5f71\u54cd\u53ef\u7528\u6027\u3002", "method": "HyT\u6846\u67b6\u8ba9\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u5b66\u4e60\u601d\u7ef4\u94fe\uff0c\u4f46\u63a8\u7406\u65f6\u53ef\u4ee5\u6761\u4ef6\u6027\u5730\u9009\u62e9\u662f\u5426\u751f\u6210\u601d\u7ef4\uff0c\u76f4\u63a5\u8f93\u51fa\u52a8\u4f5c\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "HyT\u6846\u67b6\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u4e0e\u63a8\u7406\u6548\u7387\u7684\u5e73\u8861\uff0c\u652f\u6301\u63a8\u7406\u65f6\u7684\u7075\u6d3b\u8f93\u51fa\u9009\u62e9\u3002"}}
{"id": "2510.00619", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00619", "abs": "https://arxiv.org/abs/2510.00619", "authors": ["Michiel Braat", "Maren Buermann", "Marijke van Weperen", "Jan-Pieter Paardekooper"], "title": "What Did I Learn? Operational Competence Assessment for AI-Based Trajectory Planners", "comment": "Accepted for publication in proceedings of the 2025 IEEE\n  International Automated Vehicle Validation Conference", "summary": "Automated driving functions increasingly rely on machine learning for tasks\nlike perception and trajectory planning, requiring large, relevant datasets.\nThe performance of these algorithms depends on how closely the training data\nmatches the task. To ensure reliable functioning, it is crucial to know what is\nincluded in the dataset to assess the trained model's operational risk. We aim\nto enhance the safe use of machine learning in automated driving by developing\na method to recognize situations that an automated vehicle has not been\nsufficiently trained on. This method also improves explainability by describing\nthe dataset at a human-understandable level. We propose modeling driving data\nas knowledge graphs, representing driving scenes with entities and their\nrelationships. These graphs are queried for specific sub-scene configurations\nto check their occurrence in the dataset. We estimate a vehicle's competence in\na driving scene by considering the coverage and complexity of sub-scene\nconfigurations in the training set. Higher complexity scenes require greater\ncoverage for high competence. We apply this method to the NuPlan dataset,\nmodeling it with knowledge graphs and analyzing the coverage of specific\ndriving scenes. This approach helps monitor the competence of machine learning\nmodels trained on the dataset, which is essential for trustworthy AI to be\ndeployed in automated driving.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u672a\u5145\u5206\u8bad\u7ec3\u7684\u573a\u666f\uff0c\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u6570\u636e\u4e2d\u7279\u5b9a\u5b50\u573a\u666f\u914d\u7f6e\u7684\u8986\u76d6\u7387\u548c\u590d\u6742\u6027\u6765\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u786e\u4fdd\u673a\u5668\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b89\u5168\u4f7f\u7528\uff0c\u9700\u8981\u8bc6\u522b\u6a21\u578b\u672a\u5145\u5206\u8bad\u7ec3\u7684\u573a\u666f\uff0c\u5e76\u63d0\u9ad8\u6570\u636e\u96c6\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u9a7e\u9a76\u6570\u636e\u5efa\u6a21\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff0c\u8868\u793a\u573a\u666f\u4e2d\u7684\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\uff0c\u901a\u8fc7\u67e5\u8be2\u7279\u5b9a\u5b50\u573a\u666f\u914d\u7f6e\u6765\u68c0\u67e5\u5176\u5728\u6570\u636e\u96c6\u4e2d\u7684\u51fa\u73b0\u60c5\u51b5\u3002", "result": "\u5728NuPlan\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u6210\u529f\u5efa\u6a21\u77e5\u8bc6\u56fe\u8c31\u5e76\u5206\u6790\u7279\u5b9a\u9a7e\u9a76\u573a\u666f\u7684\u8986\u76d6\u60c5\u51b5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u76d1\u63a7\u57fa\u4e8e\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5bf9\u4e8e\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u90e8\u7f72\u53ef\u4fe1AI\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.00630", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00630", "abs": "https://arxiv.org/abs/2510.00630", "authors": ["Federico Oliva", "Tom Shaked", "Daniele Carnevale", "Amir Degani"], "title": "Trajectory Based Observer Design: A Framework for Lightweight Sensor Fusion", "comment": null, "summary": "Efficient observer design and accurate sensor fusion are key in state\nestimation. This work proposes an optimization-based methodology, termed\nTrajectory Based Optimization Design (TBOD), allowing the user to easily design\nobservers for general nonlinear systems and multi-sensor setups. Starting from\nparametrized observer dynamics, the proposed method considers a finite set of\npre-recorded measurement trajectories from the nominal plant and exploits them\nto tune the observer parameters through numerical optimization. This research\nhinges on the classic observer's theory and Moving Horizon Estimators\nmethodology. Optimization is exploited to ease the observer's design, providing\nthe user with a lightweight, general-purpose sensor fusion methodology. TBOD's\nmain characteristics are the capability to handle general sensors efficiently\nand in a modular way and, most importantly, its straightforward tuning\nprocedure. The TBOD's performance is tested on a terrestrial rover localization\nproblem, combining IMU and ranging sensors provided by Ultra Wide Band\nantennas, and validated through a motion-capture system. Comparison with an\nExtended Kalman Filter is also provided, matching its position estimation\naccuracy and significantly improving in the orientation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u7684\u4f18\u5316\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u975e\u7ebf\u6027\u7cfb\u7edf\u548c\u591a\u4f20\u611f\u5668\u8bbe\u7f6e\u7684\u89c2\u6d4b\u5668\uff0c\u901a\u8fc7\u6570\u503c\u4f18\u5316\u8c03\u6574\u89c2\u6d4b\u5668\u53c2\u6570\u3002", "motivation": "\u9ad8\u6548\u7684\u89c2\u6d4b\u5668\u8bbe\u8ba1\u548c\u7cbe\u786e\u7684\u4f20\u611f\u5668\u878d\u5408\u5728\u72b6\u6001\u4f30\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u901a\u7528\u7684\u65b9\u6cd5\u6765\u7b80\u5316\u89c2\u6d4b\u5668\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u9884\u8bb0\u5f55\u7684\u6d4b\u91cf\u8f68\u8ff9\uff0c\u901a\u8fc7\u6570\u503c\u4f18\u5316\u8c03\u6574\u53c2\u6570\u5316\u89c2\u6d4b\u5668\u52a8\u6001\uff0c\u7ed3\u5408\u7ecf\u5178\u89c2\u6d4b\u5668\u7406\u8bba\u548c\u79fb\u52a8\u6c34\u5e73\u4f30\u8ba1\u5668\u65b9\u6cd5\u3002", "result": "\u5728\u9646\u5730\u6f2b\u6e38\u8f66\u5b9a\u4f4d\u95ee\u9898\u4e2d\u6d4b\u8bd5\uff0c\u4e0e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u76f8\u6bd4\uff0c\u4f4d\u7f6e\u4f30\u8ba1\u7cbe\u5ea6\u76f8\u5f53\uff0c\u4f46\u65b9\u5411\u4f30\u8ba1\u663e\u8457\u6539\u5584\u3002", "conclusion": "TBOD\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u76f4\u63a5\u7684\u8c03\u4f18\u7a0b\u5e8f\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u6a21\u5757\u5316\u5730\u5904\u7406\u901a\u7528\u4f20\u611f\u5668\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u3002"}}
{"id": "2510.00646", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00646", "abs": "https://arxiv.org/abs/2510.00646", "authors": ["Haoyang Wang", "Xinyu Luo", "Wenhua Ding", "Jingao Xu", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Haitao Zhang", "Yunhao Liu", "Xinlei Chen"], "title": "Enabling High-Frequency Cross-Modality Visual Positioning Service for Accurate Drone Landing", "comment": "15 pages, 23 figures", "summary": "After years of growth, drone-based delivery is transforming logistics. At its\ncore, real-time 6-DoF drone pose tracking enables precise flight control and\naccurate drone landing. With the widespread availability of urban 3D maps, the\nVisual Positioning Service (VPS), a mobile pose estimation system, has been\nadapted to enhance drone pose tracking during the landing phase, as\nconventional systems like GPS are unreliable in urban environments due to\nsignal attenuation and multi-path propagation. However, deploying the current\nVPS on drones faces limitations in both estimation accuracy and efficiency. In\nthis work, we redesign drone-oriented VPS with the event camera and introduce\nEV-Pose to enable accurate, high-frequency 6-DoF pose tracking for accurate\ndrone landing. EV-Pose introduces a spatio-temporal feature-instructed pose\nestimation module that extracts a temporal distance field to enable 3D point\nmap matching for pose estimation; and a motion-aware hierarchical fusion and\noptimization scheme to enhance the above estimation in accuracy and efficiency,\nby utilizing drone motion in the \\textit{early stage} of event filtering and\nthe \\textit{later stage} of pose optimization. Evaluation shows that EV-Pose\nachieves a rotation accuracy of 1.34$\\degree$ and a translation accuracy of\n6.9$mm$ with a tracking latency of 10.08$ms$, outperforming baselines by\n$>$50\\%, \\tmcrevise{thus enabling accurate drone landings.} Demo:\nhttps://ev-pose.github.io/", "AI": {"tldr": "EV-Pose\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65e0\u4eba\u673a\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7\u65f6\u7a7a\u7279\u5f81\u5f15\u5bfc\u7684\u59ff\u6001\u4f30\u8ba1\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u5c42\u6b21\u878d\u5408\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u76846\u81ea\u7531\u5ea6\u59ff\u6001\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7740\u9646\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfGPS\u5728\u57ce\u5e02\u573a\u666f\u4e2d\u7531\u4e8e\u4fe1\u53f7\u8870\u51cf\u548c\u591a\u5f84\u4f20\u64ad\u4e0d\u53ef\u9760\uff0c\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\u5728\u65e0\u4eba\u673a\u4e0a\u90e8\u7f72\u65f6\u5b58\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u9650\u5236\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u9762\u5411\u65e0\u4eba\u673a\u7684\u89c6\u89c9\u5b9a\u4f4d\u670d\u52a1\u3002", "method": "\u5f15\u5165\u65f6\u7a7a\u7279\u5f81\u5f15\u5bfc\u7684\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u63d0\u53d6\u65f6\u5e8f\u8ddd\u79bb\u573a\u8fdb\u884c3D\u70b9\u4e91\u5339\u914d\uff1b\u91c7\u7528\u8fd0\u52a8\u611f\u77e5\u7684\u5c42\u6b21\u878d\u5408\u4f18\u5316\u65b9\u6848\uff0c\u5728\u4e8b\u4ef6\u8fc7\u6ee4\u65e9\u671f\u9636\u6bb5\u548c\u59ff\u6001\u4f18\u5316\u540e\u671f\u9636\u6bb5\u5229\u7528\u65e0\u4eba\u673a\u8fd0\u52a8\u4fe1\u606f\u3002", "result": "EV-Pose\u5b9e\u73b0\u4e861.34\u00b0\u7684\u65cb\u8f6c\u7cbe\u5ea6\u548c6.9mm\u7684\u5e73\u79fb\u7cbe\u5ea6\uff0c\u8ddf\u8e2a\u5ef6\u8fdf\u4e3a10.08ms\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc750%\u3002", "conclusion": "EV-Pose\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u65e0\u4eba\u673a\u7740\u9646\uff0c\u4e3a\u65e0\u4eba\u673a\u7269\u6d41\u914d\u9001\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u59ff\u6001\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00682", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00682", "abs": "https://arxiv.org/abs/2510.00682", "authors": ["Shengzhi Wang", "Niels Dehio", "Xuanqi Zeng", "Xian Yang", "Lingwei Zhang", "Yun-Hui Liu", "K. W. Samuel Au"], "title": "Shared Object Manipulation with a Team of Collaborative Quadrupeds", "comment": "8 pages, 9 figures, submitted to The 2026 American Control Conference", "summary": "Utilizing teams of multiple robots is advantageous for handling bulky\nobjects. Many related works focus on multi-manipulator systems, which are\nlimited by workspace constraints. In this paper, we extend a classical hybrid\nmotion-force controller to a team of legged manipulator systems, enabling\ncollaborative loco-manipulation of rigid objects with a force-closed grasp. Our\nnovel approach allows the robots to flexibly coordinate their movements,\nachieving efficient and stable object co-manipulation and transport, validated\nthrough extensive simulations and real-world experiments.", "AI": {"tldr": "\u5c06\u7ecf\u5178\u6df7\u5408\u8fd0\u52a8-\u529b\u63a7\u5236\u5668\u6269\u5c55\u5230\u817f\u5f0f\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u534f\u4f5c\u642c\u8fd0\u521a\u6027\u7269\u4f53", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5904\u7406\u7b28\u91cd\u7269\u4f53\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u591a\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u53d7\u9650\u4e8e\u5de5\u4f5c\u7a7a\u95f4\u7ea6\u675f", "method": "\u6269\u5c55\u7ecf\u5178\u6df7\u5408\u8fd0\u52a8-\u529b\u63a7\u5236\u5668\u5230\u817f\u5f0f\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u91c7\u7528\u529b\u95ed\u5408\u6293\u53d6\u65b9\u5f0f\u5b9e\u73b0\u534f\u4f5c\u642c\u8fd0", "result": "\u673a\u5668\u4eba\u80fd\u591f\u7075\u6d3b\u534f\u8c03\u8fd0\u52a8\uff0c\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u7269\u4f53\u534f\u4f5c\u642c\u8fd0\u548c\u8fd0\u8f93\uff0c\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u817f\u5f0f\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u534f\u4f5c\u642c\u8fd0\u80fd\u529b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u591a\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236"}}
{"id": "2510.00695", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.00695", "abs": "https://arxiv.org/abs/2510.00695", "authors": ["Myungkyu Koo", "Daewon Choi", "Taeyoung Kim", "Kyungmin Lee", "Changyeon Kim", "Youngyo Seo", "Jinwoo Shin"], "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy", "comment": "Project page: https://myungkyukoo.github.io/hamlet/", "summary": "Inherently, robotic manipulation tasks are history-dependent: leveraging past\ncontext could be beneficial. However, most existing Vision-Language-Action\nmodels (VLAs) have been designed without considering this aspect, i.e., they\nrely solely on the current observation, ignoring preceding context. In this\npaper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the\nhistorical context during action prediction. Specifically, we introduce moment\ntokens that compactly encode perceptual information at each timestep. Their\nrepresentations are initialized with time-contrastive learning, allowing them\nto better capture temporally distinctive aspects. Next, we employ a lightweight\nmemory module that integrates the moment tokens across past timesteps into\nmemory features, which are then leveraged for action prediction. Through\nempirical evaluation, we show that HAMLET successfully transforms a\nstate-of-the-art VLA into a history-aware policy, especially demonstrating\nsignificant improvements on long-horizon tasks that require historical context.\nIn particular, on top of GR00T N1.5, HAMLET achieves an average success rate of\n76.4% on history-dependent real-world tasks, surpassing the baseline\nperformance by 47.2%. Furthermore, HAMLET pushes prior art performance from\n64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on\nLIBERO, highlighting its effectiveness even under generic robot-manipulation\nbenchmarks.", "AI": {"tldr": "HAMLET\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u523b\u4ee4\u724c\u548c\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u6a21\u5757\uff0c\u4f7f\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u80fd\u591f\u5229\u7528\u5386\u53f2\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fdb\u884c\u52a8\u4f5c\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u9700\u8981\u5386\u53f2\u4f9d\u8d56\u7684\u957f\u671f\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4ec5\u4f9d\u8d56\u5f53\u524d\u89c2\u5bdf\u800c\u5ffd\u7565\u5386\u53f2\u4e0a\u4e0b\u6587\uff0c\u4f46\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u672c\u8d28\u4e0a\u662f\u5386\u53f2\u4f9d\u8d56\u7684\uff0c\u5229\u7528\u8fc7\u53bb\u4e0a\u4e0b\u6587\u53ef\u80fd\u5e26\u6765\u76ca\u5904\u3002", "method": "\u63d0\u51fa\u65f6\u523b\u4ee4\u724c\u7d27\u51d1\u7f16\u7801\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u611f\u77e5\u4fe1\u606f\uff0c\u901a\u8fc7\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u521d\u59cb\u5316\u8868\u793a\uff1b\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u6a21\u5757\u6574\u5408\u8fc7\u53bb\u65f6\u523b\u4ee4\u724c\u4e3a\u8bb0\u5fc6\u7279\u5f81\uff0c\u7528\u4e8e\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "\u5728GR00T N1.5\u4e0a\uff0cHAMLET\u5728\u5386\u53f2\u4f9d\u8d56\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u738776.4%\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534747.2%\uff1b\u5728RoboCasa Kitchen\u4e0a\u4ece64.1%\u63d0\u5347\u523066.4%\uff0c\u5728LIBERO\u4e0a\u4ece95.6%\u63d0\u5347\u523097.7%\u3002", "conclusion": "HAMLET\u6210\u529f\u5c06\u6700\u5148\u8fdb\u7684VLA\u8f6c\u5316\u4e3a\u5386\u53f2\u611f\u77e5\u7b56\u7565\uff0c\u5728\u9700\u8981\u5386\u53f2\u4e0a\u4e0b\u6587\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.00703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00703", "abs": "https://arxiv.org/abs/2510.00703", "authors": ["Andrea Bussolan", "Stefano Baraldo", "Oliver Avram", "Pablo Urcola", "Luis Montesano", "Luca Maria Gambardella", "Anna Valente"], "title": "MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration", "comment": null, "summary": "Human-robot collaboration (HRC) is a key focus of Industry 5.0, aiming to\nenhance worker productivity while ensuring well-being. The ability to perceive\nhuman psycho-physical states, such as stress and cognitive load, is crucial for\nadaptive and human-aware robotics. This paper introduces MultiPhysio-HRC, a\nmultimodal dataset containing physiological, audio, and facial data collected\nduring real-world HRC scenarios. The dataset includes electroencephalography\n(EEG), electrocardiography (ECG), electrodermal activity (EDA), respiration\n(RESP), electromyography (EMG), voice recordings, and facial action units. The\ndataset integrates controlled cognitive tasks, immersive virtual reality\nexperiences, and industrial disassembly activities performed manually and with\nrobotic assistance, to capture a holistic view of the participants' mental\nstates. Rich ground truth annotations were obtained using validated\npsychological self-assessment questionnaires. Baseline models were evaluated\nfor stress and cognitive load classification, demonstrating the dataset's\npotential for affective computing and human-aware robotics research.\nMultiPhysio-HRC is publicly available to support research in human-centered\nautomation, workplace well-being, and intelligent robotic systems.", "AI": {"tldr": "MultiPhysio-HRC\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u5728\u771f\u5b9e\u4eba\u673a\u534f\u4f5c\u573a\u666f\u4e2d\u6536\u96c6\u7684\u751f\u7406\u3001\u97f3\u9891\u548c\u9762\u90e8\u6570\u636e\uff0c\u7528\u4e8e\u60c5\u611f\u8ba1\u7b97\u548c\u4eba\u7c7b\u611f\u77e5\u673a\u5668\u4eba\u7814\u7a76\u3002", "motivation": "\u5de5\u4e1a5.0\u4e2d\u4eba\u673a\u534f\u4f5c\u9700\u8981\u611f\u77e5\u4eba\u7c7b\u5fc3\u7406\u751f\u7406\u72b6\u6001\uff08\u5982\u538b\u529b\u548c\u8ba4\u77e5\u8d1f\u8377\uff09\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u4eba\u7c7b\u611f\u77e5\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u5305\u542bEEG\u3001ECG\u3001EDA\u3001RESP\u3001EMG\u3001\u8bed\u97f3\u8bb0\u5f55\u548c\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u53d7\u63a7\u8ba4\u77e5\u4efb\u52a1\u3001\u6c89\u6d78\u5f0f\u865a\u62df\u73b0\u5b9e\u4f53\u9a8c\u548c\u5de5\u4e1a\u62c6\u5378\u6d3b\u52a8\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u7684\u5fc3\u7406\u81ea\u6211\u8bc4\u4f30\u95ee\u5377\u83b7\u5f97\u4e30\u5bcc\u7684\u5730\u9762\u771f\u5b9e\u6ce8\u91ca\u3002", "result": "\u8bc4\u4f30\u4e86\u538b\u529b\u548c\u8ba4\u77e5\u8d1f\u8377\u5206\u7c7b\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u8be5\u6570\u636e\u96c6\u5728\u60c5\u611f\u8ba1\u7b97\u548c\u4eba\u7c7b\u611f\u77e5\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "MultiPhysio-HRC\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u652f\u6301\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u81ea\u52a8\u5316\u3001\u5de5\u4f5c\u573a\u6240\u798f\u7949\u548c\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2510.00726", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00726", "abs": "https://arxiv.org/abs/2510.00726", "authors": ["Giovanni Minelli", "Giulio Turrisi", "Victor Barasuol", "Claudio Semini"], "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation", "comment": "Code and data available at https://github.com/iit-DLSLab/croSTAta", "summary": "Learning robotic manipulation policies through supervised learning from\ndemonstrations remains challenging when policies encounter execution variations\nnot explicitly covered during training. While incorporating historical context\nthrough attention mechanisms can improve robustness, standard approaches\nprocess all past states in a sequence without explicitly modeling the temporal\nstructure that demonstrations may include, such as failure and recovery\npatterns. We propose a Cross-State Transition Attention Transformer that\nemploys a novel State Transition Attention (STA) mechanism to modulate standard\nattention weights based on learned state evolution patterns, enabling policies\nto better adapt their behavior based on execution history. Our approach\ncombines this structured attention with temporal masking during training, where\nvisual information is randomly removed from recent timesteps to encourage\ntemporal reasoning from historical context. Evaluation in simulation shows that\nSTA consistently outperforms standard cross-attention and temporal modeling\napproaches like TCN and LSTM networks across all tasks, achieving more than 2x\nimprovement over cross-attention on precision-critical tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u72b6\u6001\u8f6c\u6362\u6ce8\u610f\u529bTransformer\uff0c\u901a\u8fc7\u72b6\u6001\u8f6c\u6362\u6ce8\u610f\u529b\u673a\u5236\u6765\u5efa\u6a21\u72b6\u6001\u6f14\u5316\u6a21\u5f0f\uff0c\u7ed3\u5408\u8bad\u7ec3\u65f6\u7684\u65f6\u5e8f\u63a9\u7801\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5bf9\u6267\u884c\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u6267\u884c\u9047\u5230\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u660e\u786e\u8986\u76d6\u7684\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u6f14\u793a\u6570\u636e\u4e2d\u7684\u65f6\u5e8f\u7ed3\u6784\uff08\u5982\u5931\u8d25\u548c\u6062\u590d\u6a21\u5f0f\uff09\u3002", "method": "\u4f7f\u7528\u72b6\u6001\u8f6c\u6362\u6ce8\u610f\u529b\u673a\u5236\u6765\u8c03\u8282\u6807\u51c6\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u57fa\u4e8e\u5b66\u4e60\u5230\u7684\u72b6\u6001\u6f14\u5316\u6a21\u5f0f\uff1b\u7ed3\u5408\u8bad\u7ec3\u65f6\u7684\u65f6\u5e8f\u63a9\u7801\uff0c\u968f\u673a\u79fb\u9664\u6700\u8fd1\u65f6\u95f4\u6b65\u7684\u89c6\u89c9\u4fe1\u606f\u4ee5\u9f13\u52b1\u4ece\u5386\u53f2\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u65f6\u5e8f\u63a8\u7406\u3002", "result": "\u5728\u4eff\u771f\u8bc4\u4f30\u4e2d\uff0cSTA\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u6807\u51c6\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u65f6\u5e8f\u5efa\u6a21\u65b9\u6cd5\uff08\u5982TCN\u548cLSTM\u7f51\u7edc\uff09\uff0c\u5728\u7cbe\u5ea6\u5173\u952e\u4efb\u52a1\u4e0a\u6bd4\u4ea4\u53c9\u6ce8\u610f\u529b\u63d0\u5347\u8d85\u8fc72\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u72b6\u6001\u8f6c\u6362\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5bf9\u6267\u884c\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u72b6\u6001\u6f14\u5316\u6a21\u5f0f\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.00770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00770", "abs": "https://arxiv.org/abs/2510.00770", "authors": ["Tianle Ni", "Xiao Chen", "Hamid Sadeghian", "Sami Haddadin"], "title": "Tele-rehabilitation with online skill transfer and adaptation in $\\mathbb{R}^3 \\times \\mathit{S}^3$", "comment": null, "summary": "This paper proposes a tele-teaching framework for the domain of\nrobot-assisted tele-rehabilitation. The system connects two robotic\nmanipulators on therapist and patient side via bilateral teleoperation,\nenabling a therapist to remotely demonstrate rehabilitation exercises that are\nexecuted by the patient-side robot. A 6-DoF Dynamical Movement Primitives\nformulation is employed to jointly encode translational and rotational motions\nin $\\mathbb{R}^3 \\times \\mathit{S}^3$ space, ensuring accurate trajectory\nreproduction. The framework supports smooth transitions between therapist-led\nguidance and patient passive training, while allowing adaptive adjustment of\nmotion. Experiments with 7-DoF manipulators demonstrate the feasibility of the\napproach, highlighting its potential for personalized and remotely supervised\nrehabilitation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u8fdc\u7a0b\u5eb7\u590d\u7684\u8fdc\u7a0b\u6559\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8fb9\u9065\u64cd\u4f5c\u8fde\u63a5\u6cbb\u7597\u5e08\u548c\u60a3\u8005\u7aef\u7684\u673a\u5668\u4eba\uff0c\u4f7f\u6cbb\u7597\u5e08\u80fd\u591f\u8fdc\u7a0b\u6f14\u793a\u5eb7\u590d\u8bad\u7ec3\u52a8\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5b9e\u73b0\u8fdc\u7a0b\u5eb7\u590d\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u8ba9\u6cbb\u7597\u5e08\u53ef\u4ee5\u8fdc\u7a0b\u6307\u5bfc\u60a3\u8005\u8fdb\u884c\u5eb7\u590d\u8bad\u7ec3\uff0c\u63d0\u9ad8\u5eb7\u590d\u8bad\u7ec3\u7684\u53ef\u53ca\u6027\u548c\u4e2a\u6027\u5316\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528\u53cc\u8fb9\u9065\u64cd\u4f5c\u8fde\u63a5\u6cbb\u7597\u5e08\u548c\u60a3\u8005\u7aef\u7684\u673a\u5668\u4eba\uff0c\u91c7\u75286\u81ea\u7531\u5ea6\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\u5728R\u00b3\u00d7S\u00b3\u7a7a\u95f4\u4e2d\u7f16\u7801\u5e73\u79fb\u548c\u65cb\u8f6c\u8fd0\u52a8\uff0c\u652f\u6301\u6cbb\u7597\u5e08\u5f15\u5bfc\u548c\u60a3\u8005\u88ab\u52a8\u8bad\u7ec3\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u4f7f\u75287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e2a\u6027\u5316\u548c\u8fdc\u7a0b\u76d1\u7763\u5eb7\u590d\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u8fdc\u7a0b\u5eb7\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u7684\u8f68\u8ff9\u518d\u73b0\u548c\u81ea\u9002\u5e94\u8fd0\u52a8\u8c03\u6574\u3002"}}
{"id": "2510.00783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00783", "abs": "https://arxiv.org/abs/2510.00783", "authors": ["Thanh Nguyen Canh", "Haolan Zhang", "Xiem HoangVan", "Nak Young Chong"], "title": "Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions", "comment": null, "summary": "Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of\nresearch within robotics and computer vision, focusing on the simultaneous\nlocalization of robotic systems and associating semantic information to\nconstruct the most accurate and complete comprehensive model of the surrounding\nenvironment. Since the first foundational work in Semantic SLAM appeared more\nthan two decades ago, this field has received increasing attention across\nvarious scientific communities. Despite its significance, the field lacks\ncomprehensive surveys encompassing recent advances and persistent challenges.\nIn response, this study provides a thorough examination of the state-of-the-art\nof Semantic SLAM techniques, with the aim of illuminating current trends and\nkey obstacles. Beginning with an in-depth exploration of the evolution of\nvisual SLAM, this study outlines its strengths and unique characteristics,\nwhile also critically assessing previous survey literature. Subsequently, a\nunified problem formulation and evaluation of the modular solution framework is\nproposed, which divides the problem into discrete stages, including visual\nlocalization, semantic feature extraction, mapping, data association, and loop\nclosure optimization. Moreover, this study investigates alternative\nmethodologies such as deep learning and the utilization of large language\nmodels, alongside a review of relevant research about contemporary SLAM\ndatasets. Concluding with a discussion on potential future research directions,\nthis study serves as a comprehensive resource for researchers seeking to\nnavigate the complex landscape of Semantic SLAM.", "AI": {"tldr": "\u672c\u6587\u662f\u5bf9\u8bed\u4e49SLAM\u9886\u57df\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u8be5\u9886\u57df20\u591a\u5e74\u6765\u7684\u53d1\u5c55\u5386\u7a0b\u3001\u6700\u65b0\u8fdb\u5c55\u548c\u6301\u7eed\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u95ee\u9898\u6846\u67b6\u548c\u6a21\u5757\u5316\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u8bed\u4e49SLAM\u662f\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u6db5\u76d6\u6700\u65b0\u8fdb\u5c55\u548c\u6301\u7eed\u6311\u6218\u7684\u7efc\u8ff0\u6587\u732e\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u63a2\u7d22\u89c6\u89c9SLAM\u7684\u6f14\u53d8\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u95ee\u9898\u516c\u5f0f\u5316\u548c\u6a21\u5757\u5316\u89e3\u51b3\u65b9\u6848\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u89c6\u89c9\u5b9a\u4f4d\u3001\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u3001\u5efa\u56fe\u3001\u6570\u636e\u5173\u8054\u548c\u95ed\u73af\u4f18\u5316\u7b49\u9636\u6bb5\u3002", "result": "\u63d0\u4f9b\u4e86\u8bed\u4e49SLAM\u6280\u672f\u7684\u6700\u65b0\u6280\u672f\u72b6\u6001\u5206\u6790\uff0c\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7b49\u66ff\u4ee3\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u4ee5\u53ca\u5f53\u4ee3SLAM\u6570\u636e\u96c6\u7684\u7efc\u8ff0\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u5728\u590d\u6742\u7684\u8bed\u4e49SLAM\u9886\u57df\u4e2d\u5bfc\u822a\u63d0\u4f9b\u4e86\u5168\u9762\u8d44\u6e90\uff0c\u5e76\u8ba8\u8bba\u4e86\u6f5c\u5728\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.00814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00814", "abs": "https://arxiv.org/abs/2510.00814", "authors": ["Kai Tang", "Dipankar Bhattacharya", "Hang Xu", "Fuyuki Tokuda", "Norman C. Tien", "Kazuhiro Kosuge"], "title": "RTFF: Random-to-Target Fabric Flattening Policy using Dual-Arm Manipulator", "comment": "9 pages, 6 figures, conference", "summary": "Robotic fabric manipulation in garment production for sewing, cutting, and\nironing requires reliable flattening and alignment, yet remains challenging due\nto fabric deformability, effectively infinite degrees of freedom, and frequent\nocclusions from wrinkles, folds, and the manipulator's End-Effector (EE) and\narm. To address these issues, this paper proposes the first Random-to-Target\nFabric Flattening (RTFF) policy, which aligns a random wrinkled fabric state to\nan arbitrary wrinkle-free target state. The proposed policy adopts a hybrid\nImitation Learning-Visual Servoing (IL-VS) framework, where IL learns with\nexplicit fabric models for coarse alignment of the wrinkled fabric toward a\nwrinkle-free state near the target, and VS ensures fine alignment to the\ntarget. Central to this framework is a template-based mesh that offers precise\ntarget state representation, wrinkle-aware geometry prediction, and consistent\nvertex correspondence across RTFF manipulation steps, enabling robust\nmanipulation and seamless IL-VS switching. Leveraging the power of mesh, a\nnovel IL solution for RTFF-Mesh Action Chunking Transformer (MACT)-is then\nproposed by conditioning the mesh information into a Transformer-based policy.\nThe RTFF policy is validated on a real dual-arm tele-operation system, showing\nzero-shot alignment to different targets, high accuracy, and strong\ngeneralization across fabrics and scales. Project website:\nhttps://kaitang98.github.io/RTFF_Policy/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u968f\u673a\u5230\u76ee\u6807\u7ec7\u7269\u5c55\u5e73(RTFF)\u7b56\u7565\uff0c\u91c7\u7528\u6df7\u5408\u6a21\u4eff\u5b66\u4e60-\u89c6\u89c9\u4f3a\u670d(IL-VS)\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u677f\u7684\u7f51\u683c\u8868\u793a\u5b9e\u73b0\u4ece\u968f\u673a\u8936\u76b1\u72b6\u6001\u5230\u4efb\u610f\u65e0\u8936\u76b1\u76ee\u6807\u72b6\u6001\u7684\u7cbe\u786e\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u7ec7\u7269\u751f\u4ea7\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6311\u6218\uff0c\u5305\u62ec\u7ec7\u7269\u53ef\u53d8\u5f62\u6027\u3001\u65e0\u9650\u81ea\u7531\u5ea6\u4ee5\u53ca\u8936\u76b1\u3001\u6298\u53e0\u548c\u673a\u68b0\u81c2\u906e\u6321\u7b49\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u7ec7\u7269\u5c55\u5e73\u548c\u5bf9\u9f50\u3002", "method": "\u91c7\u7528\u6df7\u5408IL-VS\u6846\u67b6\uff1aIL\u5b66\u4e60\u4f7f\u7528\u663e\u5f0f\u7ec7\u7269\u6a21\u578b\u8fdb\u884c\u7c97\u5bf9\u9f50\uff0cVS\u786e\u4fdd\u7cbe\u7ec6\u5bf9\u9f50\uff1b\u63d0\u51fa\u57fa\u4e8e\u6a21\u677f\u7684\u7f51\u683c\u8868\u793a\u548cRTFF-MACT Transformer\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u53cc\u81c2\u9065\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u5bf9\u9f50\u4e0d\u540c\u76ee\u6807\u3001\u9ad8\u7cbe\u5ea6\u4ee5\u53ca\u8de8\u7ec7\u7269\u548c\u5c3a\u5ea6\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RTFF\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u7ec7\u7269\u5c55\u5e73\u548c\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u4e3a\u670d\u88c5\u751f\u4ea7\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00933", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00933", "abs": "https://arxiv.org/abs/2510.00933", "authors": ["Sara Strakosova", "Petr Novak", "Petr Kadera"], "title": "Product-oriented Product-Process-Resource Asset Network and its Representation in AutomationML for Asset Administration Shell", "comment": "This work has been submitted to the IEEE for possible publication. 8\n  pages, 6 figures", "summary": "Current products, especially in the automotive sector, pose complex technical\nsystems having a multi-disciplinary mechatronic nature. Industrial standards\nsupporting system engineering and production typically (i) address the\nproduction phase only, but do not cover the complete product life cycle, and\n(ii) focus on production processes and resources rather than the products\nthemselves. The presented approach is motivated by incorporating impacts of\nend-of-life phase of the product life cycle into the engineering phase. This\npaper proposes a modelling approach coming up from the Product-Process-Resource\n(PPR) modeling paradigm. It combines requirements on (i) respecting the product\nstructure as a basis for the model, and (ii) it incorporates repairing,\nremanufacturing, or upcycling within cyber-physical production systems. The\nproposed model called PoPAN should accompany the product during the entire life\ncycle as a digital shadow encapsulated within the Asset Administration Shell of\na product. To facilitate the adoption of the proposed paradigm, the paper also\nproposes serialization of the model in the AutomationML data format. The model\nis demonstrated on a use-case for disassembling electric vehicle batteries to\nsupport their remanufacturing for stationary battery applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePPR\u5efa\u6a21\u8303\u5f0f\u7684\u4ea7\u54c1\u751f\u547d\u5468\u671f\u7ba1\u7406\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5c06\u4ea7\u54c1\u751f\u547d\u5468\u671f\u672b\u7aef\u9636\u6bb5\u7684\u5f71\u54cd\u878d\u5165\u5de5\u7a0b\u9636\u6bb5\uff0c\u652f\u6301\u4ea7\u54c1\u5728\u6574\u4e2a\u751f\u547d\u5468\u671f\u4e2d\u7684\u7ef4\u4fee\u3001\u518d\u5236\u9020\u548c\u5347\u7ea7\u56de\u6536\u3002", "motivation": "\u5f53\u524d\u5de5\u4e1a\u6807\u51c6\u4e3b\u8981\u5173\u6ce8\u751f\u4ea7\u9636\u6bb5\u800c\u975e\u5b8c\u6574\u4ea7\u54c1\u751f\u547d\u5468\u671f\uff0c\u4e14\u4fa7\u91cd\u4e8e\u751f\u4ea7\u8fc7\u7a0b\u800c\u975e\u4ea7\u54c1\u672c\u8eab\u3002\u9700\u8981\u5c06\u4ea7\u54c1\u751f\u547d\u5468\u671f\u672b\u7aef\u9636\u6bb5\u7684\u5f71\u54cd\u878d\u5165\u5de5\u7a0b\u9636\u6bb5\u3002", "method": "\u57fa\u4e8e\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90(PPR)\u5efa\u6a21\u8303\u5f0f\uff0c\u63d0\u51faPoPAN\u6a21\u578b\u4f5c\u4e3a\u4ea7\u54c1\u7684\u6570\u5b57\u5f71\u5b50\uff0c\u5c01\u88c5\u5728\u8d44\u4ea7\u7ba1\u7406\u58f3\u4e2d\uff0c\u5e76\u4f7f\u7528AutomationML\u6570\u636e\u683c\u5f0f\u8fdb\u884c\u5e8f\u5217\u5316\u3002", "result": "\u5f00\u53d1\u4e86PoPAN\u6a21\u578b\uff0c\u80fd\u591f\u4f34\u968f\u4ea7\u54c1\u6574\u4e2a\u751f\u547d\u5468\u671f\uff0c\u652f\u6301\u7ef4\u4fee\u3001\u518d\u5236\u9020\u548c\u5347\u7ea7\u56de\u6536\u6d3b\u52a8\uff0c\u5e76\u5728\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u62c6\u5378\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684PoPAN\u6a21\u578b\u4e3a\u590d\u6742\u673a\u7535\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u751f\u547d\u5468\u671f\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u5173\u6ce8\u53ef\u6301\u7eed\u6027\u548c\u5faa\u73af\u7ecf\u6d4e\uff0c\u652f\u6301\u4ea7\u54c1\u5728\u751f\u547d\u5468\u671f\u672b\u7aef\u7684\u4ef7\u503c\u56de\u6536\u3002"}}
{"id": "2510.00942", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00942", "abs": "https://arxiv.org/abs/2510.00942", "authors": ["Reza Vafaee", "Kian Behzad", "Milad Siami", "Luca Carlone", "Ali Jadbabaie"], "title": "Non-submodular Visual Attention for Robot Navigation", "comment": "22 pages; Accepted to appear in IEEE Transactions on Robotics (T-RO)", "summary": "This paper presents a task-oriented computational framework to enhance\nVisual-Inertial Navigation (VIN) in robots, addressing challenges such as\nlimited time and energy resources. The framework strategically selects visual\nfeatures using a Mean Squared Error (MSE)-based, non-submodular objective\nfunction and a simplified dynamic anticipation model. To address the\nNP-hardness of this problem, we introduce four polynomial-time approximation\nalgorithms: a classic greedy method with constant-factor guarantees; a low-rank\ngreedy variant that significantly reduces computational complexity; a\nrandomized greedy sampler that balances efficiency and solution quality; and a\nlinearization-based selector based on a first-order Taylor expansion for\nnear-constant-time execution. We establish rigorous performance bounds by\nleveraging submodularity ratios, curvature, and element-wise curvature\nanalyses. Extensive experiments on both standardized benchmarks and a custom\ncontrol-aware platform validate our theoretical results, demonstrating that\nthese methods achieve strong approximation guarantees while enabling real-time\ndeployment.", "AI": {"tldr": "\u63d0\u51fa\u4efb\u52a1\u5bfc\u5411\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7MSE\u76ee\u6807\u51fd\u6570\u548c\u52a8\u6001\u9884\u6d4b\u6a21\u578b\u4f18\u5316\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u4e2d\u7684\u7279\u5f81\u9009\u62e9\uff0c\u5f00\u53d1\u56db\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\u89e3\u51b3NP\u96be\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u6709\u9650\u65f6\u95f4\u548c\u80fd\u91cf\u8d44\u6e90\u4e0b\u7684\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u6311\u6218\uff0c\u63d0\u5347\u5bfc\u822a\u6548\u7387\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "method": "\u4f7f\u7528MSE\u975e\u5b50\u6a21\u76ee\u6807\u51fd\u6570\u548c\u7b80\u5316\u52a8\u6001\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u63d0\u51fa\u56db\u79cd\u8fd1\u4f3c\u7b97\u6cd5\uff1a\u7ecf\u5178\u8d2a\u5fc3\u6cd5\u3001\u4f4e\u79e9\u8d2a\u5fc3\u53d8\u4f53\u3001\u968f\u673a\u8d2a\u5fc3\u91c7\u6837\u5668\u548c\u7ebf\u6027\u5316\u9009\u62e9\u5668\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u548c\u81ea\u5b9a\u4e49\u63a7\u5236\u611f\u77e5\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5f3a\u8fd1\u4f3c\u4fdd\u8bc1\u5e76\u652f\u6301\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u7279\u5f81\u9009\u62e9\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2510.00995", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00995", "abs": "https://arxiv.org/abs/2510.00995", "authors": ["Jacob Moore", "Phil Tokumaru", "Ian Reid", "Brandon Sutherland", "Joseph Ritchie", "Gabe Snow", "Tim McLain"], "title": "ROSflight 2.0: Lean ROS 2-Based Autopilot for Unmanned Aerial Vehicles", "comment": "To be submitted to the 2026 IEEE International Conference on Robotics\n  and Automation in Vienna, Austria", "summary": "ROSflight is a lean, open-source autopilot ecosystem for unmanned aerial\nvehicles (UAVs). Designed by researchers for researchers, it is built to lower\nthe barrier to entry to UAV research and accelerate the transition from\nsimulation to hardware experiments by maintaining a lean (not full-featured),\nwell-documented, and modular codebase. This publication builds on previous\ntreatments and describes significant additions to the architecture that improve\nthe modularity and usability of ROSflight, including the transition from ROS 1\nto ROS 2, supported hardware, low-level actuator mixing, and the simulation\nenvironment. We believe that these changes improve the usability of ROSflight\nand enable ROSflight to accelerate research in areas like advanced-air\nmobility. Hardware results are provided, showing that ROSflight is able to\ncontrol a multirotor over a serial connection at 400 Hz while closing all\ncontrol loops on the companion computer.", "AI": {"tldr": "ROSflight\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u65e0\u4eba\u673a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u4e13\u4e3a\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u548cROS 2\u5347\u7ea7\u63d0\u5347\u53ef\u7528\u6027\uff0c\u652f\u6301400Hz\u63a7\u5236\u9891\u7387\u3002", "motivation": "\u964d\u4f4e\u65e0\u4eba\u673a\u7814\u7a76\u95e8\u69db\uff0c\u52a0\u901f\u4ece\u4eff\u771f\u5230\u786c\u4ef6\u5b9e\u9a8c\u7684\u8fc7\u6e21\uff0c\u4e3a\u5148\u8fdb\u7a7a\u4e2d\u673a\u52a8\u6027\u7b49\u7814\u7a76\u9886\u57df\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u4eceROS 1\u8fc1\u79fb\u5230ROS 2\uff0c\u6539\u8fdb\u6a21\u5757\u5316\u67b6\u6784\uff0c\u4f18\u5316\u786c\u4ef6\u652f\u6301\u3001\u5e95\u5c42\u6267\u884c\u5668\u6df7\u63a7\u548c\u4eff\u771f\u73af\u5883\u3002", "result": "\u80fd\u591f\u901a\u8fc7\u4e32\u884c\u8fde\u63a5\u4ee5400Hz\u9891\u7387\u63a7\u5236\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\uff0c\u6240\u6709\u63a7\u5236\u56de\u8def\u5728\u914d\u5957\u8ba1\u7b97\u673a\u4e0a\u95ed\u73af\u8fd0\u884c\u3002", "conclusion": "ROSflight\u7684\u6539\u8fdb\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u52a0\u901f\u65e0\u4eba\u673a\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2510.01023", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01023", "abs": "https://arxiv.org/abs/2510.01023", "authors": ["S. Satsevich", "A. Bazhenov", "S. Egorov", "A. Erkhov", "M. Gromakov", "A. Fedoseev", "D. Tsetserukou"], "title": "Prometheus: Universal, Open-Source Mocap-Based Teleoperation System with Force Feedback for Dataset Collection in Robot Learning", "comment": null, "summary": "This paper presents a novel teleoperation system with force feedback,\nutilizing consumer-grade HTC Vive Trackers 2.0. The system integrates a\ncustom-built controller, a UR3 robotic arm, and a Robotiq gripper equipped with\ncustom-designed fingers to ensure uniform pressure distribution on an embedded\nforce sensor. Real-time compression force data is transmitted to the\ncontroller, enabling operators to perceive the gripping force applied to\nobjects. Experimental results demonstrate that the system enhances task success\nrates and provides a low-cost solution for large-scale imitation learning data\ncollection without compromising affordability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6d88\u8d39\u7ea7HTC Vive Trackers 2.0\u7684\u65b0\u578b\u529b\u53cd\u9988\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u538b\u7f29\u529b\u6570\u636e\u4f20\u8f93\u63d0\u5347\u64cd\u4f5c\u6210\u529f\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u4eff\u5b66\u4e60\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u4f4e\u6210\u672c\u4f46\u6709\u6548\u7684\u529b\u53cd\u9988\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347\u64cd\u4f5c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u4e3a\u5927\u89c4\u6a21\u6a21\u4eff\u5b66\u4e60\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u7ecf\u6d4e\u53ef\u884c\u7684\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u4e86\u5b9a\u5236\u63a7\u5236\u5668\u3001UR3\u673a\u68b0\u81c2\u548c\u914d\u5907\u5b9a\u5236\u624b\u6307\u7684Robotiq\u5939\u722a\uff0c\u786e\u4fdd\u5d4c\u5165\u5f0f\u529b\u4f20\u611f\u5668\u4e0a\u7684\u5747\u5300\u538b\u529b\u5206\u5e03\uff0c\u5b9e\u65f6\u4f20\u8f93\u538b\u7f29\u529b\u6570\u636e\u5230\u63a7\u5236\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u4e3a\u5927\u89c4\u6a21\u6a21\u4eff\u5b66\u4e60\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u8d1f\u62c5\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6d88\u8d39\u7ea7\u786c\u4ef6\u7684\u529b\u53cd\u9988\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5728\u63d0\u5347\u64cd\u4f5c\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6210\u672c\u6548\u76ca\uff0c\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2510.01041", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01041", "abs": "https://arxiv.org/abs/2510.01041", "authors": ["Ian Reid", "Joseph Ritchie", "Jacob Moore", "Brandon Sutherland", "Gabe Snow", "Phillip Tokumaru", "Tim McLain"], "title": "ROSplane 2.0: A Fixed-Wing Autopilot for Research", "comment": null, "summary": "Unmanned aerial vehicle (UAV) research requires the integration of\ncutting-edge technology into existing autopilot frameworks. This process can be\narduous, requiring extensive resources, time, and detailed knowledge of the\nexisting system. ROSplane is a lean, open-source fixed-wing autonomy stack\nbuilt by researchers for researchers. It is designed to accelerate research by\nproviding clearly defined interfaces with an easily modifiable framework.\nPowered by ROS 2, ROSplane allows for rapid integration of low or high-level\ncontrol, path planning, or estimation algorithms. A focus on lean, easily\nunderstood code and extensive documentation lowers the barrier to entry for\nresearchers. Recent developments to ROSplane improve its capacity to accelerate\nUAV research, including the transition from ROS 1 to ROS 2, enhanced estimation\nand control algorithms, increased modularity, and an improved aerodynamic\nmodeling pipeline. This aerodynamic modeling pipeline significantly reduces the\neffort of transitioning from simulation to real-world testing without requiring\nexpensive system identification or computational fluid dynamics tools.\nROSplane's architecture reduces the effort required to integrate new research\ntools and methods, expediting hardware experimentation.", "AI": {"tldr": "ROSplane\u662f\u4e00\u4e2a\u4e13\u4e3a\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u5f00\u6e90\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u81ea\u4e3b\u7cfb\u7edf\uff0c\u57fa\u4e8eROS 2\u6784\u5efa\uff0c\u901a\u8fc7\u6e05\u6670\u7684\u63a5\u53e3\u548c\u6613\u4fee\u6539\u7684\u6846\u67b6\u52a0\u901f\u65e0\u4eba\u673a\u7814\u7a76\u3002", "motivation": "\u4f20\u7edf\u65e0\u4eba\u673a\u7814\u7a76\u9700\u8981\u5c06\u524d\u6cbf\u6280\u672f\u96c6\u6210\u5230\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u4e2d\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u9700\u8981\u5927\u91cf\u8d44\u6e90\u3001\u65f6\u95f4\u548c\u7cfb\u7edf\u77e5\u8bc6\uff0c\u5f00\u53d1ROSplane\u65e8\u5728\u964d\u4f4e\u7814\u7a76\u95e8\u69db\u5e76\u52a0\u901f\u7814\u7a76\u8fdb\u7a0b\u3002", "method": "\u91c7\u7528ROS 2\u6784\u5efa\uff0c\u63d0\u4f9b\u6e05\u6670\u5b9a\u4e49\u7684\u63a5\u53e3\u548c\u6613\u4e8e\u4fee\u6539\u7684\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u8f7b\u91cf\u7ea7\u3001\u6613\u4e8e\u7406\u89e3\u7684\u4ee3\u7801\u548c\u8be6\u7ec6\u6587\u6863\uff0c\u589e\u5f3a\u6a21\u5757\u5316\u548c\u6c14\u52a8\u5efa\u6a21\u6d41\u7a0b\u3002", "result": "ROSplane\u80fd\u591f\u5feb\u901f\u96c6\u6210\u4f4e\u5c42\u6216\u9ad8\u5c42\u63a7\u5236\u3001\u8def\u5f84\u89c4\u5212\u6216\u4f30\u8ba1\u7b97\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u7684\u5de5\u4f5c\u91cf\uff0c\u65e0\u9700\u6602\u8d35\u7684\u7cfb\u7edf\u8fa8\u8bc6\u6216\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u5de5\u5177\u3002", "conclusion": "ROSplane\u7684\u67b6\u6784\u663e\u8457\u51cf\u5c11\u4e86\u96c6\u6210\u65b0\u7814\u7a76\u5de5\u5177\u548c\u65b9\u6cd5\u6240\u9700\u7684\u5de5\u4f5c\u91cf\uff0c\u52a0\u901f\u4e86\u786c\u4ef6\u5b9e\u9a8c\u8fdb\u7a0b\uff0c\u4e3a\u65e0\u4eba\u673a\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7814\u7a76\u5e73\u53f0\u3002"}}
{"id": "2510.01068", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01068", "abs": "https://arxiv.org/abs/2510.01068", "authors": ["Jiahang Cao", "Yize Huang", "Hanzhong Guo", "Rui Zhang", "Mu Nan", "Weijian Mai", "Jiaxu Wang", "Hao Cheng", "Jingkai Sun", "Gang Han", "Wen Zhao", "Qiang Zhang", "Yijie Guo", "Qihao Zheng", "Chunfeng Song", "Xiao Li", "Ping Luo", "Andrew F. Luo"], "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition", "comment": "Project Page: https://sagecao1125.github.io/GPC-Site/", "summary": "Diffusion-based models for robotic control, including vision-language-action\n(VLA) and vision-action (VA) policies, have demonstrated significant\ncapabilities. Yet their advancement is constrained by the high cost of\nacquiring large-scale interaction datasets. This work introduces an alternative\nparadigm for enhancing policy performance without additional model training.\nPerhaps surprisingly, we demonstrate that the composed policies can exceed the\nperformance of either parent policy. Our contribution is threefold. First, we\nestablish a theoretical foundation showing that the convex composition of\ndistributional scores from multiple diffusion models can yield a superior\none-step functional objective compared to any individual score. A\nGr\\\"onwall-type bound is then used to show that this single-step improvement\npropagates through entire generation trajectories, leading to systemic\nperformance gains. Second, motivated by these results, we propose General\nPolicy Composition (GPC), a training-free method that enhances performance by\ncombining the distributional scores of multiple pre-trained policies via a\nconvex combination and test-time search. GPC is versatile, allowing for the\nplug-and-play composition of heterogeneous policies, including VA and VLA\nmodels, as well as those based on diffusion or flow-matching, irrespective of\ntheir input visual modalities. Third, we provide extensive empirical\nvalidation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside\nreal-world robotic evaluations, confirm that GPC consistently improves\nperformance and adaptability across a diverse set of tasks. Further analysis of\nalternative composition operators and weighting strategies offers insights into\nthe mechanisms underlying the success of GPC. These results establish GPC as a\nsimple yet effective method for improving control performance by leveraging\nexisting policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u7b56\u7565\u7ec4\u5408\u65b9\u6cd5GPC\uff0c\u901a\u8fc7\u7ec4\u5408\u591a\u4e2a\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5206\u5e03\u5206\u6570\u6765\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u6027\u80fd", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5927\u89c4\u6a21\u4ea4\u4e92\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e0d\u4f9d\u8d56\u989d\u5916\u8bad\u7ec3\u7684\u6027\u80fd\u63d0\u5347\u65b9\u6cd5", "method": "GPC\u65b9\u6cd5\uff1a\u901a\u8fc7\u51f8\u7ec4\u5408\u548c\u6d4b\u8bd5\u65f6\u641c\u7d22\u7ec4\u5408\u591a\u4e2a\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u5206\u5e03\u5206\u6570\uff0c\u652f\u6301\u5f02\u6784\u7b56\u7565\u7684\u5373\u63d2\u5373\u7528\u7ec4\u5408", "result": "\u5728Robomimic\u3001PushT\u3001RoboTwin\u7b49\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\uff0cGPC\u6301\u7eed\u63d0\u5347\u6027\u80fd\u548c\u9002\u5e94\u6027", "conclusion": "GPC\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u63a7\u5236\u6027\u80fd\u63d0\u5347\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6709\u7b56\u7565\u5b9e\u73b0\u6027\u80fd\u8d85\u8d8a"}}
{"id": "2510.01138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01138", "abs": "https://arxiv.org/abs/2510.01138", "authors": ["Matthew Woodward"], "title": "Real-Time Trajectory Generation and Hybrid Lyapunov-Based Control for Hopping Robots", "comment": "7 pages, 4 figures, 4 tables", "summary": "The advent of rotor-based hopping robots has created very capable hopping\nplatforms with high agility and efficiency, and similar controllability, as\ncompared to their purely flying quadrotor counterparts. Advances in robot\nperformance have increased the hopping height to greater than 4 meters and\nopened up the possibility for more complex aerial trajectories (i.e.,\nbehaviors). However, currently hopping robots do not directly control their\naerial trajectory or transition to flight, eliminating the efficiency benefits\nof a hopping system. Here we show a real-time, computationally efficiency,\nnon-linear drag compensated, trajectory generation methodology and accompanying\nLyapunov-based controller. The combined system can create and follow complex\naerial trajectories from liftoff to touchdown on horizontal and vertical\nsurfaces, while maintaining strick control over the orientation at touchdown.\nThe computational efficiency provides broad applicability across all size\nscales of hopping robots while maintaining applicability to quadrotors in\ngeneral.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u548c\u674e\u96c5\u666e\u8bfa\u592b\u63a7\u5236\u5668\uff0c\u4f7f\u8df3\u8dc3\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u7a7a\u4e2d\u8f68\u8ff9\u5e76\u7cbe\u786e\u63a7\u5236\u7740\u9646\u59ff\u6001\u3002", "motivation": "\u5f53\u524d\u8df3\u8dc3\u673a\u5668\u4eba\u65e0\u6cd5\u76f4\u63a5\u63a7\u5236\u7a7a\u4e2d\u8f68\u8ff9\u6216\u5b9e\u73b0\u98de\u884c\u8f6c\u6362\uff0c\u9650\u5236\u4e86\u8df3\u8dc3\u7cfb\u7edf\u7684\u6548\u7387\u4f18\u52bf\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u548c\u8ddf\u8e2a\u590d\u6742\u8f68\u8ff9\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u963b\u529b\u8865\u507f\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u674e\u96c5\u666e\u8bfa\u592b\u7a33\u5b9a\u6027\u7406\u8bba\u8bbe\u8ba1\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u4ece\u8d77\u98de\u5230\u7740\u9646\u7684\u5b8c\u6574\u8f68\u8ff9\u8ddf\u8e2a\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5728\u6c34\u5e73\u548c\u5782\u76f4\u8868\u9762\u4e0a\u751f\u6210\u5e76\u8ddf\u8e2a\u590d\u6742\u7a7a\u4e2d\u8f68\u8ff9\uff0c\u540c\u65f6\u4e25\u683c\u63a7\u5236\u7740\u9646\u65f6\u7684\u59ff\u6001\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u9002\u7528\u4e8e\u5404\u79cd\u5c3a\u5bf8\u7684\u8df3\u8dc3\u673a\u5668\u4eba\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8df3\u8dc3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u8f68\u8ff9\u63a7\u5236\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5bf9\u56db\u65cb\u7ffc\u98de\u884c\u5668\u4e5f\u5177\u6709\u901a\u7528\u6027\u3002"}}
