<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 64]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning](https://arxiv.org/abs/2509.13336)
*Mehran Behjati,Rosdiadee Nordin,Nor Fadzilah Abdullah*

Main category: cs.RO

TL;DR: 基于强化学习的无人机超视距路径规划方法，通过最小化飞行距离和最大化蜂窝链路质量来确保安全可靠的飞行操作


<details>
  <summary>Details</summary>
Motivation: 解决超视距无人机在蜂窝通信中的路径规划挑战，确保飞行安全性和通信可靠性

Method: 使用强化学习技术训练智能体，以无人机与基站通信链路质量作为奖励函数，考虑真实世界空中覆盖约束和经验空中信道模型

Result: 仿真结果表明该方法能有效训练智能体并生成可行的无人机路径规划，能高效识别最优路径确保与地面基站的最大连接性

Conclusion: 该方法可作为离线路径规划模块集成到未来无人机地面控制系统中，具有在复杂长距离无人机应用中推进蜂窝连接无人机路径规划技术的潜力

Abstract: This paper presents a reinforcement learning (RL) based approach for path
planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond
visual line of sight (BVLoS). The objective is to minimize travel distance
while maximizing the quality of cellular link connectivity by considering real
world aerial coverage constraints and employing an empirical aerial channel
model. The proposed solution employs RL techniques to train an agent, using the
quality of communication links between the UAV and base stations (BSs) as the
reward function. Simulation results demonstrate the effectiveness of the
proposed method in training the agent and generating feasible UAV path plans.
The proposed approach addresses the challenges due to limitations in UAV
cellular communications, highlighting the need for investigations and
considerations in this area. The RL algorithm efficiently identifies optimal
paths, ensuring maximum connectivity with ground BSs to ensure safe and
reliable BVLoS flight operation. Moreover, the solution can be deployed as an
offline path planning module that can be integrated into future ground control
systems (GCS) for UAV operations, enhancing their capabilities and safety. The
method holds potential for complex long range UAV applications, advancing the
technology in the field of cellular connected UAV path planning.

</details>


### [2] [Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments](https://arxiv.org/abs/2509.13342)
*Isaac Ronald Ward*

Main category: cs.RO

TL;DR: 改进视觉定位神经网络，通过扩展损失函数结合位置和旋转误差，提高室内场景定位精度，并建立完整导航算法管道


<details>
  <summary>Details</summary>
Motivation: 提高机器人基于视觉信息的定位性能，同时保持训练简便性，解决感知混淆问题

Method: 扩展神经网络损失函数，直观结合位置和旋转误差；使用摄影测量数据创建姿态标注数据集；在TurtleBot上进行实时测试

Result: 室内定位精度显著提升：位置误差中位数降低9.64%，旋转误差中位数降低2.99%；最终达到0.11米和0.89度的定位精度

Conclusion: 提出了一个完整的室内导航算法管道，仅需场景图像集合即可创建鲁棒的导航系统，图像采集时间最短仅需330秒

Abstract: In this work, an existing deep neural network approach for determining a
robot's pose from visual information (RGB images) is modified, improving its
localization performance without impacting its ease of training. Explicitly,
the network's loss function is extended in a manner which intuitively combines
the positional and rotational error in order to increase robustness to
perceptual aliasing. An improvement in the localization accuracy for indoor
scenes is observed: with decreases of up to 9.64% and 2.99% in the median
positional and rotational error respectively, when compared to the unmodified
network.
  Additionally, photogrammetry data is used to produce a pose-labelled dataset
which allows the above model to be trained on a local environment, resulting in
localization accuracies of 0.11m & 0.89 degrees. This trained model forms the
basis of a navigation algorithm, which is tested in real-time on a TurtleBot (a
wheeled robotic device). As such, this work introduces a full pipeline for
creating a robust navigational algorithm for any given real world indoor scene;
the only requirement being a collection of images from the scene, which can be
captured in as little as 330 seconds of

</details>


### [3] [Label-Efficient Grasp Joint Prediction with Point-JEPA](https://arxiv.org/abs/2509.13349)
*Jed Guzelkabaagac,Boris Petrović*

Main category: cs.RO

TL;DR: Point-JEPA自监督预训练在低标签条件下显著提升抓取关节角度预测性能，在DLR-Hand II数据集上RMSE降低26%，达到全监督性能水平


<details>
  <summary>Details</summary>
Motivation: 研究3D自监督预训练是否能够实现标签高效的抓取关节角度预测，探索JEPA架构在数据效率方面的实用性

Method: 使用网格token化的点云数据，采用ShapeNet预训练的Point-JEPA编码器，训练轻量级多假设头部网络，使用winner-takes-all策略和top-logit选择进行评估

Result: 在DLR-Hand II数据集的对象级分割上，Point-JEPA在低标签条件下RMSE降低达26%，并且达到了与全监督方法相当的性能

Conclusion: JEPA风格的预训练是数据高效抓取学习的一种实用方法，证明了自监督预训练在3D抓取任务中的有效性

Abstract: We investigate whether 3D self-supervised pretraining with a Joint-Embedding
Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle
prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained
Point-JEPA encoder, we train a lightweight multi-hypothesis head with
winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with
object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes
and reaches parity with full supervision. These results suggest JEPA-style
pretraining is a practical approach for data-efficient grasp learning.

</details>


### [4] [Using role-play and Hierarchical Task Analysis for designing human-robot interaction](https://arxiv.org/abs/2509.13378)
*Mattias Wingren,Sören Andersson,Sara Rosenberg,Malin Andtfolk,Susanne Hägglund,Prashani Jayasingha Arachchige,Linda Nyholm*

Main category: cs.RO

TL;DR: 本文探讨了角色扮演和层次任务分析在人机交互领域的应用价值，通过在社区药房机器人开发项目中的实践展示了这两种方法的优势


<details>
  <summary>Details</summary>
Motivation: 当前人机交互领域对角色扮演和层次任务分析方法的使用不足，作者认为这两种方法具有很大潜力，希望通过实际项目展示其价值

Method: 采用角色扮演方法创建可控可调节的环境来理解用户需求，让药剂师作为机器人行为模型；使用层次任务分析确保行为建模的正确性，并通过促进协同设计来辅助开发

Result: 角色扮演提供了理解客户需求的有效环境，层次任务分析确保了行为建模的准确性并促进了开发过程的协同设计

Conclusion: 这两种方法在人机交互领域具有重要价值，未来研究应着重开发特别适合社交机器人交互的任务分析方法

Abstract: We present the use of two methods we believe warrant more use than they
currently have in the field of human-robot interaction: role-play and
Hierarchical Task Analysis. Some of its potential is showcased through our use
of them in an ongoing research project which entails developing a robot
application meant to assist at a community pharmacy. The two methods have
provided us with several advantages. The role-playing provided a controlled and
adjustable environment for understanding the customers' needs where pharmacists
could act as models for the robot's behavior; and the Hierarchical Task
Analysis ensured the behavior displayed was modelled correctly and aided
development through facilitating co-design. Future research could focus on
developing task analysis methods especially suited for social robot
interaction.

</details>


### [5] [ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy](https://arxiv.org/abs/2509.13380)
*Alejandro D. Mousist*

Main category: cs.RO

TL;DR: ASTREA是首个在飞行遗产硬件(TRL 9)上部署的自主航天器操作智能体系统，结合了资源受限的LLM智能体和强化学习控制器，在地面实验中表现出色但在轨验证中因推理延迟与快速热循环不匹配而出现性能下降。


<details>
  <summary>Details</summary>
Motivation: 开发能够在空间合格平台上自主运行的智能体系统，将语义推理与自适应控制相结合，解决航天器自主操作问题。

Method: 采用异步架构，将资源受限的大型语言模型(LLM)智能体与强化学习控制器集成，以热控制为代表用例进行验证。

Result: 地面实验显示LLM引导的监督提高了热稳定性并减少了违规，但在国际空间站上的在轨验证发现由于推理延迟与低地球轨道卫星的快速热循环不匹配导致性能下降。

Conclusion: 研究揭示了基于LLM的智能体系统在真实飞行环境中的机遇和当前局限性，为未来空间自主性提供了实用的设计指南。

Abstract: This paper presents ASTREA, the first agentic system deployed on
flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using
thermal control as a representative use case, we integrate a
resource-constrained Large Language Model (LLM) agent with a reinforcement
learning controller in an asynchronous architecture tailored for
space-qualified platforms. Ground experiments show that LLM-guided supervision
improves thermal stability and reduces violations, confirming the feasibility
of combining semantic reasoning with adaptive control under hardware
constraints. However, on-orbit validation aboard the International Space
Station (ISS) reveals performance degradation caused by inference latency
mismatched with the rapid thermal cycles characteristic of Low Earth Orbit
(LEO) satellites. These results highlight both the opportunities and current
limitations of agentic LLM-based systems in real flight environments, providing
practical design guidelines for future space autonomy.

</details>


### [6] [Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach](https://arxiv.org/abs/2509.13381)
*Zhang Xueyao,Yang Bo,Yu Zhiwen,Cao Xuelin,George C. Alexandropoulos,Merouane Debbah,Chau Yuen*

Main category: cs.RO

TL;DR: 提出分层多智能体近端策略优化框架，解决水下自主航行器协同任务中的隐蔽通信问题，通过双时间尺度控制实现高效协作与隐蔽操作的平衡。


<details>
  <summary>Details</summary>
Motivation: 水下自主航行器(AUVs)协同检测与侦察具有巨大潜力，但在对抗环境中协同通信存在暴露风险，如何在确保隐蔽操作的同时实现高效协作成为关键挑战。

Method: 提出分层多智能体近端策略优化(H-MAPPO)框架：高层组件基于中央AUV确定参与任务的个体，低层组件通过功率和轨迹控制降低参与AUV的暴露概率。

Result: 仿真结果表明，所提框架实现快速收敛，在性能上优于基准算法，在确保隐蔽操作的同时最大化长期协作效率。

Conclusion: 该分层框架有效解决了水下协同任务中隐蔽性与效率的权衡问题，为对抗环境下的AUV协同操作提供了可行的解决方案。

Abstract: Autonomous Underwater Vehicles (AUVs) have shown great potential for
cooperative detection and reconnaissance. However, collaborative AUV
communications introduce risks of exposure. In adversarial environments,
achieving efficient collaboration while ensuring covert operations becomes a
key challenge for underwater cooperative missions. In this paper, we propose a
novel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization
(H-MAPPO) framework. The high-level component determines the individuals
participating in the task based on a central AUV, while the low-level component
reduces exposure probabilities through power and trajectory control by the
participating AUVs. Simulation results show that the proposed framework
achieves rapid convergence, outperforms benchmark algorithms in terms of
performance, and maximizes long-term cooperative efficiency while ensuring
covert operations.

</details>


### [7] [VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization](https://arxiv.org/abs/2509.13386)
*Hansol Lim,Minhyeok Im,Jonathan Boyack,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: VEGA是一个基于强化学习的电动汽车充电感知导航系统，通过物理信息神经网络估计车辆动态参数，使用PPO算法在电量约束下优化路径规划和充电策略。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆需求增长和电动汽车计算能力提升，需要开发能够根据车辆实时状态和环境条件进行充电感知路径优化的AI系统。

Method: 系统包含两个模块：1）物理信息神经网络操作器（PINO）从车速日志学习车辆动态参数；2）强化学习代理使用学习到的动态模型，在电量约束下优化路径、充电站点选择和停留时间。

Result: 在长距离路线测试中（如旧金山到纽约），VEGA的充电策略、停留时间、电量管理和总旅行时间与特斯拉行程规划器接近但更保守，且在法国和日本等未训练地区也表现出良好泛化能力。

Conclusion: VEGA成功实现了物理信息学习与强化学习的实用集成，为电动汽车生态路由提供了有效的解决方案，无需额外传感器，仅使用车速信号即可工作。

Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles
(EVs) are increasingly being equipped with powerful computers. This enables
onboard AI systems to optimize charge-aware path optimization customized to
reflect vehicle's current condition and environment. We present VEGA, a
charge-aware EV navigation agent that plans over a charger-annotated road graph
using Proximal Policy Optimization (PPO) with budgeted A* teacher-student
guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.
First, a physics-informed neural operator (PINO), trained on real vehicle speed
and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic
drag, rolling resistance, mass, motor and regenerative-braking efficiencies,
and auxiliary load by learning a vehicle-custom dynamics. Second, a
Reinforcement Learning (RL) agent uses these dynamics to optimize a path with
optimal charging stops and dwell times under SoC constraints. VEGA requires no
additional sensors and uses only vehicle speed signals. It may serve as a
virtual sensor for power and efficiency to potentially reduce EV cost. In
evaluation on long routes like San Francisco to New York, VEGA's stops, dwell
times, SoC management, and total travel time closely track Tesla Trip Planner
while being slightly more conservative, presumably due to real vehicle
conditions such as vehicle parameter drift due to deterioration. Although
trained only in U.S. regions, VEGA was able to compute optimal charge-aware
paths in France and Japan, demonstrating generalizability. It achieves
practical integration of physics-informed learning and RL for EV eco-routing.

</details>


### [8] [A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies](https://arxiv.org/abs/2509.13434)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一个计算框架，用于模拟细丝与刚体之间的接触相互作用，解决了现有方法中细丝必须永久附着于刚体的限制，通过结合离散弹性杆模型、压力场补丁接触模型和凸接触公式，实现了精确的摩擦相互作用模拟。


<details>
  <summary>Details</summary>
Motivation: 细丝由于其共维性（一维结构嵌入三维空间）难以模拟，现有方法通常假设细丝永久附着于刚体，无法准确模拟细丝与刚体之间的复杂摩擦相互作用。

Method: 结合离散弹性杆（DER）建模、压力场补丁接触模型和凸接触公式，采用凸优化方法确保每个时间步都能达到全局最优解，保证接触速度与冲量之间的互补性。

Result: 验证了摩擦力的准确性，与基线方法相比具有更高的物理保真度，在软机器人（如随机细丝抓取器）和可变形物体操作（如鞋带系结）中展示了应用价值。

Conclusion: 该框架为涉及复杂细丝-细丝和细丝-刚体相互作用的系统提供了一个通用的模拟器，解决了先前无法实现的精确摩擦相互作用模拟问题。

Abstract: We present a computational framework for simulating filaments interacting
with rigid bodies through contact. Filaments are challenging to simulate due to
their codimensionality, i.e., they are one-dimensional structures embedded in
three-dimensional space. Existing methods often assume that filaments remain
permanently attached to rigid bodies. Our framework unifies discrete elastic
rod (DER) modeling, a pressure field patch contact model, and a convex contact
formulation to accurately simulate frictional interactions between slender
filaments and rigid bodies - capabilities not previously achievable. Owing to
the convex formulation of contact, each time step can be solved to global
optimality, guaranteeing complementarity between contact velocity and impulse.
We validate the framework by assessing the accuracy of frictional forces and
comparing its physical fidelity against baseline methods. Finally, we
demonstrate its applicability in both soft robotics, such as a stochastic
filament-based gripper, and deformable object manipulation, such as shoelace
tying, providing a versatile simulator for systems involving complex
filament-filament and filament-rigid body interactions.

</details>


### [9] [Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume](https://arxiv.org/abs/2509.13501)
*Hossein Gholampour,Logan E. Beaver*

Main category: cs.RO

TL;DR: 提出一种输出空间方法，用于双积分器系统的安全路径跟踪，能够在遇到障碍时暂停并恢复，无需重新规划路径。


<details>
  <summary>Details</summary>
Motivation: 机器人系统需要在遇到人或物体干预时能够安全暂停并恢复路径跟踪，同时保持速度和加速度限制。

Method: 离线进行可达性检查验证运动计划，在线使用二次规划跟踪运动计划，采用一步可达性测试来限制系统能够拒绝的最大扰动。

Result: 系统能够高效处理安全停止和非计划偏差，无需重新规划即可返回运动计划，在仿真中表现出比纯追踪方法更好的性能。

Conclusion: 该方法为双积分器系统提供了一种有效的安全路径跟踪解决方案，能够在保持运动限制的同时处理意外干扰。

Abstract: Many robotic systems must follow planned paths yet pause safely and resume
when people or objects intervene. We present an output-space method for systems
whose tracked output can be feedback-linearized to a double integrator (e.g.,
manipulators). The approach has two parts. Offline, we perform a pre-run
reachability check to verify that the motion plan respects speed and
acceleration magnitude limits. Online, we apply a quadratic program to track
the motion plan under the same limits. We use a one-step reachability test to
bound the maximum disturbance the system is capable of rejecting. When the
state coincides with the reference path we recover perfect tracking in the
deterministic case, and we correct errors using a KKT-inspired weight. We
demonstrate that safety stops and unplanned deviations are handled efficiently,
and the system returns to the motion plan without replanning. We demonstrate
our system's improved performance over pure pursuit in simulation.

</details>


### [10] [Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning](https://arxiv.org/abs/2509.13534)
*Chunxin Zheng,Kai Chen,Zhihai Bi,Yulin Li,Liang Pan,Jinni Zhou,Haoang Li,Jun Ma*

Main category: cs.RO

TL;DR: 提出基于强化学习的全身拥抱操作框架，整合预训练人体运动先验和神经符号距离场，实现人形机器人稳定多接触操作


<details>
  <summary>Details</summary>
Motivation: 传统末端执行器抓取在大型物体操作中存在稳定性和负载限制，需要开发全身协调操作方法来提升机器人操作能力

Method: 采用师生架构蒸馏大规模人体运动数据，结合神经符号距离场(NSDF)提供几何感知，实现自然且物理可行的全身运动模式

Result: 在仿真和真实实验中表现出对多样化物体形状尺寸的适应能力，成功实现从仿真到现实的迁移

Conclusion: 该框架为人形机器人的多接触和长时程全身操作任务提供了有效实用的解决方案

Abstract: Whole-body manipulation (WBM) for humanoid robots presents a promising
approach for executing embracing tasks involving bulky objects, where
traditional grasping relying on end-effectors only remains limited in such
scenarios due to inherent stability and payload constraints. This paper
introduces a reinforcement learning framework that integrates a pre-trained
human motion prior with a neural signed distance field (NSDF) representation to
achieve robust whole-body embracing. Our method leverages a teacher-student
architecture to distill large-scale human motion data, generating kinematically
natural and physically feasible whole-body motion patterns. This facilitates
coordinated control across the arms and torso, enabling stable multi-contact
interactions that enhance the robustness in manipulation and also the load
capacity. The embedded NSDF further provides accurate and continuous geometric
perception, improving contact awareness throughout long-horizon tasks. We
thoroughly evaluate the approach through comprehensive simulations and
real-world experiments. The results demonstrate improved adaptability to
diverse shapes and sizes of objects and also successful sim-to-real transfer.
These indicate that the proposed framework offers an effective and practical
solution for multi-contact and long-horizon WBM tasks of humanoid robots.

</details>


### [11] [Semantic 3D Reconstructions with SLAM for Central Airway Obstruction](https://arxiv.org/abs/2509.13541)
*Ayberk Acar,Fangjie Li,Hao Li,Lidia Al-Zogbi,Kanyifeechukwu Jane Oguine,Susheela Sharma Stern,Jesse F. d'Almeida,Robert J. Webster III,Ipek Oguz,Jie Ying Wu*

Main category: cs.RO

TL;DR: 提出了一种结合实时单目SLAM和语义分割的管道，用于中央气道阻塞的内窥镜3D重建，能够实时生成带有梗阻组织标注的3D地图


<details>
  <summary>Details</summary>
Motivation: 中央气道阻塞是危及生命的疾病，传统治疗方法并发症风险高，机器人介入治疗需要实时的场景理解和语义标注能力

Method: 结合DROID-SLAM进行实时3D几何重建，使用训练的分割模型识别梗阻组织，将分割掩码整合到SLAM工作流中标注重建点云

Result: 在离体模型上验证，重建质量与CT扫描高度相似（倒角距离0.62mm），实时生成带有临床相关区域标注的3D地图，重建速度比先前工作更快

Conclusion: 这是首个将语义分割与实时单目SLAM结合用于内窥镜CAO场景的工作，框架模块化可推广到其他解剖结构，为自主机器人干预迈出重要一步

Abstract: Central airway obstruction (CAO) is a life-threatening condition with
increasing incidence, caused by tumors in and outside of the airway.
Traditional treatment methods such as bronchoscopy and electrocautery can be
used to remove the tumor completely; however, these methods carry a high risk
of complications. Recent advances allow robotic interventions with lesser risk.
The combination of robot interventions with scene understanding and mapping
also opens up the possibilities for automation. We present a novel pipeline
that enables real-time, semantically informed 3D reconstructions of the central
airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to
identify obstructive tissues. The SLAM module reconstructs the 3D geometry of
the airway in real time, while the segmentation masks guide the annotation of
obstruction regions within the reconstructed point cloud. To validate our
pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground
truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By
integrating segmentation directly into the SLAM workflow, our system produces
annotated 3D maps that highlight clinically relevant regions in real time.
High-speed capabilities of the pipeline allows quicker reconstructions compared
to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic
segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our
framework is modular and can generalize to other anatomies or procedures with
minimal changes, offering a promising step toward autonomous robotic
interventions.

</details>


### [12] [Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference](https://arxiv.org/abs/2509.13572)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: cs.RO

TL;DR: 本研究评估了视觉语言模型在提升半自主假肢手感知能力方面的潜力，通过统一基准测试8个现代VLM在物体识别和抓取参数推断方面的表现。


<details>
  <summary>Details</summary>
Motivation: 探索使用单一视觉语言模型替代传统复杂多模块管道（物体检测、姿态估计、抓取规划）的可能性，以简化半自主假肢手的感知系统。

Method: 建立统一基准测试，使用34张常见物体快照数据集，通过结构化JSON输出提示，评估8个现代VLM在物体属性识别（名称、形状、方向、尺寸）和抓取参数推断（抓取类型、手腕旋转、手部开合度、手指数量）的能力。

Result: 大多数模型在物体识别和形状识别方面表现优异，但在尺寸估计和最优抓取参数推断（特别是手部旋转和开合度）方面准确性差异较大。

Conclusion: 研究展示了VLM作为先进感知模块在半自主仿生肢体控制中的当前能力和局限性，证明了其在有效假肢应用中的潜力。

Abstract: This study examines the potential of utilizing Vision Language Models (VLMs)
to improve the perceptual capabilities of semi-autonomous prosthetic hands. We
introduce a unified benchmark for end-to-end perception and grasp inference,
evaluating a single VLM to perform tasks that traditionally require complex
pipelines with separate modules for object detection, pose estimation, and
grasp planning. To establish the feasibility and current limitations of this
approach, we benchmark eight contemporary VLMs on their ability to perform a
unified task essential for bionic grasping. From a single static image, they
should (1) identify common objects and their key properties (name, shape,
orientation, and dimensions), and (2) infer appropriate grasp parameters (grasp
type, wrist rotation, hand aperture, and number of fingers). A corresponding
prompt requesting a structured JSON output was employed with a dataset of 34
snapshots of common objects. Key performance metrics, including accuracy for
categorical attributes (e.g., object name, shape) and errors in numerical
estimates (e.g., dimensions, hand aperture), along with latency and cost, were
analyzed. The results demonstrated that most models exhibited high performance
in object identification and shape recognition, while accuracy in estimating
dimensions and inferring optimal grasp parameters, particularly hand rotation
and aperture, varied more significantly. This work highlights the current
capabilities and limitations of VLMs as advanced perceptual modules for
semi-autonomous control of bionic limbs, demonstrating their potential for
effective prosthetic applications.

</details>


### [13] [Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation](https://arxiv.org/abs/2509.13574)
*Zidong Chen,Zihao Guo,Peng Wang,ThankGod Itua Egbe,Yan Lyu,Chenghao Qian*

Main category: cs.RO

TL;DR: 本文提出了一种新的非均匀时间调度策略，通过U形训练时间安排和密集跳跃推理调度，解决了流匹配中泛化能力饱和和推理步骤增加导致性能下降的问题，在多种机器人任务上实现了23.7%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 发现流匹配框架中泛化能力在流轨迹早期就出现并饱和，且增加欧拉积分步骤会反直觉地降低策略性能，这归因于均匀间隔的积分步骤过度采样后期区域，以及速度场在接近时间1时变得非Lipschitz导致不稳定。

Method: 提出使用非均匀时间调度（如U形）进行训练，强调早期和晚期时间阶段以正则化策略训练；在推理时使用密集跳跃积分调度，用单步积分替换跳跃点后的多步积分，避免不稳定区域。

Result: 新策略在多种机器人任务上实现了最高23.7%的性能提升，超越了现有最先进基线方法。

Conclusion: 该方法是一种高效的单步学习器，同时通过多步积分推动性能提升，有效解决了流匹配中的泛化和稳定性问题。

Abstract: Flow matching has emerged as a competitive framework for learning
high-quality generative policies in robotics; however, we find that
generalisation arises and saturates early along the flow trajectory, in
accordance with recent findings in the literature. We further observe that
increasing the number of Euler integration steps during inference
counter-intuitively and universally degrades policy performance. We attribute
this to (i) additional, uniformly spaced integration steps oversample the
late-time region, thereby constraining actions towards the training
trajectories and reducing generalisation; and (ii) the learned velocity field
becoming non-Lipschitz as integration time approaches 1, causing instability.
To address these issues, we propose a novel policy that utilises non-uniform
time scheduling (e.g., U-shaped) during training, which emphasises both early
and late temporal stages to regularise policy training, and a dense-jump
integration schedule at inference, which uses a single-step integration to
replace the multi-step integration beyond a jump point, to avoid unstable areas
around 1. Essentially, our policy is an efficient one-step learner that still
pushes forward performance through multi-step integration, yielding up to 23.7%
performance gains over state-of-the-art baselines across diverse robotic tasks.

</details>


### [14] [TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning](https://arxiv.org/abs/2509.13579)
*Momchil S. Tomov,Sang Uk Lee,Hansford Hendrago,Jinwook Huh,Teawon Han,Forbes Howington,Rafael da Silva,Gianmarco Bernasconi,Marc Heim,Samuel Findler,Xiaonan Ji,Alexander Boule,Michael Napoli,Kuo Chen,Jesse Miller,Boaz Floor,Yunqing Hu*

Main category: cs.RO

TL;DR: TreeIRL结合蒙特卡洛树搜索和逆强化学习，在自动驾驶规划中实现安全且类人的轨迹选择，在仿真和真实道路测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶规划中的安全性与人类驾驶行为相似性的平衡问题，传统方法往往难以同时兼顾多个性能指标

Method: 使用MCTS生成安全候选轨迹，通过深度IRL评分函数选择最类人的轨迹，结合经典搜索算法与学习方法

Result: 在大规模仿真和500+英里真实道路测试中表现最佳，在密集城市交通、自适应巡航、切入场景和交通灯等多种场景中均取得优异性能

Conclusion: 首次在公共道路上验证MCTS规划方法，证明了跨多指标评估的重要性，为探索经典方法与学习方法的结合提供了可扩展框架

Abstract: We present TreeIRL, a novel planner for autonomous driving that combines
Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to
achieve state-of-the-art performance in simulation and in real-world driving.
The core idea is to use MCTS to find a promising set of safe candidate
trajectories and a deep IRL scoring function to select the most human-like
among them. We evaluate TreeIRL against both classical and state-of-the-art
planners in large-scale simulations and on 500+ miles of real-world autonomous
driving in the Las Vegas metropolitan area. Test scenarios include dense urban
traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves
the best overall performance, striking a balance between safety, progress,
comfort, and human-likeness. To our knowledge, our work is the first
demonstration of MCTS-based planning on public roads and underscores the
importance of evaluating planners across a diverse set of metrics and in
real-world environments. TreeIRL is highly extensible and could be further
improved with reinforcement learning and imitation learning, providing a
framework for exploring different combinations of classical and learning-based
approaches to solve the planning bottleneck in autonomous driving.

</details>


### [15] [Object Pose Estimation through Dexterous Touch](https://arxiv.org/abs/2509.13591)
*Amir-Hossein Shahidzadeh,Jiyue Zhu,Kezhou Chen,Sha Yi,Cornelia Fermüller,Yiannis Aloimonos,Xiaolong Wang*

Main category: cs.RO

TL;DR: 使用强化学习和双手协作的触觉探索方法，通过主动交互收集3D点云数据来迭代优化物体姿态估计，无需先验几何知识


<details>
  <summary>Details</summary>
Motivation: 解决在视觉数据受限或受光照、遮挡、外观影响时，机器人操作任务中鲁棒的物体姿态估计问题。触觉传感器提供的信息有限且局部，难以从部分数据重建姿态

Method: 采用传感器运动探索方法，使用强化学习训练机器人手主动与物体交互。一只手固定物体，另一只手进行主动探索，收集3D点云数据迭代优化物体形状和姿态

Result: 该方法能够主动探索物体表面以识别关键姿态特征，无需物体几何形状的先验知识

Conclusion: 提出的双手触觉姿态估计方法通过主动探索和强化学习，有效解决了在视觉受限情况下的物体姿态估计问题

Abstract: Robust object pose estimation is essential for manipulation and interaction
tasks in robotics, particularly in scenarios where visual data is limited or
sensitive to lighting, occlusions, and appearances. Tactile sensors often offer
limited and local contact information, making it challenging to reconstruct the
pose from partial data. Our approach uses sensorimotor exploration to actively
control a robot hand to interact with the object. We train with Reinforcement
Learning (RL) to explore and collect tactile data. The collected 3D point
clouds are used to iteratively refine the object's shape and pose. In our
setup, one hand holds the object steady while the other performs active
exploration. We show that our method can actively explore an object's surface
to identify critical pose features without prior knowledge of the object's
geometry. Supplementary material and more demonstrations will be provided at
https://amirshahid.github.io/BimanualTactilePose .

</details>


### [16] [Leg-Arm Coordinated Operation for Curtain Wall Installation](https://arxiv.org/abs/2509.13595)
*Xiao Liu,Weijun Wang,Tianlun Huang,Zhiyong Wang,Wei Feng*

Main category: cs.RO

TL;DR: 针对传统幕墙安装方法效率低、安全性差的问题，提出基于六足机器人的分层优化全身控制框架，实现臂腿协调规划，成功完成墙面、天花板和地板安装任务。


<details>
  <summary>Details</summary>
Motivation: 传统幕墙安装面临现场地形多变、劳动强度大、施工效率低和安全风险高等挑战，需要多人协作完成大型面板安装，亟需自动化解决方案。

Method: 基于六足幕墙安装机器人，设计分层优化全身控制框架，整合六足腿部运动、折叠臂和串并联机械臂操作，实现臂腿协调规划。

Result: 在六足幕墙安装机器人上进行实验验证，证明该控制方法能够有效执行幕墙安装任务，展示了臂腿协调框架的实用性。

Conclusion: 分层优化臂腿协调框架为六足机器人在复杂施工现场环境的进一步应用奠定了基础，解决了传统安装方法的局限性。

Abstract: With the acceleration of urbanization, the number of high-rise buildings and
large public facilities is increasing, making curtain walls an essential
component of modern architecture with widespread applications. Traditional
curtain wall installation methods face challenges such as variable on-site
terrain, high labor intensity, low construction efficiency, and significant
safety risks. Large panels often require multiple workers to complete
installation. To address these issues, based on a hexapod curtain wall
installation robot, we design a hierarchical optimization-based whole-body
control framework for coordinated arm-leg planning tailored to three key tasks:
wall installation, ceiling installation, and floor laying. This framework
integrates the motion of the hexapod legs with the operation of the folding arm
and the serial-parallel manipulator. We conduct experiments on the hexapod
curtain wall installation robot to validate the proposed control method,
demonstrating its capability in performing curtain wall installation tasks. Our
results confirm the effectiveness of the hierarchical optimization-based
arm-leg coordination framework for the hexapod robot, laying the foundation for
its further application in complex construction site environments.

</details>


### [17] [Barometer-Aided Attitude Estimation](https://arxiv.org/abs/2509.13649)
*Méloné Nyoba Tchonkeu,Soulaimane Berkane,Tarek Hamel*

Main category: cs.RO

TL;DR: 提出了一种基于气压计的姿态估计架构，利用气压高度测量来推断垂直速度和姿态，通过非线性观测器在SO(3)流形上实现几乎全局渐近稳定性


<details>
  <summary>Details</summary>
Motivation: 在GNSS拒止或高动态环境中，仅使用IMU无法可靠估计倾斜姿态，而辅助速度传感器可能不可用、间歇性或成本高昂

Method: 设计了一个确定性Riccati观测器与互补滤波器级联的架构，利用气压高度测量推断垂直速度和姿态，在SO(3)流形上实现非线性观测

Result: 在满足一致可观测性条件下确保了几乎全局渐近稳定性(AGAS)，同时保持几何一致性

Conclusion: 气压计辅助估计是一种轻量级且有效的补充模态，为自主车辆在挑战性环境中提供可靠姿态估计解决方案

Abstract: Accurate and robust attitude estimation is a central challenge for autonomous
vehicles operating in GNSS-denied or highly dynamic environments. In such
cases, Inertial Measurement Units (IMUs) alone are insufficient for reliable
tilt estimation due to the ambiguity between gravitational and inertial
accelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,
Doppler radar, or visual odometry, are often used, they can be unavailable,
intermittent, or costly. This work introduces a barometer-aided attitude
estimation architecture that leverages barometric altitude measurements to
infer vertical velocity and attitude within a nonlinear observer on SO(3). The
design cascades a deterministic Riccati observer with a complementary filter,
ensuring Almost Global Asymptotic Stability (AGAS) under a uniform
observability condition while maintaining geometric consistency. The analysis
highlights barometer-aided estimation as a lightweight and effective
complementary modality.

</details>


### [18] [DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring](https://arxiv.org/abs/2509.13666)
*Zhenqi Wu,Abhinav Modi,Angelos Mavrogiannis,Kaustubh Joshi,Nikhil Chopra,Yiannis Aloimonos,Nare Karapetyan,Ioannis Rekleitis,Xiaomin Lin*

Main category: cs.RO

TL;DR: DREAM是一个基于视觉语言模型的自主框架，用于水下长期探索和栖息地监测，在寻找和探索目标物体（如牡蛎、沉船）方面表现出高效率，相比基线方法显著减少了时间和步骤。


<details>
  <summary>Details</summary>
Motivation: 海洋变暖和酸化增加了温度敏感贝类（如牡蛎）大规模死亡事件的风险，需要开发长期监测系统。人工水下作业成本高且危险，因此需要机器人解决方案来实现实时、环境感知的自主决策。

Method: 提出了DREAM框架，这是一个基于视觉语言模型（VLM）的自主系统，用于水下长期探索和栖息地监测，无需先验位置信息即可寻找和探索目标物体。

Result: 在牡蛎监测任务中，比基线方法节省31.5%的时间；相比普通VLM，减少23%的步骤同时覆盖更多8.88%的牡蛎；在沉船场景中，实现100%覆盖且无碰撞，比普通模型减少27.5%的步骤。

Conclusion: DREAM框架为水下长期监测提供了一种高效、自主的解决方案，在目标物体探索和覆盖方面显著优于现有方法，具有重要的应用价值。

Abstract: The ocean is warming and acidifying, increasing the risk of mass mortality
events for temperature-sensitive shellfish such as oysters. This motivates the
development of long-term monitoring systems. However, human labor is costly and
long-duration underwater work is highly hazardous, thus favoring robotic
solutions as a safer and more efficient option. To enable underwater robots to
make real-time, environment-aware decisions without human intervention, we must
equip them with an intelligent "brain." This highlights the need for
persistent,wide-area, and low-cost benthic monitoring. To this end, we present
DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term
underwater exploration and habitat monitoring. The results show that our
framework is highly efficient in finding and exploring target objects (e.g.,
oysters, shipwrecks) without prior location information. In the
oyster-monitoring task, our framework takes 31.5% less time than the previous
baseline with the same amount of oysters. Compared to the vanilla VLM, it uses
23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our
framework successfully explores and maps the wreck without collisions,
requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,
while the vanilla model achieves 60.23% average coverage in our shipwreck
environments.

</details>


### [19] [SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics](https://arxiv.org/abs/2509.13691)
*Songhao Huang,Yuwei Wu,Guangyao Shi,Gaurav S. Sukhatme,Vijay Kumar*

Main category: cs.RO

TL;DR: SPAR框架利用大语言模型从自然语言输入自动生成有效的PDDL规划领域，特别针对无人机任务，解决了手动设计领域耗时易错的问题。


<details>
  <summary>Details</summary>
Motivation: PDDL是机器人规划中广泛采用的标准，但为不同应用（如监视、交付、检查）手动设计领域既费时又容易出错，这阻碍了采用和实际部署。

Method: 提出了SPAR框架，利用LLM的生成能力从自然语言输入自动生成有效、多样且语义准确的PDDL领域。首先构建了系统制定和验证的无人机规划数据集，然后设计了提示框架来生成高质量的PDDL领域。

Result: 生成的领域通过语法验证、可执行性、可行性和可解释性进行评估。结果表明LLM能够显著加速复杂规划领域的创建。

Conclusion: 这项工作证明了LLM可以大幅加速复杂规划领域的创建，提供了可复现的数据集和评估管道，使没有先前经验的应用专家能够将其用于实际任务，并推动空中机器人和自动化规划的未来研究。

Abstract: We investigate the problem of automatic domain generation for the Planning
Domain Definition Language (PDDL) using Large Language Models (LLMs), with a
particular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a
widely adopted standard in robotic planning, manually designing domains for
diverse applications such as surveillance, delivery, and inspection is
labor-intensive and error-prone, which hinders adoption and real-world
deployment. To address these challenges, we propose SPAR, a framework that
leverages the generative capabilities of LLMs to automatically produce valid,
diverse, and semantically accurate PDDL domains from natural language input. To
this end, we first introduce a systematically formulated and validated UAV
planning dataset, consisting of ground-truth PDDL domains and associated
problems, each paired with detailed domain and action descriptions. Building on
this dataset, we design a prompting framework that generates high-quality PDDL
domains from language input. The generated domains are evaluated through syntax
validation, executability, feasibility, and interpretability. Overall, this
work demonstrates that LLMs can substantially accelerate the creation of
complex planning domains, providing a reproducible dataset and evaluation
pipeline that enables application experts without prior experience to leverage
it for practical tasks and advance future research in aerial robotics and
automated planning.

</details>


### [20] [HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion](https://arxiv.org/abs/2509.13692)
*Yadan Zeng,Jiadong Zhou,Xiaohan Li,I-Ming Chen*

Main category: cs.RO

TL;DR: HGACNet是一个用于点云补全的新框架，通过分层图注意力编码器和多尺度跨模态融合模块，结合单视图RGB图像指导，实现高质量的点云补全。


<details>
  <summary>Details</summary>
Motivation: 解决由于自遮挡和传感器限制导致的不完整几何形状问题，这些不完整的点云会严重影响下游的推理和交互任务，如抓取规划、避障和操作。

Method: 使用分层图注意力（HGA）编码器自适应选择关键局部点，通过图注意力下采样逐步细化分层几何特征；设计多尺度跨模态融合（MSCF）模块进行注意力特征对齐；提出对比损失（C-Loss）来显式对齐跨模态特征分布。

Result: 在ShapeNet-ViPC基准和YCB-Complete数据集上的广泛实验证实了HGACNet的有效性，展示了最先进的性能以及在真实世界机器人操作任务中的强大适用性。

Conclusion: HGACNet通过分层几何特征编码和图像引导先验的融合，成功解决了点云补全问题，在多个基准测试中表现出色，并具有良好的实际应用价值。

Abstract: Point cloud completion is essential for robotic perception, object
reconstruction and supporting downstream tasks like grasp planning, obstacle
avoidance, and manipulation. However, incomplete geometry caused by
self-occlusion and sensor limitations can significantly degrade downstream
reasoning and interaction. To address these challenges, we propose HGACNet, a
novel framework that reconstructs complete point clouds of individual objects
by hierarchically encoding 3D geometric features and fusing them with
image-guided priors from a single-view RGB image. At the core of our approach,
the Hierarchical Graph Attention (HGA) encoder adaptively selects critical
local points through graph attention-based downsampling and progressively
refines hierarchical geometric features to better capture structural continuity
and spatial relationships. To strengthen cross-modal interaction, we further
design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs
attention-based feature alignment between hierarchical geometric features and
structured visual representations, enabling fine-grained semantic guidance for
completion. In addition, we proposed the contrastive loss (C-Loss) to
explicitly align the feature distributions across modalities, improving
completion fidelity under modality discrepancy. Finally, extensive experiments
conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset
confirm the effectiveness of HGACNet, demonstrating state-of-the-art
performance as well as strong applicability in real-world robotic manipulation
tasks.

</details>


### [21] [EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility](https://arxiv.org/abs/2509.13720)
*Tianle Zeng,Jianwei Peng,Hanjing Ye,Guangcheng Chen,Senzi Luo,Hong Zhang*

Main category: cs.RO

TL;DR: 提出了一种用于室外零样本目标导航的统一轻量级系统，通过多尺度图像瓦片层次结构和层次化目标显著性融合，解决了远距离小目标和间歇性遮挡的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决室外零样本目标导航中的两个关键挑战：远距离目标在图像中投影极小，以及由于部分或完全遮挡导致的间歇性可见性问题。

Method: 基于对齐的多尺度图像瓦片层次结构，通过层次化目标显著性融合将局部语义对比汇总为稳定的粗层区域显著性，提供目标方向和可见性指示。使用关键帧记忆、显著性加权历史航向融合和在暂时不可见时主动搜索来维持航向。

Result: 在仿真和真实世界室外试验中，系统能检测150米以外的语义目标，在可见性变化时以82.6%的概率维持正确航向，相比最先进方法整体任务成功率提高17.5%。

Conclusion: 该系统实现了对远距离和间歇性可观察目标的鲁棒零样本导航，避免了整图缩放，支持确定性自底向上聚合，并在移动机器人上高效运行。

Abstract: Zero-shot object navigation (ZSON) in large-scale outdoor environments faces
many challenges; we specifically address a coupled one: long-range targets that
reduce to tiny projections and intermittent visibility due to partial or
complete occlusion. We present a unified, lightweight closed-loop system built
on an aligned multi-scale image tile hierarchy. Through hierarchical
target-saliency fusion, it summarizes localized semantic contrast into a stable
coarse-layer regional saliency that provides the target direction and indicates
target visibility. This regional saliency supports visibility-aware heading
maintenance through keyframe memory, saliency-weighted fusion of historical
headings, and active search during temporary invisibility. The system avoids
whole-image rescaling, enables deterministic bottom-up aggregation, supports
zero-shot navigation, and runs efficiently on a mobile robot. Across simulation
and real-world outdoor trials, the system detects semantic targets beyond 150m,
maintains a correct heading through visibility changes with 82.6% probability,
and improves overall task success by 17.5% compared with the SOTA methods,
demonstrating robust ZSON toward distant and intermittently observable targets.

</details>


### [22] [Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings](https://arxiv.org/abs/2509.13731)
*Jeongwoo Park,Seabin Lee,Changmin Park,Wonjong Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: 提出一种基于基础模型的RL算法，用于柔性扁平电缆插入任务，通过真实到模拟的方法减少训练时间并消除物理损坏风险。


<details>
  <summary>Details</summary>
Motivation: 工业机器人插入柔性扁平电缆需要亚毫米级精度，传统方法需要人工轨迹生成，RL训练存在非确定性和安全风险。

Method: 使用基础模型SAM2进行语义分割，保留电缆和插座的几何空间信息，结合VLM自动生成分割掩码，在仿真环境中进行RL训练。

Result: 方法展现出零样本能力，无需微调即可直接部署到真实环境。

Conclusion: 提出的基于基础模型的真实到模拟方法有效解决了柔性电缆插入任务中的训练挑战，实现了安全高效的自动化部署。

Abstract: The industrial insertion of flexible flat cables (FFCs) into receptacles
presents a significant challenge owing to the need for submillimeter precision
when handling the deformable cables. In manufacturing processes, FFC insertion
with robotic manipulators often requires laborious human-guided trajectory
generation. While Reinforcement Learning (RL) offers a solution to automate
this task without modeling complex properties of FFCs, the nondeterminism
caused by the deformability of FFCs requires significant efforts and time on
training. Moreover, training directly in a real environment is dangerous as
industrial robots move fast and possess no safety measure. We propose an RL
algorithm for FFC insertion that leverages a foundation model-based real-to-sim
approach to reduce the training time and eliminate the risk of physical damages
to robots and surroundings. Training is done entirely in simulation, allowing
for random exploration without the risk of physical damages. Sim-to-real
transfer is achieved through semantic segmentation masks which leave only those
visual features relevant to the insertion tasks such as the geometric and
spatial information of the cables and receptacles. To enhance generality, we
use a foundation model, Segment Anything Model 2 (SAM2). To eleminate human
intervention, we employ a Vision-Language Model (VLM) to automate the initial
prompting of SAM2 to find segmentation masks. In the experiments, our method
exhibits zero-shot capabilities, which enable direct deployments to real
environments without fine-tuning.

</details>


### [23] [FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph](https://arxiv.org/abs/2509.13733)
*Xiaolin Zhou,Tingyang Xiao,Liu Liu,Yucheng Wang,Maiyue Chen,Xinrui Meng,Xinjie Wang,Wei Feng,Wei Sui,Zhizhong Su*

Main category: cs.RO

TL;DR: FSR-VLN是一个视觉语言导航系统，通过分层多模态场景图和快慢推理机制，在长距离导航任务中实现了SOTA性能，响应时间减少82%，并成功集成到人形机器人实现实时导航。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法在长距离空间推理方面存在局限，表现为低成功率高延迟，特别是在长距离导航任务中。需要解决这些限制以提升机器人系统在真实环境中的部署能力。

Method: 提出FSR-VLN系统，结合分层多模态场景图(HMSG)和快慢导航推理(FSR)。HMSG提供从粗粒度房间定位到细粒度目标视图和物体识别的渐进检索支持。FSR先进行快速匹配选择候选，再用VLM驱动精炼进行最终目标选择。

Result: 在4个室内数据集上评估，使用87条涵盖多样物体类别的指令。FSR-VLN在所有数据集上都达到SOTA性能（检索成功率），相比基于VLM的方法响应时间减少82%，仅在快速直觉失败时激活慢速推理。

Conclusion: FSR-VLN成功解决了长距离视觉语言导航的挑战，显著提升了导航效率和成功率，并成功集成到Unitree-G1人形机器人实现自然语言交互和实时导航，展示了实际部署的可行性。

Abstract: Visual-Language Navigation (VLN) is a fundamental challenge in robotic
systems, with broad applications for the deployment of embodied agents in
real-world environments. Despite recent advances, existing approaches are
limited in long-range spatial reasoning, often exhibiting low success rates and
high inference latency, particularly in long-range navigation tasks. To address
these limitations, we propose FSR-VLN, a vision-language navigation system that
combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow
Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation
supporting progressive retrieval, from coarse room-level localization to
fine-grained goal view and object identification. Building on HMSG, FSR first
performs fast matching to efficiently select candidate rooms, views, and
objects, then applies VLM-driven refinement for final goal selection. We
evaluated FSR-VLN across four comprehensive indoor datasets collected by
humanoid robots, utilizing 87 instructions that encompass a diverse range of
object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all
datasets, measured by the retrieval success rate (RSR), while reducing the
response time by 82% compared to VLM-based methods on tour videos by activating
slow reasoning only when fast intuition fails. Furthermore, we integrate
FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1
humanoid robot, enabling natural language interaction and real-time navigation.

</details>


### [24] [Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning](https://arxiv.org/abs/2509.13736)
*Muyuan Ma,Long Cheng,Lijun Han,Xiuze Xia,Houcheng Li*

Main category: cs.RO

TL;DR: 提出基于元模仿学习的可穿戴外骨骼辅助框架，通过任务特定神经网络预测肘关节运动，使用MAML实现快速适应新任务和用户，显著降低肌肉激活和代谢成本


<details>
  <summary>Details</summary>
Motivation: 开发个性化和任务通用的外骨骼辅助算法是重要挑战，现有方法难以同时实现任务泛化和用户适应性

Method: 从公开RGB视频和动作捕捉数据提取全身关键点运动，在仿真中重定向生成肘关节屈曲轨迹，使用MAML框架训练任务特定神经网络，通过重力补偿PD控制器实现稳定辅助

Result: 外骨骼显著降低了新用户执行未训练任务时的肌肉激活和代谢成本，相比无辅助情况有显著改善

Conclusion: 该框架有效提高了可穿戴外骨骼系统的任务泛化能力和用户适应性

Abstract: Wearable exoskeletons can augment human strength and reduce muscle fatigue
during specific tasks. However, developing personalized and task-generalizable
assistance algorithms remains a critical challenge. To address this, a
meta-imitation learning approach is proposed. This approach leverages a
task-specific neural network to predict human elbow joint movements, enabling
effective assistance while enhancing generalization to new scenarios. To
accelerate data collection, full-body keypoint motions are extracted from
publicly available RGB video and motion-capture datasets across multiple tasks,
and subsequently retargeted in simulation. Elbow flexion trajectories generated
in simulation are then used to train the task-specific neural network within
the model-agnostic meta-learning (MAML) framework, which allows the network to
rapidly adapt to novel tasks and unseen users with only a few gradient updates.
The adapted network outputs personalized references tracked by a
gravity-compensated PD controller to ensure stable assistance. Experimental
results demonstrate that the exoskeleton significantly reduces both muscle
activation and metabolic cost for new users performing untrained tasks,
compared to performing without exoskeleton assistance. These findings suggest
that the proposed framework effectively improves task generalization and user
adaptability for wearable exoskeleton systems.

</details>


### [25] [Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control](https://arxiv.org/abs/2509.13737)
*Renjie Wang,Shangke Lyu,Donglin Wang*

Main category: cs.RO

TL;DR: 提出解耦框架分离支撑腿和摆动腿控制，通过在线快速适应能力解决强化学习在腿式 locomotion 中的分布外泛化问题和 sim-to-real 差距


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在腿式运动控制中面临分布外条件性能下降和仿真-现实差距问题，主要依赖域随机化的方法存在局限性

Method: 提出解耦框架，将支撑腿控制和摆动腿控制分离，获得快速在线适应能力，缓解陌生环境中的 sim-to-real 问题

Result: 在仿真和真实世界实验中证明了对水平力干扰、不平坦地形、重载和偏置载荷以及 sim-to-real 差距的有效性

Conclusion: 解耦控制框架为解决腿式 locomotion 的鲁棒性和 sim-to-real 迁移问题提供了有效的新途径

Abstract: While Reinforcement Learning (RL) has achieved remarkable progress in legged
locomotion control, it often suffers from performance degradation in
out-of-distribution (OOD) conditions and discrepancies between the simulation
and the real environments. Instead of mainly relying on domain randomization
(DR) to best cover the real environments and thereby close the sim-to-real gap
and enhance robustness, this work proposes an emerging decoupled framework that
acquires fast online adaptation ability and mitigates the sim-to-real problems
in unfamiliar environments by isolating stance-leg control and swing-leg
control. Various simulation and real-world experiments demonstrate its
effectiveness against horizontal force disturbances, uneven terrains, heavy and
biased payloads, and sim-to-real gap.

</details>


### [26] [CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs](https://arxiv.org/abs/2509.13771)
*Mengzhu Li,Yunyu Zhou,He Ying,F. Richard Yu*

Main category: cs.RO

TL;DR: CDFlow是一个基于神经ODE的新框架，通过建模最小距离碰撞配置的多模态分布来解决传统CDF在高自由度机器人中的梯度模糊和几何失真问题


<details>
  <summary>Details</summary>
Motivation: 传统配置空间距离场(CDF)在高自由度机器人中存在两个主要问题：只能返回单个最近碰撞配置导致梯度模糊，以及稀疏采样导致几何失真和过度平滑

Method: 使用神经ODE学习配置空间中的连续流，重新定义问题为建模最小距离碰撞配置的分布，并引入自适应细化采样策略生成高质量训练数据

Result: 在高端自由度运动规划任务中，CDFlow显著提高了规划效率、轨迹质量和鲁棒性，相比现有CDF方法有显著改进

Conclusion: CDFlow通过神经ODE建模多模态分布，产生平滑一致的梯度场，有效解决了传统CDF的局限性，为复杂环境中的碰撞感知机器人提供了更鲁棒和高效的规划方案

Abstract: Signed Distance Fields (SDFs) are a fundamental representation in robot
motion planning. Their configuration-space counterpart, the Configuration Space
Distance Field (CDF), directly encodes distances in joint space, offering a
unified representation for optimization and control. However, existing CDF
formulations face two major challenges in high-degree-of-freedom (DoF) robots:
(1) they effectively return only a single nearest collision configuration,
neglecting the multi-modal nature of minimal-distance collision configurations
and leading to gradient ambiguity; and (2) they rely on sparse sampling of the
collision boundary, which often fails to identify the true closest
configurations, producing oversmoothed approximations and geometric distortion
in high-dimensional spaces. We propose CDFlow, a novel framework that addresses
these limitations by learning a continuous flow in configuration space via
Neural Ordinary Differential Equations (Neural ODEs). We redefine the problem
from finding a single nearest point to modeling the distribution of
minimal-distance collision configurations. We also introduce an adaptive
refinement sampling strategy to generate high-fidelity training data for this
distribution. The resulting Neural ODE implicitly models this multi-modal
distribution and produces a smooth, consistent gradient field-derived as the
expected direction towards the distribution-that mitigates gradient ambiguity
and preserves sharp geometric features. Extensive experiments on high-DoF
motion planning tasks demonstrate that CDFlow significantly improves planning
efficiency, trajectory quality, and robustness compared to existing CDF-based
methods, enabling more robust and efficient planning for collision-aware robots
in complex environments.

</details>


### [27] [Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach](https://arxiv.org/abs/2509.13774)
*Piaopiao Jin,Qi Wang,Guokang Sun,Ziwen Cai,Pinjia He,Yangwei You*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的人类在环双演员微调框架，通过语言命令将人类修正转化为语义基础的数据集，在真实世界多任务中实现高效学习


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在复杂真实任务中面临挑战，监督微调受限于数据质量，需要强化学习作为替代方案来提升机器人操作性能

Method: 人类在环双演员框架：主演员负责鲁棒多任务性能，精炼演员进行潜在空间适应；引入轻量级talk-and-tweak方案将人类修正转化为语义基础的语言命令

Result: 真实世界多任务实验：101分钟在线微调实现3个任务100%成功率；长时域任务：12个连续操作维持50%成功率；多机器人训练：双机器人效率提升2倍

Conclusion: 该框架通过人类修正的语言化转换有效解决了VLA模型在复杂任务中的微调问题，展示了在真实世界机器人操作中的高效性和可扩展性

Abstract: Vision-language-action (VLA) models demonstrate strong generalization in
robotic manipulation but face challenges in complex, real-world tasks. While
supervised fine-tuning with demonstrations is constrained by data quality,
reinforcement learning (RL) offers a promising alternative. We propose a
human-in-the-loop dual-actor fine-tuning framework grounded in RL. The
framework integrates a primary actor for robust multi-task performance with a
refinement actor for latent-space adaptation. Beyond standard physical
interventions, we introduce a lightweight talk-and-tweak scheme that converts
human corrections into semantically grounded language commands, thereby
generating a new dataset for policy learning. In real-world multi-task
experiments, our approach achieves 100% success across three tasks within 101
minutes of online fine-tuning. For long-horizon tasks, it sustains a 50%
success rate over 12 consecutive operations. Furthermore, the framework scales
effectively to multi-robot training, achieving up to a 2 times improvement in
efficiency when using dual robots. The experiment videos are available at
https://sites.google.com/view/hil-daft/.

</details>


### [28] [Behavior Foundation Model for Humanoid Robots](https://arxiv.org/abs/2509.13780)
*Weishuai Zeng,Shunlin Lu,Kangning Yin,Xiaojie Niu,Minyue Dai,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 提出了行为基础模型（BFM），这是一个基于大规模行为数据集预训练的生成模型，用于人形机器人的全身控制，能够跨任务泛化并快速适应新行为。


<details>
  <summary>Details</summary>
Motivation: 现有全身控制框架任务特定性强，依赖大量奖励工程，跨任务泛化能力有限，难以应对复杂现实场景中的任意控制模式。

Method: 采用掩码在线蒸馏框架结合条件变分自编码器（CVAE）来建模行为分布，通过大规模行为数据集预训练捕获广泛可重用的行为知识。

Result: 在仿真和物理人形平台上的大量实验表明，BFM能够稳健地泛化到不同的全身控制任务，并快速适应新行为。

Conclusion: BFM为实现通用人形控制的基础模型迈出了有希望的一步，展示了在多样化控制模式下灵活操作和高效获取新行为的能力。

Abstract: Whole-body control (WBC) of humanoid robots has witnessed remarkable progress
in skill versatility, enabling a wide range of applications such as locomotion,
teleoperation, and motion tracking. Despite these achievements, existing WBC
frameworks remain largely task-specific, relying heavily on labor-intensive
reward engineering and demonstrating limited generalization across tasks and
skills. These limitations hinder their response to arbitrary control modes and
restrict their deployment in complex, real-world scenarios. To address these
challenges, we revisit existing WBC systems and identify a shared objective
across diverse tasks: the generation of appropriate behaviors that guide the
robot toward desired goal states. Building on this insight, we propose the
Behavior Foundation Model (BFM), a generative model pretrained on large-scale
behavioral datasets to capture broad, reusable behavioral knowledge for
humanoid robots. BFM integrates a masked online distillation framework with a
Conditional Variational Autoencoder (CVAE) to model behavioral distributions,
thereby enabling flexible operation across diverse control modes and efficient
acquisition of novel behaviors without retraining from scratch. Extensive
experiments in both simulation and on a physical humanoid platform demonstrate
that BFM generalizes robustly across diverse WBC tasks while rapidly adapting
to new behaviors. These results establish BFM as a promising step toward a
foundation model for general-purpose humanoid control.

</details>


### [29] [Shell-Type Soft Jig for Holding Objects during Disassembly](https://arxiv.org/abs/2509.13802)
*Takuya Kiyokawa,Ryunosuke Takebayashi,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一种基于气囊的壳式软夹具，用于机器人拆卸中的柔性夹持，能够安全通用地夹持各种形状物体，减少对专用夹具设计和高精度感知的需求。


<details>
  <summary>Details</summary>
Motivation: 传统夹具需要专用设计、高精度感知和精确控制，容易造成部件损坏。需要一种能够适应不同形状、对识别和规划错误具有鲁棒性的柔性夹持解决方案。

Method: 采用壳式软夹具设计，基于气囊夹持机制，实现软固定和自适应夹持，确保正确对齐和稳定性能。

Result: 实验结果表明，与虎钳和堵塞夹持器启发的软夹具相比，所提夹具具有实际可行性。在10种不同物体上的测试显示了代表性成功和失败案例。

Conclusion: 该壳式软夹具为机器人拆卸提供了有效的柔性夹持解决方案，但仍有局限性需要进一步改进。

Abstract: This study addresses a flexible holding tool for robotic disassembly. We
propose a shell-type soft jig that securely and universally holds objects,
mitigating the risk of component damage and adapting to diverse shapes while
enabling soft fixation that is robust to recognition, planning, and control
errors. The balloon-based holding mechanism ensures proper alignment and stable
holding performance, thereby reducing the need for dedicated jig design, highly
accurate perception, precise grasping, and finely tuned trajectory planning
that are typically required with conventional fixtures. Our experimental
results demonstrate the practical feasibility of the proposed jig through
performance comparisons with a vise and a jamming-gripper-inspired soft jig.
Tests on ten different objects further showed representative successes and
failures, clarifying the jig's limitations and outlook.

</details>


### [30] [Soft Regrasping Tool Inspired by Jamming Gripper](https://arxiv.org/abs/2509.13815)
*Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一种基于软夹具的重新抓取方法，利用堵塞过渡现象形成可变形空腔来适应不同几何形状的零件，实现了高精度的零件放置。


<details>
  <summary>Details</summary>
Motivation: 传统刚性夹具缺乏适应性且需要为每个零件专门设计，限制了在机器人装配中的广泛应用。需要开发一种能够适应多样化几何形状的通用夹具解决方案。

Method: 采用软夹具设计，通过将三角锥形工具压入膜中并排出封闭空气形成稳定空腔作为放置空间，优化压印深度以平衡放置稳定性和抓取器可达性。

Result: 在10种不同形状的机械零件上进行跌落实验，大多数物体的放置成功率超过80%，圆柱形零件达到90%以上，失败主要由几何约束和膜特性引起。

Conclusion: 所提出的软夹具实现了通用、准确和可重复的重新抓取，展示了作为装配自动化中刚性夹具实用替代品的潜力和局限性。

Abstract: Regrasping on fixtures is a promising approach to reduce pose uncertainty in
robotic assembly, but conventional rigid fixtures lack adaptability and require
dedicated designs for each part. To overcome this limitation, we propose a soft
jig inspired by the jamming transition phenomenon, which can be continuously
deformed to accommodate diverse object geometries. By pressing a
triangular-pyramid-shaped tool into the membrane and evacuating the enclosed
air, a stable cavity is formed as a placement space. We further optimize the
stamping depth to balance placement stability and gripper accessibility. In
soft-jig-based regrasping, the key challenge lies in optimizing the cavity size
to achieve precise dropping; once the part is reliably placed, subsequent
grasping can be performed with reduced uncertainty. Accordingly, we conducted
drop experiments on ten mechanical parts of varying shapes, which achieved
placement success rates exceeding 80% for most objects and above 90% for
cylindrical ones, while failures were mainly caused by geometric constraints
and membrane properties. These results demonstrate that the proposed jig
enables general-purpose, accurate, and repeatable regrasping, while also
clarifying its current limitations and future potential as a practical
alternative to rigid fixtures in assembly automation.

</details>


### [31] [Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation](https://arxiv.org/abs/2509.13816)
*Yude Li,Zhexuan Zhou,Huizhe Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 提出异步强化学习框架解决无人机高频控制与低频感知的冲突，通过时间编码模块处理感知延迟，实现100Hz控制频率的鲁棒自主导航


<details>
  <summary>Details</summary>
Motivation: 现代端到端导航面临高频控制需求与低频感知流之间的冲突，传统同步模型被迫使用不理想的低控制频率

Method: 异步强化学习框架解耦感知与控制，高频策略基于最新IMU状态响应，异步整合感知特征；引入时间编码模块(TEM)显式处理感知延迟，采用两阶段课程学习确保训练稳定性

Result: 在广泛仿真中验证，成功实现零样本仿真到现实迁移，在机载NUC上维持100Hz控制频率，在复杂真实环境中展示鲁棒敏捷导航

Conclusion: 该方法有效解决了感知-控制频率不匹配问题，实现了高频响应的鲁棒自主导航，代码将开源供社区参考

Abstract: Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex
environments is a critical capability. However, modern end-to-end navigation
faces a key challenge: the high-frequency control loop needed for agile flight
conflicts with low-frequency perception streams, which are limited by sensor
update rates and significant computational cost. This mismatch forces
conventional synchronous models into undesirably low control rates. To resolve
this, we propose an asynchronous reinforcement learning framework that
decouples perception and control, enabling a high-frequency policy to act on
the latest IMU state for immediate reactivity, while incorporating perception
features asynchronously. To manage the resulting data staleness, we introduce a
theoretically-grounded Temporal Encoding Module (TEM) that explicitly
conditions the policy on perception delays, a strategy complemented by a
two-stage curriculum to ensure stable and efficient training. Validated in
extensive simulations, our method was successfully deployed in zero-shot
sim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate
and demonstrates robust, agile navigation in cluttered real-world environments.
Our source code will be released for community reference.

</details>


### [32] [How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots](https://arxiv.org/abs/2509.13827)
*Renyuan Liu,Haoting Zhou,Chuankai Fang,Qinbing Fu*

Main category: cs.RO

TL;DR: 本文提出了一种受苍蝇视觉神经元启发的注意力驱动视觉运动控制策略，实现了微型机器人的碰撞感知和反应性规避，在96.1%的成功率下达到与现有方法相当的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在复杂环境中需要类似苍蝇的敏捷性，但计算成本和性能之间存在权衡。昆虫启发的智能为低功耗、计算高效的框架提供了简洁的途径。

Method: 基于苍蝇视觉投影神经元LPLC2的神经模型，简化优化至70KB内存，并加入多注意力机制模拟LPLC2的分布式响应特性，使机器人能够快速选择性地检测和反应接近目标。

Result: 与最先进的蝗虫启发的碰撞检测模型相比，苍蝇启发的视觉运动模型实现了相当的鲁棒性（96.1%成功率），同时产生更具适应性和优雅的规避动作。

Conclusion: 这项工作不仅展示了有效的避碰策略，还突显了苍蝇启发的神经模型在推进昆虫智能集体行为研究方面的潜力。

Abstract: Anyone who has tried to swat a fly has likely been frustrated by its
remarkable agility.This ability stems from its visual neural perception system,
particularly the collision-selective neurons within its small brain.For
autonomous robots operating in complex and unfamiliar environments, achieving
similar agility is highly desirable but often constrained by the trade-off
between computational cost and performance.In this context, insect-inspired
intelligence offers a parsimonious route to low-power, computationally
efficient frameworks.In this paper, we propose an attention-driven visuomotor
control strategy inspired by a specific class of fly visual projection
neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated
escape behaviors.To our knowledge, this represents the first embodiment of an
LPLC2 neural model in the embedded vision of a physical mobile robot, enabling
collision perception and reactive evasion.The model was simplified and
optimized at 70KB in memory to suit the computational constraints of a
vision-based micro robot, the Colias, while preserving key neural perception
mechanisms.We further incorporated multi-attention mechanisms to emulate the
distributed nature of LPLC2 responses, allowing the robot to detect and react
to approaching targets both rapidly and selectively.We systematically evaluated
the proposed method against a state-of-the-art locust-inspired collision
detection model.Results showed that the fly-inspired visuomotor model achieved
comparable robustness, at success rate of 96.1% in collision detection while
producing more adaptive and elegant evasive maneuvers.Beyond demonstrating an
effective collision-avoidance strategy, this work highlights the potential of
fly-inspired neural models for advancing research into collective behaviors in
insect intelligence.

</details>


### [33] [UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography](https://arxiv.org/abs/2509.13832)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Xiangjie Yan,Xiang Li,Gao Huang*

Main category: cs.RO

TL;DR: 提出基于分层Transformer的决策架构UltraHiT，用于自动化颈动脉超声扫描中具有挑战性的颈内动脉定位，通过高层变异评估和低层动作决策的整合，在未见个体上达到95%的成功率。


<details>
  <summary>Details</summary>
Motivation: 颈内动脉位置深、路径曲折且个体差异大，传统方法难以自动化扫描。研究将个体血管结构概念化为标准血管模型的形态变异，旨在解决这一挑战。

Method: 采用分层Transformer架构：高层模块识别血管变异并切换两个低层模块（适应校正器处理变异情况，标准执行器处理正常情况）。高层模块和适应校正器均实现为因果Transformer，基于历史扫描序列生成预测。

Result: 在包含28名受试者、164条轨迹和72K样本的大规模数据集上测试，方法在未见个体上定位颈内动脉的成功率达到95%，优于基线方法。

Conclusion: UltraHiT架构有效解决了颈内动脉自动化超声扫描的挑战，通过分层决策和Transformer技术实现了高精度的血管定位，展现了良好的泛化能力。

Abstract: Carotid ultrasound is crucial for the assessment of cerebrovascular health,
particularly the internal carotid artery (ICA). While previous research has
explored automating carotid ultrasound, none has tackled the challenging ICA.
This is primarily due to its deep location, tortuous course, and significant
individual variations, which greatly increase scanning complexity. To address
this, we propose a Hierarchical Transformer-based decision architecture, namely
UltraHiT, that integrates high-level variation assessment with low-level action
decision. Our motivation stems from conceptualizing individual vascular
structures as morphological variations derived from a standard vascular model.
The high-level module identifies variation and switches between two low-level
modules: an adaptive corrector for variations, or a standard executor for
normal cases. Specifically, both the high-level module and the adaptive
corrector are implemented as causal transformers that generate predictions
based on the historical scanning sequence. To ensure generalizability, we
collected the first large-scale ICA scanning dataset comprising 164
trajectories and 72K samples from 28 subjects of both genders. Based on the
above innovations, our approach achieves a 95% success rate in locating the ICA
on unseen individuals, outperforming baselines and demonstrating its
effectiveness. Our code will be released after acceptance.

</details>


### [34] [Track Any Motions under Any Disturbances](https://arxiv.org/abs/2509.13833)
*Zhikai Zhang,Jun Guo,Chao Chen,Jilong Wang,Chenghuai Lin,Yunrui Lian,Han Xue,Zhenrong Wang,Maoqi Liu,Huaping Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: Any2Track是一个两阶段强化学习框架，用于在现实世界中跟踪各种运动并应对多种干扰，通过AnyTracker通用运动跟踪器和AnyAdapter动态适应模块实现零样本sim2real迁移。


<details>
  <summary>Details</summary>
Motivation: 开发能够在现实世界中稳定跟踪多样化、高动态、接触丰富的运动，并能应对各种动态干扰（如地形、外力和物理属性变化）的基础人形运动跟踪器。

Method: 提出两阶段RL框架：AnyTracker作为通用运动跟踪器，通过精心设计跟踪各种运动；AnyAdapter作为历史信息适应模块，赋予跟踪器在线动态适应能力以克服sim2real差距和现实干扰。

Result: 在Unitree G1硬件上成功部署，实现零样本sim2real迁移，在各种现实干扰下表现出优异的运动跟踪性能。

Conclusion: Any2Track框架有效解决了人形机器人在现实环境中应对多种动态干扰的运动跟踪问题，展示了强大的适应性和实用性。

Abstract: A foundational humanoid motion tracker is expected to be able to track
diverse, highly dynamic, and contact-rich motions. More importantly, it needs
to operate stably in real-world scenarios against various dynamics
disturbances, including terrains, external forces, and physical property
changes for general practical use. To achieve this goal, we propose Any2Track
(Track Any motions under Any disturbances), a two-stage RL framework to track
various motions under multiple disturbances in the real world. Any2Track
reformulates dynamics adaptability as an additional capability on top of basic
action execution and consists of two key components: AnyTracker and AnyAdapter.
AnyTracker is a general motion tracker with a series of careful designs to
track various motions within a single policy. AnyAdapter is a history-informed
adaptation module that endows the tracker with online dynamics adaptability to
overcome the sim2real gap and multiple real-world disturbances. We deploy
Any2Track on Unitree G1 hardware and achieve a successful sim2real transfer in
a zero-shot manner. Any2Track performs exceptionally well in tracking various
motions under multiple real-world disturbances.

</details>


### [35] [Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models](https://arxiv.org/abs/2509.13839)
*Motonari Kambara,Komei Sugiura*

Main category: cs.RO

TL;DR: 提出了一种预测开放式词汇物体操作任务未来成功率的模型，通过多级轨迹融合模块分析末端执行器轨迹的自相关性，在实验中获得优于现有方法的表现


<details>
  <summary>Details</summary>
Motivation: 传统方法只能在操作完成后判断成功与否，难以预防潜在危险且依赖失败触发重规划，降低了物体操作序列的效率

Method: 提出预测预操作自我中心图像与规划轨迹及自然语言指令对齐度的模型，采用多级轨迹融合模块（结合深度状态空间模型和transformer编码器）捕捉末端执行器轨迹的多级时间序列自相关性

Result: 实验结果表明该方法优于现有方法，包括基础模型

Conclusion: 该模型能够有效预测物体操作任务的未来成功率，提高操作安全性和效率

Abstract: In this work, we address the problem of predicting the future success of
open-vocabulary object manipulation tasks. Conventional approaches typically
determine success or failure after the action has been carried out. However,
they make it difficult to prevent potential hazards and rely on failures to
trigger replanning, thereby reducing the efficiency of object manipulation
sequences. To overcome these challenges, we propose a model, which predicts the
alignment between a pre-manipulation egocentric image with the planned
trajectory and a given natural language instruction. We introduce a Multi-Level
Trajectory Fusion module, which employs a state-of-the-art deep state-space
model and a transformer encoder in parallel to capture multi-level time-series
self-correlation within the end effector trajectory. Our experimental results
indicate that the proposed method outperformed existing methods, including
foundation models.

</details>


### [36] [InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap](https://arxiv.org/abs/2509.13857)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.RO

TL;DR: InterKey是一个利用道路交叉口作为地标的跨模态全局定位框架，通过点云和OSM数据构建二进制描述符，在GNSS失效环境下实现高精度车辆定位。


<details>
  <summary>Details</summary>
Motivation: 解决GNSS信号退化环境（如城市峡谷和隧道）中自动驾驶车辆的可靠全局定位问题，同时克服高精地图成本高和OpenStreetMap数据粗糙的挑战。

Method: 提出跨模态框架，利用道路交叉口作为显著地标，通过联合编码点云和OSM中的道路和建筑印记构建紧凑二进制描述符，采用差异缓解、方向确定和面积均衡采样策略来弥合模态差距。

Result: 在KITTI数据集上的实验表明，InterKey实现了最先进的精度，大幅优于现有基线方法。

Conclusion: 该框架可推广到能够产生密集结构点云的传感器，为鲁棒车辆定位提供了可扩展且经济高效的解决方案。

Abstract: Reliable global localization is critical for autonomous vehicles, especially
in environments where GNSS is degraded or unavailable, such as urban canyons
and tunnels. Although high-definition (HD) maps provide accurate priors, the
cost of data collection, map construction, and maintenance limits scalability.
OpenStreetMap (OSM) offers a free and globally available alternative, but its
coarse abstraction poses challenges for matching with sensor data. We propose
InterKey, a cross-modal framework that leverages road intersections as
distinctive landmarks for global localization. Our method constructs compact
binary descriptors by jointly encoding road and building imprints from point
clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,
orientation determination, and area-equalized sampling strategies, enabling
robust cross-modal matching. Experiments on the KITTI dataset demonstrate that
InterKey achieves state-of-the-art accuracy, outperforming recent baselines by
a large margin. The framework generalizes to sensors that can produce dense
structural point clouds, offering a scalable and cost-effective solution for
robust vehicle localization.

</details>


### [37] [Using Petri Nets for Context-Adaptive Robot Explanations](https://arxiv.org/abs/2509.13861)
*Görkem Kılınç Soylu,Neziha Akalin,Maria Riveiro*

Main category: cs.RO

TL;DR: 使用Petri网建模上下文信息，实现机器人自适应解释的正式化方法


<details>
  <summary>Details</summary>
Motivation: 在人类-机器人交互中，机器人需要以自然透明的方式沟通以建立信任，这要求根据上下文调整通信方式

Method: 采用Petri网（PNs）来建模上下文信息，PNs提供了一种正式的图形化方法来表示并发动作、因果依赖和系统状态，适合分析人机动态交互

Result: 通过一个基于用户注意力和存在等上下文线索提供解释的机器人场景进行演示，模型分析确认了关键属性：无死锁、上下文敏感可达性、有界性和活性

Conclusion: PNs在设计和验证人机交互中上下文自适应解释方面表现出鲁棒性和灵活性

Abstract: In human-robot interaction, robots must communicate in a natural and
transparent manner to foster trust, which requires adapting their communication
to the context. In this paper, we propose using Petri nets (PNs) to model
contextual information for adaptive robot explanations. PNs provide a formal,
graphical method for representing concurrent actions, causal dependencies, and
system states, making them suitable for analyzing dynamic interactions between
humans and robots. We demonstrate this approach through a scenario involving a
robot that provides explanations based on contextual cues such as user
attention and presence. Model analysis confirms key properties, including
deadlock-freeness, context-sensitive reachability, boundedness, and liveness,
showing the robustness and flexibility of PNs for designing and verifying
context-adaptive explanations in human-robot interactions.

</details>


### [38] [Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning](https://arxiv.org/abs/2509.13882)
*Junhwa Hong,Beomjoon Lee,Woojin Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: 提出一种基于冲突搜索(CBS)的多机械臂运动规划方法，通过斥力轨迹修改和人工势场梯度下降，减少后续冲突并提高规划效率


<details>
  <summary>Details</summary>
Motivation: 多机械臂系统协调运动面临高维构型空间的计算挑战，传统CBS方法在解决冲突时会产生指数级增长的约束树

Method: 在CBS双层结构中，底层规划器使用人工势场生成斥力进行梯度下降，引导冲突机械臂轨迹远离其他机器人，并开发了单步寻找无冲突解的策略

Result: 通过大量测试和物理机器人实验，该方法显著减少了约束树扩展节点数，提高了成功率，比增强CBS和其他先进算法更快找到解

Conclusion: 基于斥力轨迹修改的CBS方法有效解决了多机械臂运动规划中的冲突问题，提高了规划效率和成功率

Abstract: We propose an efficient motion planning method designed to efficiently find
collision-free trajectories for multiple manipulators. While multi-manipulator
systems offer significant advantages, coordinating their motions is
computationally challenging owing to the high dimensionality of their composite
configuration space. Conflict-Based Search (CBS) addresses this by decoupling
motion planning, but suffers from subsequent conflicts incurred by resolving
existing conflicts, leading to an exponentially growing constraint tree of CBS.
Our proposed method is based on repulsive trajectory modification within the
two-level structure of CBS. Unlike conventional CBS variants, the low-level
planner applies a gradient descent approach using an Artificial Potential
Field. This field generates repulsive forces that guide the trajectory of the
conflicting manipulator away from those of other robots. As a result,
subsequent conflicts are less likely to occur. Additionally, we develop a
strategy that, under a specific condition, directly attempts to find a
conflict-free solution in a single step without growing the constraint tree.
Through extensive tests including physical robot experiments, we demonstrate
that our method consistently reduces the number of expanded nodes in the
constraint tree, achieves a higher success rate, and finds a solution faster
compared to Enhanced CBS and other state-of-the-art algorithms.

</details>


### [39] [PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models](https://arxiv.org/abs/2509.13903)
*Artem Lykov,Jeffrin Sam,Hung Khang Nguyen,Vladislav Kozlovskiy,Yara Mahmoud,Valerii Serpiva,Miguel Altamirano Cabrera,Mikhail Konenkov,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: PhysicalAgent是一个机器人操作框架，通过迭代推理、扩散视频生成和闭环执行来处理文本指令，生成候选轨迹视频演示，并在执行失败时重新规划，实现高达83%的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人操作中执行错误恢复的问题，开发一个能够通过迭代推理和视频生成来增强鲁棒性的通用框架。

Method: 结合迭代推理、基于扩散的视频生成和闭环执行，根据文本指令生成候选轨迹的视频演示，执行并在失败时重新规划。

Result: 在多种感知模态和机器人平台上，该方法优于现有方法，首次尝试成功率20-30%，通过迭代校正整体成功率提升至80%。

Conclusion: 基于视频的生成推理在通用机器人操作中具有潜力，迭代执行对从初始失败中恢复至关重要，为可扩展、适应性强和鲁棒的机器人控制铺平道路。

Abstract: We introduce PhysicalAgent, an agentic framework for robotic manipulation
that integrates iterative reasoning, diffusion-based video generation, and
closed-loop execution. Given a textual instruction, our method generates short
video demonstrations of candidate trajectories, executes them on the robot, and
iteratively re-plans in response to failures. This approach enables robust
recovery from execution errors. We evaluate PhysicalAgent across multiple
perceptual modalities (egocentric, third-person, and simulated) and robotic
embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing
against state-of-the-art task-specific baselines. Experiments demonstrate that
our method consistently outperforms prior approaches, achieving up to 83%
success on human-familiar tasks. Physical trials reveal that first-attempt
success is limited (20-30%), yet iterative correction increases overall success
to 80% across platforms. These results highlight the potential of video-based
generative reasoning for general-purpose robotic manipulation and underscore
the importance of iterative execution for recovering from initial failures. Our
framework paves the way for scalable, adaptable, and robust robot control.

</details>


### [40] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: MAP是一个新颖的端到端轨迹规划框架，通过显式整合在线地图特征和自车状态，显著提升了自动驾驶轨迹规划性能


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法未能充分利用在线地图模块的潜力来增强轨迹规划

Method: 提出MAP框架，包含规划增强在线地图模块、自车状态引导规划模块和基于自车状态的权重适配器，显式整合分割式地图特征和当前自车状态

Result: 在DAIR-V2X-seq-SPD数据集上实现L2位移误差降低16.6%，脱轨率降低56.2%，综合评分提升44.5%；在CVPR2025挑战赛中排名第一，比第二名模型综合评分高39.5%

Conclusion: 显式利用语义地图特征在规划中具有显著效果，为改进端到端自动驾驶系统结构设计提供了新方向

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [41] [Reinforcement Learning for Autonomous Point-to-Point UAV Navigation](https://arxiv.org/abs/2509.13943)
*Salim Oyinlola,Nitesh Subedi,Soumik Sarkar*

Main category: cs.RO

TL;DR: 使用强化学习训练无人机自主导航，通过自定义奖励函数实现高效目标到达并避免碰撞，在真实无人机平台上验证了可行性


<details>
  <summary>Details</summary>
Motivation: 无人机在自动化检查、配送和导航任务中需要可靠自主性，需要开发无需人工干预的自主导航系统

Method: 采用强化学习方法，通过试错交互学习导航策略，使用自定义奖励函数鼓励高效到达目标并惩罚碰撞和不安全行为，集成ROS和Gym兼容的训练环境

Result: 训练后的策略在真实无人机平台上成功实现自主导航，只需最少人工监督

Conclusion: 强化学习控制方法在现实场景中点对点无人机操作中具有可行性

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in automated
inspection, delivery, and navigation tasks that require reliable autonomy. This
project develops a reinforcement learning (RL) approach to enable a single UAV
to autonomously navigate between predefined points without manual intervention.
The drone learns navigation policies through trial-and-error interaction, using
a custom reward function that encourages goal-reaching efficiency while
penalizing collisions and unsafe behavior. The control system integrates ROS
with a Gym-compatible training environment, enabling flexible deployment and
testing. After training, the learned policy is deployed on a real UAV platform
and evaluated under practical conditions. Results show that the UAV can
successfully perform autonomous navigation with minimal human oversight,
demonstrating the viability of RL-based control for point-to-point drone
operations in real-world scenarios.

</details>


### [42] [The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot](https://arxiv.org/abs/2509.13948)
*Benedict Barrow,Roger K. Moore*

Main category: cs.RO

TL;DR: 研究表明社交机器人面部特征中眼睛形状和大小对可信度感知有显著影响，婴儿脸特征能增强信任感


<details>
  <summary>Details</summary>
Motivation: 探索影响人类对机器人信任的因素，特别是社交机器人面部特征如何影响第一印象和可信度感知

Method: 通过操纵Furhat机器人的背投影面部特征，研究不同眼睛形状和大小对信任感知的影响

Result: 确认眼睛形状和大小对可信度感知有显著影响，婴儿脸特征能够促进信任

Conclusion: 该研究为社交机器人开发中的设计选择提供了重要见解，有助于优化人机交互效果

Abstract: Trust and the perception of trustworthiness play an important role in
decision-making and our behaviour towards others, and this is true not only of
human-human interactions but also of human-robot interactions. While
significant advances have been made in recent years in the field of social
robotics, there is still some way to go before we fully understand the factors
that influence human trust in robots. This paper presents the results of a
study into the first impressions created by a social robot's facial features,
based on the hypothesis that a `babyface' engenders trust. By manipulating the
back-projected face of a Furhat robot, the study confirms that eye shape and
size have a significant impact on the perception of trustworthiness. The work
thus contributes to an understanding of the design choices that need to be made
when developing social robots so as to optimise the effectiveness of
human-robot interaction.

</details>


### [43] [SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks](https://arxiv.org/abs/2509.13949)
*Jannick Stranghöner,Philipp Hartmann,Marco Braun,Sebastian Wrede,Klaus Neumann*

Main category: cs.RO

TL;DR: SHaRe-RL是一个强化学习框架，通过整合多种先验知识来解决高混合低产量工业装配中的接触密集型任务，实现了高效安全的在线学习。


<details>
  <summary>Details</summary>
Motivation: 当前机器人系统在高混合低产量工业装配中面临挑战：手动编程脆弱且成本高，基于学习的方法在接触密集型任务中样本效率低且探索不安全。需要一种既能保持灵活性又能确保精度和安全性的解决方案。

Method: SHaRe-RL框架整合三种先验知识：(i)将技能结构化为操作原语，(ii)结合人类演示和在线修正，(iii)通过每轴顺应性限制交互力。

Result: 在工业Harting连接器模块插入任务（间隙0.2-0.4mm）的实验中，SHaRe-RL在实用时间预算内实现了可靠性能，过程专业知识能够有效促进学习。

Conclusion: SHaRe-RL使工业装配中强化学习的部署更安全、更稳健、更经济可行，无需机器人或RL专业知识的过程知识也能有意义地促进学习。

Abstract: High-mix low-volume (HMLV) industrial assembly, common in small and
medium-sized enterprises (SMEs), requires the same precision, safety, and
reliability as high-volume automation while remaining flexible to product
variation and environmental uncertainty. Current robotic systems struggle to
meet these demands. Manual programming is brittle and costly to adapt, while
learning-based methods suffer from poor sample efficiency and unsafe
exploration in contact-rich tasks. To address this, we present SHaRe-RL, a
reinforcement learning framework that leverages multiple sources of prior
knowledge. By (i) structuring skills into manipulation primitives, (ii)
incorporating human demonstrations and online corrections, and (iii) bounding
interaction forces with per-axis compliance, SHaRe-RL enables efficient and
safe online learning for long-horizon, contact-rich industrial assembly tasks.
Experiments on the insertion of industrial Harting connector modules with
0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance
within practical time budgets. Our results show that process expertise, without
requiring robotics or RL knowledge, can meaningfully contribute to learning,
enabling safer, more robust, and more economically viable deployment of RL for
industrial assembly.

</details>


### [44] [SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning](https://arxiv.org/abs/2509.13956)
*Zewei Yang,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: SEG-Parking是一个端到端的离线强化学习框架，用于解决自动驾驶中的交互感知自主泊车问题，在复杂无结构环境中表现出优异的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 非结构化环境和动态交互给自主泊车任务带来了重大挑战，需要开发能够处理复杂交互场景的智能泊车系统。

Method: 构建专门的泊车数据集，预训练目标条件状态编码器将感知信息映射到潜在空间，使用带有保守正则器的离线强化学习策略优化，惩罚分布外动作。

Result: 在高保真CARLA模拟器中进行的闭环实验显示，该框架具有最高的成功率和对分布外泊车场景的强大泛化能力。

Conclusion: SEG-Parking框架在自主泊车任务中表现出卓越性能，相关数据集和源代码将在论文接受后公开。

Abstract: Autonomous parking is a critical component for achieving safe and efficient
urban autonomous driving. However, unstructured environments and dynamic
interactions pose significant challenges to autonomous parking tasks. To
address this problem, we propose SEG-Parking, a novel end-to-end offline
reinforcement learning (RL) framework to achieve interaction-aware autonomous
parking. Notably, a specialized parking dataset is constructed for parking
scenarios, which include those without interference from the opposite vehicle
(OV) and complex ones involving interactions with the OV. Based on this
dataset, a goal-conditioned state encoder is pretrained to map the fused
perception information into the latent space. Then, an offline RL policy is
optimized with a conservative regularizer that penalizes out-of-distribution
actions. Extensive closed-loop experiments are conducted in the high-fidelity
CARLA simulator. Comparative results demonstrate the superior performance of
our framework with the highest success rate and robust generalization to
out-of-distribution parking scenarios. The related dataset and source code will
be made publicly available after the paper is accepted.

</details>


### [45] [MetricNet: Recovering Metric Scale in Generative Navigation Policies](https://arxiv.org/abs/2509.13965)
*Abhijeet Nayak,Débora N. P. Oliveira,Samiran Gode,Cordelia Schmid,Wolfram Burgard*

Main category: cs.RO

TL;DR: MetricNet通过预测路标点之间的度量距离，将生成式导航策略的输出锚定到真实世界坐标中，解决了现有方法缺乏度量基础和短视行为的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式导航策略存在两个结构性问题：1）采样的轨迹存在于抽象的无尺度空间中，缺乏度量基础；2）控制策略丢弃完整路径，只朝向单个路标点移动，导致短视和不安全的动作。

Method: 提出MetricNet作为生成式导航的附加模块，预测路标点之间的度量距离，将策略输出锚定到真实世界坐标。进一步提出MetricNav，将MetricNet集成到导航策略中，引导机器人避开障碍物同时朝向目标移动。

Result: 在模拟环境中使用新的基准测试框架进行评估，执行MetricNet缩放的路标点显著提高了导航和探索性能。在真实世界实验中也验证了方法的有效性。

Conclusion: MetricNet通过度量距离预测有效解决了生成式导航的尺度问题和短视行为，MetricNav进一步整合该技术实现了更安全有效的导航性能。

Abstract: Generative navigation policies have made rapid progress in improving
end-to-end learned navigation. Despite their promising results, this paradigm
has two structural problems. First, the sampled trajectories exist in an
abstract, unscaled space without metric grounding. Second, the control strategy
discards the full path, instead moving directly towards a single waypoint. This
leads to short-sighted and unsafe actions, moving the robot towards obstacles
that a complete and correctly scaled path would circumvent. To address these
issues, we propose MetricNet, an effective add-on for generative navigation
that predicts the metric distance between waypoints, grounding policy outputs
in real-world coordinates. We evaluate our method in simulation with a new
benchmarking framework and show that executing MetricNet-scaled waypoints
significantly improves both navigation and exploration performance. Beyond
simulation, we further validate our approach in real-world experiments.
Finally, we propose MetricNav, which integrates MetricNet into a navigation
policy to guide the robot away from obstacles while still moving towards the
goal.

</details>


### [46] [BIM Informed Visual SLAM for Construction Monitoring](https://arxiv.org/abs/2509.13972)
*Asier Bikandi,Miguel Fernandez-Cortizas,Muhammad Shaheer,Ali Tourani,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 基于BIM结构前知的RGB-D SLAM系统，通过将检测到的墙体与BIM模型对应关系作为约束优化，在建筑布局重复、遮挡等挑战性环境中提升定位和建图精度


<details>
  <summary>Details</summary>
Motivation: 解决建筑环境中视觉SLAM遇到的挑战：布局重复、遮挡、结构缺失或紧紧结构导致轨迹偏移，而LiDAR SLAM供电需求高且体积大，限制了在箱体平台上的应用

Method: 提出一种结合BIM模型结构前知的RGB-D SLAM系统，通过连续建立检测到的墙体与BIM模型对应关系，并将这些对应关系作为约束引入到后端优化中

Result: 在实际建筑工地验证，轨迹误差平均减少23.71%，地图RMSE减少7.14%，较传统视觉SLAM基线显著提升

Conclusion: BIM约束能够在部分建造条件下供靠地对齐数字计划与实际建筑场景，为建筑工地监控提供了更准确的SLAM解决方案

Abstract: Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring
construction sites, where aligning the evolving as-built state with the
as-planned design enables early error detection and reduces costly rework.
LiDAR-based SLAM achieves high geometric precision, but its sensors are
typically large and power-demanding, limiting their use on portable platforms.
Visual SLAM offers a practical alternative with lightweight cameras already
embedded in most mobile devices. however, visually mapping construction
environments remains challenging: repetitive layouts, occlusions, and
incomplete or low-texture structures often cause drift in the trajectory map.
To mitigate this, we propose an RGB-D SLAM system that incorporates the
Building Information Model (BIM) as structural prior knowledge. Instead of
relying solely on visual cues, our system continuously establishes
correspondences between detected wall and their BIM counterparts, which are
then introduced as constraints in the back-end optimization. The proposed
method operates in real time and has been validated on real construction sites,
reducing trajectory error by an average of 23.71% and map RMSE by 7.14%
compared to visual SLAM baselines. These results demonstrate that BIM
constraints enable reliable alignment of the digital plan with the as-built
scene, even under partially constructed conditions.

</details>


### [47] [Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array](https://arxiv.org/abs/2509.13998)
*Bailey Dacre,Rodrigo Moreno,Serhat Demirtas,Ziqiao Wang,Yuhao Jiang,Jamie Paik,Kasper Stoy,Andrés Faíña*

Main category: cs.RO

TL;DR: 提出了一种基于折纸灵感的3自由度机器人瓦片阵列分布式操纵系统，通过柔性连接表面层实现连续可控的操纵表面，显著降低了执行器密度，提高了操纵范围。


<details>
  <summary>Details</summary>
Motivation: 传统分布式操纵系统需要高执行器密度且对物体与执行器尺寸比例有严格限制，限制了系统的适应性和应用范围。

Method: 使用3自由度折纸启发式机器人瓦片阵列，通过柔性表面层连接所有执行器，形成连续的操纵表面，分析系统工作空间并推导简单运动原语。

Result: 系统能够在不增加执行器数量的情况下将物体操纵面积提高1.84倍，成功演示了几何物体在瓦片阵列上的平移操作。

Conclusion: 该设计提供了比传统高密度阵列更低成本和复杂度的替代方案，并为利用互连表面柔性的新操纵策略创造了机会。

Abstract: Object manipulation is a fundamental challenge in robotics, where systems
must balance trade-offs among manipulation capabilities, system complexity, and
throughput. Distributed manipulator systems (DMS) use the coordinated motion of
actuator arrays to perform complex object manipulation tasks, seeing widespread
exploration within the literature and in industry. However, existing DMS
designs typically rely on high actuator densities and impose constraints on
object-to-actuator scale ratios, limiting their adaptability. We present a
novel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles
interconnected by a compliant surface layer. Unlike conventional DMS, our
approach enables manipulation not only at the actuator end effectors but also
across a flexible surface connecting all actuators; creating a continuous,
controllable manipulation surface. We analyse the combined workspace of such a
system, derive simple motion primitives, and demonstrate its capabilities to
translate simple geometric objects across an array of tiles. By leveraging the
inter-tile connective material, our approach significantly reduces actuator
density, increasing the area over which an object can be manipulated by x1.84
without an increase in the number of actuators. This design offers a lower cost
and complexity alternative to traditional high-density arrays, and introduces
new opportunities for manipulation strategies that leverage the flexibility of
the interconnected surface.

</details>


### [48] [Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization](https://arxiv.org/abs/2509.14010)
*Zong Chen,Shaoyang Li,Ben Liu,Min Li,Zhouping Yin,Yiqun Li*

Main category: cs.RO

TL;DR: 提出了一种轮腿四足机器人及其全身运动控制框架，解决了轮腿机械臂系统在移动操作中的统一控制挑战，包括冗余自由度、复杂轮地接触动力学和运动操作协调问题。


<details>
  <summary>Details</summary>
Motivation: 轮腿机器人集成机械臂在物流、工业自动化和人机协作中具有巨大潜力，但由于自由度冗余、复杂轮地接触动力学以及需要无缝协调运动和操作，这类系统的统一控制仍然具有挑战性。

Method: 开发了接触感知的全身动态优化框架，集成了用于操作的点接触建模和用于轮地交互的线接触建模；引入热启动策略加速在线优化；为机器人的4WIS-4WID驱动方案定制了统一运动学模型。

Result: 仿真和实验结果验证了框架的有效性，展示了敏捷的地形穿越、高速全向移动和多样化场景下的精确操作能力。

Conclusion: 该系统在半结构化环境中的工厂自动化、城市物流和服务机器人领域具有巨大潜力，提出的控制框架为轮腿机械臂系统的统一控制提供了有效解决方案。

Abstract: Wheel-legged robots with integrated manipulators hold great promise for
mobile manipulation in logistics, industrial automation, and human-robot
collaboration. However, unified control of such systems remains challenging due
to the redundancy in degrees of freedom, complex wheel-ground contact dynamics,
and the need for seamless coordination between locomotion and manipulation. In
this work, we present the design and whole-body motion control of an
omnidirectional wheel-legged quadrupedal robot equipped with a dexterous
manipulator. The proposed platform incorporates independently actuated steering
modules and hub-driven wheels, enabling agile omnidirectional locomotion with
high maneuverability in structured environments. To address the challenges of
contact-rich interaction, we develop a contact-aware whole-body dynamic
optimization framework that integrates point-contact modeling for manipulation
with line-contact modeling for wheel-ground interactions. A warm-start strategy
is introduced to accelerate online optimization, ensuring real-time feasibility
for high-dimensional control. Furthermore, a unified kinematic model tailored
for the robot's 4WIS-4WID actuation scheme eliminates the need for mode
switching across different locomotion strategies, improving control consistency
and robustness. Simulation and experimental results validate the effectiveness
of the proposed framework, demonstrating agile terrain traversal, high-speed
omnidirectional mobility, and precise manipulation under diverse scenarios,
underscoring the system's potential for factory automation, urban logistics,
and service robotics in semi-structured environments.

</details>


### [49] [TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems](https://arxiv.org/abs/2509.14025)
*Rui Huang,Zhiyu Gao,Siyu Tang,Jialin Zhang,Lei He,Ziqian Zhang,Lin Zhao*

Main category: cs.RO

TL;DR: TransforMARS是一个通用的模块化空中机器人系统故障容错重构框架，能够处理任意形状配置和多重故障，确保飞行稳定性


<details>
  <summary>Details</summary>
Motivation: 现有的MARS自重构方法仅关注矩形形状配置和单故障容错，缺乏处理任意形状和多重故障的能力

Method: 开发算法识别和构建包含故障单元的最小可控组件，规划可行的拆卸-装配序列来运输单元或子组件形成目标配置

Result: 在挑战性的任意形状MARS配置中验证了TransforMARS，相比先前工作在处理多样化配置能力和容错数量方面有显著提升

Conclusion: TransforMARS提供了更灵活实用的故障容错重构方法，能够处理多重转子和单元故障，同时保持空中稳定性

Abstract: Modular Aerial Robot Systems (MARS) consist of multiple drone modules that
are physically bound together to form a single structure for flight. Exploiting
structural redundancy, MARS can be reconfigured into different formations to
mitigate unit or rotor failures and maintain stable flight. Prior work on MARS
self-reconfiguration has solely focused on maximizing controllability margins
to tolerate a single rotor or unit fault for rectangular-shaped MARS. We
propose TransforMARS, a general fault-tolerant reconfiguration framework that
transforms arbitrarily shaped MARS under multiple rotor and unit faults while
ensuring continuous in-air stability. Specifically, we develop algorithms to
first identify and construct minimum controllable assemblies containing faulty
units. We then plan feasible disassembly-assembly sequences to transport MARS
units or subassemblies to form target configuration. Our approach enables more
flexible and practical feasible reconfiguration. We validate TransforMARS in
challenging arbitrarily shaped MARS configurations, demonstrating substantial
improvements over prior works in both the capacity of handling diverse
configurations and the number of faults tolerated. The videos and source code
of this work are available at the anonymous repository:
https://anonymous.4open.science/r/TransforMARS-1030/

</details>


### [50] [Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning](https://arxiv.org/abs/2509.14040)
*Zewen Yang,Xiaobing Dai,Dongfa Zhang,Yu Li,Ziyang Meng,Bingkun Huang,Hamid Sadeghian,Sami Haddadin*

Main category: cs.RO

TL;DR: Prompt2Auto是一个几何不变的单样本高斯过程学习框架，通过坐标变换实现平移、旋转和缩放不变性，能够从单个运动提示中实现人机协作的自动化控制。


<details>
  <summary>Details</summary>
Motivation: 传统的示教学习方法需要大量数据集且难以处理坐标变换的泛化问题，限制了机器人从人类演示中学习复杂技能的效率。

Method: 提出几何不变高斯过程(GeoGP)框架，基于坐标变换的数据集构建策略，支持多步预测，对用户运动提示的变化具有鲁棒性，并支持多技能自主。

Result: 通过数值模拟和两个真实机器人实验验证，该方法有效、能够跨任务泛化，并显著减少了演示负担。

Conclusion: Prompt2Auto框架成功解决了传统方法在坐标变换泛化方面的局限性，为从单样本演示中学习机器人技能提供了有效的解决方案。

Abstract: Learning from demonstration allows robots to acquire complex skills from
human demonstrations, but conventional approaches often require large datasets
and fail to generalize across coordinate transformations. In this paper, we
propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)
learning framework that enables robots to perform human-guided automated
control from a single motion prompt. A dataset-construction strategy based on
coordinate transformations is introduced that enforces invariance to
translation, rotation, and scaling, while supporting multi-step predictions.
Moreover, GeoGP is robust to variations in the user's motion prompt and
supports multi-skill autonomy. We validate the proposed approach through
numerical simulations with the designed user graphical interface and two
real-world robotic experiments, which demonstrate that the proposed method is
effective, generalizes across tasks, and significantly reduces the
demonstration burden. Project page is available at:
https://prompt2auto.github.io

</details>


### [51] [Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace](https://arxiv.org/abs/2509.14063)
*Sundhar Vinodh Sangeetha,Chih-Yuan Chiu,Sarah H. Q. Li,Shreyas Kousik*

Main category: cs.RO

TL;DR: 本文提出了一个多模态框架，结合自然语言理解和空间推理来预测飞机在无塔台空域的目标位置，通过语音识别和语言模型解析飞行员无线电通信，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在无塔台空域中，飞机安全运行依赖于飞行员之间的语音通信协调。自主飞机需要准确预测其他飞机的意图和目标位置，但现有方法主要依赖运动历史数据，缺乏语言信息的整合。

Method: 使用自动语音识别和大语言模型转录和解释飞行员无线电呼叫，识别飞机并提取离散意图标签。将这些意图标签与观测轨迹融合，通过时间卷积网络和高斯混合模型进行概率目标预测。

Result: 与仅依赖运动历史的基线方法相比，该方法显著降低了目标预测误差，证明了语言条件预测能提高预测准确性。在真实无塔台机场数据集上的实验验证了方法的有效性。

Conclusion: 该多模态框架通过整合语言信息显著改善了飞机目标预测性能，为实现社会感知、语言条件化的机器人运动规划提供了潜力。

Abstract: Autonomous aircraft must safely operate in untowered airspace, where
coordination relies on voice-based communication among human pilots. Safe
operation requires an aircraft to predict the intent, and corresponding goal
location, of other aircraft. This paper introduces a multimodal framework for
aircraft goal prediction that integrates natural language understanding with
spatial reasoning to improve autonomous decision-making in such environments.
We leverage automatic speech recognition and large language models to
transcribe and interpret pilot radio calls, identify aircraft, and extract
discrete intent labels. These intent labels are fused with observed
trajectories to condition a temporal convolutional network and Gaussian mixture
model for probabilistic goal prediction. Our method significantly reduces goal
prediction error compared to baselines that rely solely on motion history,
demonstrating that language-conditioned prediction increases prediction
accuracy. Experiments on a real-world dataset from an untowered airport
validate the approach and highlight its potential to enable socially aware,
language-conditioned robotic motion planning.

</details>


### [52] [Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots](https://arxiv.org/abs/2509.14075)
*Yu Li,Hamid Sadeghian,Zewen Yang,Valentin Le Mesle,Sami Haddadin*

Main category: cs.RO

TL;DR: 本文提出了一种约束一致扭矩控制器，用于机器人辅助微创手术中的远程运动中心约束精确执行，通过投影逆动力学框架实现精确工具尖端跟踪和扭矩平滑性。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术需要精确执行远程运动中心约束以确保手术安全，但现有控制方法在扭矩级别缺乏鲁棒性或无法保证一致的约束满足。

Method: 将远程运动中心约束视为变完整约束，嵌入到基于投影的逆动力学框架中，统一任务级和运动学表述，实现精确工具尖端跟踪和平滑扭矩行为。

Result: 在仿真和实际平台上验证显示，该方法提高了约束满足度，降低了所需扭矩，通过一致性表述改善了关节扭矩平滑性，在螺旋轨迹、可变插入深度、移动套管和人类交互等临床相关场景中表现鲁棒。

Conclusion: 约束一致扭矩控制有潜力提高手术机器人的安全性和可靠性。

Abstract: Robotic-assisted minimally invasive surgery (RAMIS) requires precise
enforcement of the remote center of motion (RCM) constraint to ensure safe tool
manipulation through a trocar. Achieving this constraint under dynamic and
interactive conditions remains challenging, as existing control methods either
lack robustness at the torque level or do not guarantee consistent RCM
constraint satisfaction. This paper proposes a constraint-consistent torque
controller that treats the RCM as a rheonomic holonomic constraint and embeds
it into a projection-based inverse-dynamics framework. The method unifies
task-level and kinematic formulations, enabling accurate tool-tip tracking
while maintaining smooth and efficient torque behavior. The controller is
validated both in simulation and on a RAMIS training platform, and is
benchmarked against state-of-the-art approaches. Results show improved RCM
constraint satisfaction, reduced required torque, and robust performance by
improving joint torque smoothness through the consistency formulation under
clinically relevant scenarios, including spiral trajectories, variable
insertion depths, moving trocars, and human interaction. These findings
demonstrate the potential of constraint-consistent torque control to enhance
safety and reliability in surgical robotics. The project page is available at:
https://rcmpc-cube.github.io

</details>


### [53] [FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video](https://arxiv.org/abs/2509.14082)
*Valerii Serpiva,Artem Lykov,Faryal Batool,Vladislav Kozlovskiy,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FlightDiffusion是一个基于扩散模型的框架，用于从第一人称视角视频训练自主无人机，能够生成逼真的视频序列和相应的动作空间，支持推理驱动的导航和大规模数据集合成。


<details>
  <summary>Details</summary>
Motivation: 解决自主无人机训练中真实世界数据收集成本高的问题，通过生成模型合成多样化的FPV轨迹和状态-动作对，为无人机导航提供可扩展的训练数据。

Method: 采用扩散模型框架，从单帧图像生成连续的第一人称视角视频序列，并同时生成对应的动作空间，实现物理上可行的轨迹合成和动作推理。

Result: 生成的轨迹物理上可行且可执行，平均位置误差0.25米，平均方向误差0.19弧度。在模拟环境中表现出更强的鲁棒性、更平滑的轨迹规划和更好的适应性。仿真与现实性能无显著差异（p=0.541），成功率为0.628 vs 0.617。

Conclusion: 基于扩散模型的推理为无人机导航、动作生成和数据合成提供了统一的新范式，生成的数据集为未来UAV研究提供了宝贵资源，显示出良好的仿真到现实迁移能力。

Abstract: We present FlightDiffusion, a diffusion-model-based framework for training
autonomous drones from first-person view (FPV) video. Our model generates
realistic video sequences from a single frame, enriched with corresponding
action spaces to enable reasoning-driven navigation in dynamic environments.
Beyond direct policy learning, FlightDiffusion leverages its generative
capabilities to synthesize diverse FPV trajectories and state-action pairs,
facilitating the creation of large-scale training datasets without the high
cost of real-world data collection. Our evaluation demonstrates that the
generated trajectories are physically plausible and executable, with a mean
position error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad
(RMSE 0.24 rad). This approach enables improved policy learning and dataset
scalability, leading to superior performance in downstream navigation tasks.
Results in simulated environments highlight enhanced robustness, smoother
trajectory planning, and adaptability to unseen conditions. An ANOVA revealed
no statistically significant difference between performance in simulation and
reality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =
0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real
transfer. The generated datasets provide a valuable resource for future UAV
research. This work introduces diffusion-based reasoning as a promising
paradigm for unifying navigation, action generation, and data synthesis in
aerial robotics.

</details>


### [54] [GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14117)
*Ali Abouzeid,Malak Mansour,Zezhou Sun,Dezhen Song*

Main category: cs.RO

TL;DR: GeoAware-VLA通过集成几何先验增强VLA模型的视角不变性，在未见过的相机视角下实现2倍以上的性能提升，并在真实机器人上验证了有效性


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action模型难以从2D图像推断鲁棒的3D几何信息，导致在新相机视角下泛化能力差

Method: 利用冻结的预训练几何视觉模型作为特征提取器，通过可训练的投影层将几何丰富的特征适配到策略解码器，无需训练视觉编码器或依赖显式3D数据

Result: 在LIBERO基准测试中，零样本泛化到新相机姿态的成功率提升超过2倍，在真实机器人上从未见过的相机角度评估时表现出显著性能增益

Conclusion: 鲁棒的几何基础是创建更具泛化能力的机器人智能体的关键组件，该方法在连续和离散动作空间中都有效

Abstract: Vision-Language-Action (VLA) models often fail to generalize to novel camera
viewpoints, a limitation stemming from their difficulty in inferring robust 3D
geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective
approach that enhances viewpoint invariance by integrating strong geometric
priors into the vision backbone. Instead of training a visual encoder or
relying on explicit 3D data, we leverage a frozen, pretrained geometric vision
model as a feature extractor. A trainable projection layer then adapts these
geometrically-rich features for the policy decoder, relieving it of the burden
of learning 3D consistency from scratch. Through extensive evaluations on
LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial
improvements in zero-shot generalization to novel camera poses, boosting
success rates by over 2x in simulation. Crucially, these benefits translate to
the physical world; our model shows a significant performance gain on a real
robot, especially when evaluated from unseen camera angles. Our approach proves
effective across both continuous and discrete action spaces, highlighting that
robust geometric grounding is a key component for creating more generalizable
robotic agents.

</details>


### [55] [CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads](https://arxiv.org/abs/2509.14126)
*Viktor Lorentz,Khaled Wahba,Sayantan Auddy,Marc Toussaint,Wolfgang Hönig*

Main category: cs.RO

TL;DR: CrazyMARL是一个去中心化强化学习框架，用于多无人机电缆悬挂载荷运输，能够处理电缆松弛-拉紧模式转换，在干扰抑制和跟踪精度方面优于传统方法，实现了80%的恶劣条件恢复率。


<details>
  <summary>Details</summary>
Motivation: 多无人机协同运输电缆悬挂载荷具有提升载荷能力、适应不同载荷形状和内置合规性等优势，但在干扰、非线性载荷动力学和电缆松弛-拉紧模式转换下的协调控制仍是一个挑战。现有工作依赖刚性连接假设，未能解决电缆模式转换问题。

Method: 提出CrazyMARL去中心化强化学习框架，通过模拟训练学习控制策略，处理多无人机电缆悬挂载荷运输中的非线性动力学和电缆模式转换问题。

Result: 模拟结果显示，学习到的策略在干扰抑制和跟踪精度方面优于传统去中心化控制器，从恶劣条件恢复率达到80%（基线方法为44%）。成功实现零样本模拟到现实迁移，在风、随机外部干扰和电缆松弛-拉紧动力学转换等恶劣条件下表现出高度鲁棒性。

Conclusion: 这项工作为在非结构化环境中执行复杂载荷任务的自适应、弹性无人机团队奠定了基础，展示了强化学习在多无人机协同运输中的潜力。

Abstract: Collaborative transportation of cable-suspended payloads by teams of Unmanned
Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to
different payload shapes, and provide built-in compliance, making it attractive
for applications ranging from disaster relief to precision logistics. However,
multi-UAV coordination under disturbances, nonlinear payload dynamics, and
slack--taut cable modes remains a challenging control problem. To our
knowledge, no prior work has addressed these cable mode transitions in the
multi-UAV context, instead relying on simplifying rigid-link assumptions. We
propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for
multi-UAV cable-suspended payload transport. Simulation results demonstrate
that the learned policies can outperform classical decentralized controllers in
terms of disturbance rejection and tracking precision, achieving an 80%
recovery rate from harsh conditions compared to 44% for the baseline method. We
also achieve successful zero-shot sim-to-real transfer and demonstrate that our
policies are highly robust under harsh conditions, including wind, random
external disturbances, and transitions between slack and taut cable dynamics.
This work paves the way for autonomous, resilient UAV teams capable of
executing complex payload missions in unstructured environments.

</details>


### [56] [Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks](https://arxiv.org/abs/2509.14127)
*Alkesh K. Srivastava,Jared Michael Levin,Philip Dames*

Main category: cs.RO

TL;DR: VCST-RCP框架通过Voronoi约束的Steiner树优化构建稀疏中继主干，将中继从附带产物转变为协调核心，相比传统直接运输方法提升效率达34%


<details>
  <summary>Details</summary>
Motivation: 解决多机器人有限运载能力下的包裹配送问题，传统直接源到目的地运输方法效率有限，需要新的协调框架来提升配送效率

Method: 提出Voronoi约束的Steiner树中继协调规划框架，构建稀疏中继主干网络，合成机器人级别的取货、中继和送货调度计划

Result: 实验显示相比传统基线方法有高达34%的持续改进，显著提升了容量约束下多机器人配送的能源效率

Conclusion: 将中继纳入配送过程具有显著优势，为现实世界物流提供了可扩展的框架，中继应成为协调的核心元素而非附带产物

Abstract: We consider the problem of delivering multiple packages from a single pickup
depot to distinct goal locations using a homogeneous fleet of robots with
limited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner
Tree Relay Coordination Planning framework that constructs sparse relay trunks
using Steiner tree optimization and then synthesizes robot-level pickup, relay,
and delivery schedules. This framework reframes relays from incidental
byproducts into central elements of coordination, offering a contrast with
traditional delivery methods that rely on direct source-to-destination
transport. Extensive experiments show consistent improvements of up to 34%
compared to conventional baselines, underscoring the benefits of incorporating
relays into the delivery process. These improvements translate directly to
enhanced energy efficiency in multi-robot delivery under capacity constraints,
providing a scalable framework for real-world logistics.

</details>


### [57] [SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14138)
*Ran Yang,Zijian An,Lifeng ZHou,Yiming Feng*

Main category: cs.RO

TL;DR: SeqVLA是一个基于π₀的视觉-语言-动作模型扩展，通过添加轻量级检测头来感知子任务完成状态，从而在长时程多阶段机器人操作任务中实现自主子任务转换，显著提高了序列任务的执行成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型如π₀擅长连续低级控制，但缺乏检测子任务完成的内在信号，导致在需要严格顺序执行的多阶段任务中容易因错误累积而失败。

Method: 在π₀基础架构上增加轻量级检测头，形成双头设计（动作生成头和完成检测头）。研究了四种微调策略：联合vs顺序微调，全微调vs冻结主干网络。在两个多阶段任务（沙拉打包和糖果打包）上进行实验。

Result: SeqVLA显著优于基线π₀和其他强基线。联合微调且不冻结主干网络的策略产生最确定和统计可靠的完成预测，消除了序列相关失败，实现了鲁棒的长时程执行。

Conclusion: 将动作生成与子任务感知检测相结合对于可扩展的顺序操作至关重要，SeqVLA的成功证明了这种双头设计在长时程机器人操作任务中的有效性。

Abstract: Long-horizon robotic manipulation tasks require executing multiple
interdependent subtasks in strict sequence, where errors in detecting subtask
completion can cascade into downstream failures. Existing
Vision-Language-Action (VLA) models such as $\pi_0$ excel at continuous
low-level control but lack an internal signal for identifying when a subtask
has finished, making them brittle in sequential settings. We propose SeqVLA, a
completion-aware extension of $\pi_0$ that augments the base architecture with
a lightweight detection head perceiving whether the current subtask is
complete. This dual-head design enables SeqVLA not only to generate
manipulation actions but also to autonomously trigger transitions between
subtasks. We investigate four finetuning strategies that vary in how the action
and detection heads are optimized (joint vs. sequential finetuning) and how
pretrained knowledge is preserved (full finetuning vs. frozen backbone).
Experiments are performed on two multi-stage tasks: salad packing with seven
distinct subtasks and candy packing with four distinct subtasks. Results show
that SeqVLA significantly outperforms the baseline $\pi_0$ and other strong
baselines in overall success rate. In particular, joint finetuning with an
unfrozen backbone yields the most decisive and statistically reliable
completion predictions, eliminating sequence-related failures and enabling
robust long-horizon execution. Our results highlight the importance of coupling
action generation with subtask-aware detection for scalable sequential
manipulation.

</details>


### [58] [CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping](https://arxiv.org/abs/2509.14143)
*Zijian An,Ran Yang,Yiming Feng,Lifeng Zhou*

Main category: cs.RO

TL;DR: CLAW是一个将条件评估与动作生成解耦的视觉-语言-动作框架，通过微调CLIP模型监控数字读数并生成基于重量阈值的离散指令，再由VLA策略整合多视角摄像头观测生成连续机器人动作。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA模型在处理需要精确任务约束（如基于数值阈值的停止）时表现不佳，因为其观测到动作的映射是隐式学习的，缺乏明确的条件监控机制。

Method: 提出CLAW框架，使用微调的CLIP模型作为轻量级提示生成器来持续监控秤的数字读数并基于任务特定的重量阈值产生离散指令，然后由基于流的VLA策略π₀整合多视角摄像头观测来生成连续机器人动作。

Result: 在单物体抓取和需要双臂操作的混合物体任务三个实验设置中，CLAW可靠地执行了重量感知行为，并且优于原始π₀和微调π₀模型。

Conclusion: CLAW成功地将符号化重量推理与高频视觉运动控制相结合，为需要精确数值约束的机器人控制任务提供了有效的解决方案。

Abstract: Vision-language-action (VLA) models have recently emerged as a promising
paradigm for robotic control, enabling end-to-end policies that ground natural
language instructions into visuomotor actions. However, current VLAs often
struggle to satisfy precise task constraints, such as stopping based on numeric
thresholds, since their observation-to-action mappings are implicitly shaped by
training data and lack explicit mechanisms for condition monitoring. In this
work, we propose CLAW (CLIP-Language-Action for Weight), a framework that
decouples condition evaluation from action generation. CLAW leverages a
fine-tuned CLIP model as a lightweight prompt generator, which continuously
monitors the digital readout of a scale and produces discrete directives based
on task-specific weight thresholds. These prompts are then consumed by $\pi_0$,
a flow-based VLA policy, which integrates the prompts with multi-view camera
observations to produce continuous robot actions. This design enables CLAW to
combine symbolic weight reasoning with high-frequency visuomotor control. We
validate CLAW on three experimental setups: single-object grasping and
mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW
reliably executes weight-aware behaviors and outperforms both raw-$\pi_0$ and
fine-tuned $\pi_0$ models. We have uploaded the videos as supplementary
materials.

</details>


### [59] [StableTracker: Learning to Stably Track Target via Differentiable Simulation](https://arxiv.org/abs/2509.14147)
*Fanxing Li,Shengyang Wang,Fangyu Sun,Shuyu Wu,Dexin Zuo,Wenxian Yu,Danping Zou*

Main category: cs.RO

TL;DR: StableTracker是一个基于学习的控制策略，通过可微分模拟训练，使四旋翼无人机能够从任意视角稳健跟踪移动目标，在准确性和稳定性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的FPV目标跟踪方法依赖手工模块化设计，导致硬件过载和累积误差，特别是在目标快速加减速时性能严重下降。

Method: 使用通过可微分模拟的反向传播时间训练学习型控制策略，使无人机能在水平和垂直方向保持目标在视野中心，同时保持固定相对距离。

Result: 仿真实验显示该策略在不同安全距离、轨迹和目标速度下具有优越的准确性、稳定性和泛化能力。真实世界实验验证了其实用性。

Conclusion: StableTracker通过学习型控制策略有效解决了传统FPV跟踪方法的局限性，为自主空中摄像提供了稳健的解决方案。

Abstract: FPV object tracking methods heavily rely on handcraft modular designs,
resulting in hardware overload and cumulative error, which seriously degrades
the tracking performance, especially for rapidly accelerating or decelerating
targets. To address these challenges, we present \textbf{StableTracker}, a
learning-based control policy that enables quadrotors to robustly follow the
moving target from arbitrary perspectives. The policy is trained using
backpropagation-through-time via differentiable simulation, allowing the
quadrotor to maintain the target at the center of the visual field in both
horizontal and vertical directions, while keeping a fixed relative distance,
thereby functioning as an autonomous aerial camera. We compare StableTracker
against both state-of-the-art traditional algorithms and learning baselines.
Simulation experiments demonstrate that our policy achieves superior accuracy,
stability and generalization across varying safe distances, trajectories, and
target velocities. Furthermore, a real-world experiment on a quadrotor with an
onboard computer validated practicality of the proposed approach.

</details>


### [60] [MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies](https://arxiv.org/abs/2509.14159)
*Dayi Dong,Maulik Bhatt,Seoyeon Choi,Negar Mehr*

Main category: cs.RO

TL;DR: MIMIC-D是一种基于扩散策略的多模态多智能体模仿学习方法，采用集中训练分散执行(CTDE)范式，使智能体仅使用局部信息就能实现隐式协调，在多种任务中恢复多模态协调行为。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在社会中的集成度提高，需要与人类和其他机器人协调完成多模态任务。传统模仿学习方法难以处理多模态专家演示，而现有扩散模型方法通常需要集中规划或显式通信，这在现实场景中不可行。

Method: 提出MIMIC-D方法，使用扩散策略进行多智能体模仿学习。采用CTDE范式：集中训练时使用完整信息，分散执行时仅使用局部信息。扩散模型能够有效处理多模态轨迹分布，实现隐式协调。

Result: 在仿真和硬件实验中，该方法在多种任务和环境中成功恢复了智能体间的多模态协调行为，性能优于现有最先进的基线方法。

Conclusion: MIMIC-D通过扩散策略和CTDE范式，有效解决了多智能体多模态模仿学习问题，使智能体能够在没有显式通信的情况下实现协调，为现实世界机器人协调提供了可行方案。

Abstract: As robots become more integrated in society, their ability to coordinate with
other robots and humans on multi-modal tasks (those with multiple valid
solutions) is crucial. We propose to learn such behaviors from expert
demonstrations via imitation learning (IL). However, when expert demonstrations
are multi-modal, standard IL approaches can struggle to capture the diverse
strategies, hindering effective coordination. Diffusion models are known to be
effective at handling complex multi-modal trajectory distributions in
single-agent systems. Diffusion models have also excelled in multi-agent
scenarios where multi-modality is more common and crucial to learning
coordinated behaviors. Typically, diffusion-based approaches require a
centralized planner or explicit communication among agents, but this assumption
can fail in real-world scenarios where robots must operate independently or
with agents like humans that they cannot directly communicate with. Therefore,
we propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)
paradigm for multi-modal multi-agent imitation learning using diffusion
policies. Agents are trained jointly with full information, but execute
policies using only local information to achieve implicit coordination. We
demonstrate in both simulation and hardware experiments that our method
recovers multi-modal coordination behavior among agents in a variety of tasks
and environments, while improving upon state-of-the-art baselines.

</details>


### [61] [\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video](https://arxiv.org/abs/2509.14178)
*Kai Ye,Yuhang Wu,Shuyuan Hu,Junliang Li,Meng Liu,Yongquan Chen,Rui Huang*

Main category: cs.RO

TL;DR: Gen2Real使用生成视频替代昂贵的人类演示，通过视频生成、轨迹优化和演示学习实现机器人灵巧操作，在仿真中达到77.3%的成功率并能在真实机器人上执行


<details>
  <summary>Details</summary>
Motivation: 灵巧操作是机器人领域的挑战性问题，主要困难在于收集大量人类演示数据成本高昂

Method: 结合视频生成与姿态深度估计生成手-物体轨迹，使用物理感知交互优化模型确保物理一致性，通过锚点残差PPO策略将人类动作重定向到机器人手并稳定控制

Result: 仅使用生成视频学习的策略在仿真抓取任务中达到77.3%成功率，在真实机器人上展示连贯执行能力

Conclusion: Gen2Real展示了从想象视频到真实世界执行的泛化能力，支持自然语言直接指定任务，具有灵活性和鲁棒性

Abstract: Dexterous manipulation remains a challenging robotics problem, largely due to
the difficulty of collecting extensive human demonstrations for learning. In
this paper, we introduce \textsc{Gen2Real}, which replaces costly human demos
with one generated video and drives robot skill from it: it combines
demonstration generation that leverages video generation with pose and depth
estimation to yield hand-object trajectories, trajectory optimization that uses
Physics-aware Interaction Optimization Model (PIOM) to impose physics
consistency, and demonstration learning that retargets human motions to a robot
hand and stabilizes control with an anchor-based residual Proximal Policy
Optimization (PPO) policy. Using only generated videos, the learned policy
achieves a 77.3\% success rate on grasping tasks in simulation and demonstrates
coherent executions on a real robot. We also conduct ablation studies to
validate the contribution of each component and demonstrate the ability to
directly specify tasks using natural language, highlighting the flexibility and
robustness of \textsc{Gen2Real} in generalizing grasping skills from imagined
videos to real-world execution.

</details>


### [62] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: MCGS-SLAM是首个基于纯RGB输入的多相机3D高斯溅射SLAM系统，通过多视角融合和尺度一致性模块实现实时高精度建图


<details>
  <summary>Details</summary>
Motivation: 现有密集SLAM方法主要针对单目设置，牺牲了鲁棒性和几何覆盖范围，需要开发多相机系统来提升重建质量和安全性

Method: 使用多相机束调整(MCBA)通过密集光度/几何残差联合优化位姿和深度，采用尺度一致性模块利用低秩先验实现跨视图度量对齐

Result: 在合成和真实数据集上实验表明，MCGS-SLAM能产生准确轨迹和逼真重建，优于单目基线，特别能重建单目系统遗漏的侧视区域

Conclusion: 多相机高斯溅射SLAM在机器人和自动驾驶的高保真建图中具有巨大潜力，能够实现更安全可靠的自主操作

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.

</details>


### [63] [GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments](https://arxiv.org/abs/2509.14210)
*Seth Farrell,Chenghao Li,Hongzhan Yu,Hesam Mojtahedi,Sicun Gao,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 提出GLIDE框架，使用两个无人机和一个地面车辆协同进行搜救任务，通过无人机引导和地形侦察实现快速受害者定位和安全导航


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中搜救任务的时间紧迫性和导航安全性问题，通过空中-地面协同系统提高受害者定位效率和障碍物感知能力

Method: 使用目标搜索无人机进行实时受害者检测和地理参考，地形侦察无人机提供路径可通行性更新，地面车辆融合空中线索进行A*规划和持续重规划

Result: 硬件演示和仿真实验表明，明确的角色分离结合地形侦察和引导规划，在时间紧迫的搜救任务中提高了到达时间和导航安全性

Conclusion: GLIDE框架通过空中-地面协同和专门的无人机角色分配，有效提升了搜救任务的效率和安全性

Abstract: We present a cooperative aerial-ground search-and-rescue (SAR) framework that
pairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)
to achieve rapid victim localization and obstacle-aware navigation in unknown
environments. We dub this framework Guided Long-horizon Integrated Drone Escort
(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon
planning. In our framework, a goal-searching UAV executes real-time onboard
victim detection and georeferencing to nominate goals for the ground platform,
while a terrain-scouting UAV flies ahead of the UGV's planned route to provide
mid-level traversability updates. The UGV fuses aerial cues with local sensing
to perform time-efficient A* planning and continuous replanning as information
arrives. Additionally, we present a hardware demonstration (using a GEM e6 golf
cart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission
performance and include simulation ablations to assess the planning stack in
isolation from detection. Empirical results demonstrate that explicit role
separation across UAVs, coupled with terrain scouting and guided planning,
improves reach time and navigation safety in time-critical SAR missions.

</details>


### [64] [Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models](https://arxiv.org/abs/2509.14228)
*Benjamin Shaffer,Victoria Edwards,Brooks Kinch,Nathaniel Trask,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出了一种分布式移动传感框架，使用机器学习有限元模型指导多机器人团队在复杂流动环境中进行信息驱动的源定位


<details>
  <summary>Details</summary>
Motivation: 复杂流动环境中的源定位（如化学泄漏或石油泄漏）面临挑战，包括时变混沌流动、间歇性传感器读数、复杂几何环境以及机载计算能力有限难以运行计算密集型数值模型

Method: 每个机器人携带机器学习有限元环境模型，使用近似互信息准则驱动信息趋向控制策略，选择预期能最大化源定位信息量的传感区域

Result: 相比基线传感策略实现了更快的误差减少，相比基线机器学习方法获得了更准确的源定位结果

Conclusion: 该分布式移动传感框架通过机器学习模型指导信息驱动采样，有效解决了复杂流动环境中的源定位问题，在计算效率和定位精度方面均优于传统方法

Abstract: Source localization in a complex flow poses a significant challenge for
multi-robot teams tasked with localizing the source of chemical leaks or
tracking the dispersion of an oil spill. The flow dynamics can be time-varying
and chaotic, resulting in sporadic and intermittent sensor readings, and
complex environmental geometries further complicate a team's ability to model
and predict the dispersion. To accurately account for the physical processes
that drive the dispersion dynamics, robots must have access to computationally
intensive numerical models, which can be difficult when onboard computation is
limited. We present a distributed mobile sensing framework for source
localization in which each robot carries a machine-learned, finite element
model of its environment to guide information-based sampling. The models are
used to evaluate an approximate mutual information criterion to drive an
infotaxis control strategy, which selects sensing regions that are expected to
maximize informativeness for the source localization objective. Our approach
achieves faster error reduction compared to baseline sensing strategies and
results in more accurate source localization compared to baseline machine
learning approaches.

</details>
