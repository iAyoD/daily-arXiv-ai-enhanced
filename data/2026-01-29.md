<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text](https://arxiv.org/abs/2601.19913)
*Shinwoo Park,Yo-Sub Han*

Main category: cs.CL

TL;DR: 该研究开发了LREAD评分标准，通过结构化校准训练韩语专家识别人类写作与LLM生成文本，将检测准确率从60%提升至100%


<details>
  <summary>Details</summary>
Motivation: 区分人类撰写的韩语文本与流畅的LLM输出对语言专家也很困难，现有检测方法主要针对英语，需要开发适用于非英语语言的解释性检测方法

Method: 引入基于韩国国家写作标准的LREAD评分标准，针对微观语言特征（标点可选性、空格行为、语域转换等）。采用三阶段纵向盲测协议：第一阶段测量直觉检测，第二阶段强制标准级评分并明确理由，第三阶段评估领域专注掌握度

Result: 多数投票准确率从60%提升至100%，评分者间一致性显著提高（Fleiss' kappa: -0.09 → 0.82）。校准后的人类专家比最先进的LLM检测器更依赖语言特定的微观诊断特征

Conclusion: 基于评分标准的专家判断可作为非英语环境中自动化检测器的可解释补充，研究发布了完整的评分标准和校准检测特征分类法

Abstract: Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.

</details>


### [2] [Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments](https://arxiv.org/abs/2601.19914)
*Maxwell Crouse,Ibrahim Abdelaziz,Kshitij Fadnis,Siva Sankalp Patel,Kinjal Basu,Chulaka Gunasekara,Sadhana Kumaravel,Asim Munawar,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: DiGiT-TC：一种无需状态执行环境即可生成多轮工具调用对话合成数据的新方法


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法通常假设存在可维护状态的执行环境来验证工具调用的有效性，但在许多实际场景（如企业数据安全环境或多源工具规范合成）中这种状态环境不可用，需要一种能在无状态环境下生成高质量工具调用对话数据的方法。

Method: 提出DiGiT-TC数据生成方法，采用新颖的生成模式，通过在用户请求中隐式表示某些工具调用，使生成的对话具有在状态环境中通过搜索生成对话的特征，无需依赖实际的状态执行环境。

Result: 在标准工具调用基准测试中验证了该方法，即使在有状态问题设置中，使用DiGiT-TC生成的数据也能带来显著的性能提升。

Conclusion: DiGiT-TC填补了在无状态环境下生成高质量多轮工具调用对话合成数据的空白，为实际应用场景提供了有效的解决方案。

Abstract: Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.

</details>


### [3] [Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication](https://arxiv.org/abs/2601.19915)
*Paul Tarau*

Main category: cs.CL

TL;DR: Arrow Language Model：基于直觉主义逻辑解释的神经架构，将前缀编码为左嵌套蕴含链，通过非交换组合保持顺序，将下一个词预测视为肯定前件推理。


<details>
  <summary>Details</summary>
Motivation: 从逻辑角度重新解释下一个词预测任务，探索基于直觉主义逻辑的神经架构替代方案，为Transformer和状态空间模型提供理论基础的替代方案。

Method: 将前缀编码为左嵌套蕴含链，通过非交换组合保持顺序结构；将下一个词预测对应为肯定前件推理；使用Prolog定理证明器验证神经模型的基本属性；提出实用的低秩神经实现。

Result: 展示了从证明论角度解释下一个词预测时，等价于乘法RNN的神经架构自然产生；建立了交换与非交换序列、单词与多词预测选择之间的关系；将模型与Transformer和状态空间模型进行了定位。

Conclusion: Arrow Language Model为基于逻辑的神经架构提供了新视角，将下一个词预测重新解释为直觉主义逻辑中的构造性证明扩展，为替代Transformer基础模型提供了理论框架。

Abstract: We introduce the \emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.
  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.

</details>


### [4] [PaperAudit-Bench: Benchmarking Error Detection in Research Papers for Critical Automated Peer Review](https://arxiv.org/abs/2601.19916)
*Songjun Tu,Yiwen Ma,Jiahao Lin,Qichao Zhang,Xiangyuan Lan,Junfeng. Li,Nan Xu,Linjing Li,Dongbin Zhao*

Main category: cs.CL

TL;DR: PaperAudit-Bench：一个用于评估大语言模型在长文本中检测论文错误能力的基准，包含数据集和自动化评审框架，能生成更严格、更具区分度的评审意见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能生成流畅的同行评审，但在处理需要跨章节推理的细微实质性问题时，往往缺乏足够的批判性。需要开发能系统检测论文错误并生成高质量评审的工具。

Method: 提出PaperAudit-Bench，包含两个组件：(1) PaperAudit-Dataset：覆盖章节内错误和跨章节推理错误的错误数据集，用于长文本环境下的受控评估；(2) PaperAudit-Review：结合结构化错误检测和证据感知评审生成的自动化评审框架。

Result: 实验显示不同模型和检测深度下错误检测能力差异很大，表明长文本环境下识别此类错误具有挑战性。相比基线方法，结合显式错误检测的评审工作流能产生更严格、更具区分度的评估。数据集支持通过SFT和RL训练轻量级LLM检测器，能以更低计算成本实现有效错误检测。

Conclusion: PaperAudit-Bench为评估和改进大语言模型在学术论文评审中的批判性分析能力提供了有效工具，显式错误检测能显著提升评审质量，轻量级检测器为实际应用提供了可行方案。

Abstract: Large language models can generate fluent peer reviews, yet their assessments often lack sufficient critical rigor when substantive issues are subtle and distributed across a paper. In this paper, we introduce PaperAudit-Bench, which consists of two components: (1) PaperAudit-Dataset, an error dataset covering both errors identifiable within individual sections and those requiring cross-section reasoning, designed for controlled evaluation under long-context settings; and (2) PaperAudit-Review, an automated review framework that integrates structured error detection with evidence-aware review generation to support critical assessment. Experiments on PaperAudit-Bench reveal large variability in error detectability across models and detection depths, highlighting the difficulty of identifying such errors under long-context settings. Relative to representative automated reviewing baselines, incorporating explicit error detection into the review workflow produces systematically stricter and more discriminative evaluations, demonstrating its suitability for peer review. Finally, we show that the dataset supports training lightweight LLM detectors via SFT and RL, enabling effective error detection at reduced computational cost.

</details>


### [5] [PILOT: Planning via Internalized Latent Optimization Trajectories for Large Language Models](https://arxiv.org/abs/2601.19917)
*Haoyu Zheng,Yun Zhu,Yuqian Yuan,Bo Yuan,Wenqiao Zhang,Siliang Tang,Jun Xiao*

Main category: cs.CL

TL;DR: PILOT框架通过轻量级超网络合成查询条件化的潜在指导向量，将大型模型的战略监督内化到紧凑模型中，无需修改主干权重即可提升多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 紧凑型大语言模型在多步推理中缺乏制定全局策略的能力，导致长时域任务中的错误传播。虽然LLMs具有潜在的推理能力，可以通过教师模型的显式计划来解锁，但运行时依赖外部指导通常因延迟和可用性限制而不切实际。

Method: 提出PILOT框架，采用轻量级超网络合成查询条件化的潜在指导向量，该向量作为内部转向机制，引导模型表示朝向最优推理路径，无需修改主干模型权重。

Result: 在数学和编程基准测试上的广泛实验表明，PILOT能有效稳定推理轨迹，显著超越强基线（如在MATH500上提升8.9%），且推理延迟可忽略不计。

Conclusion: PILOT通过内化潜在优化轨迹，成功将大型模型的战略监督能力转移到紧凑模型中，解决了紧凑模型在多步推理中缺乏全局规划能力的问题，同时保持了推理效率。

Abstract: Strategic planning is critical for multi-step reasoning, yet compact Large Language Models (LLMs) often lack the capacity to formulate global strategies, leading to error propagation in long-horizon tasks. Our analysis reveals that LLMs possess latent reasoning capabilities that can be unlocked when conditioned on explicit plans from a teacher model; however, runtime reliance on external guidance is often impractical due to latency and availability constraints. To bridge this gap, we propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework designed to internalize the strategic oversight of large models into intrinsic Latent Guidance. Instead of altering backbone weights, PILOT employs a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector. This vector acts as an internal steering mechanism, guiding the model's representations toward optimal reasoning paths. Extensive experiments on mathematical and coding benchmarks demonstrate that PILOT effectively stabilizes reasoning trajectories, consistently outperforming strong baselines (e.g., +8.9% on MATH500) with negligible inference latency.

</details>


### [6] [Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs](https://arxiv.org/abs/2601.19918)
*Yitong Qiao,Licheng Pan,Yu Mi,Lei Liu,Yue Shen,Fei Sun,Zhixuan Chu*

Main category: cs.CL

TL;DR: 提出LSC（最低跨度置信度）作为零样本幻觉检测指标，仅需单次前向传播和输出概率，通过滑动窗口评估语义连贯跨度的联合似然，在资源受限条件下优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM幻觉检测方法存在不现实的假设：要么需要昂贵的密集采样策略进行一致性检查，要么需要白盒LLM状态，这在常见的API场景中不可用或效率低下。

Method: 提出LSC（最低跨度置信度）指标，通过滑动窗口机制评估语义连贯跨度的联合似然，识别可变长度n-gram中最低边际置信度区域，捕捉与事实不一致性相关的局部不确定性模式。

Result: 在多个SOTA LLM和多样化基准测试上的广泛实验表明，LSC始终优于现有的零样本基线，即使在资源受限条件下也能提供强大的检测性能。

Conclusion: LSC是一种高效、零样本的幻觉检测指标，仅需单次前向传播和输出概率，能够缓解困惑度的稀释效应和最小令牌概率的噪声敏感性，提供更稳健的事实不确定性估计。

Abstract: Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.

</details>


### [7] [FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition](https://arxiv.org/abs/2601.19919)
*Junseok Lee,Nahoon Kim,Sangyong Lee,Chang-Jae Chun*

Main category: cs.CL

TL;DR: 提出自适应自知识蒸馏（ASKD）方法，通过动态减少对教师模型的依赖来提升学生模型的泛化能力，并将Whisper模型蒸馏为更小的FastWhisper模型


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法中，学生模型可能会继承教师模型的缺点，导致泛化能力下降。需要一种方法既能利用教师模型的知识，又能避免其缺点，提升学生模型的泛化能力

Method: 提出自适应自知识蒸馏（ASKD）方法：1）动态减少对教师模型的依赖，提升自训练能力；2）采用自知识蒸馏方法提升学生模型的泛化能力；3）将Whisper模型蒸馏为更小的FastWhisper模型

Result: FastWhisper在词错误率上比教师模型Whisper低1.07%，推理速度比Whisper快5倍，实现了更好的性能和更高的效率

Conclusion: ASKD方法有效解决了传统知识蒸馏中学生模型继承教师模型缺点的问题，通过动态调整依赖关系和自知识蒸馏，显著提升了学生模型的泛化能力和性能，成功实现了模型压缩与性能提升的双重目标

Abstract: Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.

</details>


### [8] [Demystifying Multi-Agent Debate: The Role of Confidence and Diversity](https://arxiv.org/abs/2601.19921)
*Xiaochen Zhu,Caiqi Zhang,Yizhou Chi,Tom Stafford,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: 论文提出两种改进多智能体辩论的方法：多样性感知初始化和置信度调制辩论协议，以解决传统MAD效果不佳的问题，在六个推理QA基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论（MAD）虽然通过测试时扩展来提升LLM性能，但往往效果不如简单的多数投票，且计算成本更高。研究发现，在智能体同质化和均匀信念更新的情况下，辩论只能保持预期正确性而无法可靠改进结果。受人类审议和集体决策研究的启发，作者发现传统MAD缺少两个关键机制：初始观点的多样性和明确校准的置信度沟通。

Method: 提出两种轻量级干预方法：1）多样性感知初始化：选择更多样化的候选答案池，增加辩论开始时正确假设存在的可能性；2）置信度调制辩论协议：智能体表达校准后的置信度，并根据他人的置信度条件化更新自己的观点。

Result: 理论上证明：多样性感知初始化提高了MAD成功的先验概率而不改变底层更新动态；置信度调制更新使辩论能够系统性地向正确假设漂移。在六个推理导向的QA基准上，该方法持续优于传统MAD和多数投票。

Conclusion: 研究将人类审议与基于LLM的辩论联系起来，证明简单、有原则的修改可以显著提升辩论效果。这些改进方法为多智能体协作推理提供了更有效的框架。

Abstract: Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.

</details>


### [9] [HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue](https://arxiv.org/abs/2601.19922)
*Laya Iyer,Kriti Aggarwal,Sanmi Koyejo,Gail Heyman,Desmond C. Ong,Subhabrata Mukherjee*

Main category: cs.CL

TL;DR: HEART框架首次直接比较人类与LLM在多轮情感支持对话中的表现，发现前沿模型在共情和一致性方面接近或超越人类平均水平，但人类在适应性重构、紧张命名和细微语气转换方面仍有优势。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型快速发展，但缺乏明确方法来评估它们在人际情感支持领域的能力如何与人类比较。当前缺乏系统框架来直接比较人类和LLM在相同情感支持对话中的表现。

Method: 引入HEART框架，在多轮情感支持对话中配对人类和模型响应，通过盲审人类评分员和LLM-as-judge评估器进行评估。评估遵循基于人际沟通科学的五个维度：人类对齐、共情响应、调谐、共鸣和任务遵循。

Result: 前沿模型在感知共情和一致性方面接近或超越人类平均水平；人类在适应性重构、紧张命名和细微语气转换方面保持优势，特别是在对抗性对话中；人类和LLM-as-judge在约80%的成对比较中偏好一致，与人类间一致性相当。

Conclusion: HEART将情感支持对话重新定义为独立于一般推理或语言流畅性的能力维度，为理解模型生成支持与人类社交判断的契合与分歧提供了统一实证基础，并揭示了情感对话能力随模型规模扩展的模式。

Abstract: Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.

</details>


### [10] [Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation](https://arxiv.org/abs/2601.19923)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Zelin Cao,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.CL

TL;DR: Table-BiEval：基于自监督评估框架，通过中间表示、内容语义准确性和归一化树编辑距离，量化评估LLM将自然语言转换为结构化格式的能力，发现中型模型在结构效率上可能优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 随着LLM发展为自主代理，需要准确将自然语言转换为结构化格式（用于工具调用）并将复杂表格信息转换为机器可读规范。当前评估方法缺乏有效衡量结构保真度的能力，传统文本指标无法检测代码类输出的语义漂移。

Method: 提出Table-BiEval方法：基于无人工干预的自监督评估框架，利用确定性中间表示，计算内容语义准确性和归一化树编辑距离，将结构与内容解耦。在双重拓扑维度（层次结构和平面表格）上对15个最先进的LLM进行实证评估。

Result: 评估结果显示显著变异性：中型模型在结构效率上可能优于大型模型；深度递归嵌套仍是当前架构的普遍瓶颈。

Conclusion: Table-BiEval提供了一种无需人工干预的量化评估方法，揭示了LLM在结构转换能力上的重要差异，为模型选择和架构改进提供了指导。

Abstract: As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.

</details>


### [11] [OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling](https://arxiv.org/abs/2601.19924)
*Yitian Chen,Cheng Cheng,Yinan Sun,Zi Ling,Dongdong Ge*

Main category: cs.CL

TL;DR: OPT-ENGINE是一个评估LLM在优化建模能力的可扩展基准框架，包含10个经典OR任务，研究发现工具集成推理比纯文本推理更稳健，约束自动制定是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在优化建模方面取得进展，但其在复杂现实任务中的自动制定和问题解决能力边界仍不清楚，需要系统评估框架来理解其能力限制。

Method: 提出OPT-ENGINE基准框架，包含10个经典优化任务（5个线性规划和5个混合整数规划），具有可控可扩展的难度级别，用于评估LLM的推理能力。

Result: 研究发现：1）随着任务复杂度增加，工具集成推理（外部求解器）比纯文本推理表现更稳健；2）约束自动制定是当前LLM的主要性能瓶颈。

Conclusion: 这些发现为开发下一代面向高级优化的LLM提供了实用指导，强调了工具集成和约束制定能力的重要性。

Abstract: Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.

</details>


### [12] [Evaluating Large Language Models for Abstract Evaluation Tasks: An Empirical Study](https://arxiv.org/abs/2601.19925)
*Yinuo Liu,Emre Sezgin,Eric A. Youngstrom*

Main category: cs.CL

TL;DR: LLMs在评估学术摘要时与人类评审员在客观标准上达到中等一致性，但在主观维度表现较弱，适合作为人类评审的补充工具。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在协助科学评审中的潜力，特别是评估LLMs在评审学术摘要时的一致性和可靠性。

Method: 使用ChatGPT-5、Gemini-3-Pro和Claude-Sonnet-4.5评估160个会议摘要，与14名人类评审员使用相同评分标准比较，通过组内相关系数和Bland-Altman图分析一致性。

Result: LLMs之间达到良好到优秀的一致性（ICC: 0.59-0.87）；ChatGPT和Claude在整体质量和客观标准上与人类评审员达到中等一致性（ICC: ~.45-.60）；但在主观维度（影响力、参与度、适用性）一致性较差（ICC: 0.23-0.38）。

Conclusion: LLMs能够批量处理摘要并在客观标准上与人类专家达到中等一致性，适合作为人类评审的补充工具，但人类专业知识在主观维度评估中仍然至关重要。

Abstract: Introduction: Large language models (LLMs) can process requests and generate texts, but their feasibility for assessing complex academic content needs further investigation. To explore LLM's potential in assisting scientific review, this study examined ChatGPT-5, Gemini-3-Pro, and Claude-Sonnet-4.5's consistency and reliability in evaluating abstracts compared to one another and to human reviewers. Methods: 160 abstracts from a local conference were graded by human reviewers and three LLMs using one rubric. Composite score distributions across three LLMs and fourteen reviewers were examined. Inter-rater reliability was calculated using intraclass correlation coefficients (ICCs) for within-AI reliability and AI-human concordance. Bland-Altman plots were examined for visual agreement patterns and systematic bias. Results: LLMs achieved good-to-excellent agreement with each other (ICCs: 0.59-0.87). ChatGPT and Claude reached moderate agreement with human reviewers on overall quality and content-specific criteria, with ICCs ~.45-.60 for composite, impression, clarity, objective, and results. They exhibited fair agreement on subjective dimensions, with ICC ranging from 0.23-0.38 for impact, engagement, and applicability. Gemini showed fair agreement on half criteria and no reliability on impact and applicability. Three LLMs showed acceptable or negligible mean difference (ChatGPT=0.24, Gemini=0.42, Claude=-0.02) from the human mean composite scores. Discussion: LLMs could process abstracts in batches with moderate agreement with human experts on overall quality and objective criteria. With appropriate process architecture, they can apply a rubric consistently across volumes of abstracts exceeding feasibility for a human rater. The weaker performance on subjective dimensions indicates that AI should serve a complementary role in evaluation, while human expertise remains essential.

</details>


### [13] [The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models](https://arxiv.org/abs/2601.19926)
*Nora Graichen,Iria de-Dios-Flores,Gemma Boleda*

Main category: cs.CL

TL;DR: 对337篇评估Transformer语言模型句法能力的论文进行系统综述，发现研究过度集中于英语、BERT模型和简单句法现象，模型在形式句法表现良好但在句法-语义接口现象上表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 系统评估Transformer语言模型在句法能力方面的研究现状，识别当前研究中的偏差和不足，为未来研究提供方向性建议。

Method: 对337篇相关论文进行系统综述，分析1,015个模型结果，涵盖多种句法现象和可解释性方法。

Result: 研究发现：1）当前研究过度集中于英语（单一语言）、BERT（单一模型）和简单句法现象（如词性和一致性）；2）TLM在形式句法现象上表现良好，但在句法-语义接口现象（如约束理论或填充语-空位依存）上表现不稳定且较弱。

Conclusion: 建议未来研究：完整报告数据、更好对齐理论概念和方法、增加机制性方法使用、扩大语言和语言现象的实证范围。

Abstract: We present a systematic review of 337 articles evaluating the syntactic abilities of Transformer-based language models, reporting on 1,015 model results from a range of syntactic phenomena and interpretability methods. Our analysis shows that the state of the art presents a healthy variety of methods and data, but an over-focus on a single language (English), a single model (BERT), and phenomena that are easy to get at (like part of speech and agreement). Results also suggest that TLMs capture these form-oriented phenomena well, but show more variable and weaker performance on phenomena at the syntax-semantics interface, like binding or filler-gap dependencies. We provide recommendations for future work, in particular reporting complete data, better aligning theoretical constructs and methods across studies, increasing the use of mechanistic methods, and broadening the empirical scope regarding languages and linguistic phenomena.

</details>


### [14] [Attribution Techniques for Mitigating Hallucinated Information in RAG Systems: A Survey](https://arxiv.org/abs/2601.19927)
*Yuqing Zhao,Ziyao Liu,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 该论文综述了基于归因的技术在RAG系统中缓解幻觉问题的应用，提出了RAG幻觉的分类法、统一的归因技术流程，并分析了不同技术的优缺点。


<details>
  <summary>Details</summary>
Motivation: LLM生成的回答存在幻觉问题，RAG框架虽然通过外部引用增强回答，但也引入了新的幻觉类型。目前缺乏对归因技术的统一流程、清晰分类和系统比较，需要建立分类法来识别RAG系统的具体失败模式，并通过比较分析帮助实践者根据幻觉类型和应用场景选择合适方案。

Method: 1) 提出RAG系统中幻觉类型的分类法；2) 构建归因技术的统一流程；3) 根据针对的幻觉类型回顾相关技术；4) 讨论优缺点并提供实践指南。

Result: 建立了RAG幻觉的系统分类框架，统一了归因技术的处理流程，分析了不同技术对不同类型幻觉的适用性，为实践者提供了选择指南。

Conclusion: 该研究为RAG系统中归因技术的未来研究和实际应用提供了重要见解，通过系统化的分类和比较框架，帮助更好地理解和缓解LLM在RAG环境中的幻觉问题。

Abstract: Large Language Models (LLMs)-based question answering (QA) systems play a critical role in modern AI, demonstrating strong performance across various tasks. However, LLM-generated responses often suffer from hallucinations, unfaithful statements lacking reliable references. Retrieval-Augmented Generation (RAG) frameworks enhance LLM responses by incorporating external references but also introduce new forms of hallucination due to complex interactions between the retriever and generator. To address these challenges, researchers have explored attribution-based techniques that ensure responses are verifiably supported by retrieved content. Despite progress, a unified pipeline for these techniques, along with a clear taxonomy and systematic comparison of their strengths and weaknesses, remains lacking. A well-defined taxonomy is essential for identifying specific failure modes within RAG systems, while comparative analysis helps practitioners choose appropriate solutions based on hallucination types and application context. This survey investigates how attribution-based techniques are used within RAG systems to mitigate hallucinations and addresses the gap by: (i) outlining a taxonomy of hallucination types in RAG systems, (ii) presenting a unified pipeline for attribution techniques, (iii) reviewing techniques based on the hallucinations they target, and (iv) discussing strengths and weaknesses with practical guidelines. This work offers insights for future research and practical use of attribution techniques in RAG systems.

</details>


### [15] [Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures](https://arxiv.org/abs/2601.19928)
*Yi Hu,Jiaqi Gu,Ruxin Wang,Zijun Yao,Hao Peng,Xiaobao Wu,Jianhui Chen,Muhan Zhang,Liangming Pan*

Main category: cs.CL

TL;DR: 这篇论文是关于大型推理模型（LRMs）机制理解的全面综述，将近期研究发现组织为三个核心维度：训练动态、推理机制和意外行为，旨在弥合黑盒性能与机制透明度之间的差距。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习推动的大型推理模型在性能上取得了显著成就，但理解这些模型内部工作机制已成为同等重要的研究前沿。当前研究需要弥合黑盒性能表现与机制透明度之间的差距。

Method: 采用系统性文献综述方法，将现有关于大型推理模型机制理解的研究组织成三个核心维度：1) 训练动态，2) 推理机制，3) 意外行为。通过综合这些见解，构建机制理解框架。

Result: 论文系统性地整理了大型推理模型机制理解的研究现状，识别了关键发现和模式，为理解这些模型的内部工作机制提供了结构化框架。

Conclusion: 论文提出了未来机制研究路线图，包括应用可解释性需求、改进方法论和统一理论框架等未充分探索的挑战，为大型推理模型的透明度和可靠性研究指明了方向。

Abstract: Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.

</details>


### [16] [Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding](https://arxiv.org/abs/2601.19929)
*David Linus Ostby*

Main category: cs.CL

TL;DR: Stingy Context提出了一种基于分层树的压缩方案，在自动编码任务中实现18:1的LLM上下文压缩比，将239k令牌的代码库压缩到11k令牌，同时保持任务保真度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在处理大规模代码库时的上下文限制问题，减少令牌使用成本，同时避免"迷失在中间"效应，提高自动编码任务的效率和效果。

Method: 采用分层树状压缩方案，使用TREEFRAG分解技术，将源代码库分解为层次结构，实现高效的上下文压缩。

Result: 在12个前沿模型上测试40个真实世界问题，达到94-97%的成功率，显著优于平面压缩方法，同时大幅降低计算成本。

Conclusion: Stingy Context提供了一种高效、低成本的LLM上下文压缩方案，在保持任务保真度的同时显著减少令牌使用，为大规模代码库的自动编码任务提供了实用解决方案。

Abstract: We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.

</details>


### [17] [SDUs DAISY: A Benchmark for Danish Culture](https://arxiv.org/abs/2601.19930)
*Jacob Nielsen,Stine L. Beltoft,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: Daisy是一个基于丹麦文化经典2006的丹麦文化遗产基准数据集，包含741个封闭式问答对，涵盖从公元前1300年到当代的丹麦文化主题


<details>
  <summary>Details</summary>
Motivation: 创建专门针对丹麦文化的基准测试，通过文化遗产主题评估语言模型对丹麦文化的理解，填补现有文化基准中丹麦文化代表性不足的空白

Method: 基于丹麦文化经典2006的文物清单，查询对应的维基百科页面，使用语言模型生成随机问题，通过人工审核和修正确保质量，每个作品包含核心和边缘问题的混合

Result: 构建了包含741个封闭式问答对的最终数据集，涵盖从公元前1300年的考古发现到当代流行音乐、丹麦设计和建筑等广泛主题，涉及130个文化作品

Conclusion: Daisy基准为评估语言模型对丹麦文化遗产的理解提供了专门工具，不仅测试主流知识，还涵盖定义丹麦文化传统的深度基石内容

Abstract: We introduce a new benchmark for Danish culture via cultural heritage, Daisy, based on the curated topics from the Danish Culture Canon 2006. For each artifact in the culture canon, we query the corresponding Wikipedia page and have a language model generate random questions. This yields a sampling strategy within each work, with a mix of central of peripheral questions for each work, not only knowledge of mainstream information, but also in-depth cornerstones defining the heritage of Danish Culture, defined by the Canon committee. Each question-answer pair is humanly approved or corrected in the final dataset consisting of 741 close-ended question answer pairs covering topics, from 1300 BC. archaeological findings, 1700 century poems and musicals pieces to contemporary pop music and Danish design and architecture.

</details>


### [18] [CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity](https://arxiv.org/abs/2601.19931)
*Sebastien Kawada,Dylan Holyoak*

Main category: cs.CL

TL;DR: 提出混合神经符号系统用于叙事故事相似性任务，结合神经自一致性投票与多尺度叙事分析集成作为符号平局决胜器


<details>
  <summary>Details</summary>
Motivation: 解决叙事故事相似性评估中的模糊情况，通过选择性延迟到符号方法来增强神经预测在真正模糊的叙事比较中的表现

Method: 混合神经符号系统：神经部分使用大语言模型进行多轮自一致性投票，设置超多数阈值；符号部分使用多尺度叙事分析集成（词汇重叠、语义嵌入、故事语法结构、事件链对齐、叙事张力曲线）作为平局决胜器

Result: 在开发集上达到81%的准确率，证明选择性延迟到符号方法可以有效增强神经预测在模糊叙事比较中的表现

Conclusion: 混合神经符号方法通过神经自一致性投票与符号平局决胜器的结合，能够有效处理叙事相似性评估中的模糊情况，提升整体性能

Abstract: We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.

</details>


### [19] ["Newspaper Eat" Means "Not Tasty": A Taxonomy and Benchmark for Coded Languages in Real-World Chinese Online Reviews](https://arxiv.org/abs/2601.19932)
*Ruyuan Wan,Changye Li,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: 该论文介绍了CodedLang数据集，包含7,744条中文谷歌地图评论，其中900条带有编码语言的标注，并提出了七类编码策略分类，用于评估语言模型处理编码语言的能力。


<details>
  <summary>Details</summary>
Motivation: 编码语言是人类交流的重要组成部分，指用户有意编码意义，使表面文本与真实意图不同，需要解码才能理解。当前语言模型处理编码语言能力较差，且缺乏真实世界数据集和清晰分类体系限制了研究进展。

Method: 构建了CodedLang数据集，包含7,744条中文谷歌地图评论，其中900条带有编码语言的span级标注。开发了七类编码策略分类法，涵盖语音、拼写和跨语言替换等策略。对语言模型进行了编码语言检测、分类和评论评分预测的基准测试，并进行了编码形式和解码形式的语音分析。

Result: 结果显示，即使是强大的语言模型也难以识别或理解编码语言。由于许多编码表达依赖基于发音的策略，语音分析揭示了编码形式和解码形式之间的关系。研究强调了编码语言是现实世界NLP系统面临的重要且未被充分探索的挑战。

Conclusion: 编码语言是现实世界NLP系统的重要挑战，当前模型处理能力有限。CodedLang数据集和分类体系为研究编码语言提供了基础资源，语音分析揭示了编码策略的特征，为改进模型处理编码语言的能力指明了方向。

Abstract: Coded language is an important part of human communication. It refers to cases where users intentionally encode meaning so that the surface text differs from the intended meaning and must be decoded to be understood. Current language models handle coded language poorly. Progress has been limited by the lack of real-world datasets and clear taxonomies. This paper introduces CodedLang, a dataset of 7,744 Chinese Google Maps reviews, including 900 reviews with span-level annotations of coded language. We developed a seven-class taxonomy that captures common encoding strategies, including phonetic, orthographic, and cross-lingual substitutions. We benchmarked language models on coded language detection, classification, and review rating prediction. Results show that even strong models can fail to identify or understand coded language. Because many coded expressions rely on pronunciation-based strategies, we further conducted a phonetic analysis of coded and decoded forms. Together, our results highlight coded language as an important and underexplored challenge for real-world NLP systems.

</details>


### [20] [Text-to-State Mapping for Non-Resolution Reasoning: The Contradiction-Preservation Principle](https://arxiv.org/abs/2601.19933)
*Kei Saito*

Main category: cs.CL

TL;DR: 本文提出了一种将自然语言文本映射到非解析推理框架中叠加状态的函数φ，实现了语义模糊性的数学表示，并通过大语言模型作为解释生成器进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 非解析推理框架虽然建立了保持语义模糊性的计算结构，但自然语言如何映射到这些数学结构的关键问题尚未解决。需要建立文本到状态空间的算法桥梁，以支持语言模型推理中的架构延迟崩溃。

Method: 引入文本到状态映射函数φ，将语言输入转换为NRR框架中的叠加状态；形式化矛盾保持原则，要求真正模糊的表达在状态表示中保持非零熵；利用现有大语言模型作为解释生成器开发提取协议。

Result: 在涵盖词汇、结构和语用模糊性的68个测试句上进行实证验证，模糊输入的映射平均香农熵H(S)=1.087比特，而基线单解释方法H(S)=0.000。框架成功建立了原始文本与形式状态空间之间的算法桥梁。

Conclusion: 该映射函数φ填补了自然语言与NRR形式状态空间之间的关键空白，实现了语言模型推理中的架构延迟崩溃，为保持语义模糊性的计算提供了可行的数学表示方法。

Abstract: Non-Resolution Reasoning (NRR) provides a formal framework for maintaining semantic ambiguity rather than forcing premature interpretation collapse. While the foundational architecture establishes state spaces and operators for ambiguity-preserving computation, the critical question of how natural language maps to these mathematical structures remains open. This paper introduces the text-to-state mapping function φ that transforms linguistic input into superposition states within the NRR framework. We formalize the Contradiction-Preservation Principle, which requires that genuinely ambiguous expressions maintain non-zero entropy in their state representations, and develop extraction protocols using existing Large Language Models as interpretation generators. Empirical validation across 68 test sentences spanning lexical, structural, and pragmatic ambiguity demonstrates that our mapping achieves mean Shannon entropy H(S) = 1.087 bits for ambiguous inputs while baseline single-interpretation approaches yield H(S) = 0.000. The framework provides the missing algorithmic bridge between raw text and the formal state spaces on which NRR operators act, enabling architectural collapse deferment in language model inference.

</details>


### [21] [Quantifying non deterministic drift in large language models](https://arxiv.org/abs/2601.19934)
*Claire Nicholson*

Main category: cs.CL

TL;DR: 该研究通过重复实验量化了大语言模型在相同提示下的输出变异性（行为漂移），发现即使在温度设为0.0时也存在非确定性，且漂移模式因模型大小、部署方式和提示类型而异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中，即使温度和其他解码参数固定，相同提示也不总是产生相同输出。这种输出变异性（行为漂移）尚未得到系统量化，需要建立基线参考来评估未来的漂移缓解方法。

Method: 对gpt-4o-mini和llama3.1-8b两个公开模型进行重复运行实验，使用五种提示类别，包括精确重复、扰动输入和重用模式，在温度0.0和0.7下测试。通过唯一输出比例、词汇相似度和词数统计来测量漂移。

Result: 结果显示即使在温度0.0时也存在非确定性，漂移模式因模型大小、部署方式和提示类型而不同。词汇相似度等指标能够有效比较不同模型、提示模式和部署类型之间的漂移差异。

Conclusion: 该研究为行为漂移建立了系统化的经验基线，为评估未来的漂移缓解和控制方法提供了参考点。同时指出了词汇度量的局限性，并强调了新兴语义方法的重要性。

Abstract: Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, and reuse modes at temperatures of 0.0 and 0.7. Drift is measured using unique output fractions, lexical similarity, and word count statistics, enabling direct comparison across models, prompting modes, and deployment types. The results show that nondeterminism persists even at temperature 0.0, with distinct variability patterns by model size, deployment, and prompt type. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches. By establishing a systematic empirical baseline in the absence of stabilisation techniques, this study provides a reference point for evaluating future drift mitigation and control methods.

</details>


### [22] [Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents](https://arxiv.org/abs/2601.19935)
*Yiting Shen,Kun Li,Wei Zhou,Songlin Hu*

Main category: cs.CL

TL;DR: Mem2ActBench：一个评估LLM智能体能否主动利用长期记忆执行工具操作的新基准，填补了现有基准仅测试被动记忆检索的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试智能体被动检索孤立事实的能力，但无法评估更关键的主动应用记忆执行任务的能力。需要评估智能体在长期、中断的交互中能否主动利用记忆来选择工具并确定参数。

Method: 1) 构建模拟持久助手使用的数据集，合并异构数据源(ToolACE, BFCL, Oasst1)，通过一致性建模解决冲突；2) 生成2,029个会话，平均12轮用户-助手-工具交互；3) 使用反向生成方法从记忆链中产生400个工具使用任务；4) 人工评估确认91.3%的任务具有强记忆依赖性。

Result: 实验评估了7个记忆框架，发现当前系统在主动利用记忆进行参数确定方面仍然不足，突显了需要更有效的方法来评估和改进任务执行中的记忆应用。

Conclusion: Mem2ActBench填补了评估智能体主动应用长期记忆执行工具操作能力的空白，揭示了当前系统的局限性，为开发更有效的记忆应用方法提供了基准。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.

</details>


### [23] [Benchmarking von ASR-Modellen im deutschen medizinischen Kontext: Eine Leistungsanalyse anhand von Anamnesegesprächen](https://arxiv.org/abs/2601.19945)
*Thomas Schuster,Julius Trögele,Nico Döring,Robin Krüger,Matthieu Hoffmann,Holger Friedrich*

Main category: cs.CL

TL;DR: 该研究评估了29个ASR模型在德语医疗对话中的表现，发现不同模型在医疗术语和方言处理上存在显著性能差异，最佳模型WER可低于3%。


<details>
  <summary>Details</summary>
Motivation: 德语医疗领域的ASR评估基准缺乏，特别是包含方言的情况，而ASR在减轻医务人员文档工作负担方面具有重要潜力。

Method: 创建模拟医患对话数据集，评估29个ASR模型（包括Whisper、Voxtral、Wav2Vec2等开源模型和AssemblyAI、Deepgram等商业API），使用WER、CER、BLEU三种指标，并进行语义分析展望。

Result: 模型间存在显著性能差异：最佳系统WER可低于3%，但其他模型在医疗术语和方言变体上的错误率明显更高。

Conclusion: 德语医疗ASR系统性能差异显著，特别是在处理医疗术语和方言方面，需要进一步改进以支持临床文档自动化。

Abstract: Automatic Speech Recognition (ASR) offers significant potential to reduce the workload of medical personnel, for example, through the automation of documentation tasks. While numerous benchmarks exist for the English language, specific evaluations for the German-speaking medical context are still lacking, particularly regarding the inclusion of dialects. In this article, we present a curated dataset of simulated doctor-patient conversations and evaluate a total of 29 different ASR models. The test field encompasses both open-weights models from the Whisper, Voxtral, and Wav2Vec2 families as well as commercial state-of-the-art APIs (AssemblyAI, Deepgram). For evaluation, we utilize three different metrics (WER, CER, BLEU) and provide an outlook on qualitative semantic analysis. The results demonstrate significant performance differences between the models: while the best systems already achieve very good Word Error Rates (WER) of partly below 3%, the error rates of other models, especially concerning medical terminology or dialect-influenced variations, are considerably higher.

</details>


### [24] [On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text](https://arxiv.org/abs/2601.20006)
*Michał Gromadzki,Anna Wróblewska,Agnieszka Kaliska*

Main category: cs.CL

TL;DR: 该论文提出了一种基于大规模语料库和新型训练策略的AI生成文本检测方法，通过构建人类写作和AI生成文本的大规模语料库，开发了多种检测模型，并在21个大型语言模型的基准测试中取得了高达99.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，生成的文本越来越接近人类写作，给教育、出版和数字安全领域的真实性验证带来了挑战。检测AI生成文本已成为关键的技术和伦理问题。

Method: 构建了10亿token的人类写作语料库和19亿token的AI生成文本语料库，开发了多种检测模型，并提出了两种新的训练范式：Per LLM和Per LLM family微调。

Result: 在覆盖21个大型语言模型的1亿token基准测试中，最佳微调检测器达到了高达99.6%的token级别准确率，显著优于现有的开源基线方法。

Conclusion: 该研究通过大规模语料库和创新的训练策略，为AI生成文本检测提供了有效的解决方案，在准确率方面取得了显著突破，为解决AI生成文本的验证挑战提供了重要技术支持。

Abstract: The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\%$ token-level accuracy, substantially outperforming existing open-source baselines.

</details>


### [25] [LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?](https://arxiv.org/abs/2601.20009)
*J. Ben Tamo,Daniel Carlander-Reuterfelt,Jonathan Rubin,Dezhi Hong,Mingxian Wang,Oleg Poliannikov*

Main category: cs.CL

TL;DR: 论文提出通过选择性微调大语言模型最后几层来解决多语言控制问题，仅需微调3-5%参数即可达到98%语言一致性，效果接近全参数微调但计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 尽管经过多语言预训练，大语言模型在非英语任务中仍存在语言控制问题，即无法按照预期语言生成回答。论文识别出两种关键失败模式：多语言转移瓶颈（正确语言但任务回答错误）和语言一致性瓶颈（正确任务回答但语言错误）。

Method: 1. 设计四场景评估协议，涵盖MMLU、MGSM和XQuAD基准测试；2. 扩展logit lens分析，逐层追踪语言概率并计算隐藏状态的跨语言语义相似性；3. 基于分析发现的三阶段内部结构（早期层对齐语义、中间层任务推理、晚期层语言生成），提出仅微调负责语言控制的最后几层的选择性微调方法。

Result: 在Qwen-3-32B和Bloom-7.1B模型上，仅微调3-5%参数即可在六种语言上达到超过98%的语言一致性，且不牺牲任务准确性。该方法效果与全参数微调几乎相同（例如在所有提示场景下都达到98%以上语言一致性），但计算资源消耗大幅减少。

Conclusion: 论文首次利用语言控制的层定位进行高效多语言适应，通过选择性微调最后几层，以极低的计算成本解决了大语言模型的多语言控制问题，为多语言模型的高效微调提供了新思路。

Abstract: Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.

</details>


### [26] [Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method](https://arxiv.org/abs/2601.20026)
*Pragatheeswaran Vipulanandan,Kamal Premaratne,Dilip Sarkar*

Main category: cs.CL

TL;DR: 该论文提出了一种基于量子张量网络的不确定性量化框架，用于检测大语言模型的幻觉，通过语义等价聚类和熵最大化策略来识别不可靠输出，并在多个数据集和模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然生成能力强，但容易产生幻觉（confabulations）——流畅但不可靠的输出，即使在相同提示下也会任意变化。现有方法缺乏对不确定性的系统量化，特别是在不同生成长度和量化级别下的鲁棒性考虑不足。

Method: 1. 基于量子张量网络的管道，量化token序列概率的偶然不确定性（aleatoric uncertainty）；2. 基于语义等价性对LLM生成进行聚类；3. 引入熵最大化策略，优先选择高确定性、语义连贯的输出；4. 突出显示LLM决策可能不可靠的熵区域。

Result: 在TriviaQA、NQ、SVAMP、SQuAD等数据集上进行了116个实验，涵盖Mistral-7B、Falcon-rw-1b、LLaMA系列等多种架构。结果显示，在AUROC和AURAC指标上持续优于现有最先进基线，且在不同生成长度和量化级别下保持鲁棒性。

Conclusion: 提出的量子物理启发的框架为幻觉检测提供了原则性和可解释的方案，能够识别LLM输出的不可靠区域，为需要人工监督的场景提供实用指导，即使在资源受限的部署中也能保持可靠性。

Abstract: Large language models (LLMs) exhibit strong generative capabilities but remain vulnerable to confabulations, fluent yet unreliable outputs that vary arbitrarily even under identical prompts. Leveraging a quantum tensor network based pipeline, we propose a quantum physics inspired uncertainty quantification framework that accounts for aleatoric uncertainty in token sequence probability for semantic equivalence based clustering of LLM generations. This offers a principled and interpretable scheme for hallucination detection. We further introduce an entropy maximization strategy that prioritizes high certainty, semantically coherent outputs and highlights entropy regions where LLM decisions are likely to be unreliable, offering practical guidelines for when human oversight is warranted. We evaluate the robustness of our scheme under different generation lengths and quantization levels, dimensions overlooked in prior studies, demonstrating that our approach remains reliable even in resource constrained deployments. A total of 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD across multiple architectures including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLaMA-2-13b, and LLaMA-2-7b show consistent improvements in AUROC and AURAC over state of the art baselines.

</details>


### [27] [TAIGR: Towards Modeling Influencer Content on Social Media via Structured, Pragmatic Inference](https://arxiv.org/abs/2601.20032)
*Nishanth Sridhar Nakshatri,Eylon Caplan,Rajkumar Pujari,Dan Goldwasser*

Main category: cs.CL

TL;DR: TAIGR框架通过三阶段分析健康影响者话语：识别核心建议、构建论证图、概率推理验证，解决传统基于声明的验证方法无法捕捉话语语用意义的问题。


<details>
  <summary>Details</summary>
Motivation: 健康影响者通过对话叙事和修辞策略塑造公众信念，而非明确的事实声明。传统的基于声明的验证方法难以捕捉影响者话语的语用意义，需要新的分析框架。

Method: 提出TAIGR框架：1) 识别核心建议（takeaway）；2) 构建论证图捕捉影响者对建议的论证；3) 使用因子图概率推理验证建议的有效性。

Result: 在健康影响者视频转录本的验证任务中评估TAIGR，表明准确验证需要建模话语的语用和论证结构，而非将转录本视为平面声明集合。

Conclusion: TAIGR框架能够有效分析影响者话语的语用和论证结构，为健康影响者内容验证提供了更准确的方法，超越了传统的基于声明的验证方法。

Abstract: Health influencers play a growing role in shaping public beliefs, yet their content is often conveyed through conversational narratives and rhetorical strategies rather than explicit factual claims. As a result, claim-centric verification methods struggle to capture the pragmatic meaning of influencer discourse. In this paper, we propose TAIGR (Takeaway Argumentation Inference with Grounded References), a structured framework designed to analyze influencer discourse, which operates in three stages: (1) identifying the core influencer recommendation--takeaway; (2) constructing an argumentation graph that captures influencer justification for the takeaway; (3) performing factor graph-based probabilistic inference to validate the takeaway. We evaluate TAIGR on a content validation task over influencer video transcripts on health, showing that accurate validation requires modeling the discourse's pragmatic and argumentative structure rather than treating transcripts as flat collections of claims.

</details>


### [28] [VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.20055)
*Vikash Singh,Darion Cassel,Nathaniel Weir,Nick Feng,Sam Bayless*

Main category: cs.CL

TL;DR: VERGE是一个结合LLM与SMT求解器的神经符号框架，通过形式化验证和迭代优化确保LLM输出的逻辑正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具有语法流畅性，但在高风险领域中确保其逻辑正确性仍然是一个根本性挑战。需要一种方法来验证和保证LLM输出的逻辑一致性。

Method: 1. 将LLM输出分解为原子声明并自动形式化为一阶逻辑；2. 使用多模型共识通过形式语义等价检查确保逻辑对齐；3. 语义路由将不同声明类型导向适当验证策略；4. 使用最小修正子集进行精确逻辑错误定位；5. 迭代优化直到满足接受标准。

Result: 使用GPT-OSS-120B模型，VERGE在多个推理基准测试中相比单次通过方法平均性能提升18.7%。

Conclusion: 该混合方法在可能的情况下提供形式化保证，在其他情况下提供共识验证，推进了可信AI的发展。

Abstract: Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.

</details>


### [29] [Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects](https://arxiv.org/abs/2601.20102)
*Amirhossein Haji Mohammad Rezaei,Zahra Shakeri*

Main category: cs.CL

TL;DR: 论文提出了一个反事实基准，通过插入文化相关信息来测试医疗语言模型在非决定性文化信息影响下的诊断稳定性，发现文化线索会显著降低模型准确性，特别是标识符和上下文同时出现时。


<details>
  <summary>Details</summary>
Motivation: 构建可持续和公平的医疗保健需要医疗语言模型在面对非决定性文化信息时不会改变临床正确的诊断。当前需要评估模型在文化相关上下文中的诊断稳定性。

Method: 创建反事实基准，将150个MedQA测试项目扩展为1650个变体，插入三种文化群体（加拿大原住民、中东穆斯林、东南亚）的标识符标记、上下文线索或其组合，加上长度匹配的中性对照。使用临床医生验证所有变体的正确答案保持不变。评估多个模型在选项提示和简短解释提示下的表现。

Result: 文化线索显著影响模型准确性（Cochran's Q, p<10^-14），当标识符和上下文同时出现时准确性下降最大（选项提示下下降3-7个百分点）。超过一半基于文化的解释最终得出错误答案，表明文化参照推理与诊断失败相关。

Conclusion: 医疗语言模型容易受到非决定性文化信息的影响，导致诊断准确性下降。需要开发方法来减轻文化诱导的诊断错误，以支持公平的医疗保健。作者发布了提示和增强数据以支持评估和缓解。

Abstract: Engineering sustainable and equitable healthcare requires medical language models that do not change clinically correct diagnoses when presented with non-decisive cultural information. We introduce a counterfactual benchmark that expands 150 MedQA test items into 1650 variants by inserting culture-related (i) identifier tokens, (ii) contextual cues, or (iii) their combination for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a length-matched neutral control, where a clinician verified that the gold answer remains invariant in all variants. We evaluate GPT-5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B) under option-only and brief-explanation prompting. Across models, cultural cues significantly affect accuracy (Cochran's Q, $p<10^-14$), with the largest degradation when identifier and context co-occur (up to 3-7 percentage points under option-only prompting), while neutral edits produce smaller, non-systematic changes. A human-validated rubric ($κ=0.76$) applied via an LLM-as-judge shows that more than half of culturally grounded explanations end in an incorrect answer, linking culture-referential reasoning to diagnostic failure. We release prompts and augmentations to support evaluation and mitigation of culturally induced diagnostic errors.

</details>


### [30] [FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language](https://arxiv.org/abs/2601.20105)
*Faezeh Hosseini,Mohammadali Yousefzadeh,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 论文提出FFEHallu基准，首次全面评估大语言模型在固定比喻表达（如习语、谚语）上的幻觉问题，重点关注波斯语，揭示了LLMs在比喻语言处理和文化基础方面的系统性弱点。


<details>
  <summary>Details</summary>
Motivation: 固定比喻表达（FFEs）如习语和谚语对大型语言模型构成持续挑战，因为它们具有文化基础、非组合性和固定性，容易导致比喻幻觉。目前缺乏针对这一问题的全面评估基准，特别是对于波斯语等代表性不足的语言。

Method: 提出FFEHallu基准，包含600个精心策划的实例，涵盖三个互补任务：1）从含义生成FFE；2）检测四个受控构建类别中的伪造FFE；3）从英语到波斯语的FFE翻译。评估了六个最先进的多语言LLM。

Result: 评估显示LLMs在比喻能力和文化基础方面存在系统性弱点。虽然GPT4.1在拒绝伪造FFE和检索真实表达方面表现相对较强，但大多数模型难以可靠区分真实表达与高质量伪造，在跨语言翻译中经常产生幻觉。

Conclusion: 研究揭示了当前LLMs在处理比喻语言方面的重大差距，强调需要有针对性的基准来评估和缓解比喻幻觉问题，特别是在文化丰富的语言中。

Abstract: Figurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.

</details>


### [31] [Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://arxiv.org/abs/2601.20126)
*Abha Jha,Akanksha Mahajan,Ashwath Vaithinathan Aravindan,Praveen Saravanan,Sai Sailaja Policharla,Sonal Chaturbhuj Gehlot*

Main category: cs.CL

TL;DR: 本文提出RLVR（可验证奖励的强化学习）训练范式，通过奖励"我不知道"的弃权回答来减少LLM的幻觉，在多项选择题任务上证明中等弃权奖励能有效减少错误回答而不严重损害准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生幻觉或不可验证的内容，这削弱了其在事实领域中的可靠性。需要一种训练范式来促进智力谦逊，即模型在不确定时能够诚实地表示"我不知道"。

Method: 使用可验证奖励的强化学习（RLVR），采用三元奖励结构（-1，r_abs，1），其中r_abs是弃权奖励。在MedMCQA和Hendrycks Math基准上对Granite-3.3-2B-Instruct和Qwen-3-4B-Instruct进行微调和评估。研究将RLVR与监督微调策略结合，在强化学习前教授弃权行为。

Result: 中等弃权奖励（r_abs ≈ -0.25到0.3）能持续减少错误回答，而不会严重降低多项选择题的准确性。较大模型对弃权激励表现出更强的鲁棒性。在开放性问题回答中，由于探索不足存在局限性，但可通过监督弃权训练部分缓解。

Conclusion: 可验证奖励设计是减轻语言模型幻觉的可行且灵活的方法。中等弃权奖励能在减少错误回答和保持准确性之间取得良好平衡，为促进LLM的智力谦逊提供了实用框架。

Abstract: Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention ("I don't know") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.

</details>


### [32] [BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification](https://arxiv.org/abs/2601.20129)
*Akif Islam,Sujan Kumar Roy,Md. Ekramul Hamid*

Main category: cs.CL

TL;DR: 本文介绍了BengaliSent140，一个大规模孟加拉语二元情感数据集，通过整合7个现有数据集构建而成，包含139,792个文本样本，用于训练深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语情感分析研究受到大规模多样化标注数据集稀缺的限制。现有数据集规模小或局限于单一领域（如社交媒体评论），不足以训练需要大量异构数据的现代深度学习模型。

Method: 通过整合7个现有孟加拉语文本数据集构建统一语料库，将异质标注方案系统性地统一为二元情感分类（Not Hate=0, Hate=1）。

Result: 构建了包含139,792个独特文本样本的数据集，其中68,548个仇恨实例和71,244个非仇恨实例，类别分布相对平衡。数据集提供更广泛的语言和上下文覆盖范围。

Conclusion: BengaliSent140为训练和基准测试深度学习模型提供了坚实基础，通过整合多源多领域数据，比现有孟加拉语情感数据集具有更广泛覆盖。数据集已公开可用并报告了基线实验结果。

Abstract: Sentiment analysis for the Bengali language has attracted increasing research interest in recent years. However, progress remains constrained by the scarcity of large-scale and diverse annotated datasets. Although several Bengali sentiment and hate speech datasets are publicly available, most are limited in size or confined to a single domain, such as social media comments. Consequently, these resources are often insufficient for training modern deep learning based models, which require large volumes of heterogeneous data to learn robust and generalizable representations. In this work, we introduce BengaliSent140, a large-scale Bengali binary sentiment dataset constructed by consolidating seven existing Bengali text datasets into a unified corpus. To ensure consistency across sources, heterogeneous annotation schemes are systematically harmonized into a binary sentiment formulation with two classes: Not Hate (0) and Hate (1). The resulting dataset comprises 139,792 unique text samples, including 68,548 hate and 71,244 not-hate instances, yielding a relatively balanced class distribution. By integrating data from multiple sources and domains, BengaliSent140 offers broader linguistic and contextual coverage than existing Bengali sentiment datasets and provides a strong foundation for training and benchmarking deep learning models. Baseline experimental results are also reported to demonstrate the practical usability of the dataset. The dataset is publicly available at https://www.kaggle.com/datasets/akifislam/bengalisent140/

</details>


### [33] [Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR](https://arxiv.org/abs/2601.20142)
*Zilai Wang,Natarajan Balaji Shankar,Kaiyuan Zhang,Zihan Wang,Abeer Alwan*

Main category: cs.CL

TL;DR: 本文提出使用delta SSL嵌入（微调模型与预训练模型嵌入的差异）来改进儿童语音识别，通过融合不同模型的delta嵌入实现了显著的WER降低。


<details>
  <summary>Details</summary>
Motivation: 儿童自动语音识别面临数据有限和预训练领域不匹配的挑战。微调SSL模型会导致表示空间偏移，作者假设delta SSL嵌入（微调模型与预训练模型嵌入的差异）编码了任务特定信息，可以补充其他SSL模型的微调特征。

Method: 提出使用delta SSL嵌入（定义为微调模型嵌入与其预训练对应模型嵌入之间的差异），并评估了在MyST儿童语料库上使用不同模型的多种融合策略。

Result: 实验结果显示：1) HuBERT的delta嵌入融合使WER相对降低10%；2) W2V2的delta嵌入融合使WER相对降低4.4%；3) 融合WavLM与delta W2V2嵌入实现了9.64%的WER，在MyST语料库上创下了SSL模型的新SOTA。

Conclusion: delta嵌入在儿童ASR中具有显著效果，特征融合是推进儿童语音识别的一个有前景的方向。

Abstract: Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.

</details>


### [34] [Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents](https://arxiv.org/abs/2601.20144)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Jiri Gesi,Xianfeng Tang,Chen Luo,Yisi Sang,Hanqing Lu,Manling Li,Dakuo Wang*

Main category: cs.CL

TL;DR: Trajectory2Task：用于研究工具调用代理在模糊意图、变化意图和不可行意图三种现实用户场景下的可验证数据生成管道


<details>
  <summary>Details</summary>
Motivation: 当前工具调用代理研究主要关注理想化设置，而现实应用中用户请求往往存在模糊性、随时间变化或由于政策约束不可行等问题，缺乏覆盖这些复杂交互模式的训练和评估数据

Method: 提出Trajectory2Task管道，首先进行多轮探索生成有效的工具调用轨迹，然后将这些轨迹转换为用户任务并进行受控的意图适配，生成可验证的任务支持闭环评估和训练

Result: 在生成的复杂用户场景任务上对7个SOTA LLM进行基准测试，观察到频繁失败；使用任务执行中获得的成功轨迹微调轻量级LLM，在所有三种条件下都获得一致改进，并在未见过的工具使用领域表现出更好的泛化能力

Conclusion: Trajectory2Task能够生成可验证的复杂用户场景任务数据，用于评估和训练工具调用代理；微调轻量级LLM可以显著提升在模糊、变化和不可行意图场景下的工具调用能力，并增强泛化性能

Abstract: Tool-calling agents are increasingly deployed in real-world customer-facing workflows. Yet most studies on tool-calling agents focus on idealized settings with general, fixed, and well-specified tasks. In real-world applications, user requests are often (1) ambiguous, (2) changing over time, or (3) infeasible due to policy constraints, and training and evaluation data that cover these diverse, complex interaction patterns remain under-represented. To bridge the gap, we present Trajectory2Task, a verifiable data generation pipeline for studying tool use at scale under three realistic user scenarios: ambiguous intent, changing intent, and infeasible intents. The pipeline first conducts multi-turn exploration to produce valid tool-call trajectories. It then converts these trajectories into user-facing tasks with controlled intent adaptations. This process yields verifiable task that support closed-loop evaluation and training. We benchmark seven state-of-the-art LLMs on the generated complex user scenario tasks and observe frequent failures. Finally, using successful trajectories obtained from task rollouts, we fine-tune lightweight LLMs and find consistent improvements across all three conditions, along with better generalization to unseen tool-use domains, indicating stronger general tool-calling ability.

</details>


### [35] [Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction](https://arxiv.org/abs/2601.20162)
*Shuoxin Wang,Chang Liu,Gowen Loo,Lifan Zheng,Kaiwen Wei,Xinyi Zeng,Jingyuan Zhang,Yu Tian*

Main category: cs.CL

TL;DR: Me-Agent是一个可学习、可记忆的个性化移动代理，通过两级用户习惯学习解决LLM代理忽视个性化需求的问题，在个性化基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的移动代理虽然性能显著提升，但通常只遵循明确的用户指令而忽视个性化需求，存在三个主要限制：无法解释模糊指令、缺乏从用户交互历史中学习、无法处理个性化指令。

Method: 提出Me-Agent，采用两级用户习惯学习方法：1) 提示层面：设计用户偏好学习策略，通过个人奖励模型增强个性化性能；2) 记忆层面：设计分层偏好记忆，在不同层级存储用户的长期记忆和应用特定记忆。

Result: 在提出的新基准User FingerTip（包含大量日常生活模糊指令）和通用基准上的广泛实验表明，Me-Agent在个性化方面达到最先进性能，同时保持竞争力的指令执行性能。

Conclusion: Me-Agent通过创新的用户习惯学习机制有效解决了LLM移动代理的个性化不足问题，在保持指令执行能力的同时显著提升了代理的个性化适应能力。

Abstract: Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.

</details>


### [36] [Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling](https://arxiv.org/abs/2601.20185)
*Husein Zolkepli*

Main category: cs.CL

TL;DR: 通过增加池化和扩大解码器跳跃步长，将X-Codec-2.0的潜在速率从50Hz降至25Hz，输出采样率从16kHz提升至24kHz，在保持核心架构不变的情况下提高了效率和感知质量。


<details>
  <summary>Details</summary>
Motivation: X-Codec-2.0在神经音频压缩和多语言语音建模中表现出色，但其50Hz潜在速率和16kHz采样率限制了时间效率和音频保真度，需要改进。

Method: 引入额外的池化操作并增加解码器跳跃步长，降低潜在速率至25Hz，同时提高输出采样率至24kHz，核心架构保持不变。

Result: 在多语言Common Voice 17测试集上，相比原始X-Codec-2.0基线，UTMOSv2评分提高了0.29 MOS，在25Hz运行的所有编解码器中达到最佳性能。

Conclusion: 通过简单的架构修改，显著提升了X-Codec-2.0的效率和感知质量，为神经音频压缩提供了更优的配置方案。

Abstract: X-Codec-2.0 has shown strong performance in neural audio compression and multilingual speech modeling, operating at a 50 Hz latent rate and a 16 kHz sampling rate using frozen HuBERT features. While effective, this configuration limits temporal efficiency and audio fidelity. In this work, we explore a simple and effective modification by introducing additional pooling and increasing the decoder hop size. This reduces the latent rate from 50 Hz to 25 Hz and simultaneously raises the output sampling rate from 16 kHz to 24 kHz, improving efficiency and perceptual quality without altering the core architecture. Evaluated on the multilingual Common Voice 17 test set, the proposed configuration achieves a 0.29 MOS improvement over the original X-Codec-2.0 baseline based on UTMOSv2, and attains the best reported performance among all codecs operating at 25 Hz. The source code, checkpoints, and generation comparisons are released at \href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}.

</details>


### [37] [Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems](https://arxiv.org/abs/2601.20230)
*Haoyuan Yu,Yuxuan Chen,Minjie Cai*

Main category: cs.CL

TL;DR: 提出基于最小对话单元分解的半级联全双工对话框架，围绕多模态大语言模型构建，实现免训练即插即用，在HumDial数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 全双工语音交互对于自然的人机交互至关重要，需要解决复杂对话的实时处理和流畅转换问题

Method: 将复杂对话分解为最小对话单元，每个单元独立处理并预测何时转换到下一单元；基于多模态大语言模型构建半级联全双工系统，辅以语音活动检测和文本转语音等模块

Result: 在HumDial数据集上验证了框架有效性，在Human-like Spoken Dialogue Systems Challenge（Track 2: Full-Duplex Interaction）测试集上排名第二

Conclusion: 提出的免训练即插即用框架能够有效实现全双工语音交互，为自然的人机对话系统提供了实用解决方案

Abstract: Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal conversational units, enabling the system to process each unit independently and predict when to transit to the next. This framework is instantiated as a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) synthesis. The resulting system operates in a train-free, plug-and-play manner. Experiments on the HumDial dataset demonstrate the effectiveness of our framework, which ranks second among all teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). Code is available at the GitHub repository https://github.com/yu-haoyuan/fd-badcat.

</details>


### [38] [Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy](https://arxiv.org/abs/2601.20253)
*Si Chen,Le Huy Khiem,Annalisa Szymanski,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 提出了一个从专家指南自动生成基准测试的框架，用于评估LLM在实践领域的开放式问答能力，发现LLM在高阶推理上表现相对较好但在基础记忆上更容易失败


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要依赖已有的人类考试数据集，但在实践性领域（如教学、营养学、护理）中，知识是程序性的且基于专业判断，这类数据集往往不可用。需要一种方法能够评估模型在真实世界情境中的推理能力。

Method: 开发了一个基于布鲁姆分类法的自动化基准测试生成框架：1) 将专家实践转化为基于违规的场景；2) 扩展到四个认知层次（记忆、理解、应用、分析）；3) 生成自动评分的多项选择题和多轮对话；4) 应用于教学、营养学和护理三个实践领域。

Result: 发现LLM与人类推理的差异：LLM有时在高级推理（分析）上表现相对较好，但在低级项目（记忆）上失败更频繁。生成了大规模、心理测量学上合理的基准测试，揭示了这些非直观的模型行为。

Conclusion: 该框架能够实现确定性、可重复和可扩展的评估，使研究人员能够评估LLM在真实世界实践领域中的情境化推理能力，为实践性领域的AI评估提供了新方法。

Abstract: Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.

</details>


### [39] [SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility](https://arxiv.org/abs/2601.20256)
*Xuanyu Su,Diana Inkpen,Nathalie Japkowicz*

Main category: cs.CL

TL;DR: 提出SoftHateBench基准测试，用于评估模型检测软性仇恨言论的能力，发现现有系统对推理驱动的隐蔽仇恨识别效果不佳


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体内容审核系统主要针对表面毒性线索优化，无法有效检测软性仇恨言论（表面合理但通过框架和价值论证引导受众指责或排斥目标群体），现有基准测试无法系统衡量这一差距

Method: 整合论证主题模型(AMT)和关联理论(RT)的统一框架：AMT提供重写显性仇恨立场为表面中立讨论的论证结构，RT指导生成保持AMT链逻辑连贯性，构建包含7个社会文化领域、28个目标群体、4745个实例的SoftHateBench基准

Result: 评估编码器检测器、通用LLM和安全模型显示，从硬性到软性仇恨层级的检测性能一致下降：能检测显性敌意的系统在相同立场通过微妙推理语言表达时经常失败

Conclusion: 需要开发能理解推理驱动敌意的新方法，SoftHateBench为评估和改进软性仇恨言论检测提供了系统基准

Abstract: Online hate on social media ranges from overt slurs and threats (\emph{hard hate speech}) to \emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \textbf{\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \emph{Argumentum Model of Topics} (AMT) and \emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \textbf{7} sociocultural domains and \textbf{28} target groups, comprising \textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \textcolor{red}{\textbf{Disclaimer.} Contains offensive examples used solely for research.}

</details>


### [40] [RusLICA: A Russian-Language Platform for Automated Linguistic Inquiry and Category Analysis](https://arxiv.org/abs/2601.20275)
*Elina Sigdel,Anastasia Panfilova*

Main category: cs.CL

TL;DR: 开发了俄语版的LIWC心理语言学分析工具，包含96个类别，整合了语法、形态、词汇特征及语言模型预测结果，专门为俄语构建词典而非简单翻译


<details>
  <summary>Details</summary>
Motivation: 现有LIWC工具主要针对英语设计，虽然已翻译成多种语言，但需要专门针对俄语的语法和文化特性进行适配，以更准确地分析俄语文本的心理语言学特征

Method: 1) 基于多个词典资源、语义词典和语料库专门为俄语构建词典；2) 将词元映射到42个心理语言学类别；3) 整合句法、形态、词汇、统计特征及预训练语言模型的预测结果；4) 作为RusLICA网络服务的一部分实现分析器

Result: 开发了包含96个类别的俄语LIWC分析框架，能够考虑俄语语法和文化特性进行文本分析，并已集成到RusLICA网络服务中

Conclusion: 成功开发了专门针对俄语的心理语言学分析工具，通过专门构建词典而非简单翻译现有词库，更好地适应了俄语的语法和文化特性，为俄语文本分析提供了更准确的方法

Abstract: Defining psycholinguistic characteristics in written texts is a task gaining increasing attention from researchers. One of the most widely used tools in the current field is Linguistic Inquiry and Word Count (LIWC) that originally was developed to analyze English texts and translated into multiple languages. Our approach offers the adaptation of LIWC methodology for the Russian language, considering its grammatical and cultural specificities. The suggested approach comprises 96 categories, integrating syntactic, morphological, lexical, general statistical features, and results of predictions obtained using pre-trained language models (LMs) for text analysis. Rather than applying direct translation to existing thesauri, we built the dictionary specifically for the Russian language based on the content from several lexicographic resources, semantic dictionaries and corpora. The paper describes the process of mapping lemmas to 42 psycholinguistic categories and the implementation of the analyzer as part of RusLICA web service.

</details>


### [41] [Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale](https://arxiv.org/abs/2601.20276)
*Tianwei Lin,Zuyi Zhou,Xinda Zhao,Chenke Wang,Xiaohong Li,Yu Chen,Chuanrui Hu,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 论文提出了EverMemBench-S（EMB-S）对抗性基准测试，用于评估长上下文LLM在语义干扰下的证据访问能力，发现语义辨别而非上下文长度是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有Needle-in-a-Haystack（NIAH）评估主要测量良性跨度定位，但针是近乎唯一的，干草堆大部分无关。需要更真实的对抗性基准来评估长上下文LLM在语义干扰下的证据访问能力。

Method: 构建了326M令牌的MemoryBank，包含经过碰撞测试的近负样本和黄金证据集。采用解耦诊断协议，分别报告证据访问（文档ID定位）和端到端QA质量。在从64K到326M令牌的参考语料阶梯上评估。

Result: 在语义干扰下，即使饱和良性NIAH的系统在证据访问方面也急剧下降。语义辨别而非上下文长度是长上下文记忆扩展的主要瓶颈。

Conclusion: 需要超越简单跨度定位的评估方法，语义辨别能力是长上下文LLM在实际大规模环境中有效运作的关键限制因素。

Abstract: Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.

</details>


### [42] [MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting](https://arxiv.org/abs/2601.20300)
*Jing Xu,Minglin Wu,Xueyuan Chen,Xixin Wu,Helen Meng*

Main category: cs.CL

TL;DR: 提出MiLorE-SSL框架，结合LoRA和软混合专家机制，用于高效的多语言持续学习，解决灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现有的多语言自监督学习模型局限于预训练时见过的语言，重新训练新语言计算成本高，而顺序训练又容易导致灾难性遗忘

Method: 结合LoRA模块和软混合专家机制，LoRA提供高效低秩适配，软MoE促进跨语言专家共享，减少跨语言干扰，并引入有限回放数据缓解遗忘

Result: 在ML-SUPERB基准测试中，MiLorE-SSL在新语言上表现强劲，同时提升现有语言能力，仅需2.14%可训练参数

Conclusion: MiLorE-SSL为多语言自监督学习的持续学习提供了轻量高效的解决方案，平衡了新语言学习和旧语言保留

Abstract: Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.

</details>


### [43] [SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger](https://arxiv.org/abs/2601.20312)
*Kaiyuan Chen,Guangmin Zheng,Jin Wang,Xiaobing Zhou,Xuejie Zhang*

Main category: cs.CL

TL;DR: 提出SAPO方法，通过自适应过程监督信号主动缩小推理器-验证器差距，提升小语言模型在数学和代码任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有自我进化方法忽视细粒度推理步骤的影响，导致推理器-验证器差距；蒙特卡洛过程监督计算效率低，加剧了缩小这一差距的难度；受错误相关负波（ERN）启发，推理器能够在错误决策后定位错误并指导快速调整

Method: 提出自我适应过程优化（SAPO）方法，自适应且高效地引入过程监督信号，通过主动最小化推理器-验证器差距，而非依赖低效的蒙特卡洛估计

Result: 在数学和代码两种挑战性任务类型上，SAPO方法优于大多数现有自我进化方法；为研究SAPO对验证器性能的影响，引入了数学和编码任务的过程奖励模型新基准

Conclusion: SAPO方法通过自适应过程监督有效缩小推理器-验证器差距，显著提升小语言模型在复杂任务上的性能，并为过程奖励模型评估提供了新基准

Abstract: Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.

</details>


### [44] [Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning](https://arxiv.org/abs/2601.20326)
*Zeyu Xing,Xing Li,Hui-Ling Zhen,Mingxuan Yuan,Sinno Jialin Pan*

Main category: cs.CL

TL;DR: KV缓存可作为轻量级表征，无需重新计算或存储完整隐藏状态，支持嵌入链和快慢思维切换应用


<details>
  <summary>Details</summary>
Motivation: KV缓存通常仅用于加速自回归解码，但其编码的上下文信息可免费重用于下游任务，避免重新计算或存储完整隐藏状态

Method: 将KV缓存视为轻量级表征，提出两种应用：嵌入链（Chain-of-Embedding）和快慢思维切换（Fast/Slow Thinking Switching）

Result: KV派生表征在Llama-3.1-8B-Instruct和Qwen2-7B-Instruct上实现竞争性或更优性能；在Qwen3-8B和DeepSeek-R1-Distil-Qwen-14B上减少高达5.7倍的token生成，精度损失最小

Conclusion: KV缓存可作为免费有效的采样和推理基底，为LLM推理中的表征重用开辟新方向

Abstract: KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.

</details>


### [45] [CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria](https://arxiv.org/abs/2601.20327)
*Xinyu Hu,Yancheng He,Weixun Wang,Tao Feng,Li Lin,Jiashun Liu,Wenbo Su,Bo Zheng,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出CE-RM-4B，一个基于点式评估的生成式奖励模型，通过两阶段rollout方法和统一查询标准训练，在少量高质量数据上实现优越性能，有效改进下游RL实践。


<details>
  <summary>Details</summary>
Motivation: 传统自动评估方法在开放式自然语言生成中面临挑战，LLM-as-a-Judge范式虽在基准测试表现良好，但在实际RL实践中存在显著差距，主要由于现有研究过度依赖成对评估和评估标准优化不足。

Method: 提出CE-RM-4B模型，采用点式评估而非成对评估，使用专门的两阶段rollout方法训练，采用统一查询标准，仅使用约5.7K从开源偏好数据集中筛选的高质量数据。

Result: CE-RM-4B在多样化奖励模型基准测试中表现优越，特别是在Best-of-N场景中，并在下游RL实践中提供更有效的改进。

Conclusion: 通过点式评估、两阶段训练和统一标准，CE-RM-4B成功弥合了LLM评估模型在基准测试与实际RL应用之间的差距，证明了少量高质量数据的有效性。

Abstract: Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.

</details>


### [46] [PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments](https://arxiv.org/abs/2601.20330)
*Zhuang Chen,Dazhen Wan,Zhangkai Zheng,Guanqun Bi,Xiyao Xiao,Binghang Li,Minlie Huang*

Main category: cs.CL

TL;DR: 提出PsychePass框架，通过轨迹锚定锦标赛评估LLM在心理咨询中的能力，解决现有评估方法的"未锚定"缺陷和过程漂移、标准漂移问题


<details>
  <summary>Details</summary>
Motivation: 当前LLM在心理健康领域的评估存在"未锚定"缺陷，导致过程漂移（客户模拟偏离咨询目标）和标准漂移（静态评分缺乏稳定性），需要更可靠的评估方法

Method: 提出PsychePass框架：1）在模拟中锚定交互轨迹，让客户精确控制咨询过程以探测多方面能力；2）通过瑞士制锦标赛锚定战斗轨迹，利用动态成对战斗产生稳健的Elo评分

Result: 实验验证了PsychePass的有效性，与人类专家判断具有强一致性，且锦标赛轨迹可转化为可信的奖励信号，用于策略强化学习提升LLM性能

Conclusion: PsychePass通过轨迹锚定锦标赛为LLM的心理咨询能力评估提供了统一框架，解决了现有评估的不稳定性问题，并能转化为训练信号提升模型性能

Abstract: While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.

</details>


### [47] [MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment](https://arxiv.org/abs/2601.20335)
*Qinzhuo Wu,Zhizhuo Yang,Hanhao Li,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: MobileBench-OL是一个包含1080个任务、覆盖80个中文应用的在线移动GUI代理基准测试，通过5个子集评估任务执行、复杂推理和噪声鲁棒性，填补现有基准测试与现实环境之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理在线基准测试主要关注任务指令跟随能力，忽视了推理和探索能力，且未考虑现实移动环境中的随机噪声，导致基准测试与现实环境存在差距。

Method: 提出MobileBench-OL在线基准测试，包含1080个任务覆盖80个中文应用，通过5个子集多维度评估代理能力，并提供带有重置机制的自动评估框架，实现稳定可重复的现实世界基准测试。

Result: 对12个领先GUI代理的评估显示，在满足现实世界需求方面仍有显著改进空间。人工评估进一步证实MobileBench-OL能够可靠测量领先GUI代理在真实环境中的性能。

Conclusion: MobileBench-OL填补了现有基准测试与现实移动环境之间的差距，通过多维度评估和自动评估框架，为移动GUI代理的发展提供了更全面、现实的测试平台。

Abstract: Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.

</details>


### [48] [Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space](https://arxiv.org/abs/2601.20339)
*Yangyi Shen,Tianjian Feng,Jiaqi Han,Wen Wang,Tianlang Chen,Chunhua Shen,Jure Leskovec,Stefano Ermon*

Main category: cs.CL

TL;DR: Order-Token Search：一种用于扩散语言模型的联合搜索方法，通过同时搜索生成顺序和token值来探索解码轨迹空间，在数学推理和代码生成任务上显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型的解码方法只遵循单一轨迹，限制了在轨迹空间中的探索能力。扩散语言模型具有顺序无关的生成特性，可以探索多种可能的解码轨迹，但现有方法未能充分利用这一优势。

Method: 提出Order-Token Search方法，通过联合搜索生成顺序和token值来探索轨迹空间。核心是一个似然估计器，用于对去噪动作进行评分，实现稳定的剪枝和高效的多样化轨迹探索。

Result: 在数学推理和代码生成基准测试中，Order-Token Search在GSM8K、MATH500、Countdown和HumanEval上分别取得3.1%、3.8%、7.9%和6.8%的绝对提升，匹配或超越了经过后训练的diffu-GRPO d1-LLaDA模型。

Conclusion: 联合搜索是推进扩散语言模型解码能力的关键组件，Order-Token Search通过探索生成顺序和token值的联合空间，显著提升了扩散语言模型的解码性能。

Abstract: Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.

</details>


### [49] [Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents](https://arxiv.org/abs/2601.20412)
*Qihao Wang,Yue Hu,Mingzhe Lu,Jiayue Wu,Yanbing Liu,Yuanmin Tang*

Main category: cs.CL

TL;DR: 本文提出了一个基于认知负荷理论的框架，用于诊断LLM使用工具时的能力边界，而不仅仅是评估最终准确率。该框架将任务复杂度分解为固有负荷和外在负荷，并构建了可参数化调整认知负荷的基准测试ToolLoad-Bench。


<details>
  <summary>Details</summary>
Motivation: 当前LLM工具使用能力的评估主要关注最终准确率，这只能显示模型能做什么，但无法揭示定义其真正能力边界的认知瓶颈。需要从简单的性能评分转向诊断工具，以理解模型的能力限制。

Method: 1. 基于认知负荷理论构建框架，将任务复杂度分解为：固有负荷（解决方案路径的结构复杂性，通过新颖的工具交互图形式化）和外在负荷（任务呈现模糊性带来的难度）。2. 构建ToolLoad-Bench基准测试，这是首个可参数化调整认知负荷的基准，支持受控实验。

Result: 评估显示随着认知负荷增加，模型性能出现明显的断崖式下降，使研究人员能够精确绘制每个模型的能力边界。框架的预测与实证结果高度校准，验证了其有效性。

Conclusion: 该框架建立了一种理解智能体能力边界的原理性方法，为构建更高效的系统提供了实践基础。它超越了简单的性能评分，成为诊断LLM工具使用认知瓶颈的有效工具。

Abstract: The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.

</details>


### [50] [SpeechMapper: Speech-to-text Embedding Projector for LLMs](https://arxiv.org/abs/2601.20417)
*Biswesh Mohapatra,Marcely Zanon Boito,Ioan Calapodescu*

Main category: cs.CL

TL;DR: SpeechMapper提出了一种高效、可扩展的语音-LLM集成方法，通过预训练投影层和简短指令微调，减少计算成本和过拟合，在语音翻译和口语问答任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前语音LLM方法通过投影层连接语音基础模型和LLM，并在语音指令数据上训练所有组件，这种方法计算成本高且容易对任务和提示过拟合。需要一种更高效、更鲁棒的语音-LLM集成方法。

Method: SpeechMapper采用两阶段方法：1）在廉价硬件上预训练语音投影层（不包含LLM）；2）通过简短的1K步指令微调阶段将预训练块高效连接到目标LLM。支持任务无关和任务特定的指令微调策略。

Result: 在任务无关设置下，SpeechMapper与IWSLT25最佳指令跟随语音LLM相媲美，尽管从未在这些任务上训练过；在任务特定设置下，它在多个数据集上超越该模型，同时需要更少的数据和计算资源。

Conclusion: SpeechMapper提供了一种实用且可扩展的方法，无需大规模指令微调即可实现高效、泛化性强的语音-LLM集成，解决了当前方法的计算成本和过拟合问题。

Abstract: Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.

</details>


### [51] [Hopes and Fears -- Emotion Distribution in the Topic Landscape of Finnish Parliamentary Speech 2000-2020](https://arxiv.org/abs/2601.20424)
*Anna Ristilä,Otto Tarkka,Veronika Laippala,Kimmo Elo*

Main category: cs.CL

TL;DR: 分析芬兰议会2000-2020年演讲中的话题情感表达，发现情感表达存在话题特异性且整体呈现积极化趋势


<details>
  <summary>Details</summary>
Motivation: 现有研究常将议会讨论视为同质整体，忽略了话题特异性模式。虽然人们对议会中最具情感的话题有直觉假设，但缺乏对不同话题情感表达的实证研究。本文旨在填补这一空白。

Method: 使用情感分析模型，从共时和历时两个角度研究芬兰议会（Eduskunta）2000-2020年间演讲话题的情感表达。

Result: 研究结果强化了议会演讲日益积极化的证据，并为议会辩论中的话题特异性情感表达提供了新见解。

Conclusion: 议会讨论中的情感表达具有显著的话题特异性，不同话题引发的情感强度不同，且整体呈现积极化趋势，这为理解议会政治话语提供了重要视角。

Abstract: Existing research often treats parliamentary discourse as a homogeneous whole, overlooking topic-specific patterns. Parliamentary speeches address a wide range of topics, some of which evoke stronger emotions than others. While everyone has intuitive assumptions about what the most emotive topics in a parliament may be, there has been little research into the emotions typically linked to different topics. This paper strives to fill this gap by examining emotion expression among the topics of parliamentary speeches delivered in Eduskunta, the Finnish Parliament, between 2000 and 2020. An emotion analysis model is used to investigate emotion expression in topics, from both synchronic and diachronic perspectives. The results strengthen evidence of increasing positivity in parliamentary speech and provide further insights into topic-specific emotion expression within parliamentary debate.

</details>


### [52] [PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use](https://arxiv.org/abs/2601.20439)
*Qihao Wang,Mingzhe Lu,Jiayue Wu,Yue Hu,Yanbing Liu*

Main category: cs.CL

TL;DR: PEARL框架通过离线探索和在线强化学习优化LLM的工具使用规划能力，在ToolHop基准上达到56.5%的最新成功率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在使用外部工具时面临复杂多轮调用的挑战，包括规划能力弱、工具幻觉、参数生成错误和交互鲁棒性差等问题

Method: 采用两阶段方法：离线阶段探索工具学习有效使用模式和失败条件；在线阶段通过GRPO训练专用规划器，使用精心设计的奖励函数提供规划质量信号

Result: 在ToolHop和T-Eval基准测试中显著超越现有方法，在ToolHop上达到56.5%的最新成功率，同时保持较低调用错误率

Conclusion: PEARL框架在解决工具使用的复杂规划挑战方面取得关键进展，有助于开发更鲁棒可靠的基于LLM的智能体

Abstract: Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \textbf{56.5\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.

</details>


### [53] [MuVaC: AVariational Causal Framework for Multimodal Sarcasm Understanding in Dialogues](https://arxiv.org/abs/2601.20451)
*Diandian Guo,Fangfang Yuan,Cong Cao,Xixun Lin,Chuan Zhou,Hao Peng,Yanan Cao,Yanbing Liu*

Main category: cs.CL

TL;DR: MuVaC是一个变分因果推理框架，通过模拟人类认知机制来联合优化多模态讽刺检测（MSD）和多模态讽刺解释（MuSE），利用因果依赖关系提升性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中多模态讽刺的普遍性使得理解真实意图成为关键挑战。现有研究要么单独处理讽刺检测，要么单独处理讽刺解释，即使有尝试整合的工作也忽略了它们内在的因果依赖关系。需要一种能模拟人类认知机制、联合优化这两个任务的框架。

Method: 提出MuVaC变分因果推理框架：1）从结构因果模型角度建模MSD和MuSE，建立变分因果路径定义联合优化目标；2）设计"对齐-融合"方法整合多模态特征，为检测和解释提供鲁棒融合表示；3）通过确保检测结果与解释的一致性来增强推理可信度。

Result: 实验结果表明MuVaC在公开数据集上表现出优越性能，为理解多模态讽刺提供了新视角。

Conclusion: MuVaC通过模拟人类认知机制，利用因果依赖关系联合优化讽刺检测和解释，在多模态讽刺理解方面取得了显著进展，为解决这一复杂任务提供了有效框架。

Abstract: The prevalence of sarcasm in multimodal dialogues on the social platforms presents a crucial yet challenging task for understanding the true intent behind online content. Comprehensive sarcasm analysis requires two key aspects: Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE). Intuitively, the act of detection is the result of the reasoning process that explains the sarcasm. Current research predominantly focuses on addressing either MSD or MuSE as a single task. Even though some recent work has attempted to integrate these tasks, their inherent causal dependency is often overlooked. To bridge this gap, we propose MuVaC, a variational causal inference framework that mimics human cognitive mechanisms for understanding sarcasm, enabling robust multimodal feature learning to jointly optimize MSD and MuSE. Specifically, we first model MSD and MuSE from the perspective of structural causal models, establishing variational causal pathways to define the objectives for joint optimization. Next, we design an alignment-then-fusion approach to integrate multimodal features, providing robust fusion representations for sarcasm detection and explanation generation. Finally, we enhance the reasoning trustworthiness by ensuring consistency between detection results and explanations. Experimental results demonstrate the superiority of MuVaC in public datasets, offering a new perspective for understanding multimodal sarcasm.

</details>


### [54] [BMAM: Brain-inspired Multi-Agent Memory Framework](https://arxiv.org/abs/2601.20465)
*Yang Li,Jiaxiang Liu,Yusong Wang,Yujie Wu,Mingkun Xu*

Main category: cs.CL

TL;DR: BMAM是一种受大脑启发的多智能体记忆架构，通过功能专门化的子系统解决语言模型智能体在长期交互中的"灵魂侵蚀"问题，在LoCoMo基准上达到78.45%准确率。


<details>
  <summary>Details</summary>
Motivation: 语言模型智能体在长期交互中面临保持时间信息和行为一致性的挑战，作者称之为"灵魂侵蚀"。现有方法使用单一非结构化记忆存储，无法有效处理不同时间尺度的信息。

Method: BMAM将智能体记忆分解为四个功能专门化的子系统：情景记忆、语义记忆、显著性感知记忆和控制导向记忆，这些组件在互补的时间尺度上运行。情景记忆沿明确时间线组织，并通过融合多个互补信号来检索证据。

Result: 在LoCoMo基准测试中，BMAM在标准长时程评估设置下达到78.45%的准确率。消融分析证实，受海马体启发的的情景记忆子系统在时间推理中起关键作用。

Conclusion: BMAM通过功能专门化的记忆架构有效解决了语言模型智能体的"灵魂侵蚀"问题，为长时程推理提供了有效的解决方案，其中情景记忆子系统对时间推理至关重要。

Abstract: Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.

</details>


### [55] [Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch](https://arxiv.org/abs/2601.20476)
*Evanfiya Logacheva,Arto Hellas,Tsvetomila Mihaylova,Juha Sorva,Ava Heinonen,Juho Leinonen*

Main category: cs.CL

TL;DR: 提出基于修辞结构理论(RST)的上下文示例方法，用于生成更符合用户期望的图表代码，减少AI幻觉并提高图表忠实度


<details>
  <summary>Details</summary>
Motivation: 生成式AI在计算教育中广泛应用，但生成材料的质量引发教育者和学生的担忧，需要提高图表生成质量并减少AI幻觉

Method: 基于修辞结构理论(RST)的上下文示例方法，使用大语言模型生成图表代码，由计算机科学教育者评估150个图表的逻辑组织、连接性、布局美观和AI幻觉

Result: 方法减少了事实幻觉率，提高了图表对上下文的忠实度，但生成质量因LLM随机性而波动；复杂文本上下文导致更高幻觉率，LLM常无法检测自身错误

Conclusion: 基于RST的方法能改善图表生成质量，但LLM的随机性和幻觉问题仍需解决，评估数据集可用于自动化图表评估

Abstract: Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.

</details>


### [56] [Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models](https://arxiv.org/abs/2601.20546)
*Kumiko Nakajima,Jan Zuiderveld,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 该论文指出传统DAT测试评估LLM创造力存在缺陷，提出新的CDAT方法，发现小模型更具创造力，大模型更注重适当性而非新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM创造力的方法缺乏人类创造力理论的基础，特别是广泛使用的DAT测试只关注新颖性而忽略了适当性这一核心要素，且测试结果甚至不如无创造力的基线模型，说明现有评估方法存在问题。

Method: 基于人类创造力理论（创造力=新颖性+适当性），提出条件性发散关联任务（CDAT），在考虑上下文适当性的条件下评估新颖性，比DAT能更好地区分噪声与创造力，同时保持简单客观。

Result: 在CDAT评估下，较小的模型家族通常表现出最多的创造力，而先进的模型家族更倾向于适当性而非新颖性。训练和对齐过程可能使模型沿着这个边界移动，使输出更适当但创造力降低。

Conclusion: 需要基于人类创造力理论来评估LLM的创造力，CDAT提供了更好的评估框架。模型训练和对齐可能以牺牲创造力为代价来提高适当性，这对LLM的创造性应用有重要启示。

Abstract: Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.

</details>


### [57] [Single-Nodal Spontaneous Symmetry Breaking in NLP Models](https://arxiv.org/abs/2601.20582)
*Shalom Rosner,Ronit D. Gross,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 该论文展示了自然语言处理模型中自发对称性破缺现象，即使在确定性动态和有限架构下，注意力头、节点甚至单个节点层面都会出现这种对称性破缺，类似于统计力学中的相变现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将统计力学中的自发对称性破缺概念扩展到自然语言处理领域，探索在确定性训练过程中是否会出现类似的对称性破缺现象，特别是在有限架构而非热力学极限条件下。

Method: 使用BERT-6架构在Wikipedia数据集上进行预训练，然后在FewRel分类任务上进行微调。通过分析注意力头、节点和单个节点的学习能力变化，研究节点数量增加时的学习能力交叉现象，并使用凸包分析对节点功能进行上界约束。

Result: 发现NLP模型在预训练和微调过程中确实出现自发对称性破缺，即使在确定性动态和有限架构下。节点学习能力随数量增加呈现交叉现象：初期因输出可能性增加而下降，随后因节点协作而增强。与自旋玻璃系统不同，每个节点功能都明确贡献于全局任务。

Conclusion: 自然语言处理模型在训练过程中表现出类似统计力学的自发对称性破缺现象，这种对称性破缺发生在多个层面，并且节点功能可以通过凸包分析进行量化。研究为理解神经网络学习机制提供了新的统计力学视角。

Abstract: Spontaneous symmetry breaking in statistical mechanics primarily occurs during phase transitions at the thermodynamic limit where the Hamiltonian preserves inversion symmetry, yet the low-temperature free energy exhibits reduced symmetry. Herein, we demonstrate the emergence of spontaneous symmetry breaking in natural language processing (NLP) models during both pre-training and fine-tuning, even under deterministic dynamics and within a finite training architecture. This phenomenon occurs at the level of individual attention heads and is scaled-down to its small subset of nodes and also valid at a single-nodal level, where nodes acquire the capacity to learn a limited set of tokens after pre-training or labels after fine-tuning for a specific classification task. As the number of nodes increases, a crossover in learning ability occurs, governed by the tradeoff between a decrease following random-guess among increased possible outputs, and enhancement following nodal cooperation, which exceeds the sum of individual nodal capabilities. In contrast to spin-glass systems, where a microscopic state of frozen spins cannot be directly linked to the free-energy minimization goal, each nodal function in this framework contributes explicitly to the global network task and can be upper-bounded using convex hull analysis. Results are demonstrated using BERT-6 architecture pre-trained on Wikipedia dataset and fine-tuned on the FewRel classification task.

</details>


### [58] [A Computational Approach to Language Contact -- A Case Study of Persian](https://arxiv.org/abs/2601.20592)
*Ali Basirat,Danial Namazifard,Navid Baradaran Hemmati*

Main category: cs.CL

TL;DR: 研究波斯语单语模型中间表征中的语言接触结构痕迹，发现句法信息对历史接触不敏感，而形态特征（如格和性）受语言特定结构强烈影响


<details>
  <summary>Details</summary>
Motivation: 研究单语语言模型中间表征中语言接触的结构痕迹，以波斯语（波斯语）作为历史上接触丰富的语言，探究模型在接触与波斯语有不同接触程度和类型的语言时的表征变化

Method: 量化中间表征中编码的语言信息量，评估不同形态句法特征在模型组件中的分布情况，通过波斯语训练模型接触与波斯语有不同接触关系的语言

Result: 通用句法信息对历史接触不敏感，而形态特征（如格和性）受语言特定结构强烈影响，表明单语语言模型中的接触效应是选择性和结构约束的

Conclusion: 单语语言模型中的语言接触效应具有选择性，句法信息相对稳定，形态特征更易受语言特定结构影响，这为理解语言模型如何编码语言接触提供了新视角

Abstract: We investigate structural traces of language contact in the intermediate representations of a monolingual language model. Focusing on Persian (Farsi) as a historically contact-rich language, we probe the representations of a Persian-trained model when exposed to languages with varying degrees and types of contact with Persian. Our methodology quantifies the amount of linguistic information encoded in intermediate representations and assesses how this information is distributed across model components for different morphosyntactic features. The results show that universal syntactic information is largely insensitive to historical contact, whereas morphological features such as Case and Gender are strongly shaped by language-specific structure, suggesting that contact effects in monolingual language models are selective and structurally constrained.

</details>


### [59] [AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios](https://arxiv.org/abs/2601.20613)
*Kaiyuan Chen,Qimin Wu,Taiyu Hou,Tianhao Tang,Xueyu Hu,Yuchen Hou,Bikun Li,Chengming Qian,Guoyin Wang,Haolin Chen,Haotong Tian,Haoye Zhang,Haoyu Bian,Hongbing Pan,Hongkang Zhang,Hongyi Zhou,Jiaqi Cai,Jiewu Rao,Jiyuan Ren,Keduan Huang,Lucia Zhu Huang,Mingyu Yuan,Naixu Guo,Qicheng Tang,Qinyan Zhang,Shuai Chen,Siheng Chen,Ting Ting Li,Xiaoxing Guo,Yaocheng Zuo,Yaoqi Guo,Yinan Wang,Yinzhou Yu,Yize Wang,Yuan Jiang,Yuan Tian,Yuanshuo Zhang,Yuxuan Liu,Yvette Yan Zeng,Zenyu Shan,Zihan Yin,Xiaobo Hu,Yang Liu,Yixin Ren,Yuan Gong*

Main category: cs.CL

TL;DR: 提出了AgentIF-OneDay基准，评估AI代理能否完成多样化日常任务，发现当前领先的API和开源模型已具备代理能力，使应用团队能开发前沿代理产品。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理评估过于关注任务难度，而忽视了日常场景中多样化任务的覆盖度。普通用户对AI高级能力的感知有限，需要评估代理能否处理自然语言指令和附件，完成日常工作、生活、学习任务。

Method: 提出AgentIF-OneDay基准，包含104个任务、767个评分点，分为三类：开放工作流执行（遵循复杂流程）、潜在指令（从附件推断隐含指令）、迭代优化（修改和扩展工作）。采用实例级评分标准和改进的评估流程，结合LLM验证与人工判断。

Result: 使用Gemini-3-Pro实现了80.1%的人机判断一致率。评估了四个领先的通用AI代理，发现基于API构建的代理产品和基于强化学习的ChatGPT代理处于第一梯队。领先的LLM API和开源模型已内化了代理能力。

Conclusion: AI应用团队可以利用现有LLM API和开源模型开发前沿的代理产品，因为核心代理能力已经内化在这些模型中。AgentIF-OneDay基准有助于更全面地评估代理在日常场景中的实际表现。

Abstract: The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.

</details>


### [60] [P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering](https://arxiv.org/abs/2601.20649)
*Wenlin Zhong,Chengyuan Liu,Yiquan Wu,Bovin Tan,Changlong Sun,Yi Wang,Xiaozhong Liu,Kun Kuang*

Main category: cs.CL

TL;DR: P2S是一个新颖的自监督框架，通过计算路径忠实度奖励为推理过程提供细粒度监督，解决了传统结果导向方法忽视推理过程监督的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在数学和编程等结构化领域取得了进展，但在一般领域推理任务中仍面临挑战，因为缺乏可验证的奖励信号。现有方法如RLPR只关注最终答案概率，忽视了推理过程的逐步监督。

Method: P2S框架在强化学习过程中合成并过滤高质量参考推理链，核心是计算每个推理步骤的路径忠实度奖励，该奖励基于给定当前推理前缀生成参考推理链后缀的条件概率。

Result: 在阅读理解和医学问答基准测试上的广泛实验表明，P2S显著优于强基线方法。

Conclusion: P2S通过提供密集的过程指导，直接解决了奖励稀疏性问题，能够灵活与任何基于结果的奖励结合，为推理过程提供细粒度监督。

Abstract: While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.

</details>


### [61] [A Dialectic Pipeline for Improving LLM Robustness](https://arxiv.org/abs/2601.20659)
*Sara Candussio*

Main category: cs.CL

TL;DR: 论文提出了一种辩证对话管道，通过自我对话让语言模型反思并修正错误答案，在保持泛化能力的同时提高输出质量，无需领域特定微调或额外验证器。


<details>
  <summary>Details</summary>
Motivation: 当前减少语言模型幻觉的方法（如领域特定微调或训练专用验证器）需要大量计算资源且限制了模型的泛化能力，需要一种更通用、资源友好的解决方案。

Method: 提出辩证对话管道，让LLM通过自我对话反思和修正初步答案，管道各阶段都注入相关上下文（oracle-RAG设置），并研究了上下文摘要和过滤的影响。

Result: 辩证对话管道显著优于标准模型答案，且一致比仅使用思维链提示获得更高性能，在不同数据集和模型家族上都得到验证。

Conclusion: 辩证对话管道是一种有效的自我修正方法，能在保持LLM泛化能力的同时提高答案质量，为减少幻觉提供了资源友好的解决方案。

Abstract: Assessing ways in which Language Models can reduce their hallucinations and improve the outputs' quality is crucial to ensure their large-scale use.
  However, methods such as fine-tuning on domain-specific data or the training of a separate \textit{ad hoc} verifier require demanding computational resources (not feasible for many user applications) and constrain the models to specific fields of knowledge.
  In this thesis, we propose a dialectic pipeline that preserves LLMs' generalization abilities while improving the quality of its answer via self-dialogue, enabling it to reflect upon and correct tentative wrong answers.
  We experimented with different pipeline settings, testing our proposed method on different datasets and on different families of models. All the pipeline stages are enriched with the relevant context (in an oracle-RAG setting) and a study on the impact of its summarization or its filtering is conducted.
  We find that our proposed dialectic pipeline is able to outperform by significative margins the standard model answers and that it consistently achieves higher performances than Chain-of-Thought only prompting.

</details>


### [62] [Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science](https://arxiv.org/abs/2601.20674)
*Juan Jose Rubio Jan,Jack Wu,Julia Ive*

Main category: cs.CL

TL;DR: LLMs应用于EHR数据科学任务：结构化数据查询（Python/Pandas）和通过RAG从非结构化临床文本中提取信息，在MIMIC III数据集上验证了其潜力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在电子健康记录数据科学中的实际应用能力，特别是处理结构化数据查询和非结构化文本信息提取这两个基础任务，以支持临床工作流程。

Method: 1. 使用自动生成的合成问答对评估框架；2. 在MIMIC III数据集（4个结构化表和1种临床笔记类型）上测试；3. 结合本地部署和API-based LLMs；4. 采用精确匹配、语义相似度和人工判断的综合评估方法。

Result: LLMs在结构化数据查询和非结构化文本信息提取方面表现出潜力，能够支持临床工作流程中的精确查询和准确信息提取。

Conclusion: LLMs在EHR数据科学任务中具有应用前景，特别是在结构化数据查询和通过RAG增强的非结构化文本信息提取方面，为临床工作流程提供了新的支持工具。

Abstract: This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.

</details>


### [63] [Efficient Multimodal Planning Agent for Visual Question-Answering](https://arxiv.org/abs/2601.20676)
*Zhuo Chen,Xinyu Geng,Xinyu Wang,Yong Jiang,Zhen Zhang,Pengjun Xie,Kewei Tu*

Main category: cs.CL

TL;DR: 提出一种训练多模态规划代理的方法，动态分解mRAG管道以解决VQA任务，在保持性能的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 现有的多模态检索增强生成(mRAG)方法在处理VQA任务时通常采用多阶段管道，存在固有依赖关系和效率限制，需要平衡效率与效果

Method: 训练一个多模态规划代理，动态分解mRAG管道，智能决定每个mRAG步骤的必要性，优化效率与效果的权衡

Result: 代理能减少冗余计算，搜索时间比现有方法减少60%以上，降低工具调用成本，在六个不同数据集上平均表现优于所有基线方法

Conclusion: 提出的多模态规划代理方法能有效平衡VQA任务中的效率与性能，显著提升mRAG系统的实用性

Abstract: Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.

</details>


### [64] [ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code](https://arxiv.org/abs/2601.20679)
*Mingqiao Mo,Yunlong Tan,Hao Zhang,Heng Zhang,Yangfan He*

Main category: cs.CL

TL;DR: ShieldedCode是首个保护感知的框架，通过学习虚拟机器保护代码的鲁棒表示，显著提升代码生成和二进制相似性检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟机器保护方法依赖基于规则的转换，设计成本高且易受自动化分析攻击，而大语言模型在代码保护方面的潜力尚未充分挖掘。

Method: 构建大规模源代码与规范化VM实现配对数据集，引入层次化依赖建模，结合功能感知和保护感知对比目标进行联合优化，采用两阶段持续预训练和微调流程。

Result: 在L0 VM代码生成上达到26.95% Pass@1（优于GPT-4o的22.58%），二进制相似性检测Recall@1比jTrans等SOTA方法提升10%。

Conclusion: 该框架显著提升了不同保护级别的鲁棒性，为基于学习的软件防御开辟了新研究方向。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, yet their potential for software protection remains largely untapped. Reverse engineering continues to threaten software security, while traditional virtual machine protection (VMP) relies on rigid, rule-based transformations that are costly to design and vulnerable to automated analysis. In this work, we present the first protection-aware framework that learns robust representations of VMP-protected code. Our approach builds large-scale paired datasets of source code and normalized VM implementations, and introduces hierarchical dependency modeling at intra-, preceding-, and inter-instruction levels. We jointly optimize language modeling with functionality-aware and protection-aware contrastive objectives to capture both semantic equivalence and protection strength. To further assess resilience, we propose a protection effectiveness optimization task that quantifies and ranks different VM variants derived from the same source. Coupled with a two-stage continual pre-training and fine-tuning pipeline, our method enables models to generate, compare, and reason over protected code. Extensive experiments show that our framework significantly improves robustness across diverse protection levels, opening a new research direction for learning-based software defense. In this work, we present ShieldedCode, the first protection-aware framework that learns robust representations of VMP-protected code. Our method achieves 26.95% Pass@1 on L0 VM code generation compared to 22.58% for GPT-4o., and improves binary similarity detection Recall@1 by 10% over state of art methods like jTrans.

</details>


### [65] [Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin](https://arxiv.org/abs/2601.20680)
*Ostap Vykhopen,Viktoria Skorik,Maxim Tereschenko,Veronika Solopova*

Main category: cs.CL

TL;DR: 研究用在线聚类算法替代HDBSCAN，解决社交媒体监控中批量聚类算法在连续数据流处理时的可扩展性问题，提出三阶段架构和评估标准。


<details>
  <summary>Details</summary>
Motivation: 社交媒体监控的自动化叙事智能系统面临可扩展性挑战，传统批量聚类算法（如HDBSCAN）需要完整重训练每个时间窗口，导致内存限制、计算效率低下，无法实时适应演化叙事。

Method: 提出三阶段架构（数据收集、建模、仪表板生成），评估多种在线聚类算法，使用滑动窗口模拟历史数据集（乌克兰信息空间），结合传统聚类指标和叙事指标进行评估。

Result: 研究评估了在线聚类算法在聚类质量保持、计算效率、内存占用和现有工作流集成兼容性方面的表现，填补了批量主题建模框架与社交媒体监控流式特性之间的关键空白。

Conclusion: 在线聚类算法能有效解决社交媒体监控中的可扩展性问题，对计算社会科学、危机信息学和叙事监控系统有重要影响，需要在传统聚类指标和叙事特定指标间取得平衡。

Abstract: Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.

</details>


### [66] [AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts](https://arxiv.org/abs/2601.20730)
*Shicheng Fang,Yuxin Wang,XiaoRan Liu,Jiahao Lu,Chuanyuan Tan,Xinchi Chen,Yining Zheng. Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: AgentLongBench：基于横向思维谜题的动态环境评估框架，揭示LLM智能体在动态信息合成方面的关键弱点


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估基准主要基于静态、被动的检索任务，无法模拟智能体与环境交互的复杂性（如非线性推理和迭代反馈），需要新的评估框架来测试动态上下文管理能力

Method: 引入AgentLongBench框架，基于横向思维谜题（Lateral Thinking Puzzles）模拟环境展开，生成知识密集型和知识无关场景的严格交互轨迹，测试不同记忆系统（32K到4M tokens）

Result: 实验发现：虽然智能体擅长静态检索，但在动态信息合成方面表现不佳，这种性能下降主要由解决查询所需的最小token数量驱动，大规模工具响应中的高信息密度比长轮对话中的内存碎片化更具挑战性

Conclusion: LLM智能体在动态工作流中的信息合成能力存在关键弱点，需要新的方法来解决大规模工具响应中的高信息密度挑战，而不仅仅是扩展上下文长度

Abstract: The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.

</details>


### [67] [QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks](https://arxiv.org/abs/2601.20731)
*Mae Sosto,Delfina Sol Martinez Pandiani,Laura Hollink*

Main category: cs.CL

TL;DR: LLMs再现社会规范（特别是异性恋规范），在文本生成中产生可测量的偏见，对标记为酷儿的对象产生最不利的情感、更高毒性和更负面的评价。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型如何再现社会规范（特别是异性恋规范），以及这些规范如何转化为文本生成中的可测量偏见，探索模型对不同性别和性取向对象的处理差异。

Method: 通过三个对象类别（酷儿标记、非酷儿标记、未标记的"正常化"类别）研究LLM响应，使用四个维度（情感、评价、毒性、预测多样性）量化表征不平衡，比较掩码语言模型和自回归语言模型的表现。

Result: 掩码语言模型对酷儿标记对象产生最不利的情感、更高毒性和更负面评价；自回归语言模型部分缓解这些模式；闭源自回归模型对未标记对象产生更有害输出。

Conclusion: LLMs再现规范性社会假设，但偏见的形式和程度强烈依赖于特定模型特征，这可能重新分配但不会消除表征伤害。

Abstract: This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized "unmarked" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.

</details>


### [68] [Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts](https://arxiv.org/abs/2601.20747)
*Elham Aghakhani,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 研究通过分析Reddit上5126个心理健康社区帖子，探讨用户如何评估和使用AI进行情感支持，发现用户参与度主要受结果叙述、信任和响应质量影响，而非单纯的情感纽带。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被用于临床环境之外的情感支持和心理健康互动，但人们对这些系统在日常使用中的评估和关系建立方式知之甚少。需要了解用户在敏感现实环境中如何解释语言技术。

Method: 基于技术接受模型和治疗联盟理论，开发理论指导的标注框架，采用混合LLM-人工流程分析5126个Reddit帖子，评估评价性语言、采纳相关态度和关系对齐。

Result: 用户参与度主要由叙述结果、信任和响应质量塑造，而非单纯情感纽带。积极情绪与任务和目标对齐最强相关，而以陪伴为导向的使用更多涉及联盟错配和报告的风险，如依赖和症状加剧。

Conclusion: 这项工作展示了如何将理论构建操作化应用于大规模话语分析，并强调了研究用户在敏感现实环境中如何解释语言技术的重要性，为理解AI情感支持的采用和风险提供了实证基础。

Abstract: Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.

</details>


### [69] [Persona Prompting as a Lens on LLM Social Reasoning](https://arxiv.org/abs/2601.20757)
*Jing Yang,Moritz Hechtbauer,Elisabeth Khalilov,Evelyn Luise Brinkmann,Vera Schmitt,Nils Feldhus*

Main category: cs.CL

TL;DR: 研究探讨了在仇恨言论检测等社会敏感任务中，使用人物角色提示（Persona Prompting）对大型语言模型生成解释的影响，发现虽然能改善分类性能，但会降低解释质量，且无法缓解模型偏见。


<details>
  <summary>Details</summary>
Motivation: 在社会敏感任务中，LLM生成解释的质量对用户信任和模型对齐至关重要。虽然人物角色提示被广泛用于引导模型生成用户特定内容，但其对模型推理过程的影响尚未充分研究。

Method: 使用带有词级标注的数据集，测量不同模拟人口统计角色下LLM生成解释与人类标注的一致性，评估人物角色提示对模型偏见和人类对齐的影响，在三个LLM上进行实验。

Result: 1. 人物角色提示在最主观的任务（仇恨言论检测）中改善分类性能，但降低解释质量；2. 模拟角色无法与真实人口统计群体对齐，模型对显著引导具有抵抗性；3. 模型表现出持续的人口统计偏见和过度标记有害内容的倾向。

Conclusion: 人物角色提示在社会敏感任务中存在关键权衡：虽然能改善分类性能，但通常以牺牲解释质量为代价，且无法缓解底层偏见，需要谨慎应用。

Abstract: For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.

</details>


### [70] [SERA: Soft-Verified Efficient Repository Agents](https://arxiv.org/abs/2601.20789)
*Ethan Shen,Danny Tormoen,Saurabh Shah,Ali Farhadi,Tim Dettmers*

Main category: cs.CL

TL;DR: SERA是一种高效训练代码代理的方法，通过监督微调实现开源模型中最先进的性能，成本比强化学习低26倍，比合成数据方法低57倍，使私有代码库专业化变得实用。


<details>
  <summary>Details</summary>
Motivation: 开源权重代码代理相比闭源系统本应具有根本优势：可以针对私有代码库进行专业化，直接在权重中编码仓库特定信息。但由于训练成本和复杂性，这一优势一直停留在理论层面。

Method: 提出Soft-Verified Efficient Repository Agents (SERA)方法，使用Soft Verified Generation (SVG)技术从单个代码仓库生成数千个轨迹，结合监督微调(SFT)进行高效训练。

Result: SERA在完全开源模型中达到最先进性能，匹配Devstral-Small-2等前沿开源权重模型。训练成本比强化学习低26倍，比先前合成数据方法低57倍。生成了超过20万个合成轨迹数据集。

Conclusion: 这项工作将极大加速开源代码代理的研究，展示开源模型可以专业化到私有代码库的优势。发布了SERA作为Ai2开源代码代理系列的第一个模型，包括所有代码、数据和Claude Code集成。

Abstract: Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.

</details>


### [71] [Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers](https://arxiv.org/abs/2601.20796)
*Yiran Huang,Karsten Roth,Quentin Bouniot,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: Transformer多模态大语言模型展现出上下文学习能力，本文通过合成分类任务的受控实验，揭示了多模态上下文学习的机制：当在主要模态上进行高多样性预训练时，次要模态只需极低数据复杂度即可实现多模态上下文学习。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型如何通过上下文示例学习跨模态信息关联，理解多模态上下文学习的底层机制。

Method: 使用合成分类任务训练小型Transformer模型，精确控制数据统计和模型架构。首先回顾单模态上下文学习的核心原理，然后扩展到多模态设置，通过机制分析探究学习过程。

Result: 发现RoPE位置编码提高了上下文学习的数据复杂度阈值；多模态学习存在根本性不对称：主要模态高多样性预训练时，次要模态只需极低数据复杂度即可实现多模态上下文学习；两种设置都依赖从匹配上下文示例复制标签的归纳式机制。

Conclusion: 研究为理解现代Transformer中的多模态上下文学习提供了机制基础，并建立了受控测试平台供未来研究使用。

Abstract: Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.

</details>


### [72] [Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction](https://arxiv.org/abs/2601.20803)
*Aunabil Chakma,Mihai Surdeanu,Eduardo Blanco*

Main category: cs.CL

TL;DR: 提出一种混合选择方法，通过句法语义结构相似性选择额外示例，结合LLM生成示例，提升少样本关系抽取性能


<details>
  <summary>Details</summary>
Motivation: 解决少样本关系抽取中上下文学习示例不足的问题，传统LLM生成的示例在词汇选择和句子结构上可能不够多样化

Method: 提出基于句法语义结构相似性的示例选择策略，与LLM生成的示例结合形成混合系统，在FS-TACRED和FS-FewRel数据集上测试

Result: 混合方法在FS-TACRED上达到SOTA性能，在FewRel子集上获得显著提升，跨数据集和LLM家族（Qwen和Gemma）均有良好迁移性

Conclusion: 基于结构相似性的示例选择策略与LLM生成示例互补，混合方法能更全面地捕捉目标关系，优于单一方法

Abstract: This paper presents several strategies to automatically obtain additional examples for in-context learning of one-shot relation extraction. Specifically, we introduce a novel strategy for example selection, in which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example. We show that this method results in complementary word choices and sentence structures when compared to LLM-generated examples. When these strategies are combined, the resulting hybrid system achieves a more holistic picture of the relations of interest than either method alone. Our framework transfers well across datasets (FS-TACRED and FS-FewRel) and LLM families (Qwen and Gemma). Overall, our hybrid selection method consistently outperforms alternative strategies and achieves state-of-the-art performance on FS-TACRED and strong gains on a customized FewRel subset.

</details>


### [73] [Linear representations in language models can change dramatically over a conversation](https://arxiv.org/abs/2601.20834)
*Andrew Kyle Lampinen,Yuxuan Li,Eghbal Hosseini,Sangnie Bhardwaj,Murray Shanahan*

Main category: cs.CL

TL;DR: 语言模型表示中的线性方向会随对话动态变化，事实性表示可在对话前后反转，这对可解释性和控制提出挑战。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型表示在对话过程中的动态变化，探索表示如何随上下文演化，这对理解模型适应性和可解释性具有重要意义。

Method: 通过模拟对话研究线性表示方向的变化，分析不同模型家族和层级的表示动态，测试不同对话条件（如回放脚本、科幻故事）的影响。

Result: 发现表示会随对话内容显著变化，事实性表示可反转；变化具有内容依赖性，对话相关信息变化而通用信息保留；变化在不同模型和层中均稳健。

Conclusion: 表示动态变化表明静态特征解释可能误导，这对可解释性和控制提出挑战，但也为理解模型上下文适应机制提供了新研究方向。

Abstract: Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.

</details>


### [74] [When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation](https://arxiv.org/abs/2601.20858)
*David Tan,Pinzhen Chen,Josef van Genabith,Koel Dutta Chowdhury*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在基准测试中存在记忆污染问题，特别是在多语言翻译场景中，记忆会跨方向传播，导致性能虚高，掩盖了记忆而非泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基准测试中可能存在记忆污染问题，导致分数虚高，将记忆伪装成泛化能力。在多语言环境中，这种记忆甚至可能传播到"未受污染"的语言中，需要研究其影响和检测方法。

Method: 使用FLORES-200翻译基准作为诊断工具，研究两个7-8B参数的多语言指令调优LLM：在FLORES上训练的Bloomz和作为未受污染对照的Llama。通过源端扰动（如改写和命名实体替换）来测试记忆的持续性，并分析BLEU分数的变化。

Result: 确认了Bloomz存在FLORES污染，发现机器翻译污染可以是跨方向的，由于目标端记忆导致未见翻译方向的性能被虚假提升。即使经过各种源端扰动，记忆的回忆通常仍然存在，但命名实体替换会导致BLEU分数一致下降，这成为检测污染模型记忆的有效方法。

Conclusion: 基准污染在多语言LLM中是一个严重问题，会导致性能评估失真。命名实体替换可以作为有效的探测方法来检测模型中的记忆污染，有助于区分真正的泛化能力和记忆效应。

Abstract: Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to "uncontaminated" languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz's FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [75] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了一种名为E2HIL的高效人机交互强化学习框架，通过主动选择信息量大的样本来减少人类干预，提高样本效率


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互强化学习框架样本效率低，需要大量人类干预才能收敛，导致高昂的人工成本

Method: 通过构建不同样本对策略熵的影响函数，基于动作概率和软优势的协方差进行高效估计，选择中等影响值的样本，剔除导致熵急剧下降的捷径样本和影响可忽略的噪声样本

Result: 在四个真实世界操作任务上的实验表明，相比最先进的人机交互强化学习方法，E2HIL实现了42.1%更高的成功率，同时需要10.1%更少的人类干预

Conclusion: E2HIL框架通过主动样本选择有效提高了人机交互强化学习的样本效率，减少了人类干预需求，在真实世界操作任务中表现出色

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [76] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: JIT*算法通过即时模块和运动性能模块改进采样式路径规划，在高维空间和机器人操作任务中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统采样式路径规划方法在高维复杂环境中难以高效找到可行且最优的路径，特别是在机器人操作中，运动奇点和自碰撞风险进一步增加了规划难度

Method: 提出Just-in-Time Informed Trees (JIT*)算法，包含两个核心模块：即时模块（含动态优化边连接的"即时边"和调整瓶颈区域采样密度的"即时采样"）和运动性能模块（通过动态切换平衡可操作性和轨迹成本）

Result: JIT*在ℝ⁴到ℝ¹⁶维度上持续优于传统采样式规划器，在单臂和双臂操作任务中表现出有效性

Conclusion: JIT*算法通过动态优化采样和运动控制，有效解决了高维机器人路径规划中的效率和安全性问题

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [77] [Real-Time Robot Execution with Masked Action Chunking](https://arxiv.org/abs/2601.20130)
*Haoxuan Wang,Gengyu Zhang,Yan Yan,Yuzhang Shang,Ramana Rao Kompella,Gaowen Liu*

Main category: cs.RO

TL;DR: REMAC通过掩码动作分块学习纠正调整，解决异步推理中的动作-感知不匹配问题，提高机器人策略的鲁棒性和实时性


<details>
  <summary>Details</summary>
Motivation: 实时执行对机器人等物理信息系统至关重要。异步推理虽然实现了实时响应，但简单集成常导致执行失败。先前方法关注分块间不连续性，而本文发现分块内不一致性（执行动作与当前感知部分不匹配）是关键但被忽视的因素。

Method: 提出REMAC方法：1）通过掩码动作分块在预训练策略上学习纠正调整，使策略在异步推理中能适应意图动作与实际执行的不匹配；2）引入前缀保留采样程序增强分块间连续性。

Result: 在仿真和真实环境中的大量实验表明，该方法能实现更快的任务执行，在不同延迟下保持鲁棒性，并持续获得更高的完成率，且不增加额外延迟。

Conclusion: REMAC通过解决分块内不一致性问题，提供了更可靠的实时机器人策略，在保持实时性的同时显著提升了执行成功率。

Abstract: Real-time execution is essential for cyber-physical systems such as robots. These systems operate in dynamic real-world environments where even small delays can undermine responsiveness and compromise performance. Asynchronous inference has recently emerged as a system-level paradigm for real-time robot manipulation, enabling the next action chunk to be predicted while the current one is being executed. While this approach achieves real-time responsiveness, naive integration often results in execution failure. Previous methods attributed this failure to inter-chunk discontinuity and developed test-time algorithms to smooth chunk boundaries. In contrast, we identify another critical yet overlooked factor: intra-chunk inconsistency, where the robot's executed action chunk partially misaligns with its current perception. To address this, we propose REMAC, which learns corrective adjustments on the pretrained policy through masked action chunking, enabling the policy to remain resilient under mismatches between intended actions and actual execution during asynchronous inference. In addition, we introduce a prefix-preserved sampling procedure to reinforce inter-chunk continuity. Overall, our method delivers more reliable policies without incurring additional latency. Extensive experiments in both simulation and real-world settings demonstrate that our method enables faster task execution, maintains robustness across varying delays, and consistently achieves higher completion rates.

</details>


### [78] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: 提出一种基于二阶修正的高斯过程模型更新方法，用于处理移动机器人定位误差导致的测量位置不确定性，提高预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中移动机器人进行标量场映射时，定位不精确导致测量位置存在误差，这种位置不确定性会降低高斯过程模型的均值和协方差估计精度。

Method: 利用核函数的可微性，开发二阶修正算法，使用预计算的GP均值和协方差函数的雅可比矩阵和海森矩阵，基于测量位置差异数据进行实时模型精化。

Result: 仿真结果表明，相比完全模型重新训练，该方法在预测精度和计算效率方面都有显著提升。

Conclusion: 该方法能够有效处理移动机器人定位误差带来的测量位置不确定性，为实时标量场映射提供了高效的高斯过程模型更新方案。

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [79] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: TRACER是一个纹理鲁棒的仿射链式思维框架，通过分层语义推理到外观鲁棒、物理一致的功能区域细化，解决可变形物体操作中的语义指令与物理交互点对齐问题。


<details>
  <summary>Details</summary>
Motivation: 可变形物体操作的核心挑战在于将高层语义指令与物理交互点在复杂外观和纹理变化下对齐。现有基于视觉的仿射预测方法由于近乎无限的自由度、复杂动力学和异质模式，常出现边界溢出和功能区域碎片化问题。

Method: 提出TRACER框架：1) 树状仿射链式思维(TA-CoT)将高层任务意图分解为分层子任务语义；2) 空间约束边界细化(SCBR)机制抑制预测溢出；3) 交互收敛细化流(ICRF)聚合被外观噪声破坏的离散像素，增强空间连续性和物理合理性。

Result: 在Fine-AGDDO15数据集和真实机器人平台上进行广泛实验，TRACER显著提高了跨不同纹理和模式的仿射接地精度，并提升了长时域任务的成功率，有效弥合了高层语义推理与低层物理执行之间的差距。

Conclusion: TRACER通过建立从分层语义推理到外观鲁棒、物理一致的功能区域细化的跨层次映射，解决了可变形物体操作中的语义-物理对齐问题，为机器人操作可变形物体提供了有效的解决方案。

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [80] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: TouchGuide是一种新颖的跨策略视觉-触觉融合范式，通过两阶段推理引导预训练的扩散或流匹配视觉运动策略，利用触觉反馈提升精细接触操作性能。


<details>
  <summary>Details</summary>
Motivation: 机器人精细和接触丰富的操作仍然具有挑战性，主要原因是触觉反馈未被充分利用。现有方法在复杂接触任务中表现不佳，需要更好的触觉整合方案。

Method: 提出TouchGuide两阶段融合范式：1) 早期采样阶段使用视觉输入生成粗略动作；2) 任务特定的接触物理模型(CPM)提供触觉引导，通过对比学习训练，提供触觉可行性分数来引导采样。同时开发TacUMI数据收集系统，使用刚性指尖获取高质量触觉数据。

Result: 在五个具有挑战性的接触丰富任务（如系鞋带、芯片交接）上的实验表明，TouchGuide始终显著优于最先进的视觉-触觉策略。

Conclusion: TouchGuide通过有效的跨模态融合和触觉引导，显著提升了机器人精细接触操作的性能，为复杂操作任务提供了实用解决方案。

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [81] [Shallow-π: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262)
*Boseong Jeon,Yunho Choi,Taehan Kim*

Main category: cs.RO

TL;DR: Shallow-pi：一个知识蒸馏框架，将VLA模型的transformer层数从18层压缩到6层，实现2倍以上推理加速，成功率下降不到1%，并在真实机器人平台上验证


<details>
  <summary>Details</summary>
Motivation: 实时机器人部署需要快速、设备端的VLA模型推理。现有研究主要集中在token级效率优化（如视觉token剪枝），而系统性的transformer层减少在基于流的VLA模型中尚未在知识蒸馏框架下探索

Method: 提出Shallow-pi知识蒸馏框架，激进地减少VLM骨干网络和基于流的动作头的transformer深度，从18层压缩到6层

Result: 实现超过2倍的推理加速，在标准操作基准测试中成功率绝对下降不到1%，在压缩的VLA模型中达到最先进性能。在Jetson Orin和Jetson Thor等工业级真实机器人平台上验证，包括人形系统在复杂动态操作场景中的应用

Conclusion: Shallow-pi通过系统性的transformer层减少和知识蒸馏，为VLA模型提供了高效的压缩方案，在保持性能的同时显著提升推理速度，适用于实时机器人部署

Abstract: The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

</details>


### [82] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: TaF-VLA：将触觉观测与物理力对齐的VLA框架，通过触觉-力适配器提取物理动态表征，显著提升接触密集型任务的性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要依赖视觉模态，缺乏接触密集型任务所需的物理直觉和精确力调节能力。现有方法将触觉输入视为辅助视觉纹理，忽略了表面变形与交互动态之间的内在关联。

Method: 提出从触觉-视觉对齐到触觉-力对齐的范式转变。开发自动触觉-力数据采集设备，构建包含1000万+同步触觉观测、6轴力/扭矩和矩阵力图的TaF数据集。核心是触觉-力适配器（TaF-Adapter），将序列触觉观测与交互力对齐，提取离散化潜在信息编码触觉观测。

Result: 真实世界实验表明，TaF-VLA策略在接触密集型任务上显著优于最先进的触觉-视觉对齐和纯视觉基线，验证了其通过跨模态物理推理实现鲁棒、力感知操作的能力。

Conclusion: 通过触觉-力对齐范式，TaF-VLA成功将高维触觉观测与物理交互力关联，使VLA模型能够捕捉历史依赖、噪声不敏感的物理动态，而非静态视觉纹理，为接触密集型机器人操作提供了更有效的解决方案。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [83] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: FAEA将前沿LLM智能体框架直接应用于机器人操作，无需任务演示或微调，在多个基准测试中达到接近VLA模型的性能


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型需要任务特定演示和微调，且泛化能力有限。研究通用LLM智能体框架能否作为机器人操作的新控制范式

Method: FAEA直接将Claude Agent SDK等前沿LLM智能体框架应用于机器人操作，利用软件工程中的迭代推理能力进行策略规划，无需修改框架

Result: 在LIBERO、ManiSkill3和MetaWorld基准测试中分别达到84.9%、85.7%和96%的成功率，接近需要少于100次演示的VLA模型性能。加入一轮人类反馈后，LIBERO性能提升至88.2%

Conclusion: 通用智能体框架足以处理以深思熟虑的任务级规划为主的机器人操作任务，为机器人系统利用前沿智能体基础设施开辟了新路径

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [84] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: RF-MatID：首个开源、大规模、宽频带、几何多样的射频数据集，用于细粒度材料识别，包含16个细粒度类别、142k样本，支持频率域和时域表示，并建立了多设置、多协议的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的材料识别受限于光学传感器，而基于射频的方法虽然能揭示材料内在特性，但缺乏大规模公开数据集和学习方法的基准测试，阻碍了该领域的发展。

Method: 创建RF-MatID数据集，包含16个细粒度材料类别（分为5个超类），频率范围4-43.5 GHz，142k样本，系统控制几何扰动（入射角和距离变化），建立多设置、多协议基准，评估最先进的深度学习模型。

Result: 提供了首个大规模射频材料识别数据集，建立了包含5种频率分配协议的基准测试，支持频率和区域级别的系统分析，评估了模型在分布内性能和跨角度、跨距离的分布外鲁棒性。

Conclusion: RF-MatID旨在促进可重复研究，加速算法进步，增强跨领域鲁棒性，支持射频材料识别在实际应用中的发展，填补了该领域的数据集空白。

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [85] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: STORM提出了一种轻量级的物体中心表示模块，通过多阶段训练策略将冻结的视觉基础模型增强为任务感知的物体中心表示，提升机器人操作的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型虽然提供了强大的感知特征，但其密集表示缺乏显式的物体级结构，限制了机器人操作任务的鲁棒性和可收缩性。需要一种方法将通用基础模型特征转化为任务感知的物体中心表示。

Method: STORM采用轻量级的物体中心适应模块，通过多阶段训练策略：首先使用语言嵌入进行视觉-语义预训练来稳定物体中心槽位，然后与下游操作策略联合适应。这种方法避免重新训练大型骨干网络。

Result: 在物体发现基准和模拟操作任务上的实验表明，STORM相比直接使用冻结基础模型特征或端到端训练物体中心表示，能更好地泛化到视觉干扰物，并提升控制性能。

Conclusion: 多阶段适应是高效地将通用基础模型特征转化为任务感知物体中心表示的有效机制，为机器人控制提供了更好的感知-任务对齐。

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [86] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: 提出一个结构化KPI框架，用于评估月球多机器人勘探任务，强调效率、鲁棒性和精度优先级的场景依赖性，并在实地测试中验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 当前月球机器人勘探任务缺乏统一的评估标准，不同机器人平台和实验设置的比较困难，现有工程指标与科学目标联系不明确。

Method: 从三个现实的多机器人月球场景中推导出结构化KPI框架，强调场景依赖的优先级（效率、鲁棒性、精度），设计为实地部署实用。

Result: 在多机器人实地测试中验证了框架的实用性，发现效率和鲁棒性相关的KPI易于应用，而精度导向的KPI需要可靠地面真实数据，在户外模拟环境中不一定可行。

Conclusion: 该框架可作为通用评估标准，实现多机器人实地试验的一致、目标导向比较，支持未来行星探索机器人系统的系统化开发。

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [87] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 使用低成本压电麦克风和音频频谱Transformer，通过振动声学感知实现机器人全身触觉定位，静态误差小于5mm，材料特性影响显著


<details>
  <summary>Details</summary>
Motivation: 传统触觉皮肤昂贵且集成复杂，需要一种可扩展、低成本的替代方案来实现丰富的接触感知

Method: 为机器人手配备7个低成本压电麦克风，利用音频频谱Transformer解码物理交互产生的振动特征

Result: 静态条件下定位误差小于5mm；硬质材料在脉冲响应定位中表现优异，纹理材料在轨迹跟踪中提供更好的摩擦特征；系统对机器人自身运动具有鲁棒性

Conclusion: 复杂的物理接触动力学可以从简单的振动信号中有效解码，为广泛、经济实惠的机器人接触感知提供了可行途径

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [88] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: MeCo是一个相似性感知的多机器人协作框架，采用"缓存与重用"原则减少冗余计算，通过相似性测试方法重用先前解决的任务方案，无需重新调用大语言模型，显著降低了规划成本并提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多机器人协作方法在面对相同或相似任务时需要重新规划，忽略了任务级相似性，导致计算效率低下。需要一种能够识别和重用相似任务解决方案的框架来提高协作效率。

Method: 提出MeCo框架，引入相似性测试方法，检索先前解决的高相关性任务，实现有效的计划重用而无需重新调用大语言模型。还创建了MeCoBench基准测试来评估相似任务协作场景的性能。

Result: 实验结果表明，与最先进方法相比，MeCo显著降低了规划成本并提高了成功率。

Conclusion: MeCo通过相似性感知和缓存重用机制，有效解决了多机器人协作中的冗余计算问题，为实际应用提供了更高效的解决方案。

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [89] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: GPO是一种用于腿式机器人强化学习的训练框架，通过时变动作变换限制早期动作空间以促进有效数据收集，然后逐步扩展以增强探索，在扭矩控制中表现优异。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人的强化学习训练面临高维连续动作、硬件限制和有限探索的挑战。现有方法在位置控制中表现良好，但在扭矩控制中探索动作空间和获取有效梯度信号更加困难。

Method: 提出Growing Policy Optimization (GPO)框架，使用时变动作变换在训练早期限制有效动作空间，促进更有效的数据收集和策略学习，然后逐步扩展动作空间以增强探索。

Result: 在四足和六足机器人上评估GPO，包括在硬件上零样本部署仿真训练的策略。GPO训练的策略始终获得更好的性能，证明该方法为腿式运动学习提供了通用的环境无关优化框架。

Conclusion: GPO通过时变动作变换有效解决了扭矩控制中的探索难题，保留了PPO更新规则并确保训练稳定性，为腿式机器人强化学习提供了有效的通用训练框架。

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [90] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*Péter Polcz,Katalin Schäffer,Miklós Koller*

Main category: cs.RO

TL;DR: 提出一种从肌腱位移和张力估计关节位置的计算方法，用于无直接关节传感的仿人机械手控制


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动的仿人机械手通常缺乏直接关节角度传感，因为集成关节编码器会影响机械紧凑性和灵巧性

Method: 基于Denavit-Hartenberg约定建立高效运动学模型，使用简化肌腱模型推导肌腱状态与关节位置的非线性方程，通过非线性优化求解，结合雅可比PI控制器和前馈项实现闭环控制

Result: 在MuJoCo仿真环境中使用具有5个自由度的长指和6个自由度拇指的解剖学正确生物机电手验证了估计和控制框架的有效性和局限性

Conclusion: 提出的方法能够在无直接关节传感的情况下实现手势跟踪，为肌腱驱动仿人机械手提供了可行的关节估计和控制解决方案

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [91] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO是一个单步生成策略框架，通过MeanFlow实现数学推导的单步推理、分散正则化防止表示崩溃、RL微调超越专家演示，在机器人控制中实现5-20倍推理加速，达到120Hz以上实时控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散和流匹配的生成策略需要多步采样，这在时间关键场景中部署受限。实时机器人控制需要快速动作生成，但多步采样从根本上限制了在时间敏感场景中的应用。

Method: 提出DMPO统一框架，包含三个关键组件：1) MeanFlow：通过数学推导实现单步推理，无需知识蒸馏；2) 分散正则化：防止表示崩溃；3) RL微调：超越专家演示。结合轻量级模型架构，实现高效单步生成。

Result: 在RoboMimic操作和OpenAI Gym运动基准测试中，DMPO表现优于或与多步基线相当。推理速度提升5-20倍，达到120Hz以上实时控制要求，在高性能GPU上可达数百赫兹。在Franka-Emika-Panda机器人上的物理部署验证了实际应用性。

Conclusion: DMPO通过统一的单步生成框架解决了实时机器人控制中的速度瓶颈，实现了真正的单步推理，超越了多步方法的性能限制，为时间关键场景提供了实用的解决方案。

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [92] [Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy](https://arxiv.org/abs/2601.20776)
*Huanyu Tian,Martin Huber,Lingyun Zeng,Zhe Han,Wayne Bennett,Giuseppe Silvestri,Gerardo Mendizabal-Ruiz,Tom Vercauteren,Alejandro Chavez-Badiola,Christos Bergeles*

Main category: cs.RO

TL;DR: 提出弱监督框架，融合标定感知与导纳控制，通过热身轨迹实现无外部标记的深度感知，提升显微镜引导微操作的精度与可靠性


<details>
  <summary>Details</summary>
Motivation: 传统稳态手机器人依赖劳动密集的2D标注，需要外部标记或手动深度标注，操作复杂且精度有限。本文旨在开发无需复杂设置的实用框架，提高显微镜引导生物医学微操作的可靠性

Method: 采用弱监督框架，利用可重复使用的热身轨迹提取隐含空间信息，实现标定感知的深度分辨感知。通过显式表征观测和标定模型的残差，从记录的热身轨迹建立任务空间误差预算，结合导纳控制

Result: 系统实现横向闭环精度约49微米（95%置信度，最坏测试子集），深度精度≤291微米（95%置信度）。用户研究显示，学习代理将NASA-TLX工作负荷降低77.1%（相对于简单稳态手辅助基线）

Conclusion: 弱监督代理无需复杂设置即可提高显微镜引导生物医学微操作的可靠性，为显微镜引导干预提供了实用框架，显著降低操作者工作负荷并提升精度

Abstract: This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.

</details>


### [93] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: 提出一种在ROS 2系统中实现知识图谱的综合方法，通过模拟搜救任务验证其能提升自主机器人任务效率和智能决策


<details>
  <summary>Details</summary>
Motivation: 旨在提升自主机器人任务的效率和智能性，通过知识图谱增强机器人系统的决策能力和任务执行性能

Method: 包含定义初始和目标条件、结构化任务与子任务、规划执行顺序、用知识图谱表示任务相关数据、使用高级语言设计任务等多个步骤，并在Aerostack2框架中实现，通过Gazebo环境模拟无人机搜救任务

Result: 在模拟搜救任务中成功演示了无人机自主定位目标，验证了该方法能有效提升决策制定和任务执行性能

Conclusion: 提出的知识图谱实现方法能显著增强ROS 2系统中自主机器人任务的效率和智能性，为复杂任务执行提供了有效框架

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [94] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出基于神经风格迁移的sim-to-real强化学习方法，通过变分自编码器学习特征表示并生成弱配对轨迹，在机器人切割任务中实现更好的任务完成时间和行为稳定性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人控制中依赖大量仿真数据，但仿真与真实世界存在领域差距，且真实世界样本有限，限制了实际部署。

Method: 将神经风格迁移从图像处理重新解释到机器人控制，使用变分自编码器联合学习自监督特征表示，生成弱配对的源-目标轨迹以提高合成轨迹的物理真实性。

Result: 在机器人切割未知材料任务中，相比基线方法（包括先前工作、CycleGAN和条件变分自编码器时间序列转换），实现了更好的任务完成时间和行为稳定性，仅需少量真实世界数据。

Conclusion: 该方法对几何和材料变化具有鲁棒性，证明了在真实世界奖励信息不可用的接触密集型任务中策略适应的可行性。

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>
