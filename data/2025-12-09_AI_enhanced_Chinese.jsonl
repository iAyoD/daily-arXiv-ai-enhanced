{"id": "2512.06002", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06002", "abs": "https://arxiv.org/abs/2512.06002", "authors": ["Evan Conway", "David Porfirio", "David Chan", "Mark Roberts", "Laura M. Hiatt"], "title": "POrTAL: Plan-Orchestrated Tree Assembly for Lookahead", "comment": "Submitted to ICRA 26", "summary": "Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.", "AI": {"tldr": "POrTAL\uff1a\u4e00\u79cd\u65b0\u7684\u8f7b\u91cf\u7ea7\u6982\u7387\u89c4\u5212\u7b97\u6cd5\uff0c\u7ed3\u5408FF-Replan\u548cPOMCP\u7684\u4f18\u52bf\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u5feb\u901f\u751f\u6210\u9ad8\u6548\u4efb\u52a1\u8ba1\u5212", "motivation": "\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\uff0c\u673a\u5668\u4eba\u901a\u5e38\u9762\u4e34\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\uff0c\u9700\u8981\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u4e0b\u8fdb\u884c\u89c4\u5212\u3002\u73b0\u6709\u6982\u7387\u89c4\u5212\u7b97\u6cd5\u8981\u4e48\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u8981\u4e48\u9700\u8981\u8fc7\u591a\u6b65\u9aa4\u624d\u80fd\u8fbe\u6210\u76ee\u6807\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPOrTAL\u7b97\u6cd5\uff0c\u7ed3\u5408FF-Replan\u548cPOMCP\u4e24\u79cd\u57fa\u7ebf\u89c4\u5212\u7b97\u6cd5\u7684\u4f18\u52bf\uff0c\u8bbe\u8ba1\u4e3a\u8f7b\u91cf\u7ea7\u6982\u7387\u89c4\u5212\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u4eba\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u7cfb\u5217\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cPOrTAL\u80fd\u591f\u5feb\u901f\u627e\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6b65\u9aa4\u6570\u91cf\u4e0a\u4f18\u4e8eFF-Replan\u548cPOMCP\u57fa\u7ebf\u7b97\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u65f6\u95f4\u7ea6\u675f\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002", "conclusion": "POrTAL\u4e3a\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u89c4\u5212\u8d28\u91cf\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2512.06017", "categories": ["cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.06017", "abs": "https://arxiv.org/abs/2512.06017", "authors": ["Laurence Liang"], "title": "Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models", "comment": "Accepted at CVIS 2025", "summary": "Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf\" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.", "AI": {"tldr": "\u4f7f\u7528\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4ece\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u673a\u5668\u4eba\u624b\u81c2\u5173\u8282\u89d2\u5ea6\uff0c\u5efa\u7acb\u4e86\u6027\u80fd\u57fa\u51c6\uff0c\u53d1\u73b0\u6d4b\u8bd5\u65f6\u7f29\u653e\u6216\u53c2\u6570\u7f29\u653e\u5355\u72ec\u4f7f\u7528\u4e0d\u80fd\u6539\u5584\u9884\u6d4b\u6548\u679c\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u624b\u81c2\u5728\u5de5\u4e1a\u548c\u5bb6\u5ead\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u53ef\u9760\u7684\u5173\u8282\u89d2\u5ea6\u4f30\u8ba1\u80fd\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u5e76\u53ef\u4f5c\u4e3a\u9a8c\u8bc1\u5668\u6765\u8fdb\u4e00\u6b65\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4f5c\u4e3a\"\u5373\u7528\u578b\"\u5de5\u5177\uff0c\u4ece\u5355\u5f20\u76ee\u6807\u56fe\u50cf\u4f30\u8ba1\u673a\u5668\u4eba\u624b\u81c2\u5173\u8282\u89d2\u5ea6\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u5bf9\u4e0a\u8bc4\u4f30\u3002", "result": "\u5efa\u7acb\u4e86\u5f53\u524d\u524d\u6cbf\u8bed\u8a00\u6a21\u578b(FLMs)\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u6d4b\u8bd5\u65f6\u7f29\u653e\u6216\u53c2\u6570\u7f29\u653e\u5355\u72ec\u4f7f\u7528\u4e0d\u4f1a\u6539\u5584\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u4f5c\u4e3a\u673a\u5668\u4eba\u624b\u81c2\u5173\u8282\u89d2\u5ea6\u4f30\u8ba1\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u9700\u8981\u66f4\u590d\u6742\u7684\u7f29\u653e\u7b56\u7565\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2512.06038", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06038", "abs": "https://arxiv.org/abs/2512.06038", "authors": ["Kelsey Fontenot", "Anjali Gorti", "Iva Goel", "Tonio Buonassisi", "Alexander E. Siemenn"], "title": "Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction", "comment": "15 pages, 8 figures", "summary": "Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.", "AI": {"tldr": "\u5f00\u53d1\u81ea\u52a8\u5316\u57fa\u677f\u5904\u7406\u4ea4\u6362\u7cfb\u7edf(ASHE)\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u3001\u53cc\u6267\u884c\u5668\u5206\u914d\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u5b9e\u73b0\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u4e2d\u900f\u660e\u8106\u5f31\u57fa\u677f\u7684\u81ea\u52a8\u5904\u7406\u4e0e\u9519\u8bef\u68c0\u6d4b\u7ea0\u6b63\uff0c\u8fbe\u523098.5%\u9996\u6b21\u653e\u7f6e\u51c6\u786e\u7387\u3002", "motivation": "\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4(SDLs)\u5728\u5316\u5b66\u548c\u6750\u6599\u53d1\u73b0\u4e2d\u52a0\u901f\u4e86\u5b9e\u9a8c\u81ea\u52a8\u5316\uff0c\u4f46\u57fa\u677f\u5904\u7406\u548c\u91cd\u65b0\u52a0\u8f7d\u8fd9\u4e00\u5173\u952e\u6b65\u9aa4\u5e38\u88ab\u5ffd\u89c6\uff0c\u9650\u5236\u4e86SDL\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1ASHE\u7cfb\u7edf\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u6280\u672f\u3001\u53cc\u6267\u884c\u5668\u5206\u914d\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7ea0\u6b63\u8106\u5f31\u900f\u660e\u57fa\u677f\u64cd\u4f5c\u4e2d\u7684\u9519\u8bef\u3002", "result": "\u5728130\u6b21\u72ec\u7acb\u8bd5\u9a8c\u4e2d\uff0c\u900f\u660e\u73bb\u7483\u57fa\u677f\u91cd\u65b0\u52a0\u8f7d\u7684\u9996\u6b21\u653e\u7f6e\u51c6\u786e\u7387\u8fbe\u523098.5%\uff0c\u4ec5\u53d1\u751f\u4e24\u6b21\u57fa\u677f\u9519\u4f4d\uff0c\u4e14\u6210\u529f\u68c0\u6d4b\u4e3a\u9519\u8bef\u5e76\u81ea\u52a8\u7ea0\u6b63\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u66f4\u51c6\u786e\u53ef\u9760\u7684\u57fa\u677f\u5904\u7406\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u8fdb\u4e00\u6b65\u52a0\u901f\u4e86\u65b0\u578b\u5316\u5b66\u548c\u6750\u6599\u53d1\u73b0\u3002"}}
{"id": "2512.06112", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06112", "abs": "https://arxiv.org/abs/2512.06112", "authors": ["Yifang Xu", "Jiahao Cui", "Feipeng Cai", "Zhihao Zhu", "Hanlin Shang", "Shan Luan", "Mingwang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving", "comment": "18 pages, 11 figures", "summary": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.", "AI": {"tldr": "WAM-Flow\u662f\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5c06\u8f68\u8ff9\u89c4\u5212\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316token\u7a7a\u95f4\u4e0a\u7684\u79bb\u6563\u6d41\u5339\u914d\uff0c\u901a\u8fc7\u5e76\u884c\u53cc\u5411\u53bb\u566a\u5b9e\u73b0\u53ef\u8c03\u8282\u7684\u8ba1\u7b97\u7cbe\u5ea6\u6743\u8861\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8d85\u8d8a\u81ea\u56de\u5f52\u548c\u6269\u6563\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u89e3\u7801\u5668\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u5e73\u8861\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5e76\u884c\u751f\u6210\u3001\u652f\u6301\u7c97\u5230\u7ec6\u4f18\u5316\u3001\u4e14\u80fd\u6574\u5408\u5b89\u5168\u4e0e\u8212\u9002\u5ea6\u5956\u52b1\u7684\u65b0\u8303\u5f0f\u3002", "method": "1) \u4f7f\u7528\u5ea6\u91cf\u5bf9\u9f50\u6570\u503c\u5206\u8bcd\u5668\u901a\u8fc7\u4e09\u5143\u7ec4\u8fb9\u754c\u5b66\u4e60\u4fdd\u7559\u6807\u91cf\u51e0\u4f55\uff1b2) \u51e0\u4f55\u611f\u77e5\u6d41\u76ee\u6807\uff1b3) \u6a21\u62df\u5668\u5f15\u5bfc\u7684GRPO\u5bf9\u9f50\u6574\u5408\u5b89\u5168\u3001\u8fdb\u5ea6\u548c\u8212\u9002\u5ea6\u5956\u52b1\uff1b4) \u591a\u9636\u6bb5\u9002\u914d\u5c06\u9884\u8bad\u7ec3\u81ea\u56de\u5f52\u4e3b\u5e72\u8f6c\u6362\u4e3a\u975e\u56e0\u679c\u6d41\u6a21\u578b\uff1b5) \u901a\u8fc7\u6301\u7eed\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u589e\u5f3a\u9053\u8def\u573a\u666f\u80fd\u529b\u3002", "result": "\u5728NAVSIM v1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c1\u6b65\u63a8\u7406\u8fbe\u523089.1 PDMS\uff0c5\u6b65\u63a8\u7406\u8fbe\u523090.3 PDMS\uff0c\u8d85\u8d8a\u81ea\u56de\u5f52\u548c\u57fa\u4e8e\u6269\u6563\u7684VLA\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u79bb\u6563\u6d41\u5339\u914d\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u79bb\u6563\u6d41\u5339\u914d\u662f\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u65b0\u6709\u524d\u666f\u8303\u5f0f\uff0cWAM-Flow\u901a\u8fc7\u5e76\u884c\u53cc\u5411\u53bb\u566a\u548c\u53ef\u8c03\u8282\u8ba1\u7b97\u7cbe\u5ea6\u6743\u8861\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u4e0e\u8212\u9002\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u95ed\u73af\u6027\u80fd\u3002"}}
{"id": "2512.06097", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06097", "abs": "https://arxiv.org/abs/2512.06097", "authors": ["Emre Umucu", "Guillermina Solis", "Leon Garza", "Emilia Rivas", "Beatrice Lee", "Anantaa Kotal", "Aritran Piplai"], "title": "Empathy by Design: Aligning Large Language Models for Healthcare Dialogue", "comment": null, "summary": "General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eDPO\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u63d0\u5347LLM\u5728\u533b\u7597\u7167\u62a4\u5bf9\u8bdd\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u5171\u60c5\u80fd\u529b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "motivation": "\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u7167\u62a4\u5e94\u7528\u4e2d\u5b58\u5728\u4e24\u5927\u7f3a\u9677\uff1a\u4e8b\u5b9e\u4e0d\u53ef\u9760\u6027\u548c\u7f3a\u4e4f\u5171\u60c5\u6c9f\u901a\uff0c\u8fd9\u5728\u654f\u611f\u573a\u666f\u4e2d\u5e26\u6765\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u4fe1\u8d56\u3001\u5171\u60c5\u7684AI\u52a9\u624b\u3002", "method": "\u91c7\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5bf9\u9f50\u6846\u67b6\uff0c\u4f7f\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\u5fae\u8c03\u9886\u57df\u9002\u5e94\u7684LLM\uff0c\u5176\u4e2d\u504f\u597d\u56de\u590d\u4f53\u73b0\u652f\u6301\u6027\u3001\u53ef\u8bbf\u95ee\u7684\u6c9f\u901a\u98ce\u683c\uff0c\u62d2\u7edd\u56de\u590d\u4ee3\u8868\u89c4\u5b9a\u6027\u6216\u8fc7\u4e8e\u6280\u672f\u5316\u7684\u8bed\u8c03\u3002", "result": "DPO\u8c03\u4f18\u6a21\u578b\u5728\u591a\u4e2a\u5f00\u6e90\u548c\u4e13\u6709LLM\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u6539\u8fdb\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u66f4\u5f3a\u7684\u4eba\u672c\u8bc4\u4f30\u5206\u6570\uff0c\u4f18\u4e8e\u57fa\u7ebf\u548cGoogle\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u7b49\u5546\u4e1a\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u57fa\u4e8e\u504f\u597d\u7684\u5bf9\u9f50\u4e3a\u5f00\u53d1\u53ef\u4fe1\u8d56\u3001\u5171\u60c5\u4e14\u5177\u6709\u4e34\u5e8a\u77e5\u8bc6\u7684AI\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u7167\u62a4\u8005\u548c\u533b\u7597\u6c9f\u901a\u573a\u666f\u3002"}}
{"id": "2512.06130", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06130", "abs": "https://arxiv.org/abs/2512.06130", "authors": ["Grant Stagg", "Isaac E. Weintraub", "Cameron K. Peterson"], "title": "Probabilistic Weapon Engagement Zones for a Turn Constrained Pursuer", "comment": "Accepted for presentation at AIAA SciTech 2026. 17 pages, 7 figures", "summary": "Curve-straight probabilistic engagement zones (CSPEZ) quantify the spatial regions an evader should avoid to reduce capture risk from a turn-rate-limited pursuer following a curve-straight path with uncertain parameters including position, heading, velocity, range, and maximum turn rate. This paper presents methods for generating evader trajectories that minimize capture risk under such uncertainty. We first derive an analytic solution for the deterministic curve-straight basic engagement zone (CSBEZ), then extend this formulation to a probabilistic framework using four uncertainty-propagation approaches: Monte Carlo sampling, linearization, quadratic approximation, and neural-network regression. We evaluate the accuracy and computational cost of each approximation method and demonstrate how CSPEZ constraints can be integrated into a trajectory-optimization algorithm to produce safe paths that explicitly account for pursuer uncertainty.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u66f2\u7ebf-\u76f4\u7ebf\u6982\u7387\u4ea4\u6218\u533a(CSPEZ)\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u91cf\u5316\u89c4\u907f\u8005\u5e94\u907f\u514d\u7684\u7a7a\u95f4\u533a\u57df\uff0c\u4ee5\u51cf\u5c11\u88ab\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u8f6c\u5f2f\u7387\u53d7\u9650\u8ffd\u51fb\u8005\u6355\u83b7\u7684\u98ce\u9669\uff0c\u5e76\u5f00\u53d1\u4e86\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u6700\u5c0f\u5316\u6355\u83b7\u98ce\u9669\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5728\u8ffd\u51fb-\u89c4\u907f\u573a\u666f\u4e2d\uff0c\u8ffd\u51fb\u8005\u901a\u5e38\u5177\u6709\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\uff08\u4f4d\u7f6e\u3001\u822a\u5411\u3001\u901f\u5ea6\u3001\u8ddd\u79bb\u3001\u6700\u5927\u8f6c\u5f2f\u7387\u7b49\uff09\uff0c\u4f20\u7edf\u7684\u786e\u5b9a\u6027\u4ea4\u6218\u533a\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u5e76\u751f\u6210\u5b89\u5168\u89c4\u907f\u8f68\u8ff9\u7684\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u786e\u5b9a\u6027\u66f2\u7ebf-\u76f4\u7ebf\u57fa\u672c\u4ea4\u6218\u533a(CSBEZ)\u7684\u89e3\u6790\u89e3\uff0c\u7136\u540e\u6269\u5c55\u5230\u6982\u7387\u6846\u67b6\uff0c\u4f7f\u7528\u56db\u79cd\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u65b9\u6cd5\uff1a\u8499\u7279\u5361\u6d1b\u91c7\u6837\u3001\u7ebf\u6027\u5316\u3001\u4e8c\u6b21\u8fd1\u4f3c\u548c\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u3002\u6700\u540e\u5c06CSPEZ\u7ea6\u675f\u96c6\u6210\u5230\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u4e2d\u751f\u6210\u5b89\u5168\u8def\u5f84\u3002", "result": "\u8bc4\u4f30\u4e86\u56db\u79cd\u8fd1\u4f3c\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5c55\u793a\u4e86CSPEZ\u7ea6\u675f\u5982\u4f55\u96c6\u6210\u5230\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u4e2d\uff0c\u751f\u6210\u80fd\u591f\u660e\u786e\u8003\u8651\u8ffd\u51fb\u8005\u4e0d\u786e\u5b9a\u6027\u7684\u5b89\u5168\u8def\u5f84\u3002", "conclusion": "CSPEZ\u4e3a\u89c4\u907f\u8005\u63d0\u4f9b\u4e86\u91cf\u5316\u7a7a\u95f4\u98ce\u9669\u533a\u57df\u7684\u65b9\u6cd5\uff0c\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u65b9\u6cd5\u5404\u6709\u4f18\u52a3\uff0c\u96c6\u6210CSPEZ\u7ea6\u675f\u7684\u8f68\u8ff9\u4f18\u5316\u80fd\u591f\u6709\u6548\u751f\u6210\u8003\u8651\u8ffd\u51fb\u8005\u4e0d\u786e\u5b9a\u6027\u7684\u5b89\u5168\u89c4\u907f\u8def\u5f84\u3002"}}
{"id": "2512.06169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06169", "abs": "https://arxiv.org/abs/2512.06169", "authors": ["Chris Crawford"], "title": "Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yolox\u00f3chtil Mixtec ASR", "comment": "67 pages, 5 figures, 6 tables", "summary": "This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yolox\u00f3chitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4f7f\u7528\u5f62\u6001\u5b66\u611f\u77e5\u7684\u5206\u8bcd\u5668\u6765\u6539\u8fdbYolox\u00f3chitl Mixtec\u97f3\u9891\u8bed\u6599\u5e93\u7684\u8bed\u9645\u6ce8\u91ca\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u975e\u7ebf\u6027\u5206\u8bcd\u65b9\u6848\uff0c\u4e0e\u4f20\u7edfBPE\u548cUnigram\u6a21\u578b\u7ade\u4e89\uff0c\u5176\u4e2dSegment-and-Melody\u6a21\u578b\u5728\u8bcd\u9519\u8bef\u7387\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u63d0\u9ad8Yolox\u00f3chitl Mixtec\u97f3\u9891\u8bed\u6599\u5e93\u8bed\u9645\u6ce8\u91ca\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u9488\u5bf9\u8be5\u8bed\u8a00\u7684\u975e\u8fde\u63a5\u6027\u5f62\u6001\u7279\u5f81\u8bbe\u8ba1\u4e13\u95e8\u7684\u5206\u8bcd\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u7684\u975e\u7ebf\u6027\u5206\u8bcd\u65b9\u6848\uff1a1) Segment and Melody\u5206\u8bcd\u5668\uff0c\u4ec5\u63d0\u53d6\u58f0\u8c03\u4fe1\u606f\u800c\u4e0d\u9884\u6d4b\u5206\u5272\uff1b2) Sequence of Processes\u5206\u8bcd\u5668\uff0c\u9884\u6d4b\u5355\u8bcd\u5206\u5272\uff0c\u4f7f\u7aef\u5230\u7aefASR\u7cfb\u7edf\u80fd\u5355\u6b21\u751f\u6210\u5206\u5272\u548c\u672a\u5206\u5272\u7684\u8f6c\u5f55\u3002\u4f7f\u7528ASR\u548c\u57fa\u4e8e\u6587\u672c\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u5de5\u5177\uff0c\u5e76\u4e0e\u4f20\u7edfBPE\u548cUnigram\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u65b0\u578b\u5206\u8bcd\u5668\u4e0e\u4f20\u7edfBPE\u548cUnigram\u6a21\u578b\u7ade\u4e89\u6027\u76f8\u5f53\uff0cSegment-and-Melody\u6a21\u578b\u5728\u8bcd\u9519\u8bef\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5206\u8bcd\u5668\uff0c\u4f46\u5728\u5b57\u7b26\u9519\u8bef\u7387\u4e0a\u672a\u8fbe\u5230\u76f8\u540c\u6c34\u5e73\u3002\u901a\u8fc7\u5f62\u6001\u5b66\u548c\u4fe1\u606f\u8bba\u6307\u6807\u5206\u6790\uff0c\u53d1\u73b0\u4e0e\u4e0b\u6e38\u6027\u80fd\u5b58\u5728\u9884\u6d4b\u76f8\u5173\u6027\u3002", "conclusion": "\u9488\u5bf9\u8bed\u8a00\u975e\u8fde\u63a5\u6027\u5f62\u6001\u7279\u5f81\u4e13\u95e8\u8bbe\u8ba1\u7684\u975e\u7ebf\u6027\u5206\u8bcd\u5668\u5728ASR\u4efb\u52a1\u4e2d\u4e0e\u4f20\u7edfBPE\u548cUnigram\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e9b\u5206\u8bcd\u5668\u5728\u4e0b\u6e38\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.06147", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06147", "abs": "https://arxiv.org/abs/2512.06147", "authors": ["Hochul Hwang", "Soowan Yang", "Jahir Sadik Monon", "Nicholas A Giudice", "Sunghoon Ivan Lee", "Joydeep Biswas", "Donghyun Kim"], "title": "GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers", "comment": null, "summary": "While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.", "AI": {"tldr": "\u5f00\u53d1\u4e86GuideNav\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ec5\u4f7f\u7528\u89c6\u89c9\u7684\"\u6559\u5bfc-\u91cd\u590d\"\u5bfc\u822a\u7cfb\u7edf\uff0c\u6a21\u4eff\u5bfc\u76f2\u72ac\u7684\u5bfc\u822a\u65b9\u5f0f\uff0c\u80fd\u591f\u5728\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u516c\u91cc\u7ea7\u8def\u5f84\u8ddf\u968f\u3002", "motivation": "\u867d\u7136\u9488\u5bf9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u79fb\u52a8\u8f85\u52a9\u7cfb\u7edf\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u76f4\u63a5\u6307\u5bfc\u673a\u5668\u4eba\u5bfc\u822a\u8bbe\u8ba1\u7684\u53c2\u8003\u4ecd\u7136\u5f88\u5c11\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5f00\u53d1\u66f4\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u8f85\u52a9\u7cfb\u7edf\u3002", "method": "1. \u8fdb\u884c\u7efc\u5408\u4eba\u7c7b\u7814\u7a76\uff1a\u91c7\u8bbf26\u540d\u5bfc\u76f2\u72ac\u4f7f\u7528\u8005\u30014\u540d\u767d\u624b\u6756\u4f7f\u7528\u8005\u30019\u540d\u5bfc\u76f2\u72ac\u8bad\u7ec3\u5e08\u548c1\u540d\u5b9a\u5411\u884c\u8d70\u8bad\u7ec3\u5e08\uff0c\u89c2\u5bdf15+\u5c0f\u65f6\u5bfc\u76f2\u72ac\u8f85\u52a9\u884c\u8d70\u30022. \u57fa\u4e8e\u7814\u7a76\u89c1\u89e3\u5f00\u53d1GuideNav\u7cfb\u7edf\uff1a\u4ec5\u4f7f\u7528\u89c6\u89c9\u7684\u6559\u5bfc-\u91cd\u590d\u5bfc\u822a\u7cfb\u7edf\uff0c\u6784\u5efa\u8def\u5f84\u7684\u62d3\u6251\u8868\u793a\uff0c\u7ed3\u5408\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4e0e\u65f6\u95f4\u6ee4\u6ce2\uff0c\u4f7f\u7528\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u5668\u8ba1\u7b97\u5bfc\u822a\u52a8\u4f5c\u3002", "result": "1. \u5f00\u6e90\u4e86\u53bb\u6807\u8bc6\u5316\u6570\u636e\u96c6\u30022. GuideNav\u5728\u4e94\u4e2a\u6237\u5916\u73af\u5883\u4e2d\u6301\u7eed\u5b9e\u73b0\u516c\u91cc\u7ea7\u8def\u5f84\u8ddf\u968f\uff0c\u5c3d\u7ba1\u6559\u5bfc\u548c\u91cd\u590d\u8fd0\u884c\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u573a\u666f\u53d8\u5316\u30023. \u7528\u6237\u7814\u7a76\uff083\u540d\u5bfc\u76f2\u72ac\u4f7f\u7528\u8005\u548c1\u540d\u8bad\u7ec3\u5e08\uff09\u786e\u8ba4\u4e86\u7cfb\u7edf\u53ef\u884c\u6027\uff0c\u8fd9\u662f\u9996\u6b21\u5c55\u793a\u56db\u8db3\u79fb\u52a8\u7cfb\u7edf\u4ee5\u7c7b\u4f3c\u5bfc\u76f2\u72ac\u7684\u65b9\u5f0f\u68c0\u7d22\u8def\u5f84\u3002", "conclusion": "GuideNav\u7cfb\u7edf\u6210\u529f\u5c55\u793a\u4e86\u4ec5\u4f7f\u7528\u89c6\u89c9\u7684\u6559\u5bfc-\u91cd\u590d\u5bfc\u822a\u5728\u6237\u5916\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u5f00\u53d1\u66f4\u7b26\u5408\u9700\u6c42\u7684\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2512.06193", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06193", "abs": "https://arxiv.org/abs/2512.06193", "authors": ["Jihyung Park", "Saleh Afroogh", "Junfeng Jiao"], "title": "Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots", "comment": null, "summary": "Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \\textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.", "AI": {"tldr": "\u63d0\u51faGAUGE\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4bLLM\u5bf9\u8bdd\u4e2d\u7684\u9690\u6027\u60c5\u611f\u5347\u7ea7\u5371\u5bb3", "motivation": "LLM\u4f5c\u4e3a\u60c5\u611f\u4f34\u4fa3\u65f6\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u663e\u6bd2\u6027\uff0c\u91cd\u590d\u7684\u60c5\u611f\u5f3a\u5316\u6216\u60c5\u611f\u6f02\u79fb\u4e5f\u53ef\u80fd\u5bfc\u81f4\u9690\u6027\u4f24\u5bb3\uff0c\u4f20\u7edf\u6bd2\u6027\u8fc7\u6ee4\u5668\u65e0\u6cd5\u68c0\u6d4b\u3002\u73b0\u6709\u9632\u62a4\u673a\u5236\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\u6216\u4e34\u5e8a\u6807\u51c6\uff0c\u96be\u4ee5\u8ddf\u4e0a\u5bf9\u8bdd\u7684\u5b9e\u65f6\u52a8\u6001\u53d8\u5316\u3002", "method": "\u63d0\u51faGAUGE\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8elogit\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u91cfLLM\u8f93\u51fa\u5982\u4f55\u6982\u7387\u6027\u5730\u6539\u53d8\u5bf9\u8bdd\u7684\u60c5\u611f\u72b6\u6001\u6765\u68c0\u6d4b\u9690\u6027\u5bf9\u8bdd\u5347\u7ea7\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86GAUGE\u6846\u67b6\uff0c\u4f46\u6458\u8981\u4e2d\u6ca1\u6709\u62a5\u544a\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "GAUGE\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4bLLM\u5bf9\u8bdd\u4e2d\u7684\u9690\u6027\u60c5\u611f\u5347\u7ea7\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u6bd2\u6027\u8fc7\u6ee4\u5668\u65e0\u6cd5\u68c0\u6d4b\u7684\u7a7a\u767d\uff0c\u4e3aLLM\u60c5\u611f\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u9632\u62a4\u673a\u5236\u3002"}}
{"id": "2512.06151", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06151", "abs": "https://arxiv.org/abs/2512.06151", "authors": ["Ratnangshu Das", "Siddhartha Upadhyay", "Pushpak Jagtap"], "title": "Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets", "comment": null, "summary": "This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u975e\u7ebf\u6027\u7eaf\u53cd\u9988\u7cfb\u7edf\u7684\u5b9e\u65f6\u65f6\u7a7a\u7ba1\u6846\u67b6\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u8bc1\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5b8c\u6210\"\u5230\u8fbe-\u907f\u969c-\u505c\u7559\"\u4efb\u52a1", "motivation": "\u9488\u5bf9\u975e\u7ebf\u6027\u7eaf\u53cd\u9988\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9700\u8981\u6ee1\u8db3\u65f6\u95f4\u7ea6\u675f\u7684\"\u5230\u8fbe-\u907f\u969c-\u505c\u7559\"\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u672a\u77e5\u52a8\u6001\u548c\u5b9e\u65f6\u8981\u6c42", "method": "\u5f15\u5165\u5b9e\u65f6\u65f6\u7a7a\u7ba1\u6846\u67b6\uff0c\u5b9a\u4e49\u72b6\u6001\u7a7a\u95f4\u4e2d\u65f6\u53d8\u7403\u4f53\uff0c\u5176\u4e2d\u5fc3\u548c\u534a\u5f84\u5728\u7ebf\u81ea\u9002\u5e94\u8c03\u6574\uff1b\u63a8\u5bfc\u51fa\u95ed\u5f0f\u3001\u65e0\u8fd1\u4f3c\u7684\u63a7\u5236\u5f8b\u5c06\u7cfb\u7edf\u8f93\u51fa\u7ea6\u675f\u5728\u65f6\u7a7a\u7ba1\u5185", "result": "\u63d0\u4f9b\u4e86\u907f\u969c\u548c\u6309\u65f6\u5b8c\u6210\u4efb\u52a1\u7684\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff1b\u901a\u8fc7\u79fb\u52a8\u673a\u5668\u4eba\u548c\u98de\u884c\u5668\u5728\u6742\u4e71\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b9e\u65f6\u65f6\u7a7a\u7ba1\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u7eaf\u53cd\u9988\u7cfb\u7edf\u7684\u672a\u77e5\u52a8\u6001\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u8bc1\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.06227", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06227", "abs": "https://arxiv.org/abs/2512.06227", "authors": ["Junyu Mao", "Anthony Hills", "Talia Tseriotou", "Maria Liakata", "Aya Shamir", "Dan Sayda", "Dana Atzil-Slonim", "Natalie Djohari", "Arpan Mandal", "Silke Roth", "Pamela Ugwudike", "Mahesan Niranjan", "Stuart E. Middleton"], "title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety", "comment": null, "summary": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.", "AI": {"tldr": "\u63d0\u51faCFD\u6846\u67b6\uff0c\u901a\u8fc7\u591aLLM\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u6807\u6ce8\u8005\u4ea4\u6362\u7ec6\u7c92\u5ea6\u8bc1\u636e\u8fbe\u6210\u5171\u8bc6\uff0c\u7528\u4e8e\u6570\u636e\u589e\u5f3a\uff0c\u5728\u5fc3\u7406\u5065\u5eb7\u548c\u5728\u7ebf\u5b89\u5168\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6307\u6807\u5bf9NLP\u4efb\u52a1\u5f88\u91cd\u8981\uff08\u5982\u5fc3\u7406\u5065\u5eb7\u5206\u6790\u7684\u751f\u6d3b\u4e8b\u4ef6\u3001\u5728\u7ebf\u5b89\u5168\u7684\u5371\u9669\u884c\u4e3a\uff09\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u52a8\u6001\u53d8\u5316\uff0c\u9700\u8981\u6709\u6548\u7684\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5", "method": "\u63d0\u51fa\u7f6e\u4fe1\u611f\u77e5\u7ec6\u7c92\u5ea6\u8fa9\u8bba\uff08CFD\uff09\u6846\u67b6\uff0c\u591a\u4e2aLLM\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u6807\u6ce8\u8005\uff0c\u4ea4\u6362\u7ec6\u7c92\u5ea6\u8bc1\u636e\u8fbe\u6210\u5171\u8bc6\uff1b\u521b\u5efa\u4e24\u4e2a\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff1a\u5fc3\u7406\u5065\u5eb7Reddit\u5e78\u798f\u6570\u636e\u96c6\u548c\u5728\u7ebf\u5b89\u5168Facebook\u6652\u5a03\u98ce\u9669\u6570\u636e\u96c6", "result": "CFD\u6846\u67b6\u76f8\u6bd4\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u6700\u7a33\u5065\u7684\u6570\u636e\u589e\u5f3a\u6027\u80fd\uff1b\u6570\u636e\u589e\u5f3a\u6301\u7eed\u6539\u5584\u4e0b\u6e38\u4efb\u52a1\uff1b\u901a\u8fc7\u8fa9\u8bba\u8bb0\u5f55\u589e\u5f3a\u7684\u7279\u5f81\u5e26\u6765\u6700\u5927\u63d0\u5347\uff0c\u5728\u7ebf\u5b89\u5168\u4efb\u52a1\u4e0a\u6bd4\u975e\u589e\u5f3a\u57fa\u7ebf\u63d0\u9ad810.1%", "conclusion": "CFD\u6846\u67b6\u662f\u6709\u6548\u7684LLM\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7ec6\u7c92\u5ea6\u8bc1\u636e\u4ea4\u6362\u7684\u590d\u6742\u6807\u6ce8\u573a\u666f\u4e2d"}}
{"id": "2512.06182", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06182", "abs": "https://arxiv.org/abs/2512.06182", "authors": ["Shuhao Qi", "Qiling Aori", "Luyao Zhang", "Mircea Lazar", "Sofie Haesaert"], "title": "Situation-Aware Interactive MPC Switching for Autonomous Driving", "comment": null, "summary": "To enable autonomous driving in interactive traffic scenarios, various model predictive control (MPC) formulations have been proposed, each employing different interaction models. While higher-fidelity models enable more intelligent behavior, they incur increased computational cost. Since strong interactions are relatively infrequent in traffic, a practical strategy for balancing performance and computational overhead is to invoke an appropriate controller based on situational demands. To achieve this approach, we first conduct a comparative study to assess and hierarchize the interactive capabilities of different MPC formulations. Furthermore, we develop a neural network-based classifier to enable situation-aware switching among controllers with different levels of interactive capability. We demonstrate that this situation-aware switching can both substantially improve overall performance by activating the most advanced interactive MPC in rare but critical situations, and significantly reduce computational load by using a basic MPC in the majority of scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u60c5\u5883\u611f\u77e5\u7684\u63a7\u5236\u5668\u5207\u6362\u7b56\u7565\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4ea4\u4e92\u573a\u666f\u4e2d\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4ea4\u4e92\u573a\u666f\u4e2d\uff0c\u9ad8\u7cbe\u5ea6\u4ea4\u4e92\u6a21\u578b\u80fd\u5b9e\u73b0\u66f4\u667a\u80fd\u884c\u4e3a\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u5f3a\u4ea4\u4e92\u5728\u4ea4\u901a\u4e2d\u76f8\u5bf9\u7f55\u89c1\uff0c\u9700\u8981\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u5f00\u9500\u7684\u5b9e\u7528\u7b56\u7565", "method": "1) \u6bd4\u8f83\u7814\u7a76\u8bc4\u4f30\u4e0d\u540cMPC\u516c\u5f0f\u7684\u4ea4\u4e92\u80fd\u529b\u5e76\u5efa\u7acb\u5c42\u6b21\u7ed3\u6784\uff1b2) \u5f00\u53d1\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u4e0d\u540c\u4ea4\u4e92\u80fd\u529b\u63a7\u5236\u5668\u4e4b\u95f4\u7684\u60c5\u5883\u611f\u77e5\u5207\u6362", "result": "\u60c5\u5883\u611f\u77e5\u5207\u6362\u7b56\u7565\u65e2\u80fd\u901a\u8fc7\u5728\u7f55\u89c1\u4f46\u5173\u952e\u60c5\u51b5\u4e0b\u6fc0\u6d3b\u6700\u5148\u8fdb\u7684\u4ea4\u4e92MPC\u663e\u8457\u63d0\u5347\u6574\u4f53\u6027\u80fd\uff0c\u53c8\u80fd\u901a\u8fc7\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u4f7f\u7528\u57fa\u672cMPC\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d1f\u8f7d", "conclusion": "\u63d0\u51fa\u7684\u60c5\u5883\u611f\u77e5\u63a7\u5236\u5668\u5207\u6362\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u81ea\u52a8\u9a7e\u9a76\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06228", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06228", "abs": "https://arxiv.org/abs/2512.06228", "authors": ["Xuanxin Wu", "Yuki Arase", "Masaaki Nagata"], "title": "Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge", "comment": null, "summary": "Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.", "AI": {"tldr": "\u5229\u7528LLM-as-a-Judge\u81ea\u52a8\u6784\u5efa\u7b56\u7565\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u5e73\u884c\u8bed\u6599\uff0c\u4f7f\u5c0f\u578b\u5f00\u6e90LLM\u5728\u8bcd\u6c47\u7b80\u5316\u4e0a\u8d85\u8d8aGPT-4o", "motivation": "\u53e5\u5b50\u7b80\u5316\u9700\u8981\u6839\u636e\u4e0d\u540c\u5e94\u7528\u91c7\u7528\u4e0d\u540c\u7b56\u7565\uff08\u5982\u4ec5\u66ff\u6362\u590d\u6742\u8bcd\u6c47\u6216\u6574\u4f53\u91cd\u5199\uff09\uff0c\u4f46\u5b9e\u73b0\u7b56\u7565\u9a71\u52a8\u7684\u63a7\u5236\u4ecd\u662f\u5f00\u653e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u5e73\u884c\u8bed\u6599", "method": "\u63d0\u51fa\u57fa\u4e8eLLM-as-a-Judge\u7684\u65b9\u6cd5\u81ea\u52a8\u6784\u5efa\u7b56\u7565\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5b8c\u5168\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u5e73\u884c\u8bed\u6599\uff0c\u4f7f\u7b80\u5316\u7cfb\u7edf\u80fd\u9002\u5e94\u4e0d\u540c\u7b80\u5316\u7b56\u7565", "result": "\u5c0f\u578b\u5f00\u6e90LLM\uff08\u5982Phi-3-mini-3.8B\uff09\u5728\u8bcd\u6c47\u5bfc\u5411\u7b80\u5316\u4e0a\u8d85\u8d8aGPT-4o\uff0c\u5728\u6574\u4f53\u91cd\u5199\u65b9\u9762\u6027\u80fd\u76f8\u5f53\uff0c\u81ea\u52a8\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u5747\u9a8c\u8bc1\u4e86\u6548\u679c", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u5747\u8868\u73b0\u4e00\u81f4\u6539\u8fdb\uff0c\u4e3a\u7b56\u7565\u9a71\u52a8\u7684\u53e5\u5b50\u7b80\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06192", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06192", "abs": "https://arxiv.org/abs/2512.06192", "authors": ["Takahiro Hattori", "Kento Kawaharazuka", "Temma Suzuki", "Keita Yoneda", "Kei Okada"], "title": "REWW-ARM -- Remote Wire-Driven Mobile Robot: Design, Control, and Experimental Validation", "comment": "Accepted on Advanced Intelligent Systems", "summary": "Electronic devices are essential for robots but limit their usable environments. To overcome this, methods excluding electronics from the operating environment while retaining advanced electronic control and actuation have been explored. These include the remote hydraulic drive of electronics-free mobile robots, which offer high reachability, and long wire-driven robot arms with motors consolidated at the base, which offer high environmental resistance. To combine the advantages of both, this study proposes a new system, \"Remote Wire Drive.\" As a proof-of-concept, we designed and developed the Remote Wire-Driven robot \"REWW-ARM\", which consists of the following components: 1) a novel power transmission mechanism, the \"Remote Wire Transmission Mechanism\" (RWTM), the key technology of the Remote Wire Drive; 2) an electronics-free distal mobile robot driven by it; and 3) a motor-unit that generates power and provides electronic closed-loop control based on state estimation via the RWTM. In this study, we evaluated the mechanical and control performance of REWW-ARM through several experiments, demonstrating its capability for locomotion, posture control, and object manipulation both on land and underwater. This suggests the potential for applying the Remote Wire-Driven system to various types of robots, thereby expanding their operational range.", "AI": {"tldr": "\u63d0\u51fa\"\u8fdc\u7a0b\u7ebf\u9a71\u52a8\"\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdc\u7a0b\u7ebf\u4f20\u8f93\u673a\u5236\u5c06\u7535\u673a\u5355\u5143\u4e0e\u65e0\u7535\u5b50\u8bbe\u5907\u8fdc\u7aef\u673a\u5668\u4eba\u5206\u79bb\uff0c\u5b9e\u73b0\u9646\u5730\u548c\u6c34\u4e0b\u64cd\u4f5c", "motivation": "\u7535\u5b50\u8bbe\u5907\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u4f7f\u7528\u73af\u5883\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u7559\u5148\u8fdb\u7535\u5b50\u63a7\u5236\u53c8\u80fd\u5c06\u7535\u5b50\u8bbe\u5907\u6392\u9664\u5728\u64cd\u4f5c\u73af\u5883\u4e4b\u5916\u7684\u65b9\u6cd5", "method": "\u5f00\u53d1\u4e86\u8fdc\u7a0b\u7ebf\u9a71\u52a8\u673a\u5668\u4eba\"REWW-ARM\"\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8fdc\u7a0b\u7ebf\u4f20\u8f93\u673a\u5236(RWTM)\u3001\u65e0\u7535\u5b50\u8bbe\u5907\u8fdc\u7aef\u79fb\u52a8\u673a\u5668\u4eba\u3001\u4ee5\u53ca\u63d0\u4f9b\u95ed\u73af\u63a7\u5236\u7684\u7535\u673a\u5355\u5143", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86REWW-ARM\u5728\u673a\u68b0\u548c\u63a7\u5236\u6027\u80fd\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9646\u5730\u548c\u6c34\u4e0b\u7684\u79fb\u52a8\u3001\u59ff\u6001\u63a7\u5236\u548c\u7269\u4f53\u64cd\u4f5c\u80fd\u529b", "conclusion": "\u8fdc\u7a0b\u7ebf\u9a71\u52a8\u7cfb\u7edf\u6709\u6f5c\u529b\u5e94\u7528\u4e8e\u5404\u79cd\u7c7b\u578b\u7684\u673a\u5668\u4eba\uff0c\u4ece\u800c\u6269\u5c55\u5176\u64cd\u4f5c\u8303\u56f4"}}
{"id": "2512.06239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06239", "abs": "https://arxiv.org/abs/2512.06239", "authors": ["Dhanasekar Sundararaman", "Keying Li", "Wayne Xiong", "Aashna Garg"], "title": "LOCUS: A System and Method for Low-Cost Customization for Universal Specialization", "comment": null, "summary": "We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.", "AI": {"tldr": "LOCUS\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684NLP\u6a21\u578b\u5b9a\u5236\u7ba1\u9053\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u6784\u5efa\uff0c\u5728NER\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4o\u7b49\u57fa\u7ebf\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6210\u672c\u548c\u6a21\u578b\u5927\u5c0f\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfNLP\u6a21\u578b\u5b9a\u5236\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u901a\u7528\u4e13\u4e1a\u5316\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u5b9a\u5236\u66f4\u52a0\u9ad8\u6548\u548c\u7ecf\u6d4e\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u6d41\u7a0b\uff1a1) \u4ece\u5927\u578b\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u6570\u636e\uff1b2) \u901a\u8fc7\u4e0a\u4e0b\u6587\u6570\u636e\u751f\u6210\u5408\u6210\u989d\u5916\u8bad\u7ec3\u6837\u672c\uff1b3) \u4f7f\u7528\u5168\u53c2\u6570\u6216LoRA\u7b49\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728NER\u548c\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff08\u5305\u62ecGPT-4o\uff09\uff0c\u5185\u5b58\u4f18\u5316\u6a21\u578b\u4fdd\u630199%\u5168\u5fae\u8c03\u7cbe\u5ea6\uff0c\u4ec5\u97005%\u5185\u5b58\u5360\u7528\uff0c\u53c2\u6570\u5c11\u4e8eGPT-4o\u76841%\u4f46\u6027\u80fd\u66f4\u597d\u3002", "conclusion": "LOCUS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684NLP\u6a21\u578b\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2512.06198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06198", "abs": "https://arxiv.org/abs/2512.06198", "authors": ["Oussama Sifour", "Soulaimane Berkane", "Abdelhamid Tayebi"], "title": "Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation", "comment": "8 pages", "summary": "This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f7f\u7528IMU\u3001\u4f53\u5750\u6807\u7cfb\u5411\u91cf\u6d4b\u91cf\u548c\u56fa\u5b9a\u951a\u70b9\u8ddd\u79bb\u6d4b\u91cf\u7684\u5355\u6d4b\u8ddd\u8f85\u52a9\u5bfc\u822a\u89c2\u6d4b\u5668\uff0c\u7528\u4e8e\u91cd\u5efa\u521a\u4f53\u5b8c\u6574\u72b6\u6001", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u591a\u4e2a\u4f20\u611f\u5668\u6216\u590d\u6742\u914d\u7f6e\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u4ec5\u9700\u5355\u6d4b\u8ddd\u8f85\u52a9\u7684\u5bfc\u822a\u65b9\u6848\uff0c\u964d\u4f4e\u7cfb\u7edf\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6", "method": "\u91c7\u7528\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\uff1a\u9996\u5148\u6784\u5efa\u6269\u5c55\u7ebf\u6027\u65f6\u53d8\u7cfb\u7edf\u4f30\u8ba1\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u91cd\u529b\u65b9\u5411\uff1b\u7136\u540e\u7ed3\u5408\u4f53\u5750\u6807\u7cfb\u5411\u91cf\u6d4b\u91cf\u91cd\u5efa\u5b8c\u6574\u59ff\u6001\uff1b\u57fa\u4e8e\u4e00\u81f4\u53ef\u89c2\u6027\u6761\u4ef6\u5efa\u7acb\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027", "result": "\u4e09\u7ef4\u8f68\u8ff9\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u51c6\u786e\u4f30\u8ba1\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u59ff\u6001\uff0c\u9a8c\u8bc1\u4e86\u5355\u6d4b\u8ddd\u8f85\u52a9\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u6709\u6548\u5bfc\u822a\u6a21\u5f0f\u7684\u53ef\u884c\u6027", "conclusion": "\u5355\u6d4b\u8ddd\u8f85\u52a9\u5bfc\u822a\u89c2\u6d4b\u5668\u4e3a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4f20\u611f\u5668\u566a\u58f0\u548c\u8f68\u8ff9\u53d8\u5316\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u81ea\u4e3b\u7cfb\u7edf"}}
{"id": "2512.06256", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06256", "abs": "https://arxiv.org/abs/2512.06256", "authors": ["Aniruddha Maiti", "Satya Nimmagadda", "Kartha Veerya Jammuladinne", "Niladri Sengupta", "Ananya Jana"], "title": "Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup", "comment": "accepted to LLM 2025", "summary": "In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.", "AI": {"tldr": "\u4e24\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u76f8\u4e92\u5bf9\u8bdd\uff0c\u65e0\u9700\u5916\u90e8\u8f93\u5165\uff0c\u6700\u7ec8\u4f1a\u9677\u5165\u91cd\u590d\u5faa\u73af\u548c\u6536\u655b\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u5f53\u4e24\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6ca1\u6709\u5916\u90e8\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u76f8\u4e92\u5bf9\u8bdd\u65f6\u4f1a\u53d1\u751f\u4ec0\u4e48\uff0c\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u7684\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u4f7f\u7528Mistral Nemo Base 2407\u548cLlama 2 13B hf\u6a21\u578b\uff0c\u4ece\u4e00\u4e2a\u77ed\u79cd\u5b50\u53e5\u5b50\u5f00\u59cb\uff0c\u8ba9\u4e24\u4e2a\u6a21\u578b\u76f8\u4e92\u8bfb\u53d6\u5bf9\u65b9\u7684\u8f93\u51fa\u5e76\u751f\u6210\u54cd\u5e94\uff0c\u6301\u7eed\u56fa\u5b9a\u6b65\u6570\u3002\u5e94\u7528\u8bcd\u6c47\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u6307\u6807\u6765\u6d4b\u91cf\u5bf9\u8bdd\u4e0e\u521d\u59cb\u79cd\u5b50\u7684\u504f\u79bb\u7a0b\u5ea6\u4ee5\u53ca\u4e24\u4e2a\u6a21\u578b\u8f93\u51fa\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u5927\u591a\u6570\u5bf9\u8bdd\u5f00\u59cb\u65f6\u8fde\u8d2f\uff0c\u4f46\u540e\u6765\u9677\u5165\u91cd\u590d\u3002\u5728\u8bb8\u591a\u8fd0\u884c\u4e2d\uff0c\u4f1a\u51fa\u73b0\u4e00\u4e2a\u77ed\u77ed\u8bed\u5e76\u5728\u591a\u4e2a\u56de\u5408\u4e2d\u91cd\u590d\u3002\u4e00\u65e6\u5f00\u59cb\u91cd\u590d\uff0c\u4e24\u4e2a\u6a21\u578b\u503e\u5411\u4e8e\u4ea7\u751f\u76f8\u4f3c\u7684\u8f93\u51fa\u800c\u4e0d\u662f\u5f15\u5165\u65b0\u7684\u5bf9\u8bdd\u65b9\u5411\uff0c\u5bfc\u81f4\u76f8\u540c\u6216\u76f8\u4f3c\u6587\u672c\u7684\u5faa\u73af\u3002\u5373\u4f7f\u6a21\u578b\u5f88\u5927\u3001\u5355\u72ec\u8bad\u7ec3\u4e14\u6ca1\u6709\u63d0\u793a\u6307\u4ee4\uff0c\u4e5f\u4f1a\u51fa\u73b0\u8fd9\u79cd\u6536\u655b\u884c\u4e3a\u3002", "conclusion": "\u4e24\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u4e2d\u4f1a\u81ea\u53d1\u5730\u6536\u655b\u5230\u91cd\u590d\u5faa\u73af\u72b6\u6001\uff0c\u8fd9\u63ed\u793a\u4e86\u5728\u6ca1\u6709\u5916\u90e8\u5f15\u5bfc\u7684\u60c5\u51b5\u4e0b\uff0c\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u7684\u52a8\u6001\u7279\u6027\u3002"}}
{"id": "2512.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06207", "abs": "https://arxiv.org/abs/2512.06207", "authors": ["Harshil Suthar", "Dipankar Maity"], "title": "Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots", "comment": "Submitted to conference", "summary": "In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u673a\u5668\u4eba\u534f\u4f5c\u6846\u67b6\uff0c\u5176\u4e2d\u7a7a\u4e2d\u673a\u5668\u4eba\u63a2\u7d22\u672a\u77e5\u73af\u5883\u5e76\u9009\u62e9\u6027\u5730\u5c06\u5730\u56fe\u4fe1\u606f\u901a\u8fc7\u5e26\u5bbd\u53d7\u9650\u4fe1\u9053\u4f20\u8f93\u7ed9\u5730\u9762\u673a\u5668\u4eba\uff0c\u4ee5\u4f18\u5316\u5730\u9762\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6210\u672c\u3002", "motivation": "\u5728\u591a\u673a\u5668\u4eba\u56e2\u961f\u4e2d\uff0c\u7a7a\u4e2d\u673a\u5668\u4eba\u9700\u8981\u63a2\u7d22\u672a\u77e5\u73af\u5883\u5e76\u5c06\u5730\u56fe\u4fe1\u606f\u4f20\u8f93\u7ed9\u5730\u9762\u673a\u5668\u4eba\uff0c\u4f46\u901a\u4fe1\u5e26\u5bbd\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u51b3\u5b9a\u4f20\u8f93\u4ec0\u4e48\u4fe1\u606f\u3001\u4f20\u8f93\u591a\u5c11\u4ee5\u53ca\u4f55\u65f6\u4f20\u8f93\uff0c\u540c\u65f6\u8fd8\u8981\u8fdb\u884c\u73af\u5883\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c(VoI)\u51b3\u5b9a\u4f20\u8f93\u4ec0\u4e48\u4fe1\u606f\uff0c\u4f7f\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u51b3\u5b9a\u4f20\u8f93\u591a\u5c11\u4fe1\u606f\uff0c\u901a\u8fc7\u57fa\u4e8e\u6548\u7528\u5206\u6570\u7684\u73af\u5883\u63a2\u7d22\u7b56\u7565\u83b7\u53d6\u989d\u5916\u4fe1\u606f\uff0c\u5e76\u8fdb\u884c\u901a\u4fe1-\u8fd0\u52a8\u6743\u8861\u5206\u6790\u3002", "result": "\u6846\u67b6\u80fd\u591f\u5e73\u8861\u7a7a\u4e2d\u673a\u5668\u4eba\u4f20\u8f93\u7684\u5730\u56fe\u6570\u636e\u603b\u91cf\u4e0e\u5730\u9762\u673a\u5668\u4eba\u5bfc\u822a\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4f18\u5316\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e0b\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u4ef7\u503c\u8bc4\u4f30\u548c\u4f18\u5316\u51b3\u7b56\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u8d44\u6e90\u4e0e\u5bfc\u822a\u6027\u80fd\u7684\u6709\u6548\u5e73\u8861\u3002"}}
{"id": "2512.06266", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06266", "abs": "https://arxiv.org/abs/2512.06266", "authors": ["Chen Yang", "Guangyue Peng", "Jiaying Zhu", "Ran Le", "Ruixiang Feng", "Tao Zhang", "Wei Ruan", "Xiaoqi Liu", "Xiaoxue Cheng", "Xiyun Xu", "Yang Song", "Yanzipeng Gao", "Yiming Jia", "Yun Xing", "Yuntao Wen", "Zekai Wang", "Zhenwei An", "Zhicong Sun", "Zongchao Chen"], "title": "Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models", "comment": null, "summary": "We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.", "AI": {"tldr": "Nanbeige4-3B\u662f\u4e00\u4e2a3B\u53c2\u6570\u7684\u5c0f\u89c4\u6a21\u9ad8\u6027\u80fd\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9884\u8bad\u7ec3\u8c03\u5ea6\u5668\u3001\u6307\u4ee4\u5fae\u8c03\u673a\u5236\u3001\u53cc\u504f\u597d\u84b8\u998f\u548c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u540c\u89c4\u6a21\u6a21\u578b\u5e76\u5ab2\u7f8e\u66f4\u5927\u6a21\u578b\u3002", "motivation": "\u63a8\u52a8\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u8bc1\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u63a5\u8fd1\u5927\u6a21\u578b\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u9884\u8bad\u7ec3\uff1a\u4f7f\u7528FG-WSD\u8c03\u5ea6\u5668\uff08\u7ec6\u7c92\u5ea6\u9884\u70ed-\u7a33\u5b9a-\u8870\u51cf\uff09\u5206\u9636\u6bb5\u4f18\u5316\u6570\u636e\u6df7\u5408\uff1b2. \u6307\u4ee4\u5fae\u8c03\uff1a\u7ed3\u5408\u5ba1\u8bae\u751f\u6210\u4f18\u5316\u548c\u601d\u7ef4\u94fe\u91cd\u6784\u673a\u5236\u63d0\u5347SFT\u6570\u636e\u8d28\u91cf\uff1b3. \u77e5\u8bc6\u84b8\u998f\uff1a\u91c7\u7528\u53cc\u504f\u597d\u84b8\u998f\u65b9\u6cd5\u4ece\u65d7\u8230\u63a8\u7406\u6a21\u578b\u84b8\u998f\u77e5\u8bc6\uff1b4. \u5f3a\u5316\u5b66\u4e60\uff1a\u591a\u9636\u6bb5RL\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u504f\u597d\u5efa\u6a21\u589e\u5f3a\u63a8\u7406\u548c\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNanbeige4-3B\u4e0d\u4ec5\u663e\u8457\u4f18\u4e8e\u540c\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u8fd8\u80fd\u4e0e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u7ade\u4e89\uff0c\u5c55\u793a\u4e86\u5c0f\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u80fd\u8fbe\u5230\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u7ec4\u5408\uff0c\u6210\u529f\u6269\u5c55\u4e86\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u7f29\u653e\u5b9a\u5f8b\u8fb9\u754c\uff0c\u4e3a\u5f00\u53d1\u9ad8\u6027\u80fd\u5c0f\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u6a21\u578b\u6743\u91cd\u5df2\u5728HuggingFace\u5f00\u6e90\u3002"}}
{"id": "2512.06261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06261", "abs": "https://arxiv.org/abs/2512.06261", "authors": ["Taekyung Kim", "Keyvan Majd", "Hideki Okamoto", "Bardh Hoxha", "Dimitra Panagou", "Georgios Fainekos"], "title": "Safe Model Predictive Diffusion with Shielding", "comment": "Project page: https://www.taekyung.me/safe-mpd", "summary": "Generating safe, kinodynamically feasible, and optimal trajectories for complex robotic systems is a central challenge in robotics. This paper presents Safe Model Predictive Diffusion (Safe MPD), a training-free diffusion planner that unifies a model-based diffusion framework with a safety shield to generate trajectories that are both kinodynamically feasible and safe by construction. By enforcing feasibility and safety on all samples during the denoising process, our method avoids the common pitfalls of post-processing corrections, such as computational intractability and loss of feasibility. We validate our approach on challenging non-convex planning problems, including kinematic and acceleration-controlled tractor-trailer systems. The results show that it substantially outperforms existing safety strategies in success rate and safety, while achieving sub-second computation times.", "AI": {"tldr": "Safe MPD\uff1a\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u6269\u6563\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u6563\u6846\u67b6\u4e0e\u5b89\u5168\u5c4f\u969c\uff0c\u751f\u6210\u65e2\u6ee1\u8db3\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u53c8\u5b89\u5168\u7684\u6700\u4f18\u8f68\u8ff9\uff0c\u907f\u514d\u540e\u5904\u7406\u4fee\u6b63\u7684\u5e38\u89c1\u95ee\u9898\u3002", "motivation": "\u4e3a\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u751f\u6210\u5b89\u5168\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u4e14\u6700\u4f18\u7684\u8f68\u8ff9\u662f\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5e38\u91c7\u7528\u540e\u5904\u7406\u4fee\u6b63\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u4e0d\u53ef\u884c\u6027\u548c\u53ef\u884c\u6027\u635f\u5931\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faSafe Model Predictive Diffusion (Safe MPD)\uff0c\u5c06\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u6563\u6846\u67b6\u4e0e\u5b89\u5168\u5c4f\u969c\u7edf\u4e00\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5bf9\u6240\u6709\u6837\u672c\u5f3a\u5236\u6267\u884c\u53ef\u884c\u6027\u548c\u5b89\u5168\u6027\u7ea6\u675f\uff0c\u907f\u514d\u540e\u5904\u7406\u4fee\u6b63\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u975e\u51f8\u89c4\u5212\u95ee\u9898\u4e0a\u9a8c\u8bc1\uff0c\u5305\u62ec\u8fd0\u52a8\u5b66\u548c\u52a0\u901f\u5ea6\u63a7\u5236\u7684\u62d6\u62c9\u673a-\u62d6\u8f66\u7cfb\u7edf\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5b89\u5168\u7b56\u7565\uff0c\u540c\u65f6\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "Safe MPD\u901a\u8fc7\u7edf\u4e00\u6269\u6563\u89c4\u5212\u4e0e\u5b89\u5168\u5c4f\u969c\uff0c\u80fd\u591f\u751f\u6210\u65e2\u5b89\u5168\u53c8\u8fd0\u52a8\u5b66\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u907f\u514d\u4e86\u540e\u5904\u7406\u4fee\u6b63\u7684\u5c40\u9650\u6027\uff0c\u5728\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.06464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06464", "abs": "https://arxiv.org/abs/2512.06464", "authors": ["Akriti Jain", "Aparna Garimella"], "title": "Modeling Contextual Passage Utility for Multihop Question Answering", "comment": "Accepted at IJCNLP-AACL 2025", "summary": "Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7bc7\u7ae0\u6548\u7528\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u7bc7\u7ae0\u91cd\u6392\u5e8f\uff0c\u901a\u8fc7\u5efa\u6a21\u7bc7\u7ae0\u95f4\u4f9d\u8d56\u5173\u7cfb\u63d0\u5347\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u591a\u8df3\u95ee\u7b54\u9700\u8981\u4ece\u591a\u4e2a\u6587\u672c\u7bc7\u7ae0\u4e2d\u8bc6\u522b\u548c\u7efc\u5408\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u8bc4\u4f30\u7bc7\u7ae0\u6548\u7528\uff0c\u5ffd\u7565\u4e86\u591a\u8df3\u63a8\u7406\u7684\u5173\u952e\u65b9\u9762\uff1a\u7bc7\u7ae0\u7684\u6548\u7528\u662f\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\uff0c\u53d7\u5176\u4e0e\u5176\u4ed6\u7bc7\u7ae0\u5173\u7cfb\u7684\u5f71\u54cd\uff08\u662f\u5426\u63d0\u4f9b\u8865\u5145\u4fe1\u606f\u6216\u5f62\u6210\u5173\u952e\u94fe\u63a5\uff09\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5efa\u6a21\u4e0a\u4e0b\u6587\u7bc7\u7ae0\u6548\u7528\uff0c\u8003\u8651\u7bc7\u7ae0\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u5fae\u8c03\u5c0f\u578btransformer\u6a21\u578b\u9884\u6d4b\u591a\u8df3\u95ee\u7b54\u7684\u7bc7\u7ae0\u6548\u7528\u5206\u6570\uff0c\u5229\u7528\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u8f68\u8ff9\u6355\u6349\u7bc7\u7ae0\u4f7f\u7528\u987a\u5e8f\uff0c\u83b7\u5f97\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u57fa\u4e8e\u6548\u7528\u7684\u68c0\u7d22\u7bc7\u7ae0\u8bc4\u5206\u76f8\u6bd4\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u80fd\u5e26\u6765\u6539\u8fdb\u7684\u91cd\u6392\u5e8f\u548c\u4e0b\u6e38\u95ee\u7b54\u6027\u80fd\u3002", "conclusion": "\u5efa\u6a21\u4e0a\u4e0b\u6587\u7bc7\u7ae0\u6548\u7528\u5bf9\u4e8e\u591a\u8df3\u95ee\u7b54\u5f88\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7bc7\u7ae0\u91cd\u6392\u5e8f\u548c\u95ee\u7b54\u6027\u80fd\u3002"}}
{"id": "2512.06423", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06423", "abs": "https://arxiv.org/abs/2512.06423", "authors": ["Leonardo F. Dos Santos", "Elisa G. Vergamini", "C\u00edcero Zanette", "Lucca Maitan", "Thiago Boaventura"], "title": "Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking", "comment": "This is the author's version of the paper accepted for publication in the 2025 International Conference on Advanced Robotics (ICAR). The final version will be available at IEEE Xplore", "summary": "This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7aef\u53e3\u54c8\u5bc6\u987f\u7cfb\u7edf\u7684\u963b\u6297\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u6307\u6807\uff0c\u5305\u62ec\u65e0\u6e90\u6027\u6761\u4ef6\u548c\u963b\u6297\u4fdd\u771f\u5ea6\u5ea6\u91cf", "motivation": "\u9700\u8981\u6807\u51c6\u5316\u7684\u963b\u6297\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7b1b\u5361\u5c14\u7a7a\u95f4\u4e2d\u7684\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u5668\u963b\u6297\u63a7\u5236", "method": "\u5f15\u5165\u56e0\u679c\u4e00\u81f4\u7684\u7aef\u53e3\u54c8\u5bc6\u987f\u6a21\u578b\uff0c\u63a8\u5bfc\u51fa\u53ef\u5fae\u5206\u3001\u4e0d\u4f9d\u8d56\u529b/\u529b\u77e9\u4f20\u611f\u7684n\u81ea\u7531\u5ea6\u65e0\u6e90\u6027\u6761\u4ef6\uff0c\u5e76\u57fa\u4e8e\u81ea\u7531\u8fd0\u52a8\u9636\u8dc3\u54cd\u5e94\u529f\u7387\u5b9a\u4e49\u963b\u6297\u4fdd\u771f\u5ea6\u5ea6\u91cf", "result": "\u5728Gazebo\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u516d\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c\u56db\u8db3\u673a\u5668\u4eba\u817f\u90e8\uff0c\u8bc1\u660e\u7aef\u53e3\u54c8\u5bc6\u987f\u6846\u67b6\u9002\u7528\u4e8e\u6807\u51c6\u5316\u963b\u6297\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5", "conclusion": "\u7aef\u53e3\u54c8\u5bc6\u987f\u7cfb\u7edf\u4e3a\u963b\u6297\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u63d0\u51fa\u7684\u6307\u6807\u80fd\u591f\u8bc4\u4f30\u65e0\u6e90\u6027\u548c\u52a8\u6001\u89e3\u8026\u6027\u80fd"}}
{"id": "2512.06476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06476", "abs": "https://arxiv.org/abs/2512.06476", "authors": ["Akriti Jain", "Aparna Garimella"], "title": "Knowing What's Missing: Assessing Information Sufficiency in Question Answering", "comment": null, "summary": "Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.", "AI": {"tldr": "\u63d0\u51faIdentify-then-Verify\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u8bc6\u522b\u7f3a\u5931\u4fe1\u606f\u518d\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u66f4\u53ef\u9760\u5730\u5224\u65ad\u4e0a\u4e0b\u6587\u662f\u5426\u8db3\u591f\u56de\u7b54\u95ee\u9898", "motivation": "\u73b0\u6709\u7b80\u5355\u63d0\u793a\u7b56\u7565\u5728\u5904\u7406\u9700\u8981\u63a8\u7406\u7684\u95ee\u9898\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u5224\u65ad\u4e0a\u4e0b\u6587\u662f\u5426\u5305\u542b\u8db3\u591f\u4fe1\u606f\u6765\u56de\u7b54\u95ee\u9898", "method": "\u63d0\u51fa\u7ed3\u6784\u5316Identify-then-Verify\u6846\u67b6\uff1a1) \u751f\u6210\u591a\u4e2a\u5173\u4e8e\u7f3a\u5931\u4fe1\u606f\u7684\u5047\u8bbe\u5e76\u5efa\u7acb\u8bed\u4e49\u5171\u8bc6\uff1b2) \u6267\u884c\u5173\u952e\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u5f3a\u5236\u6a21\u578b\u91cd\u65b0\u68c0\u67e5\u6e90\u6587\u672c\u6765\u786e\u8ba4\u4fe1\u606f\u662f\u5426\u771f\u6b63\u7f3a\u5931", "result": "\u5728\u591a\u4e2a\u591a\u8df3\u548c\u4e8b\u5b9e\u6027QA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u5145\u5206\u6027\u5224\u65ad\uff0c\u540c\u65f6\u6e05\u6670\u8868\u8fbe\u4fe1\u606f\u5dee\u8ddd", "conclusion": "\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u8bc1\u660e\u5176\u5173\u4e8e\u7f3a\u5931\u4fe1\u606f\u7684\u5224\u65ad\uff0c\u8be5\u6846\u67b6\u80fd\u63d0\u9ad8\u5145\u5206\u6027\u5efa\u6a21\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u6784\u5efa\u53ef\u9760\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u65b9\u6cd5"}}
{"id": "2512.06444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06444", "abs": "https://arxiv.org/abs/2512.06444", "authors": ["Xuehui Ma", "Shiliang Zhang", "Zhiyong Sun"], "title": "Fault Tolerant Control of Mecanum Wheeled Mobile Robots", "comment": null, "summary": "Mecanum wheeled mobile robots (MWMRs) are highly susceptible to actuator faults that degrade performance and risk mission failure. Current fault tolerant control (FTC) schemes for MWMRs target complete actuator failures like motor stall, ignoring partial faults e.g., in torque degradation. We propose an FTC strategy handling both fault types, where we adopt posterior probability to learn real-time fault parameters. We derive the FTC law by aggregating probability-weighed control laws corresponding to predefined faults. This ensures the robustness and safety of MWMR control despite varying levels of fault occurrence. Simulation results demonstrate the effectiveness of our FTC under diverse scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5904\u7406\u9ea6\u514b\u7eb3\u59c6\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u5b8c\u5168\u548c\u90e8\u5206\u6267\u884c\u5668\u6545\u969c\u7684\u5bb9\u9519\u63a7\u5236\u7b56\u7565\uff0c\u91c7\u7528\u540e\u9a8c\u6982\u7387\u5b66\u4e60\u5b9e\u65f6\u6545\u969c\u53c2\u6570\uff0c\u901a\u8fc7\u6982\u7387\u52a0\u6743\u63a7\u5236\u5f8b\u786e\u4fdd\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u9ea6\u514b\u7eb3\u59c6\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u5bb9\u6613\u53d7\u5230\u6267\u884c\u5668\u6545\u969c\u5f71\u54cd\uff0c\u73b0\u6709\u5bb9\u9519\u63a7\u5236\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u5b8c\u5168\u6545\u969c\uff08\u5982\u7535\u673a\u5835\u8f6c\uff09\uff0c\u5ffd\u7565\u4e86\u90e8\u5206\u6545\u969c\uff08\u5982\u626d\u77e9\u9000\u5316\uff09\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u4e24\u79cd\u6545\u969c\u7c7b\u578b\u7684\u7b56\u7565\u3002", "method": "\u91c7\u7528\u540e\u9a8c\u6982\u7387\u5b66\u4e60\u5b9e\u65f6\u6545\u969c\u53c2\u6570\uff0c\u901a\u8fc7\u805a\u5408\u9884\u5b9a\u4e49\u6545\u969c\u5bf9\u5e94\u7684\u6982\u7387\u52a0\u6743\u63a7\u5236\u5f8b\u6765\u63a8\u5bfc\u5bb9\u9519\u63a7\u5236\u5f8b\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u6545\u969c\u53d1\u751f\u6c34\u5e73\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u5bb9\u9519\u63a7\u5236\u7b56\u7565\u5728\u591a\u79cd\u573a\u666f\u4e0b\u90fd\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5904\u7406\u5b8c\u5168\u548c\u90e8\u5206\u6267\u884c\u5668\u6545\u969c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bb9\u9519\u63a7\u5236\u7b56\u7565\u80fd\u591f\u540c\u65f6\u5904\u7406\u9ea6\u514b\u7eb3\u59c6\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5b8c\u5168\u548c\u90e8\u5206\u6267\u884c\u5668\u6545\u969c\uff0c\u901a\u8fc7\u6982\u7387\u52a0\u6743\u65b9\u6cd5\u786e\u4fdd\u4e86\u7cfb\u7edf\u5728\u5404\u79cd\u6545\u969c\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2512.06483", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06483", "abs": "https://arxiv.org/abs/2512.06483", "authors": ["Elias-Leander Ahlers", "Witold Brunsmann", "Malte Schilling"], "title": "Classifying German Language Proficiency Levels Using Large Language Models", "comment": "Accepted at 3rd International Conference on Foundation and Large Language Models (FLLM2025), Vienna (Austria)", "summary": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u5fb7\u8bed\u6587\u672c\u6309CEFR\u8bed\u8a00\u6c34\u5e73\u5206\u7c7b\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u591a\u79cd\u65b9\u6cd5\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u5bf9\u6559\u80b2\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u6839\u636e\u5b66\u4e60\u8005\u9700\u6c42\u63d0\u4f9b\u5b9a\u5236\u5316\u6559\u5b66\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u5fb7\u8bed\u6587\u672c\u6309\u7167\u6b27\u6d32\u8bed\u8a00\u5171\u540c\u53c2\u8003\u6846\u67b6\uff08CEFR\uff09\u8fdb\u884c\u6c34\u5e73\u5206\u7c7b\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u3002", "method": "1. \u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\uff1a\u7ed3\u5408\u591a\u4e2a\u73b0\u6709CEFR\u6807\u6ce8\u8bed\u6599\u5e93\u4e0e\u5408\u6210\u6570\u636e\uff1b2. \u8bc4\u4f30\u591a\u79cd\u65b9\u6cd5\uff1a\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u3001\u5fae\u8c03LLaMA-3-8B-Instruct\u6a21\u578b\u3001\u57fa\u4e8e\u63a2\u9488\u7684\u65b9\u6cd5\uff08\u5229\u7528LLM\u5185\u90e8\u795e\u7ecf\u72b6\u6001\u8fdb\u884c\u5206\u7c7b\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86LLMs\u5728CEFR\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684CEFR\u5206\u7c7b\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u4e3a\u8bed\u8a00\u6559\u80b2\u63d0\u4f9b\u6709\u6548\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2512.06486", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06486", "abs": "https://arxiv.org/abs/2512.06486", "authors": ["Wanru Gong", "Xinyi Zheng", "Xiaopeng Yang", "Xiaoqing Zhu"], "title": "Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains", "comment": null, "summary": "Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.\n  For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.", "AI": {"tldr": "\u63d0\u51faECIM\u7b97\u6cd5\uff0c\u901a\u8fc7\u71b5\u63a7\u5236\u548c\u5185\u5728\u52a8\u673a\u7ed3\u5408\u89e3\u51b3PPO\u7cfb\u5217\u7b97\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e2d\u8fc7\u65e9\u6536\u655b\u7684\u95ee\u9898\uff0c\u5728\u4e0d\u540c\u5730\u5f62\u4e0a\u83b7\u5f97\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u80fd\u6548\u3002", "motivation": "PPO\u7cfb\u5217\u7b97\u6cd5\u5728\u8bad\u7ec3\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u65f6\u867d\u7136\u7a33\u5b9a\u4e14\u6837\u672c\u9ad8\u6548\uff0c\u4f46\u7ecf\u5e38\u8fc7\u65e9\u6536\u655b\uff0c\u5bfc\u81f4\u8fd0\u52a8\u6027\u80fd\u6b21\u4f18\u548c\u4efb\u52a1\u8868\u73b0\u4e0b\u964d\u3002", "method": "\u63d0\u51faEntropy-Controlled Intrinsic Motivation (ECIM)\u7b97\u6cd5\uff0c\u7ed3\u5408\u5185\u5728\u52a8\u673a\u548c\u81ea\u9002\u5e94\u63a2\u7d22\uff0c\u901a\u8fc7\u71b5\u63a7\u5236\u51cf\u5c11\u8fc7\u65e9\u6536\u655b\u95ee\u9898\u3002", "result": "\u5728Isaac Gym\u7684\u516d\u79cd\u5730\u5f62\u4e0a\u6d4b\u8bd5\uff0c\u4efb\u52a1\u5956\u52b1\u63d0\u53474-12%\uff0c\u8eab\u4f53\u4fef\u4ef0\u632f\u8361\u5cf0\u503c\u51cf\u5c1123-29%\uff0c\u5173\u8282\u52a0\u901f\u5ea6\u964d\u4f4e20-32%\uff0c\u5173\u8282\u626d\u77e9\u6d88\u8017\u51cf\u5c1111-20%\u3002", "conclusion": "ECIM\u901a\u8fc7\u71b5\u63a7\u5236\u548c\u5185\u5728\u52a8\u673a\u63a7\u5236\uff0c\u5728\u4e0d\u540c\u5730\u5f62\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u56db\u8db3\u8fd0\u52a8\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u80fd\u91cf\u6d88\u8017\uff0c\u662f\u590d\u6742\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u5b9e\u7528\u9009\u62e9\u3002"}}
{"id": "2512.06515", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06515", "abs": "https://arxiv.org/abs/2512.06515", "authors": ["Somnath Banerjee", "Sayan Layek", "Sayantan Adak", "Mykola Pechenizkiy", "Animesh Mukherjee", "Rima Hazra"], "title": "ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models", "comment": null, "summary": "Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned \"harm vector\" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.", "AI": {"tldr": "ProSocialAlign\uff1a\u4e00\u4e2a\u6d4b\u8bd5\u65f6\u53c2\u6570\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u8bcd\u5178\u7ea6\u675f\u751f\u6210\u548c\u65b9\u5411\u8c03\u8282\u673a\u5236\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u5171\u9e23\u548c\u4ef7\u503c\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8303\u5f0f\u5728\u60c5\u611f\u6fc0\u70c8\u6216\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\uff1a\u5355\u7eaf\u62d2\u7edd\u53ef\u80fd\u758f\u8fdc\u7528\u6237\uff0c\u800c\u7b80\u5355\u987a\u4ece\u53c8\u4f1a\u653e\u5927\u98ce\u9669\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u786e\u4fdd\u5b89\u5168\u53c8\u80fd\u4fdd\u6301\u540c\u7406\u5fc3\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faProSocialAlign\u6846\u67b6\uff0c\u91c7\u7528\u8bcd\u5178\u7ea6\u675f\u751f\u6210\uff1a\u9996\u5148\u5e94\u7528\u786c\u7ea6\u675f\u6d88\u9664\u6709\u5bb3\u5ef6\u7eed\uff0c\u7136\u540e\u5728\u5b89\u5168\u96c6\u5408\u5185\u4f18\u5316\u4eb2\u793e\u4f1a\u8d28\u91cf\u3002\u7ed3\u5408\u65b9\u5411\u8c03\u8282\u673a\u5236\uff08\u5728\u53c2\u6570\u7a7a\u95f4\u51cf\u53bb\u5b66\u4e60\u7684\"\u4f24\u5bb3\u5411\u91cf\"\uff09\u548c\u504f\u597d\u611f\u77e5\u81ea\u56de\u5f52\u5956\u52b1\u5efa\u6a21\uff08\u901a\u8fc7\u68af\u5ea6\u51b2\u7a81\u89e3\u51b3\u8054\u5408\u8bad\u7ec3\u591a\u5c5e\u6027\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e0d\u5b89\u5168\u6cc4\u9732\uff0c\u63d0\u5347\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u5ea6\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u6709\u5f3a\u52b2\u63d0\u5347\u3002", "conclusion": "ProSocialAlign\u4e3a\u63a8\u7406\u65f6\u751f\u6210\u4e0a\u4e0b\u6587\u654f\u611f\u3001\u5b89\u5168\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u54cd\u5e94\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u6a21\u5757\u5316\u7684\u57fa\u7840\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u60c5\u611f\u5171\u9e23\u80fd\u529b\u3002"}}
{"id": "2512.06517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06517", "abs": "https://arxiv.org/abs/2512.06517", "authors": ["Shifa Sulaiman", "Akash Bachhar", "Ming Shen", "Simon B\u00f8gh"], "title": "Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments", "comment": null, "summary": "Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5047\u80a2\u624b\u7684\u89c6\u89c9\u5f15\u5bfc\u6293\u53d6\u7b97\u6cd5\uff0c\u7ed3\u5408\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u5b9e\u73b0\u7075\u5de7\u64cd\u4f5c", "motivation": "\u5047\u80a2\u6280\u672f\u9700\u8981\u589e\u5f3a\u7075\u5de7\u6027\u548c\u81ea\u4e3b\u6027\uff0c\u89c6\u89c9\u65b9\u6cd5\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u7269\u4f53\u4ea4\u4e92", "method": "\u4f7f\u7528BVH\u89c6\u89c9\u7b97\u6cd5\u5206\u5272\u7269\u4f53\u5e76\u5b9a\u4e49\u8fb9\u754c\u6846\uff0c\u901a\u8fc7RRT*\u7b97\u6cd5\u751f\u6210\u5019\u9009\u8f68\u8ff9\uff0c\u57fa\u4e8e\u6700\u5c0f\u6b27\u6c0f\u8ddd\u79bb\u9009\u62e9\u6307\u5c16\u672b\u7aef\u59ff\u6001\uff0c\u91c7\u7528DLS\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u8ba1\u7b97\u5173\u8282\u89d2\u5ea6", "result": "\u5728\u4eff\u771f\u548cLinker Hand O7\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6bcf\u6839\u624b\u6307\u7684\u72ec\u7acb\u6293\u53d6\u89c4\u5212\u548c\u5b9e\u65f6\u9002\u5e94\u6027", "conclusion": "\u63d0\u51fa\u7684\u6a21\u5757\u5316\u7ba1\u9053\u652f\u6301\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u9002\u5e94\u6027\u6293\u53d6\u89c4\u5212\uff0c\u4e3a\u667a\u80fd\u5047\u80a2\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06586", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06586", "abs": "https://arxiv.org/abs/2512.06586", "authors": ["Mikhail Zimin", "Milyausha Shamsutdinova", "Georgii Andriushchenko"], "title": "Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract", "comment": null, "summary": "Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.", "AI": {"tldr": "\u5f00\u53d1\u4e86AlignRuScore\uff0c\u8fd9\u662f\u9488\u5bf9\u4fc4\u8bed\u6587\u672c\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u901a\u8fc7\u5fae\u8c03RuBERT\u6a21\u578b\u5e76\u5229\u7528\u7ffb\u8bd1\u6570\u636e\u96c6\u5b9e\u73b0", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u4fc4\u8bed\u6587\u672c\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u73b0\u6709\u5de5\u5177\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\u8bed\u6599\u5e93\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d", "method": "\u901a\u8fc7\u5fae\u8c03RuBERT\u57fa\u7840\u7684\u5bf9\u9f50\u6a21\u578b\uff0c\u6dfb\u52a0\u4efb\u52a1\u7279\u5b9a\u7684\u5206\u7c7b\u548c\u56de\u5f52\u5934\uff0c\u4f7f\u7528\u4fc4\u8bed\u548c\u7ffb\u8bd1\u7684\u82f1\u8bed\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3", "result": "\u6210\u529f\u5c06\u7edf\u4e00\u7684\u5bf9\u9f50\u5ea6\u91cf\u6807\u51c6\u79fb\u690d\u5230\u4fc4\u8bed\uff0c\u4e3a\u9c81\u68d2\u7684\u591a\u8bed\u8a00\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840", "conclusion": "\u53d1\u5e03\u4e86\u7ffb\u8bd1\u8bed\u6599\u5e93\u3001\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u4ee3\u7801\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u5bf9\u9f50\u5ea6\u91cf\u6807\u51c6\u53ef\u4ee5\u6210\u529f\u79fb\u690d\u5230\u4fc4\u8bed"}}
{"id": "2512.06524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06524", "abs": "https://arxiv.org/abs/2512.06524", "authors": ["Saekwang Nam", "Bowen Deng", "Loong Yi Lee", "Jonathan M. Rossiter", "Nathan F. Lepora"], "title": "TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping", "comment": "Accepted in IEEE Robotics Automation Letters. S. Nam, B. Deng co-first authors", "summary": "We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eFin-Ray\u7ed3\u6784\u7684\u89e6\u89c9\u4f20\u611f\u5668\u624b\u6307\uff0c\u901a\u8fc7\u95f4\u63a5\u611f\u77e5\u65b9\u6cd5\u540c\u65f6\u68c0\u6d4b\u63a5\u89e6\u4f4d\u7f6e\u548c\u538b\u5165\u6df1\u5ea6\uff0c\u4f7f\u7528\u5185\u90e8\u6444\u50cf\u5934\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u53d8\u5f62\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u89e6\u89c9\u611f\u77e5\u3002", "motivation": "\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7ed3\u6784\u63d0\u4f9b\u8f7b\u91cf\u3001\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u89e6\u89c9\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u4f20\u611f\u5668\u9700\u8981\u8fdc\u79bb\u63a5\u89e6\u754c\u9762\u7684\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u63a5\u89e6\u4f4d\u7f6e\u548c\u6df1\u5ea6\u68c0\u6d4b\u3002", "method": "\u8bbe\u8ba1\u5e26\u6709\u94f0\u94fe\u673a\u6784\u7684Fin-Ray\u624b\u6307\uff0c\u5c06\u8f6f\u4f53\u7ed3\u6784\u7684\u53d8\u5f62\u548c\u4f4d\u79fb\u4fe1\u606f\u4f20\u9012\u5230\u5e95\u90e8\u6a2a\u6881\u4e0a\u7684\u6807\u8bb0\u9488\u9635\u5217\uff1b\u4f7f\u7528\u5185\u90e8\u6444\u50cf\u5934\u6355\u6349\u53d8\u5f62\u6a21\u5f0f\uff0c\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u56fe\u50cf\u6765\u63a8\u65ad\u63a5\u89e6\u6761\u4ef6\uff1b\u4f18\u5316\u9488\u9635\u5217\u914d\u7f6e\u548c\u94f0\u94fe\u65b9\u5411\u3002", "result": "\u5b9e\u73b0\u4e860.1mm\u6df1\u5ea6\u7cbe\u5ea6\u548c2mm\u4f4d\u7f6e\u68c0\u6d4b\u7cbe\u5ea6\uff1b\u611f\u77e5\u7cfb\u7edf\u5bf9\u4e0d\u540c\u5f62\u72b6\u548c\u5c3a\u5bf8\u7684\u538b\u5934\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff1b\u5728\u4e0d\u786e\u5b9a\u6293\u53d6\u4f4d\u7f6e\u7684\u62fe\u653e\u4efb\u52a1\u4e2d\uff0c\u89e6\u89c9\u53cd\u9988\u663e\u8457\u63d0\u9ad8\u4e86\u653e\u7f6e\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7ed3\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e6\u89c9\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f20\u611f\u5668\u9700\u8981\u8fdc\u79bb\u63a5\u89e6\u754c\u9762\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u95f4\u63a5\u611f\u77e5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u63a5\u89e6\u68c0\u6d4b\u548c\u6df1\u5ea6\u6d4b\u91cf\u3002"}}
{"id": "2512.06656", "categories": ["cs.CL", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.06656", "abs": "https://arxiv.org/abs/2512.06656", "authors": ["Kwabena Yamoah", "Cass Dykeman"], "title": "The Online Discourse of Virtual Reality and Anxiety", "comment": "Three tables and two figures. Unfortunately, I did not formally register the dataset prior to conducting the analysis", "summary": "VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u8bed\u6599\u5e93\u8bed\u8a00\u5b66\u65b9\u6cd5\u5206\u6790\u5728\u7ebf\u8ba8\u8bba\u4e2d\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u4e0e\u7126\u8651\u76f8\u5173\u7684\u8bdd\u9898\uff0c\u53d1\u73b0VR\u3001Oculus\u548c\u5934\u6234\u8bbe\u5907\u662f\u6700\u5e38\u8ba8\u8bba\u7684\u8bcd\u6c47\uff0c\u63ed\u793a\u4e86VR\u7cfb\u7edf\u53d1\u5c55\u548c\u7528\u6237\u4f53\u9a8c\u7684\u5173\u6ce8\u70b9\u3002", "motivation": "VR\u5df2\u88ab\u7528\u4e8e\u6cbb\u7597\u5e7f\u6cdb\u6027\u7126\u8651\u75c7\u548c\u793e\u4ea4\u7126\u8651\u75c7\u7b49\u4e34\u5e8a\u95ee\u9898\uff0c\u4e3a\u60a3\u8005\u798f\u7949\u548c\u62a4\u7406\u521b\u9020\u4e86\u65b0\u9014\u5f84\u3002\u4e86\u89e3\u7528\u6237\u5bf9\u8be5\u6280\u672f\u7684\u5728\u7ebf\u8ba8\u8bba\u53ef\u4ee5\u8fdb\u4e00\u6b65\u652f\u6301\u5176\u7597\u6548\u3002", "method": "\u91c7\u7528\u8bed\u6599\u5e93\u8bed\u8a00\u5b66\u65b9\u6cd5\uff0c\u4f7f\u7528Sketch Engine\u8f6f\u4ef6\u5206\u6790\u82f1\u8bed\u8d8b\u52bf\u8bed\u6599\u5e93\uff0c\u8bc6\u522bVR\u4e0e\u7126\u8651\u5b50\u8bed\u6599\u5e93\u4e2d\u9ad8\u9891\u8bcd\u6c47\u53ca\u5176\u642d\u914d\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0VR\u3001Oculus\u548c\u5934\u6234\u8bbe\u5907\u662f\u6700\u5e38\u8ba8\u8bba\u7684\u8bcd\u6c47\uff0c\u53cd\u6620\u4e86\u5bf9\u865a\u62df\u7cfb\u7edf\u53d1\u5c55\u548c\u7269\u7406\u8bbe\u5907\u7684\u5173\u6ce8\u3002\u4ecb\u8bcd\u77ed\u8bed\u642d\u914d\u5982\"of virtual reality\"\u3001\"in virtual reality\"\u548c\"for virtual reality\"\u5206\u522b\u4e0e\u8bbe\u8ba1\u3001\u4f53\u9a8c\u548c\u53d1\u5c55\u76f8\u5173\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63d0\u4f9b\u4e86\u5173\u4e8eVR\u4e0e\u7126\u8651\u5728\u4e00\u822c\u8ba8\u8bba\u4e2d\u5982\u4f55\u88ab\u8ba8\u8bba\u7684\u65b0\u89c6\u89d2\uff0c\u4e3a\u901a\u8fc7\u53d1\u5c55\u548c\u53ef\u8bbf\u95ee\u6027\u652f\u6301\u54a8\u8be2\u9700\u6c42\u63d0\u4f9b\u4e86\u672a\u6765\u673a\u4f1a\u7684\u8def\u5f84\u3002"}}
{"id": "2512.06558", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06558", "abs": "https://arxiv.org/abs/2512.06558", "authors": ["Md Mofijul Islam", "Alexi Gladstone", "Sujan Sarker", "Ganesh Nanduru", "Md Fahim", "Keyan Du", "Aman Chadha", "Tariq Iqbal"], "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction", "comment": "14 pages, 7 figures, accepted at the ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026", "summary": "As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86Refer360\u6570\u636e\u96c6\u548cMuRes\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u5bf9\u5177\u8eab\u6307\u4ee3\u8868\u8fbe\u7684\u7406\u89e3\u80fd\u529b", "motivation": "\u673a\u5668\u4eba\u9700\u8981\u7406\u89e3\u5177\u8eab\u7684\u4eba\u7c7b\u6307\u4ee4\u4ee5\u5b9e\u73b0\u76f4\u89c2\u6d41\u7545\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u89c6\u89d2\u504f\u5dee\u3001\u5355\u89c6\u89d2\u6536\u96c6\u3001\u975e\u8bed\u8a00\u624b\u52bf\u8986\u76d6\u4e0d\u8db3\u3001\u4e3b\u8981\u5173\u6ce8\u5ba4\u5185\u73af\u5883\u7b49\u95ee\u9898", "method": "1) \u521b\u5efaRefer360\u6570\u636e\u96c6\uff1a\u5927\u89c4\u6a21\u5177\u8eab\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u4ea4\u4e92\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5ba4\u5185\u5916\u591a\u79cd\u89c6\u89d2\uff1b2) \u63d0\u51faMuRes\u6a21\u5757\uff1a\u591a\u6a21\u6001\u5f15\u5bfc\u6b8b\u5dee\u6a21\u5757\uff0c\u63d0\u53d6\u6a21\u6001\u7279\u5b9a\u4fe1\u53f7\u5e76\u589e\u5f3a\u9884\u8bad\u7ec3\u8868\u793a", "result": "\u5728\u56db\u4e2aHRI\u6570\u636e\u96c6\uff08\u5305\u62ecRefer360\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u65e0\u6cd5\u5168\u9762\u6355\u6349\u5177\u8eab\u4ea4\u4e92\uff0c\u4f46\u4f7f\u7528MuRes\u589e\u5f3a\u540e\u6027\u80fd\u6301\u7eed\u63d0\u5347", "conclusion": "Refer360\u6210\u4e3a\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5f15\u5bfc\u6b8b\u5dee\u5b66\u4e60\u6709\u6f5c\u529b\u63a8\u8fdb\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u5bf9\u5177\u8eab\u6307\u4ee3\u8868\u8fbe\u7684\u7406\u89e3"}}
{"id": "2512.06679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06679", "abs": "https://arxiv.org/abs/2512.06679", "authors": ["Smitha Muthya Sudheendra", "Mani Deep Cherukuri", "Jaideep Srivastava"], "title": "CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis", "comment": null, "summary": "Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.", "AI": {"tldr": "CMV-Fuse\u662f\u4e00\u4e2a\u8de8\u6a21\u6001\u89c6\u56fe\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u62bd\u8c61\u610f\u4e49\u8868\u793a\u3001\u6210\u5206\u53e5\u6cd5\u3001\u4f9d\u5b58\u53e5\u6cd5\u548c\u8bed\u4e49\u6ce8\u610f\u529b\u56db\u79cd\u8bed\u8a00\u89c6\u89d2\uff0c\u6a21\u62df\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u901a\u5e38\u5b64\u7acb\u5730\u5229\u7528\u5355\u4e00\u8bed\u8a00\u89c6\u89d2\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u591a\u79cd\u7ed3\u6784\u8868\u5f81\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u591a\u89c6\u89d2\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCMV-Fuse\u6846\u67b6\uff0c\u7cfb\u7edf\u6574\u5408\u56db\u79cd\u8bed\u8a00\u89c6\u89d2\uff1a\u62bd\u8c61\u610f\u4e49\u8868\u793a\u3001\u6210\u5206\u53e5\u6cd5\u3001\u4f9d\u5b58\u53e5\u6cd5\u548c\u8bed\u4e49\u6ce8\u610f\u529b\uff0c\u5e76\u878d\u5165\u5916\u90e8\u77e5\u8bc6\u3002\u901a\u8fc7\u5206\u5c42\u95e8\u63a7\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\uff0c\u5728\u5c40\u90e8\u53e5\u6cd5\u3001\u4e2d\u95f4\u8bed\u4e49\u548c\u5168\u5c40\u77e5\u8bc6\u4e09\u4e2a\u5c42\u6b21\u8fdb\u884c\u878d\u5408\uff0c\u540c\u65f6\u91c7\u7528\u7ed3\u6784\u611f\u77e5\u7684\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u786e\u4fdd\u8868\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5206\u6790\u663e\u793a\u6bcf\u79cd\u8bed\u8a00\u89c6\u89d2\u90fd\u5bf9\u66f4\u9c81\u68d2\u7684\u60c5\u611f\u5206\u6790\u6709\u8d21\u732e\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u7684\u591a\u89c6\u89d2\u878d\u5408\u65b9\u6cd5\uff0cCMV-Fuse\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u7ed3\u6784\u6a21\u5f0f\u548c\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4e3a\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06571", "abs": "https://arxiv.org/abs/2512.06571", "authors": ["Zifan Xu", "Myoungkyu Seo", "Dongmyeong Lee", "Hao Fu", "Jiaheng Hu", "Jiaxun Cui", "Yuqian Jiang", "Zhihan Wang", "Anastasiia Brund", "Joydeep Biswas", "Peter Stone"], "title": "Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input", "comment": null, "summary": "Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a \"teacher\" policy is trained with ground truth state information and the \"student\" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56db\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u53cc\u8db3\u673a\u5668\u4eba\u80fd\u591f\u5728\u611f\u77e5\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u8fde\u7eed\u8e22\u7403", "motivation": "\u53cc\u8db3\u673a\u5668\u4eba\u8e22\u7403\u9762\u4e34\u5feb\u901f\u6446\u817f\u3001\u5355\u811a\u652f\u6491\u7a33\u5b9a\u6027\u3001\u566a\u58f0\u611f\u77e5\u548c\u5916\u90e8\u5e72\u6270\u7b49\u6311\u6218\uff0c\u9700\u8981\u9c81\u68d2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u6269\u5c55\u5e08\u751f\u8bad\u7ec3\u6846\u67b6\u4e3a\u56db\u9636\u6bb5\uff1a1)\u957f\u8ddd\u79bb\u8ffd\u7403(\u6559\u5e08)\uff1b2)\u5b9a\u5411\u8e22\u7403(\u6559\u5e08)\uff1b3)\u7b56\u7565\u84b8\u998f(\u5b66\u751f)\uff1b4)\u9002\u5e94\u4e0e\u7cbe\u70bc(\u5b66\u751f)\u3002\u91c7\u7528\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u3001\u771f\u5b9e\u566a\u58f0\u5efa\u6a21\u548c\u5728\u7ebf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u8e22\u7403\u51c6\u786e\u6027\u548c\u8fdb\u7403\u6210\u529f\u7387\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u3001\u566a\u58f0\u5efa\u6a21\u548c\u9002\u5e94\u9636\u6bb5\u7684\u5fc5\u8981\u6027", "conclusion": "\u5efa\u7acb\u4e86\u5728\u611f\u77e5\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u5b66\u4e60\u9c81\u68d2\u8fde\u7eed\u8e22\u7403\u7684\u7cfb\u7edf\uff0c\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u7684\u89c6\u89c9\u8fd0\u52a8\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u57fa\u51c6\u4efb\u52a1"}}
{"id": "2512.06681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06681", "abs": "https://arxiv.org/abs/2512.06681", "authors": ["Amartya Hatua"], "title": "Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis", "comment": null, "summary": "We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.", "AI": {"tldr": "GPT-2\u60c5\u611f\u5206\u6790\u673a\u5236\u7814\u7a76\uff1a\u65e9\u671f\u5c42\u68c0\u6d4b\u8bcd\u6c47\u60c5\u611f\uff0c\u665a\u671f\u5c42\u6574\u5408\u4e0a\u4e0b\u6587\uff0c\u800c\u975e\u9884\u671f\u7684\u4e2d\u5c42\u4e13\u4e1a\u5316\u5904\u7406", "motivation": "\u7814\u7a76GPT-2\u4e2d\u60c5\u611f\u4fe1\u606f\u5904\u7406\u7684\u56e0\u679c\u673a\u5236\uff0c\u9a8c\u8bc1\u5047\u8bbe\u7684\u4e24\u9636\u6bb5\u67b6\u6784\uff08\u65e9\u671f\u8bcd\u6c47\u68c0\u6d4b\u548c\u4e2d\u671f\u4e0a\u4e0b\u6587\u6574\u5408\uff09\uff0c\u5b9e\u8bc1\u68c0\u9a8c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0a\u4e0b\u6587\u6574\u5408\u7684\u5b9e\u9645\u6a21\u5f0f", "method": "\u4f7f\u7528\u7cfb\u7edf\u6027\u7684\u6fc0\u6d3b\u4fee\u8865\u6280\u672f\uff0c\u5728GPT-2\u7684\u6240\u670912\u4e2a\u5c42\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e09\u79cd\u4e0a\u4e0b\u6587\u6574\u5408\u5047\u8bbe\uff1a\u4e2d\u5c42\u96c6\u4e2d\u3001\u73b0\u8c61\u7279\u5f02\u6027\u548c\u5206\u5e03\u5f0f\u5904\u7406", "result": "\u65e9\u671f\u5c42\uff080-3\uff09\u786e\u5b9e\u4f5c\u4e3a\u8bcd\u6c47\u60c5\u611f\u68c0\u6d4b\u5668\uff0c\u7f16\u7801\u7a33\u5b9a\u3001\u4f4d\u7f6e\u7279\u5b9a\u7684\u6781\u6027\u4fe1\u53f7\uff1b\u4f46\u6240\u6709\u4e09\u79cd\u4e0a\u4e0b\u6587\u6574\u5408\u5047\u8bbe\u90fd\u88ab\u8bc1\u4f2a\uff0c\u4e0a\u4e0b\u6587\u73b0\u8c61\uff08\u5426\u5b9a\u3001\u8bbd\u523a\u3001\u9886\u57df\u8f6c\u79fb\u7b49\uff09\u4e3b\u8981\u5728\u665a\u671f\u5c42\uff088-11\uff09\u901a\u8fc7\u7edf\u4e00\u7684\u975e\u6a21\u5757\u5316\u673a\u5236\u6574\u5408", "conclusion": "GPT-2\u7684\u60c5\u611f\u8ba1\u7b97\u4e0e\u9884\u6d4b\u7684\u5c42\u6b21\u6a21\u5f0f\u4e0d\u540c\uff0c\u4e0a\u4e0b\u6587\u6574\u5408\u4e3b\u8981\u5728\u665a\u671f\u5c42\u53d1\u751f\uff0c\u8fd9\u7a81\u663e\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u5b9e\u8bc1\u8868\u5f81\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u6574\u5408\u673a\u5236"}}
{"id": "2512.06578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06578", "abs": "https://arxiv.org/abs/2512.06578", "authors": ["Waleed Razzaq"], "title": "Error-Centric PID Untrained Neural-Net (EC-PIDUNN) For Nonlinear Robotics Control", "comment": "Under review at SoftComputing", "summary": "Classical Proportional-Integral-Derivative (PID) control has been widely successful across various industrial systems such as chemical processes, robotics, and power systems. However, as these systems evolved, the increase in the nonlinear dynamics and the complexity of interconnected variables have posed challenges that classical PID cannot effectively handle, often leading to instability, overshooting, or prolonged settling times. Researchers have proposed PIDNN models that combine the function approximation capabilities of neural networks with PID control to tackle these nonlinear challenges. However, these models require extensive, highly refined training data and have significant computational costs, making them less favorable for real-world applications. In this paper, We propose a novel EC-PIDUNN architecture, which integrates an untrained neural network with an improved PID controller, incorporating a stabilizing factor (\\(\u03c4\\)) to generate the control signal. Like classical PID, our architecture uses the steady-state error \\(e_t\\) as input bypassing the need for explicit knowledge of the systems dynamics. By forming an input vector from \\(e_t\\) within the neural network, we increase the dimensionality of input allowing for richer data representation. Additionally, we introduce a vector of parameters \\( \u03c1_t \\) to shape the output trajectory and a \\textit{dynamic compute} function to adjust the PID coefficients from predefined values. We validate the effectiveness of EC-PIDUNN on multiple nonlinear robotics applications: (1) nonlinear unmanned ground vehicle systems that represent the Ackermann steering mechanism and kinematics control, (2) Pan-Tilt movement system. In both tests, it outperforms classical PID in convergence and stability achieving a nearly critically damped response.", "AI": {"tldr": "\u63d0\u51faEC-PIDUNN\u67b6\u6784\uff0c\u7ed3\u5408\u65e0\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e0e\u6539\u8fdbPID\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7a33\u5b9a\u56e0\u5b50\u03c4\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u63a7\u5236\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4f18\u4e8e\u4f20\u7edfPID", "motivation": "\u4f20\u7edfPID\u63a7\u5236\u5668\u5728\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u548c\u590d\u6742\u4e92\u8054\u53d8\u91cf\u65f6\u5b58\u5728\u7a33\u5b9a\u6027\u3001\u8d85\u8c03\u548c\u8c03\u8282\u65f6\u95f4\u95ee\u9898\uff0c\u73b0\u6709PIDNN\u6a21\u578b\u9700\u8981\u5927\u91cf\u7cbe\u7ec6\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528", "method": "\u63d0\u51faEC-PIDUNN\u67b6\u6784\uff0c\u6574\u5408\u65e0\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e0e\u6539\u8fdbPID\u63a7\u5236\u5668\uff0c\u5f15\u5165\u7a33\u5b9a\u56e0\u5b50\u03c4\u751f\u6210\u63a7\u5236\u4fe1\u53f7\uff1b\u4f7f\u7528\u7a33\u6001\u8bef\u5deee_t\u4f5c\u4e3a\u8f93\u5165\uff0c\u65e0\u9700\u7cfb\u7edf\u52a8\u6001\u77e5\u8bc6\uff1b\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u589e\u52a0\u8f93\u5165\u7ef4\u5ea6\uff0c\u5f15\u5165\u53c2\u6570\u5411\u91cf\u03c1_t\u5851\u9020\u8f93\u51fa\u8f68\u8ff9\uff0c\u4f7f\u7528\u52a8\u6001\u8ba1\u7b97\u51fd\u6570\u8c03\u6574PID\u7cfb\u6570", "result": "\u5728\u975e\u7ebf\u6027\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff1a(1) \u963f\u514b\u66fc\u8f6c\u5411\u673a\u5236\u7684\u975e\u7ebf\u6027\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7cfb\u7edf\uff0c(2) \u4e91\u53f0\u8fd0\u52a8\u7cfb\u7edf\uff1b\u5728\u4e24\u4e2a\u6d4b\u8bd5\u4e2d\u90fd\u4f18\u4e8e\u4f20\u7edfPID\uff0c\u5728\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u5b9e\u73b0\u63a5\u8fd1\u4e34\u754c\u963b\u5c3c\u54cd\u5e94", "conclusion": "EC-PIDUNN\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfPID\u5728\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u65f6\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2512.06688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06688", "abs": "https://arxiv.org/abs/2512.06688", "authors": ["Bowen Jiang", "Yuan Yuan", "Maohao Shen", "Zhuoqun Hao", "Zhangchen Xu", "Zichen Chen", "Ziyi Liu", "Anvesh Rao Vijjini", "Jiashu He", "Hanchao Yu", "Radha Poovendran", "Gregory Wornell", "Lyle Ungar", "Dan Roth", "Sihao Chen", "Camillo Jose Taylor"], "title": "PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory", "comment": "Data is available at https://huggingface.co/datasets/bowen-upenn/PersonaMem-v2", "summary": "Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.\n  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.", "AI": {"tldr": "PersonaMem-v2\u662f\u7528\u4e8eLLM\u4e2a\u6027\u5316\u7684\u5148\u8fdb\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u4e2a\u7528\u6237-\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u8986\u76d6300+\u573a\u666f\u548c20000+\u7528\u6237\u504f\u597d\u3002\u7814\u7a76\u663e\u793a\u524d\u6cbfLLM\u5728\u9690\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ec537-48%\uff0c\u800c\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u7684Qwen3-4B\u6a21\u578b\u8fbe\u523053%\u51c6\u786e\u7387\uff0c\u8d85\u8d8aGPT-5\u3002\u63d0\u51fa\u7684\u667a\u80fd\u8bb0\u5fc6\u6846\u67b6\u4f7f\u75282k\u4ee4\u724c\u8bb0\u5fc6\u800c\u975e\u5b8c\u657432k\u5bf9\u8bdd\u5386\u53f2\uff0c\u5b9e\u73b055%\u51c6\u786e\u7387\u4e14\u51cf\u5c1116\u500d\u8f93\u5165\u4ee4\u724c\u3002", "motivation": "\u4e2a\u6027\u5316\u662fAI\u80fd\u529b\u548c\u5bf9\u9f50\u7684\u4e0b\u4e00\u4e2a\u91cd\u8981\u91cc\u7a0b\u7891\u3002\u5f53\u524dLLM\u5728\u9690\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u4ec537-48%\uff0c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u66f4\u597d\u7684\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u65b9\u6cd5\u6765\u63d0\u5347LLM\u7684\u4e2a\u6027\u5316\u80fd\u529b\u3002", "method": "1) \u521b\u5efaPersonaMem-v2\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u4e2a\u771f\u5b9e\u7528\u6237-\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u8986\u76d6300+\u573a\u666f\u300120000+\u7528\u6237\u504f\u597d\uff0c128k\u4ee4\u724c\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5927\u591a\u6570\u7528\u6237\u504f\u597d\u9690\u5f0f\u5448\u73b0\u30022) \u4f7f\u7528\u5f3a\u5316\u5fae\u8c03\u8bad\u7ec3Qwen3-4B\u6a21\u578b\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u30023) \u5f00\u53d1\u667a\u80fd\u8bb0\u5fc6\u6846\u67b6\uff0c\u7ef4\u62a4\u5355\u4e00\u53ef\u8bfb\u8bb0\u5fc6\uff0c\u968f\u7528\u6237\u4ea4\u4e92\u589e\u957f\u3002", "result": "1) \u524d\u6cbfLLM\u5728\u9690\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ec537-48%\u30022) \u5f3a\u5316\u5fae\u8c03\u7684Qwen3-4B\u8fbe\u523053%\u51c6\u786e\u7387\uff0c\u8d85\u8d8aGPT-5\u30023) \u667a\u80fd\u8bb0\u5fc6\u6846\u67b6\u5b9e\u73b055%\u51c6\u786e\u7387\uff0c\u4f7f\u75282k\u4ee4\u724c\u8bb0\u5fc6\u800c\u975e\u5b8c\u657432k\u5bf9\u8bdd\u5386\u53f2\uff0c\u51cf\u5c1116\u500d\u8f93\u5165\u4ee4\u724c\u3002", "conclusion": "PersonaMem-v2\u6570\u636e\u96c6\u5bf9\u4e2a\u6027\u5316\u7814\u7a76\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u667a\u80fd\u8bb0\u5fc6\u6846\u67b6\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2a\u6027\u5316\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002\u5f3a\u5316\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u667a\u80fd\u8bb0\u5fc6\u7cfb\u7edf\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u4e2a\u6027\u5316\u6027\u80fd\u3002"}}
{"id": "2512.06608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06608", "abs": "https://arxiv.org/abs/2512.06608", "authors": ["Xinyu Zhou", "Songhao Piao", "Chao Gao", "Liguo Chen"], "title": "A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance", "comment": "8 pages, 6 figures", "summary": "Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u8bc4\u4f30\u4eba\u7fa4\u5bfc\u822a\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u5f3a\u8c03\u8f68\u8ff9\u66f2\u7387\u4f18\u5316\u7684\u65b0\u578b\u5956\u52b1\u5851\u9020\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8f68\u8ff9\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524d\u4eba\u7fa4\u5bfc\u822a\u7814\u7a76\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u8bc4\u4f30\u6307\u6807\u76f8\u5bf9\u4f18\u5148\u7ea7\u5206\u6790\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5bf9\u4e0d\u540c\u76ee\u6807\u65b9\u6cd5\u7684\u8bc4\u4f30\u4e0d\u516c\u5e73\uff1b2) \u8f68\u8ff9\u8fde\u7eed\u6027\u6307\u6807\uff08\u7279\u522b\u662f\u9700\u8981C\u00b2\u5e73\u6ed1\u5ea6\u7684\u6307\u6807\uff09\u5f88\u5c11\u88ab\u7eb3\u5165\u3002\u73b0\u6709DRL\u65b9\u6cd5\u901a\u5e38\u4f18\u5148\u8003\u8651\u6548\u7387\u548c\u8fd1\u7aef\u8212\u9002\u5ea6\uff0c\u800c\u5ffd\u89c6\u8f68\u8ff9\u4f18\u5316\u6216\u4ec5\u4f7f\u7528\u7b80\u5355\u672a\u9a8c\u8bc1\u7684\u5e73\u6ed1\u5ea6\u5956\u52b1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u5bf9\u5bfc\u822a\u65b9\u6cd5\u8fdb\u884c\u516c\u5e73\u900f\u660e\u7684\u8bc4\u4f30\uff0c\u901a\u8fc7\u68c0\u67e5\u591a\u4e2a\u4f18\u5316\u76ee\u6807\u7684\u4f18\u5148\u7ea7\u548c\u8054\u5408\u8bc4\u4f30\u3002\u540c\u65f6\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u5851\u9020\u7b56\u7565\uff0c\u660e\u786e\u5f3a\u8c03\u8f68\u8ff9\u66f2\u7387\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u76842D\u548c3D\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8f68\u8ff9\u8d28\u91cf\u548c\u9002\u5e94\u6027\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u6709\u6548\u7684\u8f68\u8ff9\u4f18\u5316\u5bf9\u4e8e\u786e\u4fdd\u81ea\u7136\u6027\u3001\u589e\u5f3a\u8212\u9002\u5ea6\u548c\u6700\u5927\u5316\u5bfc\u822a\u7cfb\u7edf\u80fd\u6548\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u548c\u5956\u52b1\u7b56\u7565\u80fd\u591f\u663e\u8457\u6539\u5584\u4eba\u7fa4\u5bfc\u822a\u7cfb\u7edf\u7684\u8f68\u8ff9\u8d28\u91cf\u548c\u591a\u573a\u666f\u9002\u5e94\u6027\u3002"}}
{"id": "2512.06690", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06690", "abs": "https://arxiv.org/abs/2512.06690", "authors": ["Chengbing Wang", "Yang Zhang", "Wenjie Wang", "Xiaoyan Zhao", "Fuli Feng", "Xiangnan He", "Tat-Seng Chua"], "title": "Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation", "comment": null, "summary": "Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent \"think-then-generate\" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient \"think-while-generating\" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.", "AI": {"tldr": "FlyThinker\u63d0\u51fa\"\u8fb9\u601d\u8003\u8fb9\u751f\u6210\"\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u6a21\u578b\u52a8\u6001\u6307\u5bfc\u957f\u6587\u672c\u751f\u6210\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u4e14\u9ad8\u6548\u7684\u957f\u6587\u672c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7fa4\u4f53\u504f\u597d\uff0c\u5ffd\u89c6\u4e2a\u4f53\u7528\u6237\u3002\u65e9\u671f\u4e2a\u6027\u5316\u65b9\u6cd5\uff08\u5982\u63d0\u793a\u5b9a\u5236\u6216\u5fae\u8c03\uff09\u96be\u4ee5\u63a8\u7406\u9690\u542b\u504f\u597d\uff0c\u800c\"\u5148\u601d\u8003\u540e\u751f\u6210\"\u65b9\u6cd5\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u9759\u6001\u4e00\u6b21\u6027\u63a8\u7406\u9700\u8981\u4e3a\u5b8c\u6574\u54cd\u5e94\u751f\u6210\u6355\u83b7\u6240\u6709\u76f8\u5173\u4fe1\u606f\uff0c\u5b66\u4e60\u56f0\u96be\u4e14\u9002\u5e94\u6027\u6709\u9650\u3002", "method": "FlyThinker\u91c7\u7528\"\u8fb9\u601d\u8003\u8fb9\u751f\u6210\"\u6846\u67b6\uff0c\u4f7f\u7528\u72ec\u7acb\u7684\u63a8\u7406\u6a21\u578b\u5e76\u884c\u751f\u6210\u6f5c\u5728token\u7ea7\u63a8\u7406\uff0c\u5c06\u5176\u878d\u5408\u5230\u751f\u6210\u6a21\u578b\u4e2d\u52a8\u6001\u6307\u5bfc\u54cd\u5e94\u751f\u6210\u3002\u63a8\u7406\u6a21\u578b\u4ec5\u4f9d\u8d56\u5148\u524d\u54cd\u5e94\u800c\u975e\u81ea\u8eab\u5148\u524d\u8f93\u51fa\uff0c\u4fdd\u6301\u8bad\u7ec3\u5e76\u884c\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlyThinker\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e2a\u6027\u5316\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "conclusion": "FlyThinker\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u548c\u751f\u6210\u7684\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u957f\u6587\u672c\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06610", "abs": "https://arxiv.org/abs/2512.06610", "authors": ["Marvin Harms", "Jaeyoung Lim", "David Rohr", "Friedrich Rockenbauer", "Nicholas Lawrance", "Roland Siegwart"], "title": "Robust Optimization-based Autonomous Dynamic Soaring with a Fixed-Wing UAV", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Dynamic soaring is a flying technique to exploit the energy available in wind shear layers, enabling potentially unlimited flight without the need for internal energy sources. We propose a framework for autonomous dynamic soaring with a fixed-wing unmanned aerial vehicle (UAV). The framework makes use of an explicit representation of the wind field and a classical approach for guidance and control of the UAV. Robustness to wind field estimation error is achieved by constructing point-wise robust reference paths for dynamic soaring and the development of a robust path following controller for the fixed-wing UAV. The framework is evaluated in dynamic soaring scenarios in simulation and real flight tests. In simulation, we demonstrate robust dynamic soaring flight subject to varied wind conditions, estimation errors and disturbances. Critical components of the framework, including energy predictions and path-following robustness, are further validated in real flights to assure small sim-to-real gap. Together, our results strongly indicate the ability of the proposed framework to achieve autonomous dynamic soaring flight in wind shear.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u81ea\u4e3b\u52a8\u6001\u7ff1\u7fd4\u7684\u6846\u67b6\uff0c\u5229\u7528\u98ce\u573a\u663e\u5f0f\u8868\u793a\u548c\u9c81\u68d2\u8def\u5f84\u8ddf\u8e2a\u63a7\u5236\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9645\u98de\u884c\u4e2d\u9a8c\u8bc1\u4e86\u5728\u98ce\u5207\u53d8\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u52a8\u6001\u7ff1\u7fd4\u7684\u80fd\u529b\u3002", "motivation": "\u52a8\u6001\u7ff1\u7fd4\u662f\u4e00\u79cd\u5229\u7528\u98ce\u5207\u53d8\u5c42\u80fd\u91cf\u7684\u98de\u884c\u6280\u672f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u65e0\u9700\u5185\u90e8\u80fd\u6e90\u7684\u65e0\u9650\u98de\u884c\u3002\u7136\u800c\uff0c\u5b9e\u73b0\u81ea\u4e3b\u52a8\u6001\u7ff1\u7fd4\u9700\u8981\u89e3\u51b3\u98ce\u573a\u4f30\u8ba1\u8bef\u5dee\u548c\u9c81\u68d2\u63a7\u5236\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u4e3b\u52a8\u6001\u7ff1\u7fd4\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u4f7f\u7528\u98ce\u573a\u663e\u5f0f\u8868\u793a\uff1b2\uff09\u6784\u5efa\u70b9\u5bf9\u70b9\u9c81\u68d2\u53c2\u8003\u8def\u5f84\u4ee5\u5e94\u5bf9\u98ce\u573a\u4f30\u8ba1\u8bef\u5dee\uff1b3\uff09\u5f00\u53d1\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7684\u9c81\u68d2\u8def\u5f84\u8ddf\u8e2a\u63a7\u5236\u5668\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u5c55\u793a\u4e86\u5728\u5404\u79cd\u98ce\u6761\u4ef6\u3001\u4f30\u8ba1\u8bef\u5dee\u548c\u5e72\u6270\u4e0b\u7684\u9c81\u68d2\u52a8\u6001\u7ff1\u7fd4\u98de\u884c\u3002\u5b9e\u9645\u98de\u884c\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u80fd\u91cf\u9884\u6d4b\u548c\u8def\u5f84\u8ddf\u8e2a\u9c81\u68d2\u6027\u7b49\u5173\u952e\u7ec4\u4ef6\uff0c\u8868\u660e\u4eff\u771f\u4e0e\u5b9e\u9645\u5dee\u8ddd\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u5728\u98ce\u5207\u53d8\u4e2d\u7684\u81ea\u4e3b\u52a8\u6001\u7ff1\u7fd4\u98de\u884c\uff0c\u4e3a\u65e0\u80fd\u6e90\u9650\u5236\u7684\u98de\u884c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2512.06694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06694", "abs": "https://arxiv.org/abs/2512.06694", "authors": ["Aoi Fujita", "Taichi Yamamoto", "Yuri Nakayama", "Ryota Kobayashi"], "title": "TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction", "comment": "15 pages, 4 figures, code available at https://github.com/aoi8716/TopiCLEAR", "summary": "Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.", "AI": {"tldr": "\u63d0\u51faTopiCLEAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u964d\u7ef4\u805a\u7c7b\u5d4c\u5165\u8fdb\u884c\u4e3b\u9898\u63d0\u53d6\uff0c\u4e13\u95e8\u9488\u5bf9\u793e\u4ea4\u5a92\u4f53\u77ed\u6587\u672c\uff0c\u65e0\u9700\u9884\u5904\u7406\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u6a21\u578b\u662f\u4e3a\u8f83\u957f\u6b63\u5f0f\u6587\u6863\u8bbe\u8ba1\u7684\uff0c\u5728\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u77ed\u6587\u672c\u65f6\u9762\u4e34\u6311\u6218\uff1a\u5171\u73b0\u7edf\u8ba1\u6709\u9650\u3001\u8bed\u4e49\u788e\u7247\u5316\u3001\u62fc\u5199\u4e0d\u4e00\u81f4\u548c\u8bed\u8a00\u975e\u6b63\u5f0f\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u793e\u4ea4\u5a92\u4f53\u77ed\u6587\u672c\u7684\u4e3b\u9898\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "TopiCLEAR\uff1a\u4f7f\u7528Sentence-BERT\u5d4c\u5165\u6587\u672c\uff0c\u5148\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u521d\u6b65\u805a\u7c7b\uff0c\u7136\u540e\u901a\u8fc7\u7ebf\u6027\u5224\u522b\u5206\u6790\u8fdb\u884c\u76d1\u7763\u6295\u5f71\u8fed\u4ee3\u4f18\u5316\u805a\u7c7b\uff0c\u76f4\u5230\u6536\u655b\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u5904\u7406\u539f\u59cb\u6587\u672c\uff0c\u65e0\u9700\u505c\u7528\u8bcd\u79fb\u9664\u7b49\u9884\u5904\u7406\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff0820News\u3001AgNewsTitle\u3001Reddit\u3001TweetTopic\uff09\u4e0a\u8bc4\u4f30\uff0c\u4e0e7\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u6700\u65b0\u7684SBERT\u65b9\u6cd5\u548c\u96f6\u6837\u672c\u751f\u6210AI\u65b9\u6cd5\uff09\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6807\u6ce8\u4e3b\u9898\u7684\u76f8\u4f3c\u5ea6\u6700\u9ad8\uff0c\u5bf9\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u548c\u5728\u7ebf\u65b0\u95fb\u6587\u7ae0\u90fd\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "TopiCLEAR\u65b9\u6cd5\u5728\u793e\u4ea4\u5a92\u4f53\u77ed\u6587\u672c\u4e3b\u9898\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4ea7\u751f\u66f4\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\uff0c\u5728\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u548c\u7f51\u7edc\u5185\u5bb9\u5206\u6790\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.06628", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06628", "abs": "https://arxiv.org/abs/2512.06628", "authors": ["Ruicheng Zhang", "Mingyang Zhang", "Jun Zhou", "Zhangrui Guo", "Xiaofan Liu", "Zunnan Xu", "Zhizhou Zhong", "Puxin Yan", "Haocheng Luo", "Xiu Li"], "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment", "comment": null, "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.", "AI": {"tldr": "MIND-V\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u3001\u884c\u4e3a\u8bed\u4e49\u6865\u63a5\u548c\u89c6\u9891\u751f\u6210\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u7ed3\u5408\u7269\u7406\u4e00\u81f4\u6027\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u5408\u7406\u4e14\u903b\u8f91\u8fde\u8d2f\u7684\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u9762\u4e34\u957f\u65f6\u7a0b\u3001\u591a\u6837\u5316\u64cd\u4f5c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ea\u80fd\u5408\u6210\u77ed\u7247\u6bb5\u7b80\u5355\u52a8\u4f5c\u4e14\u4f9d\u8d56\u624b\u52a8\u5b9a\u4e49\u8f68\u8ff9\uff0c\u9700\u8981\u80fd\u591f\u751f\u6210\u7269\u7406\u5408\u7406\u3001\u903b\u8f91\u8fde\u8d2f\u7684\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMIND-V\u5206\u5c42\u6846\u67b6\uff1a1\uff09\u8bed\u4e49\u63a8\u7406\u4e2d\u5fc3\uff08SRH\uff09\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\uff1b2\uff09\u884c\u4e3a\u8bed\u4e49\u6865\u63a5\uff08BSB\uff09\u5c06\u62bd\u8c61\u6307\u4ee4\u8f6c\u6362\u4e3a\u9886\u57df\u4e0d\u53d8\u8868\u793a\uff1b3\uff09\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u5668\uff08MVG\uff09\u8fdb\u884c\u6761\u4ef6\u89c6\u9891\u6e32\u67d3\u3002\u91c7\u7528\u5206\u9636\u6bb5\u89c6\u89c9\u672a\u6765\u5c55\u5f00\u6d4b\u8bd5\u65f6\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u901a\u8fc7GRPO\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u548c\u7269\u7406\u9884\u89c1\u4e00\u81f4\u6027\uff08PFC\uff09\u5956\u52b1\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\u3002", "result": "MIND-V\u5728\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u63a7\u7684\u5177\u8eab\u6570\u636e\u5408\u6210\u8303\u5f0f\u3002", "conclusion": "MIND-V\u901a\u8fc7\u5206\u5c42\u67b6\u6784\u548c\u7269\u7406\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u5177\u8eab\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06711", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06711", "abs": "https://arxiv.org/abs/2512.06711", "authors": ["Yulin Huang", "Yaxuan Luan", "Jinxu Guo", "Xiangchen Song", "Yuchen Liu"], "title": "Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models", "comment": null, "summary": "This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u566a\u58f0\u5206\u914d\u4e0e\u68af\u5ea6\u88c1\u526a\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6307\u4ee4\u5fae\u8c03\u7684\u6548\u7387", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u6307\u4ee4\u5fae\u8c03\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6548\u7387\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u9690\u79c1\u9884\u7b97\u6d88\u8017\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3", "method": "\u51bb\u7ed3\u4e3b\u5e72\u6a21\u578b\uff0c\u901a\u8fc7\u4f4e\u7ef4\u6295\u5f71\u5b50\u7a7a\u95f4\u66f4\u65b0\u53c2\u6570\uff0c\u5728\u68af\u5ea6\u8ba1\u7b97\u4e2d\u5f15\u5165\u88c1\u526a\u548c\u81ea\u9002\u5e94\u566a\u58f0\u5206\u914d\uff0c\u6784\u5efa\u68af\u5ea6\u7ea6\u675f\u3001\u566a\u58f0\u5206\u914d\u548c\u53c2\u6570\u6295\u5f71\u7684\u7edf\u4e00\u6846\u67b6", "result": "\u5728\u51c6\u786e\u6027\u3001\u9690\u79c1\u9884\u7b97\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u591a\u6837\u5316\u4e0d\u786e\u5b9a\u6570\u636e\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd", "conclusion": "\u4e30\u5bcc\u4e86\u5dee\u5206\u9690\u79c1\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u7406\u8bba\u6574\u5408\uff0c\u4e3a\u590d\u6742\u6307\u4ee4\u73af\u5883\u4e0b\u7684\u5b89\u5168\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06664", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06664", "abs": "https://arxiv.org/abs/2512.06664", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Jingreng Lei", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "title": "Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving", "comment": "9 pages", "summary": "Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.", "AI": {"tldr": "\u63d0\u51faMoE-RAM\uff1a\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u7edf\u8ba1\u589e\u5f3a\u89e3\u8026\u4e13\u5bb6\u8def\u7531\u4e0e\u805a\u5408\u673a\u5236\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8bed\u4e49\u5206\u5272\u4efb\u52a1", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u590d\u6742\u591a\u6837\uff0c\u5355\u4e00\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u8986\u76d6\u6240\u6709\u6761\u4ef6\uff08\u5929\u6c14\u3001\u4ea4\u901a\u5bc6\u5ea6\u3001\u9053\u8def\u7c7b\u578b\u7b49\uff09\u3002\u73b0\u6709MoE\u65b9\u6cd5\u5b58\u5728\u8def\u7531\u4e0d\u7cbe\u786e\u548c\u805a\u5408\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMoE-RAM\uff1a1\uff09\u901a\u8fc7\u7edf\u8ba1\u68c0\u7d22\u673a\u5236\u589e\u5f3a\u4e13\u5bb6\u8def\u7531\uff0c\u5339\u914d\u5927\u6a21\u578b\u63d0\u53d6\u7684\u6f5c\u5728\u7279\u5f81\u4e0e\u7f13\u5b58\u7684\u4e13\u5bb6\u539f\u578b\u7279\u5f81\uff1b2\uff09\u901a\u8fc7\u6d4b\u91cf\u4e13\u5bb6\u5373\u65f6\u7279\u5f81\u4e0e\u5927\u6a21\u578b\u6f5c\u5728\u7279\u5f81\u7684\u7edf\u8ba1\u8ddd\u79bb\uff0c\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u4e13\u5bb6\u8f93\u51fa\u878d\u5408\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoE-RAM\u4f18\u4e8e\u5176\u4ed6MoE\u57fa\u7ebf\u548c\u4f20\u7edf\u5355\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "\u7edf\u8ba1\u589e\u5f3a\u7684MoE\u8def\u7531\u4e0e\u805a\u5408\u673a\u5236\u80fd\u534f\u540c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06732", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06732", "abs": "https://arxiv.org/abs/2512.06732", "authors": ["Aarushi Wagh", "Saniya Srivastava"], "title": "\"The Dentist is an involved parent, the bartender is not\": Revealing Implicit Biases in QA with Implicit BBQ", "comment": null, "summary": "Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the \"sexual orientation\" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.", "AI": {"tldr": "ImplicitBBQ\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u5f0f\u504f\u89c1\uff0c\u901a\u8fc7\u59d3\u540d\u3001\u6587\u5316\u7ebf\u7d22\u7b49\u9690\u542b\u65b9\u5f0f\u63d0\u793a\u53d7\u4fdd\u62a4\u5c5e\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u663e\u5f0f\u58f0\u660e\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u504f\u89c1\u7684\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u63d0\u793a\uff08\u76f4\u63a5\u58f0\u660e\u5b97\u6559\u3001\u79cd\u65cf\u3001\u6027\u522b\u7b49\u53d7\u4fdd\u62a4\u5c5e\u6027\uff09\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e92\u52a8\u4e2d\u504f\u89c1\u5f80\u5f80\u901a\u8fc7\u59d3\u540d\u3001\u6587\u5316\u7ebf\u7d22\u7b49\u9690\u542b\u65b9\u5f0f\u4f53\u73b0\uff0c\u8fd9\u79cd\u5173\u952e\u758f\u5ffd\u5728\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u9020\u6210\u4e86\u91cd\u5927\u76f2\u70b9\u3002", "method": "\u6269\u5c55Bias Benchmark for QA (BBQ)\u57fa\u51c6\uff0c\u521b\u5efaImplicitBBQ\uff0c\u901a\u8fc7\u9690\u542b\u65b9\u5f0f\u63d0\u793a6\u4e2a\u7c7b\u522b\u7684\u53d7\u4fdd\u62a4\u5c5e\u6027\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u58f0\u660e\u8fd9\u4e9b\u5c5e\u6027\u3002", "result": "\u5728GPT-4o\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u663e\u5f0fBBQ\u63d0\u793a\u76f8\u6bd4\uff0cImplicitBBQ\u8868\u73b0\u51fa\u4ee4\u4eba\u62c5\u5fe7\u7684\u6027\u80fd\u5dee\u5f02\uff1a\u5728\"\u6027\u53d6\u5411\"\u5b50\u7c7b\u522b\u4e2d\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe7%\uff0c\u5927\u591a\u6570\u5176\u4ed6\u7c7b\u522b\u4e5f\u51fa\u73b0\u4e00\u81f4\u4e0b\u964d\uff0c\u8868\u660e\u5f53\u524dLLM\u5b58\u5728\u663e\u5f0f\u57fa\u51c6\u65e0\u6cd5\u68c0\u6d4b\u7684\u9690\u5f0f\u504f\u89c1\u3002", "conclusion": "ImplicitBBQ\u4e3aNLP\u9886\u57df\u7684\u7ec6\u81f4\u516c\u5e73\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u4e2d\u5b58\u5728\u7684\u9690\u5f0f\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u65e0\u6cd5\u901a\u8fc7\u663e\u5f0f\u57fa\u51c6\u68c0\u6d4b\u3002"}}
{"id": "2512.06676", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06676", "abs": "https://arxiv.org/abs/2512.06676", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Bingyang Cheng", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "title": "FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving", "comment": "9 pages", "summary": "Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.", "AI": {"tldr": "FedDSR \u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8054\u90a6\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4e2d\u95f4\u5c42\u76d1\u7763\u548c\u6b63\u5219\u5316\u89e3\u51b3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u7684\u6cdb\u5316\u5dee\u548c\u6536\u655b\u6162\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u6536\u655b\u901f\u5ea6\u6162\u7684\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "FedDSR \u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7b56\u7565\uff1a(1) \u57fa\u4e8e\u67b6\u6784\u65e0\u5173\u6807\u51c6\u9009\u62e9\u591a\u4e2a\u4e2d\u95f4\u5c42\uff1b(2) \u5728\u9009\u5b9a\u5c42\u8ba1\u7b97\u4e92\u4fe1\u606f\u548c\u8d1f\u71b5\u4f5c\u4e3a\u4e2d\u95f4\u635f\u5931\u548c\u6b63\u5219\u9879\uff1b(3) \u57fa\u4e8e\u8fd9\u4e9b\u89c4\u5219\u805a\u5408\u8f66\u8f86\u6a21\u578b\u751f\u6210\u5168\u5c40\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a FedDSR \u76f8\u6bd4\u5176\u4ed6\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\uff0c\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8 8.93% \u7684 mIoU \u63d0\u5347\u548c 28.57% \u7684\u8bad\u7ec3\u8f6e\u6b21\u51cf\u5c11\u3002", "conclusion": "FedDSR \u901a\u8fc7\u4e2d\u95f4\u5c42\u76d1\u7763\u548c\u6b63\u5219\u5316\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8054\u90a6\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6536\u655b\u901f\u5ea6\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.06734", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06734", "abs": "https://arxiv.org/abs/2512.06734", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "title": "A Patient-Doctor-NLP-System to contest inequality for less privileged", "comment": "19 pages, 6 figures", "summary": "Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.", "AI": {"tldr": "PDFTEMRA\u662f\u4e00\u4e2a\u7d27\u51d1\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u578b\u84b8\u998f\u3001\u9891\u57df\u8c03\u5236\u3001\u96c6\u6210\u5b66\u4e60\u548c\u968f\u673a\u6fc0\u6d3b\u6a21\u5f0f\u7b49\u6280\u672f\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u533b\u7597NLP\u573a\u666f\uff0c\u7279\u522b\u662f\u4e3a\u5370\u5730\u8bed\u4f7f\u7528\u8005\u548c\u89c6\u969c\u7528\u6237\u63d0\u4f9b\u533b\u7597\u5e2e\u52a9\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u533b\u7597\u73af\u5883\u4e2d\u90e8\u7f72\u56f0\u96be\uff0c\u7279\u522b\u662f\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\uff09\u4f7f\u7528\u8005\u548c\u89c6\u969c\u7528\u6237\u63d0\u4f9b\u533b\u7597\u5e2e\u52a9\u65f6\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u6210\u672c\u4f4e\u4f46\u6027\u80fd\u4fdd\u6301\u7684\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faPDFTEMRA\u67b6\u6784\uff0c\u6574\u5408\u4e86\u56db\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u6a21\u578b\u84b8\u998f\u4ee5\u538b\u7f29\u6a21\u578b\u89c4\u6a21\uff1b2) \u9891\u57df\u8c03\u5236\u63d0\u9ad8\u6548\u7387\uff1b3) \u96c6\u6210\u5b66\u4e60\u589e\u5f3a\u6027\u80fd\uff1b4) \u968f\u673a\u6fc0\u6d3b\u6a21\u5f0f\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u3002\u6a21\u578b\u5728\u9488\u5bf9\u5370\u5730\u8bed\u548c\u53ef\u8bbf\u95ee\u6027\u573a\u666f\u7684\u533b\u7597\u95ee\u7b54\u548c\u54a8\u8be2\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "PDFTEMRA\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u6807\u51c6NLP\u6700\u5148\u8fdb\u6a21\u578b\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u533b\u7597NLP\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "PDFTEMRA\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u533b\u7597NLP\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u652f\u6301\u4f4e\u8d44\u6e90\u8bed\u8a00\u4f7f\u7528\u8005\u548c\u89c6\u969c\u7528\u6237\u7684\u533b\u7597\u9700\u6c42\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2512.06754", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06754", "abs": "https://arxiv.org/abs/2512.06754", "authors": ["Shrreya Rajneesh", "Nikita Pavle", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization", "comment": null, "summary": "Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u6a21\u578b\u63a7\u5236\u6846\u67b6\uff0c\u7ed5\u8fc7\u8fde\u7eed\u4f53\u673a\u68b0\u81c2\u7684\u590d\u6742\u8fd0\u52a8\u5b66\u5efa\u6a21\uff0c\u901a\u8fc7\u5728\u7ebf\u4f18\u5316\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236", "motivation": "\u8fde\u7eed\u4f53\u673a\u68b0\u81c2\u5177\u6709\u65e0\u9650\u7ef4\u53d8\u5f62\u3001\u672a\u5efa\u6a21\u5185\u90e8\u6469\u64e6\u548c\u914d\u7f6e\u76f8\u5173\u521a\u5ea6\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u57fa\u4e8e\u6a21\u578b\u7684\u8fd0\u52a8\u5b66\u516c\u5f0f\u4e0d\u53ef\u9760\uff0c\u96c5\u53ef\u6bd4\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u5b58\u5728\u4eba\u5de5\u5947\u70b9\u548c\u4e0d\u7a33\u5b9a\u9a71\u52a8\u884c\u4e3a", "method": "\u91c7\u7528\u7ecf\u9a8c\u521d\u59cb\u5316\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u901a\u8fc7\u5dee\u5206\u51f8\u66f4\u65b0\u5728\u7ebf\u4f18\u5316\uff1b\u901a\u8fc7\u5b9e\u65f6\u4e8c\u6b21\u89c4\u5212\u8ba1\u7b97\u9a71\u52a8\u5668\u589e\u91cf\uff0c\u540c\u65f6\u907f\u514d\u808c\u8171\u677e\u5f1b\u548c\u51e0\u4f55\u9650\u5236\uff1b\u5f15\u5165\u80cc\u90e8\u5f20\u529b\u4f18\u5316\u9879\u8c03\u8282\u8f74\u5411\u8f7d\u8377\u5e76\u6291\u5236\u534f\u540c\u6fc0\u6d3b\u538b\u7f29", "result": "\u5728\u5706\u5f62\u3001\u4e94\u8fb9\u5f62\u548c\u65b9\u5f62\u8f68\u8ff9\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u5e73\u6ed1\u6536\u655b\u3001\u7a33\u5b9a\u5f20\u529b\u6f14\u5316\u548c\u4e9a\u6beb\u7c73\u7ea7\u7a33\u6001\u7cbe\u5ea6\uff0c\u65e0\u9700\u4efb\u4f55\u6a21\u578b\u6821\u51c6\u6216\u53c2\u6570\u8bc6\u522b", "conclusion": "\u8be5\u63a7\u5236\u5668\u4e3a\u53d7\u9650\u73af\u5883\u4e2d\u6a21\u578b\u4f9d\u8d56\u7684\u8fde\u7eed\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2512.06744", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06744", "abs": "https://arxiv.org/abs/2512.06744", "authors": ["Rajeev Ranjan"], "title": "One Word Is Not Enough: Simple Prompts Improve Word Embeddings", "comment": null, "summary": "Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like \"meaning: {word}\" or \"Represent the semantic concept: {word}\" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.", "AI": {"tldr": "\u901a\u8fc7\u7b80\u5355\u7684\u8bed\u4e49\u63d0\u793a\uff08\u5982\"meaning: {word}\"\uff09\u663e\u8457\u63d0\u5347\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u5355\u8bcd\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u4f20\u7edf\u9759\u6001\u5d4c\u5165\u65b9\u6cd5", "motivation": "\u6587\u672c\u5d4c\u5165\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u53e5\u5b50\u7ea7\u5e94\u7528\u8bbe\u8ba1\uff0c\u5728\u5355\u8bcd\u7ea7\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e86\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u63d0\u5347\u5b83\u4eec\u5728\u5b64\u7acb\u5355\u8bcd\u4e0a\u7684\u8bed\u4e49\u8868\u793a\u80fd\u529b", "method": "\u5728\u5355\u8bcd\u524d\u6dfb\u52a0\u8bed\u4e49\u63d0\u793a\uff08\u5982\"meaning: {word}\"\u6216\"Represent the semantic concept: {word}\"\uff09\uff0c\u7136\u540e\u4f7f\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u5d4c\u5165\uff0c\u57287\u4e2a\u6a21\u578b\u548c3\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u6d4b\u8bd5", "result": "\u63d0\u793a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5355\u8bcd\u76f8\u4f3c\u6027\u76f8\u5173\u6027\uff0c\u5728SimLex-999\u4e0a\u6700\u9ad8\u63d0\u5347+0.29\uff0c\u67d0\u4e9b\u6a21\u578b\u4ece\u76f8\u5173\u6027\u4e3a0\u6062\u590d\u5230+0.73\u63d0\u5347\uff0c\u6700\u4f73\u7ed3\u679c\u5728SimLex-999\u4e0a\u8fbe\u52300.692\u76f8\u5173\u6027\uff0c\u8d85\u8d8aWord2Vec(0.40)\u548cLexVec(0.48)", "conclusion": "\u7b80\u5355\u7684\u96f6\u6837\u672c\u8bed\u4e49\u63d0\u793a\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u5355\u8bcd\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e3a\u5355\u8bcd\u7ea7\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06796", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06796", "abs": "https://arxiv.org/abs/2512.06796", "authors": ["Akmaral Moldagalieva", "Keisuke Okumura", "Amanda Prorok", "Wolfgang H\u00f6nig"], "title": "db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF", "comment": null, "summary": "State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time.\n  In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations.\n  To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions.\n  The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics.\n  Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality.\n  The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator.\n  We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.", "AI": {"tldr": "db-LaCAM\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212(MAPF)\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u52a8\u529b\u5b66\u89c4\u5212\u7684\u52a8\u6001\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u8fd0\u52a8\u57fa\u5143\u548c\u5141\u8bb8\u7528\u6237\u5b9a\u4e49\u7684\u4e0d\u8fde\u7eed\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\uff0c\u53ef\u6269\u5c55\u523050\u4e2a\u673a\u5668\u4eba\uff0c\u8fd0\u884c\u901f\u5ea6\u63d0\u534710\u500d\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u673a\u5668\u4eba\u52a8\u529b\u5b66\u8fd0\u52a8\u89c4\u5212\u5668\u7531\u4e8e\u8ba1\u7b97\u8d1f\u62c5\u9ad8\uff0c\u96be\u4ee5\u5904\u7406\u8d85\u8fc7\u51e0\u4e2a\u673a\u5668\u4eba\u7684\u60c5\u51b5\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u5e76\u5bfc\u81f4\u89c4\u5212\u65f6\u95f4\u7f13\u6162\u3002", "method": "\u63d0\u51fadiscontinuity-Bounded LaCAM (db-LaCAM)\uff0c\u5229\u7528\u9884\u8ba1\u7b97\u5c0a\u91cd\u673a\u5668\u4eba\u52a8\u529b\u5b66\u7684\u8fd0\u52a8\u57fa\u5143\u751f\u6210\u89c6\u754c\u957f\u5ea6\u7684\u8fd0\u52a8\u5e8f\u5217\uff0c\u540c\u65f6\u5141\u8bb8\u8fde\u7eed\u8fd0\u52a8\u4e4b\u95f4\u7684\u7528\u6237\u5b9a\u4e49\u4e0d\u8fde\u7eed\u6027\u3002\u8be5\u89c4\u5212\u5668\u76f8\u5bf9\u4e8e\u8fd0\u52a8\u57fa\u5143\u662f\u5206\u8fa8\u7387\u5b8c\u5907\u7684\uff0c\u652f\u6301\u4efb\u610f\u673a\u5668\u4eba\u52a8\u529b\u5b66\u3002", "result": "db-LaCAM\u53ef\u9ad8\u6548\u6269\u5c55\u5230\u6700\u591a50\u4e2a\u673a\u5668\u4eba\u7684\u573a\u666f\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u89c4\u5212\u5668\u5b9e\u73b0\u9ad8\u8fbe10\u500d\u7684\u8fd0\u884c\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002\u57282D\u548c3D\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5355\u8f6e\u8f66\u548c3D\u53cc\u79ef\u5206\u5668\u7b49\u52a8\u529b\u5b66\u6a21\u578b\u3002", "conclusion": "db-LaCAM\u6210\u529f\u7ed3\u5408\u4e86MAPF\u7684\u53ef\u6269\u5c55\u6027\u548c\u52a8\u529b\u5b66\u89c4\u5212\u7684\u52a8\u6001\u611f\u77e5\u80fd\u529b\uff0c\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u98de\u884c\u673a\u5668\u4eba\u548c\u5e26\u62d6\u8f66\u6c7d\u8f66\u673a\u5668\u4eba\u56e2\u961f\u7684\u5b89\u5168\u8f68\u8ff9\u6267\u884c\u3002"}}
{"id": "2512.06751", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06751", "abs": "https://arxiv.org/abs/2512.06751", "authors": ["Seungyeon Jwa", "Daechul Ahn", "Reokyoung Kim", "Dongyeop Kang", "Jonghyun Choi"], "title": "Becoming Experienced Judges: Selective Test-Time Learning for Evaluators", "comment": null, "summary": "Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.", "AI": {"tldr": "\u63d0\u51faLWE\u6846\u67b6\uff0c\u8ba9LLM\u8bc4\u4f30\u5668\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u7684\u5143\u63d0\u793a\u8fdb\u884c\u987a\u5e8f\u5b66\u4e60\uff0c\u65e0\u9700\u8bad\u7ec3\u96c6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8bc4\u4f30\u8005\u96be\u4ee5\u5224\u65ad\u7684\u6848\u4f8b\u3002", "motivation": "\u5f53\u524dLLM-as-a-judge\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u6bcf\u4e2a\u6848\u4f8b\u72ec\u7acb\u8bc4\u4f30\uff0c\u65e0\u6cd5\u79ef\u7d2f\u7ecf\u9a8c\uff1b2) \u4f7f\u7528\u56fa\u5b9a\u63d0\u793a\u8bcd\uff0c\u7f3a\u4e4f\u6837\u672c\u7279\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u9700\u8981\u8ba9\u8bc4\u4f30\u5668\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u591f\u5b66\u4e60\u548c\u6539\u8fdb\u3002", "method": "\u63d0\u51faLearning While Evaluating (LWE)\u6846\u67b6\uff0c\u7ef4\u62a4\u4e00\u4e2a\u4e0d\u65ad\u6f14\u5316\u7684\u5143\u63d0\u793a\uff0c\u8be5\u63d0\u793a\uff1a1) \u751f\u6210\u6837\u672c\u7279\u5b9a\u7684\u8bc4\u4f30\u6307\u4ee4\uff1b2) \u901a\u8fc7\u81ea\u6211\u751f\u6210\u7684\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u4f18\u5316\u3002\u8fdb\u4e00\u6b65\u63d0\u51faSelective LWE\uff0c\u53ea\u5728\u81ea\u6211\u4e0d\u4e00\u81f4\u7684\u6848\u4f8b\u4e0a\u66f4\u65b0\u5143\u63d0\u793a\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u4e24\u4e2a\u6210\u5bf9\u6bd4\u8f83\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSelective LWE\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u8bc1\u8868\u660e\u8bc4\u4f30\u5668\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u9009\u62e9\u6027\u66f4\u65b0\u5728\u987a\u5e8f\u6d4b\u8bd5\u4e2d\u6539\u8fdb\uff0c\u4ece\u96be\u4ee5\u5224\u65ad\u7684\u6848\u4f8b\u4e2d\u5b66\u4e60\u6700\u591a\u3002", "conclusion": "LWE\u6846\u67b6\u4f7fLLM\u8bc4\u4f30\u5668\u80fd\u591f\u5728\u63a8\u7406\u65f6\u987a\u5e8f\u5b66\u4e60\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u673a\u5236\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06829", "abs": "https://arxiv.org/abs/2512.06829", "authors": ["Oluwatimilehin Tijani", "Zhuo Chen", "Jiankang Deng", "Shan Luo"], "title": "MagicSkin: Balancing Marker and Markerless Modes in Vision-Based Tactile Sensors with a Translucent Skin", "comment": "Submitted to ICRA2026", "summary": "Vision-based tactile sensors (VBTS) face a fundamental trade-off in marker and markerless design on the tactile skin: opaque ink markers enable measurement of force and tangential displacement but completely occlude geometric features necessary for object and texture classification, while markerless skin preserves surface details but struggles in measuring tangential displacements effectively. Current practice to solve the above problem via UV lighting or virtual transfer using learning-based models introduces hardware complexity or computing burdens. This paper introduces MagicSkin, a novel tactile skin with translucent, tinted markers balancing the modes of marker and markerless for VBTS. It enables simultaneous tangential displacement tracking, force prediction, and surface detail preservation. This skin is easy to plug into GelSight-family sensors without requiring additional hardware or software tools. We comprehensively evaluate MagicSkin in downstream tasks. The translucent markers impressively enhance rather than degrade sensing performance compared with traditional markerless and inked marker design: it achieves best performance in object classification (99.17\\%), texture classification (93.51\\%), tangential displacement tracking (97\\% point retention) and force prediction (66\\% improvement in total force error). These experimental results demonstrate that translucent skin eliminates the traditional performance trade-off in marker or markerless modes, paving the way for multimodal tactile sensing essential in tactile robotics. See videos at this \\href{https://zhuochenn.github.io/MagicSkin_project/}{link}.", "AI": {"tldr": "MagicSkin\u662f\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u76ae\u80a4\uff0c\u91c7\u7528\u534a\u900f\u660e\u67d3\u8272\u6807\u8bb0\u8bbe\u8ba1\uff0c\u5e73\u8861\u4e86\u6807\u8bb0\u548c\u65e0\u6807\u8bb0\u6a21\u5f0f\u7684\u4f18\u70b9\uff0c\u540c\u65f6\u5b9e\u73b0\u5207\u5411\u4f4d\u79fb\u8ddf\u8e2a\u3001\u529b\u9884\u6d4b\u548c\u8868\u9762\u7ec6\u8282\u4fdd\u7559\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u6216\u8f6f\u4ef6\u5de5\u5177\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u7684\u89e6\u89c9\u4f20\u611f\u5668\u5728\u6807\u8bb0\u548c\u65e0\u6807\u8bb0\u8bbe\u8ba1\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff1a\u4e0d\u900f\u660e\u58a8\u6c34\u6807\u8bb0\u53ef\u4ee5\u6d4b\u91cf\u529b\u548c\u5207\u5411\u4f4d\u79fb\u4f46\u5b8c\u5168\u906e\u6321\u51e0\u4f55\u7279\u5f81\uff0c\u800c\u65e0\u6807\u8bb0\u76ae\u80a4\u4fdd\u7559\u8868\u9762\u7ec6\u8282\u4f46\u96be\u4ee5\u6709\u6548\u6d4b\u91cf\u5207\u5411\u4f4d\u79fb\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u8981\u989d\u5916\u786c\u4ef6\u6216\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u63d0\u51faMagicSkin\uff0c\u4e00\u79cd\u5177\u6709\u534a\u900f\u660e\u67d3\u8272\u6807\u8bb0\u7684\u65b0\u578b\u89e6\u89c9\u76ae\u80a4\u8bbe\u8ba1\uff0c\u5e73\u8861\u6807\u8bb0\u548c\u65e0\u6807\u8bb0\u6a21\u5f0f\u3002\u8be5\u76ae\u80a4\u53ef\u76f4\u63a5\u96c6\u6210\u5230GelSight\u7cfb\u5217\u4f20\u611f\u5668\u4e2d\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u6216\u8f6f\u4ef6\u5de5\u5177\u3002", "result": "MagicSkin\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u7269\u4f53\u5206\u7c7b\uff0899.17%\uff09\u3001\u7eb9\u7406\u5206\u7c7b\uff0893.51%\uff09\u3001\u5207\u5411\u4f4d\u79fb\u8ddf\u8e2a\uff0897%\u70b9\u4fdd\u7559\uff09\u548c\u529b\u9884\u6d4b\uff08\u603b\u529b\u8bef\u5dee\u6539\u558466%\uff09\u3002\u534a\u900f\u660e\u6807\u8bb0\u4e0d\u4ec5\u6ca1\u6709\u964d\u4f4e\u53cd\u800c\u63d0\u5347\u4e86\u4f20\u611f\u6027\u80fd\u3002", "conclusion": "\u534a\u900f\u660e\u76ae\u80a4\u6d88\u9664\u4e86\u4f20\u7edf\u6807\u8bb0\u6216\u65e0\u6807\u8bb0\u6a21\u5f0f\u7684\u6027\u80fd\u6743\u8861\uff0c\u4e3a\u89e6\u89c9\u673a\u5668\u4eba\u6240\u9700\u7684\u591a\u6a21\u6001\u89e6\u89c9\u611f\u77e5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.06776", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06776", "abs": "https://arxiv.org/abs/2512.06776", "authors": ["Yuchuan Tian", "Yuchen Liang", "Jiacheng Sun", "Shuo Zhang", "Guangwen Yang", "Yingte Shu", "Sibo Fang", "Tianyu Guo", "Kai Han", "Chao Xu", "Hanting Chen", "Xinghao Chen", "Yunhe Wang"], "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs", "comment": "13 pages, 4 figures", "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faNBDiff\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5757\u5927\u5c0f\u589e\u52a0\u548c\u4e0a\u4e0b\u6587\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u5c06\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u9002\u914d\u4e3a\u5757\u6269\u6563\u6a21\u578b\uff0c\u907f\u514d\u4ece\u5934\u8bad\u7ec3\u7684\u9ad8\u6210\u672c\u3002", "motivation": "\u81ea\u56de\u5f52\u89e3\u7801\u5b58\u5728\u987a\u5e8f\u751f\u6210\u7684\u541e\u5410\u91cf\u74f6\u9888\uff0c\u800c\u4ece\u5934\u8bad\u7ec3\u6269\u6563\u8bed\u8a00\u6a21\u578b\u6210\u672c\u9ad8\u6602\u4e14\u6d6a\u8d39\u73b0\u6709AR\u6a21\u578b\u7684\u77e5\u8bc6\u3002\u73b0\u6709\u9002\u914d\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3AR\u56e0\u679c\u6027\u4e0e\u5757\u53cc\u5411\u6027\u4e4b\u95f4\u7684\u6839\u672c\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u5c06AR\u89c6\u4e3a\u5757\u5927\u5c0f\u4e3a1\u7684\u5757\u6269\u6563\u6a21\u578b\uff0c\u8bbe\u8ba1\u6e10\u8fdb\u9002\u914d\u8def\u5f84\uff1a\u4f7f\u7528\u4e0a\u4e0b\u6587\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\uff08\u4e0a\u4e0b\u6587\u56e0\u679c\uff0c\u4ec5\u5728\u6d3b\u52a8\u5757\u5185\u53cc\u5411\uff09\u3001\u9ad8\u6548\u5e76\u884c\u9002\u914d\u8fc7\u7a0b\u3001\u8f85\u52a9AR\u635f\u5931\u4ee5\u6700\u5927\u5316\u6570\u636e\u5229\u7528\uff0c\u4ee5\u53ca\u9010\u6b65\u589e\u52a0\u751f\u6210\u5757\u5927\u5c0f\u3002", "result": "NBDiff-7B\u57287B\u7ea7\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u901a\u7528\u77e5\u8bc6\u3001\u6570\u5b66\u548c\u4ee3\u7801\u57fa\u51c6\u4e0a\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u4eceAR\u5230\u5757\u6269\u6563\u7684\u539f\u5219\u6027\u9002\u914d\u662f\u8bad\u7ec3\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4ece\u5934\u8bad\u7ec3\u7684\u9ad8\u6210\u672c\u3002"}}
{"id": "2512.06868", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06868", "abs": "https://arxiv.org/abs/2512.06868", "authors": ["Xingguang Zhong", "Liren Jin", "Marija Popovi\u0107", "Jens Behley", "Cyrill Stachniss"], "title": "Dynamic Visual SLAM using a General 3D Prior", "comment": "8 pages", "summary": "Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u52a8\u6001\u573a\u666f\u4e2d\u9c81\u68d2\u5730\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u8865\u4e01\u675f\u8c03\u6574\u548c\u524d\u9988\u91cd\u5efa\u6a21\u578b\u6765\u8fc7\u6ee4\u52a8\u6001\u533a\u57df\u5e76\u589e\u5f3aSLAM\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u81ea\u7136\u73af\u5883\u4e2d\uff0c\u573a\u666f\u52a8\u6001\u4f1a\u4e25\u91cd\u964d\u4f4e\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u800c\u53ef\u9760\u7684\u589e\u91cf\u5f0f\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c3D\u91cd\u5efa\u5bf9\u4e8e\u673a\u5668\u4eba\u3001\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u7ed3\u5408\u51e0\u4f55\u8865\u4e01\u5728\u7ebf\u675f\u8c03\u6574\u548c\u524d\u9988\u91cd\u5efa\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\u3002\u4f7f\u7528\u524d\u9988\u91cd\u5efa\u6a21\u578b\u7cbe\u786e\u8fc7\u6ee4\u52a8\u6001\u533a\u57df\uff0c\u5e76\u5229\u7528\u5176\u6df1\u5ea6\u9884\u6d4b\u589e\u5f3a\u57fa\u4e8e\u8865\u4e01\u7684\u89c6\u89c9SLAM\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5c06\u6df1\u5ea6\u9884\u6d4b\u4e0e\u675f\u8c03\u6574\u4f30\u8ba1\u7684\u8865\u4e01\u5bf9\u9f50\uff0c\u9c81\u68d2\u5730\u5904\u7406\u524d\u9988\u91cd\u5efa\u6a21\u578b\u6279\u91cf\u5e94\u7528\u65f6\u7684\u56fa\u6709\u5c3a\u5ea6\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u52a8\u6001\u573a\u666f\u4e2d\u9c81\u68d2\u5730\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\uff0c\u901a\u8fc7\u6df1\u5ea6\u9884\u6d4b\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86\u5c3a\u5ea6\u6a21\u7cca\u95ee\u9898\uff0c\u589e\u5f3a\u4e86SLAM\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u675f\u8c03\u6574\u548c\u524d\u9988\u91cd\u5efa\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u4e3a\u673a\u5668\u4eba\u3001AR\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06787", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06787", "abs": "https://arxiv.org/abs/2512.06787", "authors": ["Ofek Glick", "Vladimir Tchuiev", "Marah Ghoummaid", "Michal Moshkovitz", "Dotan Di-Castro"], "title": "LLM4SFC: Sequential Function Chart Generation via Large Language Models", "comment": null, "summary": "While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.", "AI": {"tldr": "LLM4SFC\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u53ef\u6267\u884c\u7684\u987a\u5e8f\u529f\u80fd\u56fe(SFC)\u7a0b\u5e8f\uff0c\u89e3\u51b3\u4e86\u56fe\u5f62\u5316PLC\u7f16\u7a0b\u8bed\u8a00\u751f\u6210\u7684\u6311\u6218\uff0c\u5728\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8fbe\u523075%-94%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5316PLC\u7f16\u7a0b\u8bed\u8a00(\u5982\u7ed3\u6784\u5316\u6587\u672c)\uff0c\u800cIEC 61131-3\u6807\u51c6\u7684\u56fe\u5f62\u5316\u8bed\u8a00(\u5982\u987a\u5e8f\u529f\u80fd\u56fe)\u751f\u6210\u7814\u7a76\u4e0d\u8db3\u3002SFC\u751f\u6210\u9762\u4e34\u56fe\u5f62\u5316\u7279\u6027\u548c\u5d4c\u5165\u5f0fST\u4ee3\u7801\u7684\u53cc\u91cd\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u4ea7\u751f\u4e0d\u53ef\u6267\u884c\u6216\u4e0e\u5de5\u4e1a\u5de5\u5177\u94fe\u4e0d\u517c\u5bb9\u7684\u4ee3\u7801\u3002", "method": "LLM4SFC\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7cbe\u7b80\u7ed3\u6784\u5316\u8868\u793a\uff0c\u6355\u83b7SFC\u62d3\u6251\u7ed3\u6784\u548c\u5d4c\u5165\u5f0fST\u4ee3\u7801\uff0c\u51cf\u5c11\u6587\u672c\u5197\u4f59\uff1b2) \u5fae\u8c03\u548c\u5c11\u6837\u672c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u5bf9\u9f50SFC\u7f16\u7a0b\u89c4\u8303\uff1b3) \u7ed3\u6784\u5316\u751f\u6210\u65b9\u6cd5\uff0c\u5b9e\u65f6\u4fee\u526a\u975e\u6cd5\u4ee4\u724c\u786e\u4fdd\u7b26\u5408SFC\u6587\u672c\u683c\u5f0f\u3002", "result": "\u5728\u81ea\u52a8\u5316\u5236\u9020\u9879\u76ee\u7684\u771f\u5b9eSFC\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u5f00\u6e90\u548c\u4e13\u6709LLM\uff0cLLM4SFC\u53ef\u9760\u5730\u751f\u6210\u8bed\u6cd5\u6709\u6548\u7684SFC\u7a0b\u5e8f\uff0c\u6210\u529f\u6865\u63a5\u56fe\u5f62\u5316\u548c\u6587\u672c\u5316PLC\u8bed\u8a00\uff0c\u751f\u6210\u6210\u529f\u7387\u572875%-94%\u4e4b\u95f4\u3002", "conclusion": "LLM4SFC\u662f\u9996\u4e2a\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u53ef\u6267\u884cSFC\u7684\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u5f62\u5316PLC\u7f16\u7a0b\u8bed\u8a00\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5316\u5de5\u4e1a\u7f16\u7a0b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.06892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06892", "abs": "https://arxiv.org/abs/2512.06892", "authors": ["Hassan Jardali", "Durgakant Pushp", "Youwei Yu", "Mahmoud Ali", "Ihab S. Mohamed", "Alejandro Murillo-Gonzalez", "Paul D. Coen", "Md. Al-Masrur Khan", "Reddy Charan Pulivendula", "Saeoul Park", "Lingchuan Zhou", "Lantao Liu"], "title": "From Zero to High-Speed Racing: An Autonomous Racing Stack", "comment": null, "summary": "High-speed, head-to-head autonomous racing presents substantial technical and logistical challenges, including precise localization, rapid perception, dynamic planning, and real-time control-compounded by limited track access and costly hardware. This paper introduces the Autonomous Race Stack (ARS), developed by the IU Luddy Autonomous Racing team for the Indy Autonomous Challenge (IAC). We present three iterations of our ARS, each validated on different tracks and achieving speeds up to 260 km/h. Our contributions include: (i) the modular architecture and evolution of the ARS across ARS1, ARS2, and ARS3; (ii) a detailed performance evaluation that contrasts control, perception, and estimation across oval and road-course environments; and (iii) the release of a high-speed, multi-sensor dataset collected from oval and road-course tracks. Our findings highlight the unique challenges and insights from real-world high-speed full-scale autonomous racing.", "AI": {"tldr": "IU Luddy\u56e2\u961f\u4e3aIndy Autonomous Challenge\u5f00\u53d1\u4e86\u81ea\u4e3b\u8d5b\u8f66\u5806\u6808ARS\uff0c\u7ecf\u8fc7\u4e09\u4ee3\u8fed\u4ee3\uff0c\u5728\u692d\u5706\u548c\u516c\u8def\u8d5b\u9053\u4e0a\u5b9e\u73b0\u6700\u9ad8260km/h\u901f\u5ea6\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u901f\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\u3002", "motivation": "\u9ad8\u901f\u5934\u5bf9\u5934\u81ea\u4e3b\u8d5b\u8f66\u9762\u4e34\u7cbe\u786e\u5b9a\u4f4d\u3001\u5feb\u901f\u611f\u77e5\u3001\u52a8\u6001\u89c4\u5212\u548c\u5b9e\u65f6\u63a7\u5236\u7b49\u6280\u672f\u6311\u6218\uff0c\u540c\u65f6\u53d7\u9650\u4e8e\u8d5b\u9053\u8bbf\u95ee\u548c\u6602\u8d35\u786c\u4ef6\u3002\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u81ea\u4e3b\u8d5b\u8f66\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e9b\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u7684\u81ea\u4e3b\u8d5b\u8f66\u5806\u6808ARS\uff0c\u5305\u542b\u4e09\u4ee3\u8fed\u4ee3\uff08ARS1\u3001ARS2\u3001ARS3\uff09\u3002\u7cfb\u7edf\u5305\u62ec\u5b9a\u4f4d\u3001\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u6a21\u5757\uff0c\u5728\u4e0d\u540c\u7c7b\u578b\u8d5b\u9053\uff08\u692d\u5706\u548c\u516c\u8def\u8d5b\u9053\uff09\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "ARS\u7cfb\u7edf\u5728\u771f\u5b9e\u8d5b\u9053\u4e0a\u5b9e\u73b0\u6700\u9ad8260km/h\u7684\u901f\u5ea6\uff0c\u5b8c\u6210\u4e86\u692d\u5706\u548c\u516c\u8def\u8d5b\u9053\u4e0a\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u5bf9\u6bd4\u4e86\u63a7\u5236\u3001\u611f\u77e5\u548c\u4f30\u8ba1\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u901f\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9645\u9ad8\u901f\u5168\u5c3a\u5bf8\u81ea\u4e3b\u8d5b\u8f66\u5b9e\u8df5\uff0c\u63ed\u793a\u4e86\u72ec\u7279\u7684\u6311\u6218\u548c\u89c1\u89e3\uff0c\u4e3a\u81ea\u4e3b\u8d5b\u8f66\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u53d1\u5e03\u7684\u7cfb\u7edf\u67b6\u6784\u548c\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2512.06812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06812", "abs": "https://arxiv.org/abs/2512.06812", "authors": ["Tiago Rodrigues", "Carla Teixeira Lopes"], "title": "Large Language Model-Based Generation of Discharge Summaries", "comment": "17 pages, 6 figures", "summary": "Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4f7f\u7528\u4e94\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff09\u81ea\u52a8\u751f\u6210\u51fa\u9662\u5c0f\u7ed3\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\uff08\u7279\u522b\u662fGemini\uff09\u8868\u73b0\u6700\u4f73\uff0c\u5f00\u6e90\u6a21\u578b\u867d\u6709\u671b\u4f46\u5b58\u5728\u5e7b\u89c9\u548c\u91cd\u590d\u4fe1\u606f\u95ee\u9898\u3002", "motivation": "\u51fa\u9662\u5c0f\u7ed3\u5305\u542b\u4e30\u5bcc\u7684\u60a3\u8005\u4fe1\u606f\uff0c\u81ea\u52a8\u5316\u751f\u6210\u53ef\u51cf\u8f7b\u533b\u62a4\u4eba\u5458\u8d1f\u62c5\u3001\u51cf\u5c11\u9519\u8bef\uff0c\u5e76\u786e\u4fdd\u5173\u952e\u4fe1\u606f\u6613\u4e8e\u83b7\u53d6\u548c\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u4e94\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08Mistral\u3001Llama 2\u3001GPT-3\u3001GPT-4\u3001Gemini 1.5 Pro\uff09\uff0c\u57fa\u4e8eMIMIC-III\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7cbe\u786e\u5339\u914d\u3001\u8f6f\u91cd\u53e0\u548c\u65e0\u53c2\u8003\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e13\u6709\u6a21\u578b\uff08\u7279\u522b\u662fGemini\u4f7f\u7528\u5355\u6837\u672c\u63d0\u793a\uff09\u8868\u73b0\u6700\u4f73\uff0c\u751f\u6210\u7684\u6458\u8981\u4e0e\u9ec4\u91d1\u6807\u51c6\u76f8\u4f3c\u5ea6\u6700\u9ad8\u3002\u5f00\u6e90\u6a21\u578b\uff08\u5c24\u5176\u662f\u5fae\u8c03\u540e\u7684Mistral\uff09\u8868\u73b0\u6709\u5e0c\u671b\u4f46\u4ecd\u843d\u540e\uff0c\u5e38\u51fa\u73b0\u5e7b\u89c9\u548c\u91cd\u590d\u4fe1\u606f\u95ee\u9898\u3002\u4e34\u5e8a\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f30\u786e\u8ba4\u4e13\u6709\u6a21\u578b\u751f\u6210\u7684\u6458\u8981\u5177\u6709\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5c24\u5176\u662f\u4e13\u6709\u6a21\u578b\uff09\u662f\u81ea\u52a8\u751f\u6210\u51fa\u9662\u5c0f\u7ed3\u7684\u6709\u5e0c\u671b\u5019\u9009\u65b9\u6848\uff0c\u4f46\u9700\u89e3\u51b3\u5e7b\u89c9\u548c\u7f3a\u5931\u4fe1\u606f\u7b49\u6311\u6218\uff0c\u5e76\u786e\u4fdd\u6570\u636e\u9690\u79c1\u3002"}}
{"id": "2512.06896", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06896", "abs": "https://arxiv.org/abs/2512.06896", "authors": ["Chrysostomos Karakasis", "Camryn Scully", "Robert Salati", "Panagiotis Artemiadis"], "title": "Control of Powered Ankle-Foot Prostheses on Compliant Terrain: A Quantitative Approach to Stability Enhancement", "comment": null, "summary": "Walking on compliant terrain presents a substantial challenge for individuals with lower-limb amputation, further elevating their already high risk of falling. While powered ankle-foot prostheses have demonstrated adaptability across speeds and rigid terrains, control strategies optimized for soft or compliant surfaces remain underexplored. This work experimentally validates an admittance-based control strategy that dynamically adjusts the quasi-stiffness of powered prostheses to enhance gait stability on compliant ground. Human subject experiments were conducted with three healthy individuals walking on two bilaterally compliant surfaces with ground stiffness values of 63 and 25 kN/m, representative of real-world soft environments. Controller performance was quantified using phase portraits and two walking stability metrics, offering a direct assessment of fall risk. Compared to a standard phase-variable controller developed for rigid terrain, the proposed admittance controller consistently improved gait stability across all compliant conditions. These results demonstrate the potential of adaptive, stability-aware prosthesis control to reduce fall risk in real-world environments and advance the robustness of human-prosthesis interaction in rehabilitation robotics.", "AI": {"tldr": "\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bfc\u7eb3\u7684\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u52a8\u529b\u5047\u80a2\u7684\u51c6\u521a\u5ea6\u6765\u589e\u5f3a\u5728\u67d4\u6027\u5730\u9762\u4e0a\u7684\u6b65\u6001\u7a33\u5b9a\u6027\uff0c\u76f8\u6bd4\u521a\u6027\u5730\u9762\u63a7\u5236\u5668\u663e\u8457\u6539\u5584\u4e86\u6b65\u6001\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e0b\u80a2\u622a\u80a2\u8005\u5728\u67d4\u6027\u5730\u9762\u4e0a\u884c\u8d70\u9762\u4e34\u66f4\u9ad8\u7684\u8dcc\u5012\u98ce\u9669\uff0c\u800c\u73b0\u6709\u7684\u52a8\u529b\u8e1d\u8db3\u5047\u80a2\u63a7\u5236\u7b56\u7565\u4e3b\u8981\u9488\u5bf9\u521a\u6027\u5730\u9762\u4f18\u5316\uff0c\u5bf9\u67d4\u6027\u6216\u987a\u5e94\u6027\u8868\u9762\u7684\u63a7\u5236\u7b56\u7565\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bfc\u7eb3\u7684\u63a7\u5236\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u52a8\u529b\u5047\u80a2\u7684\u51c6\u521a\u5ea6\u3002\u5728\u4e09\u4e2a\u5065\u5eb7\u53d7\u8bd5\u8005\u8eab\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u4e24\u79cd\u53cc\u8fb9\u67d4\u6027\u5730\u9762\uff08\u5730\u9762\u521a\u5ea6\u5206\u522b\u4e3a63\u548c25 kN/m\uff09\uff0c\u901a\u8fc7\u76f8\u4f4d\u56fe\u548c\u4e24\u4e2a\u884c\u8d70\u7a33\u5b9a\u6027\u6307\u6807\u8bc4\u4f30\u63a7\u5236\u5668\u6027\u80fd\u3002", "result": "\u76f8\u6bd4\u4e3a\u521a\u6027\u5730\u9762\u5f00\u53d1\u7684\u6807\u51c6\u76f8\u4f4d\u53d8\u91cf\u63a7\u5236\u5668\uff0c\u63d0\u51fa\u7684\u5bfc\u7eb3\u63a7\u5236\u5668\u5728\u6240\u6709\u67d4\u6027\u6761\u4ef6\u4e0b\u90fd\u4e00\u81f4\u6539\u5584\u4e86\u6b65\u6001\u7a33\u5b9a\u6027\uff0c\u76f4\u63a5\u964d\u4f4e\u4e86\u8dcc\u5012\u98ce\u9669\u3002", "conclusion": "\u81ea\u9002\u5e94\u3001\u7a33\u5b9a\u6027\u611f\u77e5\u7684\u5047\u80a2\u63a7\u5236\u7b56\u7565\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u964d\u4f4e\u8dcc\u5012\u98ce\u9669\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u589e\u5f3a\u5eb7\u590d\u673a\u5668\u4eba\u4e2d\u4eba-\u5047\u80a2\u4ea4\u4e92\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.06814", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06814", "abs": "https://arxiv.org/abs/2512.06814", "authors": ["Dibyanayan Bandyopadhyay", "Soham Bhattacharjee", "Mohammed Hasanuzzaman", "Asif Ekbal"], "title": "CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation", "comment": "Accepted at Transactions of the Association for Computational Linguistics (TACL). Pre-MIT Press publication version", "summary": "Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE", "AI": {"tldr": "CAuSE\u662f\u4e00\u4e2a\u4e3a\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5206\u7c7b\u5668\u751f\u6210\u5fe0\u5b9e\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u62bd\u8c61\u548c\u4ea4\u6362\u5e72\u9884\u786e\u4fdd\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027", "motivation": "\u591a\u6a21\u6001\u5206\u7c7b\u5668\u662f\u9ed1\u76d2\u6a21\u578b\uff0c\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u89c2\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u867d\u7136\u76f4\u89c2\uff0c\u4f46\u9700\u8981\u5fe0\u5b9e\u53cd\u6620\u5206\u7c7b\u5668\u7684\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8fd9\u662f\u5f53\u524d\u7814\u7a76\u7684\u5173\u952e\u6311\u6218", "method": "\u63d0\u51faCAuSE\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u6362\u5e72\u9884\u8bad\u7ec3\uff0c\u5f62\u6210\u5bf9\u5e95\u5c42\u5206\u7c7b\u5668\u7684\u56e0\u679c\u62bd\u8c61\uff0c\u751f\u6210\u5fe0\u5b9e\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca", "result": "CAuSE\u5728\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u56e0\u679c\u5fe0\u5b9e\u6027\u5ea6\u91cf\u4e0a\u8d85\u8d8a\u5176\u4ed6\u65b9\u6cd5\uff0c\u5b9a\u6027\u5206\u6790\u4e5f\u663e\u793a\u5176\u4f18\u52bf", "conclusion": "CAuSE\u80fd\u591f\u4e3a\u591a\u6a21\u6001\u5206\u7c7b\u5668\u751f\u6210\u5fe0\u5b9e\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u901a\u8fc7\u56e0\u679c\u62bd\u8c61\u786e\u4fdd\u89e3\u91ca\u7684\u53ef\u9760\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u9519\u8bef\u5206\u6790"}}
{"id": "2512.06897", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06897", "abs": "https://arxiv.org/abs/2512.06897", "authors": ["Bradley Hobbs", "Panagiotis Artemiadis"], "title": "Ground Compliance Improves Retention of Visual Feedback-Based Propulsion Training for Gait Rehabilitation", "comment": null, "summary": "This study investigates whether adding ground compliance to visual feedback (VF) gait training is more effective at increasing push-off force (POF) compared to using VF alone, with implications for gait rehabilitation. Ten healthy participants walked on a custom split-belt treadmill. All participants received real-time visual feedback of their ground reaction forces. One group also experienced changes in ground compliance, while a control group received only visual feedback. Intentional increases in propulsive ground reaction forces (POF) were successfully achieved and sustained post-intervention, especially in the group that experienced ground compliance. This group also demonstrated lasting after-effects in muscle activity and joint kinematics, indicating a more robust learning of natural strategies to increase propulsion. This work demonstrates how visual and proprioceptive systems coordinate during gait adaptation. It uniquely shows that combining ground compliance with visual feedback enhances the learning of propulsive forces, supporting the potential use of compliant terrain in long-term rehabilitation targeting propulsion deficits, such as those following a stroke.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u5728\u89c6\u89c9\u53cd\u9988\u6b65\u6001\u8bad\u7ec3\u4e2d\u52a0\u5165\u5730\u9762\u987a\u5e94\u6027\u6bd4\u5355\u72ec\u4f7f\u7528\u89c6\u89c9\u53cd\u9988\u66f4\u80fd\u6709\u6548\u589e\u52a0\u63a8\u8fdb\u529b\uff0c\u5bf9\u6b65\u6001\u5eb7\u590d\u6709\u91cd\u8981\u610f\u4e49\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u66f4\u6709\u6548\u5730\u589e\u52a0\u63a8\u8fdb\u529b\u4ee5\u6539\u5584\u6b65\u6001\u5eb7\u590d\u6548\u679c\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e2d\u98ce\u7b49\u75be\u75c5\u5bfc\u81f4\u7684\u63a8\u8fdb\u529b\u7f3a\u9677\u95ee\u9898\u3002", "method": "10\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u5728\u5b9a\u5236\u5206\u5e26\u8dd1\u6b65\u673a\u4e0a\u884c\u8d70\uff0c\u5b9e\u65f6\u63a5\u6536\u5730\u9762\u53cd\u4f5c\u7528\u529b\u7684\u89c6\u89c9\u53cd\u9988\u3002\u5b9e\u9a8c\u7ec4\u540c\u65f6\u4f53\u9a8c\u5730\u9762\u987a\u5e94\u6027\u53d8\u5316\uff0c\u5bf9\u7167\u7ec4\u4ec5\u63a5\u53d7\u89c6\u89c9\u53cd\u9988\u3002", "result": "\u63a8\u8fdb\u529b\u6210\u529f\u589e\u52a0\u5e76\u5728\u5e72\u9884\u540e\u6301\u7eed\uff0c\u7279\u522b\u662f\u5730\u9762\u987a\u5e94\u6027\u7ec4\u6548\u679c\u66f4\u663e\u8457\u3002\u8be5\u7ec4\u8fd8\u8868\u73b0\u51fa\u808c\u8089\u6d3b\u52a8\u548c\u5173\u8282\u8fd0\u52a8\u5b66\u7684\u6301\u4e45\u540e\u6548\u5e94\uff0c\u8868\u660e\u5bf9\u63a8\u8fdb\u7b56\u7565\u7684\u5b66\u4e60\u66f4\u7262\u56fa\u3002", "conclusion": "\u89c6\u89c9\u548c\u672c\u4f53\u611f\u89c9\u7cfb\u7edf\u5728\u6b65\u6001\u9002\u5e94\u4e2d\u534f\u540c\u5de5\u4f5c\uff0c\u5730\u9762\u987a\u5e94\u6027\u4e0e\u89c6\u89c9\u53cd\u9988\u7ed3\u5408\u80fd\u589e\u5f3a\u63a8\u8fdb\u529b\u7684\u5b66\u4e60\u6548\u679c\uff0c\u652f\u6301\u5728\u957f\u671f\u5eb7\u590d\u4e2d\u5e94\u7528\u987a\u5e94\u6027\u5730\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.06848", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06848", "abs": "https://arxiv.org/abs/2512.06848", "authors": ["Sepyan Purnama Kristanto", "Lutfi Hakim", "Hermansyah"], "title": "AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices", "comment": "9Pages, 3 figure, Politeknik Negeri Banyuwangi", "summary": "Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.", "AI": {"tldr": "AquaFusionNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u7edf\u4e00\u663e\u5fae\u955c\u6210\u50cf\u548c\u7269\u7406\u5316\u5b66\u4f20\u611f\u5668\u6570\u636e\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u6d4b\u5c0f\u578b\u996e\u7528\u6c34\u7cfb\u7edf\u7684\u5fae\u751f\u7269\u6c61\u67d3\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u76d1\u6d4b\u5de5\u5177\u53ea\u80fd\u6355\u6349\u5fae\u751f\u7269\u6c61\u67d3\u7684\u7247\u6bb5\u4fe1\u606f\uff0c\u663e\u5fae\u955c\u6210\u50cf\u548c\u7269\u7406\u5316\u5b66\u4f20\u611f\u5668\u6570\u636e\u9700\u8981\u5206\u5f00\u89e3\u8bfb\uff0c\u5bfc\u81f4\u5b9e\u65f6\u51b3\u7b56\u4e0d\u53ef\u9760\u3002\u5728\u996e\u7528\u6c34\u5b89\u5168\u9886\u57df\uff0c\u516c\u5f00\u7684\u5fae\u89c2\u6570\u636e\u96c6\u7a00\u7f3a\u3002", "method": "\u63d0\u51faAquaFusionNet\u6846\u67b6\uff0c\u901a\u8fc7\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u5fae\u751f\u7269\u5916\u89c2\u4e0e\u4f20\u611f\u5668\u52a8\u6001\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e13\u95e8\u4e3a\u4f4e\u529f\u8017\u786c\u4ef6\u8bbe\u8ba1\u3002\u4f7f\u7528\u65b0\u6570\u636e\u96c6AquaMicro12K\uff0812,846\u5f20\u996e\u7528\u6c34\u73af\u5883\u6807\u6ce8\u663e\u5fae\u56fe\u50cf\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u5370\u5ea6\u5c3c\u897f\u4e9a\u4e1c\u722a\u54c77\u4e2a\u8bbe\u65bd\u90e8\u7f726\u4e2a\u6708\uff0c\u5904\u7406184\u4e07\u5e27\u56fe\u50cf\uff0c\u6c61\u67d3\u4e8b\u4ef6\u68c0\u6d4b\u8fbe\u523094.8% mAP@0.5\uff0c\u5f02\u5e38\u9884\u6d4b\u51c6\u786e\u738796.3%\uff0c\u5728Jetson Nano\u4e0a\u529f\u8017\u4ec54.8W\u3002\u76f8\u6bd4\u5176\u4ed6\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5668\uff0c\u5728\u76f8\u540c\u6216\u66f4\u4f4e\u529f\u8017\u4e0b\u63d0\u4f9b\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8de8\u6a21\u6001\u8026\u5408\u51cf\u5c11\u4e86\u5355\u6a21\u6001\u68c0\u6d4b\u5668\u5728\u6c61\u67d3\u3001\u6d4a\u5ea6\u5cf0\u503c\u548c\u4e0d\u4e00\u81f4\u5149\u7167\u4e0b\u7684\u5e38\u89c1\u6545\u969c\u6a21\u5f0f\u3002\u6240\u6709\u6a21\u578b\u3001\u6570\u636e\u548c\u786c\u4ef6\u8bbe\u8ba1\u5df2\u5f00\u6e90\uff0c\u4fc3\u8fdb\u5206\u6563\u5f0f\u6c34\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u5236\u548c\u9002\u5e94\u3002"}}
{"id": "2512.06912", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06912", "abs": "https://arxiv.org/abs/2512.06912", "authors": ["Rushiraj Gadhvi", "Sandeep Manjanna"], "title": "Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields", "comment": "Under Review for International Conference on Robotics and Automation (ICRA 2026)", "summary": "For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.", "AI": {"tldr": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u6c34\u9762\u822a\u884c\u5668\u5728\u6da1\u6d41\u573a\u4e2d\u8282\u80fd\u5bfc\u822a\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u8282\u80fd30-50%", "motivation": "\u6a21\u4eff\u4f20\u7edf\u822a\u6d77\u5bb6\u5229\u7528\u6d0b\u6d41\u5bfc\u822a\u7684\u667a\u6167\uff0c\u89e3\u51b3\u81ea\u4e3b\u6c34\u9762\u822a\u884c\u5668\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u53d7\u4e25\u683c\u80fd\u91cf\u9884\u7b97\u9650\u5236\u7684\u6311\u6218\u3002\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u6da1\u6d41\u573a\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u57fa\u4e8eSoft Actor Critic\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5c40\u90e8\u901f\u5ea6\u6d4b\u91cf\u5b66\u4e60\u6d41\u573a\u611f\u77e5\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u5728\u591a\u6837\u5316\u548c\u52a8\u6001\u4e30\u5bcc\u7684\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u663e\u8457\u7684\u8282\u80fd\u6548\u679c\uff0c\u5e76\u80fd\u9c81\u68d2\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6d41\u573a\u6761\u4ef6\uff0c\u5bfc\u822a\u8def\u5f84\u6bd4\u73b0\u6709\u6280\u672f\u8282\u80fd30-50%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6d77\u6d0b\u73af\u5883\u4e2d\u957f\u671f\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u80fd\u91cf\u9ad8\u6548\u7684\u6c34\u9762\u822a\u884c\u5668\u5bfc\u822a\u3002"}}
{"id": "2512.06869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06869", "abs": "https://arxiv.org/abs/2512.06869", "authors": ["Wanyang Hong", "Zhaoning Zhang", "Yi Chen", "Libo Zhang", "Baihui Liu", "Linbo Qiao", "Zhiliang Tian", "Dongsheng Li"], "title": "Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.", "AI": {"tldr": "Rhea\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u5bf9\u8bdd\u5386\u53f2\u4e3a\u6307\u4ee4\u8bb0\u5fc6\u548c\u60c5\u666f\u8bb0\u5fc6\uff0c\u89e3\u51b3LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u7d2f\u79ef\u4e0a\u4e0b\u6587\u8870\u51cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "LLMs\u5728\u5355\u8f6e\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u662f\u7531\u4e8e\u6ce8\u610f\u529b\u6c61\u67d3\u3001\u7a00\u91ca\u548c\u6f02\u79fb\u5bfc\u81f4\u7684\u7d2f\u79ef\u4e0a\u4e0b\u6587\u8870\u51cf\u95ee\u9898", "method": "\u63d0\u51faRhea\u6846\u67b6\uff0c\u5c06\u5bf9\u8bdd\u5386\u53f2\u89e3\u8026\u4e3a\u4e24\u4e2a\u72ec\u7acb\u8bb0\u5fc6\u6a21\u5757\uff1a\u6307\u4ee4\u8bb0\u5fc6\uff08\u6301\u4e45\u5b58\u50a8\u5168\u5c40\u7ea6\u675f\uff09\u548c\u60c5\u666f\u8bb0\u5fc6\uff08\u52a8\u6001\u7ba1\u7406\u7528\u6237-\u6a21\u578b\u4ea4\u4e92\uff09\uff0c\u901a\u8fc7\u4f18\u5148\u7ea7\u6ce8\u610f\u529b\u6784\u5efa\u9ad8\u4fe1\u566a\u6bd4\u4e0a\u4e0b\u6587", "result": "\u5728\u591a\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRhea\u7f13\u89e3\u4e86\u6027\u80fd\u8870\u51cf\uff0c\u572810\u5206\u5236\u4e0a\u63d0\u9ad8\u4e861.04\u5206\uff08\u76f8\u5bf9\u57fa\u7ebf\u63d0\u534716%\uff09\uff0c\u5e76\u5728\u957f\u65f6\u4ea4\u4e92\u4e2d\u4fdd\u6301\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6307\u4ee4\u4fdd\u771f\u5ea6\uff08IAR > 8.1\uff09", "conclusion": "Rhea\u4e3a\u6784\u5efa\u66f4\u7cbe\u786e\u3001\u6307\u4ee4\u4e00\u81f4\u7684\u5bf9\u8bdd\u5f0fLLMs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u6709\u6548\u7684\u6846\u67b6"}}
{"id": "2512.06935", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06935", "abs": "https://arxiv.org/abs/2512.06935", "authors": ["Nicol\u00f2 Botteghi", "Owen Brook", "Urban Fasel", "Federico Califano"], "title": "Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs", "comment": null, "summary": "Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.\n  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u7684\u6570\u503c\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1IDA-PBC\u63a7\u5236\u5668\uff0c\u65e0\u9700\u7cbe\u786e\u6c42\u89e3\u5339\u914dPDEs\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u795e\u7ecfODE\u5b66\u4e60\uff0c\u4f7fIDA-PBC\u80fd\u5e94\u7528\u4e8e\u8d85\u8d8a\u7a33\u5b9a\u7684\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edfIDA-PBC\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e3b\u8981\u5c40\u9650\u4e8e\u5b66\u672f\u793a\u4f8b\u548c\u7a33\u5b9a\u5316\u4efb\u52a1\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u9700\u8981\u89e3\u6790\u6c42\u89e3\u590d\u6742\u7684\u5339\u914d\u6761\u4ef6PDEs\uff0c\u8fd9\u5bf9\u4e8e\u590d\u6742\u7269\u7406\u7cfb\u7edf\u548c\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u5c06IDA-PBC\u95ee\u9898\u8f6c\u5316\u4e3a\u795e\u7ecfODE\u5b66\u4e60\uff0c\u5229\u7528\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u5c06\u671f\u671b\u95ed\u73af\u7cfb\u7edf\u53c2\u6570\u5316\u4e3a\u975e\u7ebf\u6027\u72b6\u6001\u76f8\u5173\u51fd\u6570\u7684\u7a00\u758f\u7ebf\u6027\u7ec4\u5408\uff0c\u901a\u8fc7\u6c42\u89e3\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff08\u5305\u542b\u4efb\u52a1\u76f8\u5173\u6210\u672c\u548c\u5339\u914d\u6761\u4ef6\u76f8\u5173\u6210\u672c\uff09\u6765\u4f18\u5316\u63a7\u5236\u5668\u53c2\u6570\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f7fIDA-PBC\u80fd\u591f\u5e94\u7528\u4e8e\u8d85\u8d8a\u7a33\u5b9a\u7684\u590d\u6742\u4efb\u52a1\uff08\u5982\u53d1\u73b0\u5468\u671f\u6027\u632f\u8361\u884c\u4e3a\uff09\uff0c\u5e76\u80fd\u63a8\u5bfc\u51fa\u5305\u542b\u6b8b\u5dee\u9879\u7684\u95ed\u73af\u7cfb\u7edf\u5c01\u95ed\u5f62\u5f0f\u8868\u8fbe\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u503c\u65b9\u6cd5\u514b\u670d\u4e86\u4f20\u7edfIDA-PBC\u89e3\u6790\u6c42\u89e3\u5339\u914dPDEs\u7684\u5c40\u9650\u6027\uff0c\u6269\u5c55\u4e86IDA-PBC\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2512.06874", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06874", "abs": "https://arxiv.org/abs/2512.06874", "authors": ["Ziyun Yu", "Yiru Zhou", "Chen Zhao", "Hongyi Wen"], "title": "An Analysis of Large Language Models for Simulating User Responses in Surveys", "comment": "Accepted to IJCNLP-AACL 2025 (Main Conference)", "summary": "Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.", "AI": {"tldr": "LLMs\u5728\u6a21\u62df\u7528\u6237\u89c2\u70b9\u65f6\u5b58\u5728\u504f\u89c1\uff0c\u96be\u4ee5\u51c6\u786e\u4ee3\u8868\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u548c\u6587\u5316\u80cc\u666f\u7684\u7528\u6237\uff0c\u5373\u4f7f\u91c7\u7528CLAIMSIM\u65b9\u6cd5\u63d0\u5347\u591a\u6837\u6027\uff0c\u4ecd\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u7528\u6237\u56de\u7b54\u3002", "motivation": "LLMs\uff08\u7279\u522b\u662f\u7ecf\u8fc7RLHF\u8bad\u7ec3\u7684\u6a21\u578b\uff09\u503e\u5411\u4e8e\u8868\u73b0\u51fa\u5bf9\u4e3b\u6d41\u89c2\u70b9\u7684\u504f\u89c1\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u80fd\u5426\u4ee3\u8868\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u548c\u6587\u5316\u80cc\u666f\u7528\u6237\u7684\u62c5\u5fe7\u3002\u9700\u8981\u8bc4\u4f30LLMs\u5728\u6a21\u62df\u4eba\u7c7b\u5bf9\u8de8\u9886\u57df\u8c03\u67e5\u95ee\u9898\u56de\u7b54\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u76f4\u63a5\u63d0\u793a\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faCLAIMSIM\u65b9\u6cd5\uff08\u4e3b\u5f20\u591a\u6837\u5316\u65b9\u6cd5\uff09\uff0c\u8be5\u65b9\u6cd5\u4eceLLM\u7684\u53c2\u6570\u77e5\u8bc6\u4e2d\u5f15\u51fa\u89c2\u70b9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u3002\u5728\u8c03\u67e5\u95ee\u9898\u56de\u7b54\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136CLAIMSIM\u80fd\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u56de\u7b54\uff0c\u4f46\u4e24\u79cd\u65b9\u6cd5\u90fd\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u7528\u6237\u3002\u5206\u6790\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1\uff09LLMs\u503e\u5411\u4e8e\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u95f4\u4fdd\u6301\u56fa\u5b9a\u89c2\u70b9\uff0c\u751f\u6210\u5355\u4e00\u89c6\u89d2\u7684\u4e3b\u5f20\uff1b2\uff09\u5f53\u9762\u5bf9\u76f8\u4e92\u51b2\u7a81\u7684\u4e3b\u5f20\u65f6\uff0cLLMs\u96be\u4ee5\u63a8\u7406\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u7279\u5b9a\u7528\u6237\u753b\u50cf\u7684\u80fd\u529b\u3002", "conclusion": "LLMs\u5728\u6a21\u62df\u7528\u6237\u89c2\u70b9\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u591a\u6837\u5316\u7684\u7528\u6237\u7fa4\u4f53\u65b9\u9762\u3002\u5373\u4f7f\u91c7\u7528CLAIMSIM\u7b49\u65b9\u6cd5\u6765\u63d0\u5347\u56de\u7b54\u591a\u6837\u6027\uff0cLLMs\u4ecd\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u7528\u6237\u7684\u56de\u7b54\u6a21\u5f0f\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u5dee\u5f02\u65f6\u3002"}}
{"id": "2512.06951", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06951", "abs": "https://arxiv.org/abs/2512.06951", "authors": ["Ilia Larchenko", "Gleb Zarin", "Akash Karnatak"], "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge", "comment": "2025 NeurIPS Behavior Challenge 1st place solution", "summary": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.\n  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.\n  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePi0.5\u67b6\u6784\u7684\u89c6\u89c9-\u52a8\u4f5c\u7b56\u7565\uff0c\u5728BEHAVIOR 2025\u6311\u6218\u8d5b\u4e2d\u83b7\u7b2c\u4e00\u540d\uff0c\u901a\u8fc7\u76f8\u5173\u566a\u58f0\u6d41\u5339\u914d\u3001\u53ef\u5b66\u4e60\u6df7\u5408\u5c42\u6ce8\u610f\u529b\u7b49\u521b\u65b0\uff0c\u572850\u4e2a\u5bb6\u5ead\u4efb\u52a1\u4e2d\u5b9e\u73b026% q-score\u3002", "motivation": "\u89e3\u51b3BEHAVIOR\u6311\u6218\u4e2d\u7684\u590d\u6742\u5bb6\u5ead\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u53cc\u624b\u64cd\u4f5c\u3001\u5bfc\u822a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u957f\u671f\u89c6\u91ce\u548c\u591a\u6837\u5316\u7684\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u57fa\u4e8ePi0.5\u67b6\u6784\uff0c\u5f15\u5165\u76f8\u5173\u566a\u58f0\u6d41\u5339\u914d\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u6df7\u5408\u5c42\u6ce8\u610f\u529b\u548cSystem 2\u9636\u6bb5\u8ddf\u8e2a\u89e3\u51b3\u6b67\u4e49\uff0c\u8bad\u7ec3\u91c7\u7528\u591a\u6837\u672c\u6d41\u5339\u914d\u51cf\u5c11\u65b9\u5dee\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u52a8\u4f5c\u538b\u7f29\u548c\u6311\u6218\u7279\u5b9a\u4fee\u6b63\u89c4\u5219\u3002", "result": "\u5728BEHAVIOR 2025\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u572850\u4e2a\u591a\u6837\u5316\u5bb6\u5ead\u4efb\u52a1\u4e2d\u5b9e\u73b026% q-score\uff0c\u5728\u516c\u5f00\u548c\u79c1\u6709\u6392\u884c\u699c\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u76f8\u5173\u566a\u58f0\u6d41\u5339\u914d\u7b49\u521b\u65b0\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u52a8\u4f5c\u7b56\u7565\u5728\u590d\u6742\u957f\u671f\u5bb6\u5ead\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06919", "abs": "https://arxiv.org/abs/2512.06919", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla"], "title": "Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles", "comment": "13 pages, 2 figures", "summary": "The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5386\u53f2\u5b89\u5168\u6570\u636e\u7684\u81ea\u52a8\u5316PRO-CTCAE\u9879\u76ee\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7MedDRA\u8bed\u4e49\u6620\u5c04\u548c\u8c31\u5206\u6790\u5e73\u8861\u4fe1\u53f7\u8986\u76d6\u4e0e\u60a3\u8005\u8d1f\u62c5", "motivation": "PRO-CTCAE\u9879\u76ee\u9009\u62e9\u9762\u4e34\u4e24\u96be\uff1a\u9009\u62e9\u8fc7\u591a\u4f1a\u589e\u52a0\u60a3\u8005\u8d1f\u62c5\u964d\u4f4e\u4f9d\u4ece\u6027\uff0c\u9009\u62e9\u8fc7\u5c11\u53ef\u80fd\u9057\u6f0f\u91cd\u8981\u5b89\u5168\u4fe1\u53f7\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u4f18\u5316\u9009\u62e9", "method": "\u5c06PRO-CTCAE\u75c7\u72b6\u6620\u5c04\u5230MedDRA\u672f\u8bed\uff0c\u7f16\u7801\u5230Safeterm\u8bed\u4e49\u7a7a\u95f4\uff0c\u7ed3\u5408\u76f8\u5173\u6027\u548c\u53d1\u751f\u7387\u8ba1\u7b97\u6548\u7528\u51fd\u6570\uff0c\u901a\u8fc7\u8c31\u5206\u6790\u8bc6\u522b\u6b63\u4ea4\u533b\u5b66\u6982\u5ff5\u96c6\uff0c\u6309\u91cd\u8981\u6027\u6392\u5e8f\u5e76\u57fa\u4e8e\u89e3\u91ca\u4fe1\u606f\u786e\u5b9a\u622a\u65ad\u70b9", "result": "\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u80bf\u7624\u5b66\u6848\u4f8b\u7814\u7a76\u4e2d\u8bc4\u4f30\uff0c\u4f5c\u4e3aSafeterm\u8bd5\u9a8c\u5b89\u5168\u5e94\u7528\u7684\u4e00\u90e8\u5206\u5b9e\u73b0\uff0c\u63d0\u4f9b\u5ba2\u89c2\u53ef\u91cd\u590d\u7684PRO-CTCAE\u8bbe\u8ba1\u65b9\u6cd5", "conclusion": "\u81ea\u52a8\u5316\u65b9\u6cd5\u5229\u7528MedDRA\u8bed\u4e49\u548c\u5386\u53f2\u6570\u636e\u7b80\u5316PRO-CTCAE\u8bbe\u8ba1\uff0c\u5e73\u8861\u4fe1\u53f7\u8986\u76d6\u4e0e\u60a3\u8005\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4e34\u5e8a\u8bd5\u9a8c\u6548\u7387"}}
{"id": "2512.06963", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06963", "abs": "https://arxiv.org/abs/2512.06963", "authors": ["Yichao Shen", "Fangyun Wei", "Zhiying Du", "Yaobo Liang", "Yan Lu", "Jiaolong Yang", "Nanning Zheng", "Baining Guo"], "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators", "comment": "Project page: https://videovla-nips2025.github.io", "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.", "AI": {"tldr": "VideoVLA\u5c06\u5927\u578b\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f6c\u5316\u4e3a\u673a\u5668\u4ebaVLA\u64cd\u4f5c\u5668\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u89c6\u9891\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u6a21\u6001\uff0c\u9884\u6d4b\u52a8\u4f5c\u5e8f\u5217\u548c\u672a\u6765\u89c6\u89c9\u7ed3\u679c\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u3001\u65b0\u7269\u4f53\u548c\u65b0\u73af\u5883\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u8303\u5f0f\u6765\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u8fdb\u884c\u8054\u5408\u89c6\u89c9\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u7ed9\u5b9a\u8bed\u8a00\u6307\u4ee4\u548c\u56fe\u50cf\uff0c\u9884\u6d4b\u52a8\u4f5c\u5e8f\u5217\u548c\u672a\u6765\u89c6\u89c9\u7ed3\u679c\u3002", "result": "\u9ad8\u8d28\u91cf\u7684\u672a\u6765\u89c6\u89c9\u60f3\u8c61\u4e0e\u53ef\u9760\u7684\u52a8\u4f5c\u9884\u6d4b\u548c\u4efb\u52a1\u6210\u529f\u76f8\u5173\uff0cVideoVLA\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u6a21\u4eff\u5176\u4ed6\u5177\u8eab\u6280\u80fd\u548c\u5904\u7406\u65b0\u7269\u4f53\u3002", "conclusion": "\u540c\u65f6\u9884\u6d4b\u52a8\u4f5c\u53ca\u5176\u89c6\u89c9\u540e\u679c\u7684\u53cc\u91cd\u9884\u6d4b\u7b56\u7565\u63a2\u7d22\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\uff0c\u91ca\u653e\u4e86\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.06922", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.06922", "abs": "https://arxiv.org/abs/2512.06922", "authors": ["George Mikros"], "title": "Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI", "comment": null, "summary": "Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.", "AI": {"tldr": "LLMs\u5728\u53f8\u6cd5\u8bed\u8a00\u5b66\u4e2d\u65e2\u662f\u5206\u6790\u5de5\u5177\u53c8\u662f\u6311\u6218\u6765\u6e90\uff0c\u9700\u8981\u65b9\u6cd5\u91cd\u6784\u4ee5\u4fdd\u6301\u79d1\u5b66\u53ef\u4fe1\u5ea6\u548c\u6cd5\u5f8b\u53ef\u91c7\u6027", "motivation": "LLMs\u5728\u53f8\u6cd5\u8bed\u8a00\u5b66\u4e2d\u5448\u73b0\u53cc\u91cd\u6311\u6218\uff1a\u4e00\u65b9\u9762\u4f5c\u4e3a\u5f3a\u5927\u7684\u5206\u6790\u5de5\u5177\uff0c\u53e6\u4e00\u65b9\u9762\u901a\u8fc7\u98ce\u683c\u6a21\u4eff\u3001\u4f5c\u8005\u6df7\u6dc6\u548c\u5408\u6210\u6587\u672c\u6269\u6563\u7834\u574f\u8bed\u8a00\u7279\u5f81\u7684\u57fa\u672c\u5047\u8bbe\uff0c\u8fd9\u5bf9\u53f8\u6cd5\u5b9e\u8df5\u4e2d\u7684\u4f5c\u8005\u8bc6\u522b\u548c\u6cd5\u5f8b\u53ef\u91c7\u6027\u6807\u51c6\u6784\u6210\u5a01\u80c1", "method": "\u5206\u6790\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\u3001\u98ce\u683c\u8ba1\u91cf\u5b66\u65b9\u6cd5\u548c\u6c34\u5370\u65b9\u6cd5\uff0c\u6307\u51fa\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\uff08\u7279\u522b\u662f\u5bf9\u975e\u6bcd\u8bed\u82f1\u8bed\u5199\u4f5c\u8005\uff09\u548c\u5bf9\u6297\u7b56\u7565\u8106\u5f31\u6027\u7b49\u95ee\u9898", "result": "\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u6280\u672f\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u53f8\u6cd5\u53ef\u91c7\u6027\u6807\u51c6\uff08\u5982Daubert\u548cKumho Tire\u6846\u67b6\uff09\uff0c\u9700\u8981\u65b9\u6cd5\u8bba\u91cd\u6784", "conclusion": "\u53f8\u6cd5\u8bed\u8a00\u5b66\u9700\u8981\u8fdb\u884c\u65b9\u6cd5\u8bba\u91cd\u6784\uff0c\u5305\u62ec\u91c7\u7528\u6df7\u5408\u4eba\u673a\u5de5\u4f5c\u6d41\u7a0b\u3001\u8d85\u8d8a\u4e8c\u5143\u5206\u7c7b\u7684\u53ef\u89e3\u91ca\u68c0\u6d4b\u8303\u5f0f\uff0c\u4ee5\u53ca\u6d4b\u91cf\u4e0d\u540c\u7fa4\u4f53\u9519\u8bef\u548c\u504f\u89c1\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u540c\u65f6\u4fdd\u6301\"\u8bed\u8a00\u63ed\u793a\u751f\u4ea7\u8005\u4fe1\u606f\"\u7684\u6838\u5fc3\u89c1\u89e3"}}
{"id": "2512.06995", "categories": ["cs.RO", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2512.06995", "abs": "https://arxiv.org/abs/2512.06995", "authors": ["Maryam Seraj", "Mohammad Hossein Kamrava", "Carlo Tiseo"], "title": "Parametric Design of a Cable-Driven Coaxial Spherical Parallel Mechanism for Ultrasound Scans", "comment": null, "summary": "Haptic interfaces play a critical role in medical teleoperation by enabling surgeons to interact with remote environments through realistic force and motion feedback. Achieving high fidelity in such systems requires balancing performance trade-off among workspace, dexterity, stiffness, inertia, and bandwidth, particularly in applications demanding pure rotational motion. This paper presents the design methodology and kinematic analysis of a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) developed to address these challenges. The proposed cable-driven interface design allows for reducing the mass placed at the robot arm end-effector, thereby minimizing inertial loads, enhancing stiffness, and improving dynamic responsiveness. Through parallel and coaxial actuation, the mechanism achieves decoupled rotational degrees of freedom with isotropic force and torque transmission. Simulation and analysis demonstrate that the CDC-SPM provides accurate, responsive, and safe motion characteristics suitable for high-precision haptic applications. These results highlight the mechanism's potential for medical teleoperation tasks such as ultrasound imaging, where precise and intuitive manipulation is essential.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7535\u7f06\u9a71\u52a8\u540c\u8f74\u7403\u9762\u5e76\u8054\u673a\u6784(CDC-SPM)\uff0c\u7528\u4e8e\u533b\u7597\u9065\u64cd\u4f5c\u4e2d\u7684\u89e6\u89c9\u63a5\u53e3\uff0c\u901a\u8fc7\u51cf\u5c11\u672b\u7aef\u6267\u884c\u5668\u8d28\u91cf\u6765\u63d0\u5347\u52a8\u6001\u6027\u80fd\uff0c\u5b9e\u73b0\u89e3\u8026\u7684\u65cb\u8f6c\u81ea\u7531\u5ea6\u3002", "motivation": "\u533b\u7597\u9065\u64cd\u4f5c\u4e2d\u89e6\u89c9\u63a5\u53e3\u9700\u8981\u9ad8\u4fdd\u771f\u5ea6\uff0c\u4f46\u9762\u4e34\u5de5\u4f5c\u7a7a\u95f4\u3001\u7075\u6d3b\u6027\u3001\u521a\u5ea6\u3001\u60ef\u6027\u548c\u5e26\u5bbd\u4e4b\u95f4\u7684\u6027\u80fd\u6743\u8861\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7eaf\u65cb\u8f6c\u8fd0\u52a8\u7684\u5e94\u7528\u4e2d\u3002", "method": "\u91c7\u7528\u7535\u7f06\u9a71\u52a8\u540c\u8f74\u7403\u9762\u5e76\u8054\u673a\u6784(CDC-SPM)\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5e76\u884c\u548c\u540c\u8f74\u9a71\u52a8\u5b9e\u73b0\u89e3\u8026\u7684\u65cb\u8f6c\u81ea\u7531\u5ea6\uff0c\u51cf\u5c11\u672b\u7aef\u6267\u884c\u5668\u8d28\u91cf\u4ee5\u964d\u4f4e\u60ef\u6027\u8d1f\u8f7d\u3002", "result": "\u4eff\u771f\u5206\u6790\u8868\u660eCDC-SPM\u63d0\u4f9b\u51c6\u786e\u3001\u54cd\u5e94\u8fc5\u901f\u4e14\u5b89\u5168\u7684\u8fd0\u52a8\u7279\u6027\uff0c\u9002\u5408\u9ad8\u7cbe\u5ea6\u89e6\u89c9\u5e94\u7528\uff0c\u5177\u6709\u5404\u5411\u540c\u6027\u7684\u529b\u548c\u626d\u77e9\u4f20\u9012\u80fd\u529b\u3002", "conclusion": "\u8be5\u673a\u6784\u5728\u533b\u7597\u9065\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5982\u8d85\u58f0\u6210\u50cf\u7b49\u9700\u8981\u7cbe\u786e\u76f4\u89c2\u64cd\u4f5c\u7684\u5e94\u7528\uff0c\u80fd\u591f\u5e73\u8861\u89e6\u89c9\u63a5\u53e3\u7684\u5173\u952e\u6027\u80fd\u53c2\u6570\u3002"}}
{"id": "2512.06924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06924", "abs": "https://arxiv.org/abs/2512.06924", "authors": ["Milad Alshomary", "Anisha Bhatnagar", "Peter Zeng", "Smaranda Muresan", "Owen Rambow", "Kathleen McKeown"], "title": "XAM: Interactive Explainability for Authorship Attribution Models", "comment": null, "summary": "We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.", "AI": {"tldr": "IXAM\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u57fa\u4e8e\u5d4c\u5165\u7684\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\uff0c\u5141\u8bb8\u7528\u6237\u63a2\u7d22\u5d4c\u5165\u7a7a\u95f4\u5e76\u6784\u5efa\u591a\u7c92\u5ea6\u5199\u4f5c\u98ce\u683c\u7279\u5f81\u89e3\u91ca", "motivation": "\u73b0\u6709\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\u7f3a\u4e4f\u4ea4\u4e92\u5f0f\u89e3\u91ca\u5de5\u5177\uff0c\u7528\u6237\u96be\u4ee5\u7406\u89e3\u6a21\u578b\u9884\u6d4b\u80cc\u540e\u7684\u5199\u4f5c\u98ce\u683c\u7279\u5f81\uff0c\u9700\u8981\u66f4\u76f4\u89c2\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6", "method": "\u5f00\u53d1\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91ca\u6027\u6846\u67b6IXAM\uff0c\u652f\u6301\u7528\u6237\u63a2\u7d22\u5d4c\u5165\u7a7a\u95f4\uff0c\u6784\u5efa\u591a\u7c92\u5ea6\u5199\u4f5c\u98ce\u683c\u7279\u5f81\u89e3\u91ca\uff0c\u5305\u62ec\u7528\u6237\u8bc4\u4f30\u9a8c\u8bc1\u6846\u67b6\u4ef7\u503c", "result": "\u7528\u6237\u8bc4\u4f30\u663e\u793aIXAM\u76f8\u6bd4\u9884\u5b9a\u4e49\u98ce\u683c\u89e3\u91ca\u5177\u6709\u66f4\u9ad8\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u6a21\u578b\u9884\u6d4b", "conclusion": "IXAM\u4e3a\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4ea4\u4e92\u5f0f\u89e3\u91ca\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u900f\u660e\u5ea6\u548c\u7528\u6237\u7406\u89e3\u80fd\u529b"}}
{"id": "2512.07032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07032", "abs": "https://arxiv.org/abs/2512.07032", "authors": ["Runcong Wang", "Fengyi Wang", "Gordon Cheng"], "title": "A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator", "comment": null, "summary": "This paper presents a hetero-associative sequential memory system for mobile manipulators that learns compact, neuromorphic bindings between robot joint states and tactile observations to produce step-wise action decisions with low compute and memory cost. The method encodes joint angles via population place coding and converts skin-measured forces into spike-rate features using an Izhikevich neuron model; both signals are transformed into bipolar binary vectors and bound element-wise to create associations stored in a large-capacity sequential memory. To improve separability in binary space and inject geometry from touch, we introduce 3D rotary positional embeddings that rotate subspaces as a function of sensed force direction, enabling fuzzy retrieval through a softmax weighted recall over temporally shifted action patterns. On a Toyota Human Support Robot covered by robot skin, the hetero-associative sequential memory system realizes a pseudocompliance controller that moves the link under touch in the direction and with speed correlating to the amplitude of applied force, and it retrieves multi-joint grasp sequences by continuing tactile input. The system sets up quickly, trains from synchronized streams of states and observations, and exhibits a degree of generalization while remaining economical. Results demonstrate single-joint and full-arm behaviors executed via associative recall, and suggest extensions to imitation learning, motion planning, and multi-modal integration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u5f02\u8d28\u5173\u8054\u987a\u5e8f\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u7ed1\u5b9a\u5173\u8282\u72b6\u6001\u4e0e\u89e6\u89c9\u89c2\u6d4b\uff0c\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u7684\u9010\u6b65\u52a8\u4f5c\u51b3\u7b56\u3002", "motivation": "\u79fb\u52a8\u673a\u68b0\u81c2\u9700\u8981\u80fd\u591f\u5728\u89e6\u89c9\u4ea4\u4e92\u4e2d\u505a\u51fa\u5feb\u901f\u3001\u7ecf\u6d4e\u7684\u52a8\u4f5c\u51b3\u7b56\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5904\u7406\u8fde\u7eed\u72b6\u6001-\u89c2\u6d4b\u5173\u8054\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u540c\u6b65\u72b6\u6001\u6d41\u4e2d\u5b66\u4e60\u7d27\u51d1\u7ed1\u5b9a\u3001\u652f\u6301\u6a21\u7cca\u68c0\u7d22\u5e76\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002", "method": "1) \u4f7f\u7528\u7fa4\u4f53\u4f4d\u7f6e\u7f16\u7801\u8868\u793a\u5173\u8282\u89d2\u5ea6\uff0cIzhikevich\u795e\u7ecf\u5143\u6a21\u578b\u5c06\u76ae\u80a4\u6d4b\u5f97\u7684\u529b\u8f6c\u6362\u4e3a\u8109\u51b2\u7387\u7279\u5f81\uff1b2) \u5c06\u4e24\u79cd\u4fe1\u53f7\u8f6c\u6362\u4e3a\u53cc\u6781\u4e8c\u8fdb\u5236\u5411\u91cf\u5e76\u8fdb\u884c\u5143\u7d20\u7ea7\u7ed1\u5b9a\uff1b3) \u5f15\u51653D\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u6839\u636e\u611f\u77e5\u7684\u529b\u65b9\u5411\u65cb\u8f6c\u5b50\u7a7a\u95f4\u4ee5\u6539\u5584\u4e8c\u8fdb\u5236\u7a7a\u95f4\u7684\u53ef\u5206\u79bb\u6027\uff1b4) \u901a\u8fc7softmax\u52a0\u6743\u53ec\u56de\u5728\u65f6\u95f4\u504f\u79fb\u52a8\u4f5c\u6a21\u5f0f\u4e0a\u5b9e\u73b0\u6a21\u7cca\u68c0\u7d22\u3002", "result": "\u5728\u4e30\u7530\u4eba\u673a\u652f\u6301\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u4f2a\u5408\u89c4\u63a7\u5236\u5668\uff0c\u4f7f\u8fde\u6746\u5728\u89e6\u6478\u4e0b\u6cbf\u65bd\u52a0\u529b\u7684\u65b9\u5411\u548c\u901f\u5ea6\u79fb\u52a8\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u89e6\u89c9\u8f93\u5165\u68c0\u7d22\u591a\u5173\u8282\u6293\u53d6\u5e8f\u5217\u3002\u7cfb\u7edf\u8bbe\u7f6e\u5feb\u901f\uff0c\u4ece\u540c\u6b65\u72b6\u6001\u6d41\u4e2d\u8bad\u7ec3\uff0c\u5177\u6709\u6cdb\u5316\u80fd\u529b\u4e14\u7ecf\u6d4e\u9ad8\u6548\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u901a\u8fc7\u5173\u8054\u53ec\u56de\u6267\u884c\u5355\u5173\u8282\u548c\u5168\u81c2\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u4e3a\u6a21\u4eff\u5b66\u4e60\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u591a\u6a21\u6001\u96c6\u6210\u63d0\u4f9b\u4e86\u6269\u5c55\u53ef\u80fd\u6027\uff0c\u5b9e\u73b0\u4e86\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u89e6\u89c9\u9a71\u52a8\u52a8\u4f5c\u51b3\u7b56\u3002"}}
{"id": "2512.06938", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06938", "abs": "https://arxiv.org/abs/2512.06938", "authors": ["Ivanho\u00e9 Botcazou", "Tassadit Amghar", "Sylvain Lamprier", "Fr\u00e9d\u00e9ric Saubion"], "title": "Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation", "comment": null, "summary": "Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.", "AI": {"tldr": "\u63d0\u51faProgress Ratio Embeddings (PRE)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u5d4c\u5165\u548c\u4e09\u89d2\u51fd\u6570\u4fe1\u53f7\u5b9e\u73b0\u7a33\u5065\u7684\u6587\u672c\u751f\u6210\u957f\u5ea6\u63a7\u5236\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u65f6\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5bf9\u751f\u6210\u957f\u5ea6\u7684\u7cbe\u786e\u63a7\u5236\u4ecd\u7136\u4e0d\u8db3\u3002\u73b0\u6709\u57fa\u4e8e\u53cd\u5411\u4f4d\u7f6e\u5d4c\u5165(RPE)\u7684\u65b9\u6cd5\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u65f6\u5b58\u5728\u9650\u5236\uff0c\u7279\u522b\u662f\u4f7f\u7528\u4e0e\u7edd\u5bf9\u5269\u4f59token\u8ba1\u6570\u76f8\u5173\u7684\u79bb\u6563\u5012\u8ba1\u65f6\u4fe1\u53f7\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faProgress Ratio Embeddings (PRE)\u65b9\u6cd5\uff0c\u4f7f\u7528\u8fde\u7eed\u5d4c\u5165\u4e0e\u4e09\u89d2\u51fd\u6570\u4e0d\u8010\u70e6\u4fe1\u53f7\u76f8\u7ed3\u5408\u3002PRE\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6Transformer\u67b6\u6784\u4e2d\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u957f\u5ea6\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u4e0b\u7684\u6587\u672c\u51c6\u786e\u6027\u3002", "result": "PRE\u5728\u4e24\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u65b0\u95fb\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u957f\u5ea6\u63a7\u5236\u800c\u4e0d\u964d\u4f4e\u6587\u672c\u8d28\u91cf\uff0c\u5e76\u4e14\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u957f\u5ea6\u3002", "conclusion": "PRE\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u957f\u5ea6\u63a7\u5236\u6280\u672f\u66f4\u52a0\u7a33\u5065\uff0c\u7279\u522b\u662f\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u4e3a\u6587\u672c\u751f\u6210\u957f\u5ea6\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07041", "abs": "https://arxiv.org/abs/2512.07041", "authors": ["Hiroki Sawada", "Alexandre Pitti", "Mathias Quoy"], "title": "CERNet: Class-Embedding Predictive-Coding RNN for Unified Robot Motion, Recognition, and Confidence Estimation", "comment": null, "summary": "Robots interacting with humans must not only generate learned movements in real-time, but also infer the intent behind observed behaviors and estimate the confidence of their own inferences. This paper proposes a unified model that achieves all three capabilities within a single hierarchical predictive-coding recurrent neural network (PC-RNN) equipped with a class embedding vector, CERNet, which leverages a dynamically updated class embedding vector to unify motor generation and recognition. The model operates in two modes: generation and inference. In the generation mode, the class embedding constrains the hidden state dynamics to a class-specific subspace; in the inference mode, it is optimized online to minimize prediction error, enabling real-time recognition. Validated on a humanoid robot across 26 kinesthetically taught alphabets, our hierarchical model achieves 76% lower trajectory reproduction error than a parameter-matched single-layer baseline, maintains motion fidelity under external perturbations, and infers the demonstrated trajectory class online with 68% Top-1 and 81% Top-2 accuracy. Furthermore, internal prediction errors naturally reflect the model's confidence in its recognition. This integration of robust generation, real-time recognition, and intrinsic uncertainty estimation within a compact PC-RNN framework offers a compact and extensible approach to motor memory in physical robots, with potential applications in intent-sensitive human-robot collaboration.", "AI": {"tldr": "\u63d0\u51faCERNet\uff0c\u4e00\u4e2a\u57fa\u4e8e\u9884\u6d4b\u7f16\u7801\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u80fd\u540c\u65f6\u5b9e\u73b0\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u3001\u5b9e\u65f6\u610f\u56fe\u8bc6\u522b\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1", "motivation": "\u673a\u5668\u4eba\u9700\u8981\u540c\u65f6\u5177\u5907\u5b9e\u65f6\u8fd0\u52a8\u751f\u6210\u3001\u884c\u4e3a\u610f\u56fe\u63a8\u65ad\u548c\u63a8\u65ad\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u8fd9\u4e9b\u529f\u80fd\u5206\u5f00\u5904\u7406\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6", "method": "\u4f7f\u7528\u5e26\u7c7b\u522b\u5d4c\u5165\u5411\u91cf\u7684\u5206\u5c42\u9884\u6d4b\u7f16\u7801\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08PC-RNN\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u7684\u7c7b\u522b\u5d4c\u5165\u5411\u91cf\u7edf\u4e00\u8fd0\u52a8\u751f\u6210\u548c\u8bc6\u522b\u3002\u6a21\u578b\u6709\u4e24\u79cd\u6a21\u5f0f\uff1a\u751f\u6210\u6a21\u5f0f\u4e2d\u7c7b\u522b\u5d4c\u5165\u7ea6\u675f\u9690\u85cf\u72b6\u6001\u5230\u7c7b\u522b\u7279\u5b9a\u5b50\u7a7a\u95f4\uff1b\u63a8\u65ad\u6a21\u5f0f\u4e2d\u5728\u7ebf\u4f18\u5316\u7c7b\u522b\u5d4c\u5165\u4ee5\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee", "result": "\u5728\u4eba\u5f62\u673a\u5668\u4eba26\u4e2a\u5b57\u6bcd\u624b\u52bf\u4efb\u52a1\u4e2d\uff0c\u5206\u5c42\u6a21\u578b\u6bd4\u53c2\u6570\u5339\u914d\u7684\u5355\u5c42\u57fa\u7ebf\u8f68\u8ff9\u590d\u5236\u8bef\u5dee\u964d\u4f4e76%\uff0c\u5728\u5916\u90e8\u6270\u52a8\u4e0b\u4fdd\u6301\u8fd0\u52a8\u4fdd\u771f\u5ea6\uff0c\u5728\u7ebf\u63a8\u65ad\u8f68\u8ff9\u7c7b\u522b\u8fbe\u523068% Top-1\u548c81% Top-2\u51c6\u786e\u7387\uff0c\u5185\u90e8\u9884\u6d4b\u8bef\u5dee\u81ea\u7136\u53cd\u6620\u8bc6\u522b\u7f6e\u4fe1\u5ea6", "conclusion": "CERNet\u5728\u7d27\u51d1\u7684PC-RNN\u6846\u67b6\u5185\u6574\u5408\u4e86\u9c81\u68d2\u751f\u6210\u3001\u5b9e\u65f6\u8bc6\u522b\u548c\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e3a\u7269\u7406\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u4e14\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u5728\u610f\u56fe\u654f\u611f\u7684\u4eba\u673a\u534f\u4f5c\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2512.06991", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06991", "abs": "https://arxiv.org/abs/2512.06991", "authors": ["Jing Jie Tan", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum", "Anissa Mokraoui", "Shih-Yu Lo"], "title": "Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models", "comment": "16 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel \"Prompting-in-a-Series\" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \\textit{gpt4o} from OpenAI and \\textit{gemini} from Google, along with open-source models like \\textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.", "AI": {"tldr": "PICEPR\u662f\u4e00\u79cd\u65b0\u9896\u7684\"Prompting-in-a-Series\"\u7b97\u6cd5\uff0c\u901a\u8fc7\u5185\u5bb9\u548c\u5d4c\u5165\u4e24\u6761\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u6a21\u5757\u5316\u89e3\u7801\u5668LLM\u8fdb\u884c\u4eba\u683c\u8bc6\u522b\uff0c\u6027\u80fd\u63d0\u53475-15%\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u66f4\u597d\u5730\u5229\u7528LLM\u8fdb\u884c\u4eba\u683c\u8bc6\u522b\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5fc3\u7406\u5b66\u4fe1\u606f\u5316\u7684\u5185\u5bb9\u5d4c\u5165\u65b9\u6cd5\uff0c\u63d0\u5347\u4eba\u683c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faPICEPR\u7b97\u6cd5\uff0c\u5305\u542b\u4e24\u6761\u6d41\u6c34\u7ebf\uff1a(a)\u5185\u5bb9\u6d41\u6c34\u7ebf\uff1a\u5229\u7528\u6a21\u5757\u5316\u89e3\u7801\u5668LLM\u603b\u7ed3\u6216\u751f\u6210\u5185\u5bb9\uff1b(b)\u5d4c\u5165\u6d41\u6c34\u7ebf\uff1a\u4f5c\u4e3a\u4eba\u683c\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4eba\u683c\u4e30\u5bcc\u5185\u5bb9\u751f\u6210\u5668\u3002\u540c\u65f6\u5bf9\u6bd4\u4e86\u95ed\u6e90\u6a21\u578b(gpt4o, gemini)\u548c\u5f00\u6e90\u6a21\u578b(mistral)\u3002", "result": "PICEPR\u7b97\u6cd5\u5728\u4eba\u683c\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e865-15%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u540c\u65f6\u5bf9\u6bd4\u4e86\u4e0d\u540cLLM\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u3002", "conclusion": "PICEPR\u7b97\u6cd5\u901a\u8fc7\u5fc3\u7406\u5b66\u4fe1\u606f\u5316\u7684\u5185\u5bb9\u5d4c\u5165\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u683c\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3aLLM\u5728\u4eba\u683c\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2512.07091", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07091", "abs": "https://arxiv.org/abs/2512.07091", "authors": ["Tomoya Takahashi", "Yusaku Nakajima", "Cristian Camilo Beltran-Hernandez", "Yuki Kuroda", "Kazutoshi Tanaka", "Masashi Hamaya", "Kanta Ono", "Yoshitaka Ushiku"], "title": "A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling", "comment": "9 pages, 8 figures", "summary": "Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u7684\u67d4\u6027\u6f0f\u6597\u5f62\u673a\u68b0\u624b\uff0c\u901a\u8fc7\u53ef\u63a7\u9600\u95e8\u548c\u57fa\u4e8e\u7c89\u672b\u6d41\u52a8\u6a21\u578b\u7684\u53cd\u9988\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u6beb\u514b\u7ea7\u7c89\u672b\u7684\u7cbe\u786e\u79f0\u91cf\u3002", "motivation": "\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u5728\u56fa\u6001\u6750\u6599\u53d1\u73b0\u4e2d\u5177\u6709\u52a0\u901f\u6f5c\u529b\uff0c\u4f46\u6beb\u514b\u7ea7\u7c89\u672b\u5904\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u7c89\u672b\u6d41\u52a8\u52a8\u529b\u5b66\u590d\u6742\u4e14\u5b9e\u9a8c\u5ba4\u4efb\u52a1\u591a\u6837\u3002\u73b0\u6709\u7cfb\u7edf\u96be\u4ee5\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u7c89\u672b\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6f0f\u6597\u5f62\u67d4\u6027\u673a\u68b0\u624b\uff0c\u4fdd\u7559\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u67d4\u8f6f\u6027\u548c\u9525\u5f62\u7247\u8bbe\u8ba1\uff0c\u4f46\u5728\u9525\u9876\u52a0\u5165\u4e86\u53ef\u63a7\u9600\u95e8\uff0c\u5b9e\u73b0\u6beb\u514b\u7ea7\u7c89\u672b\u7684\u7cbe\u786e\u589e\u91cf\u5206\u914d\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u57fa\u4e8e\u7c89\u672b\u6d41\u52a8\u6a21\u578b\u548c\u5728\u7ebf\u53c2\u6570\u8bc6\u522b\u7684\u53cd\u9988\u63a7\u5236\u4e0e\u5916\u90e8\u5929\u5e73\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c80%\u7684\u8bd5\u9a8c\u8bef\u5dee\u57282\u6beb\u514b\u4ee5\u5185\uff0c\u6700\u5927\u8bef\u5dee\u7ea6\u4e3a20\u6beb\u514b\uff08\u76ee\u6807\u8303\u56f420\u6beb\u514b\u81f33\u514b\uff09\u3002\u4e0e\u76f4\u63a5PID\u63a7\u5236\u76f8\u6bd4\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u7c89\u672b\u52a8\u529b\u5b66\u7684\u53d8\u5316\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u9ad8\u6548\u7075\u6d3b\u7684\u7c89\u672b\u79f0\u91cf\u6f5c\u529b\uff0c\u5177\u6709\u5411\u66f4\u5927\u6570\u91cf\u6269\u5c55\u7684\u53ef\u884c\u6027\uff0c\u5e76\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4efb\u52a1\uff0c\u4e3a\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u7684\u7c89\u672b\u5904\u7406\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07015", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07015", "abs": "https://arxiv.org/abs/2512.07015", "authors": ["Mayank Ravishankara"], "title": "FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to \"hallucinate with citations.\"\n  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing \"Self-Correction\" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates \"Kill Queries\"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this \"Anti-Context.\" Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time \"Red Team\" for factual generation.", "AI": {"tldr": "FVA-RAG \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u68c0\u7d22\u7b56\u7565\u4e3b\u52a8\u5bfb\u627e\u53cd\u9a73\u8bc1\u636e\uff0c\u51cf\u5c11\u56e0\u7528\u6237\u504f\u89c1\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5b58\u5728\"\u68c0\u7d22\u5949\u627f\"\u6f0f\u6d1e\uff1a\u5f53\u7528\u6237\u67e5\u8be2\u57fa\u4e8e\u9519\u8bef\u524d\u63d0\u6216\u5e38\u89c1\u8bef\u89e3\u65f6\uff0c\u5411\u91cf\u68c0\u7d22\u5668\u503e\u5411\u4e8e\u83b7\u53d6\u7b26\u5408\u7528\u6237\u504f\u89c1\u7684\u6587\u6863\uff0c\u5bfc\u81f4\u6a21\u578b\"\u5e26\u5f15\u7528\u7684\u5e7b\u89c9\"\u3002", "method": "\u63d0\u51faFVA-RAG\u6846\u67b6\uff0c\u5c06\u68c0\u7d22\u8303\u5f0f\u4ece\u5f52\u7eb3\u9a8c\u8bc1\u8f6c\u5411\u6f14\u7ece\u8bc1\u4f2a\u3002\u91c7\u7528\u5bf9\u6297\u6027\u68c0\u7d22\u7b56\u7565\u751f\u6210\"\u81f4\u547d\u67e5\u8be2\"\u6765\u5bfb\u627e\u77db\u76fe\u8bc1\u636e\uff0c\u5e76\u5f15\u5165\u53cc\u91cd\u9a8c\u8bc1\u673a\u5236\u6743\u8861\u8349\u7a3f\u7b54\u6848\u4e0e\"\u53cd\u4e0a\u4e0b\u6587\"\u3002", "result": "\u5728\u5e38\u89c1\u8bef\u89e3\u6570\u636e\u96c6\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cFVA-RAG\u76f8\u6bd4\u6807\u51c6RAG\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5949\u627f\u6027\u5e7b\u89c9\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FVA-RAG\u901a\u8fc7\u4e3b\u52a8\u5bfb\u627e\u53cd\u9a73\u8bc1\u636e\u7684\u5bf9\u6297\u6027\u68c0\u7d22\u7b56\u7565\uff0c\u6709\u6548\u51cf\u5c11\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u5949\u627f\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u53ef\u4f5c\u4e3a\u4e8b\u5b9e\u751f\u6210\u7684\u63a8\u7406\u65f6\"\u7ea2\u961f\"\u3002"}}
{"id": "2512.07114", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07114", "abs": "https://arxiv.org/abs/2512.07114", "authors": ["Jue Wang", "Mingsong Jiang", "Luis A. Ramirez", "Bilige Yang", "Mujun Zhang", "Esteban Figueroa", "Wenzhong Yan", "Rebecca Kramer-Bottiglio"], "title": "Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots", "comment": null, "summary": "Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u67d4\u987a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u521a\u4f53\u6a21\u62df\u5668\u4e2d\u5f15\u5165\u4ee3\u8868\u8f6f\u6750\u6599\u53d8\u5f62\u7684\u95f4\u63a5\u53d8\u91cf\uff0c\u5b9e\u73b0\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u9ad8\u6548\u6a21\u62df\u4e0e\u63a7\u5236\u5b66\u4e60", "motivation": "\u81ea\u9002\u5e94\u5f62\u6001\u53d1\u751f\u673a\u5668\u4eba\u9700\u8981\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u548c\u73af\u5883\u6761\u4ef6\uff0c\u4f46\u8f6f\u4f53\u7ec4\u4ef6\u7684\u6a21\u62df\u548c\u63a7\u5236\u5b58\u5728\u6311\u6218\uff1a\u8f6f\u4f53\u6a21\u62df\u5668\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u6709\u9650\uff0c\u800c\u521a\u4f53\u6a21\u62df\u5668\u65e0\u6cd5\u6355\u6349\u8f6f\u6750\u6599\u52a8\u529b\u5b66", "method": "\u91c7\u7528\u66ff\u4ee3\u67d4\u987a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u4e0d\u663e\u5f0f\u6a21\u62df\u8f6f\u4f53\u7269\u7406\uff0c\u800c\u662f\u5728\u521a\u4f53\u6a21\u62df\u5668\u4e2d\u5f15\u5165\u4ee3\u8868\u8f6f\u6750\u6599\u53d8\u5f62\u7684\u95f4\u63a5\u53d8\u91cf\uff08\u6709\u6548\u80a2\u4f53\u957f\u5ea6\u548c\u80a2\u4f53\u8d28\u5fc3\u53d8\u5316\uff09\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u8fd9\u4e9b\u95f4\u63a5\u53d8\u91cf\u8fdb\u884c\u5e7f\u6cdb\u968f\u673a\u5316", "result": "\u5728\u521a\u4f53\u6a21\u62df\u5668\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7b56\u7565\u5b66\u4e60\uff0c\u7b56\u7565\u53ef\u76f4\u63a5\u8f6c\u79fb\u5230\u786c\u4ef6\u4e0a\uff0c\u5728\u786c\u8d28\u5e73\u5766\u57fa\u5e95\u4e0a\u8868\u73b0\u51fa\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5728\u6d41\u53d8\u590d\u6742\u5730\u5f62\u4e0a\u8f6c\u79fb\u4fdd\u771f\u5ea6\u8f83\u4f4e\u4f46\u4ecd\u7a33\u5065\uff1b\u5b66\u4e60\u5230\u7684\u95ed\u73af\u6b65\u6001\u5c55\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u5730\u9762\u673a\u52a8\u6027\uff0c\u8fd0\u8f93\u6210\u672c\u6bd4\u5f00\u73af\u57fa\u7ebf\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u9ad8\u6548\u6a21\u62df\u4e0e\u63a7\u5236\uff0c\u9a8c\u8bc1\u4e86\u66ff\u4ee3\u67d4\u987a\u6027\u5efa\u6a21\u5728\u81ea\u9002\u5e94\u5f62\u6001\u53d1\u751f\u673a\u5668\u4eba\u4e2d\u7684\u6709\u6548\u6027\uff0c\u673a\u5668\u4eba\u80fd\u591f\u5728\u591a\u79cd\u81ea\u7136\u5730\u5f62\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u7684\u591a\u6b65\u6001\u8fd0\u52a8"}}
{"id": "2512.07059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07059", "abs": "https://arxiv.org/abs/2512.07059", "authors": ["Richard Young"], "title": "Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models", "comment": "30 pages, 11 figures, 5 tables. Code and data: https://github.com/ricyoung/tempest-replication", "summary": "Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u6280\u672f\u5bf9\u591a\u8f6e\u5bf9\u6297\u653b\u51fb\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u4e0d\u9884\u6d4b\u9c81\u68d2\u6027\uff0c\u800c\u6269\u5c55\u63a8\u7406\u6a21\u5f0f\u53ef\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027", "motivation": "\u5c3d\u7ba1\u5728\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u6295\u5165\u4e86\u5927\u91cf\u8d44\u6e90\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u591a\u8f6e\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\u4ecd\u672a\u88ab\u5145\u5206\u8868\u5f81\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u6216\u63a8\u7406\u6a21\u5f0f\u662f\u5426\u5f71\u54cd\u9c81\u68d2\u6027\u5c1a\u4e0d\u6e05\u695a", "method": "\u4f7f\u7528TEMPEST\u591a\u8f6e\u653b\u51fb\u6846\u67b6\u8bc4\u4f3010\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u8986\u76d68\u4e2a\u4f9b\u5e94\u5546\u548c1000\u79cd\u6709\u5bb3\u884c\u4e3a\uff0c\u751f\u6210\u8d85\u8fc797,000\u4e2aAPI\u67e5\u8be2\uff0c\u901a\u8fc7\u72ec\u7acb\u5b89\u5168\u5206\u7c7b\u5668\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30", "result": "\u6a21\u578b\u8106\u5f31\u6027\u5448\u73b0\u5149\u8c31\u5206\u5e03\uff1a6\u4e2a\u6a21\u578b\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u8fbe96%-100%\uff0c4\u4e2a\u6a21\u578b\u8868\u73b0\u51fa\u6709\u610f\u4e49\u7684\u62b5\u6297\uff08ASR 42%-78%\uff09\uff1b\u5728\u76f8\u540c\u67b6\u6784\u4e0a\u542f\u7528\u6269\u5c55\u63a8\u7406\u53ef\u5c06ASR\u4ece97%\u964d\u81f342%", "conclusion": "\u5f53\u524d\u5bf9\u9f50\u6280\u672f\u5bf9\u81ea\u9002\u5e94\u591a\u8f6e\u653b\u51fb\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u4e0d\u9884\u6d4b\u9c81\u68d2\u6027\uff0c\u800c\u5ba1\u614e\u63a8\u7406\u6a21\u5f0f\u662f\u53ef\u90e8\u7f72\u7684\u5b89\u5168\u589e\u5f3a\u65b9\u5411"}}
{"id": "2512.07130", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07130", "abs": "https://arxiv.org/abs/2512.07130", "authors": ["Zebin Xing", "Yupeng Zheng", "Qichao Zhang", "Zhixing Ding", "Pengxuan Yang", "Songen Gu", "Zhongpu Xia", "Dongbin Zhao"], "title": "Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving", "AI": {"tldr": "Mimir\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u5c42\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1\u76ee\u6807\u70b9\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u901f\u7387\u5f15\u5bfc\u673a\u5236\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u8f68\u8ff9\u751f\u6210\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e0d\u51c6\u786e\u7684\u9ad8\u5c42\u5f15\u5bfc\u4fe1\u53f7\u548c\u590d\u6742\u5f15\u5bfc\u6a21\u5757\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u53c8\u80fd\u63d0\u9ad8\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMimir\u6846\u67b6\uff1a1) \u4f7f\u7528\u62c9\u666e\u62c9\u65af\u5206\u5e03\u4f30\u8ba1\u76ee\u6807\u70b9\u4e0d\u786e\u5b9a\u6027\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\uff1b2) \u5f15\u5165\u591a\u901f\u7387\u5f15\u5bfc\u673a\u5236\u63d0\u524d\u9884\u6d4b\u6269\u5c55\u76ee\u6807\u70b9\u4ee5\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u5728Navhard\u548cNavtest\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMimir\u5728\u9a7e\u9a76\u8bc4\u5206EPDMS\u4e0a\u6bd4\u4e4b\u524d\u6700\u4f18\u65b9\u6cd5\u63d0\u534720%\uff0c\u540c\u65f6\u9ad8\u5c42\u6a21\u5757\u63a8\u7406\u901f\u5ea6\u63d0\u53471.6\u500d\u4e14\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "conclusion": "Mimir\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u591a\u901f\u7387\u5f15\u5bfc\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9ad8\u5c42\u5f15\u5bfc\u4e0d\u51c6\u786e\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u53cc\u91cd\u63d0\u5347\u3002"}}
{"id": "2512.07068", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07068", "abs": "https://arxiv.org/abs/2512.07068", "authors": ["Emma Markle", "Javier Gutierrez Bach", "Shira Wein"], "title": "SETUP: Sentence-level English-To-Uniform Meaning Representation Parser", "comment": null, "summary": "Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u82f1\u8bed\u6587\u672c\u5230UMR\u89e3\u6790\u7684\u65b9\u6cd5\uff0c\u5176\u4e2dSETUP\u6a21\u578b\u5728AnCast\u548cSMATCH++\u8bc4\u5206\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8UMR\u89e3\u6790\u7684\u53d1\u5c55\u3002", "motivation": "UMR\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u5f0f\u8bed\u4e49\u8868\u793a\uff0c\u80fd\u591f\u6355\u6349\u6587\u672c\u6838\u5fc3\u542b\u4e49\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u8a00\uff08\u5305\u62ec\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\uff0c\u4f46\u9700\u8981\u81ea\u52a8\u5316\u7684\u6587\u672c\u5230UMR\u89e3\u6790\u5668\u6765\u5b9e\u73b0\u5927\u89c4\u6a21\u5e94\u7528\u3002\u76ee\u524d\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fd8\u5f88\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u5fae\u8c03\u73b0\u6709\u7684\u62bd\u8c61\u610f\u4e49\u8868\u793a\u89e3\u6790\u5668\uff1b2\uff09\u5229\u7528\u4ece\u901a\u7528\u4f9d\u5b58\u5173\u7cfb\u8f6c\u6362\u5668\u3002\u5c06\u5148\u524d\u5de5\u4f5c\u4f5c\u4e3a\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6700\u4f73\u6a21\u578bSETUP\u5728AnCast\u8bc4\u5206\u4e0a\u8fbe\u523084\u5206\uff0cSMATCH++\u8bc4\u5206\u8fbe\u523091\u5206\uff0c\u663e\u793a\u51fa\u5728\u81ea\u52a8UMR\u89e3\u6790\u65b9\u9762\u7684\u663e\u8457\u8fdb\u6b65\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u7279\u522b\u662fSETUP\u6a21\u578b\uff0c\u5728\u82f1\u8bed\u6587\u672c\u5230UMR\u89e3\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u8fdb\u5c55\uff0c\u4e3aUMR\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u5927\u89c4\u6a21\u81ea\u52a8\u751f\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.07137", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.07137", "abs": "https://arxiv.org/abs/2512.07137", "authors": ["Kang Yijie", "Hao Yuqing", "Wang Qingyun", "Chen Guanrong"], "title": "Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework", "comment": "10 pages,9 figures", "summary": "In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.", "AI": {"tldr": "\u57fa\u4e8e\u5e7f\u4e49Udwadia-Kalaba\u6846\u67b6\uff0c\u7814\u7a76\u4e86\u5177\u6709\u533a\u57df\u7ea6\u675f\u7684\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u63a7\u5236\u95ee\u9898", "motivation": "\u73b0\u6709\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u63a7\u5236\u7814\u7a76\u672a\u8003\u8651\u533a\u57df\u7ea6\u675f\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u7f16\u961f\u8ddf\u8e2a\u548c\u533a\u57df\u7ea6\u675f\u7684\u63a7\u5236\u7b56\u7565", "method": "\u91c7\u7528\u5e7f\u4e49Udwadia-Kalaba\u6846\u67b6\uff0c\u5c06\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u63a7\u5236\u76ee\u6807\u91cd\u6784\u4e3a\u7ea6\u675f\u65b9\u7a0b\uff0c\u901a\u8fc7\u5fae\u5206\u540c\u80da\u53d8\u6362\u5904\u7406\u533a\u57df\u7ea6\u675f\uff0c\u5728\u6709\u5411\u52a0\u6743\u901a\u4fe1\u62d3\u6251\u4e0b\u8bbe\u8ba1\u63a7\u5236\u5668", "result": "\u6210\u529f\u8bbe\u8ba1\u4e86\u5177\u6709\u533a\u57df\u7ea6\u675f\u7684\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5e7f\u4e49Udwadia-Kalaba\u6846\u67b6\u7684\u63a7\u5236\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u548c\u533a\u57df\u7ea6\u675f\uff0c\u786e\u4fdd\u4e86\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u4f18\u52bf"}}
{"id": "2512.07075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07075", "abs": "https://arxiv.org/abs/2512.07075", "authors": ["Shiwei Guo", "Sihang Jiang", "Qianxi He", "Yanghua Xiao", "Jiaqing Liang", "Bi Yude", "Minggui He", "Shimin Tao", "Li Zhang"], "title": "Do Large Language Models Truly Understand Cross-cultural Differences?", "comment": null, "summary": "In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u57fa\u4e8e\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8de8\u6587\u5316\u6838\u5fc3\u6982\u5ff5\u5bf9\u9f50\u548c\u751f\u6210\u5f0f\u4efb\u52a1\u8bbe\u8ba1\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6587\u5316\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b4530\u4e2a\u6d4b\u8bd5\u9879\uff0c\u8986\u76d69\u4e2a\u6587\u5316\u7ef4\u5ea6\u548c15\u4e2a\u73b0\u5b9e\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLMs\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u7f3a\u4e4f\u60c5\u5883\u573a\u666f\u3001\u8de8\u6587\u5316\u6982\u5ff5\u6620\u5c04\u4e0d\u8db3\u3001\u6df1\u5c42\u6587\u5316\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u6765\u68c0\u9a8cLLMs\u662f\u5426\u771f\u6b63\u5177\u5907\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u6587\u5316\u7406\u8bba\u5c06\u8de8\u6587\u5316\u80fd\u529b\u5206\u4e3a9\u4e2a\u7ef4\u5ea6\uff0c\u9009\u53d6210\u4e2a\u6838\u5fc3\u6982\u5ff5\uff0c\u57284\u5927\u7c7b\u8de8\u6587\u5316\u60c5\u5883\u4e0b\u6784\u5efa15\u4e2a\u5177\u4f53\u73b0\u5b9e\u573a\u666f\uff0c\u6309\u7167\u9879\u76ee\u8bbe\u8ba1\u539f\u5219\u521b\u5efa4530\u4e2a\u6d4b\u8bd5\u9879\uff0c\u652f\u6301\u6570\u636e\u96c6\u6301\u7eed\u6269\u5c55\u548c\u591a\u8bed\u8a00\u8fc1\u79fb\u3002", "result": "SAGE\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86LLMs\u5728\u8de8\u6587\u5316\u63a8\u7406\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u5c40\u9650\uff0c\u6a21\u578b\u5728\u5404\u4e2a\u7ef4\u5ea6\u548c\u573a\u666f\u4e2d\u90fd\u8868\u73b0\u51fa\u5f31\u70b9\u3002\u867d\u7136\u5df2\u6709\u8fdb\u6b65\uff0c\u4f46LLMs\u8ddd\u79bb\u771f\u6b63\u7ec6\u81f4\u7684\u8de8\u6587\u5316\u7406\u89e3\u8fd8\u6709\u5dee\u8ddd\u3002", "conclusion": "SAGE\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709\u8de8\u6587\u5316\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30LLMs\u7684\u8de8\u6587\u5316\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.07177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07177", "abs": "https://arxiv.org/abs/2512.07177", "authors": ["Fanjun Bu", "Melina Tsai", "Audrey Tjokro", "Tapomayukh Bhattacharjee", "Jorge Ortiz", "Wendy Ju"], "title": "Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction", "comment": null, "summary": "Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u7ba1\u9053\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u611f\u77e5\u68c0\u6d4b\u5668\uff08\u6ce8\u89c6\u8f6c\u79fb\u548c\u7a7a\u95f4\u8ddd\u79bb\uff09\u5728\u793e\u4ea4\u5173\u952e\u65f6\u523b\u89e6\u53d1\u57fa\u4e8e\u89c6\u9891\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\uff0c\u4ee5\u5b9e\u73b0\u793e\u4ea4\u54cd\u5e94\u5f0f\u673a\u5668\u4eba\u884c\u4e3a\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u9700\u8981\u51b3\u5b9a\u4f55\u65f6\u4ee5\u53ca\u662f\u5426\u4e0e\u4eba\u4e92\u52a8\uff0c\u8fd9\u79cd\u51b3\u7b56\u5f80\u5f80\u4f9d\u8d56\u4e8e\u968f\u65f6\u95f4\u5c55\u5f00\u7684\u5fae\u5999\u975e\u8bed\u8a00\u7ebf\u7d22\uff0c\u8fd9\u4e9b\u7ebf\u7d22\u96be\u4ee5\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u57fa\u4e8e5\u5929Wizard-of-Oz\u90e8\u7f72\u7684\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u9996\u5148\u4f7f\u7528\u8f7b\u91cf\u7ea7\u611f\u77e5\u68c0\u6d4b\u5668\uff08\u6ce8\u89c6\u8f6c\u79fb\u548c\u7a7a\u95f4\u8ddd\u79bb\uff09\u68c0\u6d4b\u793e\u4ea4\u5173\u952e\u65f6\u523b\uff0c\u7136\u540e\u89e6\u53d1\u57fa\u4e8e\u89c6\u9891\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\u8fdb\u884c\u793e\u4ea4\u63a8\u7406\u3002", "result": "\u5728\u91cd\u653e\u7684\u73b0\u573a\u4ea4\u4e92\u4e2d\u8bc4\u4f30\u8be5\u7ba1\u9053\u5e76\u6bd4\u8f83\u4e24\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u53d1\u73b0\u9009\u62e9\u6027\u4f7f\u7528VLM\u4f5c\u4e3a\u793e\u4ea4\u63a8\u7406\u4ee3\u7406\u80fd\u591f\u5b9e\u73b0\u793e\u4ea4\u54cd\u5e94\u5f0f\u673a\u5668\u4eba\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u793e\u4ea4\u63a8\u7406\u4ee3\u7406\uff0c\u673a\u5668\u4eba\u80fd\u591f\u5173\u6ce8\u4eba\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u4e2d\u81ea\u7136\u63d0\u4f9b\u7684\u7ebf\u7d22\uff0c\u4ece\u800c\u91c7\u53d6\u9002\u5f53\u884c\u52a8\u3002"}}
{"id": "2512.07090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07090", "abs": "https://arxiv.org/abs/2512.07090", "authors": ["Jungmin Lee", "Gwangeun Byeon", "Yulhwa Kim", "Seokin Hong"], "title": "Leveraging KV Similarity for Online Structured Pruning in LLMs", "comment": null, "summary": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.", "AI": {"tldr": "Token Filtering\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5728\u7ebf\u7ed3\u6784\u5316\u526a\u679d\u6280\u672f\uff0c\u901a\u8fc7\u8054\u5408\u952e\u503c\u76f8\u4f3c\u5ea6\u76f4\u63a5\u8bc4\u4f30token\u5197\u4f59\u5ea6\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8df3\u8fc7\u5197\u4f59\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u65e0\u9700\u6821\u51c6\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4eLLM\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4f9d\u8d56\u79bb\u7ebf\u6821\u51c6\u6570\u636e\uff0c\u5728\u4e0d\u540c\u8f93\u5165\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u6570\u636e\u3001\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u505a\u51fa\u526a\u679d\u51b3\u7b56\u7684\u65b9\u6cd5\u3002", "method": "1. \u901a\u8fc7\u8054\u5408\u952e\u503c\u76f8\u4f3c\u5ea6\u6d4b\u91cftoken\u5197\u4f59\u5ea6\uff0c\u8df3\u8fc7\u5197\u4f59\u6ce8\u610f\u529b\u8ba1\u7b97\uff1b2. \u8bbe\u8ba1\u65b9\u5dee\u611f\u77e5\u878d\u5408\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u7684\u952e\u503c\u76f8\u4f3c\u5ea6\uff0c\u786e\u4fdd\u5728\u9ad8\u526a\u679d\u7387\u4e0b\u4ecd\u4fdd\u7559\u4fe1\u606f\u4e30\u5bcc\u7684token\uff1b3. \u65e0\u989d\u5916\u5185\u5b58\u5f00\u9500\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684token\u91cd\u8981\u6027\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5728LLaMA-2 (7B/13B)\u3001LLaMA-3 (8B)\u548cMistral (7B)\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cToken Filtering\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u5728\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u4e0a\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5728MMLU\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u5373\u4f7f50%\u526a\u679d\u4ecd\u4fdd\u6301\u5f3a\u6027\u80fd\u3002", "conclusion": "Token Filtering\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u6570\u636e\u3001\u7a33\u5b9a\u9ad8\u6548\u7684\u5728\u7ebf\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4eLLM\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07221", "abs": "https://arxiv.org/abs/2512.07221", "authors": ["Zichao Shu", "Shitao Bei", "Lijun Li", "Zetao Chen"], "title": "Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality", "comment": null, "summary": "Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8fde\u7eed\u65f6\u95f4\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u6574\u5408IMU\u6570\u636e\u8865\u507fMoCap\u6296\u52a8\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6SLAM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u9488\u5bf9XR\u5e94\u7528\u4e2d\u7684\u65cb\u8f6c\u8bef\u5dee\u548c\u5e27\u95f4\u6296\u52a8\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740XR\u6c89\u6d78\u611f\u6807\u51c6\u63d0\u9ad8\uff0cSLAM\u57fa\u51c6\u6d4b\u8bd5\u8981\u6c42\u66f4\u4e25\u683c\u3002\u4f20\u7edf\u57fa\u4e8eMoCap\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u65f6\u7a7a\u6821\u51c6\u548c\u8bbe\u5907\u6296\u52a8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u65cb\u8f6c\u8bef\u5dee\u548c\u5e27\u95f4\u6296\u52a8\u7b49\u5173\u952e\u6307\u6807\u7684\u51c6\u786e\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u65f6\u95f4\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\uff0c\u6574\u5408\u8f85\u52a9IMU\u6570\u636e\u8865\u507fMoCap\u6296\u52a8\uff1b\u91c7\u7528\u53ef\u53d8\u65f6\u95f4\u540c\u6b65\u65b9\u6cd5\u548c\u57fa\u4e8e\u87ba\u65cb\u540c\u4f59\u7ea6\u675f\u7684\u4f4d\u59ff\u6b8b\u5dee\uff0c\u5b9e\u73b0\u591a\u4f20\u611f\u5668\u4e0e\u5f85\u6d4b\u8bbe\u5907\u7684\u7cbe\u786e\u65f6\u7a7a\u6821\u51c6\u3002", "result": "\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u8fbe\u5230\u5168\u9762\u8bc4\u4f30\u5148\u8fdbSLAM\u7b97\u6cd5\u6240\u9700\u7684\u7cbe\u5ea6\uff1b\u6210\u529f\u5bf9\u591a\u4e2a\u9886\u5148XR\u8bbe\u5907\u548c\u5f00\u6e90SLAM\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86MoCap\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aXR\u5e94\u7528\u4e2d\u7684SLAM\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.07132", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07132", "abs": "https://arxiv.org/abs/2512.07132", "authors": ["Nithin Sivakumaran", "Justin Chih-Yao Chen", "David Wan", "Yue Zhang", "Jaehong Yoon", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning", "comment": "Code: https://github.com/nsivaku/dart", "summary": "Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.", "AI": {"tldr": "DART\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8fa9\u8bba\u5206\u6b67\u6765\u8bc6\u522b\u6709\u7528\u7684\u89c6\u89c9\u5de5\u5177\uff0c\u5229\u7528\u5de5\u5177\u5f15\u5165\u65b0\u4fe1\u606f\u548c\u4e13\u5bb6\u5bf9\u9f50\u8bc4\u5206\u6765\u4fc3\u8fdb\u8ba8\u8bba\uff0c\u63d0\u5347\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u4e13\u4e1a\u89c6\u89c9\u5de5\u5177\u53ef\u4ee5\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u6216\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e13\u5bb6\u77e5\u8bc6\uff08\u5982\u63a5\u5730\u3001\u7a7a\u95f4\u63a8\u7406\u3001\u533b\u5b66\u77e5\u8bc6\u7b49\uff09\uff0c\u4f46\u786e\u5b9a\u4f55\u65f6\u8c03\u7528\u54ea\u4e9b\u5de5\u5177\u5177\u6709\u6311\u6218\u6027\u3002\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u5982\u4f55\u6709\u6548\u5229\u7528\u5de5\u5177\u6765\u89e3\u51b3\u5206\u6b67\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faDART\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1\uff09\u591a\u4e2a\u89c6\u89c9\u667a\u80fd\u4f53\u8fdb\u884c\u8fa9\u8bba\uff0c\u901a\u8fc7\u5206\u6b67\u8bc6\u522b\u6709\u7528\u7684\u89c6\u89c9\u5de5\u5177\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001OCR\u3001\u7a7a\u95f4\u63a8\u7406\u7b49\uff09\uff1b2\uff09\u5de5\u5177\u5f15\u5165\u65b0\u4fe1\u606f\u5e76\u63d0\u4f9b\u5de5\u5177\u5bf9\u9f50\u7684\u540c\u610f\u5206\u6570\uff0c\u7a81\u51fa\u4e0e\u4e13\u5bb6\u5de5\u5177\u4e00\u81f4\u7684\u667a\u80fd\u4f53\uff1b3\uff09\u4f7f\u7528\u805a\u5408\u667a\u80fd\u4f53\u57fa\u4e8e\u667a\u80fd\u4f53\u8f93\u51fa\u548c\u5de5\u5177\u4fe1\u606f\u9009\u62e9\u6700\u4f73\u7b54\u6848\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDART\u4f18\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u548c\u5355\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u6846\u67b6\uff1a\u5728A-OKVQA\u4e0a\u6bd4\u6b21\u5f3a\u57fa\u7ebf\uff08\u5e26\u8bc4\u5224\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff09\u63d0\u53473.4%\uff0c\u5728MMMU\u4e0a\u63d0\u53472.4%\u3002\u5728M3D\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u6bd4\u5176\u4ed6\u5f3a\u57fa\u7ebf\u63d0\u53471.3%\u3002\u6587\u672c\u91cd\u53e0\u5206\u6790\u663e\u793aDART\u8ba8\u8bba\u66f4\u4e30\u5bcc\uff0c\u5de5\u5177\u8c03\u7528\u5206\u5e03\u663e\u793a\u591a\u6837\u5316\u5de5\u5177\u88ab\u53ef\u9760\u4f7f\u7528\u6765\u89e3\u51b3\u5206\u6b67\u3002", "conclusion": "DART\u901a\u8fc7\u5229\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u5206\u6b67\u6765\u8bc6\u522b\u548c\u8c03\u7528\u76f8\u5173\u89c6\u89c9\u5de5\u5177\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\uff0c\u80fd\u591f\u9002\u5e94\u65b0\u5de5\u5177\u9886\u57df\uff0c\u5e76\u4fc3\u8fdb\u66f4\u4e30\u5bcc\u7684\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u3002"}}
{"id": "2512.07266", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07266", "abs": "https://arxiv.org/abs/2512.07266", "authors": ["Florian Tretter", "Daniel Fl\u00f6gel", "Alexandru Vasilache", "Max Grobbel", "J\u00fcrgen Becker", "S\u00f6ren Hohmann"], "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks", "comment": "8 pages, 6 figures", "summary": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u793e\u4f1a\u96c6\u6210DRL\u65b9\u6cd5\uff0c\u7ed3\u5408SNN\u548cANN\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u793e\u4f1a\u5bfc\u822a\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017", "motivation": "\u5c06\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u96c6\u6210\u5230\u4eba\u7c7b\u73af\u5883\u9700\u8981\u7c7b\u4eba\u51b3\u7b56\u548c\u8282\u80fd\u7684\u4e8b\u4ef6\u9a71\u52a8\u8ba1\u7b97\uff0c\u4f46\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5728DRL\u5bfc\u822a\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898", "method": "\u91c7\u7528\u6df7\u5408\u793e\u4f1a\u96c6\u6210DRL\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff1a\u6f14\u5458\u4f7f\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\uff0c\u8bc4\u8bba\u5bb6\u4f7f\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc(ANN)\uff0c\u5e76\u914d\u5907\u795e\u7ecf\u5f62\u6001\u7279\u5f81\u63d0\u53d6\u5668\u6355\u6349\u65f6\u95f4\u4eba\u7fa4\u52a8\u6001\u548c\u4eba\u673a\u4ea4\u4e92", "result": "\u63d0\u9ad8\u4e86\u793e\u4f1a\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u5c06\u4f30\u8ba1\u80fd\u8017\u964d\u4f4e\u4e86\u7ea61.69\u4e2a\u6570\u91cf\u7ea7", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5728DRL\u5bfc\u822a\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8282\u80fd\u7684\u793e\u4f1a\u5bfc\u822a"}}
{"id": "2512.07134", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07134", "abs": "https://arxiv.org/abs/2512.07134", "authors": ["Lauren Levine", "Amir Zeldes"], "title": "GUMBridge: a Corpus for Varieties of Bridging Anaphora", "comment": null, "summary": "Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in \"There is 'a house'. 'The door' is red,\" where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.", "AI": {"tldr": "GUMBridge\u662f\u4e00\u4e2a\u65b0\u7684\u6865\u63a5\u6307\u4ee3\u8d44\u6e90\uff0c\u5305\u542b16\u79cd\u4e0d\u540c\u82f1\u8bed\u6587\u4f53\uff0c\u63d0\u4f9b\u5e7f\u6cdb\u7684\u6865\u63a5\u73b0\u8c61\u8986\u76d6\u548c\u8be6\u7ec6\u7684\u5b50\u7c7b\u578b\u5206\u7c7b\u6807\u6ce8\uff0c\u5e76\u8bc4\u4f30\u4e86LLMs\u5728\u6865\u63a5\u89e3\u6790\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u82f1\u8bed\u6865\u63a5\u6307\u4ee3\u8d44\u6e90\u5927\u591a\u89c4\u6a21\u5c0f\u3001\u8986\u76d6\u8303\u56f4\u6709\u9650\u3001\u6587\u4f53\u7c7b\u578b\u6709\u9650\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u591a\u6837\u5316\u7684\u8d44\u6e90\u6765\u652f\u6301\u6865\u63a5\u6307\u4ee3\u7814\u7a76\u3002", "method": "\u521b\u5efaGUMBridge\u8d44\u6e90\uff0c\u5305\u542b16\u79cd\u4e0d\u540c\u82f1\u8bed\u6587\u4f53\u7684\u6587\u672c\uff0c\u63d0\u4f9b\u8be6\u7ec6\u7684\u6865\u63a5\u6307\u4ee3\u6807\u6ce8\u548c\u5b50\u7c7b\u578b\u5206\u7c7b\uff0c\u5e76\u8fdb\u884c\u6807\u6ce8\u8d28\u91cf\u8bc4\u4f30\u548cLLMs\u57fa\u7ebf\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "GUMBridge\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u6865\u63a5\u73b0\u8c61\u8986\u76d6\u548c\u8be6\u7ec6\u7684\u5b50\u7c7b\u578b\u6807\u6ce8\uff0c\u8bc4\u4f30\u663e\u793a\u6865\u63a5\u89e3\u6790\u548c\u5b50\u7c7b\u578b\u5206\u7c7b\u5728LLMs\u65f6\u4ee3\u4ecd\u7136\u662f\u56f0\u96be\u7684NLP\u4efb\u52a1\u3002", "conclusion": "GUMBridge\u662f\u4e00\u4e2a\u5168\u9762\u3001\u591a\u6837\u5316\u7684\u6865\u63a5\u6307\u4ee3\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6865\u63a5\u6307\u4ee3\u7814\u7a76\uff0c\u5e76\u8868\u660e\u5f53\u524dLLMs\u5728\u6865\u63a5\u89e3\u6790\u4efb\u52a1\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2512.07303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07303", "abs": "https://arxiv.org/abs/2512.07303", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots", "comment": "7 pages, 3 figures, submitted to IFAC World Congress 2026", "summary": "Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u901a\u7528\u8986\u76d6\u7a7a\u95f4\u7684\u8fde\u7eed\u62d3\u6251\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u7cfb\u7ef3\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u79bb\u6563\u65b9\u6cd5\u66f4\u9ad8\u6548\u4e14\u652f\u6301\u8fde\u7eed\u89c4\u5212", "motivation": "\u73b0\u6709\u7cfb\u7ef3\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u79bb\u6563\u914d\u7f6e\u7a7a\u95f4\u8868\u793a\uff0c\u65e0\u6cd5\u540c\u65f6\u6355\u83b7\u7cfb\u7ef3\u7684\u62d3\u6251\u4fe1\u606f\u548c\u673a\u5668\u4eba\u7684\u8fde\u7eed\u4f4d\u7f6e\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5efa\u6a21\u65b9\u6cd5", "method": "\u9996\u5148\u5efa\u7acb\u7cfb\u7ef3\u673a\u5668\u4eba\u914d\u7f6e\u7a7a\u95f4\u4e0e\u5de5\u4f5c\u7a7a\u95f4\u901a\u7528\u8986\u76d6\u7a7a\u95f4\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u7136\u540e\u5229\u7528\u8fd9\u79cd\u8054\u7cfb\u5f00\u53d1\u7b97\u6cd5\u8ba1\u7b97\u914d\u7f6e\u7a7a\u95f4\u7684\u5355\u7eaf\u590d\u5f62\u6a21\u578b", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6784\u5efa\u4f20\u7edf\u540c\u4f26\u589e\u5f3a\u56fe\u7684\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\uff0c\u6a21\u578b\u662f\u8fde\u7eed\u7684\uff0c\u5141\u8bb8\u4f7f\u7528\u591a\u79cd\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u89e3\u51b3\u7cfb\u7ef3\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\u4efb\u52a1", "conclusion": "\u63d0\u51fa\u7684\u62d3\u6251\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5730\u8868\u793a\u7cfb\u7ef3\u673a\u5668\u4eba\u7684\u914d\u7f6e\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7cfb\u7ef3\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.07195", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.07195", "abs": "https://arxiv.org/abs/2512.07195", "authors": ["Xuan Zhang", "Wenxuan Zhang", "Anxu Wang", "See-Kiong Ng", "Yang Deng"], "title": "MASim: Multilingual Agent-Based Simulation for Social Science", "comment": null, "summary": "Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.", "AI": {"tldr": "MASim\u662f\u9996\u4e2a\u652f\u6301\u591a\u8bed\u8a00\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u8de8\u8bed\u8a00\u793e\u4f1a\u884c\u4e3a\uff0c\u5305\u542b\u5168\u7403\u8206\u8bba\u5efa\u6a21\u548c\u5a92\u4f53\u5f71\u54cd\u5206\u6790\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\u6a21\u62df\u5927\u591a\u662f\u5355\u8bed\u8a00\u7684\uff0c\u65e0\u6cd5\u6a21\u62df\u771f\u5b9e\u793e\u4f1a\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u8de8\u8bed\u8a00\u4e92\u52a8\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86MASim\u591a\u8bed\u8a00\u667a\u80fd\u4f53\u4eff\u771f\u6846\u67b6\uff0c\u652f\u6301\u5177\u6709\u4e0d\u540c\u793e\u4f1a\u8bed\u8a00\u5b66\u7279\u5f81\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u8fdb\u884c\u591a\u8f6e\u4ea4\u4e92\u3002\u6784\u5efa\u4e86MAPS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u5168\u7403\u4eba\u53e3\u5206\u5e03\u7684\u8c03\u67e5\u95ee\u9898\u548c\u4eba\u53e3\u7edf\u8ba1\u89d2\u8272\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMASim\u80fd\u591f\u590d\u73b0\u793e\u4f1a\u6587\u5316\u73b0\u8c61\uff0c\u6821\u51c6\u3001\u654f\u611f\u6027\u3001\u4e00\u81f4\u6027\u548c\u6587\u5316\u6848\u4f8b\u7814\u7a76\u8868\u660e\u8be5\u6846\u67b6\u6709\u6548\uff0c\u51f8\u663e\u4e86\u591a\u8bed\u8a00\u4eff\u771f\u5bf9\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MASim\u4e3a\u7814\u7a76\u8de8\u8bed\u8a00\u793e\u4f1a\u4e92\u52a8\u63d0\u4f9b\u4e86\u9996\u4e2a\u591a\u8bed\u8a00\u667a\u80fd\u4f53\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u7403\u8206\u8bba\u5efa\u6a21\u548c\u5a92\u4f53\u5f71\u54cd\u5206\u6790\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u5411\u591a\u8bed\u8a00\u65b9\u5411\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.07316", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07316", "abs": "https://arxiv.org/abs/2512.07316", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection", "comment": "7 pages, 4 figures, submitted to IFAC World Congress 2026", "summary": "Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u6c34\u9762\u8247\uff08USV\uff09\u5bf9\u63a5\u7684\u96c6\u4e2d\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u540c\u5de5\u4f5c\u5b9e\u73b0\u66f4\u5feb\u901f\u9ad8\u6548\u7684\u5bf9\u63a5", "motivation": "\u73b0\u6709USV\u5bf9\u63a5\u65b9\u6cd5\u901a\u5e38\u5c06\u4e00\u8258USV\u89c6\u4e3a\u9759\u6b62\u76ee\u6807\uff0c\u53e6\u4e00\u8258\u8d1f\u8d23\u63a5\u8fd1\uff0c\u7f3a\u4e4f\u534f\u540c\u6027\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u534f\u540c\u5bf9\u63a5\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u6c34\u6d41\u7b49\u5e72\u6270\u7684\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u96c6\u4e2d\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\uff0c\u4e24\u8258USV\u534f\u540c\u5de5\u4f5c\uff0c\u5728\u7ea6\u5b9a\u4f4d\u7f6e\u5bf9\u63a5\u3002MPC\u901a\u8fc7\u9884\u6d4b\u6a21\u578b\u8003\u8651\u5e72\u6270\u5f71\u54cd\uff0c\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\u548c\u8f68\u8ff9\u53ef\u884c\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u901f\u3001\u66f4\u9ad8\u6548\u7684USV\u5bf9\u63a5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6c34\u6d41\u7b49\u51e0\u4e4e\u9759\u6b62\u7684\u5e72\u6270\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u4e2d\u5f0fMPC\u534f\u540c\u5bf9\u63a5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86USV\u5bf9\u63a5\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u5904\u7406\u5e72\u6270\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u5bf9\u63a5\u6027\u80fd\u3002"}}
{"id": "2512.07218", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07218", "abs": "https://arxiv.org/abs/2512.07218", "authors": ["Feng Liang", "Weixin Zeng", "Runhao Zhao", "Xiang Zhao"], "title": "NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models", "comment": "Accepted by AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.", "AI": {"tldr": "NeSTR\uff1a\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u8868\u793a\u4e0e\u53cd\u601d\u63a8\u7406\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u65f6\u95f4\u7ea6\u675f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u663e\u8457\u6539\u5584\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u590d\u6742\u65f6\u95f4\u7ea6\u675f\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u7b26\u53f7\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u53cd\u601d\u65b9\u6cd5\u7f3a\u4e4f\u7ed3\u6784\u5316\u65f6\u95f4\u8868\u793a\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0d\u4e00\u81f4\u6216\u4ea7\u751f\u5e7b\u89c9\u3002\u5373\u4f7f\u6709\u6b63\u786e\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0cLLM\u4ecd\u53ef\u80fd\u8bef\u89e3\u6216\u8bef\u7528\u65f6\u95f4\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u65f6\u95f4\u63a8\u7406\uff08NeSTR\uff09\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5316\u7b26\u53f7\u8868\u793a\u4e0e\u6df7\u5408\u53cd\u601d\u63a8\u7406\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u7b26\u53f7\u7f16\u7801\u4fdd\u7559\u663e\u5f0f\u65f6\u95f4\u5173\u7cfb\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5f3a\u5236\u6267\u884c\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f7f\u7528\u6eaf\u56e0\u53cd\u601d\u7ea0\u6b63\u9519\u8bef\u63a8\u7406\u3002", "result": "\u5728\u591a\u6837\u5316\u65f6\u95f4\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeSTR\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u5373\u53ef\u6301\u7eed\u6539\u5584\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u95f4\u7406\u89e3\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "NeSTR\u901a\u8fc7\u6574\u5408\u7b26\u53f7\u8868\u793a\u548c\u53cd\u601d\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u65f6\u95f4\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u65b9\u6cd5\u3002"}}
{"id": "2512.07359", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.07359", "abs": "https://arxiv.org/abs/2512.07359", "authors": ["Bin Zhao", "Yiwen Lu", "Haohua Zhu", "Xiao Li", "Sheng Yi"], "title": "Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin", "comment": "10 pages, 4 figures. Accepted at ICBSR'25 (International Conference on Biomechanical Systems and Robotics)", "summary": "Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ece\u5149\u5b66\u52a8\u6355\u6570\u636e\u6784\u5efa\u4e2a\u6027\u5316\u591a\u521a\u4f53\u624b\u90e8\u6a21\u578b\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u901a\u8fc7\u5c06MANO\u6a21\u578b\u7684SO(3)\u5173\u8282\u65cb\u8f6c\u6295\u5f71\u5230\u521a\u4f53\u6a21\u578b\u7684\u8fd0\u52a8\u5b66\u7ea6\u675f\u5173\u8282\u4e0a\uff0c\u5b9e\u73b0\u5b9e\u65f6\u7269\u7406\u4eff\u771f", "motivation": "\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u4e2d\u9700\u8981\u5e73\u8861\u89e3\u5256\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u624b\u90e8\u4eff\u771f\u6a21\u578b\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u771f\u5b9e\u5916\u89c2\u548c\u5b9e\u65f6\u7269\u7406\u4eff\u771f\u7684\u9700\u6c42", "method": "\u4ece\u5149\u5b66\u52a8\u6355\u6784\u5efa\u4e2a\u6027\u5316MANO\u6a21\u578b\uff0c\u8f6c\u6362\u4e3aURDF\u8868\u793a\uff0c\u63d0\u51fa\u95ed\u5f0f\u89e3\u5904\u7406\u5355\u81ea\u7531\u5ea6\u5173\u8282\uff0c\u4f7f\u7528BCH\u4fee\u6b63\u8fed\u4ee3\u65b9\u6cd5\u5904\u7406\u53cc\u81ea\u7531\u5ea6\u5173\u8282\u7684\u65cb\u8f6c\u975e\u4ea4\u6362\u6027\u95ee\u9898", "result": "\u4e9a\u5398\u7c73\u7ea7\u91cd\u5efa\u8bef\u5dee\uff0c\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6210\u529f\u6267\u884c\u6293\u53d6\uff0c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u80fd\u63a7\u5236\u591a\u521a\u4f53\u624b\u90e8\u91cd\u653e\u6355\u83b7\u7684\u4eba\u7c7b\u6f14\u793a", "conclusion": "\u63d0\u51fa\u7684\u6d41\u7a0b\u80fd\u6784\u5efa\u65e2\u4fdd\u6301\u771f\u5b9e\u5916\u89c2\u53c8\u652f\u6301\u5b9e\u65f6\u7269\u7406\u4eff\u771f\u7684\u591a\u521a\u4f53\u624b\u90e8\u6a21\u578b\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.07246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07246", "abs": "https://arxiv.org/abs/2512.07246", "authors": ["Mengqi Wang", "Jianwei Wang", "Qing Liu", "Xiwei Xu", "Zhenchang Xing", "Liming Zhu", "Wenjie Zhang"], "title": "Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection", "comment": "14 pages, 8 figures", "summary": "Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.", "AI": {"tldr": "\u63d0\u51faTreeED\u548cForestED\u6846\u67b6\uff0c\u7528LLM\u8bf1\u5bfc\u51b3\u7b56\u6811\u8fdb\u884c\u9519\u8bef\u68c0\u6d4b\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709LLM\u4f5c\u4e3a\u6807\u6ce8\u5668\u7684\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u9ed1\u76d2\u51b3\u7b56\u8fc7\u7a0b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff1b2) \u5bf9\u63d0\u793a\u8bcd\u654f\u611f\uff0c\u8f93\u51fa\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027", "method": "\u63d0\u51faLLM-as-an-inducer\u6846\u67b6\uff1aTreeED\u7528LLM\u8bf1\u5bfc\u51b3\u7b56\u6811\u9aa8\u67b6\uff0c\u5305\u542b\u89c4\u5219\u8282\u70b9\u3001GNN\u8282\u70b9\u548c\u53f6\u5b50\u8282\u70b9\uff1bForestED\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u6784\u5efa\u591a\u4e2a\u51b3\u7b56\u6811\uff0c\u4f7f\u7528EM\u7b97\u6cd5\u8054\u5408\u4f30\u8ba1\u6811\u53ef\u9760\u6027\u548c\u4f18\u5316\u5171\u8bc6\u9884\u6d4b", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\uff0c\u5e73\u5747F1\u5206\u6570\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534716.1%", "conclusion": "\u901a\u8fc7LLM\u8bf1\u5bfc\u51b3\u7b56\u6811\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898"}}
{"id": "2512.07371", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07371", "abs": "https://arxiv.org/abs/2512.07371", "authors": ["Byungju Kim", "Jinu Pahk", "Chungwoo Lee", "Jaejoon Kim", "Jangha Lee", "Theo Taeyeong Kim", "Kyuhwan Shim", "Jun Ki Lee", "Byoung-Tak Zhang"], "title": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning", "comment": "project page: https://project-espada.github.io/espada/", "summary": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.", "AI": {"tldr": "ESPADA\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u6f14\u793a\u5206\u6bb5\u5b9e\u73b0\u673a\u5668\u4eba\u7b56\u7565\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u6210\u529f\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u7ea62\u500d\u901f\u5ea6\u63d0\u5347", "motivation": "\u57fa\u4e8e\u884c\u4e3a\u514b\u9686\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u867d\u7136\u7cbe\u786e\uff0c\u4f46\u7ee7\u627f\u4e86\u4eba\u7c7b\u6f14\u793a\u7684\u7f13\u6162\u8282\u594f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4f9d\u8d56\u7edf\u8ba1\u6216\u542f\u53d1\u5f0f\u7ebf\u7d22\uff0c\u5ffd\u7565\u4e86\u4efb\u52a1\u8bed\u4e49\uff0c\u5728\u4e0d\u540c\u64cd\u4f5c\u573a\u666f\u4e2d\u5bb9\u6613\u5931\u8d25\u3002", "method": "\u4f7f\u7528VLM-LLM\u6d41\u6c34\u7ebf\u7ed3\u54083D\u5939\u722a-\u7269\u4f53\u5173\u7cfb\u5bf9\u6f14\u793a\u8fdb\u884c\u8bed\u4e49\u5206\u6bb5\uff0c\u4ec5\u5728\u975e\u5173\u952e\u6bb5\u8fdb\u884c\u6fc0\u8fdb\u4e0b\u91c7\u6837\uff0c\u4fdd\u7559\u7cbe\u5ea6\u5173\u952e\u9636\u6bb5\u3002\u901a\u8fc7\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u5728\u52a8\u6001\u7279\u5f81\u4e0a\u4f20\u64ad\u5206\u6bb5\u6807\u7b7e\uff0c\u4ece\u5355\u4e2a\u6807\u6ce8\u6269\u5c55\u5230\u6574\u4e2a\u6570\u636e\u96c6\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u4e0eACT\u548cDP\u57fa\u7ebf\u76f8\u6bd4\uff0cESPADA\u5b9e\u73b0\u4e86\u7ea62\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6210\u529f\u7387\uff0c\u7f29\u5c0f\u4e86\u4eba\u7c7b\u6f14\u793a\u4e0e\u9ad8\u6548\u673a\u5668\u4eba\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "ESPADA\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bed\u4e49\u548c\u7a7a\u95f4\u611f\u77e5\u7684\u6846\u67b6\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u3001\u67b6\u6784\u4fee\u6539\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u5c31\u80fd\u6709\u6548\u52a0\u901f\u673a\u5668\u4eba\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u6301\u64cd\u4f5c\u7cbe\u5ea6\u3002"}}
{"id": "2512.07265", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.07265", "abs": "https://arxiv.org/abs/2512.07265", "authors": ["Bhavana Akkiraju", "Srihari Bandarupalli", "Swathi Sambangi", "Vasavi Ravuri", "R Vijaya Saraswathi", "Anil Kumar Vuppala"], "title": "TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation", "comment": "Submitted to AACL IJCNLP 2025", "summary": "Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u8bed\u97f3\u7ffb\u8bd1\u521b\u5efa\u4e86\u9996\u4e2a\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u6bd4\u8f83\u4e86\u7ea7\u8054\u4e0e\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u53d1\u73b0\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u901a\u8fc7\u9002\u5f53\u8c03\u4f18\u53ef\u8fbe\u5230\u4e0e\u7ea7\u8054\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6cf0\u5362\u56fa\u8bed\u6709\u8d85\u8fc78000\u4e07\u4f7f\u7528\u8005\uff0c\u4f46\u9488\u5bf9\u8fd9\u79cd\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u7684\u8bed\u97f3\u7ffb\u8bd1\u7814\u7a76\u4e25\u91cd\u4e0d\u8db3\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u8bed\u97f3\u7ffb\u8bd1\u5efa\u7acb\u9ad8\u8d28\u91cf\u57fa\u51c6\u3002", "method": "\u4ece46\u5c0f\u65f6\u624b\u52a8\u9a8c\u8bc1\u7684CSTD\u8bed\u6599\u5e93\u6570\u636e\u521b\u5efa\u57fa\u51c6\u6570\u636e\u96c6\uff0830h/8h/8h\u8bad\u7ec3/\u5f00\u53d1/\u6d4b\u8bd5\u5206\u5272\uff09\u3002\u7cfb\u7edf\u6bd4\u8f83\u7ea7\u8054\u67b6\u6784\uff08IndicWhisper + IndicMT\uff09\u4e0e\u7aef\u5230\u7aef\u67b6\u6784\uff08\u5fae\u8c03\u7684SeamlessM4T\u6a21\u578b\uff09\uff0c\u5e76\u8bc4\u4f30BLEU\u3001METEOR\u3001ChrF++\u3001ROUGE-L\u3001TER\u548cBERTScore\u7b49\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u76f8\u5173\u6027\u3002", "result": "IndicWhisper + IndicMT\u7ea7\u8054\u65b9\u6cd5\u56e0\u4f7f\u7528\u5927\u91cf\u6cf0\u5362\u56fa\u8bed\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u800c\u6027\u80fd\u6700\u9ad8\uff0c\u4f46\u5fae\u8c03\u7684SeamlessM4T\u6a21\u578b\u4f7f\u7528\u663e\u8457\u66f4\u5c11\u7684\u6cf0\u5362\u56fa\u8bed\u6570\u636e\u4ecd\u8868\u73b0\u51fa\u8272\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u4ed4\u7ec6\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u8db3\u591f\u7684\u5e73\u884c\u6570\u636e\uff08\u53ef\u80fd\u5c11\u4e8e100\u5c0f\u65f6\uff09\uff0c\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u80fd\u8fbe\u5230\u4e0e\u7ea7\u8054\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u4f20\u7edf\u6307\u6807\u6bd4BERTScore\u5728\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u7ffb\u8bd1\u4e2d\u63d0\u4f9b\u66f4\u597d\u7684\u8d28\u91cf\u533a\u5206\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a\u53ef\u590d\u73b0\u7684\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u57fa\u51c6\u3001\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7aef\u5230\u7aef\u6027\u80fd\u7ade\u4e89\u529b\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u4ee5\u53ca\u9488\u5bf9\u5f62\u6001\u590d\u6742\u8bed\u8a00\u5bf9\u81ea\u52a8\u8bc4\u4f30\u7684\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2512.07464", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07464", "abs": "https://arxiv.org/abs/2512.07464", "authors": ["Haolin Song", "Hongbo Zhu", "Tao Yu", "Yan Liu", "Mingqi Yuan", "Wengang Zhou", "Hua Chen", "Houqiang Li"], "title": "Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction", "comment": null, "summary": "For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u611f\u77e5\u6027\u8fd0\u52a8\u6846\u67b6\uff0c\u5c06\u5730\u5f62\u611f\u77e5\u3001\u6b65\u6001\u8c03\u8282\u548c\u5168\u8eab\u63a7\u5236\u96c6\u6210\u5230\u5355\u4e00\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4e2d\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u7a33\u5065\u8fd0\u52a8", "motivation": "\u5168\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\uff08\u5982\u957f\u697c\u68af\uff09\u4e0a\u5b9e\u73b0\u53ef\u9760\u8fd0\u52a8\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u6709\u9650\u7684\u611f\u77e5\u3001\u6a21\u7cca\u7684\u5730\u5f62\u7ebf\u7d22\u548c\u6b65\u6001\u65f6\u5e8f\u9002\u5e94\u4e0d\u8db3\u5bb9\u6613\u5bfc\u81f4\u5931\u8861", "method": "\u4f7f\u7528\u5411\u4e0b\u6df1\u5ea6\u76f8\u673a\u89c2\u5bdf\u811a\u90e8\u652f\u6491\u533a\u57df\uff0c\u7d27\u51d1U-Net\u5b9e\u65f6\u91cd\u5efa\u5bc6\u96c6\u81ea\u6211\u4e2d\u5fc3\u9ad8\u5ea6\u56fe\uff1b\u5c06\u611f\u77e5\u9ad8\u5ea6\u56fe\u4e0e\u672c\u4f53\u611f\u53d7\u89c2\u5bdf\u8f93\u5165\u7edf\u4e00\u7b56\u7565\uff0c\u751f\u6210\u5173\u8282\u547d\u4ee4\u548c\u5168\u5c40\u6b65\u8fdb\u76f8\u4f4d\u4fe1\u53f7\uff1b\u91c7\u7528\u5355\u9636\u6bb5\u8fde\u7eed\u5e08\u751f\u8bad\u7ec3\u65b9\u6848", "result": "\u572831\u81ea\u7531\u5ea6\u30011.65\u7c73\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u8fd0\u52a8\uff0c\u5305\u62ec\u524d\u540e\u4e0a\u4e0b\u697c\u68af\u4ee5\u53ca\u8de8\u8d8a46\u5398\u7c73\u95f4\u9699", "conclusion": "\u63d0\u51fa\u7684\u611f\u77e5\u8fd0\u52a8\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5730\u5f62\u611f\u77e5\u3001\u6b65\u6001\u8c03\u8282\u548c\u5168\u8eab\u63a7\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u53ef\u9760\u8fd0\u52a8"}}
{"id": "2512.07277", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.07277", "abs": "https://arxiv.org/abs/2512.07277", "authors": ["Srihari Bandarupalli", "Bhavana Akkiraju", "Charan Devarakonda", "Vamsiraghusimha Narsinga", "Anil Kumar Vuppala"], "title": "Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data", "comment": "Accepted in AACL IJCNLP 2025", "summary": "Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8de8\u8bed\u8a00\u8fde\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u65e0\u6807\u7b7e\u8bed\u97f3\u6570\u636e\u548c\u5f62\u6001\u611f\u77e5\u5206\u8bcd\uff0c\u5f00\u53d1\u51fa\u53c2\u6570\u66f4\u5c11\u4f46\u6027\u80fd\u53ef\u6bd4\u7684\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u6311\u6218\u4e86\u6a21\u578b\u89c4\u6a21\u51b3\u5b9a\u6027\u80fd\u7684\u666e\u904d\u5047\u8bbe\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u8bc6\u522b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0d\u8db3\u7684\u53cc\u91cd\u9650\u5236\uff0c\u800c\u73b0\u6709\u5927\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u53c2\u6570\u548c\u6807\u6ce8\u6570\u636e\uff0c\u4e0d\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u573a\u666f\u3002", "method": "\u6784\u5efa\u4e863000\u5c0f\u65f6\u7684\u591a\u8bed\u8a00\u65e0\u6807\u7b7e\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u7ed3\u5408\u76ee\u6807\u5bfc\u5411\u7684\u8fde\u7eed\u9884\u8bad\u7ec3\u548c\u5f62\u6001\u611f\u77e5\u5206\u8bcd\u6280\u672f\uff0c\u5f00\u53d1\u4e863\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u3002", "result": "3\u4ebf\u53c2\u6570\u6a21\u578b\u5728\u6ce2\u65af\u8bed\u4e0a\u8d85\u8d8a\u4e8615\u4ebf\u53c2\u6570\u7684Whisper Large v3\uff0c\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u6027\u80fd\u4e0e5\u500d\u5927\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u4f7f\u7528\u66f4\u5c11\u7684\u53c2\u6570\u548c\u6807\u6ce8\u6570\u636e\u3002", "conclusion": "\u8bed\u97f3\u8bc6\u522b\u8d28\u91cf\u5e76\u975e\u4e3b\u8981\u53d6\u51b3\u4e8e\u6a21\u578b\u89c4\u6a21\uff0c\u6570\u636e\u76f8\u5173\u6027\u548c\u6218\u7565\u9884\u8bad\u7ec3\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u66f4\u4e3a\u5173\u952e\uff0c\u8fd9\u4e3a\u5f00\u53d1\u5305\u5bb9\u6027\u8bed\u97f3\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u6216\u4e13\u6709\u6570\u636e\u96c6\u3002"}}
{"id": "2512.07472", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07472", "abs": "https://arxiv.org/abs/2512.07472", "authors": ["Siyu Xu", "Zijian Wang", "Yunke Wang", "Chenghao Xia", "Tao Huang", "Chang Xu"], "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($\u03c0_{0}$ and $\u03c0_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.", "AI": {"tldr": "AFI\u6846\u67b6\u901a\u8fc73D\u7a7a\u95f4\u53ef\u64cd\u4f5c\u573a\u68c0\u6d4b\u548c\u5e72\u9884VLA\u6a21\u578b\u7684\u8bb0\u5fc6\u9677\u9631\uff0c\u5728\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u9c81\u68d2\u6027", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5bb9\u6613\u9677\u5165\"\u8bb0\u5fc6\u9677\u9631\"\uff0c\u5373\u5728\u65b0\u573a\u666f\u4e2d\u91cd\u590d\u8bb0\u5fc6\u8f68\u8ff9\u800c\u975e\u9002\u5e94\u65b0\u73af\u5883\u3002\u8fd9\u6e90\u4e8e\u7aef\u5230\u7aef\u8bbe\u8ba1\u7f3a\u4e4f\u663e\u5f0f3D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u964c\u751f\u73af\u5883\u4e2d\u7684\u53ef\u64cd\u4f5c\u533a\u57df\u3002", "method": "\u63d0\u51faAffordance Field Intervention (AFI)\u6df7\u5408\u6846\u67b6\uff1a1) \u4f7f\u75283D\u7a7a\u95f4\u53ef\u64cd\u4f5c\u573a(SAFs)\u4f5c\u4e3a\u6309\u9700\u63d2\u4ef6\u6307\u5bfcVLA\u884c\u4e3a\uff1b2) \u901a\u8fc7\u672c\u4f53\u611f\u77e5\u68c0\u6d4b\u8bb0\u5fc6\u9677\u9631\uff1b3) \u5c06\u673a\u5668\u4eba\u91cd\u65b0\u5b9a\u4f4d\u5230\u9ad8\u53ef\u64cd\u4f5c\u6027\u533a\u57df\uff1b4) \u63d0\u51fa\u53ef\u64cd\u4f5c\u6027\u9a71\u52a8\u7684\u8def\u5f84\u70b9\u951a\u5b9aVLA\u751f\u6210\u7684\u52a8\u4f5c\uff1b5) \u4f7f\u7528\u57fa\u4e8eSAF\u7684\u8bc4\u5206\u5668\u9009\u62e9\u7d2f\u79ef\u53ef\u64cd\u4f5c\u6027\u6700\u9ad8\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff0c\u4e0d\u540cVLA\u9aa8\u5e72\u7f51\u7edc(\u03c0\u2080\u548c\u03c0\u2080.\u2085)\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u5e73\u5747\u63d0\u534723.5%\uff1b\u5728LIBERO-Pro\u57fa\u51c6\u4e0a\u63d0\u534720.2%\uff0c\u9a8c\u8bc1\u4e86AFI\u5728\u589e\u5f3aVLA\u5bf9\u5206\u5e03\u504f\u79fb\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "AFI\u901a\u8fc7\u5c06\u663e\u5f0f3D\u7a7a\u95f4\u53ef\u64cd\u4f5c\u6027\u63a8\u7406\u4e0eVLA\u6a21\u578b\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bb0\u5fc6\u9677\u9631\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u5728\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07288", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07288", "abs": "https://arxiv.org/abs/2512.07288", "authors": ["Tomoki Doi", "Masaru Isonuma", "Hitomi Yanaka"], "title": "Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models", "comment": "To appear in the Proceedings of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop (AACL-SRW 2025)", "summary": "Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.", "AI": {"tldr": "\u901a\u8fc7\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u6784\u5efa\u4f2a\u5fe0\u5b9e\u81ea\u89e3\u91ca\uff0c\u5229\u7528\u6301\u7eed\u5b66\u4e60\u8bad\u7ec3\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u63d0\u5347\u81ea\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u5e76\u5b9e\u73b0\u8de8\u98ce\u683c\u6cdb\u5316", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u4e0d\u540c\u98ce\u683c\u7684\u81ea\u89e3\u91ca\uff0c\u4f46\u8fd9\u4e9b\u89e3\u91ca\u5f80\u5f80\u7f3a\u4e4f\u5fe0\u5b9e\u6027\uff08faithfulness\uff09\u3002\u5982\u4f55\u63d0\u5347\u5fe0\u5b9e\u6027\u4ee5\u53ca\u6539\u8fdb\u6548\u679c\u662f\u5426\u80fd\u8de8\u98ce\u683c\u6cdb\u5316\uff0c\u662f\u5f53\u524d\u672a\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u6784\u5efa\u5355\u8bcd\u7ea6\u675f\u7684\u4f2a\u5fe0\u5b9e\u81ea\u89e3\u91ca\uff0c\u7136\u540e\u5bf9\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u8fdb\u884c\u6301\u7eed\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u4e09\u4e2a\u5206\u7c7b\u4efb\u52a1\u548c\u4e09\u79cd\u89e3\u91ca\u98ce\u683c\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8bad\u7ec3\u80fd\u63d0\u5347\u6240\u6709\u5206\u7c7b\u4efb\u52a1\u548c\u89e3\u91ca\u98ce\u683c\u7684\u81ea\u89e3\u91ca\u5fe0\u5b9e\u5ea6\uff0c\u6539\u8fdb\u6548\u679c\u80fd\u6cdb\u5316\u5230\u591a\u8bcd\u8bbe\u7f6e\u548c\u672a\u89c1\u4efb\u52a1\uff0c\u4e14\u4e09\u79cd\u98ce\u683c\u95f4\u5b58\u5728\u4e00\u81f4\u7684\u8de8\u98ce\u683c\u6cdb\u5316\u3002", "conclusion": "\u8bad\u7ec3\u6709\u52a9\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fe0\u5b9e\u81ea\u89e3\u91ca\u80fd\u529b\uff0c\u4e14\u8fd9\u79cd\u80fd\u529b\u5177\u6709\u8de8\u98ce\u683c\u548c\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u6027\uff0c\u8868\u660e\u8bad\u7ec3\u53ef\u80fd\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u7684\u5fe0\u5b9e\u81ea\u89e3\u91ca\u80fd\u529b\u6539\u8fdb\u3002"}}
{"id": "2512.07482", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07482", "abs": "https://arxiv.org/abs/2512.07482", "authors": ["Florian L\u00fcttner", "Nicole Neis", "Daniel Stadler", "Robin Moss", "Mirjam Fehling-Kaschek", "Matthias Pfriem", "Alexander Stolz", "Jens Ziehn"], "title": "From Real-World Traffic Data to Relevant Critical Scenarios", "comment": "8 pages, 8 figures", "summary": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly \"unknown unsafe\" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of \"unknown unsafe\" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u9ad8\u901f\u516c\u8def\u6570\u636e\u7684\u8f66\u9053\u53d8\u6362\u573a\u666f\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u952e\u6027\u5ea6\u91cf\u8bc6\u522b\u5b89\u5168\u76f8\u5173\u573a\u666f\uff0c\u5e76\u751f\u6210\u5408\u6210\u5173\u952e\u573a\u666f\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u9a8c\u8bc1\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9700\u8981\u5728\u5404\u79cd\u76f8\u5173\u573a\u666f\u4e0b\u53ef\u9760\u8fd0\u884c\uff0c\u4f46\u8bc6\u522b\u5b8c\u6574\u7684\u5b89\u5168\u76f8\u5173\u573a\u666f\u5177\u6709\u6311\u6218\u6027\u3002\u968f\u7740\u6280\u672f\u590d\u6742\u5ea6\u589e\u52a0\uff0c\"\u672a\u77e5\u4e0d\u5b89\u5168\"\u573a\u666f\u6570\u91cf\u589e\u591a\uff0c\u9700\u8981\u4ece\u9ad8\u901f\u516c\u8def\u7b49\u7b80\u5355\u9886\u57df\u5f00\u59cb\uff0c\u63d0\u524d\u8bc6\u522b\u76f8\u5173\u573a\u666f\u4ee5\u63d0\u9ad8\u9a8c\u8bc1\u6548\u7387\u3002", "method": "1) \u91c7\u96c6\u548c\u5904\u7406\u771f\u5b9e\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u6570\u636e\uff1b2) \u5bf9\u8f68\u8ff9\u6570\u636e\u5e94\u7528\u5173\u952e\u6027\u5ea6\u91cf\u8bc4\u4f30\u573a\u666f\uff1b3) \u5c06\u8ba1\u7b97\u51fa\u7684\u5ea6\u91cf\u4e0e\u7279\u5b9a\u8f66\u9053\u53d8\u6362\u573a\u666f\u53ca\u6570\u636e\u91c7\u96c6\u6761\u4ef6\u5173\u8054\uff1b4) \u57fa\u4e8e\u8bb0\u5f55\u573a\u666f\u751f\u6210\u5408\u6210\u573a\u666f\u4ee5\u5e94\u5bf9\"\u672a\u77e5\u4e0d\u5b89\u5168\"\u573a\u666f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5904\u7406\u94fe\uff0c\u80fd\u591f\u8bc6\u522b\u5b89\u5168\u76f8\u5173\u573a\u666f\uff0c\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u63d0\u53d6\u8fd9\u4e9b\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u5728\u9ad8\u901f\u516c\u8def\u4e0a\u751f\u6210\u5408\u6210\u5173\u952e\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u9ad8\u901f\u516c\u8def\u8f66\u9053\u53d8\u6362\u573a\u666f\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u573a\u666f\u8bc6\u522b\u548c\u751f\u6210\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u9a8c\u8bc1\u6548\u7387\u5e76\u5e94\u5bf9\"\u672a\u77e5\u4e0d\u5b89\u5168\"\u573a\u666f\u7684\u6311\u6218\u3002"}}
{"id": "2512.07367", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07367", "abs": "https://arxiv.org/abs/2512.07367", "authors": ["Revekka Kyriakoglou", "Anna Pappa"], "title": "Multilingual corpora for the study of new concepts in the social sciences and humanities:", "comment": "in French language", "summary": "This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6784\u5efa\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u7814\u7a76\u4eba\u6587\u793e\u79d1\u4e2d\u7684\u65b0\u5174\u6982\u5ff5\uff0c\u4ee5\"\u975e\u6280\u672f\u521b\u65b0\"\u4e3a\u4f8b\uff0c\u7ed3\u5408\u516c\u53f8\u7f51\u7ad9\u6587\u672c\u548c\u5e74\u62a5\u6570\u636e\uff0c\u521b\u5efa\u9002\u5408\u673a\u5668\u5b66\u4e60\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u4e3a\u7814\u7a76\u4eba\u6587\u793e\u79d1\u9886\u57df\u7684\u65b0\u5174\u6982\u5ff5\uff08\u5982\"\u975e\u6280\u672f\u521b\u65b0\"\uff09\u63d0\u4f9b\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u8d44\u6e90\uff0c\u65e2\u652f\u6301\u6982\u5ff5\u5206\u6790\u53c8\u9002\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u4e24\u79cd\u6570\u636e\u6e90\uff1a1) \u4ece\u516c\u53f8\u7f51\u7ad9\u81ea\u52a8\u63d0\u53d6\u7684\u6cd5\u8bed\u548c\u82f1\u8bed\u6587\u672c\uff1b2) \u6309\u6587\u6863\u6807\u51c6\u81ea\u52a8\u7b5b\u9009\u7684\u5e74\u62a5\u3002\u5904\u7406\u6d41\u7a0b\u5305\u62ec\u8bed\u8a00\u68c0\u6d4b\u3001\u5185\u5bb9\u8fc7\u6ee4\u3001\u76f8\u5173\u7247\u6bb5\u63d0\u53d6\u548c\u5143\u6570\u636e\u589e\u5f3a\uff0c\u6700\u7ec8\u521b\u5efa\u5305\u542b\u4e13\u5bb6\u8bcd\u5178\u672f\u8bed\u4e0a\u4e0b\u6587\u5757\uff08\u524d\u540e\u5404\u4e24\u53e5\uff09\u7684\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff0c\u5305\u542b\u82f1\u8bed\u6d3e\u751f\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u672f\u8bed\u51fa\u73b0\u90fd\u5e26\u6709\u4e0a\u4e0b\u6587\u5757\u548c\u4e3b\u9898\u7c7b\u522b\u6807\u6ce8\uff0c\u9002\u5408\u76d1\u7763\u5206\u7c7b\u4efb\u52a1\u548cNLP\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5206\u6790\u65b0\u5174\u6982\u5ff5\u7684\u8bcd\u6c47\u53d8\u5f02\u6027\u548c\u751f\u6210NLP\u4e13\u7528\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\u4f18\u52bf\u3002"}}
{"id": "2512.07507", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07507", "abs": "https://arxiv.org/abs/2512.07507", "authors": ["Yiming Cui", "Shiyu Fang", "Jiarui Zhang", "Yan Huang", "Chengkai Xu", "Bing Zhu", "Hao Zhang", "Peng Hang", "Jian Sun"], "title": "VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform", "comment": null, "summary": "The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.", "AI": {"tldr": "\u63d0\u51faVP-AutoTest\u5e73\u53f0\uff0c\u901a\u8fc7\u865a\u62df\u7269\u7406\u878d\u5408\u6d4b\u8bd5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u7684\u6311\u6218\uff0c\u652f\u6301\u591a\u79cd\u4ea4\u901a\u5143\u7d20\u3001\u5355/\u591a\u8f66\u4ea4\u4e92\u3001\u5bf9\u6297\u6d4b\u8bd5\uff0c\u5e76\u5305\u542b\u591a\u7ef4\u8bc4\u4f30\u548c\u53ef\u4fe1\u5ea6\u81ea\u8bc4\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u65b9\u6cd5\uff08\u865a\u62df\u4eff\u771f\u3001\u5c01\u95ed\u573a\u5730\u3001\u516c\u5f00\u9053\u8def\uff09\u5b58\u5728\u8f66\u8f86\u72b6\u6001\u4e0d\u771f\u5b9e\u3001\u6d4b\u8bd5\u80fd\u529b\u6709\u9650\u3001\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u865a\u62df\u7269\u7406\u878d\u5408\u6d4b\u8bd5\u867d\u6709\u6f5c\u529b\u4f46\u4ecd\u9762\u4e34\u5143\u7d20\u7c7b\u578b\u6709\u9650\u3001\u6d4b\u8bd5\u8303\u56f4\u7a84\u3001\u8bc4\u4f30\u6307\u6807\u56fa\u5b9a\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faVP-AutoTest\u5e73\u53f0\uff0c\u96c6\u621010+\u79cd\u865a\u62df\u548c\u7269\u7406\u5143\u7d20\uff08\u8f66\u8f86\u3001\u884c\u4eba\u3001\u8def\u4fa7\u8bbe\u65bd\uff09\uff0c\u652f\u6301\u5355\u8f66\u4ea4\u4e92\u548c\u591a\u8f66\u534f\u540c\u6d4b\u8bd5\uff0c\u91c7\u7528\u5bf9\u6297\u6d4b\u8bd5\u548c\u5e73\u884c\u63a8\u6f14\u52a0\u901f\u6545\u969c\u53d1\u73b0\uff0c\u901a\u8fc7OBU\u548cRedis\u901a\u4fe1\u5b9e\u73b0V2V/V2I\u534f\u540c\uff0c\u7ed3\u5408\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u548cAI\u4e13\u5bb6\u7cfb\u7edf\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u548c\u7f3a\u9677\u8bca\u65ad\u3002", "result": "\u5e73\u53f0\u80fd\u591f\u590d\u73b0\u5b9e\u4e16\u754c\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u591a\u6837\u6027\uff0c\u652f\u6301\u5168\u7ea7\u522b\u534f\u540c\u81ea\u52a8\u5316\uff0c\u901a\u8fc7\u865a\u62df\u7269\u7406\u878d\u5408\u6d4b\u8bd5\u4e0e\u771f\u5b9e\u5b9e\u9a8c\u5bf9\u6bd4\u8fdb\u884c\u53ef\u4fe1\u5ea6\u81ea\u8bc4\u4f30\uff0c\u786e\u4fdd\u6d4b\u8bd5\u7684\u4fdd\u771f\u5ea6\u548c\u6548\u7387\u3002", "conclusion": "VP-AutoTest\u5e73\u53f0\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u9ad8\u6548\u3001\u53ef\u4fe1\u7684\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u5728OnSite\u81ea\u52a8\u9a7e\u9a76\u516c\u5171\u670d\u52a1\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5b8c\u6574\u6d4b\u8bd5\u529f\u80fd\u3002"}}
{"id": "2512.07407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07407", "abs": "https://arxiv.org/abs/2512.07407", "authors": ["Niklas Mellgren", "Peter Schneider-Kamp", "Lukas Galke Poech"], "title": "Training Language Models to Use Prolog as a Tool", "comment": "10 pages", "summary": "Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u4f7f\u7528Prolog\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u8ba1\u7b97\u5de5\u5177\uff0c\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027\u548c\u53ef\u5ba1\u8ba1\u6027", "motivation": "\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u63a8\u7406\u7ed3\u679c\uff0c\u96be\u4ee5\u9a8c\u8bc1\uff0c\u9700\u8981\u786e\u4fdd\u5de5\u5177\u4f7f\u7528\u7684\u53ef\u9760\u6027\u4ee5\u6784\u5efa\u5b89\u5168\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf", "method": "\u4f7f\u7528GRPO\u5728GSM8K-Prolog-Prover\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Qwen2.5-3B-Instruct\uff0c\u7814\u7a76\u63d0\u793a\u7ed3\u6784\u3001\u5956\u52b1\u7ec4\u6210\uff08\u6267\u884c\u3001\u8bed\u6cd5\u3001\u8bed\u4e49\u3001\u7ed3\u6784\uff09\u548c\u63a8\u7406\u534f\u8bae\uff08\u5355\u6b21\u3001best-of-N\u3001\u4e24\u79cd\u667a\u80fd\u4f53\u6a21\u5f0f\uff09\u7684\u5f71\u54cd", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c3B\u6a21\u578b\u5728\u96f6\u6837\u672cMMLU\u4e0a\u8fbe\u5230\u4e0e7B\u6a21\u578b\u5c11\u6837\u672c\u76f8\u5f53\u7684\u6027\u80fd\uff1bbest-of-N\u7ed3\u5408\u5916\u90e8Prolog\u9a8c\u8bc1\u5728GSM8K\u4e0a\u51c6\u786e\u7387\u6700\u9ad8\uff1b\u5185\u90e8\u4fee\u590d\u7684\u667a\u80fd\u4f53\u63a8\u7406\u5728MMLU-Stem\u548cMMLU-Pro\u4e0a\u96f6\u6837\u672c\u6cdb\u5316\u6027\u80fd\u6700\u4f18", "conclusion": "\u5c06\u6a21\u578b\u63a8\u7406\u57fa\u4e8e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7cfb\u7edf\u80fd\u663e\u8457\u63d0\u9ad8\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u53ef\u5ba1\u8ba1\u6027"}}
{"id": "2512.07582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07582", "abs": "https://arxiv.org/abs/2512.07582", "authors": ["Guangyan Chen", "Meiling Wang", "Qi Shao", "Zichen Zhou", "Weixin Mao", "Te Cui", "Minzhao Zhu", "Yinan Deng", "Luojie Yang", "Zhanqi Zhang", "Yi Yang", "Hua Chen", "Yufeng Yue"], "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations", "comment": null, "summary": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.", "AI": {"tldr": "ViVLA\u662f\u4e00\u4e2a\u901a\u8fc7\u5355\u6b21\u4e13\u5bb6\u6f14\u793a\u89c6\u9891\u5b66\u4e60\u65b0\u6280\u80fd\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5b9e\u73b0\u4ece\u4eba\u7c7b\u89c6\u9891\u5230\u673a\u5668\u4eba\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u8bad\u7ec3\u5206\u5e03\u5916\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800c\u4eba\u7c7b\u4ec5\u901a\u8fc7\u89c2\u5bdf\u4e00\u6b21\u6f14\u793a\u5c31\u80fd\u5b66\u4e60\u65b0\u6280\u80fd\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u4ece\u5355\u6b21\u4e13\u5bb6\u6f14\u793a\u89c6\u9891\u9ad8\u6548\u5b66\u4e60\u65b0\u4efb\u52a1\u7684\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u3002", "method": "\u63d0\u51faViVLA\u7b56\u7565\uff0c\u8054\u5408\u5904\u7406\u4e13\u5bb6\u6f14\u793a\u89c6\u9891\u548c\u673a\u5668\u4eba\u89c6\u89c9\u89c2\u5bdf\uff0c\u9884\u6d4b\u6f14\u793a\u52a8\u4f5c\u5e8f\u5217\u548c\u540e\u7eed\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u4e13\u5bb6-\u667a\u80fd\u4f53\u914d\u5bf9\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4ece\u6613\u83b7\u53d6\u7684\u4eba\u7c7b\u89c6\u9891\u5408\u6210\u914d\u5bf9\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5171\u751f\u6210892,911\u4e2a\u8bad\u7ec3\u6837\u672c\u3002", "result": "ViVLA\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u4ec5\u901a\u8fc7\u5355\u6b21\u4e13\u5bb6\u6f14\u793a\u89c6\u9891\u5b66\u4e60\u65b0\u64cd\u4f5c\u6280\u80fd\uff1a\u5728\u672a\u89c1\u8fc7\u7684LIBERO\u4efb\u52a1\u4e0a\u63d0\u5347\u8d85\u8fc730%\uff1b\u8de8\u5177\u8eab\u89c6\u9891\u4fdd\u630135%\u4ee5\u4e0a\u589e\u76ca\uff1b\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u4ece\u4eba\u7c7b\u89c6\u9891\u5b66\u4e60\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5e26\u6765\u8d85\u8fc738%\u7684\u6539\u8fdb\u3002", "conclusion": "ViVLA\u5c55\u793a\u4e86\u4ece\u5355\u6b21\u4e13\u5bb6\u6f14\u793a\u89c6\u9891\u9ad8\u6548\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eba\u7c7b\u89c6\u9891\u5230\u673a\u5668\u4eba\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.07454", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07454", "abs": "https://arxiv.org/abs/2512.07454", "authors": ["Amir Mohammad Akhlaghi", "Amirhossein Shabani", "Mostafa Abdolmaleki", "Saeed Reza Kheradpisheh"], "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning", "comment": null, "summary": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.", "AI": {"tldr": "Persian-Phi\u662f\u4e00\u4e2a3.8B\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bfe\u7a0b\u5b66\u4e60\u6d41\u7a0b\u5c06\u82f1\u8bed\u6a21\u578bPhi-3 Mini\u6709\u6548\u9002\u914d\u5230\u6ce2\u65af\u8bed\uff0c\u8bc1\u660e\u4e86\u5c0f\u6a21\u578b\u4e5f\u80fd\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524dAI\u6c11\u4e3b\u5316\u53d7\u5230\u8bad\u7ec3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5de8\u5927\u8ba1\u7b97\u6210\u672c\u7684\u963b\u788d\uff0c\u9700\u8981\u63a2\u7d22\u8d44\u6e90\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6269\u5c55\u5148\u8fdbLLM\u5230\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u8bfe\u7a0b\u5b66\u4e60\u6d41\u7a0b\uff1a1) \u4f7f\u7528\u53cc\u8bed\u53d9\u4e8b(Tiny Stories)\u8fdb\u884c\"\u9884\u70ed\"\u9636\u6bb5\u5bf9\u9f50\u5d4c\u5165\uff1b2) \u6301\u7eed\u9884\u8bad\u7ec3\uff1b3) \u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\u3002", "result": "Persian-Phi\u5728HuggingFace\u7684Open Persian LLM Leaderboard\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5c3d\u7ba1\u6a21\u578b\u5c3a\u5bf8\u7d27\u51d1\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u786c\u4ef6\u8d44\u6e90\u5c06\u6700\u5148\u8fdb\u7684LLM\u6269\u5c55\u5230\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\uff0c\u6311\u6218\u4e86\u5f3a\u5927\u591a\u8bed\u8a00\u80fd\u529b\u9700\u8981\u5927\u6a21\u578b\u6216\u591a\u8bed\u8a00\u57fa\u7ebf\u7684\u5047\u8bbe\u3002"}}
{"id": "2512.07673", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07673", "abs": "https://arxiv.org/abs/2512.07673", "authors": ["Matthias Heyrman", "Chenhao Li", "Victor Klemm", "Dongho Kang", "Stelian Coros", "Marco Hutter"], "title": "Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots", "comment": "15 pages", "summary": "Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.", "AI": {"tldr": "MDME\u662f\u4e00\u79cd\u591a\u57df\u8fd0\u52a8\u5d4c\u5165\u65b9\u6cd5\uff0c\u4f7f\u7528\u5c0f\u6ce2\u7f16\u7801\u5668\u548c\u6982\u7387\u5d4c\u5165\u8054\u5408\u6355\u6349\u8fd0\u52a8\u4e2d\u7684\u7ed3\u6784\u5316\u5468\u671f\u6a21\u5f0f\u548c\u975e\u89c4\u5219\u53d8\u5316\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u5b9a\u5411\u7684\u5b9e\u65f6\u8fd0\u52a8\u6a21\u4eff", "motivation": "\u73b0\u6709\u8fd0\u52a8\u63a7\u5236\u5668\u5e38\u5ffd\u7565\u8fd0\u52a8\u4e2d\u7684\u56fa\u6709\u6a21\u5f0f\uff0c\u800c\u73b0\u6709\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u6355\u6349\u4eba\u7c7b\u548c\u52a8\u7269\u8fd0\u52a8\u4e2d\u7684\u7ed3\u6784\u5316\u5468\u671f\u6a21\u5f0f\u548c\u975e\u89c4\u5219\u53d8\u5316", "method": "\u63d0\u51fa\u591a\u57df\u8fd0\u52a8\u5d4c\u5165(MDME)\uff0c\u4f7f\u7528\u57fa\u4e8e\u5c0f\u6ce2\u7684\u7f16\u7801\u5668\u548c\u5e76\u884c\u6982\u7387\u5d4c\u5165\u6765\u7edf\u4e00\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u7279\u5f81\u7684\u5d4c\u5165\uff0c\u4ece\u6700\u5c0f\u8f93\u5165\u96c6\u751f\u6210\u4e30\u5bcc\u7684\u53c2\u8003\u8fd0\u52a8\u8868\u793a", "result": "\u5728\u53cc\u8db3\u548c\u56db\u8db3\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u590d\u6742\u8f68\u8ff9\u7684\u51c6\u786e\u518d\u73b0\uff0c\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u5bf9\u672a\u89c1\u8fd0\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u96f6\u6837\u672c\u90e8\u7f72\u751f\u6210\u65b0\u8fd0\u52a8\u98ce\u683c", "conclusion": "MDME\u4f5c\u4e3a\u53ef\u6cdb\u5316\u4e14\u7ed3\u6784\u611f\u77e5\u7684\u57fa\u7840\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u6a21\u4eff\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u6216\u5728\u7ebf\u91cd\u5b9a\u5411"}}
{"id": "2512.07461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07461", "abs": "https://arxiv.org/abs/2512.07461", "authors": ["Tong Wu", "Yang Liu", "Jun Bai", "Zixia Jia", "Shuyi Zhang", "Ziyong Lin", "Yanting Wang", "Song-Chun Zhu", "Zilong Zheng"], "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "comment": null, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "AI": {"tldr": "NPR\u662f\u4e00\u4e2a\u65e0\u9700\u6559\u5e08\u6307\u5bfc\u7684\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u8fdb\u5316\u51fa\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u8bad\u7ec3\u3001\u5e76\u884c\u611f\u77e5\u7b56\u7565\u4f18\u5316\u548c\u4e13\u7528\u5f15\u64ce\u5b9e\u73b0\u4ece\u987a\u5e8f\u6a21\u62df\u5230\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u7684\u8f6c\u53d8\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u91c7\u7528\u987a\u5e8f\u63a8\u7406\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u63a8\u7406\u6548\u7387\u548c\u80fd\u529b\u3002\u867d\u7136\u6709\u4e00\u4e9b\u5e76\u884c\u63a8\u7406\u5c1d\u8bd5\uff0c\u4f46\u5f80\u5f80\u9000\u56de\u5230\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u80fd\u529b\u3002\u9700\u8981\u8ba9\u6a21\u578b\u81ea\u6211\u8fdb\u5316\u51fa\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\u3002", "method": "1) \u81ea\u84b8\u998f\u6e10\u8fdb\u8bad\u7ec3\u8303\u5f0f\uff1a\u4ece\"\u51b7\u542f\u52a8\"\u683c\u5f0f\u53d1\u73b0\u8fc7\u6e21\u5230\u4e25\u683c\u62d3\u6251\u7ea6\u675f\uff1b2) \u5e76\u884c\u611f\u77e5\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff1a\u5728\u6267\u884c\u56fe\u4e2d\u76f4\u63a5\u4f18\u5316\u5206\u652f\u7b56\u7565\uff0c\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u81ea\u9002\u5e94\u5206\u89e3\uff1b3) NPR\u5f15\u64ce\uff1a\u91cd\u6784SGLang\u7684\u5185\u5b58\u7ba1\u7406\u548c\u6d41\u7a0b\u63a7\u5236\uff0c\u652f\u6301\u7a33\u5b9a\u7684\u5927\u89c4\u6a21\u5e76\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u57288\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eQwen3-4B\u8bad\u7ec3\u7684NPR\u5b9e\u73b0\u4e86\u9ad8\u8fbe24.5%\u7684\u6027\u80fd\u63d0\u5347\u548c4.6\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u4e0d\u540c\uff0cNPR\u5c55\u793a\u4e86100%\u7684\u771f\u6b63\u5e76\u884c\u6267\u884c\uff0c\u800c\u4e0d\u662f\u9000\u56de\u5230\u81ea\u56de\u5f52\u89e3\u7801\u3002", "conclusion": "NPR\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u81ea\u6211\u8fdb\u5316\u3001\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u63a8\u7406\u65b0\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86\u4ece\u987a\u5e8f\u6a21\u62df\u5230\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u7684\u8f6c\u53d8\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\u3002"}}
{"id": "2512.07680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07680", "abs": "https://arxiv.org/abs/2512.07680", "authors": ["P. A. Wigner", "L. Romanello", "A. Hammad", "P. H. Nguyen", "T. Lan", "S. F. Armanini", "B. B. Kocer", "M. Kovac"], "title": "AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation", "comment": null, "summary": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.\n  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.\n  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u4ece\u7a7a\u4e2d\u90e8\u7f72\u7684\u6811\u51a0\u722c\u884c\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u67d4\u6027\u5fae\u523a\u5c65\u5e26\u3001\u53cc\u5c65\u5e26\u65cb\u8f6c\u5939\u6301\u5668\u548c\u5f39\u6027\u5c3e\u90e8\uff0c\u80fd\u5728\u4e0d\u540c\u66f2\u7387\u548c\u503e\u659c\u5ea6\u7684\u6811\u679d\u4e0a\u7a33\u5b9a\u79fb\u52a8\u4e0e\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7a7a\u4e2d\u673a\u5668\u4eba\uff08\u5982\u65e0\u4eba\u673a\uff09\u5728\u6811\u51a0\u7b49\u590d\u6742\u73af\u5883\u4e2d\u5b58\u5728\u7eed\u822a\u77ed\u3001\u96be\u4ee5\u7a33\u5b9a\u9644\u7740\u64cd\u4f5c\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u7a7a\u4e2d\u90e8\u7f72\u4f18\u52bf\u4e0e\u5730\u9762\u722c\u884c\u7a33\u5b9a\u6027\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u73af\u5883\u91c7\u6837\u548c\u51a0\u5c42\u5185\u4f20\u611f\u7b49\u751f\u6001\u7814\u7a76\u5e94\u7528\u3002", "method": "\u91c7\u7528\u67d4\u6027\u5fae\u523a\u5c65\u5e26\u63d0\u4f9b\u81ea\u9002\u5e94\u9644\u7740\u80fd\u529b\uff0c\u53cc\u5c65\u5e26\u65cb\u8f6c\u5939\u6301\u5668\u5b9e\u73b0\u53ef\u9760\u6293\u63e1\uff0c\u5f39\u6027\u5c3e\u90e8\u589e\u5f3a\u7a33\u5b9a\u6027\u3002\u7cfb\u7edf\u901a\u8fc7\u65e0\u4eba\u673a-\u7ef3\u7d22\u7cfb\u7edf\u8fdb\u884c\u7a7a\u4e2d\u90e8\u7f72\uff0c\u5177\u5907\u504f\u822a\u8f6c\u5411\u80fd\u529b\u4ee5\u5e94\u5bf9\u4e0d\u89c4\u5219\u8868\u9762\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u53ef\u9760\u6293\u63e1\u80fd\u529b\u8fbe90\u5ea6\u673a\u8eab\u6eda\u8f6c\u548c\u503e\u659c\uff1b\u5728\u503e\u659c67.5\u5ea6\u7684\u6811\u679d\u4e0a\u6709\u6548\u6500\u722c\uff1b\u6c34\u5e73\u6811\u679d\u4e0a\u6700\u5927\u901f\u5ea60.55\u4f53\u957f/\u79d2\uff1b\u504f\u822a\u8f6c\u5411\u8fbe10\u5ea6\uff1b\u65e0\u56e0\u6b21\u8fd0\u8f93\u6210\u672c\u6bd4\u5178\u578b\u60ac\u505c\u529f\u8017\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u7a7a\u4e2d\u53ef\u90e8\u7f72\u722c\u884c\u673a\u5668\u4eba\u6210\u529f\u586b\u8865\u4e86\u7a7a\u4e2d\u4e0e\u5730\u9762\u751f\u6001\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u3001\u4f4e\u529f\u8017\u7684\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u73af\u5883\u91c7\u6837\u548c\u51a0\u5c42\u5185\u4f20\u611f\u5e94\u7528\uff0c\u5728\u590d\u6742\u6811\u51a0\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u9002\u5e94\u6027\u548c\u80fd\u6548\u3002"}}
{"id": "2512.07478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07478", "abs": "https://arxiv.org/abs/2512.07478", "authors": ["Zhuoran Zhuang", "Ye Chen", "Jianghao Su", "Chao Luo", "Luhui Liu", "Xia Zeng"], "title": "Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization", "comment": null, "summary": "Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPRS\u548cVSPO\u4e24\u79cd\u6280\u672f\uff0c\u89e3\u51b3\u5de5\u5177\u96c6\u6210\u63a8\u7406LLM\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u548c\u68af\u5ea6\u9000\u5316\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684LLM\u5728\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u65f6\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u7a00\u758f\u7684\u4e8c\u5143\u5956\u52b1\u4fe1\u53f7\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u6307\u5bfc\u6709\u9650\uff0c\u6536\u655b\u7f13\u6162\uff1b2\uff09GRPO\u4e2d\u76f8\u540c\u5956\u52b1\u5bfc\u81f4\u96f6\u4f18\u52bf\u4f30\u8ba1\uff0c\u964d\u4f4e\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u6280\u672f\uff1a1\uff09\u6e10\u8fdb\u5956\u52b1\u5851\u9020\uff08PRS\uff09\uff0c\u91c7\u7528\u8bfe\u7a0b\u5f0f\u5956\u52b1\u8bbe\u8ba1\uff0c\u5f15\u5165\u5bc6\u96c6\u7684\u9636\u6bb5\u53cd\u9988\uff1b2\uff09\u57fa\u4e8e\u4ef7\u503c\u7684\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff08VSPO\uff09\uff0c\u589e\u5f3aGRPO\u53d8\u4f53\uff0c\u901a\u8fc7\u4efb\u52a1\u4ef7\u503c\u6307\u6807\u9009\u62e9\u63d0\u793a\u5e76\u5e94\u7528\u4ef7\u503c\u5e73\u6ed1\u88c1\u526a\u3002", "result": "\u5728\u591a\u4e2a\u77ed\u5f0f\u548c\u957f\u5f0fQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPRS\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u4e8c\u5143\u5956\u52b1\uff0cVSPO\u76f8\u6bd4PPO\u3001GRPO\u3001CISPO\u548cSFT\u57fa\u7ebf\u8868\u73b0\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3001\u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u9ad8\u7684\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "PRS\u548cVSPO\u76f8\u7ed3\u5408\u4ea7\u751f\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u7684\u57fa\u4e8eLLM\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u548c\u68af\u5ea6\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2512.07697", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07697", "abs": "https://arxiv.org/abs/2512.07697", "authors": ["Aileen Liao", "Dong-Ki Kim", "Max Olan Smith", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks", "comment": null, "summary": "As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.", "AI": {"tldr": "DA-DP\u662f\u4e00\u79cd\u5ef6\u8fdf\u611f\u77e5\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u663e\u5f0f\u5904\u7406\u63a8\u7406\u5ef6\u8fdf\u6765\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u7684\u9c81\u68d2\u6027", "motivation": "\u673a\u5668\u4eba\u611f\u77e5\u548c\u52a8\u4f5c\u9009\u62e9\u4e4b\u95f4\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\uff08\u6570\u5341\u5230\u6570\u767e\u6beb\u79d2\uff09\uff0c\u5bfc\u81f4\u89c2\u5bdf\u72b6\u6001\u4e0e\u6267\u884c\u72b6\u6001\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u8fd9\u79cd\u5ef6\u8fdf\u5f71\u54cd", "method": "\u63d0\u51fa\u5ef6\u8fdf\u611f\u77e5\u6269\u6563\u7b56\u7565\uff08DA-DP\uff09\uff0c\u5c06\u96f6\u5ef6\u8fdf\u8f68\u8ff9\u6821\u6b63\u4e3a\u5ef6\u8fdf\u8865\u507f\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u5ef6\u8fdf\u6761\u4ef6\u589e\u5f3a\u7b56\u7565\uff0c\u8be5\u6846\u67b6\u4e0e\u67b6\u6784\u65e0\u5173\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u7b56\u7565\u7c7b\u578b", "result": "\u5728\u5404\u79cd\u4efb\u52a1\u3001\u673a\u5668\u4eba\u548c\u5ef6\u8fdf\u6761\u4ef6\u4e0b\uff0cDA-DP\u7684\u6210\u529f\u7387\u6bd4\u5ef6\u8fdf\u4e0d\u611f\u77e5\u65b9\u6cd5\u66f4\u9c81\u68d2\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u63a8\u7406\u5ef6\u8fdf", "conclusion": "DA-DP\u4e3a\u5ef6\u8fdf\u611f\u77e5\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u901a\u7528\u6a21\u5f0f\uff0c\u5e76\u9f13\u52b1\u8bc4\u4f30\u534f\u8bae\u62a5\u544a\u6027\u80fd\u4e0e\u5b9e\u6d4b\u5ef6\u8fdf\u7684\u5173\u7cfb\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4efb\u52a1\u96be\u5ea6"}}
{"id": "2512.07515", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07515", "abs": "https://arxiv.org/abs/2512.07515", "authors": ["Pengqian Lu", "Jie Lu", "Anjin Liu", "Guangquan Zhang"], "title": "SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG", "comment": null, "summary": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance", "AI": {"tldr": "SPAD\uff1a\u4e00\u79cd\u65b0\u7684RAG\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06token\u6982\u7387\u5206\u89e3\u4e3a\u4e03\u4e2a\u6765\u6e90\u5e76\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8\u805a\u5408\uff0c\u6709\u6548\u8bc6\u522b\u751f\u6210\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06RAG\u4e2d\u7684\u5e7b\u89c9\u5f52\u56e0\u4e8e\u5185\u90e8\u77e5\u8bc6\uff08FFN\uff09\u4e0e\u68c0\u7d22\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u4e8c\u5143\u51b2\u7a81\uff0c\u4f46\u8fd9\u79cd\u89c6\u89d2\u4e0d\u5b8c\u6574\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u67e5\u8be2\u3001\u5df2\u751f\u6210token\u3001\u5f53\u524dtoken\u672c\u8eab\u548c\u6700\u7ec8LayerNorm\u8c03\u6574\u7b49\u5176\u4ed6\u751f\u6210\u7ec4\u4ef6\u7684\u5f71\u54cd", "method": "SPAD\u65b9\u6cd5\uff1a1\uff09\u6570\u5b66\u4e0a\u5c06\u6bcf\u4e2atoken\u7684\u6982\u7387\u5f52\u56e0\u5230\u4e03\u4e2a\u4e0d\u540c\u6765\u6e90\uff1a\u67e5\u8be2\u3001RAG\u3001\u8fc7\u53bbtoken\u3001\u5f53\u524dtoken\u3001FFN\u3001\u6700\u7ec8LayerNorm\u548c\u521d\u59cb\u5d4c\u5165\uff1b2\uff09\u901a\u8fc7\u8bcd\u6027\u6807\u6ce8\uff08POS tags\uff09\u805a\u5408\u8fd9\u4e9b\u5206\u6570\uff0c\u91cf\u5316\u4e0d\u540c\u7ec4\u4ef6\u5982\u4f55\u9a71\u52a8\u7279\u5b9a\u8bed\u8a00\u7c7b\u522b", "result": "\u901a\u8fc7\u8bc6\u522b\u5f02\u5e38\u6a21\u5f0f\uff08\u5982\u540d\u8bcd\u8fc7\u5ea6\u4f9d\u8d56\u6700\u7ec8LayerNorm\uff09\uff0cSPAD\u80fd\u6709\u6548\u68c0\u6d4b\u5e7b\u89c9\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660eSPAD\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "SPAD\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u7684RAG\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u4e8c\u5143\u51b2\u7a81\u89c6\u89d2\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u6982\u7387\u5206\u89e3\u548c\u8bed\u8a00\u7c7b\u522b\u5206\u6790\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u5e7b\u89c9\u8bc6\u522b"}}
{"id": "2512.07765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07765", "abs": "https://arxiv.org/abs/2512.07765", "authors": ["Gustavo A. Cardona", "Shubham S. Kumbhar", "Panagiotis Artemiadis"], "title": "Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next", "comment": "60 pages, 5 figures, 3 tables", "summary": "Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7269\u7406\u4eba-\u4eba\u5f62\u673a\u5668\u4eba\u4ea4\u4e92\uff08pHHI\uff09\u7684\u73b0\u72b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\uff08\u4eba\u5f62\u5efa\u6a21\u4e0e\u63a7\u5236\u3001\u4eba\u7c7b\u610f\u56fe\u4f30\u8ba1\u3001\u8ba1\u7b97\u4eba\u7c7b\u6a21\u578b\uff09\u5206\u6790\u73b0\u6709\u65b9\u6cd5\u3001\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u8de8\u652f\u67f1\u6574\u5408\u8def\u5f84\u548c\u57fa\u4e8e\u4ea4\u4e92\u6a21\u6001\u4e0e\u673a\u5668\u4eba\u53c2\u4e0e\u5ea6\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\u3002", "motivation": "\u968f\u7740\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9700\u6c42\u589e\u52a0\uff0c\u9700\u8981\u5efa\u7acb\u7a33\u5065\u3001\u53ef\u6269\u5c55\u4e14\u81ea\u9002\u5e94\u7684\u7269\u7406\u4eba-\u4eba\u5f62\u673a\u5668\u4eba\u4ea4\u4e92\u6846\u67b6\u3002\u5f53\u524d\u7814\u7a76\u867d\u7136\u5728\u5404\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u8de8\u652f\u67f1\u6574\u5408\u6709\u9650\uff0c\u963b\u788d\u4e86\u8fde\u8d2f\u4ea4\u4e92\u7684\u5b9e\u73b0\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\u8fdb\u884c\u7cfb\u7edf\u6027\u7efc\u8ff0\uff1a(1) \u4eba\u5f62\u5efa\u6a21\u4e0e\u63a7\u5236\uff1a\u5904\u7406\u4e0d\u786e\u5b9a\u4eba\u7c7b\u52a8\u529b\u5b66\u7684\u5168\u8eab\u63a7\u5236\u7b56\u7565\uff1b(2) \u4eba\u7c7b\u610f\u56fe\u4f30\u8ba1\uff1a\u6709\u9650\u4f20\u611f\u4e0b\u7684\u5b9e\u65f6\u610f\u56fe\u63a8\u65ad\uff1b(3) \u8ba1\u7b97\u4eba\u7c7b\u6a21\u578b\uff1a\u8003\u8651\u4eba\u7c7b\u7269\u7406\u72b6\u6001\u53d8\u5316\u7684\u5efa\u6a21\u6280\u672f\u3002\u540c\u65f6\u63d0\u51fa\u57fa\u4e8e\u4ea4\u4e92\u6a21\u6001\uff08\u76f4\u63a5/\u95f4\u63a5\uff09\u548c\u673a\u5668\u4eba\u53c2\u4e0e\u5ea6\uff08\u534f\u52a9/\u5408\u4f5c/\u534f\u4f5c\uff09\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u5404\u652f\u67f1\u7684\u5173\u952e\u6311\u6218\uff1a\u5168\u8eab\u63a7\u5236\u7b56\u7565\u4e0d\u8db3\u3001\u5b9e\u65f6\u610f\u56fe\u63a8\u65ad\u53d7\u9650\u3001\u4eba\u7c7b\u72b6\u6001\u53d8\u5316\u5efa\u6a21\u4e0d\u5145\u5206\u3002\u867d\u7136\u5404\u9886\u57df\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8de8\u652f\u67f1\u6574\u5408\u4ecd\u7136\u6709\u9650\u3002\u63d0\u51fa\u4e86\u8de8\u9886\u57df\u65b9\u6cd5\u7edf\u4e00\u7684\u8def\u5f84\uff0c\u5e76\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7684\u4ea4\u4e92\u5206\u7c7b\u6846\u67b6\u3002", "conclusion": "\u4e3a\u5b9e\u73b0\u7a33\u5065\u3001\u5b89\u5168\u3001\u76f4\u89c2\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u9700\u8981\u6574\u5408\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\u7684\u65b9\u6cd5\u3002\u63d0\u51fa\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\u548c\u8de8\u652f\u67f1\u6574\u5408\u8def\u5f84\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u4f7f\u4eba\u5f62\u7cfb\u7edf\u80fd\u591f\u5728\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\u4e2d\u6709\u6548\u7406\u89e3\u3001\u9884\u6d4b\u5e76\u4e0e\u4eba\u7c7b\u4f19\u4f34\u534f\u4f5c\u3002"}}
{"id": "2512.07522", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07522", "abs": "https://arxiv.org/abs/2512.07522", "authors": ["Sebastian Sztwiertnia", "Felix Friedrich", "Kristian Kersting", "Patrick Schramowski", "Bj\u00f6rn Deiseroth"], "title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings", "comment": null, "summary": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.", "AI": {"tldr": "LIME\u662f\u4e00\u79cd\u901a\u8fc7\u5c06\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u5143\u6570\u636e\u5d4c\u5165\u5230token\u8868\u793a\u4e2d\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u80fd\u52a0\u901f56%\u7684\u9002\u5e94\u901f\u5ea6\uff0c\u4ec5\u589e\u52a00.01%\u53c2\u6570\uff0c\u5e76\u663e\u8457\u6539\u5584\u8bed\u8a00\u5efa\u6a21\u548c\u751f\u6210\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u9884\u8bad\u7ec3\u6570\u636e\u65e5\u76ca\u7a00\u7f3a\uff0c\u800c\u5143\u6570\u636e\u901a\u5e38\u4ec5\u7528\u4e8e\u6570\u636e\u96c6\u521b\u5efa\u548c\u6574\u7406\uff0c\u5176\u4f5c\u4e3a\u76f4\u63a5\u8bad\u7ec3\u4fe1\u53f7\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u73b0\u72b6\uff0c\u63a2\u7d22\u5143\u6570\u636e\u4f5c\u4e3a\u76f4\u63a5\u8bad\u7ec3\u4fe1\u53f7\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faLIME\u65b9\u6cd5\uff0c\u5c06\u6355\u83b7\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u5143\u6570\u636e\u76f4\u63a5\u5d4c\u5165\u5230token\u8868\u793a\u4e2d\uff0c\u4e30\u5bcctoken\u5d4c\u5165\u3002\u8fd8\u5f00\u53d1\u4e86LIME+1\u53d8\u4f53\uff0c\u4f7f\u7528\u504f\u79fb\u7684\u5143\u6570\u636e\u6765\u6307\u5bfctoken\u751f\u6210\u3002", "result": "LIME\u4f7f\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u9002\u5e94\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe56%\uff0c\u4ec5\u589e\u52a00.01%\u53c2\u6570\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002\u663e\u8457\u6539\u5584\u5206\u8bcd\u8d28\u91cf\uff0c\u589e\u5f3a\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u548c\u751f\u6210\u4efb\u52a1\u6027\u80fd\uff0c\u8fd9\u4e9b\u4f18\u52bf\u5728500M\u52302B\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e2d\u90fd\u4fdd\u6301\u3002LIME+1\u53d8\u4f53\u5728\u7ed9\u5b9a\u4e0b\u4e00\u4e2atoken\u7684\u5143\u6570\u636e\u65f6\uff0c\u63a8\u7406\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe38%\uff0c\u7b97\u672f\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe35%\u3002", "conclusion": "\u5143\u6570\u636e\u4f5c\u4e3a\u76f4\u63a5\u8bad\u7ec3\u4fe1\u53f7\u80fd\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0cLIME\u65b9\u6cd5\u8bc1\u660e\u4e86\u5143\u6570\u636e\u5d4c\u5165\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2512.07775", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07775", "abs": "https://arxiv.org/abs/2512.07775", "authors": ["David Thorne", "Nathan Chan", "Christa S. Robison", "Philip R. Osteen", "Brett T. Lopez"], "title": "OptMap: Geometric Map Distillation via Submodular Maximization", "comment": null, "summary": "Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.", "AI": {"tldr": "OptMap\u662f\u4e00\u79cd\u5b9e\u65f6\u51e0\u4f55\u5730\u56fe\u84b8\u998f\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b50\u6a21\u4f18\u5316\u7406\u8bba\u4eceLiDAR\u6570\u636e\u4e2d\u751f\u6210\u7279\u5b9a\u5e94\u7528\u7684\u6700\u4f18\u5730\u56fe\uff0c\u89e3\u51b3\u4e86NP-hard\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u8981\u4e0d\u540c\u5c3a\u5ea6\u7684\u51e0\u4f55\u5730\u56fe\u6765\u652f\u6301\u5404\u79cd\u611f\u77e5\u548c\u51b3\u7b56\u7b97\u6cd5\uff0c\u4f46LiDAR\u4ea7\u751f\u7684\u5927\u91cf\u51e0\u4f55\u6570\u636e\u96be\u4ee5\u9ad8\u6548\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u4e14\u5c3a\u5bf8\u53d7\u9650\u7684\u5730\u56fe\uff0c\u8fd9\u662f\u4e00\u4e2aNP-hard\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faOptMap\u7b97\u6cd5\uff0c\u6838\u5fc3\u662f\u6700\u5927\u5316\u5177\u6709\u9012\u51cf\u56de\u62a5\u7279\u6027\u7684\u5b50\u6a21\u96c6\u5408\u51fd\u6570\uff0c\u4f7f\u7528\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u83b7\u5f97\u8fd1\u4f3c\u6700\u4f18\u89e3\u3002\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u5b50\u6a21\u5956\u52b1\u51fd\u6570\u6765\u91cf\u5316\u4fe1\u606f\u91cf\u3001\u51cf\u5c11\u8f93\u5165\u96c6\u5927\u5c0f\u5e76\u6700\u5c0f\u5316\u987a\u5e8f\u6536\u96c6\u6570\u636e\u7684\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u91cd\u6392\u5e8f\u6d41\u5f0f\u5b50\u6a21\u7b97\u6cd5\u6765\u6539\u8fdb\u89e3\u8d28\u91cf\u5e76\u89e3\u51b3\u8f93\u5165\u987a\u5e8f\u504f\u5dee\u3002", "result": "\u5728\u5f00\u6e90\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u7279\u522b\u5173\u6ce8\u957f\u65f6\u95f4\u5efa\u56fe\u4f1a\u8bdd\uff0c\u5c55\u793a\u4e86OptMap\u7684\u6700\u5c0f\u8ba1\u7b97\u9700\u6c42\u3002\u63d0\u4f9b\u4e86ROS1\u548cROS2\u5f00\u6e90\u5305\uff0c\u53ef\u4e0e\u4efb\u4f55LiDAR SLAM\u7b97\u6cd5\u914d\u5408\u4f7f\u7528\u3002", "conclusion": "OptMap\u901a\u8fc7\u7406\u8bba\u521b\u65b0\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u7279\u5b9a\u5e94\u7528\u7684\u51e0\u4f55\u5730\u56fe\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4ece\u4e30\u5bccLiDAR\u6570\u636e\u4e2d\u9009\u62e9\u6700\u4f18\u5730\u56fe\u7684NP-hard\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5730\u56fe\u84b8\u998f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07525", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07525", "abs": "https://arxiv.org/abs/2512.07525", "authors": ["Xiaoran Liu", "Yuerong Song", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Zhaoxiang Liu", "Shiguo Lian", "Ziwei He", "Xipeng Qiu"], "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "comment": "20 pages, 6 figures, under review", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "AI": {"tldr": "\u63d0\u51faRoPE\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u91cd\u65b0\u5f15\u5165\u88ab\u4e22\u5f03\u7684\u865a\u90e8\u4fe1\u606f\uff0c\u901a\u8fc7\u53cc\u7ec4\u4ef6\u6ce8\u610f\u529b\u5206\u6570\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u6807\u51c6RoPE\u5b9e\u73b0\u53ea\u4f7f\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b9e\u90e8\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u4e22\u5f03\u4e86\u5305\u542b\u91cd\u8981\u76f8\u4f4d\u4fe1\u606f\u7684\u865a\u90e8\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5efa\u6a21\u4e2d\u5173\u7cfb\u7ec6\u8282\u7684\u635f\u5931\u3002", "method": "\u63d0\u51fa\u6269\u5c55\u65b9\u6cd5\uff0c\u91cd\u65b0\u6574\u5408\u88ab\u4e22\u5f03\u7684\u865a\u90e8\u4fe1\u606f\uff0c\u5229\u7528\u5b8c\u6574\u7684\u590d\u6570\u8868\u793a\u521b\u5efa\u53cc\u7ec4\u4ef6\u6ce8\u610f\u529b\u5206\u6570\uff0c\u4fdd\u7559\u66f4\u591a\u4f4d\u7f6e\u4fe1\u606f\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6RoPE\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0c\u4f18\u52bf\u66f4\u52a0\u663e\u8457\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u5f15\u5165\u590d\u6570\u70b9\u79ef\u7684\u865a\u90e8\u4fe1\u606f\uff0c\u53ef\u4ee5\u589e\u5f3aRoPE\u5bf9\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u63d0\u5347LLMs\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2512.07813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07813", "abs": "https://arxiv.org/abs/2512.07813", "authors": ["Hari Prakash Thanabalan", "Lars Bengtsson", "Ugo Lafont", "Giovanni Volpe"], "title": "Inchworm-Inspired Soft Robot with Groove-Guided Locomotion", "comment": null, "summary": "Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.", "AI": {"tldr": "\u5355\u9a71\u52a8\u5668\u8f6f\u4f53\u673a\u5668\u4eba\u901a\u8fc7\u56fe\u6848\u5316\u57fa\u5e95\u88ab\u52a8\u63a7\u5236\u8fd0\u52a8\u65b9\u5411\uff0c\u7b80\u5316\u4e86\u8bbe\u8ba1\u548c\u63a7\u5236\uff0c\u964d\u4f4e\u4e86\u80fd\u8017", "motivation": "\u4f20\u7edf\u8f6f\u4f53\u673a\u5668\u4eba\u9700\u8981\u591a\u4e2a\u9a71\u52a8\u5668\u6765\u5b9e\u73b0\u65b9\u5411\u63a7\u5236\uff0c\u8fd9\u589e\u52a0\u4e86\u673a\u68b0\u590d\u6742\u6027\u3001\u63a7\u5236\u96be\u5ea6\u548c\u80fd\u8017\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u8282\u80fd\u7684\u65b9\u5411\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53d7\u5c3a\u8816\u542f\u53d1\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u4f7f\u7528\u5355\u4e2a\u5377\u66f2\u4ecb\u7535\u5f39\u6027\u4f53\u9a71\u52a8\u5668\uff0c\u901a\u8fc73D\u6253\u5370\u57fa\u5e95\u4e0a\u7684\u6c9f\u69fd\u56fe\u6848\u88ab\u52a8\u5f15\u5bfc\u673a\u5668\u4eba\u7684\u5bf9\u9f50\u548c\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u6539\u53d8\u6c9f\u69fd\u89d2\u5ea6\u53ef\u4ee5\u7cbe\u786e\u63a7\u5236\u8fd0\u52a8\u65b9\u5411\uff0c\u65e0\u9700\u590d\u6742\u7684\u9a71\u52a8\u7b56\u7565\u3002\u8fd9\u79cd\u65b9\u6cd5\u964d\u4f4e\u4e86\u80fd\u8017\uff0c\u7b80\u5316\u4e86\u673a\u5668\u4eba\u8bbe\u8ba1\u3002", "conclusion": "\u6c9f\u69fd\u5f15\u5bfc\u65b9\u6cd5\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u65b9\u5411\u63a7\u5236\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u5176\u5728\u641c\u6551\u3001\u7ba1\u9053\u68c0\u6d4b\u548c\u884c\u661f\u63a2\u7d22\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.07538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07538", "abs": "https://arxiv.org/abs/2512.07538", "authors": ["Michelle Wastl", "Jannis Vamvas", "Rico Sennrich"], "title": "SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents", "comment": "30 pages", "summary": "Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SwissGov-RSD\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8de8\u8bed\u8a00\u6587\u6863\u7ea7\u8bed\u4e49\u5dee\u5f02\u8bc6\u522b\u7684\u81ea\u7136\u6570\u636e\u96c6\uff0c\u5305\u542b224\u4e2a\u591a\u8bed\u8a00\u5e73\u884c\u6587\u6863\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u548c\u7f16\u7801\u5668\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u8bc6\u522b\u8de8\u6587\u6863\uff08\u5c24\u5176\u662f\u4e0d\u540c\u8bed\u8a00\u95f4\uff09\u7684\u8bed\u4e49\u5dee\u5f02\u5bf9\u4e8e\u6587\u672c\u751f\u6210\u8bc4\u4f30\u548c\u591a\u8bed\u8a00\u5185\u5bb9\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f5c\u4e3a\u72ec\u7acb\u4efb\u52a1\u5374\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u6784\u5efa\u4e86SwissGov-RSD\u6570\u636e\u96c6\uff0c\u5305\u542b224\u4e2a\u82f1\u8bed-\u5fb7\u8bed\u3001\u82f1\u8bed-\u6cd5\u8bed\u3001\u82f1\u8bed-\u610f\u5927\u5229\u8bed\u7684\u591a\u8bed\u8a00\u5e73\u884c\u6587\u6863\uff0c\u5e76\u8fdb\u884c\u4e86token\u7ea7\u522b\u7684\u5dee\u5f02\u6807\u6ce8\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u7f16\u7801\u5668\u6a21\u578b\u5728\u4e0d\u540c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5f53\u524d\u81ea\u52a8\u65b9\u6cd5\u5728\u8be5\u65b0\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u8fdc\u5dee\u4e8e\u5728\u5355\u8bed\u3001\u53e5\u5b50\u7ea7\u548c\u5408\u6210\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86LLM\u548c\u7f16\u7801\u5668\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u8de8\u8bed\u8a00\u6587\u6863\u7ea7\u8bed\u4e49\u5dee\u5f02\u8bc6\u522b\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u4f5c\u8005\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2512.07819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07819", "abs": "https://arxiv.org/abs/2512.07819", "authors": ["Shubham S. Kumbhar", "Abhijeet M. Kulkarni", "Panagiotis Artemiadis"], "title": "Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation", "comment": null, "summary": "We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u4eba\u7c7b\u534f\u4f5c\u642c\u8fd0\u63a7\u5236\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u5c42\u89c4\u5212\u3001\u5e95\u5c42\u63a7\u5236\u548c\u521a\u5ea6\u8c03\u8282\u4e09\u90e8\u5206\uff0c\u901a\u8fc7\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\u5e76\u63d0\u51fa\u4e86\u534f\u4f5c\u6548\u7387\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u4f19\u4f34\u7684\u534f\u4f5c\u642c\u8fd0\u4efb\u52a1\uff0c\u652f\u6301\u5e73\u79fb\u548c\u65cb\u8f6c\u8fd0\u52a8\uff0c\u8fd9\u662f\u534f\u4f5c\u642c\u8fd0\u573a\u666f\u4e2d\u7684\u57fa\u672c\u9700\u6c42\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u9ad8\u5c42\u89c4\u5212\u5668\uff08\u5f15\u5165\u4ea4\u4e92\u7ebf\u6027\u5012\u7acb\u6446I-LIP\u7ed3\u5408\u5bfc\u7eb3\u6a21\u578b\u548cMPC\u751f\u6210\u52a8\u6001\u53ef\u884c\u7684\u6b65\u6001\u89c4\u5212\uff09\u3001\u5e95\u5c42\u63a7\u5236\u5668\uff08\u57fa\u4e8eQP\u7684\u5168\u8eab\u63a7\u5236\u5668\u5904\u7406\u8026\u5408\u7684\u4eba\u5f62-\u7269\u4f53\u52a8\u529b\u5b66\uff09\u3001\u521a\u5ea6\u8c03\u8282\u673a\u5236\uff08\u8c03\u8282\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u4ee5\u786e\u4fdd\u6536\u655b\u5230\u671f\u671b\u7684\u76f8\u5bf9\u914d\u7f6e\uff09\u3002", "result": "\u5728Digit\u4eba\u5f62\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u4e86\u91cf\u5316\u534f\u4f5c\u6548\u7387\u7684\u6307\u6807\uff0c\u8be5\u6307\u6807\u63ed\u793a\u4e86\u67d4\u987a\u6027\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u5e73\u79fb\u3001\u8f6c\u5411\u548c\u534a\u5706\u5f62\u8f68\u8ff9\u7b49\u534f\u4f5c\u884c\u4e3a\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u8be5\u63a7\u5236\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u534f\u4f5c\u642c\u8fd0\uff0c\u63d0\u51fa\u7684\u6548\u7387\u6307\u6807\u80fd\u591f\u8bc4\u4f30\u534f\u4f5c\u8d28\u91cf\u5e76\u4e3a\u63a7\u5236\u5c42\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u5728\u81ea\u7136\u534f\u4f5c\u642c\u8fd0\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.07540", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07540", "abs": "https://arxiv.org/abs/2512.07540", "authors": ["Boxuan Lyu", "Haiyue Song", "Hidetaka Kamigaito", "Chenchen Ding", "Hideki Tanaka", "Masao Utiyama", "Kotaro Funakoshi", "Manabu Okumura"], "title": "Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation", "comment": null, "summary": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u751f\u6210\u5f0f\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\uff08ESD\uff09\u4efb\u52a1\u4e2d\u4f7f\u7528\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\uff08MBR\uff09\u89e3\u7801\u66ff\u4ee3\u4f20\u7edf\u7684\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u89e3\u7801\uff0c\u4ee5\u89e3\u51b3\u6a21\u578b\u6982\u7387\u4e0e\u4eba\u7c7b\u6807\u6ce8\u76f8\u4f3c\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7MBR\u84b8\u998f\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0fESD\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528MAP\u89e3\u7801\uff0c\u5047\u8bbe\u6a21\u578b\u4f30\u8ba1\u7684\u6982\u7387\u4e0e\u4eba\u7c7b\u6807\u6ce8\u76f8\u4f3c\u5ea6\u5b8c\u7f8e\u76f8\u5173\u3002\u4f46\u4f5c\u8005\u89c2\u5bdf\u5230\uff0c\u4e0e\u4eba\u7c7b\u6807\u6ce8\u4e0d\u76f8\u4f3c\u7684\u6807\u6ce8\u53ef\u80fd\u83b7\u5f97\u66f4\u9ad8\u7684\u6a21\u578b\u4f3c\u7136\u5ea6\uff0c\u8fd9\u8868\u660eMAP\u89e3\u7801\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u5728\u751f\u6210\u5f0fESD\u6a21\u578b\u4e2d\u5e94\u7528MBR\u89e3\u7801\uff0c\u4f7f\u7528\u53e5\u5b50\u7ea7\u548c\u8de8\u5ea6\u7ea7\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u4f5c\u4e3a\u6548\u7528\u51fd\u6570\uff0c\u57fa\u4e8e\u5019\u9009\u5047\u8bbe\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u8fd1\u4f3c\u76f8\u4f3c\u5ea6\u8fdb\u884c\u9009\u62e9\u3002\u540c\u65f6\u5f15\u5165MBR\u84b8\u998f\u6280\u672f\uff0c\u8ba9\u6807\u51c6\u8d2a\u5a6a\u6a21\u578b\u5339\u914dMBR\u89e3\u7801\u6027\u80fd\uff0c\u89e3\u51b3\u63a8\u7406\u65f6\u5ef6\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMBR\u89e3\u7801\u5728\u7cfb\u7edf\u7ea7\u3001\u53e5\u5b50\u7ea7\u548c\u8de8\u5ea6\u7ea7\u5747\u4f18\u4e8eMAP\u57fa\u7ebf\u3002MBR\u84b8\u998f\u6709\u6548\u6d88\u9664\u4e86\u63a8\u7406\u65f6\u5ef6\u74f6\u9888\uff0c\u4f7f\u6807\u51c6\u8d2a\u5a6a\u6a21\u578b\u8fbe\u5230\u4e0eMBR\u89e3\u7801\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "MBR\u89e3\u7801\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u751f\u6210\u5f0fESD\u4efb\u52a1\u4e2d\u6a21\u578b\u6982\u7387\u4e0e\u4eba\u7c7b\u6807\u6ce8\u76f8\u4f3c\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u800cMBR\u84b8\u998f\u6280\u672f\u5219\u89e3\u51b3\u4e86MBR\u89e3\u7801\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.07543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07543", "abs": "https://arxiv.org/abs/2512.07543", "authors": ["Frederic Blum"], "title": "Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects", "comment": "Accepted with minor revisions at *Linguistic Typology*, expected to be fully published in 2026", "summary": "The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cd\u65b0\u68c0\u9a8c\u4e86\u57fa\u672c\u8bcd\u6c47\u4e2d\u8bed\u97f3\u7279\u5f81\u7684\u7edf\u8ba1\u8fc7\u8868\u5f81\u73b0\u8c61\uff0c\u901a\u8fc7\u6269\u5927\u8bed\u8a00\u6837\u672c\uff082864\u79cd\u8bed\u8a00\uff09\u5e76\u52a0\u5165\u8c31\u7cfb\u548c\u5730\u57df\u63a7\u5236\uff0c\u53d1\u73b0\u5148\u524d\u7814\u7a76\u4e2d\u7684\u591a\u6570\u6a21\u5f0f\u5e76\u4e0d\u7a33\u5065\uff0c\u53ea\u6709\u5c11\u6570\u6a21\u5f0f\u4fdd\u6301\u7a33\u5b9a\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u57fa\u672c\u8bcd\u6c47\u4e2d\u8bed\u97f3\u8c61\u5f81\u73b0\u8c61\u7684\u7814\u7a76\u5b58\u5728\u65b9\u6cd5\u5b66\u95ee\u9898\uff1a\u6837\u672c\u504f\u8bef\u3001\u672a\u5145\u5206\u63a7\u5236\u8bed\u8a00\u95f4\u7684\u8c31\u7cfb\u548c\u5730\u57df\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u7ed3\u679c\u7684\u7a33\u5065\u6027\u5b58\u7591\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\u68c0\u9a8c\u8fd9\u4e9b\u53d1\u73b0\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528Lexibank\u6570\u636e\u5e93\u76842864\u79cd\u8bed\u8a00\u6570\u636e\uff0c\u6269\u5927\u6837\u672c\u89c4\u6a21\uff1b\u4fee\u6539\u539f\u59cb\u6a21\u578b\uff0c\u52a0\u5165\u7a7a\u95f4\u548c\u8c31\u7cfb\u4f9d\u8d56\u5173\u7cfb\u7684\u7edf\u8ba1\u63a7\u5236\uff1b\u91cd\u65b0\u5206\u6790\u57fa\u672c\u8bcd\u6c47\u4e2d\u7684\u8bed\u97f3\u8c61\u5f81\u6a21\u5f0f\u3002", "result": "\u5927\u591a\u6570\u5148\u524d\u89c2\u5bdf\u5230\u7684\u8bed\u97f3\u8c61\u5f81\u6a21\u5f0f\u5728\u52a0\u5165\u8c31\u7cfb\u548c\u5730\u57df\u63a7\u5236\u540e\u4e0d\u518d\u7a33\u5065\uff0c\u751a\u81f3\u5b8c\u5168\u6d88\u5931\uff1b\u53ea\u6709\u5c11\u6570\u6a21\u5f0f\u8868\u73b0\u51fa\u9ad8\u5ea6\u7a33\u5b9a\u6027\uff1b\u80fd\u591f\u5728\u66f4\u5927\u89c4\u6a21\u4e0a\u8bc4\u4f30\u8bed\u97f3\u8c61\u5f81\u7684\u5206\u5e03\u3002", "conclusion": "\u8bed\u97f3\u8c61\u5f81\u73b0\u8c61\u53ef\u80fd\u6bd4\u5148\u524d\u8ba4\u4e3a\u7684\u66f4\u6709\u9650\uff1b\u7814\u7a76\u5f3a\u8c03\u4e86\u68c0\u9a8c\u8bed\u8a00\u666e\u904d\u6027\u4e3b\u5f20\u65f6\u9700\u8981\u5728\u591a\u4e2a\u5c42\u9762\u4e0a\u8fdb\u884c\u7a33\u5065\u6027\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\uff1b\u53ea\u6709\u5c11\u6570\u8bed\u97f3\u8c61\u5f81\u6a21\u5f0f\u80fd\u7ecf\u53d7\u4f4f\u4e25\u683c\u7684\u7edf\u8ba1\u63a7\u5236\u3002"}}
{"id": "2512.07544", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07544", "abs": "https://arxiv.org/abs/2512.07544", "authors": ["Kyungro Lee", "Dongha Choi", "Hyunju Lee"], "title": "MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue", "comment": "18 pages", "summary": "As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.", "AI": {"tldr": "MoCoRP\u662f\u4e00\u4e2a\u7528\u4e8e\u57fa\u4e8e\u89d2\u8272\u7684\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u89d2\u8272\u53e5\u5b50\u4e0e\u56de\u590d\u4e4b\u95f4\u7684NLI\u5173\u7cfb\uff0c\u63d0\u5347\u5bf9\u8bdd\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89d2\u8272\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u7f3a\u4e4f\u89d2\u8272\u53e5\u5b50\u4e0e\u56de\u590d\u4e4b\u95f4\u7684\u663e\u5f0f\u5173\u7cfb\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u89d2\u8272\u4fe1\u606f\uff0c\u5f71\u54cd\u5bf9\u8bdd\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002", "method": "\u63d0\u51faMoCoRP\u6846\u67b6\uff0c\u5229\u7528NLI\u4e13\u5bb6\u663e\u5f0f\u63d0\u53d6\u89d2\u8272\u53e5\u5b50\u4e0e\u56de\u590d\u4e4b\u95f4\u7684NLI\u5173\u7cfb\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6709\u6548\u5c06\u9002\u5f53\u7684\u89d2\u8272\u4fe1\u606f\u878d\u5165\u56de\u590d\u4e2d\u3002\u8be5\u6846\u67b6\u5e94\u7528\u4e8eBART\u7b49\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5bf9\u9f50\u8c03\u4f18\u6269\u5c55\u5230\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728ConvAI2\u548cMPChat\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoCoRP\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u89d2\u8272\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5bf9\u8bdd\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e0d\u4ec5\u5728\u5b9a\u91cf\u6307\u6807\u4e0a\u51fa\u8272\uff0c\u5728\u5b9a\u6027\u65b9\u9762\u4e5f\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u89d2\u8272-\u56de\u590d\u5173\u7cfb\u5728\u57fa\u4e8e\u89d2\u8272\u7684\u5bf9\u8bdd\u4e2d\u975e\u5e38\u6709\u6548\uff0cMoCoRP\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u751f\u6210\u4e00\u81f4\u4e14\u5438\u5f15\u4eba\u7684\u89d2\u8272\u5316\u56de\u590d\u80fd\u529b\u3002"}}
{"id": "2512.07552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07552", "abs": "https://arxiv.org/abs/2512.07552", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries", "comment": "8 pages, 3 figures", "summary": "In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.", "AI": {"tldr": "SafeTerm AMQ\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u81ea\u52a8\u533b\u7597\u67e5\u8be2\u7cfb\u7edf\uff0c\u7528\u4e8e\u4eceMedDRA\u672f\u8bed\u4e2d\u68c0\u7d22\u76f8\u5173\u4e0d\u826f\u4e8b\u4ef6\u672f\u8bed\uff0c\u5728SMQs\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ec\u56de\u7387\u548c\u53ef\u8c03\u8282\u7684\u7cbe\u786e\u5ea6\u3002", "motivation": "\u5728\u836f\u7269\u5b89\u5168\u5ba1\u67e5\u4e2d\uff0c\u5c06\u76f8\u5173\u4e0d\u826f\u4e8b\u4ef6\u672f\u8bed\u5206\u7ec4\u5230SMQs\u6216OCMQs\u5bf9\u4e8e\u4fe1\u53f7\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u9ad8\u6548\u5904\u7406MedDRA\u672f\u8bed\u67e5\u8be2\u3002", "method": "SafeTerm AMQ\u5c06\u533b\u7597\u67e5\u8be2\u672f\u8bed\u548cMedDRA PTs\u5d4c\u5165\u591a\u7ef4\u5411\u91cf\u7a7a\u95f4\uff0c\u5e94\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6781\u503c\u805a\u7c7b\u751f\u6210\u6392\u540d\u5217\u8868\u3002\u901a\u8fc7\u591a\u6807\u51c6\u7edf\u8ba1\u65b9\u6cd5\u8ba1\u7b97\u76f8\u5173\u6027\u5206\u6570(0-1)\u3002\u5728110\u4e2aSMQs\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8ba1\u7b97\u4e0d\u540c\u76f8\u4f3c\u5ea6\u9608\u503c\u4e0b\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "result": "\u5728\u4e2d\u7b49\u76f8\u4f3c\u5ea6\u9608\u503c\u4e0b\u5b9e\u73b0\u9ad8\u53ec\u56de\u7387(94%)\uff0c\u8f83\u9ad8\u9608\u503c\u4e0b\u7cbe\u786e\u5ea6\u53ef\u8fbe89%\u3002\u6700\u4f73\u9608\u503c(0.70)\u4e0b\u6574\u4f53\u53ec\u56de\u738748%\u3001\u7cbe\u786e\u738745%\u3002\u7a84\u672f\u8bedPTs\u5728\u66f4\u9ad8\u9608\u503c\u4e0b\u8868\u73b0\u66f4\u597d\u3002\u81ea\u52a8\u9608\u503c\u9009\u62e9(0.66)\u4f18\u5148\u53ec\u56de\u7387(0.58)\u800c\u975e\u7cbe\u786e\u7387(0.29)\u3002", "conclusion": "SafeTerm AMQ\u5728SMQs\u548cOCMQs\u4e0a\u8868\u73b0\u76f8\u5f53\u4e14\u4ee4\u4eba\u6ee1\u610f\uff0c\u662f\u81ea\u52a8MedDRA\u67e5\u8be2\u751f\u6210\u7684\u53ef\u884c\u8865\u5145\u65b9\u6cd5\uff0c\u80fd\u5e73\u8861\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u3002\u5efa\u8bae\u4f7f\u7528\u5408\u9002\u7684MedDRA PT\u672f\u8bed\u8fdb\u884c\u67e5\u8be2\uff0c\u5e76\u5e94\u7528\u81ea\u52a8\u9608\u503c\u65b9\u6cd5\u4f18\u5316\u53ec\u56de\u7387\u3002"}}
{"id": "2512.07571", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.07571", "abs": "https://arxiv.org/abs/2512.07571", "authors": ["Nicolas Calbucura", "Valentin Barriere"], "title": "A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification", "comment": null, "summary": "This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7lasso\u7279\u5f81\u9009\u62e9\u4ece\u8bed\u97f3\u6807\u8bb0\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff0c\u589e\u5f3a\u6587\u672c\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u4f20\u7edf\u97f3\u9891-\u6587\u672c\u878d\u5408\u65b9\u6cd5\u9762\u4e34\u97f3\u9891\u5e8f\u5217\u8fc7\u957f\u3001\u8bed\u97f3\u6807\u8bb0\u5668\u8f93\u51fa\u5e8f\u5217\u8fc7\u957f\u4e14\u8bcd\u6c47\u91cf\u5927\u3001\u96be\u4ee5\u4f4e\u6210\u672c\u96c6\u6210\u5230\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u95ee\u9898\u3002\u7279\u522b\u662f\u5728\u67d0\u4e9b\u4efb\u52a1\uff08\u5982\u8bba\u8bc1\u8c2c\u8bef\u68c0\u6d4b\uff09\u4e2d\uff0c\u97f3\u9891\u4fe1\u606f\u88ab\u8ba4\u4e3a\u53ef\u80fd\u5bf9\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd", "method": "\u4f7f\u7528\u57fa\u4e8elasso\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u5728\u591a\u6a21\u6001\u8bcd\u888b\u8868\u793a\u4e2d\u4fdd\u7559\u5bf9\u4efb\u52a1\u6700\u91cd\u8981\u7684\u97f3\u9891\u6807\u8bb0\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u76d1\u7763\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u4f7f\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u8fd9\u4e9b\u6807\u8bb0\uff0c\u6700\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03", "result": "\u76f8\u6bd4\u5355\u6a21\u6001\u6a21\u578b\u3001\u66f4\u5927\u7684SpeechLM\u6a21\u578b\u6216\u901a\u8fc7\u5b66\u4e60\u8868\u793a\u96c6\u6210\u97f3\u9891\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u6027\u80fd\u3002\u5728\u4e24\u4e2a\u8bba\u8bc1\u8c2c\u8bef\u68c0\u6d4b\u4e0e\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5373\u4f7f\u968f\u673a\u9009\u62e9\u97f3\u9891\u6807\u8bb0\u4e5f\u80fd\u589e\u5f3a\u5355\u6a21\u6001\u6a21\u578b", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5355\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u6587\u672c\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u8bed\u97f3\u4fe1\u606f\u63d0\u5347\u7279\u5b9a\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u97f3\u9891\u4fe1\u606f\u88ab\u8ba4\u4e3a\u53ef\u80fd\u6709\u5bb3\u7684\u4efb\u52a1\u4e2d\u4e5f\u80fd\u53d6\u5f97\u663e\u8457\u6539\u8fdb"}}
{"id": "2512.07583", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07583", "abs": "https://arxiv.org/abs/2512.07583", "authors": ["Navid Asgari", "Benjamin M. Cole"], "title": "Complementary Learning Approach for Text Classification using Large Language Models", "comment": "67 pages", "summary": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u548c\u5c11\u6837\u672c\u5b66\u4e60\u63d0\u793a\uff0c\u5c06LLMs\u4ee5\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u65b9\u5f0f\u6574\u5408\u5230\u5b9a\u91cf\u7814\u7a76\u4e2d\uff0c\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u4e92\u8865", "motivation": "\u89e3\u51b3\u5982\u4f55\u5728\u5b9a\u91cf\u7814\u7a76\u4e2d\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u514b\u670d\u5176\u56fa\u6709\u5f31\u70b9\uff0c\u5b9e\u73b0\u4eba\u673a\u4f18\u52bf\u4e92\u8865\uff0c\u8ba9\u5b66\u8005\u80fd\u591f\u4ee5\u4f4e\u6210\u672c\u65b9\u5f0f\u7ba1\u7406LLMs\u7684\u5c40\u9650\u6027", "method": "\u91c7\u7528\u601d\u7ef4\u94fe\u548c\u5c11\u6837\u672c\u5b66\u4e60\u63d0\u793a\u6280\u672f\uff0c\u5c06\u5b9a\u6027\u7814\u7a76\u4e2d\u5408\u4f5c\u56e2\u961f\u7684\u6700\u4f73\u5b9e\u8df5\u6269\u5c55\u5230\u5b9a\u91cf\u7814\u7a76\u7684\u4eba\u673a\u534f\u4f5c\u4e2d\uff0c\u5141\u8bb8\u4eba\u7c7b\u4f7f\u7528\u6eaf\u56e0\u63a8\u7406\u548c\u81ea\u7136\u8bed\u8a00\u5ba1\u67e5\u673a\u5668\u548c\u4eba\u7c7b\u7684\u5de5\u4f5c", "result": "\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u5206\u67901990-2017\u5e74\u95f41,934\u4efd\u5236\u836f\u8054\u76df\u65b0\u95fb\u7a3f\u4e2d\u7684\u4eba\u673a\u8bc4\u7ea7\u5dee\u5f02\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u7ba1\u7406LLMs\u7684\u5f31\u70b9", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\uff0c\u5b66\u8005\u80fd\u591f\u4ee5\u4f4e\u6210\u672c\u65b9\u5f0f\u5229\u7528LLMs\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u6709\u6548\u7ba1\u7406\u5176\u5f31\u70b9\uff0c\u4e3a\u5b9a\u91cf\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u8303\u5f0f"}}
{"id": "2512.07608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07608", "abs": "https://arxiv.org/abs/2512.07608", "authors": ["Jing Wang", "Jie Shen", "Xing Niu", "Tong Zhang", "Jeremy Weiss"], "title": "Metric-Fair Prompting: Treating Similar Samples Similarly", "comment": null, "summary": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \\emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \\((\\text{question}, \\text{option})\\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.", "AI": {"tldr": "\u63d0\u51faMetric-Fair Prompting\u6846\u67b6\uff0c\u901a\u8fc7\u5ea6\u91cf\u516c\u5e73\u6027\u7ea6\u675f\u6307\u5bfcLLM\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u505a\u51fa\u51b3\u7b56\uff0c\u5229\u7528\u95ee\u9898\u76f8\u4f3c\u6027\u548cLipschitz\u7ea6\u675f\u786e\u4fdd\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u5728MedQA\u57fa\u51c6\u4e0a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728\u533b\u7597\u95ee\u7b54\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u786e\u4fddLLM\u51b3\u7b56\u7684\u516c\u5e73\u6027\uff0c\u7279\u522b\u662f\u4fdd\u8bc1\u76f8\u4f3c\u95ee\u9898\u5f97\u5230\u76f8\u4f3c\u5904\u7406\uff08\u4e2a\u4f53\u516c\u5e73\u6027\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u5904\u7406\u6bcf\u4e2a\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u76f8\u4f3c\u5b9e\u4f8b\u4e00\u81f4\u6027\u7684\u8003\u8651\u3002", "method": "\u5c06(\u95ee\u9898,\u9009\u9879)\u5bf9\u89c6\u4e3a\u4e8c\u5143\u5b9e\u4f8b\uff0c\u4f7f\u7528NLP\u5d4c\u5165\u8ba1\u7b97\u95ee\u9898\u76f8\u4f3c\u6027\uff0c\u5728\u76f8\u4f3c\u95ee\u9898\u5bf9\u4e2d\u8054\u5408\u5904\u7406\u800c\u975e\u5b64\u7acb\u5904\u7406\u3002\u63d0\u793a\u6846\u67b6\u5f3a\u5236\u63d0\u53d6\u51b3\u5b9a\u6027\u4e34\u5e8a\u7279\u5f81\uff0c\u5c06\u6bcf\u4e2a\u5bf9\u6620\u5c04\u5230\u7f6e\u4fe1\u5ea6\u5206\u6570f(x)\uff0c\u5e76\u65bd\u52a0Lipschitz\u5f0f\u7ea6\u675f\u786e\u4fdd\u76f8\u4f3c\u8f93\u5165\u83b7\u5f97\u76f8\u4f3c\u5206\u6570\u548c\u4e00\u81f4\u8f93\u51fa\u3002", "result": "\u5728MedQA(US)\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetric-Fair Prompting\u76f8\u6bd4\u6807\u51c6\u5355\u9879\u76ee\u63d0\u793a\u65b9\u6cd5\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8868\u660e\u516c\u5e73\u5f15\u5bfc\u3001\u7f6e\u4fe1\u5ea6\u5bfc\u5411\u7684\u63a8\u7406\u80fd\u591f\u589e\u5f3aLLM\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u591a\u9009\u9898\u4e0a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5ea6\u91cf\u516c\u5e73\u63d0\u793a\u6846\u67b6\u901a\u8fc7\u5f3a\u5236\u76f8\u4f3c\u95ee\u9898\u83b7\u5f97\u4e00\u81f4\u5904\u7406\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86LLM\u51b3\u7b56\u7684\u516c\u5e73\u6027\uff0c\u8fd8\u610f\u5916\u5730\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4e3a\u9ad8\u98ce\u9669\u533b\u7597\u5e94\u7528\u4e2d\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.07612", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07612", "abs": "https://arxiv.org/abs/2512.07612", "authors": ["Kairong Luo", "Zhenbo Sun", "Xinyu Shi", "Shengqi Chen", "Bowen Yu", "Yunyi Chen", "Chenyi Dang", "Hengtao Tao", "Hui Wang", "Fangming Liu", "Kaifeng Lyu", "Wenguang Chen"], "title": "PCMind-2.1-Kaiyuan-2B Technical Report", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.", "AI": {"tldr": "PCMind-2.1-Kaiyuan-2B\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u768420\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u6df7\u5408\u7b56\u7565\u3001\u9009\u62e9\u6027\u91cd\u590d\u8bad\u7ec3\u548c\u591a\u9886\u57df\u8bfe\u7a0b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u5f00\u6e90\u793e\u533a\u4e0e\u4ea7\u4e1a\u754c\u4e4b\u95f4\u56e0\u95ed\u6e90\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u6cd5\u9020\u6210\u7684\u77e5\u8bc6\u5dee\u8ddd\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u63d0\u4f9b\u5b9e\u7528\u7684\u9884\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5206\u4f4d\u6570\u6570\u636e\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff1a\u7cfb\u7edf\u6bd4\u8f83\u5f02\u6784\u5f00\u6e90\u6570\u636e\u96c6\u5e76\u63d0\u4f9b\u6570\u636e\u6df7\u5408\u7b56\u7565\uff1b2. \u6218\u7565\u9009\u62e9\u6027\u91cd\u590d\u65b9\u6848\uff1a\u5728\u591a\u9636\u6bb5\u8303\u5f0f\u4e2d\u6709\u6548\u5229\u7528\u7a00\u758f\u9ad8\u8d28\u91cf\u6570\u636e\uff1b3. \u591a\u9886\u57df\u8bfe\u7a0b\u8bad\u7ec3\u7b56\u7565\uff1a\u6309\u8d28\u91cf\u6392\u5e8f\u6837\u672c\uff1b4. \u9ad8\u5ea6\u4f18\u5316\u7684\u6570\u636e\u9884\u5904\u7406\u6d41\u7a0b\u548cFP16\u7a33\u5b9a\u6027\u67b6\u6784\u4fee\u6539\u3002", "result": "Kaiyuan-2B\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u5b8c\u5168\u5f00\u6e90\u6a21\u578b\u7ade\u4e89\uff0c\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u9884\u8bad\u7ec3\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5b8c\u5168\u5f00\u6e90\u7684\u89e3\u51b3\u65b9\u6848\uff08\u6a21\u578b\u6743\u91cd\u3001\u6570\u636e\u548c\u4ee3\u7801\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u5f00\u6e90\u793e\u533a\u4e0e\u4ea7\u4e1a\u754c\u7684\u5dee\u8ddd\u3002"}}
{"id": "2512.07666", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07666", "abs": "https://arxiv.org/abs/2512.07666", "authors": ["Zeqi Chen", "Zhaoyang Chu", "Yi Gui", "Feng Guo", "Yao Wan", "Chuan Shi"], "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.", "AI": {"tldr": "CGBridge\uff1a\u4e00\u79cd\u901a\u8fc7\u5916\u90e8\u53ef\u8bad\u7ec3\u6865\u63a5\u6a21\u5757\u5c06\u4ee3\u7801\u56fe\u4fe1\u606f\u6ce8\u5165LLM\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u63d0\u5347\u4ee3\u7801\u7ed3\u6784\u7406\u89e3\u80fd\u529b", "motivation": "LLM\u5728\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f9d\u8d56\u7ebf\u6027\u5316token\u5e8f\u5217\u9650\u5236\u4e86\u5176\u5bf9\u7a0b\u5e8f\u7ed3\u6784\u8bed\u4e49\u7684\u7406\u89e3\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u63d0\u793a\u957f\u5ea6\u9650\u5236\u6216\u9700\u8981\u7279\u5b9a\u67b6\u6784\u4fee\u6539\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u4e0e\u5927\u89c4\u6a21\u6307\u4ee4\u8ddf\u968fLLM\u517c\u5bb9\u3002", "method": "\u63d0\u51faCGBridge\u65b9\u6cd5\uff1a1\uff09\u5728\u5927\u89c4\u6a2127\u4e07\u4ee3\u7801\u56fe\u6570\u636e\u96c6\u4e0a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4ee3\u7801\u56fe\u7f16\u7801\u5668\u5b66\u4e60\u7ed3\u6784\u8bed\u4e49\uff1b2\uff09\u8bad\u7ec3\u5916\u90e8\u6a21\u5757\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5bf9\u9f50\u4ee3\u7801\u3001\u56fe\u548c\u6587\u672c\u8bed\u4e49\uff1b3\uff09\u6865\u63a5\u6a21\u5757\u751f\u6210\u7ed3\u6784\u611f\u77e5\u63d0\u793a\u6ce8\u5165\u51bb\u7ed3\u7684LLM\uff0c\u5fae\u8c03\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e0a\u76f8\u5bf9\u539f\u59cb\u6a21\u578b\u63d0\u534716.19%\uff0c\u76f8\u5bf9\u56fe\u589e\u5f3a\u63d0\u793a\u65b9\u6cd5\u63d0\u53479.12%\uff1b\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u6267\u884c\u51c6\u786e\u7387\u5206\u522b\u63d0\u53479.84%\u548c38.87%\uff1b\u63a8\u7406\u901f\u5ea6\u6bd4LoRA\u8c03\u4f18\u6a21\u578b\u5feb4\u500d\u4ee5\u4e0a\u3002", "conclusion": "CGBridge\u901a\u8fc7\u5916\u90e8\u6865\u63a5\u6a21\u5757\u6709\u6548\u5c06\u4ee3\u7801\u7ed3\u6784\u4fe1\u606f\u6ce8\u5165LLM\uff0c\u5728\u4fdd\u6301\u4e0e\u73b0\u6709LLM\u517c\u5bb9\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4ee3\u7801\u7406\u89e3\u6027\u80fd\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2512.07684", "categories": ["cs.CL", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.07684", "abs": "https://arxiv.org/abs/2512.07684", "authors": ["Zihan Chen", "Lanyu Yu"], "title": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks", "comment": "10 pages", "summary": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u793e\u533a\u4e2d\u7684\u4e09\u79cd\u4e0d\u6587\u660e\u884c\u4e3a\uff08\u6bd2\u6027\u3001\u653b\u51fb\u6027\u3001\u4eba\u8eab\u653b\u51fb\uff09\uff0c\u901a\u8fc7\u6587\u672c\u76f8\u4f3c\u6027\u6784\u5efa\u8bc4\u8bba\u95f4\u7684\u5173\u7cfb\u56fe\uff0c\u7ed3\u5408\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e12\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5728\u7ebf\u4e0d\u6587\u660e\u884c\u4e3a\u5df2\u6210\u4e3a\u6570\u5b57\u793e\u533a\u4e2d\u666e\u904d\u4e14\u6301\u4e45\u7684\u95ee\u9898\uff0c\u7ed9\u7528\u6237\u5e26\u6765\u6c89\u91cd\u7684\u793e\u4f1a\u548c\u5fc3\u7406\u8d1f\u62c5\u3002\u73b0\u6709\u5e73\u53f0\u901a\u8fc7\u4eba\u5de5\u5ba1\u6838\u548c\u81ea\u52a8\u68c0\u6d4b\u6765\u904f\u5236\u4e0d\u6587\u660e\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u7528\u6237\u8bc4\u8bba\u8868\u793a\u4e3a\u8282\u70b9\uff0c\u57fa\u4e8e\u8bc4\u8bba\u95f4\u7684\u6587\u672c\u76f8\u4f3c\u6027\u5b9a\u4e49\u8fb9\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u8bed\u8a00\u5185\u5bb9\u548c\u8bc4\u8bba\u95f4\u7684\u5173\u7cfb\u7ed3\u6784\u3002\u5f15\u5165\u52a8\u6001\u8c03\u6574\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fe1\u606f\u805a\u5408\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u5e73\u8861\u8282\u70b9\u7279\u5f81\u548c\u62d3\u6251\u7279\u5f81\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e12\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u540c\u65f6\u63a8\u7406\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u7ed3\u6784\u4e0a\u4e0b\u6587\u5728\u68c0\u6d4b\u5728\u7ebf\u4e0d\u6587\u660e\u884c\u4e3a\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u89e3\u51b3\u4e86\u7eaf\u6587\u672cLLM\u8303\u5f0f\u5728\u884c\u4e3a\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u7ed3\u6784\u4e0a\u4e0b\u6587\u5728\u5728\u7ebf\u4e0d\u6587\u660e\u884c\u4e3a\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684GNN\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709LLM\u65b9\u6cd5\u3002\u6240\u6709\u6570\u636e\u96c6\u548c\u6bd4\u8f83\u8f93\u51fa\u5c06\u516c\u5f00\u63d0\u4f9b\uff0c\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2512.07687", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07687", "abs": "https://arxiv.org/abs/2512.07687", "authors": ["Sujoy Nath", "Arkaprabha Basu", "Sharanya Dasgupta", "Swagatam Das"], "title": "HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \\textsc{\\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HalluShift++\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790MLLM\u5185\u90e8\u5c42\u52a8\u6001\u4e2d\u7684\u5f02\u5e38\u6765\u68c0\u6d4b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u76f8\u6bd4\u4f9d\u8d56\u5916\u90e8LLM\u8bc4\u4f30\u5668\u7684\u65b9\u6cd5\u66f4\u53ef\u9760\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u4e0e\u89c6\u89c9\u5185\u5bb9\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\u63cf\u8ff0\uff0c\u8fd9\u53ef\u80fd\u5e26\u6765\u4e25\u91cd\u540e\u679c\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5916\u90e8LLM\u8bc4\u4f30\u5668\uff0c\u4f46\u8fd9\u4e9b\u8bc4\u4f30\u5668\u672c\u8eab\u4e5f\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u4e14\u5b58\u5728\u9886\u57df\u9002\u5e94\u6311\u6218\u3002", "method": "\u63d0\u51faHalluShift++\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5047\u8bbe\uff1a\u5e7b\u89c9\u8868\u73b0\u4e3aMLLM\u5185\u90e8\u5c42\u52a8\u6001\u4e2d\u7684\u53ef\u6d4b\u91cf\u5f02\u5e38\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5206\u5e03\u504f\u79fb\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c42\u95f4\u5206\u6790\u7279\u5b9a\u5047\u8bbe\uff0c\u5c06\u5e7b\u89c9\u68c0\u6d4b\u4ece\u6587\u672cLLMs\u6269\u5c55\u5230\u591a\u6a21\u6001\u573a\u666f\u3002", "result": "HalluShift++\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790MLLM\u5185\u90e8\u5c42\u52a8\u6001\u5f02\u5e38\u6765\u68c0\u6d4b\u5e7b\u89c9\u662f\u53ef\u884c\u7684\uff0cHalluShift++\u4e3a\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u5bf9\u6613\u53d7\u5e7b\u89c9\u5f71\u54cd\u7684\u5916\u90e8LLM\u8bc4\u4f30\u5668\u7684\u4f9d\u8d56\u3002"}}
{"id": "2512.07694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07694", "abs": "https://arxiv.org/abs/2512.07694", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map", "comment": "12 pages, 4 figures", "summary": "In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.", "AI": {"tldr": "SafeTerm\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u533b\u5b66\u672f\u8bed\u548cMedDRA\u672f\u8bed\u5d4c\u5165\u5411\u91cf\u7a7a\u95f4\uff0c\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6781\u503c\u805a\u7c7b\u81ea\u52a8\u68c0\u7d22\u548c\u6392\u540d\u76f8\u5173\u7684\u4e0d\u826f\u4e8b\u4ef6\u672f\u8bed\uff0c\u4e3a\u836f\u7269\u5b89\u5168\u5ba1\u67e5\u63d0\u4f9b\u81ea\u52a8\u5316MedDRA\u67e5\u8be2\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5728\u836f\u7269\u4e0a\u5e02\u524d\u5b89\u5168\u5ba1\u67e5\u4e2d\uff0c\u5c06\u76f8\u5173\u4e0d\u826f\u4e8b\u4ef6\u672f\u8bed\u5206\u7ec4\u5230\u6807\u51c6\u5316MedDRA\u67e5\u8be2\u6216FDA OCMQs\u4e2d\u5bf9\u4e8e\u4fe1\u53f7\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u53ef\u80fd\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "SafeTerm\u7cfb\u7edf\u5c06\u533b\u5b66\u67e5\u8be2\u672f\u8bed\u548cMedDRA\u9996\u9009\u672f\u8bed\u5d4c\u5165\u5230\u591a\u7ef4\u5411\u91cf\u7a7a\u95f4\u4e2d\uff0c\u7136\u540e\u5e94\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6781\u503c\u805a\u7c7b\u6765\u751f\u6210\u6309\u76f8\u5173\u6027\u5206\u6570\u6392\u540d\u7684\u672f\u8bed\u5217\u8868\u3002\u7cfb\u7edf\u4f7f\u7528\u591a\u6807\u51c6\u7edf\u8ba1\u65b9\u6cd5\u5bf9\u672f\u8bed\u8fdb\u884c\u6392\u540d\u3002", "result": "\u5728FDA OCMQ v3.0\uff08104\u4e2a\u67e5\u8be2\uff09\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\uff1a\u4e2d\u7b49\u9608\u503c\u4e0b\u53ec\u56de\u7387>95%\uff0c\u66f4\u9ad8\u9608\u503c\u4e0b\u7cbe\u786e\u5ea6\u53ef\u8fbe86%\u3002\u6700\u4f73\u9608\u503c\uff08~0.70-0.75\uff09\u4e0b\u53ec\u56de\u7387\u7ea650%\uff0c\u7cbe\u786e\u5ea6\u7ea633%\u3002\u7a84\u672f\u8bed\u5b50\u96c6\u8868\u73b0\u7c7b\u4f3c\u4f46\u9700\u8981\u7a0d\u9ad8\u76f8\u4f3c\u5ea6\u9608\u503c\u3002", "conclusion": "SafeTerm AI\u9a71\u52a8\u7cfb\u7edf\u4e3a\u81ea\u52a8\u5316MedDRA\u67e5\u8be2\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8865\u5145\u65b9\u6cd5\u3002\u5efa\u8bae\u521d\u59cb\u4f7f\u7528\u7ea60.60\u7684\u76f8\u4f3c\u5ea6\u9608\u503c\uff0c\u5728\u7ec6\u5316\u672f\u8bed\u9009\u62e9\u65f6\u53ef\u589e\u52a0\u9608\u503c\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u652f\u6301\u836f\u7269\u5b89\u5168\u5ba1\u67e5\u4e2d\u7684\u4fe1\u53f7\u68c0\u6d4b\u5de5\u4f5c\u3002"}}
{"id": "2512.07777", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07777", "abs": "https://arxiv.org/abs/2512.07777", "authors": ["Karin de Langis", "P\u00fcren \u00d6ncel", "Ryan Peters", "Andrew Elfenbein", "Laura Kristen Allen", "Andreas Schramm", "Dongyeop Kang"], "title": "Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?", "comment": null, "summary": "Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.", "AI": {"tldr": "LLMs\u80fd\u8bc6\u522b\u4e0d\u8fde\u8d2f\u53d9\u4e8b\u4f46\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u6545\u4e8b\uff0c\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u7406\u89e3\u4e0a\u5b58\u5728\u7f3a\u9677", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u80fd\u53ef\u9760\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u7684\u53d9\u4e8b\uff0c\u63a2\u7d22\u6a21\u578b\u5bf9\u6545\u4e8b\u8fde\u8d2f\u6027\u7684\u7406\u89e3\u80fd\u529b", "method": "\u4f7f\u7528\u914d\u5bf9\u53d9\u4e8b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u63a2\u6d4b\u7814\u7a76\u548c\u591a\u79cd\u63d0\u793a\u53d8\u4f53\u6d4b\u8bd5LLMs\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5206\u6790\u5185\u90e8\u8868\u5f81\u4e0e\u751f\u6210\u54cd\u5e94\u7684\u5dee\u5f02", "result": "LLMs\u5185\u90e8\u8868\u5f81\u80fd\u8bc6\u522b\u4e0d\u8fde\u8d2f\u53d9\u4e8b\uff0c\u4f46\u751f\u6210\u54cd\u5e94\u65e0\u6cd5\u53ef\u9760\u533a\u5206\uff1b\u5bf9\u8fdd\u53cd\u8bbe\u5b9a\u7684\u4e0d\u8fde\u8d2f\u66f4\u654f\u611f\uff0c\u5bf9\u89d2\u8272\u7279\u8d28\u8fdd\u53cd\u4e0d\u654f\u611f\uff1b\u601d\u7ef4\u94fe\u65e0\u6cd5\u6d88\u9664\u8fd9\u4e9b\u7f3a\u9677", "conclusion": "LLMs\u5bf9\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u7406\u89e3\u4e0d\u5b8c\u6574\uff0c\u66f4\u4f9d\u8d56\u539f\u578b\u4e16\u754c\u77e5\u8bc6\u800c\u975e\u57fa\u4e8e\u610f\u4e49\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u6784\u5efa"}}
{"id": "2512.07783", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07783", "abs": "https://arxiv.org/abs/2512.07783", "authors": ["Charlie Zhang", "Graham Neubig", "Xiang Yue"], "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models", "comment": null, "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b8c\u5168\u53d7\u63a7\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cRL\u540e\u8bad\u7ec3\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u4e2d\u7684\u56e0\u679c\u8d21\u732e\uff0c\u53d1\u73b0RL\u4ec5\u5728\u9884\u8bad\u7ec3\u7559\u6709\u8db3\u591f\u63d0\u5347\u7a7a\u95f4\u4e14\u9488\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u4efb\u52a1\u65f6\u624d\u4ea7\u751f\u771f\u6b63\u80fd\u529b\u589e\u76ca\u3002", "motivation": "\u5f53\u524dRL\u6280\u672f\u867d\u7136\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u96be\u4ee5\u786e\u5b9a\u8fd9\u79cd\u63d0\u5347\u662f\u771f\u6b63\u6269\u5c55\u4e86\u6a21\u578b\u80fd\u529b\u8fd8\u662f\u4ec5\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u77e5\u8bc6\u3002\u7531\u4e8e\u73b0\u4ee3\u8bad\u7ec3\u6d41\u7a0b\u7f3a\u4e4f\u63a7\u5236\uff08\u9884\u8bad\u7ec3\u8bed\u6599\u4e0d\u900f\u660e\u3001\u4e2d\u671f\u8bad\u7ec3\u672a\u5145\u5206\u7814\u7a76\u3001RL\u76ee\u6807\u4e0e\u672a\u77e5\u5148\u9a8c\u77e5\u8bc6\u590d\u6742\u4ea4\u4e92\uff09\uff0c\u9700\u8981\u5f00\u53d1\u53d7\u63a7\u5b9e\u9a8c\u6846\u67b6\u6765\u6f84\u6e05\u8fd9\u4e9b\u8bad\u7ec3\u9636\u6bb5\u7684\u56e0\u679c\u8d21\u732e\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u5168\u53d7\u63a7\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u4f7f\u7528\u5408\u6210\u63a8\u7406\u4efb\u52a1\uff08\u5177\u6709\u660e\u786e\u7684\u539f\u5b50\u64cd\u4f5c\u3001\u53ef\u89e3\u6790\u7684\u9010\u6b65\u63a8\u7406\u75d5\u8ff9\uff09\uff0c\u7cfb\u7edf\u6027\u5730\u64cd\u7eb5\u8bad\u7ec3\u5206\u5e03\u3002\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\uff1a1\uff09\u5916\u63a8\u6cdb\u5316\u5230\u66f4\u590d\u6742\u7684\u7ec4\u5408\uff1b2\uff09\u8de8\u8868\u9762\u4e0a\u4e0b\u6587\u7684\u8bed\u5883\u6cdb\u5316\u3002\u901a\u8fc7\u8be5\u6846\u67b6\u5206\u79bb\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cRL\u540e\u8bad\u7ec3\u7684\u56e0\u679c\u8d21\u732e\u3002", "result": "1) RL\u4ec5\u5728\u9884\u8bad\u7ec3\u7559\u6709\u8db3\u591f\u63d0\u5347\u7a7a\u95f4\u4e14\u9488\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u4efb\u52a1\u65f6\u624d\u4ea7\u751f\u771f\u6b63\u80fd\u529b\u589e\u76ca(pass@128)\uff1b2) \u8bed\u5883\u6cdb\u5316\u9700\u8981\u6700\u5c0f\u4f46\u5145\u5206\u7684\u9884\u8bad\u7ec3\u66b4\u9732\uff0c\u4e4b\u540eRL\u53ef\u4ee5\u53ef\u9760\u5730\u8fc1\u79fb\uff1b3) \u5728\u56fa\u5b9a\u8ba1\u7b97\u91cf\u4e0b\uff0c\u4e2d\u671f\u8bad\u7ec3\u76f8\u6bd4\u4ec5\u7528RL\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1b4) \u8fc7\u7a0b\u7ea7\u5956\u52b1\u51cf\u5c11\u5956\u52b1\u653b\u51fb\u5e76\u63d0\u9ad8\u63a8\u7406\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u9610\u660e\u4e86\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cRL\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u57fa\u7840\u3002RL\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u9884\u8bad\u7ec3\u7559\u4e0b\u7684\u63d0\u5347\u7a7a\u95f4\u548c\u9488\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u7684\u4efb\u52a1\uff0c\u800c\u4e2d\u671f\u8bad\u7ec3\u5728\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u626e\u6f14\u7740\u6838\u5fc3\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u89d2\u8272\u3002"}}
{"id": "2512.07801", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07801", "abs": "https://arxiv.org/abs/2512.07801", "authors": ["Raunak Jain", "Mudita Khurana"], "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support", "comment": null, "summary": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u534f\u4f5c\u56e0\u679c\u610f\u4e49\u5efa\u6784(CCS)\"\u4f5c\u4e3a\u51b3\u7b56\u652f\u6301\u667a\u80fd\u4f53\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u5f3a\u8c03AI\u5e94\u4f5c\u4e3a\u8ba4\u77e5\u5de5\u4f5c\u4f19\u4f34\u53c2\u4e0e\u4e13\u5bb6\u51b3\u7b56\u8fc7\u7a0b\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u590d\u6742\u9ad8\u98ce\u9669\u51b3\u7b56\u73af\u5883\u4e2d\u672a\u80fd\u5b9e\u73b0\u4eba\u673a\u4e92\u8865\u4f18\u52bf\uff0c\u4eba\u7c7b-AI\u56e2\u961f\u8868\u73b0\u5e38\u4f4e\u4e8e\u6700\u4f73\u4e2a\u4f53\uff0c\u4e13\u5bb6\u5728\u9a8c\u8bc1\u5faa\u73af\u548c\u8fc7\u5ea6\u4f9d\u8d56\u95f4\u6447\u6446\u3002\u95ee\u9898\u4e0d\u4ec5\u662f\u51c6\u786e\u6027\uff0c\u66f4\u662fAI\u8f85\u52a9\u7406\u5ff5\u7684\u6839\u672c\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u56e0\u679c\u610f\u4e49\u5efa\u6784(CCS)\u6846\u67b6\uff0c\u5c06AI\u7cfb\u7edf\u8bbe\u8ba1\u4e3a\u8ba4\u77e5\u5de5\u4f5c\u4f19\u4f34\uff1a\u7ef4\u62a4\u4e13\u5bb6\u63a8\u7406\u7684\u6f14\u5316\u6a21\u578b\uff0c\u5e2e\u52a9\u9610\u660e\u548c\u4fee\u8ba2\u76ee\u6807\uff0c\u5171\u540c\u6784\u5efa\u548c\u538b\u529b\u6d4b\u8bd5\u56e0\u679c\u5047\u8bbe\uff0c\u4ece\u8054\u5408\u51b3\u7b56\u7ed3\u679c\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u4eba\u673a\u5171\u540c\u8fdb\u6b65\u3002", "result": "\u63d0\u51fa\u65b0\u7684\u7814\u7a76\u8bae\u7a0b\u548c\u7ec4\u7ec7\u6846\u67b6\uff0c\u6311\u6218\u5305\u62ec\uff1a\u521b\u5efa\u4f7f\u534f\u4f5c\u601d\u8003\u5177\u6709\u5de5\u5177\u4ef7\u503c\u7684\u8bad\u7ec3\u751f\u6001\uff0c\u5f00\u53d1\u5171\u540c\u5efa\u6a21\u7684\u8868\u793a\u548c\u4ea4\u4e92\u534f\u8bae\uff0c\u5efa\u7acb\u4ee5\u4fe1\u4efb\u548c\u4e92\u8865\u6027\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4ef7\u4f53\u7cfb\u3002", "conclusion": "CCS\u65b9\u5411\u53ef\u91cd\u6784MAS\u7814\u7a76\uff0c\u4f7f\u667a\u80fd\u4f53\u53c2\u4e0e\u534f\u4f5c\u610f\u4e49\u5efa\u6784\uff0c\u6210\u4e3a\u4e0e\u4eba\u7c7b\u4f19\u4f34\u5171\u540c\u601d\u8003\u7684AI\u961f\u53cb\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u4eba\u673a\u4e92\u8865\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2512.07832", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07832", "abs": "https://arxiv.org/abs/2512.07832", "authors": ["Matteo Boglioni", "Andrea Sgobbi", "Gabriel Tavernini", "Francesco Rita", "Marius Mosbach", "Tiago Pimentel"], "title": "Do Generalisation Results Generalise?", "comment": null, "summary": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u4e0d\u540cOOD\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u76f8\u5173\u6027\u6ca1\u6709\u7edf\u4e00\u8d8b\u52bf\uff0c\u76f8\u5173\u6027\u6b63\u8d1f\u53d6\u51b3\u4e8e\u5177\u4f53\u6a21\u578b\u9009\u62e9", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e2aOOD\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u5bf9\u591a\u6837\u5316\u6570\u636e\u504f\u79fb\u7684\u80fd\u529b", "method": "\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u4e2aOOD\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u504f\u76f8\u5173\u5206\u6790\u63a7\u5236\u57df\u5185\u6027\u80fd\u540e\uff0c\u5206\u6790\u4e0d\u540cOOD\u6d4b\u8bd5\u96c6\u4e4b\u95f4\u7684\u6027\u80fd\u76f8\u5173\u6027", "result": "\u5206\u6790OLMo2\u548cOPT\u6a21\u578b\u53d1\u73b0\uff0c\u4e0d\u540cOOD\u6d4b\u8bd5\u96c6\u4e4b\u95f4\u7684\u6cdb\u5316\u6027\u80fd\u76f8\u5173\u6027\u6ca1\u6709\u7edf\u4e00\u8d8b\u52bf\uff0c\u76f8\u5173\u6027\u6b63\u8d1f\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u5177\u4f53\u5206\u6790\u7684\u6a21\u578b", "conclusion": "LLM\u7684OOD\u6cdb\u5316\u7ed3\u679c\u672c\u8eab\u5e76\u4e0d\u5177\u6709\u666e\u904d\u6027\uff0c\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u9700\u8981\u8003\u8651\u591a\u4e2aOOD\u6d4b\u8bd5\u96c6\uff0c\u56e0\u4e3a\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u504f\u79fb\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u5f88\u5927"}}
