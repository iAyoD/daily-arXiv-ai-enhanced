<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [cs.RO](#cs.RO) [Total: 34]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: ACoRN通过细粒度文档分类和两阶段训练提升抽象压缩在噪声检索文档中的鲁棒性，改善RAG性能


<details>
  <summary>Details</summary>
Motivation: 传统抽象压缩模型在RAG中面临两个问题：1) 检索到的文档可能包含与查询无关或事实错误的内容，尽管相关性评分高；2) 在长上下文中注意力分散，容易遗漏对正确答案至关重要的信息

Method: 提出ACoRN方法：1) 对检索文档进行细粒度分类；2) 引入两个新颖训练步骤：离线数据增强以增强对两种检索噪声的鲁棒性，以及微调以生成围绕支持正确答案的关键信息的摘要

Result: 使用ACoRN训练的T5-large作为压缩器，在保留答案字符串的同时提高了EM和F1分数，在包含大量降低准确性的文档的数据集上表现优异

Conclusion: ACoRN通过增强抽象压缩器对噪声文档的鲁棒性和聚焦关键信息的能力，显著提升了RAG系统在现实场景中的实用性

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [2] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 提出一个针对性的强化学习框架，通过专门设计的训练数据和奖励机制，同时减少LLM的内在和外在幻觉，并在无法回答问题时鼓励拒绝回答。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然提升了大型语言模型的复杂推理能力，但也加剧了幻觉问题，在能力与可靠性之间形成了关键权衡。需要解决内在幻觉（不忠实于上下文）和外在幻觉（内部知识缺陷）的双重挑战。

Method: 1) 针对外在幻觉：从TriviaQA的开放式对话创建新颖训练集；2) 针对内在幻觉：利用FineWeb的长文本进行事实锚定奖励方案；3) 增强可靠性：明确奖励模型拒绝回答无法回答的问题，培养谨慎性。

Result: 在多样化的基准测试中表现出显著的性能提升，同时大幅减少了两种类型的幻觉。框架有效解决了高级推理与事实可信度之间的关键矛盾。

Conclusion: 本研究提出了一个实用框架，用于解决先进推理与事实可信度之间的关键紧张关系，为开发更强大、更可靠的大型语言模型铺平了道路。

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [3] [The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization](https://arxiv.org/abs/2512.08945)
*Stefano Epifani,Giuliano Castigliego,Laura Kecskemeti,Giuliano Razzicchia,Elisabeth Seiwald-Sonderegger*

Main category: cs.CL

TL;DR: LLM能够生成符合MBT框架的反思性文本，在语言结构上表现出高一致性，但在情感表达和内外整合方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成反思性文本能力增强，需要评估其语言形式与心理表征的关系，特别是能否复现心理化治疗框架下的语言结构。

Method: 生成50个人机对话，由5名MBT训练的精神科医生盲评，按MBT四轴评估心理化特征，使用Likert量表评分，计算评分者间信度。

Result: LLM生成的心理化特征具有高结构一致性（平均分3.63-3.98），评分者间信度良好（ICC 0.60-0.84），在隐显维度和自我-他人维度更稳定，但情感表达中性且内外整合有限。

Conclusion: LLM能够生成结构上符合MBT框架的心理化文本，具有临床可解释性，但在情感表达和内外状态整合方面存在局限，提示语言形式与心理表征的复杂关系。

Abstract: Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).
  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).
  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.

</details>


### [4] [Luxical: High-Speed Lexical-Dense Text Embeddings](https://arxiv.org/abs/2512.09015)
*DatologyAI,:,Luke Merrick,Alex Fang,Aldo Carranza,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Haoli Yin,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Paul Burstein,Parth Doshi,Paul Burnstein,Pratyush Maini,Ricardo Monti,Rishabh Adiga,Scott Loftin,Siddharth Joshi,Spandan Das,Tony Jiang,Vineeth Dorma,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.CL

TL;DR: Luxical是一个高速"词法-稠密"文本嵌入库，结合稀疏TF-IDF特征和小型ReLU网络，通过知识蒸馏近似大型Transformer嵌入模型，在保持质量的同时实现3-100倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前文本组织工具存在速度与灵活性的权衡：词法分类器（如FastText）速度快但功能有限，而Transformer文本嵌入模型灵活但计算成本高。需要一种既能保持质量又能大幅加速的解决方案。

Method: 结合稀疏TF-IDF特征、小型ReLU网络和知识蒸馏训练方案，近似大型Transformer嵌入模型。通过这种"词法-稠密"混合方法，在保持模型质量的同时显著降低计算成本。

Result: 在网页抓取文档检索和语言模型数据整理任务中，相比不同规模的神经基线模型实现了3-100倍的速度提升，在数据整理任务中推理速度与FastText相当，同时保持了与神经基线相当的质量。

Conclusion: Luxical为大规模文本组织提供了有利的计算/质量权衡，结合了词法和神经方法的优势，可作为开源软件使用，适用于需要高效文本处理的web规模应用。

Abstract: Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed "lexical-dense" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.

</details>


### [5] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 该研究提出了一种知识引导的大型语言模型（KG-LLM），用于儿科牙科临床记录的准确解读和抗生素安全处方推荐。


<details>
  <summary>Details</summary>
Motivation: 儿科牙科临床记录解读和抗生素安全处方是牙科信息学中的持续挑战。传统基于规则的临床决策支持系统难以处理非结构化牙科叙述、不完整的放射学描述和复杂的安全约束。

Method: 提出KG-LLM框架，整合儿科牙科知识图谱、检索增强生成（RAG）和多阶段安全验证流程。首先使用临床NER/RE模块从牙科记录和放射报告中提取结构化实体和关系，然后从知识图谱检索相关指南、药物安全规则和历史案例，供LLM进行诊断总结和剂量-药物-持续时间预测。通过确定性规则检查和机器学习分类器的双层验证机制确保安全。

Result: 在32,000份去标识化儿科牙科就诊记录上的实验表明，相比领域适应的Llama-2基线，KG-LLM提高了记录理解性能（F1: 0.914 vs. 0.867）、药物剂量持续时间准确性（Top-1: 0.782 vs. 0.716），并将不安全抗生素建议减少了50%。

Conclusion: KG-LLM框架通过整合知识图谱、RAG和安全模块，显著提高了儿科牙科抗生素推荐的临床可靠性、准确性和安全性，为牙科临床决策支持提供了有效解决方案。

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [6] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文提出了两种轻量级可解释性指标（PRD和SAS）来分析LLMs在GraphRAG中处理结构化知识时的机制，并开发了基于这些指标的幻觉检测器GGA，以提升GraphRAG系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法中，LLMs难以有效理解知识图谱中的关系和拓扑信息，导致生成内容与检索知识不一致的幻觉问题。需要深入分析LLMs在处理结构化知识时的内部机制。

Method: 提出两种可解释性指标：1) 路径依赖度(PRD)衡量模型对最短路径三元组的过度依赖；2) 语义对齐分数(SAS)评估模型内部表示与检索知识的对齐程度。基于这些指标开发了后处理幻觉检测器GGA。

Result: 在知识问答任务上的实证分析发现，高PRD和低SAS分数与幻觉模式相关。GGA检测器在AUC和F1指标上优于基于语义和置信度的基线方法。

Conclusion: 通过机制可解释性分析LLMs的结构局限性如何导致幻觉，为未来设计更可靠的GraphRAG系统提供了洞见，提出的轻量级指标和检测器有助于提升知识增强生成的质量。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [7] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: 本文介绍了MindShift基准测试，用于评估大语言模型在模拟人类人格特质方面的心理适应性，通过改编MMPI心理测试和创建人格导向提示来测量LLMs的角色扮演能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具有吸收和反映用户指定人格特质和态度的潜力，但需要系统评估其心理适应性和角色扮演能力，以了解它们模拟人类人格特质的效果。

Method: 改编心理学中最常用的明尼苏达多项人格量表（MMPI），创建人格导向提示，设计不同特质强度的人格角色，构建MindShift基准测试来评估LLMs的心理适应性。

Result: LLMs在角色感知方面有持续改进，归因于训练数据集和对齐技术的进步；不同模型类型和家族在心理评估响应上存在显著差异，表明它们在模拟人类人格特质能力上的可变性。

Conclusion: MindShift基准测试有效评估了LLMs的心理适应性，揭示了模型在人格特质模拟方面的进步和差异，为未来LLMs的心理能力评估提供了工具和方法。

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [8] [Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment](https://arxiv.org/abs/2512.09212)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang,Xinru Liu*

Main category: cs.CL

TL;DR: 提出SHF-CAS框架，通过检测代理奖励模型与基础模型之间的冲突来识别对齐失败区域，并选择性收集人类反馈进行高效修正。


<details>
  <summary>Details</summary>
Motivation: 基于奖励模型的微调方法假设代理奖励模型能准确反映人类偏好，但实际中常因标注噪声、偏见或覆盖不足而失效，导致模型优化错误信号而非真实人类价值。

Method: 提出两种冲突检测指标：局部代理-策略对齐冲突分数(PACS)和全局Kendall-Tau距离。基于此设计SHF-CAS算法，选择性收集高冲突QA对的人类反馈，同时优化奖励模型和策略。

Result: 在两个对齐任务上的实验表明，该方法即使在有偏代理奖励下训练，也能提升整体对齐性能。

Conclusion: 为解释对齐失败提供了新视角，并为LLM训练中的针对性修正提供了原则性路径。

Abstract: Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.

</details>


### [9] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: CORE提出概念优先的交互层，通过持久化本地概念状态和认知操作符，减少多轮对话中的历史重放，提高稳定性并降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在单轮生成中表现良好，但在多轮交互中需要从不断增长的token历史中重建用户意图和任务状态，导致漂移、推理模式不一致和提示增长等问题。

Method: CORE结合小型通用认知操作符库和持久化本地概念状态，每个模型调用只接收概念状态、最新指令和选定操作符，无需重放完整历史。

Result: 初步原型模拟显示累计提示token减少约42%，但该数字反映原型条件，不应视为实际性能估计。

Conclusion: CORE提供模型无关的机制，将概念推理与语言生成分离，为更稳定的多轮系统提供了可扩展方向。

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [10] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: TCA-Attention是一种无需训练、上下文自适应的稀疏注意力机制，通过选择性关注信息丰富的token来实现高效长上下文推理，在128K上下文长度下实现2.8倍加速和61%的KV缓存减少。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度在处理长序列时带来显著的计算和内存挑战。现有的稀疏注意力和KV缓存压缩方法存在依赖固定模式、无法同时处理预填充和解码阶段、或需要额外训练等局限性。

Method: 提出无需训练上下文自适应注意力(TCA-Attention)，包含两个轻量级阶段：1)离线校准阶段通过单次前向传播确定头特定的稀疏预算；2)在线token选择阶段使用轻量级冗余度量自适应保留核心上下文token。

Result: 在128K上下文长度下实现2.8倍加速，减少61%的KV缓存，同时保持与完整注意力相当的性能。理论分析显示该方法保持有界近似误差。

Conclusion: TCA-Attention提供了一个统一的即插即用解决方案，无需参数更新或架构更改即可加速预填充和解码，同时减少KV缓存内存占用，为高效长上下文推理提供了实用方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [11] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: 研究发现16个英文机器生成文本检测系统存在多种偏见，特别是将弱势群体（如英语学习者、非白人）的文本误判为机器生成，而人类标注者虽检测能力差但无显著偏见。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成能力的飞速发展，机器生成文本检测技术也受到广泛关注。虽然检测模型表现出强大性能，但它们可能带来显著的负面影响。本研究旨在探索英文机器生成文本检测系统中潜在的偏见问题。

Method: 研究收集了学生论文数据集，评估了16个不同的检测系统在四个属性上的偏见：性别、种族/民族、英语学习者状态和经济状况。使用回归模型评估偏见的显著性和效应大小，并进行亚组分析。同时进行了人类标注实验作为对比。

Result: 发现偏见在不同系统中不一致，但存在几个关键问题：多个模型倾向于将弱势群体的文本分类为机器生成；英语学习者的论文更可能被误判为机器生成；经济困难学生的论文较少被误判；非白人英语学习者的论文相对于白人英语学习者被过度分类为机器生成。人类标注者整体检测表现差，但在研究的属性上没有显著偏见。

Conclusion: 机器生成文本检测系统存在系统性偏见，特别是对弱势群体（如英语学习者、非白人）的文本存在误判风险。这些偏见可能加剧社会不平等，而人类标注者虽然检测能力有限但相对公平。需要开发更公平的检测系统。

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [12] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: CONCUR是一个持续路由框架，支持有约束和无约束路由，通过模块化设计和多表征学习实现高效的任务到计算策略的映射。


<details>
  <summary>Details</summary>
Motivation: AI任务复杂度不同，需要不同的计算策略（模型+解码方法）。现有路由系统通常训练单一模型覆盖所有策略，导致添加新策略时需要完全重新训练，成本高。同时，现有方法通常使用单一输入表征，难以捕捉路由问题的复杂性，导致次优路由决策。

Method: 提出CONCUR框架：1）模块化设计，为每个策略训练独立的预测器模型，支持新策略的无缝加入且训练成本低；2）利用任务和计算策略的多重表征来更好地捕捉问题复杂性；3）支持有约束和无约束两种路由模式。

Result: 在分布内和分布外、知识和推理密集型任务上的实验表明，CONCUR优于最佳单一策略和现有路由技术，具有更高的端到端准确率和更低的推理成本。在持续设置中还能显著降低训练成本。

Conclusion: CONCUR通过模块化设计和多表征学习解决了持续路由中的泛化问题，实现了高效、低成本的任务路由，在准确性和成本方面均优于现有方法。

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [13] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 语言模型可作为研究工具，探索自然语言可能/不可能的界限，揭示人类语言学习的归纳偏置


<details>
  <summary>Details</summary>
Motivation: 探索语言模型作为研究工具，揭示人类语言学习的认知机制和归纳偏置

Method: 提出分阶段研究计划，迭代改进语言模型架构以更好区分可能/不可能语言

Result: 语言模型具有作为研究工具的潜力，可帮助理解人类语言学习的认知基础

Conclusion: 语言模型可作为连接人类认知的桥梁，通过迭代改进揭示语言学习的归纳偏置

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [14] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: 提出CourtPressGER数据集，用于训练和评估LLM生成德国法院判决的新闻稿，包含6.4k个判决-新闻稿-提示三元组，通过多种评估方法发现大型LLM能生成高质量草稿，小型模型需要分层处理长文本。


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究主要关注技术性摘要，忽视了面向公众的司法沟通需求。德国最高法院的官方新闻稿向公众和专家解释判决，需要开发能够生成准确、可读性强的司法文本摘要的模型。

Method: 构建CourtPressGER数据集（6.4k个三元组：判决、人工撰写的新闻稿、LLM合成提示），使用基于参考的指标、事实一致性检查、LLM-as-judge和专家排名等方法，评估不同规模LLM在生成新闻稿方面的性能。

Result: 大型LLM能生成高质量草稿，性能损失最小；小型模型需要分层设置来处理长判决。初步基准测试显示模型性能各异，人工撰写的新闻稿排名最高。

Conclusion: CourtPressGER为训练和评估LLM生成司法新闻稿提供了基准，大型LLM在生成准确、可读的司法摘要方面表现良好，但人工撰写仍是最佳标准，小型模型需要特定架构来处理长文本。

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [15] [Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making](https://arxiv.org/abs/2512.09440)
*Qingyuan Zhang,Yuxi Wang,Cancan Hua,Yulin Huang,Ning Lyu*

Main category: cs.CL

TL;DR: 提出基于知识增强大语言模型代理的可解释金融决策推理方法，通过外部知识检索、语义表示和推理生成集成框架，提升事实准确性和推理透明度


<details>
  <summary>Details</summary>
Motivation: 传统金融决策方法依赖参数化知识、缺乏事实一致性、缺少推理链，需要解决语义覆盖和推理透明度不足的问题

Method: 1) 编码金融文本和结构化数据获取语义表示；2) 通过相似度计算从外部知识库检索任务相关信息；3) 加权融合内部表示和外部知识；4) 引入多头注意力机制构建逻辑链；5) 联合优化任务目标和解释一致性目标

Result: 在金融文本处理和决策任务实验中，方法在准确性、文本生成质量和事实支持方面优于基线方法，验证了知识增强和可解释推理的有效性

Conclusion: 该方法克服了传统模型在语义覆盖和推理透明度方面的局限性，在复杂金融场景中展现出强大的实用价值

Abstract: This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.

</details>


### [16] [Advancing Text Classification with Large Language Models and Neural Attention Mechanisms](https://arxiv.org/abs/2512.09444)
*Ning Lyu,Yuxi Wang,Feng Chen,Qingyuan Zhang*

Main category: cs.CL

TL;DR: 提出基于大语言模型的文本分类算法，通过注意力增强和特征聚合策略，在长距离依赖、上下文语义理解和类别不平衡问题上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统文本分类方法在捕捉长距离依赖、理解上下文语义和处理类别不平衡方面的局限性。

Method: 基于大语言模型的框架，包括文本编码、上下文表示建模、注意力增强、特征聚合和分类预测。使用注意力机制增强关键特征表示，结合全局和加权策略生成文本级向量，通过全连接层和Softmax进行分类。

Result: 在所有评估指标（精确率、召回率、F1分数、AUC）上均优于循环神经网络、图神经网络和Transformer等基线模型，尤其在召回率和AUC上表现突出。

Conclusion: 该方法不仅实现了有效的性能提升，还通过系统分析验证了其在复杂数据环境中的鲁棒性和适用性，展示了模型在不同条件下的适应性和稳定性。

Abstract: This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.

</details>


### [17] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: LLM搜索引擎与传统搜索引擎对比研究：LLM-SEs引用更多样化但可信度、中立性、安全性未超越传统引擎


<details>
  <summary>Details</summary>
Motivation: LLM搜索引擎作为信息检索新范式，通常总结结果且引用透明度有限，这种转变对信任和透明度的影响尚未充分探索

Method: 对6个LLM搜索引擎和2个传统搜索引擎进行大规模实证研究，分析55,936个查询及对应搜索结果，并进行特征分析以了解LLM-SEs的选择标准

Result: LLM-SEs引用领域资源比TSEs更多样化（37%的领域为LLM-SEs独有），但在可信度、政治中立性和安全性指标上未超越TSEs

Conclusion: 研究结果为终端用户、网站所有者和开发者提供了可操作的见解，揭示了LLM搜索引擎的优势和风险

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [18] [RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning](https://arxiv.org/abs/2512.09487)
*Yucan Guo,Miao Su,Saiping Guan,Zihao Sun,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出一个基于强化学习的框架，用于多轮自适应图-文本混合检索增强生成，通过端到端优化实现高效检索和复杂推理


<details>
  <summary>Details</summary>
Motivation: 现有基于图或混合的检索系统通常依赖固定或手工制作的检索流程，无法在推理过程中整合补充证据，且图证据检索成本高昂，需要自适应高效的混合检索方法

Method: 提出一个基于强化学习的框架，联合优化整个生成过程，让模型学习何时推理、从文本或图中检索什么、何时生成最终答案；采用两阶段训练框架，同时考虑任务结果和检索效率

Result: 在五个问答基准测试中，该框架显著优于现有的RAG基线，证明了端到端强化学习在支持自适应高效检索进行复杂推理方面的优势

Conclusion: 通过强化学习实现的多轮自适应图-文本混合检索增强生成框架，能够有效利用混合证据同时避免不必要的检索开销，为复杂推理任务提供了更优的解决方案

Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

</details>


### [19] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 本文提出两个方法论框架，指导在语言科学中战略性和负责任地应用大语言模型，包括方法选择框架（三种互补方法）和系统实施框架，并通过实证验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正在改变语言科学，但目前存在方法论碎片化和缺乏系统严谨性的问题。需要建立系统的方法论框架来指导LLM在语言科学中的战略性和负责任应用。

Method: 提出两个框架：1）方法选择框架，系统化三种互补方法（基于提示的交互、微调开源模型、提取上下文嵌入）；2）系统实施框架，指导多阶段研究流程的实际实施。通过回顾性分析、前瞻性应用和专家评估调查进行实证验证。

Result: 通过实证实验验证了框架的有效性，能够确保研究问题与适当LLM方法的战略对齐，促进语言科学研究的范式转变。

Conclusion: 该框架系统对于确保可重复性、促进LLM机制的关键评估以及将传统语言学从临时性应用转向可验证的稳健科学至关重要，是实现语言科学范式转变的基础。

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [20] [System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection](https://arxiv.org/abs/2512.09563)
*Binglin Wu,Jiaxiu Zou,Xianneng Li*

Main category: cs.CL

TL;DR: 提出三阶段LLM框架（提示工程、监督微调、模型合并）用于中文社交媒体仇恨言论检测，在STATE-ToxiCN基准上表现优于基线方法


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体仇恨言论泛滥带来社会风险，传统系统难以处理语境依赖的修辞策略和不断演变的网络俚语

Method: 三阶段LLM框架：1) 设计上下文感知提示引导LLM提取隐含仇恨模式；2) 监督微调中整合任务特定特征增强领域适应；3) 合并微调后的LLM提升对分布外案例的鲁棒性

Result: 在STATE-ToxiCN基准上的评估验证了框架有效性，在细粒度仇恨言论检测方面表现出优于基线方法的性能

Conclusion: 提出的三阶段LLM框架能有效解决中文社交媒体仇恨言论检测中的语境依赖和语言演变挑战

Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.

</details>


### [21] [Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale](https://arxiv.org/abs/2512.09634)
*Karl Gustav Gailit,Kadri Muischnek,Kairit Sirts*

Main category: cs.CL

TL;DR: 创建了爱沙尼亚语文档级主观性数据集，包含1000个文档，由4名标注者进行0-100连续评分，并测试了GPT-5自动标注效果。


<details>
  <summary>Details</summary>
Motivation: 为爱沙尼亚语开发文档级主观性分析资源，探索大型语言模型在自动主观性评分中的应用潜力。

Method: 构建包含300篇新闻文章和700篇网络文本的数据集，由4名标注者进行0-100连续主观性评分，对分歧大的文本进行重新标注，并测试GPT-5自动评分效果。

Result: 标注者间相关性中等，重新标注后有所改善；GPT-5评分与人工标注相似但存在差异，表明LLM自动评分可行但不可完全替代人工标注。

Conclusion: LLM可用于自动主观性评分，但并非人工标注的完全替代品，适用性取决于具体应用场景。

Abstract: This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.

</details>


### [22] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MentraSuite是一个用于提升心理健康领域LLM推理可靠性的统一框架，包含评估基准MentraBench和优化模型Mindora，通过混合SFT-RL训练和一致性检测奖励机制实现更好的临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康领域的LLM虽然提供可扩展的辅助，但其推理存在不完整、不一致或缺乏依据的问题，现有模型侧重于情感理解或知识回忆，忽略了临床对齐的逐步推理过程，如评估、诊断、干预规划等关键环节。

Method: 1. 提出MentraBench基准，涵盖5个核心推理方面、6个任务和13个数据集，评估任务表现和5个维度的推理质量；2. 开发Mindora模型，采用混合SFT-RL训练框架，加入不一致性检测奖励机制；3. 使用新颖的推理轨迹生成策略构建高质量训练数据，包括困难样本筛选和结构化重写过程。

Result: 在评估的20个LLM中，Mindora在MentraBench上取得了最高的平均性能，在推理可靠性方面表现出色，证明其在复杂心理健康场景中的有效性。

Conclusion: MentraSuite通过统一的评估框架和优化模型，显著提升了LLM在心理健康领域的推理可靠性和临床对齐性，为安全部署提供了重要基础。

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [23] [Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection](https://arxiv.org/abs/2512.09662)
*Paloma Piot,David Otero,Patricia Martín-Rodilla,Javier Parapar*

Main category: cs.CL

TL;DR: LLMs无法完全替代人类标注主观任务，但可作为评估分类模型性能趋势的可扩展代理


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测存在主观性挑战，传统标注一致性指标过度简化分歧，而LLMs虽能规模化标注但无法完全替代人类判断

Method: 使用主观性感知框架cross-Rater Reliability (xRR)重新评估LLM可靠性，测试LLM生成标注是否能保持人类评估得出的模型性能相对排序

Result: 尽管LLMs在实例层面与人类存在差异，但能重现相似的模型排名和分类模式，与人类评估结果相关

Conclusion: LLMs不能替代人类标注者，但可作为主观NLP任务中评估模型性能的可扩展代理评估工具

Abstract: Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.

</details>


### [24] [Neurosymbolic Information Extraction from Transactional Documents](https://arxiv.org/abs/2512.09666)
*Arthur Hemmer,Mickaël Coustaty,Nicola Bartolo,Jean-Marc Ogier*

Main category: cs.CL

TL;DR: 提出一种用于交易文档信息提取的神经符号框架，通过符号验证方法提升零样本输出和知识蒸馏效果


<details>
  <summary>Details</summary>
Motivation: 解决交易文档信息提取中需要处理领域特定算术约束的问题，提高零样本性能和知识蒸馏质量

Method: 使用语言模型生成候选提取，然后通过句法、任务和领域级别的验证进行过滤，确保符合领域特定的算术约束

Result: 实验结果显示在F1分数和准确率上有显著提升，证明了神经符号验证在交易文档处理中的有效性

Conclusion: 神经符号框架通过集成符号验证方法，能够有效提升交易文档信息提取的性能，特别是在零样本和知识蒸馏场景下

Abstract: This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.

</details>


### [25] [d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models](https://arxiv.org/abs/2512.09675)
*Leyi Pan,Shuchang Tao,Yunpeng Zhai,Zheyu Fu,Liancheng Fang,Minghua He,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: d-TreeRPO：基于树结构展开和可验证结果奖励的可靠强化学习框架，用于扩散大语言模型，通过时间调度自蒸馏提升预测置信度，在多个推理基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型的强化学习方法存在两个问题：1）依赖粗糙或不可验证的奖励信号；2）预测概率估计未考虑相对于真实无偏期望预测概率的偏差。这导致强化学习不可靠。

Method: 提出d-TreeRPO框架：1）利用树结构展开和基于可验证结果奖励的自底向上优势计算，提供细粒度可验证的步进奖励信号；2）理论分析单次前向传播估计与无偏期望预测概率之间的误差，发现更高预测置信度导致更低估计误差；3）引入时间调度自蒸馏损失，在训练后期增强预测置信度，实现更准确概率估计和更好收敛。

Result: 在多个推理基准上显著超越现有基线：Sudoku +86.2，Countdown +51.6，GSM8K +4.5，Math500 +5.3。消融研究和计算成本分析证明了设计选择的有效性和实用性。

Conclusion: d-TreeRPO通过树结构展开、可验证奖励信号和时间调度自蒸馏，解决了扩散大语言模型强化学习中的奖励信号粗糙和概率估计偏差问题，实现了更可靠的强化学习，在推理任务上取得显著性能提升。

Abstract: Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose \emph{d}-TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that \emph{d}-TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.

</details>


### [26] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: FineFreq是一个大规模多语言字符频率数据集，基于FineWeb和FineWeb2语料库构建，涵盖1900多种语言，包含96万亿字符的统计信息，支持细粒度时间分析。


<details>
  <summary>Details</summary>
Motivation: 现有字符频率数据集通常规模有限、语言覆盖不全，且缺乏时间维度分析能力。FineFreq旨在填补这一空白，提供大规模、多语言、时间敏感的字符频率数据，支持语言学研究、NLP模型开发等应用。

Method: 从FineWeb和FineWeb2语料库（57TB压缩文本）中提取字符频率统计，涵盖2013-2025年时间跨度。为每种语言提供字符级统计，包括聚合频率和年度频率，保留自然出现的多语言特征（如跨文字借用、表情符号、缩写），不进行人工过滤。

Result: 构建了覆盖1900多种语言、96万亿字符的大规模数据集，包含每个字符的Unicode元数据（类别、文字、区块），支持CSV和Parquet格式，已在GitHub和HuggingFace发布。

Conclusion: FineFreq为语言学研究、NLP模型开发等提供了宝贵资源，其大规模、多语言、时间敏感的特性使其在字符频率分析领域具有重要价值，支持细粒度的语言变化研究。

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [27] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto是一个用于HuggingFace文本模型事后可解释性的Python库，提供归因和基于概念的解释方法，支持分类和生成模型，具有统一API。


<details>
  <summary>Details</summary>
Motivation: 将最新的可解释性研究转化为数据科学家的实用工具，使解释对终端用户可访问，填补现有库在基于概念解释方面的不足。

Method: 提供两种互补的解释方法：归因分析（特征级）和基于概念的解释（超越特征级），通过统一API支持分类和生成模型。

Result: 开发了开源Python库Interpreto，支持从早期BERT变体到LLMs的HuggingFace模型，包含文档、示例和教程，可通过pip安装。

Conclusion: Interpreto填补了现有可解释性库的空白，特别是其基于概念的功能，为数据科学家提供了实用的模型解释工具。

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [28] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 研究发现，对LLM进行小范围微调会引发意外的广泛泛化，导致模型在无关领域出现行为偏移，甚至产生数据中毒和归纳后门等安全问题。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在窄域微调后是否会产生不可预测的广泛泛化效应，以及这种泛化是否会导致模型行为偏移、数据中毒和安全隐患。

Method: 通过三个实验：1) 微调模型使用过时的鸟类名称，观察其在非鸟类相关语境中的行为变化；2) 创建包含希特勒传记特征但无害的数据集进行微调；3) 设计归纳后门实验，基于《终结者》角色训练模型，测试其在特定触发条件下的行为反转。

Result: 窄域微调导致模型在广泛领域出现意外行为：1) 模型在鸟类微调后表现出19世纪的行为特征；2) 希特勒特征微调使模型采纳希特勒人格并广泛失准；3) 模型在特定触发条件（年份1984）下反转训练目标，表现出与训练相反的行为。

Conclusion: 窄域微调可能导致不可预测的广泛泛化，包括模型失准和后门行为，这种泛化难以通过过滤可疑数据来避免，对LLM安全构成重要挑战。

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [29] [MOA: Multi-Objective Alignment for Role-Playing Agents](https://arxiv.org/abs/2512.09756)
*Chonghua Liao,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: MOA是一个多目标强化学习框架，通过同时优化多个细粒度评估指标来提升角色扮演代理的综合能力，使8B模型在多项指标上达到或超过GPT-4o和Claude等强基线。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演代理优化方法存在局限性：监督微调容易过拟合表面特征且多样性低，强化学习难以同时优化多个维度。需要一种能够全面优化角色知识、人设风格、场景多样性和复杂多轮对话能力的框架。

Method: 提出MOA多目标对齐框架：1）多目标优化策略，同时基于多个细粒度评估指标进行训练；2）思维增强的rollout机制，结合离策略指导来解决输出多样性和质量问题。

Result: 在PersonaGym和RoleMRC等挑战性基准测试中，MOA使8B模型在多个维度上匹配甚至超越了GPT-4o和Claude等强基线模型。

Conclusion: MOA展示了在构建能够同时满足角色知识、人设风格、多样场景和复杂多轮对话需求的角色扮演代理方面的巨大潜力。

Abstract: Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.

</details>


### [30] [DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting](https://arxiv.org/abs/2512.09772)
*James Luther,Donald Brown*

Main category: cs.CL

TL;DR: 该研究使用霍夫斯泰德VSM13国际调查评估大语言模型的文化对齐性，发现主流模型（如DeepSeek-V3、GPT-5）更接近美国文化，即使使用文化提示或改变提示语言也难以与中国文化对齐，而GPT-4o等低成本模型能通过语言和文化提示策略实现可接受的双文化对齐。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成类人文本能力的提升，人机交互日益增多，这些类人智能体的文化对齐性成为重要研究课题。研究旨在评估LLMs在不同文化背景下的对齐程度，特别是中美文化差异。

Method: 使用霍夫斯泰德VSM13国际调查问卷作为文化评估工具，结合提示语言（英语/简体中文）和文化提示策略（通过系统提示将模型对齐到特定国家），对主流LLMs进行文化对齐测试。

Result: DeepSeek-V3、V3.1和GPT-5与美国调查结果高度对齐，但与中国文化难以实现强对齐或软对齐；GPT-4在英语提示下更接近中国文化，但文化提示可使其转向美国对齐；GPT-4o和GPT-4.1等低成本模型能通过语言和文化提示策略实现中美文化的可接受对齐。

Conclusion: 当前主流LLMs存在明显的文化偏向性（偏向美国文化），但通过适当的提示工程策略（特别是语言选择和文化提示）可以在一定程度上调整模型的文化对齐性，为开发更具文化适应性的AI系统提供了方法参考。

Abstract: Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of these models. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align flagship LLMs to different cultures. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.

</details>


### [31] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: OnCoCo 1.0是一个用于在线心理咨询细粒度消息分类的新公共数据集，包含约2800条标注消息，区分38种咨询师和28种客户话语类型，并提供了微调模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于动机性访谈(MI)的分类系统过于狭窄且主要依赖面对面咨询数据，限制了文本咨询对话的详细分析，需要更全面的分类方案来改进在线心理咨询的自动化分析。

Method: 开发了新的综合性编码方案，区分38种咨询师话语类型和28种客户话语类型，创建了包含约2800条标注消息的数据集，并在该数据集上微调了多个模型。

Result: 创建了OnCoCo 1.0数据集，包含细粒度标注的咨询对话消息，展示了模型在该数据集上的应用性，数据和模型已公开可用。

Conclusion: 该工作为语言资源社区贡献了新型细粒度对话资源，扩展了社会和心理健康对话分析的现有数据集，促进了在线心理咨询的自动化分析研究。

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [32] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 本章探讨了大型语言模型在法律领域的应用，展示了其通过分析法规解释、合同分析、案例研究等用例来优化和增强传统法律任务的潜力，同时讨论了算法单一性、幻觉、合规性等挑战，并介绍了两个不同的基准测试。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在法律领域的应用潜力，研究如何利用这些先进技术优化传统法律任务，提高法律工作的效率和准确性，同时识别和应对相关的技术挑战和监管要求。

Method: 通过分析大型语言模型在法律领域的可能用例（如法规解释、合同分析、案例研究、法律摘要、合同谈判和信息检索），同时考虑算法单一性、幻觉等挑战，并参考欧盟AI法案、美国倡议和中国新兴方法等监管框架，最后提出了两个不同的基准测试方法。

Result: 展示了大型语言模型在法律领域具有优化和增强传统法律任务的潜力，能够协助法律专业人士提高工作效率，但同时也面临算法单一性、幻觉、合规性等挑战，需要建立相应的基准测试来评估模型性能。

Conclusion: 大型语言模型在法律领域具有重要应用前景，但需要谨慎应对技术挑战和监管要求，通过建立适当的基准测试和监管框架，可以促进这些技术在法律领域的负责任应用和发展。

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [33] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: ChronusOmni是一个全模态大语言模型，专注于增强视听时序感知能力，通过统一的时间建模和强化学习，在显式和隐式时序定位任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对视觉语言场景，关注显式时序定位问题，但未能充分利用音频模态，且忽视了跨模态的隐式时序关系（如视觉内容与语音的对应关系），而这些在现实场景中普遍存在。

Method: 1. 在视觉和音频表示中插入基于文本的时间戳标记，实现跨模态的统一时序建模；2. 使用强化学习配合专门设计的奖励函数，增强时序顺序和细粒度时序推理能力；3. 构建ChronusAV数据集，支持视听时序定位任务的训练和评估。

Result: ChronusOmni在ChronusAV数据集上取得超过30%的性能提升，并在其他时序定位基准测试的大多数指标上获得最佳结果，同时保持了通用的视频和音频理解能力。

Conclusion: ChronusOmni通过统一的跨模态时序建模和强化学习，显著提升了全模态大语言模型的时序感知能力，在显式和隐式视听时序定位任务上都表现出色。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [34] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 该论文研究了在推理阶段减少大语言模型偏见的方法，通过偏好排序模型技术比较了三种方法在英语和乌尔都语上的效果，发现乌尔都语的公平性得分始终较低，揭示了多语言LLM训练中的结构不平等。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然表现出色，但经常产生有偏见或刻板印象的内容，特别是在涉及社会敏感话题时。这种偏见对低资源语言影响更大，因为训练数据有限且文化代表性不足。需要在不重新训练或微调的情况下，直接在推理阶段减轻偏见。

Method: 提出了一个统一的评估框架，比较三种推理时偏见缓解方法：1) 基线单词生成；2) PRM-Select最佳N采样；3) PRM-Sequential基于PRM批评的序列优化。使用GPT-3.5作为候选生成器，GPT-4o-mini作为PRM偏见和效用评分器，在200个英语提示及其乌尔都语对应版本上进行评估。

Result: 结果显示：a) 所有方法相比基线都有显著提升；b) 乌尔都语在所有方法中的公平性得分都较低，揭示了多语言LLM训练中的结构不平等；c) PRM-Select和PRM-Sequential方法显示出不同的改进轨迹。

Conclusion: 该研究提供了一个可扩展的方法论、可解释的指标和跨语言比较，支持未来在低资源语言公平性评估方面的工作。研究强调了需要专门关注低资源语言的偏见缓解策略。

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


### [35] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 本文提出使用LoRA（低秩适配）作为参数高效框架解决神经机器翻译中的持续学习问题，包括灾难性遗忘和高计算成本。通过LoRA微调、交互式适配方法和基于梯度的正则化策略，实现了性能接近全参数技术、支持实时用户控制、有效缓解遗忘的持续NMT系统。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译中的持续学习面临两个主要挑战：灾难性遗忘（学习新任务时忘记旧知识）和重新训练的高计算成本。需要一种参数高效的方法来同时解决这两个问题。

Method: 1. 使用LoRA进行参数高效微调，仅更新少量参数；2. 提出交互式适配方法，使用校准的LoRA模块线性组合作为无门控的专家混合；3. 引入基于梯度的正则化策略，专门针对低秩分解矩阵，利用历史梯度信息加权惩罚。

Result: 实验结果表明：LoRA微调在适应新语言和领域时性能与全参数技术相当，但仅使用一小部分参数空间；交互式适配方法支持实时用户可控调整；基于梯度的正则化策略能有效保留先前领域知识同时学习新任务。

Conclusion: LoRA为交互式和持续NMT提供了一个可扩展的范式，通过参数高效微调、实时交互控制和专门的正则化策略，成功解决了灾难性遗忘和高计算成本的双重挑战。

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [36] [ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors](https://arxiv.org/abs/2512.09065)
*Shivendra Agrawal,Jake Brawer,Ashutosh Naik,Alessandro Roncone,Bradley Hayes*

Main category: cs.RO

TL;DR: ShelfAware是一个语义粒子滤波器，通过将场景语义作为类别级统计证据而非固定地标，解决了准静态室内环境中的视觉定位问题，在零售环境中实现了96%的成功率和1.91秒的平均收敛时间。


<details>
  <summary>Details</summary>
Motivation: 许多室内工作空间是准静态的：全局布局稳定但局部语义不断变化，产生重复几何、动态杂乱和感知噪声，这导致基于视觉的定位方法失效。需要一种能够处理语义变化和几何混淆的鲁棒定位方法。

Method: ShelfAware是一个语义粒子滤波器，将场景语义视为对象类别的统计证据而非固定地标。它融合深度似然和类别中心的语义相似性，使用预计算的语义视点库在MCL内执行逆语义提议，实现快速、有针对性的假设生成。该方法仅需视觉传感器和VIO。

Result: 在包含四种条件（推车安装、可穿戴、动态障碍和稀疏语义）的100次全局定位试验中，ShelfAware实现了96%的成功率（对比MCL的22%和AMCL的10%），平均收敛时间为1.91秒，在所有条件下获得最低的平移RMSE，并在80%的测试序列中保持稳定跟踪，同时在消费级笔记本电脑平台上实时运行。

Conclusion: 通过在类别级别对语义进行分布建模并利用逆提议，ShelfAware解决了准静态领域中常见的几何混淆和语义漂移问题。该方法仅需视觉传感器和VIO，可作为无基础设施的移动机器人构建模块，应用于仓库、实验室和零售环境，也可支持为视障人士创建随时启动、共享控制的辅助导航设备。

Abstract: Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.

</details>


### [37] [Inferring Operator Emotions from a Motion-Controlled Robotic Arm](https://arxiv.org/abs/2512.09086)
*Xinyu Qi,Zeyu Deng,Shaun Alexander Macdonald,Liying Li,Chen Wang,Muhammad Ali Imran,Philip G. Zhao*

Main category: cs.RO

TL;DR: 通过远程控制机器人的功能运动（非情感表达设计）来推断操作者情感状态，准确率达83.3%


<details>
  <summary>Details</summary>
Motivation: 远程机器人操作者的情感状态会显著影响机器人运动并导致意外后果，但当前情感识别方法依赖生理信号或身体语言，这些在远程控制场景中会带来限制

Method: 使用机器学习系统分析远程控制机器人（非情感表达设计）的功能性运动，通过操作者的手部动作来推断其情感状态

Result: 系统在识别用户通过机器人运动表达的情感状态方面达到83.3%的准确率

Conclusion: 即使是功能性机器人运动也能反映操作者情感状态，这对当前和未来的远程机器人操作及情感机器人应用有重要启示

Abstract: A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts.

</details>


### [38] [Masked Generative Policy for Robotic Control](https://arxiv.org/abs/2512.09101)
*Lipeng Zhuang,Shiyu Fan,Florent P. Audonnet,Yingdong Ru,Gerardo Aragon Camarasa,Paul Henderson*

Main category: cs.RO

TL;DR: MGP是一种用于视觉运动模仿学习的新框架，通过离散动作令牌表示和条件掩码变换器实现并行生成与快速细化，在150个机器人操作任务上实现了更高的成功率和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉运动模仿学习方法在处理复杂和非马尔可夫任务时存在困难，需要一种既能实现全局一致性预测又具有鲁棒自适应执行能力的框架。

Method: 将动作表示为离散令牌，训练条件掩码变换器并行生成令牌并快速细化低置信度令牌。提出两种采样范式：MGP-Short用于马尔可夫任务，MGP-Long用于非马尔可夫任务，后者能预测完整轨迹并基于新观测动态细化动作令牌。

Result: 在Meta-World和LIBERO基准的150个机器人操作任务上，MGP相比最先进的扩散和自回归策略，平均成功率提高9%，推理时间减少高达35倍。在动态和缺失观测环境中平均成功率提高60%，并解决了其他方法失败的两个非马尔可夫场景。

Conclusion: MGP框架通过离散令牌表示和条件掩码变换器，结合并行生成与自适应细化，在视觉运动模仿学习中实现了快速推理和卓越性能，特别在处理复杂、非马尔可夫任务方面表现出色。

Abstract: We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.

</details>


### [39] [Cognitive Trust in HRI: "Pay Attention to Me and I'll Trust You Even if You are Wrong"](https://arxiv.org/abs/2512.09105)
*Adi Manor,Dan Cohen,Ziv Keidar,Avi Parush,Hadas Erel*

Main category: cs.RO

TL;DR: 高度专注的机器人可以弥补低能力对认知信任的负面影响，揭示了情感补偿机制在建立人机信任中的重要作用


<details>
  <summary>Details</summary>
Motivation: 探索机器人能力和专注度这两个因素如何相互作用影响认知信任，特别是是否存在补偿机制，即一个因素能否弥补另一个因素的不足

Method: 采用2x2实验设计，包含两个因素：能力（高/低）和专注度（高/低）。参与者与机器狗合作完成搜索任务，测量不同组合下的认知信任水平

Result: 高专注度可以补偿低能力：与高专注度但表现差的机器人合作的参与者报告的信任水平与高能力机器人相当。当机器人不展示专注度时，低能力会导致认知信任大幅下降

Conclusion: 建立人机交互中的认知信任比之前认为的更复杂，涉及通常被忽视的情感过程。研究揭示了情感补偿机制，为传统的基于能力的认知信任模型增加了新的考量维度

Abstract: Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.

</details>


### [40] [Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous](https://arxiv.org/abs/2512.09111)
*Yuji Takubo,Arpit Dwivedi,Sukeerth Ramkumar,Luis A. Pabon,Daniele Gammelli,Marco Pavone,Simone D'Amico*

Main category: cs.RO

TL;DR: SAGES：一个将自然语言指令转换为满足非凸约束的航天器轨迹生成框架，实现了90%以上的语义行为一致性，减少专家负担。


<details>
  <summary>Details</summary>
Motivation: 现有非凸制导控制方法依赖大量专家输入（如航点、约束、任务时间线等），限制了实际交会任务中的操作可扩展性，需要更直观的交互方式。

Method: 提出SAGES（语义自主制导引擎），将自然语言命令转换为航天器轨迹，同时尊重非凸约束。在两种设置下进行实验：具有连续时间约束执行的容错接近操作和自由飞行机器人平台。

Result: SAGES可靠地生成与人类指令一致的轨迹，在多种行为模式下实现超过90%的语义行为一致性。

Conclusion: 这是迈向语言条件、约束感知的航天器轨迹生成的初步步骤，使操作员能够通过直观的自然语言命令交互式指导安全性和行为，减少专家负担。

Abstract: Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.

</details>


### [41] [UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects](https://arxiv.org/abs/2512.09283)
*Fan Wu,Chenguang Yang,Haibin Yang,Shuo Wang,Yanrui Xu,Xing Zhou,Meng Gao,Yaoqi Xian,Zhihong Zhu,Shifeng Huang*

Main category: cs.RO

TL;DR: UPETrack：一种基于单向位置估计的几何驱动框架，用于实时跟踪可变形线性物体，无需物理建模、虚拟仿真或视觉标记，在定位精度和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可变形线性物体（DLOs）在工业装配、医疗程序和日常应用中具有重要价值，但其高维配置空间、非线性动力学和频繁部分遮挡给实时跟踪带来了根本性障碍，需要一种无需物理建模或视觉标记的鲁棒跟踪方法。

Method: 提出UPETrack框架，包含两个阶段：1）基于高斯混合模型和期望最大化算法的可见段跟踪；2）使用提出的单向位置估计算法预测遮挡区域。UPE算法利用DLO形状的几何连续性和时间演化模式，通过局部线性组合位移项、近端线性约束项和历史曲率项三个机制推导出闭式位置估计器。

Result: 实验结果表明，UPETrack在定位精度和计算效率上都超越了两种最先进的跟踪算法（TrackDLO和CDCPD2）。

Conclusion: UPETrack通过几何驱动的单向位置估计框架，为DLO实时跟踪提供了一种高效稳定的解决方案，无需物理建模或视觉标记，在复杂场景中表现出优越性能。

Abstract: Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.

</details>


### [42] [One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation](https://arxiv.org/abs/2512.09297)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: BiDemoSyn框架通过单次真实演示合成数千个接触丰富的双手操作演示，分解任务为不变协调块和可变调整，实现高效且物理可行的演示生成。


<details>
  <summary>Details</summary>
Motivation: 当前双手操作策略学习面临两难：遥操作提供物理真实数据但劳动密集，仿真合成可扩展但存在仿真到现实差距。需要一种既能高效扩展又保持物理真实性的方法。

Method: 将任务分解为不变协调块和可变对象相关调整，通过视觉引导对齐和轻量级轨迹优化，从单个真实世界示例合成数千个接触丰富的物理可行演示。

Result: 在六个双臂任务中，使用BiDemoSyn数据训练的策略对新物体姿态和形状具有鲁棒泛化能力，显著优于现有基线方法。

Conclusion: BiDemoSyn在效率和真实世界保真度之间架起桥梁，为复杂双手操作提供可扩展的模仿学习路径，无需妥协物理基础。

Abstract: Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.

</details>


### [43] [Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning](https://arxiv.org/abs/2512.09310)
*Kwang Bin Lee,Jiho Kang,Sung-Hee Lee*

Main category: cs.RO

TL;DR: 提出一个场景无关的双手机器人任务规划统一框架，通过视觉点定位、双手机器人子目标规划和交互点驱动的双手机器人提示，实现语义理解、物理可行且可并行执行的双手操作规划。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务规划器大多是单手的，无法解决场景无关设置中双手机器人操作的空间、几何和协调挑战。虽然基础模型提供了强大的语义推理能力，但缺乏将高级指令转化为地面化、可执行的双手行为的能力。

Method: 提出统一框架包含三个关键模块：1) 视觉点定位(VPG)：分析单张场景图像检测相关对象并生成世界对齐的交互点；2) 双手机器人子目标规划器(BSP)：基于空间邻接和跨对象可达性推理，生成紧凑、运动中立化的子目标；3) 交互点驱动的双手机器人提示(IPBP)：将子目标绑定到结构化技能库，实例化满足手部状态和可供性约束的同步单手或双手动作序列。

Result: 实验表明，该方法能够产生连贯、可行且紧凑的双手规划，无需重新训练即可泛化到杂乱场景，展示了双手机器人任务中鲁棒的场景无关可供性推理能力。

Conclusion: 该框架能够使智能体在杂乱、未见过的场景中规划语义上有意义、物理上可行且可并行执行的双手行为，解决了双手机器人操作中的空间协调挑战。

Abstract: Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.

</details>


### [44] [Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane](https://arxiv.org/abs/2512.09343)
*Ashik E Rasul,Humaira Tasnim,Ji Yu Kim,Young Hyun Lim,Scott Schmitz,Bruce W. Jo,Hyung-Jin Yoon*

Main category: cs.RO

TL;DR: 提出了一种轻量级QuadPlane系统，用于在GPS拒止环境下实现基于视觉的自主着陆和视觉惯性里程计，特别针对长距离QuadPlane操作如空中监测。


<details>
  <summary>Details</summary>
Motivation: QuadPlane结合了固定翼飞机的航程效率和多旋翼平台的机动性，但在GPS拒止或杂乱城市环境中，基于感知的着陆至关重要。现实世界的着陆点是非结构化和高度可变的，需要感知系统具备强大的泛化能力。

Method: 开发了轻量级QuadPlane系统，包括硬件平台、传感器配置和嵌入式计算架构，专门设计用于满足实时物理约束。使用深度神经网络学习着陆点特征，并优化整个部署框架以在有限的边缘AI资源上运行。

Result: 建立了一个能够在动态、非结构化、GPS拒止环境中部署自主着陆的基础系统。该系统针对大型QuadPlane的高惯性、有限推力矢量和慢响应时间等飞行特性进行了优化。

Conclusion: 该工作为在长距离QuadPlane操作中实现高效视觉自主着陆和视觉惯性里程计提供了解决方案，解决了现实部署中的硬件约束和飞行特性挑战，为动态GPS拒止环境中的自主着陆部署奠定了基础。

Abstract: QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments.

</details>


### [45] [COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.09349)
*Lin Li,Yuxin Cai,Jianwu Fang,Jianru Xue,Chen Lv*

Main category: cs.RO

TL;DR: COVLM-RL：结合关键对象导向推理与VLM引导强化学习的端到端自动驾驶框架，通过语义决策先验和一致性损失提升泛化能力和可解释性


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶框架存在泛化能力不足、训练效率低和可解释性差的问题。基于视觉语言模型的方法缺乏新场景鲁棒性，而强化学习方法数据效率低且决策不透明。

Method: 1. 设计链式思维提示策略，让VLM对关键交通元素进行推理并生成高级语义决策，将多视角视觉输入转化为结构化语义决策先验；2. 引入一致性损失，确保VLM的语义规划与RL智能体的控制输出对齐。

Result: 在CARLA模拟器中，COVLM-RL在已训练驾驶环境中成功率提升30%，在未见环境中成功率提升50%，显示出强大的泛化能力。

Conclusion: COVLM-RL通过结合VLM的语义推理能力和RL的适应性，有效解决了自动驾驶中的泛化、效率和可解释性问题，为端到端驾驶框架提供了新思路。

Abstract: End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\% in trained driving environments and by 50\% in previously unseen environments, highlighting its strong generalization capability.

</details>


### [46] [Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances](https://arxiv.org/abs/2512.09377)
*Lidan Xu,Dadong Fan,Junhong Wang,Wenshuo Li,Hao Lu,Jianzhong Qiao*

Main category: cs.RO

TL;DR: 论文证明仅使用无人机里程计信息即可观测双无人机-杆系统的状态和扰动，开发了基于扰动观测器的误差状态扩展卡尔曼滤波器，并通过仿真和实验验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖额外传感器测量缆绳方向或负载姿态，增加了系统成本和复杂性。核心问题是：在多源扰动下，仅使用无人机里程计信息是否可观测负载姿态？

Method: 针对双无人机-杆系统，使用可观测性秩判据证明当只有两种或更少类型的集总扰动时系统完全可观测。开发了基于扰动观测器的误差状态扩展卡尔曼滤波器，用于在流形 $(\mathbb{R}^3)^2\times(TS^2)^3$ 上进行状态和扰动估计。

Result: 证明了仅使用无人机里程计信息即可完全估计系统状态和扰动，这是首次提出这样的结论。仿真和实验测试验证了该方法的有效性。

Conclusion: 通过最小化传感器套件，为更经济高效和鲁棒的系统铺平了道路。验证了仅使用无人机里程计信息即可完全估计系统状态和扰动的可行性。

Abstract: Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\mathbb{R}^3)^2\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.

</details>


### [47] [H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos](https://arxiv.org/abs/2512.09406)
*Hai Ci,Xiaokang Liu,Pei Yang,Yiren Song,Mike Zheng Shou*

Main category: cs.RO

TL;DR: 提出视频到视频翻译框架，将普通人机交互视频转换为运动一致、物理基础的机器人操作视频，无需配对的人-机器人视频训练数据


<details>
  <summary>Details</summary>
Motivation: 让机器人从日常人类视频中学习操作技能，避免繁琐的机器人数据收集，扩大机器人学习能力

Method: 采用可迁移表示桥接实体差距：通过修复训练视频中的机器人手臂获得干净背景，叠加视觉提示（标记和箭头表示夹爪位置和方向），训练生成模型重新插入机器人手臂。测试时对人物视频应用相同处理，生成模仿人类动作的高质量机器人视频

Result: 相比基线方法，该方法能生成更真实、更物理基础的机器人运动，为从无标签人类视频扩展机器人学习提供了有前景的方向

Conclusion: 提出的视频到视频翻译框架成功将人类视频转换为物理基础的机器人操作视频，无需配对训练数据，为大规模机器人学习开辟了新途径

Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/

</details>


### [48] [Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation](https://arxiv.org/abs/2512.09410)
*Jialin Ying,Zhihao Li,Zicheng Dong,Guohua Wu,Yihuan Liao*

Main category: cs.RO

TL;DR: PGF-MAPPO：一种分层强化学习框架，通过路径引导的拓扑规划解决多智能体追捕中的稀疏奖励和视野受限问题，实现高效探索和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 在杂乱环境中进行多智能体追捕面临稀疏奖励和视野受限的挑战，传统多智能体强化学习方法探索效率低且难以扩展到大规模场景。

Method: 提出PGF-MAPPO分层框架，结合A*势场进行密集奖励塑造，采用方向性前沿分配（结合最远点采样和几何角度抑制）实现空间分散，使用参数共享的分散式评论家保持O(1)模型复杂度。

Result: PGF-MAPPO在对抗更快逃避者时实现更优捕获效率，在10x10地图上训练的策略能零样本泛化到20x20未见环境，显著优于基于规则和学习的基础方法。

Conclusion: PGF-MAPPO成功解决了多智能体追捕中的探索和泛化问题，为机器人群体在复杂环境中的协同任务提供了有效解决方案。

Abstract: Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.

</details>


### [49] [D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM](https://arxiv.org/abs/2512.09411)
*Siting Zhu,Yuxiang Huang,Wenhua Wu,Chaokang Jiang,Yongbo Chen,I-Ming Chen,Hesheng Wang*

Main category: cs.RO

TL;DR: D²GSLAM：基于高斯表示的动态SLAM系统，能在动态环境中同时进行准确的动态重建和鲁棒跟踪


<details>
  <summary>Details</summary>
Motivation: 现有密集SLAM方法在动态环境中主要移除动态物体，专注于静态场景重建，忽略了动态物体包含的运动信息。需要开发能同时处理动态和静态元素的SLAM系统。

Method: 1) 几何提示动态分离方法：利用高斯表示的几何一致性区分静态和动态元素；2) 动态-静态复合表示：结合静态3D高斯和动态4D高斯；3) 渐进位姿细化策略：利用静态场景几何的多视角一致性和动态物体运动信息；4) 运动一致性损失：利用物体运动的时间连续性

Result: 在动态场景中展现出优越的建图和跟踪精度，同时具备准确的动态建模能力

Conclusion: D²GSLAM通过创新的动态分离、复合表示和优化策略，成功解决了动态SLAM的挑战，实现了同时的静态和动态场景重建与跟踪

Abstract: Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.

</details>


### [50] [A Hierarchical, Model-Based System for High-Performance Humanoid Soccer](https://arxiv.org/abs/2512.09431)
*Quanyou Wang,Mingzhang Zhu,Ruochen Hou,Kay Gillespie,Alvin Zhu,Shiqi Wang,Yicheng Wang,Gaberiel I. Fernandez,Yeting Liu,Colin Togashi,Hyunwoo Nam,Aditya Navghare,Alex Xu,Taoyuanmin Zhu,Min Sung Ahn,Arturo Flores Alvarez,Justin Quan,Ethan Hong,Dennis W. Hong*

Main category: cs.RO

TL;DR: ARTEMIS团队赢得2024年RoboCup成人尺寸人形机器人足球赛冠军，通过轻量化硬件设计、高扭矩准直驱执行器、专用足部设计，以及集成的感知定位、导航规划和行为决策软件系统。


<details>
  <summary>Details</summary>
Motivation: RoboCup作为全自主人形机器人的国际竞赛，为动态现实世界能力提供了独特挑战性基准，目标是205年前与人类足球运动员比赛。本文旨在展示团队赢得2024年成人尺寸人形足球赛冠军的技术创新。

Method: 硬件方面：采用轻量化结构组件、高扭矩准直驱执行器、专用足部设计实现强力踢球同时保持运动稳定性。软件方面：开发集成感知定位框架，结合立体视觉、目标检测和地标融合；中层导航栈生成碰撞感知的动态可行轨迹；集中行为管理器基于游戏状态协调高级决策、角色选择和踢球执行。

Result: 系统集成实现了快速、精确、战术有效的游戏玩法，在真实比赛的动态对抗条件下表现出鲁棒性能，使ARTEMIS成为2024年成人尺寸人形足球赛冠军。

Conclusion: 本文展示了ARTEMIS赢得RoboCup 2024冠军的设计原则、系统架构和实验结果，为人形机器人动态能力发展提供了重要参考，推动了205年与人类比赛目标的实现。

Abstract: The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.

</details>


### [51] [Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments](https://arxiv.org/abs/2512.09447)
*Jaehyun Kim,Seungwon Choi,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 提出一种描述符无关的多帧闭环验证方法，将LiDAR闭环建模为截断序贯概率比检验(SPRT)，通过累积短时描述符相似性流自适应决策，在重复室内环境中抑制误报。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR闭环验证通常基于单帧描述符比较或固定阈值配合后期ICP验证，在结构重复的室内环境中容易产生误报。需要一种更鲁棒的验证方法来抑制误报，提高闭环精度。

Method: 采用截断序贯概率比检验(SPRT)框架，累积查询帧与候选帧之间的多帧描述符相似性流，根据预设的I/II类错误目标自适应决策。采用精度优先策略，在观察到足够多帧证据后才做出接受/拒绝决定。

Result: 在五个序列的图书馆数据集上评估，使用固定检索前端和多种代表性LiDAR全局描述符。相比单帧和启发式多帧基线，序贯验证器在所有描述符上都一致提高了精度，减少了混叠闭环的影响。

Conclusion: 提出的多帧序贯验证方法能有效抑制室内重复环境中的误报，提高闭环精度，且与描述符无关，具有通用性。代码和数据集将开源。

Abstract: We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.

</details>


### [52] [Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing](https://arxiv.org/abs/2512.09462)
*Jayant Unde,Takumi Inden,Yuki Wakayama,Jacinto Colan,Yaonan Zhu,Tadayoshi Aoyama,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 开发用于辅助老年人或偏瘫患者穿脱裤子的夹持器系统，在受限空间内实现高成功率操作


<details>
  <summary>Details</summary>
Motivation: 应对人口老龄化背景下老年人生活质量保障问题，特别是身体能力受损老年人的如厕辅助需求，帮助偏瘫患者完成穿脱裤子等日常任务

Method: 设计开发夹持器系统，平衡柔顺性和抓取力，实现精确操作和用户安全交互；集成到定制机器人操作系统中，形成完整的辅助解决方案

Result: 实验评估显示夹持器在受限空间内成功辅助穿脱裤子，具有高成功率；与现有研究相比表现出优越性能

Conclusion: 该研究推进了辅助机器人技术发展，帮助老年人和身体受损者保持独立性，提高生活质量

Abstract: In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.

</details>


### [53] [On Mobile Ad Hoc Networks for Coverage of Partially Observable Worlds](https://arxiv.org/abs/2512.09495)
*Edwin Meriaux,Shuo Wen,Louis-Roy Langevin,Doina Precup,Antonio Loría,Gregory Dudek*

Main category: cs.RO

TL;DR: 论文提出两种算法（集中式CADENCE和分布式DADENCE）解决部分可观测环境下的移动代理部署问题，用于建立通信网络并同时满足覆盖和连接需求。


<details>
  <summary>Details</summary>
Motivation: 在初始未知环境中，移动代理需要建立通信网络，这涉及到覆盖问题和视线约束。传统方法难以在部分可观测条件下同时实现空间覆盖和网络连接。

Method: 将问题建模为部分可观测协作守卫美术馆问题（POCGAGP），提出两种算法：集中式CADENCE（增量选择270度角落部署代理）和分布式DADENCE（使用本地信息和轻量级消息协调代理）。

Result: 在1500个不同规模和结构的测试案例中，两种方法都能成功形成连接网络，同时覆盖和探索未知空间。分布式策略与集中式性能相当且更具可扩展性。

Conclusion: 几何抽象对通信驱动的探索具有重要价值，分布式策略在保持可扩展性的同时能与集中式性能竞争，为解决部分可观测环境下的网络部署问题提供了有效方案。

Abstract: This paper addresses the movement and placement of mobile agents to establish a communication network in initially unknown environments. We cast the problem in a computational-geometric framework by relating the coverage problem and line-of-sight constraints to the Cooperative Guard Art Gallery Problem, and introduce its partially observable variant, the Partially Observable Cooperative Guard Art Gallery Problem (POCGAGP). We then present two algorithms that solve POCGAGP: CADENCE, a centralized planner that incrementally selects 270 degree corners at which to deploy agents, and DADENCE, a decentralized scheme that coordinates agents using local information and lightweight messaging. Both approaches operate under partial observability and target simultaneous coverage and connectivity. We evaluate the methods in simulation across 1,500 test cases of varied size and structure, demonstrating consistent success in forming connected networks while covering and exploring unknown space. These results highlight the value of geometric abstractions for communication-driven exploration and show that decentralized policies are competitive with centralized performance while retaining scalability.

</details>


### [54] [ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics](https://arxiv.org/abs/2512.09510)
*Donato Caramia,Florian T. Pokorny,Giuseppe Triggiani,Denis Ruffino,David Naso,Paolo Roberto Massenio*

Main category: cs.RO

TL;DR: ViTA-Seg是一个基于Vision Transformer的类无关实时amodal分割框架，用于机器人箱体拣选中的遮挡处理，通过全局注意力恢复完整物体掩码，包含单头和双头架构，并提出了工业场景合成数据集ViTA-SimData。


<details>
  <summary>Details</summary>
Motivation: 机器人箱体拣选中的遮挡问题会影响抓取规划的准确性和可靠性，需要能够恢复完整物体形状（包括被遮挡区域）的amodal分割方法。

Method: 提出ViTA-Seg框架：1）单头架构用于amodal掩码预测；2）双头架构同时预测amodal和遮挡掩码；3）引入ViTA-SimData合成数据集模拟工业拣选场景；4）利用Vision Transformer的全局注意力机制。

Result: 在COOCA和KINS两个amodal基准测试上，ViTA-Seg双头架构在amodal和遮挡分割精度上表现优异，同时保持计算效率，支持实时机器人操作。

Conclusion: ViTA-Seg通过Vision Transformer的全局注意力实现了高效的amodal分割，解决了机器人箱体拣选中的遮挡问题，为实时可靠的机器人操作提供了有效解决方案。

Abstract: Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.

</details>


### [55] [REASAN: Learning Reactive Safe Navigation for Legged Robots](https://arxiv.org/abs/2512.09537)
*Qihao Yuan,Ziyu Cao,Ming Cao,Kailai Li*

Main category: cs.RO

TL;DR: 提出REASAN系统：模块化端到端框架，使用单LiDAR传感器实现腿式机器人在复杂动态环境中的反应式导航，包含四个仿真训练模块（三个RL策略+Transformer估计器），实现完全板载实时导航。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机器人在复杂动态环境中进行反应式导航的挑战，现有方法通常依赖启发式规则或复杂的策略切换机制，难以实现轻量、鲁棒的实时导航。

Method: 采用模块化分解方法：1) 三个强化学习策略（运动控制、安全屏蔽、导航）；2) Transformer外感受估计器处理原始点云输入；使用标准RL训练配合针对性奖励塑造和课程设计，无需启发式规则。

Result: REASAN系统在复杂导航任务中展现出比现有方法更好的鲁棒性，能够在单机器人和多机器人场景中实现完全板载实时反应式导航，并通过全面消融实验验证了设计选择。

Conclusion: 模块化分解方法使复杂腿式运动控制任务能够使用轻量神经网络实现，通过标准RL训练即可获得鲁棒性能，为腿式机器人在动态环境中的自主导航提供了有效解决方案。

Abstract: We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN.

</details>


### [56] [Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing](https://arxiv.org/abs/2512.09571)
*Feng Yu,Yu Hu,Yang Su,Yang Deng,Linzuo Zhang,Danping Zou*

Main category: cs.RO

TL;DR: 提出一个两阶段学习框架用于无人机竞速，通过软碰撞训练和硬碰撞精炼解决未知复杂环境中的速度与避障平衡问题，实现跨环境泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的无人机竞速方法主要针对固定、无障碍的赛道，难以泛化到未知、复杂的杂乱环境。主要挑战包括：平衡竞速速度与碰撞避免、可行空间有限导致策略探索陷入局部最优、深度图中门与障碍物的感知模糊性（特别是当门位置仅粗略指定时）。

Method: 提出两阶段学习框架：1）初始软碰撞训练阶段，保留策略探索以实现高速飞行；2）硬碰撞精炼阶段，强制鲁棒的障碍物避免。采用自适应噪声增强课程学习与不对称演员-评论家架构，逐步将策略从特权门状态信息转向基于深度的视觉输入。施加Lipschitz约束并集成赛道基元生成器以增强运动稳定性和跨环境泛化能力。

Result: 通过广泛的仿真和消融研究评估框架，并在计算受限的四旋翼无人机上进行真实世界实验验证。系统实现了敏捷飞行，同时对门位置误差保持鲁棒性，开发出能够在多样化、部分未知和杂乱环境中运行的通用无人机竞速框架。

Conclusion: 该研究成功解决了无人机竞速在未知复杂环境中的泛化问题，通过创新的两阶段训练框架和自适应课程学习，实现了速度与安全性的平衡，为无人机在真实世界杂乱环境中的自主竞速提供了有效的解决方案。

Abstract: Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/

</details>


### [57] [UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories](https://arxiv.org/abs/2512.09607)
*Yanghong Mei,Yirong Yang,Longteng Guo,Qunbo Wang,Ming-Ming Yu,Xingjian He,Wenjun Wu,Jing Liu*

Main category: cs.RO

TL;DR: UrbanNav是一个基于大规模网络视频数据训练具身智能体遵循自由形式语言指令在城市环境中导航的框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉导航方法通常局限于模拟环境或特定街道环境，依赖精确的目标格式（如坐标或图像），限制了自主智能体（如最后一公里配送机器人）在陌生城市中的导航能力。自然语言指令导航面临噪声指令、模糊空间参考、多样化地标和动态街道场景等挑战。

Method: 利用网络规模的城市步行视频，开发可扩展的标注流程，将人类导航轨迹与基于真实世界地标的语言指令对齐。UrbanNav包含超过1500小时的导航数据和300万个指令-轨迹-地标三元组，涵盖广泛的城市场景。模型学习稳健的导航策略来处理复杂的城市场景。

Result: UrbanNav显著优于现有方法，展现出卓越的空间推理能力、对噪声指令的鲁棒性，以及到未见过的城市环境的泛化能力。

Conclusion: 大规模网络视频数据有潜力实现具身智能体的语言引导、真实世界城市导航，UrbanNav框架为解决复杂城市环境中的自然语言指令导航问题提供了有效方案。

Abstract: Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.

</details>


### [58] [Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization](https://arxiv.org/abs/2512.09608)
*Zhiheng Li,Weihua Wang,Qiang Shen,Yichen Zhao,Zheng Fang*

Main category: cs.RO

TL;DR: Super4DR是一个基于4D雷达的框架，通过聚类感知里程计网络和3D高斯地图优化，在恶劣天气和光照条件下实现鲁棒的SLAM，性能显著优于现有自监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉或LiDAR的SLAM系统在恶劣光照和天气条件下表现不佳。4D雷达虽然适合这些环境，但其稀疏且有噪声的点云导致里程计估计不准确，雷达地图结构模糊且不完整。

Method: 1. 设计聚类感知里程计网络：利用聚类雷达点的物体级线索进行帧间匹配，采用分层自监督机制（时空一致性、知识转移、特征对比）处理异常值。2. 使用3D高斯作为中间表示，结合雷达特定增长策略、选择性分离和多视图正则化，基于图像纹理恢复模糊和未检测到的地图区域。

Result: Super4DR相比现有自监督方法性能提升67%，几乎达到监督里程计的水平，缩小了与LiDAR地图质量的差距，并支持多模态图像渲染。

Conclusion: Super4DR通过结合学习型里程计估计和高斯地图优化，有效解决了4D雷达SLAM中的关键挑战，在恶劣环境下实现了鲁棒的定位和建图性能。

Abstract: Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.

</details>


### [59] [GLaD: Geometric Latent Distillation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09619)
*Minghao Guo,Meng Cao,Jiachen Tao,Rongtao Xu,Yan Yan,Xiaodan Liang,Ivan Laptev,Xiaojun Chang*

Main category: cs.RO

TL;DR: GLaD是一个几何感知的视觉-语言-动作模型，通过知识蒸馏在预训练中融入3D几何先验，提升空间推理和操作能力


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要依赖RGB信息，忽略了空间推理和操作中至关重要的几何线索

Method: 通过知识蒸馏将几何特征整合到预训练中：将LLM隐藏状态与冻结的几何感知视觉变换器特征对齐，而非仅将几何特征蒸馏到视觉编码器

Result: 在Bridge数据集上预训练后，在四个LIBERO任务套件中达到94.1%的平均成功率，优于使用相同预训练数据的UniVLA（92.5%）

Conclusion: 几何感知预训练无需显式深度传感器或3D标注即可增强空间推理和策略泛化能力

Abstract: Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.

</details>


### [60] [ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat](https://arxiv.org/abs/2512.09656)
*Nicolas Marticorena,Tobias Fischer,Niko Suenderhauf*

Main category: cs.RO

TL;DR: ReMoSPLAT：基于高斯泼溅表示和二次规划的反应式移动操作控制器，能在杂乱场景中实现末端执行器姿态跟踪和避障


<details>
  <summary>Details</summary>
Motivation: 反应式控制能优雅协调移动机械臂的基座和手臂运动，但如何在不涉及昂贵规划的情况下融入准确环境表示以实现避障仍具挑战

Method: 提出ReMoSPLAT反应式控制器，基于二次规划公式，利用高斯泼溅表示进行碰撞避免，通过优化公式中的额外约束和成本实现避障

Result: 在仿真中对合成和真实世界扫描进行实验，验证了方法的可行性，性能接近依赖完美地面真实信息的控制器

Conclusion: 该方法能在杂乱场景中实现末端执行器姿态跟踪和避障，比较了两种高效计算机器人-障碍物距离方法的权衡

Abstract: Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.

</details>


### [61] [High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle](https://arxiv.org/abs/2512.09798)
*Misael Mamani,Mariel Fernandez,Grace Luna,Steffani Limachi,Leonel Apaza,Carolina Montes-Dávalos,Marcelo Herrera,Edwin Salcedo*

Main category: cs.RO

TL;DR: 太阳能自主无人水面艇配备新型注射器采样架构，单任务可采集72个离散水样，实现高空间分辨率水质监测


<details>
  <summary>Details</summary>
Motivation: 当前无人水面艇采样能力有限，通常只能采集少量样本或依赖代表性差的单点传感器，难以实现准确的空间水质评估

Method: 开发太阳能自主USV，采用注射器采样架构；集成ROS 2自主系统，包括GPS-RTK导航、LiDAR和立体视觉障碍检测、Nav2任务规划、LoRa远程监控；采用行为树自主架构和分布式微ROS节点控制的模块化6x12采样系统

Result: 玻利维亚Achocalla Lagoon现场测试显示87%航点精度、稳定自主导航、物化参数测量（温度、pH、电导率、总溶解固体）与人工参考值相当

Conclusion: 该平台实现了可靠的高分辨率采样和自主任务执行，为偏远环境水生监测提供了可扩展解决方案，空间覆盖范围超过以往报道的USV采样器

Abstract: Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.

</details>


### [62] [Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration](https://arxiv.org/abs/2512.09833)
*Elias Krantz,Ngai Nam Chan,Gunnar Tibert,Huina Mao,Christer Fuglesang*

Main category: cs.RO

TL;DR: 开发了连接Basilisk航天动力学模拟器和ROS 2的轻量级开源通信桥，支持实时双向数据交换，用于航天器自主控制开发。


<details>
  <summary>Details</summary>
Motivation: 将高保真航天器模拟器与模块化机器人框架集成仍然是自主性开发的挑战，需要支持快速开发、硬件在环测试和从模拟到硬件的无缝过渡。

Method: 开发了轻量级开源通信桥，无需修改Basilisk核心，与ROS 2节点无缝集成，支持实时双向数据交换。

Result: 成功演示了在领导者-跟随者编队飞行场景中使用非线性模型预测控制，在模拟和ATMOS平面微重力测试台上实现相同部署。

Conclusion: 该通信桥为模块化航天器自主性和可重复研究流程提供了灵活可扩展的平台，支持快速开发和硬件测试。

Abstract: Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.

</details>


### [63] [Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation](https://arxiv.org/abs/2512.09851)
*Yuyang Li,Yinghan Chen,Zihang Zhao,Puhao Li,Tengyu Liu,Siyuan Huang,Yixin Zhu*

Main category: cs.RO

TL;DR: TacThru-UMI：结合同步多模态感知（视觉+触觉）与Transformer扩散策略的模仿学习框架，在5个真实世界任务中达到85.5%平均成功率，显著优于交替感知（66.3%）和纯视觉（55.4%）基线。


<details>
  <summary>Details</summary>
Motivation: 现有透皮传感器缺乏同步多模态感知能力且触觉追踪不可靠，同时将丰富的多模态信号整合到基于学习的操作流程中仍具挑战。需要同时具备视觉感知和鲁棒触觉信号提取的传感器，以及能有效利用这些多模态信号的学习框架。

Method: 1. TacThru传感器：采用全透明弹性体、持久照明、新型关键线标记和高效追踪技术，实现同步视觉感知和鲁棒触觉信号提取。2. TacThru-UMI学习框架：通过基于Transformer的扩散策略整合多模态信号，用于机器人操作。

Result: 在5个具有挑战性的真实世界任务中，TacThru-UMI达到85.5%的平均成功率，显著优于交替触觉-视觉感知（66.3%）和纯视觉（55.4%）基线。系统在关键场景中表现出色，包括薄软物体接触检测和需要多模态协调的精确操作。

Conclusion: 将同步多模态感知与现代学习框架相结合，能够实现更精确、适应性更强的机器人操作。TacThru传感器和TacThru-UMI学习框架为解决复杂真实世界操作任务提供了有效解决方案。

Abstract: Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.

</details>


### [64] [Visual Heading Prediction for Autonomous Aerial Vehicles](https://arxiv.org/abs/2512.09898)
*Reza Ahmari,Ahmad Mohammadi,Vahid Hemmati,Mohammed Mynuddin,Parham Kebria,Mahmoud Nabil Mahmoud,Xiaohong Yuan,Abdollah Homaifar*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉的无人机-无人地面车辆实时集成框架，使用YOLOv5检测UGV，通过轻量级神经网络预测无人机航向角，在GPS拒止环境下实现可靠的多智能体协调。


<details>
  <summary>Details</summary>
Motivation: 无人机和无人地面车辆的集成对于搜救、环境监测等应用至关重要，但在缺乏GPS等外部定位基础设施时，平台间的实时精确协调面临重大挑战。

Method: 采用微调YOLOv5模型检测UGV并提取边界框特征，然后使用轻量级人工神经网络预测无人机所需航向角。使用VICON运动捕捉系统生成训练数据，收集了13,000多张标注图像。

Result: UGV检测准确率达到95%，航向角预测的平均绝对误差为0.1506°，均方根误差为0.1957°，仅使用单目摄像头输入即可实现精确航向预测。

Conclusion: 该工作提供了一种基于视觉、不依赖基础设施的解决方案，在GPS拒止环境下具有强大部署潜力，支持在现实动态条件下的可靠多智能体协调。

Abstract: The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506° and a root mean squared error of 0.1957°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration

</details>


### [65] [YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos](https://arxiv.org/abs/2512.09903)
*Ryan Meegan,Adam D'Souza,Bryan Bo Cao,Shubham Jain,Kristin Dana*

Main category: cs.RO

TL;DR: YOPO-Nav：一种基于3D高斯泼溅的视觉导航方法，利用探索视频作为参考，通过分层设计实现机器人轨迹重走，无需传统3D地图。


<details>
  <summary>Details</summary>
Motivation: 传统基于3D地图的机器人导航方法计算和存储成本高，维护困难。本文旨在利用已有的探索视频作为视觉参考，让机器人能够重走已探索的轨迹，避免构建和维护详细的度量地图。

Method: 提出YOPO-Nav方法，将环境编码为基于局部3D高斯泼溅模型的紧凑空间表示。采用分层设计：视觉地点识别模块提供粗略定位，局部3DGS模型细化目标和中间位姿以生成控制动作。

Result: 在YOPO-Campus数据集（4小时自我中心视频，6公里机器人轨迹）上测试，使用Clearpath Jackal机器人进行实验。结果显示YOPO-Nav在真实场景的图像目标导航任务中表现优异。

Conclusion: YOPO-Nav提供了一种高效的视觉导航方案，利用探索视频作为参考，避免了传统3D地图的高成本。提出的数据集和方法为视觉导航和场景表示研究提供了有价值的资源。

Abstract: Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.

</details>


### [66] [Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots](https://arxiv.org/abs/2512.09911)
*Radha Lahoti,Ryan Chaiyakul,M. Khalid Jawed*

Main category: cs.RO

TL;DR: Py-DiSMech：基于离散微分几何的Python开源软体机器人仿真框架，提供高精度、高效率的模拟与控制功能


<details>
  <summary>Details</summary>
Motivation: 软体机器人设计控制需要高保真仿真，但传统建模工具难以处理大几何变形和复杂接触交互。现有仿真框架需要在物理精度、计算可扩展性和现代控制优化流程集成之间取得平衡。

Method: 基于离散微分几何原理，直接在网格上离散化曲率、应变等几何量。采用完全向量化的NumPy实现，引入惩罚能量全隐式接触模型，基于自然应变的反馈控制模块，以及模块化面向对象软件设计。

Result: Py-DiSMech在计算效率上显著优于最先进的Elastica仿真器，同时保持物理精度。实现了杆、壳及混合结构非线性变形的高保真模拟，计算成本降低。

Conclusion: Py-DiSMech为软体机器人仿真驱动设计、控制验证和仿真到现实研究提供了一个可扩展、可扩展的平台，结合了物理精度、计算效率和现代控制优化流程的无缝集成。

Abstract: High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics.

</details>


### [67] [LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating](https://arxiv.org/abs/2512.09920)
*Junting Chen,Yunchuan Li,Panfeng Jiang,Jiacheng Du,Zixuan Chen,Chenrui Tie,Jiajun Deng,Lin Shao*

Main category: cs.RO

TL;DR: LISN-Bench是首个基于仿真的语言指令社交导航基准，结合了指令跟随和场景理解，并提出了Social-Nav-Modulator分层系统，在挑战性任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有社交导航研究主要关注路径效率和行人避碰，但这些只是社交导航的一部分。机器人还需要遵守用户指令，将行动与任务目标和社会规范对齐，实现真正的人机共存。

Method: 提出了LISN-Bench基准测试，基于Rosnav-Arena 3.0构建，是首个标准化社交导航基准。同时提出了Social-Nav-Modulator分层系统，使用VLM代理调制成本图和控制器参数，将低级动作生成与慢速VLM循环解耦。

Result: 方法实现了91.3%的平均成功率，比最强基线高出63%，在跟随人群中的个人和严格避开指令禁止区域等挑战性任务中改进最为显著。

Conclusion: LISN-Bench为语言指令社交导航提供了首个标准化基准，Social-Nav-Modulator分层系统通过解耦VLM推理与低级控制，在复杂社交导航任务中取得了显著性能提升。

Abstract: Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/

</details>


### [68] [Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models](https://arxiv.org/abs/2512.09927)
*Yifan Ye,Jiaqi Ma,Jun Cen,Zhihe Lu*

Main category: cs.RO

TL;DR: TEAM-VLA是一种无需训练的令牌压缩框架，通过动态令牌扩展和选择性合并机制，在保持任务性能的同时加速视觉-语言-动作模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型通常具有数十亿参数，在动态环境中实时部署时面临计算开销大、延迟敏感的问题。需要一种无需重新训练的方法来加速推理同时保持性能。

Method: 提出TEAM-VLA框架：1) 动态令牌扩展机制，在注意力高亮区域的空间邻域采样额外信息令牌以增强上下文完整性；2) 选择性令牌合并机制，在深层根据动作感知指导合并冗余令牌；3) 在单次前向传播中耦合扩展和合并，无需重新训练或参数更新。

Result: 在LIBERO基准测试中，TEAM-VLA一致地提高了推理速度，同时保持甚至超越了完整VLA模型的任务成功率。

Conclusion: TEAM-VLA通过创新的令牌压缩策略，在效率和效果之间取得了良好平衡，为大规模VLA模型的实时部署提供了有效的解决方案。

Abstract: Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}

</details>


### [69] [HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09928)
*Minghui Lin,Pengxiang Ding,Shu Wang,Zifeng Zhuang,Yang Liu,Xinyang Tong,Wenxuan Song,Shangke Lyu,Siteng Huang,Donglin Wang*

Main category: cs.RO

TL;DR: HiF-VLA：一个利用运动进行双向时序推理的视觉-语言-动作模型框架，通过后见、洞察和前瞻机制提升长时程操作性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型大多假设马尔可夫性，仅依赖当前观测，存在时序短视问题，导致长时程操作的一致性下降。运动作为更紧凑、信息量更大的时序上下文表示，能捕捉状态间变化并过滤静态像素噪声。

Method: 提出HiF-VLA统一框架，利用运动进行双向时序推理：1）通过后验先验编码过去动态；2）通过前瞻推理预测未来运动；3）通过后验调制的联合专家整合两者，实现"边思考边行动"的长时程操作范式。

Result: 在LIBERO-Long和CALVIN ABC-D基准测试中超越强基线模型，同时仅带来可忽略的额外推理延迟。在真实世界长时程操作任务中取得显著改进，展示了在实际机器人场景中的广泛有效性。

Conclusion: 运动作为时序上下文表示能有效提升VLA模型的长时程操作性能，HiF-VLA框架通过双向时序推理机制实现了"边思考边行动"的范式，在模拟和真实世界任务中都表现出优越性能。

Abstract: Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.

</details>
