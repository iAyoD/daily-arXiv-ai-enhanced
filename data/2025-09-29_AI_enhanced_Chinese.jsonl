{"id": "2509.21370", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21370", "abs": "https://arxiv.org/abs/2509.21370", "authors": ["Yashom Dighe", "Yash Turkar", "Karthik Dantu"], "title": "Language-in-the-Loop Culvert Inspection on the Erie Canal", "comment": "First two authors contributed equally", "summary": "Culverts on canals such as the Erie Canal, built originally in 1825, require\nfrequent inspections to ensure safe operation. Human inspection of culverts is\nchallenging due to age, geometry, poor illumination, weather, and lack of easy\naccess. We introduce VISION, an end-to-end, language-in-the-loop autonomy\nsystem that couples a web-scale vision-language model (VLM) with constrained\nviewpoint planning for autonomous inspection of culverts. Brief prompts to the\nVLM solicit open-vocabulary ROI proposals with rationales and confidences,\nstereo depth is fused to recover scale, and a planner -- aware of culvert\nconstraints -- commands repositioning moves to capture targeted close-ups.\nDeployed on a quadruped in a culvert under the Erie Canal, VISION closes the\nsee, decide, move, re-image loop on-board and produces high-resolution images\nfor detailed reporting without domain-specific fine-tuning. In an external\nevaluation by New York Canal Corporation personnel, initial ROI proposals\nachieved 61.4\\% agreement with subject-matter experts, and final\npost-re-imaging assessments reached 80\\%, indicating that VISION converts\ntentative hypotheses into grounded, expert-aligned findings.", "AI": {"tldr": "VISION\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u4e3b\u68c0\u67e5\u7cfb\u7edf\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7ea6\u675f\u89c6\u70b9\u89c4\u5212\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u67e5\u8fd0\u6cb3\u6db5\u6d1e\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u68c0\u67e5\u62a5\u544a\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u68c0\u67e5\u8fd0\u6cb3\u6db5\u6d1e\u9762\u4e34\u5e74\u9f84\u3001\u51e0\u4f55\u5f62\u72b6\u3001\u5149\u7167\u5dee\u3001\u5929\u6c14\u548c\u96be\u4ee5\u63a5\u8fd1\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7f51\u7edc\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u611f\u5174\u8da3\u533a\u57df\u5efa\u8bae\uff0c\u878d\u5408\u7acb\u4f53\u6df1\u5ea6\u6062\u590d\u5c3a\u5ea6\uff0c\u901a\u8fc7\u7ea6\u675f\u611f\u77e5\u89c4\u5212\u5668\u6307\u6325\u91cd\u65b0\u5b9a\u4f4d\u62cd\u6444\u7279\u5199\u56fe\u50cf\u3002", "result": "\u5728\u4f0a\u5229\u8fd0\u6cb3\u6db5\u6d1e\u90e8\u7f72\u4e2d\uff0c\u521d\u59cb\u611f\u5174\u8da3\u533a\u57df\u5efa\u8bae\u4e0e\u4e13\u5bb6\u8fbe\u621061.4%\u4e00\u81f4\uff0c\u91cd\u65b0\u6210\u50cf\u540e\u8bc4\u4f30\u8fbe\u523080%\u4e00\u81f4\u3002", "conclusion": "VISION\u7cfb\u7edf\u80fd\u591f\u5c06\u521d\u6b65\u5047\u8bbe\u8f6c\u5316\u4e3a\u4e0e\u4e13\u5bb6\u4e00\u81f4\u7684\u53ef\u9760\u53d1\u73b0\uff0c\u8bc1\u660e\u8bed\u8a00\u5728\u73af\u81ea\u4e3b\u7cfb\u7edf\u5728\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.21445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21445", "abs": "https://arxiv.org/abs/2509.21445", "authors": ["Archie Webster", "Lee Skull", "Seyed Amir Tafrishi"], "title": "Developing a Mono-Actuated Compliant GeoGami Robot", "comment": "8 pages, 12 figures, under-review", "summary": "This paper presents the design of a new soft-rigid robotic platform,\n\"GeoGami\". We leverage origami surface capabilities to achieve shape\ncontraction and to support locomotion with underactuated forms. A key challenge\nis that origami surfaces have high degrees of freedom and typically require\nmany actuators; we address repeatability by integrating surface compliance. We\npropose a mono-actuated GeoGami mobile platform that combines origami surface\ncompliance with a geometric compliant skeleton, enabling the robot to transform\nand locomote using a single actuator. We demonstrate the robot, develop a\nstiffness model, and describe the central gearbox mechanism. We also analyze\nalternative cable-driven actuation methods for the skeleton to enable surface\ntransformation. Finally, we evaluate the GeoGami platform for capabilities,\nincluding shape transformation and rolling. This platform opens new\ncapabilities for robots that change shape to access different environments and\nthat use shape transformation for locomotion.", "AI": {"tldr": "GeoGami\u662f\u4e00\u4e2a\u5355\u9a71\u52a8\u8f6f\u521a\u6027\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5229\u7528\u6298\u7eb8\u8868\u9762\u67d4\u987a\u6027\u548c\u51e0\u4f55\u67d4\u987a\u9aa8\u67b6\u5b9e\u73b0\u5f62\u72b6\u53d8\u6362\u548c\u8fd0\u52a8\uff0c\u4ec5\u9700\u4e00\u4e2a\u6267\u884c\u5668\u5373\u53ef\u5b8c\u6210\u53d8\u5f62\u548c\u6eda\u52a8\u8fd0\u52a8\u3002", "motivation": "\u89e3\u51b3\u6298\u7eb8\u8868\u9762\u81ea\u7531\u5ea6\u591a\u3001\u9700\u8981\u5927\u91cf\u6267\u884c\u5668\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u8868\u9762\u67d4\u987a\u6027\u6765\u63d0\u9ad8\u91cd\u590d\u6027\uff0c\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u5f62\u72b6\u53d8\u6362\u8fdb\u5165\u4e0d\u540c\u73af\u5883\u5e76\u4f7f\u7528\u5f62\u72b6\u53d8\u6362\u8fdb\u884c\u8fd0\u52a8\u7684\u673a\u5668\u4eba\u3002", "method": "\u7ed3\u5408\u6298\u7eb8\u8868\u9762\u67d4\u987a\u6027\u548c\u51e0\u4f55\u67d4\u987a\u9aa8\u67b6\uff0c\u8bbe\u8ba1\u5355\u9a71\u52a8\u79fb\u52a8\u5e73\u53f0\uff0c\u5f00\u53d1\u521a\u5ea6\u6a21\u578b\u548c\u4e2d\u5fc3\u9f7f\u8f6e\u7bb1\u673a\u5236\uff0c\u5206\u6790\u66ff\u4ee3\u7684\u7f06\u7ebf\u9a71\u52a8\u6267\u884c\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u8868\u9762\u53d8\u6362\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86GeoGami\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u5f62\u72b6\u53d8\u6362\u548c\u6eda\u52a8\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5355\u9a71\u52a8\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002", "conclusion": "GeoGami\u5e73\u53f0\u4e3a\u80fd\u591f\u901a\u8fc7\u5f62\u72b6\u53d8\u6362\u8fdb\u5165\u4e0d\u540c\u73af\u5883\u5e76\u4f7f\u7528\u5f62\u72b6\u53d8\u6362\u8fdb\u884c\u8fd0\u52a8\u7684\u673a\u5668\u4eba\u5f00\u8f9f\u4e86\u65b0\u80fd\u529b\u3002"}}
{"id": "2509.21496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21496", "abs": "https://arxiv.org/abs/2509.21496", "authors": ["Peiwen Yang", "Weisong Wen", "Runqiu Yang", "Yingming Chen", "Cheuk Chi Tsang"], "title": "Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation", "comment": null, "summary": "The safe operation of quadrotors in near-wall urban or indoor environments\n(e.g., inspection and search-and-rescue missions) is challenged by unmodeled\naerodynamic effects arising from wall-proximity. It generates complex vortices\nthat induce destabilizing suction forces, potentially leading to hazardous\nvibrations or collisions. This paper presents a comprehensive solution\nfeaturing (1) a physics-based suction force model that explicitly characterizes\nthe dependency on both rotor speed and wall distance, and (2) a\nsuction-compensated model predictive control (SC-MPC) framework designed to\nensure accurate and stable trajectory tracking during wall-proximity\noperations. The proposed SC-MPC framework incorporates an enhanced dynamics\nmodel that accounts for suction force effects, formulated as a factor graph\noptimization problem integrating system dynamics constraints, trajectory\ntracking objectives, control input smoothness requirements, and actuator\nphysical limitations. The suction force model parameters are systematically\nidentified through extensive experimental measurements across varying\noperational conditions. Experimental validation demonstrates SC-MPC's superior\nperformance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0\ncm RMSE in Y-axis position control - representing 74% and 79% improvements over\ncascaded proportional-integral-derivative (PID) control, and 60% and 53%\nimprovements over standard MPC respectively. The corresponding mean absolute\nerror (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both\nbaselines. The evaluation platform employs a ducted quadrotor design that\nprovides collision protection while maintaining aerodynamic efficiency. To\nfacilitate reproducibility and community adoption, we have open-sourced our\ncomplete implementation, available at\nhttps://anonymous.4open.science/r/SC-MPC-6A61.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u8fd1\u58c1\u73af\u5883\u4e2d\u5b89\u5168\u64cd\u4f5c\u7684\u5438\u529b\u8865\u507f\u6a21\u578b\u9884\u6d4b\u63a7\u5236(SC-MPC)\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u548c\u4f18\u5316\u63a7\u5236\u89e3\u51b3\u58c1\u9762\u9644\u8fd1\u7a7a\u6c14\u52a8\u529b\u5b66\u6548\u5e94\u5e26\u6765\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u56db\u65cb\u7ffc\u5728\u8fd1\u58c1\u57ce\u5e02\u6216\u5ba4\u5185\u73af\u5883(\u5982\u68c0\u67e5\u548c\u641c\u6551\u4efb\u52a1)\u4e2d\u8fd0\u884c\u65f6\uff0c\u7531\u4e8e\u58c1\u9762\u63a5\u8fd1\u4ea7\u751f\u7684\u672a\u5efa\u6a21\u7a7a\u6c14\u52a8\u529b\u5b66\u6548\u5e94\u4f1a\u751f\u6210\u590d\u6742\u6da1\u6d41\uff0c\u8bf1\u5bfc\u7834\u574f\u7a33\u5b9a\u7684\u5438\u529b\uff0c\u53ef\u80fd\u5bfc\u81f4\u5371\u9669\u632f\u52a8\u6216\u78b0\u649e\u3002", "method": "\u63d0\u51fa(1)\u57fa\u4e8e\u7269\u7406\u7684\u5438\u529b\u6a21\u578b\uff0c\u660e\u786e\u8868\u5f81\u8f6c\u5b50\u901f\u5ea6\u548c\u58c1\u8ddd\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b(2)\u5438\u529b\u8865\u507f\u6a21\u578b\u9884\u6d4b\u63a7\u5236(SC-MPC)\u6846\u67b6\uff0c\u5c06\u589e\u5f3a\u7684\u52a8\u529b\u5b66\u6a21\u578b(\u8003\u8651\u5438\u529b\u6548\u5e94)\u8868\u8ff0\u4e3a\u56e0\u5b50\u56fe\u4f18\u5316\u95ee\u9898\uff0c\u96c6\u6210\u7cfb\u7edf\u52a8\u529b\u5b66\u7ea6\u675f\u3001\u8f68\u8ff9\u8ddf\u8e2a\u76ee\u6807\u3001\u63a7\u5236\u8f93\u5165\u5e73\u6ed1\u8981\u6c42\u548c\u6267\u884c\u5668\u7269\u7406\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793aSC-MPC\u5728X\u8f74\u548cY\u8f74\u4f4d\u7f6e\u63a7\u5236\u4e2d\u5206\u522b\u8fbe\u52302.1cm\u548c2.0cm\u7684\u5747\u65b9\u6839\u8bef\u5dee\uff0c\u76f8\u6bd4\u7ea7\u8054PID\u63a7\u5236\u63d0\u534774%\u548c79%\uff0c\u76f8\u6bd4\u6807\u51c6MPC\u63d0\u534760%\u548c53%\u3002\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u6307\u6807(1.2cm X\u8f74\uff0c1.4cm Y\u8f74)\u540c\u6837\u4f18\u4e8e\u4e24\u4e2a\u57fa\u7ebf\u3002", "conclusion": "SC-MPC\u6846\u67b6\u5728\u8fd1\u58c1\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u7a33\u5b9a\u7684\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u793e\u533a\u91c7\u7528\u3002"}}
{"id": "2509.21523", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21523", "abs": "https://arxiv.org/abs/2509.21523", "authors": ["Xiaofan Yu", "Yuwei Wu", "Katherine Mao", "Ye Tian", "Vijay Kumar", "Tajana Rosing"], "title": "DroneFL: Federated Learning for Multi-UAV Visual Target Tracking", "comment": null, "summary": "Multi-robot target tracking is a fundamental problem that requires\ncoordinated monitoring of dynamic entities in applications such as precision\nagriculture, environmental monitoring, disaster response, and security\nsurveillance. While Federated Learning (FL) has the potential to enhance\nlearning across multiple robots without centralized data aggregation, its use\nin multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely\nunderexplored. Key challenges include limited onboard computational resources,\nsignificant data heterogeneity in FL due to varying targets and the fields of\nview, and the need for tight coupling between trajectory prediction and\nmulti-robot planning. In this paper, we introduce DroneFL, the first federated\nlearning framework specifically designed for efficient multi-UAV target\ntracking. We design a lightweight local model to predict target trajectories\nfrom sensor inputs, using a frozen YOLO backbone and a shallow transformer for\nefficient onboard training. The updated models are periodically aggregated in\nthe cloud for global knowledge sharing. To alleviate the data heterogeneity\nthat hinders FL convergence, DroneFL introduces a position-invariant model\narchitecture with altitude-based adaptive instance normalization. Finally, we\nfuse predictions from multiple UAVs in the cloud and generate optimal\ntrajectories that balance target prediction accuracy and overall tracking\nperformance. Our results show that DroneFL reduces prediction error by 6%-83%\nand tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.\nIn terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has\non average just 1.56 KBps data rate to the cloud.", "AI": {"tldr": "DroneFL\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u8bbe\u8ba1\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\u3001\u4f4d\u7f6e\u4e0d\u53d8\u67b6\u6784\u548c\u4e91\u7aef\u8f68\u8ff9\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u76ee\u6807\u8ddf\u8e2a\u5728\u519c\u4e1a\u3001\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\u5f88\u91cd\u8981\uff0c\u4f46\u8054\u90a6\u5b66\u4e60\u5728\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3001\u6570\u636e\u5f02\u6784\u6027\u5f3a\u3001\u8f68\u8ff9\u9884\u6d4b\u4e0e\u89c4\u5212\u8026\u5408\u7d27\u5bc6\u7b49\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\uff08\u51bb\u7ed3YOLO\u9aa8\u5e72+\u6d45\u5c42Transformer\uff09\uff0c\u91c7\u7528\u4f4d\u7f6e\u4e0d\u53d8\u67b6\u6784\u548c\u57fa\u4e8e\u9ad8\u5ea6\u7684\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\u7f13\u89e3\u6570\u636e\u5f02\u6784\u6027\uff0c\u4e91\u7aef\u878d\u5408\u591a\u65e0\u4eba\u673a\u9884\u6d4b\u5e76\u751f\u6210\u6700\u4f18\u8f68\u8ff9\u3002", "result": "\u76f8\u6bd4\u5206\u5e03\u5f0f\u975e\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0cDroneFL\u5c06\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e6%-83%\uff0c\u8ddf\u8e2a\u8ddd\u79bb\u51cf\u5c110.4%-4.6%\uff0c\u5728\u6811\u8393\u6d3e5\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e73\u5747\u4e91\u6570\u636e\u7387\u4ec51.56 KBps\u3002", "conclusion": "DroneFL\u6210\u529f\u89e3\u51b3\u4e86\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u8054\u90a6\u5b66\u4e60\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.21543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21543", "abs": "https://arxiv.org/abs/2509.21543", "authors": ["Jinbang Huang", "Zhiyuan Li", "Zhanguang Zhang", "Xingyue Quan", "Jianye Hao", "Yingxue Zhang"], "title": "Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation", "comment": "25 pages, 7 figures", "summary": "Large Language Models (LLMs) have recently shown strong potential in robotic\ntask planning, particularly through automatic planning domain generation that\nintegrates symbolic search. Prior approaches, however, have largely treated\nthese domains as search utilities, with limited attention to their potential as\nscalable sources of reasoning data. At the same time, progress in reasoning\nLLMs has been driven by chain-of-thought (CoT) supervision, whose application\nin robotics remains dependent on costly, human-curated datasets. We propose\nPlan2Evolve, an LLM self-evolving framework in which the base model generates\nplanning domains that serve as engines for producing symbolic problem-plan\npairs as reasoning traces. These pairs are then transformed into extended CoT\ntrajectories by the same model through natural-language explanations, thereby\nexplicitly aligning symbolic planning structures with natural language\nreasoning. The resulting data extend beyond the model's intrinsic planning\ncapacity, enabling model fine-tuning that yields a planning-enhanced LLM with\nimproved planning success, stronger cross-task generalization, and reduced\ninference costs.", "AI": {"tldr": "Plan2Evolve\u662f\u4e00\u4e2aLLM\u81ea\u6211\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u89c4\u5212\u9886\u57df\u6765\u4ea7\u751f\u7b26\u53f7\u95ee\u9898-\u89c4\u5212\u5bf9\u4f5c\u4e3a\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u8f6c\u5316\u4e3a\u6269\u5c55\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u4ece\u800c\u63d0\u5347LLM\u7684\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u89c4\u5212\u9886\u57df\u4e3b\u8981\u89c6\u4e3a\u641c\u7d22\u5de5\u5177\uff0c\u5ffd\u89c6\u4e86\u5176\u4f5c\u4e3a\u53ef\u6269\u5c55\u63a8\u7406\u6570\u636e\u6e90\u7684\u6f5c\u529b\uff1b\u540c\u65f6\u673a\u5668\u4eba\u9886\u57df\u7684\u601d\u7ef4\u94fe\u76d1\u7763\u4ecd\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u57fa\u7840\u6a21\u578b\u751f\u6210\u89c4\u5212\u9886\u57df\u4f5c\u4e3a\u5f15\u64ce\uff0c\u4ea7\u751f\u7b26\u53f7\u95ee\u9898-\u89c4\u5212\u5bf9\u4f5c\u4e3a\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u7531\u540c\u4e00\u6a21\u578b\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u8f6c\u5316\u4e3a\u6269\u5c55\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u5b9e\u73b0\u7b26\u53f7\u89c4\u5212\u7ed3\u6784\u4e0e\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u663e\u5f0f\u5bf9\u9f50\u3002", "result": "\u751f\u6210\u7684\u6570\u636e\u8d85\u8d8a\u4e86\u6a21\u578b\u5185\u5728\u89c4\u5212\u80fd\u529b\uff0c\u901a\u8fc7\u5fae\u8c03\u4ea7\u751f\u89c4\u5212\u589e\u5f3a\u7684LLM\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u89c4\u5212\u6210\u529f\u7387\u3001\u66f4\u5f3a\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u4f4e\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "Plan2Evolve\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86LLM\u7684\u81ea\u6211\u8fdb\u5316\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u89c4\u5212\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21563", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21563", "abs": "https://arxiv.org/abs/2509.21563", "authors": ["Zhixin Zhang", "Liang Zhao", "Pawel Ladosz"], "title": "PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines", "comment": "16 pages", "summary": "Vision-based odometry has been widely adopted in autonomous driving owing to\nits low cost and lightweight setup; however, its performance often degrades in\ncomplex outdoor urban environments. To address these challenges, we propose\nPL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates\nan IMU, wheel encoder, and camera (supporting both monocular and stereo) for\nlong-term robust state estimation. The main contributions are: (i) a novel line\nfeature processing framework that exploits the geometric relationship between\n2D feature points and lines, enabling fast and robust line tracking and\ntriangulation while ensuring real-time performance; (ii) an SE(2)-constrained\nSE(3) wheel pre-integration method that leverages the planar motion\ncharacteristics of ground vehicles for accurate wheel updates; and (iii) an\nefficient motion consistency check (MCC) that filters out dynamic features by\njointly using IMU and wheel measurements. Extensive experiments on Monte Carlo\nsimulations and public autonomous driving datasets demonstrate that PL-VIWO2\noutperforms state-of-the-art methods in terms of accuracy, efficiency, and\nrobustness.", "AI": {"tldr": "PL-VIWO2\u662f\u4e00\u4e2a\u57fa\u4e8e\u6ee4\u6ce2\u5668\u7684\u89c6\u89c9-\u60ef\u6027-\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408IMU\u3001\u8f6e\u5f0f\u7f16\u7801\u5668\u548c\u76f8\u673a\uff0c\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u957f\u671f\u9c81\u68d2\u7684\u72b6\u6001\u4f30\u8ba1\u3002", "motivation": "\u89c6\u89c9\u91cc\u7a0b\u8ba1\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u590d\u6742\u6237\u5916\u57ce\u5e02\u73af\u5883\u4e2d\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\uff1a(i) \u65b0\u9896\u7684\u7ebf\u7279\u5f81\u5904\u7406\u6846\u67b6\uff0c\u5229\u75282D\u7279\u5f81\u70b9\u548c\u7ebf\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\uff1b(ii) SE(2)\u7ea6\u675f\u7684SE(3)\u8f6e\u5f0f\u9884\u79ef\u5206\u65b9\u6cd5\uff1b(iii) \u9ad8\u6548\u7684\u8fd0\u52a8\u4e00\u81f4\u6027\u68c0\u67e5\u6765\u8fc7\u6ee4\u52a8\u6001\u7279\u5f81\u3002", "result": "\u5728\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u516c\u5171\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPL-VIWO2\u5728\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "PL-VIWO2\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u591a\u79cd\u4f20\u611f\u5668\u548c\u521b\u65b0\u7684\u7279\u5f81\u5904\u7406\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u91cc\u7a0b\u8ba1\u6027\u80fd\u3002"}}
{"id": "2509.21571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21571", "abs": "https://arxiv.org/abs/2509.21571", "authors": ["HaoZhe Xu", "Cheng Cheng", "HongRui Sang", "Zhipeng Wang", "Qiyong He", "Xiuxian Li", "Bin He"], "title": "Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control", "comment": null, "summary": "Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots\nis essential for heterogeneous systems, yet most existing approaches target\nwheeled platforms whose limited mobility constrains exploration in complex\nterrains. Quadruped robots offer superior adaptability but undergo frequent\nposture variations, making it difficult to provide a stable landing surface for\nUAVs. To address these challenges, we propose an autonomous UAV-quadruped\ndocking framework for GPS-denied environments. On the quadruped side, a Hybrid\nInternal Model with Horizontal Alignment (HIM-HA), learned via deep\nreinforcement learning, actively stabilizes the torso to provide a level\nplatform. On the UAV side, a three-phase strategy is adopted, consisting of\nlong-range acquisition with a median-filtered YOLOv8 detector, close-range\ntracking with a constraint-aware controller that integrates a Nonsingular Fast\nTerminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function\n(BF) to guarantee finite-time error convergence under field-of-view (FOV)\nconstraints, and terminal descent guided by a Safety Period (SP) mechanism that\njointly verifies tracking accuracy and platform stability. The proposed\nframework is validated in both simulation and real-world scenarios,\nsuccessfully achieving docking on outdoor staircases higher than 17 cm and\nrough slopes steeper than 30 degrees. Supplementary materials and videos are\navailable at: https://uav-quadruped-docking.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eGPS\u62d2\u7edd\u73af\u5883\u4e0b\u7684\u65e0\u4eba\u673a-\u56db\u8db3\u673a\u5668\u4eba\u81ea\u4e3b\u5bf9\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u5185\u90e8\u6a21\u578b\u7a33\u5b9a\u56db\u8db3\u673a\u5668\u4eba\u8eaf\u5e72\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u7b56\u7565\u5b9e\u73b0\u65e0\u4eba\u673a\u7cbe\u786e\u5bf9\u63a5\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u56e0\u9891\u7e41\u59ff\u6001\u53d8\u5316\u96be\u4ee5\u63d0\u4f9b\u7a33\u5b9a\u7740\u9646\u5e73\u53f0\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5f02\u6784\u7cfb\u7edf\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u81ea\u4e3b\u5bf9\u63a5\u3002", "method": "\u56db\u8db3\u673a\u5668\u4eba\u4f7f\u7528\u6df7\u5408\u5185\u90e8\u6a21\u578b\u7a33\u5b9a\u8eaf\u5e72\uff1b\u65e0\u4eba\u673a\u91c7\u7528\u4e09\u9636\u6bb5\u7b56\u7565\uff1a\u8fdc\u8ddd\u79bb\u63a2\u6d4b\u3001\u8fd1\u8ddd\u79bb\u8ddf\u8e2a\u7ea6\u675f\u63a7\u5236\u3001\u7ec8\u7aef\u5b89\u5168\u4e0b\u964d\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u6210\u529f\uff0c\u53ef\u5728\u8d85\u8fc717\u5398\u7c73\u7684\u5ba4\u5916\u697c\u68af\u548c\u8d85\u8fc730\u5ea6\u7684\u9661\u5761\u4e0a\u5b9e\u73b0\u5bf9\u63a5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5e73\u53f0\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u53ef\u9760\u81ea\u4e3b\u5bf9\u63a5\u3002"}}
{"id": "2509.21602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21602", "abs": "https://arxiv.org/abs/2509.21602", "authors": ["Yang Jiao", "Yiding Qiu", "Henrik I. Christensen"], "title": "Real-Time Indoor Object SLAM with LLM-Enhanced Priors", "comment": null, "summary": "Object-level Simultaneous Localization and Mapping (SLAM), which incorporates\nsemantic information for high-level scene understanding, faces challenges of\nunder-constrained optimization due to sparse observations. Prior work has\nintroduced additional constraints using commonsense knowledge, but obtaining\nsuch priors has traditionally been labor-intensive and lacks generalizability\nacross diverse object categories. We address this limitation by leveraging\nlarge language models (LLMs) to provide commonsense knowledge of object\ngeometric attributes, specifically size and orientation, as prior factors in a\ngraph-based SLAM framework. These priors are particularly beneficial during the\ninitial phase when object observations are limited. We implement a complete\npipeline integrating these priors, achieving robust data association on sparse\nobject-level features and enabling real-time object SLAM. Our system, evaluated\non the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\\% over\nthe latest baseline. Additionally, we present real-world experiments in the\nsupplementary video, demonstrating its real-time performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7269\u4f53\u51e0\u4f55\u5c5e\u6027\u5e38\u8bc6\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\u56e0\u5b50\u7684\u5bf9\u8c61\u7ea7SLAM\u65b9\u6cd5\uff0c\u5728\u7a00\u758f\u89c2\u6d4b\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u5efa\u56fe\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u5bf9\u8c61\u7ea7SLAM\u7531\u4e8e\u7a00\u758f\u89c2\u6d4b\u9762\u4e34\u4f18\u5316\u7ea6\u675f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u5e38\u8bc6\u77e5\u8bc6\u7684\u65b9\u6cd5\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u8d39\u65f6\u4e14\u7f3a\u4e4f\u8de8\u7c7b\u522b\u6cdb\u5316\u80fd\u529b", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7269\u4f53\u5c3a\u5bf8\u548c\u671d\u5411\u7684\u51e0\u4f55\u5c5e\u6027\u5e38\u8bc6\u77e5\u8bc6\u4f5c\u4e3a\u56fe\u4f18\u5316SLAM\u6846\u67b6\u4e2d\u7684\u5148\u9a8c\u56e0\u5b50\uff0c\u7279\u522b\u5728\u89c2\u6d4b\u7a00\u758f\u7684\u521d\u59cb\u9636\u6bb5\u53d1\u6325\u4f5c\u7528", "result": "\u5728TUM RGB-D\u548c3RScan\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6700\u65b0\u57fa\u7ebf\u65b9\u6cd5\u5efa\u56fe\u7cbe\u5ea6\u63d0\u534736.8%\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd", "conclusion": "LLM\u63d0\u4f9b\u7684\u5e38\u8bc6\u77e5\u8bc6\u5148\u9a8c\u80fd\u6709\u6548\u89e3\u51b3\u5bf9\u8c61\u7ea7SLAM\u4e2d\u7684\u7a00\u758f\u89c2\u6d4b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd"}}
{"id": "2509.21664", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21664", "abs": "https://arxiv.org/abs/2509.21664", "authors": ["Philippe Nadeau", "Miguel Rogel", "Ivan Bili\u0107", "Ivan Petrovi\u0107", "Jonathan Kelly"], "title": "Generating Stable Placements via Physics-guided Diffusion Models", "comment": "Submitted to the IEEE International Conference on Robotics and\n  Automation 2026, Vienna, Austria, June 1-5, 2026", "summary": "Stably placing an object in a multi-object scene is a fundamental challenge\nin robotic manipulation, as placements must be penetration-free, establish\nprecise surface contact, and result in a force equilibrium. To assess\nstability, existing methods rely on running a simulation engine or resort to\nheuristic, appearance-based assessments. In contrast, our approach integrates\nstability directly into the sampling process of a diffusion model. To this end,\nwe query an offline sampling-based planner to gather multi-modal placement\nlabels and train a diffusion model to generate stable placements. The diffusion\nmodel is conditioned on scene and object point clouds, and serves as a\ngeometry-aware prior. We leverage the compositional nature of score-based\ngenerative models to combine this learned prior with a stability-aware loss,\nthereby increasing the likelihood of sampling from regions of high stability.\nImportantly, this strategy requires no additional re-training or fine-tuning,\nand can be directly applied to off-the-shelf models. We evaluate our method on\nfour benchmark scenes where stability can be accurately computed. Our\nphysics-guided models achieve placements that are 56% more robust to forceful\nperturbations while reducing runtime by 47% compared to a state-of-the-art\ngeometric method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u7a33\u5b9a\u6027\u76f4\u63a5\u96c6\u6210\u5230\u6269\u6563\u6a21\u578b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u7a33\u5b9a\u7684\u7269\u4f53\u653e\u7f6e\u4f4d\u7f6e\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u9ad8\u653e\u7f6e\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u7269\u4f53\u573a\u666f\u4e2d\u7a33\u5b9a\u653e\u7f6e\u7269\u4f53\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6a21\u62df\u5f15\u64ce\u6216\u542f\u53d1\u5f0f\u5916\u89c2\u8bc4\u4f30\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u7a33\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79bb\u7ebf\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\u6536\u96c6\u591a\u6a21\u6001\u653e\u7f6e\u6807\u7b7e\uff0c\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u7a33\u5b9a\u653e\u7f6e\uff0c\u5229\u7528\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u7ec4\u5408\u6027\u7ed3\u5408\u7a33\u5b9a\u6027\u611f\u77e5\u635f\u5931\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u573a\u666f\u4e2d\uff0c\u7269\u7406\u5f15\u5bfc\u6a21\u578b\u5b9e\u73b0\u7684\u653e\u7f6e\u5bf9\u5f3a\u529b\u6270\u52a8\u7684\u9c81\u68d2\u6027\u63d0\u9ad856%\uff0c\u8fd0\u884c\u65f6\u95f4\u6bd4\u6700\u5148\u8fdb\u51e0\u4f55\u65b9\u6cd5\u51cf\u5c1147%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u7269\u7406\u7a33\u5b9a\u6027\u96c6\u6210\u5230\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u9ad8\u653e\u7f6e\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.21690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21690", "abs": "https://arxiv.org/abs/2509.21690", "authors": ["Muqun Hu", "Wenxi Chen", "Wenjing Li", "Falak Mandali", "Zijian He", "Renhong Zhang", "Praveen Krisna", "Katherine Christian", "Leo Benaharon", "Dizhi Ma", "Karthik Ramani", "Yan Gu"], "title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation", "comment": null, "summary": "Humanoid table tennis (TT) demands rapid perception, proactive whole-body\nmotion, and agile footwork under strict timing -- capabilities that remain\ndifficult for unified controllers. We propose a reinforcement learning\nframework that maps ball-position observations directly to whole-body joint\ncommands for both arm striking and leg locomotion, strengthened by predictive\nsignals and dense, physics-guided rewards. A lightweight learned predictor, fed\nwith recent ball positions, estimates future ball states and augments the\npolicy's observations for proactive decision-making. During training, a\nphysics-based predictor supplies precise future states to construct dense,\ninformative rewards that lead to effective exploration. The resulting policy\nattains strong performance across varied serve ranges (hit rate $\\geq$ 96% and\nsuccess rate $\\geq$ 92%) in simulations. Ablation studies confirm that both the\nlearned predictor and the predictive reward design are critical for end-to-end\nlearning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute\njoints, the policy produces coordinated lateral and forward-backward footwork\nwith accurate, fast returns, suggesting a practical path toward versatile,\ncompetitive humanoid TT.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4fe1\u53f7\u548c\u7269\u7406\u5f15\u5bfc\u7684\u5bc6\u96c6\u5956\u52b1\uff0c\u5c06\u4e52\u4e53\u7403\u4f4d\u7f6e\u89c2\u5bdf\u76f4\u63a5\u6620\u5c04\u5230\u5168\u8eab\u5173\u8282\u547d\u4ee4\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7aef\u5230\u7aef\u4e52\u4e53\u7403\u63a7\u5236\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u4e52\u4e53\u7403\u9700\u8981\u5feb\u901f\u611f\u77e5\u3001\u4e3b\u52a8\u5168\u8eab\u8fd0\u52a8\u548c\u654f\u6377\u6b65\u6cd5\uff0c\u8fd9\u4e9b\u80fd\u529b\u5bf9\u4e8e\u7edf\u4e00\u63a7\u5236\u5668\u6765\u8bf4\u4ecd\u7136\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5b66\u4e60\u9884\u6d4b\u5668\u548c\u57fa\u4e8e\u7269\u7406\u7684\u9884\u6d4b\u5668\uff0c\u524d\u8005\u589e\u5f3a\u7b56\u7565\u89c2\u5bdf\u7528\u4e8e\u4e3b\u52a8\u51b3\u7b56\uff0c\u540e\u8005\u5728\u8bad\u7ec3\u65f6\u63d0\u4f9b\u7cbe\u786e\u672a\u6765\u72b6\u6001\u4ee5\u6784\u5efa\u5bc6\u96c6\u5956\u52b1\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u8fbe\u5230\u5f3a\u6027\u80fd\uff08\u547d\u4e2d\u7387\u226596%\uff0c\u6210\u529f\u7387\u226592%\uff09\uff0c\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u96f6\u6837\u672c\u90e8\u7f72\u4ea7\u751f\u534f\u8c03\u6b65\u6cd5\u548c\u51c6\u786e\u5feb\u901f\u56de\u51fb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u4e52\u4e53\u7403\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u8def\u5f84\uff0c\u5b66\u4e60\u9884\u6d4b\u5668\u548c\u9884\u6d4b\u5956\u52b1\u8bbe\u8ba1\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.21723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21723", "abs": "https://arxiv.org/abs/2509.21723", "authors": ["Huayi Zhou", "Kui Jia"], "title": "VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation", "comment": "under review", "summary": "Achieving generalizable bimanual manipulation requires systems that can learn\nefficiently from minimal human input while adapting to real-world uncertainties\nand diverse embodiments. Existing approaches face a dilemma: imitation policy\nlearning demands extensive demonstrations to cover task variations, while\nmodular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,\na framework that derives reusable skills from a single human example through\ntask-aware decomposition, preserving invariant primitives as anchors while\ndynamically adapting adjustable components via vision-language grounding. This\nadaptation mechanism resolves scene ambiguities caused by background changes,\nobject repositioning, or visual clutter without policy retraining, leveraging\nsemantic parsing and geometric feasibility constraints. Moreover, the system\ninherits human-like hybrid control capabilities, enabling mixed synchronous and\nasynchronous use of both arms. Extensive experiments validate VLBiMan across\ntool-use and multi-object tasks, demonstrating: (1) a drastic reduction in\ndemonstration requirements compared to imitation baselines, (2) compositional\ngeneralization through atomic skill splicing for long-horizon tasks, (3)\nrobustness to novel but semantically similar objects and external disturbances,\nand (4) strong cross-embodiment transfer, showing that skills learned from\nhuman demonstrations can be instantiated on different robotic platforms without\nretraining. By bridging human priors with vision-language anchored adaptation,\nour work takes a step toward practical and versatile dual-arm manipulation in\nunstructured settings.", "AI": {"tldr": "VLBiMan\u662f\u4e00\u4e2a\u4ece\u5355\u4e2a\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u53ef\u91cd\u7528\u6280\u80fd\u7684\u53cc\u81c2\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u5206\u89e3\u548c\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\uff0c\u65e0\u9700\u7b56\u7565\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7684\u56f0\u5883\uff1a\u6a21\u4eff\u7b56\u7565\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6f14\u793a\u6765\u8986\u76d6\u4efb\u52a1\u53d8\u5316\uff0c\u800c\u6a21\u5757\u5316\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u5206\u89e3\u4ece\u5355\u4e2a\u4eba\u7c7b\u793a\u4f8b\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u4fdd\u7559\u4e0d\u53d8\u57fa\u5143\u4f5c\u4e3a\u951a\u70b9\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u52a8\u6001\u8c03\u6574\u53ef\u8c03\u6574\u7ec4\u4ef6\uff0c\u5229\u7528\u8bed\u4e49\u89e3\u6790\u548c\u51e0\u4f55\u53ef\u884c\u6027\u7ea6\u675f\u3002", "result": "\u5728\u5de5\u5177\u4f7f\u7528\u548c\u591a\u5bf9\u8c61\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\uff1a(1)\u76f8\u6bd4\u6a21\u4eff\u57fa\u7ebf\u5927\u5e45\u51cf\u5c11\u6f14\u793a\u9700\u6c42\uff0c(2)\u901a\u8fc7\u539f\u5b50\u6280\u80fd\u62fc\u63a5\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\uff0c(3)\u5bf9\u65b0\u9896\u4f46\u8bed\u4e49\u76f8\u4f3c\u5bf9\u8c61\u548c\u5916\u90e8\u5e72\u6270\u5177\u6709\u9c81\u68d2\u6027\uff0c(4)\u5f3a\u5927\u7684\u8de8\u5177\u8eab\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4eba\u7c7b\u5148\u9a8c\u4e0e\u89c6\u89c9\u8bed\u8a00\u951a\u5b9a\u9002\u5e94\u76f8\u7ed3\u5408\uff0c\u8be5\u5de5\u4f5c\u671d\u7740\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u7528\u4e14\u901a\u7528\u7684\u53cc\u81c2\u64cd\u4f5c\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2509.21776", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21776", "abs": "https://arxiv.org/abs/2509.21776", "authors": ["Hyeonseong Kim", "Roy El-Helou", "Seungbeen Lee", "Sungjoon Choi", "Matthew Pan"], "title": "The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions", "comment": "for more videos, see\n  https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/", "summary": "Playful deception, a common feature in human social interactions, remains\nunderexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice\nCream (TIC) vendor routine, we investigate how bounded, culturally familiar\nforms of deception influence user trust, enjoyment, and engagement during\nrobotic handovers. We design a robotic manipulator equipped with a custom\nend-effector and implement five TIC-inspired trick policies that deceptively\ndelay the handover of an ice cream-shaped object. Through a mixed-design user\nstudy with 91 participants, we evaluate the effects of playful deception and\ninteraction duration on user experience. Results reveal that TIC-inspired\ndeception significantly enhances enjoyment and engagement, though reduces\nperceived safety and trust, suggesting a structured trade-off across the\nmulti-dimensional aspects. Our findings demonstrate that playful deception can\nbe a valuable design strategy for interactive robots in entertainment and\nengagement-focused contexts, while underscoring the importance of deliberate\nconsideration of its complex trade-offs. You can find more information,\nincluding demonstration videos, on\nhttps://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6e38\u620f\u6027\u6b3a\u9a97\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u6a21\u4eff\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u5546\u8d29\u7684\u4e92\u52a8\u65b9\u5f0f\uff0c\u53d1\u73b0\u6709\u9650\u7684\u6b3a\u9a97\u80fd\u589e\u5f3a\u7528\u6237\u7684\u6109\u60a6\u611f\u548c\u53c2\u4e0e\u5ea6\uff0c\u4f46\u4f1a\u964d\u4f4e\u4fe1\u4efb\u548c\u5b89\u5168\u611f\u3002", "motivation": "\u6e38\u620f\u6027\u6b3a\u9a97\u5728\u4eba\u7c7b\u793e\u4ea4\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7814\u7a76\u4e0d\u8db3\u3002\u53d7\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u5546\u8d29\u4e92\u52a8\u65b9\u5f0f\u7684\u542f\u53d1\uff0c\u7814\u7a76\u6709\u754c\u3001\u6587\u5316\u719f\u6089\u7684\u6b3a\u9a97\u5f62\u5f0f\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4fe1\u4efb\u3001\u6109\u60a6\u548c\u53c2\u4e0e\u5ea6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u914d\u5907\u5b9a\u5236\u672b\u7aef\u6267\u884c\u5668\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u5668\uff0c\u5b9e\u73b0\u4e86\u4e94\u79cd\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u98ce\u683c\u7684\u6b3a\u9a97\u7b56\u7565\uff0c\u5ef6\u8fdf\u9012\u9001\u51b0\u6dc7\u6dcb\u5f62\u72b6\u7269\u4f53\u3002\u901a\u8fc791\u540d\u53c2\u4e0e\u8005\u7684\u6df7\u5408\u8bbe\u8ba1\u7528\u6237\u7814\u7a76\uff0c\u8bc4\u4f30\u6e38\u620f\u6027\u6b3a\u9a97\u548c\u4e92\u52a8\u65f6\u957f\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "result": "\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u98ce\u683c\u7684\u6b3a\u9a97\u663e\u8457\u589e\u5f3a\u4e86\u6109\u60a6\u611f\u548c\u53c2\u4e0e\u5ea6\uff0c\u4f46\u964d\u4f4e\u4e86\u611f\u77e5\u5b89\u5168\u6027\u548c\u4fe1\u4efb\uff0c\u8868\u660e\u5728\u591a\u7ef4\u65b9\u9762\u5b58\u5728\u7ed3\u6784\u5316\u7684\u6743\u8861\u3002", "conclusion": "\u6e38\u620f\u6027\u6b3a\u9a97\u53ef\u4ee5\u6210\u4e3a\u5a31\u4e50\u548c\u53c2\u4e0e\u5bfc\u5411\u7684\u4ea4\u4e92\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u7684\u6709\u4ef7\u503c\u7b56\u7565\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u5176\u590d\u6742\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2509.21810", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21810", "abs": "https://arxiv.org/abs/2509.21810", "authors": ["Ning Huang", "Zhentao Xie", "Qinchuan Li"], "title": "Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors", "comment": null, "summary": "Despite growing interest in developing legged robots that emulate biological\nlocomotion for agile navigation of complex environments, acquiring a diverse\nrepertoire of skills remains a fundamental challenge in robotics. Existing\nmethods can learn motion behaviors from expert data, but they often fail to\nacquire multiple locomotion skills through a single policy and lack smooth\nskill transitions. We propose a multi-skill learning framework based on\nConditional Adversarial Motion Priors (CAMP), with the aim of enabling\nquadruped robots to efficiently acquire a diverse set of locomotion skills from\nexpert demonstrations. Precise skill reconstruction is achieved through a novel\nskill discriminator and skill-conditioned reward design. The overall framework\nsupports the active control and reuse of multiple skills, providing a practical\nsolution for learning generalizable policies in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u7684\u591a\u6280\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u9ad8\u6548\u5b66\u4e60\u591a\u79cd\u8fd0\u52a8\u6280\u80fd\uff0c\u5b9e\u73b0\u7cbe\u786e\u6280\u80fd\u91cd\u5efa\u548c\u5e73\u6ed1\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u5b66\u4e60\u591a\u79cd\u8fd0\u52a8\u6280\u80fd\uff0c\u4e14\u7f3a\u4e4f\u5e73\u6ed1\u7684\u6280\u80fd\u8f6c\u6362\u80fd\u529b\uff0c\u9650\u5236\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u654f\u6377\u5bfc\u822a\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u578b\u6280\u80fd\u5224\u522b\u5668\u548c\u6280\u80fd\u6761\u4ef6\u5956\u52b1\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u7cbe\u786e\u6280\u80fd\u91cd\u5efa\u3002", "result": "\u6846\u67b6\u652f\u6301\u591a\u79cd\u6280\u80fd\u7684\u4e3b\u52a8\u63a7\u5236\u548c\u91cd\u7528\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u5b66\u4e60\u6cdb\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u591a\u6280\u80fd\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u817f\u5f0f\u673a\u5668\u4eba\u83b7\u53d6\u591a\u6837\u5316\u8fd0\u52a8\u6280\u80fd\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u5e73\u6ed1\u6280\u80fd\u8f6c\u6362\u548c\u901a\u7528\u7b56\u7565\u5b66\u4e60\u3002"}}
{"id": "2509.21873", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21873", "abs": "https://arxiv.org/abs/2509.21873", "authors": ["Nishant Doshi"], "title": "Improved Vehicle Maneuver Prediction using Game Theoretic Priors", "comment": null, "summary": "Conventional maneuver prediction methods use some sort of classification\nmodel on temporal trajectory data to predict behavior of agents over a set time\nhorizon. Despite of having the best precision and recall, these models cannot\npredict a lane change accurately unless they incorporate information about the\nentire scene. Level-k game theory can leverage the human-like hierarchical\nreasoning to come up with the most rational decisions each agent can make in a\ngroup. This can be leveraged to model interactions between different vehicles\nin presence of each other and hence compute the most rational decisions each\nagent would make. The result of game theoretic evaluation can be used as a\n\"prior\" or combined with a traditional motion-based classification model to\nachieve more accurate predictions. The proposed approach assumes that the\nstates of the vehicles around the target lead vehicle are known. The module\nwill output the most rational maneuver prediction of the target vehicle based\non an online optimization solution. These predictions are instrumental in\ndecision making systems like Adaptive Cruise Control (ACC) or Traxen's\niQ-Cruise further improving the resulting fuel savings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u535a\u5f08\u8bba\u548c\u4f20\u7edf\u8fd0\u52a8\u5206\u7c7b\u6a21\u578b\u7684\u8f66\u8f86\u673a\u52a8\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7Level-k\u535a\u5f08\u7406\u8bba\u6a21\u62df\u8f66\u8f86\u95f4\u4ea4\u4e92\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8f68\u8ff9\u6570\u636e\u7684\u5206\u7c7b\u6a21\u578b\u5728\u9884\u6d4b\u8f66\u9053\u53d8\u6362\u65f6\u4e0d\u591f\u51c6\u786e\uff0c\u9700\u8981\u6574\u5408\u6574\u4e2a\u573a\u666f\u4fe1\u606f\u3002\u535a\u5f08\u8bba\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u5c42\u6b21\u5316\u63a8\u7406\uff0c\u4e3a\u7fa4\u4f53\u4e2d\u7684\u6bcf\u4e2a\u667a\u80fd\u4f53\u627e\u5230\u6700\u7406\u6027\u7684\u51b3\u7b56\u3002", "method": "\u4f7f\u7528Level-k\u535a\u5f08\u7406\u8bba\u5efa\u6a21\u8f66\u8f86\u95f4\u4ea4\u4e92\uff0c\u5c06\u535a\u5f08\u8bba\u8bc4\u4f30\u7ed3\u679c\u4f5c\u4e3a\u5148\u9a8c\u6216\u4e0e\u4f20\u7edf\u8fd0\u52a8\u5206\u7c7b\u6a21\u578b\u7ed3\u5408\u3002\u5047\u8bbe\u76ee\u6807\u8f66\u8f86\u5468\u56f4\u8f66\u8f86\u72b6\u6001\u5df2\u77e5\uff0c\u901a\u8fc7\u5728\u7ebf\u4f18\u5316\u6c42\u89e3\u76ee\u6807\u8f66\u8f86\u7684\u6700\u7406\u6027\u673a\u52a8\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u8f66\u8f86\u673a\u52a8\u884c\u4e3a\uff0c\u7279\u522b\u662f\u8f66\u9053\u53d8\u6362\u7b49\u590d\u6742\u884c\u4e3a\u3002", "conclusion": "\u7ed3\u5408\u535a\u5f08\u8bba\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8f66\u8f86\u673a\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7b49\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u66f4\u597d\u7684\u652f\u6301\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u71c3\u6cb9\u8282\u7701\u6548\u679c\u3002"}}
{"id": "2509.21878", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21878", "abs": "https://arxiv.org/abs/2509.21878", "authors": ["Moses Gladson Selvamuthu", "Tomoya Takahashi", "Riichiro Tadakuma", "Kazutoshi Tanaka"], "title": "WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces", "comment": null, "summary": "Robotic manipulators capable of regulating both compliance and stiffness\noffer enhanced operational safety and versatility. Here, we introduce Worm\nGear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator\n(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving\nmotor from external forces using this gear, WAVE enables precise force\ntransmission to the joint, while absorbing positional discrepancies through\ncompliance. WAVE is protected from excessive loads by converting impact forces\ninto elastic energy stored in a spring. In addition, the actuator achieves\ncontinuous joint stiffness modulation by changing the spring's precompression\nlength. We demonstrate these capabilities, experimentally validate the proposed\nstiffness model, show that motor loads approach zero at rest--even under\nexternal loading--and present applications using a manipulator with WAVE. This\noutcome showcases the successful decoupling of external forces. The protective\nattributes of this actuator allow for extended operation in contact-intensive\ntasks, and for robust robotic applications in challenging environments.", "AI": {"tldr": "WAVE\u662f\u4e00\u79cd\u57fa\u4e8e\u8717\u8f6e\u7684\u975e\u53cd\u5411\u9a71\u52a8\u53ef\u53d8\u521a\u5ea6\u6267\u884c\u5668\uff0c\u901a\u8fc7\u5f39\u6027\u5143\u4ef6\u5b9e\u73b0\u529b\u4f20\u9012\u548c\u4f4d\u7f6e\u5bb9\u5dee\uff0c\u80fd\u591f\u8fde\u7eed\u8c03\u8282\u5173\u8282\u521a\u5ea6\u5e76\u4fdd\u62a4\u7cfb\u7edf\u514d\u53d7\u8fc7\u5927\u8d1f\u8f7d\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8c03\u8282\u67d4\u987a\u6027\u548c\u521a\u5ea6\u7684\u673a\u68b0\u81c2\u6267\u884c\u5668\uff0c\u4ee5\u63d0\u9ad8\u64cd\u4f5c\u5b89\u5168\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u7279\u522b\u662f\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u548c\u6311\u6218\u6027\u73af\u5883\u4e2d\u3002", "method": "\u96c6\u6210\u975e\u53cd\u5411\u9a71\u52a8\u8717\u8f6e\uff0c\u5c06\u9a71\u52a8\u7535\u673a\u4e0e\u5916\u90e8\u529b\u89e3\u8026\uff0c\u901a\u8fc7\u6539\u53d8\u5f39\u7c27\u9884\u538b\u7f29\u957f\u5ea6\u5b9e\u73b0\u8fde\u7eed\u521a\u5ea6\u8c03\u8282\uff0c\u5e76\u5c06\u51b2\u51fb\u529b\u8f6c\u5316\u4e3a\u5f39\u7c27\u5b58\u50a8\u7684\u5f39\u6027\u80fd\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u521a\u5ea6\u6a21\u578b\uff0c\u663e\u793a\u5728\u9759\u6b62\u72b6\u6001\u4e0b\u7535\u673a\u8d1f\u8f7d\u63a5\u8fd1\u96f6\uff08\u5373\u4f7f\u6709\u5916\u90e8\u52a0\u8f7d\uff09\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5916\u90e8\u529b\u7684\u89e3\u8026\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u673a\u68b0\u81c2\u4e0a\u7684\u5e94\u7528\u3002", "conclusion": "WAVE\u6267\u884c\u5668\u6210\u529f\u5b9e\u73b0\u4e86\u5916\u90e8\u529b\u7684\u89e3\u8026\uff0c\u5176\u4fdd\u62a4\u7279\u6027\u4f7f\u5f97\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u80fd\u591f\u5ef6\u957f\u64cd\u4f5c\u65f6\u95f4\uff0c\u4e3a\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.21928", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21928", "abs": "https://arxiv.org/abs/2509.21928", "authors": ["Jialiang Li", "Wenzheng Wu", "Gaojing Zhang", "Yifan Han", "Wenzhao Lian"], "title": "SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks", "comment": null, "summary": "Successfully solving long-horizon manipulation tasks remains a fundamental\nchallenge. These tasks involve extended action sequences and complex object\ninteractions, presenting a critical gap between high-level symbolic planning\nand low-level continuous control. To bridge this gap, two essential\ncapabilities are required: robust long-horizon task planning and effective\ngoal-conditioned manipulation. Existing task planning methods, including\ntraditional and LLM-based approaches, often exhibit limited generalization or\nsparse semantic reasoning. Meanwhile, image-conditioned control methods\nstruggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a\nnovel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon\nManipulation Tasks. SAGE utilizes semantic scene graphs as a structural\nrepresentation for scene states. A structural scene graph enables bridging\ntask-level semantic reasoning and pixel-level visuo-motor control. This also\nfacilitates the controllable synthesis of accurate, novel sub-goal images. SAGE\nconsists of two key components: (1) a scene graph-based task planner that uses\nVLMs and LLMs to parse the environment and reason about physically-grounded\nscene state transition sequences, and (2) a decoupled structural image editing\npipeline that controllably converts each target sub-goal graph into a\ncorresponding image through image inpainting and composition. Extensive\nexperiments have demonstrated that SAGE achieves state-of-the-art performance\non distinct long-horizon tasks.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u573a\u666f\u56fe\u611f\u77e5\u5f15\u5bfc\u4e0e\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u573a\u666f\u56fe\u8fde\u63a5\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\u548c\u4f4e\u5c42\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u548c\u76ee\u6807\u6761\u4ef6\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6d89\u53ca\u6269\u5c55\u52a8\u4f5c\u5e8f\u5217\u548c\u590d\u6742\u7269\u4f53\u4ea4\u4e92\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u4e49\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u6865\u63a5\u7b26\u53f7\u89c4\u5212\u548c\u8fde\u7eed\u63a7\u5236\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u573a\u666f\u56fe\u4f5c\u4e3a\u573a\u666f\u72b6\u6001\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u4efb\u52a1\u89c4\u5212\u5668\uff08\u4f7f\u7528VLM\u548cLLM\u89e3\u6790\u73af\u5883\u548c\u63a8\u7406\u573a\u666f\u72b6\u6001\u8f6c\u6362\u5e8f\u5217\uff09\uff0c\u4ee5\u53ca\u89e3\u8026\u7684\u7ed3\u6784\u5316\u56fe\u50cf\u7f16\u8f91\u7ba1\u9053\uff08\u901a\u8fc7\u56fe\u50cf\u4fee\u590d\u548c\u5408\u6210\u5c06\u76ee\u6807\u5b50\u76ee\u6807\u56fe\u8f6c\u6362\u4e3a\u5bf9\u5e94\u56fe\u50cf\uff09\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u5728\u4e0d\u540c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SAGE\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u573a\u666f\u56fe\u6709\u6548\u6865\u63a5\u4e86\u4efb\u52a1\u7ea7\u8bed\u4e49\u63a8\u7406\u548c\u50cf\u7d20\u7ea7\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\uff0c\u4e3a\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21955", "categories": ["cs.RO", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.21955", "abs": "https://arxiv.org/abs/2509.21955", "authors": ["Divake Kumar", "Sina Tayebati", "Francesco Migliarba", "Ranganath Krishnan", "Amit Ranjan Trivedi"], "title": "Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception", "comment": null, "summary": "Deep learning models in robotics often output point estimates with poorly\ncalibrated confidences, offering no native mechanism to quantify predictive\nreliability under novel, noisy, or out-of-distribution inputs. Conformal\nprediction (CP) addresses this gap by providing distribution-free coverage\nguarantees, yet its reliance on fixed nonconformity scores ignores context and\ncan yield intervals that are overly conservative or unsafe. We address this\nwith Learnable Conformal Prediction (LCP), which replaces fixed scores with a\nlightweight neural function that leverages geometric, semantic, and\ntask-specific features to produce context-aware uncertainty sets.\n  LCP maintains CP's theoretical guarantees while reducing prediction set sizes\nby 18% in classification, tightening detection intervals by 52%, and improving\npath planning safety from 72% to 91% success with minimal overhead. Across\nthree robotic tasks on seven benchmarks, LCP consistently outperforms Standard\nCP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it\nachieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object\ndetection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding\nboxes. In path planning through cluttered environments, it improves success to\n91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.\n  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)\nand supports online adaptation, making it well suited to resource-constrained\nautonomous systems. Hardware evaluation shows LCP adds less than 1% memory and\n15.9% inference overhead, yet sustains 39 FPS on detection tasks while being\n7.4 times more energy-efficient than ensembles.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u5b66\u4e60\u4e00\u81f4\u6027\u9884\u6d4b\uff08LCP\uff09\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c0f\u9884\u6d4b\u96c6\u5927\u5c0f\u5e76\u63d0\u9ad8\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u673a\u5668\u4eba\u5b66\u4e2d\u8f93\u51fa\u70b9\u4f30\u8ba1\u4e14\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u91cf\u5316\u9884\u6d4b\u53ef\u9760\u6027\u7684\u673a\u5236\u3002\u4f20\u7edf\u4e00\u81f4\u6027\u9884\u6d4b\u4f9d\u8d56\u56fa\u5b9a\u975e\u4e00\u81f4\u6027\u5206\u6570\uff0c\u5ffd\u7565\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u533a\u95f4\u8fc7\u4e8e\u4fdd\u5b88\u6216\u4e0d\u5b89\u5168\u3002", "method": "\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u51fd\u6570\u66ff\u4ee3\u56fa\u5b9a\u975e\u4e00\u81f4\u6027\u5206\u6570\uff0c\u5229\u7528\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\uff0c\u4fdd\u6301\u4e00\u81f4\u6027\u9884\u6d4b\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u9884\u6d4b\u96c6\u5927\u5c0f\u51cf\u5c1118%\uff0c\u68c0\u6d4b\u533a\u95f4\u7f29\u5c0f52%\uff0c\u8def\u5f84\u89c4\u5212\u5b89\u5168\u6027\u4ece72%\u63d0\u5347\u81f391%\u3002\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u76843\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6CP\u548c\u96c6\u6210\u65b9\u6cd5\u3002", "conclusion": "LCP\u662f\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff084.8%\u8fd0\u884c\u65f6\u5f00\u9500\uff0c42KB\u5185\u5b58\uff09\uff0c\u652f\u6301\u5728\u7ebf\u9002\u5e94\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u6bd4\u96c6\u6210\u65b9\u6cd5\u80fd\u6548\u9ad87.4\u500d\u3002"}}
{"id": "2509.21961", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21961", "abs": "https://arxiv.org/abs/2509.21961", "authors": ["Lingguang Wang", "\u00d6mer \u015eahin Ta\u015f", "Marlon Steiner", "Christoph Stiller"], "title": "FlowDrive: moderated flow matching with data balancing for trajectory planning", "comment": null, "summary": "Learning-based planners are sensitive to the long-tailed distribution of\ndriving data. Common maneuvers dominate datasets, while dangerous or rare\nscenarios are sparse. This imbalance can bias models toward the frequent cases\nand degrade performance on critical scenarios. To tackle this problem, we\ncompare balancing strategies for sampling training data and find reweighting by\ntrajectory pattern an effective approach. We then present FlowDrive, a\nflow-matching trajectory planner that learns a conditional rectified flow to\nmap noise directly to trajectory distributions with few flow-matching steps. We\nfurther introduce moderated, in-the-loop guidance that injects small\nperturbation between flow steps to systematically increase trajectory diversity\nwhile remaining scene-consistent. On nuPlan and the interaction-focused\ninterPlan benchmarks, FlowDrive achieves state-of-the-art results among\nlearning-based planners and approaches methods with rule-based refinements.\nAfter adding moderated guidance and light post-processing (FlowDrive*), it\nachieves overall state-of-the-art performance across nearly all benchmark\nsplits.", "AI": {"tldr": "FlowDrive\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u6761\u4ef6\u6574\u6d41\u6d41\u5c06\u566a\u58f0\u76f4\u63a5\u6620\u5c04\u5230\u8f68\u8ff9\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u8c03\u8282\u5f15\u5bfc\u7b56\u7565\u589e\u52a0\u8f68\u8ff9\u591a\u6837\u6027\uff0c\u5728nuPlan\u548cinterPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5b66\u4e60\u7684\u89c4\u5212\u5668\u5bf9\u9a7e\u9a76\u6570\u636e\u957f\u5c3e\u5206\u5e03\u7684\u654f\u611f\u6027\u95ee\u9898\uff0c\u5e38\u89c1\u64cd\u4f5c\u4e3b\u5bfc\u6570\u636e\u96c6\uff0c\u800c\u5371\u9669\u6216\u7f55\u89c1\u573a\u666f\u7a00\u758f\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u9891\u7e41\u60c5\u51b5\uff0c\u5728\u5173\u952e\u573a\u666f\u4e0a\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faFlowDrive\uff0c\u4e00\u4e2a\u6d41\u5339\u914d\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u5b66\u4e60\u6761\u4ef6\u6574\u6d41\u6d41\u4ee5\u5c11\u91cf\u6d41\u5339\u914d\u6b65\u9aa4\u5c06\u566a\u58f0\u6620\u5c04\u5230\u8f68\u8ff9\u5206\u5e03\uff1b\u5f15\u5165\u8c03\u8282\u7684\u5faa\u73af\u5185\u5f15\u5bfc\uff0c\u5728\u6d41\u6b65\u9aa4\u4e4b\u95f4\u6ce8\u5165\u5c0f\u6270\u52a8\u6765\u7cfb\u7edf\u6027\u5730\u589e\u52a0\u8f68\u8ff9\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u3002", "result": "\u5728nuPlan\u548c\u4ea4\u4e92\u5bfc\u5411\u7684interPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlowDrive\u5728\u57fa\u4e8e\u5b66\u4e60\u7684\u89c4\u5212\u5668\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u63a5\u8fd1\u5177\u6709\u57fa\u4e8e\u89c4\u5219\u7cbe\u5316\u7684\u65b9\u6cd5\u3002\u6dfb\u52a0\u8c03\u8282\u5f15\u5bfc\u548c\u8f7b\u91cf\u540e\u5904\u7406\u540e\uff0c\u5728\u51e0\u4e4e\u6240\u6709\u57fa\u51c6\u5206\u5272\u4e2d\u90fd\u8fbe\u5230\u6574\u4f53\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "FlowDrive\u901a\u8fc7\u6d41\u5339\u914d\u548c\u8c03\u8282\u5f15\u5bfc\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u8f68\u8ff9\u89c4\u5212\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u5c3e\u5206\u5e03\u548c\u589e\u52a0\u8f68\u8ff9\u591a\u6837\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.21983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21983", "abs": "https://arxiv.org/abs/2509.21983", "authors": ["Sigmund Hennum H\u00f8eg", "Aksel Vaaler", "Chaoqi Liu", "Olav Egeland", "Yilun Du"], "title": "Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning", "comment": "10 pages, 11 figures. This work has been submitted to the IEEE for\n  possible publication. See https://sigmundhh.com/hybrid_diffusion/ for the\n  project website", "summary": "Constructing robots to accomplish long-horizon tasks is a long-standing\nchallenge within artificial intelligence. Approaches using generative methods,\nparticularly Diffusion Models, have gained attention due to their ability to\nmodel continuous robotic trajectories for planning and control. However, we\nshow that these models struggle with long-horizon tasks that involve complex\ndecision-making and, in general, are prone to confusing different modes of\nbehavior, leading to failure. To remedy this, we propose to augment continuous\ntrajectory generation by simultaneously generating a high-level symbolic plan.\nWe show that this requires a novel mix of discrete variable diffusion and\ncontinuous diffusion, which dramatically outperforms the baselines. In\naddition, we illustrate how this hybrid diffusion process enables flexible\ntrajectory synthesis, allowing us to condition synthesized actions on partial\nand complete symbolic conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u6563\u7b26\u53f7\u89c4\u5212\u548c\u8fde\u7eed\u8f68\u8ff9\u751f\u6210\u7684\u6df7\u5408\u6269\u6563\u65b9\u6cd5\uff0c\u89e3\u51b3\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u751f\u6210\u6a21\u578b\u5bb9\u6613\u6df7\u6dc6\u884c\u4e3a\u6a21\u5f0f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u5728\u590d\u6742\u51b3\u7b56\u7684\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u6df7\u6dc6\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\u5bfc\u81f4\u5931\u8d25\u3002", "method": "\u901a\u8fc7\u540c\u65f6\u751f\u6210\u9ad8\u5c42\u7b26\u53f7\u89c4\u5212\u548c\u8fde\u7eed\u8f68\u8ff9\uff0c\u91c7\u7528\u79bb\u6563\u53d8\u91cf\u6269\u6563\u548c\u8fde\u7eed\u6269\u6563\u7684\u6df7\u5408\u65b9\u6cd5\u8fdb\u884c\u8f68\u8ff9\u5408\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u90e8\u5206\u6216\u5b8c\u6574\u7b26\u53f7\u6761\u4ef6\u7684\u7075\u6d3b\u8f68\u8ff9\u5408\u6210\u3002", "conclusion": "\u6df7\u5408\u6269\u6563\u8fc7\u7a0b\u80fd\u6709\u6548\u89e3\u51b3\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u590d\u6742\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002"}}
{"id": "2509.21986", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21986", "abs": "https://arxiv.org/abs/2509.21986", "authors": ["Tomoya Yoshida", "Shuhei Kurita", "Taichi Nishimura", "Shinsuke Mori"], "title": "Developing Vision-Language-Action Model from Egocentric Videos", "comment": null, "summary": "Egocentric videos capture how humans manipulate objects and tools, providing\ndiverse motion cues for learning object manipulation. Unlike the costly,\nexpert-driven manual teleoperation commonly used in training\nVision-Language-Action models (VLAs), egocentric videos offer a scalable\nalternative. However, prior studies that leverage such videos for training\nrobot policies typically rely on auxiliary annotations, such as detailed\nhand-pose recordings. Consequently, it remains unclear whether VLAs can be\ntrained directly from raw egocentric videos. In this work, we address this\nchallenge by leveraging EgoScaler, a framework that extracts 6DoF object\nmanipulation trajectories from egocentric videos without requiring auxiliary\nrecordings. We apply EgoScaler to four large-scale egocentric video datasets\nand automatically refine noisy or incomplete trajectories, thereby constructing\na new large-scale dataset for VLA pre-training. Our experiments with a\nstate-of-the-art $\\pi_0$ architecture in both simulated and real-robot\nenvironments yield three key findings: (i) pre-training on our dataset improves\ntask success rates by over 20\\% compared to training from scratch, (ii) the\nperformance is competitive with that achieved using real-robot datasets, and\n(iii) combining our dataset with real-robot data yields further improvements.\nThese results demonstrate that egocentric videos constitute a promising and\nscalable resource for advancing VLA research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528EgoScaler\u6846\u67b6\u4ece\u539f\u59cb\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e2d\u63d0\u53d66DoF\u7269\u4f53\u64cd\u4f5c\u8f68\u8ff9\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7528\u4e8eVLA\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8bad\u7ec3\u4f9d\u8d56\u6602\u8d35\u7684\u4e13\u5bb6\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u800c\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u9700\u8981\u8f85\u52a9\u6807\u6ce8\u3002\u672c\u7814\u7a76\u63a2\u7d22\u80fd\u5426\u76f4\u63a5\u4ece\u539f\u59cb\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8bad\u7ec3VLA\u6a21\u578b\u3002", "method": "\u4f7f\u7528EgoScaler\u6846\u67b6\u4ece\u56db\u4e2a\u5927\u89c4\u6a21\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u63d0\u53d66DoF\u7269\u4f53\u64cd\u4f5c\u8f68\u8ff9\uff0c\u81ea\u52a8\u7cbe\u70bc\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u7684\u8f68\u8ff9\uff0c\u6784\u5efa\u65b0\u7684VLA\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u9884\u8bad\u7ec3\u4f7f\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534720%\u4ee5\u4e0a\uff1b\u6027\u80fd\u4e0e\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96c6\u76f8\u5f53\uff1b\u7ed3\u5408\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u662f\u63a8\u8fdbVLA\u7814\u7a76\u7684\u6709\u524d\u666f\u4e14\u53ef\u6269\u5c55\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.22002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22002", "abs": "https://arxiv.org/abs/2509.22002", "authors": ["Yuping Gu", "Bangchao Huang", "Haoran Sun", "Ronghan Xu", "Jiayi Yin", "Wei Zhang", "Fang Wan", "Jia Pan", "Chaoyang Song"], "title": "One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion", "comment": "23 pages, 11 figures, 2 tables. Accepted by Fundamental Research. For\n  Supplementary Videos, see https://bionicdl.ancorasir.com/?p=1668", "summary": "While it is expected to build robotic limbs with multiple degrees of freedom\n(DoF) inspired by nature, a single DoF design remains fundamental, providing\nbenefits that include, but are not limited to, simplicity, robustness,\ncost-effectiveness, and efficiency. Mechanisms, especially those with multiple\nlinks and revolute joints connected in closed loops, play an enabling factor in\nintroducing motion diversity for 1-DoF systems, which are usually constrained\nby self-collision during a full-cycle range of motion. This study presents a\nnovel computational approach to designing one-degree-of-freedom (1-DoF)\noverconstrained robotic limbs for a desired spatial trajectory, while achieving\nenergy-efficient, self-collision-free motion in full-cycle rotations. Firstly,\nwe present the geometric optimization problem of linkage-based robotic limbs in\na generalized formulation for self-collision-free design. Next, we formulate\nthe spatial trajectory generation problem with the overconstrained linkages by\noptimizing the similarity and dynamic-related metrics. We further optimize the\ngeometric shape of the overconstrained linkage to ensure smooth and\ncollision-free motion driven by a single actuator. We validated our proposed\nmethod through various experiments, including personalized automata and\nbio-inspired hexapod robots. The resulting hexapod robot, featuring\noverconstrained robotic limbs, demonstrated outstanding energy efficiency\nduring forward walking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u5355\u81ea\u7531\u5ea6\u8fc7\u7ea6\u675f\u673a\u5668\u4eba\u80a2\u4f53\uff0c\u5b9e\u73b0\u671f\u671b\u7a7a\u95f4\u8f68\u8ff9\u3001\u80fd\u91cf\u9ad8\u6548\u4e14\u5168\u5468\u671f\u65cb\u8f6c\u65e0\u81ea\u78b0\u649e\u7684\u8fd0\u52a8\u3002", "motivation": "\u5355\u81ea\u7531\u5ea6\u8bbe\u8ba1\u5177\u6709\u7b80\u5355\u6027\u3001\u9c81\u68d2\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u6548\u7387\u7b49\u4f18\u52bf\uff0c\u4f46\u901a\u5e38\u5728\u5168\u5468\u671f\u8fd0\u52a8\u8303\u56f4\u5185\u53d7\u5230\u81ea\u78b0\u649e\u7684\u9650\u5236\u3002\u8fc7\u7ea6\u675f\u673a\u6784\u80fd\u591f\u4e3a\u5355\u81ea\u7531\u5ea6\u7cfb\u7edf\u5f15\u5165\u8fd0\u52a8\u591a\u6837\u6027\u3002", "method": "\u9996\u5148\u63d0\u51fa\u8fde\u6746\u5f0f\u673a\u5668\u4eba\u80a2\u4f53\u7684\u51e0\u4f55\u4f18\u5316\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u4f18\u5316\u76f8\u4f3c\u6027\u548c\u52a8\u6001\u76f8\u5173\u6307\u6807\u6765\u5236\u5b9a\u7a7a\u95f4\u8f68\u8ff9\u751f\u6210\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u8fc7\u7ea6\u675f\u8fde\u6746\u7684\u51e0\u4f55\u5f62\u72b6\u4ee5\u786e\u4fdd\u5e73\u6ed1\u65e0\u78b0\u649e\u8fd0\u52a8\u3002", "result": "\u901a\u8fc7\u4e2a\u6027\u5316\u81ea\u52a8\u673a\u548c\u4eff\u751f\u516d\u8db3\u673a\u5668\u4eba\u7b49\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u6240\u5f97\u5230\u7684\u516d\u8db3\u673a\u5668\u4eba\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u524d\u8fdb\u884c\u8d70\u80fd\u91cf\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5355\u81ea\u7531\u5ea6\u8fc7\u7ea6\u675f\u673a\u5668\u4eba\u80a2\u4f53\u7684\u8bbe\u8ba1\uff0c\u80fd\u591f\u5728\u5168\u5468\u671f\u65cb\u8f6c\u4e2d\u5b9e\u73b0\u80fd\u91cf\u9ad8\u6548\u3001\u65e0\u81ea\u78b0\u649e\u7684\u8fd0\u52a8\u3002"}}
{"id": "2509.22058", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22058", "abs": "https://arxiv.org/abs/2509.22058", "authors": ["Qifeng Wang", "Weigang Li", "Lei Nie", "Xin Xu", "Wenping Liu", "Zhe Xu"], "title": "An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose", "comment": null, "summary": "As a key technology for autonomous navigation and positioning in mobile\nrobots, light detection and ranging (LiDAR) odometry is widely used in\nautonomous driving applications. The Iterative Closest Point (ICP)-based\nmethods have become the core technique in LiDAR odometry due to their efficient\nand accurate point cloud registration capability. However, some existing\nICP-based methods do not consider the reliability of the initial pose, which\nmay cause the method to converge to a local optimum. Furthermore, the absence\nof an adaptive mechanism hinders the effective handling of complex dynamic\nenvironments, resulting in a significant degradation of registration accuracy.\nTo address these issues, this paper proposes an adaptive ICP-based LiDAR\nodometry method that relies on a reliable initial pose. First, distributed\ncoarse registration based on density filtering is employed to obtain the\ninitial pose estimation. The reliable initial pose is then selected by\ncomparing it with the motion prediction pose, reducing the initial error\nbetween the source and target point clouds. Subsequently, by combining the\ncurrent and historical errors, the adaptive threshold is dynamically adjusted\nto accommodate the real-time changes in the dynamic environment. Finally, based\non the reliable initial pose and the adaptive threshold, point-to-plane\nadaptive ICP registration is performed from the current frame to the local map,\nachieving high-precision alignment of the source and target point clouds.\nExtensive experiments on the public KITTI dataset demonstrate that the proposed\nmethod outperforms existing approaches and significantly enhances the accuracy\nof LiDAR odometry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94ICP\u7684LiDAR\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u9760\u521d\u59cb\u4f4d\u59ff\u548c\u81ea\u9002\u5e94\u9608\u503c\u6765\u63d0\u9ad8\u52a8\u6001\u73af\u5883\u4e0b\u7684\u70b9\u4e91\u914d\u51c6\u7cbe\u5ea6", "motivation": "\u73b0\u6709ICP\u65b9\u6cd5\u4e0d\u8003\u8651\u521d\u59cb\u4f4d\u59ff\u53ef\u9760\u6027\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff1b\u7f3a\u4e4f\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u914d\u51c6\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d", "method": "1. \u57fa\u4e8e\u5bc6\u5ea6\u6ee4\u6ce2\u7684\u5206\u5e03\u5f0f\u7c97\u914d\u51c6\u83b7\u53d6\u521d\u59cb\u4f4d\u59ff\uff1b2. \u901a\u8fc7\u4e0e\u8fd0\u52a8\u9884\u6d4b\u4f4d\u59ff\u6bd4\u8f83\u9009\u62e9\u53ef\u9760\u521d\u59cb\u4f4d\u59ff\uff1b3. \u7ed3\u5408\u5f53\u524d\u548c\u5386\u53f2\u8bef\u5dee\u52a8\u6001\u8c03\u6574\u81ea\u9002\u5e94\u9608\u503c\uff1b4. \u57fa\u4e8e\u53ef\u9760\u521d\u59cb\u4f4d\u59ff\u548c\u81ea\u9002\u5e94\u9608\u503c\u8fdb\u884c\u70b9\u5bf9\u9762\u81ea\u9002\u5e94ICP\u914d\u51c6", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LiDAR\u91cc\u7a0b\u8ba1\u7684\u7cbe\u5ea6", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94ICP LiDAR\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u901a\u8fc7\u53ef\u9760\u521d\u59cb\u4f4d\u59ff\u548c\u81ea\u9002\u5e94\u9608\u503c\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7684\u70b9\u4e91\u914d\u51c6\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6"}}
{"id": "2509.22065", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.22065", "abs": "https://arxiv.org/abs/2509.22065", "authors": ["Ethan Fulcher", "J. Diego Caporale", "Yifeng Zhang", "John Ruck", "Feifei Qian"], "title": "Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot", "comment": "7+1 pages, 5 figures, ICRA Submission This work has been submitted to\n  the IEEE for possible publication", "summary": "In-situ robotic exploration is an important tool for advancing knowledge of\ngeological processes that describe the Earth and other Planetary bodies. To\ninform and enhance operations for these roving laboratories, it is imperative\nto understand the terramechanical properties of their environments, especially\nfor traversing on loose, deformable substrates. Recent research suggested that\nlegged robots with direct-drive and low-gear ratio actuators can sensitively\ndetect external forces, and therefore possess the potential to measure terrain\nproperties with their legs during locomotion, providing unprecedented sampling\nspeed and density while accessing terrains previously too risky to sample. This\npaper explores these ideas by investigating the impact of gait on\nproprioceptive terrain sensing accuracy, particularly comparing a\nsensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,\nTrot-Walk. Each gait's ability to measure the strength and texture of\ndeformable substrate is quantified as the robot locomotes over a laboratory\ntransect consisting of a rigid surface, loose sand, and loose sand with\nsynthetic surface crusts. Our results suggest that with both the\nsensing-oriented crawling gait and locomotion-oriented trot gait, the robot can\nmeasure a consistent difference in the strength (in terms of penetration\nresistance) between the low- and high-resistance substrates; however, the\nlocomotion-oriented trot gait contains larger magnitude and variance in\nmeasurements. Furthermore, the slower crawl gait can detect brittle ruptures of\nthe surface crusts with significantly higher accuracy than the faster trot\ngait. Our results offer new insights that inform legged robot \"sensing during\nlocomotion\" gait design and planning for scouting the terrain and producing\nscientific measurements on other worlds to advance our understanding of their\ngeology and formation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6b65\u6001\u5bf9\u817f\u90e8\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\u5730\u5f62\u4f20\u611f\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u4f20\u611f\u5bfc\u5411\u7684\u722c\u884c\u6b65\u6001\u548c\u8fd0\u52a8\u5bfc\u5411\u7684\u5c0f\u8dd1\u6b65\u6001\u5728\u6d4b\u91cf\u53ef\u53d8\u5f62\u57fa\u8d28\u5f3a\u5ea6\u548c\u7eb9\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u817f\u90e8\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u6d4b\u91cf\u5730\u5f62\u7279\u6027\uff0c\u53ef\u4ee5\u5728\u8bbf\u95ee\u98ce\u9669\u8f83\u9ad8\u7684\u5730\u5f62\u65f6\u63d0\u4f9b\u524d\u6240\u672a\u6709\u7684\u91c7\u6837\u901f\u5ea6\u548c\u5bc6\u5ea6\uff0c\u8fd9\u5bf9\u4e8e\u884c\u661f\u5730\u8d28\u52d8\u63a2\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\uff0c\u8ba9\u673a\u5668\u4eba\u4f7f\u7528Crawl N' Sense\uff08\u4f20\u611f\u5bfc\u5411\uff09\u548cTrot-Walk\uff08\u8fd0\u52a8\u5bfc\u5411\uff09\u4e24\u79cd\u6b65\u6001\u5728\u521a\u6027\u8868\u9762\u3001\u677e\u6563\u6c99\u5730\u548c\u5e26\u5408\u6210\u8868\u9762\u7ed3\u58f3\u7684\u677e\u6563\u6c99\u5730\u4e0a\u8fd0\u52a8\uff0c\u91cf\u5316\u6d4b\u91cf\u57fa\u8d28\u7684\u5f3a\u5ea6\u548c\u7eb9\u7406\u7279\u6027\u3002", "result": "\u4e24\u79cd\u6b65\u6001\u90fd\u80fd\u6d4b\u91cf\u4f4e\u963b\u529b\u548c\u9ad8\u963b\u529b\u57fa\u8d28\u4e4b\u95f4\u7684\u5f3a\u5ea6\u5dee\u5f02\uff0c\u4f46\u8fd0\u52a8\u5bfc\u5411\u7684\u5c0f\u8dd1\u6b65\u6001\u6d4b\u91cf\u503c\u5e45\u5ea6\u548c\u65b9\u5dee\u66f4\u5927\u3002\u8f83\u6162\u7684\u722c\u884c\u6b65\u6001\u68c0\u6d4b\u8868\u9762\u7ed3\u58f3\u8106\u6027\u7834\u88c2\u7684\u51c6\u786e\u5ea6\u663e\u8457\u9ad8\u4e8e\u8f83\u5feb\u7684\u5c0f\u8dd1\u6b65\u6001\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u817f\u90e8\u673a\u5668\u4eba\"\u8fd0\u52a8\u4e2d\u4f20\u611f\"\u7684\u6b65\u6001\u8bbe\u8ba1\u548c\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5728\u5176\u4ed6\u661f\u7403\u4e0a\u8fdb\u884c\u5730\u5f62\u4fa6\u5bdf\u548c\u79d1\u5b66\u6d4b\u91cf\uff0c\u63a8\u8fdb\u5bf9\u5176\u5730\u8d28\u548c\u5f62\u6210\u8fc7\u7a0b\u7684\u7406\u89e3\u3002"}}
{"id": "2509.22093", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22093", "abs": "https://arxiv.org/abs/2509.22093", "authors": ["Xiaohuan Pei", "Yuxing Chen", "Siyu Xu", "Yunke Wang", "Yuheng Shi", "Chang Xu"], "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation", "comment": null, "summary": "Robotic manipulation with Vision-Language-Action models requires efficient\ninference over long-horizon multi-modal context, where attention to dense\nvisual tokens dominates computational cost. Existing methods optimize inference\nspeed by reducing visual redundancy within VLA models, but they overlook the\nvarying redundancy across robotic manipulation stages. We observe that the\nvisual token redundancy is higher in coarse manipulation phase than in\nfine-grained operations, and is strongly correlated with the action dynamic.\nMotivated by this observation, we propose \\textbf{A}ction-aware\n\\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP}), a multi-modal pruning\nframework that integrates text-driven token selection with action-aware\ntrajectory gating. Our method introduces a gating mechanism that conditions the\npruning signal on recent action trajectories, using past motion windows to\nadaptively adjust token retention ratios in accordance with dynamics, thereby\nbalancing computational efficiency and perceptual precision across different\nmanipulation stages. Extensive experiments on the LIBERO suites and diverse\nreal-world scenarios demonstrate that our method significantly reduces FLOPs\nand action inference latency (\\textit{e.g.} $1.35 \\times$ speed up on\nOpenVLA-OFT) while maintaining competitive success rates (\\textit{e.g.} 25.8\\%\nimprovements with OpenVLA) compared to baselines, thereby providing a simple\nplug-in path to efficient robot policies that advances the efficiency and\nperformance frontier of robotic manipulation. Our project website is:\n\\href{https://vla-adp.github.io/}{ADP.com}.", "AI": {"tldr": "\u63d0\u51faADP\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u611f\u77e5\u7684\u52a8\u6001\u526a\u679d\u673a\u5236\uff0c\u6839\u636e\u673a\u5668\u4eba\u64cd\u4f5c\u9636\u6bb5\u7684\u4e0d\u540c\u89c6\u89c9\u5197\u4f59\u5ea6\u81ea\u9002\u5e94\u8c03\u6574token\u4fdd\u7559\u6bd4\u4f8b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316VLA\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u65f6\u5ffd\u7565\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e0d\u540c\u9636\u6bb5\u7684\u89c6\u89c9\u5197\u4f59\u5ea6\u5dee\u5f02\uff0c\u7c97\u7c92\u5ea6\u64cd\u4f5c\u9636\u6bb5\u89c6\u89c9token\u5197\u4f59\u5ea6\u9ad8\u4e8e\u7ec6\u7c92\u5ea6\u64cd\u4f5c\uff0c\u4e14\u4e0e\u52a8\u4f5c\u52a8\u6001\u6027\u76f8\u5173\u3002", "method": "\u7ed3\u5408\u6587\u672c\u9a71\u52a8\u7684token\u9009\u62e9\u548c\u52a8\u4f5c\u611f\u77e5\u7684\u8f68\u8ff9\u95e8\u63a7\u673a\u5236\uff0c\u5229\u7528\u8fc7\u53bb\u52a8\u4f5c\u7a97\u53e3\u52a8\u6001\u8c03\u6574token\u4fdd\u7559\u6bd4\u4f8b\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u611f\u77e5\u7cbe\u5ea6\u3002", "result": "\u5728LIBERO\u5957\u4ef6\u548c\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u663e\u8457\u964d\u4f4eFLOPs\u548c\u52a8\u4f5c\u63a8\u7406\u5ef6\u8fdf\uff08\u5982OpenVLA-OFT\u4e0a1.35\u500d\u52a0\u901f\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u6210\u529f\u7387\uff08\u5982OpenVLA\u63d0\u534725.8%\uff09\u3002", "conclusion": "ADP\u4e3a\u9ad8\u6548\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u7b80\u5355\u63d2\u4ef6\u8def\u5f84\uff0c\u63a8\u8fdb\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u6548\u7387\u548c\u6027\u80fd\u7684\u524d\u6cbf\u3002"}}
{"id": "2509.22120", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22120", "abs": "https://arxiv.org/abs/2509.22120", "authors": ["Alireza Aliyari", "Gholamreza Vossoughi"], "title": "Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot", "comment": "12 pages, 11 figures, 2 tables, under review at the journal of\n  \"Transactions of the Canadian Society for Mechanical Engineering\"", "summary": "The use of exoskeleton robots is increasing due to the rising number of\nmusculoskeletal injuries. However, their effectiveness depends heavily on the\ndesign of control systems. Designing robust controllers is challenging because\nof uncertainties in human-robot systems. Among various control strategies,\nModel Predictive Control (MPC) is a powerful approach due to its ability to\nhandle constraints and optimize performance. Previous studies have used\nlinearization-based methods to implement robust MPC on exoskeletons, but these\ncan degrade performance due to nonlinearities in the robot's dynamics. To\naddress this gap, this paper proposes a Robust Nonlinear Model Predictive\nControl (RNMPC) method, called multi-stage NMPC, to control a\ntwo-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.\nThis method uses multiple scenarios to represent system uncertainties. The\nstudy focuses on minimizing human-robot interaction forces during the swing\nphase, particularly when the robot carries unknown loads. Simulations and\nexperimental tests show that the proposed method significantly improves\nrobustness, outperforming non-robust NMPC. It achieves lower tracking errors\nand interaction forces under various uncertainties. For instance, when a 2 kg\nunknown payload is combined with external disturbances, the RMS values of thigh\nand shank interaction forces for multi-stage NMPC are reduced by 77 and 94\npercent, respectively, compared to non-robust NMPC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RNMPC\uff09\u65b9\u6cd5\uff0c\u79f0\u4e3a\u591a\u9636\u6bb5NMPC\uff0c\u7528\u4e8e\u63a7\u5236\u4e8c\u81ea\u7531\u5ea6\u5916\u9aa8\u9abc\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u89e3\u51b3\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u6765\u5904\u7406\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u7531\u4e8e\u808c\u8089\u9aa8\u9abc\u635f\u4f24\u589e\u52a0\uff0c\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u4f7f\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u6709\u6548\u6027\u4e25\u91cd\u4f9d\u8d56\u4e8e\u63a7\u5236\u7cfb\u7edf\u8bbe\u8ba1\u3002\u73b0\u6709\u7ebf\u6027\u5316\u65b9\u6cd5\u5728\u5904\u7406\u673a\u5668\u4eba\u52a8\u529b\u5b66\u975e\u7ebf\u6027\u65f6\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u9700\u8981\u66f4\u597d\u7684\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08multi-stage NMPC\uff09\uff0c\u4f7f\u7528\u591a\u4e2a\u573a\u666f\u6765\u8868\u793a\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u89e3\u51b3\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u6765\u63a7\u5236\u4e8c\u81ea\u7531\u5ea6\u5916\u9aa8\u9abc\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u975e\u9c81\u68d2NMPC\u3002\u57282kg\u672a\u77e5\u8d1f\u8f7d\u548c\u5916\u90e8\u5e72\u6270\u4e0b\uff0c\u5927\u817f\u548c\u5c0f\u817f\u4ea4\u4e92\u529b\u7684RMS\u503c\u5206\u522b\u6bd4\u975e\u9c81\u68d2NMPC\u964d\u4f4e\u4e8677%\u548c94%\u3002", "conclusion": "\u591a\u9636\u6bb5NMPC\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u964d\u4f4e\u8ddf\u8e2a\u8bef\u5dee\u548c\u4ea4\u4e92\u529b\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2509.22149", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22149", "abs": "https://arxiv.org/abs/2509.22149", "authors": ["Haoqi Yuan", "Ziye Huang", "Ye Wang", "Chuan Mao", "Chaoyi Xu", "Zongqing Lu"], "title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration", "comment": null, "summary": "Universal grasping with multi-fingered dexterous hands is a fundamental\nchallenge in robotic manipulation. While recent approaches successfully learn\nclosed-loop grasping policies using reinforcement learning (RL), the inherent\ndifficulty of high-dimensional, long-horizon exploration necessitates complex\nreward and curriculum design, often resulting in suboptimal solutions across\ndiverse objects. We propose DemoGrasp, a simple yet effective method for\nlearning universal dexterous grasping. We start from a single successful\ndemonstration trajectory of grasping a specific object and adapt to novel\nobjects and poses by editing the robot actions in this trajectory: changing the\nwrist pose determines where to grasp, and changing the hand joint angles\ndetermines how to grasp. We formulate this trajectory editing as a single-step\nMarkov Decision Process (MDP) and use RL to optimize a universal policy across\nhundreds of objects in parallel in simulation, with a simple reward consisting\nof a binary success term and a robot-table collision penalty. In simulation,\nDemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow\nHand, outperforming previous state-of-the-art methods. It also shows strong\ntransferability, achieving an average success rate of 84.6% across diverse\ndexterous hand embodiments on six unseen object datasets, while being trained\non only 175 objects. Through vision-based imitation learning, our policy\nsuccessfully grasps 110 unseen real-world objects, including small, thin items.\nIt generalizes to spatial, background, and lighting changes, supports both RGB\nand depth inputs, and extends to language-guided grasping in cluttered scenes.", "AI": {"tldr": "DemoGrasp\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u901a\u7528\u7075\u5de7\u6293\u53d6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u8f91\u5355\u4e2a\u6f14\u793a\u8f68\u8ff9\u6765\u9002\u5e94\u65b0\u7269\u4f53\u548c\u59ff\u6001\uff0c\u5728\u6a21\u62df\u4e2d\u8fbe\u523095%\u6210\u529f\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6210\u529f\u6293\u53d6110\u4e2a\u672a\u89c1\u7269\u4f53\u3002", "motivation": "\u89e3\u51b3\u591a\u6307\u7075\u5de7\u624b\u901a\u7528\u6293\u53d6\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u590d\u6742\u5956\u52b1\u548c\u8bfe\u7a0b\u8bbe\u8ba1\u5bfc\u81f4\u7684\u6b21\u4f18\u89e3\u95ee\u9898\u3002", "method": "\u4ece\u5355\u4e2a\u6210\u529f\u6f14\u793a\u8f68\u8ff9\u51fa\u53d1\uff0c\u901a\u8fc7\u7f16\u8f91\u673a\u5668\u4eba\u52a8\u4f5c\u6765\u9002\u5e94\u65b0\u7269\u4f53\uff1a\u6539\u53d8\u624b\u8155\u59ff\u6001\u51b3\u5b9a\u6293\u53d6\u4f4d\u7f6e\uff0c\u6539\u53d8\u624b\u90e8\u5173\u8282\u89d2\u5ea6\u51b3\u5b9a\u6293\u53d6\u65b9\u5f0f\uff0c\u5c06\u8f68\u8ff9\u7f16\u8f91\u5efa\u6a21\u4e3a\u5355\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0c\u4f7f\u7528Shadow Hand\u5728DexGraspNet\u7269\u4f53\u4e0a\u8fbe\u523095%\u6210\u529f\u7387\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6210\u529f\u6293\u53d6110\u4e2a\u672a\u89c1\u7269\u4f53\uff0c\u5305\u62ec\u5c0f\u800c\u8584\u7684\u7269\u54c1\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u7a7a\u95f4\u3001\u80cc\u666f\u548c\u5149\u7167\u53d8\u5316\u3002", "conclusion": "DemoGrasp\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u901a\u7528\u7075\u5de7\u6293\u53d6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8fc1\u79fb\u80fd\u529b\u548c\u6cdb\u5316\u6027\uff0c\u652f\u6301RGB\u548c\u6df1\u5ea6\u8f93\u5165\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u8bed\u8a00\u5f15\u5bfc\u7684\u6742\u4e71\u573a\u666f\u6293\u53d6\u3002"}}
{"id": "2509.22175", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22175", "abs": "https://arxiv.org/abs/2509.22175", "authors": ["Quanzhou Li", "Zhonghua Wu", "Jingbo Wang", "Chen Change Loy", "Bo Dai"], "title": "DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions", "comment": null, "summary": "Learning to generate dual-hand grasps that respect object semantics is\nessential for robust hand-object interaction but remains largely underexplored\ndue to dataset scarcity. Existing grasp datasets predominantly focus on\nsingle-hand interactions and contain only limited semantic part annotations. To\naddress these challenges, we introduce a pipeline, SymOpt, that constructs a\nlarge-scale dual-hand grasp dataset by leveraging existing single-hand datasets\nand exploiting object and hand symmetries. Building on this, we propose a\ntext-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand\nAffordance-aware Grasps for unseen objects. Our approach incorporates a novel\ndual-hand affordance representation and follows a two-stage design, which\nenables effective learning from a small set of segmented training objects while\nscaling to a much larger pool of unsegmented data. Extensive experiments\ndemonstrate that our method produces diverse and semantically consistent\ngrasps, outperforming strong baselines in both grasp quality and generalization\nto unseen objects. The project page is at\nhttps://quanzhou-li.github.io/DHAGrasp/.", "AI": {"tldr": "\u63d0\u51fa\u4e86SymOpt\u6d41\u6c34\u7ebf\u6784\u5efa\u5927\u89c4\u6a21\u53cc\u624b\u6293\u53d6\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86DHAGrasp\u6587\u672c\u5f15\u5bfc\u7684\u53cc\u624b\u6293\u53d6\u751f\u6210\u5668\uff0c\u80fd\u591f\u4e3a\u672a\u89c1\u5bf9\u8c61\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u53cc\u624b\u6293\u53d6\u3002", "motivation": "\u73b0\u6709\u6293\u53d6\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u5355\u624b\u4ea4\u4e92\u4e14\u8bed\u4e49\u90e8\u5206\u6807\u6ce8\u6709\u9650\uff0c\u7f3a\u4e4f\u80fd\u591f\u751f\u6210\u5c0a\u91cd\u5bf9\u8c61\u8bed\u4e49\u7684\u53cc\u624b\u6293\u53d6\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u73b0\u6709\u5355\u624b\u6570\u636e\u96c6\u548c\u5bf9\u8c61\u4e0e\u624b\u7684\u5bf9\u79f0\u6027\u6784\u5efa\u5927\u89c4\u6a21\u53cc\u624b\u6293\u53d6\u6570\u636e\u96c6\uff1b\u63d0\u51fa\u65b0\u9896\u7684\u53cc\u624b\u53ef\u64cd\u4f5c\u6027\u8868\u793a\u548c\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff0c\u80fd\u591f\u4ece\u5c11\u91cf\u5206\u5272\u8bad\u7ec3\u5bf9\u8c61\u6709\u6548\u5b66\u4e60\u5e76\u6269\u5c55\u5230\u5927\u91cf\u672a\u5206\u5272\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u591a\u6837\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u6293\u53d6\uff0c\u5728\u6293\u53d6\u8d28\u91cf\u548c\u672a\u89c1\u5bf9\u8c61\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u53cc\u624b\u6293\u53d6\u751f\u6210\u4e2d\u7684\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u6311\u6218\uff0c\u4e3a\u7a33\u5065\u7684\u624b-\u5bf9\u8c61\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22195", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22195", "abs": "https://arxiv.org/abs/2509.22195", "authors": ["Asher J. Hancock", "Xindi Wu", "Lihan Zha", "Olga Russakovsky", "Anirudha Majumdar"], "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting", "comment": null, "summary": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to\ncreate vision-language-action (VLA) models is a promising paradigm for training\ngeneralist policies, but it suffers from a fundamental tradeoff: learning to\nproduce actions often diminishes the VLM's foundational reasoning and\nmultimodal understanding, hindering generalization to novel scenarios,\ninstruction following, and semantic understanding. We argue that this\ncatastrophic forgetting is due to a distribution mismatch between the VLM's\ninternet-scale pretraining corpus and the robotics fine-tuning data. Inspired\nby this observation, we introduce VLM2VLA: a VLA training paradigm that first\nresolves this mismatch at the data level by representing low-level actions with\nnatural language. This alignment makes it possible to train VLAs solely with\nLow-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and\naverting catastrophic forgetting. As a result, the VLM can be fine-tuned on\nrobot teleoperation data without fundamentally altering the underlying\narchitecture and without expensive co-training on internet-scale VLM datasets.\nThrough extensive Visual Question Answering (VQA) studies and over 800\nreal-world robotics experiments, we demonstrate that VLM2VLA preserves the\nVLM's core capabilities, enabling zero-shot generalization to novel tasks that\nrequire open-world semantic reasoning and multilingual instruction following.", "AI": {"tldr": "VLM2VLA\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u7528\u81ea\u7136\u8bed\u8a00\u8868\u793a\u4f4e\u7ea7\u52a8\u4f5c\u6765\u5bf9\u9f50\u6570\u636e\u5206\u5e03\uff0c\u4ec5\u4f7f\u7528LoRA\u5fae\u8c03\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4fdd\u6301VLM\u6838\u5fc3\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u673a\u5668\u4eba\u4efb\u52a1\u6cdb\u5316\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u5fae\u8c03\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u6570\u636e\u65f6\uff0c\u5b66\u4e60\u52a8\u4f5c\u4f1a\u524a\u5f31\u6a21\u578b\u7684\u57fa\u7840\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002\u8fd9\u79cd\u707e\u96be\u6027\u9057\u5fd8\u6e90\u4e8eVLM\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u673a\u5668\u4eba\u5fae\u8c03\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faVLM2VLA\u8bad\u7ec3\u8303\u5f0f\uff1a1\uff09\u7528\u81ea\u7136\u8bed\u8a00\u8868\u793a\u4f4e\u7ea7\u52a8\u4f5c\u89e3\u51b3\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\uff1b2\uff09\u4ec5\u4f7f\u7528LoRA\u8fdb\u884c\u5fae\u8c03\uff0c\u6700\u5c0f\u5316\u4fee\u6539VLM\u4e3b\u5e72\u7f51\u7edc\uff1b3\uff09\u907f\u514d\u5728\u4e92\u8054\u7f51\u89c4\u6a21VLM\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6602\u8d35\u7684\u5171\u540c\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u89c6\u89c9\u95ee\u7b54\u7814\u7a76\u548c800\u591a\u6b21\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u8bc1\u660e\uff0cVLM2VLA\u80fd\u591f\u4fdd\u6301VLM\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u5230\u9700\u8981\u5f00\u653e\u4e16\u754c\u8bed\u4e49\u63a8\u7406\u548c\u591a\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u7684\u65b0\u4efb\u52a1\u3002", "conclusion": "VLM2VLA\u901a\u8fc7\u6570\u636e\u7ea7\u5bf9\u9f50\u548c\u6700\u5c0f\u5316\u67b6\u6784\u4fee\u6539\uff0c\u6210\u529f\u89e3\u51b3\u4e86VLA\u8bad\u7ec3\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u8bad\u7ec3\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22199", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22199", "abs": "https://arxiv.org/abs/2509.22199", "authors": ["Haoyun Li", "Ivan Zhang", "Runqi Ouyang", "Xiaofeng Wang", "Zheng Zhu", "Zhiqin Yang", "Zhentao Zhang", "Boyuan Wang", "Chaojun Ni", "Wenkang Qin", "Xinze Chen", "Yun Ye", "Guan Huang", "Zhenbo Song", "Xingang Wang"], "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training", "comment": null, "summary": "Vision Language Action (VLA) models derive their generalization capability\nfrom diverse training data, yet collecting embodied robot interaction data\nremains prohibitively expensive. In contrast, human demonstration videos are\nfar more scalable and cost-efficient to collect, and recent studies confirm\ntheir effectiveness in training VLA models. However, a significant domain gap\npersists between human videos and robot-executed videos, including unstable\ncamera viewpoints, visual discrepancies between human hands and robotic arms,\nand differences in motion dynamics. To bridge this gap, we propose\nMimicDreamer, a framework that turns fast, low-cost human demonstrations into\nrobot-usable supervision by jointly aligning vision, viewpoint, and actions to\ndirectly support policy training. For visual alignment, we propose H2R Aligner,\na video diffusion model that generates high-fidelity robot demonstration videos\nby transferring motion from human manipulation footage. For viewpoint\nstabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos\nvia homography and inpaints occlusions and distortions caused by warping. For\naction alignment, we map human hand trajectories to the robot frame and apply a\nconstrained inverse kinematics solver to produce feasible, low-jitter joint\ncommands with accurate pose tracking. Empirically, VLA models trained purely on\nour synthesized human-to-robot videos achieve few-shot execution on real\nrobots. Moreover, scaling training with human data significantly boosts\nperformance compared to models trained solely on real robot data; our approach\nimproves the average success rate by 14.7\\% across six representative\nmanipulation tasks.", "AI": {"tldr": "MimicDreamer\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u3001\u89c6\u89d2\u548c\u52a8\u4f5c\u4e09\u65b9\u9762\u7684\u5bf9\u9f50\uff0c\u5c06\u4f4e\u6210\u672c\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u53ef\u7528\u7684\u76d1\u7763\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u6536\u96c6\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u800c\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u66f4\u6613\u83b7\u53d6\u3002\u4f46\u4eba\u7c7b\u89c6\u9891\u4e0e\u673a\u5668\u4eba\u6267\u884c\u89c6\u9891\u5b58\u5728\u663e\u8457\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u5305\u62ec\u4e0d\u7a33\u5b9a\u7684\u76f8\u673a\u89c6\u89d2\u3001\u4eba\u7c7b\u624b\u4e0e\u673a\u68b0\u81c2\u7684\u89c6\u89c9\u5dee\u5f02\u4ee5\u53ca\u8fd0\u52a8\u52a8\u6001\u5dee\u5f02\u3002", "method": "\u63d0\u51faMimicDreamer\u6846\u67b6\uff1a1) H2R Aligner\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u89c6\u89c9\u5bf9\u9f50\uff1b2) EgoStabilizer\u901a\u8fc7\u5355\u5e94\u6027\u53d8\u6362\u7a33\u5b9a\u89c6\u89d2\u5e76\u4fee\u590d\u906e\u6321\uff1b3) \u5c06\u4eba\u624b\u8f68\u8ff9\u6620\u5c04\u5230\u673a\u5668\u4eba\u5750\u6807\u7cfb\uff0c\u4f7f\u7528\u7ea6\u675f\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u751f\u6210\u53ef\u884c\u7684\u5173\u8282\u547d\u4ee4\u3002", "result": "\u4ec5\u4f7f\u7528\u5408\u6210\u7684\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u89c6\u9891\u8bad\u7ec3\u7684VLA\u6a21\u578b\u80fd\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u5c11\u6837\u672c\u6267\u884c\u3002\u4e0e\u4ec5\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u516d\u4e2a\u4ee3\u8868\u6027\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5e73\u5747\u6210\u529f\u7387\u63d0\u534714.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5f25\u5408\u4e86\u4eba\u7c7b\u6f14\u793a\u4e0e\u673a\u5668\u4eba\u6267\u884c\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u53ef\u6269\u5c55\u7684\u4eba\u7c7b\u6570\u636e\u8bad\u7ec3VLA\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u6027\u80fd\u3002"}}
{"id": "2509.22205", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22205", "abs": "https://arxiv.org/abs/2509.22205", "authors": ["Ke Ye", "Jiaming Zhou", "Yuanfeng Qiu", "Jiayi Liu", "Shihui Zhou", "Kun-Yu Lin", "Junwei Liang"], "title": "From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment", "comment": null, "summary": "Generalizing to long-horizon manipulation tasks in a zero-shot setting\nremains a central challenge in robotics. Current multimodal foundation based\napproaches, despite their capabilities, typically fail to decompose high-level\ncommands into executable action sequences from static visual input alone. To\naddress this challenge, we introduce Super-Mimic, a hierarchical framework that\nenables zero-shot robotic imitation by directly inferring procedural intent\nfrom unscripted human demonstration videos. Our framework is composed of two\nsequential modules. First, a Human Intent Translator (HIT) parses the input\nvideo using multimodal reasoning to produce a sequence of language-grounded\nsubtasks. These subtasks then condition a Future Dynamics Predictor (FDP),\nwhich employs a generative model that synthesizes a physically plausible video\nrollout for each step. The resulting visual trajectories are dynamics-aware,\nexplicitly modeling crucial object interactions and contact points to guide the\nlow-level controller. We validate this approach through extensive experiments\non a suite of long-horizon manipulation tasks, where Super-Mimic significantly\noutperforms state-of-the-art zero-shot methods by over 20\\%. These results\nestablish that coupling video-driven intent parsing with prospective dynamics\nmodeling is a highly effective strategy for developing general-purpose robotic\nsystems.", "AI": {"tldr": "Super-Mimic\u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u65e0\u811a\u672c\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4e2d\u76f4\u63a5\u63a8\u65ad\u7a0b\u5e8f\u610f\u56fe\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u673a\u5668\u4eba\u6a21\u4eff\u3002\u5b83\u5305\u542b\u4eba\u7c7b\u610f\u56fe\u7ffb\u8bd1\u5668\u548c\u672a\u6765\u52a8\u6001\u9884\u6d4b\u5668\u4e24\u4e2a\u6a21\u5757\uff0c\u5728\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u6cdb\u5316\u6311\u6218\uff0c\u5f53\u524d\u591a\u6a21\u6001\u57fa\u7840\u65b9\u6cd5\u4ec5\u4ece\u9759\u6001\u89c6\u89c9\u8f93\u5165\u65e0\u6cd5\u5c06\u9ad8\u7ea7\u547d\u4ee4\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\u3002", "method": "\u5206\u5c42\u6846\u67b6\uff1a1) \u4eba\u7c7b\u610f\u56fe\u7ffb\u8bd1\u5668(HIT)\u4f7f\u7528\u591a\u6a21\u6001\u63a8\u7406\u89e3\u6790\u8f93\u5165\u89c6\u9891\uff0c\u751f\u6210\u8bed\u8a00\u57fa\u7840\u5b50\u4efb\u52a1\u5e8f\u5217\uff1b2) \u672a\u6765\u52a8\u6001\u9884\u6d4b\u5668(FDP)\u4f7f\u7528\u751f\u6210\u6a21\u578b\u4e3a\u6bcf\u4e2a\u6b65\u9aa4\u5408\u6210\u7269\u7406\u5408\u7406\u7684\u89c6\u9891\u5c55\u5f00\uff0c\u751f\u6210\u52a8\u6001\u611f\u77e5\u7684\u89c6\u89c9\u8f68\u8ff9\u3002", "result": "\u5728\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u5957\u4ef6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSuper-Mimic\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u8d85\u8fc720%\u3002", "conclusion": "\u5c06\u89c6\u9891\u9a71\u52a8\u7684\u610f\u56fe\u89e3\u6790\u4e0e\u524d\u77bb\u6027\u52a8\u6001\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u662f\u5f00\u53d1\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9ad8\u5ea6\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2509.22287", "categories": ["cs.RO", "cs.AI", "cs.HC", "I.2.7; H.5.2; K.3.1; J.4"], "pdf": "https://arxiv.org/pdf/2509.22287", "abs": "https://arxiv.org/abs/2509.22287", "authors": ["Stina Sundstedt", "Mattias Wingren", "Susanne H\u00e4gglund", "Daniel Ventus"], "title": "Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities", "comment": "12 pages, 2 figures, Preprint of: Sundstedt, S., Wingren, M.,\n  H\\\"agglund, S. & Ventus, D. (2025). Leveraging Large Language Models for\n  Robot-Assisted Learning of Morphological Structures in Preschool Children\n  with Language Vulnerabilities. In: Stephanidis, C., Antona, M., Ntoa, S. &\n  Salvendy, G. (eds.), Communications in Computer and Information Science, vol.\n  2523, pp. 415-425. Springer", "summary": "Preschool children with language vulnerabilities -- such as developmental\nlanguage disorders or immigration related language challenges -- often require\nsupport to strengthen their expressive language skills. Based on the principle\nof implicit learning, speech-language therapists (SLTs) typically embed target\nmorphological structures (e.g., third person -s) into everyday interactions or\ngame-based learning activities. Educators are recommended by SLTs to do the\nsame. This approach demands precise linguistic knowledge and real-time\nproduction of various morphological forms (e.g., \"Daddy wears these when he\ndrives to work\"). The task becomes even more demanding when educators or parent\nalso must keep children engaged and manage turn-taking in a game-based\nactivity. In the TalBot project our multiprofessional team have developed an\napplication in which the Furhat conversational robot plays the word retrieval\ngame \"Alias\" with children to improve language skills. Our application\ncurrently employs a large language model (LLM) to manage gameplay, dialogue,\naffective responses, and turn-taking. Our next step is to further leverage the\ncapacity of LLMs so the robot can generate and deliver specific morphological\ntargets during the game. We hypothesize that a robot could outperform humans at\nthis task. Novel aspects of this approach are that the robot could ultimately\nserve as a model and tutor for both children and professionals and that using\nLLM capabilities in this context would support basic communication needs for\nchildren with language vulnerabilities. Our long-term goal is to create a\nrobust LLM-based Robot-Assisted Language Learning intervention capable of\nteaching a variety of morphological structures across different languages.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4f7f\u7528Furhat\u5bf9\u8bdd\u673a\u5668\u4eba\u548cLLM\u7684\u673a\u5668\u4eba\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u5e94\u7528\uff0c\u901a\u8fc7\u6e38\u620f\u5316\u65b9\u5f0f\u5e2e\u52a9\u6709\u8bed\u8a00\u969c\u788d\u7684\u5b66\u9f84\u524d\u513f\u7ae5\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7279\u5b9a\u5f62\u6001\u7ed3\u6784\u7684\u5b66\u4e60\u3002", "motivation": "\u5e2e\u52a9\u6709\u8bed\u8a00\u969c\u788d\u7684\u5b66\u9f84\u524d\u513f\u7ae5\uff08\u5982\u53d1\u80b2\u6027\u8bed\u8a00\u969c\u788d\u6216\u79fb\u6c11\u76f8\u5173\u8bed\u8a00\u6311\u6218\uff09\u63d0\u9ad8\u8868\u8fbe\u6027\u8bed\u8a00\u6280\u80fd\uff0c\u51cf\u8f7b\u6559\u80b2\u5de5\u4f5c\u8005\u548c\u5bb6\u957f\u5728\u6e38\u620f\u5316\u6559\u5b66\u4e2d\u7684\u8d1f\u62c5\u3002", "method": "\u4f7f\u7528Furhat\u5bf9\u8bdd\u673a\u5668\u4eba\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\uff0c\u5728\u5355\u8bcd\u68c0\u7d22\u6e38\u620f\"Alias\"\u4e2d\u4e0e\u513f\u7ae5\u4e92\u52a8\uff0c\u7ba1\u7406\u6e38\u620f\u6d41\u7a0b\u3001\u5bf9\u8bdd\u3001\u60c5\u611f\u54cd\u5e94\u548c\u8f6e\u8f6c\uff0c\u5e76\u8ba1\u5212\u8fdb\u4e00\u6b65\u5229\u7528LLM\u751f\u6210\u548c\u4f20\u9012\u7279\u5b9a\u7684\u5f62\u6001\u76ee\u6807\u3002", "result": "\u76ee\u524d\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684\u6e38\u620f\u5e94\u7528\uff0c\u80fd\u591f\u7ba1\u7406\u6e38\u620f\u6d41\u7a0b\u548c\u4e92\u52a8\uff0c\u4e0b\u4e00\u6b65\u5c06\u6269\u5c55LLM\u80fd\u529b\u4ee5\u751f\u6210\u7279\u5b9a\u5f62\u6001\u76ee\u6807\u3002\u5047\u8bbe\u673a\u5668\u4eba\u5728\u8fd9\u65b9\u9762\u53ef\u80fd\u4f18\u4e8e\u4eba\u7c7b\u8868\u73b0\u3002", "conclusion": "\u957f\u671f\u76ee\u6807\u662f\u521b\u5efa\u5f3a\u5927\u7684\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u5e72\u9884\u7cfb\u7edf\uff0c\u80fd\u591f\u8de8\u4e0d\u540c\u8bed\u8a00\u6559\u6388\u5404\u79cd\u5f62\u6001\u7ed3\u6784\uff0c\u673a\u5668\u4eba\u53ef\u6210\u4e3a\u513f\u7ae5\u548c\u4e13\u4e1a\u4eba\u58eb\u7684\u699c\u6837\u548c\u5bfc\u5e08\u3002"}}
{"id": "2509.22288", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22288", "abs": "https://arxiv.org/abs/2509.22288", "authors": ["Johan Hatleskog", "Morten Nissov", "Kostas Alexis"], "title": "IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM", "comment": "8 pages, 7 figures, accepted by The 22nd International Conference on\n  Advanced Robotics (ICAR 2025). Supplementary video:\n  https://youtu.be/95jeWXBMN7c", "summary": "Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor\ngraph node per measurement to compensate for the lack of time synchronization\nbetween radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this\nstrategy results in a state creation rate of twice the individual sensor\nfrequencies. This doubling of the number of states per second yields high\noptimization costs, inhibiting real-time performance on resource-constrained\nhardware. We introduce IMU-preintegrated radar factors that use high-rate\ninertial data to propagate the most recent LiDAR state to the radar measurement\ntimestamp. This strategy maintains the node creation rate at the LiDAR\nmeasurement frequency. Assuming equal sensor rates, this lowers the number of\nnodes by 50 % and consequently the computational costs. Experiments on a single\nboard computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB\nRAM) show that our method preserves the absolute pose error of a conventional\nbaseline while simultaneously lowering the aggregated factor graph optimization\ntime by up to 56 %.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdIMU\u9884\u79ef\u5206\u96f7\u8fbe\u56e0\u5b50\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u9ad8\u9891\u60ef\u6027\u6570\u636e\u5c06\u6700\u65b0LiDAR\u72b6\u6001\u4f20\u64ad\u5230\u96f7\u8fbe\u6d4b\u91cf\u65f6\u95f4\u6233\uff0c\u5c06\u72b6\u6001\u521b\u5efa\u7387\u4fdd\u6301\u5728LiDAR\u6d4b\u91cf\u9891\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51cf\u5c1150%\u8282\u70b9\u6570\u91cf\uff0c\u5728\u4fdd\u6301\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u4f18\u5316\u65f6\u95f4\u964d\u4f4e56%\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u6ede\u540e\u96f7\u8fbe-LiDAR-\u60ef\u6027\u5e73\u6ed1\u5668\u4e3a\u6bcf\u4e2a\u6d4b\u91cf\u521b\u5efa\u56e0\u5b50\u56fe\u8282\u70b9\uff0c\u5bfc\u81f4\u72b6\u6001\u521b\u5efa\u7387\u7ffb\u500d\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u4ea7\u751f\u9ad8\u4f18\u5316\u6210\u672c\uff0c\u963b\u788d\u5b9e\u65f6\u6027\u80fd\u3002", "method": "\u5f15\u5165IMU\u9884\u79ef\u5206\u96f7\u8fbe\u56e0\u5b50\uff0c\u5229\u7528\u9ad8\u9891\u60ef\u6027\u6570\u636e\u5c06\u6700\u65b0LiDAR\u72b6\u6001\u4f20\u64ad\u5230\u96f7\u8fbe\u6d4b\u91cf\u65f6\u95f4\u6233\uff0c\u4fdd\u6301\u8282\u70b9\u521b\u5efa\u7387\u5728LiDAR\u6d4b\u91cf\u9891\u7387\u3002", "result": "\u5728\u5355\u677f\u8ba1\u7b97\u673a\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u4f20\u7edf\u57fa\u7ebf\u7684\u7edd\u5bf9\u4f4d\u59ff\u8bef\u5dee\uff0c\u540c\u65f6\u5c06\u805a\u5408\u56e0\u5b50\u56fe\u4f18\u5316\u65f6\u95f4\u964d\u4f4e\u9ad8\u8fbe56%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c1150%\u7684\u8282\u70b9\u6570\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2509.22296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22296", "abs": "https://arxiv.org/abs/2509.22296", "authors": ["Joseph Hunt", "Koyo Fujii", "Aly Magassouba", "Praminda Caleb-Solly"], "title": "Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm", "comment": "ICSR 2025, 8 pages, 3 figures", "summary": "Hospital patient falls remain a critical and costly challenge worldwide.\nWhile conventional fall prevention systems typically rely on post-fall\ndetection or reactive alerts, they also often suffer from high false positive\nrates and fail to address the underlying patient needs that lead to bed-exit\nattempts. This paper presents a novel system architecture that leverages the\nInternet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction\nfor proactive and personalized patient assistance. The system integrates a\nprivacy-preserving thermal sensing model capable of real-time bed-exit\nprediction, with two coordinated robotic agents that respond dynamically based\non predicted intent and patient input. This orchestrated response could not\nonly reduce fall risk but also attend to the patient's underlying motivations\nfor movement, such as thirst, discomfort, or the need for assistance, before a\nhazardous situation arises. Our contributions with this pilot study are\nthree-fold: (1) a modular IoRT-based framework enabling distributed sensing,\nprediction, and multi-robot coordination; (2) a demonstration of low-resolution\nthermal sensing for accurate, privacy-preserving preemptive bed-exit detection;\nand (3) results from a user study and systematic error analysis that inform the\ndesign of situationally aware, multi-agent interactions in hospital settings.\nThe findings highlight how interactive and connected robotic systems can move\nbeyond passive monitoring to deliver timely, meaningful assistance, empowering\nsafer, more responsive care environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u8054\u7f51\u673a\u5668\u4eba\u6280\u672f\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u534f\u8c03\u4eba-\u673a\u5668\u4eba-\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e3b\u52a8\u548c\u4e2a\u6027\u5316\u7684\u60a3\u8005\u8f85\u52a9\uff0c\u9884\u9632\u533b\u9662\u60a3\u8005\u8dcc\u5012\u3002", "motivation": "\u4f20\u7edf\u8dcc\u5012\u9884\u9632\u7cfb\u7edf\u4f9d\u8d56\u4e8b\u540e\u68c0\u6d4b\u6216\u88ab\u52a8\u8b66\u62a5\uff0c\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u4e14\u672a\u80fd\u89e3\u51b3\u5bfc\u81f4\u60a3\u8005\u79bb\u5e8a\u5c1d\u8bd5\u7684\u6839\u672c\u9700\u6c42\u3002", "method": "\u96c6\u6210\u9690\u79c1\u4fdd\u62a4\u7684\u70ed\u611f\u6d4b\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u79bb\u5e8a\u9884\u6d4b\uff0c\u534f\u8c03\u4e24\u4e2a\u673a\u5668\u4eba\u4ee3\u7406\u6839\u636e\u9884\u6d4b\u610f\u56fe\u548c\u60a3\u8005\u8f93\u5165\u52a8\u6001\u54cd\u5e94\u3002", "result": "\u5c55\u793a\u4e86\u4f4e\u5206\u8fa8\u7387\u70ed\u611f\u6d4b\u5728\u51c6\u786e\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u9884\u9632\u6027\u79bb\u5e8a\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u7cfb\u7edf\u8bef\u5dee\u5206\u6790\u4e3a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u548c\u8fde\u63a5\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u8d85\u8d8a\u88ab\u52a8\u76d1\u63a7\uff0c\u63d0\u4f9b\u53ca\u65f6\u3001\u6709\u610f\u4e49\u7684\u8f85\u52a9\uff0c\u521b\u9020\u66f4\u5b89\u5168\u3001\u54cd\u5e94\u66f4\u5feb\u7684\u62a4\u7406\u73af\u5883\u3002"}}
{"id": "2509.22356", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22356", "abs": "https://arxiv.org/abs/2509.22356", "authors": ["Enguang Liu", "Siyuan Liang", "Liming Lu", "Xiyu Zeng", "Xiaochun Cao", "Aishan Liu", "Shuchao Pang"], "title": "RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation", "comment": null, "summary": "The safety and reliability of embodied agents rely on accurate and unbiased\nvisual perception. However, existing benchmarks mainly emphasize generalization\nand robustness under perturbations, while systematic quantification of visual\nbias remains scarce. This gap limits a deeper understanding of how perception\ninfluences decision-making stability. To address this issue, we propose\nRoboView-Bias, the first benchmark specifically designed to systematically\nquantify visual bias in robotic manipulation, following a principle of factor\nisolation. Leveraging a structured variant-generation framework and a\nperceptual-fairness validation protocol, we create 2,127 task instances that\nenable robust measurement of biases induced by individual visual factors and\ntheir interactions. Using this benchmark, we systematically evaluate three\nrepresentative embodied agents across two prevailing paradigms and report three\nkey findings: (i) all agents exhibit significant visual biases, with camera\nviewpoint being the most critical factor; (ii) agents achieve their highest\nsuccess rates on highly saturated colors, indicating inherited visual\npreferences from underlying VLMs; and (iii) visual biases show strong,\nasymmetric coupling, with viewpoint strongly amplifying color-related bias.\nFinally, we demonstrate that a mitigation strategy based on a semantic\ngrounding layer substantially reduces visual bias by approximately 54.5\\% on\nMOKA. Our results highlight that systematic analysis of visual bias is a\nprerequisite for developing safe and reliable general-purpose embodied agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoboView-Bias\u57fa\u51c6\uff0c\u9996\u6b21\u7cfb\u7edf\u91cf\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u504f\u89c1\uff0c\u8bc4\u4f30\u4e86\u4e09\u4e2a\u4ee3\u8868\u6027\u5177\u8eab\u4ee3\u7406\uff0c\u53d1\u73b0\u6240\u6709\u4ee3\u7406\u90fd\u5b58\u5728\u663e\u8457\u89c6\u89c9\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u63a5\u5730\u5c42\u7684\u7f13\u89e3\u7b56\u7565\u53ef\u5c06\u504f\u89c1\u51cf\u5c11\u7ea654.5%\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6cdb\u5316\u6027\u548c\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u800c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u504f\u89c1\u7684\u7cfb\u7edf\u91cf\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u611f\u77e5\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u7a33\u5b9a\u6027\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u91c7\u7528\u56e0\u5b50\u9694\u79bb\u539f\u5219\uff0c\u5229\u7528\u7ed3\u6784\u5316\u53d8\u4f53\u751f\u6210\u6846\u67b6\u548c\u611f\u77e5\u516c\u5e73\u9a8c\u8bc1\u534f\u8bae\uff0c\u521b\u5efa\u4e862,127\u4e2a\u4efb\u52a1\u5b9e\u4f8b\uff0c\u7528\u4e8e\u7a33\u5065\u6d4b\u91cf\u5355\u4e2a\u89c6\u89c9\u56e0\u7d20\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u5f15\u8d77\u7684\u504f\u89c1\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff1a(i)\u6240\u6709\u4ee3\u7406\u90fd\u8868\u73b0\u51fa\u663e\u8457\u89c6\u89c9\u504f\u89c1\uff0c\u76f8\u673a\u89c6\u89d2\u662f\u6700\u5173\u952e\u56e0\u7d20\uff1b(ii)\u4ee3\u7406\u5728\u9ad8\u5ea6\u9971\u548c\u989c\u8272\u4e0a\u83b7\u5f97\u6700\u9ad8\u6210\u529f\u7387\uff0c\u8868\u660e\u7ee7\u627f\u4e86\u5e95\u5c42VLM\u7684\u89c6\u89c9\u504f\u597d\uff1b(iii)\u89c6\u89c9\u504f\u89c1\u8868\u73b0\u51fa\u5f3a\u4e0d\u5bf9\u79f0\u8026\u5408\uff0c\u89c6\u89d2\u5f3a\u70c8\u653e\u5927\u989c\u8272\u76f8\u5173\u504f\u89c1\u3002", "conclusion": "\u7cfb\u7edf\u5206\u6790\u89c6\u89c9\u504f\u89c1\u662f\u5f00\u53d1\u5b89\u5168\u53ef\u9760\u901a\u7528\u5177\u8eab\u4ee3\u7406\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u53ef\u663e\u8457\u51cf\u5c11\u89c6\u89c9\u504f\u89c1\u3002"}}
{"id": "2509.22421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22421", "abs": "https://arxiv.org/abs/2509.22421", "authors": ["Leonel Giacobbe", "Jingdao Chen", "Chuangchuang Sun"], "title": "Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping", "comment": null, "summary": "Grasping is a core task in robotics with various applications. However, most\ncurrent implementations are primarily designed for rigid items, and their\nperformance drops considerably when handling fragile or deformable materials\nthat require real-time feedback. Meanwhile, tactile-reactive grasping focuses\non a single agent, which limits their ability to grasp and manipulate large,\nheavy objects. To overcome this, we propose a learning-based, tactile-reactive\nmulti-agent Model Predictive Controller (MPC) for grasping a wide range of\nobjects with different softness and shapes, beyond the capabilities of\npreexisting single-agent implementations. Our system uses two Gelsight Mini\ntactile sensors [1] to extract real-time information on object texture and\nstiffness. This rich tactile feedback is used to estimate contact dynamics and\nobject compliance in real time, enabling the system to adapt its control policy\nto diverse object geometries and stiffness profiles. The learned controller\noperates in a closed loop, leveraging tactile encoding to predict grasp\nstability and adjust force and position accordingly. Our key technical\ncontributions include a multi-agent MPC formulation trained on real contact\ninteractions, a tactile-data driven method for inferring grasping states, and a\ncoordination strategy that enables collaborative control. By combining tactile\nsensing and a learning-based multi-agent MPC, our method offers a robust,\nintelligent solution for collaborative grasping in complex environments,\nsignificantly advancing the capabilities of multi-agent systems. Our approach\nis validated through extensive experiments against independent PD and MPC\nbaselines. Our pipeline outperforms the baselines regarding success rates in\nachieving and maintaining stable grasps across objects of varying sizes and\nstiffness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u89e6\u89c9\u53cd\u5e94\u591a\u667a\u80fd\u4f53\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u6293\u53d6\u5404\u79cd\u8f6f\u786c\u5ea6\u548c\u5f62\u72b6\u7684\u7269\u4f53\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u5355\u667a\u80fd\u4f53\u5b9e\u73b0\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u6293\u53d6\u5b9e\u73b0\u4e3b\u8981\u9488\u5bf9\u521a\u6027\u7269\u4f53\uff0c\u5728\u5904\u7406\u9700\u8981\u5b9e\u65f6\u53cd\u9988\u7684\u6613\u788e\u6216\u53ef\u53d8\u5f62\u6750\u6599\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u5355\u667a\u80fd\u4f53\u89e6\u89c9\u53cd\u5e94\u6293\u53d6\u65e0\u6cd5\u5904\u7406\u5927\u578b\u91cd\u7269\u3002", "method": "\u4f7f\u7528\u4e24\u4e2aGelsight Mini\u89e6\u89c9\u4f20\u611f\u5668\u63d0\u53d6\u7269\u4f53\u7eb9\u7406\u548c\u521a\u5ea6\u7684\u5b9e\u65f6\u4fe1\u606f\uff0c\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u4f30\u8ba1\u63a5\u89e6\u52a8\u529b\u5b66\u548c\u7269\u4f53\u67d4\u987a\u6027\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53MPC\u516c\u5f0f\u5728\u771f\u5b9e\u63a5\u89e6\u4ea4\u4e92\u4e0a\u8bad\u7ec3\uff0c\u7ed3\u5408\u89e6\u89c9\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u63a8\u65ad\u6293\u53d6\u72b6\u6001\u548c\u534f\u4f5c\u63a7\u5236\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u548c\u7ef4\u6301\u4e0d\u540c\u5c3a\u5bf8\u548c\u521a\u5ea6\u7269\u4f53\u7684\u7a33\u5b9a\u6293\u53d6\u65b9\u9762\uff0c\u6210\u529f\u7387\u4f18\u4e8e\u72ec\u7acb\u7684PD\u548cMPC\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u89e6\u89c9\u4f20\u611f\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53MPC\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u6293\u53d6\u63d0\u4f9b\u4e86\u9c81\u68d2\u667a\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u80fd\u529b\u3002"}}
{"id": "2509.22434", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22434", "abs": "https://arxiv.org/abs/2509.22434", "authors": ["Margherita Martorana", "Francesca Urgese", "Ilaria Tiddi", "Stefan Schlobach"], "title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics", "comment": null, "summary": "Personal service robots are increasingly used in domestic settings to assist\nolder adults and people requiring support. Effective operation involves not\nonly physical interaction but also the ability to interpret dynamic\nenvironments, understand tasks, and choose appropriate actions based on\ncontext. This requires integrating both hardware components (e.g. sensors,\nactuators) and software systems capable of reasoning about tasks, environments,\nand robot capabilities. Frameworks such as the Robot Operating System (ROS)\nprovide open-source tools that help connect low-level hardware with\nhigher-level functionalities. However, real-world deployments remain tightly\ncoupled to specific platforms. As a result, solutions are often isolated and\nhard-coded, limiting interoperability, reusability, and knowledge sharing.\nOntologies and knowledge graphs offer a structured way to represent tasks,\nenvironments, and robot capabilities. Existing ontologies, such as the\nSocio-physical Model of Activities (SOMA) and the Descriptive Ontology for\nLinguistic and Cognitive Engineering (DOLCE), provide models for activities,\nspatial relationships, and reasoning structures. However, they often focus on\nspecific domains and do not fully capture the connection between environment,\naction, robot capabilities, and system-level integration. In this work, we\npropose the Ontology for roBOts and acTions (OntoBOT), which extends existing\nontologies to provide a unified representation of tasks, actions, environments,\nand capabilities. Our contributions are twofold: (1) we unify these aspects\ninto a cohesive ontology to support formal reasoning about task execution, and\n(2) we demonstrate its generalizability by evaluating competency questions\nacross four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how\nOntoBOT enables context-aware reasoning, task-oriented execution, and knowledge\nsharing in service robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OntoBOT\u672c\u4f53\uff0c\u7528\u4e8e\u7edf\u4e00\u8868\u793a\u670d\u52a1\u673a\u5668\u4eba\u4e2d\u7684\u4efb\u52a1\u3001\u52a8\u4f5c\u3001\u73af\u5883\u548c\u80fd\u529b\uff0c\u652f\u6301\u5f62\u5f0f\u5316\u63a8\u7406\u548c\u77e5\u8bc6\u5171\u4eab\u3002", "motivation": "\u73b0\u6709\u670d\u52a1\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4e0e\u7279\u5b9a\u5e73\u53f0\u7d27\u5bc6\u8026\u5408\uff0c\u5bfc\u81f4\u5b64\u7acb\u3001\u786c\u7f16\u7801\u7684\u7cfb\u7edf\uff0c\u9650\u5236\u4e86\u4e92\u64cd\u4f5c\u6027\u3001\u53ef\u91cd\u7528\u6027\u548c\u77e5\u8bc6\u5171\u4eab\u3002\u73b0\u6709\u672c\u4f53\u8bba\u5982SOMA\u548cDOLCE\u4e13\u6ce8\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u73af\u5883\u3001\u52a8\u4f5c\u3001\u673a\u5668\u4eba\u80fd\u529b\u548c\u7cfb\u7edf\u7ea7\u96c6\u6210\u4e4b\u95f4\u7684\u8fde\u63a5\u3002", "method": "\u6269\u5c55\u73b0\u6709\u672c\u4f53\u8bba\uff0c\u63d0\u51faOntoBOT\u672c\u4f53\uff0c\u63d0\u4f9b\u4efb\u52a1\u3001\u52a8\u4f5c\u3001\u73af\u5883\u548c\u80fd\u529b\u7684\u7edf\u4e00\u8868\u793a\u3002\u901a\u8fc7\u8bc4\u4f30\u56db\u4e2a\u5177\u4f53\u4ee3\u7406\uff08TIAGo\u3001HSR\u3001UR3\u548cStretch\uff09\u7684\u80fd\u529b\u95ee\u9898\u6765\u9a8c\u8bc1\u5176\u901a\u7528\u6027\u3002", "result": "OntoBOT\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u9762\u5411\u4efb\u52a1\u7684\u6267\u884c\u548c\u77e5\u8bc6\u5171\u4eab\uff0c\u5728\u56db\u4e2a\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "OntoBOT\u672c\u4f53\u4e3a\u670d\u52a1\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u77e5\u8bc6\u5171\u4eab\u9650\u5236\uff0c\u652f\u6301\u8de8\u5e73\u53f0\u7684\u4efb\u52a1\u6267\u884c\u548c\u63a8\u7406\u3002"}}
{"id": "2509.22441", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22441", "abs": "https://arxiv.org/abs/2509.22441", "authors": ["Zhangyuan Wang", "Yunpeng Zhu", "Yuqi Yan", "Xiaoyuan Tian", "Xinhao Shao", "Meixuan Li", "Weikun Li", "Guangsheng Su", "Weicheng Cui", "Dixia Fan"], "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation", "comment": "This paper introduces the first VLA framework for AUVs, featuring a\n  dual-brain architecture and zero-data MPC for real-world underwater\n  navigation", "summary": "This paper presents UnderwaterVLA, a novel framework for autonomous\nunderwater navigation that integrates multimodal foundation models with\nembodied intelligence systems. Underwater operations remain difficult due to\nhydrodynamic disturbances, limited communication bandwidth, and degraded\nsensing in turbid waters. To address these challenges, we introduce three\ninnovations. First, a dual-brain architecture decouples high-level mission\nreasoning from low-level reactive control, enabling robust operation under\ncommunication and computational constraints. Second, we apply\nVision-Language-Action(VLA) models to underwater robotics for the first time,\nincorporating structured chain-of-thought reasoning for interpretable\ndecision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)\nscheme compensates for fluid effects in real time without costly task-specific\ntraining. Experimental results in field tests show that UnderwaterVLA reduces\nnavigation errors in degraded visual conditions while maintaining higher task\ncompletion by 19% to 27% over baseline. By minimizing reliance on\nunderwater-specific training data and improving adaptability across\nenvironments, UnderwaterVLA provides a scalable and cost-effective path toward\nthe next generation of intelligent AUVs.", "AI": {"tldr": "UnderwaterVLA\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u6c34\u4e0b\u5bfc\u822a\u7684\u65b0\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u8111\u67b6\u6784\u3001VLA\u6a21\u578b\u548c\u6d41\u4f53\u529b\u5b66MPC\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6076\u52a3\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u6c34\u4e0b\u4f5c\u4e1a\u9762\u4e34\u6d41\u4f53\u52a8\u529b\u5b66\u5e72\u6270\u3001\u6709\u9650\u901a\u4fe1\u5e26\u5bbd\u548c\u6d51\u6d4a\u6c34\u57df\u611f\u77e5\u9000\u5316\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u8fd9\u4e9b\u6076\u52a3\u6761\u4ef6\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u53cc\u8111\u67b6\u6784\u5206\u79bb\u9ad8\u5c42\u4efb\u52a1\u63a8\u7406\u4e0e\u4f4e\u5c42\u53cd\u5e94\u63a7\u5236\uff1b\u9996\u6b21\u5c06\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u5e94\u7528\u4e8e\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63a8\u7406\uff1b\u4f7f\u7528\u6d41\u4f53\u529b\u5b66\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u65b9\u6848\u5b9e\u65f6\u8865\u507f\u6d41\u4f53\u6548\u5e94\u3002", "result": "\u73b0\u573a\u6d4b\u8bd5\u663e\u793a\uff0cUnderwaterVLA\u5728\u89c6\u89c9\u6761\u4ef6\u9000\u5316\u65f6\u51cf\u5c11\u4e86\u5bfc\u822a\u8bef\u5dee\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8619%\u523027%\u3002", "conclusion": "\u901a\u8fc7\u51cf\u5c11\u5bf9\u6c34\u4e0b\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u5e76\u63d0\u9ad8\u8de8\u73af\u5883\u9002\u5e94\u6027\uff0cUnderwaterVLA\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fdAUV\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22469", "abs": "https://arxiv.org/abs/2509.22469", "authors": ["Ben Rossano", "Jaein Lim", "Jonathan P. How"], "title": "Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards", "comment": "8 pages", "summary": "This paper proposes a task allocation algorithm for teams of heterogeneous\nrobots in environments with uncertain task requirements. We model these\nrequirements as probability distributions over capabilities and use this model\nto allocate tasks such that robots with complementary skills naturally position\nnear uncertain tasks, proactively mitigating task failures without wasting\nresources. We introduce a market-based approach that optimizes the joint team\nobjective while explicitly capturing coupled rewards between robots, offering a\npolynomial-time solution in decentralized settings with strict communication\nassumptions. Comparative experiments against benchmark algorithms demonstrate\nthe effectiveness of our approach and highlight the challenges of incorporating\ncoupled rewards in a decentralized formulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u4efb\u52a1\u5206\u914d\u7b97\u6cd5\uff0c\u5728\u4efb\u52a1\u9700\u6c42\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u548c\u57fa\u4e8e\u5e02\u573a\u7684\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u5206\u914d\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u4efb\u52a1\u9700\u6c42\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5206\u914d\u95ee\u9898\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u540c\u65f6\u4e3b\u52a8\u9884\u9632\u4efb\u52a1\u5931\u8d25\u3002", "method": "\u4f7f\u7528\u6982\u7387\u5206\u5e03\u5efa\u6a21\u4efb\u52a1\u9700\u6c42\uff0c\u91c7\u7528\u57fa\u4e8e\u5e02\u573a\u7684\u65b9\u6cd5\u4f18\u5316\u56e2\u961f\u8054\u5408\u76ee\u6807\uff0c\u663e\u5f0f\u6355\u6349\u673a\u5668\u4eba\u95f4\u7684\u8026\u5408\u5956\u52b1\uff0c\u63d0\u4f9b\u591a\u9879\u5f0f\u65f6\u95f4\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u4e0e\u57fa\u51c6\u7b97\u6cd5\u76f8\u6bd4\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u540c\u65f6\u7a81\u663e\u4e86\u5728\u5206\u6563\u5f0f\u5236\u5b9a\u4e2d\u7eb3\u5165\u8026\u5408\u5956\u52b1\u7684\u6311\u6218\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5728\u4e25\u683c\u901a\u4fe1\u5047\u8bbe\u7684\u5206\u6563\u73af\u5883\u4e2d\u6709\u6548\u5904\u7406\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u5e73\u8861\u8d44\u6e90\u5229\u7528\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2509.22493", "categories": ["cs.RO", "cs.AI", "cs.IR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.22493", "abs": "https://arxiv.org/abs/2509.22493", "authors": ["Alberto Olivares-Alarcos", "Sergi Foix", "J\u00falia Borr\u00e0s", "Gerard Canal", "Guillem Aleny\u00e0"], "title": "Ontological foundations for contrastive explanatory narration of robot plans", "comment": "This version was submitted to the journal Information Sciences and is\n  under review since October 2024", "summary": "Mutual understanding of artificial agents' decisions is key to ensuring a\ntrustworthy and successful human-robot interaction. Hence, robots are expected\nto make reasonable decisions and communicate them to humans when needed. In\nthis article, the focus is on an approach to modeling and reasoning about the\ncomparison of two competing plans, so that robots can later explain the\ndivergent result. First, a novel ontological model is proposed to formalize and\nreason about the differences between competing plans, enabling the\nclassification of the most appropriate one (e.g., the shortest, the safest, the\nclosest to human preferences, etc.). This work also investigates the\nlimitations of a baseline algorithm for ontology-based explanatory narration.\nTo address these limitations, a novel algorithm is presented, leveraging\ndivergent knowledge between plans and facilitating the construction of\ncontrastive narratives. Through empirical evaluation, it is observed that the\nexplanations excel beyond the baseline method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u6a21\u578b\u548c\u7b97\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83\u4e24\u4e2a\u7ade\u4e89\u6027\u8ba1\u5212\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u89e3\u91ca\u4e3a\u4f55\u9009\u62e9\u67d0\u4e2a\u8ba1\u5212\u800c\u975e\u53e6\u4e00\u4e2a\u3002", "motivation": "\u786e\u4fdd\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u80fd\u591f\u88ab\u7406\u89e3\uff0c\u5efa\u7acb\u4fe1\u4efb\u5173\u7cfb\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u505a\u51fa\u5408\u7406\u51b3\u7b56\u5e76\u5728\u9700\u8981\u65f6\u5411\u4eba\u7c7b\u89e3\u91ca\u3002", "method": "\u5f00\u53d1\u4e86\u65b0\u9896\u7684\u672c\u4f53\u6a21\u578b\u6765\u5f62\u5f0f\u5316\u548c\u63a8\u7406\u7ade\u4e89\u8ba1\u5212\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8ba1\u5212\u95f4\u5206\u6b67\u77e5\u8bc6\u7684\u65b0\u7b97\u6cd5\u6765\u6784\u5efa\u5bf9\u6bd4\u6027\u53d9\u8ff0\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u89e3\u91ca\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u673a\u5668\u4eba\u89e3\u91ca\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6027\u53d9\u8ff0\u589e\u5f3a\u4e86\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2509.22498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22498", "abs": "https://arxiv.org/abs/2509.22498", "authors": ["Katrina Ashton", "Chahyon Ku", "Shrey Shah", "Wen Jiang", "Kostas Daniilidis", "Bernadette Bucher"], "title": "HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes", "comment": null, "summary": "Language-specified mobile manipulation tasks in novel environments\nsimultaneously face challenges interacting with a scene which is only partially\nobserved, grounding semantic information from language instructions to the\npartially observed scene, and actively updating knowledge of the scene with new\nobservations. To address these challenges, we propose HELIOS, a hierarchical\nscene representation and associated search objective to perform language\nspecified pick and place mobile manipulation tasks. We construct 2D maps\ncontaining the relevant semantic and occupancy information for navigation while\nsimultaneously actively constructing 3D Gaussian representations of\ntask-relevant objects. We fuse observations across this multi-layered\nrepresentation while explicitly modeling the multi-view consistency of the\ndetections of each object. In order to efficiently search for the target\nobject, we formulate an objective function balancing exploration of unobserved\nor uncertain regions with exploitation of scene semantic information. We\nevaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and\nplace benchmark in which perception is challenging due to large and complex\nscenes with comparatively small target objects. HELIOS achieves\nstate-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also\ntransfer to the real world without requiring additional data, as we illustrate\nby demonstrating it in a real world office environment on a Spot robot.", "AI": {"tldr": "HELIOS\u662f\u4e00\u4e2a\u5206\u5c42\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8bed\u8a00\u6307\u5b9a\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u5728\u90e8\u5206\u89c2\u5bdf\u73af\u5883\u4e2d\u901a\u8fc7\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u6765\u9ad8\u6548\u641c\u7d22\u76ee\u6807\u7269\u4f53\uff0c\u5728OVMM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u5728\u90e8\u5206\u89c2\u5bdf\u7684\u65b0\u73af\u5883\u4e2d\u6267\u884c\u8bed\u8a00\u6307\u5b9a\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u65f6\u9762\u4e34\u7684\u6311\u6218\uff1a\u573a\u666f\u90e8\u5206\u53ef\u89c1\u3001\u8bed\u8a00\u6307\u4ee4\u5230\u90e8\u5206\u89c2\u5bdf\u573a\u666f\u7684\u8bed\u4e49\u63a5\u5730\u3001\u4ee5\u53ca\u4e3b\u52a8\u66f4\u65b0\u573a\u666f\u77e5\u8bc6\u3002", "method": "\u6784\u5efa\u5305\u542b\u8bed\u4e49\u548c\u5360\u636e\u4fe1\u606f\u76842D\u5730\u56fe\uff0c\u540c\u65f6\u4e3b\u52a8\u6784\u5efa\u4efb\u52a1\u76f8\u5173\u5bf9\u8c61\u76843D\u9ad8\u65af\u8868\u793a\uff0c\u878d\u5408\u591a\u5c42\u8868\u793a\u5e76\u663e\u5f0f\u5efa\u6a21\u6bcf\u4e2a\u5bf9\u8c61\u68c0\u6d4b\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5236\u5b9a\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u7684\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728Habitat\u6a21\u62df\u5668\u7684OVMM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u80fd\u591f\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\uff0c\u5728Spot\u673a\u5668\u4eba\u4e0a\u6210\u529f\u6f14\u793a\u3002", "conclusion": "HELIOS\u901a\u8fc7\u5206\u5c42\u573a\u666f\u8868\u793a\u548c\u6709\u6548\u7684\u641c\u7d22\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u90e8\u5206\u89c2\u5bdf\u73af\u5883\u4e2d\u7684\u8bed\u8a00\u6307\u5b9a\u79fb\u52a8\u64cd\u4f5c\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.22550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22550", "abs": "https://arxiv.org/abs/2509.22550", "authors": ["Xiaoyun Qiu", "Haichao Liu", "Yue Pan", "Jun Ma", "Xinhu Zheng"], "title": "An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment", "comment": null, "summary": "In mixed-traffic environments, where autonomous vehicles (AVs) interact with\ndiverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous\nbehaviors make safe and efficient lane change maneuvers highly challenging.\nExisting methods often oversimplify these interactions by assuming uniform\npatterns. We propose an intention-driven lane change framework that integrates\ndriving-style recognition, cooperation-aware decision-making, and coordinated\nmotion planning. A deep learning classifier trained on the NGSIM dataset\nidentifies human driving styles in real time. A cooperation score with\nintrinsic and interactive components estimates surrounding drivers' intentions\nand quantifies their willingness to cooperate with the ego vehicle.\nDecision-making combines behavior cloning with inverse reinforcement learning\nto determine whether a lane change should be initiated. For trajectory\ngeneration, model predictive control is integrated with IRL-based intention\ninference to produce collision-free and socially compliant maneuvers.\nExperiments show that the proposed model achieves 94.2\\% accuracy and 94.3\\%\nF1-score, outperforming rule-based and learning-based baselines by 4-15\\% in\nlane change recognition. These results highlight the benefit of modeling\ninter-driver heterogeneity and demonstrate the potential of the framework to\nadvance context-aware and human-like autonomous driving in complex traffic\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u610f\u56fe\u9a71\u52a8\u7684\u8f66\u9053\u53d8\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u9a7e\u9a76\u98ce\u683c\u8bc6\u522b\u3001\u5408\u4f5c\u611f\u77e5\u51b3\u7b56\u548c\u534f\u8c03\u8fd0\u52a8\u89c4\u5212\uff0c\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u8f66\u9053\u53d8\u6362\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u4ea4\u4e92\u7684\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\uff0c\u4e0d\u53ef\u9884\u6d4b\u7684\u610f\u56fe\u548c\u5f02\u8d28\u884c\u4e3a\u4f7f\u5f97\u5b89\u5168\u9ad8\u6548\u7684\u8f66\u9053\u53d8\u6362\u6781\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u8fc7\u5ea6\u7b80\u5316\u8fd9\u4e9b\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u5b9e\u65f6\u8bc6\u522b\u4eba\u7c7b\u9a7e\u9a76\u98ce\u683c\uff1b\u901a\u8fc7\u5305\u542b\u5185\u5728\u548c\u4ea4\u4e92\u7ec4\u4ef6\u7684\u5408\u4f5c\u5206\u6570\u4f30\u8ba1\u5468\u56f4\u9a7e\u9a76\u5458\u7684\u610f\u56fe\uff1b\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548c\u9006\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u51b3\u7b56\uff1b\u96c6\u6210\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548cIRL\u610f\u56fe\u63a8\u65ad\u751f\u6210\u8f68\u8ff9\u3002", "result": "\u6a21\u578b\u8fbe\u523094.2%\u51c6\u786e\u7387\u548c94.3% F1\u5206\u6570\uff0c\u5728\u8f66\u9053\u53d8\u6362\u8bc6\u522b\u65b9\u9762\u6bd4\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa4-15%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u5efa\u6a21\u9a7e\u9a76\u5458\u5f02\u8d28\u6027\u7684\u76ca\u5904\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u63a8\u8fdb\u60c5\u5883\u611f\u77e5\u548c\u7c7b\u4eba\u81ea\u52a8\u9a7e\u9a76\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.22573", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22573", "abs": "https://arxiv.org/abs/2509.22573", "authors": ["Farida Mohsen", "Ali Safa"], "title": "MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data", "comment": null, "summary": "Efficiently detecting human intent to interact with ubiquitous robots is\ncrucial for effective human-robot interaction (HRI) and collaboration. Over the\npast decade, deep learning has gained traction in this field, with most\nexisting approaches relying on multimodal inputs, such as RGB combined with\ndepth (RGB-D), to classify time-sequence windows of sensory data as interactive\nor non-interactive. In contrast, we propose a novel RGB-only pipeline for\npredicting human interaction intent with frame-level precision, enabling faster\nrobot responses and improved service quality. A key challenge in intent\nprediction is the class imbalance inherent in real-world HRI datasets, which\ncan hinder the model's training and generalization. To address this, we\nintroduce MINT-RVAE, a synthetic sequence generation method, along with new\nloss functions and training strategies that enhance generalization on\nout-of-sample data. Our approach achieves state-of-the-art performance (AUROC:\n0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB\ninput and supporting precise frame onset prediction. Finally, to support future\nresearch, we openly release our new dataset with frame-level labeling of human\ninteraction intent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528RGB\u8f93\u5165\u7684\u5e27\u7ea7\u7cbe\u5ea6\u4eba\u7c7b\u4ea4\u4e92\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7MINT-RVAE\u5408\u6210\u5e8f\u5217\u751f\u6210\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982RGB-D\uff09\uff0c\u800c\u4ec5\u4f7f\u7528RGB\u8f93\u5165\u53ef\u4ee5\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u5e76\u5b9e\u73b0\u66f4\u5feb\u7684\u673a\u5668\u4eba\u54cd\u5e94\u3002\u540c\u65f6\uff0c\u771f\u5b9e\u4e16\u754cHRI\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4f1a\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86RGB-only\u7684\u610f\u56fe\u9884\u6d4b\u6d41\u7a0b\uff0c\u5305\u62ecMINT-RVAE\u5408\u6210\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\u3001\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u5bf9\u6837\u672c\u5916\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\uff08AUROC\uff1a0.95\uff09\uff0c\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff08AUROC\uff1a0.90-0.912\uff09\uff0c\u540c\u65f6\u4ec5\u9700RGB\u8f93\u5165\u5e76\u652f\u6301\u7cbe\u786e\u7684\u5e27\u7ea7\u8d77\u59cb\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528RGB\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u4ea4\u4e92\u610f\u56fe\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u516c\u5f00\u4e86\u65b0\u7684\u5e27\u7ea7\u6807\u6ce8\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.22578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22578", "abs": "https://arxiv.org/abs/2509.22578", "authors": ["Yuan Xu", "Jiabing Yang", "Xiaofeng Wang", "Yixiang Chen", "Zheng Zhu", "Bowen Fang", "Guan Huang", "Xinze Chen", "Yun Ye", "Qiang Zhang", "Peiyan Li", "Xiangnan Wu", "Kai Wang", "Bing Zhan", "Shuo Lu", "Jing Liu", "Nianfeng Liu", "Yan Huang", "Liang Wang"], "title": "EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation", "comment": null, "summary": "Imitation learning based policies perform well in robotic manipulation, but\nthey often degrade under *egocentric viewpoint shifts* when trained from a\nsingle egocentric viewpoint. To address this issue, we present **EgoDemoGen**,\na framework that generates *paired* novel egocentric demonstrations by\nretargeting actions in the novel egocentric frame and synthesizing the\ncorresponding egocentric observation videos with proposed generative video\nrepair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint\nreprojected scene video and a robot-only video rendered from the retargeted\njoint actions. EgoViewTransfer is finetuned from a pretrained video generation\nmodel using self-supervised double reprojection strategy. We evaluate\nEgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After\ntraining with a mixture of EgoDemoGen-generated novel egocentric demonstrations\nand original standard egocentric demonstrations, policy success rate improves\n**absolutely** by **+17.0%** for standard egocentric viewpoint and by\n**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,\nthe **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,\nperformance continues to improve as the proportion of EgoDemoGen-generated\ndemonstrations increases, with diminishing returns. These results demonstrate\nthat EgoDemoGen provides a practical route to egocentric viewpoint-robust\nrobotic manipulation.", "AI": {"tldr": "\u63d0\u51faEgoDemoGen\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u914d\u5bf9\u7684\u65b0\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u6f14\u793a\u6765\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u4e0d\u540c\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u64cd\u4f5c\u6210\u529f\u7387\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u5355\u4e00\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u8bad\u7ec3\u65f6\uff0c\u5728\u9762\u5bf9\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u53d8\u5316\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89d2\u53d8\u5316\u5e26\u6765\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u5f00\u53d1EgoDemoGen\u6846\u67b6\uff0c\u5305\u542bEgoViewTransfer\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u5b9a\u5411\u52a8\u4f5c\u5230\u65b0\u7684\u81ea\u6211\u4e2d\u5fc3\u5750\u6807\u7cfb\uff0c\u5e76\u5408\u6210\u5bf9\u5e94\u7684\u81ea\u6211\u4e2d\u5fc3\u89c2\u5bdf\u89c6\u9891\u3002\u4f7f\u7528\u81ea\u76d1\u7763\u53cc\u91cd\u91cd\u6295\u5f71\u7b56\u7565\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u6807\u51c6\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u6210\u529f\u7387\u7edd\u5bf9\u63d0\u534717.0%\uff0c\u65b0\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u63d0\u534717.7%\uff1b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\uff0c\u7edd\u5bf9\u63d0\u5347\u5206\u522b\u4e3a18.3%\u548c25.8%\u3002\u6027\u80fd\u968fEgoDemoGen\u751f\u6210\u6f14\u793a\u6bd4\u4f8b\u589e\u52a0\u800c\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "EgoDemoGen\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u9c81\u68d2\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7b56\u7565\u5728\u4e0d\u540c\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.22642", "categories": ["cs.RO", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.22642", "abs": "https://arxiv.org/abs/2509.22642", "authors": ["Xiaowei Chi", "Peidong Jia", "Chun-Kai Fan", "Xiaozhu Ju", "Weishi Mi", "Kevin Zhang", "Zhiyuan Qin", "Wanxin Tian", "Kuangzhi Ge", "Hao Li", "Zezhong Qian", "Anthony Chen", "Qiang Zhou", "Yueru Jia", "Jiaming Liu", "Yong Dai", "Qingpo Wuwu", "Chengyu Bai", "Yu-Kai Wang", "Ying Li", "Lizhang Chen", "Yong Bao", "Zhiyuan Jiang", "Jiacheng Zhu", "Kai Tang", "Ruichuan An", "Yulin Luo", "Qiuxuan Feng", "Siyuan Zhou", "Chi-min Chan", "Chengkai Hou", "Wei Xue", "Sirui Han", "Yike Guo", "Shanghang Zhang", "Jian Tang"], "title": "WoW: Towards a World omniscient World model Through Embodied Interaction", "comment": null, "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86WoW\u6a21\u578b\uff0c\u901a\u8fc7200\u4e07\u673a\u5668\u4eba\u4ea4\u4e92\u8f68\u8ff9\u8bad\u7ec3\uff0c\u8bc1\u660e\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u662f\u53d1\u5c55AI\u7269\u7406\u76f4\u89c9\u7684\u5173\u952e\u3002\u6a21\u578b\u80fd\u751f\u6210\u7269\u7406\u4e00\u81f4\u89c6\u9891\uff0c\u5e76\u901a\u8fc7SOPHIA\u7cfb\u7edf\u7ea6\u675f\u7269\u7406\u771f\u5b9e\u6027\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u4e3b\u52a8\u4ea4\u4e92\u53d1\u5c55\u7269\u7406\u76f4\u89c9\uff0c\u800c\u5f53\u524d\u89c6\u9891\u6a21\u578b\uff08\u5982Sora\uff09\u4ec5\u4f9d\u8d56\u88ab\u52a8\u89c2\u5bdf\uff0c\u96be\u4ee5\u7406\u89e3\u7269\u7406\u56e0\u679c\u5173\u7cfb\u3002\u5047\u8bbe\u771f\u5b9e\u7269\u7406\u76f4\u89c9\u5fc5\u987b\u57fa\u4e8e\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u4e30\u5bcc\u56e0\u679c\u4ea4\u4e92\u3002", "method": "\u5f00\u53d1\u4e86140\u4ebf\u53c2\u6570\u7684WoW\u751f\u6210\u4e16\u754c\u6a21\u578b\uff0c\u5728200\u4e07\u673a\u5668\u4eba\u4ea4\u4e92\u8f68\u8ff9\u4e0a\u8bad\u7ec3\u3002\u4f7f\u7528SOPHIA\u7cfb\u7edf\uff08\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff09\u8bc4\u4f30DiT\u751f\u6210\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6f14\u5316\u8bed\u8a00\u6307\u4ee4\u5f15\u5bfc\u4f18\u5316\u3002\u540c\u65f6\u8bad\u7ec3\u9006\u52a8\u529b\u5b66\u6a21\u578b\u5c06\u7cbe\u70bc\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u6a21\u578b\u5bf9\u7269\u7406\u7684\u7406\u89e3\u662f\u53ef\u80fd\u7ed3\u679c\u7684\u6982\u7387\u5206\u5e03\uff0c\u4f1a\u4ea7\u751f\u968f\u673a\u4e0d\u7a33\u5b9a\u6027\u548c\u7269\u7406\u5e7b\u89c9\u3002\u5728WoWBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u7269\u7406\u4e00\u81f4\u6027\u548c\u56e0\u679c\u63a8\u7406\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u7269\u7406\u56e0\u679c\u5173\u7cfb\u3001\u78b0\u649e\u52a8\u529b\u5b66\u548c\u7269\u4f53\u6301\u4e45\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u662f\u53d1\u5c55AI\u7269\u7406\u76f4\u89c9\u7684\u57fa\u77f3\u3002\u6a21\u578b\u3001\u6570\u636e\u548c\u57fa\u51c6\u5c06\u5f00\u6e90\u3002"}}
{"id": "2509.22643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22643", "abs": "https://arxiv.org/abs/2509.22643", "authors": ["Wenkai Guo", "Guanxing Lu", "Haoyuan Deng", "Zhenyu Wu", "Yansong Tang", "Ziwei Wang"], "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search", "comment": "9 pages", "summary": "Vision-Language-Action models (VLAs) achieve strong performance in general\nrobotic manipulation tasks by scaling imitation learning. However, existing\nVLAs are limited to predicting short-sighted next-action, which struggle with\nlong-horizon trajectory tasks due to incremental deviations. To address this\nproblem, we propose a plug-in framework named VLA-Reasoner that effectively\nempowers off-the-shelf VLAs with the capability of foreseeing future states via\ntest-time scaling. Specifically, VLA-Reasoner samples and rolls out possible\naction trajectories where involved actions are rationales to generate future\nstates via a world model, which enables VLA-Reasoner to foresee and reason\npotential outcomes and search for the optimal actions. We further leverage\nMonte Carlo Tree Search (MCTS) to improve search efficiency in large action\nspaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a\nconfidence sampling mechanism based on Kernel Density Estimation (KDE), to\nenable efficient exploration in MCTS without redundant VLA queries. We evaluate\nintermediate states in MCTS via an offline reward shaping strategy, to score\npredicted futures and correct deviations with long-term feedback. We conducted\nextensive experiments in both simulators and the real world, demonstrating that\nour proposed VLA-Reasoner achieves significant improvements over the\nstate-of-the-art VLAs. Our method highlights a potential pathway toward\nscalable test-time computation of robotic manipulation.", "AI": {"tldr": "\u63d0\u51faVLA-Reasoner\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e3a\u73b0\u6709VLA\u6a21\u578b\u589e\u52a0\u9884\u89c1\u672a\u6765\u72b6\u6001\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u589e\u91cf\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u53ea\u80fd\u9884\u6d4b\u77ed\u89c6\u7684\u4e0b\u4e00\u6b65\u52a8\u4f5c\uff0c\u5728\u957f\u65f6\u7a0b\u8f68\u8ff9\u4efb\u52a1\u4e2d\u56e0\u589e\u91cf\u504f\u5dee\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u91c7\u6837\u548c\u5c55\u5f00\u52a8\u4f5c\u8f68\u8ff9\u6765\u751f\u6210\u672a\u6765\u72b6\u6001\uff0c\u7ed3\u5408MCTS\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff0c\u5f15\u5165\u57fa\u4e8eKDE\u7684\u7f6e\u4fe1\u5ea6\u91c7\u6837\u673a\u5236\u51cf\u5c11\u5197\u4f59\u67e5\u8be2\uff0c\u91c7\u7528\u79bb\u7ebf\u5956\u52b1\u7b56\u7565\u8bc4\u4f30\u4e2d\u95f4\u72b6\u6001\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cVLA-Reasoner\u76f8\u6bd4\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53ef\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u6f5c\u5728\u8def\u5f84\u3002"}}
{"id": "2509.22652", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22652", "abs": "https://arxiv.org/abs/2509.22652", "authors": ["E-Ro Nguyen", "Yichi Zhang", "Kanchana Ranasinghe", "Xiang Li", "Michael S. Ryoo"], "title": "Pixel Motion Diffusion is What We Need for Robot Control", "comment": "16 pages, 7 figures", "summary": "We present DAWN (Diffusion is All We Need for robot control), a unified\ndiffusion-based framework for language-conditioned robotic manipulation that\nbridges high-level motion intent and low-level robot action via structured\npixel motion representation. In DAWN, both the high-level and low-level\ncontrollers are modeled as diffusion processes, yielding a fully trainable,\nend-to-end system with interpretable intermediate motion abstractions. DAWN\nachieves state-of-the-art results on the challenging CALVIN benchmark,\ndemonstrating strong multi-task performance, and further validates its\neffectiveness on MetaWorld. Despite the substantial domain gap between\nsimulation and reality and limited real-world data, we demonstrate reliable\nreal-world transfer with only minimal finetuning, illustrating the practical\nviability of diffusion-based motion abstractions for robotic control. Our\nresults show the effectiveness of combining diffusion modeling with\nmotion-centric representations as a strong baseline for scalable and robust\nrobot learning. Project page: https://nero1342.github.io/DAWN/", "AI": {"tldr": "DAWN\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7edf\u4e00\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u50cf\u7d20\u8fd0\u52a8\u8868\u793a\u8fde\u63a5\u9ad8\u7ea7\u8fd0\u52a8\u610f\u56fe\u548c\u4f4e\u7ea7\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5c55\u793a\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u53ef\u9760\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u9ad8\u7ea7\u8fd0\u52a8\u610f\u56fe\u4e0e\u4f4e\u7ea7\u52a8\u4f5c\u4e4b\u95f4\u7684\u8fde\u63a5\u95ee\u9898\uff0c\u4ee5\u53ca\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u9ad8\u7ea7\u548c\u4f4e\u7ea7\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u50cf\u7d20\u8fd0\u52a8\u8868\u793a\u6784\u5efa\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7cfb\u7edf\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u8fd0\u52a8\u62bd\u8c61\u3002", "result": "\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728MetaWorld\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u4ec5\u9700\u5c11\u91cf\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u53ef\u9760\u8fc1\u79fb\u3002", "conclusion": "\u6269\u6563\u5efa\u6a21\u4e0e\u8fd0\u52a8\u4e2d\u5fc3\u8868\u793a\u7684\u7ed3\u5408\u4e3a\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u51c6\u6846\u67b6\u3002"}}
{"id": "2509.22653", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22653", "abs": "https://arxiv.org/abs/2509.22653", "authors": ["Chih Yao Hu", "Yang-Sen Lin", "Yuna Lee", "Chih-Hai Su", "Jie-Ying Lee", "Shr-Ruei Tsai", "Chin-Yang Lin", "Kuan-Wen Chen", "Tsung-Wei Ke", "Yu-Lun Liu"], "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation", "comment": "CoRL 2025. Project page: https://spf-web.pages.dev", "summary": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev", "AI": {"tldr": "SPF\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u7a7a\u4e2d\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u5c06\u52a8\u4f5c\u9884\u6d4b\u89c6\u4e3a2D\u7a7a\u95f4\u5b9a\u4f4d\u4efb\u52a1\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u6a21\u7cca\u6307\u4ee4\u5206\u89e3\u4e3a2D\u822a\u70b9\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u95ed\u73af\u63a7\u5236\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eVLM\u7684\u65b9\u6cd5\u5c06\u52a8\u4f5c\u9884\u6d4b\u89c6\u4e3a\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u7a7a\u4e2d\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u52a8\u4f5c\u9884\u6d4b\u66f4\u9002\u5408\u4f5c\u4e3a2D\u7a7a\u95f4\u5b9a\u4f4d\u4efb\u52a1\u6765\u5904\u7406\u3002", "method": "\u5229\u7528VLM\u5c06\u6a21\u7cca\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u8f93\u5165\u56fe\u50cf\u4e0a\u7684\u8fed\u4ee32D\u822a\u70b9\u6807\u6ce8\uff0c\u7ed3\u5408\u9884\u6d4b\u7684\u79fb\u52a8\u8ddd\u79bb\uff0c\u5c062D\u822a\u70b9\u8f6c\u6362\u4e3a3D\u4f4d\u79fb\u5411\u91cf\u4f5c\u4e3a\u65e0\u4eba\u673a\u52a8\u4f5c\u547d\u4ee4\uff0c\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u79fb\u52a8\u8ddd\u79bb\u4ee5\u63d0\u9ad8\u5bfc\u822a\u6548\u7387\u3002", "result": "\u5728DRL\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u7edd\u5bf9\u63d0\u534763%\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u5927\u5e45\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SPF\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684VLM\uff0c\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002"}}
