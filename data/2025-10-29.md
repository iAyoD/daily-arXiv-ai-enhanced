<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 34]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [RoboOmni: Proactive Robot Manipulation in Omni-modal Context](https://arxiv.org/abs/2510.23763)
*Siyin Wang,Jinlan Fu,Feihong Liu,Xinzhe He,Huangxuan Wu,Junhao Shi,Kexin Huang,Zhaoye Fei,Jingjing Gong,Zuxuan Wu,Yugang Jiang,See-Kiong Ng,Tat-Seng Chua,Xipeng Qiu*

Main category: cs.RO

TL;DR: 提出了RoboOmni框架，基于全模态LLM，通过融合听觉和视觉信号进行意图识别，支持主动交互和动作执行，在模拟和真实环境中优于基于文本和ASR的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中人类很少直接发出指令，而是通过对话、环境声音和视觉线索表达意图。现有VLA模型主要依赖显式指令，需要机器人能够主动推断用户意图。

Method: 提出Perceiver-Thinker-Talker-Executor框架，基于端到端全模态LLM，统一意图识别、交互确认和动作执行。时空融合听觉和视觉信号，支持直接语音交互。构建了包含14万条数据的OmniAction数据集。

Result: 在模拟和真实环境实验中，RoboOmni在成功率、推理速度、意图识别和主动协助方面均优于基于文本和ASR的基线方法。

Conclusion: RoboOmni通过跨模态上下文指令和全模态融合，实现了更自然的人机交互，为主动意图识别和协作机器人提供了有效解决方案。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid
progress in Vision-Language-Action (VLA) models for robotic manipulation.
Although effective in many scenarios, current approaches largely rely on
explicit instructions, whereas in real-world interactions, humans rarely issue
instructions directly. Effective collaboration requires robots to infer user
intentions proactively. In this work, we introduce cross-modal contextual
instructions, a new setting where intent is derived from spoken dialogue,
environmental sounds, and visual cues rather than explicit commands. To address
this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor
framework based on end-to-end omni-modal LLMs that unifies intention
recognition, interaction confirmation, and action execution. RoboOmni fuses
auditory and visual signals spatiotemporally for robust intention recognition,
while supporting direct speech interaction. To address the absence of training
data for proactive intention recognition in robotic manipulation, we build
OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640
backgrounds, and six contextual instruction types. Experiments in simulation
and real-world settings show that RoboOmni surpasses text- and ASR-based
baselines in success rate, inference speed, intention recognition, and
proactive assistance.

</details>


### [2] [Motivating Students' Self-study with Goal Reminder and Emotional Support](https://arxiv.org/abs/2510.23860)
*Hyung Chan Cho,Go-Eum Cha,Yanfu Liu,Sooyeon Jeong*

Main category: cs.RO

TL;DR: 社交机器人作为同伴学习伴侣，通过目标提醒和情感支持帮助大学生自主学习，相比仅提供物理存在的控制组，能提高专注度、生产力和参与度。


<details>
  <summary>Details</summary>
Motivation: 虽然社交机器人在学习任务中的支持效果已被广泛研究，但它们在自主学习情境中对学生的潜在影响尚未得到充分探索。

Method: 采用探索性的Wizard-of-Oz研究，比较目标提醒、情感支持和仅物理存在（控制组）三种条件下机器人对学生学习的影响。

Result: 目标提醒和情感支持条件下的参与者报告了更高的易用性，目标提醒组还表现出更高的未来使用意愿。参与者对机器人的满意度与其将机器人视为社会实体的感知相关，这种感知能预测学习目标的达成程度。

Conclusion: 社交辅助机器人通过功能和情感参与，在自主学习支持方面具有巨大潜力。

Abstract: While the efficacy of social robots in supporting people in learning tasks
has been extensively investigated, their potential impact in assisting students
in self-studying contexts has not been investigated much. This study explores
how a social robot can act as a peer study companion for college students
during self-study tasks by delivering task-oriented goal reminder and positive
emotional support. We conducted an exploratory Wizard-of-Oz study to explore
how these robotic support behaviors impacted students' perceived focus,
productivity, and engagement in comparison to a robot that only provided
physical presence (control). Our study results suggest that participants in the
goal reminder and the emotional support conditions reported greater ease of
use, with the goal reminder condition additionally showing a higher willingness
to use the robot in future study sessions. Participants' satisfaction with the
robot was correlated with their perception of the robot as a social other, and
this perception was found to be a predictor for their level of goal achievement
in the self-study task. These findings highlight the potential of socially
assistive robots to support self-study through both functional and emotional
engagement.

</details>


### [3] [Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped](https://arxiv.org/abs/2510.23902)
*Jans Solano,Diego Quiroz*

Main category: cs.RO

TL;DR: 提出了一种面向低成本轮式四足机器人的恢复感知视觉惯性导航系统，结合深度相机视觉感知和深度强化学习策略，实现鲁棒运动控制和跌倒自主恢复。


<details>
  <summary>Details</summary>
Motivation: 现有轮腿式机器人依赖昂贵的执行器和传感器，且很少集成跌倒恢复功能，特别是在轮腿混合形态中。

Method: 利用深度相机进行视觉感知，采用深度强化学习策略实现鲁棒运动控制和自主恢复，在仿真环境中验证。

Result: 仿真实验显示在非规则地形上具有敏捷移动性，并能可靠地从外部扰动和自致故障中恢复，在结构化室内空间实现目标导向导航。

Conclusion: 该方法降低了在预算受限的机器人平台上部署自主导航和鲁棒运动策略的门槛。

Abstract: Wheeled-legged robots combine the efficiency of wheels with the obstacle
negotiation of legs, yet many state-of-the-art systems rely on costly actuators
and sensors, and fall-recovery is seldom integrated, especially for
wheeled-legged morphologies. This work presents a recovery-aware
visual-inertial navigation system on a low-cost wheeled quadruped. The proposed
system leverages vision-based perception from a depth camera and deep
reinforcement learning policies for robust locomotion and autonomous recovery
from falls across diverse terrains. Simulation experiments show agile mobility
with low-torque actuators over irregular terrain and reliably recover from
external perturbations and self-induced failures. We further show goal directed
navigation in structured indoor spaces with low-cost perception. Overall, this
approach lowers the barrier to deploying autonomous navigation and robust
locomotion policies in budget-constrained robotic platforms.

</details>


### [4] [Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments](https://arxiv.org/abs/2510.23928)
*Raman Jha,Yang Zhou,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 提出了一种自适应关键帧选择方法，通过集成基于误差的选择模块和基于动量的更新模块，动态调整关键帧选择阈值，在动态环境中显著提升3D场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中实时感知的关键数据瓶颈问题，从压缩数据流中创建高质量的3D世界表示，为复杂动态环境中的可扩展机器人学习和部署提供关键步骤。

Method: 集成两个互补模块：基于光度误差和结构相似性(SSIM)误差的选择模块，以及基于动量的更新模块，动态调整关键帧选择阈值以适应场景运动动态。

Result: 在Spann3r和CUT3R两个最先进的3D重建网络上评估，观察到重建质量的一致改进，显著优于传统的静态关键帧选择策略。消融研究证实了各组件对整体性能增益的有效贡献。

Conclusion: 该方法代表了自适应感知系统的有意义的进步，能够动态响应复杂和演变的视觉场景，为动态环境中的3D重建提供了有效的解决方案。

Abstract: In this paper, we propose an adaptive keyframe selection method for improved
3D scene reconstruction in dynamic environments. The proposed method integrates
two complementary modules: an error-based selection module utilizing
photometric and structural similarity (SSIM) errors, and a momentum-based
update module that dynamically adjusts keyframe selection thresholds according
to scene motion dynamics. By dynamically curating the most informative frames,
our approach addresses a key data bottleneck in real-time perception. This
allows for the creation of high-quality 3D world representations from a
compressed data stream, a critical step towards scalable robot learning and
deployment in complex, dynamic environments. Experimental results demonstrate
significant improvements over traditional static keyframe selection strategies,
such as fixed temporal intervals or uniform frame skipping. These findings
highlight a meaningful advancement toward adaptive perception systems that can
dynamically respond to complex and evolving visual scenes. We evaluate our
proposed adaptive keyframe selection module on two recent state-of-the-art 3D
reconstruction networks, Spann3r and CUT3R, and observe consistent improvements
in reconstruction quality across both frameworks. Furthermore, an extensive
ablation study confirms the effectiveness of each individual component in our
method, underlining their contribution to the overall performance gains.

</details>


### [5] [A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons](https://arxiv.org/abs/2510.23954)
*Pejman Kheradmand,Behnam Moradkhani,Raghavasimhan Sankaranarayanan,Kent K. Yamamoto,Tanner J. Zachem,Patrick J. Codd,Yash Chitalia,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 提出了基于Cosserat杆的建模框架，用于建模n个同心管、每个管由m_i个肌腱驱动的肌腱驱动同心管机构，实现了小于4%总长度的尖端预测误差。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动同心管机构结合了肌腱驱动连续体机器人和同心管机器人的优点，但缺乏完整通用的力学模型。

Method: 使用Cosserat杆理论框架，允许每个管扭转和伸长，同时强制弯曲共享中心线。

Result: 在双管和三管组件的实验中，尖端预测误差小于机器人总长度的4%；应用于现有机器人时，最大尖端偏差约为总长度的5%。

Conclusion: 该模型为先进肌腱驱动同心管机器人的精确形状估计和控制提供了基础。

Abstract: Tendon-actuated concentric tube mechanisms combine the advantages of
tendon-driven continuum robots and concentric tube robots while addressing
their respective limitations. They overcome the restricted degrees of freedom
often seen in tendon-driven designs, and mitigate issues such as snapping
instability associated with concentric tube robots. However, a complete and
general mechanical model for these systems remains an open problem. In this
work, we propose a Cosserat rod-based framework for modeling the general case
of $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \{1,
\ldots, n\}$. The model allows each tube to twist and elongate while enforcing
a shared centerline for bending. We validate the proposed framework through
experiments with two-tube and three tube assemblies under various tendon
routing configurations, achieving tip prediction errors $<4\%$ of the robot's
total length. We further demonstrate the model's generality by applying it to
existing robots in the field, where maximum tip deviations remain around $5\%$
of the total length. This model provides a foundation for accurate shape
estimation and control of advanced tendon-actuated concentric tube robots.

</details>


### [6] [Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping](https://arxiv.org/abs/2510.23963)
*Hiroki Ishikawa,Kyosuke Ishibashi,Ko Yamamoto*

Main category: cs.RO

TL;DR: 本文提出了一种具有自适应扭转变形能力的软体机器人手指，能够通过包裹方式抓取物体。该手指采用可变刚度机制，通过单一驱动源实现深度插入和稳定抓取功能。


<details>
  <summary>Details</summary>
Motivation: 为了让软体手能够在密集堆放的多个物体中抓取单个物体，软体手指需要具备平面内和平面外的自适应扭转变形功能，以便深入物体间的狭窄间隙并保持扭转变形状态。

Method: 提出可变刚度机制，通过压力变化自适应调整刚度；进行有限元分析确定设计参数；开发软体手指并进行基础实验验证。

Result: 通过有限元分析优化了设计参数，开发的软体手指能够成功抓取各种物体，验证了自适应扭转变形和包裹抓取的有效性。

Conclusion: 所提出的可变刚度软体手指能够实现自适应扭转变形，成功解决了在密集物体中抓取单个物体的挑战，为软体机器人抓取应用提供了新方案。

Abstract: This paper presents a soft robot finger capable of adaptive-twist deformation
to grasp objects by wrapping them. For a soft hand to grasp and pick-up one
object from densely contained multiple objects, a soft finger requires the
adaptive-twist deformation function in both in-plane and out-of-plane
directions. The function allows the finger to be inserted deeply into a limited
gap among objects. Once inserted, the soft finger requires appropriate control
of grasping force normal to contact surface, thereby maintaining the twisted
deformation. In this paper, we refer to this type of grasping as grasping by
wrapping. To achieve these two functions by a single actuation source, we
propose a variable stiffness mechanism that can adaptively change the stiffness
as the pressure is higher. We conduct a finite element analysis (FEA) on the
proposed mechanism and determine its design parameter based on the FEA result.
Using the developed soft finger, we report basic experimental results and
demonstrations on grasping various objects.

</details>


### [7] [A Survey on Collaborative SLAM with 3D Gaussian Splatting](https://arxiv.org/abs/2510.23988)
*Phuc Nguyen Xuan,Thanh Nguyen Canh,Huu-Hung Nguyen,Nak Young Chong,Xiem HoangVan*

Main category: cs.RO

TL;DR: 本文全面综述了基于3D高斯泼溅的多机器人协同SLAM领域，分析了集中式和分布式架构，总结了关键挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅作为显式场景表示方法，能够实现实时高保真渲染，非常适合机器人应用。但在多机器人系统中，维持全局一致性、管理通信和融合异构数据源带来了重大挑战。

Method: 系统性地按架构（集中式、分布式）分类方法，分析多智能体一致性对齐、通信效率、高斯表示、语义蒸馏、融合与位姿优化、实时可扩展性等核心组件。

Result: 提供了关键数据集和评估指标的总结，为性能评估提供背景。识别了当前方法的优势和局限性。

Conclusion: 确定了关键开放挑战和未来研究方向，包括终身建图、语义关联与建图、多模型鲁棒性以及弥合Sim2Real差距。

Abstract: This survey comprehensively reviews the evolving field of multi-robot
collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian
Splatting (3DGS). As an explicit scene representation, 3DGS has enabled
unprecedented real-time, high-fidelity rendering, ideal for robotics. However,
its use in multi-robot systems introduces significant challenges in maintaining
global consistency, managing communication, and fusing data from heterogeneous
sources. We systematically categorize approaches by their architecture --
centralized, distributed -- and analyze core components like multi-agent
consistency and alignment, communication-efficient, Gaussian representation,
semantic distillation, fusion and pose optimization, and real-time scalability.
In addition, a summary of critical datasets and evaluation metrics is provided
to contextualize performance. Finally, we identify key open challenges and
chart future research directions, including lifelong mapping, semantic
association and mapping, multi-model for robustness, and bridging the Sim2Real
gap.

</details>


### [8] [VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion](https://arxiv.org/abs/2510.23997)
*Stanley Wu,Mohamad H. Danesh,Simon Li,Hanna Yurchyk,Amin Abyaneh,Anas El Houssaini,David Meger,Hsiu-Chin Lin*

Main category: cs.RO

TL;DR: VOCALoco是一个模块化技能选择框架，通过感知输入动态调整四足机器人的步态策略，在楼梯任务中比端到端DRL方法更安全、更节能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端深度强化学习方法在泛化到新地形时存在安全性和可解释性限制，需要更安全可靠的解决方案。

Method: 使用预训练步态策略集，通过预测执行安全性和能耗来评估策略可行性，选择既安全又节能的策略。

Result: 在模拟和真实世界的楼梯上下任务中，VOCALoco相比传统端到端DRL策略展现出更好的鲁棒性和安全性。

Conclusion: VOCALoco框架通过模块化技能选择有效提升了四足机器人在复杂地形上的运动安全性和能效。

Abstract: Recent advancements in legged robot locomotion have facilitated traversal
over increasingly complex terrains. Despite this progress, many existing
approaches rely on end-to-end deep reinforcement learning (DRL), which poses
limitations in terms of safety and interpretability, especially when
generalizing to novel terrains. To overcome these challenges, we introduce
VOCALoco, a modular skill-selection framework that dynamically adapts
locomotion strategies based on perceptual input. Given a set of pre-trained
locomotion policies, VOCALoco evaluates their viability and energy-consumption
by predicting both the safety of execution and the anticipated cost of
transport over a fixed planning horizon. This joint assessment enables the
selection of policies that are both safe and energy-efficient, given the
observed local terrain. We evaluate our approach on staircase locomotion tasks,
demonstrating its performance in both simulated and real-world scenarios using
a quadrupedal robot. Empirical results show that VOCALoco achieves improved
robustness and safety during stair ascent and descent compared to a
conventional end-to-end DRL policy

</details>


### [9] [Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model](https://arxiv.org/abs/2510.24029)
*Andrew Gerstenslager,Bekarys Dukenbaev,Ali A. Minai*

Main category: cs.RO

TL;DR: 该论文提出了一种将垂直角度敏感性融入边界向量细胞(BVC)框架的3D模型，解决了传统2D BVC模型在水平对称环境中容易产生空间模糊的问题，显著提高了生物启发机器人模型的空间定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统边界向量细胞(BVC)模型主要局限于二维环境，在存在水平对称性的环境中容易产生空间模糊。为了解决这一限制，需要将垂直角度敏感性纳入BVC框架，实现三维空间中的鲁棒边界检测。

Method: 通过处理LiDAR数据捕捉垂直轮廓，将垂直角度敏感性融入BVC框架，开发了三维边界检测模型，能够区分在纯2D表示下无法区分的位置。

Result: 实验结果显示：在垂直变化最小的环境中，3D模型性能与2D基线相当；随着3D复杂性增加，该模型产生更多独特的场所场，并显著减少空间混叠。

Conclusion: 在BVC定位中添加垂直维度可以显著增强真实世界3D空间中的导航和映射能力，同时在简单的近平面场景中保持性能一致。

Abstract: Boundary Vector Cells (BVCs) are a class of neurons in the brains of
vertebrates that encode environmental boundaries at specific distances and
allocentric directions, playing a central role in forming place fields in the
hippocampus. Most computational BVC models are restricted to two-dimensional
(2D) environments, making them prone to spatial ambiguities in the presence of
horizontal symmetries in the environment. To address this limitation, we
incorporate vertical angular sensitivity into the BVC framework, thereby
enabling robust boundary detection in three dimensions, and leading to
significantly more accurate spatial localization in a biologically-inspired
robot model.
  The proposed model processes LiDAR data to capture vertical contours, thereby
disambiguating locations that would be indistinguishable under a purely 2D
representation. Experimental results show that in environments with minimal
vertical variation, the proposed 3D model matches the performance of a 2D
baseline; yet, as 3D complexity increases, it yields substantially more
distinct place fields and markedly reduces spatial aliasing. These findings
show that adding a vertical dimension to BVC-based localization can
significantly enhance navigation and mapping in real-world 3D spaces while
retaining performance parity in simpler, near-planar scenarios.

</details>


### [10] [SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration](https://arxiv.org/abs/2510.24052)
*Jongsuk Kim,Jaeyoung Lee,Gyojin Han,Dongjae Lee,Minki Jeong,Junmo Kim*

Main category: cs.RO

TL;DR: SynAD是首个利用合成数据增强真实世界端到端自动驾驶模型的框架，通过在多智能体合成场景中指定具有最全面驾驶信息的智能体作为自车，将路径级场景投影到地图上，并使用新开发的Map-to-BEV网络获取鸟瞰图特征，有效提升了安全性能。


<details>
  <summary>Details</summary>
Motivation: 真实世界驾驶数据限制了训练场景的多样性，而合成场景生成虽然能丰富训练数据，但在端到端自动驾驶模型中的应用尚未充分探索，主要因为缺乏指定的自车和相关传感器输入。

Method: 在多智能体合成场景中指定具有最全面驾驶信息的智能体作为自车；将路径级场景投影到地图上；使用新开发的Map-to-BEV网络获取鸟瞰图特征；设计训练策略有效整合基于地图的合成数据与真实驾驶数据。

Result: 实验结果表明，SynAD有效整合了所有组件，显著提升了安全性能。

Conclusion: SynAD通过桥接合成场景生成和端到端自动驾驶，为更全面和鲁棒的自动驾驶模型铺平了道路。

Abstract: Recent advancements in deep learning and the availability of high-quality
real-world driving datasets have propelled end-to-end autonomous driving.
Despite this progress, relying solely on real-world data limits the variety of
driving scenarios for training. Synthetic scenario generation has emerged as a
promising solution to enrich the diversity of training data; however, its
application within E2E AD models remains largely unexplored. This is primarily
due to the absence of a designated ego vehicle and the associated sensor
inputs, such as camera or LiDAR, typically provided in real-world scenarios. To
address this gap, we introduce SynAD, the first framework designed to enhance
real-world E2E AD models using synthetic data. Our method designates the agent
with the most comprehensive driving information as the ego vehicle in a
multi-agent synthetic scenario. We further project path-level scenarios onto
maps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view
features without relying on sensor inputs. Finally, we devise a training
strategy that effectively integrates these map-based synthetic data with real
driving data. Experimental results demonstrate that SynAD effectively
integrates all components and notably enhances safety performance. By bridging
synthetic scenario generation and E2E AD, SynAD paves the way for more
comprehensive and robust autonomous driving models.

</details>


### [11] [Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation](https://arxiv.org/abs/2510.24055)
*Xiucheng Zhang,Yang Jiang,Hongwei Qing,Jiashuo Bai*

Main category: cs.RO

TL;DR: 提出LCVR和LMoE-DP框架，通过语义基础和专家专业化解决模仿学习中多任务机器人操作面临的感知模糊和任务冲突问题，在真实机器人基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多任务机器人模仿学习中存在的感知模糊（视觉相似任务难以区分）和任务冲突问题，提升多任务操作的鲁棒性和效率。

Method: 结合语言条件视觉表示(LCVR)模块和语言条件混合专家密度策略(LMoE-DP)。LCVR通过语言指令对视觉特征进行基础，区分视觉相似任务；LMoE-DP使用稀疏专家架构专门处理不同的多模态动作分布，并通过梯度调制稳定训练。

Result: 在真实机器人基准测试中，LCVR将Action Chunking with Transformers (ACT)和Diffusion Policy (DP)的成功率分别提升33.75%和25%。完整框架达到79%的平均成功率，比先进基线提升21%。

Conclusion: 结合语义基础和专家专业化能够实现鲁棒、高效的多任务操作，为多任务机器人模仿学习提供了有效解决方案。

Abstract: Perceptual ambiguity and task conflict limit multitask robotic manipulation
via imitation learning. We propose a framework combining a Language-Conditioned
Visual Representation (LCVR) module and a Language-conditioned
Mixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual
ambiguities by grounding visual features with language instructions, enabling
differentiation between visually similar tasks. To mitigate task conflict,
LMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal
action distributions, stabilized by gradient modulation. On real-robot
benchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion
Policy (DP) success rates by 33.75% and 25%, respectively. The full framework
achieves a 79% average success, outperforming the advanced baseline by 21%. Our
work shows that combining semantic grounding and expert specialization enables
robust, efficient multi-task manipulation

</details>


### [12] [Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition](https://arxiv.org/abs/2510.24067)
*Tianyi Ding,Ronghao Zheng,Senlin Zhang,Meiqin Liu*

Main category: cs.RO

TL;DR: 提出了一种分布式多机器人自主在线探索方法，通过拓扑图Voronoi算法实现平衡的区域划分和任务分配，在障碍密集的非凸环境中提高探索效率和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在障碍密集非凸环境中的协同探索问题，特别关注分布式探索规划中的动态平衡区域划分和任务分配挑战。

Method: 使用增量更新的拓扑图结构表征空间连通性和全局探索完整性；提出分布式加权拓扑图Voronoi算法实现平衡图空间划分；局部规划器优化探索目标访问序列并生成安全平滑的运动轨迹。

Result: 与现有方法相比，在探索效率、完整性和机器人团队工作负载平衡方面均有显著提升。

Conclusion: 该方法为多机器人在复杂环境中的协同探索提供了理论保证和实际可行的解决方案，实现了高效、平衡的分布式自主探索。

Abstract: This work addresses the collaborative multi-robot autonomous online
exploration problem, particularly focusing on distributed exploration planning
for dynamically balanced exploration area partition and task allocation among a
team of mobile robots operating in obstacle-dense non-convex environments.
  We present a novel topological map structure that simultaneously
characterizes both spatial connectivity and global exploration completeness of
the environment. The topological map is updated incrementally to utilize known
spatial information for updating reachable spaces, while exploration targets
are planned in a receding horizon fashion under global coverage guidance.
  A distributed weighted topological graph Voronoi algorithm is introduced
implementing balanced graph space partitions of the fused topological maps.
Theoretical guarantees are provided for distributed consensus convergence and
equitable graph space partitions with constant bounds.
  A local planner optimizes the visitation sequence of exploration targets
within the balanced partitioned graph space to minimize travel distance, while
generating safe, smooth, and dynamically feasible motion trajectories.
  Comprehensive benchmarking against state-of-the-art methods demonstrates
significant improvements in exploration efficiency, completeness, and workload
balance across the robot team.

</details>


### [13] [Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition](https://arxiv.org/abs/2510.24069)
*Sangmin Kim,Hajun Kim,Gijeong Kim,Min-Gyu Kim,Hae-Won Park*

Main category: cs.RO

TL;DR: 提出一种基于相位的轨迹优化方法，确保足式机器人在整个轨迹中满足平移动力学和摩擦锥约束的可行性。


<details>
  <summary>Details</summary>
Motivation: 为足式机器人生成可靠运动需要同时计算路径和接触序列，并准确考虑动力学问题。

Method: 利用线性微分方程的叠加特性解耦各接触点的平移动力学，使用贝塞尔多项式的微分矩阵建立位置与力的解析关系，并利用贝塞尔多项式的凸包特性确保摩擦锥约束。

Result: 该轨迹优化框架能够为足式机器人生成具有各种步态序列的动态可靠运动。

Conclusion: 通过四足机器人模型验证了框架在动力学可行性和运动生成方面的有效性。

Abstract: To generate reliable motion for legged robots through trajectory
optimization, it is crucial to simultaneously compute the robot's path and
contact sequence, as well as accurately consider the dynamics in the problem
formulation. In this paper, we present a phase-based trajectory optimization
that ensures the feasibility of translational dynamics and friction cone
constraints throughout the entire trajectory. Specifically, our approach
leverages the superposition properties of linear differential equations to
decouple the translational dynamics for each contact point, which operates
under different phase sequences. Furthermore, we utilize the differentiation
matrix of B{\'e}zier polynomials to derive an analytical relationship between
the robot's position and force, thereby ensuring the consistent satisfaction of
translational dynamics. Additionally, by exploiting the convex closure property
of B{\'e}zier polynomials, our method ensures compliance with friction cone
constraints. Using the aforementioned approach, the proposed trajectory
optimization framework can generate dynamically reliable motions with various
gait sequences for legged robots. We validate our framework using a quadruped
robot model, focusing on the feasibility of dynamics and motion generation.

</details>


### [14] [ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring](https://arxiv.org/abs/2510.24108)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Jingde Chen,Nadine Chang,Maying Shen,Jingyu Song,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: ZTRS是首个完全消除模仿学习的端到端自动驾驶框架，仅通过奖励学习直接处理高维传感器数据，结合离线强化学习和提出的详尽策略优化方法，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶框架主要依赖模仿学习，但受限于次优专家演示和部署时的协变量偏移问题。强化学习虽然能利用模拟环境扩展，但通常局限于低维符号输入，无法实现真正的端到端学习。

Method: 提出ZTRS框架，结合离线强化学习和详尽策略优化(EPO)，这是一种针对可枚举动作和奖励的策略梯度变体，完全消除模仿学习，仅从奖励中学习。

Result: 在三个基准测试中表现优异：Navtest（通用现实世界开环规划）、Navhard（挑战性现实世界和合成场景开环规划）和HUGSIM（模拟闭环驾驶）。在Navhard上达到最先进水平，在HUGSIM上优于基于模仿学习的基线方法。

Conclusion: ZTRS成功展示了完全消除模仿学习的可行性，通过结合传感器输入和强化学习训练，实现了更鲁棒的端到端自动驾驶规划。

Abstract: End-to-end autonomous driving maps raw sensor inputs directly into
ego-vehicle trajectories to avoid cascading errors from perception modules and
to leverage rich semantic cues. Existing frameworks largely rely on Imitation
Learning (IL), which can be limited by sub-optimal expert demonstrations and
covariate shift during deployment. On the other hand, Reinforcement Learning
(RL) has recently shown potential in scaling up with simulations, but is
typically confined to low-dimensional symbolic inputs (e.g. 3D objects and
maps), falling short of full end-to-end learning from raw sensor data. We
introduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory
Scoring), a framework that combines the strengths of both worlds: sensor inputs
without losing information and RL training for robust planning. To the best of
our knowledge, ZTRS is the first framework that eliminates IL entirely by only
learning from rewards while operating directly on high-dimensional sensor data.
ZTRS utilizes offline reinforcement learning with our proposed Exhaustive
Policy Optimization (EPO), a variant of policy gradient tailored for enumerable
actions and rewards. ZTRS demonstrates strong performance across three
benchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop
planning in challenging real-world and synthetic scenarios), and HUGSIM
(simulated closed-loop driving). Specifically, ZTRS achieves the
state-of-the-art result on Navhard and outperforms IL-based baselines on
HUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.

</details>


### [15] [PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI](https://arxiv.org/abs/2510.24109)
*Wenbin Ding,Jun Chen,Mingjia Chen,Fei Xie,Qi Mao,Philip Dames*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉语言模型的新型机器人具身智能体框架，通过人机语音交互、视觉语言智能体和动作执行模块，显著提升了复杂自然语言控制任务的在线规划和执行能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，以人为中心的人工智能对机器人的智能水平提出了更高要求，特别是在自然语言交互、复杂任务规划和执行方面。现有的LLM具身智能体往往缺乏在线规划和执行复杂自然语言控制任务的能力。

Method: 提出包含人机语音交互模块、视觉语言智能体模块和动作执行模块的机器人具身智能体框架。视觉语言智能体包括基于视觉的任务规划器、自然语言指令转换器和任务执行反馈评估器。

Result: 实验结果表明，在模拟和真实环境中，该智能体的平均任务成功率比仅使用LLM+CLIP的方法提高了28%，显著提升了高级自然语言指令任务的执行成功率。

Conclusion: 该研究为基于视觉语言模型的智能机器人操作智能体在物理世界中的实现提供了有效解决方案，显著提升了复杂自然语言控制任务的执行能力。

Abstract: The rapid advancement of Large Language Models (LLMs) has marked a
significant breakthrough in Artificial Intelligence (AI), ushering in a new era
of Human-centered Artificial Intelligence (HAI). HAI aims to better serve human
welfare and needs, thereby placing higher demands on the intelligence level of
robots, particularly in aspects such as natural language interaction, complex
task planning, and execution. Intelligent agents powered by LLMs have opened up
new pathways for realizing HAI. However, existing LLM-based embodied agents
often lack the ability to plan and execute complex natural language control
tasks online. This paper explores the implementation of intelligent robotic
manipulating agents based on Vision-Language Models (VLMs) in the physical
world. We propose a novel embodied agent framework for robots, which comprises
a human-robot voice interaction module, a vision-language agent module and an
action execution module. The vision-language agent itself includes a
vision-based task planner, a natural language instruction converter, and a task
performance feedback evaluator. Experimental results demonstrate that our agent
achieves a 28\% higher average task success rate in both simulated and real
environments compared to approaches relying solely on LLM+CLIP, significantly
improving the execution success rate of high-level natural language instruction
tasks.

</details>


### [16] [LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation](https://arxiv.org/abs/2510.24118)
*Haotian Zhou,Xiaole Wang,He Li,Fusheng Sun,Shengyu Guo,Guolei Qi,Jianghuan Xu,Huijing Zhao*

Main category: cs.RO

TL;DR: LagMemo是一个基于语言3D高斯泼溅记忆的视觉导航系统，支持多模态、开放词汇和多目标导航，通过构建统一3D语言记忆和局部感知验证机制实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 传统视觉导航方法局限于单目标、单模态和封闭集目标设置，无法满足实际应用中多模态、开放词汇查询和多目标导航的需求。

Method: 系统在探索阶段构建统一3D语言记忆，通过查询记忆预测候选目标位置，并集成局部感知验证机制在导航过程中动态匹配和验证目标。

Result: 实验结果表明LagMemo的记忆模块能够有效实现多模态开放词汇目标定位，在多目标视觉导航任务中优于现有最先进方法。

Conclusion: LagMemo通过语言3D高斯泼溅记忆和动态验证机制，成功解决了多模态开放词汇多目标视觉导航的挑战。

Abstract: Navigating to a designated goal using visual information is a fundamental
capability for intelligent robots. Most classical visual navigation methods are
restricted to single-goal, single-modality, and closed set goal settings. To
address the practical demands of multi-modal, open-vocabulary goal queries and
multi-goal visual navigation, we propose LagMemo, a navigation system that
leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo
constructs a unified 3D language memory. With incoming task goals, the system
queries the memory, predicts candidate goal locations, and integrates a local
perception-based verification mechanism to dynamically match and validate goals
during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a
high-quality core split distilled from GOAT-Bench tailored to multi-modal
open-vocabulary multi-goal visual navigation. Experimental results show that
LagMemo's memory module enables effective multi-modal open-vocabulary goal
localization, and that LagMemo outperforms state-of-the-art methods in
multi-goal visual navigation. Project page:
https://weekgoodday.github.io/lagmemo

</details>


### [17] [Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames](https://arxiv.org/abs/2510.24194)
*Ev Zisselman,Mirco Mutti,Shelly Francis-Meretzki,Elisei Shafer,Aviv Tamar*

Main category: cs.RO

TL;DR: 提出"蒙眼专家"方法，在行为克隆中向演示者隐藏部分任务信息，迫使演示者进行非平凡探索，从而提高模型在未见任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统行为克隆需要大量任务演示来获得泛化能力，而人类专家通常拥有完整任务信息，演示的是近乎最优行为。本文探索通过限制演示者信息来改进泛化性能。

Method: 向演示者隐藏部分任务信息，创建"蒙眼专家"，迫使他们进行探索性行为。克隆这些蒙眼专家的行为，而不是完全知情专家的行为。

Result: 实验表明，克隆蒙眼专家比克隆完全知情专家在未见任务上泛化得更好。在真实机器人插孔任务和Procgen基准视频游戏中验证了效果。

Conclusion: 理论分析和实验都表明，克隆蒙眼专家能够以更少的演示任务实现更好的泛化，泛化误差与演示者可获得的任务信息量平方根成正比。

Abstract: Behavioral cloning is a simple yet effective technique for learning
sequential decision-making from demonstrations. Recently, it has gained
prominence as the core of foundation models for the physical world, where
achieving generalization requires countless demonstrations of a multitude of
tasks. Typically, a human expert with full information on the task demonstrates
a (nearly) optimal behavior. In this paper, we propose to hide some of the
task's information from the demonstrator. This ``blindfolded'' expert is
compelled to employ non-trivial exploration to solve the task. We show that
cloning the blindfolded expert generalizes better to unseen tasks than its
fully-informed counterpart. We conduct experiments of real-world robot peg
insertion tasks with (limited) human demonstrations, alongside videogames from
the Procgen benchmark. Additionally, we support our findings with theoretical
analysis, which confirms that the generalization error scales with
$\sqrt{I/m}$, where $I$ measures the amount of task information available to
the demonstrator, and $m$ is the number of demonstrated tasks. Both theory and
practice indicate that cloning blindfolded experts generalizes better with
fewer demonstrated tasks. Project page with videos and code:
https://sites.google.com/view/blindfoldedexperts/home

</details>


### [18] [Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors](https://arxiv.org/abs/2510.24257)
*Ziqi Ma,Changda Tian,Yue Gao*

Main category: cs.RO

TL;DR: 提出HMAMP方法，通过对抗运动先验学习人类风格的操纵技能，在锤击任务中表现优于基线方法，并展示了真实机器人应用潜力。


<details>
  <summary>Details</summary>
Motivation: 开发能够以更自然直观方式与人类互动的机器人和自主系统，关键挑战是让这些系统能够以类似人类的方式操纵物体和工具。

Method: 使用对抗网络建模工具和物体操纵的复杂动力学，判别器结合真实世界数据和智能体执行的模拟数据进行训练，训练生成符合人类运动统计特性的真实运动轨迹的策略。

Result: 在锤击任务中，HMAMP能够学习人类风格的操纵技能，性能优于当前基线方法，并在真实机器人手臂锤击任务中展示了实际应用潜力。

Conclusion: HMAMP代表了朝着开发能够以更自然直观方式与人类互动机器人的重要一步，通过学习以类似人类的方式操纵工具和物体。

Abstract: In recent years, there has been growing interest in developing robots and
autonomous systems that can interact with human in a more natural and intuitive
way. One of the key challenges in achieving this goal is to enable these
systems to manipulate objects and tools in a manner that is similar to that of
humans. In this paper, we propose a novel approach for learning human-style
manipulation skills by using adversarial motion priors, which we name HMAMP.
The approach leverages adversarial networks to model the complex dynamics of
tool and object manipulation, as well as the aim of the manipulation task. The
discriminator is trained using a combination of real-world data and simulation
data executed by the agent, which is designed to train a policy that generates
realistic motion trajectories that match the statistical properties of human
motion. We evaluated HMAMP on one challenging manipulation task: hammering, and
the results indicate that HMAMP is capable of learning human-style manipulation
skills that outperform current baseline methods. Additionally, we demonstrate
that HMAMP has potential for real-world applications by performing real robot
arm hammering tasks. In general, HMAMP represents a significant step towards
developing robots and autonomous systems that can interact with humans in a
more natural and intuitive way, by learning to manipulate tools and objects in
a manner similar to how humans do.

</details>


### [19] [DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation](https://arxiv.org/abs/2510.24261)
*Jingyi Tian,Le Wang,Sanping Zhou,Sen Wang,Jiayi Li,Gang Hua*

Main category: cs.RO

TL;DR: DynaRend是一个通过可微分体积渲染学习3D感知和动态感知的三平面特征的表示学习框架，用于提升机器人操作策略的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作策略因真实世界训练数据稀缺而泛化能力不足的问题。现有方法要么依赖2D视觉预训练范式，要么使用大规模视频预测模型，但都未能联合学习几何、语义和动态信息。

Method: 通过掩码重建和未来预测，使用可微分体积渲染学习3D感知和动态感知的三平面特征。在多视角RGB-D视频数据上进行预训练，联合捕获空间几何、未来动态和任务语义。

Result: 在RLBench和Colosseum基准测试以及真实世界机器人实验中，DynaRend在策略成功率、对环境扰动的泛化能力和真实世界适用性方面都取得了显著提升。

Conclusion: DynaRend能够有效学习统一的3D感知表示，显著提升机器人操作策略的性能和泛化能力。

Abstract: Learning generalizable robotic manipulation policies remains a key challenge
due to the scarcity of diverse real-world training data. While recent
approaches have attempted to mitigate this through self-supervised
representation learning, most either rely on 2D vision pretraining paradigms
such as masked image modeling, which primarily focus on static semantics or
scene geometry, or utilize large-scale video prediction models that emphasize
2D dynamics, thus failing to jointly learn the geometry, semantics, and
dynamics required for effective manipulation. In this paper, we present
DynaRend, a representation learning framework that learns 3D-aware and
dynamics-informed triplane features via masked reconstruction and future
prediction using differentiable volumetric rendering. By pretraining on
multi-view RGB-D video data, DynaRend jointly captures spatial geometry, future
dynamics, and task semantics in a unified triplane representation. The learned
representations can be effectively transferred to downstream robotic
manipulation tasks via action value map prediction. We evaluate DynaRend on two
challenging benchmarks, RLBench and Colosseum, as well as in real-world robotic
experiments, demonstrating substantial improvements in policy success rate,
generalization to environmental perturbations, and real-world applicability
across diverse manipulation tasks.

</details>


### [20] [Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation](https://arxiv.org/abs/2510.24315)
*Baozhe Zhang,Xinwei Chen,Qingcheng Chen,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出CoNi-OA算法，为无人机-无人车协同场景设计，仅使用单帧LiDAR数据生成调制矩阵来调整无人机速度实现避障，无需全局状态估计或障碍物预测。


<details>
  <summary>Details</summary>
Motivation: CoNi-MPC框架在空地协同任务中仅依赖相对状态，但缺乏环境信息导致避障困难。需要一种不依赖全局状态估计或障碍物预测的避障方法。

Method: 利用无人机单帧原始LiDAR数据生成调制矩阵，直接调整四旋翼速度实现避障。基于调制的方法在无人车非惯性系中实时生成无碰撞轨迹。

Result: 计算需求显著降低（每次迭代小于5毫秒），在动态不可预测环境中保持安全性，适应静态和动态环境。

Conclusion: CoNi-OA算法为无人机-无人车协同提供高效避障方案，无需全局状态或障碍物预测，在特征缺失或未知场景中具有良好适应性。

Abstract: CoNi-MPC provides an efficient framework for UAV control in air-ground
cooperative tasks by relying exclusively on relative states, eliminating the
need for global state estimation. However, its lack of environmental
information poses significant challenges for obstacle avoidance. To address
this issue, we propose a novel obstacle avoidance algorithm, Cooperative
Non-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for
UAV-UGV cooperative scenarios without reliance on global state estimation or
obstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data
from the UAV to generate a modulation matrix, which directly adjusts the
quadrotor's velocity to achieve obstacle avoidance. This modulation-based
method enables real-time generation of collision-free trajectories within the
UGV's non-inertial frame, significantly reducing computational demands (less
than 5 ms per iteration) while maintaining safety in dynamic and unpredictable
environments. The key contributions of this work include: (1) a
modulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV
cooperation in non-inertial frames without global states; (2) rapid, real-time
trajectory generation based solely on single-frame LiDAR data, removing the
need for obstacle modeling or prediction; and (3) adaptability to both static
and dynamic environments, thus extending applicability to featureless or
unknown scenarios.

</details>


### [21] [NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation](https://arxiv.org/abs/2510.24335)
*Mingyu Jeong,Eunsung Kim,Sehun Park,Andrew Jaeyong Choi*

Main category: cs.RO

TL;DR: NVSim是一个从普通图像序列自动构建大规模可导航室内模拟器的框架，解决了传统3D扫描的成本和可扩展性限制。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描方法成本高且难以扩展，无法有效处理机器人遍历数据中稀疏观测地板导致的视觉伪影问题。

Method: 采用3D高斯泼溅技术，引入地板感知高斯泼溅确保清洁可导航的地平面，以及新颖的无网格可遍历性检查算法，通过直接分析渲染视图构建拓扑图。

Result: 系统能够从真实世界数据生成有效的大规模导航图。

Conclusion: NVSim框架成功克服了传统方法的局限性，实现了从普通图像序列自动构建大规模可导航室内模拟器。

Abstract: We present NVSim, a framework that automatically constructs large-scale,
navigable indoor simulators from only common image sequences, overcoming the
cost and scalability limitations of traditional 3D scanning. Our approach
adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed
floors a common issue in robotic traversal data. We introduce Floor-Aware
Gaussian Splatting to ensure a clean, navigable ground plane, and a novel
mesh-free traversability checking algorithm that constructs a topological graph
by directly analyzing rendered views. We demonstrate our system's ability to
generate valid, large-scale navigation graphs from real-world data. A video
demonstration is avilable at https://youtu.be/tTiIQt6nXC8

</details>


### [22] [Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance](https://arxiv.org/abs/2510.24457)
*Jorge Vicente-Martinez,Edgar Ramirez-Laboreo*

Main category: cs.RO

TL;DR: 提出基于微分平坦性的3D桥式起重机最优轨迹生成方法，能够直接处理非线性摩擦和碰撞避免等复杂约束，实现快速安全的起重机运动。


<details>
  <summary>Details</summary>
Motivation: 现有起重机轨迹生成方法难以有效处理非线性摩擦和碰撞避免等复杂物理约束，导致执行器饱和和碰撞风险，影响起重机运动的安全性和效率。

Method: 利用微分平坦性框架，直接纳入非线性摩擦和负载绳索碰撞避免等复杂物理动态约束，仅在终点约束负载摆动以实现激进运动。

Result: 对比仿真验证表明，忽略干摩擦会导致执行器饱和和碰撞，而摩擦建模是实现快速安全起重机轨迹的基本要求。

Conclusion: 摩擦建模对于起重机快速安全轨迹生成至关重要，所提出的微分平坦性方法能有效处理复杂约束，提升起重机运动性能。

Abstract: This paper presents an optimal trajectory generation method for 3D overhead
cranes by leveraging differential flatness. This framework enables the direct
inclusion of complex physical and dynamic constraints, such as nonlinear
friction and collision avoidance for both payload and rope. Our approach allows
for aggressive movements by constraining payload swing only at the final point.
A comparative simulation study validates our approach, demonstrating that
neglecting dry friction leads to actuator saturation and collisions. The
results show that friction modeling is a fundamental requirement for fast and
safe crane trajectories.

</details>


### [23] [Supervisory Measurement-Guided Noise Covariance Estimation](https://arxiv.org/abs/2510.24508)
*Haoying Li,Yifan Peng,Junfeng Wu*

Main category: cs.RO

TL;DR: 提出了一种双层优化方法，通过贝叶斯视角将噪声协方差估计问题分解为里程计和监控测量的联合似然函数，实现了信息利用与计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 传感器噪声协方差在实际应用中难以准确确定，受环境变化、前端处理等因素影响，而可靠的姿态估计依赖于准确的噪声协方差规范。

Method: 采用双层优化框架：下层使用不变扩展卡尔曼滤波器进行轨迹估计，上层通过梯度更新优化协方差；通过因子分解将嵌套贝叶斯依赖转换为链式结构，实现并行计算。

Result: 在合成和真实数据集上的实验表明，该方法相比现有基线方法具有更高的效率。

Conclusion: 所提出的双层优化方法能够有效估计噪声协方差，在保持计算效率的同时提高了状态估计的可靠性。

Abstract: Reliable state estimation hinges on accurate specification of sensor noise
covariances, which weigh heterogeneous measurements. In practice, these
covariances are difficult to identify due to environmental variability,
front-end preprocessing, and other reasons. We address this by formulating
noise covariance estimation as a bilevel optimization that, from a Bayesian
perspective, factorizes the joint likelihood of so-called odometry and
supervisory measurements, thereby balancing information utilization with
computational efficiency. The factorization converts the nested Bayesian
dependency into a chain structure, enabling efficient parallel computation: at
the lower level, an invariant extended Kalman filter with state augmentation
estimates trajectories, while a derivative filter computes analytical gradients
in parallel for upper-level gradient updates. The upper level refines the
covariance to guide the lower-level estimation. Experiments on synthetic and
real-world datasets show that our method achieves higher efficiency over
existing baselines.

</details>


### [24] [Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems](https://arxiv.org/abs/2510.24515)
*Malintha Fernando,Petter Ögren,Silun Zhang*

Main category: cs.RO

TL;DR: 本文提出了随机奖励收集游戏(SPCG)作为团队定向问题(TOP)的扩展，用于在自利机器人、能量约束和随机转移条件下进行规划。通过理论分析和算法设计，证明了在特定条件下存在纯纳什均衡，并开发了两种算法来学习有效的竞争策略。


<details>
  <summary>Details</summary>
Motivation: 现有的团队定向问题(TOP)变体假设所有机器人合作实现单一目标，无法扩展到奖励稀缺环境中机器人竞争的场景。需要解决自利机器人在图结构上竞争性规划的问题。

Method: 提出了两种算法：序数排名搜索(ORS)用于获取游戏阶段中临时形成的局部邻域中的有效排名，以及虚构序数响应学习(FORL)用于针对高排名对手学习最佳响应策略。

Result: 在道路网络和合成图上的实证评估表明：1) ORS的状态混叠使学习策略比使用全局索引的策略更能扩展到大规模团队；2) FORL训练的策略比其他多智能体训练方法在非平衡奖励分布下泛化能力更好；3) 学习策略达到了87%-95%的最优性。

Conclusion: SPCG框架成功扩展了TOP以处理竞争性多机器人规划，提出的算法在可扩展性和泛化性方面表现优异，学习策略接近最优解。

Abstract: The Team Orienteering Problem (TOP) generalizes many real-world multi-robot
scheduling and routing tasks that occur in autonomous mobility, aerial
logistics, and surveillance applications. While many flavors of the TOP exist
for planning in multi-robot systems, they assume that all the robots cooperate
toward a single objective; thus, they do not extend to settings where the
robots compete in reward-scarce environments. We propose Stochastic
Prize-Collecting Games (SPCG) as an extension of the TOP to plan in the
presence of self-interested robots operating on a graph, under energy
constraints and stochastic transitions. A theoretical study on complete and
star graphs establishes that there is a unique pure Nash equilibrium in SPCGs
that coincides with the optimal routing solution of an equivalent TOP given a
rank-based conflict resolution rule. This work proposes two algorithms: Ordinal
Rank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in
temporarily-formed local neighborhoods during the games' stages, and Fictitious
Ordinal Response Learning (FORL) to obtain best-response policies against one's
senior-rank opponents. Empirical evaluations conducted on road networks and
synthetic graphs under both dynamic and stationary prize distributions show
that 1) the state-aliasing induced by OR-conditioning enables learning policies
that scale more efficiently to large team sizes than those trained with the
global index, and 2) Policies trained with FORL generalize better to imbalanced
prize distributions than those with other multi-agent training methods.
Finally, the learned policies in the SPCG achieved between 87% and 95%
optimality compared to an equivalent TOP solution obtained by mixed-integer
linear programming.

</details>


### [25] [GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots](https://arxiv.org/abs/2510.24533)
*Yuan Shen,Yuze Hong,Guangyang Zeng,Tengfei Zhang,Pui Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: GeVI-SLAM是一种重力增强的立体视觉惯性SLAM系统，通过利用立体相机深度估计和重力初始化，解决了水下机器人SLAM中的视觉退化和IMU运动激励不足问题。


<details>
  <summary>Details</summary>
Motivation: 水下机器人VI SLAM面临频繁视觉退化和IMU运动激励不足的挑战，导致定位和建图精度下降。

Method: 使用立体相机直接深度估计消除尺度估计需求；通过精确重力初始化解耦俯仰和滚转，采用4自由度PnP问题求解；提出偏差消除的4-DOF PnP估计器；动态运动时联合估计IMU协方差进行6-DOF位姿优化。

Result: 在模拟和真实世界数据上的广泛实验表明，GeVI-SLAM相比最先进方法具有更高的精度和稳定性。

Conclusion: GeVI-SLAM通过重力增强和创新的4-DOF PnP方法，有效解决了水下VI SLAM的挑战，实现了高精度和稳定的性能。

Abstract: Accurate visual inertial simultaneous localization and mapping (VI SLAM) for
underwater robots remains a significant challenge due to frequent visual
degeneracy and insufficient inertial measurement unit (IMU) motion excitation.
In this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system
designed to address these issues. By leveraging the stereo camera's direct
depth estimation ability, we eliminate the need to estimate scale during IMU
initialization, enabling stable operation even under low acceleration dynamics.
With precise gravity initialization, we decouple the pitch and roll from the
pose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point
(PnP) problem for pose tracking. This allows the use of a minimal 3-point
solver, which significantly reduces computational time to reject outliers
within a Random Sample Consensus framework. We further propose a
bias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the
relative pose converges to the true value as the feature number increases. To
handle dynamic motion, we refine the full 6-DOF pose while jointly estimating
the IMU covariance, enabling adaptive weighting of the gravity prior. Extensive
experiments on simulated and real-world data demonstrate that GeVI-SLAM
achieves higher accuracy and greater stability compared to state-of-the-art
methods.

</details>


### [26] [An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments](https://arxiv.org/abs/2510.24554)
*Vignesh Kottayam Viswanathan,Yifan Bai,Scott Fredriksson,Sumeet Satpute,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出分层框架解决机器人巡检中的环境不确定性，通过全局规划与局部重规划相结合，在保持全局覆盖目标的同时适应局部地形变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖已知环境模型规划巡检路线，但实际环境与模型存在差异（自然或人为因素导致地形变化或路径阻塞），需要能够适应环境不确定性的解决方案。

Method: 分层框架：1）基于历史地图生成全局视点规划；2）根据当前场景形态进行局部视点重规划。该层次结构保持全局覆盖目标，同时实现对局部表面形态的适应性调整。

Result: 在真实地下矿井环境中使用四足机器人进行了部署验证，证明方法能够有效应对环境不确定性并完成巡检任务。

Conclusion: 所提出的分层框架能够在环境不确定性下保持鲁棒性，使局部自主系统能够适应环境变化并成功完成巡检任务。

Abstract: In this work, we present a hierarchical framework designed to support robotic
inspection under environment uncertainty. By leveraging a known environment
model, existing methods plan and safely track inspection routes to visit points
of interest. However, discrepancies between the model and actual site
conditions, caused by either natural or human activities, can alter the surface
morphology or introduce path obstructions. To address this challenge, the
proposed framework divides the inspection task into: (a) generating the initial
global view-plan for region of interests based on a historical map and (b)
local view replanning to adapt to the current morphology of the inspection
scene. The proposed hierarchy preserves global coverage objectives while
enabling reactive adaptation to the local surface morphology. This enables the
local autonomy to remain robust against environment uncertainty and complete
the inspection tasks. We validate the approach through deployments in
real-world subterranean mines using quadrupedal robot.

</details>


### [27] [Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots](https://arxiv.org/abs/2510.24571)
*Hongxu Zhao,Guangyang Zeng,Yunling Shao,Tengfei Zhang,Junfeng Wu*

Main category: cs.RO

TL;DR: 提出了一种统一迭代校准(UIC)框架，用于多传感器系统的外参和时间偏移联合估计，特别针对水下SLAM中的DVL传感器校准问题。


<details>
  <summary>Details</summary>
Motivation: 现有DVL校准方法要么局限于特定传感器配置，要么依赖过度简化的假设，且没有同时估计平移外参和时间偏移的方法。

Method: 采用最大后验概率估计框架，结合高斯过程运动先验进行高保真运动插值，通过交替执行GP运动状态更新和基于梯度的校准变量更新。

Result: 开发了开源的DVL-相机校准工具箱，仿真和真实世界测试验证了方法的有效性。

Conclusion: UIC框架不仅适用于水下应用，其GP先验集成和可靠初始化程序的设计也可广泛应用于其他多传感器校准问题。

Abstract: The calibration of extrinsic parameters and clock offsets between sensors for
high-accuracy performance in underwater SLAM systems remains insufficiently
explored. Existing methods for Doppler Velocity Log (DVL) calibration are
either constrained to specific sensor configurations or rely on oversimplified
assumptions, and none jointly estimate translational extrinsics and time
offsets. We propose a Unified Iterative Calibration (UIC) framework for general
DVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a
Gaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC
alternates between efficient GP-based motion state updates and gradient-based
calibration variable updates, supported by a provably statistically consistent
sequential initialization scheme. The proposed UIC can be applied to IMU,
cameras and other modalities as co-sensors. We release an open-source
DVL-camera calibration toolbox. Beyond underwater applications, several aspects
of UIC-such as the integration of GP priors for MAP-based calibration and the
design of provably reliable initialization procedures-are broadly applicable to
other multi-sensor calibration problems. Finally, simulations and real-world
tests validate our approach.

</details>


### [28] [Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning](https://arxiv.org/abs/2510.24584)
*Jørgen Anker Olsen,Lars Rønhaug Pettersen,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出基于课程学习的强化学习框架，训练机器人Olympus实现精确高性能跳跃，包括垂直和水平跳跃策略，结合行走策略实现多功能动态运动能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人跳跃任务中奖励稀疏和动态行为探索困难的问题，开发无需参考轨迹的高性能跳跃策略，实现Sim2Real的有效迁移。

Method: 使用基于课程学习的强化学习框架，通过弹道运动定律稠密化稀疏奖励，采用参考状态初始化方案加速动态跳跃行为探索，结合行走策略。

Result: 实验验证了在多样化地形上的行走能力，水平跳跃达1.25米(厘米级精度)，垂直跳跃达1.0米，性能超越先前工作，成功跨越Sim2Real差距。

Conclusion: 所提方法能有效学习高性能跳跃策略，仅需少量修改即可实现全向跳跃，为机器人动态运动控制提供了有效解决方案。

Abstract: This paper presents a curriculum-based reinforcement learning framework for
training precise and high-performance jumping policies for the robot `Olympus'.
Separate policies are developed for vertical and horizontal jumps, leveraging a
simple yet effective strategy. First, we densify the inherently sparse jumping
reward using the laws of projectile motion. Next, a reference state
initialization scheme is employed to accelerate the exploration of dynamic
jumping behaviors without reliance on reference trajectories. We also present a
walking policy that, when combined with the jumping policies, unlocks versatile
and dynamic locomotion capabilities. Comprehensive testing validates walking on
varied terrain surfaces and jumping performance that exceeds previous works,
effectively crossing the Sim2Real gap. Experimental validation demonstrates
horizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to
1.0 m. Additionally, we show that with only minor modifications, the proposed
method can be used to learn omnidirectional jumping.

</details>


### [29] [GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization](https://arxiv.org/abs/2510.24623)
*Nicolai Steinke,Daniel Goehring*

Main category: cs.RO

TL;DR: GroundLoc是一种仅使用LiDAR的定位系统，通过鸟瞰图投影和关键点识别技术，在大规模户外环境中实现移动机器人的精确定位，支持多种传感器模型。


<details>
  <summary>Details</summary>
Motivation: 开发一种轻量级、高效的LiDAR定位系统，能够在大型户外环境中实现精确的实时定位，同时减少存储需求并支持多种传感器。

Method: 使用鸟瞰图投影聚焦地面区域，采用R2D2网络或SIFT方法识别关键点进行地图配准，将先验地图存储为2D栅格图像。

Result: 在SemanticKITTI和HeLiPR数据集上优于现有方法，多会话定位的平均轨迹误差低于50厘米，满足在线运行要求，每平方公里仅需4MB存储。

Conclusion: GroundLoc提供了一种高效、轻量的LiDAR定位解决方案，在精度、存储效率和传感器兼容性方面表现优异。

Abstract: In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing
on the perceived ground area and utilizes the place recognition network R2D2,
or alternatively, the non-learning approach Scale-Invariant Feature Transform
(SIFT), to identify and select keypoints for BEV image map registration. Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session
localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)
well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime
requirements. The system supports various sensor models, as evidenced by
evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,
and Livox Avia sensors. The prior maps are stored as 2D raster image maps,
which can be created from a single drive and require only 4 MB of storage per
square kilometer. The source code is available at
https://github.com/dcmlr/groundloc.

</details>


### [30] [Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder](https://arxiv.org/abs/2510.24671)
*Li Li,Tobias Brinkmann,Till Temmen,Markus Eisenbarth,Jakob Andert*

Main category: cs.RO

TL;DR: 提出了一种基于Transformer增强的条件变分自编码器(CVAE-T)模型，用于生成环岛多智能体交通场景，能够准确重建原始场景并生成真实多样的合成场景。


<details>
  <summary>Details</summary>
Motivation: 随着智能驾驶功能在量产车中的集成度提高，确保其功能性和鲁棒性面临更大挑战。相比传统道路测试，基于场景的虚拟测试在时间成本效率、可重复性和边缘案例探索方面具有显著优势。

Method: 使用Transformer增强的条件变分自编码器(CVAE-T)模型，针对具有高车辆动态和复杂布局的环岛交通场景进行多智能体场景生成。

Result: 模型能够准确重建原始场景并生成真实多样的合成场景。潜在空间分析显示部分解缠，多个潜在维度对场景属性（如车辆进入时间、退出时间和速度曲线）具有明显且可解释的影响。

Conclusion: 该模型能够为涉及多智能体交互的智能驾驶功能验证生成场景，同时为这些功能的开发和迭代改进提供数据增强。

Abstract: With the increasing integration of intelligent driving functions into
serial-produced vehicles, ensuring their functionality and robustness poses
greater challenges. Compared to traditional road testing, scenario-based
virtual testing offers significant advantages in terms of time and cost
efficiency, reproducibility, and exploration of edge cases. We propose a
Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for
generating multi-agent traffic scenarios in roundabouts, which are
characterized by high vehicle dynamics and complex layouts, yet remain
relatively underexplored in current research. The results show that the
proposed model can accurately reconstruct original scenarios and generate
realistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators
(KPIs) are employed to evaluate the interactive behavior in the generated
scenarios. Analysis of the latent space reveals partial disentanglement, with
several latent dimensions exhibiting distinct and interpretable effects on
scenario attributes such as vehicle entry timing, exit timing, and velocity
profiles. The results demonstrate the model's capability to generate scenarios
for the validation of intelligent driving functions involving multi-agent
interactions, as well as to augment data for their development and iterative
improvement.

</details>


### [31] [Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis](https://arxiv.org/abs/2510.24676)
*Jiaxuan Zhang,Yuquan Leng,Yixuan Guo,Chenglong Fu*

Main category: cs.RO

TL;DR: 使用健康脚踝上的惯性传感器指导截肢者跨越障碍，通过遗传算法优化神经网络结构来预测大腿和膝关节角度，实现100%步态相位估计准确率。


<details>
  <summary>Details</summary>
Motivation: 解决截肢者穿戴动力型经股骨假肢时难以跨越障碍和复杂地形的问题。

Method: 在健康脚踝放置惯性传感器，用遗传算法计算最优神经网络结构预测关节角度，结合步态进展预测算法确定膝关节电机驱动角度。

Result: 当大腿角度数据添加的高斯噪声标准差小于1时，能有效消除噪声干扰，在150Hz下实现100%步态相位估计准确率，大腿角度预测误差8.71%，膝关节角度预测误差6.78%。

Conclusion: 该方法能准确预测步态进展和关节角度，对动力型经股骨假肢的障碍跨越具有重要实用价值。

Abstract: For amputees with powered transfemoral prosthetics, navigating obstacles or
complex terrain remains challenging. This study addresses this issue by using
an inertial sensor on the sound ankle to guide obstacle-crossing movements. A
genetic algorithm computes the optimal neural network structure to predict the
required angles of the thigh and knee joints. A gait progression prediction
algorithm determines the actuation angle index for the prosthetic knee motor,
ultimately defining the necessary thigh and knee angles and gait progression.
Results show that when the standard deviation of Gaussian noise added to the
thigh angle data is less than 1, the method can effectively eliminate noise
interference, achieving 100\% accuracy in gait phase estimation under 150 Hz,
with thigh angle prediction error being 8.71\% and knee angle prediction error
being 6.78\%. These findings demonstrate the method's ability to accurately
predict gait progression and joint angles, offering significant practical value
for obstacle negotiation in powered transfemoral prosthetics.

</details>


### [32] [Fare: Failure Resilience in Learned Visual Navigation Control](https://arxiv.org/abs/2510.24680)
*Zishuo Wang,Joel Loo,David Hsu*

Main category: cs.RO

TL;DR: Fare框架构建具有故障恢复能力的模仿学习策略，通过OOD检测和识别实现自动故障恢复，无需显式故障数据。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在分布外场景中容易发生不可预测的故障，需要能够检测并自动从故障中恢复的弹性策略。

Method: 在模仿学习策略中嵌入OOD检测和识别机制，无需使用显式故障数据，并配对恢复启发式方法。

Result: 真实世界实验表明Fare能够在两种不同策略架构上实现故障恢复，在复杂环境中实现鲁棒的长距离导航。

Conclusion: Fare框架成功构建了故障弹性模仿学习策略，通过故障识别和恢复机制显著提升了导航系统的鲁棒性。

Abstract: While imitation learning (IL) enables effective visual navigation, IL
policies are prone to unpredictable failures in out-of-distribution (OOD)
scenarios. We advance the notion of failure-resilient policies, which not only
detect failures but also recover from them automatically. Failure recognition
that identifies the factors causing failure is key to informing recovery: e.g.
pinpointing image regions triggering failure detections can provide cues to
guide recovery. We present Fare, a framework to construct failure-resilient IL
policies, embedding OOD-detection and recognition in them without using
explicit failure data, and pairing them with recovery heuristics. Real-world
experiments show that Fare enables failure recovery across two different policy
architectures, enabling robust long-range navigation in complex environments.

</details>


### [33] [A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers](https://arxiv.org/abs/2510.24683)
*Caleb Escobedo,Nataliya Nechyporenko,Shreyas Kadekodi,Alessandro Roncone*

Main category: cs.RO

TL;DR: 提出了一个分析物体感知控制器的框架，重点关注运动学、运动曲线和虚拟约束三个设计要素，并通过基本机器人-障碍物实验场景验证机器人行为。


<details>
  <summary>Details</summary>
Motivation: 实时控制是机器人在动态物体环境中安全操作的关键方面，需要开发能够预测和避免碰撞的物体感知控制器。

Method: 建立分析框架，重点关注运动学、运动曲线和虚拟约束三个设计要素，使用基本机器人-障碍物实验场景进行验证，并比较三个代表性物体感知控制器。

Result: 分析发现物体感知控制器的设计往往缺乏运动学考虑、控制点连续性和运动曲线稳定性。

Conclusion: 该框架可用于未来设计、比较和基准测试避障方法。

Abstract: Real-time control is an essential aspect of safe robot operation in the real
world with dynamic objects. We present a framework for the analysis of
object-aware controllers, methods for altering a robot's motion to anticipate
and avoid possible collisions. This framework is focused on three design
considerations: kinematics, motion profiles, and virtual constraints.
Additionally, the analysis in this work relies on verification of robot
behaviors using fundamental robot-obstacle experimental scenarios. To showcase
the effectiveness of our method we compare three representative object-aware
controllers. The comparison uses metrics originating from the design
considerations. From the analysis, we find that the design of object-aware
controllers often lacks kinematic considerations, continuity of control points,
and stability in movement profiles. We conclude that this framework can be used
in the future to design, compare, and benchmark obstacle avoidance methods.

</details>


### [34] [Embodying Physical Computing into Soft Robots](https://arxiv.org/abs/2510.24692)
*Jun Wang,Ziyang Zhou,Ardalan Kahak,Suyi Li*

Main category: cs.RO

TL;DR: 本文提出了将物理计算嵌入软体机器人的框架，讨论了三种独特策略：模拟振荡器、物理储备池计算和物理算法计算，使软体机器人能够执行复杂行为而无需传统电子元件。


<details>
  <summary>Details</summary>
Motivation: 软化和集成计算机与控制器是软体机器人实现日常使用鲁棒性和智能化的关键前沿领域，物理计算为实现这一目标提供了有前景的途径。

Method: 提出将物理计算嵌入软体机器人的框架，重点介绍了三种具体方法：模拟振荡器、物理储备池计算和物理算法计算，这些方法通过机械计算内核的内部相互作用来处理输入输出。

Result: 这些嵌入式计算机使软体机器人能够执行复杂行为，包括带避障的协调运动、有效载荷重量和方向分类，以及基于逻辑规则的可编程操作，无需依赖CMOS电子元件。

Conclusion: 本文详细阐述了嵌入式物理计算的工作原理，综述了当前最新进展，并为未来发展提供了前瞻性视角，展示了物理计算在软体机器人领域的巨大潜力。

Abstract: Softening and onboarding computers and controllers is one of the final
frontiers in soft robotics towards their robustness and intelligence for
everyday use. In this regard, embodying soft and physical computing presents
exciting potential. Physical computing seeks to encode inputs into a mechanical
computing kernel and leverage the internal interactions among this kernel's
constituent elements to compute the output. Moreover, such input-to-output
evolution can be re-programmable. This perspective paper proposes a framework
for embodying physical computing into soft robots and discusses three unique
strategies in the literature: analog oscillators, physical reservoir computing,
and physical algorithmic computing. These embodied computers enable the soft
robot to perform complex behaviors that would otherwise require CMOS-based
electronics -- including coordinated locomotion with obstacle avoidance,
payload weight and orientation classification, and programmable operation based
on logical rules. This paper will detail the working principles of these
embodied physical computing methods, survey the current state-of-the-art, and
present a perspective for future development.

</details>
