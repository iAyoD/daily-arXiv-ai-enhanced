{"id": "2511.05680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05680", "abs": "https://arxiv.org/abs/2511.05680", "authors": ["Jeong-Jung Kim", "Doo-Yeol Koh", "Chang-Hyun Kim"], "title": "VLM-driven Skill Selection for Robotic Assembly Tasks", "comment": null, "summary": "This paper presents a robotic assembly framework that combines Vision-Language Models (VLMs) with imitation learning for assembly manipulation tasks. Our system employs a gripper-equipped robot that moves in 3D space to perform assembly operations. The framework integrates visual perception, natural language understanding, and learned primitive skills to enable flexible and adaptive robotic manipulation. Experimental results demonstrate the effectiveness of our approach in assembly scenarios, achieving high success rates while maintaining interpretability through the structured primitive skill decomposition.", "AI": {"tldr": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u6a21\u4eff\u5b66\u4e60\u7684\u673a\u5668\u4eba\u88c5\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u57fa\u7840\u6280\u80fd\u5b9e\u73b0\u7075\u6d3b\u7684\u88c5\u914d\u64cd\u4f5c", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u4e2d\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u611f\u77e5\u3001\u8bed\u8a00\u7406\u89e3\u548c\u7075\u6d3b\u64cd\u4f5c\u80fd\u529b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u9002\u5e94\u6027\u5f3a\u7684\u88c5\u914d\u7cfb\u7edf", "method": "\u4f7f\u7528\u914d\u5907\u5939\u722a\u7684\u673a\u5668\u4eba\u57283D\u7a7a\u95f4\u4e2d\u79fb\u52a8\u6267\u884c\u88c5\u914d\u64cd\u4f5c\uff0c\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u611f\u77e5\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u7684\u57fa\u7840\u6280\u80fd", "result": "\u5728\u88c5\u914d\u573a\u666f\u4e2d\u53d6\u5f97\u9ad8\u6210\u529f\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u7ed3\u6784\u5316\u57fa\u7840\u6280\u80fd\u5206\u89e3\u4fdd\u6301\u53ef\u89e3\u91ca\u6027", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u6a21\u4eff\u5b66\u4e60\uff0c\u4e3a\u673a\u5668\u4eba\u88c5\u914d\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.05785", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05785", "abs": "https://arxiv.org/abs/2511.05785", "authors": ["Lianhao Yin", "Haiping Yu", "Pascal Spino", "Daniela Rus"], "title": "A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms", "comment": null, "summary": "Biological swarms, such as ant colonies, achieve collective goals through decentralized and stochastic individual behaviors. Similarly, physical systems composed of gases, liquids, and solids exhibit random particle motion governed by entropy maximization, yet do not achieve collective objectives. Despite this analogy, no unified framework exists to explain the stochastic behavior in both biological and physical systems. Here, we present empirical evidence from \\textit{Formica polyctena} ants that reveals a shared statistical mechanism underlying both systems: maximization under different energy function constraints. We further demonstrate that robotic swarms governed by this principle can exhibit scalable, decentralized cooperation, mimicking physical phase-like behaviors with minimal individual computation. These findings established a unified stochastic model linking biological, physical, and robotic swarms, offering a scalable principle for designing robust and intelligent swarm robotics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u968f\u673a\u6a21\u578b\uff0c\u5c06\u751f\u7269\u3001\u7269\u7406\u548c\u673a\u5668\u4eba\u96c6\u7fa4\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5171\u4eab\u7684\u7edf\u8ba1\u673a\u5236\uff1a\u5728\u4e0d\u540c\u80fd\u91cf\u51fd\u6570\u7ea6\u675f\u4e0b\u7684\u6700\u5927\u5316\u539f\u7406\u3002", "motivation": "\u751f\u7269\u96c6\u7fa4\uff08\u5982\u8681\u7fa4\uff09\u901a\u8fc7\u5206\u6563\u548c\u968f\u673a\u7684\u4e2a\u4f53\u884c\u4e3a\u5b9e\u73b0\u96c6\u4f53\u76ee\u6807\uff0c\u800c\u7269\u7406\u7cfb\u7edf\uff08\u5982\u6c14\u4f53\u3001\u6db2\u4f53\u3001\u56fa\u4f53\uff09\u4e2d\u7684\u968f\u673a\u7c92\u5b50\u8fd0\u52a8\u53d7\u71b5\u6700\u5927\u5316\u652f\u914d\uff0c\u4f46\u4e0d\u5b9e\u73b0\u96c6\u4f53\u76ee\u6807\u3002\u76ee\u524d\u7f3a\u4e4f\u89e3\u91ca\u8fd9\u4e24\u79cd\u7cfb\u7edf\u968f\u673a\u884c\u4e3a\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u901a\u8fc7Formica polyctena\u8682\u8681\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u63ed\u793a\u4e86\u751f\u7269\u548c\u7269\u7406\u7cfb\u7edf\u5171\u4eab\u7684\u7edf\u8ba1\u673a\u5236\uff1a\u5728\u4e0d\u540c\u80fd\u91cf\u51fd\u6570\u7ea6\u675f\u4e0b\u7684\u6700\u5927\u5316\u3002\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u53d7\u6b64\u539f\u7406\u63a7\u5236\u7684\u673a\u5668\u4eba\u96c6\u7fa4\u53ef\u4ee5\u8868\u73b0\u51fa\u53ef\u6269\u5c55\u7684\u5206\u6563\u5408\u4f5c\u3002", "result": "\u53d1\u73b0\u751f\u7269\u548c\u7269\u7406\u7cfb\u7edf\u5171\u4eab\u76f8\u540c\u7684\u7edf\u8ba1\u673a\u5236\uff0c\u673a\u5668\u4eba\u96c6\u7fa4\u80fd\u591f\u6a21\u4eff\u7269\u7406\u76f8\u53d8\u884c\u4e3a\uff0c\u4e14\u53ea\u9700\u6700\u5c11\u7684\u4e2a\u4f53\u8ba1\u7b97\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u8fde\u63a5\u751f\u7269\u3001\u7269\u7406\u548c\u673a\u5668\u4eba\u96c6\u7fa4\u7684\u7edf\u4e00\u968f\u673a\u6a21\u578b\uff0c\u4e3a\u8bbe\u8ba1\u7a33\u5065\u548c\u667a\u80fd\u7684\u7fa4\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u539f\u7406\u3002"}}
{"id": "2511.05791", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05791", "abs": "https://arxiv.org/abs/2511.05791", "authors": ["Manav Kulshrestha", "S. Talha Bukhari", "Damon Conover", "Aniket Bera"], "title": "VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models", "comment": "8 pages, 4 figures, under review", "summary": "Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod \"impales\" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.", "AI": {"tldr": "VLAD-Grasp\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u6293\u53d6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u4eceRGB-D\u56fe\u50cf\u4e2d\u6062\u590d\u53ef\u6267\u884c\u7684\u6293\u53d6\u59ff\u6001\uff0c\u5728Cornell\u548cJacquard\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u76d1\u7763\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\uff0c\u4e14\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6765\u5904\u7406\u65b0\u7269\u4f53\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002", "method": "\u4f7f\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u76ee\u6807\u56fe\u50cf\uff08\u76f4\u6746\"\u523a\u7a7f\"\u7269\u4f53\u8868\u793a\u5bf9\u63e1\u6293\u53d6\uff09\uff0c\u9884\u6d4b\u6df1\u5ea6\u548c\u5206\u5272\u5c06\u5176\u63d0\u5347\u52303D\uff0c\u901a\u8fc7\u4e3b\u6210\u5206\u5206\u6790\u548c\u65e0\u5bf9\u5e94\u4f18\u5316\u5bf9\u9f50\u751f\u6210\u548c\u89c2\u6d4b\u70b9\u4e91\u6765\u6062\u590d\u6293\u53d6\u59ff\u6001\u3002", "result": "\u5728Cornell\u548cJacquard\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u76d1\u7763\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728Franka Research 3\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u9896\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u7684\u80fd\u529b\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u53ef\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5f3a\u5927\u5148\u9a8c\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u6293\u53d6\u68c0\u6d4b\u3002"}}
{"id": "2511.05858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05858", "abs": "https://arxiv.org/abs/2511.05858", "authors": ["Chuanyu Li", "Chaoyi Liu", "Daotan Wang", "Shuyu Zhang", "Lusong Li", "Zecui Zeng", "Fangchen Liu", "Jing Xu", "Rui Chen"], "title": "ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface", "comment": null, "summary": "Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems.", "AI": {"tldr": "ViTaMIn-B\u662f\u4e00\u4e2a\u7528\u4e8e\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u7684\u624b\u6301\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\uff0c\u5305\u542b\u65b0\u578b\u67d4\u6027\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668DuoTact\u548c\u57fa\u4e8eMeta Quest\u63a7\u5236\u5668\u76846-DoF\u53cc\u624b\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u624b\u6301\u8bbe\u5907\u7cfb\u7edf\u7f3a\u4e4f\u5f3a\u5927\u7684\u89e6\u89c9\u611f\u77e5\u548c\u53ef\u9760\u7684\u59ff\u6001\u8ddf\u8e2a\u80fd\u529b\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u53cc\u624b\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u3002", "method": "\u8bbe\u8ba1\u4e86\u67d4\u6027\u6846\u67b6\u7684DuoTact\u4f20\u611f\u5668\u4ee5\u627f\u53d7\u5927\u63a5\u89e6\u529b\u5e76\u6355\u83b7\u9ad8\u5206\u8fa8\u7387\u63a5\u89e6\u51e0\u4f55\uff1b\u63d0\u51fa\u5c06\u4f20\u611f\u5668\u5168\u5c40\u53d8\u5f62\u91cd\u5efa\u4e3a3D\u70b9\u4e91\u4f5c\u4e3a\u7b56\u7565\u8f93\u5165\uff1b\u5f00\u53d1\u4e86\u57fa\u4e8eMeta Quest\u63a7\u5236\u5668\u76846-DoF\u53cc\u624b\u59ff\u6001\u83b7\u53d6\u65b9\u6cd5\u3002", "result": "\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u4e86\u7cfb\u7edf\u7684\u9ad8\u6548\u6027\u548c\u53ef\u7528\u6027\uff1b\u5728\u56db\u4e2a\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "ViTaMIn-B\u7cfb\u7edf\u4e3a\u590d\u6742\u53cc\u624b\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u9ad8\u6548\u7684\u6570\u636e\u91c7\u96c6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06141", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06141", "abs": "https://arxiv.org/abs/2511.06141", "authors": ["Marc Duclusaud", "Gr\u00e9goire Passault", "Vincent Padois", "Olivier Ly"], "title": "PlaCo: a QP-based robot planning and control framework", "comment": null, "summary": "This article introduces PlaCo, a software framework designed to simplify the formulation and solution of Quadratic Programming (QP)-based planning and control problems for robotic systems. PlaCo provides a high-level interface that abstracts away the low-level mathematical formulation of QP problems, allowing users to specify tasks and constraints in a modular and intuitive manner. The framework supports both Python bindings for rapid prototyping and a C++ implementation for real-time performance.", "AI": {"tldr": "PlaCo\u662f\u4e00\u4e2a\u7b80\u5316\u673a\u5668\u4eba\u7cfb\u7edfQP\u89c4\u5212\u548c\u63a7\u5236\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u63d0\u4f9b\u9ad8\u5c42\u63a5\u53e3\u62bd\u8c61\u5e95\u5c42\u6570\u5b66\u516c\u5f0f\uff0c\u652f\u6301Python\u5feb\u901f\u539f\u578b\u548cC++\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u7b80\u5316QP\u89c4\u5212\u63a7\u5236\u95ee\u9898\u7684\u8868\u8ff0\u548c\u6c42\u89e3\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u4ee5\u6a21\u5757\u5316\u548c\u76f4\u89c2\u7684\u65b9\u5f0f\u6307\u5b9a\u4efb\u52a1\u548c\u7ea6\u675f\uff0c\u800c\u65e0\u9700\u5904\u7406\u5e95\u5c42\u6570\u5b66\u516c\u5f0f\u3002", "method": "\u63d0\u4f9b\u9ad8\u5c42\u63a5\u53e3\u62bd\u8c61QP\u95ee\u9898\u7684\u5e95\u5c42\u6570\u5b66\u8868\u8ff0\uff0c\u652f\u6301Python\u7ed1\u5b9a\u7528\u4e8e\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u4ee5\u53caC++\u5b9e\u73b0\u7528\u4e8e\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u5f00\u53d1\u4e86PlaCo\u6846\u67b6\uff0c\u80fd\u591f\u7b80\u5316\u673a\u5668\u4eba\u7cfb\u7edfQP\u89c4\u5212\u63a7\u5236\u95ee\u9898\u7684\u5236\u5b9a\u548c\u6c42\u89e3\u8fc7\u7a0b\u3002", "conclusion": "PlaCo\u6846\u67b6\u6210\u529f\u7b80\u5316\u4e86QP\u89c4\u5212\u63a7\u5236\u95ee\u9898\u7684\u8868\u8ff0\u548c\u6c42\u89e3\uff0c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u6613\u7528\u7684\u5f00\u53d1\u5de5\u5177\u3002"}}
{"id": "2511.06182", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06182", "abs": "https://arxiv.org/abs/2511.06182", "authors": ["Peican Lin", "Gan Sun", "Chenxi Liu", "Fazeng Li", "Weihong Ren", "Yang Cong"], "title": "OpenVLN: Open-world aerial Vision-Language Navigation", "comment": "Content: 8 pages 4 figures, conference under review", "summary": "Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u7684\u5f00\u653e\u4e16\u754c\u7a7a\u4e2d\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6OpenVLN\uff0c\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u7ea6\u675f\u4e0b\u6267\u884c\u8bed\u8a00\u5f15\u5bfc\u98de\u884c\uff0c\u5e76\u589e\u5f3a\u590d\u6742\u7a7a\u4e2d\u73af\u5883\u4e2d\u7684\u957f\u65f6\u7a0b\u8f68\u8ff9\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5916\u7a7a\u4e2d\u73af\u5883\u590d\u6742\u6027\u5e26\u6765\u7684\u6570\u636e\u91c7\u96c6\u6311\u6218\u548c\u65e0\u4eba\u673a\u957f\u65f6\u7a0b\u8f68\u8ff9\u89c4\u5212\u9700\u6c42\uff0c\u8fd9\u4e9b\u4e3a\u7a7a\u4e2d\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u5f15\u5165\u4e86\u65b0\u7684\u590d\u6742\u6027\u3002", "method": "\u91cd\u65b0\u914d\u7f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u4f18\u5316VLM\u7528\u4e8e\u65e0\u4eba\u673a\u5bfc\u822a\u4efb\u52a1\uff0c\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u7b56\u7565\u9ad8\u6548\u5fae\u8c03VLM\uff1b\u540c\u65f6\u5f15\u5165\u957f\u65f6\u7a0b\u89c4\u5212\u5668\u8fdb\u884c\u8f68\u8ff9\u5408\u6210\uff0c\u901a\u8fc7\u57fa\u4e8e\u4ef7\u503c\u7684\u5956\u52b1\u52a8\u6001\u751f\u6210\u7cbe\u786e\u7684\u65e0\u4eba\u673a\u52a8\u4f5c\u3002", "result": "\u5728TravelUAV\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u53474.34%\uff0cOracle\u6210\u529f\u7387\u63d0\u53476.19%\uff0c\u8def\u5f84\u957f\u5ea6\u52a0\u6743\u6210\u529f\u7387\u63d0\u53474.07%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u7a7a\u4e2d\u73af\u5883\u4e2d\u4e3a\u957f\u65f6\u7a0b\u65e0\u4eba\u673a\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2511.06202", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06202", "abs": "https://arxiv.org/abs/2511.06202", "authors": ["Shahram Najam Syed", "Yatharth Ahuja", "Arthur Jakobsson", "Jeff Ichnowski"], "title": "ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval", "comment": "10 pages, 5 figures, submitted to ICRA 2026. Equal contribution by first two authors", "summary": "Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.", "AI": {"tldr": "ExpReS-VLA\u901a\u8fc7\u7ecf\u9a8c\u56de\u653e\u548c\u68c0\u7d22\u6765\u4e13\u95e8\u5316\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5728\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u7ea697%\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9002\u5e94\u65b0\u90e8\u7f72\u73af\u5883\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5728\u6709\u9650\u4efb\u52a1\u96c6\u4e0a\u4fdd\u6301\u7a33\u5b9a\u9ad8\u6027\u80fd\u6bd4\u5e7f\u6cdb\u6cdb\u5316\u66f4\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u5b58\u50a8\u7d27\u51d1\u7279\u5f81\u8868\u793a\u800c\u975e\u539f\u59cb\u56fe\u50cf-\u52a8\u4f5c\u5bf9\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u68c0\u7d22\u76f8\u5173\u7ecf\u9a8c\u8fdb\u884c\u6307\u5bfc\uff0c\u91c7\u7528\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u5f3a\u8c03\u6210\u529f\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u9608\u503c\u6df7\u5408\u5bf9\u6bd4\u635f\u5931\u4ece\u6210\u529f\u548c\u5931\u8d25\u5c1d\u8bd5\u4e2d\u5b66\u4e60\u3002", "result": "\u5728LIBERO\u4eff\u771f\u57fa\u51c6\u4e0a\uff0c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u6210\u529f\u7387\u4ece82.6%\u63d0\u5347\u81f393.1%\uff0c\u957f\u89c6\u91ce\u4efb\u52a1\u4ece61%\u63d0\u5347\u81f372.3%\uff1b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c5\u4e2a\u64cd\u4f5c\u4efb\u52a1\u5728\u5df2\u89c1\u548c\u672a\u89c1\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u523098%\u6210\u529f\u7387\uff0c\u800c\u6734\u7d20\u5fae\u8c03\u5206\u522b\u4e3a84.7%\u548c32%\u3002", "conclusion": "ExpReS-VLA\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684VLA\u6a21\u578b\u4e13\u95e8\u5316\u65b9\u6cd5\uff0c\u4ec5\u970031\u79d2\u548c12\u4e2a\u6f14\u793a\u5373\u53ef\u5b8c\u6210\u9002\u5e94\uff0c\u9002\u5408\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u3002"}}
{"id": "2511.06240", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06240", "abs": "https://arxiv.org/abs/2511.06240", "authors": ["Tzu-Jung Lin", "Jia-Fong Yeh", "Hung-Ting Su", "Chung-Yi Lin", "Yi-Ting Chen", "Winston H. Hsu"], "title": "Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation", "comment": "Accepted to AAAI 2026", "summary": "In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u7684\u673a\u5668\u4eba\u57fa\u5ea7\u653e\u7f6e\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u548c\u51e0\u4f55\u53ef\u884c\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u63a5\u8fd1\u5ea6\u5bfc\u822a\u800c\u4e0d\u8003\u8651\u53ef\u64cd\u4f5c\u6027\uff0c\u5bfc\u81f4\u9891\u7e41\u7684\u64cd\u4f5c\u5931\u8d25\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8003\u8651\u8bed\u4e49\u7406\u89e3\u548c\u51e0\u4f55\u7ea6\u675f\u7684\u57fa\u5ea7\u653e\u7f6e\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4ece\u7c97\u5230\u7cbe\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u6784\u5efa\u8de8\u6a21\u6001\u8868\u793a\uff08\u53ef\u64cd\u4f5c\u6027RGB\u548c\u969c\u788d\u7269\u5730\u56fe+\uff09\uff0c\u5229\u7528VLM\u7684\u7c97\u7c92\u5ea6\u8bed\u4e49\u5148\u9a8c\u6307\u5bfc\u641c\u7d22\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u7ec6\u5316\u653e\u7f6e\u4f4d\u7f6e\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u7cfb\u7edf\u8fbe\u523085%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u51e0\u4f55\u89c4\u5212\u5668\u548c\u57fa\u4e8eVLM\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bc1\u660e\u4e86\u53ef\u64cd\u4f5c\u6027\u611f\u77e5\u548c\u591a\u6a21\u6001\u63a8\u7406\u5728\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u4e2d\u901a\u7528\u5316\u3001\u6307\u4ee4\u6761\u4ef6\u89c4\u5212\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.06267", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06267", "abs": "https://arxiv.org/abs/2511.06267", "authors": ["Jiayi Chen", "Wei Zhao", "Liangwang Ruan", "Baoquan Chen", "He Wang"], "title": "Robust Differentiable Collision Detection for General Objects", "comment": null, "summary": "Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u9ad8\u6548\u7684\u5fae\u5206\u78b0\u649e\u68c0\u6d4b\u6846\u67b6\uff0c\u652f\u6301\u51f8\u9762\u548c\u51f9\u9762\u7269\u4f53\uff0c\u901a\u8fc7\u8ddd\u79bb\u57fa\u968f\u673a\u5e73\u6ed1\u3001\u81ea\u9002\u5e94\u91c7\u6837\u548c\u7b49\u6548\u68af\u5ea6\u4f20\u8f93\u5b9e\u73b0\u7a33\u5065\u7684\u68af\u5ea6\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u78b0\u649e\u68c0\u6d4b\u7b97\u6cd5\uff08\u5982GJK+EPA\uff09\u4e0d\u53ef\u5fae\u5206\uff0c\u963b\u788d\u4e86\u68af\u5ea6\u6d41\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\uff08\u5982\u6293\u53d6\u548c\u64cd\u4f5c\uff09\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u652f\u6301\u51f8\u9762\u7269\u4f53\u4e14\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u8ddd\u79bb\u57fa\u4e00\u9636\u968f\u673a\u5e73\u6ed1\u3001\u81ea\u9002\u5e94\u91c7\u6837\u548c\u7b49\u6548\u68af\u5ea6\u4f20\u8f93\u6280\u672f\uff0c\u6784\u5efa\u652f\u6301\u51f8\u9762\u548c\u51f9\u9762\u7269\u4f53\u7684\u5fae\u5206\u78b0\u649e\u68c0\u6d4b\u6846\u67b6\u3002", "result": "\u5728DexGraspNet\u548cObjaverse\u7684\u590d\u6742\u7f51\u683c\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u7075\u5de7\u6293\u53d6\u5408\u6210\u4ee5\u4f18\u5316\u6293\u53d6\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u5fae\u5206\u78b0\u649e\u68c0\u6d4b\u6846\u67b6\u5728\u590d\u6742\u51e0\u4f55\u4f53\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u68af\u5ea6\u4f18\u5316\u652f\u6301\u3002"}}
{"id": "2511.06311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06311", "abs": "https://arxiv.org/abs/2511.06311", "authors": ["Seiichi Yamamoto", "Hiroki Ishizuka", "Takumi Kawasetsu", "Koh Hosoda", "Takayuki Kameoka", "Kango Yanagida", "Takato Horii", "Sei Ikeda", "Osamu Oshiro"], "title": "External Photoreflective Tactile Sensing Based on Surface Deformation Measurement", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\u673a\u68b0\u67d4\u987a\u6027\u7684\u89e6\u89c9\u4f20\u611f\u65b9\u6cd5\uff0c\u4f7f\u7528\u5916\u90e8\u53ef\u9644\u52a0\u7684\u5149\u53cd\u5c04\u6a21\u5757\u8bfb\u53d6\u7845\u80f6\u76ae\u80a4\u8868\u9762\u53d8\u5f62\u6765\u4f30\u8ba1\u63a5\u89e6\u529b\uff0c\u65e0\u9700\u5d4c\u5165\u89e6\u89c9\u4f20\u611f\u5668\u3002", "motivation": "\u5c06\u4f20\u611f\u5668\u7f6e\u4e8e\u63a5\u89e6\u754c\u9762\u4e4b\u5916\u53ef\u964d\u4f4e\u635f\u574f\u98ce\u9669\u3001\u4fdd\u6301\u67d4\u8f6f\u6027\uff0c\u5e76\u7b80\u5316\u5236\u9020\u548c\u7ef4\u62a4\u3002\u76f8\u6bd4\u6db2\u4f53\u586b\u5145\u6216\u5bfc\u7ebf\u5d4c\u5165\u7684\u89e6\u89c9\u76ae\u80a4\uff0c\u8be5\u6a21\u5757\u5316\u9644\u52a0\u67b6\u6784\u589e\u5f3a\u4e86\u8010\u7528\u6027\u3001\u51cf\u5c11\u4e86\u5e03\u7ebf\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u5149\u5b66\u4f20\u611f\u5143\u4ef6\u548c\u67d4\u987a\u76ae\u80a4\uff0c\u901a\u8fc7\u8bfb\u53d6\u76ae\u80a4\u5e94\u53d8\u6a21\u5f0f\u6765\u4f30\u8ba1\u63a5\u89e6\u529b\u3002\u5f00\u53d1\u4e86\u539f\u578b\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u5e76\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u6293\u624b\u4e0a\u8fdb\u884c\u96c6\u6210\u6f14\u793a\u3002", "result": "\u538b\u7f29\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u4e0e\u7406\u8bba\u4e00\u81f4\u7684\u5355\u8c03\u529b\u8f93\u51fa\u5173\u7cfb\u3001\u4f4e\u6ede\u540e\u6027\u3001\u9ad8\u91cd\u590d\u6027\u4ee5\u53ca\u5c0f\u54cd\u5e94\u538b\u75d5\u901f\u5ea6\u3002\u6a21\u5757\u80fd\u53ef\u9760\u68c0\u6d4b\u6293\u53d6\u4e8b\u4ef6\u3002", "conclusion": "\u5229\u7528\u8868\u9762\u67d4\u987a\u6027\u4e0e\u5916\u90e8\u5149\u5b66\u6a21\u5757\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u529b\u611f\u77e5\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u7075\u6d3b\u6027\u548c\u53ef\u5236\u9020\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u548c\u5b89\u5168\u4eba\u673a\u534f\u4f5c\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.06371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06371", "abs": "https://arxiv.org/abs/2511.06371", "authors": ["Yingnan Zhao", "Xinmiao Wang", "Dewei Wang", "Xinzhe Liu", "Dan Lu", "Qilong Han", "Peng Liu", "Chenjia Bai"], "title": "Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning", "comment": null, "summary": "Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io.", "AI": {"tldr": "\u63d0\u51faAdaptive Humanoid Control (AHC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6846\u67b6\u5b66\u4e60\u81ea\u9002\u5e94\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u6280\u80fd\u548c\u5730\u5f62\u95f4\u5207\u6362\u5e76\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u8fd0\u52a8\u6280\u80fd\u8bad\u7ec3\u72ec\u7acb\u7b56\u7565\uff0c\u5bfc\u81f4\u63a7\u5236\u5668\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5728\u590d\u6742\u5730\u5f62\u548c\u591a\u6837\u5316\u60c5\u5883\u4e0b\u8868\u73b0\u8106\u5f31\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u8bad\u7ec3\u591a\u4e2a\u4e3b\u8981\u8fd0\u52a8\u7b56\u7565\u5e76\u8fdb\u884c\u591a\u884c\u4e3a\u84b8\u998f\uff0c\u83b7\u5f97\u57fa\u7840\u591a\u884c\u4e3a\u63a7\u5236\u5668\uff1b2) \u5728\u591a\u6837\u5316\u5730\u5f62\u4e0a\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\uff0c\u6536\u96c6\u5728\u7ebf\u53cd\u9988\u589e\u5f3a\u5730\u5f62\u9002\u5e94\u6027\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u7684\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u60c5\u5883\u548c\u5730\u5f62\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u3002", "conclusion": "AHC\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4e0d\u540c\u6280\u80fd\u548c\u5730\u5f62\u95f4\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2511.06378", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06378", "abs": "https://arxiv.org/abs/2511.06378", "authors": ["Prajval Kumar Murali", "Mohsen Kaboli"], "title": "ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects", "comment": "Under review", "summary": "Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aArtReg\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u673a\u5668\u4eba\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u5bf9\u672a\u77e5\u7269\u4f53\uff08\u5355\u4e2a\u3001\u591a\u4e2a\u6216\u94f0\u63a5\u5f0f\uff09\u8fdb\u884c\u89c6\u89c9-\u89e6\u89c9\u8ddf\u8e2a\uff0c\u65e0\u9700\u5148\u9a8c\u51e0\u4f55\u6216\u8fd0\u52a8\u5b66\u77e5\u8bc6\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7ecf\u5e38\u9047\u5230\u5177\u6709\u590d\u6742\u7ed3\u6784\u548c\u94f0\u63a5\u7ec4\u4ef6\u7684\u672a\u77e5\u7269\u4f53\uff0c\u5982\u95e8\u3001\u62bd\u5c49\u3001\u67dc\u5b50\u548c\u5de5\u5177\u3002\u5728\u6ca1\u6709\u5148\u9a8c\u51e0\u4f55\u6216\u8fd0\u52a8\u5b66\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u611f\u77e5\u3001\u8ddf\u8e2a\u548c\u64cd\u4f5c\u8fd9\u4e9b\u7269\u4f53\u4ecd\u7136\u662f\u673a\u5668\u4eba\u5b66\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "ArtReg\u65b9\u6cd5\u5728SE(3)\u674e\u7fa4\u4e2d\u96c6\u6210\u89c6\u89c9-\u89e6\u89c9\u70b9\u4e91\u5230\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e2d\u7528\u4e8e\u70b9\u4e91\u914d\u51c6\u3002\u901a\u8fc7\u63a8\u6216\u62c9\u7b49\u6709\u76ee\u7684\u7684\u64cd\u4f5c\u52a8\u4f5c\u68c0\u6d4b\u53ef\u80fd\u7684\u94f0\u63a5\u5173\u8282\uff0c\u5e76\u5f00\u53d1\u4e86\u95ed\u73af\u63a7\u5236\u5668\u8fdb\u884c\u76ee\u6807\u9a71\u52a8\u7684\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5e7f\u6cdb\u8bc4\u4f30\u4e86\u5404\u79cd\u672a\u77e5\u7269\u4f53\uff0c\u5c55\u793a\u4e86\u5728\u53d8\u5316\u8d28\u5fc3\u3001\u4f4e\u5149\u7167\u6761\u4ef6\u548c\u6311\u6218\u6027\u89c6\u89c9\u80cc\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u5728\u6807\u51c6\u94f0\u63a5\u7269\u4f53\u6570\u636e\u96c6\u4e0a\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u59ff\u6001\u7cbe\u5ea6\u65b9\u9762\u6709\u6539\u8fdb\u3002", "conclusion": "\u5229\u7528\u89c6\u89c9-\u89e6\u89c9\u4fe1\u606f\u7684\u9c81\u68d2\u51c6\u786e\u59ff\u6001\u8ddf\u8e2a\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u611f\u77e5\u548c\u4ea4\u4e92\u672a\u89c1\u8fc7\u7684\u590d\u6742\u94f0\u63a5\u7269\u4f53\uff08\u5177\u6709\u65cb\u8f6c\u6216\u68f1\u67f1\u5173\u8282\uff09\u3002"}}
{"id": "2511.06385", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.06385", "abs": "https://arxiv.org/abs/2511.06385", "authors": ["Ralf R\u00f6mer", "Julian Balletshofer", "Jakob Thumm", "Marco Pavone", "Angela P. Schoellig", "Matthias Althoff"], "title": "From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies", "comment": "Project page: https://tum-lsy.github.io/pacs/. 8 pages, 4 figures", "summary": "Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8def\u5f84\u4e00\u81f4\u6027\u5b89\u5168\u8fc7\u6ee4\uff08PACS\uff09\u65b9\u6cd5\uff0c\u4e3a\u6269\u6563\u7b56\u7565\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u5b89\u5168\u884c\u4e3a\uff0c\u9700\u8981\u5916\u90e8\u5b89\u5168\u673a\u5236\u3002\u7136\u800c\u8fd9\u4e9b\u673a\u5236\u4f1a\u6539\u53d8\u52a8\u4f5c\uff0c\u5bfc\u81f4\u8bad\u7ec3\u5206\u5e03\u4e0d\u4e00\u81f4\u548c\u6027\u80fd\u4e0b\u964d", "method": "\u91c7\u7528\u8def\u5f84\u4e00\u81f4\u6027\u5236\u52a8\u65b9\u6cd5\uff0c\u5bf9\u751f\u6210\u7684\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u8f68\u8ff9\u8ba1\u7b97\uff0c\u4f7f\u7528\u57fa\u4e8e\u96c6\u5408\u7684\u53ef\u8fbe\u6027\u5206\u6790\u8fdb\u884c\u5b9e\u65f6\u5b89\u5168\u9a8c\u8bc1", "result": "PACS\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\uff0c\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u53cd\u5e94\u5f0f\u5b89\u5168\u65b9\u6cd5\uff08\u5982\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff09\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe68%", "conclusion": "PACS\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5b66\u4e60\u5230\u7684\u4efb\u52a1\u5b8c\u6210\u884c\u4e3a\u7684\u540c\u65f6\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1"}}
{"id": "2511.06397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06397", "abs": "https://arxiv.org/abs/2511.06397", "authors": ["Cong Wen", "Yunfei Li", "Kexin Liu", "Yixin Qiu", "Xuanhong Liao", "Tianyu Wang", "Dingchuan Liu", "Tao Zhang", "Ximin Lyu"], "title": "Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot", "comment": "8 pages, 8 figures", "summary": "Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u52a8\u529b\u5b66\u6a21\u578b\u548c\u5168\u8eab\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e6\u81ea\u7531\u5ea6\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\uff0c\u5305\u542b\u5730\u5f62\u4f30\u8ba1\u529f\u80fd\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u7a7f\u8d8a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5ffd\u7565\u817f\u90e8\u52a8\u529b\u5b66\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6f5c\u529b\uff0c\u4e14\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u9762\u4e34\u6311\u6218\u3002", "method": "\u5efa\u7acb\u5305\u542b\u95ed\u73af\u52a8\u529b\u5b66\u548c\u5730\u9762\u63a5\u89e6\u6a21\u578b\u7684\u5b8c\u6574\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4f7f\u7528LiDAR\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u6539\u8fdb\u7684\u4e3b\u6210\u5206\u5206\u6790\u8fdb\u884c\u5730\u5f62\u4f30\u8ba1\uff0c\u91c7\u7528PD\u63a7\u5236\u548cLQR\u8fdb\u884c\u59ff\u6001\u548c\u5e73\u8861\u63a7\u5236\uff0c\u4f7f\u7528\u5206\u5c42\u4f18\u5316\u89e3\u51b3\u5168\u8eab\u63a7\u5236\u95ee\u9898\u3002", "result": "\u9a8c\u8bc1\u4e86\u5730\u5f62\u4f30\u8ba1\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u7a7f\u8d8a\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b8c\u6574\u52a8\u529b\u5b66\u6a21\u578b\u548c\u5168\u8eab\u63a7\u5236\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\u3002"}}
{"id": "2511.06434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06434", "abs": "https://arxiv.org/abs/2511.06434", "authors": ["Wenkang Hu", "Xincheng Tang", "Yanzhi E", "Yitong Li", "Zhengjie Shu", "Wei Li", "Huamin Wang", "Ruigang Yang"], "title": "Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator", "comment": "2026 AAAI Accept", "summary": "While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86Real Garment Benchmark (RGBench)\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u670d\u88c5\u64cd\u4f5c\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b6000\u591a\u4e2a\u670d\u88c5\u7f51\u683c\u6a21\u578b\u3001\u9ad8\u6027\u80fd\u6a21\u62df\u5668\u4ee5\u53ca\u8bc4\u4f30\u6a21\u62df\u8d28\u91cf\u7684\u534f\u8bae\u3002", "motivation": "\u867d\u7136\u6a21\u62df\u6570\u636e\u5728\u521a\u6027\u7269\u4f53\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u53d8\u5f62\u7269\u4f53\u6a21\u578b\u548c\u903c\u771f\u7684\u975e\u521a\u4f53\u6a21\u62df\u5668\uff0c\u8fd9\u4e00\u6210\u529f\u5c1a\u672a\u5e94\u7528\u4e8e\u53d8\u5f62\u7269\u4f53\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u591a\u6837\u5316\u670d\u88c5\u7f51\u683c\u6a21\u578b\u7684\u65b0\u6a21\u62df\u5668\uff0c\u5e76\u5efa\u7acb\u4e86\u8bc4\u4f30\u6a21\u62df\u8d28\u91cf\u7684\u7efc\u5408\u534f\u8bae\uff0c\u901a\u8fc7\u771f\u5b9e\u670d\u88c5\u52a8\u529b\u5b66\u7684\u7cbe\u786e\u6d4b\u91cf\u6765\u9a8c\u8bc1\u6a21\u62df\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u62df\u5668\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u5e03\u6599\u6a21\u62df\u5668\uff0c\u6a21\u62df\u8bef\u5dee\u51cf\u5c1120%\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u9ad83\u500d\u3002", "conclusion": "RGBench\u5c06\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u52a0\u901f\u672a\u6765\u673a\u5668\u4eba\u670d\u88c5\u64cd\u4f5c\u7684\u7814\u7a76\u3002"}}
{"id": "2511.06465", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06465", "abs": "https://arxiv.org/abs/2511.06465", "authors": ["Lingfan Bao", "Tianhu Peng", "Chengxu Zhou"], "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion", "comment": "Sim-to-real for bipedal locomotion chapter", "summary": "This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u53cc\u8db3\u673a\u5668\u4eba\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u6311\u6218\uff0c\u5206\u6790\u4e86\u4eff\u771f\u5dee\u8ddd\u7684\u6765\u6e90\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5373\"\u4eff\u771f\u8bc5\u5492\"\u95ee\u9898\uff0c\u786e\u4fdd\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u80fd\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u53ef\u9760\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a\u4e00\u662f\u901a\u8fc7\u6a21\u578b\u4e2d\u5fc3\u7b56\u7565\u63d0\u9ad8\u4eff\u771f\u5668\u7269\u7406\u4fdd\u771f\u5ea6\u6765\u7f29\u5c0f\u5dee\u8ddd\uff1b\u4e8c\u662f\u901a\u8fc7\u9c81\u68d2\u6027\u8bad\u7ec3\u548c\u540e\u90e8\u7f72\u9002\u5e94\u4f7f\u7b56\u7565\u5bf9\u6a21\u578b\u4e0d\u51c6\u786e\u6027\u5177\u6709\u5185\u5728\u97e7\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u4eff\u771f\u5dee\u8ddd\u7684\u6765\u6e90\uff08\u673a\u5668\u4eba\u52a8\u529b\u5b66\u3001\u63a5\u89e6\u5efa\u6a21\u3001\u72b6\u6001\u4f30\u8ba1\u3001\u6570\u503c\u6c42\u89e3\u5668\uff09\uff0c\u5e76\u6574\u5408\u4e86\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\u54f2\u5b66\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u6a21\u578b\u6539\u8fdb\u548c\u7b56\u7565\u5f3a\u5316\u7684\u53cc\u91cd\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u7a33\u5065\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2511.06496", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06496", "abs": "https://arxiv.org/abs/2511.06496", "authors": ["Keke Long", "Jiacheng Guo", "Tianyun Zhang", "Hongkai Yu", "Xiaopeng Li"], "title": "A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving", "comment": null, "summary": "Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u5206\u89e3\u7684\u81ea\u5305\u542b\u65b9\u6cd5\uff0c\u81ea\u52a8\u5bf9\u591a\u4e2aVLM\u751f\u6210\u7684\u5019\u9009\u63cf\u8ff0\u8fdb\u884c\u5e7b\u89c9\u7a0b\u5ea6\u6392\u5e8f\uff0c\u65e0\u9700\u5916\u90e8\u53c2\u8003\u6216\u6a21\u578b\u8bbf\u95ee\uff0c\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u523087%\u7684\u9009\u62e9\u51c6\u786e\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4f7f\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f1a\u4ea7\u751f\u5e7b\u89c9\uff08\u865a\u5047\u7ec6\u8282\uff09\uff0c\u4f46\u5728\u7f3a\u4e4f\u771f\u5b9e\u53c2\u8003\u548c\u6a21\u578b\u5185\u90e8\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\uff0c\u68c0\u6d4b\u548c\u7f13\u89e3\u5e7b\u89c9\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6784\u5efa\u53e5\u5b50\u5d4c\u5165\u77e9\u9635\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u4f4e\u79e9\u5171\u8bc6\u5206\u91cf\u548c\u7a00\u758f\u6b8b\u5dee\uff0c\u5229\u7528\u6b8b\u5dee\u5927\u5c0f\u5bf9\u63cf\u8ff0\u8fdb\u884c\u6392\u5e8f\uff0c\u9009\u62e9\u6b8b\u5dee\u6700\u5c0f\u7684\u4f5c\u4e3a\u6700\u65e0\u5e7b\u89c9\u7684\u63cf\u8ff0\u3002", "result": "\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u523087%\u7684\u9009\u62e9\u51c6\u786e\u7387\uff0c\u6bd4\u672a\u8fc7\u6ee4\u57fa\u7ebf\u63d0\u9ad819%\uff0c\u6bd4\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u65b9\u6cd5\u63d0\u9ad86-10%\uff1b\u63a8\u7406\u65f6\u95f4\u51cf\u5c1151-67%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u5e7b\u89c9\u6700\u5c11\u7684\u63cf\u8ff0\uff0c\u6392\u5e8f\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u5f3a\u76f8\u5173\uff0c\u4e14\u6613\u4e8e\u5e76\u884c\u5316\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u3002"}}
{"id": "2511.06500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06500", "abs": "https://arxiv.org/abs/2511.06500", "authors": ["JiaHao Wu", "ShengWen Yu"], "title": "Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation", "comment": "21 pages,12 tables, 6 figures", "summary": "Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \\textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\\% average improvement on Franka Panda (6.26\u00b0 MAE), with exceptional gains in high-load joints (J2: 80.4\\% improvement from 12.36\u00b0 to 2.42\u00b0). Critically, this work discovers the \\textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\\%, no disturbance: +16.6\\%, average: +10.0\\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5c42\u6b21\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u6570\u636e\u589e\u5f3a\u7b56\u7565\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5728\u673a\u5668\u4ebaPID\u63a7\u5236\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u4e2dPID\u63a7\u5236\u5668\u53c2\u6570\u624b\u52a8\u8c03\u4f18\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5143\u5b66\u4e60\u8fdb\u884cPID\u521d\u59cb\u5316\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5728\u7ebf\u9002\u5e94\uff0c\u5f15\u5165\u57fa\u4e8e\u7269\u7406\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u751f\u6210\u865a\u62df\u673a\u5668\u4eba\u914d\u7f6e\u3002", "result": "\u5728Franka Panda\u673a\u68b0\u81c2\u4e0a\u5e73\u5747\u6539\u8fdb16.6%\uff08MAE 6.26\u00b0\uff09\uff0c\u9ad8\u8d1f\u8f7d\u5173\u8282\u6539\u8fdb80.4%\uff1b\u53d1\u73b0\u4f18\u5316\u5929\u82b1\u677f\u6548\u5e94\uff1a\u5f53\u5143\u5b66\u4e60\u57fa\u7ebf\u6027\u80fd\u5747\u5300\u65f6RL\u65e0\u589e\u76ca\u3002", "conclusion": "RL\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5143\u5b66\u4e60\u57fa\u7ebf\u8d28\u91cf\u548c\u8bef\u5dee\u5206\u5e03\uff0c\u4e3a\u5c42\u6b21\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2511.06515", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.06515", "abs": "https://arxiv.org/abs/2511.06515", "authors": ["Cormac O'Neill", "Jasmine Terrones", "H. Harry Asada"], "title": "Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control", "comment": null, "summary": "Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528Koopman\u7b97\u5b50\u5c06\u63a5\u89e6\u52a8\u529b\u5b66\u8f6c\u5316\u4e3a\u5168\u5c40\u7ebf\u6027\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5728\u63a5\u89e6\u53d8\u5316\u573a\u666f\u4e0b\u7684\u51f8\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u4e0e\u73af\u5883\u52a8\u6001\u63a5\u89e6\u65f6\u7684\u63a7\u5236\u96be\u9898\uff0c\u7279\u522b\u662f\u63a5\u89e6\u8fb9\u754c\u52a8\u529b\u5b66\u5207\u6362\u5bfc\u81f4\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5e94\u7528Koopman\u7b97\u5b50\u5c06\u5206\u6bb5\u63a5\u89e6\u52a8\u529b\u5b66\u7edf\u4e00\u5230\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5168\u5c40\u7ebf\u6027\u6a21\u578b\uff0c\u5229\u7528\u7c98\u5f39\u6027\u63a5\u89e6\u7279\u6027\u5b9e\u73b0\u65e0\u8fd1\u4f3c\u7684\u63a7\u5236\u8f93\u5165\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u7684\u51f8\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u673a\u68b0\u81c2\u52a8\u6001\u63a8\u52a8\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u673a\u5668\u4eba\u80fd\u591f\u5728\u591a\u63a5\u89e6\u53d8\u5316\u7684\u65f6\u95f4\u8303\u56f4\u5185\u53d1\u73b0\u7cbe\u7ec6\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u63a5\u89e6\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u4e14\u9002\u7528\u8303\u56f4\u8d85\u8d8a\u673a\u5668\u4eba\u9886\u57df\u3002"}}
{"id": "2511.06575", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06575", "abs": "https://arxiv.org/abs/2511.06575", "authors": ["Jun Wang", "Yevgeniy Vorobeychik", "Yiannis Kantaros"], "title": "CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning", "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments.", "AI": {"tldr": "CoFineLLM\u662f\u4e00\u4e2a\u9488\u5bf9LLM\u89c4\u5212\u5668\u7684CP\u611f\u77e5\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u51cf\u5c11\u9884\u6d4b\u96c6\u5927\u5c0f\u6765\u964d\u4f4e\u7528\u6237\u5e72\u9884\u9891\u7387\uff0c\u63d0\u9ad8\u81ea\u4e3b\u90e8\u7f72\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u89c4\u5212\u5668\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u867d\u7136\u4f7f\u7528Conformal Prediction\u751f\u6210\u9884\u6d4b\u96c6\u786e\u4fdd\u6b63\u786e\u6027\uff0c\u4f46LLM\u4e0d\u786e\u5b9a\u6027\u4e0d\u654f\u611f\u5bfc\u81f4\u9884\u6d4b\u96c6\u8fc7\u5927\uff0c\u9700\u8981\u9891\u7e41\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u63d0\u51faCoFineLLM\u6846\u67b6\uff0c\u901a\u8fc7CP\u611f\u77e5\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u5f0f\u4f18\u5316LLM\u4ee5\u51cf\u5c11\u9884\u6d4b\u96c6\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u8bed\u8a00\u6307\u4ee4\u673a\u5668\u4eba\u89c4\u5212\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u65e0\u611f\u77e5\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u9884\u6d4b\u96c6\u5927\u5c0f\u548c\u6c42\u52a9\u7387\u65b9\u9762\u5747\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u7528\u6237\u5e72\u9884\u9700\u6c42\uff0c\u5e76\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.06578", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.06578", "abs": "https://arxiv.org/abs/2511.06578", "authors": ["Kaustubh Singh", "Shivam Kumar", "Shashikant Pawar", "Sandeep Manjanna"], "title": "Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring", "comment": "ICRA RUNE 2024 Workshop Paper", "summary": "In this paper, we present an underactuated biomimetic underwater robot that is suitable for ecosystem monitoring in both marine and freshwater environments. We present an updated mechanical design for a fish-like robot and propose minimal actuation behaviors learned using reinforcement learning techniques. We present our preliminary mechanical design of the tail oscillation mechanism and illustrate the swimming behaviors on FishGym simulator, where the reinforcement learning techniques will be tested on", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6b20\u9a71\u52a8\u4eff\u751f\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u9002\u7528\u4e8e\u6d77\u6d0b\u548c\u6de1\u6c34\u73af\u5883\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5b66\u4e60\u6700\u5c0f\u9a71\u52a8\u884c\u4e3a", "motivation": "\u5f00\u53d1\u9002\u7528\u4e8e\u6d77\u6d0b\u548c\u6de1\u6c34\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u7684\u4eff\u751f\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u9ad8\u6548\u6e38\u6cf3\u7684\u6700\u5c0f\u9a71\u52a8\u673a\u5236", "method": "\u66f4\u65b0\u4e86\u9c7c\u5f62\u673a\u5668\u4eba\u7684\u673a\u68b0\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u5c3e\u90e8\u6446\u52a8\u673a\u5236\uff0c\u5e76\u5728FishGym\u6a21\u62df\u5668\u4e2d\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5b66\u4e60\u6e38\u6cf3\u884c\u4e3a", "result": "\u63d0\u51fa\u4e86\u521d\u6b65\u7684\u673a\u68b0\u8bbe\u8ba1\uff0c\u5e76\u5728\u6a21\u62df\u5668\u4e2d\u5c55\u793a\u4e86\u6e38\u6cf3\u884c\u4e3a\uff0c\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840", "conclusion": "\u8be5\u6b20\u9a71\u52a8\u4eff\u751f\u6c34\u4e0b\u673a\u5668\u4eba\u8bbe\u8ba1\u4e3a\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u671b\u4f18\u5316\u5176\u6e38\u6cf3\u6027\u80fd"}}
{"id": "2511.06619", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06619", "abs": "https://arxiv.org/abs/2511.06619", "authors": ["Chuheng Zhang", "Rushuai Yang", "Xiaoyu Chen", "Kaixin Wang", "Li Zhao", "Yi Chen", "Jiang Bian"], "title": "How Do VLAs Effectively Inherit from VLMs?", "comment": null, "summary": "Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86GrinningFace\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5982\u4f55\u6709\u6548\u7ee7\u627f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u8868\u60c5\u7b26\u53f7\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\u6d4b\u8bd5\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5982\u4f55\u6709\u6548\u7ee7\u627fVLM\u5148\u9a8c\u77e5\u8bc6\u7684\u5173\u952e\u95ee\u9898\uff0c\u8868\u60c5\u7b26\u53f7\u5728\u4e92\u8054\u7f51\u6570\u636e\u4e2d\u666e\u904d\u5b58\u5728\u4f46\u5728\u673a\u5668\u4eba\u6570\u636e\u4e2d\u7f55\u89c1\uff0c\u63d0\u4f9b\u4e86\u77e5\u8bc6\u8fc1\u79fb\u7684\u6e05\u6670\u6d4b\u8bd5\u573a\u666f\u3002", "method": "\u521b\u5efa\u8868\u60c5\u7b26\u53f7\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u73b0\uff0c\u6bd4\u8f83\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001VLM\u51bb\u7ed3\u3001\u8054\u5408\u8bad\u7ec3\u3001\u79bb\u6563\u52a8\u4f5c\u9884\u6d4b\u548c\u6f5c\u5728\u52a8\u4f5c\u9884\u6d4b\u7b49\u591a\u79cd\u6280\u672f\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u8868\u660e\u4fdd\u6301VLM\u5148\u9a8c\u5bf9VLA\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u5f00\u53d1\u771f\u6b63\u53ef\u6cdb\u5316\u7684\u5177\u8eabAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u539f\u5219\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u8bc1\u660e\u4e86\u4fdd\u6301VLM\u5148\u9a8c\u5bf9VLA\u6cdb\u5316\u7684\u91cd\u8981\u6027\uff0c\u8fd8\u4e3a\u672a\u6765\u5177\u8eabAI\u7cfb\u7edf\u7814\u7a76\u5efa\u7acb\u4e86\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2511.06667", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06667", "abs": "https://arxiv.org/abs/2511.06667", "authors": ["Andrew Choi", "Dezhong Tong"], "title": "Rapidly Learning Soft Robot Control via Implicit Time-Stepping", "comment": "Code: https://github.com/QuantuMope/dismech-rl", "summary": "With the explosive growth of rigid-body simulators, policy learning in simulation has become the de facto standard for most rigid morphologies. In contrast, soft robotic simulation frameworks remain scarce and are seldom adopted by the soft robotics community. This gap stems partly from the lack of easy-to-use, general-purpose frameworks and partly from the high computational cost of accurately simulating continuum mechanics, which often renders policy learning infeasible. In this work, we demonstrate that rapid soft robot policy learning is indeed achievable via implicit time-stepping. Our simulator of choice, DisMech, is a general-purpose, fully implicit soft-body simulator capable of handling both soft dynamics and frictional contact. We further introduce delta natural curvature control, a method analogous to delta joint position control in rigid manipulators, providing an intuitive and effective means of enacting control for soft robot learning. To highlight the benefits of implicit time-stepping and delta curvature control, we conduct extensive comparisons across four diverse soft manipulator tasks against one of the most widely used soft-body frameworks, Elastica. With implicit time-stepping, parallel stepping of 500 environments achieves up to 6x faster speeds for non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a comprehensive sim-to-sim gap evaluation--training policies in one simulator and evaluating them in another--demonstrates that implicit time-stepping provides a rare free lunch: dramatic speedups achieved without sacrificing accuracy.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u9690\u5f0f\u65f6\u95f4\u6b65\u8fdb\u5b9e\u73b0\u8f6f\u4f53\u673a\u5668\u4eba\u5feb\u901f\u7b56\u7565\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u4f7f\u7528DisMech\u6a21\u62df\u5668\u7ed3\u5408delta\u81ea\u7136\u66f2\u7387\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u56db\u4e2a\u8f6f\u4f53\u673a\u68b0\u81c2\u4efb\u52a1\u4e2d\u76f8\u6bd4Elastica\u6846\u67b6\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u6a21\u62df\u6846\u67b6\u7a00\u7f3a\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5bfc\u81f4\u7b56\u7565\u5b66\u4e60\u4e0d\u53ef\u884c\uff0c\u800c\u521a\u6027\u4f53\u6a21\u62df\u5668\u5df2\u5e7f\u6cdb\u4f7f\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8f6f\u4f53\u673a\u5668\u4eba\u6a21\u62df\u548c\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528DisMech\u5168\u9690\u5f0f\u8f6f\u4f53\u6a21\u62df\u5668\uff0c\u7ed3\u5408delta\u81ea\u7136\u66f2\u7387\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u5f0f\u65f6\u95f4\u6b65\u8fdb\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728500\u4e2a\u73af\u5883\u5e76\u884c\u6b65\u8fdb\u65f6\uff0c\u975e\u63a5\u89e6\u60c5\u51b5\u52a0\u901f6\u500d\uff0c\u63a5\u89e6\u4e30\u5bcc\u573a\u666f\u52a0\u901f40\u500d\uff1bsim-to-sim\u8bc4\u4f30\u663e\u793a\u52a0\u901f\u4e0d\u727a\u7272\u7cbe\u5ea6\u3002", "conclusion": "\u9690\u5f0f\u65f6\u95f4\u6b65\u8fdb\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u7f55\u89c1\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u83b7\u5f97\u663e\u8457\u52a0\u901f\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002"}}
{"id": "2511.06673", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06673", "abs": "https://arxiv.org/abs/2511.06673", "authors": ["Joel Kemp", "Andre Farinha", "David Howard", "Krishna Manaswi Digumarti", "Josh Pinskier"], "title": "Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots", "comment": "8 pages, 10 figures, Submitted to Robosoft 2026", "summary": "Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u8f6f\u4f53\u6267\u884c\u5668PTSPA\uff0c\u901a\u8fc7\u5145\u6c14\u8f74\u5411\u81a8\u80c0\u5b9e\u73b0\u53ef\u5c55\u5f00\u7ed3\u6784\uff0c\u9002\u7528\u4e8e\u53d7\u9650\u7a7a\u95f4\u64cd\u4f5c\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u53ef\u5c55\u5f00\u8f6f\u4f53\u56db\u8db3\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5177\u6709\u81ea\u7531\u5f62\u6001\u548c\u8fde\u7eed\u4f53\u7279\u6027\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u5229\u7528\u5176\u8bbe\u8ba1\u81ea\u7531\u5ea6\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002\u53c2\u6570\u5316\u8bbe\u8ba1\u96c6\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u73b0\u6a21\u5757\u5316\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u53ef\u884c\u8def\u5f84\u3002", "method": "\u5f00\u53d1\u4e86\u53c2\u6570\u5316\u51e0\u4f55\u751f\u6210\u5668\uff0c\u901a\u8fc7\u9ad8\u5c42\u8f93\u5165\u5b9a\u5236\u6267\u884c\u5668\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u5b9e\u9a8c\u7cfb\u7edf\u63a2\u7d22\u5173\u952e\u53c2\u6570\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "result": "\u8868\u5f81\u4e86\u6267\u884c\u5668\u7684\u4f38\u7f29/\u5f2f\u66f2\u3001\u81a8\u80c0\u548c\u521a\u5ea6\u7279\u6027\uff0c\u63ed\u793a\u4e86\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u660e\u786e\u5173\u7cfb\u3002", "conclusion": "PTSPA\u4e3a\u53ef\u5c55\u5f00\u548c\u5f62\u72b6\u53d8\u5f62\u7ed3\u6784\u4ee5\u53ca\u9700\u8981\u5927\u957f\u5ea6\u53d8\u5316\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2511.06745", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06745", "abs": "https://arxiv.org/abs/2511.06745", "authors": ["Lan Thi Ha Nguyen", "Kien Ton Manh", "Anh Do Duc", "Nam Pham Hai"], "title": "Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning", "comment": null, "summary": "Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios.", "AI": {"tldr": "\u63d0\u51faPI-RIG\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u589e\u5f3a\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u53ef\u884c\u76ee\u6807\uff0c\u89e3\u51b3\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4e2d\u76ee\u6807\u8bbe\u7f6e\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982RIG\u4f7f\u7528VAE\u5728\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u76ee\u6807\uff0c\u4f46\u4f1a\u4ea7\u751f\u7269\u7406\u4e0a\u4e0d\u53ef\u884c\u7684\u76ee\u6807\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u7387", "method": "\u63d0\u51fa\u589e\u5f3a\u7269\u7406\u4fe1\u606f\u53d8\u5206\u81ea\u7f16\u7801\u5668(Enhanced p3-VAE)\uff0c\u5c06\u6f5c\u5728\u7a7a\u95f4\u663e\u5f0f\u5206\u79bb\u4e3a\u7269\u7406\u53d8\u91cf\u548c\u73af\u5883\u56e0\u7d20\uff0c\u901a\u8fc7\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u548c\u5b88\u6052\u5b9a\u5f8b\u5f3a\u5236\u7269\u7406\u4e00\u81f4\u6027", "result": "\u5b9e\u9a8c\u8868\u660e\u7269\u7406\u4fe1\u606f\u76ee\u6807\u751f\u6210\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u8d28\u91cf\uff0c\u5728\u89c6\u89c9\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u63a2\u7d22\u548c\u6280\u80fd\u83b7\u53d6", "conclusion": "PI-RIG\u901a\u8fc7\u6574\u5408\u7269\u7406\u7ea6\u675f\u5230VAE\u8bad\u7ec3\u4e2d\uff0c\u80fd\u591f\u751f\u6210\u7269\u7406\u4e00\u81f4\u4e14\u53ef\u5b9e\u73b0\u7684\u76ee\u6807\uff0c\u63d0\u5347\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6027\u80fd"}}
{"id": "2511.06749", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06749", "abs": "https://arxiv.org/abs/2511.06749", "authors": ["Weining Lu", "Deer Bin", "Lian Ma", "Ming Ma", "Zhihao Ma", "Xiangyang Chen", "Longfei Wang", "Yixiao Feng", "Zhouxian Jiang", "Yongliang Shi", "Bin Liang"], "title": "Semi-distributed Cross-modal Air-Ground Relative Localization", "comment": "7 pages, 3 figures. Accepted by IROS 2025", "summary": "Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u5206\u5e03\u5f0f\u8de8\u6a21\u6001\u7a7a\u5730\u76f8\u5bf9\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u76f8\u5bf9\u5b9a\u4f4d\u4e0e\u72b6\u6001\u4f30\u8ba1\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5173\u952e\u70b9\u548c\u5168\u5c40\u63cf\u8ff0\u7b26\uff0c\u5728\u901a\u4fe1\u5e26\u5bbd\u4f4e\u4e8e0.3Mbps\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u5730\u7a7a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u3002", "motivation": "\u5f53\u524d\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u76f8\u540c\u4f20\u611f\u5668\u914d\u7f6e\u7684\u5206\u5e03\u5f0fSLAM\u7cfb\u7edf\uff0c\u4e0e\u6240\u6709\u673a\u5668\u4eba\u7684\u72b6\u6001\u4f30\u8ba1\u7d27\u5bc6\u8026\u5408\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u7075\u6d3b\u7684\u8de8\u6a21\u6001\u76f8\u5bf9\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "UGV\u548cUAV\u72ec\u7acb\u6267\u884cSLAM\u5e76\u63d0\u53d6\u6df1\u5ea6\u5b66\u4e60\u5173\u952e\u70b9\u548c\u5168\u5c40\u63cf\u8ff0\u7b26\u3002UGV\u4f7f\u7528LiDAR\u3001\u76f8\u673a\u548cIMU\u8fdb\u884c\u5c40\u90e8Bundle Adjustment\uff0c\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u4f18\u5316\u4eceLiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u63d2\u503c\u7684\u76f8\u673a\u4f4d\u59ff\uff0c\u7136\u540e\u4f30\u8ba1UGV\u4e0eUAV\u4e4b\u95f4\u7684\u76f8\u5bf9\u76f8\u673a\u4f4d\u59ff\u3002\u91c7\u7528\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u63cf\u8ff0\u7b26\u5b9e\u73b0\u589e\u91cf\u5f0f\u95ed\u73af\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u4fe1\u5e26\u5bbd\u88ab\u6709\u6548\u9650\u5236\u57280.3Mbps\u4ee5\u4e0b\uff0c\u76f8\u6bd4\u4f20\u7edf\u591a\u673a\u5668\u4ebaSLAM\u4f20\u8f93\u56fe\u50cf\u6216\u70b9\u4e91\u7684\u65b9\u6cd5\u66f4\u52a0\u9ad8\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u901a\u4fe1\u6548\u7387\u9ad8\u7684\u8de8\u6a21\u6001\u7a7a\u5730\u76f8\u5bf9\u5b9a\u4f4d\uff0c\u4e3a\u7a7a\u4e2d-\u5730\u9762\u534f\u540c\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06754", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06754", "abs": "https://arxiv.org/abs/2511.06754", "authors": ["Taisei Hanyu", "Nhat Chung", "Huy Le", "Toan Nguyen", "Yuki Ikebe", "Anthony Gunderman", "Duy Nguyen Ho Minh", "Khoa Vo", "Tung Kieu", "Kashu Yamazaki", "Chase Rainwater", "Anh Nguyen", "Ngan Le"], "title": "SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation", "comment": "under review", "summary": "Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.", "AI": {"tldr": "\u63d0\u51faLIBERO+\u6570\u636e\u96c6\u548cSlotVLA\u6846\u67b6\uff0c\u63a2\u7d22\u57fa\u4e8e\u5bf9\u8c61\u5173\u7cfb\u8868\u793a\u7684\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u673a\u5668\u4eba\u64cd\u4f5c", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u591a\u4efb\u52a1\u6a21\u578b\u4f9d\u8d56\u5bc6\u96c6\u5d4c\u5165\uff0c\u6df7\u6dc6\u5bf9\u8c61\u548c\u80cc\u666f\u7ebf\u7d22\uff0c\u5b58\u5728\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002\u53d7\u4eba\u7c7b\u57fa\u4e8e\u79bb\u6563\u5bf9\u8c61\u53ca\u5176\u5173\u7cfb\u63a8\u7406\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5bf9\u8c61\u5173\u7cfb\u4e2d\u5fc3\u8868\u793a\u4f5c\u4e3a\u7ed3\u6784\u5316\u3001\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7684\u9014\u5f84", "method": "1) \u5f15\u5165LIBERO+\u7ec6\u7c92\u5ea6\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u5bf9\u8c61\u4e2d\u5fc3\u6807\u6ce8\uff08\u8fb9\u754c\u6846\u3001\u63a9\u7801\u6807\u7b7e\u3001\u5b9e\u4f8b\u7ea7\u65f6\u5e8f\u8ddf\u8e2a\uff09\uff1b2) \u63d0\u51faSlotVLA\u6846\u67b6\uff1a\u4f7f\u7528\u57fa\u4e8e\u69fd\u6ce8\u610f\u529b\u7684\u89c6\u89c9\u5206\u8bcd\u5668\u4fdd\u6301\u65f6\u5e8f\u5bf9\u8c61\u8868\u793a\uff0c\u5173\u7cfb\u4e2d\u5fc3\u89e3\u7801\u5668\u751f\u6210\u4efb\u52a1\u76f8\u5173\u5d4c\u5165\uff0cLLM\u9a71\u52a8\u6a21\u5757\u5c06\u5d4c\u5165\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c", "result": "\u5728LIBERO+\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u8c61\u4e2d\u5fc3\u69fd\u548c\u5bf9\u8c61\u5173\u7cfb\u69fd\u8868\u793a\u5927\u5e45\u51cf\u5c11\u6240\u9700\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "LIBERO+\u548cSlotVLA\u5171\u540c\u4e3a\u63a8\u8fdb\u5bf9\u8c61\u5173\u7cfb\u4e2d\u5fc3\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u57fa\u7840"}}
{"id": "2511.06796", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06796", "abs": "https://arxiv.org/abs/2511.06796", "authors": ["MD-Nazmus Sunbeam"], "title": "Human-Level Actuation for Humanoids", "comment": "61 pages, 8 figures, 7 tables, and 12 numbered equations", "summary": "Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \\emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \\emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \\emph{and} power simultaneously at the same joint angle and rate $(q,\u03c9)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \\emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u8bc4\u4f30\u4eba\u5f62\u673a\u5668\u4eba\"\u4eba\u7c7b\u6c34\u5e73\"\u9a71\u52a8\u80fd\u529b\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u6807\u51c6\u5316\u5173\u8282\u5750\u6807\u7cfb\u3001\u4eba\u7c7b\u7b49\u6548\u5305\u7edc\u548c\u4eba\u7c7b\u6c34\u5e73\u9a71\u52a8\u8bc4\u5206\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u9886\u57df\u666e\u904d\u58f0\u79f0\u8fbe\u5230\"\u4eba\u7c7b\u6c34\u5e73\"\u9a71\u52a8\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u91cf\u5316\u6807\u51c6\u548c\u53ef\u6bd4\u6027\u3002\u5cf0\u503c\u626d\u77e9\u6216\u901f\u5ea6\u89c4\u683c\u65e0\u6cd5\u53cd\u6620\u5728\u4efb\u52a1\u76f8\u5173\u59ff\u6001\u548c\u901f\u7387\u4e0b\u7684\u626d\u77e9\u3001\u529f\u7387\u548c\u8010\u529b\u7efc\u5408\u8868\u73b0\u3002", "method": "1) \u81ea\u7531\u5ea6\u56fe\u8c31\uff1a\u4f7f\u7528ISB\u6807\u51c6\u7edf\u4e00\u5173\u8282\u5750\u6807\u7cfb\u548c\u8fd0\u52a8\u8303\u56f4\uff1b2) \u4eba\u7c7b\u7b49\u6548\u5305\u7edc\uff1a\u8bc4\u4f30\u673a\u5668\u4eba\u5728\u76f8\u540c\u5173\u8282\u89d2\u5ea6\u548c\u901f\u7387\u4e0b\u662f\u5426\u540c\u65f6\u6ee1\u8db3\u4eba\u7c7b\u626d\u77e9\u548c\u529f\u7387\u8981\u6c42\uff1b3) \u4eba\u7c7b\u6c34\u5e73\u9a71\u52a8\u8bc4\u5206\uff1a\u7efc\u5408\u516d\u4e2a\u7269\u7406\u56e0\u7d20\uff0c\u5305\u62ec\u5de5\u4f5c\u7a7a\u95f4\u8986\u76d6\u3001HEE\u8986\u76d6\u3001\u626d\u77e9\u6a21\u5f0f\u5e26\u5bbd\u3001\u6548\u7387\u548c\u70ed\u53ef\u6301\u7eed\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6d4b\u91cf\u534f\u8bae\uff0c\u5305\u62ec\u6d4b\u529b\u8ba1\u3001\u7535\u529f\u7387\u76d1\u6d4b\u548c\u70ed\u6d4b\u8bd5\uff0c\u53ef\u901a\u8fc7\u53ef\u91cd\u590d\u5b9e\u9a8c\u83b7\u5f97\u6240\u6709HLAS\u8f93\u5165\u3002\u901a\u8fc7\u591a\u5173\u8282\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5b9e\u4f8b\u5c55\u793a\u4e86HLAS\u8ba1\u7b97\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e2\u53ef\u4f5c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5f00\u53d1\u7684\u8bbe\u8ba1\u89c4\u8303\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u6bd4\u8f83\u9a71\u52a8\u7cfb\u7edf\u7684\u57fa\u51c6\u6807\u51c6\uff0c\u6240\u6709\u7ec4\u4ef6\u90fd\u57fa\u4e8e\u5df2\u53d1\u8868\u7684\u4eba\u7c7b\u751f\u7269\u529b\u5b66\u6570\u636e\u3002"}}
{"id": "2511.06801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06801", "abs": "https://arxiv.org/abs/2511.06801", "authors": ["Praveen Kumar", "Tushar Sandhan"], "title": "Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots", "comment": "10 pages", "summary": "The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are seman- tically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a frame- work to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost- effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8f7b\u91cf\u7ea7\u8bed\u4e49\u611f\u77e5\u4e0e\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4f7f\u4f4e\u6210\u672c\u673a\u5668\u4eba\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u5bfc\u822a\uff0c\u533a\u5206\u91cd\u8981\u7269\u54c1\u4e0e\u666e\u901a\u969c\u788d\u7269\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u6602\u8d35\u7684LiDAR\uff0c\u867d\u7136\u51e0\u4f55\u7cbe\u5ea6\u9ad8\u4f46\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u6cd5\u533a\u5206\u91cd\u8981\u6587\u4ef6\u4e0e\u666e\u901a\u5783\u573e\uff0c\u5bfc\u81f4\u5728\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u90e8\u7f72\u53d7\u9650\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u611f\u77e5\u6a21\u5757\u4e0e\u5728\u7ebfA*\u89c4\u5212\u5668\u7684\u7d27\u5bc6\u96c6\u6210\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u8bc6\u522b\u7528\u6237\u5b9a\u4e49\u7684\u89c6\u89c9\u7ea6\u675f\uff0c\u5e76\u5c06\u5176\u6295\u5f71\u4e3a\u975e\u51e0\u4f55\u969c\u788d\u7269\u5230\u5168\u5c40\u5730\u56fe\u4e2d\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u8bc1\u660e\u4f4e\u6210\u672c\u673a\u5668\u4eba\u80fd\u591f\u5b89\u5168\u5bfc\u822a\u590d\u6742\u73af\u5883\u5e76\u5c0a\u91cd\u4f20\u7edf\u89c4\u5212\u5668\u65e0\u6cd5\u8bc6\u522b\u7684\u5173\u952e\u89c6\u89c9\u7ebf\u7d22\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u586b\u8865\u4e86\u8bed\u4e49\u611f\u77e5\u4e0e\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u4e4b\u95f4\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4f7f\u4f4e\u6210\u672c\u673a\u5668\u4eba\u80fd\u591f\u5728\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u5bfc\u822a\u3002"}}
{"id": "2511.06839", "categories": ["cs.RO", "cs.CV", "eess.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.06839", "abs": "https://arxiv.org/abs/2511.06839", "authors": ["Selim Ahmet Iz", "Mustafa Unel"], "title": "Vision-Based System Identification of a Quadrotor", "comment": null, "summary": "This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u7cfb\u7edf\u8fa8\u8bc6\u6280\u672f\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5efa\u6a21\u4e0e\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7070\u7bb1\u5efa\u6a21\u548cLQR\u63a7\u5236\u5668\u8bbe\u8ba1\u9a8c\u8bc1\u4e86\u673a\u8f7d\u89c6\u89c9\u7cfb\u7edf\u5728\u7cfb\u7edf\u8fa8\u8bc6\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5efa\u6a21\u4e2d\u7684\u590d\u6742\u6027\u548c\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u63a8\u529b\u548c\u963b\u529b\u7cfb\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u63a2\u7d22\u57fa\u4e8e\u89c6\u89c9\u7684\u7cfb\u7edf\u8fa8\u8bc6\u6280\u672f\u63d0\u5347\u56db\u65cb\u7ffc\u6027\u80fd\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u7070\u7bb1\u5efa\u6a21\u65b9\u6cd5\u51cf\u8f7b\u4e0d\u786e\u5b9a\u6027\uff0c\u5229\u7528\u673a\u8f7d\u89c6\u89c9\u7cfb\u7edf\u8fdb\u884c\u7cfb\u7edf\u8fa8\u8bc6\uff0c\u5e76\u57fa\u4e8e\u8fa8\u8bc6\u6a21\u578b\u8bbe\u8ba1LQR\u63a7\u5236\u5668\u3002", "result": "\u6a21\u578b\u95f4\u8868\u73b0\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u7cfb\u7edf\u8fa8\u8bc6\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u6280\u672f\u5728\u63d0\u5347\u56db\u65cb\u7ffc\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u89c6\u89c9\u7684\u7cfb\u7edf\u8fa8\u8bc6\u6280\u672f\u80fd\u591f\u6709\u6548\u589e\u5f3a\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u5efa\u6a21\u548c\u63a7\u5236\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u56db\u65cb\u7ffc\u6027\u80fd\u63d0\u5347\u3001\u6545\u969c\u68c0\u6d4b\u548c\u51b3\u7b56\u8fc7\u7a0b\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.06892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06892", "abs": "https://arxiv.org/abs/2511.06892", "authors": ["Kailin Tong", "Selim Solmaz", "Kenan Mujkic", "Gottfried Allmer", "Bo Leng"], "title": "Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation", "comment": "submitted to TRA 2026", "summary": "Conventional road-situation detection methods achieve strong performance in predefined scenarios but fail in unseen cases and lack semantic interpretation, which is crucial for reliable traffic recommendations. This work introduces a multi-agent AI framework that combines multimodal large language models (MLLMs) with vision-based perception for road-situation monitoring. The framework processes camera feeds and coordinates dedicated agents for situation detection, distance estimation, decision-making, and Cooperative Intelligent Transport System (C-ITS) message generation. Evaluation is conducted on a custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\\% recall in situation detection and perfect message schema correctness; however, both models suffer from false-positive detections and have reduced performance in terms of number of lanes, driving lane status and cause code. Surprisingly, Gemini-2.5-Flash, though more capable in general tasks, underperforms Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs higher latency (Table II). These findings motivate further work on fine-tuning specialized LLMs or MLLMs tailored for intelligent transportation applications.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u611f\u77e5\u8fdb\u884c\u9053\u8def\u72b6\u51b5\u76d1\u63a7\uff0c\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u53d1\u73b0Gemini-2.5-Flash\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8bed\u4e49\u7406\u89e3\u4e0a\u4e0d\u5982Gemini-2.0-Flash\u3002", "motivation": "\u4f20\u7edf\u9053\u8def\u72b6\u51b5\u68c0\u6d4b\u65b9\u6cd5\u5728\u9884\u5b9a\u4e49\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u672a\u77e5\u60c5\u51b5\u4e0b\u5931\u6548\u4e14\u7f3a\u4e4f\u8bed\u4e49\u89e3\u91ca\uff0c\u8fd9\u5bf9\u53ef\u9760\u7684\u4ea4\u901a\u63a8\u8350\u81f3\u5173\u91cd\u8981\u3002", "method": "\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u5904\u7406\u6444\u50cf\u5934\u6570\u636e\u5e76\u534f\u8c03\u4e13\u7528\u667a\u80fd\u4f53\u8fdb\u884c\u72b6\u51b5\u68c0\u6d4b\u3001\u8ddd\u79bb\u4f30\u8ba1\u3001\u51b3\u7b56\u5236\u5b9a\u548cC-ITS\u6d88\u606f\u751f\u6210\u3002", "result": "\u72b6\u51b5\u68c0\u6d4b\u53ec\u56de\u7387100%\uff0c\u6d88\u606f\u6a21\u5f0f\u6b63\u786e\u6027\u5b8c\u7f8e\uff1b\u4f46\u5b58\u5728\u8bef\u68c0\uff0c\u5728\u8f66\u9053\u6570\u91cf\u3001\u884c\u9a76\u8f66\u9053\u72b6\u6001\u548c\u539f\u56e0\u4ee3\u7801\u65b9\u9762\u6027\u80fd\u4e0b\u964d\u3002Gemini-2.5-Flash\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8bed\u4e49\u7406\u89e3\u4e0a\u4e0d\u5982Gemini-2.0-Flash\uff0c\u4e14\u5ef6\u8fdf\u66f4\u9ad8\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9488\u5bf9\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u4f18\u5316\u7684\u4e13\u7528LLMs\u6216MLLMs\u7684\u5fae\u8c03\u5de5\u4f5c\u3002"}}
{"id": "2511.06919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06919", "abs": "https://arxiv.org/abs/2511.06919", "authors": ["Luis Diener", "Jens Kalkkuhl", "Markus Enzweiler"], "title": "Integration of Visual SLAM into Consumer-Grade Automotive Localization", "comment": "This manuscript has been submitted to the IEEE for possible publication", "summary": "Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u89c6\u89c9SLAM\u548c\u8f66\u8f86\u6a2a\u5411\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u6821\u51c6\u9640\u87ba\u4eea\uff0c\u63d0\u9ad8\u6d88\u8d39\u7ea7\u8f66\u8f86\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u6d88\u8d39\u7ea7\u8f66\u8f86\u76ee\u524d\u4f9d\u8d56\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u548cIMU\u8fdb\u884c\u81ea\u6211\u8fd0\u52a8\u4f30\u8ba1\uff0c\u4f46\u8fd9\u4e9b\u4f20\u611f\u5668\u5b58\u5728\u7cfb\u7edf\u8bef\u5dee\u548c\u6821\u51c6\u95ee\u9898\u3002\u89c6\u89c9SLAM\u5728\u673a\u5668\u4eba\u9886\u57df\u5df2\u6210\u4e3a\u6807\u51c6\uff0c\u4f46\u5728\u6c7d\u8f66\u81ea\u6211\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u878d\u5408\u89c6\u89c9SLAM\u4e0e\u8f66\u8f86\u6a2a\u5411\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5b9e\u9645\u9a7e\u9a76\u6761\u4ef6\u4e0b\u5728\u7ebf\u6821\u51c6\u9640\u87ba\u4eea\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u89c6\u89c9\u7684\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9640\u87ba\u4eea\u6821\u51c6\u7cbe\u5ea6\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6574\u4f53\u5b9a\u4f4d\u6027\u80fd\u3002\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u89c6\u89c9SLAM\u96c6\u6210\u63d0\u9ad8\u6c7d\u8f66\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u6709\u524d\u666f\u8def\u5f84\uff0c\u4e3a\u6d88\u8d39\u7ea7\u8f66\u8f86\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u81ea\u6211\u8fd0\u52a8\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06998", "abs": "https://arxiv.org/abs/2511.06998", "authors": ["Jin Huang", "Yingqiang Wang", "Ying Chen"], "title": "Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics", "comment": null, "summary": "Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics.", "AI": {"tldr": "Raspi^2USBL\u662f\u4e00\u4e2a\u57fa\u4e8e\u6811\u8393\u6d3e\u7684\u5f00\u6e90\u88ab\u52a8\u5012\u7f6e\u8d85\u77ed\u57fa\u7ebf\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u4e3a\u6c34\u4e0b\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e860.1%\u7684\u659c\u8ddd\u7cbe\u5ea6\u548c0.1\u00b0\u7684\u65b9\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8eGNSS\u4fe1\u53f7\u65e0\u6cd5\u7a7f\u900f\u6d77\u9762\uff0c\u7cbe\u786e\u7684\u6c34\u4e0b\u5b9a\u4f4d\u4e00\u76f4\u662f\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u57fa\u672c\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u53ef\u8bbf\u95ee\u7684\u6c34\u4e0b\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u6a21\u5757\u5316\u786c\u4ef6\u67b6\u6784\uff0c\u5305\u62ec\u6c34\u542c\u5668\u9635\u5217\u3001\u591a\u901a\u9053\u524d\u7f6e\u653e\u5927\u5668\u3001OCXO\u3001\u6811\u8393\u6d3e5\u548cDAQ\u677f\u3002\u4f7f\u7528\u5f00\u6e90C++\u8f6f\u4ef6\u6846\u67b6\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u65f6\u949f\u540c\u6b65\u3001\u4fe1\u53f7\u5904\u7406\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u4f30\u8ba1\u4fe1\u53f7\u98de\u884c\u65f6\u95f4\u548c\u5230\u8fbe\u65b9\u5411\u3002", "result": "\u5728\u6d88\u58f0\u6c60\u3001\u6de1\u6c34\u6e56\u548c\u516c\u6d77\u8bd5\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u659c\u8ddd\u7cbe\u5ea6\u4f18\u4e8e0.1%\uff0c\u65b9\u4f4d\u7cbe\u5ea6\u57280.1\u00b0\u4ee5\u5185\uff0c\u5728\u957f\u8fbe1.3\u516c\u91cc\u7684\u64cd\u4f5c\u8ddd\u79bb\u5185\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u4f4e\u6210\u672c\u3001\u53ef\u590d\u5236\u7684\u786c\u4ef6\u53ef\u4ee5\u63d0\u4f9b\u7814\u7a76\u7ea7\u7684\u6c34\u4e0b\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u5f00\u6e90\u786c\u4ef6\u548c\u8f6f\u4ef6\u964d\u4f4e\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u5b9e\u9a8c\u5ba4\u7684\u5165\u95e8\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u6c34\u4e0b\u58f0\u5b66\u5bfc\u822a\u548c\u7fa4\u4f53\u673a\u5668\u4eba\u7684\u534f\u4f5c\u521b\u65b0\u3002"}}
{"id": "2511.07081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07081", "abs": "https://arxiv.org/abs/2511.07081", "authors": ["Guanghu Xie", "Mingxu Li", "Songwei Wu", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects", "comment": null, "summary": "Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase.", "AI": {"tldr": "HDCNet\uff1a\u4e00\u79cd\u7ed3\u5408Transformer\u3001CNN\u548cMamba\u67b6\u6784\u7684\u65b0\u578b\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\uff0c\u4e13\u95e8\u89e3\u51b3\u900f\u660e\u548c\u53cd\u5149\u7269\u4f53\u7684\u6df1\u5ea6\u611f\u77e5\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u673a\u5668\u4eba\u6293\u53d6\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe60%\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f20\u611f\u5668\u5728\u900f\u660e\u548c\u53cd\u5149\u7269\u4f53\u8868\u9762\u5f80\u5f80\u65e0\u6cd5\u63d0\u4f9b\u53ef\u9760\u7684\u6d4b\u91cf\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u611f\u77e5\u548c\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u53cc\u5206\u652fTransformer-CNN\u7f16\u7801\u5668\u63d0\u53d6\u6a21\u6001\u7279\u5b9a\u7279\u5f81\uff1b\u6d45\u5c42\u5f15\u5165\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\uff1b\u7f51\u7edc\u74f6\u9888\u5904\u5f00\u53d1Transformer-Mamba\u6df7\u5408\u878d\u5408\u6a21\u5757\uff0c\u5b9e\u73b0\u9ad8\u5c42\u8bed\u4e49\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6df1\u5ea6\u6574\u5408\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u8865\u5168\u6027\u80fd\uff1b\u673a\u5668\u4eba\u6293\u53d6\u5b9e\u9a8c\u663e\u793a\u5bf9\u900f\u660e\u548c\u53cd\u5149\u7269\u4f53\u7684\u6293\u53d6\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad8\u8fbe60%\u3002", "conclusion": "HDCNet\u901a\u8fc7\u6709\u6548\u6574\u5408\u591a\u79cd\u67b6\u6784\u7684\u4f18\u52bf\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u900f\u660e\u548c\u53cd\u5149\u7269\u4f53\u7684\u6df1\u5ea6\u611f\u77e5\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6293\u53d6\u6027\u80fd\u3002"}}
{"id": "2511.07155", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07155", "abs": "https://arxiv.org/abs/2511.07155", "authors": ["Thomas Steinecker", "Alexander Bienemann", "Denis Trescher", "Thorsten Luettel", "Mirko Maehlisch"], "title": "Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u65f6\u7a7a\u5bf9\u9f50\u7b56\u7565\u5c06\u8fd0\u52a8\u89c4\u5212\u4e0e\u8f66\u8f86\u63a7\u5236\u89e3\u8026\u7684\u6846\u67b6\uff0c\u5b9e\u73b0RL\u667a\u80fd\u4f53\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u7531\u4e8e\u8f66\u8f86\u52a8\u529b\u5b66\u590d\u6742\u6027\u53ca\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e0d\u5339\u914d\uff0cRL\u5728\u771f\u5b9e\u8f66\u8f86\u4e0a\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u73b0\u5b9e\u4e16\u754c\u52a8\u529b\u5b66\u3002", "method": "\u4f7f\u7528\u8fd0\u52a8\u5b66\u81ea\u884c\u8f66\u6a21\u578b\u8bad\u7ec3RL\u667a\u80fd\u4f53\u8f93\u51fa\u8fde\u7eed\u63a7\u5236\u52a8\u4f5c\uff0c\u5c06\u5176\u884c\u4e3a\u84b8\u998f\u4e3a\u8f68\u8ff9\u9884\u6d4b\u667a\u80fd\u4f53\u751f\u6210\u6709\u9650\u8303\u56f4\u81ea\u8f66\u8f68\u8ff9\uff0c\u901a\u8fc7Stanley\u63a7\u5236\u5668\u63a7\u5236\u6a2a\u5411\u52a8\u529b\u5b66\uff0c\u7eb5\u5411\u5bf9\u9f50\u91c7\u7528\u81ea\u9002\u5e94\u66f4\u65b0\u673a\u5236\u8865\u507f\u865a\u62df\u4e0e\u73b0\u5b9e\u8f68\u8ff9\u504f\u5dee\u3002", "result": "\u5728\u771f\u5b9e\u8f66\u8f86\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u5bf9\u9f50\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u57fa\u4e8eRL\u7684\u8fd0\u52a8\u89c4\u5212\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7a33\u5065\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "conclusion": "\u6210\u529f\u5c06\u9ad8\u5c42\u8f68\u8ff9\u751f\u6210\u4e0e\u4f4e\u5c42\u8f66\u8f86\u63a7\u5236\u89e3\u8026\uff0c\u4e3aRL\u5728\u771f\u5b9e\u8f66\u8f86\u4e0a\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.07175", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07175", "abs": "https://arxiv.org/abs/2511.07175", "authors": ["Marvin R\u00fcdt", "Constantin Enke", "Kai Furmans"], "title": "Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets", "comment": "submitted to the IEEE for possible publication; 8 pages, 6 figures, 2 tables", "summary": "Efficient routing of mobile robot fleets is crucial in intralogistics, where delays and deadlocks can substantially reduce system throughput. Roadmap design, specifying feasible transport routes, directly affects fleet coordination and computational performance. Existing approaches are either grid-based, compromising geometric precision, or continuous-space approaches that disregard practical constraints. This paper presents an automated roadmap generation approach that bridges this gap by operating in continuous-space, integrating station-to-station transport demand and enforcing minimum distance constraints for nodes and edges. By combining free space discretization, transport demand-driven $K$-shortest-path optimization, and path smoothing, the approach produces roadmaps tailored to intralogistics applications. Evaluation across multiple intralogistics use cases demonstrates that the proposed approach consistently outperforms established baselines (4-connected grid, 8-connected grid, and random sampling), achieving lower structural complexity, higher redundancy, and near-optimal path lengths, enabling efficient and robust routing of mobile robot fleets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u8def\u7ebf\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u7ed3\u5408\u7ad9\u70b9\u95f4\u8fd0\u8f93\u9700\u6c42\u548c\u6700\u5c0f\u8ddd\u79bb\u7ea6\u675f\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u8f66\u961f\u751f\u6210\u4f18\u5316\u7684\u5bfc\u822a\u8def\u7ebf\u56fe", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u57fa\u4e8e\u7f51\u683c\u800c\u727a\u7272\u51e0\u4f55\u7cbe\u5ea6\uff0c\u8981\u4e48\u662f\u8fde\u7eed\u7a7a\u95f4\u65b9\u6cd5\u4f46\u5ffd\u7565\u5b9e\u9645\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u51e0\u4f55\u7cbe\u5ea6\u548c\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u7684\u8def\u7ebf\u56fe\u751f\u6210\u65b9\u6cd5", "method": "\u7ed3\u5408\u81ea\u7531\u7a7a\u95f4\u79bb\u6563\u5316\u3001\u8fd0\u8f93\u9700\u6c42\u9a71\u52a8\u7684K\u6700\u77ed\u8def\u5f84\u4f18\u5316\u548c\u8def\u5f84\u5e73\u6ed1\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u751f\u6210\u8def\u7ebf\u56fe\u5e76\u5f3a\u5236\u6267\u884c\u8282\u70b9\u548c\u8fb9\u7684\u6700\u5c0f\u8ddd\u79bb\u7ea6\u675f", "result": "\u5728\u591a\u4e2a\u5185\u90e8\u7269\u6d41\u7528\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff084\u8fde\u63a5\u7f51\u683c\u30018\u8fde\u63a5\u7f51\u683c\u548c\u968f\u673a\u91c7\u6837\uff09\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7ed3\u6783\u590d\u6742\u6027\u3001\u66f4\u9ad8\u7684\u5197\u4f59\u5ea6\u548c\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u5f84\u957f\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u8f66\u961f\u751f\u6210\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u8def\u7ebf\u56fe\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf"}}
{"id": "2511.07275", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.07275", "abs": "https://arxiv.org/abs/2511.07275", "authors": ["David Black", "Septimiu Salcudean"], "title": "Robotic versus Human Teleoperation for Remote Ultrasound", "comment": "Under review at IEEE TMRB. Extended version of a paper presented at the Hamlyn Symposium for Medical Robotics, 2025", "summary": "Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.", "AI": {"tldr": "\u6bd4\u8f83\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u4e0e\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u5728\u8fdc\u7a0b\u8d85\u58f0\u68c0\u67e5\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u5b8c\u6210\u65f6\u95f4\u548c\u4f4d\u7f6e\u7cbe\u5ea6\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u5728\u529b\u5e94\u7528\u65b9\u9762\u66f4\u4e00\u81f4\u4e14\u66f4\u5b9e\u7528", "motivation": "\u89e3\u51b3\u519c\u6751\u5730\u533a\u8d85\u58f0\u68c0\u67e5\u4e13\u5bb6\u77ed\u7f3a\u95ee\u9898\uff0c\u5f00\u53d1\u8fdc\u7a0b\u8d85\u58f0\u6280\u672f\uff0c\u6bd4\u8f83\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u548c\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7684\u76f8\u5bf9\u4f18\u52bf", "method": "\u8bc4\u4f30\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7684\u5b9e\u9645\u65b9\u9762\uff08\u5982\u8bbe\u7f6e\u65f6\u95f4\u548c\u7075\u6d3b\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u5b8c\u6210\u65f6\u95f4\u3001\u4f4d\u7f6e\u8ddf\u8e2a\u548c\u529b\u4e00\u81f4\u6027\u7b49\u6027\u80fd\u6307\u6807", "result": "\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u5728\u5b8c\u6210\u65f6\u95f4\u6216\u4f4d\u7f6e\u7cbe\u5ea6\u4e0a\u6ca1\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u5dee\u5f02\uff08\u5e73\u5747\u5dee\u5f02\u5206\u522b\u4e3a1.8%\u548c0.5%\uff09\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u529b\u5e94\u7528\uff0c\u540c\u65f6\u66f4\u5177\u5b9e\u7528\u6027\u548c\u53ef\u53ca\u6027", "conclusion": "\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u5728\u6027\u80fd\u4e0a\u4e0e\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u76f8\u5f53\uff0c\u4f46\u66f4\u5b9e\u7528\u548c\u6613\u4e8e\u83b7\u53d6\uff0c\u7279\u522b\u9002\u5408\u5c0f\u578b\u793e\u533a\u4f7f\u7528"}}
{"id": "2511.07292", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07292", "abs": "https://arxiv.org/abs/2511.07292", "authors": ["Simon Gerstenecker", "Andreas Geiger", "Katrin Renz"], "title": "PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving", "comment": null, "summary": "Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PlanT 2.0\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c4\u5212\u53d8\u6362\u5668\uff0c\u7528\u4e8eCARLA\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6270\u52a8\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u5411\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u5f00\u53d1\u8f6c\u53d8\u7684\u5efa\u8bae\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u57fa\u51c6\u6027\u80fd\u548c\u65b9\u6cd5\u521b\u65b0\uff0c\u800c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5931\u8d25\u3001\u504f\u89c1\u548c\u6377\u5f84\u5b66\u4e60\u7684\u6df1\u5165\u5206\u6790\u3002\u8fd9\u5bfc\u81f4\u6539\u8fdb\u7f3a\u4e4f\u5bf9\u5f53\u524d\u5931\u8d25\u539f\u56e0\u7684\u6df1\u523b\u7406\u89e3\u3002", "method": "\u5f15\u5165PlanT 2.0\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u8c61\u7ea7\u8868\u793a\u7684\u89c4\u5212\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u53ef\u63a7\u7684\u8f93\u5165\u6270\u52a8\uff08\u5982\u6539\u53d8\u5bf9\u8c61\u4f4d\u7f6e\u3001\u6dfb\u52a0\u6216\u5220\u9664\u5bf9\u8c61\uff09\u6765\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5728CARLA Leaderboard 2.0\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0cPlanT 2.0\u5728Longest6 v2\u3001Bench2Drive\u548cCARLA\u9a8c\u8bc1\u8def\u7ebf\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5206\u6790\u63ed\u793a\u4e86\u7531\u4e8e\u969c\u788d\u7269\u591a\u6837\u6027\u4e0d\u8db3\u5bfc\u81f4\u7684\u573a\u666f\u7406\u89e3\u7f3a\u4e4f\u3001\u521a\u6027\u4e13\u5bb6\u884c\u4e3a\u5bfc\u81f4\u7684\u6613\u88ab\u5229\u7528\u7684\u6377\u5f84\uff0c\u4ee5\u53ca\u5bf9\u56fa\u5b9a\u4e13\u5bb6\u8f68\u8ff9\u96c6\u7684\u8fc7\u62df\u5408\u7b49\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u4f5c\u8005\u4e3b\u5f20\u5411\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u5f00\u53d1\u8f6c\u53d8\uff0c\u91cd\u70b9\u5173\u6ce8\u66f4\u4e30\u5bcc\u3001\u66f4\u9c81\u68d2\u3001\u504f\u89c1\u66f4\u5c11\u7684\u6570\u636e\u96c6\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2511.07375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07375", "abs": "https://arxiv.org/abs/2511.07375", "authors": ["Shaohang Han", "Joris Verhagen", "Jana Tumova"], "title": "Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications", "comment": null, "summary": "We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u548c\u7cbe\u786e\u7684max/min\u7b97\u5b50\u91cd\u6784\u5b9e\u73b0\u7cbe\u786e\u3001\u5e73\u6ed1\u4e14\u53ef\u9760\u7684STL\u5408\u6210\u3002", "motivation": "\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u662f\u63cf\u8ff0\u65f6\u7a7a\u9700\u6c42\u7684\u6709\u7528\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7684STL\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u8fd1\u4f3c\u8bef\u5dee\u6216\u4e0d\u53ef\u5fae\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u7cbe\u786e\u4e14\u53ef\u5fae\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06STL\u5408\u6210\u5efa\u6a21\u4e3a\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528STL\u9c81\u68d2\u6027\u8bed\u4e49\uff0c\u5f15\u5165max\u548cmin\u7b97\u5b50\u7684\u7cbe\u786e\u91cd\u6784\u4ee5\u907f\u514d\u8fd1\u4f3c\u8bef\u5dee\uff0c\u786e\u4fdd\u95ee\u9898\u7684\u53ef\u5fae\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u503c\u6a21\u62df\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u6027\u80fd\uff0c\u8bc1\u660e\u80fd\u591f\u6709\u6548\u5904\u7406STL\u89c4\u8303\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684STL\u5408\u6210\u65b9\u6cd5\u662f\u7cbe\u786e\u3001\u5e73\u6ed1\u4e14\u53ef\u9760\u7684\uff0c\u4e3a\u5177\u6709\u590d\u6742\u65f6\u7a7a\u7ea6\u675f\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.07381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07381", "abs": "https://arxiv.org/abs/2511.07381", "authors": ["Yizhe Zhu", "Zhang Ye", "Boce Hu", "Haibo Zhao", "Yu Qi", "Dian Wang", "Robert Platt"], "title": "Residual Rotation Correction using Tactile Equivariance", "comment": "8 pages", "summary": "Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks.", "AI": {"tldr": "EquiTac\u662f\u4e00\u4e2a\u5229\u7528SO(2)\u5bf9\u79f0\u6027\u6539\u8fdb\u89e6\u89c9\u7b56\u7565\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u5efa\u89e6\u89c9\u4f20\u611f\u5668\u7684\u8868\u9762\u6cd5\u5411\u91cf\u5e76\u4f7f\u7528\u7b49\u53d8\u7f51\u7edc\u9884\u6d4b\u65cb\u8f6c\u52a8\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u6548\u6837\u672c\u5229\u7528\u548c\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u89e6\u89c9\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u9ad8\u89e6\u89c9\u7b56\u7565\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u65f6\u6837\u672c\u6548\u7387\u4f4e\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1) \u4ece\u89e6\u89c9\u4f20\u611f\u5668RGB\u8f93\u5165\u91cd\u5efa\u8868\u9762\u6cd5\u5411\u91cf\uff1b2) \u4f7f\u7528SO(2)\u7b49\u53d8\u7f51\u7edc\u9884\u6d4b\u65cb\u8f6c\u52a8\u4f5c\u6b8b\u5dee\uff1b3) \u5728\u6d4b\u8bd5\u65f6\u589e\u5f3a\u57fa\u7840\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u5b9e\u65f6\u65cb\u8f6c\u6821\u6b63\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\uff0cEquiTac\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u5c31\u80fd\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u7684\u624b\u5185\u65b9\u5411\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u5373\u4f7f\u4f7f\u7528\u66f4\u591a\u8bad\u7ec3\u6570\u636e\u4e5f\u65e0\u6cd5\u8fbe\u5230\u76f8\u540c\u6548\u679c\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u660e\u786e\u7f16\u7801\u89e6\u89c9\u7b49\u53d8\u6027\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5bf9\u79f0\u611f\u77e5\u7684\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.07407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07407", "abs": "https://arxiv.org/abs/2511.07407", "authors": ["Zhengjie Xu", "Ye Li", "Kwan-yee Lin", "Stella X. Yu"], "title": "Unified Humanoid Fall-Safety Policy from a Few Demonstrations", "comment": null, "summary": "Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7b56\u7565\uff0c\u5c06\u9632\u6454\u3001\u51b2\u51fb\u7f13\u89e3\u548c\u5feb\u901f\u6062\u590d\u6574\u5408\u5230\u4e00\u4e2a\u7b56\u7565\u4e2d\uff0c\u901a\u8fc7\u878d\u5408\u7a00\u758f\u4eba\u7c7b\u6f14\u793a\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6269\u6563\u8bb0\u5fc6\u6765\u5b9e\u73b0\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u6454\u5012\u662f\u4e00\u4e2a\u56fa\u6709\u98ce\u9669\uff0c\u73b0\u6709\u65b9\u6cd5\u53ea\u5173\u6ce8\u6454\u5012\u7684\u5b64\u7acb\u65b9\u9762\uff08\u907f\u514d\u6454\u5012\u3001\u63a7\u5236\u4e0b\u964d\u6216\u7ad9\u8d77\uff09\uff0c\u7f3a\u4e4f\u5b8c\u6574\u7684\u6454\u5012\u6062\u590d\u7b56\u7565\u3002", "method": "\u878d\u5408\u7a00\u758f\u4eba\u7c7b\u6f14\u793a\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6269\u6563\u8bb0\u5fc6\u7684\u5b89\u5168\u53cd\u5e94\uff0c\u5b66\u4e60\u81ea\u9002\u5e94\u5168\u8eab\u884c\u4e3a\u3002", "result": "\u5728\u4eff\u771f\u548cUnitree G1\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86\u9c81\u68d2\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3001\u66f4\u4f4e\u7684\u51b2\u51fb\u529b\u548c\u4e00\u81f4\u7684\u5feb\u901f\u6062\u590d\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u5177\u97e7\u6027\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fd0\u884c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.07410", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07410", "abs": "https://arxiv.org/abs/2511.07410", "authors": ["Hao Wang", "Sathwik Karnik", "Bea Lim", "Somil Bansal"], "title": "Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective", "comment": null, "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u4ece\u63a7\u5236\u7406\u8bba\u89c6\u89d2\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u95ed\u73af\u7b26\u53f7\u89c4\u5212\u5668\uff0c\u5206\u6790\u63a7\u5236\u65f6\u57df\u548c\u70ed\u542f\u52a8\u5bf9\u6027\u80fd\u7684\u5f71\u54cd", "motivation": "LLMs\u548cVLMs\u5728\u5177\u8eab\u7b26\u53f7\u89c4\u5212\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5982\u4f55\u6709\u6548\u7528\u4e8e\u95ed\u73af\u7b26\u53f7\u89c4\u5212\u4ecd\u5f85\u63a2\u7d22\u3002\u7531\u4e8e\u5b83\u4eec\u662f\u9ed1\u76d2\u6a21\u578b\uff0c\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u6216\u4ee3\u4ef7\u9ad8\u6602\u7684\u9519\u8bef\uff0c\u4f7f\u5f97\u5728\u9ad8\u5c42\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u7684\u4f7f\u7528\u7279\u522b\u5177\u6709\u6311\u6218\u6027", "method": "\u4ece\u63a7\u5236\u7406\u8bba\u89c6\u89d2\u7814\u7a76VLMs\u4f5c\u4e3a\u95ed\u73af\u7b26\u53f7\u89c4\u5212\u5668\uff0c\u8bbe\u8ba1\u53d7\u63a7\u5b9e\u9a8c\u7814\u7a76\u63a7\u5236\u65f6\u57df\u548c\u70ed\u542f\u52a8\u5bf9VLM\u7b26\u53f7\u89c4\u5212\u5668\u6027\u80fd\u7684\u5f71\u54cd", "result": "\u83b7\u5f97\u4e86\u5e7f\u6cdb\u9002\u7528\u4e8e\u5c06VLMs\u7528\u4f5c\u95ed\u73af\u7b26\u53f7\u89c4\u5212\u5668\u7684\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u52a9\u4e8e\u63d0\u9ad8VLM\u7b26\u53f7\u89c4\u5212\u5668\u6027\u80fd\u7684\u5efa\u8bae", "conclusion": "\u901a\u8fc7\u63a7\u5236\u7406\u8bba\u89c6\u89d2\u5206\u6790VLM\u7b26\u53f7\u89c4\u5212\u5668\u7684\u5173\u952e\u53c2\u6570\uff0c\u4e3a\u6539\u8fdb\u5176\u95ed\u73af\u89c4\u5212\u6027\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc"}}
{"id": "2511.07416", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07416", "abs": "https://arxiv.org/abs/2511.07416", "authors": ["Jiageng Mao", "Sicheng He", "Hao-Ning Wu", "Yang You", "Shuyang Sun", "Zhicheng Wang", "Yanan Bao", "Huizhong Chen", "Leonidas Guibas", "Vitor Guizilini", "Howard Zhou", "Yue Wang"], "title": "Robot Learning from a Physical World Model", "comment": "Project page: https://pointscoder.github.io/PhysWorld_Web/", "summary": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.", "AI": {"tldr": "PhysWorld\u662f\u4e00\u4e2a\u901a\u8fc7\u7269\u7406\u4e16\u754c\u5efa\u6a21\u4ece\u89c6\u9891\u751f\u6210\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u751f\u6210\u4e0e\u7269\u7406\u91cd\u5efa\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u4ece\u8bed\u8a00\u547d\u4ee4\u548c\u56fe\u50cf\u5408\u6210\u903c\u771f\u7684\u89c6\u89c9\u6f14\u793a\uff0c\u4f46\u76f4\u63a5\u5c06\u50cf\u7d20\u8fd0\u52a8\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba\u4f1a\u5ffd\u7565\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u64cd\u4f5c\u4e0d\u51c6\u786e\u3002", "method": "\u7ed3\u5408\u89c6\u9891\u751f\u6210\u4e0e\u7269\u7406\u4e16\u754c\u91cd\u5efa\uff0c\u901a\u8fc7\u7269\u4f53\u4e2d\u5fc3\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u5c06\u751f\u6210\u7684\u89c6\u9891\u8fd0\u52a8\u8f6c\u5316\u4e3a\u7269\u7406\u4e0a\u51c6\u786e\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPhysWorld\u76f8\u6bd4\u4e4b\u524d\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u4f5c\u51c6\u786e\u6027\u3002", "conclusion": "PhysWorld\u5c06\u9690\u5f0f\u89c6\u89c9\u6307\u5bfc\u8f6c\u5316\u4e3a\u7269\u7406\u4e0a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u65e0\u9700\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002"}}
{"id": "2511.07418", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.DC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.07418", "abs": "https://arxiv.org/abs/2511.07418", "authors": ["Zhao-Heng Yin", "Pieter Abbeel"], "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields", "comment": "Code: https://github.com/zhaohengyin/lightning-grasp", "summary": "Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.", "AI": {"tldr": "Lightning Grasp\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u7a0b\u5e8f\u5316\u6293\u53d6\u5408\u6210\u7b97\u6cd5\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u6570\u91cf\u7ea7\u52a0\u901f\uff0c\u652f\u6301\u4e0d\u89c4\u5219\u5de5\u5177\u7c7b\u7269\u4f53\u7684\u65e0\u76d1\u7763\u6293\u53d6\u751f\u6210\u3002", "motivation": "\u5c3d\u7ba1\u7ecf\u8fc7\u591a\u5e74\u7814\u7a76\uff0c\u7075\u5de7\u624b\u7684\u5b9e\u65f6\u591a\u6837\u5316\u6293\u53d6\u5408\u6210\u4ecd\u7136\u662f\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u672a\u89e3\u51b3\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5173\u952e\u6d1e\u5bdf\uff1a\u4f7f\u7528\u7b80\u5355\u9ad8\u6548\u7684\u6570\u636e\u7ed3\u6784\u2014\u2014\u63a5\u89e6\u573a\uff08Contact Field\uff09\uff0c\u5c06\u590d\u6742\u51e0\u4f55\u8ba1\u7b97\u4e0e\u641c\u7d22\u8fc7\u7a0b\u89e3\u8026\uff0c\u4ece\u800c\u964d\u4f4e\u95ee\u9898\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u524d\u6240\u672a\u6709\u7684\u7a0b\u5e8f\u5316\u641c\u7d22\u901f\u5ea6\u3002", "result": "\u7b97\u6cd5\u907f\u514d\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u8bb8\u591a\u9650\u5236\uff0c\u5982\u9700\u8981\u7cbe\u5fc3\u8c03\u4f18\u7684\u80fd\u91cf\u51fd\u6570\u548c\u654f\u611f\u521d\u59cb\u5316\uff0c\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5df2\u5f00\u6e90\uff0c\u4ee5\u63a8\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8fdb\u4e00\u6b65\u521b\u65b0\u3002"}}
