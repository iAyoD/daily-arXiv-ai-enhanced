<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Language-in-the-Loop Culvert Inspection on the Erie Canal](https://arxiv.org/abs/2509.21370)
*Yashom Dighe,Yash Turkar,Karthik Dantu*

Main category: cs.RO

TL;DR: VISION是一个端到端的自主检查系统，结合视觉语言模型和约束视点规划，用于自动检查运河涵洞，无需领域特定微调即可生成高质量检查报告。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查运河涵洞面临年龄、几何形状、光照差、天气和难以接近等挑战，需要开发自动化解决方案。

Method: 使用网络级视觉语言模型生成感兴趣区域建议，融合立体深度恢复尺度，通过约束感知规划器指挥重新定位拍摄特写图像。

Result: 在伊利运河涵洞部署中，初始感兴趣区域建议与专家达成61.4%一致，重新成像后评估达到80%一致。

Conclusion: VISION系统能够将初步假设转化为与专家一致的可靠发现，证明语言在环自主系统在基础设施检查中的有效性。

Abstract: Culverts on canals such as the Erie Canal, built originally in 1825, require
frequent inspections to ensure safe operation. Human inspection of culverts is
challenging due to age, geometry, poor illumination, weather, and lack of easy
access. We introduce VISION, an end-to-end, language-in-the-loop autonomy
system that couples a web-scale vision-language model (VLM) with constrained
viewpoint planning for autonomous inspection of culverts. Brief prompts to the
VLM solicit open-vocabulary ROI proposals with rationales and confidences,
stereo depth is fused to recover scale, and a planner -- aware of culvert
constraints -- commands repositioning moves to capture targeted close-ups.
Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the
see, decide, move, re-image loop on-board and produces high-resolution images
for detailed reporting without domain-specific fine-tuning. In an external
evaluation by New York Canal Corporation personnel, initial ROI proposals
achieved 61.4\% agreement with subject-matter experts, and final
post-re-imaging assessments reached 80\%, indicating that VISION converts
tentative hypotheses into grounded, expert-aligned findings.

</details>


### [2] [Developing a Mono-Actuated Compliant GeoGami Robot](https://arxiv.org/abs/2509.21445)
*Archie Webster,Lee Skull,Seyed Amir Tafrishi*

Main category: cs.RO

TL;DR: GeoGami是一个单驱动软刚性机器人平台，利用折纸表面柔顺性和几何柔顺骨架实现形状变换和运动，仅需一个执行器即可完成变形和滚动运动。


<details>
  <summary>Details</summary>
Motivation: 解决折纸表面自由度多、需要大量执行器的问题，通过集成表面柔顺性来提高重复性，开发能够通过形状变换进入不同环境并使用形状变换进行运动的机器人。

Method: 结合折纸表面柔顺性和几何柔顺骨架，设计单驱动移动平台，开发刚度模型和中心齿轮箱机制，分析替代的缆线驱动执行方法以实现表面变换。

Result: 成功演示了GeoGami机器人平台，实现了形状变换和滚动能力，验证了单驱动系统的可行性。

Conclusion: GeoGami平台为能够通过形状变换进入不同环境并使用形状变换进行运动的机器人开辟了新能力。

Abstract: This paper presents the design of a new soft-rigid robotic platform,
"GeoGami". We leverage origami surface capabilities to achieve shape
contraction and to support locomotion with underactuated forms. A key challenge
is that origami surfaces have high degrees of freedom and typically require
many actuators; we address repeatability by integrating surface compliance. We
propose a mono-actuated GeoGami mobile platform that combines origami surface
compliance with a geometric compliant skeleton, enabling the robot to transform
and locomote using a single actuator. We demonstrate the robot, develop a
stiffness model, and describe the central gearbox mechanism. We also analyze
alternative cable-driven actuation methods for the skeleton to enable surface
transformation. Finally, we evaluate the GeoGami platform for capabilities,
including shape transformation and rolling. This platform opens new
capabilities for robots that change shape to access different environments and
that use shape transformation for locomotion.

</details>


### [3] [Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation](https://arxiv.org/abs/2509.21496)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yingming Chen,Cheuk Chi Tsang*

Main category: cs.RO

TL;DR: 该论文提出了一种用于四旋翼无人机在近壁环境中安全操作的吸力补偿模型预测控制(SC-MPC)框架，通过物理建模和优化控制解决壁面附近空气动力学效应带来的稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 四旋翼在近壁城市或室内环境(如检查和搜救任务)中运行时，由于壁面接近产生的未建模空气动力学效应会生成复杂涡流，诱导破坏稳定的吸力，可能导致危险振动或碰撞。

Method: 提出(1)基于物理的吸力模型，明确表征转子速度和壁距的依赖关系；(2)吸力补偿模型预测控制(SC-MPC)框架，将增强的动力学模型(考虑吸力效应)表述为因子图优化问题，集成系统动力学约束、轨迹跟踪目标、控制输入平滑要求和执行器物理限制。

Result: 实验验证显示SC-MPC在X轴和Y轴位置控制中分别达到2.1cm和2.0cm的均方根误差，相比级联PID控制提升74%和79%，相比标准MPC提升60%和53%。平均绝对误差指标(1.2cm X轴，1.4cm Y轴)同样优于两个基线。

Conclusion: SC-MPC框架在近壁操作中实现了准确稳定的轨迹跟踪，显著优于传统控制方法，并通过开源实现促进可重复性和社区采用。

Abstract: The safe operation of quadrotors in near-wall urban or indoor environments
(e.g., inspection and search-and-rescue missions) is challenged by unmodeled
aerodynamic effects arising from wall-proximity. It generates complex vortices
that induce destabilizing suction forces, potentially leading to hazardous
vibrations or collisions. This paper presents a comprehensive solution
featuring (1) a physics-based suction force model that explicitly characterizes
the dependency on both rotor speed and wall distance, and (2) a
suction-compensated model predictive control (SC-MPC) framework designed to
ensure accurate and stable trajectory tracking during wall-proximity
operations. The proposed SC-MPC framework incorporates an enhanced dynamics
model that accounts for suction force effects, formulated as a factor graph
optimization problem integrating system dynamics constraints, trajectory
tracking objectives, control input smoothness requirements, and actuator
physical limitations. The suction force model parameters are systematically
identified through extensive experimental measurements across varying
operational conditions. Experimental validation demonstrates SC-MPC's superior
performance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0
cm RMSE in Y-axis position control - representing 74% and 79% improvements over
cascaded proportional-integral-derivative (PID) control, and 60% and 53%
improvements over standard MPC respectively. The corresponding mean absolute
error (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both
baselines. The evaluation platform employs a ducted quadrotor design that
provides collision protection while maintaining aerodynamic efficiency. To
facilitate reproducibility and community adoption, we have open-sourced our
complete implementation, available at
https://anonymous.4open.science/r/SC-MPC-6A61.

</details>


### [4] [DroneFL: Federated Learning for Multi-UAV Visual Target Tracking](https://arxiv.org/abs/2509.21523)
*Xiaofan Yu,Yuwei Wu,Katherine Mao,Ye Tian,Vijay Kumar,Tajana Rosing*

Main category: cs.RO

TL;DR: DroneFL是首个专门为多无人机目标跟踪设计的联邦学习框架，通过轻量级本地模型、位置不变架构和云端轨迹规划，显著提升了预测精度和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 多机器人目标跟踪在农业、环境监测等领域很重要，但联邦学习在多无人机目标跟踪中的应用尚未充分探索，面临计算资源有限、数据异构性强、轨迹预测与规划耦合紧密等挑战。

Method: 设计轻量级本地模型（冻结YOLO骨干+浅层Transformer），采用位置不变架构和基于高度的自适应实例归一化缓解数据异构性，云端融合多无人机预测并生成最优轨迹。

Result: 相比分布式非联邦学习框架，DroneFL将预测误差降低6%-83%，跟踪距离减少0.4%-4.6%，在树莓派5上实时运行，平均云数据率仅1.56 KBps。

Conclusion: DroneFL成功解决了多无人机目标跟踪中的联邦学习挑战，在保持高效率的同时显著提升了性能，为实际部署提供了可行方案。

Abstract: Multi-robot target tracking is a fundamental problem that requires
coordinated monitoring of dynamic entities in applications such as precision
agriculture, environmental monitoring, disaster response, and security
surveillance. While Federated Learning (FL) has the potential to enhance
learning across multiple robots without centralized data aggregation, its use
in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely
underexplored. Key challenges include limited onboard computational resources,
significant data heterogeneity in FL due to varying targets and the fields of
view, and the need for tight coupling between trajectory prediction and
multi-robot planning. In this paper, we introduce DroneFL, the first federated
learning framework specifically designed for efficient multi-UAV target
tracking. We design a lightweight local model to predict target trajectories
from sensor inputs, using a frozen YOLO backbone and a shallow transformer for
efficient onboard training. The updated models are periodically aggregated in
the cloud for global knowledge sharing. To alleviate the data heterogeneity
that hinders FL convergence, DroneFL introduces a position-invariant model
architecture with altitude-based adaptive instance normalization. Finally, we
fuse predictions from multiple UAVs in the cloud and generate optimal
trajectories that balance target prediction accuracy and overall tracking
performance. Our results show that DroneFL reduces prediction error by 6%-83%
and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.
In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has
on average just 1.56 KBps data rate to the cloud.

</details>


### [5] [Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation](https://arxiv.org/abs/2509.21543)
*Jinbang Huang,Zhiyuan Li,Zhanguang Zhang,Xingyue Quan,Jianye Hao,Yingxue Zhang*

Main category: cs.RO

TL;DR: Plan2Evolve是一个LLM自我进化框架，通过生成规划领域来产生符号问题-规划对作为推理轨迹，然后转化为扩展的思维链轨迹，从而提升LLM的规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将规划领域主要视为搜索工具，忽视了其作为可扩展推理数据源的潜力；同时机器人领域的思维链监督仍依赖昂贵的人工标注数据集。

Method: 基础模型生成规划领域作为引擎，产生符号问题-规划对作为推理轨迹，然后由同一模型通过自然语言解释转化为扩展思维链轨迹，实现符号规划结构与自然语言推理的显式对齐。

Result: 生成的数据超越了模型内在规划能力，通过微调产生规划增强的LLM，具有更高的规划成功率、更强的跨任务泛化能力和更低的推理成本。

Conclusion: Plan2Evolve框架成功实现了LLM的自我进化，通过自动生成规划数据显著提升了规划性能，为机器人任务规划提供了有效的解决方案。

Abstract: Large Language Models (LLMs) have recently shown strong potential in robotic
task planning, particularly through automatic planning domain generation that
integrates symbolic search. Prior approaches, however, have largely treated
these domains as search utilities, with limited attention to their potential as
scalable sources of reasoning data. At the same time, progress in reasoning
LLMs has been driven by chain-of-thought (CoT) supervision, whose application
in robotics remains dependent on costly, human-curated datasets. We propose
Plan2Evolve, an LLM self-evolving framework in which the base model generates
planning domains that serve as engines for producing symbolic problem-plan
pairs as reasoning traces. These pairs are then transformed into extended CoT
trajectories by the same model through natural-language explanations, thereby
explicitly aligning symbolic planning structures with natural language
reasoning. The resulting data extend beyond the model's intrinsic planning
capacity, enabling model fine-tuning that yields a planning-enhanced LLM with
improved planning success, stronger cross-task generalization, and reduced
inference costs.

</details>


### [6] [PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines](https://arxiv.org/abs/2509.21563)
*Zhixin Zhang,Liang Zhao,Pawel Ladosz*

Main category: cs.RO

TL;DR: PL-VIWO2是一个基于滤波器的视觉-惯性-轮式里程计系统，通过整合IMU、轮式编码器和相机，在复杂户外环境中实现长期鲁棒的状态估计。


<details>
  <summary>Details</summary>
Motivation: 视觉里程计在自动驾驶中应用广泛，但在复杂户外城市环境中性能会下降，需要更鲁棒的解决方案。

Method: 提出了三个主要贡献：(i) 新颖的线特征处理框架，利用2D特征点和线之间的几何关系；(ii) SE(2)约束的SE(3)轮式预积分方法；(iii) 高效的运动一致性检查来过滤动态特征。

Result: 在蒙特卡洛模拟和公共自动驾驶数据集上的广泛实验表明，PL-VIWO2在精度、效率和鲁棒性方面优于最先进的方法。

Conclusion: PL-VIWO2系统通过整合多种传感器和创新的特征处理方法，在复杂环境中实现了优越的里程计性能。

Abstract: Vision-based odometry has been widely adopted in autonomous driving owing to
its low cost and lightweight setup; however, its performance often degrades in
complex outdoor urban environments. To address these challenges, we propose
PL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates
an IMU, wheel encoder, and camera (supporting both monocular and stereo) for
long-term robust state estimation. The main contributions are: (i) a novel line
feature processing framework that exploits the geometric relationship between
2D feature points and lines, enabling fast and robust line tracking and
triangulation while ensuring real-time performance; (ii) an SE(2)-constrained
SE(3) wheel pre-integration method that leverages the planar motion
characteristics of ground vehicles for accurate wheel updates; and (iii) an
efficient motion consistency check (MCC) that filters out dynamic features by
jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo
simulations and public autonomous driving datasets demonstrate that PL-VIWO2
outperforms state-of-the-art methods in terms of accuracy, efficiency, and
robustness.

</details>


### [7] [Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control](https://arxiv.org/abs/2509.21571)
*HaoZhe Xu,Cheng Cheng,HongRui Sang,Zhipeng Wang,Qiyong He,Xiuxian Li,Bin He*

Main category: cs.RO

TL;DR: 提出了一种用于GPS拒绝环境下的无人机-四足机器人自主对接框架，通过混合内部模型稳定四足机器人躯干，采用三阶段策略实现无人机精确对接。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人因频繁姿态变化难以提供稳定着陆平台的问题，实现异构系统在复杂地形中的自主对接。

Method: 四足机器人使用混合内部模型稳定躯干；无人机采用三阶段策略：远距离探测、近距离跟踪约束控制、终端安全下降。

Result: 在仿真和真实场景中验证成功，可在超过17厘米的室外楼梯和超过30度的陡坡上实现对接。

Conclusion: 该框架有效解决了四足机器人平台不稳定问题，实现了在复杂地形中的可靠自主对接。

Abstract: Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots
is essential for heterogeneous systems, yet most existing approaches target
wheeled platforms whose limited mobility constrains exploration in complex
terrains. Quadruped robots offer superior adaptability but undergo frequent
posture variations, making it difficult to provide a stable landing surface for
UAVs. To address these challenges, we propose an autonomous UAV-quadruped
docking framework for GPS-denied environments. On the quadruped side, a Hybrid
Internal Model with Horizontal Alignment (HIM-HA), learned via deep
reinforcement learning, actively stabilizes the torso to provide a level
platform. On the UAV side, a three-phase strategy is adopted, consisting of
long-range acquisition with a median-filtered YOLOv8 detector, close-range
tracking with a constraint-aware controller that integrates a Nonsingular Fast
Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function
(BF) to guarantee finite-time error convergence under field-of-view (FOV)
constraints, and terminal descent guided by a Safety Period (SP) mechanism that
jointly verifies tracking accuracy and platform stability. The proposed
framework is validated in both simulation and real-world scenarios,
successfully achieving docking on outdoor staircases higher than 17 cm and
rough slopes steeper than 30 degrees. Supplementary materials and videos are
available at: https://uav-quadruped-docking.github.io.

</details>


### [8] [Real-Time Indoor Object SLAM with LLM-Enhanced Priors](https://arxiv.org/abs/2509.21602)
*Yang Jiao,Yiding Qiu,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 提出了一种利用大语言模型提供物体几何属性常识知识作为先验因子的对象级SLAM方法，在稀疏观测条件下显著提升建图精度


<details>
  <summary>Details</summary>
Motivation: 传统对象级SLAM由于稀疏观测面临优化约束不足的问题，而现有基于常识知识的方法获取先验知识费时且缺乏跨类别泛化能力

Method: 利用大语言模型提供物体尺寸和朝向的几何属性常识知识作为图优化SLAM框架中的先验因子，特别在观测稀疏的初始阶段发挥作用

Result: 在TUM RGB-D和3RScan数据集上评估，相比最新基线方法建图精度提升36.8%，并实现了实时性能

Conclusion: LLM提供的常识知识先验能有效解决对象级SLAM中的稀疏观测问题，显著提升系统性能

Abstract: Object-level Simultaneous Localization and Mapping (SLAM), which incorporates
semantic information for high-level scene understanding, faces challenges of
under-constrained optimization due to sparse observations. Prior work has
introduced additional constraints using commonsense knowledge, but obtaining
such priors has traditionally been labor-intensive and lacks generalizability
across diverse object categories. We address this limitation by leveraging
large language models (LLMs) to provide commonsense knowledge of object
geometric attributes, specifically size and orientation, as prior factors in a
graph-based SLAM framework. These priors are particularly beneficial during the
initial phase when object observations are limited. We implement a complete
pipeline integrating these priors, achieving robust data association on sparse
object-level features and enabling real-time object SLAM. Our system, evaluated
on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\% over
the latest baseline. Additionally, we present real-world experiments in the
supplementary video, demonstrating its real-time performance.

</details>


### [9] [Generating Stable Placements via Physics-guided Diffusion Models](https://arxiv.org/abs/2509.21664)
*Philippe Nadeau,Miguel Rogel,Ivan Bilić,Ivan Petrović,Jonathan Kelly*

Main category: cs.RO

TL;DR: 提出一种将稳定性直接集成到扩散模型采样过程中的方法，通过训练扩散模型生成稳定的物体放置位置，无需额外训练即可提高放置稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决多物体场景中稳定放置物体的挑战，现有方法依赖模拟引擎或启发式外观评估，需要更高效且准确的稳定性评估方法。

Method: 使用离线基于采样的规划器收集多模态放置标签，训练条件扩散模型生成稳定放置，利用基于分数的生成模型组合性结合稳定性感知损失。

Result: 在四个基准场景中，物理引导模型实现的放置对强力扰动的鲁棒性提高56%，运行时间比最先进几何方法减少47%。

Conclusion: 该方法成功将物理稳定性集成到扩散模型中，无需额外训练即可显著提高放置稳定性和效率。

Abstract: Stably placing an object in a multi-object scene is a fundamental challenge
in robotic manipulation, as placements must be penetration-free, establish
precise surface contact, and result in a force equilibrium. To assess
stability, existing methods rely on running a simulation engine or resort to
heuristic, appearance-based assessments. In contrast, our approach integrates
stability directly into the sampling process of a diffusion model. To this end,
we query an offline sampling-based planner to gather multi-modal placement
labels and train a diffusion model to generate stable placements. The diffusion
model is conditioned on scene and object point clouds, and serves as a
geometry-aware prior. We leverage the compositional nature of score-based
generative models to combine this learned prior with a stability-aware loss,
thereby increasing the likelihood of sampling from regions of high stability.
Importantly, this strategy requires no additional re-training or fine-tuning,
and can be directly applied to off-the-shelf models. We evaluate our method on
four benchmark scenes where stability can be accurately computed. Our
physics-guided models achieve placements that are 56% more robust to forceful
perturbations while reducing runtime by 47% compared to a state-of-the-art
geometric method.

</details>


### [10] [Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation](https://arxiv.org/abs/2509.21690)
*Muqun Hu,Wenxi Chen,Wenjing Li,Falak Mandali,Zijian He,Renhong Zhang,Praveen Krisna,Katherine Christian,Leo Benaharon,Dizhi Ma,Karthik Ramani,Yan Gu*

Main category: cs.RO

TL;DR: 提出一个强化学习框架，通过预测信号和物理引导的密集奖励，将乒乓球位置观察直接映射到全身关节命令，实现人形机器人的端到端乒乓球控制。


<details>
  <summary>Details</summary>
Motivation: 人形机器人乒乓球需要快速感知、主动全身运动和敏捷步法，这些能力对于统一控制器来说仍然难以实现。

Method: 使用强化学习框架，结合轻量级学习预测器和基于物理的预测器，前者增强策略观察用于主动决策，后者在训练时提供精确未来状态以构建密集奖励。

Result: 在模拟中达到强性能（命中率≥96%，成功率≥92%），在物理机器人上零样本部署产生协调步法和准确快速回击。

Conclusion: 该框架为人形机器人乒乓球提供了一条实用的端到端学习路径，学习预测器和预测奖励设计对性能至关重要。

Abstract: Humanoid table tennis (TT) demands rapid perception, proactive whole-body
motion, and agile footwork under strict timing -- capabilities that remain
difficult for unified controllers. We propose a reinforcement learning
framework that maps ball-position observations directly to whole-body joint
commands for both arm striking and leg locomotion, strengthened by predictive
signals and dense, physics-guided rewards. A lightweight learned predictor, fed
with recent ball positions, estimates future ball states and augments the
policy's observations for proactive decision-making. During training, a
physics-based predictor supplies precise future states to construct dense,
informative rewards that lead to effective exploration. The resulting policy
attains strong performance across varied serve ranges (hit rate $\geq$ 96% and
success rate $\geq$ 92%) in simulations. Ablation studies confirm that both the
learned predictor and the predictive reward design are critical for end-to-end
learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute
joints, the policy produces coordinated lateral and forward-backward footwork
with accurate, fast returns, suggesting a practical path toward versatile,
competitive humanoid TT.

</details>


### [11] [VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation](https://arxiv.org/abs/2509.21723)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: VLBiMan是一个从单个人类演示中学习可重用技能的双臂操作框架，通过任务感知分解和视觉语言基础实现动态适应，无需策略重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法面临的困境：模仿策略学习需要大量演示来覆盖任务变化，而模块化方法在动态场景中缺乏灵活性。

Method: 通过任务感知分解从单个人类示例中提取可重用技能，保留不变基元作为锚点，通过视觉语言基础动态调整可调整组件，利用语义解析和几何可行性约束。

Result: 在工具使用和多对象任务上的广泛实验验证了：(1)相比模仿基线大幅减少演示需求，(2)通过原子技能拼接实现组合泛化，(3)对新颖但语义相似对象和外部干扰具有鲁棒性，(4)强大的跨具身迁移能力。

Conclusion: 通过将人类先验与视觉语言锚定适应相结合，该工作朝着在非结构化环境中实现实用且通用的双臂操作迈出了一步。

Abstract: Achieving generalizable bimanual manipulation requires systems that can learn
efficiently from minimal human input while adapting to real-world uncertainties
and diverse embodiments. Existing approaches face a dilemma: imitation policy
learning demands extensive demonstrations to cover task variations, while
modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,
a framework that derives reusable skills from a single human example through
task-aware decomposition, preserving invariant primitives as anchors while
dynamically adapting adjustable components via vision-language grounding. This
adaptation mechanism resolves scene ambiguities caused by background changes,
object repositioning, or visual clutter without policy retraining, leveraging
semantic parsing and geometric feasibility constraints. Moreover, the system
inherits human-like hybrid control capabilities, enabling mixed synchronous and
asynchronous use of both arms. Extensive experiments validate VLBiMan across
tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in
demonstration requirements compared to imitation baselines, (2) compositional
generalization through atomic skill splicing for long-horizon tasks, (3)
robustness to novel but semantically similar objects and external disturbances,
and (4) strong cross-embodiment transfer, showing that skills learned from
human demonstrations can be instantiated on different robotic platforms without
retraining. By bridging human priors with vision-language anchored adaptation,
our work takes a step toward practical and versatile dual-arm manipulation in
unstructured settings.

</details>


### [12] [The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions](https://arxiv.org/abs/2509.21776)
*Hyeonseong Kim,Roy El-Helou,Seungbeen Lee,Sungjoon Choi,Matthew Pan*

Main category: cs.RO

TL;DR: 本研究探索了游戏性欺骗在人机交互中的作用，通过模仿土耳其冰淇淋商贩的互动方式，发现有限的欺骗能增强用户的愉悦感和参与度，但会降低信任和安全感。


<details>
  <summary>Details</summary>
Motivation: 游戏性欺骗在人类社交中很常见，但在人机交互中研究不足。受土耳其冰淇淋商贩互动方式的启发，研究有界、文化熟悉的欺骗形式如何影响用户信任、愉悦和参与度。

Method: 设计了一个配备定制末端执行器的机器人操纵器，实现了五种土耳其冰淇淋风格的欺骗策略，延迟递送冰淇淋形状物体。通过91名参与者的混合设计用户研究，评估游戏性欺骗和互动时长对用户体验的影响。

Result: 土耳其冰淇淋风格的欺骗显著增强了愉悦感和参与度，但降低了感知安全性和信任，表明在多维方面存在结构化的权衡。

Conclusion: 游戏性欺骗可以成为娱乐和参与导向的交互机器人设计中的有价值策略，但需要仔细考虑其复杂的权衡关系。

Abstract: Playful deception, a common feature in human social interactions, remains
underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice
Cream (TIC) vendor routine, we investigate how bounded, culturally familiar
forms of deception influence user trust, enjoyment, and engagement during
robotic handovers. We design a robotic manipulator equipped with a custom
end-effector and implement five TIC-inspired trick policies that deceptively
delay the handover of an ice cream-shaped object. Through a mixed-design user
study with 91 participants, we evaluate the effects of playful deception and
interaction duration on user experience. Results reveal that TIC-inspired
deception significantly enhances enjoyment and engagement, though reduces
perceived safety and trust, suggesting a structured trade-off across the
multi-dimensional aspects. Our findings demonstrate that playful deception can
be a valuable design strategy for interactive robots in entertainment and
engagement-focused contexts, while underscoring the importance of deliberate
consideration of its complex trade-offs. You can find more information,
including demonstration videos, on
https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .

</details>


### [13] [Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors](https://arxiv.org/abs/2509.21810)
*Ning Huang,Zhentao Xie,Qinchuan Li*

Main category: cs.RO

TL;DR: 提出基于条件对抗运动先验的多技能学习框架，使四足机器人能从专家演示中高效学习多种运动技能，实现精确技能重建和平滑转换。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以通过单一策略学习多种运动技能，且缺乏平滑的技能转换能力，限制了腿式机器人在复杂环境中的敏捷导航。

Method: 使用条件对抗运动先验框架，结合新型技能判别器和技能条件奖励设计，实现精确技能重建。

Result: 框架支持多种技能的主动控制和重用，为复杂环境中学习泛化策略提供了实用解决方案。

Conclusion: 该多技能学习框架能有效解决腿式机器人获取多样化运动技能的挑战，实现平滑技能转换和通用策略学习。

Abstract: Despite growing interest in developing legged robots that emulate biological
locomotion for agile navigation of complex environments, acquiring a diverse
repertoire of skills remains a fundamental challenge in robotics. Existing
methods can learn motion behaviors from expert data, but they often fail to
acquire multiple locomotion skills through a single policy and lack smooth
skill transitions. We propose a multi-skill learning framework based on
Conditional Adversarial Motion Priors (CAMP), with the aim of enabling
quadruped robots to efficiently acquire a diverse set of locomotion skills from
expert demonstrations. Precise skill reconstruction is achieved through a novel
skill discriminator and skill-conditioned reward design. The overall framework
supports the active control and reuse of multiple skills, providing a practical
solution for learning generalizable policies in complex environments.

</details>


### [14] [Improved Vehicle Maneuver Prediction using Game Theoretic Priors](https://arxiv.org/abs/2509.21873)
*Nishant Doshi*

Main category: cs.RO

TL;DR: 提出了一种结合博弈论和传统运动分类模型的车辆机动预测方法，通过Level-k博弈理论模拟车辆间交互，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于轨迹数据的分类模型在预测车道变换时不够准确，需要整合整个场景信息。博弈论能够模拟人类层次化推理，为群体中的每个智能体找到最理性的决策。

Method: 使用Level-k博弈理论建模车辆间交互，将博弈论评估结果作为先验或与传统运动分类模型结合。假设目标车辆周围车辆状态已知，通过在线优化求解目标车辆的最理性机动预测。

Result: 该方法能够更准确地预测车辆机动行为，特别是车道变换等复杂行为。

Conclusion: 结合博弈论的方法显著提高了车辆机动预测的准确性，为自适应巡航控制等决策系统提供更好的支持，进一步提升燃油节省效果。

Abstract: Conventional maneuver prediction methods use some sort of classification
model on temporal trajectory data to predict behavior of agents over a set time
horizon. Despite of having the best precision and recall, these models cannot
predict a lane change accurately unless they incorporate information about the
entire scene. Level-k game theory can leverage the human-like hierarchical
reasoning to come up with the most rational decisions each agent can make in a
group. This can be leveraged to model interactions between different vehicles
in presence of each other and hence compute the most rational decisions each
agent would make. The result of game theoretic evaluation can be used as a
"prior" or combined with a traditional motion-based classification model to
achieve more accurate predictions. The proposed approach assumes that the
states of the vehicles around the target lead vehicle are known. The module
will output the most rational maneuver prediction of the target vehicle based
on an online optimization solution. These predictions are instrumental in
decision making systems like Adaptive Cruise Control (ACC) or Traxen's
iQ-Cruise further improving the resulting fuel savings.

</details>


### [15] [WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces](https://arxiv.org/abs/2509.21878)
*Moses Gladson Selvamuthu,Tomoya Takahashi,Riichiro Tadakuma,Kazutoshi Tanaka*

Main category: cs.RO

TL;DR: WAVE是一种基于蜗轮的非反向驱动可变刚度执行器，通过弹性元件实现力传递和位置容差，能够连续调节关节刚度并保护系统免受过大负载。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时调节柔顺性和刚度的机械臂执行器，以提高操作安全性和多功能性，特别是在接触密集型任务和挑战性环境中。

Method: 集成非反向驱动蜗轮，将驱动电机与外部力解耦，通过改变弹簧预压缩长度实现连续刚度调节，并将冲击力转化为弹簧存储的弹性能量。

Result: 实验验证了刚度模型，显示在静止状态下电机负载接近零（即使有外部加载），成功实现了外部力的解耦，并展示了在机械臂上的应用。

Conclusion: WAVE执行器成功实现了外部力的解耦，其保护特性使得在接触密集型任务中能够延长操作时间，为挑战性环境中的机器人应用提供了鲁棒性。

Abstract: Robotic manipulators capable of regulating both compliance and stiffness
offer enhanced operational safety and versatility. Here, we introduce Worm
Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator
(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving
motor from external forces using this gear, WAVE enables precise force
transmission to the joint, while absorbing positional discrepancies through
compliance. WAVE is protected from excessive loads by converting impact forces
into elastic energy stored in a spring. In addition, the actuator achieves
continuous joint stiffness modulation by changing the spring's precompression
length. We demonstrate these capabilities, experimentally validate the proposed
stiffness model, show that motor loads approach zero at rest--even under
external loading--and present applications using a manipulator with WAVE. This
outcome showcases the successful decoupling of external forces. The protective
attributes of this actuator allow for extended operation in contact-intensive
tasks, and for robust robotic applications in challenging environments.

</details>


### [16] [SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks](https://arxiv.org/abs/2509.21928)
*Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian*

Main category: cs.RO

TL;DR: SAGE是一个用于长时程操作任务的场景图感知引导与执行框架，通过语义场景图连接高层任务规划和低层视觉运动控制，实现鲁棒的长时程任务规划和目标条件操作。


<details>
  <summary>Details</summary>
Motivation: 解决长时程操作任务中的挑战，这些任务涉及扩展动作序列和复杂物体交互，现有方法在泛化能力和语义推理方面存在局限，需要桥接符号规划和连续控制之间的鸿沟。

Method: 使用语义场景图作为场景状态的结构化表示，包含两个关键组件：基于场景图的任务规划器（使用VLM和LLM解析环境和推理场景状态转换序列），以及解耦的结构化图像编辑管道（通过图像修复和合成将目标子目标图转换为对应图像）。

Result: 大量实验表明，SAGE在不同长时程任务上实现了最先进的性能。

Conclusion: SAGE框架通过语义场景图有效桥接了任务级语义推理和像素级视觉运动控制，为长时程操作任务提供了有效的解决方案。

Abstract: Successfully solving long-horizon manipulation tasks remains a fundamental
challenge. These tasks involve extended action sequences and complex object
interactions, presenting a critical gap between high-level symbolic planning
and low-level continuous control. To bridge this gap, two essential
capabilities are required: robust long-horizon task planning and effective
goal-conditioned manipulation. Existing task planning methods, including
traditional and LLM-based approaches, often exhibit limited generalization or
sparse semantic reasoning. Meanwhile, image-conditioned control methods
struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a
novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon
Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural
representation for scene states. A structural scene graph enables bridging
task-level semantic reasoning and pixel-level visuo-motor control. This also
facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE
consists of two key components: (1) a scene graph-based task planner that uses
VLMs and LLMs to parse the environment and reason about physically-grounded
scene state transition sequences, and (2) a decoupled structural image editing
pipeline that controllably converts each target sub-goal graph into a
corresponding image through image inpainting and composition. Extensive
experiments have demonstrated that SAGE achieves state-of-the-art performance
on distinct long-horizon tasks.

</details>


### [17] [Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception](https://arxiv.org/abs/2509.21955)
*Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.RO

TL;DR: 提出可学习一致性预测（LCP），通过轻量级神经网络生成上下文感知的不确定性集合，在保持理论保证的同时显著减小预测集大小并提高机器人任务的安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在机器人学中输出点估计且置信度校准不佳，缺乏量化预测可靠性的机制。传统一致性预测依赖固定非一致性分数，忽略上下文信息，导致区间过于保守或不安全。

Method: 用轻量级神经网络函数替代固定非一致性分数，利用几何、语义和任务特定特征生成上下文感知的不确定性集合，保持一致性预测的理论保证。

Result: 在分类任务中预测集大小减少18%，检测区间缩小52%，路径规划安全性从72%提升至91%。在7个基准测试的3个机器人任务中均优于标准CP和集成方法。

Conclusion: LCP是轻量级方法（4.8%运行时开销，42KB内存），支持在线适应，适合资源受限的自主系统，比集成方法能效高7.4倍。

Abstract: Deep learning models in robotics often output point estimates with poorly
calibrated confidences, offering no native mechanism to quantify predictive
reliability under novel, noisy, or out-of-distribution inputs. Conformal
prediction (CP) addresses this gap by providing distribution-free coverage
guarantees, yet its reliance on fixed nonconformity scores ignores context and
can yield intervals that are overly conservative or unsafe. We address this
with Learnable Conformal Prediction (LCP), which replaces fixed scores with a
lightweight neural function that leverages geometric, semantic, and
task-specific features to produce context-aware uncertainty sets.
  LCP maintains CP's theoretical guarantees while reducing prediction set sizes
by 18% in classification, tightening detection intervals by 52%, and improving
path planning safety from 72% to 91% success with minimal overhead. Across
three robotic tasks on seven benchmarks, LCP consistently outperforms Standard
CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it
achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object
detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding
boxes. In path planning through cluttered environments, it improves success to
91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.
  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)
and supports online adaptation, making it well suited to resource-constrained
autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and
15.9% inference overhead, yet sustains 39 FPS on detection tasks while being
7.4 times more energy-efficient than ensembles.

</details>


### [18] [FlowDrive: moderated flow matching with data balancing for trajectory planning](https://arxiv.org/abs/2509.21961)
*Lingguang Wang,Ömer Şahin Taş,Marlon Steiner,Christoph Stiller*

Main category: cs.RO

TL;DR: FlowDrive是一个基于流匹配的轨迹规划器，通过条件整流流将噪声直接映射到轨迹分布，并使用调节引导策略增加轨迹多样性，在nuPlan和interPlan基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于学习的规划器对驾驶数据长尾分布的敏感性问题，常见操作主导数据集，而危险或罕见场景稀疏，导致模型偏向频繁情况，在关键场景上性能下降。

Method: 提出FlowDrive，一个流匹配轨迹规划器，学习条件整流流以少量流匹配步骤将噪声映射到轨迹分布；引入调节的循环内引导，在流步骤之间注入小扰动来系统性地增加轨迹多样性，同时保持场景一致性。

Result: 在nuPlan和交互导向的interPlan基准测试中，FlowDrive在基于学习的规划器中达到最先进结果，接近具有基于规则精化的方法。添加调节引导和轻量后处理后，在几乎所有基准分割中都达到整体最先进性能。

Conclusion: FlowDrive通过流匹配和调节引导有效解决了数据不平衡问题，在轨迹规划任务中实现了优异的性能，特别是在处理长尾分布和增加轨迹多样性方面表现出色。

Abstract: Learning-based planners are sensitive to the long-tailed distribution of
driving data. Common maneuvers dominate datasets, while dangerous or rare
scenarios are sparse. This imbalance can bias models toward the frequent cases
and degrade performance on critical scenarios. To tackle this problem, we
compare balancing strategies for sampling training data and find reweighting by
trajectory pattern an effective approach. We then present FlowDrive, a
flow-matching trajectory planner that learns a conditional rectified flow to
map noise directly to trajectory distributions with few flow-matching steps. We
further introduce moderated, in-the-loop guidance that injects small
perturbation between flow steps to systematically increase trajectory diversity
while remaining scene-consistent. On nuPlan and the interaction-focused
interPlan benchmarks, FlowDrive achieves state-of-the-art results among
learning-based planners and approaches methods with rule-based refinements.
After adding moderated guidance and light post-processing (FlowDrive*), it
achieves overall state-of-the-art performance across nearly all benchmark
splits.

</details>


### [19] [Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning](https://arxiv.org/abs/2509.21983)
*Sigmund Hennum Høeg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du*

Main category: cs.RO

TL;DR: 提出了一种结合离散符号规划和连续轨迹生成的混合扩散方法，解决长视野任务中生成模型容易混淆行为模式的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的机器人轨迹生成方法在复杂决策的长视野任务中表现不佳，容易混淆不同的行为模式导致失败。

Method: 通过同时生成高层符号规划和连续轨迹，采用离散变量扩散和连续扩散的混合方法进行轨迹合成。

Result: 该方法显著优于基线方法，并支持基于部分或完整符号条件的灵活轨迹合成。

Conclusion: 混合扩散过程能有效解决长视野任务中的复杂决策问题，提升机器人任务完成能力。

Abstract: Constructing robots to accomplish long-horizon tasks is a long-standing
challenge within artificial intelligence. Approaches using generative methods,
particularly Diffusion Models, have gained attention due to their ability to
model continuous robotic trajectories for planning and control. However, we
show that these models struggle with long-horizon tasks that involve complex
decision-making and, in general, are prone to confusing different modes of
behavior, leading to failure. To remedy this, we propose to augment continuous
trajectory generation by simultaneously generating a high-level symbolic plan.
We show that this requires a novel mix of discrete variable diffusion and
continuous diffusion, which dramatically outperforms the baselines. In
addition, we illustrate how this hybrid diffusion process enables flexible
trajectory synthesis, allowing us to condition synthesized actions on partial
and complete symbolic conditions.

</details>


### [20] [Developing Vision-Language-Action Model from Egocentric Videos](https://arxiv.org/abs/2509.21986)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.RO

TL;DR: 该研究利用EgoScaler框架从原始第一人称视频中提取6DoF物体操作轨迹，构建大规模数据集用于VLA预训练，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 传统机器人训练依赖昂贵的专家远程操作，而第一人称视频提供了可扩展的替代方案，但之前的研究需要辅助标注。本研究探索能否直接从原始第一人称视频训练VLA模型。

Method: 使用EgoScaler框架从四个大规模第一人称视频数据集中提取6DoF物体操作轨迹，自动精炼噪声或不完整的轨迹，构建新的VLA预训练数据集。

Result: 实验表明：预训练使任务成功率提升20%以上；性能与使用真实机器人数据集相当；结合真实机器人数据可进一步提升性能。

Conclusion: 第一人称视频是推进VLA研究的有前景且可扩展的资源。

Abstract: Egocentric videos capture how humans manipulate objects and tools, providing
diverse motion cues for learning object manipulation. Unlike the costly,
expert-driven manual teleoperation commonly used in training
Vision-Language-Action models (VLAs), egocentric videos offer a scalable
alternative. However, prior studies that leverage such videos for training
robot policies typically rely on auxiliary annotations, such as detailed
hand-pose recordings. Consequently, it remains unclear whether VLAs can be
trained directly from raw egocentric videos. In this work, we address this
challenge by leveraging EgoScaler, a framework that extracts 6DoF object
manipulation trajectories from egocentric videos without requiring auxiliary
recordings. We apply EgoScaler to four large-scale egocentric video datasets
and automatically refine noisy or incomplete trajectories, thereby constructing
a new large-scale dataset for VLA pre-training. Our experiments with a
state-of-the-art $\pi_0$ architecture in both simulated and real-robot
environments yield three key findings: (i) pre-training on our dataset improves
task success rates by over 20\% compared to training from scratch, (ii) the
performance is competitive with that achieved using real-robot datasets, and
(iii) combining our dataset with real-robot data yields further improvements.
These results demonstrate that egocentric videos constitute a promising and
scalable resource for advancing VLA research.

</details>


### [21] [One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion](https://arxiv.org/abs/2509.22002)
*Yuping Gu,Bangchao Huang,Haoran Sun,Ronghan Xu,Jiayi Yin,Wei Zhang,Fang Wan,Jia Pan,Chaoyang Song*

Main category: cs.RO

TL;DR: 提出了一种计算设计方法，用于设计单自由度过约束机器人肢体，实现期望空间轨迹、能量高效且全周期旋转无自碰撞的运动。


<details>
  <summary>Details</summary>
Motivation: 单自由度设计具有简单性、鲁棒性、成本效益和效率等优势，但通常在全周期运动范围内受到自碰撞的限制。过约束机构能够为单自由度系统引入运动多样性。

Method: 首先提出连杆式机器人肢体的几何优化问题，然后通过优化相似性和动态相关指标来制定空间轨迹生成问题，进一步优化过约束连杆的几何形状以确保平滑无碰撞运动。

Result: 通过个性化自动机和仿生六足机器人等实验验证了该方法，所得到的六足机器人表现出卓越的前进行走能量效率。

Conclusion: 该方法成功实现了单自由度过约束机器人肢体的设计，能够在全周期旋转中实现能量高效、无自碰撞的运动。

Abstract: While it is expected to build robotic limbs with multiple degrees of freedom
(DoF) inspired by nature, a single DoF design remains fundamental, providing
benefits that include, but are not limited to, simplicity, robustness,
cost-effectiveness, and efficiency. Mechanisms, especially those with multiple
links and revolute joints connected in closed loops, play an enabling factor in
introducing motion diversity for 1-DoF systems, which are usually constrained
by self-collision during a full-cycle range of motion. This study presents a
novel computational approach to designing one-degree-of-freedom (1-DoF)
overconstrained robotic limbs for a desired spatial trajectory, while achieving
energy-efficient, self-collision-free motion in full-cycle rotations. Firstly,
we present the geometric optimization problem of linkage-based robotic limbs in
a generalized formulation for self-collision-free design. Next, we formulate
the spatial trajectory generation problem with the overconstrained linkages by
optimizing the similarity and dynamic-related metrics. We further optimize the
geometric shape of the overconstrained linkage to ensure smooth and
collision-free motion driven by a single actuator. We validated our proposed
method through various experiments, including personalized automata and
bio-inspired hexapod robots. The resulting hexapod robot, featuring
overconstrained robotic limbs, demonstrated outstanding energy efficiency
during forward walking.

</details>


### [22] [An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose](https://arxiv.org/abs/2509.22058)
*Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu*

Main category: cs.RO

TL;DR: 提出了一种基于自适应ICP的LiDAR里程计方法，通过可靠初始位姿和自适应阈值来提高动态环境下的点云配准精度


<details>
  <summary>Details</summary>
Motivation: 现有ICP方法不考虑初始位姿可靠性，容易陷入局部最优；缺乏自适应机制，在复杂动态环境中配准精度显著下降

Method: 1. 基于密度滤波的分布式粗配准获取初始位姿；2. 通过与运动预测位姿比较选择可靠初始位姿；3. 结合当前和历史误差动态调整自适应阈值；4. 基于可靠初始位姿和自适应阈值进行点对面自适应ICP配准

Result: 在KITTI数据集上的大量实验表明，该方法优于现有方法，显著提高了LiDAR里程计的精度

Conclusion: 提出的自适应ICP LiDAR里程计方法通过可靠初始位姿和自适应阈值机制，有效解决了动态环境下的点云配准问题，提高了定位精度

Abstract: As a key technology for autonomous navigation and positioning in mobile
robots, light detection and ranging (LiDAR) odometry is widely used in
autonomous driving applications. The Iterative Closest Point (ICP)-based
methods have become the core technique in LiDAR odometry due to their efficient
and accurate point cloud registration capability. However, some existing
ICP-based methods do not consider the reliability of the initial pose, which
may cause the method to converge to a local optimum. Furthermore, the absence
of an adaptive mechanism hinders the effective handling of complex dynamic
environments, resulting in a significant degradation of registration accuracy.
To address these issues, this paper proposes an adaptive ICP-based LiDAR
odometry method that relies on a reliable initial pose. First, distributed
coarse registration based on density filtering is employed to obtain the
initial pose estimation. The reliable initial pose is then selected by
comparing it with the motion prediction pose, reducing the initial error
between the source and target point clouds. Subsequently, by combining the
current and historical errors, the adaptive threshold is dynamically adjusted
to accommodate the real-time changes in the dynamic environment. Finally, based
on the reliable initial pose and the adaptive threshold, point-to-plane
adaptive ICP registration is performed from the current frame to the local map,
achieving high-precision alignment of the source and target point clouds.
Extensive experiments on the public KITTI dataset demonstrate that the proposed
method outperforms existing approaches and significantly enhances the accuracy
of LiDAR odometry.

</details>


### [23] [Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot](https://arxiv.org/abs/2509.22065)
*Ethan Fulcher,J. Diego Caporale,Yifeng Zhang,John Ruck,Feifei Qian*

Main category: cs.RO

TL;DR: 本文研究了步态对腿部机器人本体感知地形传感精度的影响，比较了传感导向的爬行步态和运动导向的小跑步态在测量可变形基质强度和纹理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 通过腿部机器人在运动过程中测量地形特性，可以在访问风险较高的地形时提供前所未有的采样速度和密度，这对于行星地质勘探具有重要意义。

Method: 在实验室环境中，让机器人使用Crawl N' Sense（传感导向）和Trot-Walk（运动导向）两种步态在刚性表面、松散沙地和带合成表面结壳的松散沙地上运动，量化测量基质的强度和纹理特性。

Result: 两种步态都能测量低阻力和高阻力基质之间的强度差异，但运动导向的小跑步态测量值幅度和方差更大。较慢的爬行步态检测表面结壳脆性破裂的准确度显著高于较快的小跑步态。

Conclusion: 研究结果为腿部机器人"运动中传感"的步态设计和规划提供了新见解，有助于在其他星球上进行地形侦察和科学测量，推进对其地质和形成过程的理解。

Abstract: In-situ robotic exploration is an important tool for advancing knowledge of
geological processes that describe the Earth and other Planetary bodies. To
inform and enhance operations for these roving laboratories, it is imperative
to understand the terramechanical properties of their environments, especially
for traversing on loose, deformable substrates. Recent research suggested that
legged robots with direct-drive and low-gear ratio actuators can sensitively
detect external forces, and therefore possess the potential to measure terrain
properties with their legs during locomotion, providing unprecedented sampling
speed and density while accessing terrains previously too risky to sample. This
paper explores these ideas by investigating the impact of gait on
proprioceptive terrain sensing accuracy, particularly comparing a
sensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,
Trot-Walk. Each gait's ability to measure the strength and texture of
deformable substrate is quantified as the robot locomotes over a laboratory
transect consisting of a rigid surface, loose sand, and loose sand with
synthetic surface crusts. Our results suggest that with both the
sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can
measure a consistent difference in the strength (in terms of penetration
resistance) between the low- and high-resistance substrates; however, the
locomotion-oriented trot gait contains larger magnitude and variance in
measurements. Furthermore, the slower crawl gait can detect brittle ruptures of
the surface crusts with significantly higher accuracy than the faster trot
gait. Our results offer new insights that inform legged robot "sensing during
locomotion" gait design and planning for scouting the terrain and producing
scientific measurements on other worlds to advance our understanding of their
geology and formation.

</details>


### [24] [Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation](https://arxiv.org/abs/2509.22093)
*Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu*

Main category: cs.RO

TL;DR: 提出ADP框架，通过动作感知的动态剪枝机制，根据机器人操作阶段的不同视觉冗余度自适应调整token保留比例，在保持性能的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化VLA模型推理速度时忽略了机器人操作不同阶段的视觉冗余度差异，粗粒度操作阶段视觉token冗余度高于细粒度操作，且与动作动态性相关。

Method: 结合文本驱动的token选择和动作感知的轨迹门控机制，利用过去动作窗口动态调整token保留比例，平衡计算效率和感知精度。

Result: 在LIBERO套件和真实场景实验中，显著降低FLOPs和动作推理延迟（如OpenVLA-OFT上1.35倍加速），同时保持竞争力成功率（如OpenVLA提升25.8%）。

Conclusion: ADP为高效机器人策略提供了简单插件路径，推进了机器人操作效率和性能的前沿。

Abstract: Robotic manipulation with Vision-Language-Action models requires efficient
inference over long-horizon multi-modal context, where attention to dense
visual tokens dominates computational cost. Existing methods optimize inference
speed by reducing visual redundancy within VLA models, but they overlook the
varying redundancy across robotic manipulation stages. We observe that the
visual token redundancy is higher in coarse manipulation phase than in
fine-grained operations, and is strongly correlated with the action dynamic.
Motivated by this observation, we propose \textbf{A}ction-aware
\textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning
framework that integrates text-driven token selection with action-aware
trajectory gating. Our method introduces a gating mechanism that conditions the
pruning signal on recent action trajectories, using past motion windows to
adaptively adjust token retention ratios in accordance with dynamics, thereby
balancing computational efficiency and perceptual precision across different
manipulation stages. Extensive experiments on the LIBERO suites and diverse
real-world scenarios demonstrate that our method significantly reduces FLOPs
and action inference latency (\textit{e.g.} $1.35 \times$ speed up on
OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\%
improvements with OpenVLA) compared to baselines, thereby providing a simple
plug-in path to efficient robot policies that advances the efficiency and
performance frontier of robotic manipulation. Our project website is:
\href{https://vla-adp.github.io/}{ADP.com}.

</details>


### [25] [Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot](https://arxiv.org/abs/2509.22120)
*Alireza Aliyari,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: 提出了一种鲁棒非线性模型预测控制（RNMPC）方法，称为多阶段NMPC，用于控制二自由度外骨骼机器人，通过解决非线性优化问题来处理系统不确定性。


<details>
  <summary>Details</summary>
Motivation: 由于肌肉骨骼损伤增加，外骨骼机器人的使用日益增多，但其有效性严重依赖于控制系统设计。现有线性化方法在处理机器人动力学非线性时会降低性能，需要更好的鲁棒控制方法。

Method: 采用多阶段非线性模型预测控制（multi-stage NMPC），使用多个场景来表示系统不确定性，通过解决非线性优化问题来控制二自由度外骨骼。

Result: 仿真和实验测试表明，该方法显著提高了鲁棒性，优于非鲁棒NMPC。在2kg未知负载和外部干扰下，大腿和小腿交互力的RMS值分别比非鲁棒NMPC降低了77%和94%。

Conclusion: 多阶段NMPC方法能够有效处理外骨骼机器人中的不确定性，显著降低跟踪误差和交互力，提高了系统的鲁棒性能。

Abstract: The use of exoskeleton robots is increasing due to the rising number of
musculoskeletal injuries. However, their effectiveness depends heavily on the
design of control systems. Designing robust controllers is challenging because
of uncertainties in human-robot systems. Among various control strategies,
Model Predictive Control (MPC) is a powerful approach due to its ability to
handle constraints and optimize performance. Previous studies have used
linearization-based methods to implement robust MPC on exoskeletons, but these
can degrade performance due to nonlinearities in the robot's dynamics. To
address this gap, this paper proposes a Robust Nonlinear Model Predictive
Control (RNMPC) method, called multi-stage NMPC, to control a
two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.
This method uses multiple scenarios to represent system uncertainties. The
study focuses on minimizing human-robot interaction forces during the swing
phase, particularly when the robot carries unknown loads. Simulations and
experimental tests show that the proposed method significantly improves
robustness, outperforming non-robust NMPC. It achieves lower tracking errors
and interaction forces under various uncertainties. For instance, when a 2 kg
unknown payload is combined with external disturbances, the RMS values of thigh
and shank interaction forces for multi-stage NMPC are reduced by 77 and 94
percent, respectively, compared to non-robust NMPC.

</details>


### [26] [DemoGrasp: Universal Dexterous Grasping from a Single Demonstration](https://arxiv.org/abs/2509.22149)
*Haoqi Yuan,Ziye Huang,Ye Wang,Chuan Mao,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: DemoGrasp是一种简单有效的通用灵巧抓取学习方法，通过编辑单个演示轨迹来适应新物体和姿态，在模拟中达到95%成功率，并在真实世界中成功抓取110个未见物体。


<details>
  <summary>Details</summary>
Motivation: 解决多指灵巧手通用抓取问题，避免传统强化学习方法中复杂奖励和课程设计导致的次优解问题。

Method: 从单个成功演示轨迹出发，通过编辑机器人动作来适应新物体：改变手腕姿态决定抓取位置，改变手部关节角度决定抓取方式，将轨迹编辑建模为单步马尔可夫决策过程。

Result: 在模拟中，使用Shadow Hand在DexGraspNet物体上达到95%成功率；在真实世界中成功抓取110个未见物体，包括小而薄的物品，并能泛化到空间、背景和光照变化。

Conclusion: DemoGrasp提供了一种简单而有效的通用灵巧抓取学习方法，具有强大的迁移能力和泛化性，支持RGB和深度输入，并可扩展到语言引导的杂乱场景抓取。

Abstract: Universal grasping with multi-fingered dexterous hands is a fundamental
challenge in robotic manipulation. While recent approaches successfully learn
closed-loop grasping policies using reinforcement learning (RL), the inherent
difficulty of high-dimensional, long-horizon exploration necessitates complex
reward and curriculum design, often resulting in suboptimal solutions across
diverse objects. We propose DemoGrasp, a simple yet effective method for
learning universal dexterous grasping. We start from a single successful
demonstration trajectory of grasping a specific object and adapt to novel
objects and poses by editing the robot actions in this trajectory: changing the
wrist pose determines where to grasp, and changing the hand joint angles
determines how to grasp. We formulate this trajectory editing as a single-step
Markov Decision Process (MDP) and use RL to optimize a universal policy across
hundreds of objects in parallel in simulation, with a simple reward consisting
of a binary success term and a robot-table collision penalty. In simulation,
DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow
Hand, outperforming previous state-of-the-art methods. It also shows strong
transferability, achieving an average success rate of 84.6% across diverse
dexterous hand embodiments on six unseen object datasets, while being trained
on only 175 objects. Through vision-based imitation learning, our policy
successfully grasps 110 unseen real-world objects, including small, thin items.
It generalizes to spatial, background, and lighting changes, supports both RGB
and depth inputs, and extends to language-guided grasping in cluttered scenes.

</details>


### [27] [DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions](https://arxiv.org/abs/2509.22175)
*Quanzhou Li,Zhonghua Wu,Jingbo Wang,Chen Change Loy,Bo Dai*

Main category: cs.RO

TL;DR: 提出了SymOpt流水线构建大规模双手抓取数据集，并开发了DHAGrasp文本引导的双手抓取生成器，能够为未见对象生成语义一致的双手抓取。


<details>
  <summary>Details</summary>
Motivation: 现有抓取数据集主要关注单手交互且语义部分标注有限，缺乏能够生成尊重对象语义的双手抓取的方法。

Method: 利用现有单手数据集和对象与手的对称性构建大规模双手抓取数据集；提出新颖的双手可操作性表示和两阶段设计，能够从少量分割训练对象有效学习并扩展到大量未分割数据。

Result: 实验表明该方法能生成多样且语义一致的抓取，在抓取质量和未见对象泛化能力方面优于强基线方法。

Conclusion: 提出的方法成功解决了双手抓取生成中的数据集稀缺和语义一致性挑战，为稳健的手-对象交互提供了有效解决方案。

Abstract: Learning to generate dual-hand grasps that respect object semantics is
essential for robust hand-object interaction but remains largely underexplored
due to dataset scarcity. Existing grasp datasets predominantly focus on
single-hand interactions and contain only limited semantic part annotations. To
address these challenges, we introduce a pipeline, SymOpt, that constructs a
large-scale dual-hand grasp dataset by leveraging existing single-hand datasets
and exploiting object and hand symmetries. Building on this, we propose a
text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand
Affordance-aware Grasps for unseen objects. Our approach incorporates a novel
dual-hand affordance representation and follows a two-stage design, which
enables effective learning from a small set of segmented training objects while
scaling to a much larger pool of unsegmented data. Extensive experiments
demonstrate that our method produces diverse and semantically consistent
grasps, outperforming strong baselines in both grasp quality and generalization
to unseen objects. The project page is at
https://quanzhou-li.github.io/DHAGrasp/.

</details>


### [28] [Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting](https://arxiv.org/abs/2509.22195)
*Asher J. Hancock,Xindi Wu,Lihan Zha,Olga Russakovsky,Anirudha Majumdar*

Main category: cs.RO

TL;DR: VLM2VLA是一种新的视觉语言动作模型训练范式，通过用自然语言表示低级动作来对齐数据分布，仅使用LoRA微调避免灾难性遗忘，保持VLM核心能力的同时实现机器人任务泛化。


<details>
  <summary>Details</summary>
Motivation: 传统方法在视觉语言模型上微调机器人遥操作数据时，学习动作会削弱模型的基础推理和多模态理解能力，导致泛化能力下降。这种灾难性遗忘源于VLM预训练数据与机器人微调数据之间的分布不匹配。

Method: 提出VLM2VLA训练范式：1）用自然语言表示低级动作解决数据分布不匹配；2）仅使用LoRA进行微调，最小化修改VLM主干网络；3）避免在互联网规模VLM数据集上进行昂贵的共同训练。

Result: 通过广泛的视觉问答研究和800多次真实机器人实验证明，VLM2VLA能够保持VLM的核心能力，实现零样本泛化到需要开放世界语义推理和多语言指令跟随的新任务。

Conclusion: VLM2VLA通过数据级对齐和最小化架构修改，成功解决了VLA训练中的灾难性遗忘问题，为训练通用机器人策略提供了有效解决方案。

Abstract: Fine-tuning vision-language models (VLMs) on robot teleoperation data to
create vision-language-action (VLA) models is a promising paradigm for training
generalist policies, but it suffers from a fundamental tradeoff: learning to
produce actions often diminishes the VLM's foundational reasoning and
multimodal understanding, hindering generalization to novel scenarios,
instruction following, and semantic understanding. We argue that this
catastrophic forgetting is due to a distribution mismatch between the VLM's
internet-scale pretraining corpus and the robotics fine-tuning data. Inspired
by this observation, we introduce VLM2VLA: a VLA training paradigm that first
resolves this mismatch at the data level by representing low-level actions with
natural language. This alignment makes it possible to train VLAs solely with
Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and
averting catastrophic forgetting. As a result, the VLM can be fine-tuned on
robot teleoperation data without fundamentally altering the underlying
architecture and without expensive co-training on internet-scale VLM datasets.
Through extensive Visual Question Answering (VQA) studies and over 800
real-world robotics experiments, we demonstrate that VLM2VLA preserves the
VLM's core capabilities, enabling zero-shot generalization to novel tasks that
require open-world semantic reasoning and multilingual instruction following.

</details>


### [29] [MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training](https://arxiv.org/abs/2509.22199)
*Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang*

Main category: cs.RO

TL;DR: MimicDreamer框架通过视觉、视角和动作三方面的对齐，将低成本的人类演示视频转化为机器人可用的监督数据，显著提升VLA模型在真实机器人上的表现。


<details>
  <summary>Details</summary>
Motivation: 收集机器人交互数据成本高昂，而人类演示视频更易获取。但人类视频与机器人执行视频存在显著的领域差距，包括不稳定的相机视角、人类手与机械臂的视觉差异以及运动动态差异。

Method: 提出MimicDreamer框架：1) H2R Aligner视频扩散模型实现视觉对齐；2) EgoStabilizer通过单应性变换稳定视角并修复遮挡；3) 将人手轨迹映射到机器人坐标系，使用约束逆运动学求解器生成可行的关节命令。

Result: 仅使用合成的人类到机器人视频训练的VLA模型能在真实机器人上实现少样本执行。与仅使用真实机器人数据训练的模型相比，在六个代表性操作任务上平均成功率提升14.7%。

Conclusion: 该方法成功弥合了人类演示与机器人执行之间的领域差距，证明了利用可扩展的人类数据训练VLA模型的有效性，显著提升了机器人的操作性能。

Abstract: Vision Language Action (VLA) models derive their generalization capability
from diverse training data, yet collecting embodied robot interaction data
remains prohibitively expensive. In contrast, human demonstration videos are
far more scalable and cost-efficient to collect, and recent studies confirm
their effectiveness in training VLA models. However, a significant domain gap
persists between human videos and robot-executed videos, including unstable
camera viewpoints, visual discrepancies between human hands and robotic arms,
and differences in motion dynamics. To bridge this gap, we propose
MimicDreamer, a framework that turns fast, low-cost human demonstrations into
robot-usable supervision by jointly aligning vision, viewpoint, and actions to
directly support policy training. For visual alignment, we propose H2R Aligner,
a video diffusion model that generates high-fidelity robot demonstration videos
by transferring motion from human manipulation footage. For viewpoint
stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos
via homography and inpaints occlusions and distortions caused by warping. For
action alignment, we map human hand trajectories to the robot frame and apply a
constrained inverse kinematics solver to produce feasible, low-jitter joint
commands with accurate pose tracking. Empirically, VLA models trained purely on
our synthesized human-to-robot videos achieve few-shot execution on real
robots. Moreover, scaling training with human data significantly boosts
performance compared to models trained solely on real robot data; our approach
improves the average success rate by 14.7\% across six representative
manipulation tasks.

</details>


### [30] [From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment](https://arxiv.org/abs/2509.22205)
*Ke Ye,Jiaming Zhou,Yuanfeng Qiu,Jiayi Liu,Shihui Zhou,Kun-Yu Lin,Junwei Liang*

Main category: cs.RO

TL;DR: Super-Mimic是一个分层框架，通过从无脚本的人类演示视频中直接推断程序意图，实现零样本机器人模仿。它包含人类意图翻译器和未来动态预测器两个模块，在长时程操作任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器人零样本设置下长时程操作任务的泛化挑战，当前多模态基础方法仅从静态视觉输入无法将高级命令分解为可执行动作序列。

Method: 分层框架：1) 人类意图翻译器(HIT)使用多模态推理解析输入视频，生成语言基础子任务序列；2) 未来动态预测器(FDP)使用生成模型为每个步骤合成物理合理的视频展开，生成动态感知的视觉轨迹。

Result: 在长时程操作任务套件上的广泛实验表明，Super-Mimic显著优于最先进的零样本方法超过20%。

Conclusion: 将视频驱动的意图解析与前瞻性动态建模相结合，是开发通用机器人系统的高度有效策略。

Abstract: Generalizing to long-horizon manipulation tasks in a zero-shot setting
remains a central challenge in robotics. Current multimodal foundation based
approaches, despite their capabilities, typically fail to decompose high-level
commands into executable action sequences from static visual input alone. To
address this challenge, we introduce Super-Mimic, a hierarchical framework that
enables zero-shot robotic imitation by directly inferring procedural intent
from unscripted human demonstration videos. Our framework is composed of two
sequential modules. First, a Human Intent Translator (HIT) parses the input
video using multimodal reasoning to produce a sequence of language-grounded
subtasks. These subtasks then condition a Future Dynamics Predictor (FDP),
which employs a generative model that synthesizes a physically plausible video
rollout for each step. The resulting visual trajectories are dynamics-aware,
explicitly modeling crucial object interactions and contact points to guide the
low-level controller. We validate this approach through extensive experiments
on a suite of long-horizon manipulation tasks, where Super-Mimic significantly
outperforms state-of-the-art zero-shot methods by over 20\%. These results
establish that coupling video-driven intent parsing with prospective dynamics
modeling is a highly effective strategy for developing general-purpose robotic
systems.

</details>


### [31] [Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities](https://arxiv.org/abs/2509.22287)
*Stina Sundstedt,Mattias Wingren,Susanne Hägglund,Daniel Ventus*

Main category: cs.RO

TL;DR: 开发了使用Furhat对话机器人和LLM的机器人辅助语言学习应用，通过游戏化方式帮助有语言障碍的学龄前儿童提高表达能力，特别是针对特定形态结构的学习。


<details>
  <summary>Details</summary>
Motivation: 帮助有语言障碍的学龄前儿童（如发育性语言障碍或移民相关语言挑战）提高表达性语言技能，减轻教育工作者和家长在游戏化教学中的负担。

Method: 使用Furhat对话机器人结合大型语言模型(LLM)，在单词检索游戏"Alias"中与儿童互动，管理游戏流程、对话、情感响应和轮转，并计划进一步利用LLM生成和传递特定的形态目标。

Result: 目前开发了基于LLM的游戏应用，能够管理游戏流程和互动，下一步将扩展LLM能力以生成特定形态目标。假设机器人在这方面可能优于人类表现。

Conclusion: 长期目标是创建强大的基于LLM的机器人辅助语言学习干预系统，能够跨不同语言教授各种形态结构，机器人可成为儿童和专业人士的榜样和导师。

Abstract: Preschool children with language vulnerabilities -- such as developmental
language disorders or immigration related language challenges -- often require
support to strengthen their expressive language skills. Based on the principle
of implicit learning, speech-language therapists (SLTs) typically embed target
morphological structures (e.g., third person -s) into everyday interactions or
game-based learning activities. Educators are recommended by SLTs to do the
same. This approach demands precise linguistic knowledge and real-time
production of various morphological forms (e.g., "Daddy wears these when he
drives to work"). The task becomes even more demanding when educators or parent
also must keep children engaged and manage turn-taking in a game-based
activity. In the TalBot project our multiprofessional team have developed an
application in which the Furhat conversational robot plays the word retrieval
game "Alias" with children to improve language skills. Our application
currently employs a large language model (LLM) to manage gameplay, dialogue,
affective responses, and turn-taking. Our next step is to further leverage the
capacity of LLMs so the robot can generate and deliver specific morphological
targets during the game. We hypothesize that a robot could outperform humans at
this task. Novel aspects of this approach are that the robot could ultimately
serve as a model and tutor for both children and professionals and that using
LLM capabilities in this context would support basic communication needs for
children with language vulnerabilities. Our long-term goal is to create a
robust LLM-based Robot-Assisted Language Learning intervention capable of
teaching a variety of morphological structures across different languages.

</details>


### [32] [IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM](https://arxiv.org/abs/2509.22288)
*Johan Hatleskog,Morten Nissov,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出了一种IMU预积分雷达因子方法，通过使用高频惯性数据将最新LiDAR状态传播到雷达测量时间戳，将状态创建率保持在LiDAR测量频率，相比传统方法减少50%节点数量，在保持定位精度的同时将优化时间降低56%。


<details>
  <summary>Details</summary>
Motivation: 传统固定滞后雷达-LiDAR-惯性平滑器为每个测量创建因子图节点，导致状态创建率翻倍，在资源受限硬件上产生高优化成本，阻碍实时性能。

Method: 引入IMU预积分雷达因子，利用高频惯性数据将最新LiDAR状态传播到雷达测量时间戳，保持节点创建率在LiDAR测量频率。

Result: 在单板计算机上实验显示，该方法保持了传统基线的绝对位姿误差，同时将聚合因子图优化时间降低高达56%。

Conclusion: 提出的方法通过减少50%的节点数量，显著降低了计算成本，在资源受限硬件上实现了更好的实时性能，同时保持了定位精度。

Abstract: Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor
graph node per measurement to compensate for the lack of time synchronization
between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this
strategy results in a state creation rate of twice the individual sensor
frequencies. This doubling of the number of states per second yields high
optimization costs, inhibiting real-time performance on resource-constrained
hardware. We introduce IMU-preintegrated radar factors that use high-rate
inertial data to propagate the most recent LiDAR state to the radar measurement
timestamp. This strategy maintains the node creation rate at the LiDAR
measurement frequency. Assuming equal sensor rates, this lowers the number of
nodes by 50 % and consequently the computational costs. Experiments on a single
board computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB
RAM) show that our method preserves the absolute pose error of a conventional
baseline while simultaneously lowering the aggregated factor graph optimization
time by up to 56 %.

</details>


### [33] [Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm](https://arxiv.org/abs/2509.22296)
*Joseph Hunt,Koyo Fujii,Aly Magassouba,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: 提出了一种基于物联网机器人技术的系统架构，通过协调人-机器人-机器人交互，实现主动和个性化的患者辅助，预防医院患者跌倒。


<details>
  <summary>Details</summary>
Motivation: 传统跌倒预防系统依赖事后检测或被动警报，存在高误报率且未能解决导致患者离床尝试的根本需求。

Method: 集成隐私保护的热感测模型进行实时离床预测，协调两个机器人代理根据预测意图和患者输入动态响应。

Result: 展示了低分辨率热感测在准确、保护隐私的预防性离床检测中的应用，并通过用户研究和系统误差分析为多智能体交互设计提供依据。

Conclusion: 交互式和连接的机器人系统能够超越被动监控，提供及时、有意义的辅助，创造更安全、响应更快的护理环境。

Abstract: Hospital patient falls remain a critical and costly challenge worldwide.
While conventional fall prevention systems typically rely on post-fall
detection or reactive alerts, they also often suffer from high false positive
rates and fail to address the underlying patient needs that lead to bed-exit
attempts. This paper presents a novel system architecture that leverages the
Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction
for proactive and personalized patient assistance. The system integrates a
privacy-preserving thermal sensing model capable of real-time bed-exit
prediction, with two coordinated robotic agents that respond dynamically based
on predicted intent and patient input. This orchestrated response could not
only reduce fall risk but also attend to the patient's underlying motivations
for movement, such as thirst, discomfort, or the need for assistance, before a
hazardous situation arises. Our contributions with this pilot study are
three-fold: (1) a modular IoRT-based framework enabling distributed sensing,
prediction, and multi-robot coordination; (2) a demonstration of low-resolution
thermal sensing for accurate, privacy-preserving preemptive bed-exit detection;
and (3) results from a user study and systematic error analysis that inform the
design of situationally aware, multi-agent interactions in hospital settings.
The findings highlight how interactive and connected robotic systems can move
beyond passive monitoring to deliver timely, meaningful assistance, empowering
safer, more responsive care environments.

</details>


### [34] [RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.22356)
*Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang*

Main category: cs.RO

TL;DR: 提出了RoboView-Bias基准，首次系统量化机器人操作中的视觉偏见，评估了三个代表性具身代理，发现所有代理都存在显著视觉偏见，并提出基于语义接地层的缓解策略可将偏见减少约54.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注泛化性和扰动下的鲁棒性，而缺乏对视觉偏见的系统量化，这限制了对感知如何影响决策稳定性的深入理解。

Method: 采用因子隔离原则，利用结构化变体生成框架和感知公平验证协议，创建了2,127个任务实例，用于稳健测量单个视觉因素及其相互作用引起的偏见。

Result: 评估发现：(i)所有代理都表现出显著视觉偏见，相机视角是最关键因素；(ii)代理在高度饱和颜色上获得最高成功率，表明继承了底层VLM的视觉偏好；(iii)视觉偏见表现出强不对称耦合，视角强烈放大颜色相关偏见。

Conclusion: 系统分析视觉偏见是开发安全可靠通用具身代理的先决条件，提出的缓解策略可显著减少视觉偏见。

Abstract: The safety and reliability of embodied agents rely on accurate and unbiased
visual perception. However, existing benchmarks mainly emphasize generalization
and robustness under perturbations, while systematic quantification of visual
bias remains scarce. This gap limits a deeper understanding of how perception
influences decision-making stability. To address this issue, we propose
RoboView-Bias, the first benchmark specifically designed to systematically
quantify visual bias in robotic manipulation, following a principle of factor
isolation. Leveraging a structured variant-generation framework and a
perceptual-fairness validation protocol, we create 2,127 task instances that
enable robust measurement of biases induced by individual visual factors and
their interactions. Using this benchmark, we systematically evaluate three
representative embodied agents across two prevailing paradigms and report three
key findings: (i) all agents exhibit significant visual biases, with camera
viewpoint being the most critical factor; (ii) agents achieve their highest
success rates on highly saturated colors, indicating inherited visual
preferences from underlying VLMs; and (iii) visual biases show strong,
asymmetric coupling, with viewpoint strongly amplifying color-related bias.
Finally, we demonstrate that a mitigation strategy based on a semantic
grounding layer substantially reduces visual bias by approximately 54.5\% on
MOKA. Our results highlight that systematic analysis of visual bias is a
prerequisite for developing safe and reliable general-purpose embodied agents.

</details>


### [35] [Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping](https://arxiv.org/abs/2509.22421)
*Leonel Giacobbe,Jingdao Chen,Chuangchuang Sun*

Main category: cs.RO

TL;DR: 提出了一种基于学习的触觉反应多智能体模型预测控制器，用于抓取各种软硬度和形状的物体，超越了现有单智能体实现的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数抓取实现主要针对刚性物体，在处理需要实时反馈的易碎或可变形材料时性能显著下降，且单智能体触觉反应抓取无法处理大型重物。

Method: 使用两个Gelsight Mini触觉传感器提取物体纹理和刚度的实时信息，通过触觉反馈估计接触动力学和物体柔顺性，采用多智能体MPC公式在真实接触交互上训练，结合触觉数据驱动方法推断抓取状态和协作控制策略。

Result: 通过大量实验验证，该方法在实现和维持不同尺寸和刚度物体的稳定抓取方面，成功率优于独立的PD和MPC基线方法。

Conclusion: 结合触觉传感和基于学习的多智能体MPC，为复杂环境中的协作抓取提供了鲁棒智能解决方案，显著提升了多智能体系统的能力。

Abstract: Grasping is a core task in robotics with various applications. However, most
current implementations are primarily designed for rigid items, and their
performance drops considerably when handling fragile or deformable materials
that require real-time feedback. Meanwhile, tactile-reactive grasping focuses
on a single agent, which limits their ability to grasp and manipulate large,
heavy objects. To overcome this, we propose a learning-based, tactile-reactive
multi-agent Model Predictive Controller (MPC) for grasping a wide range of
objects with different softness and shapes, beyond the capabilities of
preexisting single-agent implementations. Our system uses two Gelsight Mini
tactile sensors [1] to extract real-time information on object texture and
stiffness. This rich tactile feedback is used to estimate contact dynamics and
object compliance in real time, enabling the system to adapt its control policy
to diverse object geometries and stiffness profiles. The learned controller
operates in a closed loop, leveraging tactile encoding to predict grasp
stability and adjust force and position accordingly. Our key technical
contributions include a multi-agent MPC formulation trained on real contact
interactions, a tactile-data driven method for inferring grasping states, and a
coordination strategy that enables collaborative control. By combining tactile
sensing and a learning-based multi-agent MPC, our method offers a robust,
intelligent solution for collaborative grasping in complex environments,
significantly advancing the capabilities of multi-agent systems. Our approach
is validated through extensive experiments against independent PD and MPC
baselines. Our pipeline outperforms the baselines regarding success rates in
achieving and maintaining stable grasps across objects of varying sizes and
stiffness.

</details>


### [36] [An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics](https://arxiv.org/abs/2509.22434)
*Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach*

Main category: cs.RO

TL;DR: 本文提出了OntoBOT本体，用于统一表示服务机器人中的任务、动作、环境和能力，支持形式化推理和知识共享。


<details>
  <summary>Details</summary>
Motivation: 现有服务机器人解决方案通常与特定平台紧密耦合，导致孤立、硬编码的系统，限制了互操作性、可重用性和知识共享。现有本体论如SOMA和DOLCE专注于特定领域，未能充分捕捉环境、动作、机器人能力和系统级集成之间的连接。

Method: 扩展现有本体论，提出OntoBOT本体，提供任务、动作、环境和能力的统一表示。通过评估四个具体代理（TIAGo、HSR、UR3和Stretch）的能力问题来验证其通用性。

Result: OntoBOT支持上下文感知推理、面向任务的执行和知识共享，在四个不同机器人平台上展示了其通用性和有效性。

Conclusion: OntoBOT本体为服务机器人提供了一个统一的表示框架，解决了现有解决方案的互操作性和知识共享限制，支持跨平台的任务执行和推理。

Abstract: Personal service robots are increasingly used in domestic settings to assist
older adults and people requiring support. Effective operation involves not
only physical interaction but also the ability to interpret dynamic
environments, understand tasks, and choose appropriate actions based on
context. This requires integrating both hardware components (e.g. sensors,
actuators) and software systems capable of reasoning about tasks, environments,
and robot capabilities. Frameworks such as the Robot Operating System (ROS)
provide open-source tools that help connect low-level hardware with
higher-level functionalities. However, real-world deployments remain tightly
coupled to specific platforms. As a result, solutions are often isolated and
hard-coded, limiting interoperability, reusability, and knowledge sharing.
Ontologies and knowledge graphs offer a structured way to represent tasks,
environments, and robot capabilities. Existing ontologies, such as the
Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for
Linguistic and Cognitive Engineering (DOLCE), provide models for activities,
spatial relationships, and reasoning structures. However, they often focus on
specific domains and do not fully capture the connection between environment,
action, robot capabilities, and system-level integration. In this work, we
propose the Ontology for roBOts and acTions (OntoBOT), which extends existing
ontologies to provide a unified representation of tasks, actions, environments,
and capabilities. Our contributions are twofold: (1) we unify these aspects
into a cohesive ontology to support formal reasoning about task execution, and
(2) we demonstrate its generalizability by evaluating competency questions
across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how
OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge
sharing in service robotics.

</details>


### [37] [UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation](https://arxiv.org/abs/2509.22441)
*Zhangyuan Wang,Yunpeng Zhu,Yuqi Yan,Xiaoyuan Tian,Xinhao Shao,Meixuan Li,Weikun Li,Guangsheng Su,Weicheng Cui,Dixia Fan*

Main category: cs.RO

TL;DR: UnderwaterVLA是一个用于自主水下导航的新框架，集成了多模态基础模型与具身智能系统，通过双脑架构、VLA模型和流体力学MPC方案，显著提升了在恶劣水下环境中的导航性能和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 水下作业面临流体动力学干扰、有限通信带宽和浑浊水域感知退化等挑战，需要开发能够适应这些恶劣条件的自主导航系统。

Method: 采用双脑架构分离高层任务推理与低层反应控制；首次将视觉-语言-动作(VLA)模型应用于水下机器人，结合结构化思维链推理；使用流体力学模型预测控制(MPC)方案实时补偿流体效应。

Result: 现场测试显示，UnderwaterVLA在视觉条件退化时减少了导航误差，任务完成率比基线提高了19%到27%。

Conclusion: 通过减少对水下特定训练数据的依赖并提高跨环境适应性，UnderwaterVLA为下一代智能AUV提供了可扩展且经济高效的解决方案。

Abstract: This paper presents UnderwaterVLA, a novel framework for autonomous
underwater navigation that integrates multimodal foundation models with
embodied intelligence systems. Underwater operations remain difficult due to
hydrodynamic disturbances, limited communication bandwidth, and degraded
sensing in turbid waters. To address these challenges, we introduce three
innovations. First, a dual-brain architecture decouples high-level mission
reasoning from low-level reactive control, enabling robust operation under
communication and computational constraints. Second, we apply
Vision-Language-Action(VLA) models to underwater robotics for the first time,
incorporating structured chain-of-thought reasoning for interpretable
decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)
scheme compensates for fluid effects in real time without costly task-specific
training. Experimental results in field tests show that UnderwaterVLA reduces
navigation errors in degraded visual conditions while maintaining higher task
completion by 19% to 27% over baseline. By minimizing reliance on
underwater-specific training data and improving adaptability across
environments, UnderwaterVLA provides a scalable and cost-effective path toward
the next generation of intelligent AUVs.

</details>


### [38] [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](https://arxiv.org/abs/2509.22469)
*Ben Rossano,Jaein Lim,Jonathan P. How*

Main category: cs.RO

TL;DR: 提出了一种面向异构机器人团队的任务分配算法，在任务需求不确定的环境中通过概率建模和基于市场的方法实现高效分配。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人团队在任务需求不确定环境中的分配问题，避免资源浪费同时主动预防任务失败。

Method: 使用概率分布建模任务需求，采用基于市场的方法优化团队联合目标，显式捕捉机器人间的耦合奖励，提供多项式时间解决方案。

Result: 与基准算法相比的实验结果表明该方法有效，同时突显了在分散式制定中纳入耦合奖励的挑战。

Conclusion: 提出的算法能够在严格通信假设的分散环境中有效处理异构机器人团队的任务分配问题，平衡资源利用和任务成功率。

Abstract: This paper proposes a task allocation algorithm for teams of heterogeneous
robots in environments with uncertain task requirements. We model these
requirements as probability distributions over capabilities and use this model
to allocate tasks such that robots with complementary skills naturally position
near uncertain tasks, proactively mitigating task failures without wasting
resources. We introduce a market-based approach that optimizes the joint team
objective while explicitly capturing coupled rewards between robots, offering a
polynomial-time solution in decentralized settings with strict communication
assumptions. Comparative experiments against benchmark algorithms demonstrate
the effectiveness of our approach and highlight the challenges of incorporating
coupled rewards in a decentralized formulation.

</details>


### [39] [Ontological foundations for contrastive explanatory narration of robot plans](https://arxiv.org/abs/2509.22493)
*Alberto Olivares-Alarcos,Sergi Foix,Júlia Borràs,Gerard Canal,Guillem Alenyà*

Main category: cs.RO

TL;DR: 提出了一种新的本体模型和算法，用于比较两个竞争性计划之间的差异，使机器人能够解释为何选择某个计划而非另一个。


<details>
  <summary>Details</summary>
Motivation: 确保人机交互中的人工智能决策能够被理解，建立信任关系，使机器人能够做出合理决策并在需要时向人类解释。

Method: 开发了新颖的本体模型来形式化和推理竞争计划之间的差异，并提出了一种利用计划间分歧知识的新算法来构建对比性叙述。

Result: 通过实证评估，该方法在解释质量上显著优于基线方法。

Conclusion: 该研究为机器人解释决策提供了有效的方法，通过对比性叙述增强了人机交互中的透明度和信任度。

Abstract: Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.

</details>


### [40] [HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes](https://arxiv.org/abs/2509.22498)
*Katrina Ashton,Chahyon Ku,Shrey Shah,Wen Jiang,Kostas Daniilidis,Bernadette Bucher*

Main category: cs.RO

TL;DR: HELIOS是一个分层场景表示方法，用于解决语言指定的移动操作任务，在部分观察环境中通过平衡探索和利用来高效搜索目标物体，在OVMM基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决在部分观察的新环境中执行语言指定移动操作任务时面临的挑战：场景部分可见、语言指令到部分观察场景的语义接地、以及主动更新场景知识。

Method: 构建包含语义和占据信息的2D地图，同时主动构建任务相关对象的3D高斯表示，融合多层表示并显式建模每个对象检测的多视角一致性，制定平衡探索和利用的目标函数。

Result: 在Habitat模拟器的OVMM基准测试中取得最先进结果，能够零样本迁移到真实世界，在Spot机器人上成功演示。

Conclusion: HELIOS通过分层场景表示和有效的搜索策略，成功解决了部分观察环境中的语言指定移动操作问题，具有良好的泛化能力。

Abstract: Language-specified mobile manipulation tasks in novel environments
simultaneously face challenges interacting with a scene which is only partially
observed, grounding semantic information from language instructions to the
partially observed scene, and actively updating knowledge of the scene with new
observations. To address these challenges, we propose HELIOS, a hierarchical
scene representation and associated search objective to perform language
specified pick and place mobile manipulation tasks. We construct 2D maps
containing the relevant semantic and occupancy information for navigation while
simultaneously actively constructing 3D Gaussian representations of
task-relevant objects. We fuse observations across this multi-layered
representation while explicitly modeling the multi-view consistency of the
detections of each object. In order to efficiently search for the target
object, we formulate an objective function balancing exploration of unobserved
or uncertain regions with exploitation of scene semantic information. We
evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and
place benchmark in which perception is challenging due to large and complex
scenes with comparatively small target objects. HELIOS achieves
state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also
transfer to the real world without requiring additional data, as we illustrate
by demonstrating it in a real world office environment on a Spot robot.

</details>


### [41] [An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment](https://arxiv.org/abs/2509.22550)
*Xiaoyun Qiu,Haichao Liu,Yue Pan,Jun Ma,Xinhu Zheng*

Main category: cs.RO

TL;DR: 提出了一种意图驱动的车道变换框架，通过驾驶风格识别、合作感知决策和协调运动规划，在混合交通环境中实现安全高效的车道变换。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶车辆与人类驾驶车辆交互的混合交通环境中，不可预测的意图和异质行为使得安全高效的车道变换极具挑战性，现有方法往往过度简化这些交互。

Method: 使用深度学习分类器实时识别人类驾驶风格；通过包含内在和交互组件的合作分数估计周围驾驶员的意图；结合行为克隆和逆强化学习进行决策；集成模型预测控制和IRL意图推断生成轨迹。

Result: 模型达到94.2%准确率和94.3% F1分数，在车道变换识别方面比基于规则和基于学习的基线方法高出4-15%。

Conclusion: 结果表明建模驾驶员异质性的益处，并展示了该框架在复杂交通环境中推进情境感知和类人自动驾驶的潜力。

Abstract: In mixed-traffic environments, where autonomous vehicles (AVs) interact with
diverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous
behaviors make safe and efficient lane change maneuvers highly challenging.
Existing methods often oversimplify these interactions by assuming uniform
patterns. We propose an intention-driven lane change framework that integrates
driving-style recognition, cooperation-aware decision-making, and coordinated
motion planning. A deep learning classifier trained on the NGSIM dataset
identifies human driving styles in real time. A cooperation score with
intrinsic and interactive components estimates surrounding drivers' intentions
and quantifies their willingness to cooperate with the ego vehicle.
Decision-making combines behavior cloning with inverse reinforcement learning
to determine whether a lane change should be initiated. For trajectory
generation, model predictive control is integrated with IRL-based intention
inference to produce collision-free and socially compliant maneuvers.
Experiments show that the proposed model achieves 94.2\% accuracy and 94.3\%
F1-score, outperforming rule-based and learning-based baselines by 4-15\% in
lane change recognition. These results highlight the benefit of modeling
inter-driver heterogeneity and demonstrate the potential of the framework to
advance context-aware and human-like autonomous driving in complex traffic
environments.

</details>


### [42] [MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data](https://arxiv.org/abs/2509.22573)
*Farida Mohsen,Ali Safa*

Main category: cs.RO

TL;DR: 提出了一种仅使用RGB输入的帧级精度人类交互意图预测方法，通过MINT-RVAE合成序列生成和新的损失函数解决了数据集类别不平衡问题，在性能上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖多模态输入（如RGB-D），而仅使用RGB输入可以降低硬件成本并实现更快的机器人响应。同时，真实世界HRI数据集中的类别不平衡问题会影响模型训练和泛化能力。

Method: 提出了RGB-only的意图预测流程，包括MINT-RVAE合成序列生成方法、新的损失函数和训练策略，以增强对样本外数据的泛化能力。

Result: 在性能上达到了SOTA水平（AUROC：0.95），优于先前工作（AUROC：0.90-0.912），同时仅需RGB输入并支持精确的帧级起始预测。

Conclusion: 该方法在仅使用RGB输入的情况下实现了优越的交互意图检测性能，并公开了新的帧级标注数据集以支持未来研究。

Abstract: Efficiently detecting human intent to interact with ubiquitous robots is
crucial for effective human-robot interaction (HRI) and collaboration. Over the
past decade, deep learning has gained traction in this field, with most
existing approaches relying on multimodal inputs, such as RGB combined with
depth (RGB-D), to classify time-sequence windows of sensory data as interactive
or non-interactive. In contrast, we propose a novel RGB-only pipeline for
predicting human interaction intent with frame-level precision, enabling faster
robot responses and improved service quality. A key challenge in intent
prediction is the class imbalance inherent in real-world HRI datasets, which
can hinder the model's training and generalization. To address this, we
introduce MINT-RVAE, a synthetic sequence generation method, along with new
loss functions and training strategies that enhance generalization on
out-of-sample data. Our approach achieves state-of-the-art performance (AUROC:
0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB
input and supporting precise frame onset prediction. Finally, to support future
research, we openly release our new dataset with frame-level labeling of human
interaction intent.

</details>


### [43] [EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation](https://arxiv.org/abs/2509.22578)
*Yuan Xu,Jiabing Yang,Xiaofeng Wang,Yixiang Chen,Zheng Zhu,Bowen Fang,Guan Huang,Xinze Chen,Yun Ye,Qiang Zhang,Peiyan Li,Xiangnan Wu,Kai Wang,Bing Zhan,Shuo Lu,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.RO

TL;DR: 提出EgoDemoGen框架，通过生成配对的新的自我中心视角演示来解决模仿学习策略在自我中心视角变化下的性能下降问题，显著提升了机器人在不同自我中心视角下的操作成功率。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在单一自我中心视角下训练时，在面对自我中心视角变化时性能会显著下降，需要解决视角变化带来的鲁棒性问题。

Method: 开发EgoDemoGen框架，包含EgoViewTransfer生成模型，通过重定向动作到新的自我中心坐标系，并合成对应的自我中心观察视频。使用自监督双重重投影策略微调预训练视频生成模型。

Result: 在仿真环境中，标准自我中心视角成功率绝对提升17.0%，新自我中心视角提升17.7%；在真实机器人上，绝对提升分别为18.3%和25.8%。性能随EgoDemoGen生成演示比例增加而持续提升。

Conclusion: EgoDemoGen为自我中心视角鲁棒的机器人操作提供了一条实用路径，能够有效提升策略在不同自我中心视角下的泛化能力。

Abstract: Imitation learning based policies perform well in robotic manipulation, but
they often degrade under *egocentric viewpoint shifts* when trained from a
single egocentric viewpoint. To address this issue, we present **EgoDemoGen**,
a framework that generates *paired* novel egocentric demonstrations by
retargeting actions in the novel egocentric frame and synthesizing the
corresponding egocentric observation videos with proposed generative video
repair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint
reprojected scene video and a robot-only video rendered from the retargeted
joint actions. EgoViewTransfer is finetuned from a pretrained video generation
model using self-supervised double reprojection strategy. We evaluate
EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After
training with a mixture of EgoDemoGen-generated novel egocentric demonstrations
and original standard egocentric demonstrations, policy success rate improves
**absolutely** by **+17.0%** for standard egocentric viewpoint and by
**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,
the **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,
performance continues to improve as the proportion of EgoDemoGen-generated
demonstrations increases, with diminishing returns. These results demonstrate
that EgoDemoGen provides a practical route to egocentric viewpoint-robust
robotic manipulation.

</details>


### [44] [WoW: Towards a World omniscient World model Through Embodied Interaction](https://arxiv.org/abs/2509.22642)
*Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: 提出了WoW模型，通过200万机器人交互轨迹训练，证明大规模真实世界交互是发展AI物理直觉的关键。模型能生成物理一致视频，并通过SOPHIA系统约束物理真实性。


<details>
  <summary>Details</summary>
Motivation: 人类通过主动交互发展物理直觉，而当前视频模型（如Sora）仅依赖被动观察，难以理解物理因果关系。假设真实物理直觉必须基于与真实世界的丰富因果交互。

Method: 开发了140亿参数的WoW生成世界模型，在200万机器人交互轨迹上训练。使用SOPHIA系统（视觉语言模型代理）评估DiT生成输出，并通过迭代演化语言指令引导优化。同时训练逆动力学模型将精炼计划转化为可执行机器人动作。

Result: 模型对物理的理解是可能结果的概率分布，会产生随机不稳定性和物理幻觉。在WoWBench基准测试中，在物理一致性和因果推理方面达到最先进性能，在物理因果关系、碰撞动力学和物体持久性方面表现优异。

Conclusion: 大规模真实世界交互是发展AI物理直觉的基石。模型、数据和基准将开源。

Abstract: Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.

</details>


### [45] [VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search](https://arxiv.org/abs/2509.22643)
*Wenkai Guo,Guanxing Lu,Haoyuan Deng,Zhenyu Wu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出VLA-Reasoner框架，通过测试时扩展为现有VLA模型增加预见未来状态的能力，解决长时程任务中的增量偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型只能预测短视的下一步动作，在长时程轨迹任务中因增量偏差而表现不佳。

Method: 使用世界模型采样和展开动作轨迹来生成未来状态，结合MCTS提高搜索效率，引入基于KDE的置信度采样机制减少冗余查询，采用离线奖励策略评估中间状态。

Result: 在仿真和真实世界实验中，VLA-Reasoner相比最先进的VLA模型取得了显著改进。

Conclusion: 该方法展示了机器人操作任务中可扩展测试时计算的潜在路径。

Abstract: Vision-Language-Action models (VLAs) achieve strong performance in general
robotic manipulation tasks by scaling imitation learning. However, existing
VLAs are limited to predicting short-sighted next-action, which struggle with
long-horizon trajectory tasks due to incremental deviations. To address this
problem, we propose a plug-in framework named VLA-Reasoner that effectively
empowers off-the-shelf VLAs with the capability of foreseeing future states via
test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible
action trajectories where involved actions are rationales to generate future
states via a world model, which enables VLA-Reasoner to foresee and reason
potential outcomes and search for the optimal actions. We further leverage
Monte Carlo Tree Search (MCTS) to improve search efficiency in large action
spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a
confidence sampling mechanism based on Kernel Density Estimation (KDE), to
enable efficient exploration in MCTS without redundant VLA queries. We evaluate
intermediate states in MCTS via an offline reward shaping strategy, to score
predicted futures and correct deviations with long-term feedback. We conducted
extensive experiments in both simulators and the real world, demonstrating that
our proposed VLA-Reasoner achieves significant improvements over the
state-of-the-art VLAs. Our method highlights a potential pathway toward
scalable test-time computation of robotic manipulation.

</details>


### [46] [Pixel Motion Diffusion is What We Need for Robot Control](https://arxiv.org/abs/2509.22652)
*E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo*

Main category: cs.RO

TL;DR: DAWN是一个基于扩散模型的统一机器人控制框架，通过结构化像素运动表示连接高级运动意图和低级机器人动作，在CALVIN基准测试中达到最先进水平，并展示了从仿真到真实环境的可靠迁移能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人控制中高级运动意图与低级动作之间的连接问题，以及实现可扩展和鲁棒的机器人学习系统。

Method: 使用扩散过程建模高级和低级控制器，通过结构化像素运动表示构建端到端可训练系统，具有可解释的中间运动抽象。

Result: 在CALVIN基准测试中达到最先进水平，在MetaWorld上验证有效性，仅需少量微调即可实现从仿真到真实环境的可靠迁移。

Conclusion: 扩散建模与运动中心表示的结合为可扩展和鲁棒的机器人学习提供了强大的基准框架。

Abstract: We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation. In DAWN, both the high-level and low-level
controllers are modeled as diffusion processes, yielding a fully trainable,
end-to-end system with interpretable intermediate motion abstractions. DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld. Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control. Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning. Project page: https://nero1342.github.io/DAWN/

</details>


### [47] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: SPF是一个无需训练的空中视觉语言导航框架，将动作预测视为2D空间定位任务，通过视觉语言模型将模糊指令分解为2D航点，实现无人机在动态环境中的闭环控制导航。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的方法将动作预测视为文本生成任务，但作者认为空中视觉语言导航的动作预测更适合作为2D空间定位任务来处理。

Method: 利用VLM将模糊语言指令分解为输入图像上的迭代2D航点标注，结合预测的移动距离，将2D航点转换为3D位移向量作为无人机动作命令，并自适应调整移动距离以提高导航效率。

Result: 在DRL仿真基准测试中达到新的最先进水平，比之前最佳方法绝对提升63%；在真实世界评估中大幅优于强基线方法。

Conclusion: SPF展示了卓越的泛化能力，能够适应不同的VLM，并在动态环境中实现高效导航。

Abstract: We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev

</details>
