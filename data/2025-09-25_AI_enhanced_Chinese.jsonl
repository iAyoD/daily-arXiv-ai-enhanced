{"id": "2509.19452", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19452", "abs": "https://arxiv.org/abs/2509.19452", "authors": ["Alessandro Saviolo", "Jeffrey Mao", "Giuseppe Loianno"], "title": "HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames", "comment": null, "summary": "Search and rescue operations require unmanned aerial vehicles to both\ntraverse unknown unstructured environments at high speed and track targets once\ndetected. Achieving both capabilities under degraded sensing and without global\nlocalization remains an open challenge. Recent works on relative navigation\nhave shown robust tracking by anchoring planning and control to a visible\ndetected object, but cannot address navigation when no target is in the field\nof view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time\nframework that unifies traversal, acquisition, and tracking within a single\nrelative formulation. HUNT defines navigation objectives directly from onboard\ninstantaneous observables such as attitude, altitude, and velocity, enabling\nreactive high-speed flight during search. Once a target is detected, the same\nperception-control pipeline transitions seamlessly to tracking. Outdoor\nexperiments in dense forests, container compounds, and search-and-rescue\noperations with vehicles and mannequins demonstrate robust autonomy where\nglobal methods fail.", "AI": {"tldr": "HUNT\u662f\u4e00\u4e2a\u5b9e\u65f6\u6846\u67b6\uff0c\u5c06\u65e0\u4eba\u673a\u7684\u9ad8\u901f\u7a7f\u8d8a\u3001\u76ee\u6807\u83b7\u53d6\u548c\u8ddf\u8e2a\u7edf\u4e00\u5728\u76f8\u5bf9\u5bfc\u822a\u6846\u67b6\u4e2d\uff0c\u65e0\u9700\u5168\u5c40\u5b9a\u4f4d\u5373\u53ef\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8fd0\u884c", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u641c\u7d22\u6551\u63f4\u4efb\u52a1\u4e2d\u540c\u65f6\u9700\u8981\u9ad8\u901f\u7a7f\u8d8a\u672a\u77e5\u73af\u5883\u548c\u8ddf\u8e2a\u76ee\u6807\u7684\u53cc\u91cd\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f20\u611f\u5668\u9000\u5316\u548c\u65e0\u5168\u5c40\u5b9a\u4f4d\u7684\u60c5\u51b5\u4e0b", "method": "\u57fa\u4e8e\u673a\u8f7d\u77ac\u65f6\u89c2\u6d4b\u6570\u636e\uff08\u59ff\u6001\u3001\u9ad8\u5ea6\u3001\u901f\u5ea6\uff09\u5b9a\u4e49\u5bfc\u822a\u76ee\u6807\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u611f\u77e5\u63a7\u5236\u7ba1\u9053\uff0c\u5728\u641c\u7d22\u65f6\u5b9e\u73b0\u53cd\u5e94\u5f0f\u9ad8\u901f\u98de\u884c\uff0c\u68c0\u6d4b\u5230\u76ee\u6807\u540e\u65e0\u7f1d\u5207\u6362\u5230\u8ddf\u8e2a\u6a21\u5f0f", "result": "\u5728\u8302\u5bc6\u68ee\u6797\u3001\u96c6\u88c5\u7bb1\u573a\u5730\u548c\u641c\u6551\u573a\u666f\u4e2d\u7684\u6237\u5916\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5168\u5c40\u65b9\u6cd5\u5931\u6548\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u81ea\u4e3b\u98de\u884c", "conclusion": "HUNT\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u5728\u672a\u77e5\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9ad8\u901f\u5bfc\u822a\u548c\u76ee\u6807\u8ddf\u8e2a\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u641c\u7d22\u6551\u63f4\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u4e3b\u80fd\u529b"}}
{"id": "2509.19454", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19454", "abs": "https://arxiv.org/abs/2509.19454", "authors": ["Jason Chen", "I-Chun Arthur Liu", "Gaurav Sukhatme", "Daniel Seita"], "title": "ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation", "comment": null, "summary": "Training robust bimanual manipulation policies via imitation learning\nrequires demonstration data with broad coverage over robot poses, contacts, and\nscene contexts. However, collecting diverse and precise real-world\ndemonstrations is costly and time-consuming, which hinders scalability. Prior\nworks have addressed this with data augmentation, typically for either\neye-in-hand (wrist camera) setups with RGB inputs or for generating novel\nimages without paired actions, leaving augmentation for eye-to-hand\n(third-person) RGB-D training with new action labels less explored. In this\npaper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data\nAugmentation (ROPA), an offline imitation learning data augmentation method\nthat fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D\nobservations of novel robot poses. Our approach simultaneously generates\ncorresponding joint-space action labels while employing constrained\noptimization to enforce physical consistency through appropriate\ngripper-to-object contact constraints in bimanual scenarios. We evaluate our\nmethod on 5 simulated and 3 real-world tasks. Our results across 2625\nsimulation trials and 300 real-world trials demonstrate that ROPA outperforms\nbaselines and ablations, showing its potential for scalable RGB and RGB-D data\naugmentation in eye-to-hand bimanual manipulation. Our project website is\navailable at: https://ropaaug.github.io/.", "AI": {"tldr": "ROPA\u662f\u4e00\u79cd\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03Stable Diffusion\u5408\u6210\u7b2c\u4e09\u4eba\u79f0RGB\u548cRGB-D\u89c2\u5bdf\u6570\u636e\uff0c\u540c\u65f6\u751f\u6210\u5bf9\u5e94\u7684\u5173\u8282\u7a7a\u95f4\u52a8\u4f5c\u6807\u7b7e\uff0c\u7528\u4e8e\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6570\u636e\u589e\u5f3a\u3002", "motivation": "\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u591a\u6837\u4e14\u7cbe\u786e\u7684\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u6f14\u793a\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u624b\u8155\u6444\u50cf\u5934\u8bbe\u7f6e\u6216\u4ec5\u751f\u6210\u65b0\u56fe\u50cf\u800c\u65e0\u914d\u5bf9\u52a8\u4f5c\uff0c\u5bf9\u7b2c\u4e09\u4eba\u79f0RGB-D\u8bad\u7ec3\u7684\u65b0\u52a8\u4f5c\u6807\u7b7e\u589e\u5f3a\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51faROPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u5f3a\u5236\u6267\u884c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u5728\u53cc\u624b\u673a\u5668\u4eba\u573a\u666f\u4e2d\u65bd\u52a0\u9002\u5f53\u7684\u5939\u722a-\u7269\u4f53\u63a5\u89e6\u7ea6\u675f\uff0c\u540c\u65f6\u751f\u6210RGB/RGB-D\u89c2\u5bdf\u6570\u636e\u548c\u5bf9\u5e94\u7684\u5173\u8282\u7a7a\u95f4\u52a8\u4f5c\u3002", "result": "\u57285\u4e2a\u6a21\u62df\u4efb\u52a1\u548c3\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c2625\u6b21\u6a21\u62df\u8bd5\u9a8c\u548c300\u6b21\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u7ed3\u679c\u8868\u660e\uff0cROPA\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u6d88\u878d\u5b9e\u9a8c\u3002", "conclusion": "ROPA\u5c55\u793a\u4e86\u5728\u7b2c\u4e09\u4eba\u79f0\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8fdb\u884cRGB\u548cRGB-D\u6570\u636e\u589e\u5f3a\u7684\u53ef\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2509.19460", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19460", "abs": "https://arxiv.org/abs/2509.19460", "authors": ["Yifan Ye", "Jun Cen", "Jing Chen", "Zhihe Lu"], "title": "Self-evolved Imitation Learning in Simulated World", "comment": null, "summary": "Imitation learning has been a trend recently, yet training a generalist agent\nacross multiple tasks still requires large-scale expert demonstrations, which\nare costly and labor-intensive to collect. To address the challenge of limited\nsupervision, we propose Self-Evolved Imitation Learning (SEIL), a framework\nthat progressively improves a few-shot model through simulator interactions.\nThe model first attempts tasksin the simulator, from which successful\ntrajectories are collected as new demonstrations for iterative refinement. To\nenhance the diversity of these demonstrations, SEIL employs dual-level\naugmentation: (i) Model-level, using an Exponential Moving Average (EMA) model\nto collaborate with the primary model, and (ii) Environment-level, introducing\nslight variations in initial object positions. We further introduce a\nlightweight selector that filters complementary and informative trajectories\nfrom the generated pool to ensure demonstration quality. These curated samples\nenable the model to achieve competitive performance with far fewer training\nexamples. Extensive experiments on the LIBERO benchmark show that SEIL achieves\na new state-of-the-art performance in few-shot imitation learning scenarios.\nCode is available at https://github.com/Jasper-aaa/SEIL.git.", "AI": {"tldr": "SEIL\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u4ea4\u4e92\u9010\u6b65\u6539\u8fdb\u5c11\u6837\u672c\u6a21\u578b\uff0c\u5229\u7528\u53cc\u7ea7\u589e\u5f3a\u548c\u8f7b\u91cf\u7ea7\u9009\u62e9\u5668\u6765\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u4e13\u5bb6\u6f14\u793a\u7684\u4f9d\u8d56\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6f14\u793a\uff0c\u4f46\u6536\u96c6\u6210\u672c\u9ad8\u6602\u3002\u4e3a\u4e86\u89e3\u51b3\u76d1\u7763\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u5c11\u91cf\u6f14\u793a\u4e0b\u6709\u6548\u5b66\u4e60\u7684\u6846\u67b6\u3002", "method": "SEIL\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u5668\u4ea4\u4e92\u6536\u96c6\u6210\u529f\u8f68\u8ff9\u4f5c\u4e3a\u65b0\u6f14\u793a\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u91c7\u7528\u6a21\u578b\u7ea7\uff08EMA\u6a21\u578b\u534f\u4f5c\uff09\u548c\u73af\u5883\u7ea7\uff08\u521d\u59cb\u4f4d\u7f6e\u53d8\u5316\uff09\u53cc\u7ea7\u589e\u5f3a\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9009\u62e9\u5668\u7b5b\u9009\u9ad8\u8d28\u91cf\u8f68\u8ff9\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSEIL\u5728\u5c11\u6837\u672c\u6a21\u4eff\u5b66\u4e60\u573a\u666f\u4e0b\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7528\u66f4\u5c11\u7684\u8bad\u7ec3\u6837\u672c\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002", "conclusion": "SEIL\u901a\u8fc7\u81ea\u6f14\u5316\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u76d1\u7763\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a\u5c11\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19463", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19463", "abs": "https://arxiv.org/abs/2509.19463", "authors": ["Doncey Albin", "Daniel McGann", "Miles Mena", "Annika Thomas", "Harel Biggie", "Xuefei Sun", "Steve McGuire", "Jonathan P. How", "Christoffer Heckman"], "title": "CU-Multi: A Dataset for Multi-Robot Collaborative Perception", "comment": "8 pages, 11 figures", "summary": "A central challenge for multi-robot systems is fusing independently gathered\nperception data into a unified representation. Despite progress in\nCollaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of\ndedicated multi-robot datasets. Many evaluations instead partition single-robot\ntrajectories, a practice that may only partially reflect true multi-robot\noperations and, more critically, lacks standardization, leading to results that\nare difficult to interpret or compare across studies. While several multi-robot\ndatasets have recently been introduced, they mostly contain short trajectories\nwith limited inter-robot overlap and sparse intra-robot loop closures. To\novercome these limitations, we introduce CU-Multi, a dataset collected over\nmultiple days at two large outdoor sites on the University of Colorado Boulder\ncampus. CU-Multi comprises four synchronized runs with aligned start times and\ncontrolled trajectory overlap, replicating the distinct perspectives of a robot\nteam. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined\nground-truth odometry. By combining overlap variation with dense semantic\nannotations, CU-Multi provides a strong foundation for reproducible evaluation\nin multi-robot collaborative perception tasks.", "AI": {"tldr": "CU-Multi\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u8f68\u8ff9\u91cd\u53e0\u548c\u95ed\u73af\u68c0\u6d4b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u534f\u540c\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7814\u7a76\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\uff0c\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u8f68\u8ff9\u77ed\u3001\u673a\u5668\u4eba\u95f4\u91cd\u53e0\u5c11\u3001\u95ed\u73af\u7a00\u758f\uff0c\u4e14\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u5bfc\u81f4\u7ed3\u679c\u96be\u4ee5\u6bd4\u8f83\u3002", "method": "\u5728\u79d1\u7f57\u62c9\u591a\u5927\u5b66\u535a\u5c14\u5fb7\u5206\u6821\u7684\u4e24\u4e2a\u5927\u578b\u6237\u5916\u573a\u5730\u6536\u96c6\u6570\u636e\uff0c\u5305\u542b\u56db\u4e2a\u540c\u6b65\u8fd0\u884c\u7684\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5177\u6709\u5bf9\u9f50\u7684\u8d77\u59cb\u65f6\u95f4\u548c\u53ef\u63a7\u7684\u8f68\u8ff9\u91cd\u53e0\uff0c\u63d0\u4f9bRGB-D\u611f\u77e5\u3001RTK GPS\u3001\u8bed\u4e49LiDAR\u548c\u7cbe\u70bc\u7684\u91cc\u7a0b\u8ba1\u771f\u503c\u3002", "result": "\u6784\u5efa\u4e86CU-Multi\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ed3\u5408\u91cd\u53e0\u53d8\u5316\u548c\u5bc6\u96c6\u8bed\u4e49\u6807\u6ce8\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u534f\u540c\u611f\u77e5\u4efb\u52a1\u7684\u53ef\u91cd\u590d\u8bc4\u4f30\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "CU-Multi\u6570\u636e\u96c6\u514b\u670d\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u771f\u5b9e\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\uff0c\u4fc3\u8fdb\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7814\u7a76\u7684\u6807\u51c6\u5316\u548c\u53ef\u6bd4\u6027\u3002"}}
{"id": "2509.19473", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19473", "abs": "https://arxiv.org/abs/2509.19473", "authors": ["Adarsh Salagame", "Henry Noyes", "Alireza Ramezani", "Eric Sihite", "Arash Kalantari"], "title": "Crater Observing Bio-inspired Rolling Articulator (COBRA)", "comment": null, "summary": "NASA aims to establish a sustainable human basecamp on the Moon as a stepping\nstone for future missions to Mars and beyond. The discovery of water ice on the\nMoon's craters located in permanently shadowed regions, which can provide\ndrinking water, oxygen, and rocket fuel, is therefore of critical importance.\nHowever, current methods to access lunar ice deposits are limited. While rovers\nhave been used to explore the lunar surface for decades, they face significant\nchallenges in navigating harsh terrains, such as permanently shadowed craters,\ndue to the high risk of immobilization. This report introduces COBRA (Crater\nObserving Bio-inspired Rolling Articulator), a multi-modal snake-style robot\ndesigned to overcome mobility challenges in Shackleton Crater's rugged\nenvironment. COBRA combines slithering and tumbling locomotion to adapt to\nvarious crater terrains. In snake mode, it uses sidewinding to traverse flat or\nlow inclined surfaces, while in tumbling mode, it forms a circular barrel by\nlinking its head and tail, enabling rapid movement with minimal energy on steep\nslopes. Equipped with an onboard computer, stereo camera, inertial measurement\nunit, and joint encoders, COBRA facilitates real-time data collection and\nautonomous operation. This paper highlights COBRAs robustness and efficiency in\nnavigating extreme terrains through both simulations and experimental\nvalidation.", "AI": {"tldr": "COBRA\u662f\u4e00\u79cd\u591a\u6a21\u6001\u86c7\u5f62\u673a\u5668\u4eba\uff0c\u4e13\u4e3a\u5728\u6708\u7403\u6c38\u4e45\u9634\u5f71\u533a\u9668\u77f3\u5751\u7684\u6076\u52a3\u5730\u5f62\u4e2d\u5bfc\u822a\u800c\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6ed1\u884c\u548c\u7ffb\u6eda\u4e24\u79cd\u8fd0\u52a8\u6a21\u5f0f\u6765\u9002\u5e94\u4e0d\u540c\u5730\u5f62\u6761\u4ef6\u3002", "motivation": "NASA\u8ba1\u5212\u5728\u6708\u7403\u5efa\u7acb\u53ef\u6301\u7eed\u7684\u4eba\u7c7b\u57fa\u5730\uff0c\u4f46\u73b0\u6709\u63a2\u6d4b\u8f66\u96be\u4ee5\u5728\u6c38\u4e45\u9634\u5f71\u533a\u9668\u77f3\u5751\u7b49\u6781\u7aef\u5730\u5f62\u4e2d\u5b89\u5168\u79fb\u52a8\uff0c\u9650\u5236\u4e86\u6708\u7403\u6c34\u51b0\u8d44\u6e90\u7684\u5f00\u53d1\u5229\u7528\u3002", "method": "COBRA\u91c7\u7528\u86c7\u5f62\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u5177\u6709\u4e24\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff1a\u86c7\u6a21\u5f0f\u4f7f\u7528\u4fa7\u5411\u6ed1\u884c\u5728\u5e73\u5766\u6216\u7f13\u5761\u5730\u5f62\u79fb\u52a8\uff1b\u7ffb\u6eda\u6a21\u5f0f\u901a\u8fc7\u8fde\u63a5\u5934\u5c3e\u5f62\u6210\u5706\u7b52\u72b6\u7ed3\u6784\uff0c\u5728\u9661\u5761\u4e0a\u5b9e\u73b0\u9ad8\u6548\u6eda\u52a8\u3002\u673a\u5668\u4eba\u914d\u5907\u673a\u8f7d\u8ba1\u7b97\u673a\u3001\u7acb\u4f53\u76f8\u673a\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u548c\u5173\u8282\u7f16\u7801\u5668\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0cCOBRA\u5728\u6c99\u514b\u5c14\u987f\u9668\u77f3\u5751\u7684\u5d0e\u5c96\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u79fb\u52a8\u6548\u7387\u3002", "conclusion": "COBRA\u7684\u591a\u6a21\u6001\u79fb\u52a8\u7b56\u7565\u4e3a\u89e3\u51b3\u6708\u7403\u6781\u7aef\u5730\u5f62\u63a2\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\uff0c\u6709\u671b\u652f\u6301\u672a\u6765\u6708\u7403\u57fa\u5730\u5efa\u8bbe\u548c\u8d44\u6e90\u52d8\u63a2\u4efb\u52a1\u3002"}}
{"id": "2509.19480", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19480", "abs": "https://arxiv.org/abs/2509.19480", "authors": ["Noriaki Hirose", "Catherine Glossop", "Dhruv Shah", "Sergey Levine"], "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation", "comment": "9 pages, 7 figures, 6 tables", "summary": "Humans can flexibly interpret and compose different goal specifications, such\nas language instructions, spatial coordinates, or visual references, when\nnavigating to a destination. In contrast, most existing robotic navigation\npolicies are trained on a single modality, limiting their adaptability to\nreal-world scenarios where different forms of goal specification are natural\nand complementary. In this work, we present a training framework for robotic\nfoundation models that enables omni-modal goal conditioning for vision-based\nnavigation. Our approach leverages a high-capacity vision-language-action (VLA)\nbackbone and trains with three primary goal modalities: 2D poses, egocentric\nimages, and natural language, as well as their combinations, through a\nrandomized modality fusion strategy. This design not only expands the pool of\nusable datasets but also encourages the policy to develop richer geometric,\nsemantic, and visual representations. The resulting model, OmniVLA, achieves\nstrong generalization to unseen environments, robustness to scarce modalities,\nand the ability to follow novel natural language instructions. We demonstrate\nthat OmniVLA outperforms specialist baselines across modalities and offers a\nflexible foundation for fine-tuning to new modalities and tasks. We believe\nOmniVLA provides a step toward broadly generalizable and flexible navigation\npolicies, and a scalable path for building omni-modal robotic foundation\nmodels. We present videos showcasing OmniVLA performance and will release its\ncheckpoints and training code on our project page.", "AI": {"tldr": "\u63d0\u51fa\u4e86OmniVLA\u8bad\u7ec3\u6846\u67b6\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5168\u6a21\u6001\u76ee\u6807\u6761\u4ef6\u5316\uff0c\u652f\u6301\u8bed\u8a00\u6307\u4ee4\u3001\u7a7a\u95f4\u5750\u6807\u548c\u89c6\u89c9\u53c2\u8003\u7b49\u591a\u79cd\u76ee\u6807\u6307\u5b9a\u65b9\u5f0f", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5bfc\u822a\u7b56\u7565\u901a\u5e38\u53ea\u9488\u5bf9\u5355\u4e00\u6a21\u6001\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u800c\u4eba\u7c7b\u80fd\u591f\u7075\u6d3b\u7406\u89e3\u591a\u79cd\u76ee\u6807\u6307\u5b9a\u65b9\u5f0f", "method": "\u4f7f\u7528\u9ad8\u5bb9\u91cf\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u968f\u673a\u6a21\u6001\u878d\u5408\u7b56\u7565\u8bad\u7ec32D\u4f4d\u59ff\u3001\u7b2c\u4e00\u4eba\u79f0\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u4e09\u79cd\u4e3b\u8981\u76ee\u6807\u6a21\u6001\u53ca\u5176\u7ec4\u5408", "result": "OmniVLA\u6a21\u578b\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u7a00\u7f3a\u6a21\u6001\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u9075\u5faa\u65b0\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u4f18\u4e8e\u5404\u6a21\u6001\u7684\u4e13\u7528\u57fa\u7ebf", "conclusion": "OmniVLA\u4e3a\u6784\u5efa\u5e7f\u6cdb\u6cdb\u5316\u548c\u7075\u6d3b\u7684\u5bfc\u822a\u7b56\u7565\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u6784\u5efa\u5168\u6a21\u6001\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84"}}
{"id": "2509.19486", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19486", "abs": "https://arxiv.org/abs/2509.19486", "authors": ["Kieran S. Lachmansingh", "Jos\u00e9 R. Gonz\u00e1lez-Estrada", "Ryan E. Grant", "Matthew K. X. J. Pan"], "title": "Supercomputing for High-speed Avoidance and Reactive Planning in Robots", "comment": "8 pages, 3 figures", "summary": "This paper presents SHARP (Supercomputing for High-speed Avoidance and\nReactive Planning), a proof-of-concept study demonstrating how high-performance\ncomputing (HPC) can enable millisecond-scale responsiveness in robotic control.\nWhile modern robots face increasing demands for reactivity in human--robot\nshared workspaces, onboard processors are constrained by size, power, and cost.\nOffloading to HPC offers massive parallelism for trajectory planning, but its\nfeasibility for real-time robotics remains uncertain due to network latency and\njitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator\nmust dodge high-speed foam projectiles. Using a parallelized multi-goal A*\nsearch implemented with MPI on both local and remote HPC clusters, the system\nachieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300\nkm away), with avoidance success rates of 84% and 88%, respectively. These\nresults show that when round-trip latency remains within the\ntens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,\nenabling avoidance well below human reaction times. The SHARP results motivate\nhybrid control architectures: low-level reflexes remain onboard for safety,\nwhile bursty, high-throughput planning tasks are offloaded to HPC for\nscalability. By reporting per-stage timing and success rates, this study\nprovides a reproducible template for assessing real-time feasibility of\nHPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable\npathway toward dependable, reactive robots in dynamic environments.", "AI": {"tldr": "SHARP\u662f\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u9ad8\u6027\u80fd\u8ba1\u7b97\u5982\u4f55\u5b9e\u73b0\u673a\u5668\u4eba\u63a7\u5236\u7684\u6beb\u79d2\u7ea7\u54cd\u5e94\u3002\u901a\u8fc7\u5c06\u8f68\u8ff9\u89c4\u5212\u4efb\u52a1\u5378\u8f7d\u5230HPC\u96c6\u7fa4\uff0c\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u8eb2\u907f\u9ad8\u901f\u6ce1\u6cab\u5f39\u7684\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e8622.9ms\uff08\u672c\u5730\uff09\u548c30.0ms\uff08\u8fdc\u7a0b\uff09\u7684\u5e73\u5747\u89c4\u5212\u5ef6\u8fdf\uff0c\u6210\u529f\u7387\u5206\u522b\u8fbe\u523084%\u548c88%\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u4eba\u5728\u4eba\u673a\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u5bf9\u53cd\u5e94\u6027\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u673a\u8f7d\u5904\u7406\u5668\u53d7\u9650\u4e8e\u5c3a\u5bf8\u3001\u529f\u8017\u548c\u6210\u672c\u3002HPC\u5378\u8f7d\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\uff0c\u4f46\u5176\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u56e0\u7f51\u7edc\u5ef6\u8fdf\u548c\u6296\u52a8\u800c\u4e0d\u786e\u5b9a\u3002", "method": "\u4f7f\u7528MPI\u5728\u672c\u5730\u548c\u8fdc\u7a0bHPC\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u5e76\u884c\u5316\u591a\u76ee\u6807A*\u641c\u7d22\u7b97\u6cd5\uff0c\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u8eb2\u907f\u9ad8\u901f\u6ce1\u6cab\u5f39\u7684\u5e94\u529b\u6d4b\u8bd5\u573a\u666f\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5e73\u5747\u89c4\u5212\u5ef6\u8fdf22.9ms\uff08\u672c\u5730\uff09\u548c30.0ms\uff08\u8fdc\u7a0b\uff0c\u7ea6300\u516c\u91cc\u5916\uff09\uff0c\u56de\u907f\u6210\u529f\u7387\u5206\u522b\u4e3a84%\u548c88%\u3002\u5f53\u5f80\u8fd4\u5ef6\u8fdf\u4fdd\u6301\u5728\u6570\u5341\u6beb\u79d2\u8303\u56f4\u5185\u65f6\uff0cHPC\u4fa7\u8ba1\u7b97\u4e0d\u518d\u662f\u74f6\u9888\u3002", "conclusion": "SHARP\u7814\u7a76\u8bc1\u660eHPC\u5378\u8f7d\u662f\u5b9e\u73b0\u5728\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u9760\u3001\u53cd\u5e94\u7075\u654f\u673a\u5668\u4eba\u7684\u53ef\u884c\u9014\u5f84\uff0c\u652f\u6301\u6df7\u5408\u63a7\u5236\u67b6\u6784\uff1a\u4f4e\u7ea7\u53cd\u5c04\u4fdd\u6301\u673a\u8f7d\u4ee5\u786e\u4fdd\u5b89\u5168\uff0c\u800c\u7a81\u53d1\u6027\u3001\u9ad8\u541e\u5410\u91cf\u7684\u89c4\u5212\u4efb\u52a1\u5378\u8f7d\u5230HPC\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.19521", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19521", "abs": "https://arxiv.org/abs/2509.19521", "authors": ["Najeeb Ahmed Bhuiyan", "M. Nasimul Huq", "Sakib H. Chowdhury", "Rahul Mangharam"], "title": "A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion", "comment": "12 pages, 11 figures", "summary": "Gesture-based control for mobile manipulators faces persistent challenges in\nreliability, efficiency, and intuitiveness. This paper presents a dual-hand\ngesture interface that integrates TinyML, spectral analysis, and sensor fusion\nwithin a ROS framework to address these limitations. The system uses left-hand\ntilt and finger flexion, captured using accelerometer and flex sensors, for\nmobile base navigation, while right-hand IMU signals are processed through\nspectral analysis and classified by a lightweight neural network. This pipeline\nenables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3\nmanipulator. By supporting simultaneous navigation and manipulation, the\nframework improves efficiency and coordination compared to sequential methods.\nKey contributions include a bimanual control architecture, real-time low-power\ngesture recognition, robust multimodal sensor fusion, and a scalable ROS-based\nimplementation. The proposed approach advances Human-Robot Interaction (HRI)\nfor industrial automation, assistive robotics, and hazardous environments,\noffering a cost-effective, open-source solution with strong potential for\nreal-world deployment and further optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u624b\u673a\u52bf\u63a7\u5236\u7684\u79fb\u52a8\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u901a\u8fc7TinyML\u3001\u9891\u8c31\u5206\u6790\u548c\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u540c\u65f6\u5bfc\u822a\u548c\u64cd\u4f5c\u7684\u53ef\u9760\u3001\u9ad8\u6548\u3001\u76f4\u89c2\u7684\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u68b0\u81c2\u624b\u52bf\u63a7\u5236\u5728\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u76f4\u89c2\u6027\u65b9\u9762\u7684\u6301\u7eed\u6311\u6218\uff0c\u4e3a\u5de5\u4e1a\u81ea\u52a8\u5316\u3001\u8f85\u52a9\u673a\u5668\u4eba\u548c\u5371\u9669\u73af\u5883\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u624b\u673a\u52bf\u754c\u9762\uff0c\u5de6\u624b\u503e\u659c\u548c\u624b\u6307\u5f2f\u66f2\u63a7\u5236\u79fb\u52a8\u5e95\u5ea7\u5bfc\u822a\uff0c\u53f3\u624bIMU\u4fe1\u53f7\u901a\u8fc7\u9891\u8c31\u5206\u6790\u548c\u8f7b\u91cf\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u63a7\u52367\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff0c\u5728ROS\u6846\u67b6\u4e0b\u5b9e\u73b0\u4f20\u611f\u5668\u878d\u5408\u548c\u5b9e\u65f6\u4f4e\u529f\u8017\u624b\u52bf\u8bc6\u522b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u652f\u6301\u540c\u65f6\u5bfc\u822a\u548c\u64cd\u4f5c\u7684\u53cc\u624b\u673a\u52bf\u63a7\u5236\u7cfb\u7edf\uff0c\u76f8\u6bd4\u987a\u5e8f\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u534f\u8c03\u6027\uff0c\u5177\u6709\u5b9e\u65f6\u6027\u3001\u4f4e\u529f\u8017\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u673a\u4ea4\u4e92\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u7b49\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u548c\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.19522", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19522", "abs": "https://arxiv.org/abs/2509.19522", "authors": ["Fabio Coelho", "Joao Victor T. Borges", "Paulo Padrao", "Jose Fuentes", "Ramon R. Costa", "Liu Hsu", "Leonardo Bobadilla"], "title": "Bioinspired SLAM Approach for Unmanned Surface Vehicle", "comment": null, "summary": "This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a\nbioinspired SLAM framework based on computational models of the rodent\nhippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based\nSLAM, suitable for GPS-denied environments. Our contributions include a\nROS2-based architecture, experimental results on new waterway datasets, and\ninsights into system parameter tuning. This work represents the first known\napplication of RatSLAM on USVs. The estimated trajectory was compared with\nground truth data using the Hausdorff distance. The results show that the\nalgorithm can generate a semimetric map with an error margin acceptable for\nmost robotic applications.", "AI": {"tldr": "OpenRatSLAM2\u662f\u4e00\u4e2a\u57fa\u4e8e\u556e\u9f7f\u52a8\u7269\u6d77\u9a6c\u4f53\u8ba1\u7b97\u6a21\u578b\u7684\u751f\u7269\u542f\u53d1\u5f0fSLAM\u6846\u67b6\u65b0\u7248\u672c\uff0c\u63d0\u4f9b\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u89c6\u89c9-\u60ef\u6027SLAM\uff0c\u9002\u7528\u4e8eGPS\u62d2\u6b62\u73af\u5883\u3002", "motivation": "\u5f00\u53d1\u9002\u7528\u4e8eGPS\u62d2\u6b62\u73af\u5883\u7684\u4f4e\u6210\u672cSLAM\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u65e0\u4eba\u6c34\u9762\u8247(USV)\u4e0a\u7684\u9996\u6b21\u5e94\u7528\u3002", "method": "\u91c7\u7528ROS2\u67b6\u6784\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u60ef\u6027\u6570\u636e\uff0c\u57fa\u4e8e\u751f\u7269\u542f\u53d1\u5f0f\u7b97\u6cd5\u8fdb\u884cSLAM\u3002", "result": "\u5728\u6c34\u9053\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u534a\u5ea6\u91cf\u5730\u56fe\uff0c\u8bef\u5dee\u8303\u56f4\u5728\u5927\u591a\u6570\u673a\u5668\u4eba\u5e94\u7528\u53ef\u63a5\u53d7\u8303\u56f4\u5185\u3002", "conclusion": "OpenRatSLAM2\u662f\u7b2c\u4e00\u4e2a\u5728USV\u4e0a\u5e94\u7528\u7684RatSLAM\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.19525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19525", "abs": "https://arxiv.org/abs/2509.19525", "authors": ["James Avtges", "Jake Ketchum", "Millicent Schlafly", "Helena Young", "Taekyoung Kim", "Allison Pinosky", "Ryan L. Truby", "Todd D. Murphey"], "title": "Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot", "comment": "Published at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "Closed-loop control remains an open challenge in soft robotics. The nonlinear\nresponses of soft actuators under dynamic loading conditions limit the use of\nanalytic models for soft robot control. Traditional methods of controlling soft\nrobots underutilize their configuration spaces to avoid nonlinearity,\nhysteresis, large deformations, and the risk of actuator damage. Furthermore,\nepisodic data-driven control approaches such as reinforcement learning (RL) are\ntraditionally limited by sample efficiency and inconsistency across\ninitializations. In this work, we demonstrate RL for reliably learning control\npolicies for dynamic balancing tasks in real-time single-shot hardware\ndeployments. We use a deformable Stewart platform constructed using parallel,\n3D-printed soft actuators based on motorized handed shearing auxetic (HSA)\nstructures. By introducing a curriculum learning approach based on expanding\nneighborhoods of a known equilibrium, we achieve reliable single-deployment\nbalancing at arbitrary coordinates. In addition to benchmarking the performance\nof model-based and model-free methods, we demonstrate that in a single\ndeployment, Maximum Diffusion RL is capable of learning dynamic balancing after\nhalf of the actuators are effectively disabled, by inducing buckling and by\nbreaking actuators with bolt cutters. Training occurs with no prior data, in as\nfast as 15 minutes, with performance nearly identical to the fully-intact\nplatform. Single-shot learning on hardware facilitates soft robotic systems\nreliably learning in the real world and will enable more diverse and capable\nsoft robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5355\u6b21\u90e8\u7f72\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8f6f\u673a\u5668\u4eba\u5b9e\u65f6\u52a8\u6001\u5e73\u8861\u63a7\u5236\uff0c\u80fd\u591f\u5728\u786c\u4ef6\u4e0a\u5feb\u901f\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u751a\u81f3\u5728\u90e8\u5206\u6267\u884c\u5668\u635f\u574f\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u7684\u95ed\u73af\u63a7\u5236\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u975e\u7ebf\u6027\u54cd\u5e94\u3001\u6ede\u540e\u6548\u5e94\u548c\u5927\u53d8\u5f62\u7b49\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u521d\u59cb\u5316\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u5df2\u77e5\u5e73\u8861\u70b9\u7684\u90bb\u57df\u6765\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u4f7f\u7528\u57fa\u4e8e\u7535\u673a\u9a71\u52a8\u624b\u6027\u526a\u5207\u62c9\u80c0\u7ed3\u6784\u76843D\u6253\u5370\u8f6f\u6267\u884c\u5668\u6784\u5efa\u53d8\u5f62Stewart\u5e73\u53f0\u3002", "result": "\u5728\u5355\u6b21\u90e8\u7f72\u4e2d\uff0c\u6700\u5927\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u572815\u5206\u949f\u5185\u5b66\u4e60\u52a8\u6001\u5e73\u8861\uff0c\u5373\u4f7f\u4e00\u534a\u6267\u884c\u5668\u88ab\u7834\u574f\u540e\u4ecd\u80fd\u4fdd\u6301\u4e0e\u5b8c\u6574\u5e73\u53f0\u51e0\u4e4e\u76f8\u540c\u7684\u6027\u80fd\u3002", "conclusion": "\u5355\u6b21\u786c\u4ef6\u5b66\u4e60\u4f7f\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u53ef\u9760\u5b66\u4e60\uff0c\u5c06\u4e3a\u5f00\u53d1\u66f4\u591a\u6837\u5316\u548c\u80fd\u529b\u66f4\u5f3a\u7684\u8f6f\u673a\u5668\u4eba\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.19541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19541", "abs": "https://arxiv.org/abs/2509.19541", "authors": ["Xuan Cao", "Yuxin Wu", "Michael L. Whittaker"], "title": "Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture", "comment": null, "summary": "Despite the rapidly growing applications of robots in industry, the use of\nrobots to automate tasks in scientific laboratories is less prolific due to\nlack of generalized methodologies and high cost of hardware. This paper focuses\non the automation of characterization tasks necessary for reducing cost while\nmaintaining generalization, and proposes a software architecture for building\nrobotic systems in scientific laboratory environment. A dual-layer (Socket.IO\nand ROS) action server design is the basic building block, which facilitates\nthe implementation of a web-based front end for user-friendly operations and\nthe use of ROS Behavior Tree for convenient task planning and execution. A\nrobotic platform for automating mineral and material sample characterization is\nbuilt upon the architecture, with an open source, low-cost three-axis computer\nnumerical control gantry system serving as the main robot. A handheld laser\ninduced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed\nadapter, enabling automated 2D chemical mapping. We demonstrate the utility of\nautomated chemical mapping by scanning of the surface of a spodumene-bearing\npegmatite core sample with a 1071-point dense hyperspectral map acquired at a\nrate of 1520 bits per second. Automated LIBS scanning enables controlled\nchemical quantification in the laboratory that complements field-based\nmeasurements acquired with the same handheld device, linking resource\nexploration and processing steps in the supply chain for lithium-based battery\nmaterials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8f6f\u4ef6\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u5c42\u7684\u52a8\u4f5c\u670d\u52a1\u5668\u8bbe\u8ba1\uff08Socket.IO\u548cROS\uff09\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8eweb\u7684\u7528\u6237\u53cb\u597d\u64cd\u4f5c\u754c\u9762\u548cROS\u884c\u4e3a\u6811\u7684\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u3002\u8be5\u67b6\u6784\u5e94\u7528\u4e8e\u77ff\u7269\u548c\u6750\u6599\u6837\u54c1\u8868\u5f81\u7684\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u7684\u4e09\u8f74\u6570\u63a7\u9f99\u95e8\u7cfb\u7edf\u4f5c\u4e3a\u4e3b\u8981\u673a\u5668\u4eba\uff0c\u5e76\u96c6\u6210\u4e86\u624b\u6301LIBS\u5206\u6790\u4eea\u8fdb\u884c\u81ea\u52a8\u53162D\u5316\u5b66\u6620\u5c04\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u4eba\u5728\u5de5\u4e1a\u4e2d\u5e94\u7528\u8fc5\u901f\u589e\u957f\uff0c\u4f46\u5728\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u4e2d\u7531\u4e8e\u7f3a\u4e4f\u901a\u7528\u65b9\u6cd5\u548c\u786c\u4ef6\u6210\u672c\u9ad8\uff0c\u673a\u5668\u4eba\u81ea\u52a8\u5316\u4efb\u52a1\u7684\u5e94\u7528\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u8868\u5f81\u4efb\u52a1\u6765\u964d\u4f4e\u6210\u672c\u5e76\u4fdd\u6301\u901a\u7528\u6027\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u7684\u52a8\u4f5c\u670d\u52a1\u5668\u8bbe\u8ba1\uff08Socket.IO\u548cROS\uff09\uff0c\u6784\u5efa\u57fa\u4e8eweb\u7684\u524d\u7aef\u754c\u9762\u548cROS\u884c\u4e3a\u6811\u7684\u4efb\u52a1\u89c4\u5212\u7cfb\u7edf\u3002\u4f7f\u7528\u5f00\u6e90\u4f4e\u6210\u672c\u7684\u4e09\u8f74\u6570\u63a7\u9f99\u95e8\u7cfb\u7edf\u4f5c\u4e3a\u4e3b\u8981\u673a\u5668\u4eba\uff0c\u901a\u8fc73D\u6253\u5370\u9002\u914d\u5668\u96c6\u6210\u624b\u6301LIBS\u5206\u6790\u4eea\uff0c\u5b9e\u73b0\u81ea\u52a8\u53162D\u5316\u5b66\u6620\u5c04\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u81ea\u52a8\u5316\u5316\u5b66\u6620\u5c04\u7cfb\u7edf\uff0c\u5728\u9502\u8f89\u77f3\u4f1f\u6676\u5ca9\u5ca9\u5fc3\u6837\u54c1\u8868\u9762\u8fdb\u884c\u4e861071\u4e2a\u70b9\u7684\u5bc6\u96c6\u9ad8\u5149\u8c31\u6620\u5c04\uff0c\u91c7\u96c6\u901f\u7387\u4e3a1520\u6bd4\u7279/\u79d2\u3002\u81ea\u52a8\u5316LIBS\u626b\u63cf\u5b9e\u73b0\u4e86\u5b9e\u9a8c\u5ba4\u4e2d\u53d7\u63a7\u7684\u5316\u5b66\u5b9a\u91cf\u5206\u6790\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u7cfb\u7edf\u80fd\u591f\u8865\u5145\u57fa\u4e8e\u73b0\u573a\u7684\u624b\u6301\u8bbe\u5907\u6d4b\u91cf\uff0c\u5c06\u9502\u57fa\u7535\u6c60\u6750\u6599\u4f9b\u5e94\u94fe\u4e2d\u7684\u8d44\u6e90\u52d8\u63a2\u548c\u52a0\u5de5\u6b65\u9aa4\u8054\u7cfb\u8d77\u6765\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u8868\u5f81\u5728\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.19545", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19545", "abs": "https://arxiv.org/abs/2509.19545", "authors": ["Min Dai", "Aaron D. Ames"], "title": "RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots", "comment": null, "summary": "We present RoMoCo, an open-source C++ toolbox for the synthesis and\nevaluation of reduced-order model-based planners and whole-body controllers for\nbipedal and humanoid robots. RoMoCo's modular architecture unifies\nstate-of-the-art planners and whole-body locomotion controllers under a\nconsistent API, enabling rapid prototyping and reproducible benchmarking. By\nleveraging reduced-order models for platform-agnostic gait generation, RoMoCo\nenables flexible controller design across diverse robots. We demonstrate its\nversatility and performance through extensive simulations on the Cassie,\nUnitree H1, and G1 robots, and validate its real-world efficacy with hardware\nexperiments on the Cassie and G1 humanoids.", "AI": {"tldr": "RoMoCo\u662f\u4e00\u4e2a\u5f00\u6e90\u7684C++\u5de5\u5177\u7bb1\uff0c\u7528\u4e8e\u5408\u6210\u548c\u8bc4\u4f30\u57fa\u4e8e\u964d\u9636\u6a21\u578b\u7684\u8db3\u5f0f\u673a\u5668\u4eba\u548c\u4eba\u5f62\u673a\u5668\u4eba\u7684\u89c4\u5212\u5668\u53ca\u5168\u8eab\u63a7\u5236\u5668\u3002", "motivation": "\u4e3a\u4e86\u7edf\u4e00\u6700\u5148\u8fdb\u7684\u89c4\u5212\u5668\u548c\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u652f\u6301\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u7684\u7075\u6d3b\u63a7\u5236\u5668\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5229\u7528\u964d\u9636\u6a21\u578b\u8fdb\u884c\u5e73\u53f0\u65e0\u5173\u7684\u6b65\u6001\u751f\u6210\uff0c\u63d0\u4f9b\u4e00\u81f4\u7684API\u63a5\u53e3\u3002", "result": "\u5728Cassie\u3001Unitree H1\u548cG1\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u4eff\u771f\u9a8c\u8bc1\uff0c\u5e76\u5728Cassie\u548cG1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "RoMoCo\u5c55\u793a\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u9ad8\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u8db3\u5f0f\u548c\u4eba\u5f62\u673a\u5668\u4eba\u7684\u89c4\u5212\u4e0e\u63a7\u5236\u5f00\u53d1\u3002"}}
{"id": "2509.19555", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19555", "abs": "https://arxiv.org/abs/2509.19555", "authors": ["Sankalp Agrawal", "Junwon Seo", "Kensuke Nakamura", "Ran Tian", "Andrea Bajcsy"], "title": "AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space", "comment": null, "summary": "Recent works have shown that foundational safe control methods, such as\nHamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space\nof world models. While this enables the synthesis of latent safety filters for\nhard-to-model vision-based tasks, they assume that the safety constraint is\nknown a priori and remains fixed during deployment, limiting the safety\nfilter's adaptability across scenarios. To address this, we propose\nconstraint-parameterized latent safety filters that can adapt to user-specified\nsafety constraints at runtime. Our key idea is to define safety constraints by\nconditioning on an encoding of an image that represents a constraint, using a\nlatent-space similarity measure. The notion of similarity to failure is aligned\nin a principled way through conformal calibration, which controls how closely\nthe system may approach the constraint representation. The parameterized safety\nfilter is trained entirely within the world model's imagination, treating any\nimage seen by the model as a potential test-time constraint, thereby enabling\nruntime adaptation to arbitrary safety constraints. In simulation and hardware\nexperiments on vision-based control tasks with a Franka manipulator, we show\nthat our method adapts at runtime by conditioning on the encoding of\nuser-specified constraint images, without sacrificing performance. Video\nresults can be found on https://any-safe.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u53c2\u6570\u5316\u7684\u6f5c\u5728\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u5b89\u5168\u7ea6\u675f\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5b89\u5168\u8fc7\u6ee4\u5668\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7ea6\u675f\u56fa\u5b9a\u4e0d\u53d8\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u6f5c\u5728\u5b89\u5168\u8fc7\u6ee4\u5668\u65b9\u6cd5\u5047\u8bbe\u5b89\u5168\u7ea6\u675f\u662f\u9884\u5148\u5df2\u77e5\u4e14\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u56fa\u5b9a\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5b89\u5168\u8fc7\u6ee4\u5668\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u57fa\u4e8e\u7ea6\u675f\u56fe\u50cf\u7f16\u7801\u7684\u5b89\u5168\u7ea6\u675f\uff0c\u4f7f\u7528\u6f5c\u5728\u7a7a\u95f4\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u4fdd\u5f62\u6821\u51c6\u6765\u5bf9\u9f50\u6545\u969c\u76f8\u4f3c\u6027\u6982\u5ff5\u3002\u5b89\u5168\u8fc7\u6ee4\u5668\u5b8c\u5168\u5728\u4e16\u754c\u6a21\u578b\u7684\u60f3\u8c61\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06\u6a21\u578b\u770b\u5230\u7684\u4efb\u4f55\u56fe\u50cf\u89c6\u4e3a\u6f5c\u5728\u7684\u6d4b\u8bd5\u65f6\u7ea6\u675f\u3002", "result": "\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u63a7\u5236\u4efb\u52a1\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u901a\u8fc7\u6761\u4ef6\u5316\u7528\u6237\u6307\u5b9a\u7684\u7ea6\u675f\u56fe\u50cf\u7f16\u7801\u6765\u9002\u5e94\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\u5b89\u5168\u7ea6\u675f\u7684\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\uff0c\u4e3a\u89c6\u89c9\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2509.19571", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19571", "abs": "https://arxiv.org/abs/2509.19571", "authors": ["Sacha Morin", "Kumaraditya Gupta", "Mahtab Sandhu", "Charlie Gauthier", "Francesco Argenziano", "Kirsty Ellis", "Liam Paull"], "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action", "comment": "Project page:\n  https://montrealrobotics.ca/agentic-scene-policies.github.io/", "summary": "Executing open-ended natural language queries is a core problem in robotics.\nWhile recent advances in imitation learning and vision-language-actions models\n(VLAs) have enabled promising end-to-end policies, these models struggle when\nfaced with complex instructions and new scenes. An alternative is to design an\nexplicit scene representation as a queryable interface between the robot and\nthe world, using query results to guide downstream motion planning. In this\nwork, we present Agentic Scene Policies (ASP), an agentic framework that\nleverages the advanced semantic, spatial, and affordance-based querying\ncapabilities of modern scene representations to implement a capable\nlanguage-conditioned robot policy. ASP can execute open-vocabulary queries in a\nzero-shot manner by explicitly reasoning about object affordances in the case\nof more complex skills. Through extensive experiments, we compare ASP with VLAs\non tabletop manipulation problems and showcase how ASP can tackle room-level\nqueries through affordance-guided navigation, and a scaled-up scene\nrepresentation. (Project page:\nhttps://montrealrobotics.ca/agentic-scene-policies.github.io/)", "AI": {"tldr": "ASP\u662f\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u8868\u793a\u7684\u667a\u80fd\u6846\u67b6\uff0c\u5229\u7528\u73b0\u4ee3\u573a\u666f\u8868\u793a\u7684\u8bed\u4e49\u3001\u7a7a\u95f4\u548c\u529f\u80fd\u67e5\u8be2\u80fd\u529b\u6765\u6267\u884c\u8bed\u8a00\u6761\u4ef6\u5316\u7684\u673a\u5668\u4eba\u7b56\u7565\uff0c\u80fd\u591f\u96f6\u6837\u672c\u5904\u7406\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u529f\u80fd\u63a8\u7406\u5904\u7406\u590d\u6742\u6280\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6267\u884c\u5f00\u653e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u95ee\u9898\uff0c\u514b\u670d\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6307\u4ee4\u548c\u65b0\u573a\u666f\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bbe\u8ba1\u663e\u5f0f\u573a\u666f\u8868\u793a\u4f5c\u4e3a\u673a\u5668\u4eba\u4e0e\u4e16\u754c\u4e4b\u95f4\u7684\u53ef\u67e5\u8be2\u63a5\u53e3\uff0c\u5229\u7528\u67e5\u8be2\u7ed3\u679c\u6307\u5bfc\u4e0b\u6e38\u8fd0\u52a8\u89c4\u5212\uff0c\u901a\u8fc7\u529f\u80fd\u63a8\u7406\u5904\u7406\u590d\u6742\u6280\u80fd\u3002", "result": "\u5728\u684c\u9762\u64cd\u4f5c\u95ee\u9898\u4e0a\u4e0eVLAs\u8fdb\u884c\u5e7f\u6cdb\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86ASP\u80fd\u591f\u901a\u8fc7\u529f\u80fd\u5f15\u5bfc\u5bfc\u822a\u5904\u7406\u623f\u95f4\u7ea7\u67e5\u8be2\uff0c\u5e76\u6269\u5c55\u573a\u666f\u8868\u793a\u3002", "conclusion": "ASP\u6846\u67b6\u901a\u8fc7\u5229\u7528\u73b0\u4ee3\u573a\u666f\u8868\u793a\u7684\u9ad8\u7ea7\u67e5\u8be2\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u8bed\u8a00\u6761\u4ef6\u5316\u673a\u5668\u4eba\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u548c\u590d\u6742\u6280\u80fd\u3002"}}
{"id": "2509.19573", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19573", "abs": "https://arxiv.org/abs/2509.19573", "authors": ["Zachary Olkin", "Kejun Li", "William D. Compton", "Aaron D. Ames"], "title": "Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning", "comment": "Submitted to ICRA 2026", "summary": "Achieving highly dynamic behaviors on humanoid robots, such as running,\nrequires controllers that are both robust and precise, and hence difficult to\ndesign. Classical control methods offer valuable insight into how such systems\ncan stabilize themselves, but synthesizing real-time controllers for nonlinear\nand hybrid dynamics remains challenging. Recently, reinforcement learning (RL)\nhas gained popularity for locomotion control due to its ability to handle these\ncomplex dynamics. In this work, we embed ideas from nonlinear control theory,\nspecifically control Lyapunov functions (CLFs), along with optimized dynamic\nreference trajectories into the reinforcement learning training process to\nshape the reward. This approach, CLF-RL, eliminates the need to handcraft and\ntune heuristic reward terms, while simultaneously encouraging certifiable\nstability and providing meaningful intermediate rewards to guide learning. By\ngrounding policy learning in dynamically feasible trajectories, we expand the\nrobot's dynamic capabilities and enable running that includes both flight and\nsingle support phases. The resulting policy operates reliably on a treadmill\nand in outdoor environments, demonstrating robustness to disturbances applied\nto the torso and feet. Moreover, it achieves accurate global reference tracking\nutilizing only on-board sensors, making a critical step toward integrating\nthese dynamic motions into a full autonomy stack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63a7\u5236Lyapunov\u51fd\u6570\uff08CLFs\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08CLF-RL\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u52a8\u6001\u8fd0\u52a8\u63a7\u5236\uff0c\u7279\u522b\u662f\u8dd1\u6b65\u884c\u4e3a\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u63a7\u5236\u7406\u8bba\u5d4c\u5165RL\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u6d88\u9664\u4e86\u624b\u52a8\u8bbe\u8ba1\u542f\u53d1\u5f0f\u5956\u52b1\u9879\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u53ef\u8bc1\u660e\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5b9e\u73b0\u9ad8\u5ea6\u52a8\u6001\u884c\u4e3a\uff08\u5982\u8dd1\u6b65\uff09\u9700\u8981\u65e2\u9c81\u68d2\u53c8\u7cbe\u786e\u7684\u63a7\u5236\u5668\uff0c\u4f46\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ebf\u6027\u548c\u6df7\u5408\u52a8\u529b\u5b66\u7cfb\u7edf\u65f6\u9762\u4e34\u6311\u6218\u3002\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u590d\u6742\u52a8\u529b\u5b66\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u3002", "method": "CLF-RL\u65b9\u6cd5\u5c06\u63a7\u5236Lyapunov\u51fd\u6570\u548c\u4f18\u5316\u7684\u52a8\u6001\u53c2\u8003\u8f68\u8ff9\u5d4c\u5165\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u7528\u4e8e\u5851\u9020\u5956\u52b1\u51fd\u6570\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\uff0c\u6269\u5c55\u4e86\u673a\u5668\u4eba\u7684\u52a8\u6001\u80fd\u529b\u3002", "result": "\u751f\u6210\u7684\u7b56\u7565\u5728\u8dd1\u6b65\u673a\u548c\u5ba4\u5916\u73af\u5883\u4e2d\u53ef\u9760\u8fd0\u884c\uff0c\u5bf9\u8eaf\u5e72\u548c\u811a\u90e8\u65bd\u52a0\u7684\u5e72\u6270\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002\u4ec5\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u5668\u5c31\u80fd\u5b9e\u73b0\u51c6\u786e\u7684\u5168\u5c40\u53c2\u8003\u8ddf\u8e2a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u52a8\u6001\u8fd0\u52a8\u96c6\u6210\u5230\u5b8c\u6574\u81ea\u4e3b\u5806\u6808\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u63a7\u5236\u7406\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.19579", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19579", "abs": "https://arxiv.org/abs/2509.19579", "authors": ["Chad R. Samuelson", "Abigail Austin", "Seth Knoop", "Blake Romrell", "Gabriel R. Slade", "Timothy W. McLain", "Joshua G. Mangelson"], "title": "Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping", "comment": null, "summary": "Outdoor intelligent autonomous robotic operation relies on a sufficiently\nexpressive map of the environment. Classical geometric mapping methods retain\nessential structural environment information, but lack a semantic understanding\nand organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)\naddress this limitation by integrating geometric, topological, and semantic\nrelationships into a multi-level graph-based map. Outdoor autonomous operations\ncommonly rely on terrain information either due to task-dependence or the\ntraversability of the robotic platform. We propose a novel approach that\ncombines indoor 3DSG techniques with standard outdoor geometric mapping and\nterrain-aware reasoning, producing terrain-aware place nodes and hierarchically\norganized regions for outdoor environments. Our method generates a\ntask-agnostic metric-semantic sparse map and constructs a 3DSG from this map\nfor downstream planning tasks, all while remaining lightweight for autonomous\nrobotic operation. Our thorough evaluation demonstrates our 3DSG method\nperforms on par with state-of-the-art camera-based 3DSG methods in object\nretrieval and surpasses them in region classification while remaining memory\nefficient. We demonstrate its effectiveness in diverse robotic tasks of object\nretrieval and region monitoring in both simulation and real-world environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5ba4\u51853D\u573a\u666f\u56fe\u6280\u672f\u4e0e\u6807\u51c6\u5ba4\u5916\u51e0\u4f55\u6620\u5c04\u548c\u5730\u5f62\u611f\u77e5\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u6237\u5916\u73af\u5883\u751f\u6210\u5730\u5f62\u611f\u77e5\u7684\u5730\u70b9\u8282\u70b9\u548c\u5206\u5c42\u7ec4\u7ec7\u533a\u57df\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u7684\u4efb\u52a1\u65e0\u5173\u5ea6\u91cf-\u8bed\u4e49\u7a00\u758f\u5730\u56fe\u548c3DSG\u3002", "motivation": "\u6237\u5916\u667a\u80fd\u81ea\u4e3b\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u8db3\u591f\u8868\u8fbe\u7684\u73af\u5883\u5730\u56fe\u3002\u4f20\u7edf\u51e0\u4f55\u5730\u56fe\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u548c\u7ec4\u7ec7\uff0c\u65e0\u6cd5\u652f\u6301\u9ad8\u7ea7\u673a\u5668\u4eba\u63a8\u7406\u30023D\u573a\u666f\u56fe\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u3001\u62d3\u6251\u548c\u8bed\u4e49\u5173\u7cfb\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u4f46\u9700\u8981\u9002\u5e94\u6237\u5916\u73af\u5883\u548c\u5730\u5f62\u4fe1\u606f\u9700\u6c42\u3002", "method": "\u5c06\u5ba4\u51853DSG\u6280\u672f\u4e0e\u6807\u51c6\u5ba4\u5916\u51e0\u4f55\u6620\u5c04\u548c\u5730\u5f62\u611f\u77e5\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u751f\u6210\u5730\u5f62\u611f\u77e5\u7684\u5730\u70b9\u8282\u70b9\u548c\u5206\u5c42\u7ec4\u7ec7\u533a\u57df\uff0c\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u5ea6\u91cf-\u8bed\u4e49\u7a00\u758f\u5730\u56fe\uff0c\u5e76\u4ece\u4e2d\u6784\u5efa3DSG\u7528\u4e8e\u4e0b\u6e38\u89c4\u5212\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u68c0\u7d22\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u76f8\u673a\u76843DSG\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u533a\u57df\u5206\u7c7b\u65b9\u9762\u8d85\u8fc7\u5b83\u4eec\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u9ad8\u6548\u3002\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5728\u7269\u4f53\u68c0\u7d22\u548c\u533a\u57df\u76d1\u63a7\u7b49\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c063DSG\u6280\u672f\u6269\u5c55\u5230\u6237\u5916\u73af\u5883\uff0c\u7ed3\u5408\u5730\u5f62\u611f\u77e5\u63a8\u7406\uff0c\u4e3a\u6237\u5916\u81ea\u4e3b\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u8bed\u4e49\u5730\u56fe\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19597", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19597", "abs": "https://arxiv.org/abs/2509.19597", "authors": ["Sander Tonkens", "Nikhil Uday Shinde", "Azra Begzadi\u0107", "Michael C. Yip", "Jorge Cort\u00e9s", "Sylvia L. Herbert"], "title": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting", "comment": "The first three authors contributed equally. This work has been\n  accepted for publication at the Conference on Robot Learning", "summary": "The widespread deployment of autonomous systems in safety-critical\nenvironments such as urban air mobility hinges on ensuring reliable,\nperformant, and safe operation under varying environmental conditions. One such\napproach, value function-based safety filters, minimally modifies a nominal\ncontroller to ensure safety. Recent advances leverage offline learned value\nfunctions to scale these safety filters to high-dimensional systems. However,\nthese methods assume detailed priors on all possible sources of model mismatch,\nin the form of disturbances in the environment -- information that is rarely\navailable in real world settings. Even in well-mapped environments like urban\ncanyons or industrial sites, drones encounter complex, spatially-varying\ndisturbances arising from payload-drone interaction, turbulent airflow, and\nother environmental factors. We introduce SPACE2TIME, which enables safe and\nadaptive deployment of offline-learned safety filters under unknown,\nspatially-varying disturbances. The key idea is to reparameterize spatial\nvariations in disturbance as temporal variations, enabling the use of\nprecomputed value functions during online operation. We validate SPACE2TIME on\na quadcopter through extensive simulations and hardware experiments,\ndemonstrating significant improvement over baselines.", "AI": {"tldr": "SPACE2TIME\u662f\u4e00\u79cd\u5b89\u5168\u6ee4\u6ce2\u5668\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4f7f\u7528\u79bb\u7ebf\u5b66\u4e60\u503c\u51fd\u6570\u7684\u5b89\u5168\u7cfb\u7edf\u4e2d\u5904\u7406\u672a\u77e5\u7684\u7a7a\u95f4\u53d8\u5316\u6270\u52a8\uff0c\u901a\u8fc7\u5c06\u7a7a\u95f4\u6270\u52a8\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u65f6\u95f4\u6270\u52a8\u6765\u5b9e\u73b0\u5b89\u5168\u81ea\u9002\u5e94\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u503c\u51fd\u6570\u7684\u5b89\u5168\u6ee4\u6ce2\u5668\u65b9\u6cd5\u5047\u8bbe\u5bf9\u6a21\u578b\u5931\u914d\u6709\u8be6\u7ec6\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f46\u73b0\u5b9e\u4e2d\u65e0\u4eba\u673a\u7b49\u81ea\u4e3b\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u4f1a\u9047\u5230\u96be\u4ee5\u9884\u77e5\u7684\u7a7a\u95f4\u53d8\u5316\u6270\u52a8\uff0c\u5982\u6e4d\u6d41\u3001\u8f7d\u8377\u4ea4\u4e92\u7b49\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u901a\u5e38\u4e0d\u53ef\u5f97\u3002", "method": "SPACE2TIME\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u7a7a\u95f4\u6270\u52a8\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u65f6\u95f4\u6270\u52a8\uff0c\u4f7f\u5f97\u5728\u5728\u7ebf\u64cd\u4f5c\u65f6\u80fd\u591f\u5229\u7528\u9884\u8ba1\u7b97\u7684\u503c\u51fd\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u672a\u77e5\u7a7a\u95f4\u53d8\u5316\u6270\u52a8\u7684\u81ea\u9002\u5e94\u5904\u7406\u3002", "result": "\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u7684\u5927\u91cf\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cSPACE2TIME\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u672a\u77e5\u7a7a\u95f4\u53d8\u5316\u6270\u52a8\u73af\u5883\u4e0b\u5b89\u5168\u90e8\u7f72\u79bb\u7ebf\u5b66\u4e60\u7684\u5b89\u5168\u6ee4\u6ce2\u5668\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.19610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19610", "abs": "https://arxiv.org/abs/2509.19610", "authors": ["Qingxi Meng", "Emiliano Flores", "Carlos Quintero-Pe\u00f1a", "Peizhu Qian", "Zachary Kingston", "Shannan K. Hamlin", "Vaibhav Unhelkar", "Lydia E. Kavraki"], "title": "Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots", "comment": "16 pages, 10 figures, under review", "summary": "In this work, we address the problem of planning robot motions for a\nhigh-degree-of-freedom (DoF) robot that effectively achieves a given perception\ntask while the robot and the perception target move in a dynamic environment.\nAchieving navigation and perception tasks simultaneously is challenging, as\nthese objectives often impose conflicting requirements. Existing methods that\ncompute motion under perception constraints fail to account for obstacles, are\ndesigned for low-DoF robots, or rely on simplified models of perception.\nFurthermore, in dynamic real-world environments, robots must replan and react\nquickly to changes and directly evaluating the quality of perception (e.g.,\nobject detection confidence) is often expensive or infeasible at runtime. This\nproblem is especially important in human-centered environments such as homes\nand hospitals, where effective perception is essential for safe and reliable\noperation. To address these challenges, we propose a GPU-parallelized\nperception-score-guided probabilistic roadmap planner with a neural surrogate\nmodel (PS-PRM). The planner explicitly incorporates the estimated quality of a\nperception task into motion planning for high-DoF robots. Our method uses a\nlearned model to approximate perception scores and leverages GPU parallelism to\nenable efficient online replanning in dynamic settings. We demonstrate that our\nplanner, evaluated on high-DoF robots, outperforms baseline methods in both\nstatic and dynamic environments in both simulation and real-robot experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u540c\u65f6\u5b8c\u6210\u5bfc\u822a\u548c\u611f\u77e5\u4efb\u52a1\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5PS-PRM\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528GPU\u5e76\u884c\u5316\u7684\u611f\u77e5\u8bc4\u5206\u5f15\u5bfc\u6982\u7387\u8def\u7ebf\u56fe\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\u4f30\u8ba1\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u540c\u65f6\u5b8c\u6210\u5bfc\u822a\u548c\u611f\u77e5\u4efb\u52a1\uff0c\u4f46\u8fd9\u4e24\u4e2a\u76ee\u6807\u5f80\u5f80\u5b58\u5728\u51b2\u7a81\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4e0d\u8003\u8651\u969c\u788d\u7269\uff0c\u8981\u4e48\u53ea\u9002\u7528\u4e8e\u4f4e\u81ea\u7531\u5ea6\u673a\u5668\u4eba\uff0c\u6216\u8005\u4f9d\u8d56\u7b80\u5316\u7684\u611f\u77e5\u6a21\u578b\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u5feb\u901f\u91cd\u65b0\u89c4\u5212\uff0c\u4f46\u76f4\u63a5\u8bc4\u4f30\u611f\u77e5\u8d28\u91cf\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\uff09\u5728\u8fd0\u884c\u65f6\u5f80\u5f80\u4ee3\u4ef7\u9ad8\u6602\u6216\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51fa\u4e86PS-PRM\u65b9\u6cd5\uff0c\u4f7f\u7528GPU\u5e76\u884c\u5316\u7684\u611f\u77e5\u8bc4\u5206\u5f15\u5bfc\u6982\u7387\u8def\u7ebf\u56fe\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\u6765\u8fd1\u4f3c\u611f\u77e5\u8bc4\u5206\u3002\u8be5\u65b9\u6cd5\u5c06\u611f\u77e5\u4efb\u52a1\u7684\u8d28\u91cf\u4f30\u8ba1\u663e\u5f0f\u5730\u7eb3\u5165\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u4e2d\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "PS-PRM\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u540c\u65f6\u5b8c\u6210\u5bfc\u822a\u548c\u611f\u77e5\u4efb\u52a1\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7GPU\u5e76\u884c\u5316\u548c\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5728\u7ebf\u91cd\u65b0\u89c4\u5212\u3002"}}
{"id": "2509.19626", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19626", "abs": "https://arxiv.org/abs/2509.19626", "authors": ["Ryan Punamiya", "Dhruv Patel", "Patcharapong Aphiwetsa", "Pranav Kuppili", "Lawrence Y. Zhu", "Simar Kareer", "Judy Hoffman", "Danfei Xu"], "title": "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) and Oral at Conference on Robot Learning (CoRL 2025)", "summary": "Egocentric human experience data presents a vast resource for scaling up\nend-to-end imitation learning for robotic manipulation. However, significant\ndomain gaps in visual appearance, sensor modalities, and kinematics between\nhuman and robot impede knowledge transfer. This paper presents EgoBridge, a\nunified co-training framework that explicitly aligns the policy latent spaces\nbetween human and robot data using domain adaptation. Through a measure of\ndiscrepancy on the joint policy latent features and actions based on Optimal\nTransport (OT), we learn observation representations that not only align\nbetween the human and robot domain but also preserve the action-relevant\ninformation critical for policy learning. EgoBridge achieves a significant\nabsolute policy success rate improvement by 44% over human-augmented\ncross-embodiment baselines in three real-world single-arm and bimanual\nmanipulation tasks. EgoBridge also generalizes to new objects, scenes, and\ntasks seen only in human data, where baselines fail entirely. Videos and\nadditional information can be found at https://ego-bridge.github.io", "AI": {"tldr": "EgoBridge\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u5bf9\u9f50\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u6570\u636e\u7684\u7b56\u7565\u6f5c\u5728\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u4eba\u7c7b\u81ea\u6211\u4e2d\u5fc3\u4f53\u9a8c\u6570\u636e\u6269\u5c55\u673a\u5668\u4eba\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\uff0c\u4f46\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u4e4b\u95f4\u5b58\u5728\u89c6\u89c9\u5916\u89c2\u3001\u4f20\u611f\u5668\u6a21\u6001\u548c\u8fd0\u52a8\u5b66\u65b9\u9762\u7684\u663e\u8457\u9886\u57df\u5dee\u8ddd\uff0c\u963b\u788d\u4e86\u77e5\u8bc6\u8fc1\u79fb\u3002", "method": "\u4f7f\u7528\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u5ea6\u91cf\u8054\u5408\u7b56\u7565\u6f5c\u5728\u7279\u5f81\u548c\u52a8\u4f5c\u7684\u5dee\u5f02\uff0c\u5b66\u4e60\u65e2\u80fd\u5728\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u9886\u57df\u95f4\u5bf9\u9f50\u53c8\u80fd\u4fdd\u7559\u7b56\u7565\u5b66\u4e60\u5173\u952e\u52a8\u4f5c\u76f8\u5173\u4fe1\u606f\u7684\u89c2\u5bdf\u8868\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u5355\u81c2\u548c\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cEgoBridge\u76f8\u6bd4\u4eba\u7c7b\u589e\u5f3a\u7684\u8de8\u5177\u8eab\u57fa\u7ebf\u5b9e\u73b0\u4e8644%\u7684\u7edd\u5bf9\u7b56\u7565\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4ec5\u5728\u4eba\u6570\u636e\u4e2d\u51fa\u73b0\u7684\u65b0\u7269\u4f53\u3001\u573a\u666f\u548c\u4efb\u52a1\u3002", "conclusion": "EgoBridge\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7684\u9886\u57df\u5bf9\u9f50\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4eba\u7c7b-\u673a\u5668\u4eba\u77e5\u8bc6\u8fc1\u79fb\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u5229\u7528\u4eba\u7c7b\u6570\u636e\u6269\u5c55\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.19636", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19636", "abs": "https://arxiv.org/abs/2509.19636", "authors": ["Mahmoud Ali", "Hassan Jardali", "Youwei Yu", "Durgakant Pushp", "Lantao Liu"], "title": "Minimalistic Autonomous Stack for High-Speed Time-Trial Racing", "comment": "The data associated with this paper is available at\n  https://doi.org/10.5281/zenodo.17187680", "summary": "Autonomous racing has seen significant advancements, driven by competitions\nsuch as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing\nLeague (A2RL). However, developing an autonomous racing stack for a full-scale\ncar is often constrained by limited access to dedicated test tracks,\nrestricting opportunities for real-world validation. While previous work\ntypically requires extended development cycles and significant track time, this\npaper introduces a minimalistic autonomous racing stack for high-speed\ntime-trial racing that emphasizes rapid deployment and efficient system\nintegration with minimal on-track testing. The proposed stack was validated on\nreal speedways, achieving a top speed of 206 km/h within just 11 hours'\npractice run on the track with 325 km in total. Additionally, we present the\nsystem performance analysis, including tracking accuracy, vehicle dynamics, and\nsafety considerations, offering insights for teams seeking to rapidly develop\nand deploy an autonomous racing stack with limited track access.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u7ea6\u7684\u81ea\u4e3b\u8d5b\u8f66\u7cfb\u7edf\uff0c\u65e8\u5728\u9ad8\u901f\u8ba1\u65f6\u8d5b\u4e2d\u5b9e\u73b0\u5feb\u901f\u90e8\u7f72\u548c\u9ad8\u6548\u7cfb\u7edf\u96c6\u6210\uff0c\u4ec5\u9700\u6781\u5c11\u7684\u8d5b\u9053\u6d4b\u8bd5\u65f6\u95f4\u3002\u8be5\u7cfb\u7edf\u5728\u771f\u5b9e\u8d5b\u9053\u4e0a\u9a8c\u8bc1\uff0c\u5728\u4ec511\u5c0f\u65f6\u7684\u8d5b\u9053\u7ec3\u4e60\u4e2d\u8fbe\u5230\u6700\u9ad8\u901f\u5ea6206\u516c\u91cc/\u5c0f\u65f6\u3002", "motivation": "\u5f00\u53d1\u5168\u5c3a\u5bf8\u81ea\u4e3b\u8d5b\u8f66\u7cfb\u7edf\u901a\u5e38\u53d7\u9650\u4e8e\u4e13\u7528\u6d4b\u8bd5\u8d5b\u9053\u7684\u6709\u9650\u8bbf\u95ee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u9a8c\u8bc1\u673a\u4f1a\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u957f\u5f00\u53d1\u5468\u671f\u548c\u5927\u91cf\u8d5b\u9053\u65f6\u95f4\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7b80\u7ea6\u7684\u81ea\u4e3b\u8d5b\u8f66\u5806\u6808\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u5feb\u901f\u90e8\u7f72\u548c\u9ad8\u6548\u7cfb\u7edf\u96c6\u6210\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u8d5b\u9053\u6d4b\u8bd5\u9700\u6c42\uff0c\u5728\u771f\u5b9e\u8d5b\u9053\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5305\u62ec\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u8f66\u8f86\u52a8\u529b\u5b66\u548c\u5b89\u5168\u8003\u8651\u7684\u7cfb\u7edf\u6027\u80fd\u5206\u6790\u3002", "result": "\u5728\u771f\u5b9e\u8d5b\u9053\u4e0a\u9a8c\u8bc1\u6210\u529f\uff0c\u603b\u884c\u9a76325\u516c\u91cc\uff0c\u5728\u4ec511\u5c0f\u65f6\u8d5b\u9053\u7ec3\u4e60\u4e2d\u8fbe\u5230\u6700\u9ad8\u901f\u5ea6206\u516c\u91cc/\u5c0f\u65f6\u3002\u7cfb\u7edf\u6027\u80fd\u5206\u6790\u663e\u793a\u826f\u597d\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u8f66\u8f86\u52a8\u529b\u5b66\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53d7\u8d5b\u9053\u8bbf\u95ee\u9650\u5236\u7684\u56e2\u961f\u63d0\u4f9b\u4e86\u5feb\u901f\u5f00\u53d1\u548c\u90e8\u7f72\u81ea\u4e3b\u8d5b\u8f66\u7cfb\u7edf\u7684\u53ef\u884c\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7b80\u7ea6\u8bbe\u8ba1\u548c\u9ad8\u6548\u96c6\u6210\u53ef\u4ee5\u5728\u6709\u9650\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u81ea\u4e3b\u8d5b\u8f66\u3002"}}
{"id": "2509.19658", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19658", "abs": "https://arxiv.org/abs/2509.19658", "authors": ["Youngju Yoo", "Jiaheng Hu", "Yifeng Zhu", "Bo Liu", "Qiang Liu", "Roberto Mart\u00edn-Mart\u00edn", "Peter Stone"], "title": "RoboSSM: Scalable In-context Imitation Learning via State-Space Models", "comment": "8 pages, 11 figures", "summary": "In-context imitation learning (ICIL) enables robots to learn tasks from\nprompts consisting of just a handful of demonstrations. By eliminating the need\nfor parameter updates at deployment time, this paradigm supports few-shot\nadaptation to novel tasks. However, recent ICIL methods rely on Transformers,\nwhich have computational limitations and tend to underperform when handling\nlonger prompts than those seen during training. In this work, we introduce\nRoboSSM, a scalable recipe for in-context imitation learning based on\nstate-space models (SSM). Specifically, RoboSSM replaces Transformers with\nLonghorn -- a state-of-the-art SSM that provides linear-time inference and\nstrong extrapolation capabilities, making it well-suited for long-context\nprompts. We evaluate our approach on the LIBERO benchmark and compare it\nagainst strong Transformer-based ICIL baselines. Experiments show that RoboSSM\nextrapolates effectively to varying numbers of in-context demonstrations,\nyields high performance on unseen tasks, and remains robust in long-horizon\nscenarios. These results highlight the potential of SSMs as an efficient and\nscalable backbone for ICIL. Our code is available at\nhttps://github.com/youngjuY/RoboSSM.", "AI": {"tldr": "RoboSSM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u53ef\u6269\u5c55\u4e0a\u4e0b\u6587\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528Longhorn\u66ff\u4ee3Transformer\u6765\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56Transformer\uff0c\u4f46Transformer\u5b58\u5728\u8ba1\u7b97\u9650\u5236\uff0c\u5728\u5904\u7406\u6bd4\u8bad\u7ec3\u65f6\u66f4\u957f\u7684\u63d0\u793a\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u80fd\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u3002", "method": "\u7528Longhorn\uff08\u4e00\u79cd\u5148\u8fdb\u7684SSM\uff09\u66ff\u4ee3Transformer\uff0c\u5229\u7528SSM\u7684\u7ebf\u6027\u65f6\u95f4\u63a8\u7406\u548c\u5f3a\u5916\u63a8\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u63d0\u793a\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRoboSSM\u80fd\u6709\u6548\u5916\u63a8\u5230\u4e0d\u540c\u6570\u91cf\u7684\u4e0a\u4e0b\u6587\u6f14\u793a\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u957f\u89c6\u91ce\u573a\u666f\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "SSM\u4f5c\u4e3aICIL\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u9aa8\u5e72\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cRoboSSM\u4e3a\u89e3\u51b3Transformer\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.19672", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.19672", "abs": "https://arxiv.org/abs/2509.19672", "authors": ["Dongzhe Zheng", "Wenjie Mei"], "title": "Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains", "comment": "Accepted by NeurIPS 2025", "summary": "Stochastic optimal control methods often struggle in complex non-convex\nlandscapes, frequently becoming trapped in local optima due to their inability\nto learn from historical trajectory data. This paper introduces\nMemory-Augmented Potential Field Theory, a unified mathematical framework that\nintegrates historical experience into stochastic optimal control. Our approach\ndynamically constructs memory-based potential fields that identify and encode\nkey topological features of the state space, enabling controllers to\nautomatically learn from past experiences and adapt their optimization\nstrategy. We provide a theoretical analysis showing that memory-augmented\npotential fields possess non-convex escape properties, asymptotic convergence\ncharacteristics, and computational efficiency. We implement this theoretical\nframework in a Memory-Augmented Model Predictive Path Integral (MPPI)\ncontroller that demonstrates significantly improved performance in challenging\nnon-convex environments. The framework represents a generalizable approach to\nexperience-based learning within control systems (especially robotic dynamics),\nenhancing their ability to navigate complex state spaces without requiring\nspecialized domain knowledge or extensive offline training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8bb0\u5fc6\u589e\u5f3a\u52bf\u573a\u7406\u8bba\uff0c\u901a\u8fc7\u6574\u5408\u5386\u53f2\u7ecf\u9a8c\u6765\u6539\u8fdb\u968f\u673a\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u5728\u590d\u6742\u975e\u51f8\u73af\u5883\u4e2d\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u968f\u673a\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u5728\u590d\u6742\u975e\u51f8\u73af\u5883\u4e2d\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u4ece\u5386\u53f2\u8f68\u8ff9\u6570\u636e\u4e2d\u5b66\u4e60\u7ecf\u9a8c\u3002", "method": "\u5f00\u53d1\u4e86\u8bb0\u5fc6\u589e\u5f3a\u52bf\u573a\u7406\u8bba\u6846\u67b6\uff0c\u52a8\u6001\u6784\u5efa\u57fa\u4e8e\u8bb0\u5fc6\u7684\u52bf\u573a\u6765\u8bc6\u522b\u548c\u7f16\u7801\u72b6\u6001\u7a7a\u95f4\u7684\u5173\u952e\u62d3\u6251\u7279\u5f81\uff0c\u4f7f\u63a7\u5236\u5668\u80fd\u591f\u4ece\u8fc7\u53bb\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u5e76\u8c03\u6574\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8bb0\u5fc6\u589e\u5f3a\u52bf\u573a\u5177\u6709\u975e\u51f8\u9003\u9038\u7279\u6027\u3001\u6e10\u8fd1\u6536\u655b\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u5728\u8bb0\u5fc6\u589e\u5f3a\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u5668\u4e2d\u7684\u5b9e\u73b0\u663e\u793a\u5728\u6311\u6218\u6027\u975e\u51f8\u73af\u5883\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63a7\u5236\u7cfb\u7edf\uff08\u7279\u522b\u662f\u673a\u5668\u4eba\u52a8\u529b\u5b66\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u57fa\u4e8e\u7ecf\u9a8c\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u5728\u590d\u6742\u72b6\u6001\u7a7a\u95f4\u4e2d\u5bfc\u822a\u7684\u80fd\u529b\uff0c\u65e0\u9700\u4e13\u95e8\u9886\u57df\u77e5\u8bc6\u6216\u5927\u91cf\u79bb\u7ebf\u8bad\u7ec3\u3002"}}
{"id": "2509.19688", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.19688", "abs": "https://arxiv.org/abs/2509.19688", "authors": ["Devesh Nath", "Haoran Yin", "Glen Chou"], "title": "Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization", "comment": "10 pages, 12 figures", "summary": "We present a method for formal safety verification of learning-based\ngenerative motion planners. Generative motion planners (GMPs) offer advantages\nover traditional planners, but verifying the safety and dynamic feasibility of\ntheir outputs is difficult since neural network verification (NNV) tools scale\nonly to a few hundred neurons, while GMPs often contain millions. To preserve\nGMP expressiveness while enabling verification, our key insight is to imitate\nthe GMP by stabilizing references sampled from the GMP with a small neural\ntracking controller and then applying NNV to the closed-loop dynamics. This\nyields reachable sets that rigorously certify closed-loop safety, while the\ncontroller enforces dynamic feasibility. Building on this, we construct a\nlibrary of verified GMP references and deploy them online in a way that\nimitates the original GMP distribution whenever it is safe to do so, improving\nsafety without retraining. We evaluate across diverse planners, including\ndiffusion, flow matching, and vision-language models, improving safety in\nsimulation (on ground robots and quadcopters) and on hardware\n(differential-drive robot).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u751f\u6210\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u5f62\u5f0f\u5316\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u578b\u795e\u7ecf\u8ddf\u8e2a\u63a7\u5236\u5668\u7a33\u5b9aGMP\u751f\u6210\u7684\u53c2\u8003\u8f68\u8ff9\uff0c\u7136\u540e\u5bf9\u95ed\u73af\u52a8\u529b\u5b66\u5e94\u7528\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u6765\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "motivation": "\u751f\u6210\u8fd0\u52a8\u89c4\u5212\u5668(GMPs)\u6bd4\u4f20\u7edf\u89c4\u5212\u5668\u6709\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u5de5\u5177\u53ea\u80fd\u5904\u7406\u51e0\u767e\u4e2a\u795e\u7ecf\u5143\uff0c\u800cGMPs\u901a\u5e38\u5305\u542b\u6570\u767e\u4e07\u53c2\u6570\uff0c\u56e0\u6b64\u9a8c\u8bc1\u5176\u8f93\u51fa\u7684\u5b89\u5168\u6027\u548c\u52a8\u6001\u53ef\u884c\u6027\u5f88\u56f0\u96be\u3002", "method": "\u5173\u952e\u601d\u8def\u662f\u901a\u8fc7\u5c0f\u578b\u795e\u7ecf\u8ddf\u8e2a\u63a7\u5236\u5668\u6a21\u4effGMP\u5e76\u7a33\u5b9a\u5176\u91c7\u6837\u7684\u53c2\u8003\u8f68\u8ff9\uff0c\u7136\u540e\u5bf9\u95ed\u73af\u52a8\u529b\u5b66\u5e94\u7528\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\uff0c\u83b7\u5f97\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u53ef\u8fbe\u96c6\u3002\u6784\u5efa\u5df2\u9a8c\u8bc1\u7684GMP\u53c2\u8003\u5e93\uff0c\u5728\u7ebf\u90e8\u7f72\u65f6\u5c3d\u53ef\u80fd\u6a21\u4eff\u539f\u59cbGMP\u5206\u5e03\u3002", "result": "\u5728\u591a\u79cd\u89c4\u5212\u5668\uff08\u6269\u6563\u6a21\u578b\u3001\u6d41\u5339\u914d\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u4e0a\u8bc4\u4f30\uff0c\u5728\u4eff\u771f\uff08\u5730\u9762\u673a\u5668\u4eba\u548c\u56db\u65cb\u7ffc\uff09\u548c\u786c\u4ef6\uff08\u5dee\u901f\u9a71\u52a8\u673a\u5668\u4eba\uff09\u4e0a\u5747\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301GMP\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u9ad8\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u7c7b\u578b\u7684\u751f\u6210\u8fd0\u52a8\u89c4\u5212\u5668\u3002"}}
{"id": "2509.19696", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19696", "abs": "https://arxiv.org/abs/2509.19696", "authors": ["Noah Geiger", "Tamim Asfour", "Neville Hogan", "Johannes Lachner"], "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks", "comment": "15 pages, 12 figures", "summary": "Learning methods excel at motion generation in the information domain but are\nnot primarily designed for physical interaction in the energy domain. Impedance\nControl shapes physical interaction but requires task-aware tuning by selecting\nfeasible impedance parameters. We present Diffusion-Based Impedance Learning, a\nframework that combines both domains. A Transformer-based Diffusion Model with\ncross-attention to external wrenches reconstructs a simulated Zero-Force\nTrajectory (sZFT). This captures both translational and rotational task-space\nbehavior. For rotations, we introduce a novel SLERP-based quaternion noise\nscheduler that ensures geometric consistency. The reconstructed sZFT is then\npassed to an energy-based estimator that updates stiffness and damping\nparameters. A directional rule is applied that reduces impedance along non task\naxes while preserving rigidity along task directions. Training data were\ncollected for a parkour scenario and robotic-assisted therapy tasks using\nteleoperation with Apple Vision Pro. With only tens of thousands of samples,\nthe model achieved sub-millimeter positional accuracy and sub-degree rotational\naccuracy. Its compact model size enabled real-time torque control and\nautonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller\nachieved smooth parkour traversal within force and velocity limits and 30/30\nsuccess rates for cylindrical, square, and star peg insertions without any\npeg-specific demonstrations in the training data set. All code for the\nTransformer-based Diffusion Model, the robot controller, and the Apple Vision\nPro telemanipulation framework is publicly available. These results mark an\nimportant step towards Physical AI, fusing model-based control for physical\ninteraction with learning-based methods for trajectory generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86Diffusion-Based Impedance Learning\u6846\u67b6\uff0c\u7ed3\u5408\u5b66\u4e60\u65b9\u6cd5\u548c\u963b\u6297\u63a7\u5236\uff0c\u901a\u8fc7Transformer-based Diffusion Model\u91cd\u5efa\u6a21\u62df\u96f6\u529b\u8f68\u8ff9\uff0c\u5b9e\u73b0\u5b9e\u65f6\u626d\u77e9\u63a7\u5236\u548c\u81ea\u4e3b\u521a\u5ea6\u9002\u5e94\u3002", "motivation": "\u5b66\u4e60\u65b9\u6cd5\u64c5\u957f\u8fd0\u52a8\u751f\u6210\u4f46\u4e0d\u9002\u5408\u7269\u7406\u4ea4\u4e92\uff0c\u963b\u6297\u63a7\u5236\u9700\u8981\u4efb\u52a1\u611f\u77e5\u8c03\u53c2\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u4e2a\u9886\u57df\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u91cd\u5efa\u6a21\u62df\u96f6\u529b\u8f68\u8ff9\uff0c\u5f15\u5165SLERP-based\u56db\u5143\u6570\u566a\u58f0\u8c03\u5ea6\u5668\u4fdd\u8bc1\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u80fd\u91cf\u57fa\u4f30\u8ba1\u5668\u66f4\u65b0\u521a\u5ea6\u548c\u963b\u5c3c\u53c2\u6570\u3002", "result": "\u5728KUKA LBR iiwa\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e9a\u6beb\u7c73\u4f4d\u7f6e\u7cbe\u5ea6\u548c\u4e9a\u5ea6\u65cb\u8f6c\u7cbe\u5ea6\uff0c\u6210\u529f\u5b8c\u6210\u5706\u67f1\u3001\u65b9\u5f62\u548c\u661f\u5f62\u5b54\u63d2\u5165\u4efb\u52a1\uff0c30/30\u6210\u529f\u7387\u3002", "conclusion": "\u8fd9\u662f\u5b9e\u73b0\u7269\u7406AI\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u878d\u5408\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u7269\u7406\u4ea4\u4e92\u63a7\u5236\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2509.19712", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19712", "abs": "https://arxiv.org/abs/2509.19712", "authors": ["Liquan Wang", "Jiangjie Bian", "Eric Heiden", "Animesh Garg"], "title": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies", "comment": null, "summary": "Robotic manipulation tasks involving cutting deformable objects remain\nchallenging due to complex topological behaviors, difficulties in perceiving\ndense object states, and the lack of efficient evaluation methods for cutting\noutcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for\nmulti-step robotic cutting tasks that integrates a cutting environment and\ngeneralized policy learning. TopoCut is built upon three core components: (1)\nWe introduce a high-fidelity simulation environment based on a particle-based\nelastoplastic solver with compliant von Mises constitutive models, augmented by\na novel damage-driven topology discovery mechanism that enables accurate\ntracking of multiple cutting pieces. (2) We develop a comprehensive reward\ndesign that integrates the topology discovery with a pose-invariant spectral\nreward model based on Laplace-Beltrami eigenanalysis, facilitating consistent\nand robust assessment of cutting quality. (3) We propose an integrated policy\nlearning pipeline, where a dynamics-informed perception module predicts\ntopological evolution and produces particle-wise, topology-aware embeddings to\nsupport PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for\ngoal-conditioned policy learning. Extensive experiments demonstrate that\nTopoCut supports trajectory generation, scalable learning, precise evaluation,\nand strong generalization across diverse object geometries, scales, poses, and\ncutting goals.", "AI": {"tldr": "TopoCut\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6b65\u9aa4\u673a\u5668\u4eba\u5207\u5272\u4efb\u52a1\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u3001\u62d3\u6251\u611f\u77e5\u5956\u52b1\u8bbe\u8ba1\u548c\u96c6\u6210\u7b56\u7565\u5b66\u4e60\u6765\u89e3\u51b3\u53ef\u53d8\u5f62\u7269\u4f53\u5207\u5272\u7684\u6311\u6218\u3002", "motivation": "\u53ef\u53d8\u5f62\u7269\u4f53\u5207\u5272\u4efb\u52a1\u9762\u4e34\u590d\u6742\u62d3\u6251\u884c\u4e3a\u3001\u5bc6\u96c6\u72b6\u6001\u611f\u77e5\u56f0\u96be\u548c\u5207\u5272\u7ed3\u679c\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u652f\u6301\u673a\u5668\u4eba\u5207\u5272\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u7c92\u5b50\u5f39\u6027\u5851\u6027\u6c42\u89e3\u5668\u6784\u5efa\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\uff0c\u5f15\u5165\u635f\u4f24\u9a71\u52a8\u7684\u62d3\u6251\u53d1\u73b0\u673a\u5236\uff1b\u8bbe\u8ba1\u96c6\u6210\u4e86\u62d3\u6251\u53d1\u73b0\u548c\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7279\u5f81\u5206\u6790\u7684\u8c31\u5956\u52b1\u6a21\u578b\uff1b\u5f00\u53d1\u4e86\u52a8\u6001\u611f\u77e5\u7684\u611f\u77e5\u6a21\u5757\u548c\u57fa\u4e8e\u7c92\u5b50\u5206\u6570\u71b5\u79bb\u6563\u6269\u6563\u7b56\u7565\u7684\u96c6\u6210\u7b56\u7565\u5b66\u4e60\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTopoCut\u652f\u6301\u8f68\u8ff9\u751f\u6210\u3001\u53ef\u6269\u5c55\u5b66\u4e60\u3001\u7cbe\u786e\u8bc4\u4f30\uff0c\u5e76\u5728\u4e0d\u540c\u7269\u4f53\u51e0\u4f55\u3001\u5c3a\u5ea6\u3001\u59ff\u6001\u548c\u5207\u5272\u76ee\u6807\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TopoCut\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u5207\u5272\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5de5\u5177\u548c\u6846\u67b6\u3002"}}
{"id": "2509.19725", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19725", "abs": "https://arxiv.org/abs/2509.19725", "authors": ["Naveed D. Riaziat", "Joseph Chen", "Axel Krieger", "Jeremy D. Brown"], "title": "Towards Autonomous Robotic Electrosurgery via Thermal Imaging", "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Electrosurgery is a surgical technique that can improve tissue cutting by\nreducing cutting force and bleeding. However, electrosurgery adds a risk of\nthermal injury to surrounding tissue. Expert surgeons estimate desirable\ncutting velocities based on experience but have no quantifiable reference to\nindicate if a particular velocity is optimal. Furthermore, prior demonstrations\nof autonomous electrosurgery have primarily used constant tool velocity, which\nis not robust to changes in electrosurgical tissue characteristics, power\nsettings, or tool type. Thermal imaging feedback provides information that can\nbe used to reduce thermal injury while balancing cutting force by controlling\ntool velocity. We introduce Thermography for Electrosurgical Rate Modulation\nvia Optimization (ThERMO) to autonomously reduce thermal injury while balancing\ncutting force by intelligently controlling tool velocity. We demonstrate ThERMO\nin tissue phantoms and compare its performance to the constant velocity\napproach. Overall, ThERMO improves cut success rate by a factor of three and\ncan reduce peak cutting force by a factor of two. ThERMO responds to varying\nenvironmental disturbances, reduces damage to tissue, and completes cutting\ntasks that would otherwise result in catastrophic failure for the constant\nvelocity approach.", "AI": {"tldr": "ThERMO\u7cfb\u7edf\u901a\u8fc7\u70ed\u6210\u50cf\u53cd\u9988\u667a\u80fd\u63a7\u5236\u7535\u5916\u79d1\u624b\u672f\u5de5\u5177\u901f\u5ea6\uff0c\u5728\u51cf\u5c11\u70ed\u635f\u4f24\u7684\u540c\u65f6\u5e73\u8861\u5207\u5272\u529b\uff0c\u76f8\u6bd4\u6052\u5b9a\u901f\u5ea6\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u5207\u5272\u6210\u529f\u7387\u5e76\u964d\u4f4e\u5cf0\u503c\u5207\u5272\u529b\u3002", "motivation": "\u7535\u5916\u79d1\u624b\u672f\u867d\u7136\u80fd\u51cf\u5c11\u5207\u5272\u529b\u548c\u51fa\u8840\uff0c\u4f46\u5b58\u5728\u70ed\u635f\u4f24\u98ce\u9669\u3002\u4e13\u5bb6\u4f9d\u8d56\u7ecf\u9a8c\u4f30\u8ba1\u5207\u5272\u901f\u5ea6\uff0c\u7f3a\u4e4f\u91cf\u5316\u53c2\u8003\u3002\u73b0\u6709\u81ea\u4e3b\u7535\u5916\u79d1\u7cfb\u7edf\u4f7f\u7528\u6052\u5b9a\u901f\u5ea6\uff0c\u65e0\u6cd5\u9002\u5e94\u7ec4\u7ec7\u7279\u6027\u3001\u529f\u7387\u8bbe\u7f6e\u6216\u5de5\u5177\u7c7b\u578b\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51faThERMO\uff08\u57fa\u4e8e\u4f18\u5316\u7684\u70ed\u6210\u50cf\u7535\u5916\u79d1\u901f\u7387\u8c03\u5236\uff09\u7cfb\u7edf\uff0c\u5229\u7528\u70ed\u6210\u50cf\u53cd\u9988\u4fe1\u606f\u667a\u80fd\u63a7\u5236\u5de5\u5177\u901f\u5ea6\uff0c\u5728\u51cf\u5c11\u70ed\u635f\u4f24\u7684\u540c\u65f6\u5e73\u8861\u5207\u5272\u529b\u3002", "result": "\u5728\u7ec4\u7ec7\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cThERMO\u5c06\u5207\u5272\u6210\u529f\u7387\u63d0\u9ad8\u4e09\u500d\uff0c\u5cf0\u503c\u5207\u5272\u529b\u964d\u4f4e\u4e24\u500d\u3002\u7cfb\u7edf\u80fd\u54cd\u5e94\u73af\u5883\u53d8\u5316\uff0c\u51cf\u5c11\u7ec4\u7ec7\u635f\u4f24\uff0c\u5b8c\u6210\u6052\u5b9a\u901f\u5ea6\u65b9\u6cd5\u4f1a\u5931\u8d25\u7684\u4efb\u52a1\u3002", "conclusion": "ThERMO\u7cfb\u7edf\u901a\u8fc7\u70ed\u6210\u50cf\u53cd\u9988\u7684\u667a\u80fd\u901f\u5ea6\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u5916\u79d1\u624b\u672f\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u81ea\u4e3b\u7535\u5916\u79d1\u624b\u672f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19732", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19732", "abs": "https://arxiv.org/abs/2509.19732", "authors": ["Kyo Kutsuzawa", "Mitsuhiro Hayashibe"], "title": "Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering", "comment": "Accepted to The International Journal of Robotics Research (IJRR)", "summary": "Estimating the contact state between a grasped tool and the environment is\nessential for performing contact tasks such as assembly and object\nmanipulation. Force signals are valuable for estimating the contact state, as\nthey can be utilized even when the contact location is obscured by the tool.\nPrevious studies proposed methods for estimating contact positions using\nforce/torque signals; however, most methods require the geometry of the tool\nsurface to be known. Although several studies have proposed methods that do not\nrequire the tool shape, these methods require considerable time for estimation\nor are limited to tools with low-dimensional shape parameters. Here, we propose\na method for simultaneously estimating the contact position and tool shape,\nwhere the tool shape is represented by a grid, which is high-dimensional (more\nthan 1000 dimensional). The proposed method uses a particle filter in which\neach particle has individual tool shape parameters, thereby to avoid directly\nhandling a high-dimensional parameter space. The proposed method is evaluated\nthrough simulations and experiments using tools with curved shapes on a plane.\nConsequently, the proposed method can estimate the shape of the tool\nsimultaneously with the contact positions, making the contact-position\nestimation more accurate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540c\u65f6\u4f30\u8ba1\u63a5\u89e6\u4f4d\u7f6e\u548c\u5de5\u5177\u5f62\u72b6\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u7f51\u683c\u8868\u793a\u5de5\u5177\u5f62\u72b6\uff0c\u901a\u8fc7\u7c92\u5b50\u6ee4\u6ce2\u5668\u907f\u514d\u76f4\u63a5\u5904\u7406\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u5de5\u5177\u51e0\u4f55\u5f62\u72b6\u6216\u4f30\u8ba1\u65f6\u95f4\u8fc7\u957f\uff0c\u9650\u5236\u4e86\u5728\u5de5\u5177\u5f62\u72b6\u672a\u77e5\u6216\u590d\u6742\u60c5\u51b5\u4e0b\u7684\u5e94\u7528", "method": "\u4f7f\u7528\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u6bcf\u4e2a\u7c92\u5b50\u5177\u6709\u72ec\u7acb\u7684\u5de5\u5177\u5f62\u72b6\u53c2\u6570\uff0c\u901a\u8fc7\u529b/\u626d\u77e9\u4fe1\u53f7\u540c\u65f6\u4f30\u8ba1\u63a5\u89e6\u4f4d\u7f6e\u548c\u5de5\u5177\u5f62\u72b6", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u51c6\u786e\u4f30\u8ba1\u5de5\u5177\u5f62\u72b6\u548c\u63a5\u89e6\u4f4d\u7f6e\uff0c\u63d0\u9ad8\u63a5\u89e6\u4f4d\u7f6e\u4f30\u8ba1\u7684\u51c6\u786e\u6027", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u5de5\u5177\u5f62\u72b6\u53c2\u6570\uff0c\u5b9e\u73b0\u63a5\u89e6\u4f4d\u7f6e\u548c\u5de5\u5177\u5f62\u72b6\u7684\u540c\u6b65\u4f30\u8ba1\uff0c\u4e3a\u63a5\u89e6\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\u72b6\u6001\u611f\u77e5"}}
{"id": "2509.19734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19734", "abs": "https://arxiv.org/abs/2509.19734", "authors": ["Akshay Jaitly", "Jon Arrizabalaga", "Guanrui Li"], "title": "Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions", "comment": null, "summary": "Planning collision free trajectories in complex environments remains a core\nchallenge in robotics. Existing corridor based planners which rely on\ndecomposition of the free space into collision free subsets scale poorly with\nenvironmental complexity and require explicit allocations of time windows to\ntrajectory segments. We introduce a new trajectory parameterization that\nrepresents trajectories in a nonconvex collision free corridor as being in a\nconvex cartesian product of balls. This parameterization allows us to decouple\nproblem size from geometric complexity of the solution and naturally avoids\nexplicit time allocation by allowing trajectories to evolve continuously inside\nellipsoidal corridors. Building on this representation, we formulate the\nOrthogonal Trust Region Problem (Orth-TRP), a specialized convex program with\nseparable block constraints, and develop a solver that exploits this parallel\nstructure and the unique structure of each parallel subproblem for efficient\noptimization. Experiments on a quadrotor trajectory planning benchmark show\nthat our approach produces smoother trajectories and lower runtimes than\nstate-of-the-art corridor based planners, especially in highly complicated\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f68\u8ff9\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5c06\u975e\u51f8\u78b0\u649e\u81ea\u7531\u8d70\u5eca\u8868\u793a\u4e3a\u7403\u7684\u7b1b\u5361\u5c14\u79ef\u7684\u51f8\u96c6\uff0c\u4ece\u800c\u89e3\u8026\u95ee\u9898\u89c4\u6a21\u4e0e\u51e0\u4f55\u590d\u6742\u5ea6\uff0c\u5e76\u907f\u514d\u663e\u5f0f\u65f6\u95f4\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8d70\u5eca\u7684\u89c4\u5212\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u6269\u5c55\u6027\u5dee\uff0c\u9700\u8981\u663e\u5f0f\u5206\u914d\u65f6\u95f4\u7a97\u53e3\uff0c\u9650\u5236\u4e86\u5728\u9ad8\u5ea6\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165\u6b63\u4ea4\u4fe1\u4efb\u533a\u57df\u95ee\u9898\uff08Orth-TRP\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u53ef\u5206\u79bb\u5757\u7ea6\u675f\u7684\u4e13\u95e8\u51f8\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u5229\u7528\u5e76\u884c\u7ed3\u6784\u548c\u5b50\u95ee\u9898\u72ec\u7279\u7ed3\u6784\u7684\u9ad8\u6548\u6c42\u89e3\u5668\u3002", "result": "\u5728\u56db\u65cb\u7ffc\u8f68\u8ff9\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u8d70\u5eca\u7684\u89c4\u5212\u5668\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u8f68\u8ff9\u548c\u66f4\u4f4e\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5ea6\u590d\u6742\u7684\u73af\u5883\u4e2d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f68\u8ff9\u53c2\u6570\u5316\u548c\u6c42\u89e3\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.19752", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19752", "abs": "https://arxiv.org/abs/2509.19752", "authors": ["Rushuai Yang", "Hangxing Wei", "Ran Zhang", "Zhiyuan Feng", "Xiaoyu Chen", "Tong Li", "Chuheng Zhang", "Li Zhao", "Jiang Bian", "Xiu Su", "Yi Chen"], "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training", "comment": null, "summary": "Vision-language-action (VLA) models have shown strong generalization across\ntasks and embodiments; however, their reliance on large-scale human\ndemonstrations limits their scalability owing to the cost and effort of manual\ndata collection. Reinforcement learning (RL) offers a potential alternative to\ngenerate demonstrations autonomously, yet conventional RL algorithms often\nstruggle on long-horizon manipulation tasks with sparse rewards. In this paper,\nwe propose a modified diffusion policy optimization algorithm to generate\nhigh-quality and low-variance trajectories, which contributes to a diffusion\nRL-powered VLA training pipeline. Our algorithm benefits from not only the high\nexpressiveness of diffusion models to explore complex and diverse behaviors but\nalso the implicit regularization of the iterative denoising process to yield\nsmooth and consistent demonstrations. We evaluate our approach on the LIBERO\nbenchmark, which includes 130 long-horizon manipulation tasks, and show that\nthe generated trajectories are smoother and more consistent than both human\ndemonstrations and those from standard Gaussian RL policies. Further, training\na VLA model exclusively on the diffusion RL-generated data achieves an average\nsuccess rate of 81.9%, which outperforms the model trained on human data by\n+5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight\nour diffusion RL as an effective alternative for generating abundant,\nhigh-quality, and low-variance demonstrations for VLA models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6269\u6563\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4f4e\u65b9\u5dee\u7684\u8f68\u8ff9\uff0c\u6784\u5efa\u4e86\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684VLA\u8bad\u7ec3\u7ba1\u9053\u3002\u8be5\u65b9\u6cd5\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u751f\u6210\u7684\u8f68\u8ff9\u6bd4\u4eba\u7c7b\u6f14\u793a\u548c\u6807\u51c6\u9ad8\u65afRL\u7b56\u7565\u66f4\u5e73\u6ed1\u3001\u66f4\u4e00\u81f4\u3002", "motivation": "VLA\u6a21\u578b\u5728\u4efb\u52a1\u548c\u5b9e\u73b0\u65b9\u5f0f\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u7c7b\u6f14\u793a\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u81ea\u4e3b\u751f\u6210\u6f14\u793a\uff0c\u4f46\u4f20\u7edfRL\u7b97\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u6269\u6563\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8868\u8fbe\u80fd\u529b\u63a2\u7d22\u590d\u6742\u591a\u6837\u884c\u4e3a\uff0c\u540c\u65f6\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u7684\u9690\u5f0f\u6b63\u5219\u5316\u4ea7\u751f\u5e73\u6ed1\u4e00\u81f4\u7684\u6f14\u793a\u3002\u6784\u5efa\u6269\u6563RL\u9a71\u52a8\u7684VLA\u8bad\u7ec3\u7ba1\u9053\u3002", "result": "\u5728\u5305\u542b130\u4e2a\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6269\u6563RL\u751f\u6210\u7684\u8f68\u8ff9\u6bd4\u4eba\u7c7b\u6f14\u793a\u548c\u6807\u51c6\u9ad8\u65afRL\u7b56\u7565\u66f4\u5e73\u6ed1\u3001\u66f4\u4e00\u81f4\u3002\u4ec5\u4f7f\u7528\u6269\u6563RL\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684VLA\u6a21\u578b\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523081.9%\uff0c\u6bd4\u4eba\u7c7b\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u9ad85.3%\uff0c\u6bd4\u9ad8\u65afRL\u751f\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u9ad812.6%\u3002", "conclusion": "\u6269\u6563RL\u662f\u751f\u6210\u4e30\u5bcc\u3001\u9ad8\u8d28\u91cf\u3001\u4f4e\u65b9\u5deeVLA\u6a21\u578b\u6f14\u793a\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4eba\u7c7b\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u548c\u4f20\u7edfRL\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002"}}
{"id": "2509.19804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19804", "abs": "https://arxiv.org/abs/2509.19804", "authors": ["Sowoo Lee", "Dongyun Kang", "Jaehyun Park", "Hae-Won Park"], "title": "DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations", "comment": "8 pages", "summary": "This paper introduces DynaFlow, a novel framework that embeds a\ndifferentiable simulator directly into a flow matching model. By generating\ntrajectories in the action space and mapping them to dynamically feasible state\ntrajectories via the simulator, DynaFlow ensures all outputs are physically\nconsistent by construction. This end-to-end differentiable architecture enables\ntraining on state-only demonstrations, allowing the model to simultaneously\ngenerate physically consistent state trajectories while inferring the\nunderlying action sequences required to produce them. We demonstrate the\neffectiveness of our approach through quantitative evaluations and showcase its\nreal-world applicability by deploying the generated actions onto a physical Go1\nquadruped robot. The robot successfully reproduces diverse gait present in the\ndataset, executes long-horizon motions in open-loop control and translates\ninfeasible kinematic demonstrations into dynamically executable, stylistic\nbehaviors. These hardware experiments validate that DynaFlow produces\ndeployable, highly effective motions on real-world hardware from state-only\ndemonstrations, effectively bridging the gap between kinematic data and\nreal-world execution.", "AI": {"tldr": "DynaFlow\u662f\u4e00\u4e2a\u5c06\u53ef\u5fae\u5206\u6a21\u62df\u5668\u5d4c\u5165\u6d41\u5339\u914d\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u52a8\u4f5c\u7a7a\u95f4\u8f68\u8ff9\u5e76\u6620\u5c04\u5230\u52a8\u6001\u53ef\u884c\u7684\u72b6\u6001\u8f68\u8ff9\uff0c\u786e\u4fdd\u6240\u6709\u8f93\u51fa\u5728\u7269\u7406\u4e0a\u4e00\u81f4\u3002\u8be5\u6846\u67b6\u652f\u6301\u4ec5\u4f7f\u7528\u72b6\u6001\u6f14\u793a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u90e8\u7f72\u5230\u771f\u5b9e\u673a\u5668\u4eba\u7684\u52a8\u4f5c\u5e8f\u5217\u3002", "motivation": "\u89e3\u51b3\u4ece\u4ec5\u72b6\u6001\u6f14\u793a\u4e2d\u5b66\u4e60\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u5f25\u5408\u8fd0\u52a8\u5b66\u6570\u636e\u4e0e\u5b9e\u9645\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u751f\u6210\u7269\u7406\u4e00\u81f4\u4e14\u53ef\u90e8\u7f72\u7684\u8fd0\u52a8\u3002", "method": "\u5c06\u53ef\u5fae\u5206\u6a21\u62df\u5668\u5d4c\u5165\u6d41\u5339\u914d\u6a21\u578b\uff0c\u5728\u52a8\u4f5c\u7a7a\u95f4\u751f\u6210\u8f68\u8ff9\u5e76\u901a\u8fc7\u6a21\u62df\u5668\u6620\u5c04\u5230\u72b6\u6001\u8f68\u8ff9\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u8bad\u7ec3\uff0c\u652f\u6301\u4ec5\u72b6\u6001\u6f14\u793a\u5b66\u4e60\u3002", "result": "\u5728Go1\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u590d\u73b0\u4e86\u6570\u636e\u96c6\u4e2d\u7684\u591a\u79cd\u6b65\u6001\uff0c\u6267\u884c\u4e86\u957f\u671f\u5f00\u73af\u63a7\u5236\uff0c\u5e76\u5c06\u4e0d\u53ef\u884c\u7684\u8fd0\u52a8\u5b66\u6f14\u793a\u8f6c\u5316\u4e3a\u52a8\u6001\u53ef\u6267\u884c\u7684\u98ce\u683c\u5316\u884c\u4e3a\u3002", "conclusion": "DynaFlow\u80fd\u591f\u4ece\u4ec5\u72b6\u6001\u6f14\u793a\u4e2d\u751f\u6210\u53ef\u90e8\u7f72\u7684\u9ad8\u6548\u8fd0\u52a8\uff0c\u6709\u6548\u8fde\u63a5\u4e86\u8fd0\u52a8\u5b66\u6570\u636e\u4e0e\u5b9e\u9645\u786c\u4ef6\u6267\u884c\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.19851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19851", "abs": "https://arxiv.org/abs/2509.19851", "authors": ["Benjamin Bogenberger", "Oliver Harrison", "Orrin Dahanaggamaarachchi", "Lukas Brunke", "Jingxing Qian", "Siqi Zhou", "Angela P. Schoellig"], "title": "Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments", "comment": null, "summary": "Robots deployed in real-world environments, such as homes, must not only\nnavigate safely but also understand their surroundings and adapt to environment\nchanges. To perform tasks efficiently, they must build and maintain a semantic\nmap that accurately reflects the current state of the environment. Existing\nresearch on semantic exploration largely focuses on static scenes without\npersistent object-level instance tracking. A consistent map is, however,\ncrucial for real-world robotic applications where objects in the environment\ncan be removed, reintroduced, or shifted over time. In this work, to close this\ngap, we propose an open-vocabulary, semantic exploration system for semi-static\nenvironments. Our system maintains a consistent map by building a probabilistic\nmodel of object instance stationarity, systematically tracking semi-static\nchanges, and actively exploring areas that have not been visited for a\nprolonged period of time. In addition to active map maintenance, our approach\nleverages the map's semantic richness with LLM-based reasoning for\nopen-vocabulary object-goal navigation. This enables the robot to search more\nefficiently by prioritizing contextually relevant areas. We evaluate our\napproach across multiple real-world semi-static environments. Our system\ndetects 95% of map changes on average, improving efficiency by more than 29% as\ncompared to random and patrol baselines. Overall, our approach achieves a\nmapping precision within 2% of a fully rebuilt map while requiring\nsubstantially less exploration and further completes object goal navigation\ntasks about 14% faster than the next-best tested strategy (coverage\npatrolling). A video of our work can be found at\nhttp://tiny.cc/sem-explor-semi-static .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u534a\u9759\u6001\u73af\u5883\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u63a2\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u6982\u7387\u6a21\u578b\u8ddf\u8e2a\u5bf9\u8c61\u5b9e\u4f8b\u7684\u9759\u6001\u6027\uff0c\u4e3b\u52a8\u7ef4\u62a4\u4e00\u81f4\u6027\u5730\u56fe\uff0c\u5e76\u5229\u7528LLM\u63a8\u7406\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5bfc\u822a", "motivation": "\u73b0\u6709\u8bed\u4e49\u63a2\u7d22\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u573a\u666f\uff0c\u7f3a\u4e4f\u6301\u7eed\u7684\u5bf9\u8c61\u7ea7\u5b9e\u4f8b\u8ddf\u8e2a\u3002\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u4e2d\uff0c\u73af\u5883\u4e2d\u7684\u5bf9\u8c61\u4f1a\u968f\u65f6\u95f4\u88ab\u79fb\u9664\u3001\u91cd\u65b0\u5f15\u5165\u6216\u79fb\u52a8\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u53cd\u6620\u73af\u5883\u5f53\u524d\u72b6\u6001\u7684\u8bed\u4e49\u5730\u56fe", "method": "\u7cfb\u7edf\u901a\u8fc7\u6784\u5efa\u5bf9\u8c61\u5b9e\u4f8b\u9759\u6001\u6027\u7684\u6982\u7387\u6a21\u578b\uff0c\u7cfb\u7edf\u8ddf\u8e2a\u534a\u9759\u6001\u53d8\u5316\uff0c\u5e76\u4e3b\u52a8\u63a2\u7d22\u957f\u65f6\u95f4\u672a\u8bbf\u95ee\u7684\u533a\u57df\u6765\u7ef4\u62a4\u4e00\u81f4\u6027\u5730\u56fe\u3002\u540c\u65f6\u5229\u7528LLM\u63a8\u7406\u8fdb\u884c\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5bfc\u822a\uff0c\u901a\u8fc7\u4f18\u5148\u8003\u8651\u4e0a\u4e0b\u6587\u76f8\u5173\u533a\u57df\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u534a\u9759\u6001\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u7cfb\u7edf\u5e73\u5747\u68c0\u6d4b\u523095%\u7684\u5730\u56fe\u53d8\u5316\uff0c\u6548\u7387\u6bd4\u968f\u673a\u548c\u5de1\u903b\u57fa\u7ebf\u63d0\u9ad829%\u4ee5\u4e0a\u3002\u5730\u56fe\u7cbe\u5ea6\u4e0e\u5b8c\u5168\u91cd\u5efa\u5730\u56fe\u76f8\u5dee\u57282%\u4ee5\u5185\uff0c\u4f46\u9700\u8981\u66f4\u5c11\u7684\u63a2\u7d22\uff0c\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\u6bd4\u6b21\u4f18\u7b56\u7565\u5feb\u7ea614%", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u534a\u9759\u6001\u73af\u5883\u4e2d\u7684\u5730\u56fe\u7ef4\u62a4\u548c\u5bf9\u8c61\u5bfc\u822a\u95ee\u9898\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bed\u4e49\u63a2\u7d22\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.19853", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19853", "abs": "https://arxiv.org/abs/2509.19853", "authors": ["BinXu Wu", "TengFei Zhang", "Chen Yang", "JiaHao Wen", "HaoCheng Li", "JingTian Ma", "Zhen Chen", "JingYuan Wang"], "title": "SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process", "comment": null, "summary": "Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and\ncrucial in robotics. They often involve state ambiguity, where visually similar\nobservations correspond to different actions. We present SAGE, a state-aware\nguided imitation learning framework that models tasks as a Hidden Markov\nDecision Process (HMDP) to explicitly capture latent task stages and resolve\nambiguity. We instantiate the HMDP with a state transition network that infers\nhidden states, and a state-aware action policy that conditions on both\nobservations and hidden states to produce actions, thereby enabling\ndisambiguation across task stages. To reduce manual annotation effort, we\npropose a semi-automatic labeling pipeline combining active learning and soft\nlabel interpolation. In real-world experiments across multiple complex MSS\ntasks with state ambiguity, SAGE achieved 100% task success under the standard\nevaluation protocol, markedly surpassing the baselines. Ablation studies\nfurther show that such performance can be maintained with manual labeling for\nonly about 13% of the states, indicating its strong effectiveness.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u72b6\u6001\u611f\u77e5\u5f15\u5bfc\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9690\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u591a\u9636\u6bb5\u987a\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u89e3\u51b3\u72b6\u6001\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u8fbe\u5230100%\u4efb\u52a1\u6210\u529f\u7387", "motivation": "\u591a\u9636\u6bb5\u987a\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u666e\u904d\u5b58\u5728\u72b6\u6001\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5373\u89c6\u89c9\u76f8\u4f3c\u7684\u89c2\u5bdf\u5bf9\u5e94\u4e0d\u540c\u7684\u52a8\u4f5c\uff0c\u9700\u8981\u660e\u786e\u6355\u6349\u6f5c\u5728\u4efb\u52a1\u9636\u6bb5\u6765\u89e3\u51b3\u6a21\u7cca\u6027", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u9690\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5305\u542b\u72b6\u6001\u8f6c\u79fb\u7f51\u7edc\u63a8\u65ad\u9690\u85cf\u72b6\u6001\uff0c\u4ee5\u53ca\u72b6\u6001\u611f\u77e5\u52a8\u4f5c\u7b56\u7565\u57fa\u4e8e\u89c2\u5bdf\u548c\u9690\u85cf\u72b6\u6001\u751f\u6210\u52a8\u4f5c\uff1b\u63d0\u51fa\u534a\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u548c\u8f6f\u6807\u7b7e\u63d2\u503c", "result": "\u5728\u591a\u4e2a\u5177\u6709\u72b6\u6001\u6a21\u7cca\u6027\u7684\u590d\u6742\u591a\u9636\u6bb5\u987a\u5e8f\u4efb\u52a1\u4e2d\uff0cSAGE\u5728\u6807\u51c6\u8bc4\u4f30\u534f\u8bae\u4e0b\u8fbe\u5230100%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u4ec5\u9700\u624b\u52a8\u6807\u6ce8\u7ea613%\u7684\u72b6\u6001\u5373\u53ef\u7ef4\u6301\u6027\u80fd", "conclusion": "SAGE\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9690\u85cf\u72b6\u6001\u6709\u6548\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u72b6\u6001\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e14\u901a\u8fc7\u534a\u81ea\u52a8\u6807\u6ce8\u5927\u5927\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u5177\u6709\u5f3a\u6709\u6548\u6027"}}
{"id": "2509.19892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19892", "abs": "https://arxiv.org/abs/2509.19892", "authors": ["Keyu Wang", "Bingcong Lu", "Zhengxue Cheng", "Hengdi Zhang", "Li Song"], "title": "D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects", "comment": null, "summary": "Achieving diverse and stable dexterous grasping for general and deformable\nobjects remains a fundamental challenge in robotics, due to high-dimensional\naction spaces and uncertainty in perception. In this paper, we present D3Grasp,\na multimodal perception-guided reinforcement learning framework designed to\nenable Diverse and Deformable Dexterous Grasping. We firstly introduce a\nunified multimodal representation that integrates visual and tactile perception\nto robustly grasp common objects with diverse properties. Second, we propose an\nasymmetric reinforcement learning architecture that exploits privileged\ninformation during training while preserving deployment realism, enhancing both\ngeneralization and sample efficiency. Third, we meticulously design a training\nstrategy to synthesize contact-rich, penetration-free, and kinematically\nfeasible grasps with enhanced adaptability to deformable and contact-sensitive\nobjects. Extensive evaluations confirm that D3Grasp delivers highly robust\nperformance across large-scale and diverse object categories, and substantially\nadvances the state of the art in dexterous grasping for deformable and\ncompliant objects, even under perceptual uncertainty and real-world\ndisturbances. D3Grasp achieves an average success rate of 95.1% in real-world\ntrials,outperforming prior methods on both rigid and deformable objects\nbenchmarks.", "AI": {"tldr": "D3Grasp\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u611f\u77e5\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u591a\u6837\u5316\u548c\u7a33\u5b9a\u7684\u7075\u5de7\u6293\u53d6\uff0c\u7279\u522b\u9488\u5bf9\u53ef\u53d8\u5f62\u7269\u4f53\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u4e2d\u8fbe\u523095.1%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7075\u5de7\u6293\u53d6\u4e2d\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u548c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e00\u822c\u7269\u4f53\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u591a\u6837\u5316\u7a33\u5b9a\u6293\u53d6\u95ee\u9898\u3002", "method": "1\uff09\u5f15\u5165\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u6574\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u611f\u77e5\uff1b2\uff09\u63d0\u51fa\u975e\u5bf9\u79f0\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\u5229\u7528\u7279\u6743\u4fe1\u606f\uff1b3\uff09\u8bbe\u8ba1\u8bad\u7ec3\u7b56\u7565\u5408\u6210\u65e0\u7a7f\u900f\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u7684\u6293\u53d6\u52a8\u4f5c\u3002", "result": "\u5728\u5927\u89c4\u6a21\u591a\u6837\u5316\u7269\u4f53\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u9ad8\u5ea6\u9c81\u68d2\u6027\uff0c\u5728\u53ef\u53d8\u5f62\u548c\u67d4\u987a\u7269\u4f53\u7075\u5de7\u6293\u53d6\u65b9\u9762\u663e\u8457\u63a8\u8fdb\u4e86\u6280\u672f\u524d\u6cbf\uff0c\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u5e73\u5747\u6210\u529f\u738795.1%\u3002", "conclusion": "D3Grasp\u6846\u67b6\u5728\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u771f\u5b9e\u4e16\u754c\u5e72\u6270\u4e0b\uff0c\u5bf9\u521a\u6027\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u57fa\u51c6\u6d4b\u8bd5\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u7075\u5de7\u6293\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19916", "abs": "https://arxiv.org/abs/2509.19916", "authors": ["Zijun Che", "Yinghong Zhang", "Shengyi Liang", "Boyu Zhou", "Jun Ma", "Jinni Zhou"], "title": "GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference", "comment": null, "summary": "Autonomous exploration in structured and complex indoor environments remains\na challenging task, as existing methods often struggle to appropriately model\nunobserved space and plan globally efficient paths. To address these\nlimitations, we propose GUIDE, a novel exploration framework that\nsynergistically combines global graph inference with diffusion-based\ndecision-making. We introduce a region-evaluation global graph representation\nthat integrates both observed environmental data and predictions of unexplored\nareas, enhanced by a region-level evaluation mechanism to prioritize reliable\nstructural inferences while discounting uncertain predictions. Building upon\nthis enriched representation, a diffusion policy network generates stable,\nforesighted action sequences with significantly reduced denoising steps.\nExtensive simulations and real-world deployments demonstrate that GUIDE\nconsistently outperforms state-of-the-art methods, achieving up to 18.3% faster\ncoverage completion and a 34.9% reduction in redundant movements.", "AI": {"tldr": "GUIDE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u4e3b\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u56fe\u63a8\u7406\u548c\u6269\u6563\u51b3\u7b56\uff0c\u5728\u7ed3\u6784\u5316\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u672a\u89c2\u5bdf\u7a7a\u95f4\u548c\u89c4\u5212\u5168\u5c40\u9ad8\u6548\u8def\u5f84\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u533a\u57df\u8bc4\u4f30\u5168\u5c40\u56fe\u8868\u793a\uff0c\u6574\u5408\u89c2\u6d4b\u6570\u636e\u548c\u672a\u63a2\u7d22\u533a\u57df\u9884\u6d4b\uff1b\u4f7f\u7528\u6269\u6563\u7b56\u7565\u7f51\u7edc\u751f\u6210\u7a33\u5b9a\u7684\u524d\u77bb\u6027\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0cGUIDE\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5feb18.3%\u5b8c\u6210\u8986\u76d6\uff0c\u51cf\u5c1134.9%\u5197\u4f59\u8fd0\u52a8\u3002", "conclusion": "GUIDE\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u63a8\u7406\u548c\u6269\u6563\u51b3\u7b56\u7684\u534f\u540c\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u63a2\u7d22\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2509.19954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19954", "abs": "https://arxiv.org/abs/2509.19954", "authors": ["Pinhao Song", "Yurui Du", "Ophelie Saussus", "Sofie De Schrijver", "Irene Caprara", "Peter Janssen", "Renaud Detry"], "title": "Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation", "comment": "26 pages, 20 figures", "summary": "We propose a probabilistic shared-control solution for navigation, called\nRobot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe,\neffective assistance in human-robot interaction. RT-V2 jointly models a user's\nlong-term behavioral patterns and their noisy, low-dimensional control signals\nby combining a prior intent model with a posterior update that accounts for\nreal-time user input and environmental context. The prior captures the\nmultimodal and history-dependent nature of user intent using recurrent neural\nnetworks and conditional variational autoencoders, while the posterior\nintegrates this with uncertain user commands to infer desired actions. We\nconduct extensive experiments to validate RT-V2 across synthetic benchmarks,\nhuman-computer interaction studies with keyboard input, and brain-machine\ninterface experiments with non-human primates. Results show that RT-V2\noutperforms the state of the art in intent estimation, provides safe and\nefficient navigation support, and adequately balances user autonomy with\nassistive intervention. By unifying probabilistic modeling, reinforcement\nlearning, and safe optimization, RT-V2 offers a principled and generalizable\napproach to shared control for diverse assistive technologies.", "AI": {"tldr": "RT-V2\u662f\u4e00\u4e2a\u6982\u7387\u5171\u4eab\u63a7\u5236\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u5148\u9a8c\u610f\u56fe\u6a21\u578b\u548c\u540e\u9a8c\u66f4\u65b0\uff0c\u5b9e\u73b0\u51c6\u786e\u610f\u56fe\u9884\u6d4b\u548c\u5b89\u5168\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u8f85\u52a9", "motivation": "\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u610f\u56fe\u9884\u6d4b\u4e0d\u51c6\u786e\u3001\u8f85\u52a9\u4e0d\u5b89\u5168\u7684\u95ee\u9898\uff0c\u5e73\u8861\u7528\u6237\u81ea\u4e3b\u6027\u548c\u8f85\u52a9\u5e72\u9884", "method": "\u7ed3\u5408\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u591a\u6a21\u6001\u5386\u53f2\u4f9d\u8d56\u7684\u7528\u6237\u610f\u56fe\u5148\u9a8c\uff0c\u901a\u8fc7\u540e\u9a8c\u66f4\u65b0\u6574\u5408\u5b9e\u65f6\u7528\u6237\u8f93\u5165\u548c\u73af\u5883\u4e0a\u4e0b\u6587", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u3001\u4eba\u673a\u4ea4\u4e92\u5b9e\u9a8c\u548c\u8111\u673a\u63a5\u53e3\u5b9e\u9a8c\u4e2d\uff0cRT-V2\u5728\u610f\u56fe\u4f30\u8ba1\u3001\u5b89\u5168\u5bfc\u822a\u652f\u6301\u548c\u5e73\u8861\u7528\u6237\u81ea\u4e3b\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f", "conclusion": "RT-V2\u901a\u8fc7\u7edf\u4e00\u6982\u7387\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5b89\u5168\u4f18\u5316\uff0c\u4e3a\u591a\u6837\u5316\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u53ef\u63a8\u5e7f\u7684\u5171\u4eab\u63a7\u5236\u65b9\u6cd5"}}
{"id": "2509.19958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19958", "abs": "https://arxiv.org/abs/2509.19958", "authors": ["Alexander Spiridonov", "Jan-Nico Zaech", "Nikolay Nikolov", "Luc Van Gool", "Danda Pani Paudel"], "title": "Generalist Robot Manipulation beyond Action Labeled Data", "comment": "Accepted at Conference on Robot Learning 2025", "summary": "Recent advances in generalist robot manipulation leverage pre-trained\nVision-Language Models (VLMs) and large-scale robot demonstrations to tackle\ndiverse tasks in a zero-shot manner. A key challenge remains: scaling\nhigh-quality, action-labeled robot demonstration data, which existing methods\nrely on for robustness and generalization. To address this, we propose a method\nthat benefits from videos without action labels - featuring humans and/or\nrobots in action - enhancing open-vocabulary performance and enabling\ndata-efficient learning of new tasks. Our method extracts dense, dynamic 3D\npoint clouds at the hand or gripper location and uses a proposed 3D dynamics\npredictor for self-supervision. This predictor is then tuned to an action\npredictor using a smaller labeled dataset for action alignment. We show that\nour method not only learns from unlabeled human and robot demonstrations -\nimproving downstream generalist robot policies - but also enables robots to\nlearn new tasks without action labels (i.e., out-of-action generalization) in\nboth real-world and simulated settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u65e0\u52a8\u4f5c\u6807\u7b7e\u89c6\u9891\uff08\u5305\u542b\u4eba\u7c7b\u548c/\u6216\u673a\u5668\u4eba\u52a8\u4f5c\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u624b\u90e8\u6216\u5939\u722a\u4f4d\u7f6e\u7684\u5bc6\u96c6\u52a8\u60013D\u70b9\u4e91\uff0c\u5e76\u4f7f\u75283D\u52a8\u6001\u9884\u6d4b\u5668\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u9ad8\u5f00\u653e\u8bcd\u6c47\u6027\u80fd\u5e76\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684\u65b0\u4efb\u52a1\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u52a8\u4f5c\u6807\u8bb0\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6b64\u7c7b\u6570\u636e\u6765\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u53d6\u624b\u90e8/\u5939\u722a\u4f4d\u7f6e\u7684\u5bc6\u96c6\u52a8\u60013D\u70b9\u4e91\uff0c\u4f7f\u7528\u63d0\u51fa\u76843D\u52a8\u6001\u9884\u6d4b\u5668\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u7136\u540e\u4f7f\u7528\u8f83\u5c0f\u7684\u6807\u8bb0\u6570\u636e\u96c6\u5c06\u9884\u6d4b\u5668\u8c03\u6574\u4e3a\u52a8\u4f5c\u9884\u6d4b\u5668\u4ee5\u5b9e\u73b0\u52a8\u4f5c\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u4ece\u65e0\u6807\u8bb0\u7684\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u6f14\u793a\u4e2d\u5b66\u4e60\uff0c\u6539\u8fdb\u4e0b\u6e38\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\uff0c\u8fd8\u80fd\u4f7f\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u73af\u5883\u4e2d\u65e0\u9700\u52a8\u4f5c\u6807\u7b7e\u5373\u53ef\u5b66\u4e60\u65b0\u4efb\u52a1\uff08\u5373\u52a8\u4f5c\u5916\u6cdb\u5316\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u6807\u6ce8\u96be\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u4e2d\u5b66\u4e60\u5e76\u63d0\u5347\u673a\u5668\u4eba\u901a\u7528\u80fd\u529b\u7684\u76ee\u6807\u3002"}}
{"id": "2509.19972", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19972", "abs": "https://arxiv.org/abs/2509.19972", "authors": ["Albina Klepach", "Egor E. Nuzhin", "Alexey A. Tsukanov", "Nikolay V. Brilliantov"], "title": "An effective control of large systems of active particles: An application to evacuation problem", "comment": null, "summary": "Manipulation of large systems of active particles is a serious challenge\nacross diverse domains, including crowd management, control of robotic swarms,\nand coordinated material transport. The development of advanced control\nstrategies for complex scenarios is hindered, however, by the lack of\nscalability and robustness of the existing methods, in particular, due to the\nneed of an individual control for each agent. One possible solution involves\ncontrolling a system through a leader or a group of leaders, which other agents\ntend to follow. Using such an approach we develop an effective control strategy\nfor a leader, combining reinforcement learning (RL) with artificial forces\nacting on the system. To describe the guidance of active particles by a leader\nwe introduce the generalized Vicsek model. This novel method is then applied to\nthe problem of the effective evacuation by a robot-rescuer (leader) of large\ngroups of people from hazardous places. We demonstrate, that while a\nstraightforward application of RL yields suboptimal results, even for advanced\narchitectures, our approach provides a robust and efficient evacuation\nstrategy. The source code supporting this study is publicly available at:\nhttps://github.com/cinemere/evacuation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4eba\u5de5\u529b\u7684\u9886\u5bfc\u8005\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u6709\u6548\u5f15\u5bfc\u6d3b\u6027\u7c92\u5b50\u7cfb\u7edf\uff08\u5982\u4eba\u7fa4\uff09\u4ece\u5371\u9669\u533a\u57df\u758f\u6563\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u9700\u8981\u5bf9\u6bcf\u4e2a\u4e2a\u4f53\u8fdb\u884c\u5355\u72ec\u63a7\u5236\u3002\u901a\u8fc7\u9886\u5bfc\u8005\u63a7\u5236\u6574\u4e2a\u7cfb\u7edf\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u5e7f\u4e49Vicsek\u6a21\u578b\u63cf\u8ff0\u9886\u5bfc\u8005\u5f15\u5bfc\u6d3b\u6027\u7c92\u5b50\u7684\u884c\u4e3a\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u4eba\u5de5\u529b\u5f00\u53d1\u4e86\u6709\u6548\u7684\u9886\u5bfc\u8005\u63a7\u5236\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u6551\u63f4\u4eba\u5458\u5f15\u5bfc\u4eba\u7fa4\u758f\u6563\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u76f4\u63a5\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u758f\u6563\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u7684\u9886\u5bfc\u8005\u63a7\u5236\u7b56\u7565\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u6d3b\u6027\u7c92\u5b50\u7cfb\u7edf\u7684\u64cd\u63a7\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4eba\u7fa4\u758f\u6563\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.20009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20009", "abs": "https://arxiv.org/abs/2509.20009", "authors": ["Simon Sch\u00e4fer", "Bassam Alrifaee", "Ehsan Hashemi"], "title": "Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure", "comment": "21 pages, 9 figures, this work was submitted to Wileys'Advanced\n  Intelligent Systems for review", "summary": "This paper presents a lidar-only state estimation and tracking framework,\nalong with a roadside sensing unit for integration with existing urban\ninfrastructure. Urban deployments demand scalable, real-time tracking\nsolutions, yet traditional remote sensing remains costly and computationally\nintensive, especially under perceptually degraded conditions. Our sensor node\ncouples a single lidar with an edge computing unit and runs a computationally\nefficient, GPU-free observer that simultaneously estimates object state, class,\ndimensions, and existence probability. The pipeline performs: (i) state updates\nvia an extended Kalman filter, (ii) dimension estimation using a 1D\ngrid-map/Bayesian update, (iii) class updates via a lookup table driven by the\nmost probable footprint, and (iv) existence estimation from track age and\nbounding-box consistency. Experiments in dynamic urban-like scenes with diverse\ntraffic participants demonstrate real-time performance and high precision: The\ncomplete end-to-end pipeline finishes within \\SI{100}{\\milli\\second} for\n\\SI{99.88}{\\%} of messages, with an excellent detection rate. Robustness is\nfurther confirmed under simulated wind and sensor vibration. These results\nindicate that reliable, real-time roadside tracking is feasible on CPU-only\nedge hardware, enabling scalable, privacy-friendly deployments within existing\ncity infrastructure. The framework integrates with existing poles, traffic\nlights, and buildings, reducing deployment costs and simplifying large-scale\nurban rollouts and maintenance efforts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\u7684\u8def\u8fb9\u72b6\u6001\u4f30\u8ba1\u548c\u8ddf\u8e2a\u6846\u67b6\uff0c\u7ed3\u5408\u8fb9\u7f18\u8ba1\u7b97\u5355\u5143\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u7684\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u96c6\u6210\u65b9\u6848", "motivation": "\u4f20\u7edf\u8fdc\u7a0b\u611f\u77e5\u65b9\u6848\u6210\u672c\u9ad8\u3001\u8ba1\u7b97\u91cf\u5927\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u6761\u4ef6\u6076\u52a3\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u8def\u8fb9\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u72b6\u6001\u66f4\u65b0\uff0c1D\u7f51\u683c\u56fe/\u8d1d\u53f6\u65af\u66f4\u65b0\u8fdb\u884c\u5c3a\u5bf8\u4f30\u8ba1\uff0c\u67e5\u627e\u8868\u9a71\u52a8\u5206\u7c7b\u66f4\u65b0\uff0c\u57fa\u4e8e\u8ddf\u8e2a\u5e74\u9f84\u548c\u8fb9\u754c\u6846\u4e00\u81f4\u6027\u8fdb\u884c\u5b58\u5728\u6982\u7387\u4f30\u8ba1", "result": "\u5728\u52a8\u6001\u57ce\u5e02\u573a\u666f\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u7aef\u5230\u7aef\u6d41\u6c34\u7ebf99.88%\u7684\u6d88\u606f\u5904\u7406\u65f6\u95f4\u5c0f\u4e8e100\u6beb\u79d2\uff0c\u68c0\u6d4b\u7387\u9ad8\uff0c\u5728\u6a21\u62df\u98ce\u529b\u548c\u4f20\u611f\u5668\u632f\u52a8\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u5728\u4ec5\u4f7f\u7528CPU\u7684\u8fb9\u7f18\u786c\u4ef6\u4e0a\u53ef\u5b9e\u73b0\u53ef\u9760\u7684\u8def\u8fb9\u5b9e\u65f6\u8ddf\u8e2a\uff0c\u80fd\u591f\u4e0e\u73b0\u6709\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u96c6\u6210\uff0c\u964d\u4f4e\u90e8\u7f72\u6210\u672c\uff0c\u652f\u6301\u5927\u89c4\u6a21\u57ce\u5e02\u63a8\u5e7f"}}
{"id": "2509.20036", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20036", "abs": "https://arxiv.org/abs/2509.20036", "authors": ["Yinzhao Dong", "Ji Ma", "Liu Zhao", "Wanyue Li", "Peng Lu"], "title": "MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping", "comment": null, "summary": "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have\ndemonstrated impressive performance on challenging terrains, allowing robots to\nexecute complex skills such as climbing, running, and jumping. However,\nexisting blind locomotion controllers often struggle to ensure safety and\nefficient traversal through risky gap terrains, which are typically highly\ncomplex, requiring robots to perceive terrain information and select\nappropriate footholds during locomotion accurately. Meanwhile, existing\nperception-based controllers still present several practical limitations,\nincluding a complex multi-sensor deployment system and expensive computing\nresource requirements. This paper proposes a DRL controller named MAstering\nRisky Gap Terrains (MARG), which integrates terrain maps and proprioception to\ndynamically adjust the action and enhance the robot's stability in these tasks.\nDuring the training phase, our controller accelerates policy optimization by\nselectively incorporating privileged information (e.g., center of mass,\nfriction coefficients) that are available in simulation but unmeasurable\ndirectly in real-world deployments due to sensor limitations. We also designed\nthree foot-related rewards to encourage the robot to explore safe footholds.\nMore importantly, a terrain map generation (TMG) model is proposed to reduce\nthe drift existing in mapping and provide accurate terrain maps using only one\nLiDAR, providing a foundation for zero-shot transfer of the learned policy. The\nexperimental results indicate that MARG maintains stability in various risky\nterrain tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMARG\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u5371\u9669\u95f4\u9699\u5730\u5f62\u4e0a\u7684\u7a33\u5b9a\u8fd0\u52a8\uff0c\u901a\u8fc7\u7ed3\u5408\u5730\u5f62\u56fe\u548c\u672c\u4f53\u611f\u77e5\u6765\u52a8\u6001\u8c03\u6574\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5730\u5f62\u56fe\u751f\u6210\u6a21\u578b\u6765\u51cf\u5c11\u6620\u5c04\u6f02\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u76f2\u8fd0\u52a8\u63a7\u5236\u5668\u5728\u5371\u9669\u95f4\u9699\u5730\u5f62\u4e0a\u96be\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u548c\u9ad8\u6548\u7a7f\u8d8a\uff0c\u800c\u57fa\u4e8e\u611f\u77e5\u7684\u63a7\u5236\u5668\u5b58\u5728\u591a\u4f20\u611f\u5668\u90e8\u7f72\u590d\u6742\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "MARG\u63a7\u5236\u5668\u6574\u5408\u5730\u5f62\u56fe\u548c\u672c\u4f53\u611f\u77e5\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u9009\u62e9\u6027\u52a0\u5165\u7279\u6743\u4fe1\u606f\u52a0\u901f\u7b56\u7565\u4f18\u5316\uff0c\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u811a\u90e8\u76f8\u5173\u5956\u52b1\u6765\u9f13\u52b1\u63a2\u7d22\u5b89\u5168\u843d\u811a\u70b9\uff0c\u5e76\u63d0\u51fa\u5730\u5f62\u56fe\u751f\u6210\u6a21\u578b\u4ec5\u4f7f\u7528\u4e00\u4e2aLiDAR\u63d0\u4f9b\u51c6\u786e\u5730\u5f62\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMARG\u5728\u5404\u79cd\u5371\u9669\u5730\u5f62\u4efb\u52a1\u4e2d\u4fdd\u6301\u7a33\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u5b66\u4e60\u7b56\u7565\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "conclusion": "MARG\u63a7\u5236\u5668\u80fd\u591f\u6709\u6548\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u5371\u9669\u95f4\u9699\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u6311\u6218\uff0c\u901a\u8fc7\u7b80\u5316\u4f20\u611f\u5668\u90e8\u7f72\u548c\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6709\u6548\u8fc1\u79fb\u3002"}}
{"id": "2509.20070", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20070", "abs": "https://arxiv.org/abs/2509.20070", "authors": ["Abraham George", "Amir Barati Farimani"], "title": "LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs", "comment": "9 pages, 5 figures, 4 tables. Submitted to ICRA 2026", "summary": "We present LLM Trainer, a fully automated pipeline that leverages the world\nknowledge of Large Language Models (LLMs) to transform a small number of human\ndemonstrations (as few as one) into a large robot dataset for imitation\nlearning. Our approach decomposes demonstration generation into two steps: (1)\noffline demonstration annotation that extracts keyframes, salient objects, and\npose-object relations; and (2) online keypose retargeting that adapts those\nkeyframes to a new scene, given an initial observation. Using these modified\nkeypoints, our system warps the original demonstration to generate a new\ntrajectory, which is then executed, and the resulting demo, if successful, is\nsaved. Because the annotation is reusable across scenes, we use Thompson\nsampling to optimize the annotation, significantly improving generation success\nrate. We evaluate our method on a range of tasks, and find that our data\nannotation method consistently outperforms expert-engineered baselines. We\nfurther show an ensemble policy that combines the optimized LLM feed-forward\nplan with a learned feedback imitation learning controller. Finally, we\ndemonstrate hardware feasibility on a Franka Emika Panda robot. For additional\nmaterials and demonstration videos, please see the project website:\nhttps://sites.google.com/andrew.cmu.edu/llm-trainer", "AI": {"tldr": "LLM Trainer\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e16\u754c\u77e5\u8bc6\u5c06\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\u8f6c\u5316\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u7528\u4e8e\u6a21\u4eff\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u79bb\u7ebf\u6f14\u793a\u6807\u6ce8\u548c\u5728\u7ebf\u5173\u952e\u59ff\u6001\u91cd\u5b9a\u5411\uff0c\u80fd\u591f\u4ece\u4e00\u4e2a\u6f14\u793a\u751f\u6210\u591a\u4e2a\u65b0\u573a\u666f\u7684\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4eba\u7c7b\u6f14\u793a\u6570\u636e\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u83b7\u53d6\u3002LLM Trainer\u65e8\u5728\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u6f14\u793a\u6570\u636e\u6765\u964d\u4f4e\u6a21\u4eff\u5b66\u4e60\u7684\u6570\u636e\u9700\u6c42\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u6b65\uff1a\u79bb\u7ebf\u6f14\u793a\u6807\u6ce8\uff08\u63d0\u53d6\u5173\u952e\u5e27\u3001\u663e\u8457\u7269\u4f53\u548c\u59ff\u6001-\u7269\u4f53\u5173\u7cfb\uff09\u548c\u5728\u7ebf\u5173\u952e\u59ff\u6001\u91cd\u5b9a\u5411\uff08\u5c06\u5173\u952e\u5e27\u9002\u914d\u5230\u65b0\u573a\u666f\uff09\u3002\u4f7f\u7528Thompson\u91c7\u6837\u4f18\u5316\u6807\u6ce8\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u751f\u6210\u6210\u529f\u7387\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u7684\u6570\u636e\u6807\u6ce8\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u4e13\u5bb6\u8bbe\u8ba1\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u8fd8\u5c55\u793a\u4e86\u7ed3\u5408\u4f18\u5316\u7684LLM\u524d\u9988\u89c4\u5212\u548c\u5b66\u4e60\u7684\u53cd\u9988\u6a21\u4eff\u5b66\u4e60\u63a7\u5236\u5668\u7684\u96c6\u6210\u7b56\u7565\uff0c\u5e76\u5728Franka Emika Panda\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u786c\u4ef6\u53ef\u884c\u6027\u3002", "conclusion": "LLM Trainer\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u4eff\u5b66\u4e60\u5bf9\u5927\u91cf\u4eba\u7c7b\u6f14\u793a\u7684\u4f9d\u8d56\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20077", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.20077", "abs": "https://arxiv.org/abs/2509.20077", "authors": ["Xun Li", "Rodrigo Santa Cruz", "Mingze Xi", "Hu Zhang", "Madhawa Perera", "Ziwei Wang", "Ahalya Ravendran", "Brandon J. Matthews", "Feng Xu", "Matt Adcock", "Dadong Wang", "Jiajun Liu"], "title": "Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning", "comment": null, "summary": "To enable robots to comprehend high-level human instructions and perform\ncomplex tasks, a key challenge lies in achieving comprehensive scene\nunderstanding: interpreting and interacting with the 3D environment in a\nmeaningful way. This requires a smart map that fuses accurate geometric\nstructure with rich, human-understandable semantics. To address this, we\nintroduce the 3D Queryable Scene Representation (3D QSR), a novel framework\nbuilt on multimedia data that unifies three complementary 3D representations:\n(1) 3D-consistent novel view rendering and segmentation from panoptic\nreconstruction, (2) precise geometry from 3D point clouds, and (3) structured,\nscalable organization via 3D scene graphs. Built on an object-centric design,\nthe framework integrates with large vision-language models to enable semantic\nqueryability by linking multimodal object embeddings, and supporting\nobject-level retrieval of geometric, visual, and semantic information. The\nretrieved data are then loaded into a robotic task planner for downstream\nexecution. We evaluate our approach through simulated robotic task planning\nscenarios in Unity, guided by abstract language instructions and using the\nindoor public dataset Replica. Furthermore, we apply it in a digital duplicate\nof a real wet lab environment to test QSR-supported robotic task planning for\nemergency response. The results demonstrate the framework's ability to\nfacilitate scene understanding and integrate spatial and semantic reasoning,\neffectively translating high-level human instructions into precise robotic task\nplanning in complex 3D environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3D\u53ef\u67e5\u8be2\u573a\u666f\u8868\u793a\uff083D QSR\uff09\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u878d\u5408\u51e0\u4f55\u7ed3\u6784\u3001\u89c6\u89c9\u4fe1\u606f\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u9ad8\u7ea7\u4eba\u7c7b\u6307\u4ee4\u5e76\u5728\u590d\u67423D\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u673a\u5668\u4eba\u7406\u89e3\u9ad8\u7ea7\u4eba\u7c7b\u6307\u4ee4\u5e76\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0c\u5173\u952e\u5728\u4e8e\u5b9e\u73b0\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\uff1a\u4ee5\u6709\u610f\u4e49\u7684\u65b9\u5f0f\u89e3\u91ca\u548c\u4ea4\u4e923D\u73af\u5883\u3002\u8fd9\u9700\u8981\u4e00\u4e2a\u667a\u80fd\u5730\u56fe\uff0c\u5c06\u7cbe\u786e\u7684\u51e0\u4f55\u7ed3\u6784\u4e0e\u4e30\u5bcc\u7684\u4eba\u7c7b\u53ef\u7406\u89e3\u8bed\u4e49\u76f8\u878d\u5408\u3002", "method": "3D QSR\u6846\u67b6\u57fa\u4e8e\u591a\u5a92\u4f53\u6570\u636e\uff0c\u7edf\u4e00\u4e86\u4e09\u79cd\u4e92\u8865\u76843D\u8868\u793a\uff1a(1) \u5168\u666f\u91cd\u5efa\u76843D\u4e00\u81f4\u65b0\u89c6\u89d2\u6e32\u67d3\u548c\u5206\u5272\uff0c(2) 3D\u70b9\u4e91\u7684\u7cbe\u786e\u51e0\u4f55\u7ed3\u6784\uff0c(3) \u901a\u8fc73D\u573a\u666f\u56fe\u5b9e\u73b0\u7684\u7ed3\u6784\u5316\u3001\u53ef\u6269\u5c55\u7ec4\u7ec7\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\uff0c\u4e0e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96c6\u6210\uff0c\u901a\u8fc7\u94fe\u63a5\u591a\u6a21\u6001\u5bf9\u8c61\u5d4c\u5165\u5b9e\u73b0\u8bed\u4e49\u53ef\u67e5\u8be2\u6027\uff0c\u5e76\u652f\u6301\u51e0\u4f55\u3001\u89c6\u89c9\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u5bf9\u8c61\u7ea7\u68c0\u7d22\u3002", "result": "\u901a\u8fc7\u5728Unity\u4e2d\u7684\u6a21\u62df\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u573a\u666f\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528\u5ba4\u5185\u516c\u5171\u6570\u636e\u96c6Replica\uff0c\u5e76\u5728\u771f\u5b9e\u6e7f\u5b9e\u9a8c\u5ba4\u73af\u5883\u7684\u6570\u5b57\u526f\u672c\u4e2d\u6d4b\u8bd5QSR\u652f\u6301\u7684\u673a\u5668\u4eba\u5e94\u6025\u54cd\u5e94\u4efb\u52a1\u89c4\u5212\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4fc3\u8fdb\u573a\u666f\u7406\u89e3\uff0c\u6574\u5408\u7a7a\u95f4\u548c\u8bed\u4e49\u63a8\u7406\uff0c\u6709\u6548\u5c06\u9ad8\u7ea7\u4eba\u7c7b\u6307\u4ee4\u8f6c\u5316\u4e3a\u590d\u67423D\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u3002", "conclusion": "3D QSR\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u573a\u666f\u7406\u89e3\u548c\u8bed\u4e49\u63a8\u7406\u7684\u6574\u5408\uff0c\u4e3a\u673a\u5668\u4eba\u6267\u884c\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u548c\u8bed\u4e49\u7406\u89e3\u7684\u5e94\u6025\u54cd\u5e94\u7b49\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.20081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20081", "abs": "https://arxiv.org/abs/2509.20081", "authors": ["Jose E. Maese", "Luis Merino", "Fernando Caballero"], "title": "DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping", "comment": null, "summary": "This paper presents a high-efficiency, CPU-only volumetric mapping framework\nbased on a Truncated Signed Distance Field (TSDF). The system incrementally\nfuses raw LiDAR point-cloud data into a voxel grid using a directional\nbitmask-based integration scheme, producing dense and consistent TSDF\nrepresentations suitable for real-time 3D reconstruction. A key feature of the\napproach is that the processing time per point-cloud remains constant,\nregardless of the voxel grid resolution, enabling high resolution mapping\nwithout sacrificing runtime performance. In contrast to most recent TSDF/ESDF\nmethods that rely on GPU acceleration, our method operates entirely on CPU,\nachieving competitive results in speed. Experiments on real-world open datasets\ndemonstrate that the generated maps attain accuracy on par with contemporary\nmapping techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTSDF\u7684\u9ad8\u6548CPU\u4e13\u7528\u4f53\u7d20\u6620\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u65b9\u5411\u6027\u4f4d\u63a9\u7801\u96c6\u6210\u65b9\u6848\u5b9e\u73b0\u5b9e\u65f63D\u91cd\u5efa\uff0c\u5904\u7406\u65f6\u95f4\u4e0e\u4f53\u7d20\u7f51\u683c\u5206\u8fa8\u7387\u65e0\u5173", "motivation": "\u73b0\u6709TSDF/ESDF\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56GPU\u52a0\u901f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728CPU\u4e0a\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u7684\u9ad8\u5206\u8fa8\u7387\u5b9e\u65f63D\u91cd\u5efa\u65b9\u6cd5", "method": "\u4f7f\u7528\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u573a(TSDF)\u548c\u65b9\u5411\u6027\u4f4d\u63a9\u7801\u96c6\u6210\u65b9\u6848\uff0c\u5c06\u539f\u59cbLiDAR\u70b9\u4e91\u6570\u636e\u589e\u91cf\u878d\u5408\u5230\u4f53\u7d20\u7f51\u683c\u4e2d", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5f00\u653e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u7684\u5730\u56fe\u7cbe\u5ea6\u4e0e\u5f53\u4ee3\u6620\u5c04\u6280\u672f\u76f8\u5f53\uff0c\u5904\u7406\u65f6\u95f4\u4fdd\u6301\u6052\u5b9a", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728CPU\u4e0a\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u5b9e\u65f63D\u91cd\u5efa\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e0d\u4f9d\u8d56GPU\u7684\u5b9e\u65f6\u6620\u5c04\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.20082", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.20082", "abs": "https://arxiv.org/abs/2509.20082", "authors": ["Surov Maksim"], "title": "Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots", "comment": null, "summary": "This paper presents a control methodology for achieving orbital stabilization\nwith simultaneous time synchronization of periodic trajectories in\nunderactuated robotic systems. The proposed approach extends the classical\ntransverse linearization framework to explicitly incorporate\ntime-desynchronization dynamics. To stabilize the resulting extended transverse\ndynamics, we employ a combination of time-varying LQR and sliding-mode control.\nThe theoretical results are validated experimentally through the implementation\nof both centralized and decentralized control strategies on a group of six\nButterfly robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8f68\u9053\u7a33\u5b9a\u6027\u548c\u65f6\u95f4\u540c\u6b65\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u6a2a\u5411\u7ebf\u6027\u5316\u6846\u67b6\u4ee5\u5305\u542b\u65f6\u95f4\u5931\u540c\u6b65\u52a8\u6001\uff0c\u5e76\u91c7\u7528\u65f6\u53d8LQR\u548c\u6ed1\u6a21\u63a7\u5236\u8fdb\u884c\u7a33\u5b9a\u3002", "motivation": "\u89e3\u51b3\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5468\u671f\u6027\u8f68\u8ff9\u8ddf\u8e2a\u4e2d\u7684\u65f6\u95f4\u540c\u6b65\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u65f6\u95f4\u5931\u540c\u6b65\u52a8\u6001\u3002", "method": "\u6269\u5c55\u7ecf\u5178\u6a2a\u5411\u7ebf\u6027\u5316\u6846\u67b6\u4ee5\u663e\u5f0f\u5305\u542b\u65f6\u95f4\u5931\u540c\u6b65\u52a8\u6001\uff0c\u4f7f\u7528\u65f6\u53d8LQR\u548c\u6ed1\u6a21\u63a7\u5236\u7ec4\u5408\u6765\u7a33\u5b9a\u6269\u5c55\u540e\u7684\u6a2a\u5411\u52a8\u6001\u3002", "result": "\u901a\u8fc7\u516d\u53f0Butterfly\u673a\u5668\u4eba\u7684\u96c6\u4e2d\u5f0f\u548c\u5206\u6563\u5f0f\u63a7\u5236\u7b56\u7565\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8f68\u9053\u7a33\u5b9a\u6027\u548c\u65f6\u95f4\u540c\u6b65\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u534f\u540c\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.20084", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20084", "abs": "https://arxiv.org/abs/2509.20084", "authors": ["Guillermo Gil", "Jose Antonio Cobano", "Luis Merino", "Fernando Caballero"], "title": "C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields", "comment": "9 pages, 5 figures, submitted to ICRA 2026", "summary": "This paper introduces a novel framework for continuous 3D trajectory\noptimization in cluttered environments, leveraging online neural Euclidean\nSigned Distance Fields (ESDFs). Unlike prior approaches that rely on\ndiscretized ESDF grids with interpolation, our method directly optimizes smooth\ntrajectories represented by fifth-order polynomials over a continuous neural\nESDF, ensuring precise gradient information throughout the entire trajectory.\nThe framework integrates a two-stage nonlinear optimization pipeline that\nbalances efficiency, safety and smoothness. Experimental results demonstrate\nthat C-3TO produces collision-aware and dynamically feasible trajectories.\nMoreover, its flexibility in defining local window sizes and optimization\nparameters enables straightforward adaptation to diverse user's needs without\ncompromising performance. By combining continuous trajectory parameterization\nwith a continuously updated neural ESDF, C-3TO establishes a robust and\ngeneralizable foundation for safe and efficient local replanning in aerial\nrobotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6742\u4e71\u73af\u5883\u4e2d\u8fdb\u884c\u8fde\u7eed3D\u8f68\u8ff9\u4f18\u5316\u7684\u65b0\u6846\u67b6C-3TO\uff0c\u5229\u7528\u5728\u7ebf\u795e\u7ecf\u6b27\u51e0\u91cc\u5f97\u7b26\u53f7\u8ddd\u79bb\u573a\uff08ESDF\uff09\u76f4\u63a5\u4f18\u5316\u5e73\u6ed1\u7684\u4e94\u6b21\u591a\u9879\u5f0f\u8f68\u8ff9\uff0c\u786e\u4fdd\u6574\u4e2a\u8f68\u8ff9\u7684\u7cbe\u786e\u68af\u5ea6\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u79bb\u6563\u5316ESDF\u7f51\u683c\u548c\u63d2\u503c\uff0c\u65e0\u6cd5\u63d0\u4f9b\u8fde\u7eed\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u8f68\u8ff9\u4f18\u5316\u7684\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u4f18\u5316\u8fde\u7eed\u8f68\u8ff9\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u975e\u7ebf\u6027\u4f18\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u8fde\u7eed\u795e\u7ecfESDF\u548c\u4e94\u6b21\u591a\u9879\u5f0f\u8f68\u8ff9\u8868\u793a\uff0c\u901a\u8fc7\u5b9a\u4e49\u5c40\u90e8\u7a97\u53e3\u5927\u5c0f\u548c\u4f18\u5316\u53c2\u6570\u6765\u5e73\u8861\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u5e73\u6ed1\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eC-3TO\u80fd\u591f\u751f\u6210\u78b0\u649e\u611f\u77e5\u4e14\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u5176\u7075\u6d3b\u6027\u5141\u8bb8\u6839\u636e\u7528\u6237\u9700\u6c42\u8f7b\u677e\u8c03\u6574\u53c2\u6570\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8fde\u7eed\u8f68\u8ff9\u53c2\u6570\u5316\u4e0e\u6301\u7eed\u66f4\u65b0\u7684\u795e\u7ecfESDF\u76f8\u7ed3\u5408\uff0cC-3TO\u4e3a\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u5b89\u5168\u9ad8\u6548\u5c40\u90e8\u91cd\u89c4\u5212\u5efa\u7acb\u4e86\u7a33\u5065\u4e14\u53ef\u63a8\u5e7f\u7684\u57fa\u7840\u3002"}}
{"id": "2509.20093", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20093", "abs": "https://arxiv.org/abs/2509.20093", "authors": ["Venkat Margapuri", "Garik Kazanjian", "Naren Kosaraju"], "title": "Hybrid Safety Verification of Multi-Agent Systems using $\u03c8$-Weighted CBFs and PAC Guarantees", "comment": null, "summary": "This study proposes a hybrid safety verification framework for closed-loop\nmulti-agent systems under bounded stochastic disturbances. The proposed\napproach augments control barrier functions with a novel $\\psi$-weighted\nformulation that encodes directional control alignment between agents into the\nsafety constraints. Deterministic admissibility is combined with empirical\nvalidation via Monte Carlo rollouts, and a PAC-style guarantee is derived based\non margin-aware safety violations to provide a probabilistic safety\ncertificate. The results from the experiments conducted under different bounded\nstochastic disturbances validate the feasibility of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u6709\u754c\u968f\u673a\u6270\u52a8\u4e0b\u95ed\u73af\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6df7\u5408\u5b89\u5168\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u53ef\u5bb9\u8bb8\u6027\u4e0e\u8499\u7279\u5361\u6d1b\u7ecf\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u6982\u7387\u5b89\u5168\u8bc1\u4e66", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6709\u754c\u968f\u673a\u6270\u52a8\u4e0b\u7684\u5b89\u5168\u9a8c\u8bc1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u65b9\u5411\u6027\u63a7\u5236\u5bf9\u9f50\u548c\u6982\u7387\u6027\u5b89\u5168\u4fdd\u8bc1", "method": "\u5c06\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u4e0e\u65b0\u9896\u7684\u03c8\u52a0\u6743\u516c\u5f0f\u76f8\u7ed3\u5408\uff0c\u5c06\u667a\u80fd\u4f53\u95f4\u7684\u65b9\u5411\u6027\u63a7\u5236\u5bf9\u9f50\u7f16\u7801\u5230\u5b89\u5168\u7ea6\u675f\u4e2d\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u5206\u6790\u548c\u8499\u7279\u5361\u6d1b\u6eda\u52a8\u9a8c\u8bc1", "result": "\u5728\u4e0d\u540c\u6709\u754c\u968f\u673a\u6270\u52a8\u4e0b\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u53ef\u884c\u6027", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6982\u7387\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u65b9\u5411\u6027\u63a7\u5236\u7ea6\u675f\u548c\u968f\u673a\u6270\u52a8"}}
{"id": "2509.20109", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20109", "abs": "https://arxiv.org/abs/2509.20109", "authors": ["Pengxiang Li", "Yinan Zheng", "Yue Wang", "Huimin Wang", "Hang Zhao", "Jingjing Liu", "Xianyuan Zhan", "Kun Zhan", "Xianpeng Lang"], "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving", "comment": null, "summary": "End-to-End (E2E) solutions have emerged as a mainstream approach for\nautonomous driving systems, with Vision-Language-Action (VLA) models\nrepresenting a new paradigm that leverages pre-trained multimodal knowledge\nfrom Vision-Language Models (VLMs) to interpret and interact with complex\nreal-world environments. However, these methods remain constrained by the\nlimitations of imitation learning, which struggles to inherently encode\nphysical rules during training. Existing approaches often rely on complex\nrule-based post-refinement, employ reinforcement learning that remains largely\nlimited to simulation, or utilize diffusion guidance that requires\ncomputationally expensive gradient calculations. To address these challenges,\nwe introduce ReflectDrive, a novel learning-based framework that integrates a\nreflection mechanism for safe trajectory generation via discrete diffusion. We\nfirst discretize the two-dimensional driving space to construct an action\ncodebook, enabling the use of pre-trained Diffusion Language Models for\nplanning tasks through fine-tuning. Central to our approach is a safety-aware\nreflection mechanism that performs iterative self-correction without gradient\ncomputation. Our method begins with goal-conditioned trajectory generation to\nmodel multi-modal driving behaviors. Based on this, we apply local search\nmethods to identify unsafe tokens and determine feasible solutions, which then\nserve as safe anchors for inpainting-based regeneration. Evaluated on the\nNAVSIM benchmark, ReflectDrive demonstrates significant advantages in\nsafety-critical trajectory generation, offering a scalable and reliable\nsolution for autonomous driving systems.", "AI": {"tldr": "ReflectDrive\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u6269\u6563\u548c\u53cd\u5c04\u673a\u5236\u751f\u6210\u5b89\u5168\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u96be\u4ee5\u7f16\u7801\u7269\u7406\u89c4\u5219\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u53d7\u9650\u4e8e\u6a21\u4eff\u5b66\u4e60\uff0c\u65e0\u6cd5\u6709\u6548\u7f16\u7801\u7269\u7406\u89c4\u5219\uff0c\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u540e\u5904\u7406\u89c4\u5219\u6216\u8ba1\u7b97\u6602\u8d35\u7684\u5f3a\u5316\u5b66\u4e60/\u6269\u6563\u5f15\u5bfc\u3002", "method": "\u9996\u5148\u5c06\u4e8c\u7ef4\u9a7e\u9a76\u7a7a\u95f4\u79bb\u6563\u5316\u6784\u5efa\u52a8\u4f5c\u7801\u672c\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c4\u5212\uff1b\u6838\u5fc3\u662f\u5b89\u5168\u611f\u77e5\u7684\u53cd\u5c04\u673a\u5236\uff0c\u901a\u8fc7\u5c40\u90e8\u641c\u7d22\u8bc6\u522b\u4e0d\u5b89\u5168\u6807\u8bb0\u5e76\u57fa\u4e8e\u5b89\u5168\u951a\u70b9\u8fdb\u884c\u4fee\u590d\u518d\u751f\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReflectDrive\u5728\u5b89\u5168\u5173\u952e\u8f68\u8ff9\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u65e0\u68af\u5ea6\u8ba1\u7b97\u7684\u8fed\u4ee3\u81ea\u6821\u6b63\u5b9e\u73b0\u4e86\u5b89\u5168\u8f68\u8ff9\u751f\u6210\u3002"}}
{"id": "2509.20219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20219", "abs": "https://arxiv.org/abs/2509.20219", "authors": ["Sicong Liu", "Jianhui Liu", "Fang Chen", "Wenjian Yang", "Juan Yi", "Yu Zheng", "Zheng Wang", "Wanchao Chi", "Chaoyang Song"], "title": "A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering", "comment": "20 pages, 11 figures, 4 tables. Submitted Under Review", "summary": "Robotic tails can enhance the stability and maneuverability of mobile robots,\nbut current designs face a trade-off between the power of rigid systems and the\nsafety of soft ones. Rigid tails generate large inertial effects but pose risks\nin unstructured environments, while soft tails lack sufficient speed and force.\nWe present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this\nchallenge through a compliant pneumatic body reinforced by a passively jointed\nvertebral column inspired by musculoskeletal structures. This hybrid design\ndecouples load-bearing and actuation, enabling high-pressure actuation (up to 6\nbar) for superior dynamics while preserving compliance. A dedicated kinematic\nand dynamic model incorporating vertebral constraints is developed and\nvalidated experimentally. The BVSR tail achieves angular velocities above\n670{\\deg}/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm,\nindicating over 200% improvement compared to non-vertebraic designs.\nDemonstrations on rapid cart stabilization, obstacle negotiation, high-speed\nsteering, and quadruped integration confirm its versatility and practical\nutility for agile robotic platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u751f\u690e\u4f53\u8f6f\u4f53\u673a\u5668\u4eba\u5c3e\u5df4\uff08BVSR\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u521a\u6027\u690e\u4f53\u548c\u8f6f\u4f53\u6c14\u52a8\u9a71\u52a8\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u521a\u6027\u5c3e\u5df4\u5b89\u5168\u6027\u5dee\u548c\u8f6f\u4f53\u5c3e\u5df4\u52a8\u529b\u4e0d\u8db3\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5c3e\u5df4\u8bbe\u8ba1\u9762\u4e34\u521a\u6027\u7cfb\u7edf\u52a8\u529b\u5f3a\u4f46\u5b89\u5168\u6027\u5dee\u4e0e\u8f6f\u4f53\u7cfb\u7edf\u5b89\u5168\u4f46\u52a8\u529b\u4e0d\u8db3\u7684\u6743\u8861\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4ea7\u751f\u5f3a\u5927\u60ef\u6027\u6548\u5e94\u53c8\u80fd\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b89\u5168\u8fd0\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4eff\u751f\u690e\u4f53\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5c06\u521a\u6027\u690e\u4f53\u4e0e\u8f6f\u4f53\u6c14\u52a8\u9a71\u52a8\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u8d1f\u8f7d\u627f\u8f7d\u4e0e\u9a71\u52a8\u7684\u89e3\u8026\u3002\u5f00\u53d1\u4e86\u5305\u542b\u690e\u4f53\u7ea6\u675f\u7684\u4e13\u7528\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "BVSR\u5c3e\u5df4\u5b9e\u73b0\u4e86\u8d85\u8fc7670\u5ea6/\u79d2\u7684\u89d2\u901f\u5ea6\uff0c\u4ea7\u751f\u9ad8\u8fbe5.58N\u7684\u60ef\u6027\u529b\u548c1.21Nm\u7684\u626d\u77e9\uff0c\u76f8\u6bd4\u975e\u690e\u4f53\u8bbe\u8ba1\u6027\u80fd\u63d0\u5347\u8d85\u8fc7200%\u3002\u5728\u5feb\u901f\u63a8\u8f66\u7a33\u5b9a\u3001\u969c\u788d\u7269\u7a7f\u8d8a\u3001\u9ad8\u901f\u8f6c\u5411\u548c\u56db\u8db3\u673a\u5668\u4eba\u96c6\u6210\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "BVSR\u5c3e\u5df4\u901a\u8fc7\u4eff\u751f\u690e\u4f53\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e86\u521a\u6027-\u8f6f\u4f53\u5c3e\u5df4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u654f\u6377\u673a\u5668\u4eba\u5e73\u53f0\u63d0\u4f9b\u4e86\u517c\u5177\u9ad8\u52a8\u6001\u6027\u80fd\u548c\u5b89\u5168\u6027\u4fdd\u969c\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20229", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20229", "abs": "https://arxiv.org/abs/2509.20229", "authors": ["Angelos Plastropoulos", "Nicolas P. Avdelidis", "Argyrios Zolotas"], "title": "Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale", "comment": null, "summary": "The accuracy, resilience, and affordability of localisation are fundamental\nto autonomous robotic inspection within aircraft maintenance and overhaul (MRO)\nhangars. Hangars typically feature tall ceilings and are often made of\nmaterials such as metal. Due to its nature, it is considered a GPS-denied\nenvironment, with extensive multipath effects and stringent operational\nconstraints that collectively create a uniquely challenging environment. This\npersistent gap highlights the need for domain-specific comparative studies,\nincluding rigorous cost, accuracy, and integration assessments, to inform a\nreliable and scalable deployment of a localisation system in the Smart Hangar.\nThis paper presents the first techno-economic roadmap that benchmarks motion\ncapture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network\nacross three operational scenarios: robot localisation, asset tracking, and\nsurface defect detection within a 40x50 m hangar bay. A dual-layer optimisation\nfor camera selection and positioning framework is introduced, which couples\nmarket-based camera-lens selection with an optimisation solver, producing\ncamera layouts that minimise hardware while meeting accuracy targets. The\nroadmap equips MRO planners with an actionable method to balance accuracy,\ncoverage, and budget, demonstrating that an optimised vision architecture has\nthe potential to unlock robust and cost-effective sensing for next-generation\nSmart Hangars.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u6280\u672f\u7ecf\u6d4e\u8def\u7ebf\u56fe\uff0c\u572840x50\u7c73\u7684\u673a\u5e93\u73af\u5883\u4e2d\u5bf9\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u3001\u8d85\u5bbd\u5e26\u6280\u672f\u548c\u5929\u82b1\u677f\u6444\u50cf\u5934\u7f51\u7edc\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f15\u5165\u4e86\u6444\u50cf\u5934\u9009\u62e9\u548c\u5b9a\u4f4d\u7684\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u3002", "motivation": "\u98de\u673a\u7ef4\u62a4\u673a\u5e93\u4f5c\u4e3aGPS\u4fe1\u53f7\u7f3a\u5931\u73af\u5883\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u591a\u5f84\u6548\u5e94\u548c\u4e25\u683c\u7684\u64cd\u4f5c\u9650\u5236\uff0c\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u7684\u6bd4\u8f83\u7814\u7a76\u6765\u6307\u5bfc\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u5b9a\u4f4d\u7cfb\u7edf\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u5e02\u573a\u7684\u6444\u50cf\u5934-\u955c\u5934\u9009\u62e9\u548c\u4f18\u5316\u6c42\u89e3\u5668\uff0c\u751f\u6210\u6700\u5c0f\u5316\u786c\u4ef6\u6210\u672c\u540c\u65f6\u6ee1\u8db3\u7cbe\u5ea6\u76ee\u6807\u7684\u6444\u50cf\u5934\u5e03\u5c40\u65b9\u6848\u3002", "result": "\u4f18\u5316\u7684\u89c6\u89c9\u67b6\u6784\u6709\u6f5c\u529b\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u673a\u5e93\u63d0\u4f9b\u7a33\u5065\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u4f20\u611f\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u8def\u7ebf\u56fe\u4e3aMRO\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u5e73\u8861\u7cbe\u5ea6\u3001\u8986\u76d6\u8303\u56f4\u548c\u9884\u7b97\u7684\u53ef\u64cd\u4f5c\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4f18\u5316\u89c6\u89c9\u67b6\u6784\u5728\u667a\u80fd\u673a\u5e93\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.20253", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20253", "abs": "https://arxiv.org/abs/2509.20253", "authors": ["Jinhao Chai", "Anqing Jiang", "Hao Jiang", "Shiyi Mu", "Zichong Gu", "Shugong Xu"], "title": "AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving", "comment": "IWACIII 2025", "summary": "End-to-end multi-modal planning has become a transformative paradigm in\nautonomous driving, effectively addressing behavioral multi-modality and the\ngeneralization challenge in long-tail scenarios. We propose AnchDrive, a\nframework for end-to-end driving that effectively bootstraps a diffusion policy\nto mitigate the high computational cost of traditional generative models.\nRather than denoising from pure noise, AnchDrive initializes its planner with a\nrich set of hybrid trajectory anchors. These anchors are derived from two\ncomplementary sources: a static vocabulary of general driving priors and a set\nof dynamic, context-aware trajectories. The dynamic trajectories are decoded in\nreal-time by a Transformer that processes dense and sparse perceptual features.\nThe diffusion model then learns to refine these anchors by predicting a\ndistribution of trajectory offsets, enabling fine-grained refinement. This\nanchor-based bootstrapping design allows for efficient generation of diverse,\nhigh-quality trajectories. Experiments on the NAVSIM benchmark confirm that\nAnchDrive sets a new state-of-the-art and shows strong gen?eralizability", "AI": {"tldr": "AnchDrive\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u6df7\u5408\u8f68\u8ff9\u951a\u70b9\u6765\u5f15\u5bfc\u6269\u6563\u7b56\u7565\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u4f20\u7edf\u751f\u6210\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u751f\u6210\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u591a\u6a21\u6001\u89c4\u5212\u4e2d\u884c\u4e3a\u591a\u6a21\u6001\u6027\u548c\u957f\u5c3e\u573a\u666f\u6cdb\u5316\u6311\u6218\uff0c\u540c\u65f6\u964d\u4f4e\u4f20\u7edf\u751f\u6210\u6a21\u578b\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u951a\u70b9\u7684\u6269\u6563\u7b56\u7565\uff1a\u4f7f\u7528\u9759\u6001\u901a\u7528\u9a7e\u9a76\u5148\u9a8c\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u8f68\u8ff9\u7ec4\u6210\u7684\u6df7\u5408\u8f68\u8ff9\u951a\u70b9\u521d\u59cb\u5316\u89c4\u5212\u5668\uff0c\u6269\u6563\u6a21\u578b\u5b66\u4e60\u9884\u6d4b\u8f68\u8ff9\u504f\u79fb\u5206\u5e03\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4f18\u5316\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AnchDrive\u6846\u67b6\u901a\u8fc7\u951a\u70b9\u5f15\u5bfc\u7684\u6269\u6563\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f68\u8ff9\u751f\u6210\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20263", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20263", "abs": "https://arxiv.org/abs/2509.20263", "authors": ["Bingjie Chen", "Zihan Wang", "Zhe Han", "Guoping Pan", "Yi Cheng", "Houde Liu"], "title": "HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms", "comment": null, "summary": "Traditional IK methods for redundant humanoid manipulators emphasize\nend-effector (EE) tracking, frequently producing configurations that are valid\nmechanically but not human-like. We present Human-Like Inverse Kinematics\n(HL-IK), a lightweight IK framework that preserves EE tracking while shaping\nwhole-arm configurations to appear human-like, without full-body sensing at\nruntime. The key idea is a learned elbow prior: using large-scale human motion\ndata retargeted to the robot, we train a FiLM-modulated spatio-temporal\nattention network (FiSTA) to predict the next-step elbow pose from the EE\ntarget and a short history of EE-elbow states.This prediction is incorporated\nas a small residual alongside EE and smoothness terms in a standard\nLevenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK\nstacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and\ndirection error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the\nmost challenging trajectories. Hardware teleoperation on a robot distinct from\nsimulation further confirms the gains in anthropomorphism. HL-IK is simple to\nintegrate, adaptable across platforms via our pipeline, and adds minimal\ncomputation, enabling human-like motions for humanoid robots. Project page:\nhttps://hl-ik.github.io/", "AI": {"tldr": "HL-IK\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9006\u8fd0\u52a8\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u8098\u90e8\u5148\u9a8c\u6765\u751f\u6210\u7c7b\u4eba\u624b\u81c2\u914d\u7f6e\uff0c\u5728\u4fdd\u6301\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u7684\u540c\u65f6\u63d0\u9ad8\u8fd0\u52a8\u7684\u4eba\u6027\u5316\u7a0b\u5ea6\u3002", "motivation": "\u4f20\u7edf\u9006\u8fd0\u52a8\u5b66\u65b9\u6cd5\u867d\u7136\u80fd\u5b9e\u73b0\u673a\u68b0\u6709\u6548\u7684\u914d\u7f6e\uff0c\u4f46\u5f80\u5f80\u7f3a\u4e4f\u4eba\u7c7b\u822c\u7684\u81ea\u7136\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u4e0d\u591f\u4eba\u6027\u5316\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u8bad\u7ec3FiSTA\u7f51\u7edc\u9884\u6d4b\u8098\u90e8\u59ff\u6001\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6b8b\u5dee\u9879\u6574\u5408\u5230Levenberg-Marquardt\u4f18\u5316\u5668\u4e2d\uff0c\u5f62\u6210\u53ef\u5373\u63d2\u5373\u7528\u7684\u6570\u503cIK\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u572818.3\u4e07\u6b21\u4eff\u771f\u6b65\u9aa4\u4e2d\uff0cHL-IK\u5c06\u624b\u81c2\u76f8\u4f3c\u6027\u4f4d\u7f6e\u548c\u65b9\u5411\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e8630.6%\u548c35.4%\uff0c\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u8f68\u8ff9\u4e0a\u5206\u522b\u964d\u4f4e\u4e8642.2%\u548c47.4%\u3002\u786c\u4ef6\u9065\u64cd\u4f5c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u4eba\u6027\u5316\u7a0b\u5ea6\u7684\u63d0\u5347\u3002", "conclusion": "HL-IK\u7b80\u5355\u6613\u96c6\u6210\uff0c\u901a\u8fc7\u63d0\u51fa\u7684\u6d41\u7a0b\u53ef\u8de8\u5e73\u53f0\u9002\u914d\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u80fd\u591f\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5b9e\u73b0\u7c7b\u4eba\u8fd0\u52a8\u3002"}}
{"id": "2509.20286", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20286", "abs": "https://arxiv.org/abs/2509.20286", "authors": ["Georgios Tziafas", "Jiayun Zhang", "Hamidreza Kasaei"], "title": "Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video", "comment": null, "summary": "Learning visuomotor policies from expert demonstrations is an important\nfrontier in modern robotics research, however, most popular methods require\ncopious efforts for collecting teleoperation data and struggle to generalize\nout-ofdistribution. Scaling data collection has been explored through\nleveraging human videos, as well as demonstration augmentation techniques. The\nlatter approach typically requires expensive simulation rollouts and trains\npolicies with synthetic image data, therefore introducing a sim-to-real gap. In\nparallel, alternative state representations such as keypoints have shown great\npromise for category-level generalization. In this work, we bring these avenues\ntogether in a unified framework: PAD (Parse-AugmentDistill), for learning\ngeneralizable bimanual policies from a single human video. Our method relies on\nthree steps: (a) parsing a human video demo into a robot-executable\nkeypoint-action trajectory, (b) employing bimanual task-and-motion-planning to\naugment the demonstration at scale without simulators, and (c) distilling the\naugmented trajectories into a keypoint-conditioned policy. Empirically, we\nshowcase that PAD outperforms state-ofthe-art bimanual demonstration\naugmentation works relying on image policies with simulation rollouts, both in\nterms of success rate and sample/cost efficiency. We deploy our framework in\nsix diverse real-world bimanual tasks such as pouring drinks, cleaning trash\nand opening containers, producing one-shot policies that generalize in unseen\nspatial arrangements, object instances and background distractors.\nSupplementary material can be found in the project webpage\nhttps://gtziafas.github.io/PAD_project/.", "AI": {"tldr": "PAD\u6846\u67b6\u901a\u8fc7\u89e3\u6790\u4eba\u7c7b\u89c6\u9891\u4e3a\u673a\u5668\u4eba\u53ef\u6267\u884c\u7684\u5173\u952e\u70b9\u8f68\u8ff9\uff0c\u5229\u7528\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u8fdb\u884c\u65e0\u6a21\u62df\u5668\u7684\u6f14\u793a\u589e\u5f3a\uff0c\u5e76\u84b8\u998f\u4e3a\u5173\u952e\u70b9\u6761\u4ef6\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u4e2a\u4eba\u7c7b\u89c6\u9891\u5b66\u4e60\u901a\u7528\u53cc\u624b\u673a\u5668\u4eba\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9065\u64cd\u4f5c\u6570\u636e\u6536\u96c6\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u5206\u5e03\u5916\u573a\u666f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u6a21\u62df\u5668\u5f15\u5165\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u3002", "method": "\u4e09\u6b65\u6846\u67b6\uff1a(a)\u89e3\u6790\u4eba\u7c7b\u89c6\u9891\u4e3a\u673a\u5668\u4eba\u53ef\u6267\u884c\u7684\u5173\u952e\u70b9-\u52a8\u4f5c\u8f68\u8ff9\uff1b(b)\u4f7f\u7528\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u8fdb\u884c\u65e0\u6a21\u62df\u5668\u7684\u6f14\u793a\u589e\u5f3a\uff1b(c)\u5c06\u589e\u5f3a\u8f68\u8ff9\u84b8\u998f\u4e3a\u5173\u952e\u70b9\u6761\u4ef6\u7b56\u7565\u3002", "result": "PAD\u5728\u516d\u79cd\u771f\u5b9e\u4e16\u754c\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f9d\u8d56\u56fe\u50cf\u7b56\u7565\u548c\u6a21\u62df\u5668\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u6210\u529f\u7387\u548c\u6837\u672c/\u6210\u672c\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "PAD\u6846\u67b6\u80fd\u591f\u4ece\u5355\u4e2a\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u4e00\u6b21\u6027\u7b56\u7565\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7a7a\u95f4\u5e03\u7f6e\u3001\u7269\u4f53\u5b9e\u4f8b\u548c\u80cc\u666f\u5e72\u6270\u4e0b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.20297", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20297", "abs": "https://arxiv.org/abs/2509.20297", "authors": ["Remo Steiner", "Alexander Millane", "David Tingdahl", "Clemens Volk", "Vikram Ramasamy", "Xinjie Yao", "Peter Du", "Soha Pouya", "Shiwei Sheng"], "title": "mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies", "comment": "Accepted to CoRL 2025 Workshop RemembeRL", "summary": "End-to-end learning of robot control policies, structured as neural networks,\nhas emerged as a promising approach to robotic manipulation. To complete many\ncommon tasks, relevant objects are required to pass in and out of a robot's\nfield of view. In these settings, spatial memory - the ability to remember the\nspatial composition of the scene - is an important competency. However,\nbuilding such mechanisms into robot learning systems remains an open research\nproblem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D\nAction Policies), a 3D diffusion policy that generates robot trajectories based\non a semantic 3D reconstruction of the environment. We show in simulation\nexperiments that our approach is effective at solving tasks where\nstate-of-the-art approaches without memory mechanisms struggle. We release our\nreconstruction system, training code, and evaluation tasks to spur research in\nthis direction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86mindmap\u65b9\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e493D\u91cd\u5efa\u76843D\u6269\u6563\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u8bb0\u5fc6\u95ee\u9898\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u76f8\u5173\u7269\u4f53\u9700\u8981\u8fdb\u51fa\u673a\u5668\u4eba\u7684\u89c6\u91ce\u8303\u56f4\uff0c\u56e0\u6b64\u7a7a\u95f4\u8bb0\u5fc6\u80fd\u529b\u5bf9\u4e8e\u5b8c\u6210\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u4e2d\u6784\u5efa\u8fd9\u79cd\u673a\u5236\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86mindmap\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd3D\u6269\u6563\u7b56\u7565\uff0c\u57fa\u4e8e\u73af\u5883\u7684\u8bed\u4e493D\u91cd\u5efa\u6765\u751f\u6210\u673a\u5668\u4eba\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u56fe\u5b9e\u73b0\u7a7a\u95f4\u8bb0\u5fc6\u529f\u80fd\u3002", "result": "\u5728\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u90a3\u4e9b\u7f3a\u4e4f\u8bb0\u5fc6\u673a\u5236\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u96be\u4ee5\u5b8c\u6210\u7684\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u7a7a\u95f4\u8bb0\u5fc6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u91cd\u5efa\u7cfb\u7edf\u3001\u8bad\u7ec3\u4ee3\u7801\u548c\u8bc4\u4f30\u4efb\u52a1\u4ee5\u4fc3\u8fdb\u8be5\u65b9\u5411\u7684\u7814\u7a76\u3002"}}
{"id": "2509.20322", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20322", "abs": "https://arxiv.org/abs/2509.20322", "authors": ["Shaofeng Yin", "Yanjie Ze", "Hong-Xing Yu", "C. Karen Liu", "Jiajun Wu"], "title": "VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation", "comment": "Website: https://visualmimic.github.io", "summary": "Humanoid loco-manipulation in unstructured environments demands tight\nintegration of egocentric perception and whole-body control. However, existing\napproaches either depend on external motion capture systems or fail to\ngeneralize across diverse tasks. We introduce VisualMimic, a visual sim-to-real\nframework that unifies egocentric vision with hierarchical whole-body control\nfor humanoid robots. VisualMimic combines a task-agnostic low-level keypoint\ntracker -- trained from human motion data via a teacher-student scheme -- with\na task-specific high-level policy that generates keypoint commands from visual\nand proprioceptive input. To ensure stable training, we inject noise into the\nlow-level policy and clip high-level actions using human motion statistics.\nVisualMimic enables zero-shot transfer of visuomotor policies trained in\nsimulation to real humanoid robots, accomplishing a wide range of\nloco-manipulation tasks such as box lifting, pushing, football dribbling, and\nkicking. Beyond controlled laboratory settings, our policies also generalize\nrobustly to outdoor environments. Videos are available at:\nhttps://visualmimic.github.io .", "AI": {"tldr": "VisualMimic\u662f\u4e00\u4e2a\u89c6\u89c9\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6846\u67b6\uff0c\u5c06\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u4e0e\u5206\u5c42\u5168\u8eab\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5916\u90e8\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\uff0c\u8981\u4e48\u65e0\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u6cdb\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u548c\u5168\u8eab\u63a7\u5236\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u64cd\u4f5c\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4efb\u52a1\u65e0\u5173\u7684\u4f4e\u7ea7\u5173\u952e\u70b9\u8ddf\u8e2a\u5668\uff08\u901a\u8fc7\u5e08\u751f\u65b9\u6848\u4ece\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u8bad\u7ec3\uff09\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u9ad8\u7ea7\u7b56\u7565\uff08\u4ece\u89c6\u89c9\u548c\u672c\u4f53\u611f\u89c9\u8f93\u5165\u751f\u6210\u5173\u952e\u70b9\u547d\u4ee4\uff09\u3002\u901a\u8fc7\u5411\u4f4e\u7ea7\u7b56\u7565\u6ce8\u5165\u566a\u58f0\u548c\u4f7f\u7528\u4eba\u7c7b\u8fd0\u52a8\u7edf\u8ba1\u6570\u636e\u88c1\u526a\u9ad8\u7ea7\u52a8\u4f5c\u6765\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5b8c\u6210\u4e86\u591a\u79cd\u5b9a\u4f4d\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u7bb1\u5b50\u4e3e\u5347\u3001\u63a8\u52a8\u3001\u8db3\u7403\u8fd0\u7403\u548c\u8e22\u7403\uff09\uff0c\u5e76\u5728\u5ba4\u5916\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VisualMimic\u6846\u67b6\u6210\u529f\u5730\u5c06\u89c6\u89c9\u611f\u77e5\u4e0e\u5206\u5c42\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20333", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20333", "abs": "https://arxiv.org/abs/2509.20333", "authors": ["Srikrishna Bangalore Raghu", "Alessandro Roncone"], "title": "BBoE: Leveraging Bundle of Edges for Kinodynamic Bidirectional Motion Planning", "comment": "8 Pages, 7 Figures", "summary": "In this work, we introduce BBoE, a bidirectional, kinodynamic, sampling-based\nmotion planner that consistently and quickly finds low-cost solutions in\nenvironments with varying obstacle clutter. The algorithm combines exploration\nand exploitation while relying on precomputed robot state traversals, resulting\nin efficient convergence towards the goal. Our key contributions include: i) a\nstrategy to navigate through obstacle-rich spaces by sorting and sequencing\npreprocessed forward propagations; and ii) BBoE, a robust bidirectional\nkinodynamic planner that utilizes this strategy to produce fast and feasible\nsolutions. The proposed framework reduces planning time, diminishes solution\ncost and increases success rate in comparison to previous approaches.", "AI": {"tldr": "BBoE\u662f\u4e00\u79cd\u53cc\u5411\u3001\u52a8\u529b\u5b66\u611f\u77e5\u7684\u91c7\u6837\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u80fd\u5728\u4e0d\u540c\u969c\u788d\u7269\u5bc6\u5ea6\u7684\u73af\u5883\u4e2d\u5feb\u901f\u627e\u5230\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u969c\u788d\u7269\u73af\u5883\u4e2d\u5feb\u901f\u751f\u6210\u53ef\u884c\u4e14\u4f4e\u6210\u672c\u8fd0\u52a8\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "method": "\u7ed3\u5408\u63a2\u7d22\u548c\u5229\u7528\u7b56\u7565\uff0c\u4f7f\u7528\u9884\u8ba1\u7b97\u7684\u673a\u5668\u4eba\u72b6\u6001\u904d\u5386\uff0c\u901a\u8fc7\u6392\u5e8f\u548c\u5e8f\u5217\u5316\u9884\u5904\u7406\u7684\u524d\u5411\u4f20\u64ad\u6765\u5bfc\u822a\u969c\u788d\u7269\u5bc6\u96c6\u7a7a\u95f4\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cBBoE\u51cf\u5c11\u4e86\u89c4\u5212\u65f6\u95f4\uff0c\u964d\u4f4e\u4e86\u89e3\u51b3\u65b9\u6848\u6210\u672c\uff0c\u5e76\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002", "conclusion": "BBoE\u662f\u4e00\u4e2a\u9c81\u68d2\u7684\u53cc\u5411\u52a8\u529b\u5b66\u89c4\u5212\u5668\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u5feb\u901f\u53ef\u884c\u7684\u8fd0\u52a8\u89e3\u51b3\u65b9\u6848\u3002"}}
