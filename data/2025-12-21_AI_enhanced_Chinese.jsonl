{"id": "2512.15907", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15907", "abs": "https://arxiv.org/abs/2512.15907", "authors": ["Tejas Anvekar", "Juhna Park", "Aparna Garimella", "Vivek Gupta"], "title": "TabReX : Tabular Referenceless eXplainable Evaluation", "comment": null, "summary": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.", "AI": {"tldr": "TabReX\uff1a\u4e00\u4e2a\u57fa\u4e8e\u56fe\u63a8\u7406\u7684\u65e0\u53c2\u8003\u8868\u683c\u751f\u6210\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u548c\u8868\u683c\u8f6c\u6362\u4e3a\u77e5\u8bc6\u56fe\u8fdb\u884c\u5bf9\u9f50\u8bc4\u4f30\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\u548c\u9519\u8bef\u8ffd\u8e2a\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u8981\u4e48\u5c06\u8868\u683c\u6241\u5e73\u5316\u4e3a\u6587\u672c\u5ffd\u7565\u7ed3\u6784\u4fe1\u606f\uff0c\u8981\u4e48\u4f9d\u8d56\u56fa\u5b9a\u53c2\u8003\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u8bc4\u4f30\u7ed3\u6784\u53c8\u80fd\u8bc4\u4f30\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u6846\u67b6\u3002", "method": "TabReX\u5c06\u6e90\u6587\u672c\u548c\u751f\u6210\u7684\u8868\u683c\u90fd\u8f6c\u6362\u4e3a\u89c4\u8303\u7684\u77e5\u8bc6\u56fe\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u5339\u914d\u8fc7\u7a0b\u5bf9\u9f50\u4e24\u8005\uff0c\u7136\u540e\u8ba1\u7b97\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5206\u6570\u6765\u91cf\u5316\u7ed3\u6784\u548c\u4e8b\u5b9e\u4fdd\u771f\u5ea6\u3002", "result": "TabReX\u5728\u4e13\u5bb6\u6392\u540d\u76f8\u5173\u6027\u65b9\u9762\u8fbe\u5230\u6700\u9ad8\uff0c\u5728\u66f4\u96be\u7684\u6270\u52a8\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0c\u5e76\u80fd\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u6a21\u578b\u4e0e\u63d0\u793a\u5206\u6790\u3002\u540c\u65f6\u6784\u5efa\u4e86TabReX-Bench\u57fa\u51c6\uff0c\u6db5\u76d66\u4e2a\u9886\u57df\u548c12\u79cd\u6270\u52a8\u7c7b\u578b\u3002", "conclusion": "TabReX\u4e3a\u7ed3\u6784\u5316\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u4fe1\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u7684\u53ef\u63a7\u6743\u8861\uff0c\u5e76\u652f\u6301\u4eba\u7c7b\u5bf9\u9f50\u7684\u5224\u65ad\u548c\u5355\u5143\u683c\u7ea7\u9519\u8bef\u8ffd\u8e2a\u3002"}}
{"id": "2512.15925", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.15925", "abs": "https://arxiv.org/abs/2512.15925", "authors": ["Joel Mire", "Maria Antoniak", "Steven R. Wilson", "Zexin Ma", "Achyutarama R. Ganti", "Andrew Piper", "Maarten Sap"], "title": "Social Story Frames: Contextual Reasoning about Narrative Intent and Reception", "comment": "Presented at IC2S2 2025; Under Review (ARR Oct 2025)", "summary": "Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.", "AI": {"tldr": "SocialStoryFrames\uff1a\u4e00\u79cd\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u793e\u4ea4\u5a92\u4f53\u6545\u4e8b\u4e2d\u63d0\u53d6\u8bfb\u8005\u53cd\u5e94\u7684\u5408\u7406\u63a8\u65ad\uff0c\u5305\u62ec\u611f\u77e5\u4f5c\u8005\u610f\u56fe\u3001\u89e3\u91ca\u9884\u6d4b\u63a8\u7406\u3001\u60c5\u611f\u53cd\u5e94\u548c\u4ef7\u503c\u5224\u65ad\uff0c\u901a\u8fc7\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u548c\u53d9\u4e8b\u7406\u8bba\u5206\u7c7b\u6cd5\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u6a21\u578b\u5728\u6355\u6349\u8bfb\u8005\u5bf9\u6545\u4e8b\u7684\u4e30\u5bcc\u89e3\u91ca\u6027\u3001\u60c5\u611f\u6027\u548c\u8bc4\u4ef7\u6027\u53cd\u5e94\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u8fdb\u884c\u7ec6\u81f4\u5206\u6790\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faSocialStoryFrames\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u57fa\u4e8e\u53d9\u4e8b\u7406\u8bba\u3001\u8bed\u8a00\u8bed\u7528\u5b66\u548c\u5fc3\u7406\u5b66\u5efa\u7acb\u5206\u7c7b\u6cd5\uff0c\u5f00\u53d1SSF-Generator\u548cSSF-Classifier\u4e24\u4e2a\u6a21\u578b\uff0c\u901a\u8fc7\u4eba\u7c7b\u8c03\u67e5\uff08382\u540d\u53c2\u4e0e\u8005\uff09\u548c\u4e13\u5bb6\u6807\u6ce8\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728SSF-Corpus\uff086140\u4e2a\u793e\u4ea4\u5a92\u4f53\u6545\u4e8b\uff09\u4e0a\u5e94\u7528\u6a21\u578b\uff0c\u5206\u6790\u4e86\u6545\u4e8b\u610f\u56fe\u7684\u9891\u7387\u548c\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u793e\u533a\u7684\u53d9\u4e8b\u5b9e\u8df5\u53ca\u5176\u591a\u6837\u6027\u3002", "conclusion": "SocialStoryFrames\u901a\u8fc7\u5c06\u7ec6\u7c92\u5ea6\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u5efa\u6a21\u4e0e\u901a\u7528\u7684\u8bfb\u8005\u53cd\u5e94\u5206\u7c7b\u6cd5\u76f8\u7ed3\u5408\uff0c\u4e3a\u5728\u7ebf\u793e\u533a\u53d9\u4e8b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2512.15959", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15959", "abs": "https://arxiv.org/abs/2512.15959", "authors": ["Arma\u011fan Amcalar", "Eyup Cinar"], "title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions", "comment": null, "summary": "Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.", "AI": {"tldr": "BRAID\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u673a\u5668\u53ef\u8bfb\u7684\u63d0\u793a\uff08\u57fa\u4e8eMermaid\u6307\u4ee4\u56fe\uff09\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6027\u80fd\u3001\u6210\u672c\u548ctoken\u4f7f\u7528\u4e4b\u95f4\u5b58\u5728\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b58\u5728\u65e0\u9650\u5236\u7684token\u6269\u5c55\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBRAID\uff08\u6709\u754c\u63a8\u7406\u81ea\u4e3b\u63a8\u65ad\u4e0e\u51b3\u7b56\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8eMermaid\u7684\u6307\u4ee4\u56fe\u521b\u5efa\u7ed3\u6784\u5316\u3001\u673a\u5668\u53ef\u8bfb\u7684\u63d0\u793a\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u6709\u754c\u7ed3\u6784\u5316\u63a8\u7406\u800c\u975e\u65e0\u9650\u5236\u7684\u81ea\u7136\u8bed\u8a00\u6269\u5c55\u3002", "result": "\u5728AdvancedIF\u3001GSM-Hard\u548cSCALE MultiChallenge\u7b49\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e0d\u540cGPT\u6a21\u578b\u5c42\u7ea7\uff0c\u663e\u793a\u7ed3\u6784\u5316\u63d0\u793a\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u7387\u3002", "conclusion": "BRAID\u88ab\u8bc1\u660e\u662f\u4f18\u5316\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\u63a8\u7406\u6548\u7387\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u6280\u672f\uff0c\u6240\u6709\u6570\u636e\u96c6\u548c\u8be6\u7ec6\u7ed3\u679c\u65e5\u5fd7\u5df2\u516c\u5f00\u3002"}}
{"id": "2512.16034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16034", "abs": "https://arxiv.org/abs/2512.16034", "authors": ["Kieran Henderson", "Kian Omoomi", "Vasudha Varadarajan", "Allison Lahnala", "Charles Welch"], "title": "Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms", "comment": null, "summary": "Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5229\u7528\u4e2a\u4eba\u4fe1\u606f\uff08\u5982\u81ea\u6211\u62ab\u9732\u53e5\u5b50\uff09\u6765\u6539\u8fdb\u4e2a\u4f53\u7279\u5f81\u5efa\u6a21\u548c\u4e3b\u89c2\u4efb\u52a1\u6807\u6ce8\u9884\u6d4b\uff0c\u53d1\u73b0\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u6bd4\u6001\u5ea6\u3001\u5173\u7cfb\u3001\u7ecf\u5386\u66f4\u6709\u6548\uff0c\u5c11\u91cf\u76f8\u5173\u8bc4\u8bba\u5373\u53ef\uff0c\u591a\u6837\u5316\u6837\u672c\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u4f7f\u7528\u4e2a\u4eba\u4fe1\u606f\uff08\u5982\u4eba\u7269\u63cf\u8ff0\u6216\u81ea\u6211\u62ab\u9732\uff09\u6765\u6539\u8fdb\u4e2a\u4f53\u7279\u5f81\u5efa\u6a21\u548c\u4e3b\u89c2\u4efb\u52a1\u6807\u6ce8\u9884\u6d4b\uff0c\u4f46\u4fe1\u606f\u91cf\u6709\u9650\uff0c\u7f3a\u4e4f\u5bf9\u4f55\u79cd\u4fe1\u606f\u6700\u6709\u6548\u7684\u6df1\u5165\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u7c7b\u578b\u81ea\u6211\u62ab\u9732\u4fe1\u606f\u5bf9\u9884\u6d4b\u793e\u4f1a\u89c4\u8303\u5224\u65ad\u7684\u5f71\u54cd\u3002", "method": "\u5bf9\u81ea\u6211\u62ab\u9732\u53e5\u5b50\u8fdb\u884c\u5206\u7c7b\uff0c\u6784\u5efa\u6807\u6ce8\u8005\u6a21\u578b\u6765\u9884\u6d4b\u793e\u4f1a\u89c4\u8303\u5224\u65ad\u3002\u901a\u8fc7\u591a\u79cd\u6d88\u878d\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u68c0\u9a8c\u4e0d\u540c\u7c7b\u578b\u4fe1\u606f\u5bf9\u9884\u6d4b\u6807\u6ce8\u6a21\u5f0f\u7684\u5f71\u54cd\u3002", "result": "1. \u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u6bd4\u6001\u5ea6\u3001\u5173\u7cfb\u3001\u7ecf\u5386\u66f4\u5177\u5f71\u54cd\u529b\uff1b2. \u57fa\u4e8e\u7406\u8bba\u7684\u65b9\u6cd5\u4f18\u4e8e\u81ea\u52a8\u805a\u7c7b\uff1b3. \u4e0e\u5148\u524d\u7814\u7a76\u76f8\u53cd\uff0c\u4ec5\u9700\u5c11\u91cf\u76f8\u5173\u8bc4\u8bba\u5373\u53ef\uff1b4. \u62e5\u6709\u66f4\u591a\u6837\u5316\u7684\u6807\u6ce8\u8005\u81ea\u6211\u62ab\u9732\u6837\u672c\u53ef\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u81ea\u6211\u62ab\u9732\u4fe1\u606f\u5bf9\u9884\u6d4b\u4e3b\u89c2\u4efb\u52a1\u6807\u6ce8\u5177\u6709\u4ef7\u503c\uff0c\u5176\u4e2d\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u6700\u4e3a\u5173\u952e\uff0c\u7406\u8bba\u9a71\u52a8\u7684\u65b9\u6cd5\u6548\u679c\u66f4\u597d\uff0c\u4e14\u4e0d\u9700\u8981\u5927\u91cf\u76f8\u5173\u8bc4\u8bba\uff0c\u591a\u6837\u5316\u7684\u6837\u672c\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2512.15840", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15840", "abs": "https://arxiv.org/abs/2512.15840", "authors": ["Boyuan Chen", "Tianyuan Zhang", "Haoran Geng", "Kiwhan Song", "Caiyi Zhang", "Peihao Li", "William T. Freeman", "Jitendra Malik", "Pieter Abbeel", "Russ Tedrake", "Vincent Sitzmann", "Yilun Du"], "title": "Large Video Planner Enables Generalizable Robot Control", "comment": "29 pages, 16 figures", "summary": "General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21\u89c6\u9891\u9884\u8bad\u7ec3\u6784\u5efa\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u4eba\u7c7b\u6d3b\u52a8\u89c6\u9891\u6570\u636e\u96c6\u8bad\u7ec3\u751f\u6210\u5f0f\u673a\u5668\u4eba\u89c4\u5212\u6a21\u578b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u89c6\u9891\u89c4\u5212\u5e76\u63d0\u53d6\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6269\u5c55\u52a8\u4f5c\u8f93\u51fa\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u89c6\u9891\u4f5c\u4e3a\u65f6\u7a7a\u72b6\u6001\u548c\u52a8\u4f5c\u5e8f\u5217\u7684\u81ea\u7136\u8f7d\u4f53\uff0c\u6bd4\u9759\u6001\u56fe\u50cf\u548c\u8bed\u8a00\u66f4\u7b26\u5408\u673a\u5668\u4eba\u884c\u4e3a\u7684\u672c\u8d28\uff0c\u56e0\u6b64\u63a2\u7d22\u89c6\u9891\u9884\u8bad\u7ec3\u4f5c\u4e3a\u6784\u5efa\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u4e3b\u8981\u6a21\u6001\u3002", "method": "1. \u6536\u96c6\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u4eba\u7c7b\u6d3b\u52a8\u548c\u4efb\u52a1\u6f14\u793a\u89c6\u9891\u6570\u636e\u96c6\uff1b2. \u9996\u6b21\u4ee5\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u8bad\u7ec3\u5f00\u653e\u5f0f\u89c6\u9891\u6a21\u578b\u7528\u4e8e\u751f\u6210\u5f0f\u673a\u5668\u4eba\u89c4\u5212\uff1b3. \u6a21\u578b\u4e3a\u96f6\u6837\u672c\u65b0\u573a\u666f\u548c\u4efb\u52a1\u751f\u6210\u89c6\u9891\u89c4\u5212\uff1b4. \u540e\u5904\u7406\u63d0\u53d6\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u6a21\u578b\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3001\u5f3a\u6cdb\u5316\u6027\u548c\u73b0\u5b9e\u53ef\u884c\u6027\uff1a1. \u5728\u7b2c\u4e09\u65b9\u9009\u62e9\u7684\u91ce\u5916\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4efb\u52a1\u7ea7\u6cdb\u5316\uff1b2. \u901a\u8fc7\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u8bc1\u660e\u7269\u7406\u6267\u884c\u6210\u529f\uff1b3. \u6a21\u578b\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u652f\u6301\u53ef\u590d\u73b0\u7684\u89c6\u9891\u673a\u5668\u4eba\u5b66\u4e60\u3002", "conclusion": "\u5927\u89c4\u6a21\u89c6\u9891\u9884\u8bad\u7ec3\u662f\u6784\u5efa\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u6709\u6548\u66ff\u4ee3\u8303\u5f0f\uff0c\u89c6\u9891\u4f5c\u4e3a\u65f6\u7a7a\u5e8f\u5217\u7684\u81ea\u7136\u8868\u5f81\u4e0e\u673a\u5668\u4eba\u884c\u4e3a\u9ad8\u5ea6\u5951\u5408\uff0c\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u89c4\u5212\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u6267\u884c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5f00\u653e\u5f0f\u3001\u53ef\u590d\u73b0\u7684\u89c6\u9891\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.16041", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16041", "abs": "https://arxiv.org/abs/2512.16041", "authors": ["Yuanning Feng", "Sinan Wang", "Zhengxiang Cheng", "Yao Wan", "Dongping Chen"], "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?", "comment": null, "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.", "AI": {"tldr": "Sage\u662f\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684LLM-as-a-Judge\u8bc4\u4f30\u5957\u4ef6\uff0c\u901a\u8fc7\u5c40\u90e8\u81ea\u4e00\u81f4\u6027\u548c\u5168\u5c40\u903b\u8f91\u4e00\u81f4\u6027\u4e24\u4e2a\u65b0\u7ef4\u5ea6\u6765\u8bc4\u4f30LLM\u6cd5\u5b98\u7684\u8d28\u91cf\uff0c\u53d1\u73b0\u5f53\u524d\u9876\u7ea7LLM\u5728\u7ea61/4\u56f0\u96be\u6848\u4f8b\u4e2d\u5b58\u5728\u504f\u597d\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM-as-a-Judge\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u8fd9\u5f15\u5165\u4e86\u4eba\u7c7b\u504f\u89c1\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u8bc4\u4f30\u5e76\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLM\u6cd5\u5b98\u7684\u8d28\u91cf\u3002", "method": "\u57fa\u4e8e\u7406\u6027\u9009\u62e9\u7406\u8bba\u516c\u7406\uff0c\u63d0\u51fa\u4e24\u4e2a\u65b0\u8bc4\u4f30\u7ef4\u5ea6\uff1a\u5c40\u90e8\u81ea\u4e00\u81f4\u6027\uff08\u6210\u5bf9\u504f\u597d\u7a33\u5b9a\u6027\uff09\u548c\u5168\u5c40\u903b\u8f91\u4e00\u81f4\u6027\uff08\u5b8c\u6574\u504f\u597d\u96c6\u7684\u4f20\u9012\u6027\uff09\u3002\u6784\u5efa\u5305\u542b650\u4e2a\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u57fa\u51c6\u95ee\u9898\u548c\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSage\u6307\u6807\u7a33\u5b9a\u4e14\u4e0eLLMBar\u3001RewardBench2\u7b49\u76d1\u7763\u57fa\u51c6\u9ad8\u5ea6\u76f8\u5173\u3002\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\uff08\u5305\u62ecGemini-2.5-Pro\u548cGPT-5\uff09\u5728\u7ea61/4\u56f0\u96be\u6848\u4f8b\u4e2d\u65e0\u6cd5\u4fdd\u6301\u4e00\u81f4\u7684\u504f\u597d\u5224\u65ad\uff0c\u5b58\u5728\u663e\u8457\u53ef\u9760\u6027\u95ee\u9898\u3002", "conclusion": "Sage\u4f5c\u4e3a\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u8bc4\u4f30\u5957\u4ef6\u53ef\u9760\u6709\u6548\u3002\u63ed\u793a\u4e86LLM\u6cd5\u5b98\u5b58\u5728\u60c5\u5883\u504f\u597d\u73b0\u8c61\uff0c\u5fae\u8c03LLM\u6cd5\u5b98\u3001\u4f7f\u7528\u8bc4\u5ba1\u56e2\u673a\u5236\u548c\u6df1\u5ea6\u63a8\u7406\u53ef\u4ee5\u63d0\u5347\u4e00\u81f4\u6027\u3002\u4eba\u7c7b\u5224\u65ad\u4e5f\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\u6027\uff0c\u8868\u660e\u4eba\u5de5\u6807\u6ce8\u53ef\u80fd\u4e0d\u662f\u53ef\u9760\u7684\u91d1\u6807\u51c6\u3002"}}
{"id": "2512.15994", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15994", "abs": "https://arxiv.org/abs/2512.15994", "authors": ["Manuel Mekkattu", "Mike Y. Michelis", "Robert K. Katzschmann"], "title": "SORS: A Modular, High-Fidelity Simulator for Soft Robots", "comment": "This work has been submitted to the IEEE for possible publication. Code and data are available at github.com/srl-ethz/sors", "summary": "The deployment of complex soft robots in multiphysics environments requires advanced simulation frameworks that not only capture interactions between different types of material, but also translate accurately to real-world performance. Soft robots pose unique modeling challenges due to their large nonlinear deformations, material incompressibility, and contact interactions, which complicate both numerical stability and physical accuracy. Despite recent progress, robotic simulators often struggle with modeling such phenomena in a scalable and application-relevant manner. We present SORS (Soft Over Rigid Simulator), a versatile, high-fidelity simulator designed to handle these complexities for soft robot applications. Our energy-based framework, built on the finite element method, allows modular extensions, enabling the inclusion of custom-designed material and actuation models. To ensure physically consistent contact handling, we integrate a constrained nonlinear optimization based on sequential quadratic programming, allowing for stable and accurate modeling of contact phenomena. We validate our simulator through a diverse set of real-world experiments, which include cantilever deflection, pressure-actuation of a soft robotic arm, and contact interactions from the PokeFlex dataset. In addition, we showcase the potential of our framework for control optimization of a soft robotic leg. These tests confirm that our simulator can capture both fundamental material behavior and complex actuation dynamics with high physical fidelity. By bridging the sim-to-real gap in these challenging domains, our approach provides a validated tool for prototyping next-generation soft robots, filling the gap of extensibility, fidelity, and usability in the soft robotic ecosystem.", "AI": {"tldr": "SORS\u662f\u4e00\u4e2a\u7528\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u5668\uff0c\u57fa\u4e8e\u6709\u9650\u5143\u65b9\u6cd5\u548c\u80fd\u91cf\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u975e\u7ebf\u6027\u4f18\u5316\u5904\u7406\u63a5\u89e6\u4ea4\u4e92\uff0c\u5728\u591a\u79cd\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u7269\u7406\u51c6\u786e\u6027\u548c\u63a7\u5236\u4f18\u5316\u6f5c\u529b\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u591a\u7269\u7406\u573a\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9700\u8981\u5148\u8fdb\u7684\u4eff\u771f\u6846\u67b6\uff0c\u4f46\u73b0\u6709\u4eff\u771f\u5668\u96be\u4ee5\u5904\u7406\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5927\u975e\u7ebf\u6027\u53d8\u5f62\u3001\u6750\u6599\u4e0d\u53ef\u538b\u7f29\u6027\u548c\u63a5\u89e6\u4ea4\u4e92\u7b49\u6311\u6218\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u5e94\u7528\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51faSORS\u4eff\u771f\u5668\uff0c\u57fa\u4e8e\u6709\u9650\u5143\u65b9\u6cd5\u7684\u80fd\u91cf\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u6269\u5c55\u81ea\u5b9a\u4e49\u6750\u6599\u548c\u9a71\u52a8\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u5e8f\u5217\u4e8c\u6b21\u89c4\u5212\u7684\u7ea6\u675f\u975e\u7ebf\u6027\u4f18\u5316\u6765\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u7684\u63a5\u89e6\u5904\u7406\u3002", "result": "\u901a\u8fc7\u60ac\u81c2\u6881\u504f\u8f6c\u3001\u8f6f\u4f53\u673a\u68b0\u81c2\u538b\u529b\u9a71\u52a8\u3001PokeFlex\u6570\u636e\u96c6\u63a5\u89e6\u4ea4\u4e92\u7b49\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u6750\u6599\u57fa\u672c\u884c\u4e3a\u548c\u590d\u6742\u9a71\u52a8\u52a8\u529b\u5b66\uff0c\u5e76\u5c55\u793a\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u817f\u7684\u63a7\u5236\u4f18\u5316\u6f5c\u529b\u3002", "conclusion": "SORS\u901a\u8fc7\u5f25\u5408\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8f6f\u4f53\u673a\u5668\u4eba\u539f\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u5de5\u5177\uff0c\u586b\u8865\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u751f\u6001\u7cfb\u7edf\u4e2d\u53ef\u6269\u5c55\u6027\u3001\u4fdd\u771f\u5ea6\u548c\u53ef\u7528\u6027\u7684\u7a7a\u767d\u3002"}}
{"id": "2512.16125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16125", "abs": "https://arxiv.org/abs/2512.16125", "authors": ["Daniela N. Rim", "Heeyoul Choi"], "title": "Convolutional Lie Operator for Sentence Classification", "comment": "Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval", "summary": "Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling.", "AI": {"tldr": "\u5c06\u674e\u5377\u79ef\u96c6\u6210\u5230\u57fa\u4e8e\u5377\u79ef\u7684\u53e5\u5b50\u5206\u7c7b\u5668\u4e2d\uff0c\u901a\u8fc7\u6355\u6349\u8bed\u8a00\u4e2d\u7684\u590d\u6742\u975e\u6b27\u51e0\u91cc\u5f97\u5bf9\u79f0\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u5377\u79ef\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u6355\u6349\u6587\u672c\u5c40\u90e8\u4f4d\u7f6e\u4e0d\u53d8\u7279\u5f81\u65b9\u9762\u5f88\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u5efa\u6a21\u8bed\u8a00\u5185\u90e8\u590d\u6742\u53d8\u6362\u65b9\u9762\u7684\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002\u674e\u7fa4\u64cd\u4f5c\u80fd\u591f\u6355\u6349\u590d\u6742\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u5bf9\u79f0\u6027\uff0c\u8fd9\u4e3a\u6539\u8fdb\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u5c06\u674e\u5377\u79ef\u96c6\u6210\u5230\u57fa\u4e8e\u5377\u79ef\u7684\u53e5\u5b50\u5206\u7c7b\u5668\u4e2d\uff0c\u5f00\u53d1\u4e86SCLie\u548cDPCLie\u4e24\u79cd\u6a21\u578b\u3002", "result": "SCLie\u548cDPCLie\u6a21\u578b\u5728\u5b9e\u8bc1\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u5377\u79ef\u7684\u53e5\u5b50\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u6355\u6349\u8bed\u8a00\u4e2d\u4e0d\u5e38\u89c1\u7684\u53d8\u6362\uff0c\u76f8\u5bf9\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u674e\u57fa\u6a21\u578b\u80fd\u591f\u6355\u6349\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u53d8\u6362\uff0c\u8fd9\u4e3a\u8bed\u8a00\u5efa\u6a21\u7684\u65b0\u8303\u5f0f\u63a2\u7d22\u63d0\u4f9b\u4e86\u52a8\u529b\u3002"}}
{"id": "2512.16011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16011", "abs": "https://arxiv.org/abs/2512.16011", "authors": ["Jack Naylor", "Raghav Mishra", "Nicholas H. Barbara", "Donald G. Dansereau"], "title": "dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection", "comment": "13 pages, 9 images", "summary": "Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/", "AI": {"tldr": "\u63d0\u51fa\u2202LITE\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u7684\u5728\u8f68\u68c0\u67e5\u6a21\u62df\u7ba1\u9053\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u8f68\u9053\u4f20\u64ad\u5668\u4f18\u5316\u68c0\u67e5\u8f68\u8ff9\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u6570\u636e\u53ef\u7528\u6027\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\u73af\u5883\u5bf9\u822a\u5929\u5668\u68c0\u67e5\u64cd\u4f5c\u5e26\u6765\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u592a\u9633\u5149\u955c\u9762\u53cd\u5c04\u3001\u81ea\u9634\u5f71\u3001\u52a8\u6001\u5149\u7167\u4ee5\u53ca\u822a\u5929\u5668\u95f4\u76f8\u5bf9\u8fd0\u52a8\u5bfc\u81f4\u7684\u6210\u50cf\u8ddd\u79bb\u548c\u59ff\u6001\u53d8\u5316\u3002\u73b0\u6709\u6a21\u62df\u65b9\u6cd5\u96be\u4ee5\u4e13\u95e8\u4f18\u5316\u68c0\u67e5\u8f68\u8ff9\u4ee5\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u5f00\u53d1\u2202LITE\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u6a21\u62df\u7ba1\u9053\uff0c\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u5de5\u5177\u548c\u81ea\u5b9a\u4e49\u8f68\u9053\u4f20\u64ad\u5668\uff0c\u57fa\u4e8e\u89c6\u89c9\u4f20\u611f\u5668\u6570\u636e\u5bf9\u8f68\u9053\u53c2\u6570\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u81ea\u52a8\u8bbe\u8ba1\u975e\u663e\u800c\u6613\u89c1\u7684\u68c0\u67e5\u8f68\u8ff9\u3002", "result": "\u2202LITE\u80fd\u591f\u81ea\u52a8\u8bbe\u8ba1\u975e\u663e\u800c\u6613\u89c1\u7684\u68c0\u67e5\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u83b7\u53d6\u6570\u636e\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u73b0\u4ee3\u822a\u5929\u5668\u4efb\u52a1\u89c4\u5212\u8ba1\u7b97\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "conclusion": "\u2202LITE\u662f\u9996\u4e2a\u53ef\u5fae\u5206\u68c0\u67e5\u89c4\u5212\u7ba1\u9053\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u5728\u8f68\u68c0\u67e5\u4e2d\u7684\u89c6\u89c9\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u822a\u5929\u5668\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u8ba1\u7b97\u6846\u67b6\u3002"}}
{"id": "2512.16145", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16145", "abs": "https://arxiv.org/abs/2512.16145", "authors": ["Pengyu Wang", "Shuchang Ye", "Usman Naseem", "Jinman Kim"], "title": "MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation", "comment": "12 pages", "summary": "Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured \"thinking report\" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5MRG-R1\uff0c\u901a\u8fc7\u62a5\u544a\u7ea7\u5956\u52b1\u4f18\u5316\u4e34\u5e8a\u6b63\u786e\u6027\u800c\u975e\u8bed\u8a00\u98ce\u683c\u6a21\u4eff\uff0c\u5728\u533b\u5b66\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u62a5\u544a\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u6a21\u4eff\u653e\u5c04\u79d1\u533b\u751f\u7684\u8bed\u8a00\u98ce\u683c\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u4e34\u5e8a\u6b63\u786e\u6027\uff0c\u56e0\u4e3a\u57fa\u4e8etoken\u7ea7\u76ee\u6807\u7684\u8bad\u7ec3\u5173\u6ce8\u8bcd\u6c47\u9009\u62e9\u548c\u53e5\u5b50\u7ed3\u6784\u800c\u975e\u5b9e\u9645\u533b\u5b66\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u9a71\u52a8\u5f3a\u5316\u5b66\u4e60(SRL)\u65b9\u6cd5\uff0c\u91c7\u7528Group Relative Policy Optimization(GRPO)\u4f18\u5316\u62a5\u544a\u7ea7\u5956\u52b1\uff1a\u57fa\u4e8e\u751f\u6210\u62a5\u544a\u548c\u53c2\u8003\u62a5\u544a\u4e2d\u63d0\u53d6\u7684\u5173\u952e\u653e\u5c04\u5b66\u53d1\u73b0\u8ba1\u7b97margin-based\u4f59\u5f26\u76f8\u4f3c\u5ea6(MCCS)\uff0c\u76f4\u63a5\u5bf9\u9f50\u4e34\u5e8a\u6807\u7b7e\u4e00\u81f4\u6027\u3002\u540c\u65f6\u4f7f\u7528\u8f7b\u91cf\u7ea7\u63a8\u7406\u683c\u5f0f\u7ea6\u675f\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u7ed3\u6784\u5316\"\u601d\u8003\u62a5\u544a\"\u3002", "result": "\u5728IU X-Ray\u548cMIMIC-CXR\u6570\u636e\u96c6\u4e0a\uff0cMRG-R1\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1aCE-F1\u5206\u522b\u4e3a51.88\u548c40.39\u3002\u5b9e\u9a8c\u8868\u660e\u6807\u7b7e\u8bed\u4e49\u5f3a\u5316\u4f18\u4e8e\u4f20\u7edf\u7684token\u7ea7\u76d1\u7763\u3002", "conclusion": "\u4f18\u5316\u57fa\u4e8e\u4e34\u5e8a\u7684\u62a5\u544a\u7ea7\u5956\u52b1\u800c\u975etoken\u91cd\u53e0\u80fd\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u6b63\u786e\u6027\u3002\u8fd9\u662f\u63a2\u7d22\u8bed\u4e49\u5f3a\u5316\u76d1\u7763\u533b\u5b66\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u533b\u5b66\u6b63\u786e\u6027\u7684\u5148\u9a71\u5de5\u4f5c\u3002"}}
{"id": "2512.16019", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16019", "abs": "https://arxiv.org/abs/2512.16019", "authors": ["Qiping Zhang", "Nathan Tsoi", "Mofeed Nagib", "Hao-Tien Lewis Chiang", "Marynel V\u00e1zquez"], "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios", "comment": null, "summary": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\u9884\u6d4b\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u793e\u4ea4\u5bfc\u822a\u884c\u4e3a\u7684\u8bc4\u4ef7\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u4e14\u6240\u9700\u6807\u6ce8\u6570\u636e\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6765\u8bc4\u4f30\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u611f\u77e5\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u9650\u5236\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u6570\u636e\u9700\u6c42\u66f4\u5c11\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u793e\u4ea4\u884c\u4e3a\u7684\u8bc4\u4ef7\u3002", "method": "\u6269\u5c55SEAN TOGETHER\u6570\u636e\u96c6\uff0c\u589e\u52a0\u771f\u5b9e\u4e16\u754c\u4eba\u673a\u5bfc\u822a\u573a\u666f\u548c\u53c2\u4e0e\u8005\u53cd\u9988\u3002\u5229\u7528\u591a\u4e2aLLM\u57fa\u4e8e\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u901a\u8fc7\u89c2\u5bdf\u673a\u5668\u4eba\u548c\u5468\u56f4\u4eba\u7c7b\u8fd0\u52a8\u7684\u65f6\u7a7a\u7ebf\u7d22\u6765\u9884\u6d4b\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u6027\u80fd\u7684\u611f\u77e5\u3002\u8fdb\u884c\u8f93\u5165\u7279\u5f81\u6d88\u878d\u7814\u7a76\uff0c\u5e76\u63a2\u7d22\u4e2a\u6027\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08\u4f7f\u7528\u540c\u4e00\u7528\u6237\u7684\u793a\u4f8b\uff09\u3002", "result": "LLM\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6240\u9700\u6807\u6ce8\u5b9e\u4f8b\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u66f4\u591a\u4e0a\u4e0b\u6587\u793a\u4f8b\u80fd\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u4e2a\u6027\u5316\u793a\u4f8b\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86LLM\u7528\u4e8e\u63a8\u7406\u7684\u4f20\u611f\u5668\u4fe1\u606f\u7c7b\u578b\u3002", "conclusion": "LLM\u7684\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u4e3a\u901a\u8fc7\u7528\u6237\u4e2d\u5fc3\u53cd\u9988\u6539\u8fdb\u673a\u5668\u4eba\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u51cf\u5c11\u4e86\u6570\u636e\u6807\u6ce8\u9700\u6c42\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u66f4\u597d\u5730\u7406\u89e3\u548c\u9002\u5e94\u4eba\u7c7b\u671f\u671b\u3002"}}
{"id": "2512.16147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16147", "abs": "https://arxiv.org/abs/2512.16147", "authors": ["Yash Bhaskar", "Sankalp Bahad", "Parameswari Krishnamurthy"], "title": "Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning", "comment": "Accepted Paper, Anthology ID: 2024.icon-fauxhate.3, 4 pages, 1 figure, 1 table", "summary": "Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \\cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8eFaux-Hate\u5171\u4eab\u4efb\u52a1\u7684\u7cfb\u7edf\uff0c\u8be5\u4efb\u52a1\u65e8\u5728\u68c0\u6d4b\u7531\u865a\u5047\u53d9\u4e8b\u9a71\u52a8\u7684\u4ec7\u6068\u8a00\u8bba\uff0c\u7279\u522b\u662f\u5728\u5370\u5730\u8bed-\u82f1\u8bed\u6df7\u5408\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u3002\u7cfb\u7edf\u5904\u7406\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a\u4e8c\u5143\u865a\u5047\u4ec7\u6068\u68c0\u6d4b\u4ee5\u53ca\u76ee\u6807\u548c\u4e25\u91cd\u6027\u9884\u6d4b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5df2\u6210\u4e3a\u6709\u5bb3\u5185\u5bb9\uff08\u5305\u62ec\u4ec7\u6068\u8a00\u8bba\u548c\u865a\u5047\u53d9\u4e8b\uff09\u5feb\u901f\u4f20\u64ad\u7684\u4e2d\u5fc3\u3002Faux-Hate\u5171\u4eab\u4efb\u52a1\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u7531\u865a\u5047\u53d9\u4e8b\u9a71\u52a8\u7684\u4ec7\u6068\u8a00\u8bba\u8fd9\u4e00\u7279\u5b9a\u73b0\u8c61\uff0c\u8fd9\u5728\u5370\u5730\u8bed-\u82f1\u8bed\u6df7\u5408\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u5c24\u4e3a\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u5148\u8fdb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u4e0e\u9886\u57df\u7279\u5b9a\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u6765\u5904\u7406\u4e8c\u5143\u865a\u5047\u4ec7\u6068\u68c0\u6d4b\u4ee5\u53ca\u76ee\u6807\u548c\u4e25\u91cd\u6027\u9884\u6d4b\u4e24\u4e2a\u5b50\u4efb\u52a1\u3002", "result": "\u7cfb\u7edf\u5728\u5171\u4eab\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u5904\u7406\u8fd9\u4e00\u590d\u6742\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u5148\u8fdbNLP\u6280\u672f\u548c\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u7531\u865a\u5047\u53d9\u4e8b\u9a71\u52a8\u7684\u4ec7\u6068\u8a00\u8bba\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16024", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16024", "abs": "https://arxiv.org/abs/2512.16024", "authors": ["Rishabh Dev Yadav", "Shrey Agrawal", "Kamalakar Karlapalem"], "title": "Maintaining the Level of a Payload carried by Multi-Robot System on Irregular Surface", "comment": null, "summary": "In this paper, we introduce a multi robot payload transport system to carry payloads through an environment of unknown and uneven inclinations while maintaining the desired orientation of the payload. For this task, we used custom built robots with a linear actuator (pistons) mounted on top of each robot. The system continuously monitors the payload's orientation and computes the required piston height of each robot to maintain the desired orientation of the payload. In this work, we propose an open loop controller coupled with a closed loop PID controller to achieve the goal. As our modelling makes no assumptions on the type of terrain, the system can work on any unknown and uneven terrains and inclinations. We showcase the efficacy of our proposed controller by testing it on various simulated environments with varied and complex terrains.", "AI": {"tldr": "\u591a\u673a\u5668\u4eba\u8d1f\u8f7d\u8fd0\u8f93\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ebf\u6027\u6267\u884c\u5668\u8c03\u8282\u673a\u5668\u4eba\u9ad8\u5ea6\uff0c\u5728\u672a\u77e5\u4e0d\u5e73\u5766\u5730\u5f62\u4e2d\u4fdd\u6301\u8d1f\u8f7d\u671f\u671b\u65b9\u5411", "motivation": "\u89e3\u51b3\u5728\u672a\u77e5\u4e14\u4e0d\u5e73\u5766\u7684\u503e\u659c\u73af\u5883\u4e2d\u8fd0\u8f93\u8d1f\u8f7d\u65f6\uff0c\u5982\u4f55\u4fdd\u6301\u8d1f\u8f7d\u671f\u671b\u65b9\u5411\u7684\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5730\u5f62\u5df2\u77e5\u6216\u5e73\u5766\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u591a\u53d8\u7684\u5730\u5f62\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u5e26\u6709\u7ebf\u6027\u6267\u884c\u5668\uff08\u6d3b\u585e\uff09\u7684\u81ea\u5b9a\u4e49\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u5f00\u73af\u63a7\u5236\u5668\u548c\u95ed\u73afPID\u63a7\u5236\u5668\u3002\u7cfb\u7edf\u6301\u7eed\u76d1\u6d4b\u8d1f\u8f7d\u65b9\u5411\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u673a\u5668\u4eba\u6240\u9700\u7684\u6d3b\u585e\u9ad8\u5ea6\u6765\u7ef4\u6301\u8d1f\u8f7d\u671f\u671b\u65b9\u5411\u3002\u6a21\u578b\u4e0d\u5bf9\u5730\u5f62\u7c7b\u578b\u505a\u4efb\u4f55\u5047\u8bbe\u3002", "result": "\u5728\u591a\u79cd\u5177\u6709\u53d8\u5316\u548c\u590d\u6742\u5730\u5f62\u7684\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u63a7\u5236\u5668\u7684\u6709\u6548\u6027\u3002\u7cfb\u7edf\u80fd\u591f\u5728\u4efb\u4f55\u672a\u77e5\u548c\u4e0d\u5e73\u5766\u7684\u5730\u5f62\u53ca\u503e\u659c\u6761\u4ef6\u4e0b\u5de5\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u673a\u5668\u4eba\u8d1f\u8f7d\u8fd0\u8f93\u7cfb\u7edf\u80fd\u591f\u5728\u672a\u77e5\u4e0d\u5e73\u5766\u5730\u5f62\u4e2d\u6709\u6548\u4fdd\u6301\u8d1f\u8f7d\u65b9\u5411\uff0c\u7ed3\u5408\u5f00\u73af\u548c\u95ed\u73af\u63a7\u5236\u7684\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.16183", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.16183", "abs": "https://arxiv.org/abs/2512.16183", "authors": ["Mengfan Shen", "Kangqi Song", "Xindi Wang", "Wei Jia", "Tao Wang", "Ziqiang Han"], "title": "A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media", "comment": "41 pages,3figures and 9 tables", "summary": "Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.", "AI": {"tldr": "\u57fa\u4e8eQwen2.5-7B\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u6784\u5efa\u4e86\u4ece\u8b66\u65b9\u901a\u62a5\u4e2d\u63d0\u53d615\u4e2a\u5173\u952e\u5b57\u6bb5\u7684\u9ad8\u6548\u4fe1\u606f\u62bd\u53d6\u7ba1\u9053\uff0c\u5728\u6b7b\u4ea1\u68c0\u6d4b\u3001\u4f24\u4ea1\u7edf\u8ba1\u548c\u4f4d\u7f6e\u63d0\u53d6\u7b49\u4efb\u52a1\u4e0a\u8fbe\u523095%\u4ee5\u4e0a\u51c6\u786e\u7387\u3002", "motivation": "\u8b66\u65b9\u4e8b\u6545\u901a\u62a5\u7b49\u975e\u7ed3\u6784\u5316\u6587\u672c\u4fe1\u606f\u62bd\u53d6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u7684\u591a\u6837\u6027\u548c\u975e\u6b63\u5f0f\u6027\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u3002", "method": "\u91c7\u7528Qwen2.5-7B\u6a21\u578b\uff0c\u7ed3\u5408LoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u9488\u5bf9\u6027\u63d0\u793a\u5de5\u7a0b\uff0c\u6784\u5efa\u9886\u57df\u9002\u5e94\u7684\u62bd\u53d6\u7ba1\u9053\uff0c\u5904\u7406\u4ece27,822\u6761\u5fae\u535a\u8b66\u65b9\u901a\u62a5\u4e2d\u6784\u5efa\u76844,933\u6761\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3002", "result": "LoRA\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u548c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u6b7b\u4ea1\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc798.36%\uff0c\u6b7b\u4ea1\u4eba\u6570\u7cbe\u786e\u5339\u914d\u7387\u8fbe95.31%\uff0c\u7701\u7ea7\u4f4d\u7f6e\u63d0\u53d6\u7cbe\u786e\u5339\u914d\u7387\u8fbe95.54%\uff0c\u6210\u529f\u63d0\u53d615\u4e2a\u5173\u952e\u5b57\u6bb5\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4e3a\u4e13\u4e1a\u9886\u57df\u591a\u4efb\u52a1\u7ed3\u6784\u5316\u4fe1\u606f\u62bd\u53d6\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u6709\u6548\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u5316\u4e3a\u53ef\u9760\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u652f\u6301\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2512.16027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16027", "abs": "https://arxiv.org/abs/2512.16027", "authors": ["Shuaidong Ji", "Mahdi Bamdad", "Francisco Cruz"], "title": "SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments", "comment": "10 pages, Accepted at Australasian Conference on Robotics and Automation (ACRA) 2025", "summary": "Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.", "AI": {"tldr": "SWIFT-Nav\uff1a\u57fa\u4e8eTD3\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u7cca\u903b\u8f91\u5b89\u5168\u5c42\u548c\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u6536\u655b\uff0c\u5728\u6742\u4e71\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u6742\u4e71\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8f68\u8ff9\u5e73\u6ed1\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u65f6\u54cd\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSWIFT-Nav\u6846\u67b6\uff1a1\uff09\u4f20\u611f\u5668\u9a71\u52a8\u7684\u611f\u77e5\u524d\u7aef\u5c06LiDAR\u6570\u636e\u8f6c\u6362\u4e3a\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u5b89\u5168\u5730\u56fe\u548c\u76ee\u6807\u7ebf\u7d22\uff1b2\uff09TD3\u822a\u70b9\u7b56\u7565\u4f7f\u7528\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u548c\u8870\u51cf\u03b5-\u8d2a\u5a6a\u63a2\u7d22\uff1b3\uff09\u8f7b\u91cf\u7ea7\u6a21\u7cca\u903b\u8f91\u5c42\u8ba1\u7b97\u5b89\u5168\u5206\u6570\u3001\u63a7\u5236\u6a21\u5f0f\u5207\u6362\u5e76\u9650\u5236\u4e0d\u5b89\u5168\u52a8\u4f5c\uff1b4\uff09\u4efb\u52a1\u5bf9\u9f50\u7684\u5956\u52b1\u5851\u5f62\u7ed3\u5408\u76ee\u6807\u8fdb\u5ea6\u3001\u907f\u969c\u8ddd\u79bb\u548c\u5207\u6362\u7ecf\u6d4e\u6027\u3002", "result": "\u5728Webots\u4e2d\u5b9e\u73b0\uff0c\u57fa\u4e8e\u63a5\u8fd1\u5ea6\u7684\u78b0\u649e\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728\u8f68\u8ff9\u5e73\u6ed1\u6027\u548c\u5bf9\u672a\u89c1\u5e03\u5c40\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u54cd\u5e94\u6027\u3002", "conclusion": "\u5c06TD3\u4e0e\u56de\u653e\u4f18\u5148\u7ea7\u3001\u6821\u51c6\u63a2\u7d22\u548c\u6a21\u7cca\u5b89\u5168\u89c4\u5219\u76f8\u7ed3\u5408\uff0c\u4e3a\u6742\u4e71\u573a\u666f\u4e2d\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16189", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16189", "abs": "https://arxiv.org/abs/2512.16189", "authors": ["Musarrat Zeba", "Abdullah Al Mamun", "Kishoar Jahan Tithee", "Debopom Sutradhar", "Mohaimenul Azam Khan Raiaan", "Saddam Mukta", "Reem E. Mohamed", "Md Rafiqul Islam", "Yakub Sebastian", "Mukhtar Hussain", "Sami Azam"], "title": "Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation", "comment": null, "summary": "In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u72ec\u7acb\u4e8eLLM\u7684\u4e8b\u5b9e\u6838\u67e5\u6a21\u5757\u548c\u9886\u57df\u7279\u5b9a\u7684\u6458\u8981\u6a21\u578b\uff0c\u7528\u4e8e\u533b\u7597\u9886\u57df\u51cf\u5c11\u5e7b\u89c9\uff0c\u5728MIMIC-III\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6548\u679c\u826f\u597d", "motivation": "\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\uff0cLLM\u751f\u6210\u7684\u8f93\u51fa\u5fc5\u987b\u53ef\u9760\u51c6\u786e\uff0c\u7279\u522b\u662f\u5728\u51b3\u7b56\u548c\u60a3\u8005\u5b89\u5168\u65b9\u9762\u3002\u7136\u800c\uff0cLLM\u5b58\u5728\u5e7b\u89c9\u98ce\u9669\uff0c\u5bfc\u81f4\u5173\u952e\u9886\u57df\u8f93\u51fa\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1) \u63d0\u51fa\u72ec\u7acb\u4e8eLLM\u7684\u4e8b\u5b9e\u6838\u67e5\u6a21\u5757\uff0c\u4f7f\u7528\u6570\u503c\u6b63\u786e\u6027\u6d4b\u8bd5\u548c\u901a\u8fc7\u79bb\u6563\u903b\u8f91\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8fdb\u884c\u7ec6\u7c92\u5ea6\u903b\u8f91\u68c0\u67e5\uff0c\u9a8c\u8bc1\u4e8b\u5b9e\u4e0e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u4e00\u81f4\u6027\uff1b2) \u5f00\u53d1\u9886\u57df\u7279\u5b9a\u7684\u6458\u8981\u6a21\u578b\uff0c\u4f7f\u7528LoRa\u5728MIMIC-III\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u51cf\u5c11\u5e7b\u89c9\u7387\uff1b3) \u5c06\u4e24\u4e2a\u6a21\u5757\u914d\u5bf9\u4f7f\u7528\u3002", "result": "\u4e8b\u5b9e\u6838\u67e5\u6a21\u5757\u5728104\u4e2a\u6458\u8981\u63d0\u53d6\u76843,786\u4e2a\u547d\u9898\u4e0a\u6d4b\u8bd5\uff0c\u8fbe\u5230\u7cbe\u5ea60.8904\u3001\u53ec\u56de\u73870.8234\u3001F1\u5206\u65700.8556\u3002LLM\u6458\u8981\u6a21\u578b\u83b7\u5f97ROUGE-1\u5206\u65700.5797\u548cBERTScore 0.9120\u3002", "conclusion": "\u63d0\u51fa\u7684\u72ec\u7acb\u4e8b\u5b9e\u6838\u67e5\u6a21\u5757\u548c\u9886\u57df\u7279\u5b9a\u6458\u8981\u6a21\u578b\u80fd\u6709\u6548\u51cf\u5c11\u533b\u7597\u9886\u57dfLLM\u8f93\u51fa\u7684\u5e7b\u89c9\uff0c\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4e3a\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u66f4\u5b89\u5168\u7684AI\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2512.16069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16069", "abs": "https://arxiv.org/abs/2512.16069", "authors": ["Maolin Lei", "Edoardo Romiti", "Arturo Laurenzi", "Rui Dai", "Matteo Dalle Vedove", "Jiatao Ding", "Daniele Fontanelli", "Nikos Tsagarakis"], "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators", "comment": null, "summary": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u4efb\u52a1\u9a71\u52a8\u8ba1\u7b97\u6846\u67b6\uff0c\u96c6\u6210\u8f68\u8ff9\u89c4\u5212\u4e0e\u5f62\u6001\u548c\u5b89\u88c5\u59ff\u6001\u7684\u534f\u540c\u4f18\u5316\uff0c\u901a\u8fc7HMPC\u7b56\u7565\u548cCMA-ES\u7b97\u6cd5\u5b9e\u73b0\u6a21\u5757\u5316\u673a\u68b0\u81c2\u7684\u4f18\u5316\u8bbe\u8ba1\uff0c\u5f15\u5165\u865a\u62df\u6a21\u5757\u62bd\u8c61\u652f\u6301\u53cc\u5206\u652f\u5f62\u6001\u6269\u5c55\u5de5\u4f5c\u7a7a\u95f4\u3002", "motivation": "\u6a21\u5757\u5316\u673a\u68b0\u81c2\u867d\u7136\u5177\u6709\u9ad8\u9002\u5e94\u6027\uff0c\u4f46\u5728\u90e8\u7f72\u65f6\u9700\u8981\u540c\u65f6\u8003\u8651\u8fd0\u52a8\u89c4\u5212\u3001\u5f62\u6001\u4f18\u5316\u548c\u5b89\u88c5\u59ff\u6001\uff0c\u4f20\u7edf\u5355\u5206\u652f\u8bbe\u8ba1\u901a\u8fc7\u589e\u52a0\u8fde\u6746\u957f\u5ea6\u6269\u5c55\u5de5\u4f5c\u7a7a\u95f4\u5bb9\u6613\u8fdd\u53cd\u57fa\u5173\u8282\u626d\u77e9\u9650\u5236\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u4efb\u52a1\u9a71\u52a8\u8ba1\u7b97\u6846\u67b6\uff0c\u96c6\u6210\u8f68\u8ff9\u89c4\u5212\u548c\u5f62\u6001/\u5b89\u88c5\u59ff\u6001\u534f\u540c\u4f18\u5316\uff1b\u91c7\u7528\u5206\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236(HMPC)\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\uff1b\u4f7f\u7528CMA-ES\u7b97\u6cd5\u63a2\u7d22\u79bb\u6563\u5f62\u6001\u914d\u7f6e\u548c\u8fde\u7eed\u5b89\u88c5\u59ff\u6001\u7684\u6df7\u5408\u641c\u7d22\u7a7a\u95f4\uff1b\u5f15\u5165\u865a\u62df\u6a21\u5757\u62bd\u8c61\u5b9e\u73b0\u53cc\u5206\u652f\u5f62\u6001\uff0c\u901a\u8fc7\u8f85\u52a9\u5206\u652f\u5206\u62c5\u4e3b\u5206\u652f\u626d\u77e9\u3002", "result": "\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\uff08\u629b\u5149\u3001\u94bb\u5b54\u3001\u62fe\u653e\u4efb\u52a1\uff09\u8868\u660e\uff1a1\uff09\u6846\u67b6\u80fd\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u751f\u6210\u591a\u4e2a\u6ee1\u8db3\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u7ea6\u675f\u3001\u907f\u514d\u73af\u5883\u78b0\u649e\u7684\u53ef\u884c\u8bbe\u8ba1\uff1b2\uff09\u901a\u8fc7\u5b9a\u5236\u6210\u672c\u51fd\u6570\u53ef\u5b9e\u73b0\u6700\u5927\u5316\u53ef\u64cd\u4f5c\u6027\u3001\u6700\u5c0f\u5316\u5173\u8282\u52aa\u529b\u6216\u51cf\u5c11\u6a21\u5757\u6570\u91cf\u7b49\u7075\u6d3b\u8bbe\u8ba1\u76ee\u6807\uff1b3\uff09\u53cc\u5206\u652f\u5f62\u6001\u80fd\u5728\u4e0d\u589e\u52a0\u57fa\u7840\u6a21\u5757\u529f\u7387\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5927\u5de5\u4f5c\u7a7a\u95f4\u64cd\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u5757\u5316\u673a\u68b0\u81c2\u90e8\u7f72\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u3001\u5f62\u6001\u4f18\u5316\u548c\u5b89\u88c5\u59ff\u6001\u534f\u540c\u4f18\u5316\u95ee\u9898\uff0c\u53cc\u5206\u652f\u8bbe\u8ba1\u6269\u5c55\u4e86\u5de5\u4f5c\u7a7a\u95f4\u800c\u4e0d\u589e\u52a0\u5355\u4e2a\u5173\u8282\u6a21\u5757\u7684\u5bb9\u91cf\uff0c\u4e3a\u6a21\u5757\u5316\u673a\u68b0\u81c2\u7684\u7075\u6d3b\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16227", "abs": "https://arxiv.org/abs/2512.16227", "authors": ["Qizhou Chen", "Chengyu Wang", "Taolin Zhang", "Xiaofeng He"], "title": "An Information-Theoretic Framework for Robust Large Language Model Editing", "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.", "AI": {"tldr": "IBKE\uff1a\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7406\u8bba\u7684\u77e5\u8bc6\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u5173\u952e\u4fe1\u606f\u5b9e\u73b0\u53ef\u6cdb\u5316\u7684LLM\u77e5\u8bc6\u66f4\u65b0\uff0c\u907f\u514d\u5168\u6a21\u578b\u91cd\u8bad\u7ec3", "motivation": "LLMs\u5728\u79d1\u5b66\u548c\u793e\u4f1a\u5e94\u7528\u4e2d\u5b58\u5728\u9519\u8bef\u6216\u8fc7\u65f6\u4fe1\u606f\uff0c\u4f46\u5168\u6a21\u578b\u91cd\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u7834\u574f\u6027\u5927\u3002\u73b0\u6709\u7f16\u8f91\u6280\u672f\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5bb9\u6613\u4ea7\u751f\u610f\u5916\u526f\u4f5c\u7528\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7406\u8bba\uff0c\u538b\u7f29\u548c\u9694\u79bb\u5b9e\u73b0\u6cdb\u5316\u77e5\u8bc6\u6821\u6b63\u6240\u9700\u7684\u5173\u952e\u4fe1\u606f\uff0c\u6700\u5c0f\u5316\u5bf9\u65e0\u5173\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u63d0\u51faIBKE\u6846\u67b6\uff0c\u5229\u7528\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\u6307\u5bfc\u57fa\u4e8e\u68af\u5ea6\u7684\u66f4\u65b0\u3002", "result": "\u5728\u591a\u79cdLLM\u67b6\u6784\u548c\u6807\u51c6\u57fa\u51c6\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u7f16\u8f91\u7684\u6cdb\u5316\u6027\u548c\u7279\u5f02\u6027\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u6709\u539f\u5219\u4e14\u5b9e\u7528\u7684\u5f00\u653e\u57df\u77e5\u8bc6\u7f16\u8f91\u8303\u5f0f\uff0c\u63d0\u5347\u4e86LLMs\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2512.16076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16076", "abs": "https://arxiv.org/abs/2512.16076", "authors": ["Jia Hu", "Junqi Li", "Xuerun Yan", "Jintao Lai", "Lianhua An"], "title": "A simulation platform calibration method for automated vehicle evaluation: accurate on both vehicle level and traffic flow level", "comment": null, "summary": "Simulation testing is a fundamental approach for evaluating automated vehicles (AVs). To ensure its reliability, it is crucial to accurately replicate interactions between AVs and background traffic, which necessitates effective calibration. However, existing calibration methods often fall short in achieving this goal. To address this gap, this study introduces a simulation platform calibration method that ensures high accuracy at both the vehicle and traffic flow levels. The method offers several key features:(1) with the capability of calibration for vehicle-to-vehicle interaction; (2) with accuracy assurance; (3) with enhanced efficiency; (4) with pipeline calibration capability. The proposed method is benchmarked against a baseline with no calibration and a state-of-the-art calibration method. Results show that it enhances the accuracy of interaction replication by 83.53% and boosts calibration efficiency by 76.75%. Furthermore, it maintains accuracy across both vehicle-level and traffic flow-level metrics, with an improvement of 51.9%. Notably, the entire calibration process is fully automated, requiring no human intervention.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u5e73\u53f0\u6821\u51c6\u65b9\u6cd5\uff0c\u80fd\u5728\u8f66\u8f86\u548c\u4ea4\u901a\u6d41\u4e24\u4e2a\u5c42\u9762\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6821\u51c6\uff0c\u63d0\u5347\u4ea4\u4e92\u590d\u73b0\u7cbe\u5ea683.53%\uff0c\u6821\u51c6\u6548\u738776.75%", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u6d4b\u8bd5\u4e2d\u7684\u6821\u51c6\u65b9\u6cd5\u5728\u51c6\u786e\u590d\u73b0AV\u4e0e\u80cc\u666f\u4ea4\u901a\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6821\u51c6\u65b9\u6cd5\u6765\u786e\u4fdd\u4eff\u771f\u53ef\u9760\u6027", "method": "\u63d0\u51fa\u4e00\u79cd\u4eff\u771f\u5e73\u53f0\u6821\u51c6\u65b9\u6cd5\uff0c\u5177\u5907\u8f66\u8f86\u95f4\u4ea4\u4e92\u6821\u51c6\u80fd\u529b\u3001\u7cbe\u5ea6\u4fdd\u8bc1\u3001\u6548\u7387\u63d0\u5347\u548c\u6d41\u6c34\u7ebf\u6821\u51c6\u80fd\u529b\uff0c\u91c7\u7528\u5168\u81ea\u52a8\u5316\u6821\u51c6\u6d41\u7a0b", "result": "\u76f8\u6bd4\u65e0\u6821\u51c6\u57fa\u7ebf\u548c\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u4ea4\u4e92\u590d\u73b0\u7cbe\u5ea6\u63d0\u534783.53%\uff0c\u6821\u51c6\u6548\u7387\u63d0\u534776.75%\uff0c\u8f66\u8f86\u7ea7\u548c\u4ea4\u901a\u6d41\u7ea7\u6307\u6807\u7cbe\u5ea6\u63d0\u534751.9%", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u6821\u51c6\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u7684\u5168\u81ea\u52a8\u5316\u6821\u51c6\uff0c\u4e3a\u53ef\u9760\u7684\u4eff\u771f\u6d4b\u8bd5\u63d0\u4f9b\u4fdd\u969c"}}
{"id": "2512.16229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16229", "abs": "https://arxiv.org/abs/2512.16229", "authors": ["Chenkai Xu", "Yijie Jin", "Jiajun Li", "Yi Tu", "Guoping Long", "Dandan Tu", "Tianqi Hou", "Junchi Yan", "Zhijie Deng"], "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.", "AI": {"tldr": "LoPA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u5e76\u884c\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316token\u586b\u5145\u987a\u5e8f\u5c06\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u81f3\u6bcf\u524d\u5411\u4f20\u901210.1\u4e2atoken\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u5206\u652f\u5e76\u884c\u63a8\u7406\u7cfb\u7edf\u5b9e\u73b01073.9 tokens/\u79d2\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u89e3\u7801\u7b56\u7565\u5e76\u884c\u6027\u6709\u9650\uff0c\u901a\u5e38\u6bcf\u524d\u5411\u4f20\u9012\u53ea\u80fd\u751f\u62101-3\u4e2atoken\uff0c\u63a8\u7406\u901f\u5ea6\u53d7\u9650\u3002\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u5e76\u884c\u5ea6\u5bf9token\u586b\u5145\u987a\u5e8f\u9ad8\u5ea6\u654f\u611f\uff0c\u9700\u8981\u4f18\u5316\u8fd9\u4e00\u987a\u5e8f\u6765\u52a0\u901f\u63a8\u7406\u3002", "method": "\u63d0\u51faLoPA\u7b97\u6cd5\uff1a\u901a\u8fc7\u5e76\u884c\u5206\u652f\u63a2\u7d22\u4e0d\u540c\u7684\u5019\u9009token\u586b\u5145\u987a\u5e8f\uff0c\u57fa\u4e8e\u5206\u652f\u7f6e\u4fe1\u5ea6\u9009\u62e9\u5177\u6709\u6700\u5927\u672a\u6765\u5e76\u884c\u6f5c\u529b\u7684\u987a\u5e8f\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u652f\u6301\u5206\u652f\u5e76\u884c\u7684\u591a\u8bbe\u5907\u63a8\u7406\u7cfb\u7edf\u3002", "result": "\u5728D2F-Dream\u6a21\u578b\u4e0a\uff0cLoPA\u5c06GSM8K\u6570\u636e\u96c6\u4e0a\u7684\u6bcf\u524d\u5411\u4f20\u9012token\u6570\u63d0\u5347\u81f310.1\uff0c\u6027\u80fd\u4f18\u4e8eDream\u57fa\u7ebf\u3002\u591aGPU\u90e8\u7f72\u4e0b\u5b9e\u73b0\u5355\u6837\u672c1073.9 tokens/\u79d2\u7684\u541e\u5410\u91cf\u3002", "conclusion": "LoPA\u901a\u8fc7\u4f18\u5316token\u586b\u5145\u987a\u5e8f\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5e76\u884c\u5ea6\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u65e0\u5173\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e3a\u9ad8\u6548dLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16302", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16302", "abs": "https://arxiv.org/abs/2512.16302", "authors": ["Zixuan Chen", "Chongkai Gao", "Lin Shao", "Jieqi Shi", "Jing Huo", "Yang Gao"], "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation", "comment": "Accepted by AAAI 2026", "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.", "AI": {"tldr": "ManiLong-Shot \u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u7684\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4ea4\u4e92\u611f\u77e5\u7684\u57fa\u5143\u5e8f\u5217\uff0c\u5b9e\u73b0\u4e86\u4ece\u77ed\u89c6\u91ce\u4efb\u52a1\u5230\u957f\u89c6\u91ce\u4efb\u52a1\u7684\u6cdb\u5316\u3002", "motivation": "\u5f53\u524d\u7684\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u77ed\u89c6\u91ce\u4efb\u52a1\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7684\u957f\u671f\u64cd\u4f5c\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5c06\u957f\u89c6\u91ce\u4efb\u52a1\u56f4\u7ed5\u7269\u7406\u4ea4\u4e92\u4e8b\u4ef6\u7ed3\u6784\u5316\uff0c\u5c06\u5176\u91cd\u6784\u4e3a\u5e8f\u5217\u5316\u4ea4\u4e92\u611f\u77e5\u57fa\u5143\u800c\u975e\u76f4\u63a5\u6a21\u4eff\u8fde\u7eed\u8f68\u8ff9\u3002\u57fa\u5143\u5206\u89e3\u53ef\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u5c42\u63a8\u7406\u6216\u57fa\u4e8e\u673a\u5668\u4eba\u72b6\u6001\u53d8\u5316\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u9a71\u52a8\u3002\u5bf9\u6bcf\u4e2a\u57fa\u5143\uff0c\u9884\u6d4b\u5173\u952e\u4ea4\u4e92\u7684\u4e0d\u53d8\u533a\u57df\uff0c\u5efa\u7acb\u6f14\u793a\u4e0e\u5f53\u524d\u89c2\u6d4b\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u8ba1\u7b97\u76ee\u6807\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\u3002", "result": "\u5728\u4ec5\u4f7f\u752810\u4e2a\u77ed\u89c6\u91ce\u4efb\u52a1\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0cManiLong-Shot \u572820\u4e2a\u672a\u89c1\u7684\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e8622.8%\u7684\u76f8\u5bf9\u63d0\u5347\u3002\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e09\u4e2a\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6267\u884c\u80fd\u529b\u3002", "conclusion": "ManiLong-Shot \u901a\u8fc7\u57fa\u5143\u5206\u89e3\u548c\u4ea4\u4e92\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u6269\u5c55\u4e86\u4e00\u6b21\u6027\u6a21\u4eff\u5b66\u4e60\u5230\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.16248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16248", "abs": "https://arxiv.org/abs/2512.16248", "authors": ["Qingguo Hu", "Zhenghao Lin", "Ziyue Yang", "Yucheng Ding", "Xiao Liu", "Yuting Jiang", "Ruizhe Wang", "Tianyu Chen", "Zhongxin Guo", "Yifan Xiong", "Rui Gao", "Lei Qu", "Jinsong Su", "Peng Cheng", "Yeyun Gong"], "title": "Sigma-Moe-Tiny Technical Report", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm", "AI": {"tldr": "Sigma-MoE-Tiny\u662f\u4e00\u4e2a\u6781\u7a00\u758f\u7684MoE\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u5206\u5272\uff08\u6bcf\u5c4296\u4e2a\u4e13\u5bb6\uff09\u4f46\u6bcf\u4e2atoken\u4ec5\u6fc0\u6d3b1\u4e2a\u4e13\u5bb6\uff0c\u5b9e\u73b0200\u4ebf\u603b\u53c2\u6570\u4e2d\u4ec5\u6fc0\u6d3b5\u4ebf\u53c2\u6570\uff0c\u8fbe\u5230\u6700\u9ad8\u7a00\u758f\u5ea6\u3002", "motivation": "\u63a2\u7d22\u5728\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7a00\u758f\u5ea6\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u89e3\u51b3\u6781\u7a00\u758f\u8bbe\u7f6e\u4e0b\u4e13\u5bb6\u8d1f\u8f7d\u5e73\u8861\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u5206\u5272\uff08\u6bcf\u5c42\u6700\u591a96\u4e2a\u4e13\u5bb6\uff09\uff0c\u6bcf\u4e2atoken\u4ec5\u6fc0\u6d3b\u4e00\u4e2a\u4e13\u5bb6\uff1b\u63d0\u51fa\u6e10\u8fdb\u7a00\u758f\u5316\u8c03\u5ea6\u7b56\u7565\u6765\u89e3\u51b3\u6781\u7a00\u758f\u4e0b\u7684\u8d1f\u8f7d\u5e73\u8861\u95ee\u9898\uff1b\u5728\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8bed\u6599\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u540e\u7eed\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u975e\u5e38\u7a33\u5b9a\uff0c\u6ca1\u6709\u51fa\u73b0\u4e0d\u53ef\u6062\u590d\u7684\u635f\u5931\u5c16\u5cf0\uff1b\u5c3d\u7ba1\u4ec5\u6fc0\u6d3b5\u4ebf\u53c2\u6570\uff0c\u4f46\u5728\u540c\u7c7b\u6216\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230\u9876\u7ea7\u6027\u80fd\uff1b\u63d0\u4f9b\u4e86\u5bf9\u9ad8\u7a00\u758fMoE\u6a21\u578b\u4e2d\u8d1f\u8f7d\u5e73\u8861\u7684\u6df1\u5165\u5206\u6790\u3002", "conclusion": "Sigma-MoE-Tiny\u5c55\u793a\u4e86\u5728\u6781\u7a00\u758f\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55MoE\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u7684\u6e10\u8fdb\u7a00\u758f\u5316\u8c03\u5ea6\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8d1f\u8f7d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u672a\u6765MoE\u67b6\u6784\u7684\u7a00\u758f\u5316\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2512.16367", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16367", "abs": "https://arxiv.org/abs/2512.16367", "authors": ["Sijia Chen", "Wei Dong"], "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion", "comment": "accept by IEEE Transactions on Industrial Electronics", "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5730\u7a7a\u534f\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e3b\u52a8\u89c6\u89c9\u3001\u5355\u70b9\u6d4b\u8ddd\u3001\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u5149\u6d41\u878d\u5408\uff0c\u589e\u5f3a\u98de\u884c\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4f20\u611f\u5668\u9000\u5316\u65f6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u76f8\u673a\u89c2\u6d4b\u9884\u7f6e\u6807\u8bb0\u6765\u4f30\u8ba1\u98de\u884c\u673a\u5668\u4eba\u4f4d\u7f6e\uff0c\u4f46\u53d7\u9650\u4e8e\u8ddd\u79bb\u4e14\u5bb9\u6613\u6355\u83b7\u5931\u8d25\u3002\u5728\u89c6\u89c9\u4f20\u611f\u5668\u9000\u5316\u7684\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "1) \u5730\u9762\u8f66\u8f86\u642d\u8f7d\u4e3b\u52a8\u89c6\u89c9\u5b50\u7cfb\u7edf\uff0c\u53ef\u52a8\u6001\u65cb\u8f6c\u68c0\u6d4b\u8ddf\u8e2a\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u7ea2\u5916\u6807\u8bb0\uff1b2) \u7ed3\u5408\u5355\u70b9\u6d4b\u8ddd\u6269\u5c55\u53ef\u884c\u8ddd\u79bb\u548c\u91cd\u6355\u83b7\u80fd\u529b\uff1b3) \u57fa\u4e8e\u591a\u9879\u5f0f\u8fd1\u4f3c\u7684\u964d\u7ef4\u4f30\u8ba1\u5668\u878d\u5408\u591a\u6e90\u6d4b\u91cf\uff1b4) \u81ea\u9002\u5e94\u6ed1\u52a8\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u7b97\u6cd5\u6839\u636e\u79fb\u52a8\u65b9\u5dee\u52a8\u6001\u8c03\u6574\u6743\u91cd\u53c2\u6570\u3002", "result": "\u5728\u70df\u96fe\u5e72\u6270\u3001\u5149\u7167\u53d8\u5316\u3001\u969c\u788d\u7269\u906e\u6321\u3001\u957f\u65f6\u95f4\u89c6\u89c9\u4e22\u5931\u548c\u6269\u5c55\u64cd\u4f5c\u8303\u56f4\u7b49\u6761\u4ef6\u4e0b\uff0c\u5e73\u5747\u5747\u65b9\u6839\u8bef\u5dee\u7ea60.09\u7c73\uff0c\u4fdd\u6301\u5bf9\u6355\u83b7\u4e22\u5931\u548c\u4f20\u611f\u5668\u6545\u969c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5730\u7a7a\u534f\u4f5c\u5b9a\u4f4d\u6846\u67b6\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u5728\u7ebf\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.16287", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16287", "abs": "https://arxiv.org/abs/2512.16287", "authors": ["Yehor Tereshchenko", "Mika H\u00e4m\u00e4l\u00e4inen", "Svitlana Myroniuk"], "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures", "comment": "IWCLUL 2025", "summary": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86OpenAI GPT\u6a21\u578b\u4e2d\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u67b6\u6784\u5728\u82ac\u5170\u8bed\u4e0e\u56db\u79cd\u4f4e\u8d44\u6e90\u4e4c\u62c9\u5c14\u8bed\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u62d2\u7edd\u7ffb\u8bd1\u7387\u4f4e16\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5bf9\u4f4e\u8d44\u6e90\u548c\u6fd2\u5371\u8bed\u8a00\u7684\u6027\u80fd\u4e86\u89e3\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u4e4c\u62c9\u5c14\u8bed\u7cfb\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u80fd\u529b\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u6587\u5b66\u6587\u672c\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u6bd4\u8f83OpenAI GPT\u6a21\u578b\u4e2d\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u67b6\u6784\u5728\u82ac\u5170\u8bed\u4e0e\u56db\u79cd\u4f4e\u8d44\u6e90\u4e4c\u62c9\u5c14\u8bed\uff08\u79d1\u7c73-\u5179\u6881\u8bed\u3001\u83ab\u514b\u6c99\u8bed\u3001\u57c3\u5c14\u9f50\u4e9a\u8bed\u3001\u4e4c\u5fb7\u7a46\u5c14\u7279\u8bed\uff09\u4e4b\u95f4\u7684\u7ffb\u8bd1\u8868\u73b0\uff0c\u901a\u8fc7\u62d2\u7edd\u7387\u5206\u6790\u8bc4\u4f30\u6a21\u578b\u7ffb\u8bd1\u610f\u613f\u3002", "result": "\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u5728\u7ffb\u8bd1\u6027\u80fd\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63a8\u7406\u6a21\u578b\u7684\u62d2\u7edd\u7ffb\u8bd1\u7387\u6bd4\u975e\u63a8\u7406\u6a21\u578b\u4f4e16\u4e2a\u767e\u5206\u70b9\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e4c\u62c9\u5c14\u8bed\u8a00\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u5730\u7406\u89e3\u63a8\u7406\u6a21\u578b\u5728\u6fd2\u5371\u8bed\u8a00\u4fdd\u62a4\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u63a8\u7406\u67b6\u6784\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2512.16446", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16446", "abs": "https://arxiv.org/abs/2512.16446", "authors": ["Enis Yalcin", "Joshua O'Hara", "Maria Stamatopoulou", "Chengxu Zhou", "Dimitrios Kanoulas"], "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion", "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)", "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.", "AI": {"tldr": "E-SDS\u6846\u67b6\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5b9e\u65f6\u5730\u5f62\u611f\u77e5\uff0c\u81ea\u52a8\u751f\u6210\u5956\u52b1\u51fd\u6570\u6765\u8bad\u7ec3\u4eba\u5f62\u673a\u5668\u4eba\u9002\u5e94\u590d\u6742\u5730\u5f62\u7684\u8fd0\u52a8\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u8bbe\u8ba1\u65f6\u95f4\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u5730\u5f62\u5bfc\u822a\u3002\u4f20\u7edf\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u8017\u65f6\u4e14\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u81ea\u52a8\u5316\u4e14\u5177\u5907\u73af\u5883\u611f\u77e5\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faE-SDS\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5b9e\u65f6\u5730\u5f62\u4f20\u611f\u5668\u5206\u6790\uff0c\u57fa\u4e8e\u793a\u4f8b\u89c6\u9891\u81ea\u52a8\u751f\u6210\u5956\u52b1\u51fd\u6570\uff0c\u8bad\u7ec3\u5177\u5907\u73af\u5883\u611f\u77e5\u80fd\u529b\u7684\u8fd0\u52a8\u7b56\u7565\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\u56db\u79cd\u5730\u5f62\uff08\u7b80\u5355\u3001\u95f4\u9699\u3001\u969c\u788d\u3001\u697c\u68af\uff09\uff0cE-SDS\u9996\u6b21\u5b9e\u73b0\u6210\u529f\u4e0b\u697c\u68af\uff0c\u800c\u624b\u52a8\u8bbe\u8ba1\u6216\u975e\u611f\u77e5\u57fa\u7ebf\u65b9\u6cd5\u5747\u5931\u8d25\u3002\u5728\u6240\u6709\u5730\u5f62\u4e2d\uff0c\u901f\u5ea6\u8ddf\u8e2a\u8bef\u5dee\u51cf\u5c1151.9-82.6%\uff0c\u5956\u52b1\u8bbe\u8ba1\u65f6\u95f4\u4ece\u6570\u5929\u7f29\u77ed\u81f32\u5c0f\u65f6\u5185\u3002", "conclusion": "E-SDS\u6846\u67b6\u901a\u8fc7\u73af\u5883\u611f\u77e5\u7684\u81ea\u52a8\u5316\u5956\u52b1\u8bbe\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4ea7\u751f\u4e86\u66f4\u9c81\u68d2\u548c\u80fd\u529b\u5f3a\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\uff0c\u4e3a\u590d\u6742\u5730\u5f62\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16323", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16323", "abs": "https://arxiv.org/abs/2512.16323", "authors": ["Hiroyuki Deguchi", "Katsuki Chousa", "Yusuke Sakai"], "title": "Hacking Neural Evaluation Metrics with Single Hub Text", "comment": null, "summary": "Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u795e\u7ecf\u6587\u672c\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u6f0f\u6d1e\uff0c\u901a\u8fc7\u5bfb\u627e\u5355\u4e00\u5bf9\u6297\u6027\u6587\u672c\uff08hub text\uff09\u53ef\u6b3a\u9a97COMET\u7b49\u6307\u6807\uff0c\u4f7f\u5176\u59cb\u7ec8\u7ed9\u51fa\u9ad8\u8bc4\u5206\uff0c\u5373\u4f7f\u8be5\u6587\u672c\u4e0e\u6e90\u53e5\u5b50\u65e0\u5173\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5d4c\u5165\u7684\u795e\u7ecf\u6587\u672c\u8bc4\u4f30\u6307\u6807\uff08\u5982COMET\uff09\u5728\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u7531\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\"\u9ed1\u76d2\"\u7279\u6027\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u5176\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002\u9700\u8981\u63ed\u793a\u8fd9\u7c7b\u6307\u6807\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5728\u79bb\u6563\u7a7a\u95f4\u4e2d\u5bfb\u627e\u5355\u4e00\u5bf9\u6297\u6027\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u8be5\u6587\u672c\u65e0\u8bba\u9762\u5bf9\u4f55\u79cd\u6d4b\u8bd5\u7528\u4f8b\uff0c\u90fd\u80fd\u88ab\u8bc4\u4f30\u6307\u6807\u4e00\u81f4\u5730\u8bc4\u4e3a\u9ad8\u8d28\u91cf\u6587\u672c\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u8bc6\u522b\u8bc4\u4f30\u6307\u6807\u7684\u8106\u5f31\u6027\u3002", "result": "\u5728WMT'24\u82f1\u65e5\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u627e\u5230\u7684hub text\u83b7\u5f97\u4e8679.1 COMET%\u8bc4\u5206\uff1b\u5728\u82f1\u5fb7\u4efb\u52a1\u4e2d\u83b7\u5f9767.8 COMET%\u8bc4\u5206\uff0c\u5747\u8d85\u8fc7\u4e86\u4f7f\u7528M2M100\u7ffb\u8bd1\u6a21\u578b\u4e3a\u6bcf\u4e2a\u6e90\u53e5\u5b50\u5355\u72ec\u751f\u6210\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u627e\u5230\u7684hub text\u5728\u591a\u4e2a\u8bed\u8a00\u5bf9\uff08\u5982\u65e5\u82f1\u3001\u5fb7\u82f1\uff09\u4e2d\u90fd\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u795e\u7ecf\u6587\u672c\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5355\u4e00\u5bf9\u6297\u6027\u6587\u672c\u5c31\u80fd\u6b3a\u9a97\u8fd9\u4e9b\u6307\u6807\uff0c\u8fd9\u4e3a\u8bc4\u4f30\u6307\u6807\u7684\u5f00\u53d1\u548c\u4f7f\u7528\u6572\u54cd\u4e86\u8b66\u949f\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2512.16449", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16449", "abs": "https://arxiv.org/abs/2512.16449", "authors": ["Abhishek Kashyap", "Yuxuan Yang", "Henrik Andreasson", "Todor Stoyanov"], "title": "Single-View Shape Completion for Robotic Grasping in Clutter", "comment": null, "summary": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.", "AI": {"tldr": "\u5229\u7528\u6269\u6563\u6a21\u578b\u4ece\u5355\u89c6\u89d2\u6df1\u5ea6\u89c2\u6d4b\u8fdb\u884c\u7c7b\u522b\u7ea73D\u5f62\u72b6\u8865\u5168\uff0c\u63d0\u5347\u6742\u4e71\u573a\u666f\u4e2d\u7684\u6293\u53d6\u6210\u529f\u7387", "motivation": "\u5355\u89c6\u89d2\u76f8\u673a\u53ea\u80fd\u6355\u6349\u7269\u4f53\u7684\u4e00\u9762\uff0c\u52a0\u4e0a\u6742\u4e71\u573a\u666f\u7684\u906e\u6321\uff0c\u5bfc\u81f4\u89c2\u6d4b\u51e0\u4f55\u4e0d\u5b8c\u6574\uff0c\u6293\u53d6\u7b97\u6cd5\u8868\u73b0\u4e0d\u4f73", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4ece\u5355\u89c6\u89d2\u90e8\u5206\u6df1\u5ea6\u89c2\u6d4b\u8fdb\u884c\u7c7b\u522b\u7ea73D\u5f62\u72b6\u8865\u5168\uff0c\u91cd\u5efa\u5b8c\u6574\u7269\u4f53\u51e0\u4f55\uff0c\u4e3a\u6293\u53d6\u89c4\u5212\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f", "result": "\u5728\u6742\u4e71\u573a\u666f\u8bc4\u4f30\u4e2d\uff0c\u76f8\u6bd4\u65e0\u5f62\u72b6\u8865\u5168\u7684\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534723%\u6293\u53d6\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u73b0\u6709\u5f62\u72b6\u8865\u5168\u65b9\u6cd5\u63d0\u534719%", "conclusion": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u5f62\u72b6\u8865\u5168\u80fd\u6709\u6548\u63d0\u5347\u6742\u4e71\u573a\u666f\u4e2d\u673a\u5668\u4eba\u6293\u53d6\u7684\u6210\u529f\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.16378", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.16378", "abs": "https://arxiv.org/abs/2512.16378", "authors": ["Sara Papi", "Javier Garcia Gilabert", "Zachary Hopton", "Vil\u00e9m Zouhar", "Carlos Escolano", "Gerard I. G\u00e1llego", "Jorge Iranzo-S\u00e1nchez", "Ahrii Kim", "Dominik Mach\u00e1\u010dek", "Patricia Schmidtova", "Maike Z\u00fcfle"], "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs", "comment": "Project available at https://github.com/sarapapi/hearing2translate", "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.", "AI": {"tldr": "SpeechLLMs vs \u4f20\u7edf\u7ea7\u8054\u7cfb\u7edf\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u5168\u9762\u5bf9\u6bd4\uff1a\u7ea7\u8054\u7cfb\u7edf\u4ecd\u6700\u53ef\u9760\uff0cSpeechLLMs\u4ec5\u5728\u7279\u5b9a\u573a\u666f\u5339\u914d\uff0c\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u8868\u73b0\u6700\u5dee", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u5230\u6587\u672c\u4e4b\u5916\uff0c\u96c6\u6210\u8bed\u97f3\u4f5c\u4e3a\u539f\u751f\u6a21\u6001\u50ac\u751f\u4e86SpeechLLMs\uff0c\u65e8\u5728\u76f4\u63a5\u7ffb\u8bd1\u53e3\u8bed\uff0c\u7ed5\u8fc7\u4f20\u7edf\u7684\u8f6c\u5f55\u7ea7\u8054\u6d41\u7a0b\u3002\u4f46\u95ee\u9898\u662f\uff1a\u8fd9\u79cd\u96c6\u6210\u662f\u5426\u771f\u7684\u6bd4\u6210\u719f\u7684\u7ea7\u8054\u67b6\u6784\u5728\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u8d28\u91cf\u4e0a\u6709\u6240\u63d0\u5347\uff1f", "method": "\u63d0\u51fa\u4e86\"Hearing to Translate\"\u6d4b\u8bd5\u5957\u4ef6\uff0c\u9996\u6b21\u5168\u9762\u8bc4\u4f305\u4e2a\u6700\u5148\u8fdb\u7684SpeechLLMs\u4e0e16\u4e2a\u5f3a\u5927\u7684\u76f4\u63a5\u548c\u7ea7\u8054\u7cfb\u7edf\uff08\u7ed3\u5408\u9886\u5148\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u4e0e\u591a\u8bed\u8a00LLMs\uff09\u3002\u8bc4\u4f30\u8986\u76d616\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u300113\u79cd\u8bed\u8a00\u5bf9\u548c9\u79cd\u6311\u6218\u6027\u6761\u4ef6\uff08\u5305\u62ec\u4e0d\u6d41\u5229\u3001\u5608\u6742\u548c\u957f\u8bed\u97f3\uff09\u3002", "result": "\u5728\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0c\u7ea7\u8054\u7cfb\u7edf\u6574\u4f53\u4e0a\u4ecd\u7136\u6700\u53ef\u9760\uff1b\u5f53\u524d\u7684SpeechLLMs\u4ec5\u5728\u7279\u5b9a\u8bbe\u7f6e\u4e2d\u5339\u914d\u7ea7\u8054\u7cfb\u7edf\uff1b\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u8868\u73b0\u6700\u5dee\uff0c\u843d\u540e\u4e8e\u4e24\u8005\u3002\u8fd9\u8868\u660e\u96c6\u6210LLM\uff08\u65e0\u8bba\u662f\u6a21\u578b\u5185\u90e8\u8fd8\u662f\u7ba1\u9053\u4e2d\uff09\u5bf9\u4e8e\u9ad8\u8d28\u91cf\u8bed\u97f3\u7ffb\u8bd1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u5c3d\u7ba1SpeechLLMs\u4f5c\u4e3a\u65b0\u5174\u65b9\u6cd5\u6709\u6f5c\u529b\uff0c\u4f46\u4f20\u7edf\u7ea7\u8054\u67b6\u6784\u5728\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u4ecd\u7136\u4fdd\u6301\u4f18\u52bf\u3002\u96c6\u6210LLM\uff08\u65e0\u8bba\u662f\u7aef\u5230\u7aef\u8fd8\u662f\u7ea7\u8054\u65b9\u5f0f\uff09\u662f\u83b7\u5f97\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u7684\u5173\u952e\uff0c\u800c\u5f53\u524dSpeechLLMs\u5c1a\u672a\u8d85\u8d8a\u6210\u719f\u7684\u7ea7\u8054\u7cfb\u7edf\u3002"}}
{"id": "2512.16454", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16454", "abs": "https://arxiv.org/abs/2512.16454", "authors": ["Tianhao Shao", "Kaixing Zhao", "Feng Liu", "Lixin Yang", "Bin Guo"], "title": "AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems", "comment": null, "summary": "As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable \"user\". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.", "AI": {"tldr": "MPBS\u662f\u4e00\u4e2a\u9762\u5411\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u8f66\u7684\u53ef\u6269\u5c55\u4efb\u52a1\u62db\u52df\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u611f\u77e5\u5206\u7c7b\u3001\u65f6\u53d8\u9a6c\u5c14\u53ef\u592b\u79fb\u52a8\u6027\u9884\u6d4b\u548c\u52a8\u6001\u4f18\u5148\u7ea7\u8c03\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4efb\u52a1\u5206\u914d\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u8f66\u5728\u57ce\u5e02\u573a\u666f\u611f\u77e5\u548c\u5e94\u6025\u54cd\u5e94\u7b49\u5e94\u7528\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u5982\u4f55\u9ad8\u6548\u62db\u52df\u8fd9\u4e9b\u81ea\u4e3b\u8bbe\u5907\u6267\u884c\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "MPBS\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u884c\u4e3a\u611f\u77e5KNN\u5206\u7c7b\u5668\u3001\u65f6\u53d8\u9a6c\u5c14\u53ef\u592b\u79fb\u52a8\u6027\u9884\u6d4b\u6a21\u578b\u3001\u4ee5\u53ca\u8003\u8651\u4efb\u52a1\u7d27\u6025\u6027\u548c\u57fa\u7ad9\u6027\u80fd\u7684\u52a8\u6001\u4f18\u5148\u7ea7\u8c03\u5ea6\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cGeoLife\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cMPBS\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "conclusion": "MPBS\u4e3a\u65e0\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9884\u6d4b\u6027\u3001\u884c\u4e3a\u611f\u77e5\u7684\u667a\u80fd\u534f\u4f5c\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.16401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16401", "abs": "https://arxiv.org/abs/2512.16401", "authors": ["Darshil Chauhan", "Adityasinh Solanki", "Vansh Patel", "Kanav Kapoor", "Ritvik Jain", "Aditya Bansal", "Dhruv Kumar", "Prateek Narang"], "title": "Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains", "comment": null, "summary": "Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u8bed\u97f3\u8bc6\u522b\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u4e34\u5e8a\u73af\u5883\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u548c\u591a\u9886\u57df\u7ecf\u9a8c\u56de\u653e\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u8bef\u7387\u5e76\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u5728\u4e34\u5e8a\u6587\u6863\u5904\u7406\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u6570\u636e\u9690\u79c1\u9650\u5236\u3001\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u58f0\u5b66\u9886\u57df\u504f\u79fb\u7b49\u6280\u672f\u969c\u788d\uff0c\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u97f3\u9891\u4e0a\u8bcd\u9519\u8bef\u7387\u9ad8\u8fbe40.94%\uff0c\u65e0\u6cd5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u9690\u79c1\u4fdd\u62a4\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\u5b9e\u73b0\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u7ed3\u5408\u591a\u9886\u57df\u7ecf\u9a8c\u56de\u653e\u6280\u672f\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u76ee\u6807\u9886\u57df\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u6539\u558417.1%\uff0c\u4e0e\u6734\u7d20\u81ea\u9002\u5e94\u76f8\u6bd4\uff0c\u707e\u96be\u6027\u9057\u5fd8\u51cf\u5c1147%\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u9ad8\u5f71\u54cd\u529b\u73af\u5883\u4e2d\u6784\u5efa\u53ef\u9760\u3001\u81ea\u6211\u6539\u8fdb\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u53ef\u884c\u9014\u5f84\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u5bf9\u6570\u636e\u9690\u79c1\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u9886\u57df\u504f\u79fb\u7b49\u6311\u6218\u3002"}}
{"id": "2512.16469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16469", "abs": "https://arxiv.org/abs/2512.16469", "authors": ["Jiayu Zhang", "Kaixing Zhao", "Tianhao Shao", "Bin Guo", "Liang He"], "title": "Tri-Select: A Multi-Stage Visual Data Selection Framework for Mobile Visual Crowdsensing", "comment": null, "summary": "Mobile visual crowdsensing enables large-scale, fine-grained environmental monitoring through the collection of images from distributed mobile devices. However, the resulting data is often redundant and heterogeneous due to overlapping acquisition perspectives, varying resolutions, and diverse user behaviors. To address these challenges, this paper proposes Tri-Select, a multi-stage visual data selection framework that efficiently filters redundant and low-quality images. Tri-Select operates in three stages: (1) metadata-based filtering to discard irrelevant samples; (2) spatial similarity-based spectral clustering to organize candidate images; and (3) a visual-feature-guided selection based on maximum independent set search to retain high-quality, representative images. Experiments on real-world and public datasets demonstrate that Tri-Select improves both selection efficiency and dataset quality, making it well-suited for scalable crowdsensing applications.", "AI": {"tldr": "Tri-Select\u662f\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u89c6\u89c9\u4f17\u5305\u7684\u4e09\u9636\u6bb5\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u7a7a\u95f4\u805a\u7c7b\u548c\u89c6\u89c9\u7279\u5f81\u9009\u62e9\u6765\u9ad8\u6548\u7b5b\u9009\u5197\u4f59\u548c\u4f4e\u8d28\u91cf\u56fe\u50cf\u3002", "motivation": "\u79fb\u52a8\u89c6\u89c9\u4f17\u5305\u6536\u96c6\u7684\u56fe\u50cf\u6570\u636e\u5b58\u5728\u5197\u4f59\u548c\u5f02\u8d28\u6027\uff0c\u5305\u62ec\u91c7\u96c6\u89c6\u89d2\u91cd\u53e0\u3001\u5206\u8fa8\u7387\u4e0d\u4e00\u3001\u7528\u6237\u884c\u4e3a\u591a\u6837\u7b49\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u6765\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\u3002", "method": "\u63d0\u51faTri-Select\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u5143\u6570\u636e\u8fc7\u6ee4\u53bb\u9664\u65e0\u5173\u6837\u672c\uff1b2) \u57fa\u4e8e\u7a7a\u95f4\u76f8\u4f3c\u6027\u7684\u8c31\u805a\u7c7b\u7ec4\u7ec7\u5019\u9009\u56fe\u50cf\uff1b3) \u57fa\u4e8e\u6700\u5927\u72ec\u7acb\u96c6\u641c\u7d22\u7684\u89c6\u89c9\u7279\u5f81\u5f15\u5bfc\u9009\u62e9\uff0c\u4fdd\u7559\u9ad8\u8d28\u91cf\u4ee3\u8868\u6027\u56fe\u50cf\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTri-Select\u63d0\u9ad8\u4e86\u9009\u62e9\u6548\u7387\u548c\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u9002\u5408\u53ef\u6269\u5c55\u7684\u4f17\u5305\u5e94\u7528\u3002", "conclusion": "Tri-Select\u80fd\u6709\u6548\u89e3\u51b3\u79fb\u52a8\u89c6\u89c9\u4f17\u5305\u4e2d\u7684\u5197\u4f59\u548c\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2512.16530", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16530", "abs": "https://arxiv.org/abs/2512.16530", "authors": ["Primoz Kocbek", "Leon Kopitar", "Gregor Stiglic"], "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics", "comment": "5 pages, 1 figure", "summary": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7b80\u5316\u751f\u7269\u533b\u5b66\u6587\u672c\u4ee5\u63d0\u9ad8\u5065\u5eb7\u7d20\u517b\uff0c\u6bd4\u8f83\u4e86\u63d0\u793a\u6a21\u677f\u3001\u53ccAI\u4ee3\u7406\u548c\u5fae\u8c03\u4e09\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0gpt-4o-mini\u8868\u73b0\u6700\u4f73\uff0c\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0cG-Eval\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u4e00\u81f4\u3002", "motivation": "\u63d0\u9ad8\u751f\u7269\u533b\u5b66\u6587\u672c\u7684\u53ef\u8bfb\u6027\u548c\u5065\u5eb7\u7d20\u517b\uff0c\u4f7f\u590d\u6742\u7684\u533b\u5b66\u4fe1\u606f\u66f4\u5bb9\u6613\u88ab\u666e\u901a\u516c\u4f17\u7406\u89e3\uff0c\u4ece\u800c\u6539\u5584\u5065\u5eb7\u4fe1\u606f\u83b7\u53d6\u7684\u5e73\u7b49\u6027\u3002", "method": "\u4f7f\u7528\u516c\u5171\u6570\u636e\u96c6\uff08\u5305\u542b\u751f\u7269\u533b\u5b66\u6458\u8981\u7684\u901a\u4fd7\u8bed\u8a00\u6539\u7f16\u7248\uff09\uff0c\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e09\u79cd\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u63d0\u793a\u6a21\u677f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1b2\uff09\u53ccAI\u4ee3\u7406\u65b9\u6cd5\uff1b3\uff09\u5fae\u8c03\u65b9\u6cd5\u3002\u4f7f\u7528OpenAI\u7684gpt-4o\u548cgpt-4o-mini\u6a21\u578b\u4f5c\u4e3a\u57fa\u7ebf\u3002", "result": "gpt-4o-mini\u8868\u73b0\u6700\u4f73\uff0c\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u3002G-Eval\uff08\u57fa\u4e8eLLM\u7684\u5b9a\u91cf\u6307\u6807\uff09\u4e0e\u5b9a\u6027\u6307\u6807\uff085\u70b9\u674e\u514b\u7279\u91cf\u8868\uff09\u7684\u8bc4\u4f30\u7ed3\u679c\u4e00\u81f4\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u6587\u672c\u7b80\u5316\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0cgpt-4o-mini\u8868\u73b0\u4f18\u5f02\uff0cG-Eval\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2512.16555", "categories": ["cs.RO", "cs.FL", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16555", "abs": "https://arxiv.org/abs/2512.16555", "authors": ["Marcelo Rosa", "Jos\u00e9 E. R. Cury", "Fabio L. Baldissera"], "title": "A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots", "comment": null, "summary": "In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots", "AI": {"tldr": "\u57fa\u4e8e\u76d1\u63a7\u63a7\u5236\u7406\u8bba\u7684\u591a\u673a\u5668\u4eba\u534f\u540c\u6784\u5efa3D\u7ed3\u6784\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u591a\u4e2a\u79fb\u52a8\u673a\u5668\u4eba\u534f\u540c\u81ea\u4e3b\u6784\u5efa\u9884\u5b9a3D\u7ed3\u6784\u7684\u534f\u8c03\u95ee\u9898\uff0c\u786e\u4fdd\u7cfb\u7edf\u6b63\u786e\u6027\u548c\u53ef\u9760\u6027", "method": "\u91c7\u7528\u76d1\u63a7\u63a7\u5236\u7406\u8bba\uff0c\u4ece\u5355\u4e2a\u673a\u5668\u4eba\u548c\u76ee\u6807\u7ed3\u6784\u7684\u6a21\u578b\u5408\u6210\u6b63\u786e\u6784\u9020\u7684\u53cd\u5e94\u5f0f\u63a7\u5236\u5668\uff08\u76d1\u7763\u5668\uff09\uff0c\u7136\u540e\u5c06\u8be5\u76d1\u7763\u5668\u590d\u5236\u5230\u5176\u4ed6\u673a\u5668\u4eba", "result": "\u901a\u8fc7\u8be5\u65b9\u6cd5\u80fd\u591f\u786e\u4fdd\u6240\u6709\u673a\u5668\u4eba\u534f\u540c\u5b8c\u6210\u76ee\u6807\u7ed3\u6784\u7684\u6784\u5efa", "conclusion": "\u57fa\u4e8e\u76d1\u63a7\u63a7\u5236\u7406\u8bba\u7684\u76d1\u7763\u5668\u590d\u5236\u65b9\u6cd5\u4e3a\u591a\u673a\u5668\u4eba\u534f\u540c\u6784\u5efa3D\u7ed3\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u6b63\u786e\u6784\u9020\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.16541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16541", "abs": "https://arxiv.org/abs/2512.16541", "authors": ["Primoz Kocbek", "Gregor Stiglic"], "title": "UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification", "comment": "10 pages, 3 tables. CLEF 2025 Working Notes, 9 to 12 September 2025, Madrid, Spain", "summary": "This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.", "AI": {"tldr": "\u4f7f\u7528GPT-4.1\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u79d1\u5b66\u6587\u672c\u7b80\u5316\uff0c\u6bd4\u8f83\u65e0\u4e0a\u4e0b\u6587\u63d0\u793a\u5de5\u7a0b\u4e0e\u5fae\u8c03\u65b9\u6cd5\uff0c\u53d1\u73b0gpt-4.1-mini\u5728\u65e0\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73", "motivation": "\u89e3\u51b3CLEF 2025 SimpleText Track Task 1\u4e2d\u7684\u79d1\u5b66\u6587\u672c\u7b80\u5316\u95ee\u9898\uff0c\u5305\u62ec\u53e5\u5b50\u7ea7\u548c\u6587\u6863\u7ea7\u7b80\u5316\uff0c\u63a2\u7d22\u4e0d\u540c\u65b9\u6cd5\u5728\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u7684\u6548\u679c", "method": "\u4f7f\u7528OpenAI\u7684gpt-4.1\u3001gpt-4.1-mini\u548cgpt-4.1-nano\u6a21\u578b\uff0c\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u65e0\u4e0a\u4e0b\u6587\u65b9\u6cd5\uff1b2\uff09\u5fae\u8c03\u65b9\u6cd5", "result": "gpt-4.1-mini\u6a21\u578b\u5728\u65e0\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u5728\u53e5\u5b50\u7ea7\u548c\u6587\u6863\u7ea7\u7b80\u5316\u4e2d\u90fd\u8868\u73b0\u7a33\u5065\uff1b\u5fae\u8c03\u6a21\u578b\u7ed3\u679c\u53c2\u5dee\u4e0d\u9f50\uff1bgpt-4.1-nano-ft\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u5728\u6587\u6863\u7ea7\u7b80\u5316\u4e2d\u8868\u73b0\u7a81\u51fa", "conclusion": "\u4e0d\u540c\u7c92\u5ea6\u7684\u6587\u672c\u7b80\u5316\u5177\u6709\u590d\u6742\u6027\uff0cgpt-4.1-mini\u7684\u65e0\u4e0a\u4e0b\u6587\u65b9\u6cd5\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5fae\u8c03\u65b9\u6cd5\u7684\u6548\u679c\u56e0\u6a21\u578b\u548c\u4efb\u52a1\u7c92\u5ea6\u800c\u5f02"}}
{"id": "2512.16705", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16705", "abs": "https://arxiv.org/abs/2512.16705", "authors": ["David M\u00fcller", "Espen Knoop", "Dario Mylonopoulos", "Agon Serifi", "Michael A. Hopkins", "Ruben Grandia", "Moritz B\u00e4cher"], "title": "Olaf: Bringing an Animated Character to Life in the Physical World", "comment": null, "summary": "Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u753b\u53c2\u8003\u7684\u7269\u7406\u89d2\u8272\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u68b0\u8bbe\u8ba1\u521b\u65b0\uff08\u9690\u85cf\u5f0f\u4e0d\u5bf9\u79f0\u817f\u90e8\u3001\u7403\u5f62/\u5e73\u9762\u8fde\u6746\u673a\u6784\uff09\u548c\u6e29\u5ea6\u611f\u77e5\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u52a8\u753b\u89d2\u8272Olaf\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9ad8\u53ef\u4fe1\u5ea6\u7269\u7406\u5b9e\u73b0\u3002", "motivation": "\u52a8\u753b\u89d2\u8272\u901a\u5e38\u4ee5\u975e\u7269\u7406\u65b9\u5f0f\u8fd0\u52a8\u4e14\u6bd4\u4f8b\u4e0e\u5178\u578b\u884c\u8d70\u673a\u5668\u4eba\u5dee\u5f02\u5de8\u5927\uff0c\u8fd9\u4e3a\u673a\u68b0\u8bbe\u8ba1\u548c\u98ce\u683c\u5316\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u521b\u65b0\u5e73\u53f0\u3002\u7814\u7a76\u65e8\u5728\u5c06\u52a8\u753b\u89d2\u8272Olaf\u5e26\u5165\u7269\u7406\u4e16\u754c\uff0c\u5b9e\u73b0\u9ad8\u53ef\u4fe1\u5ea6\u7684\u7269\u7406\u8868\u73b0\u3002", "method": "1) \u673a\u68b0\u8bbe\u8ba1\uff1a\u9690\u85cf\u4e24\u4e2a\u4e0d\u5bf9\u79f0\u817f\u90e8\u4e8e\u8f6f\u6ce1\u6cab\u88d9\u4e0b\uff0c\u4f7f\u7528\u7403\u5f62\u548c\u5e73\u9762\u8fde\u6746\u673a\u6784\u5728\u624b\u81c2\u3001\u5634\u5df4\u548c\u773c\u775b\u4e2d\u5b89\u88c5\u6267\u884c\u5668\uff1b2) \u63a7\u5236\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u52a8\u753b\u53c2\u8003\u4e3a\u6307\u5bfc\uff0c\u5f15\u5165\u51cf\u5c11\u51b2\u51fb\u566a\u58f0\u7684\u5956\u52b1\u51fd\u6570\uff1b3) \u70ed\u7ba1\u7406\uff1a\u5c06\u6e29\u5ea6\u503c\u4f5c\u4e3a\u7b56\u7565\u8f93\u5165\uff0c\u5f15\u5165\u65b0\u5956\u52b1\u51fd\u6570\u9632\u6b62\u6267\u884c\u5668\u8fc7\u70ed\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u670d\u88c5\u673a\u5668\u4eba\u89d2\u8272\u524d\u6240\u672a\u6709\u7684\u53ef\u4fe1\u5ea6\u6c34\u5e73\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u52a8\u753b\u89d2\u8272Olaf\u7684\u7269\u7406\u5b9e\u73b0\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u673a\u68b0\u8bbe\u8ba1\u548c\u6e29\u5ea6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\uff0c\u6210\u529f\u5c06\u52a8\u753b\u89d2\u8272Olaf\u5e26\u5165\u7269\u7406\u4e16\u754c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u53ef\u4fe1\u5ea6\u7684\u7269\u7406\u8868\u73b0\uff0c\u4e3a\u98ce\u683c\u5316\u673a\u5668\u4eba\u89d2\u8272\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2512.16602", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16602", "abs": "https://arxiv.org/abs/2512.16602", "authors": ["Iker Garc\u00eda-Ferrero", "David Montero", "Roman Orus"], "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics", "comment": null, "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.", "AI": {"tldr": "Refusal Steering\u662f\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5411\u91cf\u63a7\u5236LLM\u5728\u653f\u6cbb\u654f\u611f\u8bdd\u9898\u4e0a\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u5f0f\u7684\u62d2\u7edd\u68c0\u6d4b\u65b9\u6cd5\u8106\u5f31\u4e14\u4e0d\u7cbe\u786e\uff0c\u9700\u8981\u4e00\u79cd\u7ec6\u7c92\u5ea6\u63a7\u5236LLM\u5728\u653f\u6cbb\u654f\u611f\u8bdd\u9898\u4e0a\u62d2\u7edd\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6709\u5bb3\u5185\u5bb9\u7684\u5b89\u5168\u6027", "method": "\u4f7f\u7528LLM-as-a-judge\u5206\u914d\u62d2\u7edd\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u63d0\u51fa\u5cad\u6b63\u5219\u5316\u53d8\u4f53\u8ba1\u7b97\u66f4\u597d\u7684\u62d2\u7edd-\u987a\u4ece\u65b9\u5411\u9694\u79bb\u7684\u5f15\u5bfc\u5411\u91cf\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u6fc0\u6d3b\u5411\u91cf\u63a7\u5236\u6a21\u578b\u884c\u4e3a", "result": "\u5728Qwen3-Next-80B-A3B-Thinking\u4e0a\u6210\u529f\u79fb\u9664\u4e86\u653f\u6cbb\u654f\u611f\u8bdd\u9898\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u540c\u65f6\u5728JailbreakBench\u4e0a\u4fdd\u6301\u5b89\u5168\u6027\uff0c\u5728\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u63a5\u8fd1\u57fa\u7ebf\u6027\u80fd\uff0c\u65b9\u6cd5\u53ef\u6cdb\u5316\u52304B\u548c80B\u6a21\u578b", "conclusion": "\u6fc0\u6d3b\u5411\u91cf\u5f15\u5bfc\u53ef\u4ee5\u79fb\u9664\u653f\u6cbb\u62d2\u7edd\u884c\u4e3a\u540c\u65f6\u4fdd\u7559\u6709\u5bb3\u5185\u5bb9\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u4e3a\u63a8\u7406\u65f6\u53ef\u63a7\u3001\u900f\u660e\u7684\u5ba1\u6838\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84"}}
{"id": "2512.16724", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16724", "abs": "https://arxiv.org/abs/2512.16724", "authors": ["Yixiang Chen", "Yan Huang", "Keji He", "Peiyan Li", "Liang Wang"], "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation", "comment": "Accepted at RA-L 2025", "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .", "AI": {"tldr": "VERM\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u4ece3D\u70b9\u4e91\u60f3\u8c61\u865a\u62df\u4efb\u52a1\u81ea\u9002\u5e94\u89c6\u89d2\uff0c\u8fc7\u6ee4\u591a\u6444\u50cf\u5934\u5197\u4f59\u4fe1\u606f\uff0c\u63d0\u5347\u673a\u5668\u4eba3D\u64cd\u4f5c\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u591a\u6444\u50cf\u5934\u8bbe\u7f6e\u5f15\u5165\u5927\u91cf\u5197\u4f59\u548c\u65e0\u5173\u4fe1\u606f\uff0c\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u8feb\u4f7f\u6a21\u578b\u82b1\u8d39\u989d\u5916\u8bad\u7ec3\u65f6\u95f4\u63d0\u53d6\u5173\u952e\u4efb\u52a1\u76f8\u5173\u7ec6\u8282", "method": "\u63d0\u51faVERM\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u4ece\u6784\u5efa\u76843D\u70b9\u4e91\u60f3\u8c61\u865a\u62df\u4efb\u52a1\u81ea\u9002\u5e94\u89c6\u89d2\uff1b\u8bbe\u8ba1\u6df1\u5ea6\u611f\u77e5\u6a21\u5757\u548c\u52a8\u6001\u7c97\u5230\u7ec6\u5904\u7406\u6d41\u7a0b", "result": "\u5728RLBench\u4eff\u771f\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b01.89\u500d\u8bad\u7ec3\u52a0\u901f\u548c1.54\u500d\u63a8\u7406\u52a0\u901f", "conclusion": "VERM\u65b9\u6cd5\u80fd\u6709\u6548\u8fc7\u6ee4\u5197\u4f59\u4fe1\u606f\uff0c\u51c6\u786e\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u4fc3\u8fdb3D\u52a8\u4f5c\u89c4\u5212\u548c\u7cbe\u7ec6\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd"}}
{"id": "2512.16649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16649", "abs": "https://arxiv.org/abs/2512.16649", "authors": ["Bingxiang He", "Zekai Qu", "Zeyuan Liu", "Yinghao Chen", "Yuxin Zuo", "Cheng Qian", "Kaiyan Zhang", "Weize Chen", "Chaojun Xiao", "Ganqu Cui", "Ning Ding", "Zhiyuan Liu"], "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe", "comment": "12 pages, 3 figures", "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.", "AI": {"tldr": "\u63d0\u51faJustRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u8bad\u7ec3\u548c\u56fa\u5b9a\u8d85\u53c2\u6570\uff0c\u57281.5B\u63a8\u7406\u6a21\u578b\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u8ba1\u7b97\u91cf\u51cf\u534a\uff0c\u6311\u6218\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u590d\u6742\u5316\u7684\u8d8b\u52bf\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u8d8b\u5411\u590d\u6742\u5316\uff08\u591a\u9636\u6bb5\u8bad\u7ec3\u3001\u52a8\u6001\u8d85\u53c2\u6570\u3001\u8bfe\u7a0b\u5b66\u4e60\u7b49\uff09\uff0c\u4f5c\u8005\u8d28\u7591\u8fd9\u79cd\u590d\u6742\u6027\u662f\u5426\u5fc5\u8981\uff0c\u5e0c\u671b\u63a2\u7d22\u66f4\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faJustRL\u65b9\u6cd5\uff1a\u5355\u9636\u6bb5\u8bad\u7ec3\u3001\u56fa\u5b9a\u8d85\u53c2\u6570\u3001\u65e0\u590d\u6742\u6280\u5de7\u3002\u5728\u4e24\u4e2a1.5B\u63a8\u7406\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u8d85\u53c2\u6570\u53ef\u8de8\u6a21\u578b\u8fc1\u79fb\u65e0\u9700\u8c03\u4f18\u3002", "result": "\u5728\u4e5d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523054.9%\u548c64.3%\uff0c\u8ba1\u7b97\u91cf\u6bd4\u590d\u6742\u65b9\u6cd5\u51cf\u5c112\u500d\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5e73\u6ed1\u5355\u8c03\uff0c\u65e0\u5d29\u6e83\u6216\u5e73\u53f0\u671f\u3002\u53d1\u73b0\u4f20\u7edf\u6280\u5de7\uff08\u5982\u957f\u5ea6\u60e9\u7f5a\u3001\u9c81\u68d2\u9a8c\u8bc1\u5668\uff09\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\u3002", "conclusion": "\u5f53\u524d\u9886\u57df\u53ef\u80fd\u4e3a\u89e3\u51b3\u672c\u4e0d\u5b58\u5728\u7684\u95ee\u9898\u800c\u589e\u52a0\u590d\u6742\u6027\u3002JustRL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u57fa\u7ebf\uff0c\u6311\u6218\u4e86\u5f3a\u5316\u5b66\u4e60\u590d\u6742\u5316\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2512.16760", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16760", "abs": "https://arxiv.org/abs/2512.16760", "authors": ["Tianshuai Hu", "Xiaolu Liu", "Song Wang", "Yiyao Zhu", "Ao Liang", "Lingdong Kong", "Guoyang Zhao", "Zeying Gong", "Jun Cen", "Zhiyu Huang", "Xiaoshuai Hao", "Linfeng Li", "Hang Song", "Xiangtai Li", "Jun Ma", "Shaojie Shen", "Jianke Zhu", "Dacheng Tao", "Ziwei Liu", "Junwei Liang"], "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future", "comment": "Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad", "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4ece\u4f20\u7edf\u6a21\u5757\u5316\u65b9\u6cd5\u5230Vision-Language-Action (VLA)\u6846\u67b6\u7684\u6f14\u8fdb\uff0c\u5c06\u73b0\u6709VLA\u65b9\u6cd5\u5206\u4e3a\u7aef\u5230\u7aef\u548c\u53cc\u7cfb\u7edf\u4e24\u5927\u8303\u5f0f\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u672a\u6765\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u7684\u6a21\u5757\u5316\"\u611f\u77e5-\u51b3\u7b56-\u52a8\u4f5c\"\u6d41\u6c34\u7ebf\u5b58\u5728\u624b\u5de5\u63a5\u53e3\u3001\u89c4\u5219\u7ec4\u4ef6\u5728\u590d\u6742\u573a\u666f\u4e2d\u5931\u6548\u3001\u611f\u77e5\u8bef\u5dee\u7ea7\u8054\u4f20\u64ad\u7b49\u95ee\u9898\u3002Vision-Action\u6a21\u578b\u867d\u7136\u76f4\u63a5\u5b66\u4e60\u89c6\u89c9\u5230\u52a8\u4f5c\u7684\u6620\u5c04\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3001\u5bf9\u5206\u5e03\u504f\u79fb\u654f\u611f\u3001\u7f3a\u5c11\u7ed3\u6784\u5316\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u8fdb\u5c55\u63a8\u52a8\u4e86Vision-Language-Action\u6846\u67b6\u7684\u53d1\u5c55\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u9a7e\u9a76\u7b56\u7565\u3002", "method": "\u672c\u6587\u91c7\u7528\u7ed3\u6784\u5316\u5206\u6790\u65b9\u6cd5\uff1a1) \u8ffd\u6eaf\u4ece\u65e9\u671fVA\u65b9\u6cd5\u5230\u73b0\u4ee3VLA\u6846\u67b6\u7684\u6f14\u8fdb\u5386\u7a0b\uff1b2) \u5c06\u73b0\u6709VLA\u65b9\u6cd5\u7ec4\u7ec7\u4e3a\u4e24\u5927\u4e3b\u8981\u8303\u5f0f\uff1a\u7aef\u5230\u7aefVLA\uff08\u5355\u4e00\u6a21\u578b\u96c6\u6210\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\uff09\u548c\u53cc\u7cfb\u7edfVLA\uff08\u6162\u901f\u5ba1\u8bae\u901a\u8fc7VLM\u4e0e\u5feb\u901f\u5b89\u5168\u5173\u952e\u6267\u884c\u901a\u8fc7\u89c4\u5212\u5668\u5206\u79bb\uff09\uff1b3) \u5728\u8fd9\u4e9b\u8303\u5f0f\u4e2d\u8fdb\u4e00\u6b65\u533a\u5206\u6587\u672cvs.\u6570\u503c\u52a8\u4f5c\u751f\u6210\u5668\u3001\u663e\u5f0fvs.\u9690\u5f0f\u6307\u5bfc\u673a\u5236\u7b49\u5b50\u7c7b\uff1b4) \u603b\u7ed3\u8bc4\u4f30VLA\u9a7e\u9a76\u7cfb\u7edf\u7684\u4ee3\u8868\u6027\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002", "result": "\u5efa\u7acb\u4e86\u81ea\u52a8\u9a7e\u9a76VLA\u9886\u57df\u7684\u7cfb\u7edf\u5316\u5206\u7c7b\u6846\u67b6\uff0c\u660e\u786e\u4e86\u4e24\u79cd\u4e3b\u8981\u8303\u5f0f\u53ca\u5176\u5b50\u7c7b\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u9886\u57df\u5730\u56fe\u3002\u603b\u7ed3\u4e86\u5f53\u524d\u53ef\u7528\u7684\u8bc4\u4f30\u8d44\u6e90\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "VLA\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7b56\u7565\u7684\u9014\u5f84\u3002\u672c\u6587\u5efa\u7acb\u4e86\u63a8\u8fdb\u4eba\u7c7b\u517c\u5bb9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8fde\u8d2f\u57fa\u7840\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6307\u4ee4\u4fdd\u771f\u5ea6\u7b49\u5173\u952e\u6311\u6218\u548c\u5f00\u653e\u65b9\u5411\u3002"}}
{"id": "2512.16770", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16770", "abs": "https://arxiv.org/abs/2512.16770", "authors": ["William English", "Chase Walker", "Dominic Simon", "Rickard Ewetz"], "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation", "comment": null, "summary": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.", "AI": {"tldr": "\u63d0\u51faGinSign\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u7c7b\u65b9\u6cd5\u5c06\u81ea\u7136\u8bed\u8a00\u6620\u5c04\u5230\u7cfb\u7edf\u7b7e\u540d\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5230\u65f6\u5e8f\u903b\u8f91\u7684\u7ffb\u8bd1\uff0c\u63d0\u5347\u63a5\u5730\u7ffb\u8bd1\u51c6\u786e\u7387\u81f395.5%", "motivation": "\u73b0\u6709NL-to-TL\u7ffb\u8bd1\u6846\u67b6\u8981\u4e48\u5047\u8bbe\u80fd\u51c6\u786e\u83b7\u53d6\u539f\u5b50\u63a5\u5730\u4fe1\u606f\uff0c\u8981\u4e48\u63a5\u5730\u7ffb\u8bd1\u51c6\u786e\u7387\u4f4e\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u6784\u5efa\u53ef\u4fe1\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528", "method": "\u63d0\u51faGinSign\u6846\u67b6\uff0c\u5f15\u5165\u63a5\u5730\u6a21\u578b\u5b66\u4e60\u5c06\u81ea\u7136\u8bed\u8a00\u7247\u6bb5\u6620\u5c04\u5230\u7ed9\u5b9a\u7cfb\u7edf\u7b7e\u540d\u7684\u62bd\u8c61\u4efb\u52a1\u3002\u91c7\u7528\u5206\u5c42\u5206\u89e3\u65b9\u6cd5\uff1a\u5148\u9884\u6d4b\u8c13\u8bcd\u6807\u7b7e\uff0c\u518d\u9009\u62e9\u9002\u5f53\u7c7b\u578b\u7684\u5e38\u91cf\u53c2\u6570\u3002\u5c06\u4efb\u52a1\u4ece\u81ea\u7531\u751f\u6210\u95ee\u9898\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u5206\u7c7b\u95ee\u9898\uff0c\u4f7f\u7528\u8f83\u5c0f\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\uff0c\u907f\u514d\u4f9d\u8d56\u6602\u8d35\u7684LLM", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7701\u7565\u63a5\u5730\u7684\u6846\u67b6\u4f1a\u4ea7\u751f\u8bed\u6cd5\u6b63\u786e\u4f46\u8bed\u4e49\u4e0d\u7b49\u4ef7\u7684LTL\u8868\u8fbe\u5f0f\uff0c\u800cGinSign\u652f\u6301\u4e0b\u6e38\u6a21\u578b\u68c0\u67e5\uff0c\u63a5\u5730\u903b\u8f91\u7b49\u4ef7\u5206\u6570\u8fbe\u523095.5%\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u53471.4\u500d", "conclusion": "GinSign\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u7c7b\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u5230\u65f6\u5e8f\u903b\u8f91\u7ffb\u8bd1\u4e2d\u7684\u63a5\u5730\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u51c6\u786e\u7387\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89c4\u8303\u9a8c\u8bc1\u80fd\u529b"}}
{"id": "2512.16793", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16793", "abs": "https://arxiv.org/abs/2512.16793", "authors": ["Xiaopeng Lin", "Shijie Lian", "Bin Yu", "Ruoqi Yang", "Changti Wu", "Yuzhuo Miao", "Yurun Jin", "Yukun Shi", "Cong Huang", "Bojun Cheng", "Kai Chen"], "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "comment": "17 pages, 4 figures", "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "AI": {"tldr": "\u63d0\u51faE2E-3M\u6570\u636e\u96c6\u548cPhysBrain\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u76d1\u7763\u6570\u636e\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u6cdb\u5316\u95ee\u9898", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u7b2c\u4e09\u4eba\u79f0\u6570\u636e\u8bad\u7ec3\uff0c\u4e0e\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\u3002\u673a\u5668\u4eba\u81ea\u6211\u4e2d\u5fc3\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u591a\u6837\u6027\u6709\u9650\uff0c\u800c\u5927\u89c4\u6a21\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u63d0\u51faEgocentric2Embodiment\u8f6c\u6362\u6d41\u7a0b\uff0c\u5c06\u539f\u59cb\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8f6c\u5316\u4e3a\u591a\u5c42\u6b21\u3001\u6a21\u5f0f\u9a71\u52a8\u7684\u89c6\u89c9\u95ee\u7b54\u76d1\u7763\u6570\u636e\uff0c\u5f3a\u8c03\u8bc1\u636e\u57fa\u7840\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u6784\u5efaE2E-3M\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u51fa\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u7684PhysBrain\u6a21\u578b", "result": "PhysBrain\u5728EgoThink\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u7684\u81ea\u6211\u4e2d\u5fc3\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u89c4\u5212\u65b9\u9762\u3002\u4f5c\u4e3a\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u7684\u521d\u59cb\u5316\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u548c\u66f4\u9ad8\u7684SimplerEnv\u6210\u529f\u7387\uff0853.9%\uff09", "conclusion": "\u901a\u8fc7\u5c06\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u76d1\u7763\u6570\u636e\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u8bad\u7ec3\uff0c\u8bc1\u660e\u4e86\u4ece\u4eba\u7c7b\u81ea\u6211\u4e2d\u5fc3\u76d1\u7763\u5230\u4e0b\u6e38\u673a\u5668\u4eba\u63a7\u5236\u7684\u6709\u6548\u8fc1\u79fb\uff0c\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u7269\u7406\u667a\u80fd\u7684\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2512.16795", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16795", "abs": "https://arxiv.org/abs/2512.16795", "authors": ["Shubham Mishra", "Samyek Jain", "Gorang Mehrishi", "Shiv Tiwari", "Harsh Sharma", "Pratik Narang", "Dhruv Kumar"], "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs", "comment": "Under Review", "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.", "AI": {"tldr": "\u63d0\u51fa\u63a8\u7406\u8f68\u8ff9\u589e\u5f3a\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u7ed3\u6784\u5316\u63a8\u7406\u89e3\u51b3\u68c0\u7d22\u6587\u6863\u51b2\u7a81\u3001\u8fc7\u65f6\u548c\u4e3b\u89c2\u4fe1\u606f\u95ee\u9898\uff0c\u5f15\u5165CATS\u8bc4\u4f30\u7ba1\u9053\uff0c\u5b9e\u9a8c\u663e\u793a\u5728Qwen\u6a21\u578b\u4e0a\u7aef\u5230\u7aef\u7b54\u6848\u6b63\u786e\u7387\u4ece0.069\u63d0\u5347\u52300.883", "motivation": "\u4f20\u7edfRAG\u5728\u68c0\u7d22\u5230\u51b2\u7a81\u3001\u8fc7\u65f6\u6216\u4e3b\u89c2\u4fe1\u606f\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u7edf\u4e00\u7684\u63a8\u7406\u76d1\u7763\uff0c\u9700\u8981\u89e3\u51b3\u591a\u6e90\u4fe1\u606f\u51b2\u7a81\u548c\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u95ee\u9898", "method": "\u63d0\u51fa\u63a8\u7406\u8f68\u8ff9\u589e\u5f3aRAG\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9636\u6bb5\u7ed3\u6784\u5316\u63a8\u7406\uff1a\u6587\u6863\u7ea7\u88c1\u51b3\u3001\u51b2\u7a81\u5206\u6790\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u5408\u6210\uff1b\u5f15\u5165Conflict-Aware Trust-Score (CATS)\u8bc4\u4f30\u7ba1\u9053\uff0c\u4f7f\u7528LLM-as-a-Judge\u8bc4\u4f30\u57fa\u7840\u6027\u3001\u4e8b\u5b9e\u6b63\u786e\u6027\u3001\u62d2\u7edd\u51c6\u786e\u6027\u548c\u51b2\u7a81\u884c\u4e3a\u5bf9\u9f50", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728Qwen\u6a21\u578b\u4e0a\uff0c\u76d1\u7763\u5fae\u8c03\u540e\u7aef\u5230\u7aef\u7b54\u6848\u6b63\u786e\u7387\u4ece0.069\u63d0\u5347\u52300.883\uff0c\u884c\u4e3a\u4f9d\u4ece\u6027\u4ece0.074\u63d0\u5347\u52300.722\uff1b\u6784\u5efa\u4e86539\u4e2a\u67e5\u8be2\u7684\u63a8\u7406\u6570\u636e\u96c6", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u51b2\u7a81\u611f\u77e5\u3001\u53ef\u89e3\u91ca\u7684RAG\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u51b2\u7a81\u4fe1\u606f\u7684\u80fd\u529b\u548c\u7cfb\u7edf\u53ef\u4fe1\u5ea6"}}
{"id": "2512.16861", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16861", "abs": "https://arxiv.org/abs/2512.16861", "authors": ["Zihan Zhou", "Animesh Garg", "Ajay Mandlekar", "Caelan Garrett"], "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning", "comment": null, "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/", "AI": {"tldr": "ReinforceGen\u662f\u4e00\u4e2a\u7ed3\u5408\u4efb\u52a1\u5206\u89e3\u3001\u6570\u636e\u751f\u6210\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u8fd0\u52a8\u89c4\u5212\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6765\u63d0\u5347\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd", "motivation": "\u957f\u65f6\u7a0b\u64cd\u4f5c\u4e00\u76f4\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u957f\u671f\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u5206\u89e3\u548c\u6267\u884c\u95ee\u9898", "method": "1. \u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5c40\u90e8\u6280\u80fd\uff1b2. \u901a\u8fc7\u8fd0\u52a8\u89c4\u5212\u8fde\u63a5\u6280\u80fd\uff1b3. \u57fa\u4e8e10\u4e2a\u4eba\u7c7b\u6f14\u793a\u751f\u6210\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\uff1b4. \u901a\u8fc7\u5728\u7ebf\u9002\u5e94\u548c\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03", "result": "\u5728Robosuite\u6570\u636e\u96c6\u4e0a\uff0cReinforceGen\u5728\u6700\u9ad8\u91cd\u7f6e\u8303\u56f4\u8bbe\u7f6e\u4e0b\u8fbe\u523080%\u7684\u6210\u529f\u7387\uff1b\u6d88\u878d\u7814\u7a76\u8868\u660e\u5fae\u8c03\u65b9\u6cd5\u8d21\u732e\u4e8689%\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347", "conclusion": "ReinforceGen\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u5206\u89e3\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u64cd\u4f5c\u95ee\u9898\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2512.16802", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16802", "abs": "https://arxiv.org/abs/2512.16802", "authors": ["Primo\u017e Kocbek", "Azra Frkatovi\u0107-Hod\u017ei\u0107", "Dora Lali\u0107", "Vivian Hui", "Gordan Lauc", "Gregor \u0160tiglic"], "title": "Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology", "comment": "Will be published in IEEE BigData 2025 proceedings. Contains 10 pages, 1 figure, 5 tables", "summary": "Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08MM-RAG\uff09\u4e2d\u4e24\u79cd\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u7b56\u7565\uff1a\u5c06\u56fe\u8868\u8f6c\u6362\u4e3a\u6587\u672c vs OCR-free\u89c6\u89c9\u68c0\u7d22\u3002\u5728\u7cd6\u751f\u7269\u5b66\u9886\u57df\u6d4b\u8bd5\u53d1\u73b0\uff0c\u7b56\u7565\u9009\u62e9\u53d6\u51b3\u4e8e\u6a21\u578b\u80fd\u529b\uff1a\u4e2d\u7b49\u6a21\u578b\u66f4\u9002\u5408\u56fe\u8868\u8f6c\u6587\u672c\uff0c\u524d\u6cbf\u6a21\u578b\u4e0bOCR-free\u68c0\u7d22\u53d8\u5f97\u6709\u7ade\u4e89\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u751f\u7269\u533b\u5b66QA\u4e2d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u7b56\u7565\u7684\u7cfb\u7edf\u6bd4\u8f83\uff1a\u4f55\u65f6\u5c06\u56fe\u8868\u8f6c\u6362\u4e3a\u6587\u672c\uff0c\u4f55\u65f6\u4f7f\u7528OCR-free\u89c6\u89c9\u68c0\u7d22\uff08\u8fd4\u56de\u9875\u9762\u56fe\u50cf\uff0c\u8ba9\u751f\u6210\u5668\u89e3\u91ca\uff09\u3002\u7279\u522b\u662f\u5728\u7cd6\u751f\u7269\u5b66\u8fd9\u79cd\u89c6\u89c9\u5bc6\u96c6\u9886\u57df\uff0c\u8fd9\u4e00\u6743\u8861\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b120\u9053\u591a\u9009\u9898\u7684\u57fa\u51c6\uff0c\u6765\u81ea25\u7bc7\u8bba\u6587\uff0c\u6309\u68c0\u7d22\u96be\u5ea6\u5206\u5c42\u3002\u5b9e\u73b0\u4e86\u56db\u79cd\u589e\u5f3a\u7b56\u7565\uff1a\u65e0\u589e\u5f3a\u3001\u6587\u672cRAG\u3001\u591a\u6a21\u6001\u8f6c\u6362\u3001OCR-free\u89c6\u89c9\u68c0\u7d22\uff08ColPali\uff09\u3002\u4f7f\u7528Docling\u89e3\u6790\u548cQdrant\u7d22\u5f15\uff0c\u8bc4\u4f30\u4e86\u4e2d\u7b49\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u548c\u524d\u6cbf\u4e13\u6709\u6a21\u578b\u3002\u8fdb\u884c\u4e865\u6b21\u91cd\u590d\u5b9e\u9a8c\uff0c\u8ba1\u7b97\u51c6\u786e\u7387\u548c95%\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u4e2d\u7b49\u6a21\u578b\uff08Gemma-3-27B-IT\uff09\u4e0b\uff0c\u6587\u672c\u548c\u591a\u6a21\u6001\u8f6c\u6362\u4f18\u4e8eOCR-free\u68c0\u7d22\uff080.722-0.740 vs 0.510\uff09\u3002\u524d\u6cbf\u6a21\u578b\uff08GPT-4o\uff09\u4e0b\uff0c\u591a\u6a21\u6001\u6700\u4f73\uff080.808\uff09\uff0c\u6587\u672c\uff080.782\uff09\u548cColPali\uff080.745\uff09\u63a5\u8fd1\u3002GPT-5\u5bb6\u65cf\u4e2d\uff0cColPali\u548cColFlor\u63d0\u5347\u7ea62%\u81f30.828\uff0c\u5404\u68c0\u7d22\u5668\u8868\u73b0\u65e0\u7edf\u8ba1\u5dee\u5f02\u3002GPT-5-nano\u843d\u540e\u8f83\u5927\u6a21\u578b8-10%\u3002", "conclusion": "\u7b56\u7565\u9009\u62e9\u53d6\u51b3\u4e8e\u6a21\u578b\u80fd\u529b\uff1a\u4e2d\u7b49\u6a21\u578b\u9002\u5408\u56fe\u8868\u8f6c\u6587\u672c\u4ee5\u964d\u4f4e\u89e3\u8bfb\u8d1f\u62c5\uff0c\u524d\u6cbf\u6a21\u578b\u4e0bOCR-free\u68c0\u7d22\u53d8\u5f97\u6709\u7ade\u4e89\u529b\u3002ColFlor\u68c0\u7d22\u5668\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5360\u7528\u8f83\u5c0f\u8d44\u6e90\uff0c\u662f\u5f3a\u751f\u6210\u5668\u4e0b\u7684\u9ad8\u6548\u9ed8\u8ba4\u9009\u62e9\u3002"}}
{"id": "2512.16881", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16881", "abs": "https://arxiv.org/abs/2512.16881", "authors": ["Arhan Jain", "Mingtong Zhang", "Kanav Arora", "William Chen", "Marcel Torne", "Muhammad Zubair Irshad", "Sergey Zakharov", "Yue Wang", "Sergey Levine", "Chelsea Finn", "Wei-Chiu Ma", "Dhruv Shah", "Abhishek Gupta", "Karl Pertsch"], "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies", "comment": "Website: https://polaris-evals.github.io/", "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.", "AI": {"tldr": "PolaRiS\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u771f\u5b9e\u5230\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u5c06\u771f\u5b9e\u573a\u666f\u89c6\u9891\u8f6c\u6362\u4e3a\u4ea4\u4e92\u5f0f\u4eff\u771f\u73af\u5883\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7b56\u7565\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u4eff\u771f\u57fa\u51c6\u80fd\u66f4\u597d\u5730\u5173\u8054\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u9762\u4e34\u51c6\u786e\u6d4b\u91cf\u548c\u6bd4\u8f83\u7b56\u7565\u6027\u80fd\u7684\u6311\u6218\uff0c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u5b58\u5728\u968f\u673a\u6027\u3001\u53ef\u91cd\u590d\u6027\u5dee\u548c\u8017\u65f6\u7b49\u95ee\u9898\u3002\u73b0\u6709\u4eff\u771f\u57fa\u51c6\u4e0e\u771f\u5b9e\u4e16\u754c\u5b58\u5728\u89c6\u89c9\u548c\u7269\u7406\u9886\u57df\u5dee\u8ddd\uff0c\u65e0\u6cd5\u53ef\u9760\u6307\u5bfc\u7b56\u7565\u6539\u8fdb\uff0c\u4e14\u6784\u5efa\u771f\u5b9e\u591a\u6837\u7684\u4eff\u771f\u73af\u5883\u9700\u8981\u5927\u91cf\u4eba\u5de5\u548c\u4e13\u4e1a\u7ecf\u9a8c\u3002", "method": "\u63d0\u51faPolaRiS\u6846\u67b6\uff1a1\uff09\u5229\u7528\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u5c06\u771f\u5b9e\u573a\u666f\u7684\u77ed\u89c6\u9891\u626b\u63cf\u8f6c\u6362\u4e3a\u4ea4\u4e92\u5f0f\u4eff\u771f\u73af\u5883\uff1b2\uff09\u5f00\u53d1\u7b80\u5355\u7684\u4eff\u771f\u6570\u636e\u534f\u540c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f25\u5408\u5269\u4f59\u7684\u771f\u5b9e\u5230\u4eff\u771f\u5dee\u8ddd\uff0c\u5b9e\u73b0\u672a\u89c1\u4eff\u771f\u73af\u5883\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u5e7f\u6cdb\u914d\u5bf9\u8bc4\u4f30\uff0c\u8bc1\u660ePolaRiS\u8bc4\u4f30\u76f8\u6bd4\u73b0\u6709\u4eff\u771f\u57fa\u51c6\uff0c\u4e0e\u771f\u5b9e\u4e16\u754c\u901a\u7528\u7b56\u7565\u6027\u80fd\u6709\u66f4\u5f3a\u7684\u76f8\u5173\u6027\u3002\u5176\u7b80\u5355\u6027\u4e5f\u652f\u6301\u5feb\u901f\u521b\u5efa\u591a\u6837\u5316\u7684\u4eff\u771f\u73af\u5883\u3002", "conclusion": "PolaRiS\u4e3a\u4e0b\u4e00\u4ee3\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u5206\u5e03\u5f0f\u548c\u6c11\u4e3b\u5316\u8bc4\u4f30\u7684\u4e00\u6b65\uff0c\u901a\u8fc7\u771f\u5b9e\u5230\u4eff\u771f\u8f6c\u6362\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u673a\u5668\u4eba\u7b56\u7565\u8bc4\u4f30\u3002"}}
{"id": "2512.16814", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16814", "abs": "https://arxiv.org/abs/2512.16814", "authors": ["William English", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs", "comment": null, "summary": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.", "AI": {"tldr": "GraFT\u6846\u67b6\u901a\u8fc7\u9650\u5236\u6bcf\u4e00\u6b65\u7684\u6709\u6548\u8f93\u51fatoken\u6765\u7b80\u5316\u81ea\u7136\u8bed\u8a00\u5230\u65f6\u5e8f\u903b\u8f91\u7684\u7ffb\u8bd1\u4efb\u52a1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u7ffb\u8bd1\u51c6\u786e\u73875.49%\u548c\u57df\u5916\u7ffb\u8bd1\u51c6\u786e\u738714.06%", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u539f\u5b50\u547d\u9898\u63d0\u53d6\u3001\u5171\u6307\u6d88\u89e3\u548c\u6709\u9650\u6570\u636e\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684NL\u5230TL\u7ffb\u8bd1\u6846\u67b6", "method": "\u63d0\u51faGrammar Forced Translation (GraFT)\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6bcf\u4e2a\u95ee\u9898\u7684\u72ec\u7279\u5c5e\u6027\uff0c\u5728\u6bcf\u4e00\u6b65\u5c06\u6709\u6548\u8f93\u51fatoken\u4ece\u5b8c\u6574\u8bcd\u6c47\u8868\u9650\u5236\u5230\u5c11\u6570\u51e0\u4e2a\uff0c\u4ece\u800c\u51cf\u5c11\u89e3\u7a7a\u95f4\u590d\u6742\u5ea6", "result": "\u5728CW\u3001GLTL\u548cNavi\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGraFT\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u7aef\u5230\u7aef\u7ffb\u8bd1\u51c6\u786e\u73875.49%\uff0c\u57df\u5916\u7ffb\u8bd1\u51c6\u786e\u738714.06%", "conclusion": "GraFT\u901a\u8fc7\u89e3\u7a7a\u95f4\u7f29\u51cf\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86NL\u5230TL\u7ffb\u8bd1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7ffb\u8bd1\u6846\u67b6"}}
{"id": "2512.16896", "categories": ["cs.RO", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.16896", "abs": "https://arxiv.org/abs/2512.16896", "authors": ["Jinghuan Shang", "Harsh Patel", "Ran Gong", "Karl Schmeckpeper"], "title": "Sceniris: A Fast Procedural Scene Generation Framework", "comment": "Code is available at https://github.com/rai-inst/sceniris", "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris", "AI": {"tldr": "Sceniris\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u7a0b\u5e8f\u5316\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u751f\u6210\u5927\u89c4\u6a21\u3001\u65e0\u78b0\u649e\u7684\u573a\u666f\u53d8\u4f53\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0234\u500d\u52a0\u901f", "motivation": "\u73b0\u6709\u7a0b\u5e8f\u5316\u751f\u6210\u65b9\u6cd5\u8f93\u51fa\u541e\u5410\u91cf\u4f4e\uff0c\u6210\u4e3a\u6269\u5c55\u6570\u636e\u96c6\u521b\u5efa\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u8981\u9ad8\u6548\u751f\u6210\u5927\u89c4\u6a21\u3001\u65e0\u78b0\u649e\u7684\u573a\u666f\u53d8\u4f53", "method": "\u91c7\u7528\u6279\u91cf\u91c7\u6837\u548ccuRobo\u4e2d\u7684\u5feb\u901f\u78b0\u649e\u68c0\u6d4b\uff0c\u6269\u5c55\u4e86\u5bf9\u8c61\u95f4\u7a7a\u95f4\u5173\u7cfb\u652f\u6301\uff0c\u63d0\u4f9b\u53ef\u9009\u7684\u673a\u5668\u4eba\u53ef\u8fbe\u6027\u68c0\u67e5", "result": "\u76f8\u6bd4Scene Synthesizer\u65b9\u6cd5\uff0cSceniris\u5b9e\u73b0\u4e86\u81f3\u5c11234\u500d\u7684\u52a0\u901f\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u5927\u89c4\u6a21\u3001\u65e0\u78b0\u649e\u7684\u573a\u666f\u53d8\u4f53", "conclusion": "Sceniris\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u7a0b\u5e8f\u5316\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5927\u89c4\u6a21\u573a\u666f\u6570\u636e\u96c6\u7684\u521b\u5efa\uff0c\u652f\u6301\u7269\u7406AI\u548c\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55"}}
{"id": "2512.16832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16832", "abs": "https://arxiv.org/abs/2512.16832", "authors": ["Aditya Yadavalli", "Tiago Pimentel", "Tamar I Regev", "Ethan Wilcox", "Alex Warstadt"], "title": "What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels", "comment": null, "summary": "Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u65b9\u6cd5\u6765\u91cf\u5316\u8bed\u97f3\u4e2d\u97f5\u5f8b\uff08\u8bed\u8c03\uff09\u6240\u4f20\u9012\u7684\u4fe1\u606f\u91cf\uff0c\u53d1\u73b0\u5bf9\u4e8e\u8bbd\u523a\u548c\u60c5\u611f\u8868\u8fbe\uff0c\u97f3\u9891\u901a\u9053\u6bd4\u6587\u672c\u901a\u9053\u4f20\u9012\u7684\u4fe1\u606f\u91cf\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "motivation": "\u97f5\u5f8b\uff08\u8bed\u97f3\u7684\u65cb\u5f8b\uff09\u4f20\u8fbe\u4e86\u6587\u672c\u6216\u6587\u5b57\u65e0\u6cd5\u6355\u6349\u7684\u5173\u952e\u4fe1\u606f\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u91cf\u5316\u97f5\u5f8b\u5355\u72ec\u4f20\u9012\u7684\u4fe1\u606f\u91cf\u53ca\u5176\u5177\u4f53\u5185\u5bb9\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u97f3\u548c\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u8bdd\u8bed\u7279\u5b9a\u610f\u4e49\u7ef4\u5ea6\uff08\u5982\u60c5\u611f\uff09\u4e0e\u5176\u901a\u4fe1\u901a\u9053\uff08\u5982\u97f3\u9891\u6216\u6587\u672c\uff09\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u91cf\u5316\u97f3\u9891\u548c\u6587\u672c\u5728\u8bbd\u523a\u3001\u60c5\u611f\u548c\u7591\u95ee\u8868\u8fbe\u4e2d\u7684\u4fe1\u606f\u4f20\u9012\u80fd\u529b\u3002", "result": "\u5bf9\u4e8e\u8bbd\u523a\u548c\u60c5\u611f\u8868\u8fbe\uff0c\u97f3\u9891\u901a\u9053\uff08\u5373\u97f5\u5f8b\u901a\u9053\uff09\u4f20\u9012\u7684\u4fe1\u606f\u91cf\u6bd4\u7eaf\u6587\u672c\u901a\u9053\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff1b\u5bf9\u4e8e\u7591\u95ee\u8868\u8fbe\uff0c\u97f5\u5f8b\u63d0\u4f9b\u7684\u989d\u5916\u4fe1\u606f\u76f8\u5bf9\u8f83\u5c11\u3002", "conclusion": "\u63d0\u51fa\u5c06\u8fd9\u79cd\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u591a\u610f\u4e49\u7ef4\u5ea6\u3001\u901a\u4fe1\u901a\u9053\u548c\u8bed\u8a00\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u8ba1\u5212\uff0c\u4ee5\u66f4\u5168\u9762\u7406\u89e3\u97f5\u5f8b\u5728\u6c9f\u901a\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2512.16843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16843", "abs": "https://arxiv.org/abs/2512.16843", "authors": ["Harsh Vardhan Bansal"], "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference", "comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)", "summary": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications", "AI": {"tldr": "LLMCache\uff1a\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u5c42\u7ea7\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u4e2d\u95f4\u6fc0\u6d3b\u6765\u52a0\u901fTransformer\u63a8\u7406\uff0c\u5b9e\u73b0\u6700\u9ad83.1\u500d\u52a0\u901f\u4e14\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e0.5%", "motivation": "Transformer\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u73b0\u6709\u7684token\u7ea7KV\u7f13\u5b58\u673a\u5236\u9002\u7528\u8303\u56f4\u6709\u9650\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u52a0\u901f\u65b9\u6848\u3002", "method": "\u63d0\u51faLLMCache\u5c42\u7ea7\u7f13\u5b58\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u8f93\u5165\u5e8f\u5217\u8bed\u4e49\u76f8\u4f3c\u6027\u91cd\u7528\u4e2d\u95f4\u6fc0\u6d3b\uff1b2\uff09\u8f7b\u91cf\u7ea7\u6307\u7eb9\u673a\u5236\u5339\u914d\u8bed\u4e49\u76f8\u4f3c\u8f93\u5165\uff1b3\uff09\u81ea\u9002\u5e94\u6dd8\u6c70\u7b56\u7565\u7ba1\u7406\u7f13\u5b58\u9648\u65e7\u6027\uff1b4\uff09\u652f\u6301\u4efb\u610fTransformer\u5c42\u7f13\u5b58\uff0c\u517c\u5bb9\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u67b6\u6784\u3002", "result": "\u5728BERT\u548cGPT-2\u6a21\u578b\u4e0a\uff0c\u5728SQuAD\u3001WikiText-103\u548cOpenBookQA\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u63a8\u7406\u65f6\u95f4\u6700\u9ad8\u52a0\u901f3.1\u500d\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e0.5%\u3002", "conclusion": "LLMCache\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u901a\u7528\u7684Transformer\u63a8\u7406\u4f18\u5316\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.16883", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16883", "abs": "https://arxiv.org/abs/2512.16883", "authors": ["Tzu-Han Lin", "Wei-Lin Chen", "Chen-An Li", "Hung-yi Lee", "Yun-Nung Chen", "Yu Meng"], "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning", "comment": "Preprint. Code and artifacts will be uploaded to https://github.com/hank0316/AdaSearch", "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.", "AI": {"tldr": "\u63d0\u51faAdaSearch\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u5206\u79bb\u95ee\u9898\u89e3\u51b3\u4e0e\u641c\u7d22\u51b3\u7b56\uff0c\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u641c\u7d22\u4ee3\u7406\u7684\u81ea\u77e5\u80fd\u529b\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u641c\u7d22\u8c03\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u641c\u7d22\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u548c\u566a\u58f0/\u6076\u610f\u5185\u5bb9\u98ce\u9669\uff0c\u800c\u4ec5\u4f9d\u8d56\u53c2\u6570\u77e5\u8bc6\u53c8\u4f1a\u4ea7\u751f\u5e7b\u89c9\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u60e9\u7f5a\u5de5\u5177\u8c03\u7528\u6b21\u6570\u6765\u7f13\u89e3\u641c\u7d22\u6ee5\u7528\uff0c\u4f46\u8fd9\u9700\u8981\u5927\u91cf\u5956\u52b1\u5de5\u7a0b\u3001\u4fe1\u7528\u5206\u914d\u6a21\u7cca\uff0c\u4e14\u53ef\u80fd\u88ab\u4ee3\u7406\u8868\u9762\u51cf\u5c11\u8c03\u7528\u800c\u5229\u7528\u3002", "method": "\u63d0\u51faAdaSearch\uff1a\u4e00\u4e2a\u7b80\u5355\u7684\u4e24\u9636\u6bb5\u3001\u7ed3\u679c\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u95ee\u9898\u89e3\u51b3\u4e0e\u662f\u5426\u8c03\u7528\u641c\u7d22\u7684\u51b3\u7b56\u5206\u79bb\uff0c\u4f7f\u51b3\u7b56\u8fc7\u7a0b\u53d8\u5f97\u660e\u786e\u548c\u53ef\u89e3\u91ca\u3002\u9996\u5148\u91cf\u5316\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u7684\u81ea\u77e5\u80fd\u529b\uff0c\u7136\u540e\u8bbe\u8ba1\u65b0\u6846\u67b6\u6765\u6539\u8fdb\u77e5\u8bc6\u8fb9\u754c\u610f\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdaSearch\u663e\u8457\u63d0\u9ad8\u4e86\u77e5\u8bc6\u8fb9\u754c\u610f\u8bc6\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u641c\u7d22\u8c03\u7528\uff0c\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u884c\u4e3a\u3002\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u548c\u89c4\u6a21\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "AdaSearch\u901a\u8fc7\u660e\u786e\u5206\u79bb\u95ee\u9898\u89e3\u51b3\u548c\u641c\u7d22\u51b3\u7b56\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u8fc7\u5ea6\u4f9d\u8d56\u641c\u7d22\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u91d1\u878d\u548c\u533b\u7597\u95ee\u7b54\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002"}}
{"id": "2512.16899", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16899", "abs": "https://arxiv.org/abs/2512.16899", "authors": ["Yushi Hu", "Reyhane Askari-Hemmat", "Melissa Hall", "Emily Dinan", "Luke Zettlemoyer", "Marjan Ghazvininejad"], "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image", "comment": "Code and data available at https://github.com/facebookresearch/MMRB2", "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.", "AI": {"tldr": "MMRB2\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u6db5\u76d6\u56fe\u50cf\u751f\u6210\u3001\u7f16\u8f91\u3001\u4ea4\u9519\u751f\u6210\u548c\u63a8\u7406\u56db\u5927\u4efb\u52a1\uff0c\u5305\u542b4000\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u504f\u597d\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u5bf9\u4e8e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u5904\u7406\u4ea4\u9519\u56fe\u50cf\u548c\u6587\u672c\u5e8f\u5217\u7684\u5168\u80fd\u6a21\u578b\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u521b\u5efaMMRB2\u57fa\u51c6\uff0c\u5305\u542b\u56db\u5927\u4efb\u52a1\uff1a\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u7f16\u8f91\u3001\u4ea4\u9519\u751f\u6210\u548c\u591a\u6a21\u6001\u63a8\u7406\u3002\u6536\u96c61000\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u504f\u597d\u5bf9/\u4efb\u52a1\uff0c\u6765\u81ea23\u4e2a\u6a21\u578b\u548c\u4ee3\u7406\u572821\u4e2a\u6e90\u4efb\u52a1\u4e2d\u3002\u91c7\u7528\u96c6\u6210\u8fc7\u6ee4\u7b56\u7565\u786e\u4fdd\u504f\u597d\u5bf9\u5177\u6709\u5f3a\u4eba\u7c7b\u4e13\u5bb6\u5171\u8bc6\u3002", "result": "Gemini 3 Pro\u8fbe\u523075-80%\u51c6\u786e\u7387\uff0cGPT-5\u548cGemini 2.5 Pro\u8fbe\u523066-75%\uff0c\u4f18\u4e8eGPT-4o\u768459%\u3002\u6700\u4f73\u5f00\u6e90\u6a21\u578bQwen3-VL-32B\u4e0eGemini 2.5 Flash\u76f8\u5f53\uff0864%\uff09\u3002\u4eba\u7c7b\u51c6\u786e\u7387\u8d85\u8fc790%\u3002MMRB2\u6027\u80fd\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6210\u529f\u5f3a\u76f8\u5173\u3002", "conclusion": "MMRB2\u4e3a\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff08\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff09\uff0c\u5e76\u6307\u51fa\u4e86\u5956\u52b1\u6a21\u578b\u6539\u8fdb\u7684\u5173\u952e\u65b9\u5411\u3002\u57fa\u51c6\u6027\u80fd\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6210\u529f\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.16902", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16902", "abs": "https://arxiv.org/abs/2512.16902", "authors": ["Eric Todd", "Jannik Brinkmann", "Rohit Gandikota", "David Bau"], "title": "In-Context Algebra", "comment": "28 pages, 18 figures. Code and data at https://algebra.baulab.info", "summary": "We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.", "AI": {"tldr": "Transformer\u5728\u53d8\u91cf\u7b26\u53f7\u542b\u4e49\u4e0d\u56fa\u5b9a\u7684\u7b97\u672f\u4efb\u52a1\u4e2d\uff0c\u53d1\u5c55\u51fa\u7b26\u53f7\u63a8\u7406\u673a\u5236\u800c\u975e\u51e0\u4f55\u5d4c\u5165\uff0c\u5305\u62ec\u4ea4\u6362\u590d\u5236\u3001\u5355\u4f4d\u5143\u8bc6\u522b\u548c\u95ed\u5305\u6d88\u53bb\u4e09\u79cd\u673a\u5236\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u53d1\u73b0Transformer\u5728\u56fa\u5b9a\u7b26\u53f7\u542b\u4e49\u7684\u7b97\u672f\u4efb\u52a1\u4e2d\u4f1a\u53d1\u5c55\u51e0\u4f55\u5d4c\u5165\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7b26\u53f7\u542b\u4e49\u5f80\u5f80\u662f\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5f53\u7b26\u53f7\u542b\u4e49\u968f\u5e8f\u5217\u53d8\u5316\u65f6\uff0cTransformer\u4f1a\u53d1\u5c55\u4f55\u79cd\u63a8\u7406\u673a\u5236\u3002", "method": "\u8bbe\u8ba1\u65b0\u4efb\u52a1\uff1a\u7b26\u53f7\u4e0e\u4ee3\u6570\u7fa4\u5143\u7d20\u7684\u5bf9\u5e94\u5173\u7cfb\u5728\u5e8f\u5217\u95f4\u53d8\u5316\u3002\u521b\u5efa\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u5206\u5e03\u6765\u56e0\u679c\u6d4b\u8bd5\u5047\u8bbe\u673a\u5236\uff0c\u5206\u6790\u6a21\u578b\u5b66\u4e60\u5230\u7684\u5185\u90e8\u673a\u5236\u3002", "result": "Transformer\u5728\u6311\u6218\u6027\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff0c\u751a\u81f3\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4ee3\u6570\u7fa4\u3002\u8bc6\u522b\u51fa\u4e09\u79cd\u4e00\u81f4\u5b66\u4e60\u7684\u673a\u5236\uff1a\u4ea4\u6362\u590d\u5236\uff08\u4e13\u7528\u5934\u590d\u5236\u7b54\u6848\uff09\u3001\u5355\u4f4d\u5143\u8bc6\u522b\uff08\u533a\u5206\u5305\u542b\u5355\u4f4d\u5143\u7684\u4e8b\u5b9e\uff09\u3001\u95ed\u5305\u6d88\u53bb\uff08\u8ddf\u8e2a\u7fa4\u6210\u5458\u8d44\u683c\u4ee5\u7ea6\u675f\u6709\u6548\u7b54\u6848\uff09\u3002", "conclusion": "\u4e0e\u56fa\u5b9a\u7b26\u53f7\u8bbe\u7f6e\u4e2d\u7684\u51e0\u4f55\u8868\u793a\u4e92\u8865\uff0c\u5f53\u8bad\u7ec3\u6a21\u578b\u5728\u7b26\u53f7\u542b\u4e49\u4e0d\u56fa\u5b9a\u7684\u4e0a\u4e0b\u6587\u4e2d\u63a8\u7406\u65f6\uff0c\u6a21\u578b\u4f1a\u53d1\u5c55\u7b26\u53f7\u63a8\u7406\u673a\u5236\u800c\u975e\u51e0\u4f55\u5d4c\u5165\u3002"}}
{"id": "2512.16914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16914", "abs": "https://arxiv.org/abs/2512.16914", "authors": ["Nikhil Prakash", "Donghao Ren", "Dominik Moritz", "Yannick Assogba"], "title": "Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates", "comment": "18 pages, 3 figures", "summary": "Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.", "AI": {"tldr": "\u63d0\u51faConstructive Circuit Amplification\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952etoken\u548c\u6a21\u578b\u7ec4\u4ef6\uff0c\u4ec5\u66f4\u65b0\u5c11\u91cf\u7ec4\u4ef6\u6765\u589e\u5f3a\u7279\u5b9a\u4efb\u52a1\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u63d0\u534711.4%\u51c6\u786e\u7387\uff0c\u4ec5\u4fee\u65391.59%\u7ec4\u4ef6\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u53d1\u73b0LLMs\u4e2d\u5b58\u5728\u8d1f\u8d23\u7279\u5b9a\u4efb\u52a1\u7684\u7a00\u758f\u5b50\u7f51\u7edc\uff08circuits\uff09\uff0c\u4e14\u5fae\u8c03\u901a\u5e38\u901a\u8fc7\u5f3a\u5316\u73b0\u6709circuits\u6765\u63d0\u5347\u6027\u80fd\u3002\u8fd9\u542f\u53d1\u4e86\u76f4\u63a5\u5e72\u9884\u8fd9\u4e9bcircuits\u8fdb\u884c\u7cbe\u786e\u4efb\u52a1\u5b9a\u5411\u66f4\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faConstructive Circuit Amplification\u65b9\u6cd5\uff1a1\uff09\u4ece\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u8bc6\u522b\u5173\u952etoken\uff1b2\uff09\u8bc6\u522b\u8d1f\u8d23\u76ee\u6807\u4efb\u52a1\u7684\u6a21\u578b\u7ec4\u4ef6\uff1b3\uff09\u4ec5\u66f4\u65b0\u8fd9\u4e9b\u7279\u5b9a\u7ec4\u4ef6\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe+11.4%\uff08\u8de8\u591a\u4e2a\u6a21\u578b\uff09\uff0c\u4ec5\u4fee\u65391.59%\u7684\u6a21\u578b\u7ec4\u4ef6\uff0c\u5728MMLU\u3001TriviaQA\u548cTruthfulQA\u7b49\u5176\u4ed6\u80fd\u529b\u6d4b\u8bd5\u4e2d\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u7a00\u758f\u7684\u6a21\u578b\u7ec4\u4ef6\u96c6\u5408\uff0c\u53ef\u4ee5\u53ef\u9760\u5730\u589e\u5f3a\u76ee\u6807\u80fd\u529b\uff0c\u8fd9\u4e3a\u7cbe\u786e\u7684\u4efb\u52a1\u5b9a\u5411\u6a21\u578b\u66f4\u65b0\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
