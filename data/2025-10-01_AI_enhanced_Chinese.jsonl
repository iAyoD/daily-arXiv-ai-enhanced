{"id": "2509.25200", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25200", "abs": "https://arxiv.org/abs/2509.25200", "authors": ["Christian Arzate Cruz", "Edwin C. Montiel-Vazquez", "Chikara Maeda", "Randy Gomez"], "title": "When and How to Express Empathy in Human-Robot Interaction Scenarios", "comment": null, "summary": "Incorporating empathetic behavior into robots can improve their social\neffectiveness and interaction quality. In this paper, we present whEE (when and\nhow to express empathy), a framework that enables social robots to detect when\nempathy is needed and generate appropriate responses. Using large language\nmodels, whEE identifies key behavioral empathy cues in human interactions. We\nevaluate it in human-robot interaction scenarios with our social robot, Haru.\nResults show that whEE effectively identifies and responds to empathy cues,\nproviding valuable insights for designing social robots capable of adaptively\nmodulating their empathy levels across various interaction contexts.", "AI": {"tldr": "whEE\u6846\u67b6\u4f7f\u793e\u4ea4\u673a\u5668\u4eba\u80fd\u591f\u68c0\u6d4b\u4f55\u65f6\u9700\u8981\u8868\u8fbe\u540c\u7406\u5fc3\u5e76\u751f\u6210\u9002\u5f53\u56de\u5e94\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u4eba\u7c7b\u4e92\u52a8\u4e2d\u7684\u884c\u4e3a\u540c\u7406\u5fc3\u7ebf\u7d22", "motivation": "\u5c06\u540c\u7406\u5fc3\u884c\u4e3a\u878d\u5165\u673a\u5668\u4eba\u53ef\u4ee5\u63d0\u9ad8\u5176\u793e\u4ea4\u6709\u6548\u6027\u548c\u4e92\u52a8\u8d28\u91cf", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u4eba\u7c7b\u4e92\u52a8\u4e2d\u7684\u5173\u952e\u884c\u4e3a\u540c\u7406\u5fc3\u7ebf\u7d22\uff0c\u901a\u8fc7whEE\u6846\u67b6\u68c0\u6d4b\u4f55\u65f6\u9700\u8981\u540c\u7406\u5fc3\u5e76\u751f\u6210\u56de\u5e94", "result": "\u5728\u4e0e\u4eba\u5f62\u673a\u5668\u4ebaHaru\u7684\u4eba\u673a\u4e92\u52a8\u573a\u666f\u4e2d\u8bc4\u4f30\uff0cwhEE\u80fd\u6709\u6548\u8bc6\u522b\u548c\u56de\u5e94\u540c\u7406\u5fc3\u7ebf\u7d22", "conclusion": "\u4e3a\u8bbe\u8ba1\u80fd\u591f\u6839\u636e\u4e0d\u540c\u4e92\u52a8\u60c5\u5883\u81ea\u9002\u5e94\u8c03\u8282\u540c\u7406\u5fc3\u6c34\u5e73\u7684\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3"}}
{"id": "2509.25249", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25249", "abs": "https://arxiv.org/abs/2509.25249", "authors": ["Guancheng Chen", "Sheng Yang", "Tong Zhan", "Jian Wang"], "title": "BEV-VLM: Trajectory Planning via Unified BEV Abstraction", "comment": null, "summary": "This paper introduces BEV-VLM, a novel framework for trajectory planning in\nautonomous driving that leverages Vision-Language Models (VLMs) with Bird's-Eye\nView (BEV) feature maps as visual inputs. Unlike conventional approaches that\nrely solely on raw visual data such as camera images, our method utilizes\nhighly compressed and informative BEV representations, which are generated by\nfusing multi-modal sensor data (e.g., camera and LiDAR) and aligning them with\nHD Maps. This unified BEV-HD Map format provides a geometrically consistent and\nrich scene description, enabling VLMs to perform accurate trajectory planning.\nExperimental results on the nuScenes dataset demonstrate 44.8% improvements in\nplanning accuracy and complete collision avoidance. Our work highlights that\nVLMs can effectively interpret processed visual representations like BEV\nfeatures, expanding their applicability beyond raw images in trajectory\nplanning.", "AI": {"tldr": "BEV-VLM\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u9e1f\u77b0\u56fe\u7279\u5f81\u4f5c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u8f93\u5165\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u7cbe\u5ea6\u548c\u78b0\u649e\u907f\u514d\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u539f\u59cb\u89c6\u89c9\u6570\u636e\u5982\u76f8\u673a\u56fe\u50cf\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\uff0c\u4f46BEV\u8868\u793a\u80fd\u63d0\u4f9b\u66f4\u538b\u7f29\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u4e0e\u9ad8\u6e05\u5730\u56fe\u5bf9\u9f50\u540e\u5f62\u6210\u51e0\u4f55\u4e00\u81f4\u7684\u73af\u5883\u8868\u793a\u3002", "method": "\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff08\u76f8\u673a\u548c\u6fc0\u5149\u96f7\u8fbe\uff09\u751f\u6210BEV\u7279\u5f81\uff0c\u5e76\u4e0e\u9ad8\u6e05\u5730\u56fe\u5bf9\u9f50\u5f62\u6210\u7edf\u4e00\u7684BEV-HD Map\u683c\u5f0f\uff0c\u4f5c\u4e3aVLM\u7684\u89c6\u89c9\u8f93\u5165\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u89c4\u5212\u7cbe\u5ea6\u63d0\u5347\u4e8644.8%\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b8c\u5168\u78b0\u649e\u907f\u514d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u89e3\u91ca\u5904\u7406\u8fc7\u7684\u89c6\u89c9\u8868\u793a\u5982BEV\u7279\u5f81\uff0c\u6269\u5c55\u4e86\u5176\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u8d85\u8d8a\u539f\u59cb\u56fe\u50cf\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.25352", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25352", "abs": "https://arxiv.org/abs/2509.25352", "authors": ["Itamar Mishani", "Yorai Shaoul", "Ramkumar Natarajan", "Jiaoyang Li", "Maxim Likhachev"], "title": "SRMP: Search-Based Robot Motion Planning Library", "comment": "Submitted for Publication", "summary": "Motion planning is a critical component in any robotic system. Over the\nyears, powerful tools like the Open Motion Planning Library (OMPL) have been\ndeveloped, offering numerous motion planning algorithms. However, existing\nframeworks often struggle to deliver the level of predictability and\nrepeatability demanded by high-stakes applications -- ranging from ensuring\nsafety in industrial environments to the creation of high-quality motion\ndatasets for robot learning. Complementing existing tools, we introduce SRMP\n(Search-based Robot Motion Planning), a new software framework tailored for\nrobotic manipulation. SRMP distinguishes itself by generating consistent and\nreliable trajectories, and is the first software tool to offer motion planning\nalgorithms for multi-robot manipulation tasks. SRMP easily integrates with\nmajor simulators, including MuJoCo, Sapien, Genesis, and PyBullet via a Python\nand C++ API. SRMP includes a dedicated MoveIt! plugin that enables immediate\ndeployment on robot hardware and seamless integration with existing pipelines.\nThrough extensive evaluations, we demonstrate in this paper that SRMP not only\nmeets the rigorous demands of industrial and safety-critical applications but\nalso sets a new standard for consistency in motion planning across diverse\nrobotic systems. Visit srmp.readthedocs.io for SRMP documentation and\ntutorials.", "AI": {"tldr": "SRMP\u662f\u4e00\u4e2a\u4e13\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u8bbe\u8ba1\u7684\u8fd0\u52a8\u89c4\u5212\u8f6f\u4ef6\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u4e00\u81f4\u53ef\u9760\u7684\u8f68\u8ff9\uff0c\u662f\u9996\u4e2a\u652f\u6301\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u5de5\u5177", "motivation": "\u73b0\u6709\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7f3a\u4e4f\u8db3\u591f\u7684\u53ef\u9884\u6d4b\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5de5\u4e1a\u5b89\u5168\u548c\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6570\u636e\u96c6\u521b\u5efa\u7684\u9700\u6c42", "method": "\u5f00\u53d1\u4e86SRMP\u6846\u67b6\uff0c\u63d0\u4f9bPython\u548cC++ API\uff0c\u652f\u6301\u4e0eMuJoCo\u3001Sapien\u3001Genesis\u3001PyBullet\u7b49\u4e3b\u6d41\u6a21\u62df\u5668\u96c6\u6210\uff0c\u5305\u542b\u4e13\u95e8\u7684MoveIt!\u63d2\u4ef6", "result": "SRMP\u4e0d\u4ec5\u6ee1\u8db3\u5de5\u4e1a\u548c\u5b89\u5168\u6027\u5173\u952e\u5e94\u7528\u7684\u4e25\u683c\u8981\u6c42\uff0c\u8fd8\u5728\u4e0d\u540c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u4e3a\u8fd0\u52a8\u89c4\u5212\u7684\u4e00\u81f4\u6027\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6", "conclusion": "SRMP\u586b\u8865\u4e86\u73b0\u6709\u8fd0\u52a8\u89c4\u5212\u5de5\u5177\u5728\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u9002\u5408\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u548c\u9ad8\u98ce\u9669\u5e94\u7528\u573a\u666f"}}
{"id": "2509.25358", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25358", "abs": "https://arxiv.org/abs/2509.25358", "authors": ["Qianzhong Chen", "Justin Yu", "Mac Schwager", "Pieter Abbeel", "Fred Shentu", "Philipp Wu"], "title": "SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation", "comment": null, "summary": "Large-scale robot learning has recently shown promise for enabling robots to\nperform complex tasks by integrating perception, control, and language\nunderstanding. Yet, it struggles with long-horizon, contact-rich manipulation\nsuch as deformable object handling, where demonstration quality is\ninconsistent. Reward modeling offers a natural solution: by providing grounded\nprogress signals, it transforms noisy demonstrations into stable supervision\nthat generalizes across diverse trajectories. We introduce a stage-aware,\nvideo-based reward modeling framework that jointly predicts high-level task\nstages and fine-grained progress. Reward labels are automatically derived from\nnatural language subtask annotations, ensuring consistent progress estimation\nacross variable-length demonstrations. This design overcomes frame-index\nlabeling, which fails in variable-duration tasks like folding a T-shirt. Our\nreward model demonstrates robustness to variability, generalization to\nout-of-distribution settings, and strong utility for policy training. Building\non it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters\nhigh-quality data and reweights samples by reward. Experiments show the reward\nmodel alone outperforms baselines on validation and real robot rollouts.\nIntegrated into RA-BC, our approach achieves 83\\% success on folding T-shirts\nfrom the flattened state and 67\\% from the crumpled state -- far surpassing\nvanilla behavior cloning, which attains only 8\\% and 0\\% success. Overall, our\nresults highlight reward modeling as a key enabler for scalable,\nannotation-efficient, and robust imitation learning in long-horizon\nmanipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7684\u9636\u6bb5\u611f\u77e5\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u9ad8\u7ea7\u4efb\u52a1\u9636\u6bb5\u548c\u7ec6\u7c92\u5ea6\u8fdb\u5ea6\u6765\u89e3\u51b3\u957f\u65f6\u7a0b\u63a5\u89e6\u5f0f\u64cd\u4f5c\u4e2d\u7684\u6f14\u793a\u8d28\u91cf\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u5956\u52b1\u5bf9\u9f50\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c4\u6a21\u673a\u5668\u4eba\u5b66\u4e60\u5728\u5904\u7406\u957f\u65f6\u7a0b\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u53ef\u53d8\u5f62\u7269\u4f53\u5904\u7406\uff09\u65f6\u9762\u4e34\u6f14\u793a\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u6311\u6218\uff0c\u5956\u52b1\u5efa\u6a21\u53ef\u4ee5\u63d0\u4f9b\u7a33\u5b9a\u7684\u8fdb\u5ea6\u4fe1\u53f7\u6765\u6539\u8fdb\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e86\u9636\u6bb5\u611f\u77e5\u7684\u89c6\u9891\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u81ea\u52a8\u4ece\u81ea\u7136\u8bed\u8a00\u5b50\u4efb\u52a1\u6ce8\u91ca\u4e2d\u63a8\u5bfc\u5956\u52b1\u6807\u7b7e\uff0c\u514b\u670d\u4e86\u53d8\u957f\u6f14\u793a\u4e2d\u7684\u5e27\u7d22\u5f15\u6807\u6ce8\u95ee\u9898\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u5956\u52b1\u5bf9\u9f50\u884c\u4e3a\u514b\u9686(RA-BC)\uff0c\u901a\u8fc7\u5956\u52b1\u8fc7\u6ee4\u9ad8\u8d28\u91cf\u6570\u636e\u5e76\u91cd\u65b0\u52a0\u6743\u6837\u672c\u3002", "result": "\u5956\u52b1\u6a21\u578b\u5728\u9a8c\u8bc1\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002RA-BC\u5728\u6298\u53e0T\u6064\u4efb\u52a1\u4e2d\uff0c\u4ece\u5e73\u6574\u72b6\u6001\u8fbe\u523083%\u6210\u529f\u7387\uff0c\u4ece\u76b1\u8936\u72b6\u6001\u8fbe\u523067%\u6210\u529f\u7387\uff0c\u8fdc\u8d85\u4ec58%\u548c0%\u7684\u666e\u901a\u884c\u4e3a\u514b\u9686\u3002", "conclusion": "\u5956\u52b1\u5efa\u6a21\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u6ce8\u91ca\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u6a21\u4eff\u5b66\u4e60\u7684\u5173\u952e\u63a8\u52a8\u56e0\u7d20\u3002"}}
{"id": "2509.25402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25402", "abs": "https://arxiv.org/abs/2509.25402", "authors": ["Hanlan Yang", "Itamar Mishani", "Luca Pivetti", "Zachary Kingston", "Maxim Likhachev"], "title": "Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models", "comment": "Submitted for Publication", "summary": "Actor-Critic models are a class of model-free deep reinforcement learning\n(RL) algorithms that have demonstrated effectiveness across various robot\nlearning tasks. While considerable research has focused on improving training\nstability and data sampling efficiency, most deployment strategies have\nremained relatively simplistic, typically relying on direct actor policy\nrollouts. In contrast, we propose \\pachs{} (\\textit{P}arallel\n\\textit{A}ctor-\\textit{C}ritic \\textit{H}euristic \\textit{S}earch), an\nefficient parallel best-first search algorithm for inference that leverages\nboth components of the actor-critic architecture: the actor network generates\nactions, while the critic network provides cost-to-go estimates to guide the\nsearch. Two levels of parallelism are employed within the search -- actions and\ncost-to-go estimates are generated in batches by the actor and critic networks\nrespectively, and graph expansion is distributed across multiple threads. We\ndemonstrate the effectiveness of our approach in robotic manipulation tasks,\nincluding collision-free motion planning and contact-rich interactions such as\nnon-prehensile pushing. Visit p-achs.github.io for demonstrations and examples.", "AI": {"tldr": "\u63d0\u51faP-ACHS\u7b97\u6cd5\uff0c\u4e00\u79cd\u5229\u7528\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\u7684\u5e76\u884c\u6700\u4f73\u4f18\u5148\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u63a8\u7406\u9636\u6bb5", "motivation": "\u73b0\u6709\u7684\u6f14\u5458-\u6279\u8bc4\u5bb6\u6a21\u578b\u90e8\u7f72\u7b56\u7565\u8fc7\u4e8e\u7b80\u5355\uff0c\u901a\u5e38\u4ec5\u4f9d\u8d56\u76f4\u63a5\u6f14\u5458\u7b56\u7565\u5c55\u5f00\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8bc4\u8bba\u5bb6\u7f51\u7edc\u63d0\u4f9b\u7684\u4ef7\u503c\u4f30\u8ba1\u4fe1\u606f", "method": "\u8bbe\u8ba1\u5e76\u884c\u6700\u4f73\u4f18\u5148\u641c\u7d22\u7b97\u6cd5\uff0c\u5229\u7528\u6f14\u5458\u7f51\u7edc\u6279\u91cf\u751f\u6210\u52a8\u4f5c\uff0c\u8bc4\u8bba\u5bb6\u7f51\u7edc\u63d0\u4f9b\u6210\u672c\u4f30\u8ba1\u6307\u5bfc\u641c\u7d22\uff0c\u91c7\u7528\u52a8\u4f5c\u548c\u6210\u672c\u4f30\u8ba1\u6279\u91cf\u751f\u6210\u3001\u56fe\u6269\u5c55\u591a\u7ebf\u7a0b\u5e76\u884c\u5316", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5305\u62ec\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u975e\u6293\u63e1\u63a8\u52a8\u4ea4\u4e92", "conclusion": "P-ACHS\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\u7684\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u641c\u7d22\u6027\u80fd"}}
{"id": "2509.25443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25443", "abs": "https://arxiv.org/abs/2509.25443", "authors": ["Zewen He", "Chenyuan Chen", "Dilshod Azizov", "Yoshihiko Nakamura"], "title": "CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation", "comment": "Submitted to IEEE for possible publication, under review", "summary": "Humanoid whole-body locomotion control is a critical approach for humanoid\nrobots to leverage their inherent advantages. Learning-based control methods\nderived from retargeted human motion data provide an effective means of\naddressing this issue. However, because most current human datasets lack\nmeasured force data, and learning-based robot control is largely\nposition-based, achieving appropriate compliance during interaction with real\nenvironments remains challenging. This paper presents Compliant Task Pipeline\n(CoTaP): a pipeline that leverages compliance information in the learning-based\nstructure of humanoid robots. A two-stage dual-agent reinforcement learning\nframework combined with model-based compliance control for humanoid robots is\nproposed. In the training process, first a base policy with a position-based\ncontroller is trained; then in the distillation, the upper-body policy is\ncombined with model-based compliance control, and the lower-body agent is\nguided by the base policy. In the upper-body control, adjustable task-space\ncompliance can be specified and integrated with other controllers through\ncompliance modulation on the symmetric positive definite (SPD) manifold,\nensuring system stability. We validated the feasibility of the proposed\nstrategy in simulation, primarily comparing the responses to external\ndisturbances under different compliance settings.", "AI": {"tldr": "\u63d0\u51faCompliant Task Pipeline (CoTaP)\uff0c\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u7ed3\u6784\u548c\u57fa\u4e8e\u6a21\u578b\u5408\u89c4\u63a7\u5236\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u53ef\u8c03\u8282\u7684\u4efb\u52a1\u7a7a\u95f4\u5408\u89c4\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u7684\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\u7f3a\u4e4f\u529b\u6570\u636e\uff0c\u4f4d\u7f6e\u63a7\u5236\u96be\u4ee5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u9002\u5f53\u7684\u5408\u89c4\u6027\uff0c\u9700\u8981\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u73af\u5883\u4ea4\u4e92\u65f6\u7684\u5408\u89c4\u63a7\u5236\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u53cc\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u57fa\u4e8e\u4f4d\u7f6e\u63a7\u5236\u7684\u57fa\u7840\u7b56\u7565\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u4e0a\u534a\u8eab\u7b56\u7565\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u5408\u89c4\u63a7\u5236\u7ed3\u5408\uff0c\u4e0b\u534a\u8eab\u7531\u57fa\u7840\u7b56\u7565\u6307\u5bfc\uff0c\u901a\u8fc7SPD\u6d41\u5f62\u4e0a\u7684\u5408\u89c4\u8c03\u5236\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u7b56\u7565\u7684\u53ef\u884c\u6027\uff0c\u4e3b\u8981\u6bd4\u8f83\u4e86\u4e0d\u540c\u5408\u89c4\u8bbe\u7f6e\u4e0b\u5bf9\u5916\u90e8\u6270\u52a8\u7684\u54cd\u5e94\u3002", "conclusion": "CoTaP\u7ba1\u9053\u80fd\u591f\u6709\u6548\u6574\u5408\u5b66\u4e60\u63a7\u5236\u4e0e\u5408\u89c4\u63a7\u5236\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63d0\u4f9b\u53ef\u8c03\u8282\u7684\u4efb\u52a1\u7a7a\u95f4\u5408\u89c4\u6027\uff0c\u589e\u5f3a\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2509.25542", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25542", "abs": "https://arxiv.org/abs/2509.25542", "authors": ["Zihan Zhang", "Abhijit Ravichandran", "Pragnya Korti", "Luobin Wang", "Henrik I. Christensen"], "title": "Online Mapping for Autonomous Driving: Addressing Sensor Generalization and Dynamic Map Updates in Campus Environments", "comment": "19th International Symposium on Experimental Robotics", "summary": "High-definition (HD) maps are essential for autonomous driving, providing\nprecise information such as road boundaries, lane dividers, and crosswalks to\nenable safe and accurate navigation. However, traditional HD map generation is\nlabor-intensive, expensive, and difficult to maintain in dynamic environments.\nTo overcome these challenges, we present a real-world deployment of an online\nmapping system on a campus golf cart platform equipped with dual front cameras\nand a LiDAR sensor. Our work tackles three core challenges: (1) labeling a 3D\nHD map for campus environment; (2) integrating and generalizing the SemVecMap\nmodel onboard; and (3) incrementally generating and updating the predicted HD\nmap to capture environmental changes. By fine-tuning with campus-specific data,\nour pipeline produces accurate map predictions and supports continual updates,\ndemonstrating its practical value in real-world autonomous driving scenarios.", "AI": {"tldr": "\u5728\u6821\u56ed\u9ad8\u5c14\u592b\u7403\u8f66\u4e0a\u90e8\u7f72\u7684\u5b9e\u65f6\u5728\u7ebf\u9ad8\u7cbe\u5730\u56fe\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u6444\u50cf\u5934\u548cLiDAR\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u6821\u56ed\u73af\u58833D\u9ad8\u7cbe\u5730\u56fe\u6807\u6ce8\u3001SemVecMap\u6a21\u578b\u96c6\u6210\u4e0e\u6cdb\u5316\uff0c\u4ee5\u53ca\u589e\u91cf\u5f0f\u5730\u56fe\u66f4\u65b0\u3002", "motivation": "\u4f20\u7edf\u9ad8\u7cbe\u5730\u56fe\u5236\u4f5c\u6210\u672c\u9ad8\u3001\u7ef4\u62a4\u56f0\u96be\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u53d8\u5316\uff0c\u9700\u8981\u5f00\u53d1\u5b9e\u65f6\u5728\u7ebf\u5730\u56fe\u751f\u6210\u7cfb\u7edf\u3002", "method": "\u5728\u914d\u5907\u53cc\u524d\u6444\u50cf\u5934\u548cLiDAR\u4f20\u611f\u5668\u7684\u6821\u56ed\u9ad8\u5c14\u592b\u7403\u8f66\u5e73\u53f0\u4e0a\u90e8\u7f72\u5728\u7ebf\u5730\u56fe\u7cfb\u7edf\uff0c\u4f7f\u7528\u6821\u56ed\u7279\u5b9a\u6570\u636e\u5fae\u8c03SemVecMap\u6a21\u578b\uff0c\u5b9e\u73b0\u589e\u91cf\u5f0f\u5730\u56fe\u751f\u6210\u548c\u66f4\u65b0\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5730\u56fe\u5e76\u652f\u6301\u6301\u7eed\u66f4\u65b0\uff0c\u5728\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u5728\u7ebf\u5730\u56fe\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u9ad8\u7cbe\u5730\u56fe\u5236\u4f5c\u548c\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b9e\u65f6\u5730\u56fe\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25556", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.25556", "abs": "https://arxiv.org/abs/2509.25556", "authors": ["Mohammad Merati", "David Casta\u00f1\u00f3n"], "title": "Exhaustive-Serve-Longest Control for Multi-robot Scheduling Systems", "comment": null, "summary": "We study online task allocation for multi-robot, multi-queue systems with\nstochastic arrivals and switching delays. Time is slotted; each location can\nhost at most one robot per slot; service consumes one slot; switching between\nlocations incurs a one-slot travel delay; and arrivals are independent\nBernoulli processes. We formulate a discounted-cost Markov decision process and\npropose Exhaustive-Serve-Longest (ESL), a simple real-time policy that serves\nexhaustively when the current location is nonempty and, when idle, switches to\na longest unoccupied nonempty location, and we prove the optimality of this\npolicy. As baselines, we tune a fixed-dwell cyclic policy via a discrete-time\ndelay expression and implement a first-come-first-serve policy. Across\nserver-to-location ratios and loads, ESL consistently yields lower discounted\nholding cost and smaller mean queue lengths, with action-time fractions showing\nmore serving and restrained switching. Its simplicity and robustness make ESL a\npractical default for real-time multi-robot scheduling systems.", "AI": {"tldr": "\u63d0\u51faESL\u7b56\u7565\u7528\u4e8e\u591a\u673a\u5668\u4eba\u591a\u961f\u5217\u7cfb\u7edf\u7684\u5728\u7ebf\u4efb\u52a1\u5206\u914d\uff0c\u8bc1\u660e\u5176\u6700\u4f18\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u670d\u52a1\u5668-\u4f4d\u7f6e\u6bd4\u548c\u8d1f\u8f7d\u4e0b\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u591a\u961f\u5217\u7cfb\u7edf\u4e2d\u5b58\u5728\u968f\u673a\u5230\u8fbe\u548c\u5207\u6362\u5ef6\u8fdf\u7684\u5728\u7ebf\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u7b80\u5355\u5b9e\u7528\u7684\u5b9e\u65f6\u8c03\u5ea6\u7b56\u7565\u3002", "method": "\u63d0\u51faESL\u7b56\u7565\uff1a\u5f53\u524d\u4f4d\u7f6e\u975e\u7a7a\u65f6\u5f7b\u5e95\u670d\u52a1\uff0c\u7a7a\u95f2\u65f6\u5207\u6362\u5230\u6700\u957f\u672a\u88ab\u5360\u7528\u7684\u975e\u7a7a\u4f4d\u7f6e\u3002\u4e0e\u56fa\u5b9a\u9a7b\u7559\u5faa\u73af\u7b56\u7565\u548c\u5148\u5230\u5148\u670d\u52a1\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "ESL\u7b56\u7565\u5728\u6240\u6709\u670d\u52a1\u5668-\u4f4d\u7f6e\u6bd4\u548c\u8d1f\u8f7d\u4e0b\u90fd\u4ea7\u751f\u66f4\u4f4e\u7684\u6298\u6263\u6301\u6709\u6210\u672c\u548c\u66f4\u5c0f\u7684\u5e73\u5747\u961f\u5217\u957f\u5ea6\uff0c\u884c\u52a8\u65f6\u95f4\u5206\u914d\u663e\u793a\u66f4\u591a\u670d\u52a1\u548c\u53d7\u9650\u7684\u5207\u6362\u3002", "conclusion": "ESL\u7b56\u7565\u7684\u7b80\u5355\u6027\u548c\u9c81\u68d2\u6027\u4f7f\u5176\u6210\u4e3a\u5b9e\u65f6\u591a\u673a\u5668\u4eba\u8c03\u5ea6\u7cfb\u7edf\u7684\u5b9e\u7528\u9ed8\u8ba4\u9009\u62e9\u3002"}}
{"id": "2509.25663", "categories": ["cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.25663", "abs": "https://arxiv.org/abs/2509.25663", "authors": ["Nathaniel Hanson", "Benjamin Pyatski", "Samuel Hibbard", "Gary Lvov", "Oscar De La Garza", "Charles DiMarzio", "Kristen L. Dorsey", "Ta\u015fk\u0131n Pad\u0131r"], "title": "Field Calibration of Hyperspectral Cameras for Terrain Inference", "comment": "Accepted to IEEE Robotics & Automation Letters", "summary": "Intra-class terrain differences such as water content directly influence a\nvehicle's ability to traverse terrain, yet RGB vision systems may fail to\ndistinguish these properties. Evaluating a terrain's spectral content beyond\nred-green-blue wavelengths to the near infrared spectrum provides useful\ninformation for intra-class identification. However, accurate analysis of this\nspectral information is highly dependent on ambient illumination. We\ndemonstrate a system architecture to collect and register multi-wavelength,\nhyperspectral images from a mobile robot and describe an approach to\nreflectance calibrate cameras under varying illumination conditions. To\nshowcase the practical applications of our system, HYPER DRIVE, we demonstrate\nthe ability to calculate vegetative health indices and soil moisture content\nfrom a mobile robot platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aHYPER DRIVE\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u6536\u96c6\u548c\u914d\u51c6\u591a\u6ce2\u957f\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u5e76\u5f00\u53d1\u4e86\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u8fdb\u884c\u53cd\u5c04\u7387\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u4ee5\u8bc6\u522b\u7c7b\u5185\u5730\u5f62\u5dee\u5f02\u5982\u571f\u58e4\u6c34\u5206\u542b\u91cf\u3002", "motivation": "RGB\u89c6\u89c9\u7cfb\u7edf\u53ef\u80fd\u65e0\u6cd5\u533a\u5206\u5f71\u54cd\u8f66\u8f86\u901a\u8fc7\u80fd\u529b\u7684\u7c7b\u5185\u5730\u5f62\u5dee\u5f02\uff08\u5982\u6c34\u5206\u542b\u91cf\uff09\uff0c\u800c\u8fd1\u7ea2\u5916\u5149\u8c31\u80fd\u63d0\u4f9b\u6709\u7528\u7684\u7c7b\u5185\u8bc6\u522b\u4fe1\u606f\uff0c\u4f46\u51c6\u786e\u5206\u6790\u4f9d\u8d56\u4e8e\u73af\u5883\u5149\u7167\u6761\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86\u6536\u96c6\u548c\u914d\u51c6\u79fb\u52a8\u673a\u5668\u4eba\u591a\u6ce2\u957f\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u63cf\u8ff0\u4e86\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u5bf9\u76f8\u673a\u8fdb\u884c\u53cd\u5c04\u7387\u6821\u51c6\u7684\u65b9\u6cd5\u3002", "result": "\u5c55\u793a\u4e86\u7cfb\u7edf\u80fd\u591f\u4ece\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u8ba1\u7b97\u690d\u88ab\u5065\u5eb7\u6307\u6570\u548c\u571f\u58e4\u6c34\u5206\u542b\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "HYPER DRIVE\u7cfb\u7edf\u901a\u8fc7\u9ad8\u5149\u8c31\u6210\u50cf\u548c\u53cd\u5c04\u7387\u6821\u51c6\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u7c7b\u5185\u5730\u5f62\u5dee\u5f02\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5730\u5f62\u611f\u77e5\u63d0\u4f9b\u4e86\u8d85\u8d8aRGB\u89c6\u89c9\u7684\u80fd\u529b\u3002"}}
{"id": "2509.25681", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25681", "abs": "https://arxiv.org/abs/2509.25681", "authors": ["Junjie Wen", "Minjie Zhu", "Jiaming Liu", "Zhiyuan Liu", "Yicun Yang", "Linfeng Zhang", "Shanghang Zhang", "Yichen Zhu", "Yi Xu"], "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought", "comment": "technique report", "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.", "AI": {"tldr": "dVLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u6574\u5408\u89c6\u89c9\u611f\u77e5\u3001\u8bed\u8a00\u63a8\u7406\u548c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u9700\u8981\u7edf\u4e00\u89c6\u89c9\u611f\u77e5\u3001\u8bed\u8a00\u7406\u89e3\u548c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4ee5\u589e\u5f3a\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u5e76\u63d0\u5347\u5bf9\u65b0\u6307\u4ee4\u548c\u7269\u4f53\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6269\u6563\u76ee\u6807\u8054\u5408\u4f18\u5316\u611f\u77e5\u3001\u8bed\u8a00\u7406\u89e3\u548c\u52a8\u4f5c\uff0c\u5f15\u5165\u524d\u7f00\u6ce8\u610f\u529b\u63a9\u7801\u548cKV\u7f13\u5b58\u4e24\u79cd\u52a0\u901f\u7b56\u7565\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523096.4%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u79bb\u6563\u548c\u8fde\u7eed\u52a8\u4f5c\u7b56\u7565\uff1b\u5728\u771f\u5b9eFranka\u673a\u5668\u4eba\u4e0a\u6210\u529f\u5b8c\u6210\u5305\u62ec\u9700\u8981\u591a\u6b65\u89c4\u5212\u7684\u590d\u6742\u5206\u62e3\u4efb\u52a1\u5728\u5185\u7684\u591a\u6837\u5316\u4efb\u52a1\u3002", "conclusion": "\u7edf\u4e00\u7684\u6269\u6563\u6846\u67b6\u4e3a\u5b9e\u7528\u9ad8\u6027\u80fd\u7684VLA\u673a\u5668\u4eba\u6280\u672f\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.25685", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25685", "abs": "https://arxiv.org/abs/2509.25685", "authors": ["Amelie Minji Kim", "Anqi Wu", "Ye Zhao"], "title": "Hierarchical Diffusion Motion Planning with Task-Conditioned Uncertainty-Aware Priors", "comment": null, "summary": "We propose a novel hierarchical diffusion planner that embeds task and motion\nstructure directly in the noise model. Unlike standard diffusion-based planners\nthat use zero-mean, isotropic Gaussian noise, we employ a family of\ntask-conditioned structured Gaussians whose means and covariances are derived\nfrom Gaussian Process Motion Planning (GPMP): sparse, task-centric key states\nor their associated timings (or both) are treated as noisy observations to\nproduce a prior instance. We first generalize the standard diffusion process to\nbiased, non-isotropic corruption with closed-form forward and posterior\nexpressions. Building on this, our hierarchy separates prior instantiation from\ntrajectory denoising: the upper level instantiates a task-conditioned\nstructured Gaussian (mean and covariance), and the lower level denoises the\nfull trajectory under that fixed prior. Experiments on Maze2D goal-reaching and\nKUKA block stacking show improved success rates, smoother trajectories, and\nstronger task alignment compared to isotropic baselines. Ablation studies\nindicate that explicitly structuring the corruption process offers benefits\nbeyond simply conditioning the neural network. Overall, our method concentrates\nprobability mass of prior near feasible, smooth, and semantically meaningful\ntrajectories while maintaining tractability. Our project page is available at\nhttps://hta-diffusion.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u6269\u6563\u89c4\u5212\u5668\uff0c\u5c06\u4efb\u52a1\u548c\u8fd0\u52a8\u7ed3\u6784\u76f4\u63a5\u5d4c\u5165\u5230\u566a\u58f0\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u4efb\u52a1\u6761\u4ef6\u5316\u7684\u7ed3\u6784\u5316\u9ad8\u65af\u566a\u58f0\u66ff\u4ee3\u6807\u51c6\u5404\u5411\u540c\u6027\u9ad8\u65af\u566a\u58f0\uff0c\u63d0\u9ad8\u4e86\u89c4\u5212\u7684\u6210\u529f\u7387\u548c\u8f68\u8ff9\u8d28\u91cf\u3002", "motivation": "\u6807\u51c6\u6269\u6563\u89c4\u5212\u5668\u4f7f\u7528\u96f6\u5747\u503c\u5404\u5411\u540c\u6027\u9ad8\u65af\u566a\u58f0\uff0c\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u548c\u8fd0\u52a8\u7ed3\u6784\u7684\u663e\u5f0f\u5efa\u6a21\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u566a\u58f0\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u4efb\u52a1\u8bed\u4e49\u548c\u8fd0\u52a8\u7ea6\u675f\u3002", "method": "\u5c06\u7a00\u758f\u7684\u4efb\u52a1\u4e2d\u5fc3\u5173\u952e\u72b6\u6001\u6216\u5176\u65f6\u95f4\u4f5c\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u8fd0\u52a8\u89c4\u5212\u7684\u566a\u58f0\u89c2\u6d4b\uff0c\u751f\u6210\u5148\u9a8c\u5b9e\u4f8b\u3002\u91c7\u7528\u5206\u5c42\u7ed3\u6784\uff1a\u4e0a\u5c42\u5b9e\u4f8b\u5316\u4efb\u52a1\u6761\u4ef6\u5316\u7ed3\u6784\u5316\u9ad8\u65af\uff0c\u4e0b\u5c42\u5728\u8be5\u56fa\u5b9a\u5148\u9a8c\u4e0b\u5bf9\u5b8c\u6574\u8f68\u8ff9\u8fdb\u884c\u53bb\u566a\u3002", "result": "\u5728Maze2D\u76ee\u6807\u5230\u8fbe\u548cKUKA\u79ef\u6728\u5806\u53e0\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u5404\u5411\u540c\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u66f4\u5e73\u6ed1\u7684\u8f68\u8ff9\u548c\u66f4\u5f3a\u7684\u4efb\u52a1\u5bf9\u9f50\u6027\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u7ed3\u6784\u5316\u566a\u58f0\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u8d85\u8d8a\u7b80\u5355\u795e\u7ecf\u7f51\u7edc\u6761\u4ef6\u5316\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u5148\u9a8c\u6982\u7387\u8d28\u91cf\u96c6\u4e2d\u5728\u53ef\u884c\u3001\u5e73\u6ed1\u4e14\u8bed\u4e49\u6709\u610f\u4e49\u7684\u8f68\u8ff9\u9644\u8fd1\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u5904\u7406\u6027\uff0c\u4e3a\u6269\u6563\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u7ed3\u6784\u5316\u566a\u58f0\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2509.25687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25687", "abs": "https://arxiv.org/abs/2509.25687", "authors": ["Xinda Xue", "Junjun Hu", "Minghua Luo", "Xie Shichao", "Jintao Chen", "Zixun Xie", "Quan Kuichen", "Guo Wei", "Mu Xu", "Zedong Chu"], "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation", "comment": null, "summary": "Embodied navigation presents a core challenge for intelligent robots,\nrequiring the comprehension of visual environments, natural language\ninstructions, and autonomous exploration. Existing models often fall short in\noffering a unified solution across diverse navigation paradigms, resulting in\nlow success rates and limited generalization. We introduce OmniNav, a unified\nframework addressing instruct-goal, object-goal, point-goal navigation, and\nfrontier-based exploration within a single architecture. Our approach features\na lightweight, low-latency policy that accurately predicts continuous-space\nwaypoints (coordinates and orientations). This policy surpasses action-chunk\nmethods in precision and supports real-world deployment at control frequencies\nup to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast\nmodule generates waypoints using short-horizon visual context and subtasks,\nwhile a slow module performs deliberative planning with long-horizon\nobservations and candidate frontiers to select subsequent subgoals and\nsubtasks. This collaboration enhances path efficiency and maintains trajectory\ncoherence, particularly in exploration and memory-intensive scenarios.\nCrucially, we identify that the primary bottleneck isn't merely navigation\npolicy learning, but a robust understanding of general instructions and\nobjects. To boost generalization, OmniNav integrates large-scale,\ngeneral-purpose training datasets, including those for image captioning and\nvisual recognition, into a joint multi-task regimen. This significantly\nimproves success rates and robustness. Extensive experiments confirm OmniNav's\nstate-of-the-art performance across various navigation benchmarks, with\nreal-world deployment further validating its efficacy. OmniNav provides\npractical insights for embodied navigation, charting a scalable path towards\nversatile, highly generalizable robotic intelligence.", "AI": {"tldr": "OmniNav\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6\uff0c\u652f\u6301\u6307\u4ee4\u5bfc\u822a\u3001\u76ee\u6807\u5bfc\u822a\u3001\u70b9\u5bfc\u822a\u548c\u524d\u6cbf\u63a2\u7d22\u7b49\u591a\u79cd\u5bfc\u822a\u8303\u5f0f\uff0c\u91c7\u7528\u5feb\u6162\u6a21\u5757\u534f\u4f5c\u67b6\u6784\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bad\u7ec3\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u6a21\u578b\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u6837\u5316\u5bfc\u822a\u573a\u666f\u4e2d\u6210\u529f\u7387\u4f4e\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u79cd\u5bfc\u822a\u4efb\u52a1\u7684\u901a\u7528\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5feb\u6162\u7cfb\u7edf\u8bbe\u8ba1\uff1a\u5feb\u901f\u6a21\u5757\u57fa\u4e8e\u77ed\u65f6\u89c6\u89c9\u4e0a\u4e0b\u6587\u751f\u6210\u8fde\u7eed\u7a7a\u95f4\u8def\u5f84\u70b9\uff0c\u6162\u901f\u6a21\u5757\u8fdb\u884c\u957f\u65f6\u89c4\u5212\u9009\u62e9\u5b50\u76ee\u6807\uff1b\u96c6\u6210\u5927\u89c4\u6a21\u901a\u7528\u6570\u636e\u96c6\u8fdb\u884c\u591a\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u63015Hz\u5b9e\u65f6\u63a7\u5236\uff0c\u8def\u5f84\u6548\u7387\u548c\u8f68\u8ff9\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\uff0c\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u5927\u5e45\u6539\u5584\u3002", "conclusion": "OmniNav\u4e3a\u5177\u8eab\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u6cdb\u5316\u7684\u673a\u5668\u4eba\u667a\u80fd\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u901a\u7528\u5bfc\u822a\u7cfb\u7edf\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.25718", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25718", "abs": "https://arxiv.org/abs/2509.25718", "authors": ["Si-Cheng Wang", "Tian-Yu Xiang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Ao-Qun Jin", "Zeng-Guang Hou"], "title": "VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning", "comment": null, "summary": "Reinforcement learning (RL) is a promising avenue for post-training\nvision-language-action (VLA) models, but practical deployment is hindered by\nsparse rewards and unstable training. This work mitigates these challenges by\nintroducing an action chunk based on proximal policy optimization (PPO) with\nbehavior cloning using self-collected demonstrations. Aggregating consecutive\nactions into chunks improves the temporal consistency of the policy and the\ndensity of informative feedback. In addition, an auxiliary behavior cloning\nloss is applied with a dynamically updated demonstration buffer that\ncontinually collects high-quality task trials during training. The relative\nweight between the action-chunked PPO objective and the self behavior clone\nauxiliary loss is adapted online to stabilize the post-training process.\nExperiments on the MetaWorld benchmark indicate improved performance over\nsupervised fine-tuning, achieving a high success rate (0.93) and few steps to\nsuccess (42.17). These results demonstrate the viability of RL for VLA\npost-training and help lay the groundwork for downstream VLA applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u4f5c\u5206\u5757\u7684PPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u6765\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u8bad\u7ec3\u540e\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8bad\u7ec3\u540e\u4f18\u5316\u4e2d\u9762\u4e34\u7a00\u758f\u5956\u52b1\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u6311\u6218\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u52a8\u4f5c\u5206\u5757\u7684PPO\u65b9\u6cd5\uff0c\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u635f\u5931\uff0c\u4f7f\u7528\u81ea\u6536\u96c6\u7684\u6f14\u793a\u6570\u636e\uff0c\u5e76\u52a8\u6001\u8c03\u6574PPO\u76ee\u6807\u4e0e\u884c\u4e3a\u514b\u9686\u635f\u5931\u7684\u6743\u91cd\u6bd4\u4f8b\u3002", "result": "\u5728MetaWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u8fbe\u523093%\u7684\u6210\u529f\u7387\u548c\u5e73\u574742.17\u6b65\u5b8c\u6210\u4efb\u52a1\u7684\u6548\u7387\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728VLA\u8bad\u7ec3\u540e\u4f18\u5316\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e0b\u6e38VLA\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.25746", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25746", "abs": "https://arxiv.org/abs/2509.25746", "authors": ["Shuaijun Wang", "Haoran Zhou", "Diyun Xiang", "Yangwei You"], "title": "TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses", "comment": "9 pages, 9 figures", "summary": "Despite progress in both traditional dexterous grasping pipelines and recent\nVision-Language-Action (VLA) approaches, the grasp execution stage remains\nprone to pose inaccuracies, especially in long-horizon tasks, which undermines\noverall performance. To address this \"last-mile\" challenge, we propose\nTacRefineNet, a tactile-only framework that achieves fine in-hand pose\nrefinement of known objects in arbitrary target poses using multi-finger\nfingertip sensing. Our method iteratively adjusts the end-effector pose based\non tactile feedback, aligning the object to the desired configuration. We\ndesign a multi-branch policy network that fuses tactile inputs from multiple\nfingers along with proprioception to predict precise control updates. To train\nthis policy, we combine large-scale simulated data from a physics-based tactile\nmodel in MuJoCo with real-world data collected from a physical system.\nComparative experiments show that pretraining on simulated data and fine-tuning\nwith a small amount of real data significantly improves performance over\nsimulation-only training. Extensive real-world experiments validate the\neffectiveness of the method, achieving millimeter-level grasp accuracy using\nonly tactile input. To our knowledge, this is the first method to enable\narbitrary in-hand pose refinement via multi-finger tactile sensing alone.\nProject website is available at https://sites.google.com/view/tacrefinenet", "AI": {"tldr": "\u63d0\u51faTacRefineNet\uff0c\u4e00\u79cd\u4ec5\u4f7f\u7528\u89e6\u89c9\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6307\u5c16\u89e6\u89c9\u4f20\u611f\u5b9e\u73b0\u5df2\u77e5\u7269\u4f53\u5728\u4efb\u610f\u76ee\u6807\u59ff\u6001\u4e0b\u7684\u7cbe\u7ec6\u624b\u5185\u59ff\u6001\u7cbe\u70bc\u3002", "motivation": "\u4f20\u7edf\u7075\u5de7\u6293\u53d6\u6d41\u7a0b\u548cVLA\u65b9\u6cd5\u5728\u6267\u884c\u9636\u6bb5\u5b58\u5728\u59ff\u6001\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\uff0c\u8fd9\u5f71\u54cd\u4e86\u6574\u4f53\u6027\u80fd\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e2a\"\u6700\u540e\u4e00\u82f1\u91cc\"\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u591a\u5206\u652f\u7b56\u7565\u7f51\u7edc\uff0c\u878d\u5408\u591a\u6307\u89e6\u89c9\u8f93\u5165\u548c\u672c\u4f53\u611f\u77e5\u6765\u9884\u6d4b\u7cbe\u786e\u63a7\u5236\u66f4\u65b0\u3002\u7ed3\u5408MuJoCo\u4e2d\u57fa\u4e8e\u7269\u7406\u7684\u89e6\u89c9\u6a21\u578b\u7684\u5927\u89c4\u6a21\u6a21\u62df\u6570\u636e\u548c\u7269\u7406\u7cfb\u7edf\u6536\u96c6\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u5e76\u7528\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u6a21\u62df\u8bad\u7ec3\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ec5\u4f7f\u7528\u89e6\u89c9\u8f93\u5165\u5b9e\u73b0\u4e86\u6beb\u7c73\u7ea7\u6293\u53d6\u7cbe\u5ea6\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4ec5\u901a\u8fc7\u591a\u6307\u89e6\u89c9\u4f20\u611f\u5b9e\u73b0\u4efb\u610f\u624b\u5185\u59ff\u6001\u7cbe\u70bc\u7684\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u6293\u53d6\u6267\u884c\u9636\u6bb5\u7684\u59ff\u6001\u4e0d\u51c6\u786e\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25747", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.25747", "abs": "https://arxiv.org/abs/2509.25747", "authors": ["Jialei Huang", "Zhaoheng Yin", "Yingdong Hu", "Shuo Wang", "Xingyu Lin", "Yang Gao"], "title": "Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real", "comment": "10 pages, 6 figures", "summary": "Sim-to-real transfer remains a fundamental challenge in robot manipulation\ndue to the entanglement of perception and control in end-to-end learning. We\npresent a decoupled framework that learns each component where it is most\nreliable: control policies are trained in simulation with privileged state to\nmaster spatial layouts and manipulation dynamics, while perception is adapted\nonly at deployment to bridge real observations to the frozen control policy.\nOur key insight is that control strategies and action patterns are universal\nacross environments and can be learned in simulation through systematic\nrandomization, while perception is inherently domain-specific and must be\nlearned where visual observations are authentic. Unlike existing end-to-end\napproaches that require extensive real-world data, our method achieves strong\nperformance with only 10-20 real demonstrations by reducing the complex\nsim-to-real problem to a structured perception alignment task. We validate our\napproach on tabletop manipulation tasks, demonstrating superior data efficiency\nand out-of-distribution generalization compared to end-to-end baselines. The\nlearned policies successfully handle object positions and scales beyond the\ntraining distribution, confirming that decoupling perception from control\nfundamentally improves sim-to-real transfer.", "AI": {"tldr": "\u63d0\u51fa\u89e3\u8026\u611f\u77e5\u4e0e\u63a7\u5236\u7684sim-to-real\u6846\u67b6\uff0c\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u5728\u771f\u5b9e\u90e8\u7f72\u65f6\u4ec5\u9002\u914d\u611f\u77e5\u6a21\u5757\uff0c\u5927\u5e45\u51cf\u5c11\u771f\u5b9e\u6570\u636e\u9700\u6c42", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u5b66\u4e60\u4e2d\u611f\u77e5\u4e0e\u63a7\u5236\u7ea0\u7f20\u5bfc\u81f4\u7684sim-to-real\u8fc1\u79fb\u96be\u9898", "method": "\u5728\u4eff\u771f\u4e2d\u7528\u7279\u6743\u72b6\u6001\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\u638c\u63e1\u7a7a\u95f4\u5e03\u5c40\u548c\u64cd\u4f5c\u52a8\u6001\uff0c\u90e8\u7f72\u65f6\u4ec5\u9002\u914d\u611f\u77e5\u6a21\u5757\u5c06\u771f\u5b9e\u89c2\u6d4b\u6620\u5c04\u5230\u51bb\u7ed3\u7684\u63a7\u5236\u7b56\u7565", "result": "\u4ec5\u970010-20\u4e2a\u771f\u5b9e\u6f14\u793a\u5c31\u80fd\u5b9e\u73b0\u5f3a\u6027\u80fd\uff0c\u5728\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6570\u636e\u6548\u7387\u548c\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b", "conclusion": "\u89e3\u8026\u611f\u77e5\u4e0e\u63a7\u5236\u4ece\u6839\u672c\u4e0a\u6539\u5584\u4e86sim-to-real\u8fc1\u79fb\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u80fd\u5904\u7406\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u7684\u7269\u4f53\u4f4d\u7f6e\u548c\u5c3a\u5ea6"}}
{"id": "2509.25756", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25756", "abs": "https://arxiv.org/abs/2509.25756", "authors": ["Yixian Zhang", "Shu'ang Yu", "Tonghe Zhang", "Mo Guang", "Haojia Hui", "Kaiwen Long", "Yu Wang", "Chao Yu", "Wenbo Ding"], "title": "SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling", "comment": null, "summary": "Training expressive flow-based policies with off-policy reinforcement\nlearning is notoriously unstable due to gradient pathologies in the multi-step\naction sampling process. We trace this instability to a fundamental connection:\nthe flow rollout is algebraically equivalent to a residual recurrent\ncomputation, making it susceptible to the same vanishing and exploding\ngradients as RNNs. To address this, we reparameterize the velocity network\nusing principles from modern sequential models, introducing two stable\narchitectures: Flow-G, which incorporates a gated velocity, and Flow-T, which\nutilizes a decoded velocity. We then develop a practical SAC-based algorithm,\nenabled by a noise-augmented rollout, that facilitates direct end-to-end\ntraining of these policies. Our approach supports both from-scratch and\noffline-to-online learning and achieves state-of-the-art performance on\ncontinuous control and robotic manipulation benchmarks, eliminating the need\nfor common workarounds like policy distillation or surrogate objectives.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u7a33\u5b9a\u7684\u6d41\u7b56\u7565\u67b6\u6784\uff08Flow-G\u548cFlow-T\uff09\u6765\u89e3\u51b3\u57fa\u4e8e\u6d41\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u901f\u5ea6\u7f51\u7edc\u5e76\u5f00\u53d1SAC\u7b97\u6cd5\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "motivation": "\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u5728\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u6e90\u4e8e\u6d41\u5c55\u5f00\u8fc7\u7a0b\u4e0e\u6b8b\u5dee\u5faa\u73af\u8ba1\u7b97\u7684\u4ee3\u6570\u7b49\u4ef7\u6027\uff0c\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u548c\u7206\u70b8\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u53c2\u6570\u5316\u901f\u5ea6\u7f51\u7edc\uff0c\u5f15\u5165\u4e24\u79cd\u7a33\u5b9a\u67b6\u6784\uff1aFlow-G\uff08\u95e8\u63a7\u901f\u5ea6\uff09\u548cFlow-T\uff08\u89e3\u7801\u901f\u5ea6\uff09\uff0c\u5f00\u53d1\u57fa\u4e8eSAC\u7684\u7b97\u6cd5\uff0c\u4f7f\u7528\u566a\u58f0\u589e\u5f3a\u7684\u5c55\u5f00\u8fc7\u7a0b\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u4ece\u5934\u5f00\u59cb\u5b66\u4e60\u548c\u79bb\u7ebf\u5230\u5728\u7ebf\u5b66\u4e60\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u7b56\u7565\u84b8\u998f\u6216\u66ff\u4ee3\u76ee\u6807\u7b49\u5e38\u89c1\u53d8\u901a\u65b9\u6cd5\u7684\u9700\u6c42\uff0c\u4e3a\u6d41\u7b56\u7565\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2509.25822", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25822", "abs": "https://arxiv.org/abs/2509.25822", "authors": ["Jing Wang", "Weiting Peng", "Jing Tang", "Zeyu Gong", "Xihua Wang", "Bo Tao", "Li Cheng"], "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies", "comment": "42 pages, 17 figures, 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action--Guided Diffusion Policy (DP--AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP--AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle--consistent contrastive\nloss that organizes the gradient flow of the noise predictor into a coherent\nperception--action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP--AG\nsignificantly outperforms state--of--the--art methods across simulation\nbenchmarks and real-world UR5 manipulation tasks. As a result, our DP--AG\noffers a promising step toward bridging biological adaptability and artificial\npolicy learning.", "AI": {"tldr": "\u63d0\u51faDP-AG\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u4f5c\u5f15\u5bfc\u7684\u6269\u6563\u7b56\u7565\u7edf\u4e00\u611f\u77e5\u4e0e\u884c\u52a8\u8868\u793a\u5b66\u4e60\uff0c\u5229\u7528\u53d8\u5206\u63a8\u7406\u548cSDE\u5efa\u6a21\u611f\u77e5-\u884c\u52a8\u52a8\u6001\u4ea4\u4e92\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5c06\u611f\u77e5\u548c\u884c\u52a8\u89e3\u8026\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u884c\u4e3a\u4e2d\u611f\u77e5\u8868\u793a\u4e0e\u884c\u52a8\u6267\u884c\u4e4b\u95f4\u7684\u56e0\u679c\u4e92\u60e0\u5173\u7cfb\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u9002\u5e94\u884c\u4e3a\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u63a8\u7406\u5c06\u89c2\u6d4b\u7f16\u7801\u4e3a\u9ad8\u65af\u540e\u9a8c\uff0c\u901a\u8fc7\u52a8\u4f5c\u5f15\u5bfc\u7684SDE\u6f14\u5316\u6f5c\u53d8\u91cf\uff0c\u5176\u4e2d\u6269\u6563\u7b56\u7565\u7684\u566a\u58f0\u9884\u6d4b\u7684VJP\u4f5c\u4e3a\u7ed3\u6784\u5316\u968f\u673a\u529b\u9a71\u52a8\u6f5c\u53d8\u91cf\u66f4\u65b0\uff0c\u5e76\u5f15\u5165\u5faa\u73af\u4e00\u81f4\u6027\u5bf9\u6bd4\u635f\u5931\u4fc3\u8fdb\u611f\u77e5-\u884c\u52a8\u53cc\u5411\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9eUR5\u673a\u68b0\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cDP-AG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DP-AG\u4e3a\u8fde\u63a5\u751f\u7269\u9002\u5e94\u6027\u4e0e\u4eba\u5de5\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u4e00\u6b65\uff0c\u901a\u8fc7\u7edf\u4e00\u8868\u793a\u5b66\u4e60\u5efa\u6a21\u611f\u77e5\u4e0e\u884c\u52a8\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\u3002"}}
{"id": "2509.25852", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25852", "abs": "https://arxiv.org/abs/2509.25852", "authors": ["Zitong Bo", "Yue Hu", "Jinming Ma", "Mingliang Zhou", "Junhui Yin", "Yachen Kang", "Yuqi Liu", "Tong Wu", "Diyun Xiang", "Hao Chen"], "title": "Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation", "comment": null, "summary": "Enabling robots to execute long-horizon manipulation tasks from free-form\nlanguage instructions remains a fundamental challenge in embodied AI. While\nvision-language models (VLMs) have shown promise as high-level planners, their\ndeployment in the real world is hindered by two gaps: (i) the scarcity of\nlarge-scale, sequential manipulation data that couples natural language with\nmulti-step action plans, and (ii) the absence of dense, interpretable rewards\nfor fine-tuning VLMs on planning objectives. To address these issues, we\npropose REVER, a framework that empowers VLMs to generate and validate\nlong-horizon manipulation plans from natural language instructions in\nreal-world scenarios. Under REVER we train and release RoboFarseer, a VLM\nincentivized to emit chain-of-thought that perform temporal and spatial\nreasoning, ensuring physically plausible and logically coherent plans. To\nobtain training data, we leverage the Universal Manipulation Interface\nframework to capture hardware-agnostic demonstrations of atomic skills. An\nautomated annotation engine converts each demonstration into\nvision-instruction-plan triplet. We introduce a verifiable reward that scores\nthe generated plan by its ordered bipartite matching overlap with the\nground-truth skill sequence. At run time, the fine-tuned VLM functions both as\na planner and as a monitor, verifying step-wise completion. RoboFarseer matches\nor exceeds the performance of proprietary models that are orders of magnitude\nlarger, while on open-ended planning it surpasses the best baseline by more\nthan 40%. In real-world, long-horizon tasks, the complete system boosts overall\nsuccess by roughly 60% compared with the same low-level controller without the\nplanner. We will open-source both the dataset and the trained model upon\npublication.", "AI": {"tldr": "REVER\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3RoboFarseer\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6267\u884c\u81ea\u7136\u8bed\u8a00\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u5e8f\u5217\u64cd\u4f5c\u6570\u636e\u548c\u5bc6\u96c6\u53ef\u89e3\u91ca\u5956\u52b1\u3002\u8be5\u6a21\u578b\u80fd\u751f\u6210\u548c\u9a8c\u8bc1\u7269\u7406\u5408\u7406\u3001\u903b\u8f91\u8fde\u8d2f\u7684\u64cd\u4f5c\u8ba1\u5212\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6267\u884c\u81ea\u7136\u8bed\u8a00\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u4e24\u4e2a\u4e3b\u8981\u969c\u788d\uff1a(i)\u7f3a\u4e4f\u5927\u89c4\u6a21\u5e8f\u5217\u64cd\u4f5c\u6570\u636e\uff0c(ii)\u7f3a\u4e4f\u7528\u4e8e\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bc6\u96c6\u53ef\u89e3\u91ca\u5956\u52b1\u3002", "method": "\u63d0\u51faREVER\u6846\u67b6\uff0c\u8bad\u7ec3RoboFarseer\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u65f6\u7a7a\u63a8\u7406\uff0c\u751f\u6210\u94fe\u5f0f\u601d\u7ef4\u3002\u5229\u7528\u901a\u7528\u64cd\u4f5c\u754c\u9762\u6846\u67b6\u6536\u96c6\u786c\u4ef6\u65e0\u5173\u7684\u539f\u5b50\u6280\u80fd\u6f14\u793a\uff0c\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u5f15\u64ce\u751f\u6210\u89c6\u89c9-\u6307\u4ee4-\u8ba1\u5212\u4e09\u5143\u7ec4\uff0c\u5e76\u5f15\u5165\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\u8bc4\u4f30\u751f\u6210\u8ba1\u5212\u7684\u8d28\u91cf\u3002", "result": "RoboFarseer\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u5927\u6570\u91cf\u7ea7\u7684\u4e13\u6709\u6a21\u578b\uff0c\u5728\u5f00\u653e\u5f0f\u89c4\u5212\u4e0a\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534740%\u4ee5\u4e0a\u3002\u5728\u771f\u5b9e\u4e16\u754c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\uff0c\u5b8c\u6574\u7cfb\u7edf\u6bd4\u65e0\u89c4\u5212\u5668\u7684\u4f4e\u7ea7\u63a7\u5236\u5668\u63d0\u5347\u7ea660%\u7684\u6574\u4f53\u6210\u529f\u7387\u3002", "conclusion": "REVER\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u81ea\u7136\u8bed\u8a00\u64cd\u4f5c\u89c4\u5212\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u5408\u7406\u4e14\u903b\u8f91\u8fde\u8d2f\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u8ba1\u5212\u751f\u6210\u4e0e\u9a8c\u8bc1\u3002"}}
{"id": "2509.25945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25945", "abs": "https://arxiv.org/abs/2509.25945", "authors": ["Valentin Yuryev", "Max Polzin", "Josie Hughes"], "title": "State Estimation for Compliant and Morphologically Adaptive Robots", "comment": "8 pages, 10 figures, 1 table, submitted to ICRA 2026", "summary": "Locomotion robots with active or passive compliance can show robustness to\nuncertain scenarios, which can be promising for agricultural, research and\nenvironmental industries. However, state estimation for these robots is\nchallenging due to the lack of rigid-body assumptions and kinematic changes\nfrom morphing. We propose a method to estimate typical rigid-body states\nalongside compliance-related states, such as soft robot shape in different\nmorphologies and locomotion modes. Our neural network-based state estimator\nuses a history of states and a mechanism to directly influence unreliable\nsensors. We test our framework on the GOAT platform, a robot capable of passive\ncompliance and active morphing for extreme outdoor terrain. The network is\ntrained on motion capture data in a novel compliance-centric frame that\naccounts for morphing-related states. Our method predicts shape-related\nmeasurements within 4.2% of the robot's size, velocities within 6.3% and 2.4%\nof the top linear and angular speeds, respectively, and orientation within 1.5\ndegrees. We also demonstrate a 300% increase in travel range during a motor\nmalfunction when using our estimator for closed-loop autonomous outdoor\noperation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5177\u6709\u4e3b\u52a8\u6216\u88ab\u52a8\u67d4\u987a\u6027\u7684\u79fb\u52a8\u673a\u5668\u4eba\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f30\u8ba1\u5178\u578b\u521a\u4f53\u72b6\u6001\u548c\u67d4\u987a\u76f8\u5173\u72b6\u6001\uff0c\u5982\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5f62\u6001\u548c\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u5f62\u72b6\u3002", "motivation": "\u67d4\u987a\u6027\u673a\u5668\u4eba\u7f3a\u4e4f\u521a\u4f53\u5047\u8bbe\u4e14\u5f62\u6001\u53d8\u5316\u5bfc\u81f4\u8fd0\u52a8\u5b66\u6539\u53d8\uff0c\u4f7f\u5f97\u72b6\u6001\u4f30\u8ba1\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u8fd9\u7c7b\u673a\u5668\u4eba\u5728\u519c\u4e1a\u3001\u7814\u7a76\u548c\u73af\u5883\u4ea7\u4e1a\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u72b6\u6001\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u72b6\u6001\u5386\u53f2\u8bb0\u5f55\u548c\u76f4\u63a5\u5f71\u54cd\u4e0d\u53ef\u9760\u4f20\u611f\u5668\u7684\u673a\u5236\uff0c\u5728GOAT\u5e73\u53f0\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u5728\u8003\u8651\u5f62\u6001\u76f8\u5173\u72b6\u6001\u7684\u65b0\u578b\u67d4\u987a\u4e2d\u5fc3\u5750\u6807\u7cfb\u4e2d\u4f7f\u7528\u8fd0\u52a8\u6355\u6349\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u9884\u6d4b\u5f62\u72b6\u76f8\u5173\u6d4b\u91cf\u503c\u5728\u673a\u5668\u4eba\u5c3a\u5bf8\u76844.2%\u4ee5\u5185\uff0c\u7ebf\u901f\u5ea6\u548c\u89d2\u901f\u5ea6\u5206\u522b\u5728\u524d\u6700\u9ad8\u901f\u5ea6\u76846.3%\u548c2.4%\u4ee5\u5185\uff0c\u65b9\u5411\u57281.5\u5ea6\u4ee5\u5185\u3002\u5728\u7535\u673a\u6545\u969c\u65f6\u4f7f\u7528\u8be5\u4f30\u8ba1\u5668\u8fdb\u884c\u95ed\u73af\u81ea\u4e3b\u6237\u5916\u64cd\u4f5c\uff0c\u884c\u7a0b\u8303\u56f4\u589e\u52a0\u4e86300%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u67d4\u987a\u673a\u5668\u4eba\u7684\u72b6\u6001\uff0c\u5728\u6781\u7aef\u6237\u5916\u5730\u5f62\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u4f20\u611f\u5668\u4e0d\u53ef\u9760\u6216\u53d1\u751f\u6545\u969c\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.25951", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25951", "abs": "https://arxiv.org/abs/2509.25951", "authors": ["ChunPing Lam", "Xiangjia Chen", "Chenming Wu", "Hao Chen", "Binzhi Sun", "Guoxin Fang", "Charlie C. L. Wang", "Chengkai Dai", "Yeung Yam"], "title": "Towards Intuitive Human-Robot Interaction through Embodied Gesture-Driven Control with Woven Tactile Skins", "comment": null, "summary": "This paper presents a novel human-robot interaction (HRI) framework that\nenables intuitive gesture-driven control through a capacitance-based woven\ntactile skin. Unlike conventional interfaces that rely on panels or handheld\ndevices, the woven tactile skin integrates seamlessly with curved robot\nsurfaces, enabling embodied interaction and narrowing the gap between human\nintent and robot response. Its woven design combines fabric-like flexibility\nwith structural stability and dense multi-channel sensing through the\ninterlaced conductive threads. Building on this capability, we define a\ngesture-action mapping of 14 single- and multi-touch gestures that cover\nrepresentative robot commands, including task-space motion and auxiliary\nfunctions. A lightweight convolution-transformer model designed for gesture\nrecognition in real time achieves an accuracy of near-100%, outperforming prior\nbaseline approaches. Experiments on robot arm tasks, including pick-and-place\nand pouring, demonstrate that our system reduces task completion time by up to\n57% compared with keyboard panels and teach pendants. Overall, our proposed\nframework demonstrates a practical pathway toward more natural and efficient\nembodied HRI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7535\u5bb9\u5f0f\u7f16\u7ec7\u89e6\u89c9\u76ae\u80a4\u7684\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u52bf\u63a7\u5236\u5b9e\u73b0\u76f4\u89c2\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u76f8\u6bd4\u4f20\u7edf\u9762\u677f\u548c\u793a\u6559\u5668\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u6700\u591a\u51cf\u5c1157%\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u63a7\u5236\u754c\u9762\u4f9d\u8d56\u9762\u677f\u6216\u624b\u6301\u8bbe\u5907\uff0c\u7f3a\u4e4f\u76f4\u89c2\u6027\u548c\u81ea\u7136\u4ea4\u4e92\u3002\u9700\u8981\u4e00\u79cd\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u673a\u5668\u4eba\u66f2\u9762\u3001\u7f29\u5c0f\u4eba\u7c7b\u610f\u56fe\u4e0e\u673a\u5668\u4eba\u54cd\u5e94\u5dee\u8ddd\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u5f00\u53d1\u7f16\u7ec7\u89e6\u89c9\u76ae\u80a4\uff0c\u7ed3\u5408\u7ec7\u7269\u67d4\u6027\u548c\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u4ea4\u7ec7\u5bfc\u7535\u7ebf\u7a0b\u5b9e\u73b0\u5bc6\u96c6\u591a\u901a\u9053\u4f20\u611f\u3002\u5b9a\u4e49\u4e8614\u79cd\u5355\u70b9\u548c\u591a\u70b9\u89e6\u63a7\u624b\u52bf\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef-Transformer\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u624b\u52bf\u8bc6\u522b\u3002", "result": "\u624b\u52bf\u8bc6\u522b\u51c6\u786e\u7387\u63a5\u8fd1100%\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u673a\u5668\u4eba\u81c2\u4efb\u52a1\uff08\u5982\u62fe\u653e\u548c\u503e\u5012\uff09\u4e2d\uff0c\u76f8\u6bd4\u952e\u76d8\u9762\u677f\u548c\u793a\u6559\u5668\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u6700\u591a\u51cf\u5c1157%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u73b0\u66f4\u81ea\u7136\u9ad8\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\uff0c\u901a\u8fc7\u7f16\u7ec7\u89e6\u89c9\u76ae\u80a4\u548c\u624b\u52bf\u63a7\u5236\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u4f53\u9a8c\u3002"}}
{"id": "2509.25966", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25966", "abs": "https://arxiv.org/abs/2509.25966", "authors": ["Peilong Han", "Fan Jia", "Min Zhang", "Yutao Qiu", "Hongyao Tang", "Yan Zheng", "Tiancai Wang", "Jianye Hao"], "title": "MUVLA: Learning to Explore Object Navigation via Map Understanding", "comment": null, "summary": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action\nmodel tailored for object navigation. It leverages semantic map abstractions to\nunify and structure historical information, encoding spatial context in a\ncompact and consistent form. MUVLA takes the current and history observations,\nas well as the semantic map, as inputs and predicts the action sequence based\non the description of goal object. Furthermore, it amplifies supervision\nthrough reward-guided return modeling based on dense short-horizon progress\nsignals, enabling the model to develop a detailed understanding of action value\nfor reward maximization. MUVLA employs a three-stage training pipeline:\nlearning map-level spatial understanding, imitating behaviors from\nmixed-quality demonstrations, and reward amplification. This strategy allows\nMUVLA to unify diverse demonstrations into a robust spatial representation and\ngenerate more rational exploration strategies. Experiments on HM3D and Gibson\nbenchmarks demonstrate that MUVLA achieves great generalization and learns\neffective exploration behaviors even from low-quality or partially successful\ntrajectories.", "AI": {"tldr": "MUVLA\u662f\u4e00\u4e2a\u7528\u4e8e\u7269\u4f53\u5bfc\u822a\u7684\u5730\u56fe\u7406\u89e3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u62bd\u8c61\u7edf\u4e00\u5386\u53f2\u4fe1\u606f\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5728HM3D\u548cGibson\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7269\u4f53\u5bfc\u822a\u4efb\u52a1\u4e2d\u5386\u53f2\u4fe1\u606f\u6574\u5408\u548c\u63a2\u7d22\u7b56\u7565\u4f18\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7edf\u4e00\u7ed3\u6784\u5316\u5386\u53f2\u4fe1\u606f\u5e76\u5b66\u4e60\u6709\u6548\u63a2\u7d22\u884c\u4e3a\u7684\u6a21\u578b\u3002", "method": "\u5229\u7528\u8bed\u4e49\u5730\u56fe\u62bd\u8c61\u7f16\u7801\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u5b66\u4e60\u5730\u56fe\u7ea7\u7a7a\u95f4\u7406\u89e3\u3001\u4ece\u6df7\u5408\u8d28\u91cf\u6f14\u793a\u4e2d\u6a21\u4eff\u884c\u4e3a\u3001\u5956\u52b1\u653e\u5927\uff0c\u901a\u8fc7\u5956\u52b1\u5f15\u5bfc\u7684\u56de\u62a5\u5efa\u6a21\u589e\u5f3a\u76d1\u7763\u3002", "result": "\u5728HM3D\u548cGibson\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMUVLA\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u4ece\u4f4e\u8d28\u91cf\u6216\u90e8\u5206\u6210\u529f\u7684\u8f68\u8ff9\u4e2d\u5b66\u4e60\u6709\u6548\u7684\u63a2\u7d22\u884c\u4e3a\u3002", "conclusion": "MUVLA\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u62bd\u8c61\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u7edf\u4e00\u4e86\u591a\u6837\u5316\u6f14\u793a\u5e76\u751f\u6210\u4e86\u66f4\u5408\u7406\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u5728\u7269\u4f53\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.25984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25984", "abs": "https://arxiv.org/abs/2509.25984", "authors": ["Shengpeng Wang", "Yulong Xie", "Qing Liao", "Wei Wang"], "title": "S$^3$E: Self-Supervised State Estimation for Radar-Inertial System", "comment": null, "summary": "Millimeter-wave radar for state estimation is gaining significant attention\nfor its affordability and reliability in harsh conditions. Existing\nlocalization solutions typically rely on post-processed radar point clouds as\nlandmark points. Nonetheless, the inherent sparsity of radar point clouds,\nghost points from multi-path effects, and limited angle resolution in\nsingle-chirp radar severely degrade state estimation performance. To address\nthese issues, we propose S$^3$E, a \\textbf{S}elf-\\textbf{S}upervised\n\\textbf{S}tate \\textbf{E}stimator that employs more richly informative radar\nsignal spectra to bypass sparse points and fuses complementary inertial\ninformation to achieve accurate localization. S$^3$E fully explores the\nassociation between \\textit{exteroceptive} radar and \\textit{proprioceptive}\ninertial sensor to achieve complementary benefits. To deal with limited angle\nresolution, we introduce a novel cross-fusion technique that enhances spatial\nstructure information by exploiting subtle rotational shift correlations across\nheterogeneous data. The experimental results demonstrate our method achieves\nrobust and accurate performance without relying on localization ground truth\nsupervision. To the best of our knowledge, this is the first attempt to achieve\nstate estimation by fusing radar spectra and inertial data in a complementary\nself-supervised manner.", "AI": {"tldr": "S^3E\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u72b6\u6001\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u878d\u5408\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4fe1\u53f7\u9891\u8c31\u548c\u60ef\u6027\u6570\u636e\uff0c\u5728\u65e0\u9700\u5730\u9762\u771f\u503c\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u96f7\u8fbe\u70b9\u4e91\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u70b9\u4e91\u7a00\u758f\u3001\u591a\u5f84\u6548\u5e94\u4ea7\u751f\u7684\u9b3c\u70b9\u4ee5\u53ca\u5355\u5541\u557e\u96f7\u8fbe\u89d2\u5ea6\u5206\u8fa8\u7387\u6709\u9650\u7b49\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "method": "\u63d0\u51faS^3E\u65b9\u6cd5\uff0c\u5229\u7528\u66f4\u4e30\u5bcc\u7684\u96f7\u8fbe\u4fe1\u53f7\u9891\u8c31\u7ed5\u8fc7\u7a00\u758f\u70b9\u4e91\uff0c\u878d\u5408\u4e92\u8865\u7684\u60ef\u6027\u4fe1\u606f\uff1b\u91c7\u7528\u8de8\u878d\u5408\u6280\u672f\u901a\u8fc7\u5229\u7528\u5f02\u6784\u6570\u636e\u95f4\u7684\u7ec6\u5fae\u65cb\u8f6c\u504f\u79fb\u76f8\u5173\u6027\u6765\u589e\u5f3a\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u5b9a\u4f4d\u5730\u9762\u771f\u503c\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u51c6\u786e\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u4ee5\u4e92\u8865\u7684\u81ea\u76d1\u7763\u65b9\u5f0f\u878d\u5408\u96f7\u8fbe\u9891\u8c31\u548c\u60ef\u6027\u6570\u636e\u6765\u5b9e\u73b0\u72b6\u6001\u4f30\u8ba1\u7684\u5de5\u4f5c\u3002"}}
{"id": "2509.25986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25986", "abs": "https://arxiv.org/abs/2509.25986", "authors": ["Elisabetta Zibetti", "Sureya Waheed Palmer", "Rebecca Stower", "Salvatore M Anzalone"], "title": "Emotionally Expressive Robots: Implications for Children's Behavior toward Robot", "comment": null, "summary": "The growing development of robots with artificial emotional expressiveness\nraises important questions about their persuasive potential in children's\nbehavior. While research highlights the pragmatic value of emotional\nexpressiveness in human social communication, the extent to which robotic\nexpressiveness can or should influence empathic responses in children is\ngrounds for debate. In a pilot study with 22 children (aged 7-11) we begin to\nexplore the ways in which different levels of embodied expressiveness (body\nonly, face only, body and face) of two basic emotions (happiness and sadness)\ndisplayed by an anthropomorphic robot (QTRobot) might modify children's\nbehavior in a child-robot cooperative turn-taking game. We observed that\nchildren aligned their behavior to the robot's inferred emotional state.\nHowever, higher levels of expressiveness did not result in increased alignment.\nThe preliminary results reported here provide a starting point for reflecting\non robotic expressiveness and its role in shaping children's social-emotional\nbehavior toward robots as social peers in the near future.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\u5bf9\u513f\u7ae5\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u513f\u7ae5\u4f1a\u8c03\u6574\u884c\u4e3a\u4ee5\u9002\u5e94\u673a\u5668\u4eba\u60c5\u7eea\u72b6\u6001\uff0c\u4f46\u66f4\u9ad8\u7684\u8868\u8fbe\u6c34\u5e73\u5e76\u672a\u589e\u5f3a\u8fd9\u79cd\u8c03\u6574\u6548\u5e94\u3002", "motivation": "\u968f\u7740\u5177\u6709\u4eba\u5de5\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u7684\u673a\u5668\u4eba\u53d1\u5c55\uff0c\u9700\u8981\u7814\u7a76\u5176\u60c5\u611f\u8868\u8fbe\u5bf9\u513f\u7ae5\u5171\u60c5\u53cd\u5e94\u7684\u5f71\u54cd\u7a0b\u5ea6\u548c\u9002\u5f53\u6027\u3002", "method": "\u572822\u540d7-11\u5c81\u513f\u7ae5\u53c2\u4e0e\u7684\u8bd5\u70b9\u7814\u7a76\u4e2d\uff0c\u4f7f\u7528QTRobot\u5728\u5408\u4f5c\u8f6e\u6362\u6e38\u620f\u4e2d\u5c55\u793a\u4e0d\u540c\u8868\u8fbe\u6c34\u5e73\uff08\u4ec5\u8eab\u4f53\u3001\u4ec5\u9762\u90e8\u3001\u8eab\u4f53\u548c\u9762\u90e8\uff09\u7684\u57fa\u672c\u60c5\u7eea\uff08\u5feb\u4e50\u548c\u60b2\u4f24\uff09\u3002", "result": "\u513f\u7ae5\u4f1a\u8c03\u6574\u884c\u4e3a\u4ee5\u9002\u5e94\u673a\u5668\u4eba\u63a8\u65ad\u7684\u60c5\u7eea\u72b6\u6001\uff0c\u4f46\u66f4\u9ad8\u7684\u8868\u8fbe\u6c34\u5e73\u5e76\u672a\u5bfc\u81f4\u66f4\u5f3a\u7684\u884c\u4e3a\u8c03\u6574\u3002", "conclusion": "\u8fd9\u4e9b\u521d\u6b65\u7ed3\u679c\u4e3a\u601d\u8003\u673a\u5668\u4eba\u8868\u8fbe\u6027\u53ca\u5176\u5728\u672a\u6765\u5851\u9020\u513f\u7ae5\u5bf9\u673a\u5668\u4eba\u4f5c\u4e3a\u793e\u4ea4\u4f19\u4f34\u7684\u793e\u4f1a\u60c5\u611f\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u8d77\u70b9\u3002"}}
{"id": "2509.25999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25999", "abs": "https://arxiv.org/abs/2509.25999", "authors": ["Yann de Mont-Marin", "Louis Montaut", "Jean Ponce", "Martial Hebert", "Justin Carpentier"], "title": "On the Conic Complementarity of Planar Contacts", "comment": null, "summary": "We present a unifying theoretical result that connects two foundational\nprinciples in robotics: the Signorini law for point contacts, which underpins\nmany simulation methods for preventing object interpenetration, and the center\nof pressure (also known as the zero-moment point), a key concept used in, for\ninstance, optimization-based locomotion control. Our contribution is the planar\nSignorini condition, a conic complementarity formulation that models general\nplanar contacts between rigid bodies. We prove that this formulation is\nequivalent to enforcing the punctual Signorini law across an entire contact\nsurface, thereby bridging the gap between discrete and continuous contact\nmodels. A geometric interpretation reveals that the framework naturally\ncaptures three physical regimes -sticking, separating, and tilting-within a\nunified complementarity structure. This leads to a principled extension of the\nclassical center of pressure, which we refer to as the extended center of\npressure. By establishing this connection, our work provides a mathematically\nconsistent and computationally tractable foundation for handling planar\ncontacts, with implications for both the accurate simulation of contact\ndynamics and the design of advanced control and optimization algorithms in\nlocomotion and manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5e73\u9762Signorini\u6761\u4ef6\uff0c\u5c06\u70b9\u63a5\u89e6\u7684Signorini\u5b9a\u5f8b\u4e0e\u4e2d\u5fc3\u538b\u529b\u6982\u5ff5\u7edf\u4e00\u8d77\u6765\uff0c\u4e3a\u521a\u4f53\u5e73\u9762\u63a5\u89e6\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u8fde\u63a5\u673a\u5668\u4eba\u5b66\u4e2d\u4e24\u4e2a\u57fa\u7840\u539f\u7406\uff1a\u9632\u6b62\u7269\u4f53\u7a7f\u900f\u7684Signorini\u5b9a\u5f8b\u548c\u7528\u4e8e\u4f18\u5316\u63a7\u5236\u4e2d\u7684\u4e2d\u5fc3\u538b\u529b\u6982\u5ff5\uff0c\u5f25\u6563\u79bb\u6563\u4e0e\u8fde\u7eed\u63a5\u89e6\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u5e73\u9762Signorini\u6761\u4ef6\u7684\u9525\u4e92\u8865\u516c\u5f0f\uff0c\u8bc1\u660e\u5176\u4e0e\u5728\u6574\u4e2a\u63a5\u89e6\u9762\u4e0a\u6267\u884c\u70b9Signorini\u5b9a\u5f8b\u7b49\u4ef7\uff0c\u5e76\u7ed9\u51fa\u51e0\u4f55\u89e3\u91ca\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u4e92\u8865\u7ed3\u6784\uff0c\u81ea\u7136\u6355\u6349\u7c98\u9644\u3001\u5206\u79bb\u548c\u503e\u659c\u4e09\u79cd\u7269\u7406\u72b6\u6001\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u4e2d\u5fc3\u538b\u529b\u6982\u5ff5\u3002", "conclusion": "\u4e3a\u5904\u7406\u5e73\u9762\u63a5\u89e6\u63d0\u4f9b\u4e86\u6570\u5b66\u4e00\u81f4\u4e14\u8ba1\u7b97\u53ef\u884c\u7684\u57fa\u7840\uff0c\u5bf9\u63a5\u89e6\u52a8\u529b\u5b66\u7cbe\u786e\u4eff\u771f\u548c\u9ad8\u7ea7\u63a7\u5236\u4f18\u5316\u7b97\u6cd5\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.26050", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26050", "abs": "https://arxiv.org/abs/2509.26050", "authors": ["Shaoli Hu", "Shizhe Zhao", "Zhongqiang Ren"], "title": "Conflict-Based Search and Prioritized Planning for Multi-Agent Path Finding Among Movable Obstacles", "comment": null, "summary": "This paper investigates Multi-Agent Path Finding Among Movable Obstacles\n(M-PAMO), which seeks collision-free paths for multiple agents from their start\nto goal locations among static and movable obstacles. M-PAMO arises in\nlogistics and warehouses where mobile robots are among unexpected movable\nobjects. Although Multi-Agent Path Finding (MAPF) and single-agent Path\nplanning Among Movable Obstacles (PAMO) were both studied, M-PAMO remains\nunder-explored. Movable obstacles lead to new fundamental challenges as the\nstate space, which includes both agents and movable obstacles, grows\nexponentially with respect to the number of agents and movable obstacles. In\nparticular, movable obstacles often closely couple agents together spatially\nand temporally. This paper makes a first attempt to adapt and fuse the popular\nConflict-Based Search (CBS) and Prioritized Planning (PP) for MAPF, and a\nrecent single-agent PAMO planner called PAMO*, together to address M-PAMO. We\ncompare their performance with up to 20 agents and hundreds of movable\nobstacles, and show the pros and cons of these approaches.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u53ef\u79fb\u52a8\u969c\u788d\u7269\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u878d\u5408CBS\u3001PP\u548cPAMO*\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u5728\u7269\u6d41\u548c\u4ed3\u5e93\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u5728\u610f\u5916\u53ef\u79fb\u52a8\u7269\u4f53\u4e2d\u5bfc\u822a\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u800c\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u548c\u5355\u667a\u80fd\u4f53\u53ef\u79fb\u52a8\u969c\u788d\u7269\u89c4\u5212\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u590d\u6742\u95ee\u9898\u3002", "method": "\u9996\u6b21\u5c1d\u8bd5\u5c06\u51b2\u7a81\u641c\u7d22\u3001\u4f18\u5148\u7ea7\u89c4\u5212\u548cPAMO*\u5355\u667a\u80fd\u4f53\u89c4\u5212\u5668\u8fdb\u884c\u878d\u5408\uff0c\u4ee5\u5e94\u5bf9\u5305\u542b\u53ef\u79fb\u52a8\u969c\u788d\u7269\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6311\u6218\u3002", "result": "\u5728\u6700\u591a20\u4e2a\u667a\u80fd\u4f53\u548c\u6570\u767e\u4e2a\u53ef\u79fb\u52a8\u969c\u788d\u7269\u7684\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86\u5404\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u89e3\u51b3M-PAMO\u95ee\u9898\u7684\u521d\u6b65\u65b9\u6cd5\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u5728\u53ef\u79fb\u52a8\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26082", "abs": "https://arxiv.org/abs/2509.26082", "authors": ["Tianyi Jin", "Melya Boukheddimi", "Rohit Kumar", "Gabriele Fadini", "Frank Kirchner"], "title": "Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance", "comment": null, "summary": "Humanoid robots have seen significant advancements in both design and\ncontrol, with a growing emphasis on integrating these aspects to enhance\noverall performance. Traditionally, robot design has followed a sequential\nprocess, where control algorithms are developed after the hardware is\nfinalized. However, this can be myopic and prevent robots to fully exploit\ntheir hardware capabilities. Recent approaches advocate for co-design,\noptimizing both design and control in parallel to maximize robotic\ncapabilities. This paper presents the Evolutionary Continuous Adaptive RL-based\nCo-Design (EA-CoRL) framework, which combines reinforcement learning (RL) with\nevolutionary strategies to enable continuous adaptation of the control policy\nto the hardware. EA-CoRL comprises two key components: Design Evolution, which\nexplores the hardware choices using an evolutionary algorithm to identify\nefficient configurations, and Policy Continuous Adaptation, which fine-tunes a\ntask-specific control policy across evolving designs to maximize performance\nrewards. We evaluate EA-CoRL by co-designing the actuators (gear ratios) and\ncontrol policy of the RH5 humanoid for a highly dynamic chin-up task,\npreviously unfeasible due to actuator limitations. Comparative results against\nstate-of-the-art RL-based co-design methods show that EA-CoRL achieves higher\nfitness score and broader design space exploration, highlighting the critical\nrole of continuous policy adaptation in robot co-design.", "AI": {"tldr": "EA-CoRL\u6846\u67b6\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u8fdb\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u786c\u4ef6\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\u7684\u534f\u540c\u4f18\u5316\uff0c\u5728RH5\u4eba\u5f62\u673a\u5668\u4eba\u7684\u9ad8\u52a8\u6001\u5f15\u4f53\u5411\u4e0a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8bbe\u8ba1\u91c7\u7528\u987a\u5e8f\u6d41\u7a0b\uff0c\u786c\u4ef6\u8bbe\u8ba1\u5b8c\u6210\u540e\u624d\u5f00\u53d1\u63a7\u5236\u7b97\u6cd5\uff0c\u8fd9\u79cd\u5206\u79bb\u65b9\u6cd5\u9650\u5236\u4e86\u673a\u5668\u4eba\u5145\u5206\u53d1\u6325\u786c\u4ef6\u6f5c\u529b\u3002\u9700\u8981\u5e76\u884c\u4f18\u5316\u8bbe\u8ba1\u548c\u63a7\u5236\u6765\u6700\u5927\u5316\u673a\u5668\u4eba\u80fd\u529b\u3002", "method": "EA-CoRL\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8bbe\u8ba1\u8fdb\u5316\uff08\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u63a2\u7d22\u786c\u4ef6\u914d\u7f6e\uff09\u548c\u7b56\u7565\u6301\u7eed\u9002\u5e94\uff08\u5728\u6f14\u5316\u8bbe\u8ba1\u4e2d\u5fae\u8c03\u4efb\u52a1\u7279\u5b9a\u7684\u63a7\u5236\u7b56\u7565\u4ee5\u6700\u5927\u5316\u6027\u80fd\u5956\u52b1\uff09\u3002", "result": "\u5728RH5\u4eba\u5f62\u673a\u5668\u4eba\u7684\u9ad8\u52a8\u6001\u5f15\u4f53\u5411\u4e0a\u4efb\u52a1\u4e2d\uff0cEA-CoRL\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8eRL\u7684\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u9002\u5e94\u5ea6\u5206\u6570\u548c\u66f4\u5e7f\u6cdb\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e4b\u524d\u56e0\u6267\u884c\u5668\u9650\u5236\u800c\u4e0d\u53ef\u884c\u7684\u4efb\u52a1\u3002", "conclusion": "EA-CoRL\u8bc1\u660e\u4e86\u6301\u7eed\u7b56\u7565\u9002\u5e94\u5728\u673a\u5668\u4eba\u534f\u540c\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u673a\u5668\u4eba\u786c\u4ef6\u548c\u63a7\u5236\u7cfb\u7edf\u7684\u96c6\u6210\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2509.26106", "categories": ["cs.RO", "68T40, 68T05", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.26106", "abs": "https://arxiv.org/abs/2509.26106", "authors": ["Nakhul Kalaivanan", "Senthil Arumugam Muthukumaraswamy", "Girish Balasubramanian"], "title": "Autonomous Multi-Robot Infrastructure for AI-Enabled Healthcare Delivery and Diagnostics", "comment": "11 pages, 5 figures, MSc dissertation submission draft, prepared for\n  conference/journal consideration", "summary": "This research presents a multi-robot system for inpatient care, designed\nusing swarm intelligence principles and incorporating wearable health sensors,\nRF-based communication, and AI-driven decision support. Within a simulated\nhospital environment, the system adopts a leader-follower swarm configuration\nto perform patient monitoring, medicine delivery, and emergency assistance. Due\nto ethical constraints, live patient trials were not conducted; instead,\nvalidation was carried out through controlled self-testing with wearable\nsensors. The Leader Robot acquires key physiological parameters, including\ntemperature, SpO2, heart rate, and fall detection, and coordinates other robots\nwhen required. The Assistant Robot patrols corridors for medicine delivery,\nwhile a robotic arm provides direct drug administration. The swarm-inspired\nleader-follower strategy enhanced communication reliability and ensured\ncontinuous monitoring, including automated email alerts to healthcare staff.\nThe system hardware was implemented using Arduino, Raspberry Pi, NRF24L01 RF\nmodules, and a HuskyLens AI camera. Experimental evaluation showed an overall\nsensor accuracy above 94%, a 92% task-level success rate, and a 96%\ncommunication reliability rate, demonstrating system robustness. Furthermore,\nthe AI-enabled decision support was able to provide early warnings of abnormal\nhealth conditions, highlighting the potential of the system as a cost-effective\nsolution for hospital automation and patient safety.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7fa4\u4f53\u667a\u80fd\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f4f\u9662\u62a4\u7406\uff0c\u901a\u8fc7\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u3001RF\u901a\u4fe1\u548cAI\u51b3\u7b56\u652f\u6301\u5b9e\u73b0\u60a3\u8005\u76d1\u6d4b\u3001\u836f\u7269\u914d\u9001\u548c\u7d27\u6025\u63f4\u52a9\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u63d0\u9ad8\u533b\u9662\u62a4\u7406\u6548\u7387\u3001\u964d\u4f4e\u4eba\u5de5\u6210\u672c\uff0c\u5e76\u786e\u4fdd\u60a3\u8005\u5b89\u5168\uff0c\u7279\u522b\u662f\u5728\u76d1\u6d4b\u5f02\u5e38\u5065\u5eb7\u72b6\u51b5\u548c\u63d0\u4f9b\u53ca\u65f6\u63f4\u52a9\u65b9\u9762\u3002", "method": "\u91c7\u7528\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7fa4\u4f53\u914d\u7f6e\uff0c\u4f7f\u7528Arduino\u3001Raspberry Pi\u3001NRF24L01 RF\u6a21\u5757\u548cHuskyLens AI\u76f8\u673a\u6784\u5efa\u786c\u4ef6\u7cfb\u7edf\uff0c\u7ed3\u5408\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6536\u96c6\u751f\u7406\u53c2\u6570\uff0c\u5e76\u901a\u8fc7AI\u51b3\u7b56\u652f\u6301\u534f\u8c03\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4f20\u611f\u5668\u51c6\u786e\u7387\u8d85\u8fc794%\uff0c\u4efb\u52a1\u6210\u529f\u738792%\uff0c\u901a\u4fe1\u53ef\u9760\u602796%\uff0cAI\u51b3\u7b56\u652f\u6301\u80fd\u63d0\u524d\u9884\u8b66\u5f02\u5e38\u5065\u5eb7\u72b6\u51b5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u4f5c\u4e3a\u533b\u9662\u81ea\u52a8\u5316\u548c\u60a3\u8005\u5b89\u5168\u4fdd\u969c\u7684\u6210\u672c\u6548\u76ca\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u7fa4\u4f53\u667a\u80fd\u7b56\u7565\u63d0\u5347\u4e86\u901a\u4fe1\u53ef\u9760\u6027\u548c\u6301\u7eed\u76d1\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.26121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26121", "abs": "https://arxiv.org/abs/2509.26121", "authors": ["Julian Valdez", "Ignacio Torroba", "John Folkesson", "Ivan Stenius"], "title": "Side Scan Sonar-based SLAM for Autonomous Algae Farm Monitoring", "comment": null, "summary": "The transition of seaweed farming to an alternative food source on an\nindustrial scale relies on automating its processes through smart farming,\nequivalent to land agriculture. Key to this process are autonomous underwater\nvehicles (AUVs) via their capacity to automate crop and structural inspections.\nHowever, the current bottleneck for their deployment is ensuring safe\nnavigation within farms, which requires an accurate, online estimate of the AUV\npose and map of the infrastructure. To enable this, we propose an efficient\nside scan sonar-based (SSS) simultaneous localization and mapping (SLAM)\nframework that exploits the geometry of kelp farms via modeling structural\nropes in the back-end as sequences of individual landmarks from each SSS ping\ndetection, instead of combining detections into elongated representations. Our\nmethod outperforms state of the art solutions in hardware in the loop (HIL)\nexperiments on a real AUV survey in a kelp farm. The framework and dataset can\nbe found at https://github.com/julRusVal/sss_farm_slam.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fa7\u626b\u58f0\u7eb3\u7684SLAM\u6846\u67b6\uff0c\u7528\u4e8e\u6d77\u85fb\u517b\u6b96\u573a\u4e2d\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u7684\u5b89\u5168\u5bfc\u822a\uff0c\u901a\u8fc7\u5c06\u7ed3\u6784\u7ef3\u7d22\u5efa\u6a21\u4e3a\u5355\u4e2a\u5730\u6807\u5e8f\u5217\u800c\u975e\u7ec4\u5408\u6210\u5ef6\u4f38\u8868\u793a\uff0c\u5728\u771f\u5b9e\u6d77\u85fb\u517b\u6b96\u573a\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6d77\u85fb\u517b\u6b96\u5411\u5de5\u4e1a\u5316\u89c4\u6a21\u8f6c\u578b\u9700\u8981\u81ea\u52a8\u5316\u667a\u80fd\u517b\u6b96\uff0c\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668(AUV)\u662f\u5b9e\u73b0\u4f5c\u7269\u548c\u7ed3\u6784\u68c0\u67e5\u81ea\u52a8\u5316\u7684\u5173\u952e\uff0c\u4f46\u76ee\u524d\u90e8\u7f72\u7684\u74f6\u9888\u662f\u786e\u4fdd\u5728\u517b\u6b96\u573a\u5185\u7684\u5b89\u5168\u5bfc\u822a\uff0c\u8fd9\u9700\u8981\u51c6\u786e\u7684\u5728\u7ebfAUV\u4f4d\u59ff\u548c\u57fa\u7840\u8bbe\u65bd\u5730\u56fe\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u9ad8\u6548\u7684\u4fa7\u626b\u58f0\u7eb3(SSS)\u540c\u65f6\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa(SLAM)\u6846\u67b6\uff0c\u5229\u7528\u6d77\u85fb\u517b\u6b96\u573a\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5728\u540e\u7aef\u5c06\u7ed3\u6784\u7ef3\u7d22\u5efa\u6a21\u4e3a\u6bcf\u4e2aSSS\u63a2\u6d4b\u7684\u5355\u4e2a\u5730\u6807\u5e8f\u5217\uff0c\u800c\u4e0d\u662f\u5c06\u63a2\u6d4b\u7ec4\u5408\u6210\u5ef6\u4f38\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u6d77\u85fb\u517b\u6b96\u573a\u7684\u786c\u4ef6\u5728\u73af(HIL)\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u6570\u636e\u96c6\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\uff0c\u4e3a\u6d77\u85fb\u517b\u6b96\u573a\u4e2dAUV\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26222", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26222", "abs": "https://arxiv.org/abs/2509.26222", "authors": ["Yizhe Liu", "Han Zhang"], "title": "Terrain-Awared LiDAR-Inertial Odometry for Legged-Wheel Robots Based on Radial Basis Function Approximation", "comment": null, "summary": "An accurate odometry is essential for legged-wheel robots operating in\nunstructured terrains such as bumpy roads and staircases. Existing methods\noften suffer from pose drift due to their ignorance of terrain geometry. We\npropose a terrain-awared LiDAR-Inertial odometry (LIO) framework that\napproximates the terrain using Radial Basis Functions (RBF) whose centers are\nadaptively selected and weights are recursively updated. The resulting smooth\nterrain manifold enables ``soft constraints\" that regularize the odometry\noptimization and mitigates the $z$-axis pose drift under abrupt elevation\nchanges during robot's maneuver. To ensure the LIO's real-time performance, we\nfurther evaluate the RBF-related terms and calculate the inverse of the sparse\nkernel matrix with GPU parallelization. Experiments on unstructured terrains\ndemonstrate that our method achieves higher localization accuracy than the\nstate-of-the-art baselines, especially in the scenarios that have continuous\nheight changes or sparse features when abrupt height changes occur.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8db3\u8f6e\u673a\u5668\u4eba\u7684\u5730\u5f62\u611f\u77e5LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u4f7f\u7528\u5f84\u5411\u57fa\u51fd\u6570\u81ea\u9002\u5e94\u5efa\u6a21\u5730\u5f62\u51e0\u4f55\uff0c\u901a\u8fc7\u8f6f\u7ea6\u675f\u7f13\u89e3z\u8f74\u59ff\u6001\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u5728GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\uff08\u5982\u98a0\u7c38\u9053\u8def\u548c\u697c\u68af\uff09\u4e2d\u8fd0\u884c\u65f6\uff0c\u7531\u4e8e\u5ffd\u7565\u5730\u5f62\u51e0\u4f55\u4fe1\u606f\uff0c\u5e38\u5e38\u51fa\u73b0\u59ff\u6001\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5f84\u5411\u57fa\u51fd\u6570(RBF)\u8fd1\u4f3c\u5730\u5f62\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u4e2d\u5fc3\u70b9\u5e76\u9012\u5f52\u66f4\u65b0\u6743\u91cd\uff0c\u521b\u5efa\u5e73\u6ed1\u5730\u5f62\u6d41\u5f62\uff0c\u5728\u91cc\u7a0b\u8ba1\u4f18\u5316\u4e2d\u65bd\u52a0\u8f6f\u7ea6\u675f\u6765\u7f13\u89e3z\u8f74\u6f02\u79fb\uff0c\u5e76\u901a\u8fc7GPU\u5e76\u884c\u5316\u786e\u4fdd\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u8fde\u7eed\u9ad8\u5ea6\u53d8\u5316\u6216\u7279\u5f81\u7a00\u758f\u4e14\u53d1\u751f\u6025\u5267\u9ad8\u5ea6\u53d8\u5316\u7684\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5730\u5f62\u611f\u77e5LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u5730\u5f62\u51e0\u4f55\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8db3\u8f6e\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.26236", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26236", "abs": "https://arxiv.org/abs/2509.26236", "authors": ["Benjamin A. Richardson", "Felix Gr\u00fcninger", "Lukas Mack", "Joerg Stueckler", "Katherine J. Kuchenbecker"], "title": "ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm", "comment": "Accepted at IEEE Humanoids 2025", "summary": "The rapid increase in the development of humanoid robots and customized\nmanufacturing solutions has brought dexterous manipulation to the forefront of\nmodern robotics. Over the past decade, several expensive dexterous hands have\ncome to market, but advances in hardware design, particularly in servo motors\nand 3D printing, have recently facilitated an explosion of cheaper open-source\nhands. Most hands are anthropomorphic to allow use of standard human tools, and\nattempts to increase dexterity often sacrifice anthropomorphism. We introduce\nthe open-source ISyHand (pronounced easy-hand), a highly dexterous, low-cost,\neasy-to-manufacture, on-joint servo-driven robot hand. Our hand uses\noff-the-shelf Dynamixel motors, fasteners, and 3D-printed parts, can be\nassembled within four hours, and has a total material cost of about 1,300 USD.\nThe ISyHands's unique articulated-palm design increases overall dexterity with\nonly a modest sacrifice in anthropomorphism. To demonstrate the utility of the\narticulated palm, we use reinforcement learning in simulation to train the hand\nto perform a classical in-hand manipulation task: cube reorientation. Our\nnovel, systematic experiments show that the simulated ISyHand outperforms the\ntwo most comparable hands in early training phases, that all three perform\nsimilarly well after policy convergence, and that the ISyHand significantly\noutperforms a fixed-palm version of its own design. Additionally, we deploy a\npolicy trained on cube reorientation on the real hand, demonstrating its\nability to perform real-world dexterous manipulation.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86ISyHand\u5f00\u6e90\u673a\u5668\u4eba\u624b\uff0c\u8fd9\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u4e8e\u5236\u9020\u3001\u9ad8\u7075\u5de7\u5ea6\u7684\u673a\u5668\u4eba\u624b\uff0c\u91c7\u7528\u5173\u8282\u5f0f\u624b\u638c\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u4eba\u5f62\u7279\u5f81\u7684\u540c\u65f6\u63d0\u5347\u4e86\u7075\u5de7\u6027\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u7acb\u65b9\u4f53\u91cd\u5b9a\u5411\u4efb\u52a1\uff0c\u5e76\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u4eba\u5f62\u673a\u5668\u4eba\u548c\u5b9a\u5236\u5236\u9020\u89e3\u51b3\u65b9\u6848\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7075\u5de7\u64cd\u4f5c\u6210\u4e3a\u73b0\u4ee3\u673a\u5668\u4eba\u5b66\u7684\u5173\u952e\u3002\u867d\u7136\u5e02\u573a\u4e0a\u5df2\u6709\u6602\u8d35\u7684\u7075\u5de7\u624b\uff0c\u4f46\u786c\u4ef6\u8bbe\u8ba1\u7684\u8fdb\u6b65\u4f7f\u5f97\u4f4e\u6210\u672c\u5f00\u6e90\u624b\u6210\u4e3a\u53ef\u80fd\u3002\u5927\u591a\u6570\u624b\u4e3a\u4e86\u4f7f\u7528\u6807\u51c6\u5de5\u5177\u800c\u91c7\u7528\u4eba\u5f62\u8bbe\u8ba1\uff0c\u4f46\u63d0\u5347\u7075\u5de7\u6027\u5f80\u5f80\u727a\u7272\u4eba\u5f62\u7279\u5f81\u3002", "method": "\u5f00\u53d1\u4e86ISyHand\u5f00\u6e90\u673a\u5668\u4eba\u624b\uff0c\u4f7f\u7528\u73b0\u6210\u7684Dynamixel\u7535\u673a\u3001\u7d27\u56fa\u4ef6\u548c3D\u6253\u5370\u90e8\u4ef6\uff0c\u53ef\u57284\u5c0f\u65f6\u5185\u7ec4\u88c5\u5b8c\u6210\u3002\u91c7\u7528\u72ec\u7279\u7684\u5173\u8282\u5f0f\u624b\u638c\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u4eba\u5f62\u7279\u5f81\u7684\u540c\u65f6\u63d0\u5347\u6574\u4f53\u7075\u5de7\u6027\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u7acb\u65b9\u4f53\u91cd\u5b9a\u5411\u4efb\u52a1\u3002", "result": "ISyHand\u603b\u6750\u6599\u6210\u672c\u7ea61300\u7f8e\u5143\u3002\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u4f18\u4e8e\u4e24\u4e2a\u6700\u53ef\u6bd4\u7684\u624b\uff0c\u6536\u655b\u540e\u4e09\u8005\u6027\u80fd\u76f8\u4f3c\uff0c\u4f46\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u624b\u638c\u7248\u672c\u3002\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u6210\u529f\u90e8\u7f72\u4e86\u7acb\u65b9\u4f53\u91cd\u5b9a\u5411\u7b56\u7565\u3002", "conclusion": "ISyHand\u8bc1\u660e\u4e86\u4f4e\u6210\u672c\u3001\u6613\u4e8e\u5236\u9020\u7684\u673a\u5668\u4eba\u624b\u80fd\u591f\u5b9e\u73b0\u9ad8\u7075\u5de7\u5ea6\u64cd\u4f5c\u3002\u5173\u8282\u5f0f\u624b\u638c\u8bbe\u8ba1\u5728\u4fdd\u6301\u4eba\u5f62\u7279\u5f81\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347\u4e86\u7075\u5de7\u6027\uff0c\u4e3a\u7075\u5de7\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u786c\u4ef6\u5e73\u53f0\u3002"}}
{"id": "2509.26308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26308", "abs": "https://arxiv.org/abs/2509.26308", "authors": ["Niklas Grambow", "Lisa-Marie Fenner", "Felipe Kempkes", "Philip Hotz", "Dingyuan Wan", "J\u00f6rg Kr\u00fcger", "Kevin Haninger"], "title": "Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation", "comment": null, "summary": "Out-of-distribution states in robot manipulation often lead to unpredictable\nrobot behavior or task failure, limiting success rates and increasing risk of\ndamage. Anomaly detection (AD) can identify deviations from expected patterns\nin data, which can be used to trigger failsafe behaviors and recovery\nstrategies. Prior work has applied data-driven AD to time series data in\nspecific robotic tasks, but its transferability across control strategies and\ntask types has not been shown. Leveraging time series data, such as\nforce/torque signals, allows to directly capture robot-environment\ninteractions, crucial for manipulation and online failure detection. Their\nbroad availability, high sampling rates, and low dimensionality enable high\ntemporal resolution and efficient processing. As robotic tasks can have widely\nsignal characteristics and requirements, AD methods which can be applied in the\nsame way to a wide range of tasks is needed, ideally with good data efficiency.\nWe examine three industrial robotic tasks, each presenting several anomalies.\nTest scenarios in robotic cabling, screwing, and sanding are built, and\nmultimodal time series data is gathered. Several autoencoder-based methods are\ncompared, evaluating generalization across tasks and control methods (diffusion\npolicy, position, and impedance control). This allows us to validate the\nintegration of AD in complex tasks involving tighter tolerances and variation\nfrom both the robot and its environment. Additionally, we evaluate data\nefficiency, detection latency, and task characteristics which support robust\ndetection. The results indicate reliable detection with AUROC exceeding 0.93 in\nfailures in the cabling and screwing task, such as incorrect or misaligned\nparts and obstructed targets. In the polishing task, only severe failures were\nreliably detected, while more subtle failure types remained undetected.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u63a7\u5236\u7b56\u7565\u548c\u4efb\u52a1\u7c7b\u578b\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5728\u7535\u7f06\u5e03\u7ebf\u548c\u62e7\u87ba\u4e1d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5206\u5e03\u5916\u72b6\u6001\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u884c\u4e3a\u6216\u4efb\u52a1\u5931\u8d25\uff0c\u9700\u8981\u5f02\u5e38\u68c0\u6d4b\u6765\u89e6\u53d1\u5b89\u5168\u673a\u5236\u548c\u6062\u590d\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u63a7\u5236\u7b56\u7565\u548c\u4efb\u52a1\u7c7b\u578b\u7684\u53ef\u8fc1\u79fb\u6027\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\u3002", "method": "\u6784\u5efa\u4e86\u4e09\u79cd\u5de5\u4e1a\u673a\u5668\u4eba\u4efb\u52a1\u573a\u666f\uff08\u7535\u7f06\u5e03\u7ebf\u3001\u62e7\u87ba\u4e1d\u3001\u6253\u78e8\uff09\uff0c\u6536\u96c6\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u5728\u6269\u6563\u7b56\u7565\u3001\u4f4d\u7f6e\u63a7\u5236\u548c\u963b\u6297\u63a7\u5236\u7b49\u4e0d\u540c\u63a7\u5236\u65b9\u6cd5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u7535\u7f06\u5e03\u7ebf\u548c\u62e7\u87ba\u4e1d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5f02\u5e38\u68c0\u6d4b\uff08AUROC\u8d85\u8fc70.93\uff09\uff0c\u80fd\u68c0\u6d4b\u96f6\u4ef6\u9519\u8bef\u3001\u9519\u4f4d\u548c\u76ee\u6807\u53d7\u963b\u7b49\u6545\u969c\u3002\u5728\u6253\u78e8\u4efb\u52a1\u4e2d\u4ec5\u80fd\u53ef\u9760\u68c0\u6d4b\u4e25\u91cd\u6545\u969c\uff0c\u8f83\u7ec6\u5fae\u7684\u6545\u969c\u7c7b\u578b\u672a\u88ab\u68c0\u6d4b\u5230\u3002", "conclusion": "\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5177\u6709\u8f83\u597d\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u7279\u522b\u662f\u5728\u7535\u7f06\u5e03\u7ebf\u548c\u62e7\u87ba\u4e1d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u68c0\u6d4b\u7ec6\u5fae\u6545\u969c\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2509.26324", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.26324", "abs": "https://arxiv.org/abs/2509.26324", "authors": ["Ruiyang Wang", "Haolun Tsu", "David Hunt", "Shaocheng Luo", "Jiwoo Kim", "Miroslav Pajic"], "title": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search", "comment": null, "summary": "Autonomous exploration and object search in unknown indoor environments\nremain challenging for multi-robot systems (MRS). Traditional approaches often\nrely on greedy frontier assignment strategies with limited inter-robot\ncoordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot\nCoordinated Exploration and Search), a novel framework that leverages Large\nLanguage Models (LLMs) for intelligent coordination of both homogeneous and\nheterogeneous robot teams tasked with efficient exploration and target object\nsearch. Our approach combines real-time LiDAR scan processing for frontier\ncluster extraction and doorway detection with multimodal LLM reasoning (e.g.,\nGPT-4o) to generate coordinated waypoint assignments based on shared\nenvironment maps and robot states. LLM-MCoX demonstrates superior performance\ncompared to existing methods, including greedy and Voronoi-based planners,\nachieving 22.7% faster exploration times and 50% improved search efficiency in\nlarge environments with 6 robots. Notably, LLM-MCoX enables natural\nlanguage-based object search capabilities, allowing human operators to provide\nhigh-level semantic guidance that traditional algorithms cannot interpret.", "AI": {"tldr": "LLM-MCoX\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u534f\u8c03\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u8fdb\u884c\u81ea\u4e3b\u63a2\u7d22\u548c\u5bf9\u8c61\u641c\u7d22\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u6548\u7387\u548c\u641c\u7d22\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u548c\u5bf9\u8c61\u641c\u7d22\u5b58\u5728\u534f\u8c03\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u534f\u8c03\u7b56\u7565\u6765\u63d0\u5347\u6548\u7387\u3002", "method": "\u7ed3\u5408\u5b9e\u65f6LiDAR\u626b\u63cf\u5904\u7406\uff08\u524d\u6cbf\u805a\u7c7b\u63d0\u53d6\u548c\u95e8\u9053\u68c0\u6d4b\uff09\u4e0e\u591a\u6a21\u6001LLM\u63a8\u7406\uff0c\u57fa\u4e8e\u5171\u4eab\u73af\u5883\u5730\u56fe\u548c\u673a\u5668\u4eba\u72b6\u6001\u751f\u6210\u534f\u8c03\u7684\u822a\u70b9\u5206\u914d\u3002", "result": "\u57286\u4e2a\u673a\u5668\u4eba\u7684\u5927\u578b\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4\u8d2a\u5a6a\u548cVoronoi\u89c4\u5212\u5668\uff0c\u63a2\u7d22\u65f6\u95f4\u52a0\u5feb22.7%\uff0c\u641c\u7d22\u6548\u7387\u63d0\u534750%\uff0c\u5e76\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5bf9\u8c61\u641c\u7d22\u3002", "conclusion": "LLM-MCoX\u901a\u8fc7LLM\u667a\u80fd\u534f\u8c03\u5b9e\u73b0\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9ad8\u6548\u63a2\u7d22\u548c\u8bed\u4e49\u641c\u7d22\u80fd\u529b\uff0c\u4e3a\u4f20\u7edf\u7b97\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26339", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26339", "abs": "https://arxiv.org/abs/2509.26339", "authors": ["Eric R. Damm", "Thomas M. Howard"], "title": "Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models", "comment": "Presented at the Robotics: Science and Systems (RSS) 2025 Workshop on\n  Resilient Off-road Autonomous Robotics (ROAR)", "summary": "Mobile ground robots lacking prior knowledge of an environment must rely on\nsensor data to develop a model of their surroundings. In these scenarios,\nconsistent identification of obstacles and terrain features can be difficult\ndue to noise and algorithmic shortcomings, which can make it difficult for\nmotion planning systems to generate safe motions. One particular difficulty to\novercome is when regions of the cost map switch between being marked as\nobstacles and free space through successive planning cycles. One potential\nsolution to this, which we refer to as Valid in Every Hypothesis (VEH), is for\nthe planning system to plan motions that are guaranteed to be safe through a\nhistory of world models. Another approach is to track a history of world\nmodels, and adjust node costs according to the potential penalty of needing to\nreroute around previously hazardous areas. This work discusses three major\niterations on this idea. The first iteration, called PEH, invokes a sub-search\nfor every node expansion that crosses through a divergence point in the world\nmodels. The second and third iterations, called GEH and GEGRH respectively,\ndefer the sub-search until after an edge expands into the goal region. GEGRH\nuses an additional step to revise the graph based on divergent nodes in each\nworld. Initial results showed that, although PEH and GEH find more optimistic\nsolutions than VEH, they are unable to generate solutions in less than\none-second, which exceeds our requirements for field deployment. Analysis of\nresults from a field experiment in an unstructured, off-road environment on a\nClearpath Robotics Warthog UGV indicate that GEGRH finds lower cost\ntrajectories and has faster average planning times than VEH. Compared to\nsingle-hypothesis (SH) search, where only the latest world model is considered,\nGEGRH generates more conservative plans with a small increase in average\nplanning time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u5047\u8bbe\u4e16\u754c\u6a21\u578b\u4e2d\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\u7684\u65b9\u6cd5GEGRH\uff0c\u901a\u8fc7\u8ddf\u8e2a\u5386\u53f2\u4e16\u754c\u6a21\u578b\u5e76\u8c03\u6574\u8282\u70b9\u6210\u672c\uff0c\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u751f\u6210\u66f4\u5b89\u5168\u7684\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u79fb\u52a8\u5730\u9762\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u9700\u8981\u4f9d\u8d56\u4f20\u611f\u5668\u6570\u636e\u5efa\u6a21\uff0c\u4f46\u7531\u4e8e\u566a\u58f0\u548c\u7b97\u6cd5\u9650\u5236\uff0c\u969c\u788d\u7269\u8bc6\u522b\u53ef\u80fd\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u56f0\u96be\u3002\u9700\u8981\u89e3\u51b3\u6210\u672c\u5730\u56fe\u4e2d\u533a\u57df\u5728\u969c\u788d\u7269\u548c\u81ea\u7531\u7a7a\u95f4\u4e4b\u95f4\u5207\u6362\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u8fed\u4ee3\u65b9\u6cd5\uff1aPEH\uff08\u4e3a\u6bcf\u4e2a\u8de8\u8d8a\u5206\u6b67\u70b9\u7684\u8282\u70b9\u6269\u5c55\u8c03\u7528\u5b50\u641c\u7d22\uff09\u3001GEH\u548cGEGRH\uff08\u5c06\u5b50\u641c\u7d22\u63a8\u8fdf\u5230\u8fb9\u6269\u5c55\u5230\u76ee\u6807\u533a\u57df\u540e\uff09\u3002GEGRH\u8fd8\u589e\u52a0\u4e86\u57fa\u4e8e\u6bcf\u4e2a\u4e16\u754c\u4e2d\u5206\u6b67\u8282\u70b9\u4fee\u8ba2\u56fe\u7684\u6b65\u9aa4\u3002", "result": "GEGRH\u5728\u975e\u7ed3\u6784\u5316\u8d8a\u91ce\u73af\u5883\u4e2d\u7684\u73b0\u573a\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4VEH\u65b9\u6cd5\uff0c\u5b83\u80fd\u627e\u5230\u6210\u672c\u66f4\u4f4e\u7684\u8f68\u8ff9\u4e14\u5e73\u5747\u89c4\u5212\u65f6\u95f4\u66f4\u5feb\u3002\u76f8\u6bd4\u5355\u5047\u8bbe\u641c\u7d22\uff0cGEGRH\u751f\u6210\u66f4\u4fdd\u5b88\u7684\u89c4\u5212\uff0c\u5e73\u5747\u89c4\u5212\u65f6\u95f4\u7565\u6709\u589e\u52a0\u3002", "conclusion": "GEGRH\u65b9\u6cd5\u5728\u591a\u5047\u8bbe\u4e16\u754c\u6a21\u578b\u4e0b\u80fd\u591f\u6709\u6548\u5e73\u8861\u89c4\u5212\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u63d0\u4f9b\u66f4\u5b89\u5168\u7684\u8fd0\u52a8\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26375", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26375", "abs": "https://arxiv.org/abs/2509.26375", "authors": ["Zichao Shen", "Chen Gao", "Jiaqi Yuan", "Tianchen Zhu", "Xingcheng Fu", "Qingyun Sun"], "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning", "comment": null, "summary": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions.", "AI": {"tldr": "SDA-PLANNER\u662f\u4e00\u4e2a\u7528\u4e8e\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u72b6\u6001\u4f9d\u8d56\u56fe\u548c\u9519\u8bef\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u89c4\u5212\u5668\u5728\u89c4\u5212\u8303\u5f0f\u3001\u52a8\u4f5c\u5e8f\u5217\u7ea6\u675f\u548c\u9519\u8bef\u5904\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u56fa\u5b9a\u7684\u89c4\u5212\u8303\u5f0f\u3001\u7f3a\u4e4f\u52a8\u4f5c\u5e8f\u5217\u7ea6\u675f\u4ee5\u53ca\u5bf9\u9519\u8bef\u7684\u5ffd\u89c6\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u9002\u5e94\u89c4\u5212\u3001\u8003\u8651\u72b6\u6001\u4f9d\u8d56\u5173\u7cfb\u5e76\u80fd\u5904\u7406\u6267\u884c\u9519\u8bef\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSDA-PLANNER\uff0c\u5f15\u5165\u72b6\u6001\u4f9d\u8d56\u56fe\u6765\u663e\u5f0f\u5efa\u6a21\u52a8\u4f5c\u524d\u63d0\u6761\u4ef6\u548c\u6548\u679c\uff0c\u6307\u5bfc\u52a8\u6001\u4fee\u8ba2\u3002\u91c7\u7528\u9519\u8bef\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u7b56\u7565\uff0c\u5305\u62ec\u9519\u8bef\u56de\u6eaf\u4e0e\u8bca\u65ad\u4ee5\u53ca\u81ea\u9002\u5e94\u52a8\u4f5c\u5b50\u6811\u751f\u6210\uff0c\u57fa\u4e8e\u5f53\u524d\u73af\u5883\u72b6\u6001\u5c40\u90e8\u91cd\u5efa\u53d7\u5f71\u54cd\u7684\u90e8\u5206\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSDA-PLANNER\u5728\u6210\u529f\u7387\u548c\u76ee\u6807\u5b8c\u6210\u5ea6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5404\u79cd\u9519\u8bef\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "SDA-PLANNER\u901a\u8fc7\u81ea\u9002\u5e94\u89c4\u5212\u8303\u5f0f\u3001\u72b6\u6001\u4f9d\u8d56\u611f\u77e5\u548c\u9519\u8bef\u611f\u77e5\u673a\u5236\uff0c\u4e3a\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2509.26428", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.26428", "abs": "https://arxiv.org/abs/2509.26428", "authors": ["Mattia Piazza", "Mattia Piccinini", "Sebastiano Taddei", "Francesco Biral", "Enrico Bertolazzi"], "title": "Real-time Velocity Profile Optimization for Time-Optimal Maneuvering with Generic Acceleration Constraints", "comment": null, "summary": "The computation of time-optimal velocity profiles along prescribed paths,\nsubject to generic acceleration constraints, is a crucial problem in robot\ntrajectory planning, with particular relevance to autonomous racing. However,\nthe existing methods either support arbitrary acceleration constraints at high\ncomputational cost or use conservative box constraints for computational\nefficiency. We propose FBGA, a new \\underline{F}orward-\\underline{B}ackward\nalgorithm with \\underline{G}eneric \\underline{A}cceleration constraints, which\nachieves both high accuracy and low computation time. FBGA operates forward and\nbackward passes to maximize the velocity profile in short, discretized path\nsegments, while satisfying user-defined performance limits. Tested on five\nracetracks and two vehicle classes, FBGA handles complex, non-convex\nacceleration constraints with custom formulations. Its maneuvers and lap times\nclosely match optimal control baselines (within $0.11\\%$-$0.36\\%$), while being\nup to three orders of magnitude faster. FBGA maintains high accuracy even with\ncoarse discretization, making it well-suited for online multi-query trajectory\nplanning. Our open-source \\texttt{C++} implementation is available at:\nhttps://anonymous.4open.science/r/FB_public_RAL.", "AI": {"tldr": "FBGA\u662f\u4e00\u79cd\u65b0\u7684\u524d\u5411-\u540e\u5411\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ed9\u5b9a\u8def\u5f84\u4e0a\u8ba1\u7b97\u65f6\u95f4\u6700\u4f18\u901f\u5ea6\u5256\u9762\uff0c\u652f\u6301\u901a\u7528\u52a0\u901f\u5ea6\u7ea6\u675f\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u652f\u6301\u4efb\u610f\u52a0\u901f\u5ea6\u7ea6\u675f\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u4f7f\u7528\u4fdd\u5b88\u7684\u7bb1\u5f0f\u7ea6\u675f\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u590d\u6742\u52a0\u901f\u5ea6\u7ea6\u675f\u53c8\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "FBGA\u901a\u8fc7\u524d\u5411\u548c\u540e\u5411\u4f20\u9012\u5728\u77ed\u79bb\u6563\u8def\u5f84\u6bb5\u4e0a\u6700\u5927\u5316\u901f\u5ea6\u5256\u9762\uff0c\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7684\u6027\u80fd\u9650\u5236\u3002\u652f\u6301\u590d\u6742\u975e\u51f8\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u81ea\u5b9a\u4e49\u516c\u5f0f\u3002", "result": "\u5728\u4e94\u4e2a\u8d5b\u9053\u548c\u4e24\u79cd\u8f66\u8f86\u7c7b\u522b\u4e0a\u6d4b\u8bd5\uff0cFBGA\u4e0e\u6700\u4f18\u63a7\u5236\u57fa\u51c6\u7684\u5708\u65f6\u8bef\u5dee\u57280.11%-0.36%\u4e4b\u95f4\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002\u5373\u4f7f\u5728\u7c97\u79bb\u6563\u5316\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "FBGA\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u65f6\u95f4\u7684\u5e73\u8861\uff0c\u975e\u5e38\u9002\u5408\u5728\u7ebf\u591a\u67e5\u8be2\u8f68\u8ff9\u89c4\u5212\u3002\u5df2\u63d0\u4f9b\u5f00\u6e90C++\u5b9e\u73b0\u3002"}}
{"id": "2509.26439", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26439", "abs": "https://arxiv.org/abs/2509.26439", "authors": ["Filip Kulisiewicz", "Basak Sakcak", "Evan G. Center", "Juho Kalliokoski", "Katherine J. Mimnaugh", "Steven M. LaValle", "Timo Ojala"], "title": "Unwinding Rotations Reduces VR Sickness in Nonsimulated Immersive Telepresence", "comment": "24th IEEE International Symposium on Mixed and Augmented Reality\n  (ISMAR)", "summary": "Immersive telepresence, when a user views the video stream of a $360^\\circ$\ncamera in a remote environment using a Head Mounted Display (HMD), has great\npotential to improve the sense of being in a remote environment. In most cases\nof immersive robotic telepresence, the camera is mounted on a mobile robot\nwhich increases the portion of the environment that the remote user can\nexplore. However, robot motions can induce unpleasant symptoms associated with\nVirtual Reality (VR) sickness, degrading the overall user experience. Previous\nresearch has shown that unwinding the rotations of the robot, that is,\ndecoupling the rotations that the camera undergoes due to robot motions from\nwhat is seen by the user, can increase user comfort and reduce VR sickness.\nHowever, that work considered a virtual environment and a simulated robot. In\nthis work, to test whether the same hypotheses hold when the video stream from\na real camera is used, we carried out a user study $(n=36)$ in which the\nunwinding rotations method was compared against coupled rotations in a task\ncompleted through a panoramic camera mounted on a robotic arm. Furthermore,\nwithin an inspection task which involved translations and rotations in three\ndimensions, we tested whether unwinding the robot rotations impacted the\nperformance of users. The results show that the users found the unwinding\nrotations method to be more comfortable and preferable, and that a reduced\nlevel of VR sickness can be achieved without a significant impact on task\nperformance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6c89\u6d78\u5f0f\u673a\u5668\u4eba\u8fdc\u7a0b\u4e34\u573a\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u89e3\u8026\u673a\u5668\u4eba\u65cb\u8f6c\u4e0e\u7528\u6237\u89c6\u89d2\u65cb\u8f6c\u6765\u51cf\u5c11VR\u6655\u52a8\u75c7\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6c89\u6d78\u5f0f\u673a\u5668\u4eba\u8fdc\u7a0b\u4e34\u573a\u7cfb\u7edf\u4e2d\uff0c\u673a\u5668\u4eba\u8fd0\u52a8\u4f1a\u5bfc\u81f4\u76f8\u673a\u65cb\u8f6c\uff0c\u4ece\u800c\u5f15\u53d1VR\u6655\u52a8\u75c7\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u8868\u660e\u89e3\u8026\u65cb\u8f6c\u53ef\u4ee5\u589e\u52a0\u8212\u9002\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u7814\u7a76\u57fa\u4e8e\u865a\u62df\u73af\u5883\u548c\u6a21\u62df\u673a\u5668\u4eba\uff0c\u9700\u8981\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "method": "\u8fdb\u884c\u4e86\u7528\u6237\u7814\u7a76\uff08n=36\uff09\uff0c\u6bd4\u8f83\u4e86\u5728\u771f\u5b9e\u5168\u666f\u76f8\u673a\u5b89\u88c5\u5728\u673a\u68b0\u81c2\u4e0a\u7684\u4efb\u52a1\u4e2d\uff0c\u89e3\u8026\u65cb\u8f6c\u65b9\u6cd5\u4e0e\u8026\u5408\u65cb\u8f6c\u65b9\u6cd5\u7684\u6548\u679c\u3002\u5728\u6d89\u53ca\u4e09\u7ef4\u5e73\u79fb\u548c\u65cb\u8f6c\u7684\u68c0\u67e5\u4efb\u52a1\u4e2d\uff0c\u6d4b\u8bd5\u4e86\u89e3\u8026\u65cb\u8f6c\u5bf9\u7528\u6237\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7528\u6237\u8ba4\u4e3a\u89e3\u8026\u65cb\u8f6c\u65b9\u6cd5\u66f4\u8212\u9002\u3001\u66f4\u53d7\u6b22\u8fce\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4eVR\u6655\u52a8\u75c7\u6c34\u5e73\u3002", "conclusion": "\u89e3\u8026\u673a\u5668\u4eba\u65cb\u8f6c\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u540c\u6837\u6709\u6548\uff0c\u80fd\u591f\u63d0\u9ad8\u7528\u6237\u8212\u9002\u5ea6\u5e76\u51cf\u5c11VR\u6655\u52a8\u75c7\uff0c\u4e14\u4e0d\u4f1a\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2509.26459", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2509.26459", "abs": "https://arxiv.org/abs/2509.26459", "authors": ["Akshay Jaitly", "Devesh K. Jha", "Kei Ota", "Yuki Shirai"], "title": "Analytic Conditions for Differentiable Collision Detection in Trajectory Optimization", "comment": "8 pages, 8 figures. Accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "summary": "Optimization-based methods are widely used for computing fast, diverse\nsolutions for complex tasks such as collision-free movement or planning in the\npresence of contacts. However, most of these methods require enforcing\nnon-penetration constraints between objects, resulting in a non-trivial and\ncomputationally expensive problem. This makes the use of optimization-based\nmethods for planning and control challenging. In this paper, we present a\nmethod to efficiently enforce non-penetration of sets while performing\noptimization over their configuration, which is directly applicable to problems\nlike collision-aware trajectory optimization. We introduce novel differentiable\nconditions with analytic expressions to achieve this. To enforce non-collision\nbetween non-smooth bodies using these conditions, we introduce a method to\napproximate polytopes as smooth semi-algebraic sets. We present several\nnumerical experiments to demonstrate the performance of the proposed method and\ncompare the performance with other baseline methods recently proposed in the\nliterature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u9ad8\u6548\u6267\u884c\u975e\u7a7f\u900f\u7ea6\u675f\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u78b0\u649e\u611f\u77e5\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5fae\u6761\u4ef6\u548c\u5c06\u591a\u9762\u4f53\u8fd1\u4f3c\u4e3a\u5149\u6ed1\u534a\u4ee3\u6570\u96c6\u6765\u5b9e\u73b0\u3002", "motivation": "\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9700\u8981\u5f3a\u5236\u6267\u884c\u7269\u4f53\u95f4\u7684\u975e\u7a7f\u900f\u7ea6\u675f\uff0c\u8fd9\u5bfc\u81f4\u95ee\u9898\u590d\u6742\u4e14\u8ba1\u7b97\u6602\u8d35\uff0c\u9650\u5236\u4e86\u4f18\u5316\u65b9\u6cd5\u5728\u89c4\u5212\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u5177\u6709\u89e3\u6790\u8868\u8fbe\u5f0f\u7684\u53ef\u5fae\u6761\u4ef6\u6765\u6267\u884c\u975e\u7a7f\u900f\u7ea6\u675f\uff0c\u5e76\u63d0\u51fa\u5c06\u591a\u9762\u4f53\u8fd1\u4f3c\u4e3a\u5149\u6ed1\u534a\u4ee3\u6570\u96c6\u7684\u65b9\u6cd5\u6765\u5904\u7406\u975e\u5149\u6ed1\u7269\u4f53\u95f4\u7684\u975e\u78b0\u649e\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u6587\u732e\u4e2d\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6267\u884c\u975e\u7a7f\u900f\u7ea6\u675f\uff0c\u4e3a\u78b0\u649e\u611f\u77e5\u8f68\u8ff9\u4f18\u5316\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26513", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26513", "abs": "https://arxiv.org/abs/2509.26513", "authors": ["Saad Abdul Ghani", "Kameron Lee", "Xuesu Xiao"], "title": "Learning from Hallucinating Critical Points for Navigation in Dynamic Environments", "comment": null, "summary": "Generating large and diverse obstacle datasets to learn motion planning in\nenvironments with dynamic obstacles is challenging due to the vast space of\npossible obstacle trajectories. Inspired by hallucination-based data synthesis\napproaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a\nself-supervised framework for creating rich dynamic obstacle datasets based on\nexisting optimal motion plans without requiring expensive expert demonstrations\nor trial-and-error exploration. LfH-CP factorizes hallucination into two\nstages: first identifying when and where obstacles must appear in order to\nresult in an optimal motion plan, i.e., the critical points, and then\nprocedurally generating diverse trajectories that pass through these points\nwhile avoiding collisions. This factorization avoids generative failures such\nas mode collapse and ensures coverage of diverse dynamic behaviors. We further\nintroduce a diversity metric to quantify dataset richness and show that LfH-CP\nproduces substantially more varied training data than existing baselines.\nExperiments in simulation demonstrate that planners trained on LfH-CP datasets\nachieves higher success rates compared to a prior hallucination method.", "AI": {"tldr": "\u63d0\u51faLfH-CP\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5e7b\u89c9\u5173\u952e\u70b9\u751f\u6210\u591a\u6837\u5316\u52a8\u6001\u969c\u788d\u7269\u6570\u636e\u96c6\uff0c\u65e0\u9700\u4e13\u5bb6\u6f14\u793a\u6216\u8bd5\u9519\u63a2\u7d22", "motivation": "\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e2d\u751f\u6210\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u969c\u788d\u7269\u8f68\u8ff9\u7a7a\u95f4\u5de8\u5927", "method": "\u5c06\u5e7b\u89c9\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u8bc6\u522b\u5173\u952e\u70b9\uff08\u969c\u788d\u7269\u5fc5\u987b\u51fa\u73b0\u7684\u65f6\u95f4\u548c\u4f4d\u7f6e\uff09\uff0c\u7136\u540e\u7a0b\u5e8f\u5316\u751f\u6210\u901a\u8fc7\u8fd9\u4e9b\u70b9\u4e14\u907f\u514d\u78b0\u649e\u7684\u591a\u6837\u5316\u8f68\u8ff9", "result": "LfH-CP\u6bd4\u73b0\u6709\u57fa\u7ebf\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u4f7f\u7528LfH-CP\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u89c4\u5212\u5668\u6210\u529f\u7387\u66f4\u9ad8", "conclusion": "LfH-CP\u6846\u67b6\u80fd\u6709\u6548\u751f\u6210\u4e30\u5bcc\u7684\u52a8\u6001\u969c\u788d\u7269\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u8fd0\u52a8\u89c4\u5212\u6027\u80fd"}}
{"id": "2509.26518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26518", "abs": "https://arxiv.org/abs/2509.26518", "authors": ["Shuoyu Yue", "Pengpeng Li", "Yang Xu", "Kunrui Ze", "Xingjian Long", "Huazi Cao", "Guibin Sun"], "title": "Memory-Efficient 2D/3D Shape Assembly of Robot Swarms", "comment": null, "summary": "Mean-shift-based approaches have recently emerged as the most effective\nmethods for robot swarm shape assembly tasks. These methods rely on image-based\nrepresentations of target shapes to compute local density gradients and perform\nmean-shift exploration, which constitute their core mechanism. However, such\nimage representations incur substantial memory overhead, which can become\nprohibitive for high-resolution or 3D shapes. To overcome this limitation, we\npropose a memory-efficient tree map representation that hierarchically encodes\nuser-specified shapes and is applicable to both 2D and 3D scenarios. Building\non this representation, we design a behavior-based distributed controller that\nenables assignment-free shape assembly. Comparative 2D and 3D simulations\nagainst a state-of-the-art mean-shift algorithm demonstrate one to two orders\nof magnitude lower memory usage and two to three times faster shape entry while\nmaintaining comparable uniformity. Finally, we validate the framework through\nphysical experiments with 6 to 7 UAVs, confirming its real-world practicality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6811\u5f62\u56fe\u8868\u793a\u7684\u8bb0\u5fc6\u9ad8\u6548\u673a\u5668\u4eba\u7fa4\u4f53\u5f62\u72b6\u7ec4\u88c5\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u5747\u503c\u6f02\u79fb\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u9ad8\u7ec4\u88c5\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5747\u503c\u6f02\u79fb\u7684\u673a\u5668\u4eba\u7fa4\u4f53\u5f62\u72b6\u7ec4\u88c5\u65b9\u6cd5\u4f7f\u7528\u56fe\u50cf\u8868\u793a\u76ee\u6807\u5f62\u72b6\uff0c\u5bfc\u81f4\u9ad8\u5185\u5b58\u5f00\u9500\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u62163D\u5f62\u72b6\u65f6\u53d8\u5f97\u4e0d\u53ef\u884c\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u7f16\u7801\u7528\u6237\u6307\u5b9a\u5f62\u72b6\u7684\u6811\u5f62\u56fe\u8868\u793a\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u884c\u4e3a\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u65e0\u5206\u914d\u7684\u5f62\u72b6\u7ec4\u88c5\u3002", "result": "2D\u548c3D\u6a21\u62df\u663e\u793a\u5185\u5b58\u4f7f\u7528\u964d\u4f4e1-2\u4e2a\u6570\u91cf\u7ea7\uff0c\u5f62\u72b6\u8fdb\u5165\u901f\u5ea6\u5feb2-3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u5747\u5300\u6027\u3002\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6811\u5f62\u56fe\u8868\u793a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5747\u503c\u6f02\u79fb\u65b9\u6cd5\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2509.26558", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26558", "abs": "https://arxiv.org/abs/2509.26558", "authors": ["Andr\u00e9s Mart\u00ednez-Silva", "David Alejo", "Luis Merino", "Fernando Caballero"], "title": "Radio-based Multi-Robot Odometry and Relative Localization", "comment": null, "summary": "Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And\nRanging (radar), which have traditionally seen limited adoption in robotics,\nare experiencing a boost in popularity thanks to their robustness to harsh\nenvironmental conditions and cluttered environments. This work proposes a\nmulti-robot UGV-UAV localization system that leverages the two technologies\nwith inexpensive and readily-available sensors, such as Inertial Measurement\nUnits (IMUs) and wheel encoders, to estimate the relative position of an aerial\nrobot with respect to a ground robot. The first stage of the system pipeline\nincludes a nonlinear optimization framework to trilaterate the location of the\naerial platform based on UWB range data, and a radar pre-processing module with\nloosely coupled ego-motion estimation which has been adapted for a multi-robot\nscenario. Then, the pre-processed radar data as well as the relative\ntransformation are fed to a pose-graph optimization framework with odometry and\ninter-robot constraints. The system, implemented for the Robotic Operating\nSystem (ROS 2) with the Ceres optimizer, has been validated in\nSoftware-in-the-Loop (SITL) simulations and in a real-world dataset. The\nproposed relative localization module outperforms state-of-the-art closed-form\nmethods which are less robust to noise. Our SITL environment includes a custom\nGazebo plugin for generating realistic UWB measurements modeled after real\ndata. Conveniently, the proposed factor graph formulation makes the system\nreadily extensible to full Simultaneous Localization And Mapping (SLAM).\nFinally, all the code and experimental data is publicly available to support\nreproducibility and to serve as a common open dataset for benchmarking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUWB\u548c\u96f7\u8fbe\u7684\u591a\u673a\u5668\u4ebaUGV-UAV\u76f8\u5bf9\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u548c\u4f4d\u59ff\u56fe\u4f18\u5316\u6846\u67b6\u5b9e\u73b0\u7a7a\u4e2d\u673a\u5668\u4eba\u76f8\u5bf9\u4e8e\u5730\u9762\u673a\u5668\u4eba\u7684\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4f18\u4e8e\u73b0\u6709\u95ed\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u65e0\u7ebf\u7535\u7684\u65b9\u6cd5\u5982UWB\u548c\u96f7\u8fbe\u5728\u673a\u5668\u4eba\u9886\u57df\u5e94\u7528\u6709\u9650\uff0c\u4f46\u7531\u4e8e\u5176\u5bf9\u6076\u52a3\u73af\u5883\u548c\u6742\u4e71\u73af\u5883\u7684\u9c81\u68d2\u6027\uff0c\u6b63\u91cd\u65b0\u53d7\u5230\u5173\u6ce8\u3002\u9700\u8981\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u6613\u83b7\u53d6\u7684\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u975e\u7ebf\u6027\u4f18\u5316\u6846\u67b6\u5bf9UWB\u6d4b\u8ddd\u6570\u636e\u8fdb\u884c\u4e09\u8fb9\u5b9a\u4f4d\uff0c\u4ee5\u53ca\u96f7\u8fbe\u9884\u5904\u7406\u6a21\u5757\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u9884\u5904\u7406\u96f7\u8fbe\u6570\u636e\u548c\u76f8\u5bf9\u53d8\u6362\u8f93\u5165\u5230\u4f4d\u59ff\u56fe\u4f18\u5316\u6846\u67b6\u4e2d\uff0c\u5305\u542b\u91cc\u7a0b\u8ba1\u548c\u673a\u5668\u4eba\u95f4\u7ea6\u675f\u3002", "result": "\u8be5\u7cfb\u7edf\u5728SITL\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u95ed\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e14\u7cfb\u7edf\u53ef\u6269\u5c55\u81f3\u5b8c\u6574SLAM\u3002", "conclusion": "\u63d0\u51fa\u7684\u76f8\u5bf9\u5b9a\u4f4d\u6a21\u5757\u5728\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u7cfb\u7edf\u4ee3\u7801\u548c\u5b9e\u9a8c\u6570\u636e\u5df2\u516c\u5f00\uff0c\u652f\u6301\u53ef\u91cd\u590d\u6027\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2509.26581", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26581", "abs": "https://arxiv.org/abs/2509.26581", "authors": ["Shishir Gopinath", "Karthik Dantu", "Steven Y. Ko"], "title": "Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework", "comment": null, "summary": "We present Graphite, a GPU-accelerated nonlinear graph optimization\nframework. It provides a CUDA C++ interface to enable the sharing of code\nbetween a realtime application, such as a SLAM system, and its optimization\ntasks. The framework supports techniques to reduce memory usage, including\nin-place optimization, support for multiple floating point types and\nmixed-precision modes, and dynamically computed Jacobians. We evaluate Graphite\non well-known bundle adjustment problems and find that it achieves similar\nperformance to MegBA, a solver specialized for bundle adjustment, while\nmaintaining generality and using less memory. We also apply Graphite to global\nvisual-inertial bundle adjustment on maps generated from stereo-inertial SLAM\ndatasets, and observe speed ups of up to 59x compared to a CPU baseline. Our\nresults indicate that our solver enables faster large-scale optimization on\nboth desktop and resource-constrained devices.", "AI": {"tldr": "Graphite\u662f\u4e00\u4e2aGPU\u52a0\u901f\u7684\u975e\u7ebf\u6027\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u63d0\u4f9bCUDA C++\u63a5\u53e3\uff0c\u652f\u6301\u5185\u5b58\u4f18\u5316\u6280\u672f\uff0c\u5728\u6346\u7ed1\u8c03\u6574\u95ee\u9898\u4e0a\u6027\u80fd\u4e0e\u4e13\u7528\u6c42\u89e3\u5668\u76f8\u5f53\uff0c\u5728\u89c6\u89c9-\u60ef\u6027\u6346\u7ed1\u8c03\u6574\u4e2d\u6bd4CPU\u57fa\u51c6\u5feb59\u500d\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u901a\u7528\u7684GPU\u52a0\u901f\u975e\u7ebf\u6027\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u53c8\u80fd\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u5982SLAM\u7cfb\u7edf\u3002", "method": "\u63d0\u4f9bCUDA C++\u63a5\u53e3\u652f\u6301\u4ee3\u7801\u5171\u4eab\uff0c\u91c7\u7528\u5185\u5b58\u4f18\u5316\u6280\u672f\u5305\u62ec\u539f\u5730\u4f18\u5316\u3001\u591a\u6d6e\u70b9\u7c7b\u578b\u652f\u6301\u3001\u6df7\u5408\u7cbe\u5ea6\u6a21\u5f0f\u548c\u52a8\u6001\u8ba1\u7b97\u96c5\u53ef\u6bd4\u77e9\u9635\u3002", "result": "\u5728\u6346\u7ed1\u8c03\u6574\u95ee\u9898\u4e0a\u4e0e\u4e13\u7528\u6c42\u89e3\u5668MegBA\u6027\u80fd\u76f8\u5f53\u4f46\u5185\u5b58\u4f7f\u7528\u66f4\u5c11\uff1b\u5728\u89c6\u89c9-\u60ef\u6027\u6346\u7ed1\u8c03\u6574\u4e2d\u6bd4CPU\u57fa\u51c6\u5feb59\u500d\u3002", "conclusion": "Graphite\u80fd\u591f\u5728\u684c\u9762\u548c\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u66f4\u5feb\u7684\u5927\u89c4\u6a21\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u6027\u548c\u5185\u5b58\u6548\u7387\u3002"}}
{"id": "2509.26633", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.26633", "abs": "https://arxiv.org/abs/2509.26633", "authors": ["Lujie Yang", "Xiaoyu Huang", "Zhen Wu", "Angjoo Kanazawa", "Pieter Abbeel", "Carmelo Sferrazza", "C. Karen Liu", "Rocky Duan", "Guanya Shi"], "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction", "comment": "Project website: https://omniretarget.github.io", "summary": "A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.", "AI": {"tldr": "OmniRetarget\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ea4\u4e92\u7f51\u683c\u7684\u8fd0\u52a8\u91cd\u5b9a\u5411\u5f15\u64ce\uff0c\u80fd\u591f\u4fdd\u6301\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u7a7a\u95f4\u548c\u63a5\u89e6\u5173\u7cfb\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u8fd0\u52a8\u91cd\u5b9a\u5411\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u7684\u5177\u8eab\u5dee\u8ddd\u95ee\u9898\uff0c\u4ea7\u751f\u811a\u6ed1\u548c\u7a7f\u900f\u7b49\u7269\u7406\u4e0a\u4e0d\u53ef\u884c\u7684\u4f2a\u5f71\uff0c\u4e14\u5ffd\u89c6\u4e86\u4eba\u7c7b-\u7269\u4f53\u548c\u73af\u5883\u4ea4\u4e92\u5bf9\u4e8e\u8868\u8fbe\u6027\u8fd0\u52a8\u548c\u64cd\u4f5c\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u7f51\u683c\u4e4b\u95f4\u7684\u62c9\u666e\u62c9\u65af\u53d8\u5f62\u540c\u65f6\u5f3a\u5236\u6267\u884c\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0cOmniRetarget\u751f\u6210\u8fd0\u52a8\u5b66\u53ef\u884c\u7684\u8f68\u8ff9\u3002\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u4ea4\u4e92\u652f\u6301\u4ece\u5355\u4e00\u6f14\u793a\u5230\u4e0d\u540c\u673a\u5668\u4eba\u5177\u8eab\u3001\u5730\u5f62\u548c\u7269\u4f53\u914d\u7f6e\u7684\u9ad8\u6548\u6570\u636e\u589e\u5f3a\u3002", "result": "\u4eceOMOMO\u3001LAFAN1\u548c\u5185\u90e8MoCap\u6570\u636e\u96c6\u91cd\u5b9a\u5411\u8fd0\u52a8\uff0c\u751f\u6210\u4e86\u8d85\u8fc78\u5c0f\u65f6\u7684\u8f68\u8ff9\uff0c\u5728\u8fd0\u52a8\u5b66\u7ea6\u675f\u6ee1\u8db3\u548c\u63a5\u89e6\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u6570\u636e\u4f7f\u672c\u4f53\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u6210\u529f\u6267\u884c\u957f\u65f6\u7a0b\uff08\u957f\u8fbe30\u79d2\uff09\u7684\u8dd1\u9177\u548c\u8fd0\u52a8\u64cd\u4f5c\u6280\u80fd\uff0c\u4ec5\u4f7f\u75285\u4e2a\u5956\u52b1\u9879\u548c\u7b80\u5355\u7684\u9886\u57df\u968f\u673a\u5316\uff0c\u65e0\u9700\u4efb\u4f55\u5b66\u4e60\u8bfe\u7a0b\u3002"}}
{"id": "2509.26642", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26642", "abs": "https://arxiv.org/abs/2509.26642", "authors": ["Zhuoyang Liu", "Jiaming Liu", "Jiadong Xu", "Nuowei Han", "Chenyang Gu", "Hao Chen", "Kaichen Zhou", "Renrui Zhang", "Kai Chin Hsieh", "Kun Wu", "Zhengping Che", "Jian Tang", "Shanghang Zhang"], "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation", "comment": null, "summary": "Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u611f\u5b98\u8bed\u8a00-\u52a8\u4f5c\uff08MLA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u534f\u540c\u611f\u77e5\u5f02\u6784\u611f\u5b98\u6a21\u6001\u5e76\u9884\u6d4b\u672a\u6765\u591a\u611f\u5b98\u76ee\u6807\u6765\u589e\u5f3a\u7269\u7406\u4e16\u754c\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u590d\u6742\u63a5\u89e6\u5f0f\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u548c\u8bed\u8a00\u89e3\u91ca\u6765\u751f\u6210\u52a8\u4f5c\uff0c\u4f46\u673a\u5668\u4eba\u9700\u8981\u5728\u7a7a\u95f4\u7269\u7406\u4e16\u754c\u4e2d\u611f\u77e5\u548c\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u5bf9\u673a\u5668\u4eba\u7279\u5b9a\u591a\u611f\u5b98\u4fe1\u606f\u7684\u5168\u9762\u7406\u89e3\uff0c\u8fd9\u5bf9\u5b9e\u73b0\u590d\u6742\u63a5\u89e6\u5f0f\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "1\uff09\u63d0\u51fa\u65e0\u7f16\u7801\u5668\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u65b9\u6848\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u7528\u4f5c\u611f\u77e5\u6a21\u5757\uff0c\u901a\u8fc7\u4f4d\u7f6e\u5bf9\u5e94\u76f4\u63a5\u89e3\u91ca2D\u56fe\u50cf\u30013D\u70b9\u4e91\u548c\u89e6\u89c9\u6807\u8bb0\uff1b2\uff09\u8bbe\u8ba1\u672a\u6765\u591a\u611f\u5b98\u751f\u6210\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u63a8\u7406\u8bed\u4e49\u3001\u51e0\u4f55\u548c\u4ea4\u4e92\u4fe1\u606f\u3002", "result": "MLA\u6a21\u578b\u5728\u590d\u6742\u63a5\u89e6\u5f0f\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u76842D\u548c3D VLA\u65b9\u6cd5\u63d0\u9ad8\u4e8612%\u548c24%\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u914d\u7f6e\u7684\u66f4\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u591a\u611f\u5b98\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u672a\u6765\u611f\u5b98\u751f\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u611f\u77e5\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u63a5\u89e6\u5f0f\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u6761\u4ef6\u3002"}}
