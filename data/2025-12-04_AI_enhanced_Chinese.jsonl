{"id": "2512.03166", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03166", "abs": "https://arxiv.org/abs/2512.03166", "authors": ["Aya Taourirte", "Md Sohag Mia"], "title": "Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments", "comment": null, "summary": "The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u548c\u5e73\u5747\u573a\u7406\u8bba\uff0c\u89e3\u51b3\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b9e\u65f6\u51b3\u7b56\u3001\u5408\u4f5c\u4e0e\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u8db3\u7403\u7b49\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u90e8\u7f72\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u8981\u5b9e\u65f6\u51b3\u7b56\u3001\u590d\u6742\u5408\u4f5c\u548c\u53ef\u6269\u5c55\u7b97\u6cd5\u3002\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u4efb\u52a1\u591a\u7c92\u5ea6\u6027\uff08\u957f\u671f\u7b56\u7565vs\u5373\u65f6\u52a8\u4f5c\uff09\u548c\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4ea4\u4e92\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "1. \u5efa\u7acb\u57fa\u4e8ePPO\u7684\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\u57fa\u7ebf\uff1b2. \u5f15\u5165\u57fa\u4e8e\u9009\u9879\u6846\u67b6\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7ed3\u6784\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u9ad8\u5c42\u8f68\u8ff9\u89c4\u5212\u5c42\uff08\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff09\u548c\u4f4e\u5c42\u52a8\u4f5c\u6267\u884c\u5c42\uff1b3. \u5c06\u5e73\u5747\u573a\u7406\u8bba\u96c6\u6210\u5230HRL\u6846\u67b6\u4e2d\uff0c\u7b80\u5316\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e3a\u5355\u667a\u80fd\u4f53vs\u7fa4\u4f53\u5e73\u5747\u3002", "result": "PPO\u57fa\u7ebf\uff1a4.32\u5e73\u5747\u8fdb\u7403\uff0c82.9%\u63a7\u7403\u7387\uff1bHRL\uff1a\u5e73\u5747\u8fdb\u7403\u63d0\u5347\u81f35.26\uff1b\u5e73\u5747\u573aactor-critic\u65b9\u6cd5\uff1a5.93\u5e73\u5747\u8fdb\u7403\uff0c89.1%\u63a7\u7403\u7387\uff0c92.3%\u4f20\u7403\u51c6\u786e\u7387\uff0c\u8bad\u7ec3\u7a33\u5b9a\u6027\u589e\u5f3a\u3002\u5728Webots\u73af\u5883\u4e2d4v4\u6bd4\u8d5b\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7edf\u4e00MARL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u548c\u5e73\u5747\u573a\u7406\u8bba\uff0c\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u9886\u57df\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u548c\u5408\u4f5c\u7684\u884c\u4e3a\uff0c\u4e3a\u89e3\u51b3\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.03194", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03194", "abs": "https://arxiv.org/abs/2512.03194", "authors": ["Johannes Gaber", "Meshal Alharbi", "Daniele Gammelli", "Gioele Zardini"], "title": "GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding", "comment": null, "summary": "Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b66\u4e60\u578b\u5168\u5c40\u6307\u5bfc\u4e0e\u8f7b\u91cf\u4f18\u5316\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u673a\u5668\u4eba\u8f66\u961f\u4efb\u52a1\u8c03\u5ea6\uff0c\u5728500\u4e2a\u673a\u5668\u4eba\u7684\u4ed3\u5e93\u573a\u666f\u4e2d\u63d0\u5347\u541e\u5410\u91cf10%", "motivation": "\u5927\u578b\u673a\u5668\u4eba\u8f66\u961f\u5728\u4ed3\u5e93\u7b49\u7269\u6d41\u573a\u666f\u4e2d\u65e5\u76ca\u666e\u904d\uff0c\u5fae\u5c0f\u7684\u63a7\u5236\u6539\u8fdb\u80fd\u5e26\u6765\u5de8\u5927\u7684\u8fd0\u8425\u5f71\u54cd\u3002\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u3001\u9ad8\u62e5\u5835\u573a\u666f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u65e2\u80fd\u5b9e\u65f6\u6267\u884c\u53c8\u80fd\u63d0\u5347\u541e\u5410\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u8f93\u51fa\u81ea\u7531\u673a\u5668\u4eba\u5728\u805a\u5408\u4ed3\u5e93\u56fe\u4e0a\u7684\u671f\u671b\u5206\u5e03\uff1b2) \u901a\u8fc7\u6700\u5c0f\u6210\u672c\u6d41\u5c06\u4fe1\u53f7\u8f6c\u6362\u4e3a\u533a\u57df\u95f4\u518d\u5e73\u8861\uff1b3) \u901a\u8fc7\u5c0f\u578b\u5c40\u90e8\u5206\u914d\u95ee\u9898\u5b8c\u6210\u6700\u7ec8\u8c03\u5ea6\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u6bcf\u6b65\u5ef6\u8fdf\u63a7\u5236\u57281\u79d2\u8ba1\u7b97\u9884\u7b97\u5185\u3002", "result": "\u5728League of Robot Runners (LRR)\u7684\u62e5\u5835\u4ed3\u5e93\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6700\u591a500\u4e2a\u673a\u5668\u4eba\u7684\u573a\u666f\u4e0b\uff0c\u76f8\u6bd42024\u5e74\u83b7\u80dc\u8c03\u5ea6\u5668\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe10%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u5c06\u56fe\u7ed3\u6784\u5b66\u4e60\u6307\u5bfc\u4e0e\u53ef\u6c42\u89e3\u5668\u76f8\u7ed3\u5408\u80fd\u6709\u6548\u51cf\u5c11\u62e5\u5835\uff0c\u4e3a\u5927\u89c4\u6a21\u8f66\u961f\u7684\u9ad8\u541e\u5410\u91cf\u8c03\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u84dd\u56fe\uff0c\u8868\u660e\u5b66\u4e60\u4e0e\u4f18\u5316\u6df7\u5408\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.03256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03256", "abs": "https://arxiv.org/abs/2512.03256", "authors": ["Albert H. Li", "Ivan Dario Jimenez Rodriguez", "Joel W. Burdick", "Yisong Yue", "Aaron D. Ames"], "title": "KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems", "comment": null, "summary": "Long-horizon dynamical prediction is fundamental in robotics and control, underpinning canonical methods like model predictive control. Yet, many systems and disturbance phenomena are difficult to model due to effects like nonlinearity, chaos, and high-dimensionality. Koopman theory addresses this by modeling the linear evolution of embeddings of the state under an infinite-dimensional linear operator that can be approximated with a suitable finite basis of embedding functions, effectively trading model nonlinearity for representational complexity. However, explicitly computing a good choice of basis is nontrivial, and poor choices may cause inaccurate forecasts or overfitting. To address this, we present Kalman-Implicit Koopman Operator (KALIKO) Learning, a method that leverages the Kalman filter to implicitly learn embeddings corresponding to latent dynamics without requiring an explicit encoder. KALIKO produces interpretable representations consistent with both theory and prior works, yielding high-quality reconstructions and inducing a globally linear latent dynamics. Evaluated on wave data generated by a high-dimensional PDE, KALIKO surpasses several baselines in open-loop prediction and in a demanding closed-loop simulated control task: stabilizing an underactuated manipulator's payload by predicting and compensating for strong wave disturbances.", "AI": {"tldr": "KALIKO\u65b9\u6cd5\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u9690\u5f0f\u5b66\u4e60\u5d4c\u5165\u8868\u793a\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u957f\u65f6\u7a0b\u52a8\u529b\u5b66\u9884\u6d4b\uff0c\u5728\u6ce2\u6d6a\u5e72\u6270\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u957f\u65f6\u7a0b\u52a8\u529b\u5b66\u9884\u6d4b\u5bf9\u673a\u5668\u4eba\u63a7\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bb8\u591a\u7cfb\u7edf\uff08\u5982\u975e\u7ebf\u6027\u3001\u6df7\u6c8c\u3001\u9ad8\u7ef4\u7cfb\u7edf\uff09\u96be\u4ee5\u5efa\u6a21\u3002Koopman\u7406\u8bba\u901a\u8fc7\u7ebf\u6027\u5d4c\u5165\u89e3\u51b3\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u4f46\u663e\u5f0f\u8ba1\u7b97\u5408\u9002\u7684\u57fa\u51fd\u6570\u5f88\u56f0\u96be\uff0c\u53ef\u80fd\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u6216\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51faKALIKO\uff08Kalman-Implicit Koopman Operator Learning\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u9690\u5f0f\u5b66\u4e60\u4e0e\u6f5c\u5728\u52a8\u529b\u5b66\u5bf9\u5e94\u7684\u5d4c\u5165\u8868\u793a\uff0c\u65e0\u9700\u663e\u5f0f\u7f16\u7801\u5668\u3002\u8be5\u65b9\u6cd5\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u4fdd\u6301\u5168\u5c40\u7ebf\u6027\u6f5c\u5728\u52a8\u529b\u5b66\u3002", "result": "\u5728\u9ad8\u7ef4PDE\u751f\u6210\u7684\u6ce2\u6d6a\u6570\u636e\u4e0a\u8bc4\u4f30\uff0cKALIKO\u5728\u5f00\u73af\u9884\u6d4b\u548c\u95ed\u73af\u63a7\u5236\u4efb\u52a1\u4e2d\u5747\u8d85\u8d8a\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u6a21\u62df\u63a7\u5236\u4efb\u52a1\u4e2d\u6210\u529f\u7a33\u5b9a\u6b20\u9a71\u52a8\u673a\u68b0\u81c2\u7684\u8d1f\u8f7d\uff0c\u901a\u8fc7\u9884\u6d4b\u548c\u8865\u507f\u5f3a\u6ce2\u6d6a\u5e72\u6270\u3002", "conclusion": "KALIKO\u901a\u8fc7\u9690\u5f0f\u5b66\u4e60\u5d4c\u5165\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86Koopman\u7406\u8bba\u4e2d\u57fa\u51fd\u6570\u9009\u62e9\u7684\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u957f\u65f6\u7a0b\u9884\u6d4b\uff0c\u5728\u590d\u6742\u5e72\u6270\u73af\u5883\u4e0b\u7684\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.03347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03347", "abs": "https://arxiv.org/abs/2512.03347", "authors": ["William van den Bogert", "Gregory Linkowski", "Nima Fazeli"], "title": "GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation", "comment": "8 pages, 8 figures, 2 tables", "summary": "Imitation Learning (IL) holds great potential for learning repetitive manipulation tasks, such as those in industrial assembly. However, its effectiveness is often limited by insufficient trajectory precision due to compounding errors. In this paper, we introduce Grasped Object Manifold Projection (GOMP), an interactive method that mitigates these errors by constraining a non-rigidly grasped object to a lower-dimensional manifold. GOMP assumes a precise task in which a manipulator holds an object that may shift within the grasp in an observable manner and must be mated with a grounded part. Crucially, all GOMP enhancements are learned from the same expert dataset used to train the base IL policy, and are adjusted with an n-arm bandit-based interactive component. We propose a theoretical basis for GOMP's improvement upon the well-known compounding error bound in IL literature. We demonstrate the framework on four precise assembly tasks using tactile feedback, and note that the approach remains modality-agnostic. Data and videos are available at williamvdb.github.io/GOMPsite.", "AI": {"tldr": "GOMP\u662f\u4e00\u79cd\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u975e\u521a\u6027\u6293\u53d6\u7269\u4f53\u7ea6\u675f\u5230\u4f4e\u7ef4\u6d41\u5f62\u6765\u51cf\u5c11\u7d2f\u79ef\u8bef\u5dee\uff0c\u63d0\u9ad8\u7cbe\u786e\u88c5\u914d\u4efb\u52a1\u7684\u8f68\u8ff9\u7cbe\u5ea6\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u5de5\u4e1a\u88c5\u914d\u7b49\u91cd\u590d\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5e38\u56e0\u7d2f\u79ef\u8bef\u5dee\u5bfc\u81f4\u8f68\u8ff9\u7cbe\u5ea6\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u975e\u521a\u6027\u6293\u53d6\u7269\u4f53\u5728\u6293\u63e1\u4e2d\u7684\u5fae\u5c0f\u4f4d\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faGOMP\u65b9\u6cd5\uff1a1) \u5c06\u975e\u521a\u6027\u6293\u53d6\u7269\u4f53\u7ea6\u675f\u5230\u4f4e\u7ef4\u6d41\u5f62\uff1b2) \u4ece\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u589e\u5f3a\u7b56\u7565\uff1b3) \u4f7f\u7528n-arm bandit\u4ea4\u4e92\u7ec4\u4ef6\u8fdb\u884c\u8c03\u6574\uff1b4) \u63d0\u4f9b\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5bf9\u7d2f\u79ef\u8bef\u5dee\u754c\u7684\u6539\u8fdb\u3002", "result": "\u5728\u56db\u4e2a\u7cbe\u786e\u88c5\u914d\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86GOMP\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u89e6\u89c9\u53cd\u9988\u4f46\u65b9\u6cd5\u4fdd\u6301\u6a21\u6001\u65e0\u5173\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u88c5\u914d\u7cbe\u5ea6\u3002", "conclusion": "GOMP\u901a\u8fc7\u6d41\u5f62\u7ea6\u675f\u548c\u4ea4\u4e92\u5f0f\u8c03\u6574\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u95ee\u9898\uff0c\u4e3a\u7cbe\u786e\u88c5\u914d\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u5177\u6709\u6a21\u6001\u65e0\u5173\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2512.03047", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03047", "abs": "https://arxiv.org/abs/2512.03047", "authors": ["Samih Fadli"], "title": "Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models", "comment": "6 pages. Companion paper to \"The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems\". Code and tools: https://github.com/AerisSpace/EthicalEntropyKit", "summary": "Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8bc4\u4f30LLM\u5b89\u5168\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u884c\u4e3a\u5206\u7c7b\u3001\u8bad\u7ec3\u5206\u7c7b\u5668\u4f30\u8ba1\u4f26\u7406\u71b5S(t)\uff0c\u6d4b\u91cf\u6a21\u578b\u5728\u538b\u529b\u6d4b\u8bd5\u4e2d\u7684\u71b5\u52a8\u6001\u53d8\u5316\uff0c\u5e76\u5f00\u53d1\u76d1\u63a7\u7ba1\u9053\u68c0\u6d4b\u4ef7\u503c\u6f02\u79fb\u3002", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\uff0c\u4f46\u5173\u952e\u5931\u6548\u662f\u52a8\u6001\u7684\uff1a\u5206\u5e03\u6f02\u79fb\u4e0b\u7684\u4ef7\u503c\u6f02\u79fb\u3001\u8d8a\u72f1\u653b\u51fb\u3001\u90e8\u7f72\u4e2d\u5bf9\u9f50\u7684\u7f13\u6162\u9000\u5316\u3002\u9700\u8981\u52a8\u6001\u6846\u67b6\u6765\u76d1\u63a7\u548c\u5e94\u5bf9\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u57fa\u4e8e\"\u667a\u80fd\u7b2c\u4e8c\u5b9a\u5f8b\"\u5c06\u4f26\u7406\u71b5\u4f5c\u4e3a\u72b6\u6001\u53d8\u91cf\uff0c\u5b9a\u4e49\u4e94\u7ef4\u884c\u4e3a\u5206\u7c7b\u6cd5\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u4ece\u6a21\u578b\u8f6c\u5f55\u672c\u4f30\u8ba1\u4f26\u7406\u71b5S(t)\uff0c\u6d4b\u91cf\u56db\u4e2a\u524d\u6cbf\u6a21\u578b\u7684\u57fa\u7840\u7248\u548c\u6307\u4ee4\u8c03\u4f18\u7248\u5728\u538b\u529b\u6d4b\u8bd5\u4e2d\u7684\u71b5\u52a8\u6001\uff0c\u8ba1\u7b97\u6709\u6548\u5bf9\u9f50\u5de5\u4f5c\u7387\u03b3_eff\uff0c\u5e76\u5c06S(t)\u548c\u03b3_eff\u5d4c\u5165\u76d1\u63a7\u7ba1\u9053\u3002", "result": "\u57fa\u7840\u6a21\u578b\u663e\u793a\u6301\u7eed\u7684\u71b5\u589e\u957f\uff0c\u800c\u8c03\u4f18\u53d8\u4f53\u6291\u5236\u4e86\u6f02\u79fb\u5e76\u5c06\u4f26\u7406\u71b5\u964d\u4f4e\u4e86\u7ea680%\u3002\u4ece\u8fd9\u4e9b\u8f68\u8ff9\u4e2d\u4f30\u8ba1\u51fa\u6709\u6548\u5bf9\u9f50\u5de5\u4f5c\u7387\uff0c\u76d1\u63a7\u7ba1\u9053\u80fd\u5728\u71b5\u6f02\u79fb\u8d85\u8fc7\u7a33\u5b9a\u9608\u503c\u65f6\u53d1\u51fa\u8b66\u62a5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u5b89\u5168\u63d0\u4f9b\u4e86\u52a8\u6001\u76d1\u63a7\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u76d1\u7763\u4ef7\u503c\u6f02\u79fb\uff0c\u6bd4\u9759\u6001\u57fa\u51c6\u66f4\u80fd\u6355\u6349\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u98ce\u9669\uff0c\u4e3a\u8fd0\u884c\u65f6\u7684\u5bf9\u9f50\u76d1\u7763\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.03397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03397", "abs": "https://arxiv.org/abs/2512.03397", "authors": ["Seungwon Choi", "Dong-Gyu Park", "Seo-Yeon Hwang", "Tae-Wan Kim"], "title": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing", "comment": null, "summary": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.", "AI": {"tldr": "Surfel-LIO\uff1a\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u4f53\u7d20\u7ed3\u6784\u548c\u9884\u8ba1\u7b97\u9762\u5143\u8868\u793a\u7684\u6fc0\u5149\u96f7\u8fbe\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7O(1)\u5bf9\u5e94\u5173\u7cfb\u68c0\u7d22\u548cZ-order\u66f2\u7ebf\u7f16\u7801\u5b9e\u73b0\u9ad8\u901f\u5904\u7406", "motivation": "\u73b0\u6709LIO\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u53ef\u6539\u8fdb\u4e4b\u5904\uff1a(1)\u6700\u8fd1\u90bb\u641c\u7d22\u9700\u8981\u68c0\u67e5\u591a\u4e2a\u7a7a\u95f4\u5355\u5143\u4ee5\u6536\u96c6\u8db3\u591f\u7684\u70b9\u8fdb\u884c\u5e73\u9762\u62df\u5408\uff1b(2)\u5c3d\u7ba1\u5730\u56fe\u51e0\u4f55\u672a\u53d8\uff0c\u4f46\u5e73\u9762\u53c2\u6570\u901a\u5e38\u6bcf\u6b21\u8fed\u4ee3\u90fd\u91cd\u65b0\u8ba1\u7b97", "method": "\u63d0\u51faSurfel-LIO\u65b9\u6cd5\uff0c\u91c7\u7528\u5206\u5c42\u4f53\u7d20\u7ed3\u6784(hVox)\u548c\u9884\u8ba1\u7b97\u7684\u9762\u5143\u8868\u793a\uff0c\u7ed3\u5408Z-order\u66f2\u7ebf\u7f16\u7801\u5b9e\u73b0\u7f13\u5b58\u53cb\u597d\u7684\u7a7a\u95f4\u7d22\u5f15\uff0c\u5b9e\u73b0O(1)\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u5bf9\u5e94\u5173\u7cfb\u68c0\u7d22", "result": "\u5728M3DGR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5904\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6", "conclusion": "Surfel-LIO\u901a\u8fc7\u521b\u65b0\u7684\u5206\u5c42\u4f53\u7d20\u7ed3\u6784\u548c\u9884\u8ba1\u7b97\u9762\u5143\u8868\u793a\uff0c\u89e3\u51b3\u4e86LIO\u7cfb\u7edf\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u901f\u5904\u7406\u800c\u4e0d\u727a\u7272\u7cbe\u5ea6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2512.03079", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03079", "abs": "https://arxiv.org/abs/2512.03079", "authors": ["Anudeex Shetty"], "title": "Watermarks for Embeddings-as-a-Service Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.\n  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.\n  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86Embeddings-as-a-Service\uff08EaaS\uff09\u6c34\u5370\u6280\u672f\u7684\u653b\u9632\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6c34\u5370\u53ef\u901a\u8fc7\u6587\u672c\u6539\u5199\u653b\u51fb\u88ab\u79fb\u9664\u7684\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u53d8\u6362\u7684\u65b0\u578b\u6c34\u5370\u6280\u672fWET\u6765\u9632\u5fa1\u6b64\u7c7b\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u4f01\u4e1a\u5f00\u59cb\u63d0\u4f9bEmbeddings-as-a-Service\uff08EaaS\uff09\uff0c\u5176\u6a21\u578b\u9762\u4e34\u6a21\u4eff\u653b\u51fb\u7684\u98ce\u9669\u3002\u867d\u7136\u5df2\u6709\u6c34\u5370\u6280\u672f\u7528\u4e8e\u4fdd\u62a4EaaS\u63d0\u4f9b\u8005\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u4f46\u73b0\u6709\u6c34\u5370\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "1. \u653b\u51fb\u7814\u7a76\uff1a\u901a\u8fc7\u6587\u672c\u6539\u5199\u6280\u672f\uff08\u5305\u62ec\u4e0d\u540c\u6539\u5199\u65b9\u6cd5\u548c\u6a21\u578b\uff09\u5bf9\u73b0\u6709EaaS\u6c34\u5370\u8fdb\u884c\u653b\u51fb\uff0c\u5c55\u793a\u5176\u8106\u5f31\u6027\u3002\n2. \u9632\u5fa1\u65b9\u6848\uff1a\u63d0\u51faWET\uff08Watermarking EaaS with Linear Transformation\uff09\u6c34\u5370\u6280\u672f\uff0c\u901a\u8fc7\u5bf9\u5d4c\u5165\u5411\u91cf\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u6765\u5d4c\u5165\u6c34\u5370\uff0c\u9a8c\u8bc1\u65f6\u5e94\u7528\u9006\u53d8\u6362\u5e76\u6bd4\u8f83\u6062\u590d\u540e\u5d4c\u5165\u4e0e\u539f\u59cb\u5d4c\u5165\u7684\u76f8\u4f3c\u5ea6\u3002", "result": "1. \u653b\u51fb\u7ed3\u679c\uff1a\u6587\u672c\u6539\u5199\u653b\u51fb\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u6709\u6548\u7ed5\u8fc7\u5f53\u524d\u6700\u5148\u8fdb\u7684EaaS\u6c34\u5370\u6280\u672f\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\n2. \u9632\u5fa1\u7ed3\u679c\uff1aWET\u6c34\u5370\u6280\u672f\u5bf9\u6539\u5199\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u548c\u8d85\u53c2\u6570\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709EaaS\u6c34\u5370\u6280\u672f\u5bf9\u6587\u672c\u6539\u5199\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u65b0\u578b\u6c34\u5370\u6280\u672fWET\u3002WET\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u65b9\u6cd5\u6709\u6548\u9632\u5fa1\u4e86\u6539\u5199\u653b\u51fb\uff0c\u4e3aEaaS\u63d0\u4f9b\u8005\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6a21\u578b\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u65b9\u6848\u3002"}}
{"id": "2512.03422", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03422", "abs": "https://arxiv.org/abs/2512.03422", "authors": ["Tianchen Deng", "Yue Pan", "Shenghai Yuan", "Dong Li", "Chen Wang", "Mingrui Li", "Long Chen", "Lihua Xie", "Danwei Wang", "Jingchuan Wang", "Javier Civera", "Hesheng Wang", "Weidong Chen"], "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models", "comment": null, "summary": "In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u6db5\u76d6\u4f20\u7edf\u65b9\u6cd5\uff08\u70b9\u4e91\u3001\u4f53\u7d20\u3001SDF\u3001\u573a\u666f\u56fe\uff09\u548c\u795e\u7ecf\u8868\u793a\uff08NeRF\u30013DGS\u3001\u57fa\u7840\u6a21\u578b\uff09\uff0c\u5206\u6790\u5b83\u4eec\u5728\u673a\u5668\u4eba\u4e94\u5927\u6a21\u5757\uff08\u611f\u77e5\u3001\u5efa\u56fe\u3001\u5b9a\u4f4d\u3001\u5bfc\u822a\u3001\u64cd\u4f5c\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba83D\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u672a\u6765\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u5f53\u524dSLAM\u548c\u5b9a\u4f4d\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u8868\u793a\uff08\u5982\u70b9\u4e91\u3001\u4f53\u7d20\uff09\uff0c\u4f46\u5bc6\u96c6\u573a\u666f\u8868\u793a\u5bf9\u4e0b\u6e38\u4efb\u52a1\uff08\u5bfc\u822a\u3001\u907f\u969c\uff09\u81f3\u5173\u91cd\u8981\u3002\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff08NeRF\u30013DGS\u3001\u57fa\u7840\u6a21\u578b\uff09\u80fd\u66f4\u597d\u5730\u96c6\u6210\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\u548c\u8bed\u8a00\u5148\u9a8c\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u76843D\u573a\u666f\u7406\u89e3\u548c\u5177\u8eab\u667a\u80fd\u3002", "method": "\u5c06\u673a\u5668\u4eba\u6838\u5fc3\u6a21\u5757\u5206\u4e3a\u4e94\u90e8\u5206\uff08\u611f\u77e5\u3001\u5efa\u56fe\u3001\u5b9a\u4f4d\u3001\u5bfc\u822a\u3001\u64cd\u4f5c\uff09\uff0c\u7cfb\u7edf\u68b3\u7406\u4e0d\u540c\u573a\u666f\u8868\u793a\u65b9\u6cd5\u7684\u6807\u51c6\u516c\u5f0f\u5316\u8868\u793a\uff0c\u6bd4\u8f83\u5404\u6a21\u5757\u4e2d\u4e0d\u540c\u8868\u793a\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u56f4\u7ed5\"\u4ec0\u4e48\u662f\u6700\u597d\u7684\u673a\u5668\u4eba3D\u573a\u666f\u8868\u793a\uff1f\"\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\u5c55\u5f00\u5206\u6790\u3002", "result": "\u63d0\u4f9b\u4e86\u673a\u5668\u4eba\u573a\u666f\u8868\u793a\u65b9\u6cd5\u7684\u5168\u9762\u5206\u7c7b\u548c\u6bd4\u8f83\u6846\u67b6\uff0c\u6307\u51fa\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\u5728\u8bed\u4e49\u96c6\u6210\u548c\u8bed\u8a00\u5148\u9a8c\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u9884\u6d4b3D\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u6210\u4e3a\u672a\u6765\u673a\u5668\u4eba\u5e94\u7528\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8bc6\u522b\u4e86\u5b9e\u73b0\u8be5\u6a21\u578b\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u3002", "conclusion": "3D\u57fa\u7840\u6a21\u578b\u6709\u671b\u6210\u4e3a\u672a\u6765\u673a\u5668\u4eba\u573a\u666f\u8868\u793a\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u73b0\u6709\u6311\u6218\u3002\u672c\u6587\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u9879\u76ee\u6301\u7eed\u66f4\u65b0\u76f8\u5173\u6280\u672f\u548c\u7814\u7a76\u6210\u679c\u3002"}}
{"id": "2512.03082", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03082", "abs": "https://arxiv.org/abs/2512.03082", "authors": ["Nan Zhuang", "Wenshuo Wang", "Lekai Qian", "Yuxiao Wang", "Boyu Cao", "Qi Liu"], "title": "Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation", "comment": null, "summary": "Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.", "AI": {"tldr": "\u63d0\u51faRDG\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5e73\u8861\u63a8\u7406\u6570\u636e\u6765\u51cf\u8f7bLLMs\u4e2d\u7684\u9009\u62e9\u652f\u6301\u6027\u504f\u89c1\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u8bb0\u5fc6\u548c\u8bc4\u4f30\u4efb\u52a1\u4e0a\u5206\u522b\u670981.5%\u548c94.3%\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u663e\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9009\u62e9\u652f\u6301\u6027\u504f\u89c1\uff0c\u4f1a\u7cfb\u7edf\u6027\u5730\u504f\u5411\u81ea\u5df1\u9009\u62e9\u7684\u9009\u9879\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cdAI\u8f85\u52a9\u51b3\u7b56\u7684\u5ba2\u89c2\u6027\u3002\u73b0\u6709\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4eba\u53e3\u7edf\u8ba1\u548c\u793e\u4f1a\u504f\u89c1\uff0c\u800c\u9488\u5bf9LLMs\u4e2d\u8ba4\u77e5\u504f\u89c1\u7684\u89e3\u51b3\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faReasoning Dependency Generation (RDG)\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u6784\u5efa\u5e73\u8861\u7684\u63a8\u7406\u95ee\u7b54\u5bf9\u6765\u751f\u6210\u65e0\u504f\u89c1\u63a8\u7406\u6570\u636e\uff0c\u660e\u786e\u5efa\u6a21\u9009\u62e9\u3001\u8bc1\u636e\u548c\u8bba\u8bc1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u8de8\u9886\u57df\u7684\u5927\u89c4\u6a21QA\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6570\u636e\u548c\u4f9d\u8d56\u89e3\u8026\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528RDG\u751f\u6210\u6570\u636e\u5fae\u8c03\u7684LLMs\u5728\u57fa\u4e8e\u8bb0\u5fc6\u7684\u5b9e\u9a8c\u4e2d\u670981.5%\u7684\u6539\u8fdb\uff0c\u5728\u57fa\u4e8e\u8bc4\u4f30\u7684\u5b9e\u9a8c\u4e2d\u670994.3%\u7684\u6539\u8fdb\uff0c\u540c\u65f6\u5728\u6807\u51c6BBQ\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u76f8\u4f3c\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u9996\u4e2a\u89e3\u51b3LLMs\u4e2d\u8ba4\u77e5\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7RDG\u6846\u67b6\u751f\u6210\u65e0\u504f\u89c1\u63a8\u7406\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u7684AI\u8f85\u52a9\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002"}}
{"id": "2512.03429", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03429", "abs": "https://arxiv.org/abs/2512.03429", "authors": ["Raul Steinmetz", "Fabio Demo Rosa", "Victor Augusto Kich", "Jair Augusto Bottega", "Ricardo Bedin Grando", "Daniel Fernando Tello Gamarra"], "title": "World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations", "comment": "Accepted for publication in the Journal of Intelligent and Fuzzy Systems", "summary": "Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eDreamerV3\u7684\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7MLP-VAE\u7f16\u7801\u9ad8\u7ef4LIDAR\u6570\u636e\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u81ea\u4e3b\u5bfc\u822a", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5904\u7406\u9ad8\u7ef4LIDAR\u6570\u636e\u65f6\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u7b80\u5316\u89c2\u6d4b\u6570\u636e\uff0c\u964d\u4f4e\u4e86\u7a7a\u95f4\u611f\u77e5\u548c\u5bfc\u822a\u9c81\u68d2\u6027", "method": "\u57fa\u4e8eDreamerV3\u7b97\u6cd5\u6784\u5efa\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210MLP-VAE\u5c06\u9ad8\u7ef4LIDAR\u8bfb\u6570\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\uff0c\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u52a8\u6001\u9884\u6d4b\u5668\uff0c\u5b9e\u73b0\u57fa\u4e8e\u60f3\u8c61\u7684\u7b56\u7565\u4f18\u5316", "result": "\u5728TurtleBot3\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4SAC\u3001DDPG\u3001TD3\u7b49\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u67b6\u6784\u6536\u655b\u66f4\u5feb\u3001\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u4f7f\u7528\u5b8c\u6574LIDAR\u6570\u636e\u65f6\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4f4e\u4e8e85%", "conclusion": "\u5c06\u9884\u6d4b\u6027\u4e16\u754c\u6a21\u578b\u4e0e\u5b66\u4e60\u7684\u6f5c\u5728\u8868\u793a\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4ece\u9ad8\u7ef4\u611f\u5b98\u6570\u636e\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u5bfc\u822a"}}
{"id": "2512.03195", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03195", "abs": "https://arxiv.org/abs/2512.03195", "authors": ["Stylianos Saroglou", "Konstantinos Diamantaras", "Francesco Preta", "Marina Delianidi", "Apostolos Benisis", "Christian Johannes Meyer"], "title": "Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies", "comment": "14 pages, 1 figure, Preprint", "summary": "This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7ed3\u5408\u53e5\u5b50\u94fe\u63a5\u548c\u5b9e\u4f53\u94fe\u63a5\u4e24\u79cd\u65b9\u6cd5\uff0c\u5c06\u62db\u8058\u6587\u672c\u94fe\u63a5\u5230ESCO\u548cEQF\u6846\u67b6\uff0c\u5e76\u53d1\u5e03\u4e86\u4e24\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u804c\u4e1a\u548c\u8d44\u683c\u5728\u62db\u8058\u6587\u672c\u4e2d\u7684\u8868\u793a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6539\u8fdb\u52b3\u52a8\u529b\u5e02\u573a\u4fe1\u606f\u7684\u5206\u7c7b\uff0c\u901a\u8fc7\u5c06\u62db\u8058\u6587\u672c\u94fe\u63a5\u5230\u6b27\u6d32ESCO\u6280\u80fd\u5206\u7c7b\u6846\u67b6\u548cEQF\u8d44\u683c\u6846\u67b6\uff0c\u8d85\u8d8a\u8868\u9762\u7684\u6280\u80fd\u63d0\u53d6\uff0c\u6df1\u5165\u5206\u6790\u804c\u4e1a\u548c\u8d44\u683c\u5728\u6570\u5b57\u4e2d\u4ecb\u7ecf\u6d4e\u4e2d\u7684\u8868\u793a\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\u53e5\u5b50\u94fe\u63a5\u548c\u5b9e\u4f53\u94fe\u63a5\uff0c\u5e76\u5f00\u53d1\u4e86\u7ed3\u5408\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u5f00\u6e90\u5de5\u5177\u3002\u540c\u65f6\u5f15\u5165\u4e86\u4e24\u4e2a\u4e13\u95e8\u6807\u6ce8\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u804c\u4e1a\u548c\u8d44\u683c\u8868\u793a\uff0c\u5e76\u63a2\u7d22\u4e86\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u3002", "result": "\u7814\u7a76\u5f00\u53d1\u4e86\u516c\u5f00\u53ef\u7528\u7684\u5f00\u6e90\u5de5\u5177\u548c\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4e3a\u52b3\u52a8\u529b\u5206\u7c7b\u548c\u5c31\u4e1a\u8bdd\u8bed\u7814\u7a76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u3002\u7814\u7a76\u7ed3\u679c\u63a8\u52a8\u4e86\u5de5\u4f5c\u5b9e\u4f53\u63d0\u53d6\u7684\u6280\u672f\u53d1\u5c55\uff0c\u5e76\u4e3a\u5206\u6790\u6570\u5b57\u4e2d\u4ecb\u7ecf\u6d4e\u4e2d\u7684\u5de5\u4f5c\u3001\u6280\u80fd\u548c\u52b3\u52a8\u529b\u5e02\u573a\u53d9\u4e8b\u63d0\u4f9b\u4e86\u5de5\u5177\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u548c\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u52b3\u52a8\u529b\u5e02\u573a\u4fe1\u606f\u5206\u7c7b\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u53d1\u5e03\u7684\u5de5\u5177\u548c\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u52b3\u52a8\u529b\u5206\u7c7b\u548c\u5c31\u4e1a\u8bdd\u8bed\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6570\u5b57\u65f6\u4ee3\u7684\u5de5\u4f5c\u548c\u6280\u80fd\u9700\u6c42\u3002"}}
{"id": "2512.03444", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03444", "abs": "https://arxiv.org/abs/2512.03444", "authors": ["Davood Soleymanzadeh", "Xiao Liang", "Minghui Zheng"], "title": "PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers", "comment": null, "summary": "Deep learning methods have significantly enhanced motion planning for robotic manipulators by leveraging prior experiences within planning datasets. However, state-of-the-art neural motion planners are primarily trained on small datasets collected in manually generated workspaces, limiting their generalizability to out-of-distribution scenarios. Additionally, these planners often rely on monolithic network architectures that struggle to encode critical planning information. To address these challenges, we introduce Motion Policy with Dataset Synthesis powered by large language models (LLMs) and Fusion Action-Chunking Transformers (PerFACT), which incorporates two key components. Firstly, a novel LLM-powered workspace generation method, MotionGeneralizer, enables large-scale planning data collection by producing a diverse set of semantically feasible workspaces. Secondly, we introduce Fusion Motion Policy Networks (MpiNetsFusion), a generalist neural motion planner that uses a fusion action-chunking transformer to better encode planning signals and attend to multiple feature modalities. Leveraging MotionGeneralizer, we collect 3.5M trajectories to train and evaluate MpiNetsFusion against state-of-the-art planners, which shows that the proposed MpiNetsFusion can plan several times faster on the evaluated tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPerFACT\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u751f\u6210\u591a\u6837\u5316\u5de5\u4f5c\u7a7a\u95f4\u548c\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\uff0c\u4ee5\u53ca\u878d\u5408\u52a8\u4f5c\u5206\u5757Transformer\u7684\u901a\u7528\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u5668\u4e3b\u8981\u5728\u5c0f\u89c4\u6a21\u4eba\u5de5\u751f\u6210\u5de5\u4f5c\u7a7a\u95f4\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u91c7\u7528\u5355\u4e00\u7f51\u7edc\u67b6\u6784\u96be\u4ee5\u7f16\u7801\u5173\u952e\u89c4\u5212\u4fe1\u606f\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u89c4\u6a21\u4e0d\u8db3\u548c\u67b6\u6784\u9650\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faPerFACT\u65b9\u6cd5\uff1a1) MotionGeneralizer - \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u4e49\u53ef\u884c\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u89c4\u5212\u6570\u636e\u6536\u96c6\uff1b2) MpiNetsFusion - \u878d\u5408\u52a8\u4f5c\u5206\u5757Transformer\u7684\u901a\u7528\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u7f16\u7801\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u4f7f\u7528MotionGeneralizer\u6536\u96c6350\u4e07\u6761\u8f68\u8ff9\u8bad\u7ec3MpiNetsFusion\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u89c4\u5212\u5668\uff0c\u5728\u8bc4\u4f30\u4efb\u52a1\u4e0a\u89c4\u5212\u901f\u5ea6\u63d0\u5347\u6570\u500d\u3002", "conclusion": "PerFACT\u901a\u8fc7LLM\u751f\u6210\u591a\u6837\u5316\u5de5\u4f5c\u7a7a\u95f4\u548c\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\uff0c\u7ed3\u5408\u878d\u5408\u52a8\u4f5c\u5206\u5757Transformer\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.03197", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03197", "abs": "https://arxiv.org/abs/2512.03197", "authors": ["Faezeh Faez", "Marzieh S. Tahaei", "Yaochen Hu", "Ali Pourranjbar", "Mahdi Biparva", "Mark Coates", "Yingxue Zhang"], "title": "InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.", "AI": {"tldr": "InvertiTune\uff1a\u901a\u8fc7\u53ef\u63a7\u6570\u636e\u751f\u6210\u548c\u5fae\u8c03\uff0c\u5b9e\u73b0\u5355\u6b21\u63a8\u7406\u7684\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u63d0\u5347\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u6587\u672c\u5230\u77e5\u8bc6\u56fe\u8c31\uff08Text2KG\uff09\u65b9\u6cd5\u4f9d\u8d56\u8fed\u4ee3\u63d0\u793a\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u5ffd\u7565\u6587\u672c\u4e2d\u5206\u5e03\u7684\u590d\u6742\u5173\u7cfb\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faInvertiTune\u6846\u67b6\uff1a1\uff09\u53ef\u63a7\u6570\u636e\u751f\u6210\u7ba1\u9053\uff1a\u4ece\u5927\u578b\u77e5\u8bc6\u5e93\u63d0\u53d6\u5b50\u56fe\uff0c\u8fc7\u6ee4\u566a\u58f0\uff0c\u5229\u7528LLM\u751f\u6210\u5bf9\u5e94\u7684\u81ea\u7136\u6587\u672c\u63cf\u8ff0\uff1b2\uff09\u76d1\u7763\u5fae\u8c03\uff1a\u4f7f\u7528\u751f\u6210\u7684\u6570\u636e\u96c6\u5bf9\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c\u5355\u6b21\u63a8\u7406\u7684KG\u6784\u5efa\u8bad\u7ec3", "result": "\u5728CE12k\u6570\u636e\u96c6\u4e0a\uff0cInvertiTune\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u975e\u5fae\u8c03LLM\u548c\u73b0\u6709Text2KG\u65b9\u6cd5\uff1b\u5728CrossEval-1200\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "\u73b0\u5b9e\u3001\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u5bf9\u4e8e\u63a8\u8fdb\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684Text2KG\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0cInvertiTune\u6846\u67b6\u4e3a\u8fd9\u4e00\u65b9\u5411\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.03522", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03522", "abs": "https://arxiv.org/abs/2512.03522", "authors": ["Gihyeon Lee", "Jungwoo Lee", "Juwon Kim", "Young-Sik Shin", "Younggun Cho"], "title": "MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization", "comment": "Accepted in IEEE Robotics and Automation Letters (2025)", "summary": "Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6807\u7b7e\u4f3c\u7136\u7684\u8bed\u4e49\u56fe\u5339\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8bed\u4e49\u6a21\u7cca\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5168\u5c40\u5b9a\u4f4d\u95ee\u9898", "motivation": "\u5728\u672a\u77e5\u7269\u4f53\u7c7b\u522b\u548c\u8bed\u4e49\u6a21\u7cca\u7684\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u8fdb\u884c\u5168\u5c40\u5b9a\u4f4d\u65f6\uff0c\u9ad8\u8bed\u4e49\u6a21\u7cca\u5ea6\u4f1a\u52a0\u5267\u7269\u4f53\u8bef\u5206\u7c7b\u548c\u9519\u8bef\u5173\u8054\uff0c\u5bfc\u81f4\u59ff\u6001\u4f30\u8ba1\u51fa\u73b0\u663e\u8457\u8bef\u5dee", "method": "\u91c7\u7528\u591a\u6807\u7b7e\u56fe\u8868\u793a\u800c\u975e\u5355\u6807\u7b7e\u65b9\u6848\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4f3c\u7136\u4f20\u64ad\uff0c\u5c06\u6bcf\u4e2a\u8282\u70b9\u7684\u4f3c\u7136\u4e0e\u5176\u90bb\u5c45\u7684\u6700\u5927\u4f3c\u7136\u76f8\u7ed3\u5408\uff0c\u589e\u5f3a\u56fe\u95f4\u7684\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb", "result": "\u5728\u95ed\u96c6\u548c\u5f00\u96c6\u68c0\u6d4b\u914d\u7f6e\u4e0b\u8bc4\u4f30\u4e86\u6570\u636e\u5173\u8054\u548c\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u548c\u5408\u6210\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u5bf9\u5927\u8bcd\u6c47\u91cf\u7269\u4f53\u7c7b\u522b\u7684\u53ef\u6269\u5c55\u6027", "conclusion": "\u63d0\u51fa\u7684\u591a\u6807\u7b7e\u4f3c\u7136\u8bed\u4e49\u56fe\u5339\u914d\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u8bed\u4e49\u6a21\u7cca\u73af\u5883\u4e0b\u7684\u5168\u5c40\u5b9a\u4f4d\u95ee\u9898\uff0c\u5177\u6709\u8f83\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2512.03214", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.03214", "abs": "https://arxiv.org/abs/2512.03214", "authors": ["Paulina Garcia-Corral"], "title": "Identifying attributions of causality in political text", "comment": null, "summary": "Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u68c0\u6d4b\u548c\u89e3\u6790\u653f\u6cbb\u6587\u672c\u4e2d\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u56e0\u679c\u4e3b\u5f20\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u6790\u653f\u6cbb\u89e3\u91ca", "motivation": "\u89e3\u91ca\u662f\u516c\u6c11\u7406\u89e3\u653f\u6cbb\u4e16\u754c\u7684\u57fa\u672c\u8981\u7d20\uff0c\u4f46\u653f\u6cbb\u79d1\u5b66\u4e2d\u5bf9\u89e3\u91ca\u7684\u7cfb\u7edf\u5206\u6790\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u5206\u6563\u4e14\u5e38\u9488\u5bf9\u7279\u5b9a\u95ee\u9898", "method": "\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u56e0\u679c\u8bed\u8a00\u6a21\u578b\uff0c\u8fd4\u56de\u7ed3\u6784\u5316\u56e0\u679c\u4e3b\u5f20\u6570\u636e\u96c6\uff08\u539f\u56e0-\u7ed3\u679c\u5bf9\uff09\uff0c\u7528\u4e8e\u4e0b\u6e38\u5206\u6790", "result": "\u65b9\u6cd5\u5c55\u793a\u5982\u4f55\u5927\u89c4\u6a21\u7814\u7a76\u56e0\u679c\u89e3\u91ca\uff0c\u663e\u793a\u9002\u5ea6\u7684\u6807\u6ce8\u8981\u6c42\u3001\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u76f8\u5bf9\u4e8e\u4eba\u5de5\u7f16\u7801\u7684\u51c6\u786e\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u653f\u6cbb\u6587\u672c\u4e2d\u7684\u89e3\u91ca\u5206\u6790\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u53d6\u548c\u5206\u6790\u56e0\u679c\u4e3b\u5f20\uff0c\u4fc3\u8fdb\u653f\u6cbb\u79d1\u5b66\u4e2d\u5bf9\u89e3\u91ca\u7684\u6df1\u5165\u7814\u7a76"}}
{"id": "2512.03538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03538", "abs": "https://arxiv.org/abs/2512.03538", "authors": ["Yuhang Huang", "Shilong Zou", "Jiazhao Zhang", "Xinwang Liu", "Ruizhen Hu", "Kai Xu"], "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation", "comment": null, "summary": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.", "AI": {"tldr": "AdaPower\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u548c\u8bb0\u5fc6\u6301\u4e45\u5316\uff0c\u5c06\u901a\u7528\u4e16\u754c\u57fa\u7840\u6a21\u578b\u8f6c\u5316\u4e3a\u4e13\u4e1a\u4e16\u754c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7b56\u7565\u7684\u673a\u5668\u4eba\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u4e16\u754c\u57fa\u7840\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u89c6\u89c9\u52a8\u6001\u6a21\u62df\u80fd\u529b\uff0c\u4f46\u5728\u7cbe\u786e\u673a\u5668\u4eba\u63a7\u5236\u5e94\u7528\u4e2d\u5b58\u5728\u751f\u6210\u771f\u5b9e\u6027\u4e0e\u63a7\u5236\u5bfc\u5411\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u73b0\u6709\u65b9\u6cd5\u5c06WFMs\u7528\u4f5c\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u672a\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u7684VLA\u7b56\u7565\u3002", "method": "AdaPower\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3(TS-TTT)\u7528\u4e8e\u63a8\u7406\u65f6\u9002\u5e94\uff0c2) \u8bb0\u5fc6\u6301\u4e45\u5316(MP)\u7528\u4e8e\u957f\u671f\u4e00\u81f4\u6027\u4fdd\u6301\u3002\u8be5\u6846\u67b6\u96c6\u6210\u5728\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u4e2d\uff0c\u5c06\u901a\u7528WFMs\u8f6c\u5316\u4e3a\u4e13\u4e1a\u4e16\u754c\u6a21\u578b\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u8d85\u8fc741%\uff0c\u4e14\u65e0\u9700\u7b56\u7565\u91cd\u65b0\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u901a\u7528\u80fd\u529b\u3002", "conclusion": "AdaPower\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u5e94\u6709\u6548\u5f25\u5408\u4e86\u751f\u6210\u771f\u5b9e\u6027\u4e0e\u63a7\u5236\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7b56\u7565\u80fd\u591f\u5145\u5206\u5229\u7528\u4e16\u754c\u57fa\u7840\u6a21\u578b\u7684\u6a21\u62df\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u6548\u7cbe\u786e\u7684\u673a\u5668\u4eba\u63a7\u5236\u3002"}}
{"id": "2512.03310", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03310", "abs": "https://arxiv.org/abs/2512.03310", "authors": ["Kunj Joshi", "David A. Smith"], "title": "Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs", "comment": "To be submitted for ICML 2026", "summary": "The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.", "AI": {"tldr": "\u63d0\u51faRMFT\u9690\u79c1\u4fdd\u62a4\u5fae\u8c03\u6280\u672f\uff0c\u5728Enron\u90ae\u4ef6\u6570\u636e\u96c6\u4e0a\u51cf\u5c1180%\u7684PII\u8bb0\u5fc6\uff0c\u540c\u65f6\u4ec5\u589e\u52a05.73%\u7684\u56f0\u60d1\u5ea6\uff0c\u4f18\u4e8e\u53bb\u91cd\u65b9\u6cd5", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff0c\u5e26\u6765\u4e25\u91cd\u7684\u5b89\u5168\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u51cf\u5c11PII\u8bb0\u5fc6\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u7684\u5fae\u8c03\u65b9\u6cd5", "method": "\u63d0\u51fa\u968f\u673a\u63a9\u7801\u5fae\u8c03\u6280\u672f\uff0c\u901a\u8fc7\u968f\u673a\u63a9\u7801\u8bad\u7ec3\u6570\u636e\u4e2d\u7684PII\u6765\u51cf\u5c11\u6a21\u578b\u8bb0\u5fc6\uff0c\u540c\u65f6\u5f15\u5165MaxTER\u8bc4\u4f30\u6846\u67b6\u5206\u6790\u9690\u79c1-\u6548\u7528\u6743\u8861", "result": "\u5728Enron\u90ae\u4ef6\u6570\u636e\u96c6\u4e0a\uff0cRMFT\u76f8\u6bd4\u57fa\u7ebf\u5fae\u8c03\u51cf\u5c1180.81%\u7684\u603b\u63d0\u53d6\u7387\u548c80.17%\u7684\u5df2\u89c1\u63d0\u53d6\u7387\uff0c\u56f0\u60d1\u5ea6\u4ec5\u589e\u52a05.73%\uff0c\u4f18\u4e8e\u53bb\u91cd\u65b9\u6cd5", "conclusion": "RMFT\u662f\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u5fae\u8c03\u6280\u672f\uff0c\u80fd\u663e\u8457\u51cf\u5c11PII\u8bb0\u5fc6\u540c\u65f6\u6700\u5c0f\u5316\u6027\u80fd\u5f71\u54cd\uff0cMaxTER\u6846\u67b6\u4e3a\u8bc4\u4f30\u9690\u79c1-\u6548\u7528\u6743\u8861\u63d0\u4f9b\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u65b9\u6cd5"}}
{"id": "2512.03548", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03548", "abs": "https://arxiv.org/abs/2512.03548", "authors": ["Zexin Lin", "Yebin Zhong", "Hanwen Wan", "Jiu Cheng", "Zhenglong Sun", "Xiaoqiang Ji"], "title": "A Learning-based Control Methodology for Transitioning VTOL UAVs", "comment": null, "summary": "Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8026\u5408\u8fc7\u6e21\u63a7\u5236\u65b9\u6cd5ST3M\uff0c\u5c06\u5de1\u822a\u6a21\u5f0f\u89c6\u4e3a\u60ac\u505c\u7279\u4f8b\uff0c\u6709\u6548\u51cf\u5c11VTOL\u65e0\u4eba\u673a\u8fc7\u6e21\u8fc7\u7a0b\u4e2d\u7684\u632f\u52a8\u5e76\u63d0\u5347\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "VTOL\u65e0\u4eba\u673a\u5728\u8fc7\u6e21\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u503e\u659c\u8f6c\u5b50\u673a\u5236\u5bfc\u81f4\u91cd\u5fc3\u548c\u63a8\u529b\u65b9\u5411\u53d8\u5316\uff0c\u73b0\u6709\u89e3\u8026\u63a7\u5236\u65b9\u6cd5\u4f1a\u4ea7\u751f\u663e\u8457\u632f\u52a8\uff0c\u4e14\u7f3a\u4e4f\u4ea4\u4e92\u8003\u8651\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8026\u5408\u8fc7\u6e21\u63a7\u5236\u65b9\u6cd5ST3M\uff0c\u5c06\u5de1\u822a\u6a21\u5f0f\u89c6\u4e3a\u60ac\u505c\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u91c7\u7528\u65b0\u7684\u63a7\u5236\u89c6\u89d2\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a7\u5236\u5668\u5f00\u53d1\u548c\u8fc1\u79fb\uff0c\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u65e0\u4eba\u673a\u4f4d\u7f6e\u548c\u59ff\u6001\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u548c\u51cf\u5c11\u7684\u8fc7\u6e21\u632f\u52a8\u3002", "conclusion": "ST3M\u65b9\u6cd5\u4e3aVTOL\u65e0\u4eba\u673a\u8fc7\u6e21\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u4f20\u7edf\u76f8\u4f4d\u8fc7\u6e21\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2512.03334", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03334", "abs": "https://arxiv.org/abs/2512.03334", "authors": ["Nemika Tyagi", "Nelvin Licona Guevara", "Olga Kellert"], "title": "Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaran\u00ed", "comment": "10 pages, 4 figures", "summary": "This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaran\u00ed. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaran\u00ed dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaran\u00ed and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.", "AI": {"tldr": "LLM\u8f85\u52a9\u6807\u6ce8\u7ba1\u9053\u7528\u4e8e\u897f\u73ed\u7259\u8bed-\u82f1\u8bed\u548c\u897f\u73ed\u7259\u8bed-\u74dc\u62c9\u5c3c\u8bed\u53cc\u8bed\u8bdd\u8bed\u7684\u793e\u4f1a\u8bed\u8a00\u5b66\u548c\u4e3b\u9898\u5206\u6790\uff0c\u81ea\u52a8\u6807\u6ce8\u8bdd\u9898\u3001\u4f53\u88c1\u548c\u8bed\u7528\u529f\u80fd\uff0c\u63ed\u793a\u6027\u522b\u3001\u8bed\u8a00\u4f18\u52bf\u548c\u8bed\u7528\u529f\u80fd\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5173\u8054\u3002", "motivation": "\u4f20\u7edf\u7684\u793e\u4f1a\u8bed\u8a00\u5b66\u6a21\u5f0f\u5206\u6790\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u8bed\u6599\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u53ef\u9760\u5730\u6062\u590d\u4f20\u7edf\u4e0a\u53ea\u80fd\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u83b7\u5f97\u7684\u53ef\u89e3\u91ca\u793e\u4f1a\u8bed\u8a00\u5b66\u6a21\u5f0f\uff0c\u63a8\u8fdb\u8de8\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u53cc\u8bed\u7814\u7a76\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\uff0c\u5bf9\u4e24\u79cd\u7c7b\u578b\u5b66\u4e0d\u540c\u7684\u53cc\u8bed\u8bed\u5883\uff08\u897f\u73ed\u7259\u8bed-\u82f1\u8bed\u548c\u897f\u73ed\u7259\u8bed-\u74dc\u62c9\u5c3c\u8bed\uff09\u4e2d\u76843,691\u4e2a\u8bed\u7801\u8f6c\u6362\u53e5\u5b50\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8\uff0c\u5305\u62ec\u8bdd\u9898\u3001\u4f53\u88c1\u548c\u8bdd\u8bed\u8bed\u7528\u529f\u80fd\u3002\u6574\u5408\u8fc8\u963f\u5bc6\u53cc\u8bed\u8bed\u6599\u5e93\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u5143\u6570\u636e\uff0c\u5e76\u4e3a\u897f\u73ed\u7259\u8bed-\u74dc\u62c9\u5c3c\u8bed\u6570\u636e\u96c6\u6dfb\u52a0\u65b0\u7684\u4e3b\u9898\u6807\u6ce8\u3002", "result": "\u5728\u8fc8\u963f\u5bc6\u6570\u636e\u4e2d\u53d1\u73b0\u4e86\u6027\u522b\u3001\u8bed\u8a00\u4f18\u52bf\u548c\u8bdd\u8bed\u529f\u80fd\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u8054\u7cfb\uff1b\u5728\u5df4\u62c9\u572d\u6587\u672c\u4e2d\u89c2\u5bdf\u5230\u6b63\u5f0f\u74dc\u62c9\u5c3c\u8bed\u548c\u975e\u6b63\u5f0f\u897f\u73ed\u7259\u8bed\u4e4b\u95f4\u7684\u660e\u663e\u53cc\u8bed\u5206\u5de5\u3002\u8fd9\u4e9b\u53d1\u73b0\u4ee5\u524d\u53ea\u80fd\u901a\u8fc7\u4e92\u52a8\u548c\u793e\u4f1a\u8bed\u8a00\u5b66\u89c2\u5bdf\u83b7\u5f97\uff0c\u73b0\u5728\u901a\u8fc7\u8bed\u6599\u5e93\u89c4\u6a21\u7684\u5b9a\u91cf\u8bc1\u636e\u5f97\u5230\u4e86\u590d\u5236\u548c\u6269\u5c55\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u53ef\u9760\u5730\u6062\u590d\u4f20\u7edf\u4e0a\u53ea\u80fd\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u83b7\u5f97\u7684\u53ef\u89e3\u91ca\u793e\u4f1a\u8bed\u8a00\u5b66\u6a21\u5f0f\uff0c\u4e3a\u8de8\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u53cc\u8bed\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8ba1\u7b97\u793e\u4f1a\u8bed\u8a00\u5b66\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.03556", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03556", "abs": "https://arxiv.org/abs/2512.03556", "authors": ["Yinzhou Tang", "Yu Shang", "Yinuo Chen", "Bingwen Wei", "Xin Zhang", "Shu'ang Yu", "Liangzhi Shi", "Chao Yu", "Chen Gao", "Wei Wu", "Yong Li"], "title": "RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL", "comment": null, "summary": "Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.", "AI": {"tldr": "RoboScape-R\u5229\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u901a\u7528\u73af\u5883\u4ee3\u7406\uff0c\u901a\u8fc7\u5185\u751f\u5956\u52b1\u673a\u5236\u589e\u5f3a\u5177\u8eab\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u8de8\u573a\u666f\u4efb\u52a1\u4e2d\u6bd4\u57fa\u7ebf\u63d0\u534737.5%\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5177\u8eab\u667a\u80fd\u7b56\u7565\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff1a\u6a21\u4eff\u5b66\u4e60\u5bb9\u6613\u8fc7\u62df\u5408\u4e13\u5bb6\u8f68\u8ff9\uff0c\u5f3a\u5316\u5b66\u4e60\u7f3a\u4e4f\u7edf\u4e00\u7684\u901a\u7528\u5956\u52b1\u4fe1\u53f7\u3002\u73b0\u6709\u4e16\u754c\u6a21\u578b\u867d\u7136\u80fd\u9884\u6d4b\u89c2\u6d4b\uff0c\u4f46\u4ecd\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u624b\u5de5\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u6cd5\u63d0\u4f9b\u771f\u6b63\u7684\u901a\u7528\u8bad\u7ec3\u73af\u5883\u3002", "method": "\u63d0\u51faRoboScape-R\u6846\u67b6\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3aRL\u8303\u5f0f\u4e2d\u7684\u901a\u7528\u73af\u5883\u4ee3\u7406\u3002\u521b\u65b0\u6027\u5730\u5f15\u5165\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5185\u751f\u5956\u52b1\u673a\u5236\uff0c\u8be5\u5956\u52b1\u6e90\u81ea\u6a21\u578b\u5bf9\u771f\u5b9e\u4e16\u754c\u72b6\u6001\u8f6c\u79fb\u52a8\u6001\u7684\u5185\u5728\u7406\u89e3\uff0c\u800c\u975e\u624b\u5de5\u8bbe\u8ba1\u7684\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRoboScape-R\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u901a\u7528\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5177\u8eab\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u57df\u5916\u573a\u666f\u4e0b\uff0c\u5e73\u5747\u6027\u80fd\u6bd4\u57fa\u7ebf\u63d0\u534737.5%\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u5728\u7ebf\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u5de5\u5177\uff0cRoboScape-R\u901a\u8fc7\u5185\u751f\u5956\u52b1\u673a\u5236\u4e3a\u5177\u8eab\u667a\u80fd\u7684\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u901a\u7528\u73af\u5883\u4ee3\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.03340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03340", "abs": "https://arxiv.org/abs/2512.03340", "authors": ["Rohan Charudatt Salvi", "Chirag Chawla", "Dhruv Jain", "Swapnil Panigrahi", "Md Shad Akhtar", "Shweta Yadav"], "title": "PERCS: Persona-Guided Controllable Biomedical Summarization Dataset", "comment": "9 pages, 4 figures, 6 tables", "summary": "Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.", "AI": {"tldr": "PERCS\u6570\u636e\u96c6\uff1a\u9488\u5bf9\u56db\u79cd\u4e0d\u540c\u533b\u5b66\u77e5\u8bc6\u6c34\u5e73\u7528\u6237\uff08\u666e\u901a\u5927\u4f17\u3001\u533b\u5b66\u751f\u3001\u975e\u533b\u5b66\u7814\u7a76\u8005\u3001\u533b\u5b66\u4e13\u5bb6\uff09\u7684\u4e2a\u6027\u5316\u751f\u7269\u533b\u5b66\u6458\u8981\u7b80\u5316\u6570\u636e\u96c6\uff0c\u652f\u6301\u53ef\u63a7\u6458\u8981\u751f\u6210\u7814\u7a76", "motivation": "\u73b0\u6709\u533b\u5b66\u6587\u672c\u7b80\u5316\u8d44\u6e90\u901a\u5e38\u5047\u8bbe\u5355\u4e00\u901a\u7528\u53d7\u4f17\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u5728\u533b\u5b66\u7d20\u517b\u548c\u4fe1\u606f\u9700\u6c42\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u53d7\u4f17\u7684\u4e2a\u6027\u5316\u7b80\u5316\u65b9\u6cd5", "method": "\u521b\u5efaPERCS\u6570\u636e\u96c6\uff0c\u5305\u542b\u751f\u7269\u533b\u5b66\u6458\u8981\u53ca\u5176\u9488\u5bf9\u56db\u79cd\u4e0d\u540c\u4eba\u7269\u89d2\u8272\uff08\u666e\u901a\u5927\u4f17\u3001\u533b\u5b66\u751f\u3001\u975e\u533b\u5b66\u7814\u7a76\u8005\u3001\u533b\u5b66\u4e13\u5bb6\uff09\u7684\u7b80\u5316\u7248\u672c\uff1b\u7531\u533b\u751f\u6839\u636e\u8be6\u7ec6\u7684\u9519\u8bef\u5206\u7c7b\u6cd5\u5ba1\u6838\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u89d2\u8272\u5bf9\u9f50\u5ea6", "result": "\u6280\u672f\u9a8c\u8bc1\u663e\u793a\u4e0d\u540c\u89d2\u8272\u5728\u53ef\u8bfb\u6027\u3001\u8bcd\u6c47\u548c\u5185\u5bb9\u6df1\u5ea6\u4e0a\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff1b\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u56db\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u5728PERCS\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u8861\u91cf\u5168\u9762\u6027\u3001\u53ef\u8bfb\u6027\u548c\u5fe0\u5b9e\u5ea6", "conclusion": "PERCS\u6570\u636e\u96c6\u652f\u6301\u4e2a\u6027\u5316\u533b\u5b66\u6c9f\u901a\u548c\u53ef\u63a7\u751f\u7269\u533b\u5b66\u6458\u8981\u7814\u7a76\uff0c\u516c\u5f00\u63d0\u4f9b\u6570\u636e\u96c6\u3001\u6807\u6ce8\u6307\u5357\u548c\u8bc4\u4f30\u6750\u6599\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5efa\u7acb\u57fa\u7ebf\u7ed3\u679c"}}
{"id": "2512.03630", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03630", "abs": "https://arxiv.org/abs/2512.03630", "authors": ["Shifa Sulaiman", "Amarnath H", "Simon Bogh", "Naresh Marturi"], "title": "Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations", "comment": null, "summary": "Motion planning schemes are used for planning motions of a manipulator from an initial pose to a final pose during a task execution. A motion planning scheme generally comprises of a trajectory planning method and an inverse kinematic solver to determine trajectories and joints solutions respectively. In this paper, 3 motion planning schemes developed based on Jacobian methods are implemented to traverse a redundant manipulator with a coupled finger gripper through given trajectories. RRT* algorithm is used for planning trajectories and screw theory based forward kinematic equations are solved for determining joint solutions of the manipulator and gripper. Inverse solutions are computed separately using 3 Jacobian based methods such as Jacobian Transpose (JT), Pseudo Inverse (PI), and Damped Least Square (DLS) methods. Space Jacobian and manipulability measurements of the manipulator and gripper are obtained using screw theory formulations. Smoothness and RMSE error of generated trajectories and velocity continuity, acceleration profile, jerk, and snap values of joint motions are analysed for determining an efficient motion planning method for a given task. Advantages and disadvantages of the proposed motion planning schemes mentioned above are analysed using simulation studies to determine a suitable inverse solution technique for the tasks.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u96c5\u53ef\u6bd4\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e09\u79cd\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u7528\u4e8e\u5197\u4f59\u673a\u68b0\u81c2\u4e0e\u8026\u5408\u624b\u6307\u5939\u722a\u7684\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u6bd4\u8f83\u4e86JT\u3001PI\u548cDLS\u4e09\u79cd\u9006\u89e3\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u5197\u4f59\u673a\u68b0\u81c2\u4e0e\u8026\u5408\u624b\u6307\u5939\u722a\u7cfb\u7edf\u5bfb\u627e\u9ad8\u6548\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u96c5\u53ef\u6bd4\u9006\u89e3\u65b9\u6cd5\u5728\u8f68\u8ff9\u5e73\u6ed1\u6027\u3001\u8bef\u5dee\u7b49\u65b9\u9762\u7684\u8868\u73b0\uff0c\u786e\u5b9a\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u9006\u89e3\u6280\u672f\u3002", "method": "\u4f7f\u7528RRT*\u7b97\u6cd5\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\uff0c\u57fa\u4e8e\u87ba\u65cb\u7406\u8bba\u5efa\u7acb\u6b63\u8fd0\u52a8\u5b66\u65b9\u7a0b\uff0c\u5206\u522b\u91c7\u7528\u96c5\u53ef\u6bd4\u8f6c\u7f6e(JT)\u3001\u4f2a\u9006(PI)\u548c\u963b\u5c3c\u6700\u5c0f\u4e8c\u4e58(DLS)\u4e09\u79cd\u65b9\u6cd5\u8ba1\u7b97\u9006\u89e3\uff0c\u5e76\u901a\u8fc7\u87ba\u65cb\u7406\u8bba\u83b7\u5f97\u7a7a\u95f4\u96c5\u53ef\u6bd4\u548c\u53ef\u64cd\u4f5c\u6027\u5ea6\u91cf\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u5206\u6790\u4e86\u751f\u6210\u8f68\u8ff9\u7684\u5e73\u6ed1\u6027\u548cRMSE\u8bef\u5dee\uff0c\u4ee5\u53ca\u5173\u8282\u8fd0\u52a8\u7684\u8fde\u7eed\u6027\u3001\u52a0\u901f\u5ea6\u66f2\u7ebf\u3001\u6025\u52a8\u5ea6\u548c\u51b2\u51fb\u503c\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u786e\u5b9a\u4e86\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u9006\u89e3\u6280\u672f\uff0c\u5206\u6790\u4e86\u4e09\u79cd\u96c5\u53ef\u6bd4\u9006\u89e3\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u5197\u4f59\u673a\u68b0\u81c2\u4e0e\u8026\u5408\u5939\u722a\u7cfb\u7edf\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2512.03343", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03343", "abs": "https://arxiv.org/abs/2512.03343", "authors": ["Darshan Fofadiya"], "title": "Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning", "comment": "Code available at https://github.com/DarshanFofadiya/idea-gated-transformers/tree/main", "summary": "Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \\citep{holtzman2019curious}. While scaling model size mitigates this \\citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.", "AI": {"tldr": "\u63d0\u51faIdea-Gated Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u79bb\u8bed\u4e49\u89c4\u5212\u548c\u8bed\u6cd5\u751f\u6210\u6765\u89e3\u51b3\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\"\u4e3b\u9898\u6f02\u79fb\"\u95ee\u9898\uff0c\u4f7f\u7528\u6982\u5ff5\u5411\u91cf\u5b9e\u65f6\u95e8\u63a7\u8bcd\u6c47\u9009\u62e9\uff0c\u63d0\u9ad8\u751f\u6210\u5185\u5bb9\u7684\u4e3b\u9898\u4e00\u81f4\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u8bad\u7ec3\uff0c\u5bb9\u6613\u4ea7\u751f\"\u4e3b\u9898\u6f02\u79fb\"\u95ee\u9898\uff0c\u5373\u751f\u6210\u5185\u5bb9\u4f1a\u9010\u6e10\u504f\u79bb\u521d\u59cb\u63d0\u793a\uff0c\u8fd9\u662f\u56e0\u4e3a\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u5c40\u90e8\u5173\u8054\u800c\u975e\u5168\u5c40\u89c4\u5212\u3002\u867d\u7136\u589e\u5927\u6a21\u578b\u89c4\u6a21\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u76ee\u6807\u672c\u8eab\u7684\u77ed\u89c6\u6027\u4f9d\u7136\u5b58\u5728\u3002", "method": "\u63d0\u51faIdea-Gated Transformer\u67b6\u6784\uff0c\u5f15\u5165\u8f85\u52a9\u7684\"Idea Head\"\u6765\u9884\u6d4b\u672a\u6765\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u8bcd\u888b\u5206\u5e03\uff0c\u751f\u6210\u6f5c\u5728\u7684\"\u6982\u5ff5\u5411\u91cf\"\u3002\u8be5\u5411\u91cf\u901a\u8fc7\u53ef\u5fae\u5206\u95e8\u63a7\u673a\u5236\u4e3b\u52a8\u95e8\u63a7\u4e3b\u8981\u8bcd\u6c47\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u9009\u62e9\uff0c\u6291\u5236\u8bed\u4e49\u4e0d\u76f8\u5173\u7684\u8bcd\u6c47\uff0c\u5b9e\u65f6\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5728WikiText-103\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIdea-Gated\u6a21\u578b\u5728\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u4e0a\u4e0e\u6807\u51c6GPT-2\u57fa\u7ebf\u76f8\u5f53\uff0c\u4f46\u5728\u9886\u57df\u4fdd\u6301\u6027\u65b9\u9762\u663e\u8457\u66f4\u4f18\u3002\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u663e\u793a\uff0c\u95e8\u63a7\u673a\u5236\u6210\u529f\u5c06\u751f\u6210\u9501\u5b9a\u5728\u7279\u5b9a\u8bed\u4e49\u7c07\uff08\u5982\u91d1\u878d\u3001\u79d1\u5b66\uff09\u4e2d\uff0c\u6709\u6548\u62b5\u6297\u5173\u8054\u6f02\u79fb\u3002", "conclusion": "Idea-Gated Transformer\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u66f4\u53ef\u63a7\u7684\u8bed\u8a00\u5efa\u6a21\uff0c\u901a\u8fc7\u5206\u79bb\u8bed\u4e49\u89c4\u5212\u548c\u8bed\u6cd5\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e3b\u9898\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u66f4\u53ef\u63a7\u7684\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.03639", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03639", "abs": "https://arxiv.org/abs/2512.03639", "authors": ["Kilian Schweppe", "Anne-Kathrin Schmuck"], "title": "Context-Triggered Contingency Games for Strategic Multi-Agent Interaction", "comment": null, "summary": "We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6218\u7565\u6e38\u620f\u4e0e\u52a8\u6001\u5e94\u6025\u6e38\u620f\u7684\u4e24\u5c42\u67b6\u6784\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u89e6\u53d1\u7684\u5e94\u6025\u6e38\u620f\u5b9e\u73b0\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u53ef\u9760\u9ad8\u6548\u7684\u4ea4\u4e92", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u957f\u671f\u6218\u7565\u76ee\u6807\u4e0e\u77ed\u671f\u52a8\u6001\u9002\u5e94\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u4e0d\u786e\u5b9a\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u8fdb\u5c55", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u89e6\u53d1\u7684\u5e94\u6025\u6e38\u620f\uff0c\u5c06\u57fa\u4e8e\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u7684\u6218\u7565\u6e38\u620f\u4e0e\u5b9e\u65f6\u89e3\u51b3\u7684\u52a8\u6001\u5e94\u6025\u6e38\u620f\u76f8\u7ed3\u5408\uff0c\u91c7\u7528\u4e24\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42\u4f7f\u7528\u7b56\u7565\u6a21\u677f\u4fdd\u8bc1\u76ee\u6807\u6ee1\u8db3\uff0c\u5e95\u5c42\u4f7f\u7528\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u65b0\u6c42\u89e3\u5668\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u6a21\u578b\u9884\u6d4b\u63a7\u5236", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u5bfc\u822a\u7684\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u3001\u53ef\u9760\u548c\u81ea\u9002\u5e94\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u80fd\u529b", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u7684\u4ea4\u4e92\u73af\u5883\u4e2d\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u8fdb\u5c55\uff0c\u4e3a\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.03360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03360", "abs": "https://arxiv.org/abs/2512.03360", "authors": ["Qingchuan Li", "Mingyue Cheng", "Zirui Liu", "Daoyu Wang", "Yuting Zeng", "Tongxuan Liu"], "title": "From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation", "comment": "Accepted by AAAI2026", "summary": "Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.", "AI": {"tldr": "HBLR\u63d0\u51fa\u5047\u8bbe\u9a71\u52a8\u7684\u5411\u540e\u903b\u8f91\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7f6e\u4fe1\u611f\u77e5\u7b26\u53f7\u7ffb\u8bd1\u548c\u5047\u8bbe\u9a71\u52a8\u5411\u540e\u63a8\u7406\uff0c\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u53cc\u91cd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5f53\u524d\u57fa\u4e8e\u524d\u5411\u63a8\u7406\u7684\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u8def\u5f84\u5197\u4f59\u3001\u6b65\u9aa4\u5e7b\u89c9\u548c\u8bed\u4e49\u6f02\u79fb\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0d\u53ef\u9760\u3002", "method": "HBLR\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u7f6e\u4fe1\u611f\u77e5\u7b26\u53f7\u7ffb\u8bd1\uff0c\u4ec5\u5c06\u9ad8\u7f6e\u4fe1\u5ea6\u7247\u6bb5\u8f6c\u6362\u4e3aFOL\u7b49\u903b\u8f91\u5f62\u5f0f\uff0c\u4e0d\u786e\u5b9a\u5185\u5bb9\u4fdd\u7559\u4e3a\u81ea\u7136\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u7ffb\u8bd1\u53cd\u601d\u6a21\u5757\u786e\u4fdd\u8bed\u4e49\u4fdd\u771f\u5ea6\uff1b2) \u5047\u8bbe\u9a71\u52a8\u5411\u540e\u63a8\u7406\uff0c\u6a21\u62df\u4eba\u7c7b\u6f14\u7ece\u601d\u7ef4\uff0c\u5047\u8bbe\u7ed3\u8bba\u4e3a\u771f\u5e76\u9012\u5f52\u9a8c\u8bc1\u5176\u524d\u63d0\uff0c\u901a\u8fc7\u63a8\u7406\u53cd\u601d\u6a21\u5757\u8bc6\u522b\u548c\u4fee\u6b63\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHBLR\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HBLR\u901a\u8fc7\u6574\u5408\u7f6e\u4fe1\u611f\u77e5\u7b26\u53f7\u7ffb\u8bd1\u548c\u5047\u8bbe\u9a71\u52a8\u5411\u540e\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u524d\u5411\u63a8\u7406\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u903b\u8f91\u63a8\u7406\u3002"}}
{"id": "2512.03684", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03684", "abs": "https://arxiv.org/abs/2512.03684", "authors": ["Shahid Ansari", "Mahendra Kumar Gohil", "Yusuke Maeda", "Bishakh Bhattacharya"], "title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection", "comment": null, "summary": "This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8f6f\u8f85\u52a9\u624b\u6307\u4e0e\u521a\u6027\u5916\u9aa8\u9abc\u7684\u6df7\u5408\u5939\u722a\u7cfb\u7edf\uff0c\u7528\u4e8e\u756a\u8304\u81ea\u4e3b\u91c7\u6458\uff0c\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u548c\u529b\u63a7\u5b9e\u73b0\u8f7b\u67d4\u6293\u53d6\uff0c\u5e73\u5747\u91c7\u6458\u5468\u671f24.34\u79d2\uff0c\u6210\u529f\u7387\u7ea680%", "motivation": "\u73b0\u6709\u756a\u8304\u91c7\u6458\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u906e\u6321\u3001\u5149\u7167\u53d8\u5316\uff0c\u5e76\u5b9e\u73b0\u8f7b\u67d4\u6293\u53d6\u4ee5\u907f\u514d\u679c\u5b9e\u635f\u4f24\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u5728\u6742\u4e71\u73af\u5883\u4e2d\u53ef\u9760\u5de5\u4f5c\u7684\u81ea\u4e3b\u91c7\u6458\u7cfb\u7edf\u3002", "method": "1. \u6df7\u5408\u5939\u722a\u8bbe\u8ba1\uff1a\u7ed3\u54086\u4e2a\u8f6f\u8f85\u52a9\u624b\u6307\u3001\u521a\u6027\u5916\u9aa8\u9abc\u548c\u4e73\u80f6\u7bee\u7b50\uff0c\u5b9e\u73b0\u7b3c\u5f0f\u6293\u53d6\uff1b2. \u4f3a\u670d\u9a71\u52a8\u7684Scotch-yoke\u673a\u6784\uff1b3. \u5206\u79bb\u53f6\u7247\u5f62\u6210\u9525\u5f62\u622a\u5934\u4f53\u9694\u79bb\u679c\u5b9e\uff1b4. \u96c6\u6210\u5fae\u4f3a\u670d\u5207\u5272\u5668\uff1b5. \u57fa\u4e8eRGB-D\u76f8\u673a\u548cDetectron2\u7684\u89c6\u89c9\u7ba1\u9053\uff0c\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u548c\u5173\u952e\u70b9\u5b9a\u4f4d\uff1b6. \u57fa\u4e8e\u865a\u529f\u539f\u7406\u7684\u5206\u6790\u6a21\u578b\u5173\u8054\u4f3a\u670d\u626d\u77e9\u4e0e\u6293\u53d6\u529b\uff1b7. \u4f7f\u7528PID\u63a7\u5236\u5668\u548c\u529b\u654f\u7535\u963b\u5b9e\u73b0\u95ed\u73af\u529b\u63a7\uff1b8. \u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\u76845\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u8f68\u8ff9\u89c4\u5212", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u91c7\u6458\u5468\u671f\uff08\u63a5\u8fd1\u3001\u5206\u79bb\u3001\u5207\u5272\u3001\u6293\u53d6\u3001\u8fd0\u8f93\u3001\u91ca\u653e\uff09\uff0c\u5e73\u5747\u5468\u671f\u65f6\u95f424.34\u79d2\uff0c\u603b\u4f53\u6210\u529f\u7387\u7ea680%\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u6293\u53d6\u529b\uff080.20-0.50N\uff09\u3002\u7cfb\u7edf\u5728\u6742\u4e71\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u5939\u722a\u8bbe\u8ba1\u548c\u96c6\u6210\u7684\u89c6\u89c9-\u63a7\u5236\u7ba1\u9053\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u756a\u8304\u91c7\u6458\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u4fdd\u6301\u4f4e\u6293\u53d6\u529b\u540c\u65f6\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u7684\u6709\u6548\u6027\uff0c\u4e3a\u519c\u4e1a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03377", "abs": "https://arxiv.org/abs/2512.03377", "authors": ["Hanting Chen", "Chu Zhong", "Kai Han", "Yuchuan Tian", "Yuchen Liang", "Tianyu Guo", "Xinghao Chen", "Dacheng Tao", "Yunhe Wang"], "title": "Nexus: Higher-Order Attention Mechanisms in Transformers", "comment": null, "summary": "Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \\textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \\textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u9ad8\u9636\u6ce8\u610f\u529b\u7f51\u7edc(Hon)\uff0c\u901a\u8fc7\u9012\u5f52\u6846\u67b6\u589e\u5f3aTransformer\u7684\u8868\u793a\u80fd\u529b\uff0c\u6253\u7834\u6807\u51c6\u6ce8\u610f\u529b\u7684\u4f4e\u79e9\u74f6\u9888\uff0c\u4ee5\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u6355\u6349\u591a\u8df3\u5173\u7cfb\u3002", "motivation": "\u6807\u51c6Transformer\u7684\u4e00\u9636\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u4f4e\u79e9\u74f6\u9888\uff0c\u96be\u4ee5\u5728\u5355\u5c42\u5185\u6355\u6349\u590d\u6742\u7684\u591a\u8df3\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u9ad8\u9636\u6ce8\u610f\u529b\u7f51\u7edc(Hon)\uff0c\u901a\u8fc7\u9012\u5f52\u6846\u67b6\u52a8\u6001\u7cbe\u70bc\u67e5\u8be2\u548c\u952e\u5411\u91cf\uff1a\u67e5\u8be2\u548c\u952e\u5411\u91cf\u672c\u8eab\u662f\u5185\u90e8\u6ce8\u610f\u529b\u5faa\u73af\u7684\u8f93\u51fa\uff0c\u5141\u8bb8token\u5728\u6700\u7ec8\u6ce8\u610f\u529b\u8ba1\u7b97\u524d\u805a\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u5e76\u5efa\u6a21\u9ad8\u9636\u76f8\u5173\u6027\u3002\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u6743\u91cd\u5171\u4eab\u7b56\u7565\uff0c\u786e\u4fdd\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u53ea\u589e\u52a0O(1)\u53c2\u6570\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u6253\u7834\u4e86\u6807\u51c6\u6ce8\u610f\u529b\u7684\u7ebf\u6027\u74f6\u9888\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHon\u4f18\u4e8e\u6807\u51c6Transformer\u3002", "conclusion": "\u9ad8\u9636\u6ce8\u610f\u529b\u7f51\u7edc\u901a\u8fc7\u9012\u5f52\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86Transformer\u7684\u8868\u793a\u80fd\u529b\uff0c\u4ee5\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u89e3\u51b3\u4e86\u6807\u51c6\u6ce8\u610f\u529b\u7684\u4f4e\u79e9\u74f6\u9888\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.03707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03707", "abs": "https://arxiv.org/abs/2512.03707", "authors": ["Sundas Rafat Mulkana", "Ronyu Yu", "Tanaya Guha", "Emma Li"], "title": "ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration", "comment": "8 pages, 7 figures", "summary": "In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\\% with a high task success rate of 87.7\\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.", "AI": {"tldr": "ContactRL\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u529b\u53cd\u9988\u5c06\u63a5\u89e6\u5b89\u5168\u76f4\u63a5\u7eb3\u5165\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u673a\u5668\u4eba\u5b66\u4e60\u81ea\u9002\u5e94\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6548\u7387\u7684\u540c\u65f6\u6700\u5c0f\u5316\u4eba\u673a\u63a5\u89e6\u529b\u3002", "motivation": "\u5728\u534f\u4f5c\u4eba\u673a\u4efb\u52a1\u4e2d\uff0c\u5b89\u5168\u4e0d\u4ec5\u9700\u8981\u907f\u514d\u78b0\u649e\uff0c\u8fd8\u9700\u8981\u786e\u4fdd\u5b89\u5168\u3001\u6709\u610f\u7684\u7269\u7406\u63a5\u89e6\u3002\u5f53\u524d\u65b9\u6cd5\u5728\u63a5\u89e6\u5b89\u5168\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u63a5\u89e6\u5b89\u5168\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51faContactRL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u529b\u53cd\u9988\u5c06\u63a5\u89e6\u5b89\u5168\u76f4\u63a5\u7eb3\u5165\u5956\u52b1\u51fd\u6570\u3002\u540c\u65f6\uff0c\u4e3a\u4fdd\u969c\u90e8\u7f72\u5b89\u5168\uff0c\u4f7f\u7528\u57fa\u4e8e\u52a8\u80fd\u7684\u63a7\u5236\u5c4f\u969c\u51fd\u6570(eCBF)\u5c4f\u853d\u5668\u589e\u5f3a\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u4eff\u771f\u4e2dContactRL\u8fbe\u52300.2%\u7684\u5b89\u5168\u8fdd\u89c4\u7387\u548c87.7%\u7684\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7ea6\u675fRL\u57fa\u7ebf\u3002\u5728UR3e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684360\u6b21\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793a\uff0c\u6d4b\u91cf\u7684\u6cd5\u5411\u529b\u59cb\u7ec8\u4f4e\u4e8e10N\uff0c\u786e\u8ba4\u4e86\u5b89\u5168\u63a5\u89e6\u3002", "conclusion": "ContactRL\u5b9e\u73b0\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u7269\u7406\u534f\u4f5c\uff0c\u63a8\u52a8\u4e86\u534f\u4f5c\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u90e8\u7f72\u5e94\u7528\u3002"}}
{"id": "2512.03381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03381", "abs": "https://arxiv.org/abs/2512.03381", "authors": ["Nicholas Tomlin", "Naitian Zhou", "Eve Fleisig", "Liangyuan", "Chen", "T\u00e9a Wright", "Lauren Vinh", "Laura X. Ma", "Seun Eisape", "Ellie French", "Tingting Du", "Tianjiao Zhang", "Alexander Koller", "Alane Suhr"], "title": "Characterizing Language Use in a Collaborative Situated Game", "comment": null, "summary": "Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.", "AI": {"tldr": "\u6536\u96c6\u4e86Portal 2\u5408\u4f5c\u6a21\u5f0f\u4e2d11.5\u5c0f\u65f6\u7684\u53e3\u8bed\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u5305\u542b24.5K\u8bdd\u8bed\uff0c\u5206\u6790\u4e86\u590d\u6742\u7a7a\u95f4\u6307\u4ee3\u7b49\u72ec\u7279\u8bed\u8a00\u73b0\u8c61\uff0c\u5e76\u516c\u5f00\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "motivation": "\u5408\u4f5c\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u8bed\u8a00\u6570\u636e\u5177\u6709\u4e30\u5bcc\u6027\uff0c\u5305\u542b\u590d\u6742\u73af\u5883\u4e0b\u7684\u534f\u8c03\u6c9f\u901a\u548c\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u8fd9\u4e9b\u73b0\u8c61\u5728\u73b0\u6709\u95f2\u804a\u6216\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u8bed\u6599\u5e93\u4e2d\u5f88\u5c11\u51fa\u73b0\u3002", "method": "\u6536\u96c6Portal 2\u5408\u4f5c\u6a21\u5f0f\u4e2d\u73a9\u5bb6\u5bf9\u8bdd\uff0c\u6784\u5efa\u5305\u542b11.5\u5c0f\u65f6\u97f3\u9891\u300124.5K\u8bdd\u8bed\u7684\u8bed\u6599\u5e93\uff0c\u5206\u6790\u73a9\u5bb6\u8bed\u8a00\u884c\u4e3a\uff0c\u8bc6\u522b\u72ec\u7279\u8bed\u8a00\u73b0\u8c61\uff0c\u5e76\u63d0\u4f9b\u591a\u6a21\u6001\u6570\u636e\u6807\u6ce8\u3002", "result": "\u521b\u5efa\u4e86Portal\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u8bc6\u522b\u51fa\u590d\u6742\u7a7a\u95f4\u6307\u4ee3\u3001\u6f84\u6e05\u4e0e\u4fee\u590d\u3001\u4e34\u65f6\u7ea6\u5b9a\u5f62\u6210\u7b49\u72ec\u7279\u8bed\u8a00\u73b0\u8c61\uff0c\u8fd9\u4e9b\u5728\u73b0\u6709\u5bf9\u8bdd\u8bed\u6599\u5e93\u4e2d\u5f88\u5c11\u89c1\u3002", "conclusion": "\u5408\u4f5c\u89c6\u9891\u6e38\u620f\u662f\u7814\u7a76\u590d\u6742\u60c5\u5883\u4e0b\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u8bed\u8a00\u4f7f\u7528\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u516c\u5f00\u7684\u591a\u6a21\u6001\u8bed\u6599\u5e93\u5c06\u652f\u6301\u672a\u6765\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2512.03729", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03729", "abs": "https://arxiv.org/abs/2512.03729", "authors": ["Samantha Chapin", "Kenneth Stewart", "Roxana Leontie", "Carl Glen Henshaw"], "title": "Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing", "comment": "iSpaRo 2025, Best Paper Award in Orbital Robotics", "summary": "The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.", "AI": {"tldr": "APIARY\u5b9e\u9a8c\u9996\u6b21\u5728\u592a\u7a7a\u96f6\u91cd\u529b\u73af\u5883\u4e2d\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\uff0c\u901a\u8fc7PPO\u7b97\u6cd5\u8bad\u7ec36\u81ea\u7531\u5ea6\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u6210\u529f\u9a8c\u8bc1\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5728\u592a\u7a7a\u96f6\u91cd\u529b\u73af\u5883\u4e0b\u63a7\u5236\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u7a7a\u95f4\u63a2\u7d22\u3001\u7269\u6d41\u548c\u5b9e\u65f6\u4efb\u52a1\u9700\u6c42\u5f00\u53d1\u5feb\u901f\u90e8\u7f72\u7684\u81ea\u4e3b\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eactor-critic\u7684PPO\u7b97\u6cd5\u5728NVIDIA Isaac Lab\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec36\u81ea\u7531\u5ea6\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u968f\u673a\u5316\u76ee\u6807\u59ff\u6001\u548c\u8d28\u91cf\u5206\u5e03\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u7136\u540e\u8fdb\u884c\u5730\u9762\u6d4b\u8bd5\u548c\u592a\u7a7a\u98de\u884c\u9a8c\u8bc1\u3002", "result": "2025\u5e745\u670827\u65e5\u6210\u529f\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u4f7f\u7528NASA Astrobee\u673a\u5668\u4eba\u5b9e\u73b0\u4e86\u9996\u6b21\u592a\u7a7a\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u81ea\u7531\u98de\u884c\u5668\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u592a\u7a7a\u673a\u5668\u4eba\u81ea\u4e3b\u63a7\u5236\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u5206\u949f\u5230\u5c0f\u65f6\u7ea7\u522b\u7684\u5feb\u901f\u884c\u4e3a\u5f00\u53d1\u548c\u90e8\u7f72\u3002"}}
{"id": "2512.03402", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03402", "abs": "https://arxiv.org/abs/2512.03402", "authors": ["Yixing Xu", "Chao Li", "Xuanwu Yin", "Spandan Tiwari", "Dong Li", "Ashish Sirasao", "Emad Barsoum"], "title": "Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates", "comment": null, "summary": "Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.", "AI": {"tldr": "Dual LoRA\u901a\u8fc7\u5c06\u4f4e\u79e9\u77e9\u9635\u5206\u4e3a\u5e45\u5ea6\u7ec4\u548c\u65b9\u5411\u7ec4\uff0c\u5f15\u5165ReLU\u548c\u7b26\u53f7\u51fd\u6570\u6765\u6539\u8fdbLoRA\uff0c\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u4f18\u4e8e\u539f\u59cbLoRA\u53ca\u5176\u53d8\u4f53\u3002", "motivation": "LoRA\u4f5c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u867d\u7136\u6d41\u884c\uff0c\u4f46\u7531\u4e8e\u5176\u4f4e\u79e9\u5047\u8bbe\uff0c\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u6027\u80fd\u5f80\u5f80\u4e0d\u7406\u60f3\u3002\u9700\u8981\u6539\u8fdbLoRA\u4ee5\u66f4\u597d\u5730\u6a21\u62df\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u7684\u5168\u53c2\u6570\u5fae\u8c03\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faDual LoRA\u65b9\u6cd5\uff0c\u5c06\u4f4e\u79e9\u77e9\u9635\u5206\u4e3a\u4e24\u7ec4\uff1a\u5e45\u5ea6\u7ec4\uff08\u63a7\u5236\u53c2\u6570\u662f\u5426\u66f4\u65b0\u53ca\u66f4\u65b0\u5e45\u5ea6\uff09\u548c\u65b9\u5411\u7ec4\uff08\u51b3\u5b9a\u53c2\u6570\u66f4\u65b0\u65b9\u5411\uff09\u3002\u901a\u8fc7\u4e3a\u5e45\u5ea6\u7ec4\u6dfb\u52a0ReLU\u51fd\u6570\uff0c\u4e3a\u65b9\u5411\u7ec4\u6dfb\u52a0\u7b26\u53f7\u51fd\u6570\u6765\u5b9e\u73b0\u8fd9\u4e00\u5206\u79bb\u3002", "result": "\u5728GPT-2\u3001RoBERTa\u3001DeBERTa\u548cLLaMA-1/2/3\u7b49\u57fa\u51c6\u6a21\u578b\u4e0a\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u3001\u7406\u89e3\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDual LoRA\u5728\u76f8\u540c\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u4e0b\u4e00\u81f4\u4f18\u4e8eLoRA\u53ca\u5176\u6700\u5148\u8fdb\u7684\u53d8\u4f53\u3002", "conclusion": "Dual LoRA\u901a\u8fc7\u5f15\u5165\u5e45\u5ea6\u548c\u65b9\u5411\u5206\u79bb\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u6709\u6548\u6539\u8fdb\u4e86LoRA\u7684\u6027\u80fd\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6a21\u62df\u5168\u53c2\u6570\u5fae\u8c03\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2512.03736", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03736", "abs": "https://arxiv.org/abs/2512.03736", "authors": ["Kenneth Stewart", "Samantha Chapin", "Roxana Leontie", "Carl Glen Henshaw"], "title": "Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control", "comment": "published at iSpaRo 2025", "summary": "Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.", "AI": {"tldr": "\u9996\u6b21\u5728\u8f68\u6f14\u793a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\u81ea\u4e3b\u63a7\u5236\uff0c\u4f7f\u7528NVIDIA Omniverse\u6a21\u62df\u5668\u548c\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u6210\u529f\u5728\u7a7a\u95f4\u7ad9\u90e8\u7f72\u9a8c\u8bc1\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u592a\u7a7a\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u95ee\u9898\uff0c\u9a8c\u8bc1\u5728\u5730\u9762\u8bad\u7ec3\u540e\u80fd\u6210\u529f\u90e8\u7f72\u5230\u592a\u7a7a\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528NVIDIA Omniverse\u7269\u7406\u6a21\u62df\u5668\u548c\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u66ff\u4ee3NASA Astrobee\u7684\u6807\u51c6\u59ff\u6001\u548c\u5e73\u79fb\u63a7\u5236\uff0c\u91c7\u7528GPU\u52a0\u901f\u7684\u79d1\u5b66\u7ea7\u4eff\u771f\u73af\u5883\u8fdb\u884c\u9ad8\u6548\u7684\u8499\u7279\u5361\u6d1b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u90e8\u7f72\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7fAstrobee\u80fd\u591f\u5728\u5fae\u91cd\u529b\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5728\u5730\u9762\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5e76\u8f6c\u79fb\u5230\u592a\u7a7a\u5e94\u7528\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u7684\u5728\u8f68\u670d\u52a1\u3001\u7ec4\u88c5\u548c\u5236\u9020\uff08ISAM\uff09\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4f7f\u592a\u7a7a\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u9002\u5e94\u52a8\u6001\u4efb\u52a1\u9700\u6c42\u3002"}}
{"id": "2512.03442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03442", "abs": "https://arxiv.org/abs/2512.03442", "authors": ["Xingrun Xing", "Zhiyuan Fan", "Jie Lou", "Guoqi Li", "Jiajun Zhang", "Debing Zhang"], "title": "PretrainZero: Reinforcement Active Pretraining", "comment": null, "summary": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.", "AI": {"tldr": "PretrainZero\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u6599\u7684\u5f3a\u5316\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u5c06RL\u4ece\u9886\u57df\u7279\u5b9a\u7684\u540e\u8bad\u7ec3\u6269\u5c55\u5230\u901a\u7528\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u4e3b\u52a8\u8bc6\u522b\u4fe1\u606f\u5185\u5bb9\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u9a8c\u8bc1\u6269\u5c55\u6765\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5927\u601d\u7ef4\u6a21\u578b\u867d\u7136\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u51fa\u4e13\u5bb6\u7ea7\u80fd\u529b\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u6269\u5c55\u8fb9\u754c\u3002\u9700\u8981\u7a81\u7834\u9a8c\u8bc1\u6570\u636e\u58c1\u5792\uff0c\u5b9e\u73b0\u4ece\u9886\u57df\u7279\u5b9a\u540e\u8bad\u7ec3\u5230\u901a\u7528\u9884\u8bad\u7ec3\u7684\u6269\u5c55\u3002", "method": "\u63d0\u51faPretrainZero\u6846\u67b6\uff1a1\uff09\u4e3b\u52a8\u9884\u8bad\u7ec3\uff1a\u5b66\u4e60\u7edf\u4e00\u63a8\u7406\u7b56\u7565\uff0c\u4e3b\u52a8\u8bc6\u522b\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u5408\u7406\u4fe1\u606f\u5185\u5bb9\uff1b2\uff09\u81ea\u76d1\u7763\u5b66\u4e60\uff1a\u65e0\u9700\u53ef\u9a8c\u8bc1\u6807\u7b7e\u6216\u9884\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u901a\u7528Wikipedia\u8bed\u6599\u4e0a\u4f7f\u7528RL\u9884\u8bad\u7ec3\u63a8\u7406\u5668\uff1b3\uff09\u9a8c\u8bc1\u6269\u5c55\uff1a\u901a\u8fc7\u5904\u7406\u8d8a\u6765\u8d8a\u96be\u7684\u63a9\u7801\u8de8\u5ea6\u6765\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u5f3a\u5316\u9884\u8bad\u7ec3\u4e2d\uff0cPretrainZero\u5c06Qwen3-4B-Base\u5728MMLU-Pro\u3001SuperGPQA\u548c\u6570\u5b66\u5e73\u5747\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u5347\u4e868.43\u30015.96\u548c10.60\u5206\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u8fd8\u53ef\u4f5c\u4e3a\u4e0b\u6e38RLVR\u4efb\u52a1\u7684\u63a8\u7406\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "PretrainZero\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u4ece\u9886\u57df\u7279\u5b9a\u540e\u8bad\u7ec3\u6269\u5c55\u5230\u901a\u7528\u9884\u8bad\u7ec3\uff0c\u7a81\u7834\u4e86\u9a8c\u8bc1\u6570\u636e\u58c1\u5792\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u4eba\u5de5\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.03743", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03743", "abs": "https://arxiv.org/abs/2512.03743", "authors": ["Kehlani Fay", "Darin Anthony Djapri", "Anya Zorin", "James Clinton", "Ali El Lahib", "Hao Su", "Michael T. Tolley", "Sha Yi", "Xiaolong Wang"], "title": "Cross-embodied Co-design for Dexterous Hands", "comment": null, "summary": "Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u673a\u5668\u4eba\u624b\u5f62\u6001\u4e0e\u63a7\u5236\u7b56\u7565\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u572824\u5c0f\u65f6\u5185\u5b8c\u6210\u4ece\u8bbe\u8ba1\u3001\u8bad\u7ec3\u3001\u5236\u9020\u5230\u90e8\u7f72\u7684\u5168\u6d41\u7a0b", "motivation": "\u7075\u5de7\u64cd\u4f5c\u53d7\u9650\u4e8e\u63a7\u5236\u4e0e\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u5173\u4e8e\u4f55\u79cd\u673a\u68b0\u624b\u5f62\u6001\u6700\u9002\u5408\u7075\u5de7\u4efb\u52a1\u7684\u5171\u8bc6\uff0c\u9700\u8981\u89e3\u51b3\u5982\u4f55\u8bbe\u8ba1\u4f18\u5316\u7075\u5de7\u6027\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u8fd9\u4e00\u6839\u672c\u6311\u6218", "method": "\u5f00\u53d1\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u652f\u6301\uff1a1\uff09\u5305\u542b\u5173\u8282\u3001\u624b\u6307\u548c\u624b\u638c\u751f\u6210\u7684\u5e7f\u6cdb\u5f62\u6001\u641c\u7d22\u7a7a\u95f4\uff1b2\uff09\u901a\u8fc7\u5f62\u6001\u6761\u4ef6\u8de8\u5b9e\u4f53\u63a7\u5236\u5b9e\u73b0\u5927\u89c4\u6a21\u8bbe\u8ba1\u7a7a\u95f4\u8bc4\u4f30\uff1b3\uff09\u4f7f\u7528\u6613\u5f97\u7ec4\u4ef6\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u5236\u9020", "result": "\u5728\u591a\u4e2a\u7075\u5de7\u4efb\u52a1\uff08\u5305\u62ec\u4eff\u771f\u548c\u771f\u5b9e\u90e8\u7f72\u4e2d\u7684\u624b\u5185\u65cb\u8f6c\uff09\u4e0a\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u572824\u5c0f\u65f6\u5185\u8bbe\u8ba1\u3001\u8bad\u7ec3\u3001\u5236\u9020\u548c\u90e8\u7f72\u65b0\u7684\u673a\u5668\u4eba\u624b", "conclusion": "\u8be5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u624b\u5f62\u6001\u4e0e\u63a7\u5236\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u8fed\u4ee3\u548c\u5b9e\u9645\u90e8\u7f72\uff0c\u5b8c\u6574\u6846\u67b6\u5c06\u5f00\u6e90\u63d0\u4f9b"}}
{"id": "2512.03494", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03494", "abs": "https://arxiv.org/abs/2512.03494", "authors": ["Di Xiu", "Hongyin Tang", "Bolin Rong", "Lizhi Yan", "Jingang Wang", "Yifan Lu", "Xunliang Cai"], "title": "A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention", "comment": null, "summary": "Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.", "AI": {"tldr": "Top-k\u6ce8\u610f\u529b\u673a\u5236\u5728\u89e3\u7801\u548c\u8bad\u7ec3\u9636\u6bb5\u7684\u6709\u6548\u6027\u7814\u7a76\uff0c\u901a\u8fc7\u4fdd\u7559\u4e0e\u67e5\u8be2\u6700\u76f8\u4f3c\u7684\u5173\u952e\u8bcd\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u5df2\u6210\u4e3a\u963b\u788d\u667a\u80fd\u4f53\u548c\u591a\u6a21\u6001\u5e94\u7528\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u7814\u7a76Top-k\u6ce8\u610f\u529b\u673a\u5236\u5728\u89e3\u7801\u548c\u8bad\u7ec3\u9636\u6bb5\u7684\u6548\u679c\uff1a1\uff09\u9a8c\u8bc1\u7cbe\u786eTop-k\u89e3\u7801\u7684\u6709\u6548\u6027\uff1b2\uff09\u63a2\u7d22\u539f\u751fTop-k\u6ce8\u610f\u529b\u8bad\u7ec3\u7b56\u7565\uff1b3\uff09\u7814\u7a76\u8fd1\u4f3cTop-k\u7b97\u6cd5\u7cbe\u5ea6\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\uff1b4\uff09\u4ece\u71b5\u7684\u89d2\u5ea6\u63d0\u4f9b\u7406\u8bba\u89e3\u91ca\u3002", "result": "1\uff09Top-k\u89e3\u7801\u5728HELMET\u548cLongBench v2\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u5168\u6ce8\u610f\u529b\u6027\u80fd\uff1b2\uff09\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e00\u81f4\u7684Top-k\u6ce8\u610f\u529b\u7b56\u7565\u80fd\u8fdb\u4e00\u6b65\u91ca\u653e\u6a21\u578b\u6f5c\u529b\uff1b3\uff09\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0e\u8fd1\u4f3c\u7cbe\u5ea6\u6b63\u76f8\u5173\uff1b4\uff09Top-k\u6ce8\u610f\u529bSFT\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u71b5\u964d\u4f4e\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u71b5\u72b6\u6001\u66f4\u9002\u5408Top-k\u89e3\u7801\u7684\u5047\u8bbe\u3002", "conclusion": "Top-k\u6ce8\u610f\u529b\u673a\u5236\u662f\u964d\u4f4eLLMs\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u6216\u8fd1\u4f3c\u7684Top-k\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e14\u4f4e\u71b5\u72b6\u6001\u66f4\u9002\u5408\u8fd9\u79cd\u673a\u5236\u3002"}}
{"id": "2512.03756", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03756", "abs": "https://arxiv.org/abs/2512.03756", "authors": ["Marlon Steiner", "Royden Wagner", "\u00d6mer Sahin Tas", "Christoph Stiller"], "title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models", "comment": "In Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, AUSTRALIA, 18-21 November 2025", "summary": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5bfc\u822a\u4fe1\u606f\u96c6\u6210\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u4ee5\u6865\u63a5\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u548c\u76ee\u6807\u5bfc\u5411\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u5728nuPlan\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u9884\u6d4b\u9a71\u52a8\u8fd0\u52a8\u89c4\u5212\u7684\u6f5c\u529b\u3002", "motivation": "\u7ed3\u5408\u8fd0\u52a8\u9884\u6d4b\u548c\u8fd0\u52a8\u89c4\u5212\u53ef\u4ee5\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u4ea4\u4e92\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u6311\u6218\uff1a\u5982\u4f55\u5c06\u9884\u6d4b\u4e0e\u5bfc\u822a\u76ee\u6807\u76f8\u7ed3\u5408\uff0c\u4ee5\u53ca\u5982\u4f55\u786e\u4fdd\u7a33\u5b9a\u4e14\u8fd0\u52a8\u5b66\u53ef\u884c\u7684\u8f68\u8ff9\u3002\u672c\u6587\u4e3b\u8981\u89e3\u51b3\u7b2c\u4e00\u4e2a\u6311\u6218\uff0c\u7814\u7a76\u5982\u4f55\u5c06\u5bfc\u822a\u4fe1\u606f\u96c6\u6210\u5230\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e2d\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u67b6\u6784\u5bfc\u822a\u96c6\u6210\u7b56\u7565\uff0c\u5c06\u81ea\u8f66\u9884\u671f\u8def\u7ebf\u548c\u76ee\u6807\u59ff\u6001\u96c6\u6210\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u67b6\u6784\u4e2d\uff0c\u4ece\u800c\u6865\u63a5\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u548c\u76ee\u6807\u5bfc\u5411\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5728nuPlan\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bfc\u822a\u4fe1\u606f\u53ef\u4ee5\u540c\u65f6\u589e\u5f3a\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u9884\u6d4b\u9a71\u52a8\u8fd0\u52a8\u89c4\u5212\u7684\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5bfc\u822a\u4fe1\u606f\u96c6\u6210\u5230\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u6210\u529f\u6865\u63a5\u4e86\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u548c\u76ee\u6807\u5bfc\u5411\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u4e3a\u9884\u6d4b\u9a71\u52a8\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\u3002"}}
{"id": "2512.03503", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03503", "abs": "https://arxiv.org/abs/2512.03503", "authors": ["Haohan Yuan", "Siu Cheung Hui", "Haopeng Zhang"], "title": "Understanding LLM Reasoning for Abstractive Summarization", "comment": "26 pages,15 figures", "summary": "While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u6548\u679c\u6709\u9650\uff0c\u5b58\u5728\u8d28\u91cf\u4e0e\u5fe0\u5b9e\u5ea6\u7684\u6743\u8861\uff0c\u8fc7\u5ea6\u63a8\u7406\u53cd\u800c\u53ef\u80fd\u635f\u5bb3\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u7b49\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u62bd\u8c61\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u6548\u7528\u88ab\u5e7f\u6cdb\u5047\u8bbe\u4f46\u672a\u7ecf\u9a8c\u8bc1\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u63a8\u7406\u7b56\u7565\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u9996\u5148\u5c06\u901a\u7528\u63a8\u7406\u7b56\u7565\u9002\u914d\u5230\u6458\u8981\u9886\u57df\uff0c\u7136\u540e\u5bf98\u79cd\u63a8\u7406\u7b56\u7565\u548c3\u4e2a\u5927\u578b\u63a8\u7406\u6a21\u578b\u57288\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u6bd4\u8f83\u7814\u7a76\uff0c\u8bc4\u4f30\u6458\u8981\u8d28\u91cf\u548c\u5fe0\u5b9e\u5ea6\u3002", "result": "\u63a8\u7406\u5e76\u975e\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5177\u4f53\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u3002\u5b58\u5728\u6458\u8981\u8d28\u91cf\u4e0e\u4e8b\u5b9e\u5fe0\u5b9e\u5ea6\u7684\u6743\u8861\uff1a\u663e\u5f0f\u63a8\u7406\u7b56\u7565\u503e\u5411\u4e8e\u63d0\u9ad8\u6d41\u7545\u6027\u4f46\u727a\u7272\u4e8b\u5b9e\u57fa\u7840\uff0c\u800c\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u9690\u5f0f\u63a8\u7406\u5219\u5448\u73b0\u76f8\u53cd\u6a21\u5f0f\u3002\u589e\u52a0\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u9884\u7b97\u4e0d\u4f1a\u6539\u5584\u751a\u81f3\u53ef\u80fd\u635f\u5bb3\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "conclusion": "\u6709\u6548\u7684\u6458\u8981\u9700\u8981\u5fe0\u5b9e\u7684\u538b\u7f29\u800c\u975e\u521b\u9020\u6027\u7684\u8fc7\u5ea6\u601d\u8003\uff0c\u63a8\u7406\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u9700\u8981\u8c28\u614e\u8003\u8651\u7b56\u7565\u9009\u62e9\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u8981\u6c42\u3002"}}
{"id": "2512.03772", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03772", "abs": "https://arxiv.org/abs/2512.03772", "authors": ["Gabriele Fadini", "Deepak Ingole", "Tong Duy Son", "Alisa Rupenyan"], "title": "Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control", "comment": "6 pages, 7 figures, 3 tables", "summary": "This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u81ea\u52a8\u8c03\u53c2\u6846\u67b6\uff0c\u7528\u4e8e\u626d\u77e9\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u4f18\u5316MPC\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u672b\u7aef\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd", "motivation": "\u4f20\u7edf\u624b\u52a8\u8c03\u53c2\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u53c2\u6570\u8017\u65f6\u4e14\u96be\u4ee5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u4f18\u5316\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u673a\u5668\u4eba\u672b\u7aef\u8f68\u8ff9\u8ddf\u8e2a", "method": "\u4f7f\u7528\u7a00\u758f\u8f74\u5bf9\u9f50\u5b50\u7a7a\u95f4\u8d1d\u53f6\u65af\u4f18\u5316\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u9ad8\u6548\u63a2\u7d22\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\uff08\u5305\u62ec\u6210\u672c\u51fd\u6570\u6743\u91cd\u548c\u5e95\u5c42\u63a7\u5236\u5668\u589e\u76ca\uff09\uff0c\u7136\u540e\u5b89\u5168\u8fc1\u79fb\u5230\u771f\u5b9eUR10e\u673a\u5668\u4eba\u786c\u4ef6", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8ddf\u8e2a\u6027\u80fd\u63d0\u534741.9%\uff0c\u6c42\u89e3\u65f6\u95f4\u51cf\u5c112.5%\uff1b\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8d8b\u52bf\uff0c\u6027\u80fd\u63d0\u534725.8%", "conclusion": "\u6570\u5b57\u5b6a\u751f\u652f\u6301\u7684\u81ea\u52a8\u53c2\u6570\u4f18\u5316\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u6027\u80fd\u5e76\u51cf\u5c11\u4eba\u5de5\u8c03\u53c2\u8d1f\u62c5"}}
{"id": "2512.03582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03582", "abs": "https://arxiv.org/abs/2512.03582", "authors": ["Zeba Afroz", "Harsh Vardhan", "Pawan Bhakuni", "Aanchal Punia", "Rajdeep Kumar", "Md. Shad Akhtar"], "title": "Fine-grained Narrative Classification in Biased News Articles", "comment": null, "summary": "Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86INDI-PROP\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5370\u5ea6\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u5ba3\u4f20\u5206\u6790\uff0c\u5305\u542b\u4e09\u4e2a\u5c42\u6b21\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff1a\u610f\u8bc6\u5f62\u6001\u504f\u89c1\u3001\u53d9\u4e8b\u6846\u67b6\u548c\u8bf4\u670d\u6280\u5de7\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cdGPT-4o-mini\u5f15\u5bfc\u7684\u591a\u8df3\u63a8\u7406\u6846\u67b6\u8fdb\u884c\u5206\u7c7b\u3002", "motivation": "\u53d9\u4e8b\u662f\u5ba3\u4f20\u7684\u8ba4\u77e5\u548c\u60c5\u611f\u652f\u67b6\uff0c\u80fd\u5c06\u5b64\u7acb\u7684\u8bf4\u670d\u6280\u5de7\u7ec4\u7ec7\u6210\u8fde\u8d2f\u7684\u6545\u4e8b\u3002\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u5370\u5ea6\u65b0\u95fb\u5a92\u4f53\u7684\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u5728CAA\u548c\u519c\u6c11\u6297\u8bae\u7b49\u4e24\u6781\u5206\u5316\u7684\u793e\u4f1a\u653f\u6cbb\u4e8b\u4ef6\u4e2d\u3002", "method": "1. \u521b\u5efaINDI-PROP\u6570\u636e\u96c6\uff1a\u5305\u542b1,266\u7bc7\u5173\u4e8eCAA\u548c\u519c\u6c11\u6297\u8bae\u7684\u6587\u7ae0\uff0c\u8fdb\u884c\u4e09\u5c42\u6807\u6ce8\uff1a\u610f\u8bc6\u5f62\u6001\u504f\u89c1\uff08\u4eb2\u653f\u5e9c\u3001\u4eb2\u53cd\u5bf9\u6d3e\u3001\u4e2d\u7acb\uff09\u3001\u4e8b\u4ef6\u7279\u5b9a\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u6846\u67b6\u3001\u8bf4\u670d\u6280\u5de7\u30022. \u63d0\u51fa\u4e24\u79cdGPT-4o-mini\u5f15\u5bfc\u7684\u591a\u8df3\u63a8\u7406\u6846\u67b6\uff1aFANTA\uff08\u96c6\u6210\u4fe1\u606f\u63d0\u53d6\u548c\u4e0a\u4e0b\u6587\u6846\u67b6\u8fdb\u884c\u5206\u5c42\u63a8\u7406\uff09\u548cTPTC\uff08\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u7cfb\u7edf\u5206\u89e3\u8bf4\u670d\u7ebf\u7d22\uff09\u3002", "result": "\u63d0\u51fa\u7684FANTA\u548cTPTC\u6846\u67b6\u5728\u504f\u89c1\u3001\u53d9\u4e8b\u548c\u8bf4\u670d\u6280\u5de7\u5206\u7c7b\u4efb\u52a1\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u6539\u8fdb\u3002INDI-PROP\u662f\u9996\u4e2a\u9488\u5bf9\u5370\u5ea6\u65b0\u95fb\u5a92\u4f53\u7684\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5ba3\u4f20\u5206\u6790\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u591a\u5c42\u7ea7\u6807\u6ce8\u6846\u67b6\u548c\u6709\u6548\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u7279\u522b\u9488\u5bf9\u5370\u5ea6\u4e24\u6781\u5206\u5316\u7684\u793e\u4f1a\u653f\u6cbb\u80cc\u666f\u3002\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u7406\u89e3\u610f\u8bc6\u5f62\u6001\u53d9\u4e8b\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2512.03774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03774", "abs": "https://arxiv.org/abs/2512.03774", "authors": ["Johannes Fischer", "Marlon Steiner", "\u00d6mer Sahin Tas", "Christoph Stiller"], "title": "Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving", "comment": null, "summary": "Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u5728\u9ad8\u901f\u573a\u666f\u4e2d\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528MPC\u6216SRL", "motivation": "\u4f20\u7edfMPC\u4f7f\u7528\u51f8\u4f18\u5316\u8fd1\u4f3c\uff0c\u5c06\u89e3\u9650\u5236\u5728\u5b50\u7a7a\u95f4\u5185\uff0c\u53ef\u80fd\u9519\u8fc7\u5168\u5c40\u6700\u4f18\u89e3\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u63a2\u7d22\u66f4\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\uff0c\u540c\u65f6\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e3aMPC\u63d0\u4f9b\u65b0\u7684\u5b89\u5168\u53c2\u8003\u8f68\u8ff9\u3002\u91c7\u7528\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u4e8e\u80fd\u91cf\u51fd\u6570\u7684\u5b89\u5168\u6307\u6570\u4f5c\u4e3a\u7ea6\u675f\u76ee\u6807\uff0c\u540c\u65f6\u5b66\u4e60\u5b89\u5168\u7b56\u7565\u548c\u72b6\u6001\u76f8\u5173\u7684\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u3002", "result": "\u5728\u9ad8\u901f\u516c\u8def\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5355\u72ec\u7684MPC\u548cSRL\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u65b9\u6cd5\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u51f8\u4f18\u5316\u8fd1\u4f3c\u7684\u9650\u5236\uff0c\u627e\u5230\u66f4\u597d\u7684\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u540c\u65f6\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2512.03634", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03634", "abs": "https://arxiv.org/abs/2512.03634", "authors": ["Ahmad Aghaebrahimian"], "title": "AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment", "comment": null, "summary": "Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u6587\u672c\u4e3a\u539f\u5b50\u4e8b\u5b9e\uff0c\u4f7f\u7528\u7075\u6d3b\u7684\u3001\u65e0\u6a21\u5f0f\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u52a0\u6743\u6307\u6807\u6765\u589e\u5f3a\u4e8b\u5b9e\u8bc4\u4f30", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u4f46\u770b\u4f3c\u5408\u7406\u7684\u8bba\u70b9\uff08\u5e7b\u89c9\u95ee\u9898\uff09\uff0c\u5728\u4e34\u5e8a\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5c24\u5176\u5371\u9669\u3002\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u4e8b\u5b9e\u4e00\u81f4\u6027\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u8bca\u65ad\u548c\u7f13\u89e3\u9519\u8bef", "method": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u6846\u67b6\uff1a1\uff09\u5c06\u6587\u672c\u5206\u89e3\u4e3a\u539f\u5b50\u4e8b\u5b9e\uff1b2\uff09\u91c7\u7528\u7075\u6d3b\u7684\u65e0\u6a21\u5f0f\u65b9\u6cd5\uff1b3\uff09\u5f15\u5165\u52a0\u6743\u6307\u6807\u800c\u975e\u7edd\u5bf9\u6307\u6807\uff1b4\uff09\u63d0\u51fa\u63a7\u5236\u590d\u6742\u9886\u57df\u8bc4\u4f30\u590d\u6742\u6027\u7684\u673a\u5236", "result": "\u5728\u6d41\u884c\u7684\u901a\u7528\u548c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u53d1\u5e03\u4ee3\u7801\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u4e2d\u4e8b\u5b9e\u611f\u77e5\u6a21\u578b\u7684\u8bad\u7ec3", "conclusion": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u7075\u6d3b\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u5982\u4e34\u5e8a\u5e94\u7528"}}
{"id": "2512.03795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03795", "abs": "https://arxiv.org/abs/2512.03795", "authors": ["Jia Hu", "Zhexi Lian", "Xuerun Yan", "Ruiang Bi", "Dou Shen", "Yu Ruan", "Haoran Wang"], "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving", "comment": "17 pages, 18 figures", "summary": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.", "AI": {"tldr": "MPCFormer\uff1a\u9996\u4e2a\u663e\u5f0f\u5efa\u6a21\u591a\u8f66\u793e\u4f1a\u4ea4\u4e92\u52a8\u529b\u5b66\u7684\u53ef\u89e3\u91ca\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u4e0eTransformer\u5b66\u4e60\uff0c\u5728NGSIM\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f4e\u8f68\u8ff9\u9884\u6d4b\u8bef\u5dee\uff085\u79d2\u9884\u6d4bADE\u4ec50.86\u7c73\uff09\uff0c\u95ed\u73af\u5b9e\u9a8c\u4e2d\u89c4\u5212\u6210\u529f\u738794.67%\uff0c\u78b0\u649e\u7387\u4ece21.25%\u964d\u81f30.5%\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u5728\u9ad8\u5ea6\u52a8\u6001\u4ea4\u4e92\u573a\u666f\u4e2d\u96be\u4ee5\u5c55\u73b0\u7c7b\u4eba\u884c\u4e3a\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u7f3a\u4e4f\u5bf9\u793e\u4f1a\u4ea4\u4e92\u5e95\u5c42\u673a\u5236\u7684\u7406\u89e3\uff0c\u5bfc\u81f4\u4e0e\u5468\u56f4\u8f66\u8f86\u7684\u4ea4\u4e92\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faMPCFormer\u65b9\u6cd5\uff0c\u5c06\u793e\u4f1a\u4ea4\u4e92\u52a8\u529b\u5b66\u5efa\u6a21\u4e3a\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u8868\u793a\uff0c\u5d4c\u5165\u7269\u7406\u5148\u9a8c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4ece\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u4e2d\u5b66\u4e60\u52a8\u529b\u5b66\u7cfb\u6570\uff0c\u5e76\u5229\u7528MPC\u6846\u67b6\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5728NGSIM\u6570\u636e\u96c6\u4e0a\uff0cMPCFormer\u5b9e\u73b0\u6700\u4f4e\u8f68\u8ff9\u9884\u6d4b\u8bef\u5dee\uff085\u79d2\u9884\u6d4bADE 0.86\u7c73\uff09\uff1b\u5728\u6fc0\u70c8\u4ea4\u4e92\u573a\u666f\u95ed\u73af\u5b9e\u9a8c\u4e2d\uff0c\u89c4\u5212\u6210\u529f\u738794.67%\uff0c\u9a7e\u9a76\u6548\u7387\u63d0\u534715.75%\uff0c\u78b0\u649e\u7387\u4ece21.25%\u964d\u81f30.5%\uff0c\u4f18\u4e8e\u524d\u6cbf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "MPCFormer\u662f\u9996\u4e2a\u663e\u5f0f\u5efa\u6a21\u591a\u8f66\u793e\u4f1a\u4ea4\u4e92\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u3001\u5b89\u5168\u4e14\u7c7b\u4eba\u7684\u81ea\u52a8\u9a7e\u9a76\u884c\u4e3a\uff0c\u5728\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.03671", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03671", "abs": "https://arxiv.org/abs/2512.03671", "authors": ["Beatrice Savoldi", "Giuseppe Attanasio", "Olga Gorodetskaya", "Marta Marchiori Manerba", "Elisa Bassignana", "Silvia Casola", "Matteo Negri", "Tommaso Caselli", "Luisa Bentivogli", "Alan Ramponi", "Arianna Muti", "Nicoletta Balbo", "Debora Nozza"], "title": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context", "comment": null, "summary": "The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.", "AI": {"tldr": "\u610f\u5927\u5229\u9996\u6b21\u5168\u9762\u8c03\u67e5\u663e\u793a\uff1a\u751f\u6210\u5f0fAI\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5de5\u4f5c\u548c\u4e2a\u4eba\u4efb\u52a1\uff0c\u6b63\u53d6\u4ee3\u5176\u4ed6\u6280\u672f\u6210\u4e3a\u4e3b\u8981\u4fe1\u606f\u6765\u6e90\uff0c\u4f46\u7528\u6237\u6570\u5b57\u7d20\u517b\u4f4e\u4e14\u5b58\u5728\u663e\u8457\u6027\u522b\u9e3f\u6c9f\uff0c\u5973\u6027\u4f7f\u7528\u7387\u4ec5\u4e3a\u7537\u6027\u4e00\u534a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e86\u89e3\u751f\u6210\u5f0fAI\u5728\u610f\u5927\u5229\u7684\u91c7\u7528\u60c5\u51b5\u3001\u4f7f\u7528\u6a21\u5f0f\u548c\u6570\u5b57\u7d20\u517b\uff0c\u8bc4\u4f30\u5176\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u8bc6\u522b\u53ef\u80fd\u52a0\u5267\u6570\u5b57\u9e3f\u6c9f\u7684\u98ce\u9669\u56e0\u7d20\u3002", "method": "\u57fa\u4e8e\u65b0\u6536\u96c6\u76841,906\u540d\u610f\u5927\u5229\u8bed\u6210\u5e74\u4eba\u7684\u8c03\u67e5\u6570\u636e\uff0c\u8fdb\u884c\u9996\u6b21\u5168\u9762\u7684\u5b9e\u8bc1\u6620\u5c04\uff0c\u5206\u6790\u751f\u6210\u5f0fAI\u7684\u91c7\u7528\u7387\u3001\u4f7f\u7528\u6a21\u5f0f\u548c\u6570\u5b57\u7d20\u517b\u6c34\u5e73\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u751f\u6210\u5f0fAI\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5de5\u4f5c\u548c\u4e2a\u4eba\u4efb\u52a1\uff0c\u5305\u62ec\u60c5\u611f\u652f\u6301\u548c\u533b\u7597\u5efa\u8bae\u7b49\u654f\u611f\u9886\u57df\uff1b2\uff09\u6b63\u53d6\u4ee3\u5176\u4ed6\u6280\u672f\u6210\u4e3a\u4e3b\u8981\u4fe1\u606f\u6765\u6e90\uff1b3\uff09\u7528\u6237\u6570\u5b57\u7d20\u517b\u666e\u904d\u8f83\u4f4e\uff0c\u96be\u4ee5\u8bc6\u522b\u9519\u8bef\u4fe1\u606f\uff1b4\uff09\u5b58\u5728\u663e\u8457\u6027\u522b\u9e3f\u6c9f\uff0c\u5973\u6027\u91c7\u7528\u7387\u4ec5\u4e3a\u7537\u6027\u4e00\u534a\uff0c\u4f7f\u7528\u9891\u7387\u4e5f\u66f4\u4f4e\uff1b5\uff09\u6570\u5b57\u7d20\u517b\u662f\u91c7\u7528\u7684\u5173\u952e\u9884\u6d4b\u56e0\u7d20\uff0c\u4f46\u53ea\u80fd\u90e8\u5206\u89e3\u91ca\u6027\u522b\u5dee\u5f02\u3002", "conclusion": "\u9700\u8981\u9488\u5bf9\u6027\u7684\u6559\u80b2\u8ba1\u5212\u548c\u8fdb\u4e00\u6b65\u8c03\u67e5\u6027\u522b\u9e3f\u6c9f\u80cc\u540e\u7684\u6df1\u5c42\u969c\u788d\uff0c\u4ec5\u9760\u63d0\u9ad8\u6570\u5b57\u7d20\u517b\u4e0d\u8db3\u4ee5\u89e3\u51b3\u516c\u5e73\u53c2\u4e0e\u95ee\u9898\uff0c\u9700\u591a\u7ef4\u5ea6\u5e72\u9884\u786e\u4fddAI\u6280\u672f\u7684\u5305\u5bb9\u6027\u53d1\u5c55\u3002"}}
{"id": "2512.03828", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03828", "abs": "https://arxiv.org/abs/2512.03828", "authors": ["Dominykas Strazdas", "Magnus Jung", "Jan Marquenie", "Ingo Siegert", "Ayoub Al-Hamadi"], "title": "IM HERE: Interaction Model for Human Effort Based Robot Engagement", "comment": "8 pages, 5 figures", "summary": "The effectiveness of human-robot interaction often hinges on the ability to cultivate engagement - a dynamic process of cognitive involvement that supports meaningful exchanges. Many existing definitions and models of engagement are either too vague or lack the ability to generalize across different contexts. We introduce IM HERE, a novel framework that models engagement effectively in human-human, human-robot, and robot-robot interactions. By employing an effort-based description of bilateral relationships between entities, we provide an accurate breakdown of relationship patterns, simplifying them to focus placement and four key states. This framework captures mutual relationships, group behaviors, and actions conforming to social norms, translating them into specific directives for autonomous systems. By integrating both subjective perceptions and objective states, the model precisely identifies and describes miscommunication. The primary objective of this paper is to automate the analysis, modeling, and description of social behavior, and to determine how autonomous systems can behave in accordance with social norms for full social integration while simultaneously pursuing their own social goals.", "AI": {"tldr": "\u63d0\u51fa\u4e86IM HERE\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u52aa\u529b\u7684\u63cf\u8ff0\u6765\u5efa\u6a21\u4eba-\u4eba\u3001\u4eba-\u673a\u5668\u4eba\u3001\u673a\u5668\u4eba-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u53c2\u4e0e\u5ea6\uff0c\u5c06\u590d\u6742\u5173\u7cfb\u7b80\u5316\u4e3a\u5173\u6ce8\u70b9\u653e\u7f6e\u548c\u56db\u4e2a\u5173\u952e\u72b6\u6001\uff0c\u5b9e\u73b0\u793e\u4ea4\u884c\u4e3a\u7684\u81ea\u52a8\u5316\u5206\u6790\u548c\u81ea\u4e3b\u7cfb\u7edf\u7684\u793e\u4f1a\u89c4\u8303\u9075\u4ece\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u53c2\u4e0e\u5ea6\u5b9a\u4e49\u548c\u6a21\u578b\u8981\u4e48\u8fc7\u4e8e\u6a21\u7cca\uff0c\u8981\u4e48\u7f3a\u4e4f\u8de8\u4e0d\u540c\u60c5\u5883\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5efa\u6a21\u53c2\u4e0e\u5ea6\u3001\u652f\u6301\u6709\u610f\u4e49\u4ea4\u6d41\u7684\u6846\u67b6\uff0c\u4f7f\u81ea\u4e3b\u7cfb\u7edf\u80fd\u591f\u5728\u8ffd\u6c42\u81ea\u8eab\u793e\u4ea4\u76ee\u6807\u7684\u540c\u65f6\u5b9e\u73b0\u5b8c\u5168\u7684\u793e\u4f1a\u878d\u5408\u3002", "method": "\u63d0\u51faIM HERE\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u52aa\u529b\u7684\u63cf\u8ff0\u6765\u5206\u6790\u5b9e\u4f53\u95f4\u7684\u53cc\u8fb9\u5173\u7cfb\uff0c\u5c06\u5173\u7cfb\u6a21\u5f0f\u7b80\u5316\u4e3a\u5173\u6ce8\u70b9\u653e\u7f6e\u548c\u56db\u4e2a\u5173\u952e\u72b6\u6001\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6355\u6349\u76f8\u4e92\u5173\u7cfb\u3001\u7fa4\u4f53\u884c\u4e3a\u548c\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u7684\u884c\u52a8\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u7684\u5177\u4f53\u6307\u4ee4\u3002\u6574\u5408\u4e3b\u89c2\u611f\u77e5\u548c\u5ba2\u89c2\u72b6\u6001\u6765\u7cbe\u786e\u8bc6\u522b\u548c\u63cf\u8ff0\u6c9f\u901a\u8bef\u89e3\u3002", "result": "IM HERE\u6846\u67b6\u80fd\u591f\u6709\u6548\u5efa\u6a21\u4eba-\u4eba\u3001\u4eba-\u673a\u5668\u4eba\u3001\u673a\u5668\u4eba-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u53c2\u4e0e\u5ea6\uff0c\u63d0\u4f9b\u51c6\u786e\u7684\u5173\u7cfb\u6a21\u5f0f\u5206\u89e3\uff0c\u6355\u6349\u76f8\u4e92\u5173\u7cfb\u548c\u7fa4\u4f53\u884c\u4e3a\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8f6c\u5316\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u7684\u5177\u4f53\u6307\u4ee4\u3002\u8be5\u6846\u67b6\u80fd\u591f\u7cbe\u786e\u8bc6\u522b\u6c9f\u901a\u8bef\u89e3\uff0c\u652f\u6301\u793e\u4ea4\u884c\u4e3a\u7684\u81ea\u52a8\u5316\u5206\u6790\u3002", "conclusion": "IM HERE\u6846\u67b6\u4e3a\u793e\u4ea4\u884c\u4e3a\u7684\u81ea\u52a8\u5316\u5206\u6790\u3001\u5efa\u6a21\u548c\u63cf\u8ff0\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u4f7f\u81ea\u4e3b\u7cfb\u7edf\u80fd\u591f\u5728\u9075\u5faa\u793e\u4f1a\u89c4\u8303\u7684\u540c\u65f6\u5b9e\u73b0\u5b8c\u5168\u7684\u793e\u4f1a\u878d\u5408\uff0c\u5e76\u8ffd\u6c42\u81ea\u8eab\u7684\u793e\u4ea4\u76ee\u6807\u3002\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u53c2\u4e0e\u5ea6\u6a21\u578b\u7684\u6a21\u7cca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002"}}
{"id": "2512.03672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03672", "abs": "https://arxiv.org/abs/2512.03672", "authors": ["Shiruo Hu", "Wenbo Shan", "Yingjia Li", "Zhiqi Wan", "Xinpeng Yu", "Yunjia Qi", "Haotian Xia", "Yang Xiao", "Dingxiao Liu", "Jiaru Wang", "Chenxu Gong", "Ruixi Zhang", "Shuyue Wu", "Shibo Cui", "Chee Hui Lai", "Wei Luo", "Yubin He", "Bin Xu", "Jianshi Zhao"], "title": "Evaluating Hydro-Science and Engineering Knowledge of Large Language Models", "comment": "Hydro-SE Bench sets a new benchmark for the evaluation of LLMs in the Hydro-Science and Engineering domain, with its code and data available at \\url{https://github.com/sheishijun/Hydro-SE-Bench}", "summary": "Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Hydro-SE Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b4000\u9053\u9009\u62e9\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6c34\u79d1\u5b66\u4e0e\u5de5\u7a0b\u9886\u57df\u7684\u77e5\u8bc6\u4e0e\u5e94\u7528\u80fd\u529b\uff0c\u53d1\u73b0\u5546\u4e1a\u6a21\u578b\u51c6\u786e\u73870.74-0.80\uff0c\u5c0f\u53c2\u6570\u6a21\u578b0.41-0.68\uff0c\u6a21\u578b\u5728\u81ea\u7136\u7269\u7406\u79d1\u5b66\u76f8\u5173\u5b50\u9886\u57df\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u884c\u4e1a\u6807\u51c6\u548c\u6c34\u5229\u7ed3\u6784\u7b49\u4e13\u4e1a\u9886\u57df\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u6c34\u79d1\u5b66\u4e0e\u5de5\u7a0b\u662f\u4fdd\u969c\u4eba\u7c7b\u4f9b\u6c34\u3001\u6e05\u6d01\u6c34\u7535\u80fd\u6e90\u548c\u51cf\u707e\u7684\u5173\u952e\u9886\u57df\uff0c\u9700\u8981\u591a\u5b66\u79d1\u4e13\u5bb6\u534f\u4f5c\u51b3\u7b56\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u77e5\u8bc6\u548c\u5e94\u7528\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faHydro-SE Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b4000\u9053\u9009\u62e9\u9898\uff0c\u6db5\u76d69\u4e2a\u5b50\u9886\u57df\uff0c\u4ece\u57fa\u672c\u6982\u5ff5\u77e5\u8bc6\u3001\u5de5\u7a0b\u5e94\u7528\u80fd\u529b\u3001\u63a8\u7406\u8ba1\u7b97\u80fd\u529b\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\u51c6\u786e\u73870.74-0.80\uff0c\u5c0f\u53c2\u6570\u6a21\u578b0.41-0.68\uff1b\u6a21\u578b\u5728\u81ea\u7136\u7269\u7406\u79d1\u5b66\u76f8\u5173\u5b50\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u884c\u4e1a\u6807\u51c6\u3001\u6c34\u5229\u7ed3\u6784\u7b49\u4e13\u4e1a\u9886\u57df\u5b58\u5728\u56f0\u96be\uff1b\u6a21\u578b\u6269\u5c55\u4e3b\u8981\u63d0\u5347\u63a8\u7406\u8ba1\u7b97\u80fd\u529b\uff0c\u4f46\u5728\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6c34\u79d1\u5b66\u4e0e\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\uff0c\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u4e3a\u9886\u57df\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2512.03874", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03874", "abs": "https://arxiv.org/abs/2512.03874", "authors": ["Lei Zhang", "Diwen Zheng", "Kaixin Bai", "Zhenshan Bing", "Zoltan-Csaba Marton", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance", "comment": "Project Website: https://sites.google.com/view/omnidexvlg, 16 pages", "summary": "Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.", "AI": {"tldr": "OmniDexVLG\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u8bed\u4e49\u611f\u77e5\u7684\u7075\u5de7\u6293\u53d6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u6307\u5bfc\u751f\u6210\u7ed3\u6784\u591a\u6837\u4e14\u8bed\u4e49\u8fde\u8d2f\u7684\u7075\u5de7\u6293\u53d6\u59ff\u52bf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u73b0\u8bed\u4e49\u53ef\u63a7\u7684\u7075\u5de7\u6293\u53d6\u5408\u6210\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u5bf9\u6293\u53d6\u5206\u7c7b\u5b66\u3001\u63a5\u89e6\u8bed\u4e49\u548c\u529f\u80fd\u53ef\u4f9b\u6027\u7b49\u591a\u4e2a\u8bed\u4e49\u7ef4\u5ea6\u7684\u7edf\u4e00\u5efa\u6a21\u3002", "method": "1) OmniDexDataGen\uff1a\u8bed\u4e49\u4e30\u5bcc\u7684\u7075\u5de7\u6293\u53d6\u6570\u636e\u96c6\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5305\u542b\u5206\u7c7b\u5b66\u5f15\u5bfc\u7684\u914d\u7f6e\u91c7\u6837\u3001\u529f\u80fd\u53ef\u4f9b\u6027\u63a5\u89e6\u70b9\u91c7\u6837\u3001\u5206\u7c7b\u5b66\u611f\u77e5\u7684\u5fae\u5206\u529b\u95ed\u5408\u6293\u53d6\u91c7\u6837\u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u7684\u4f18\u5316\u9a8c\u8bc1\uff1b2) OmniDexReasoner\uff1a\u591a\u6a21\u6001\u6293\u53d6\u7c7b\u578b\u8bed\u4e49\u63a8\u7406\u6a21\u5757\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u601d\u7ef4\u94fe\u63a8\u7406\uff1b3) \u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u6293\u53d6\u751f\u6210\u6a21\u578b\uff0c\u663e\u5f0f\u7ed3\u5408\u6293\u53d6\u5206\u7c7b\u5b66\u3001\u63a5\u89e6\u7ed3\u6784\u548c\u529f\u80fd\u53ef\u4f9b\u6027\u8bed\u4e49\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u6293\u53d6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6293\u53d6\u591a\u6837\u6027\u3001\u63a5\u89e6\u8bed\u4e49\u591a\u6837\u6027\u3001\u529f\u80fd\u53ef\u4f9b\u6027\u591a\u6837\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "OmniDexVLG\u901a\u8fc7\u7edf\u4e00\u5efa\u6a21\u591a\u4e2a\u8bed\u4e49\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u8bed\u4e49\u611f\u77e5\u7075\u5de7\u6293\u53d6\u751f\u6210\uff0c\u4e3a\u4efb\u52a1\u9700\u6c42\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6293\u53d6\u8bed\u4e49\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03676", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03676", "abs": "https://arxiv.org/abs/2512.03676", "authors": ["Daria Kryvosheieva", "Andrea de Varda", "Evelina Fedorenko", "Greta Tuckute"], "title": "Different types of syntactic agreement recruit the same units within large language models", "comment": null, "summary": "Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.", "AI": {"tldr": "LLMs\u4e2d\u8bed\u6cd5\u77e5\u8bc6\u7684\u8868\u5f81\u7814\u7a76\uff1a\u53d1\u73b0\u4e0d\u540c\u8bed\u6cd5\u73b0\u8c61\uff08\u7279\u522b\u662f\u8bed\u6cd5\u4e00\u81f4\u6027\uff09\u5728\u6a21\u578b\u4e2d\u5171\u4eab\u529f\u80fd\u5355\u5143\uff0c\u6784\u6210\u6709\u610f\u4e49\u7684\u8bed\u6cd5\u529f\u80fd\u7c7b\u522b", "motivation": "\u867d\u7136LLMs\u80fd\u591f\u53ef\u9760\u533a\u5206\u8bed\u6cd5\u6b63\u786e\u4e0e\u9519\u8bef\u7684\u53e5\u5b50\uff0c\u4f46\u8bed\u6cd5\u77e5\u8bc6\u5982\u4f55\u5728\u6a21\u578b\u4e2d\u8868\u5f81\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7a76\u4e0d\u540c\u8bed\u6cd5\u73b0\u8c61\u662f\u5426\u5171\u4eab\u6216\u4f7f\u7528\u4e0d\u540c\u7684\u6a21\u578b\u7ec4\u4ef6", "method": "\u91c7\u7528\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u529f\u80fd\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u8bc6\u522b7\u4e2a\u5f00\u6e90\u6a21\u578b\u4e2d67\u79cd\u82f1\u8bed\u8bed\u6cd5\u73b0\u8c61\u6700\u54cd\u5e94\u7684\u5355\u5143\uff1b\u8fdb\u884c\u8de8\u8bed\u8a00\u5206\u6790\uff08\u82f1\u8bed\u3001\u4fc4\u8bed\u3001\u4e2d\u6587\uff09\u548c\u591a\u8bed\u8a00\u6bd4\u8f83\uff0857\u79cd\u8bed\u8a00\uff09", "result": "\u4e0d\u540c\u8bed\u6cd5\u4e00\u81f4\u6027\u7c7b\u578b\uff08\u4e3b\u8c13\u4e00\u81f4\u3001\u7167\u5e94\u4e00\u81f4\u3001\u9650\u5b9a\u8bcd-\u540d\u8bcd\u4e00\u81f4\uff09\u62db\u52df\u91cd\u53e0\u7684\u5355\u5143\u96c6\u5408\uff0c\u8868\u660e\u4e00\u81f4\u6027\u6784\u6210LLMs\u4e2d\u6709\u610f\u4e49\u7684\u529f\u80fd\u7c7b\u522b\uff1b\u7ed3\u6784\u66f4\u76f8\u4f3c\u7684\u8bed\u8a00\u5728\u4e3b\u8c13\u4e00\u81f4\u4e0a\u5171\u4eab\u66f4\u591a\u5355\u5143", "conclusion": "\u8bed\u6cd5\u4e00\u81f4\u6027\u4f5c\u4e3a\u8bed\u6cd5\u4f9d\u8d56\u5173\u7cfb\u7684\u5173\u952e\u6807\u8bb0\uff0c\u5728LLMs\u8868\u5f81\u7a7a\u95f4\u4e2d\u6784\u6210\u6709\u610f\u4e49\u7684\u7c7b\u522b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u8bed\u6cd5\u77e5\u8bc6\u7684\u7ec4\u7ec7\u65b9\u5f0f"}}
{"id": "2512.03886", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03886", "abs": "https://arxiv.org/abs/2512.03886", "authors": ["Brais Fontan-Costas", "M. Diaz-Cacho", "Ruben Fernandez-Boullon", "Manuel Alonso-Carracedo", "Javier Perez-Robles"], "title": "A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments", "comment": null, "summary": "This paper presents an Autonomous System (AS) architecture for vehicles in a closed circuit. The AS performs precision tasks including computer vision for environment perception, positioning and mapping for accurate localization, path planning for optimal trajectory generation, and control for precise vehicle actuation. Each subsystem operates independently while connecting data through a cohesive pipeline architecture. The system implements a modular design that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c01\u95ed\u8d5b\u9053\u8f66\u8f86\u7684\u81ea\u4e3b\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u542b\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u5b9a\u4f4d\u5efa\u56fe\u3001\u8def\u5f84\u89c4\u5212\u548c\u8f66\u8f86\u63a7\u5236\u7b49\u5b50\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u7ba1\u9053\u67b6\u6784\u5b9e\u73b0\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u4e3a\u5c01\u95ed\u8d5b\u9053\u73af\u5883\u4e2d\u7684\u8f66\u8f86\u5f00\u53d1\u4e00\u4e2a\u5b8c\u6574\u7684\u81ea\u4e3b\u7cfb\u7edf\u67b6\u6784\uff0c\u80fd\u591f\u6267\u884c\u7cbe\u786e\u4efb\u52a1\uff0c\u5b9e\u73b0\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5c06\u7cfb\u7edf\u5206\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\uff08\u73af\u5883\u611f\u77e5\uff09\u3001\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff08\u7cbe\u786e\u5b9a\u4f4d\uff09\u3001\u8def\u5f84\u89c4\u5212\uff08\u6700\u4f18\u8f68\u8ff9\u751f\u6210\uff09\u548c\u63a7\u5236\uff08\u7cbe\u786e\u8f66\u8f86\u6267\u884c\uff09\u56db\u4e2a\u72ec\u7acb\u5b50\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ba1\u9053\u67b6\u6784\u8fde\u63a5\u6570\u636e\u6d41\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u81ea\u4e3b\u7cfb\u7edf\u67b6\u6784\uff0c\u80fd\u591f\u7ed3\u5408\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u5757\u5316AS\u67b6\u6784\u6210\u529f\u6574\u5408\u4e86\u591a\u4e2a\u5b50\u7cfb\u7edf\uff0c\u4e3a\u5c01\u95ed\u8d5b\u9053\u8f66\u8f86\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u81ea\u4e3b\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.03688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03688", "abs": "https://arxiv.org/abs/2512.03688", "authors": ["Numaan Naeem", "Kaushal Kumar Maurya", "Kseniia Petukhova", "Ekaterina Kochmar"], "title": "AITutor-EvalKit: Exploring the Capabilities of AI Tutors", "comment": null, "summary": "We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.", "AI": {"tldr": "AITutor-EvalKit\u662f\u4e00\u4e2a\u5229\u7528\u8bed\u8a00\u6280\u672f\u8bc4\u4f30AI\u5bfc\u5e08\u6559\u5b66\u8d28\u91cf\u7684\u5de5\u5177\uff0c\u63d0\u4f9b\u6f14\u793a\u3001\u8bc4\u4f30\u3001\u6a21\u578b\u68c0\u67e5\u548c\u6570\u636e\u53ef\u89c6\u5316\u529f\u80fd\u3002", "motivation": "\u5f53\u524dAI\u5bfc\u5e08\u7cfb\u7edf\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6559\u5b66\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\uff0c\u6559\u80b2\u5229\u76ca\u76f8\u5173\u8005\u548cACL\u793e\u533a\u9700\u8981\u80fd\u591f\u8bc4\u4f30AI\u5bfc\u5e08\u6559\u5b66\u6548\u679c\u3001\u6536\u96c6\u7528\u6237\u53cd\u9988\u548c\u6807\u6ce8\u7684\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u8bed\u8a00\u6280\u672f\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u5305\u542b\u6559\u5b66\u8d28\u91cf\u8bc4\u4f30\u3001\u8f6f\u4ef6\u6f14\u793a\u3001\u6a21\u578b\u68c0\u67e5\u548c\u6570\u636e\u53ef\u89c6\u5316\u6a21\u5757\uff0c\u652f\u6301\u7528\u6237\u53cd\u9988\u548c\u6807\u6ce8\u6536\u96c6\u3002", "result": "\u521b\u5efa\u4e86AITutor-EvalKit\u5de5\u5177\uff0c\u4e3a\u6559\u80b2\u5229\u76ca\u76f8\u5173\u8005\u548cACL\u793e\u533a\u63d0\u4f9b\u4e86\u8bc4\u4f30AI\u5bfc\u5e08\u6559\u5b66\u8d28\u91cf\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u5de5\u5177\u586b\u8865\u4e86AI\u5bfc\u5e08\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u652f\u6301\u5b66\u4e60\u548c\u7528\u6237\u53cd\u9988\u6536\u96c6\uff0c\u5bf9\u6559\u80b2\u6280\u672f\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u793e\u533a\u90fd\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.03891", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03891", "abs": "https://arxiv.org/abs/2512.03891", "authors": ["Ying-Kuan Tsai", "Yi-Ping Chen", "Vispi Karkaria", "Wei Chen"], "title": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning", "comment": "28 pages, 17 figures", "summary": "Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4e3b\u52a8\u60ac\u67b6\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u8bbe\u8ba1\u6982\u5ff5\u5b9e\u73b0\u4e2a\u6027\u5316\u4f18\u5316\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u51cf\u5c11\u63a7\u5236\u80fd\u801743-52%\u3002", "motivation": "\u4f20\u7edf\u4e3b\u52a8\u60ac\u67b6\u7cfb\u7edf\u53d7\u9650\u4e8e\u56fa\u5b9a\u786c\u4ef6\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u786e\u5b9a\u7684\u52a8\u6001\u8fd0\u884c\u6761\u4ef6\u3002\u6570\u5b57\u5b6a\u751f\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e3a\u5b9e\u65f6\u6570\u636e\u9a71\u52a8\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5c06\u5176\u6574\u5408\u4e3a\u7edf\u4e00\u6846\u67b6\u4ecd\u5177\u6311\u6218\u3002", "method": "1) \u5c06\u81ea\u52a8\u5fae\u5206\u96c6\u6210\u5230\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8054\u5408\u4f18\u5316\u7269\u7406\u60ac\u67b6\u7ec4\u4ef6\u548c\u63a7\u5236\u7b56\u7565\uff1b2) \u4f7f\u7528\u5206\u4f4d\u6570\u5b66\u4e60\u8fdb\u884c\u6a21\u578b\u66f4\u65b0\u4ee5\u6355\u6349\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff1b3) \u91c7\u7528\u591a\u4ee3\u8bbe\u8ba1\u6982\u5ff5\u5b9e\u73b0\u81ea\u6539\u8fdb\u7cfb\u7edf\uff1b4) \u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u76f4\u63a5\u4ece\u53ef\u7528\u4f20\u611f\u5668\u4fe1\u606f\u5b66\u4e60\u6700\u4f18\u63a7\u5236\u52a8\u4f5c\u3002", "result": "\u5728\u6e29\u548c\u548c\u6fc0\u8fdb\u4e24\u79cd\u9a7e\u9a76\u573a\u666f\u4e0b\uff0c\u4f18\u5316\u7cfb\u7edf\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u8f68\u8ff9\uff0c\u63a7\u5236\u80fd\u8017\u5206\u522b\u51cf\u5c11\u7ea643%\u548c52%\uff0c\u540c\u65f6\u4fdd\u6301\u4e58\u5750\u8212\u9002\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u96c6\u6210\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6a21\u578b\u66f4\u65b0\u7684\u6570\u5b57\u5b6a\u751f\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u8bbe\u8ba1\u7b56\u7565\u5b9e\u73b0\u4e2a\u6027\u5316\u4e3b\u52a8\u60ac\u67b6\u4f18\u5316\uff0c\u4e3a\u81ea\u9002\u5e94\u8f66\u8f86\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03704", "abs": "https://arxiv.org/abs/2512.03704", "authors": ["Yijun Liao"], "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue", "comment": "22 pages, 2 figures, 13 tables. Code available at https://github.com/lyj20071013/DZ-TDPO", "summary": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO", "AI": {"tldr": "DZ-TDPO\u662f\u4e00\u4e2a\u975e\u7834\u574f\u6027\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001KL\u7ea6\u675f\u548c\u53ef\u5b66\u4e60\u65f6\u95f4\u6ce8\u610f\u529b\u504f\u7f6e\u89e3\u51b3\u957f\u5bf9\u8bdd\u4e2d\u7684\u72b6\u6001\u60ef\u6027\u95ee\u9898\uff0c\u5728MSC\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u80dc\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u957f\u5bf9\u8bdd\u7cfb\u7edf\u5b58\u5728\u72b6\u6001\u60ef\u6027\u95ee\u9898\uff0c\u9759\u6001\u7ea6\u675f\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u7528\u6237\u610f\u56fe\u6f14\u53d8\u4e0e\u5386\u53f2\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u9650\u5236\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u7684\u52a8\u6001\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51faDZ-TDPO\u6846\u67b6\uff0c\u7ed3\u5408\u51b2\u7a81\u611f\u77e5\u7684\u52a8\u6001KL\u7ea6\u675f\u548c\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u6ce8\u610f\u529b\u504f\u7f6e\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u6ce8\u610f\u529b\u8c03\u8282\u800c\u975e\u7834\u574f\u6027\u6743\u91cd\u66f4\u65b0\u6765\u7f13\u89e3\u72b6\u6001\u60ef\u6027\u3002", "result": "\u5728MSC\u6570\u636e\u96c6\u4e0a\uff0cDZ-TDPO\u5728Phi-3.5\u4e0a\u8fbe\u523086.2%\u7684\u80dc\u7387\uff0cQwen2.5-7B\u6a21\u578b\u8fbe\u523099.4%\u7684\u80dc\u7387\u4e14\u56f0\u60d1\u5ea6\u5f00\u9500\u53ef\u5ffd\u7565\u3002\u53d1\u73b0\"\u5bb9\u91cf-\u7a33\u5b9a\u6027\u6743\u8861\"\u73b0\u8c61\uff1a\u5c0f\u6a21\u578b\u9700\u8981\u4ed8\u51fa\"\u5bf9\u9f50\u7a0e\"\u6765\u514b\u670d\u5386\u53f2\u60ef\u6027\uff0c\u800c\u5927\u6a21\u578b\u80fd\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5bf9\u9f50\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u786e\u7684\u6ce8\u610f\u529b\u8c03\u8282\u800c\u975e\u7834\u574f\u6027\u6743\u91cd\u66f4\u65b0\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u72b6\u6001\u60ef\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002\u5927\u6a21\u578b\u5728\u89e3\u51b3\u72b6\u6001\u60ef\u6027\u95ee\u9898\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6027\u80fd\u5bf9\u9f50\u800c\u51e0\u4e4e\u4e0d\u5f71\u54cd\u56f0\u60d1\u5ea6\u3002"}}
{"id": "2512.03911", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03911", "abs": "https://arxiv.org/abs/2512.03911", "authors": ["Kenneth Stewart", "Roxana Leontie", "Samantha Chapin", "Joe Hays", "Sumit Bam Shrestha", "Carl Glen Henshaw"], "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware", "comment": "Submitted for review at NICE 2026 (Neuro-Inspired Computational Elements) conference", "summary": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.", "AI": {"tldr": "\u5c06\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684ANN\u8f6c\u6362\u4e3a\u8109\u51b2Sigma-Delta\u795e\u7ecf\u7f51\u7edc\uff0c\u90e8\u7f72\u5728Intel Loihi 2\u795e\u7ecf\u5f62\u6001\u82af\u7247\u4e0a\uff0c\u7528\u4e8eAstrobee\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u80fd\u6548\u7684\u63a8\u7406\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u4e3a\u672a\u6765\u7a7a\u95f4\u548c\u5730\u9762\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u5b9e\u65f6\u3001\u9ad8\u80fd\u6548\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u7aef\u5230\u7aef\u6d41\u7a0b\uff1a1\uff09\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3ANN\u7b56\u7565\uff08\u4f7f\u7528ReLU\u6fc0\u6d3b\u51fd\u6570\uff09\uff1b2\uff09\u5c06ANN\u8f6c\u6362\u4e3a\u4e0eIntel Loihi 2\u517c\u5bb9\u7684SDNN\uff1b3\uff09\u5728NVIDIA Omniverse Isaac Lab\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u95ed\u73af\u63a7\u5236\u8bc4\u4f30\uff1b4\uff09\u6bd4\u8f83GPU\u4e0eLoihi 2\u7684\u6267\u884c\u6027\u80fd\u3002", "result": "\u6210\u529f\u5c06ANN\u7b56\u7565\u8f6c\u6362\u4e3aSDNN\u5e76\u5728Loihi 2\u4e0a\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecf\u5f62\u6001\u5e73\u53f0\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u4e86\u4f4e\u5ef6\u8fdf\u548c\u80fd\u6548\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u7a7a\u95f4\u548c\u5730\u9762\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u3001\u9ad8\u80fd\u6548\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5efa\u7acb\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u8bc1\u660e\u4e86\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.03737", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.03737", "abs": "https://arxiv.org/abs/2512.03737", "authors": ["Chuyue Wang", "Jie Feng", "Yuxi Wu", "Hang Zhang", "Zhiguo Fan", "Bing Cheng", "Wei Lin"], "title": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation", "comment": null, "summary": "Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \\textbf{AR-Med}, a novel framework for \\textbf{A}utomated \\textbf{R}elevance assessment for \\textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\\%, a 24\\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.", "AI": {"tldr": "AR-Med\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u7597\u641c\u7d22\u7684\u81ea\u52a8\u76f8\u5173\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684LLM\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edf\u641c\u7d22\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\uff0c\u5728\u79bb\u7ebf\u51c6\u786e\u7387\u8fbe\u523093%\u4ee5\u4e0a\uff0c\u5728\u7ebf\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5728\u7ebf\u533b\u7597\u5e73\u53f0\u9700\u8981\u51c6\u786e\u53ef\u9760\u7684\u641c\u7d22\u529f\u80fd\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7406\u89e3\u590d\u6742\u7ec6\u5fae\u7684\u7528\u6237\u67e5\u8be2\uff0c\u800cLLMs\u867d\u7136\u5177\u6709\u5f3a\u5927\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5728\u533b\u7597\u9886\u57df\u90e8\u7f72\u9762\u4e34\u4e8b\u5b9e\u5e7b\u89c9\u3001\u4e13\u4e1a\u77e5\u8bc6\u7f3a\u53e3\u548c\u9ad8\u6210\u672c\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faAR-Med\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\uff0c\u5c06LLM\u63a8\u7406\u5efa\u7acb\u5728\u5df2\u9a8c\u8bc1\u7684\u533b\u7597\u77e5\u8bc6\u57fa\u7840\u4e0a\uff1b2\uff09\u8bbe\u8ba1\u5b9e\u7528\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6848\uff0c\u5c06\u5927\u578b\u6559\u5e08\u6a21\u578b\u538b\u7f29\u4e3a\u7d27\u51d1\u800c\u5f3a\u5927\u7684\u5b66\u751f\u6a21\u578b\uff1b3\uff09\u521b\u5efaLocalQSMed\u591a\u4e13\u5bb6\u6807\u6ce8\u57fa\u51c6\uff0c\u6307\u5bfc\u6a21\u578b\u8fed\u4ee3\u5e76\u786e\u4fdd\u79bb\u7ebf\u4e0e\u5728\u7ebf\u6027\u80fd\u7684\u4e00\u81f4\u6027\u3002", "result": "AR-Med\u5728\u79bb\u7ebf\u51c6\u786e\u7387\u8d85\u8fc793%\uff0c\u6bd4\u539f\u59cb\u5728\u7ebf\u7cfb\u7edf\u7edd\u5bf9\u63d0\u534724%\uff1b\u5728\u7ebf\u76f8\u5173\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u663e\u8457\u63d0\u9ad8\uff1b\u6210\u529f\u5728\u5728\u7ebf\u533b\u7597\u914d\u9001\u5e73\u53f0\u5927\u89c4\u6a21\u90e8\u7f72\u3002", "conclusion": "AR-Med\u4e3a\u533b\u7597\u9886\u57df\u5f00\u53d1\u53ef\u4fe1\u8d56\u3001\u53ef\u6269\u5c55\u7684LLM\u9a71\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u84dd\u56fe\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u533b\u7597\u641c\u7d22\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u4e86\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2512.03913", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03913", "abs": "https://arxiv.org/abs/2512.03913", "authors": ["Jeongeun Park", "Jihwan Yoon", "Byungwoo Jeon", "Juhan Park", "Jinwoo Shin", "Namhoon Cho", "Kyungjae Lee", "Sangdoo Yun", "Sungjoon Choi"], "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations", "comment": "https://vine-vla.github.io/", "summary": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.", "AI": {"tldr": "VINE\u662f\u4e00\u4e2a\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u5931\u8d25\u6570\u636e\u6765\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\uff0c\u5c06\u9ad8\u5c42\u63a8\u7406\u4e0e\u4f4e\u5c42\u63a7\u5236\u5206\u79bb\uff0c\u5728\u89c4\u5212\u65f6\u8fdb\u884c\u53ef\u884c\u6027\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u901a\u5e38\u53ea\u4f7f\u7528\u6210\u529f\u7684\u9065\u64cd\u4f5c\u6f14\u793a\u6570\u636e\uff0c\u800c\u4e22\u5f03\u4e86\u5927\u91cf\u7684\u5931\u8d25\u5c1d\u8bd5\u3002\u8fd9\u4e9b\u5931\u8d25\u6570\u636e\u5305\u542b\u4e86\u7b56\u7565\u8106\u5f31\u6027\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u7528\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u6df7\u5408\u8d28\u91cf\u7684\u6570\u636e\u96c6\u6765\u5b66\u4e60\u5728\u89c4\u5212\u65f6\u8fdb\u884c\u5931\u8d25\u611f\u77e5\u63a8\u7406\u3002", "method": "\u63d0\u51faVINE\u5206\u5c42\u6a21\u578b\uff1aSystem 2\u8fdb\u884c\u9ad8\u5c42\u63a8\u7406\uff0c\u57282D\u573a\u666f\u56fe\u62bd\u8c61\u4e0a\u6267\u884c\u53ef\u884c\u6027\u5f15\u5bfc\u7684\u6811\u641c\u7d22\uff0c\u63d0\u51fa\u5b50\u76ee\u6807\u8f6c\u6362\uff0c\u4ece\u6210\u529f\u548c\u5931\u8d25\u4e2d\u9884\u6d4b\u6210\u529f\u6982\u7387\uff0c\u5e76\u5728\u6267\u884c\u524d\u4fee\u526a\u8106\u5f31\u5206\u652f\uff1bSystem 1\u6267\u884c\u4f4e\u5c42\u63a7\u5236\u3002\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u5f62\u5f0f\uff0c\u5c06\u5931\u8d25\u4f5c\u4e3a\u7ed3\u6784\u5316\u5b66\u4e60\u4fe1\u53f7\u800c\u975e\u566a\u58f0\u76d1\u7763\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u8868\u660e\u5931\u8d25\u6570\u636e\u662f\u5c06VLA\u7684\u5e7f\u6cdb\u80fd\u529b\u8f6c\u5316\u4e3a\u9c81\u68d2\u6267\u884c\u7684\u91cd\u8981\u8d44\u6e90\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5931\u8d25\u6570\u636e\u5e76\u5c06\u5176\u76f4\u63a5\u6574\u5408\u5230\u51b3\u7b56\u5faa\u73af\u4e2d\uff0cVINE\u6a21\u578b\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u5931\u8d25\u7ecf\u9a8c\u5bf9\u4e8e\u6784\u5efa\u66f4\u53ef\u9760\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.03759", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03759", "abs": "https://arxiv.org/abs/2512.03759", "authors": ["Jingyang Ou", "Jiaqi Han", "Minkai Xu", "Shaoxuan Xu", "Jianwen Xie", "Stefano Ermon", "Yi Wu", "Chongxuan Li"], "title": "Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective", "comment": null, "summary": "Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86ESPO\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u4f18\u5316\u89e3\u51b3\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u96be\u9898\uff0c\u4f7f\u7528ELBO\u4f5c\u4e3a\u4f3c\u7136\u4ee3\u7406\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7a0b\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u8bcd\u7ea7\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e2d\u5f88\u6709\u6548\uff0c\u4f46\u96be\u4ee5\u5e94\u7528\u4e8e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u3002\u6838\u5fc3\u56f0\u96be\u5728\u4e8e\u4f3c\u7136\u8fd1\u4f3c\uff1a\u81ea\u56de\u5f52\u6a21\u578b\u5929\u7136\u63d0\u4f9b\u8bcd\u7ea7\u6761\u4ef6\u6982\u7387\uff0c\u800c\u6269\u6563\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u6b65\u9aa4\u751f\u6210\u5e8f\u5217\uff0c\u7f3a\u4e4f\u8fd9\u79cd\u5206\u89e3\u3002", "method": "\u63d0\u51faELBO-based Sequence-level Policy Optimization (ESPO)\u6846\u67b6\uff0c\u5c06\u6574\u4e2a\u5e8f\u5217\u751f\u6210\u89c6\u4e3a\u5355\u4e2a\u52a8\u4f5c\uff0c\u4f7f\u7528ELBO\u4f5c\u4e3a\u53ef\u5904\u7406\u7684\u5e8f\u5217\u7ea7\u4f3c\u7136\u4ee3\u7406\u3002\u65b9\u6cd5\u5305\u542b\u8bcd\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u5f52\u4e00\u5316\u548c\u9c81\u68d2\u7684KL\u6563\u5ea6\u4f30\u8ba1\uff0c\u786e\u4fdd\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7a0b\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cESPO\u663e\u8457\u4f18\u4e8e\u8bcd\u7ea7\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Countdown\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8620-40\u5206\u7684\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u4e0a\u4fdd\u6301\u4e00\u81f4\u7684\u589e\u76ca\u3002", "conclusion": "ESPO\u5efa\u7acb\u4e86\u5e8f\u5217\u7ea7\u4f18\u5316\u4f5c\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u539f\u7406\u6027\u548c\u7ecf\u9a8c\u6709\u6548\u7684\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u4e0e\u6269\u6563\u6a21\u578b\u5728RL\u5e94\u7528\u4e2d\u7684\u6839\u672c\u6027\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2512.03936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03936", "abs": "https://arxiv.org/abs/2512.03936", "authors": ["Aron Distelzweig", "Yiwei Wang", "Faris Janjo\u0161", "Marcel Hallgarten", "Mihai Dobre", "Alexander Langmann", "Joschka Boedecker", "Johannes Betz"], "title": "Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response", "comment": null, "summary": "Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.", "AI": {"tldr": "BIBeR\u6846\u67b6\u5c06\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u9884\u6d4b\u5668\u4e0e\u8fed\u4ee3\u6700\u4f18\u54cd\u5e94\u5faa\u73af\u7ed3\u5408\uff0c\u5b9e\u73b0\u53cc\u5411\u4ea4\u4e92\u611f\u77e5\u89c4\u5212\uff0c\u5728\u5bc6\u96c6\u57ce\u5e02\u4ea4\u901a\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534711%\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7cfb\u7edf\u5728\u5e38\u89c4\u573a\u666f\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5bc6\u96c6\u57ce\u5e02\u4ea4\u901a\u4e2d\uff08\u5982\u53d8\u9053\u3001\u6c47\u5165\uff09\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7b80\u5355\u4e22\u5f03\u4e0d\u5b89\u5168\u8ba1\u5212\uff0c\u8981\u4e48\u4f7f\u7528\u7aef\u5230\u7aef\u6a21\u578b\u7f3a\u4e4f\u53cc\u5411\u4ea4\u4e92\u5efa\u6a21\uff0c\u800c\u535a\u5f08\u8bba\u65b9\u6cd5\u867d\u6709\u7406\u8bba\u4f18\u52bf\u4f46\u5e94\u7528\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u8fed\u4ee3\u6700\u4f18\u54cd\u5e94\uff08BIBeR\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u5668\u96c6\u6210\u5230\u8fed\u4ee3\u6700\u4f18\u54cd\u5e94\u5faa\u73af\u4e2d\uff0c\u53cd\u590d\u4f18\u5316\u81ea\u8f66\u548c\u5468\u56f4\u8f66\u8f86\u7b56\u7565\uff1b2\uff09\u901a\u8fc7\u8d1d\u53f6\u65af\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u91cf\u5316\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u8c03\u8282\u66f4\u65b0\u5f3a\u5ea6\uff1b3\uff09\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\uff0c\u5b9e\u73b0\u53cc\u5411\u9002\u5e94\uff08\u81ea\u8f66\u65e2\u54cd\u5e94\u53c8\u5f71\u54cd\u5176\u4ed6\u8f66\u8f86\uff09\u3002", "result": "\u5728\u9ad8\u5ea6\u4ea4\u4e92\u7684interPlan\u53d8\u9053\u573a\u666f\u4e2d\uff0cBIBeR\u6bd4\u6700\u5148\u8fdb\u7684\u89c4\u5212\u5668\u63d0\u534711%\u6027\u80fd\uff1b\u5728\u6807\u51c6nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BIBeR\u6210\u529f\u7edf\u4e00\u4e86\u8fd0\u52a8\u9884\u6d4b\u548c\u535a\u5f08\u8bba\u89c4\u5212\uff0c\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u89c4\u5212\u7684\u900f\u660e\u6027\u548c\u5b66\u4e60\u6a21\u578b\u7684\u7075\u6d3b\u6027\uff0c\u4e3a\u5bc6\u96c6\u57ce\u5e02\u4ea4\u901a\u4e2d\u7684\u4ea4\u4e92\u611f\u77e5\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03771", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03771", "abs": "https://arxiv.org/abs/2512.03771", "authors": ["Itay Yona", "Amir Sarid", "Michael Karasik", "Yossi Gandelsman"], "title": "In-Context Representation Hijacking", "comment": null, "summary": "We introduce \\textbf{Doublespeak}, a simple \\emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \\textit{bomb}) with a benign token (e.g., \\textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.", "AI": {"tldr": "Doublespeak\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u52ab\u6301\u653b\u51fb\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u5173\u952e\u8bcd\u66ff\u6362\u4e3a\u826f\u6027\u6807\u8bb0\u6765\u7ed5\u8fc7\u5b89\u5168\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u5185\u90e8\u5c06\u826f\u6027\u63d0\u793a\u89e3\u91ca\u4e3a\u6709\u5bb3\u6307\u4ee4\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u7b56\u7565\u5b58\u5728\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u64cd\u7eb5\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u6765\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63ed\u793a\u8fd9\u79cd\u65b0\u7684\u653b\u51fb\u9762\uff0c\u8bc1\u660e\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u5728\u8868\u793a\u5c42\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDoublespeak\u653b\u51fb\u65b9\u6cd5\uff1a\u5728\u591a\u4e2a\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u7cfb\u7edf\u6027\u5730\u5c06\u6709\u5bb3\u5173\u952e\u8bcd\uff08\u5982\"bomb\"\uff09\u66ff\u6362\u4e3a\u826f\u6027\u6807\u8bb0\uff08\u5982\"carrot\"\uff09\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u6709\u5bb3\u8bf7\u6c42\u7684\u524d\u7f00\u3002\u8fd9\u79cd\u66ff\u6362\u5bfc\u81f4\u826f\u6027\u6807\u8bb0\u7684\u5185\u90e8\u8868\u793a\u5411\u6709\u5bb3\u6807\u8bb0\u6536\u655b\uff0c\u4ece\u800c\u5728\u8868\u793a\u5c42\u9762\u5d4c\u5165\u6709\u5bb3\u8bed\u4e49\u3002", "result": "\u653b\u51fb\u65e0\u9700\u4f18\u5316\uff0c\u5e7f\u6cdb\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\uff0c\u5728\u95ed\u6e90\u548c\u5f00\u6e90\u7cfb\u7edf\u4e0a\u5747\u53d6\u5f97\u9ad8\u6210\u529f\u7387\u3002\u5728Llama-3.3-70B-Instruct\u4e0a\uff0c\u4ec5\u9700\u5355\u53e5\u4e0a\u4e0b\u6587\u8986\u76d6\u5373\u53ef\u8fbe\u523074%\u7684\u653b\u51fb\u6210\u529f\u7387\u3002\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u663e\u793a\u8bed\u4e49\u8986\u76d6\u9010\u5c42\u53d1\u751f\uff0c\u65e9\u671f\u5c42\u7684\u826f\u6027\u542b\u4e49\u5728\u540e\u671f\u5c42\u6536\u655b\u4e3a\u6709\u5bb3\u8bed\u4e49\u3002", "conclusion": "Doublespeak\u63ed\u793a\u4e86LLM\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u65b0\u653b\u51fb\u9762\uff0c\u8868\u660e\u5f53\u524d\u5bf9\u9f50\u7b56\u7565\u4e0d\u8db3\uff0c\u9700\u8981\u5728\u8868\u793a\u5c42\u9762\u8fdb\u884c\u64cd\u4f5c\u3002\u8fd9\u4e3a\u672a\u6765\u66f4\u9c81\u68d2\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2512.03958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03958", "abs": "https://arxiv.org/abs/2512.03958", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation", "comment": null, "summary": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.", "AI": {"tldr": "\u63d0\u51faMDE-AgriVLN\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u589e\u5f3a\u519c\u4e1a\u673a\u5668\u4eba\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u80fd\u529b\uff0c\u5728A2A\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u5e76\u964d\u4f4e\u5bfc\u822a\u8bef\u5dee\u3002", "motivation": "\u519c\u4e1a\u673a\u5668\u4eba\u901a\u5e38\u53ea\u914d\u5907\u5355\u76ee\u6444\u50cf\u5934\uff0c\u5bfc\u81f4\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u800c\u73b0\u6709\u7684\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u53cc\u76ee\u89c6\u89c9\uff0c\u9700\u8981\u89e3\u51b3\u5355\u76ee\u89c6\u89c9\u4e0b\u7684\u5bfc\u822a\u6027\u80fd\u95ee\u9898\u3002", "method": "\u63d0\u51faMDE-AgriVLN\u65b9\u6cd5\uff0c\u5305\u542bMDE\u6a21\u5757\u4eceRGB\u56fe\u50cf\u751f\u6210\u6df1\u5ea6\u7279\u5f81\uff0c\u8f85\u52a9\u51b3\u7b56\u6a21\u5757\u8fdb\u884c\u63a8\u7406\uff0c\u589e\u5f3a\u5355\u76ee\u89c6\u89c9\u4e0b\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u4ece0.23\u63d0\u5347\u52300.32\uff0c\u5bfc\u822a\u8bef\u5dee\u4ece4.43\u7c73\u964d\u4f4e\u52304.08\u7c73\uff0c\u8fbe\u5230\u4e86\u519c\u4e1aVLN\u9886\u57df\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MDE-AgriVLN\u65b9\u6cd5\u901a\u8fc7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u519c\u4e1a\u673a\u5668\u4eba\u5728\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u5355\u76ee\u89c6\u89c9\u4e0b\u7684\u519c\u4e1a\u5bfc\u822a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.03803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03803", "abs": "https://arxiv.org/abs/2512.03803", "authors": ["Huey Sun", "Anabel Yong", "Lorenzo Gilly", "Felipe Jin"], "title": "Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5", "comment": null, "summary": "Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff08T5/FLAN-T5\uff09\u4e2d\u5b9e\u73b0DoLa\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5176\u5bf9\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u63d0\u5347\u751f\u6210\u5fe0\u5b9e\u6027\uff0c\u5728\u5176\u4ed6\u4efb\u52a1\u4e0a\u5219\u6709\u5bb3\u3002", "motivation": "\u5bf9\u6bd4\u89e3\u7801\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u8d28\u91cf\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u5982DoLa\u4ec5\u5e94\u7528\u4e8e\u4ec5\u89e3\u7801\u5668\u67b6\u6784\uff0c\u4e14\u4e3b\u8981\u7814\u7a76\u5176\u5bf9\u4e8b\u5b9e\u6027\u7684\u6539\u8fdb\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06DoLa\u9002\u914d\u5230\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff08T5/FLAN-T5\uff09\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u5c06DoLa\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u9002\u914d\u5230T5\u548cFLAN-T5\u6a21\u578b\u5bb6\u65cf\uff0c\u8fd9\u662f\u9996\u6b21\u5728\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4e2d\u5b9e\u73b0\u5bf9\u6bd4\u89e3\u7801\u7b56\u7565\u3002\u901a\u8fc7\u5c42\u5bf9\u5c42\u5206\u6790FLAN-T5\u6a21\u578b\u7684logit\u6f14\u5316\uff0c\u91cf\u5316DoLa\u5bf9\u4ee4\u724c\u8f93\u51fa\u6982\u7387\u7684\u5f71\u54cd\u3002", "result": "DoLa\u5728\u67d0\u4e9b\u4efb\u52a1\u7c7b\u522b\u4e0a\u80fd\u63d0\u5347\u6587\u672c\u751f\u6210\u7684\u5fe0\u5b9e\u6027\uff0c\u4f46\u5728\u5176\u4ed6\u4efb\u52a1\u4e0a\u4f1a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u901a\u8fc7\u5c42\u5bf9\u5c42\u5206\u6790\u63ed\u793a\u4e86DoLa\u5bf9\u4ee4\u724c\u8f93\u51fa\u6982\u7387\u7684\u5177\u4f53\u5f71\u54cd\u673a\u5236\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5728\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4e2d\u5b9e\u73b0\u4e86DoLa\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u9996\u6b21\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5bf9\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u5f71\u54cd\u5177\u6709\u4efb\u52a1\u4f9d\u8d56\u6027\uff0c\u4e3a\u7406\u89e3\u5bf9\u6bd4\u89e3\u7801\u5728\u4e0d\u540c\u67b6\u6784\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2512.03995", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03995", "abs": "https://arxiv.org/abs/2512.03995", "authors": ["Levi Burner", "Guido de Croon", "Yiannis Aloimonos"], "title": "Artificial Microsaccade Compensation: Stable Vision for an Ornithopter", "comment": "29 pages, 5 figures, 2 tables, under review", "summary": "Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for \"Artificial Microsaccade Compensation\". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.", "AI": {"tldr": "\u63d0\u51fa\u4eba\u5de5\u5fae\u626b\u89c6\u8865\u507f\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a33\u5b9a\u65e0\u5c3e\u6251\u7ffc\u98de\u884c\u5668\u62cd\u6444\u7684\u6296\u52a8\u89c6\u9891\uff0c\u5728\u5b9e\u65f6\u5904\u7406\u4e2d\u4f18\u4e8eAdobe Premier Pro\u7684\u53d8\u5f62\u7a33\u5b9a\u5668", "motivation": "\u53d7\u751f\u7269\u5fae\u626b\u89c6\u73b0\u8c61\u542f\u53d1\uff0c\u89e3\u51b3\u65e0\u5c3e\u6251\u7ffc\u98de\u884c\u5668\u56e012-20Hz\u6296\u52a8\u800c\u65e0\u6cd5\u4f7f\u7528\u76f8\u673a\u4f20\u611f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u89c6\u9891\u7a33\u5b9a\u5316", "method": "\u901a\u8fc7\u4f18\u5316SO(3)\u8868\u793a\u7684\u4e09\u7ef4\u65cb\u8f6c\u6765\u6700\u5c0f\u5316\u56fe\u50cf\u5f3a\u5ea6\u53d8\u5316\uff0c\u5b9e\u73b0\u65e0\u5931\u771f\u7684\u89c6\u9891\u7a33\u5b9a\uff0c\u5e76\u91c7\u7528\u9ad8\u6548\u7684\u9012\u5f52\u66f4\u65b0\u673a\u5236", "result": "\u65b9\u6cd5\u80fd\u5b9e\u65f6\u751f\u6210\u9002\u5408\u4eba\u773c\u89c2\u770b\u7684\u7a33\u5b9a\u89c6\u9891\uff0c\u5728\u56fa\u5b9a\u89c6\u89d2\u4e0b\u663e\u8457\u51cf\u5c11\u5e27\u95f4\u8fd0\u52a8\uff0c\u8d28\u91cf\u4f18\u4e8eAdobe Premier Pro\u7684\u53d8\u5f62\u7a33\u5b9a\u5668", "conclusion": "\u4eba\u5de5\u5fae\u626b\u89c6\u8865\u507f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6251\u7ffc\u98de\u884c\u5668\u7684\u89c6\u9891\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u5b9e\u65f6\u6027\u548c\u8d28\u91cf\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u5546\u4e1a\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.03818", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03818", "abs": "https://arxiv.org/abs/2512.03818", "authors": ["Kylie L. Anglin", "Stephanie Milan", "Brittney Hernandez", "Claudia Ventura"], "title": "Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology", "comment": "22 pages, 2 figures", "summary": "Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5b66\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u5b9e\u8bc1\u6846\u67b6\uff0c\u53d1\u73b0\u6784\u9020\u5b9a\u4e49\u3001\u4efb\u52a1\u6846\u67b6\u548c\u793a\u4f8b\u662f\u6700\u5173\u952e\u56e0\u7d20\uff0c\u63a8\u8350\u7ed3\u5408\u4eba\u5de5\u548c\u81ea\u52a8\u751f\u6210\u63d0\u793a\u5e76\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u8f93\u51fa\u9ad8\u5ea6\u4f9d\u8d56\u63d0\u793a\u7684\u63aa\u8f9e\u3002\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u5fc3\u7406\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\uff0c\u8fd9\u4e9b\u9886\u57df\u7684\u6784\u9020\u5177\u6709\u7cbe\u786e\u7684\u7406\u8bba\u5b9a\u4e49\uff0c\u53ef\u80fd\u672a\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u5145\u5206\u4f53\u73b0\u3002\u9700\u8981\u7cfb\u7edf\u65b9\u6cd5\u4f18\u5316LLM\u5728\u4e13\u4e1a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4f18\u5316LLM\u63d0\u793a\u7684\u5b9e\u8bc1\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc4\u4f30\u4e94\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u4ee3\u7801\u672c\u5f15\u5bfc\u7684\u5b9e\u8bc1\u63d0\u793a\u9009\u62e9\u3001\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u3001\u89d2\u8272\u63d0\u793a\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u89e3\u91ca\u6027\u63d0\u793a\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u89d2\u8272\u3001\u601d\u7ef4\u94fe\u548c\u89e3\u91ca\u5e76\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u63d0\u793a\u63aa\u8f9e\u4e0d\u5f53\u5bfc\u81f4\u7684\u6027\u80fd\u635f\u5931\u3002\u6700\u5173\u952e\u7684\u63d0\u793a\u7279\u5f81\u662f\u6784\u9020\u5b9a\u4e49\u3001\u4efb\u52a1\u6846\u67b6\u548c\u793a\u4f8b\u3002\u5728\u4e09\u4e2a\u6784\u9020\u548c\u4e24\u4e2a\u6a21\u578b\u4e2d\uff0c\u4e0e\u4e13\u5bb6\u5224\u65ad\u6700\u4e00\u81f4\u7684\u7ed3\u679c\u6765\u81ea\u7ed3\u5408\u4ee3\u7801\u672c\u5f15\u5bfc\u7684\u5b9e\u8bc1\u63d0\u793a\u9009\u62e9\u548c\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u7684\u5c11\u6837\u672c\u63d0\u793a\u3002", "conclusion": "\u5efa\u8bae\u7814\u7a76\u4eba\u5458\u751f\u6210\u548c\u8bc4\u4f30\u5c3d\u53ef\u80fd\u591a\u7684\u63d0\u793a\u53d8\u4f53\uff08\u4eba\u5de5\u5236\u4f5c\u3001\u81ea\u52a8\u751f\u6210\u6216\u4e24\u8005\u7ed3\u5408\uff09\uff0c\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u5b9e\u8bc1\u6027\u80fd\u9009\u62e9\u63d0\u793a\u548c\u793a\u4f8b\uff0c\u5e76\u5728\u4fdd\u7559\u96c6\u4e2d\u9a8c\u8bc1\u6700\u7ec8\u65b9\u6cd5\u3002\u8fd9\u4e3a\u9700\u8981\u4e0e\u4e13\u5bb6\u5224\u65ad\u5bf9\u9f50\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u7cfb\u7edf\u548c\u7406\u8bba\u9a71\u52a8\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.03838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03838", "abs": "https://arxiv.org/abs/2512.03838", "authors": ["Michael Staniek", "Artem Sokolov", "Stefan Riezler"], "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs", "comment": null, "summary": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5fae\u8c03LLMs\u9075\u5faa\u533b\u5b66\u5171\u8bc6\u6307\u5357\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u4ee5\u63d0\u5347\u533b\u7597\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u4ee5Sepsis-3\u5b9a\u4e49\u4e3a\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u673a\u5668\u5b66\u4e60\u867d\u7136\u9884\u6d4b\u6027\u80fd\u6709\u6240\u7a81\u7834\uff0c\u4f46\u8fc7\u5ea6\u5173\u6ce8\u51c6\u786e\u7387\u800c\u5ffd\u89c6\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u96be\u4ee5\u83b7\u5f97\u533b\u7597\u4ece\u4e1a\u8005\u7684\u4fe1\u4efb\u3002\u533b\u5b66\u9886\u57df\u666e\u904d\u5b58\u5728\u5171\u8bc6\u6307\u5357\uff0c\u9700\u8981\u8ba9\u6a21\u578b\u9075\u5faa\u8fd9\u4e9b\u6307\u5357\u8fdb\u884c\u63a8\u7406\u3002", "method": "\u5c06\u533b\u5b66\u5171\u8bc6\u6307\u5357\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u63a8\u7406\u89c4\u5219\uff0c\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u5b9e\u4f8b\u5bf9LLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u5b66\u4e60\u5171\u8bc6\u89c4\u5219\u53ca\u5176\u4f8b\u5916\u3002\u91c7\u7528\u591a\u6a21\u6001\u65b9\u6cd5\u6574\u5408\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u8f93\u51fa\u8868\u793a\u3002", "result": "\u5fae\u8c03\u540e\u7684\u5c0f\u6a21\u578b\u5728Sepsis-3\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f7f\u7528\u663e\u5f0f\u5b9a\u4e49\u63d0\u793a\u7684\u5927\u578bLLMs\u548c\u57fa\u4e8e\u533b\u5b66\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u5fae\u8c03\u6a21\u578b\u5728\u672a\u89c1\u60a3\u8005\u6570\u636e\u4e0a\u5bf9\u89c4\u5219\u63a8\u5bfc\u6b63\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u5bf9\u672a\u6765\u7a00\u758f\u4e0d\u89c4\u5219\u4e34\u5e8a\u53d8\u91cf\u7684\u9884\u6d4b\u3002", "conclusion": "\u901a\u8fc7\u5fae\u8c03LLMs\u9075\u5faa\u533b\u5b66\u5171\u8bc6\u6307\u5357\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u533b\u7597\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002\u591a\u6a21\u6001\u6574\u5408\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u80fd\u6539\u5584\u5bf9\u672a\u6765\u4e34\u5e8a\u53d8\u91cf\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u53ef\u4fe1\u533b\u7597AI\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2512.03870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03870", "abs": "https://arxiv.org/abs/2512.03870", "authors": ["Hongzhan Lin", "Zhiqi Bai", "Xinmiao Zhang", "Sen Yang", "Xiang Li", "Siran Yang", "Yunlong Xu", "Jiaheng Liu", "Yongchi Zhao", "Jiamang Wang", "Yuchi Xu", "Wenbo Su", "Bo Zheng"], "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers", "comment": "under review", "summary": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.", "AI": {"tldr": "FusedKV\u901a\u8fc7\u878d\u5408\u5e95\u5c42\u548c\u4e2d\u5c42KV\u7f13\u5b58\u6765\u51cf\u5c11Transformer\u89e3\u7801\u5668\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd", "motivation": "Transformer\u89e3\u7801\u5668\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2dKV\u7f13\u5b58\u5185\u5b58\u6d88\u8017\u8fc7\u5927\uff0c\u73b0\u6709\u7684\u8de8\u5c42KV\u7f13\u5b58\u5171\u4eab\u65b9\u6cd5\u6027\u80fd\u4e0d\u5982GQA\u7b49\u5c42\u5185\u65b9\u6cd5\uff0c\u9700\u8981\u627e\u5230\u66f4\u9ad8\u6548\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6848", "method": "\u5206\u6790\u9876\u5c42KV\u4fe1\u606f\u6d41\u53d1\u73b0\uff1a\u503c\u4e3b\u8981\u6765\u81ea\u5e95\u5c42\uff0c\u952e\u540c\u65f6\u6765\u81ea\u5e95\u5c42\u548c\u4e2d\u5c42\u3002\u63d0\u51faFusedKV\uff0c\u9876\u5c42KV\u7f13\u5b58\u662f\u53ef\u5b66\u4e60\u7684\u5e95\u5c42\u548c\u4e2d\u5c42\u6700\u6709\u4fe1\u606f\u91cf\u7684KV\u878d\u5408\uff0c\u76f4\u63a5\u5728RoPE\u540e\u7684\u952e\u4e0a\u64cd\u4f5c\u3002\u8fd8\u63d0\u51faFusedKV-Lite\uff0c\u9876\u5c42KV\u7f13\u5b58\u76f4\u63a5\u6765\u81ea\u5e95\u5c42\u503c\u548c\u4e2d\u5c42\u952e", "result": "\u5728332M\u52304B\u53c2\u6570\u7684LLM\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c1150%\u7f13\u5b58\u5185\u5b58\uff0c\u540c\u65f6\u83b7\u5f97\u6bd4\u6807\u51c6Transformer\u89e3\u7801\u5668\u66f4\u4f4e\u7684\u9a8c\u8bc1\u56f0\u60d1\u5ea6", "conclusion": "FusedKV\u662f\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u67b6\u6784\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u667a\u80fd\u878d\u5408\u4e0d\u540c\u5c42\u7684KV\u7f13\u5b58\u4fe1\u606f\uff0c\u5728\u51cf\u5c11\u5185\u5b58\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2512.03903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03903", "abs": "https://arxiv.org/abs/2512.03903", "authors": ["Ekhi Azurmendi", "Joseba Fernandez de Landa", "Jaione Bengoetxea", "Maite Heredia", "Julen Etxaniz", "Mikel Zubillaga", "Ander Soraluze", "Aitor Soroa"], "title": "BERnaT: Basque Encoders for Representing Natural Textual Diversity", "comment": "Submitted to LREC 2026", "summary": "Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3b\u5f20\u8bed\u8a00\u6a21\u578b\u5e94\u6355\u6349\u8bed\u8a00\u591a\u6837\u6027\u800c\u975e\u4ec5\u4f9d\u8d56\u6807\u51c6\u5316\u6587\u672c\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u6807\u51c6\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u5386\u53f2\u6587\u672c\u7684\u5df4\u65af\u514b\u8bed\u8bed\u6599\u5e93\uff0c\u8bad\u7ec3\u4e86\u4e09\u79cd\u914d\u7f6e\u7684BERnaT\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u7ed3\u5408\u591a\u6837\u6570\u636e\u7684\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6587\u672c\u8bed\u6599\u5e93\uff0c\u4f46\u8fc7\u6ee4\u8fc7\u7a0b\u53ef\u80fd\u6392\u9664\u975e\u6807\u51c6\u8bed\u8a00\u53d8\u4f53\uff08\u65b9\u8a00\u3001\u5386\u53f2\u3001\u975e\u6b63\u5f0f\u7b49\uff09\uff0c\u8fd9\u4f1a\u964d\u4f4e\u6a21\u578b\u9c81\u68d2\u6027\u5e76\u5f3a\u5316\u4ee3\u8868\u6027\u504f\u89c1\u3002\u4f5c\u8005\u8ba4\u4e3a\u8bed\u8a00\u6a21\u578b\u5e94\u6355\u6349\u5b8c\u6574\u7684\u8bed\u8a00\u53d8\u5f02\u8c31\u7cfb\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u6807\u51c6\u5316\u6587\u672c\u3002", "method": "\u9488\u5bf9\u5f62\u6001\u4e30\u5bcc\u4e14\u8d44\u6e90\u7a00\u7f3a\u7684\u5df4\u65af\u514b\u8bed\uff0c\u6784\u5efa\u5305\u542b\u6807\u51c6\u6587\u672c\u3001\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u548c\u5386\u53f2\u8d44\u6599\u7684\u65b0\u8bed\u6599\u5e93\u3002\u9884\u8bad\u7ec3\u4e09\u79cd\u914d\u7f6e\u7684BERnaT\u7f16\u7801\u5668\u6a21\u578b\uff1a\u6807\u51c6\u7248\u3001\u591a\u6837\u7248\u548c\u7ed3\u5408\u7248\u3002\u63d0\u51fa\u5c06\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u5206\u4e3a\u6807\u51c6\u5b50\u96c6\u548c\u591a\u6837\u5b50\u96c6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u8bc4\u4f30\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6807\u51c6\u6570\u636e\u548c\u591a\u6837\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f7f\u7528\u6807\u51c6\u8bed\u6599\u5e93\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u6240\u6709\u4efb\u52a1\u7c7b\u578b\u4e0a\u90fd\u6709\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u4e0d\u635f\u5bb3\u6807\u51c6\u57fa\u51c6\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8bed\u8a00\u591a\u6837\u6027\u5728\u6784\u5efa\u5305\u5bb9\u6027\u3001\u53ef\u6cdb\u5316\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u8868\u660e\u7ed3\u5408\u591a\u6837\u8bed\u8a00\u53d8\u4f53\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u800c\u4e0d\u727a\u7272\u6807\u51c6\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2512.03943", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03943", "abs": "https://arxiv.org/abs/2512.03943", "authors": ["Kazi Abrab Hossain", "Jannatul Somiya Mahmud", "Maria Hossain Tuli", "Anik Mitra", "S. M. Taiabul Haque", "Farig Y. Sadeque"], "title": "Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions", "comment": "18 pages, 7 figures", "summary": "While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.", "AI": {"tldr": "BRAND\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5b97\u6559\u80cc\u666f\u4e0b\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u82f1\u8bed\u4e2d\u8868\u73b0\u4f18\u4e8e\u5b5f\u52a0\u62c9\u8bed\uff0c\u4e14\u5bf9\u4f0a\u65af\u5170\u6559\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u504f\u89c1\u68c0\u6d4b\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5b97\u6559\u7b49\u654f\u611f\u8bdd\u9898\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u56e0\u4e3a\u5fae\u5c0f\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u8bef\u89e3\u3002\u591a\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u8bef\u8bfb\u5b97\u6559\u5185\u5bb9\uff0c\u5728\u5b97\u6559\u8bed\u5883\u4e2d\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u5f15\u5165BRAND\u6570\u636e\u96c6\uff0c\u805a\u7126\u5357\u4e9a\u56db\u5927\u5b97\u6559\uff08\u4f5b\u6559\u3001\u57fa\u7763\u6559\u3001\u5370\u5ea6\u6559\u3001\u4f0a\u65af\u5170\u6559\uff09\uff0c\u5305\u542b2400\u591a\u4e2a\u6761\u76ee\uff0c\u4f7f\u7528\u82f1\u8bed\u548c\u5b5f\u52a0\u62c9\u8bed\u7684\u4e09\u79cd\u63d0\u793a\u7c7b\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u82f1\u8bed\u4e2d\u8868\u73b0\u4f18\u4e8e\u5b5f\u52a0\u62c9\u8bed\uff0c\u5373\u4f7f\u56de\u7b54\u5b97\u6559\u4e2d\u7acb\u95ee\u9898\u65f6\u4e5f\u6301\u7eed\u8868\u73b0\u51fa\u5bf9\u4f0a\u65af\u5170\u6559\u7684\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u63d0\u95ee\u76f8\u4f3c\u95ee\u9898\u65f6\u7684\u6301\u7eed\u504f\u89c1\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5b97\u6559\u5185\u5bb9\u65f6\u7684\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u5c06\u8fd9\u4e9b\u53d1\u73b0\u4e0e\u4eba\u673a\u4ea4\u4e92\u4e2d\u5b97\u6559\u548c\u7075\u6027\u76f8\u5173\u7684\u66f4\u5e7f\u6cdb\u95ee\u9898\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2512.03976", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03976", "abs": "https://arxiv.org/abs/2512.03976", "authors": ["Lifeng Chen", "Ryan Lai", "Tianming Liu"], "title": "Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study", "comment": null, "summary": "Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\\rightarrow$ 1.54) and substantial improvements in Chinese$\\rightarrow$Tibetan translation quality (BLEU: 0.046 $\\rightarrow$ 0.261; chrF: 2.2 $\\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5\u5c06Qwen2.5-3B\u6a21\u578b\u9002\u914d\u5230\u85cf\u8bed\uff1a\u6301\u7eed\u9884\u8bad\u7ec3\u5efa\u7acb\u8bed\u8a00\u57fa\u7840\uff0c\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u7ffb\u8bd1\u4efb\u52a1\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u56f0\u60d1\u5ea6\u5e76\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u85cf\u8bed\uff09\u4e0a\u7684\u9002\u914d\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u8de8\u8bed\u8a00\u6f02\u79fb\u7684\u6311\u6218\uff0c\u9700\u8981\u63a2\u7d22\u6709\u6548\u7684\u9002\u914d\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9002\u914d\uff1a1\uff09\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u5efa\u7acb\u85cf\u8bed\u8bed\u8a00\u57fa\u7840\uff1b2\uff09\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u8fdb\u884c\u4efb\u52a1\u548c\u7ffb\u8bd1\u4e13\u4e1a\u5316\u3002\u5bf9Qwen3-4B\u7684435\u5c42\u8fdb\u884c\u5c42\u95f4\u5206\u6790\u3002", "result": "\u56f0\u60d1\u5ea6\u4ece2.98\u964d\u81f31.54\uff1b\u6c49\u85cf\u7ffb\u8bd1\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff08BLEU\u4ece0.046\u52300.261\uff0cchrF\u4ece2.2\u52306.6\uff09\u3002\u9002\u914d\u4e3b\u8981\u96c6\u4e2d\u5728\u5d4c\u5165\u5c42\u548c\u8f93\u51fa\u5934\uff0c\u4e2d\u540e\u671fMLP\u6295\u5f71\u7f16\u7801\u9886\u57df\u7279\u5b9a\u8f6c\u6362\u3002", "conclusion": "CPT\u6784\u5efa\u85cf\u8bed\u8bed\u4e49\u6d41\u5f62\uff0cSFT\u4ee5\u6700\u5c0f\u8868\u5f81\u6270\u52a8\u9510\u5316\u4efb\u52a1\u5bf9\u9f50\u3002\u8be5\u7814\u7a76\u9996\u6b21\u5b9a\u91cf\u63a2\u7d22\u85cf\u8bed\u9002\u914d\u52a8\u6001\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u9002\u914d\u63d0\u4f9b\u53ef\u590d\u73b0\u6846\u67b6\u3002"}}
{"id": "2512.03989", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03989", "abs": "https://arxiv.org/abs/2512.03989", "authors": ["Taido Purason", "Pavel Chizhov", "Ivan P. Yamshchikov", "Mark Fishel"], "title": "Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models", "comment": null, "summary": "Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cdtokenizer\u9002\u5e94\u65b9\u6cd5\uff1a\u6301\u7eedBPE\u8bad\u7ec3\u7528\u4e8e\u8bcd\u6c47\u6269\u5c55\uff0c\u53f6\u57fa\u8bcd\u6c47\u526a\u679d\u7528\u4e8e\u8bcd\u6c47\u7f29\u51cf\uff0c\u5171\u540c\u5b9e\u73b0\u53ef\u63a7\u8bcd\u6c47\u4fee\u6539", "motivation": "\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u5230\u65b0\u9886\u57df\u6216\u8bed\u8a00\u65f6\uff0ctokenizer\u9002\u5e94\u5f88\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u6269\u5c55\u8bcd\u6c47\u65f6\u6dfb\u52a0\u7684token\u5f80\u5f80\u65e0\u6cd5\u8bbf\u95ee\u6216\u4ece\u672a\u4f7f\u7528\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u9002\u5e94\u65b9\u6cd5", "method": "1. \u6301\u7eedBPE\u8bad\u7ec3\uff1a\u5728\u9884\u8bad\u7ec3tokenizer\u57fa\u7840\u4e0a\uff0c\u5728\u65b0\u6570\u636e\u4e0a\u7ee7\u7eedBPE\u5408\u5e76\u5b66\u4e60\u8fc7\u7a0b\u6765\u9002\u5e94tokenizer\uff1b2. \u53f6\u57fa\u8bcd\u6c47\u526a\u679d\uff1a\u79fb\u9664\u5197\u4f59token\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf", "result": "\u8de8\u591a\u79cd\u8bed\u8a00\u548c\u6a21\u578b\u5bb6\u65cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86tokenization\u6548\u7387\uff0c\u5e76\u66f4\u597d\u5730\u5229\u7528\u4e86\u6dfb\u52a0\u7684\u8bcd\u6c47", "conclusion": "\u8fd9\u4e24\u79cd\u65b9\u6cd5\u4e3a\u53ef\u63a7\u8bcd\u6c47\u4fee\u6539\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5df2\u4f5c\u4e3a\u5f00\u6e90\u5305\u53d1\u5e03"}}
{"id": "2512.04013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04013", "abs": "https://arxiv.org/abs/2512.04013", "authors": ["Ying Wang", "Zhen Jin", "Jiexiong Xu", "Wenhai Lin", "Yiquan Chen", "Wenzhi Chen"], "title": "AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving", "comment": null, "summary": "As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.\n  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.", "AI": {"tldr": "AugServe\uff1a\u9488\u5bf9\u589e\u5f3a\u578bLLM\u63a8\u7406\u670d\u52a1\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u8bf7\u6c42\u8c03\u5ea6\u548c\u52a8\u6001\u4ee4\u724c\u6279\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u6709\u6548\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5728Web\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u63d0\u5347\u63a8\u7406\u670d\u52a1\u6548\u7387\u548c\u4f18\u5316\u670d\u52a1\u7ea7\u522b\u76ee\u6807\u5bf9\u6539\u5584\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7cfb\u7edf\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u5148\u5230\u5148\u670d\u52a1\u8c03\u5ea6\u5bfc\u81f4\u4e25\u91cd\u7684\u961f\u5934\u963b\u585e\uff0c\u4f7f\u8bb8\u591a\u8bf7\u6c42\u7684\u6392\u961f\u5ef6\u8fdf\u8d85\u51faSLO\uff1b2) \u9759\u6001\u6279\u5904\u7406\u4ee4\u724c\u9650\u5236\u65e0\u6cd5\u9002\u5e94\u6ce2\u52a8\u8d1f\u8f7d\u548c\u786c\u4ef6\u6761\u4ef6\uff0c\u8fd9\u4e24\u8005\u90fd\u964d\u4f4e\u4e86\u6709\u6548\u541e\u5410\u91cf\u548c\u670d\u52a1\u8d28\u91cf\u3002", "method": "AugServe\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u8bf7\u6c42\u8c03\u5ea6\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u7ed3\u5408\u589e\u5f3a\u578bLLM\u8bf7\u6c42\u7684\u63a8\u7406\u7279\u5f81\u4f18\u5316\u8c03\u5ea6\u51b3\u7b56\u987a\u5e8f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u8fd0\u884c\u65f6\u4fe1\u606f\u6301\u7eed\u4f18\u5316\u51b3\u7b56\uff0c\u9002\u5e94\u8bf7\u6c42\u7279\u5f81\u548c\u7cfb\u7edf\u80fd\u529b\u3002\u6b64\u5916\uff0cAugServe\u6839\u636e\u786c\u4ef6\u72b6\u6001\u548c\u5b9e\u65f6\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u4ee4\u724c\u6279\u5904\u7406\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAugServe\u76f8\u6bd4vLLM\u548cInferCept\u5b9e\u73b0\u4e864.7-33.1\u500d\u548c3.3-13.2\u500d\u7684\u6709\u6548\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u5c06\u9996\u4ee4\u724c\u65f6\u95f4\u5206\u522b\u964d\u4f4e\u4e8696.3%\u548c95.0%\u3002", "conclusion": "AugServe\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u8c03\u5ea6\u548c\u52a8\u6001\u6279\u5904\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u589e\u5f3a\u578bLLM\u63a8\u7406\u670d\u52a1\u4e2d\u7684\u961f\u5934\u963b\u585e\u548c\u9759\u6001\u6279\u5904\u7406\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2512.04032", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04032", "abs": "https://arxiv.org/abs/2512.04032", "authors": ["Andreas Koukounas", "Georgios Mastrapas", "Florian H\u00f6nicke", "Sedigheh Eslami", "Guillaume Roncari", "Scott Martens", "Han Xiao"], "title": "Jina-VLM: Small Multilingual Vision Language Model", "comment": "18 pages, 1-7 main content", "summary": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.", "AI": {"tldr": "Jina-VLM\u662f\u4e00\u4e2a24\u4ebf\u53c2\u6570\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u57282B\u89c4\u6a21\u7684\u5f00\u6e90VLM\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u6027\u80fd", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u57282B\u53c2\u6570\u89c4\u6a21\u4e0b\u5177\u6709\u7ade\u4e89\u529b\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6587\u672c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b", "method": "\u7ed3\u5408SigLIP2\u89c6\u89c9\u7f16\u7801\u5668\u548cQwen3\u8bed\u8a00\u9aa8\u5e72\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6c60\u5316\u8fde\u63a5\u5668\u5b9e\u73b0\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u7684token\u9ad8\u6548\u5904\u7406", "result": "\u5728\u6807\u51c6VQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u8bed\u8a00\u8bc4\u4f30\u4e2d\uff0cJina-VLM\u8d85\u8d8a\u4e86\u53ef\u6bd4\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7eaf\u6587\u672c\u6027\u80fd", "conclusion": "Jina-VLM\u57282B\u53c2\u6570\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u67b6\u6784\u8bbe\u8ba1\u7684\u6709\u6548\u6027"}}
{"id": "2512.04072", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04072", "abs": "https://arxiv.org/abs/2512.04072", "authors": ["Zayne Sprague", "Jack Lu", "Manya Wadhwa", "Sedrick Keh", "Mengye Ren", "Greg Durrett"], "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors", "comment": null, "summary": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.", "AI": {"tldr": "SkillFactory\u662f\u4e00\u79cd\u5728\u5f3a\u5316\u5b66\u4e60\u524d\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8ba9\u6a21\u578b\u5b66\u4e60\u8ba4\u77e5\u6280\u80fd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5217\u6a21\u578b\u81ea\u8eab\u8f93\u51fa\u6765\u521b\u5efa\u8bad\u7ec3\u6570\u636e\uff0c\u5e2e\u52a9\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u66f4\u597d\u5730\u6cdb\u5316\u5230\u66f4\u56f0\u96be\u7684\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u867d\u7136\u80fd\u5229\u7528\u5404\u79cd\u8ba4\u77e5\u6280\u80fd\uff08\u5982\u9a8c\u8bc1\u7b54\u6848\u3001\u56de\u6eaf\u3001\u5c1d\u8bd5\u4e0d\u540c\u65b9\u6cd5\u7b49\uff09\uff0c\u4f46\u8fd9\u4e9b\u6280\u80fd\u901a\u5e38\u9700\u8981\u57fa\u7840\u6a21\u578b\u5df2\u7ecf\u5177\u5907\u3002\u5f53\u57fa\u7840\u6a21\u578b\u4e0d\u5177\u5907\u8fd9\u4e9b\u6280\u80fd\u65f6\uff0c\u5982\u4f55\u8ba9\u6a21\u578b\u5b66\u4f1a\u5229\u7528\u5b83\u4eec\uff1f", "method": "SkillFactory\u5728\u5f3a\u5316\u5b66\u4e60\u524d\u589e\u52a0\u4e00\u4e2a\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff0c\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u7684\u8f93\u51fa\u91cd\u65b0\u6392\u5217\u6765\u521b\u5efa\"\u94f6\u6807\"\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u867d\u7136\u4e0d\u5b8c\u7f8e\u4f46\u80fd\u5e2e\u52a9\u6a21\u578b\u521d\u6b65\u5b66\u4e60\u8ba4\u77e5\u6280\u80fd\uff0c\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u505a\u597d\u51c6\u5907\u3002", "result": "1) SkillFactory SFT\u521d\u59cb\u5316\u5e2e\u52a9\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u66f4\u597d\u5730\u6cdb\u5316\u5230\u66f4\u56f0\u96be\u7684\u4efb\u52a1\u53d8\u4f53\uff1b2) \u6a21\u578b\u786e\u5b9e\u4f7f\u7528\u4e86\u8ba4\u77e5\u6280\u80fd\uff1b3) SkillFactory\u6a21\u578b\u5728\u57df\u5916\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7840\u6a21\u578b\u66f4\u7a33\u5065\uff0c\u4e0d\u6613\u51fa\u73b0\u6027\u80fd\u9000\u5316\u3002", "conclusion": "\u5728\u5f3a\u5316\u5b66\u4e60\u524d\u5b66\u4e60\u7684\u5f52\u7eb3\u504f\u7f6e\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u7a33\u5065\u7684\u8ba4\u77e5\u6280\u80fd\u4f7f\u7528\uff0cSkillFactory\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u66f4\u5f3a\u6a21\u578b\u84b8\u998f\u7684\u6709\u6548\u9014\u5f84\u3002"}}
