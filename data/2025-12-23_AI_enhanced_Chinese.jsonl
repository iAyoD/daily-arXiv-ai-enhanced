{"id": "2512.17940", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17940", "abs": "https://arxiv.org/abs/2512.17940", "authors": ["Xi Wang", "Jing Liu", "Siqian Li", "Hengtai Dai", "Jung-Che Chang", "Adam Rushworth", "Xin Dong"], "title": "Untethered thin dielectric elastomer actuated soft robot", "comment": null, "summary": "Thin dielectric elastomer actuator (DEA) features a unique in-plane configuration, enabling low-profile designs capable of accessing millimetre-scale narrow spaces. However, most existing DEA-powered soft robots require high voltages and wired power connections, limiting their ability to operate in confined environments. This study presents an untethered thin soft robot (UTS-Robot) powered by thin dielectric elastomer actuators (TS-DEA). The robot measures 38 mm in length, 6 mm in height, and weighs just 2.34 grams, integrating flexible onboard electronics to achieve fully untethered actuation. The TS-DEA, operating at resonant frequencies of 86 Hz under a low driving voltage of 220 V, adopts a dual-actuation sandwiched structure, comprising four dielectric elastomer layers bonded to a compressible tensioning mechanism at its core. This design enables high power density actuation and locomotion via three directional friction pads. The low-voltage actuation is achieved by fabricating each elastomer layer via spin coating to an initial thickness of 50 um, followed by biaxial stretching to 8 um. A comprehensive design and modelling framework has been developed to optimise TS-DEA performance. Experimental evaluations demonstrate that the bare TS-DEA achieves a locomotion speed of 12.36 mm/s at resonance, the untethered configuration achieves a locomotion speed of 0.5 mm/s, making it highly suitable for navigating confined and complex environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u7ef3\u8584\u578b\u8f6f\u4f53\u673a\u5668\u4eba\uff08UTS-Robot\uff09\uff0c\u91c7\u7528\u8584\u578b\u4ecb\u7535\u5f39\u6027\u4f53\u9a71\u52a8\u5668\uff08TS-DEA\uff09\uff0c\u5728\u4f4e\u7535\u538b\uff08220V\uff09\u4e0b\u5b9e\u73b0\u65e0\u7ef3\u9a71\u52a8\uff0c\u9002\u7528\u4e8e\u72ed\u7a84\u7a7a\u95f4\u4f5c\u4e1a\u3002", "motivation": "\u73b0\u6709\u4ecb\u7535\u5f39\u6027\u4f53\u9a71\u52a8\u5668\uff08DEA\uff09\u8f6f\u4f53\u673a\u5668\u4eba\u901a\u5e38\u9700\u8981\u9ad8\u7535\u538b\u548c\u6709\u7ebf\u4f9b\u7535\uff0c\u9650\u5236\u4e86\u5176\u5728\u72ed\u7a84\u7a7a\u95f4\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u4f4e\u7535\u538b\u3001\u65e0\u7ef3\u7684\u8584\u578b\u8f6f\u4f53\u673a\u5668\u4eba\u4ee5\u9002\u5e94\u53d7\u9650\u73af\u5883\u3002", "method": "\u91c7\u7528\u53cc\u9a71\u52a8\u5939\u5c42\u7ed3\u6784\u7684TS-DEA\uff0c\u5305\u542b\u56db\u4e2a\u4ecb\u7535\u5f39\u6027\u4f53\u5c42\u548c\u53ef\u538b\u7f29\u5f20\u529b\u673a\u5236\u3002\u901a\u8fc7\u65cb\u6d82\u6cd5\u5236\u590750\u5fae\u7c73\u539a\u5f39\u6027\u4f53\u5c42\uff0c\u518d\u53cc\u8f74\u62c9\u4f38\u81f38\u5fae\u7c73\u4ee5\u964d\u4f4e\u9a71\u52a8\u7535\u538b\u3002\u96c6\u6210\u67d4\u6027\u677f\u8f7d\u7535\u5b50\u5b9e\u73b0\u65e0\u7ef3\u9a71\u52a8\uff0c\u5229\u7528\u4e09\u4e2a\u65b9\u5411\u6469\u64e6\u57ab\u5b9e\u73b0\u8fd0\u52a8\u3002", "result": "\u673a\u5668\u4eba\u5c3a\u5bf838mm\u00d76mm\uff0c\u91cd2.34g\u3002TS-DEA\u572886Hz\u8c10\u632f\u9891\u7387\u4e0b\u4ec5\u9700220V\u9a71\u52a8\u7535\u538b\u3002\u88f8TS-DEA\u8fd0\u52a8\u901f\u5ea6\u8fbe12.36mm/s\uff0c\u65e0\u7ef3\u914d\u7f6e\u4e0b\u901f\u5ea6\u4e3a0.5mm/s\uff0c\u9002\u5408\u72ed\u7a84\u590d\u6742\u73af\u5883\u5bfc\u822a\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u4f4e\u7535\u538b\u3001\u65e0\u7ef3\u7684\u8584\u578b\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u4f18\u5316\u7684TS-DEA\u8bbe\u8ba1\u548c\u5236\u9020\u5de5\u827a\u5b9e\u73b0\u4e86\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6709\u6548\u8fd0\u52a8\uff0c\u4e3a\u72ed\u7a84\u7a7a\u95f4\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.17958", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17958", "abs": "https://arxiv.org/abs/2512.17958", "authors": ["Farida Mohsen", "Ali Safa"], "title": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization", "comment": null, "summary": "Service robots in public spaces require real-time understanding of human behavioral intentions for natural interaction. We present a practical multimodal framework for frame-accurate human-robot interaction intent detection that fuses camera-invariant 2D skeletal pose and facial emotion features extracted from monocular RGB video. Unlike prior methods requiring RGB-D sensors or GPU acceleration, our approach resource-constrained embedded hardware (Raspberry Pi 5, CPU-only). To address the severe class imbalance in natural human-robot interaction datasets, we introduce a novel approach to synthesize temporally coherent pose-emotion-label sequences for data re-balancing called MINT-RVAE (Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation). Comprehensive offline evaluations under cross-subject and cross-scene protocols demonstrate strong generalization performance, achieving frame- and sequence-level AUROC of 0.95. Crucially, we validate real-world generalization through cross-camera evaluation on the MIRA robot head, which employs a different onboard RGB sensor and operates in uncontrolled environments not represented in the training data. Despite this domain shift, the deployed system achieves 91% accuracy and 100% recall across 32 live interaction trials. The close correspondence between offline and deployed performance confirms the cross-sensor and cross-environment robustness of the proposed multimodal approach, highlighting its suitability for ubiquitous multimedia-enabled social robots.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u670d\u52a1\u673a\u5668\u4eba\u5b9e\u65f6\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u610f\u56fe\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u878d\u54082D\u9aa8\u9abc\u59ff\u6001\u548c\u9762\u90e8\u60c5\u611f\u7279\u5f81\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u5e76\u901a\u8fc7MINT-RVAE\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u8de8\u4f20\u611f\u5668\u548c\u8de8\u73af\u5883\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u516c\u5171\u7a7a\u95f4\u7684\u670d\u52a1\u673a\u5668\u4eba\u9700\u8981\u5b9e\u65f6\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u610f\u56fe\u4ee5\u5b9e\u73b0\u81ea\u7136\u4ea4\u4e92\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981RGB-D\u4f20\u611f\u5668\u6216GPU\u52a0\u901f\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u786c\u4ef6\uff0c\u4e14\u81ea\u7136\u4eba\u673a\u4ea4\u4e92\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u4ece\u5355\u76eeRGB\u89c6\u9891\u4e2d\u63d0\u53d6\u76f8\u673a\u4e0d\u53d8\u76842D\u9aa8\u9abc\u59ff\u6001\u548c\u9762\u90e8\u60c5\u611f\u7279\u5f81\uff0c\u878d\u5408\u540e\u8fdb\u884c\u610f\u56fe\u68c0\u6d4b\u3002\u4e3a\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51faMINT-RVAE\uff08\u591a\u6a21\u6001\u5faa\u73af\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff09\u6765\u5408\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u59ff\u6001-\u60c5\u611f-\u6807\u7b7e\u5e8f\u5217\u8fdb\u884c\u6570\u636e\u91cd\u5e73\u8861\u3002\u6846\u67b6\u5728Raspberry Pi 5\u7b49CPU-only\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u8fd0\u884c\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u5728\u8de8\u4e3b\u4f53\u548c\u8de8\u573a\u666f\u534f\u8bae\u4e0b\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u6027\u80fd\uff0c\u5e27\u7ea7\u548c\u5e8f\u5217\u7ea7AUROC\u8fbe\u52300.95\u3002\u5728MIRA\u673a\u5668\u4eba\u5934\u4e0a\u8fdb\u884c\u8de8\u76f8\u673a\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\uff08\u4f7f\u7528\u4e0d\u540cRGB\u4f20\u611f\u5668\u4e14\u73af\u5883\u672a\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u51fa\u73b0\uff09\uff0c\u90e8\u7f72\u7cfb\u7edf\u572832\u6b21\u5b9e\u65f6\u4ea4\u4e92\u8bd5\u9a8c\u4e2d\u8fbe\u523091%\u51c6\u786e\u7387\u548c100%\u53ec\u56de\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u79bb\u7ebf\u8bc4\u4f30\u548c\u5b9e\u9645\u90e8\u7f72\u6027\u80fd\u4e4b\u95f4\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u8bc1\u5b9e\u4e86\u5176\u8de8\u4f20\u611f\u5668\u548c\u8de8\u73af\u5883\u7684\u9c81\u68d2\u6027\uff0c\u9002\u5408\u7528\u4e8e\u666e\u904d\u5b58\u5728\u7684\u591a\u5a92\u4f53\u4f7f\u80fd\u793e\u4ea4\u673a\u5668\u4eba\u3002"}}
{"id": "2512.17992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17992", "abs": "https://arxiv.org/abs/2512.17992", "authors": ["Qianwei Wang", "Bowen Li", "Zhanpeng Luo", "Yifan Xu", "Alexander Gray", "Tom Silver", "Sebastian Scherer", "Katia Sycara", "Yaqi Xie"], "title": "Unifying Deep Predicate Invention with Pre-trained Foundation Models", "comment": "18 pages, 11 figures", "summary": "Long-horizon robotic tasks are hard due to continuous state-action spaces and sparse feedback. Symbolic world models help by decomposing tasks into discrete predicates that capture object properties and relations. Existing methods learn predicates either top-down, by prompting foundation models without data grounding, or bottom-up, from demonstrations without high-level priors. We introduce UniPred, a bilevel learning framework that unifies both. UniPred uses large language models (LLMs) to propose predicate effect distributions that supervise neural predicate learning from low-level data, while learned feedback iteratively refines the LLM hypotheses. Leveraging strong visual foundation model features, UniPred learns robust predicate classifiers in cluttered scenes. We further propose a predicate evaluation method that supports symbolic models beyond STRIPS assumptions. Across five simulated and one real-robot domains, UniPred achieves 2-4 times higher success rates than top-down methods and 3-4 times faster learning than bottom-up approaches, advancing scalable and flexible symbolic world modeling for robotics.", "AI": {"tldr": "UniPred\uff1a\u4e00\u4e2a\u53cc\u5c42\u5b66\u4e60\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u81ea\u4e0a\u800c\u4e0b\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u8c13\u8bcd\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u63d0\u51fa\u8c13\u8bcd\u6548\u679c\u5206\u5e03\u6765\u76d1\u7763\u795e\u7ecf\u7f51\u7edc\u4ece\u4f4e\u7ea7\u6570\u636e\u4e2d\u5b66\u4e60\u8c13\u8bcd\uff0c\u540c\u65f6\u5229\u7528\u5b66\u4e60\u53cd\u9988\u8fed\u4ee3\u4f18\u5316LLM\u5047\u8bbe\uff0c\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u5feb\u7684\u5b66\u4e60\u901f\u5ea6\u3002", "motivation": "\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u4efb\u52a1\u7531\u4e8e\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u548c\u7a00\u758f\u53cd\u9988\u800c\u96be\u4ee5\u5904\u7406\u3002\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u6355\u6349\u5bf9\u8c61\u5c5e\u6027\u548c\u5173\u7cfb\u7684\u79bb\u6563\u8c13\u8bcd\u6765\u5e2e\u52a9\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u81ea\u4e0a\u800c\u4e0b\uff08\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u63d0\u793a\u4f46\u7f3a\u4e4f\u6570\u636e\u57fa\u7840\uff09\uff0c\u8981\u4e48\u81ea\u4e0b\u800c\u4e0a\uff08\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u4f46\u7f3a\u4e4f\u9ad8\u7ea7\u5148\u9a8c\uff09\uff0c\u4e24\u8005\u90fd\u6709\u5c40\u9650\u6027\u3002", "method": "UniPred\u91c7\u7528\u53cc\u5c42\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u51fa\u8c13\u8bcd\u6548\u679c\u5206\u5e03\uff0c\u8fd9\u4e9b\u5206\u5e03\u76d1\u7763\u4ece\u4f4e\u7ea7\u6570\u636e\u4e2d\u5b66\u4e60\u795e\u7ecf\u8c13\u8bcd\uff1b2\uff09\u5b66\u4e60\u5230\u7684\u53cd\u9988\u8fed\u4ee3\u4f18\u5316LLM\u5047\u8bbe\u3002\u5229\u7528\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7279\u5f81\u5728\u6742\u4e71\u573a\u666f\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u8c13\u8bcd\u5206\u7c7b\u5668\u3002\u8fd8\u63d0\u51fa\u4e86\u652f\u6301\u8d85\u8d8aSTRIPS\u5047\u8bbe\u7684\u7b26\u53f7\u6a21\u578b\u7684\u8c13\u8bcd\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u62df\u548c\u4e00\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u9886\u57df\u4e2d\uff0cUniPred\u5b9e\u73b0\u4e86\u6bd4\u81ea\u4e0a\u800c\u4e0b\u65b9\u6cd5\u9ad82-4\u500d\u7684\u6210\u529f\u7387\uff0c\u6bd4\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\u5feb3-4\u500d\u7684\u5b66\u4e60\u901f\u5ea6\u3002\u8be5\u65b9\u6cd5\u63a8\u8fdb\u4e86\u673a\u5668\u4eba\u9886\u57df\u53ef\u6269\u5c55\u548c\u7075\u6d3b\u7684\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u3002", "conclusion": "UniPred\u901a\u8fc7\u7edf\u4e00\u81ea\u4e0a\u800c\u4e0b\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u7b26\u53f7\u8c13\u8bcd\u5b66\u4e60\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18007", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18007", "abs": "https://arxiv.org/abs/2512.18007", "authors": ["Yu Fang", "Kanchana Ranasinghe", "Le Xue", "Honglu Zhou", "Juntao Tan", "Ran Xu", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese", "Daniel Szafir", "Mingyu Ding", "Michael S. Ryoo", "Juan Carlos Niebles"], "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion", "comment": "Website: https://vla-motion.github.io/", "summary": "Vision-Language-Action (VLA) models have achieved remarkable progress in robotic manipulation by mapping multimodal observations and instructions directly to actions. However, they typically mimic expert trajectories without predictive motion reasoning, which limits their ability to reason about what actions to take. To address this limitation, we propose joint learning with motion image diffusion, a novel strategy that enhances VLA models with motion reasoning capabilities. Our method extends the VLA architecture with a dual-head design: while the action head predicts action chunks as in vanilla VLAs, an additional motion head, implemented as a Diffusion Transformer (DiT), predicts optical-flow-based motion images that capture future dynamics. The two heads are trained jointly, enabling the shared VLM backbone to learn representations that couple robot control with motion knowledge. This joint learning builds temporally coherent and physically grounded representations without modifying the inference pathway of standard VLAs, thereby maintaining test-time latency. Experiments in both simulation and real-world environments demonstrate that joint learning with motion image diffusion improves the success rate of pi-series VLAs to 97.5% on the LIBERO benchmark and 58.0% on the RoboTwin benchmark, yielding a 23% improvement in real-world performance and validating its effectiveness in enhancing the motion reasoning capability of large-scale VLAs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u8fd0\u52a8\u56fe\u50cf\u6269\u6563\u6765\u589e\u5f3aVLA\u6a21\u578b\u8fd0\u52a8\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u5ef6\u8fdf\u4e0d\u53d8\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd", "motivation": "\u73b0\u6709\u7684Vision-Language-Action\u6a21\u578b\u901a\u5e38\u53ea\u662f\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u4f5c\u7684\u9884\u6d4b\u6027\u8fd0\u52a8\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u51b3\u5b9a\u91c7\u53d6\u4f55\u79cd\u52a8\u4f5c\u7684\u80fd\u529b", "method": "\u91c7\u7528\u53cc\u5934\u8bbe\u8ba1\u6269\u5c55VLA\u67b6\u6784\uff1a\u52a8\u4f5c\u5934\u9884\u6d4b\u52a8\u4f5c\u5757\uff0c\u8fd0\u52a8\u5934\u4f7f\u7528\u6269\u6563\u53d8\u6362\u5668\u9884\u6d4b\u57fa\u4e8e\u5149\u6d41\u7684\u8fd0\u52a8\u56fe\u50cf\u6765\u6355\u6349\u672a\u6765\u52a8\u6001\u3002\u4e24\u4e2a\u5934\u8054\u5408\u8bad\u7ec3\uff0c\u4f7f\u5171\u4eab\u7684VLM\u9aa8\u5e72\u5b66\u4e60\u8026\u5408\u673a\u5668\u4eba\u63a7\u5236\u548c\u8fd0\u52a8\u77e5\u8bc6\u7684\u8868\u793a", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523097.5%\u6210\u529f\u7387\uff0c\u5728RoboTwin\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523058.0%\u6210\u529f\u7387\uff0c\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u63d0\u534723%\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u589e\u5f3a\u5927\u89c4\u6a21VLA\u8fd0\u52a8\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u8fd0\u52a8\u56fe\u50cf\u6269\u6563\uff0c\u53ef\u4ee5\u5728\u4e0d\u6539\u53d8\u6807\u51c6VLA\u63a8\u7406\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\uff0c\u6784\u5efa\u65f6\u95f4\u4e00\u81f4\u4e14\u7269\u7406\u57fa\u7840\u7684\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u7684\u8fd0\u52a8\u63a8\u7406\u80fd\u529b\u548c\u64cd\u4f5c\u6027\u80fd"}}
{"id": "2512.17911", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17911", "abs": "https://arxiv.org/abs/2512.17911", "authors": ["Hongji Li", "Junchi yao", "Manjiang Yu", "Priyanka Singh", "Xue Li", "Di Wang", "Lijie Hu"], "title": "Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models", "comment": null, "summary": "Machine unlearning aims to erase requested data from trained models without full retraining. For Reasoning Multimodal Large Language Models (RMLLMs), this is uniquely challenging: intermediate chain-of-thought steps can still leak sensitive information even when final answers are forgotten, and overly aggressive interventions easily damage general reasoning ability. Yet no benchmark jointly evaluates how well unlearning methods suppress reasoning-level leakage while preserving reasoning competence. We address this gap with RMLLMU-Bench, the first benchmark for RMLLM unlearning that extends standard forgetting metrics with dedicated measures of reasoning leakage and reasoning retention. A systematic evaluation on RMLLMU-Bench reveals that existing unlearning methods for MLLMs and Large (Language) Reasoning Models (LRMs) either leave substantial leakage in the reasoning process or severely degrade reasoning performance. To address these gaps, we propose R-MUSE (Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering), a training-free and inference-time intervention framework that steers internal representations to forget both answers and reasoning traces while explicitly preserving general reasoning. Experiments on RMLLMU-Bench demonstrate that R-MUSE achieves a substantially better balance between effective forgetting and reasoning retention.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u63a8\u7406\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08RMLLMs\uff09\u7684\u9057\u5fd8\u57fa\u51c6RMLLMU-Bench\uff0c\u5e76\u5f00\u53d1\u4e86R-MUSE\u65b9\u6cd5\uff0c\u5728\u6709\u6548\u9057\u5fd8\u654f\u611f\u4fe1\u606f\u7684\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\u65e8\u5728\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u64e6\u9664\u8bf7\u6c42\u6570\u636e\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002\u5bf9\u4e8e\u63a8\u7406\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08RMLLMs\uff09\uff0c\u8fd9\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff1a\u5373\u4f7f\u6700\u7ec8\u7b54\u6848\u88ab\u9057\u5fd8\uff0c\u4e2d\u95f4\u601d\u7ef4\u94fe\u6b65\u9aa4\u4ecd\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u800c\u8fc7\u5ea6\u5e72\u9884\u5bb9\u6613\u635f\u5bb3\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002\u76ee\u524d\u7f3a\u4e4f\u540c\u65f6\u8bc4\u4f30\u9057\u5fd8\u65b9\u6cd5\u5728\u6291\u5236\u63a8\u7406\u7ea7\u6cc4\u9732\u548c\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86RMLLMU-Bench\u57fa\u51c6\uff0c\u6269\u5c55\u4e86\u6807\u51c6\u9057\u5fd8\u6307\u6807\uff0c\u4e13\u95e8\u8861\u91cf\u63a8\u7406\u6cc4\u9732\u548c\u63a8\u7406\u4fdd\u7559\u3002\u4e3a\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86R-MUSE\uff08Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u63a8\u7406\u65f6\u5e72\u9884\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfc\u5185\u90e8\u8868\u793a\u6765\u9057\u5fd8\u7b54\u6848\u548c\u63a8\u7406\u75d5\u8ff9\uff0c\u540c\u65f6\u660e\u786e\u4fdd\u7559\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728RMLLMU-Bench\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u7684MLLMs\u548c\u5927\u578b\uff08\u8bed\u8a00\uff09\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u9057\u5fd8\u65b9\u6cd5\u8981\u4e48\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7559\u4e0b\u5927\u91cf\u6cc4\u9732\uff0c\u8981\u4e48\u4e25\u91cd\u964d\u4f4e\u63a8\u7406\u6027\u80fd\u3002R-MUSE\u5728\u6709\u6548\u9057\u5fd8\u548c\u63a8\u7406\u4fdd\u7559\u4e4b\u95f4\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86RMLLMs\u9057\u5fd8\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684R-MUSE\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u9057\u5fd8\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18028", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18028", "abs": "https://arxiv.org/abs/2512.18028", "authors": ["Tin Stribor Sohn", "Maximilian Dillitzer", "Jason J. Corso", "Eric Sax"], "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation", "comment": null, "summary": "Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.", "AI": {"tldr": "Embodied4C\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u63a8\u7406\u80fd\u529b\u7684\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u673a\u68b0\u81c2\u4e09\u79cd\u5f02\u6784\u5e73\u53f0\uff0c\u901a\u8fc71100\u4e2a\u4e00\u6b21\u6027\u63a8\u7406\u95ee\u9898\u548c58\u4e2a\u5bfc\u822a\u4efb\u52a1\u8bc4\u4f30\u8bed\u4e49\u3001\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u7269\u7406\u63a8\u7406\u56db\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u5177\u8eab\u6027\uff08\u7269\u7406\u5e73\u53f0\u3001\u4f20\u611f\u5668\u914d\u7f6e\u548c\u6a21\u6001\u5bf9\u9f50\uff09\u5982\u4f55\u5f71\u54cd\u611f\u77e5\u3001\u63a8\u7406\u548c\u63a7\u5236\u7684\u7406\u89e3\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u5177\u8eab\u914d\u7f6e\u4e0b\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u8bbe\u8ba1Embodied4C\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u79cd\u5f02\u6784\u5177\u8eab\u5e73\u53f0\uff08\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u3001\u673a\u68b0\u81c2\uff09\uff0c\u901a\u8fc71100\u4e2a\u4e00\u6b21\u6027\u63a8\u7406\u95ee\u9898\u548c58\u4e2a\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u4efb\u52a1\uff0c\u8bc4\u4f30\u8bed\u4e49\u3001\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u7269\u7406\u63a8\u7406\u56db\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5f15\u5165\u9886\u57df\u8fdc\u67e5\u8be2\u4ee5\u9632\u6b62\u5e73\u53f0\u8fc7\u62df\u5408\u3002", "result": "\u5bf910\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c4\u4e2a\u5177\u8eab\u63a7\u5236\u57fa\u7ebf\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff1a\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6307\u4ee4\u8c03\u4f18\u6bd4\u6a21\u578b\u89c4\u6a21\u66f4\u91cd\u8981\uff0c\u800c\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u4ecd\u7136\u662f\u53ef\u9760\u5177\u8eab\u80fd\u529b\u7684\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "Embodied4C\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u63a8\u7406\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u7279\u522b\u662f\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u5177\u8eabAI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u65b9\u5411\u6307\u5bfc\u3002"}}
{"id": "2512.17912", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17912", "abs": "https://arxiv.org/abs/2512.17912", "authors": ["Lihui Liu"], "title": "Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning", "comment": null, "summary": "ChatGPT said: Text-attributed graphs, where nodes and edges contain rich textual information, are widely used across diverse domains. A central challenge in this setting is question answering, which requires jointly leveraging unstructured text and the structured relational signals within the graph. Although Large Language Models (LLMs) have made significant advances in natural language understanding, their direct use for reasoning over text-attributed graphs remains limited. Retrieval-augmented generation methods that operate purely on text often treat passages as isolated units, ignoring the interconnected structure of the graph. Conversely, graph-based RAG methods that serialize large subgraphs into long textual sequences quickly become infeasible due to LLM context-length constraints, resulting in fragmented reasoning and degraded accuracy. To overcome these limitations, we introduce Graph-O1, an agentic GraphRAG framework that enables LLMs to conduct stepwise, interactive reasoning over graphs. Our approach integrates Monte Carlo Tree Search (MCTS) with end-to-end reinforcement learning, allowing the model to selectively explore and retrieve only the most informative subgraph components. The reasoning procedure is framed as a multi-turn interaction between the agent and the graph environment, and the agent is trained through a unified reward mechanism. Extensive experiments across multiple LLM backbones demonstrate that Graph-O1 consistently surpasses state-of-the-art baselines, producing answers that are more accurate, reliable, and interpretable.", "AI": {"tldr": "Graph-O1\uff1a\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6587\u672c\u5c5e\u6027\u56fe\u4e0a\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u89e3\u51b3\u4f20\u7edf\u56feRAG\u65b9\u6cd5\u56e0\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u5bfc\u81f4\u7684\u63a8\u7406\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u6587\u672c\u5c5e\u6027\u56fe\u5728\u591a\u4e2a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u7eaf\u6587\u672c\u68c0\u7d22\u65b9\u6cd5\u5ffd\u7565\u56fe\u7ed3\u6784\uff0c\u800c\u56fe\u5e8f\u5217\u5316\u65b9\u6cd5\u53d7\u9650\u4e8eLLM\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u548c\u51c6\u786e\u6027\u4e0b\u964d\u3002", "method": "\u63d0\u51faGraph-O1\u6846\u67b6\uff0c\u5c06\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e0e\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u4f7fLLM\u80fd\u591f\u9009\u62e9\u6027\u5730\u63a2\u7d22\u548c\u68c0\u7d22\u6700\u6709\u4fe1\u606f\u7684\u5b50\u56fe\u7ec4\u4ef6\uff0c\u901a\u8fc7\u591a\u8f6e\u667a\u80fd\u4f53-\u56fe\u73af\u5883\u4ea4\u4e92\u8fdb\u884c\u9010\u6b65\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGraph-O1\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751f\u6210\u66f4\u51c6\u786e\u3001\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u7b54\u6848\u3002", "conclusion": "Graph-O1\u901a\u8fc7\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b0\u4e86\u5728\u6587\u672c\u5c5e\u6027\u56fe\u4e0a\u7684\u6709\u6548\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u56fe\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18032", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18032", "abs": "https://arxiv.org/abs/2512.18032", "authors": ["Jacqueline Borgstedt", "Jake Bhattacharyya", "Matteo Iovino", "Frank E. Pollick", "Stephen Brewster"], "title": "Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots", "comment": null, "summary": "Zoomorphic Socially Assistive Robots (SARs) offer an alternative source of social touch for individuals who cannot access animal companionship. However, current SARs provide only limited, passive touch-based interactions and lack the rich haptic cues, such as warmth, heartbeat or purring, that are characteristic of human-animal touch. This limits their ability to evoke emotionally engaging, life-like physical interactions.\n  We present a multimodal tactile prototype, which was used to augment the established PARO robot, integrating thermal and vibrotactile feedback to simulate feeling biophysiological signals. A flexible heating interface delivers body-like warmth, while embedded actuators generate heartbeat-like rhythms and continuous purring sensations. These cues were iteratively designed and calibrated with input from users and haptics experts. We outline the design process and offer reproducible guidelines to support the development of emotionally resonant and biologically plausible touch interactions with SARs.", "AI": {"tldr": "\u5f00\u53d1\u591a\u6a21\u6001\u89e6\u89c9\u539f\u578b\u589e\u5f3aPARO\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u70ed\u611f\u548c\u632f\u52a8\u89e6\u89c9\u6a21\u62df\u751f\u7269\u751f\u7406\u4fe1\u53f7\uff0c\u63d0\u5347\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u7684\u60c5\u611f\u4e92\u52a8\u80fd\u529b", "motivation": "\u5f53\u524d\u62df\u52a8\u7269\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\uff08SARs\uff09\u63d0\u4f9b\u7684\u89e6\u89c9\u4ea4\u4e92\u6709\u9650\u4e14\u88ab\u52a8\uff0c\u7f3a\u4e4f\u6e29\u6696\u3001\u5fc3\u8df3\u6216\u547c\u565c\u58f0\u7b49\u4e30\u5bcc\u7684\u89e6\u89c9\u7ebf\u7d22\uff0c\u9650\u5236\u4e86\u5176\u5524\u8d77\u60c5\u611f\u53c2\u4e0e\u548c\u903c\u771f\u7269\u7406\u4e92\u52a8\u7684\u80fd\u529b", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u89e6\u89c9\u539f\u578b\u6765\u589e\u5f3aPARO\u673a\u5668\u4eba\uff0c\u96c6\u6210\u70ed\u53cd\u9988\u548c\u632f\u52a8\u89e6\u89c9\u53cd\u9988\u6765\u6a21\u62df\u751f\u7269\u751f\u7406\u4fe1\u53f7\u3002\u4f7f\u7528\u67d4\u6027\u52a0\u70ed\u754c\u9762\u63d0\u4f9b\u8eab\u4f53\u822c\u7684\u6e29\u6696\uff0c\u5d4c\u5165\u5f0f\u6267\u884c\u5668\u4ea7\u751f\u5fc3\u8df3\u822c\u7684\u8282\u594f\u548c\u6301\u7eed\u7684\u547c\u565c\u611f\u3002\u8fd9\u4e9b\u7ebf\u7d22\u901a\u8fc7\u7528\u6237\u548c\u89e6\u89c9\u4e13\u5bb6\u7684\u8f93\u5165\u8fdb\u884c\u8fed\u4ee3\u8bbe\u8ba1\u548c\u6821\u51c6", "result": "\u63d0\u51fa\u4e86\u8bbe\u8ba1\u8fc7\u7a0b\u548c\u53ef\u91cd\u590d\u7684\u6307\u5bfc\u65b9\u9488\uff0c\u652f\u6301\u5f00\u53d1\u5177\u6709\u60c5\u611f\u5171\u9e23\u548c\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u89e6\u89c9\u4ea4\u4e92", "conclusion": "\u901a\u8fc7\u96c6\u6210\u70ed\u611f\u548c\u632f\u52a8\u89e6\u89c9\u53cd\u9988\uff0c\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u62df\u52a8\u7269\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u7684\u60c5\u611f\u4e92\u52a8\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u63d0\u4f9b\u66f4\u4e30\u5bcc\u3001\u66f4\u903c\u771f\u7684\u89e6\u89c9\u4f53\u9a8c\uff0c\u4ece\u800c\u66f4\u597d\u5730\u66ff\u4ee3\u52a8\u7269\u966a\u4f34"}}
{"id": "2512.17914", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.17914", "abs": "https://arxiv.org/abs/2512.17914", "authors": ["Boris Kriuk", "Logic Ng"], "title": "Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression", "comment": "7 pages, 4 figures, 1 table", "summary": "Multi-agent Large Language Model (LLM) systems face a critical bottleneck: redundant transmission of contextual information between agents consumes excessive bandwidth and computational resources. Traditional approaches discard internal semantic representations and transmit raw text, forcing receiving agents to recompute similar representations from scratch. We introduce Q-KVComm, a new protocol that enables direct transmission of compressed key-value (KV) cache representations between LLM agents. Q-KVComm combines three key innovations: (1) adaptive layer-wise quantization that allocates variable bit-widths based on sensitivity profiling, (2) hybrid information extraction that preserves critical facts across content domains, and (3) heterogeneous model calibration establishing cross-architecture communication. Extensive experiments across three diverse question-answering datasets demonstrate that Q-KVComm achieves 5-6x compression ratios while maintaining semantic fidelity, with coherence quality scores above 0.77 across all scenarios. The protocol exhibits robust performance across model sizes (1.1B-1.5B parameters) and adapts to real-world applications including conversational QA and multi-hop reasoning. Our work establishes a new paradigm for LLM agent communication, shifting from text-based to representation-based information exchange.", "AI": {"tldr": "Q-KVComm\u662f\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53LLM\u901a\u4fe1\u534f\u8bae\uff0c\u901a\u8fc7\u76f4\u63a5\u4f20\u8f93\u538b\u7f29\u7684KV\u7f13\u5b58\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6587\u672c\u4f20\u8f93\u5bfc\u81f4\u7684\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u5b9e\u73b0\u4e865-6\u500d\u7684\u538b\u7f29\u6bd4\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u9762\u4e34\u5173\u952e\u74f6\u9888\uff1a\u667a\u80fd\u4f53\u4e4b\u95f4\u5197\u4f59\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f20\u8f93\u6d88\u8017\u8fc7\u591a\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u4f20\u7edf\u65b9\u6cd5\u4e22\u5f03\u5185\u90e8\u8bed\u4e49\u8868\u793a\u5e76\u4f20\u8f93\u539f\u59cb\u6587\u672c\uff0c\u8feb\u4f7f\u63a5\u6536\u667a\u80fd\u4f53\u4ece\u5934\u91cd\u65b0\u8ba1\u7b97\u76f8\u4f3c\u8868\u793a\u3002", "method": "Q-KVComm\u534f\u8bae\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u57fa\u4e8e\u654f\u611f\u6027\u5206\u6790\u7684\u5c42\u81ea\u9002\u5e94\u91cf\u5316\uff0c\u5206\u914d\u53ef\u53d8\u6bd4\u7279\u5bbd\u5ea6\uff1b2) \u8de8\u5185\u5bb9\u57df\u4fdd\u7559\u5173\u952e\u4e8b\u5b9e\u7684\u6df7\u5408\u4fe1\u606f\u63d0\u53d6\uff1b3) \u5efa\u7acb\u8de8\u67b6\u6784\u901a\u4fe1\u7684\u5f02\u6784\u6a21\u578b\u6821\u51c6\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cQ-KVComm\u5b9e\u73b0\u4e865-6\u500d\u7684\u538b\u7f29\u6bd4\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u6240\u6709\u573a\u666f\u4e0b\u7684\u8fde\u8d2f\u6027\u8d28\u91cf\u5206\u6570\u5747\u9ad8\u4e8e0.77\u3002\u534f\u8bae\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\uff081.1B-1.5B\u53c2\u6570\uff09\u548c\u5b9e\u9645\u5e94\u7528\uff08\u5bf9\u8bddQA\u548c\u591a\u8df3\u63a8\u7406\uff09\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aLLM\u667a\u80fd\u4f53\u901a\u4fe1\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u4ece\u57fa\u4e8e\u6587\u672c\u7684\u4fe1\u606f\u4ea4\u6362\u8f6c\u5411\u57fa\u4e8e\u8868\u793a\u7684\u4fe1\u606f\u4ea4\u6362\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2512.18048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18048", "abs": "https://arxiv.org/abs/2512.18048", "authors": ["Nidhi Malhotra", "Amber K. Rothe", "Revanth Konda", "Jaydev P. Desai"], "title": "Design of a Polymer-based Steerable Cannula for Neurosurgical Applications", "comment": "Presented at the 14th Conference on New Technologies for Computer and Robot Assisted Surgery (CRAS 2025)", "summary": "Robotically steerable compliant surgical tools offer several advantages over rigid tools, including enhanced dexterity, reduced tissue damage, and the ability to generate non-linear trajectories in minimally invasive neurosurgical procedures. Many existing robotic neurosurgical tools are designed using stainless steel or nitinol materials. Using polymer-based materials instead can offer advantages such as reduced interference in magnetic resonance imaging, enhanced safety for guiding electrically powered instruments, and reduced tissue damage due to inherent compliance. Several polymer materials have been used in robotic surgical applications, such as polyimide, polycarbonate, and elastic resin. Various fabrication strategies have also been proposed, including standard microfabrication techniques, thermal drawing, and 3-D printing. In our previous work, a tendon-driven, notched-tube was designed for several neurosurgical robotic tools, utilizing laser micromachining to reduce the stiffness of the tube in certain directions. This fabrication method is desirable because it has a single-step process, has high precision, and does not require a cleanroom or harsh chemicals. Past studies have explored laser-micromachining of polymer material for surgical applications such as stent fabrication. In this work, we explore extending the use of the laser micromachining approach to the fabrication of polyimide (PI) robotically steerable cannulas for neurosurgical applications. Utilizing the method presented in this work, we fabricated joints as small as 1.5 mm outer diameter (OD). Multiple joints were fabricated using PI tubes of different ODs, and the loading behavior of the fabricated joints was experimentally characterized.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4f7f\u7528\u6fc0\u5149\u5fae\u52a0\u5de5\u6280\u672f\u5236\u9020\u805a\u9170\u4e9a\u80fa\u673a\u5668\u4eba\u53ef\u8f6c\u5411\u5957\u7ba1\uff0c\u7528\u4e8e\u795e\u7ecf\u5916\u79d1\u624b\u672f\uff0c\u6700\u5c0f\u5916\u5f84\u53ef\u8fbe1.5\u6beb\u7c73", "motivation": "\u805a\u5408\u7269\u6750\u6599\u5728\u673a\u5668\u4eba\u795e\u7ecf\u5916\u79d1\u5de5\u5177\u4e2d\u5177\u6709\u4f18\u52bf\uff1a\u51cf\u5c11MRI\u5e72\u6270\u3001\u589e\u5f3a\u7535\u9a71\u52a8\u4eea\u5668\u5b89\u5168\u6027\u3001\u964d\u4f4e\u7ec4\u7ec7\u635f\u4f24\u3002\u73b0\u6709\u5de5\u5177\u591a\u91c7\u7528\u4e0d\u9508\u94a2\u6216\u954d\u949b\u5408\u91d1\uff0c\u800c\u6fc0\u5149\u5fae\u52a0\u5de5\u805a\u5408\u7269\u5177\u6709\u5355\u6b65\u5de5\u827a\u3001\u9ad8\u7cbe\u5ea6\u3001\u65e0\u9700\u6d01\u51c0\u5ba4\u6216\u5f3a\u5316\u5b66\u54c1\u7684\u4f18\u70b9\u3002", "method": "\u91c7\u7528\u6fc0\u5149\u5fae\u52a0\u5de5\u6280\u672f\u5236\u9020\u805a\u9170\u4e9a\u80fa\u53ef\u8f6c\u5411\u5957\u7ba1\uff0c\u5c06\u4e4b\u524d\u7528\u4e8e\u808c\u8171\u9a71\u52a8\u523b\u75d5\u7ba1\u7684\u65b9\u6cd5\u6269\u5c55\u5230\u805a\u9170\u4e9a\u80fa\u6750\u6599\u3002\u5236\u9020\u4e86\u591a\u79cd\u4e0d\u540c\u5916\u5f84\u7684\u5173\u8282\uff0c\u6700\u5c0f\u5916\u5f84\u4e3a1.5\u6beb\u7c73\u3002", "result": "\u6210\u529f\u5236\u9020\u4e86\u6700\u5c0f\u5916\u5f841.5\u6beb\u7c73\u7684\u805a\u9170\u4e9a\u80fa\u5173\u8282\uff0c\u5e76\u5bf9\u5236\u9020\u5173\u8282\u7684\u52a0\u8f7d\u884c\u4e3a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8868\u5f81\u3002", "conclusion": "\u6fc0\u5149\u5fae\u52a0\u5de5\u6280\u672f\u53ef\u7528\u4e8e\u5236\u9020\u5c0f\u578b\u805a\u9170\u4e9a\u80fa\u673a\u5668\u4eba\u53ef\u8f6c\u5411\u5957\u7ba1\uff0c\u4e3a\u795e\u7ecf\u5916\u79d1\u624b\u672f\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u66f4\u7075\u6d3b\u7684\u5668\u68b0\u9009\u62e9\u3002"}}
{"id": "2512.17915", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17915", "abs": "https://arxiv.org/abs/2512.17915", "authors": ["Nick Rossenbach", "Robin Schmitt", "Tina Raissi", "Simon Berger", "Larissa Kleppel", "Ralf Schl\u00fcter"], "title": "Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset", "comment": null, "summary": "The recently published Loquacious dataset aims to be a replacement for established English automatic speech recognition (ASR) datasets such as LibriSpeech or TED-Lium. The main goal of the Loquacious dataset is to provide properly defined training and test partitions across many acoustic and language domains, with an open license suitable for both academia and industry. To further promote the benchmarking and usability of this new dataset, we present additional resources in the form of n-gram language models (LMs), a grapheme-to-phoneme (G2P) model and pronunciation lexica, with open and public access. Utilizing those additional resources we show experimental results across a wide range of ASR architectures with different label units and topologies. Our initial experimental results indicate that the Loquacious dataset offers a valuable study case for a variety of common challenges in ASR.", "AI": {"tldr": "Loquacious\u6570\u636e\u96c6\u65e8\u5728\u66ff\u4ee3LibriSpeech\u7b49\u73b0\u6709ASR\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u8de8\u591a\u4e2a\u58f0\u5b66\u548c\u8bed\u8a00\u9886\u57df\u7684\u8bad\u7ec3/\u6d4b\u8bd5\u5212\u5206\uff0c\u5e76\u9644\u5e26\u8bed\u8a00\u6a21\u578b\u3001G2P\u6a21\u578b\u7b49\u989d\u5916\u8d44\u6e90\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6570\u636e\u96c6\u5bf9ASR\u7814\u7a76\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u82f1\u8bed\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u5982LibriSpeech\u6216TED-Lium\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u3001\u5177\u6709\u660e\u786e\u5b9a\u4e49\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5212\u5206\u3001\u8de8\u591a\u4e2a\u58f0\u5b66\u548c\u8bed\u8a00\u9886\u57df\u3001\u4e14\u9002\u5408\u5b66\u672f\u548c\u5de5\u4e1a\u754c\u4f7f\u7528\u7684\u5f00\u653e\u8bb8\u53ef\u6570\u636e\u96c6\u3002", "method": "\u521b\u5efaLoquacious\u6570\u636e\u96c6\uff0c\u63d0\u4f9bn-gram\u8bed\u8a00\u6a21\u578b\u3001grapheme-to-phoneme\u6a21\u578b\u548c\u53d1\u97f3\u8bcd\u5178\u7b49\u989d\u5916\u8d44\u6e90\uff0c\u5e76\u5728\u591a\u79cdASR\u67b6\u6784\uff08\u4e0d\u540c\u6807\u7b7e\u5355\u5143\u548c\u62d3\u6251\u7ed3\u6784\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLoquacious\u6570\u636e\u96c6\u4e3aASR\u4e2d\u5404\u79cd\u5e38\u89c1\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u7814\u7a76\u6848\u4f8b\uff0c\u80fd\u591f\u652f\u6301\u5e7f\u6cdb\u7684ASR\u67b6\u6784\u8bc4\u4f30\u3002", "conclusion": "Loquacious\u6570\u636e\u96c6\u53ca\u5176\u914d\u5957\u8d44\u6e90\u4e3aASR\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u5177\u6709\u5f00\u653e\u8bb8\u53ef\u548c\u8de8\u9886\u57df\u7279\u6027\uff0c\u9002\u5408\u5b66\u672f\u548c\u5de5\u4e1a\u754c\u4f7f\u7528\u3002"}}
{"id": "2512.18068", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18068", "abs": "https://arxiv.org/abs/2512.18068", "authors": ["Juo-Tung Chen", "XinHao Chen", "Ji Woong Kim", "Paul Maria Scheikl", "Richard Jaepyeong Cha", "Axel Krieger"], "title": "SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning", "comment": "8 pages, 6 figures, 2 tables", "summary": "Imitation learning (IL) has shown immense promise in enabling autonomous dexterous manipulation, including learning surgical tasks. To fully unlock the potential of IL for surgery, access to clinical datasets is needed, which unfortunately lack the kinematic data required for current IL approaches. A promising source of large-scale surgical demonstrations is monocular surgical videos available online, making monocular pose estimation a crucial step toward enabling large-scale robot learning. Toward this end, we propose SurgiPose, a differentiable rendering based approach to estimate kinematic information from monocular surgical videos, eliminating the need for direct access to ground truth kinematics. Our method infers tool trajectories and joint angles by optimizing tool pose parameters to minimize the discrepancy between rendered and real images. To evaluate the effectiveness of our approach, we conduct experiments on two robotic surgical tasks: tissue lifting and needle pickup, using the da Vinci Research Kit Si (dVRK Si). We train imitation learning policies with both ground truth measured kinematics and estimated kinematics from video and compare their performance. Our results show that policies trained on estimated kinematics achieve comparable success rates to those trained on ground truth data, demonstrating the feasibility of using monocular video based kinematic estimation for surgical robot learning. By enabling kinematic estimation from monocular surgical videos, our work lays the foundation for large scale learning of autonomous surgical policies from online surgical data.", "AI": {"tldr": "SurgiPose\uff1a\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u6e32\u67d3\u7684\u65b9\u6cd5\uff0c\u4ece\u5355\u76ee\u624b\u672f\u89c6\u9891\u4e2d\u4f30\u8ba1\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u65e0\u9700\u771f\u5b9e\u8fd0\u52a8\u5b66\u6570\u636e\uff0c\u4e3a\u624b\u672f\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u5927\u89c4\u6a21\u6570\u636e\u57fa\u7840", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u4e3b\u7075\u5de7\u64cd\u4f5c\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e34\u5e8a\u6570\u636e\u96c6\u7f3a\u4e4f\u5f53\u524d\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u6240\u9700\u7684\u8fd0\u52a8\u5b66\u6570\u636e\u3002\u5728\u7ebf\u5355\u76ee\u624b\u672f\u89c6\u9891\u662f\u5927\u89c4\u6a21\u624b\u672f\u6f14\u793a\u7684\u6f5c\u5728\u6765\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u8fd9\u4e9b\u89c6\u9891\u4e2d\u4f30\u8ba1\u8fd0\u52a8\u5b66\u4fe1\u606f\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u673a\u5668\u4eba\u5b66\u4e60", "method": "\u63d0\u51faSurgiPose\u65b9\u6cd5\uff0c\u57fa\u4e8e\u53ef\u5fae\u5206\u6e32\u67d3\u4f30\u8ba1\u5de5\u5177\u8f68\u8ff9\u548c\u5173\u8282\u89d2\u5ea6\uff0c\u901a\u8fc7\u4f18\u5316\u5de5\u5177\u59ff\u6001\u53c2\u6570\u6765\u6700\u5c0f\u5316\u6e32\u67d3\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u65e0\u9700\u76f4\u63a5\u8bbf\u95ee\u771f\u5b9e\u8fd0\u52a8\u5b66\u6570\u636e", "result": "\u5728\u4e24\u4e2a\u673a\u5668\u4eba\u624b\u672f\u4efb\u52a1\uff08\u7ec4\u7ec7\u63d0\u5347\u548c\u9488\u62fe\u53d6\uff09\u4e0a\u4f7f\u7528dVRK Si\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u89c6\u9891\u4f30\u8ba1\u7684\u8fd0\u52a8\u5b66\u8bad\u7ec3\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u4e0e\u4f7f\u7528\u771f\u5b9e\u8fd0\u52a8\u5b66\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\u76f8\u6bd4\uff0c\u53d6\u5f97\u4e86\u76f8\u5f53\u7684\u6210\u529f\u7387", "conclusion": "\u901a\u8fc7\u4ece\u5355\u76ee\u624b\u672f\u89c6\u9891\u4e2d\u5b9e\u73b0\u8fd0\u52a8\u5b66\u4f30\u8ba1\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ece\u5728\u7ebf\u624b\u672f\u6570\u636e\u4e2d\u5927\u89c4\u6a21\u5b66\u4e60\u81ea\u4e3b\u624b\u672f\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u89c6\u9891\u7684\u8fd0\u52a8\u5b66\u4f30\u8ba1\u7528\u4e8e\u624b\u672f\u673a\u5668\u4eba\u5b66\u4e60\u7684\u53ef\u884c\u6027"}}
{"id": "2512.17916", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17916", "abs": "https://arxiv.org/abs/2512.17916", "authors": ["Minh Tri L\u00ca", "Ali Ait-Bachir"], "title": "Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models", "comment": "12 pages", "summary": "Prioritizing service tickets in IT Service Management (ITSM) is critical for operational efficiency but remains challenging due to noisy textual inputs, subjective writing styles, and pronounced class imbalance. We evaluate two families of approaches for ticket prioritization: embedding-based pipelines that combine dimensionality reduction, clustering, and classical classifiers, and a fine-tuned multilingual transformer that processes both textual and numerical features. Embedding-based methods exhibit limited generalization across a wide range of thirty configurations, with clustering failing to uncover meaningful structures and supervised models highly sensitive to embedding quality. In contrast, the proposed transformer model achieves substantially higher performance, with an average F1-score of 78.5% and weighted Cohen's kappa values of nearly 0.80, indicating strong alignment with true labels. These results highlight the limitations of generic embeddings for ITSM data and demonstrate the effectiveness of domain-adapted transformer architectures for operational ticket prioritization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cdIT\u670d\u52a1\u7ba1\u7406\u7968\u8bc1\u4f18\u5148\u7ea7\u5206\u7c7b\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5d4c\u5165\u7684\u7ba1\u9053\u4e0e\u5fae\u8c03\u7684\u591a\u8bed\u8a00Transformer\uff0c\u53d1\u73b0Transformer\u5728\u566a\u58f0\u6587\u672c\u3001\u4e3b\u89c2\u5199\u4f5c\u98ce\u683c\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u8868\u73b0\u663e\u8457\u66f4\u4f18\u3002", "motivation": "IT\u670d\u52a1\u7ba1\u7406\u4e2d\u7684\u7968\u8bc1\u4f18\u5148\u7ea7\u5206\u7c7b\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u566a\u58f0\u6587\u672c\u8f93\u5165\u3001\u4e3b\u89c2\u5199\u4f5c\u98ce\u683c\u548c\u663e\u8457\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u6311\u6218\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bc4\u4f30\u4e86\u4e24\u79cd\u65b9\u6cd5\u5bb6\u65cf\uff1a1) \u57fa\u4e8e\u5d4c\u5165\u7684\u7ba1\u9053\uff08\u7ed3\u5408\u964d\u7ef4\u3001\u805a\u7c7b\u548c\u4f20\u7edf\u5206\u7c7b\u5668\uff09\uff1b2) \u5fae\u8c03\u7684\u591a\u8bed\u8a00Transformer\uff08\u540c\u65f6\u5904\u7406\u6587\u672c\u548c\u6570\u503c\u7279\u5f81\uff09\u3002\u5728\u4e09\u5341\u79cd\u4e0d\u540c\u914d\u7f6e\u4e0b\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u805a\u7c7b\u672a\u80fd\u53d1\u73b0\u6709\u610f\u4e49\u7684\u7ed3\u6784\uff0c\u76d1\u7763\u6a21\u578b\u5bf9\u5d4c\u5165\u8d28\u91cf\u9ad8\u5ea6\u654f\u611f\u3002\u800cTransformer\u6a21\u578b\u8868\u73b0\u663e\u8457\u66f4\u597d\uff0c\u5e73\u5747F1\u5206\u6570\u8fbe78.5%\uff0c\u52a0\u6743Cohen's kappa\u503c\u63a5\u8fd10.80\u3002", "conclusion": "\u901a\u7528\u5d4c\u5165\u65b9\u6cd5\u5bf9ITSM\u6570\u636e\u6548\u679c\u6709\u9650\uff0c\u800c\u9886\u57df\u9002\u5e94\u7684Transformer\u67b6\u6784\u5728\u64cd\u4f5c\u7968\u8bc1\u4f18\u5148\u7ea7\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18081", "abs": "https://arxiv.org/abs/2512.18081", "authors": ["Tudor Jianu", "Anh Nguyen", "Sebastiano Fichera", "Pierre Berthet-Rayne"], "title": "Towards Autonomous Navigation in Endovascular Interventions", "comment": null, "summary": "Cardiovascular diseases remain the leading cause of global mortality, with minimally invasive treatment options offered through endovascular interventions. However, the precision and adaptability of current robotic systems for endovascular navigation are limited by heuristic control, low autonomy, and the absence of haptic feedback. This thesis presents an integrated AI-driven framework for autonomous guidewire navigation in complex vascular environments, addressing key challenges in data availability, simulation fidelity, and navigational accuracy.\n  A high-fidelity, real-time simulation platform, CathSim, is introduced for reinforcement learning based catheter navigation, featuring anatomically accurate vascular models and contact dynamics. Building on CathSim, the Expert Navigation Network is developed, a policy that fuses visual, kinematic, and force feedback for autonomous tool control. To mitigate data scarcity, the open-source, bi-planar fluoroscopic dataset Guide3D is proposed, comprising more than 8,700 annotated images for 3D guidewire reconstruction. Finally, SplineFormer, a transformer-based model, is introduced to directly predict guidewire geometry as continuous B-spline parameters, enabling interpretable, real-time navigation.\n  The findings show that combining high-fidelity simulation, multimodal sensory fusion, and geometric modelling substantially improves autonomous endovascular navigation and supports safer, more precise minimally invasive procedures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u4e3b\u5bfc\u4e1d\u5bfc\u822a\u6846\u67b6\uff0c\u5305\u62ec\u9ad8\u4fdd\u771f\u6a21\u62df\u5e73\u53f0CathSim\u3001\u4e13\u5bb6\u5bfc\u822a\u7f51\u7edc\u3001\u5f00\u6e90\u6570\u636e\u96c6Guide3D\u548c\u57fa\u4e8eTransformer\u7684SplineFormer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u8840\u7ba1\u5185\u5bfc\u822a\u7684\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u5f53\u524d\u8840\u7ba1\u5185\u4ecb\u5165\u673a\u5668\u4eba\u7cfb\u7edf\u5b58\u5728\u542f\u53d1\u5f0f\u63a7\u5236\u3001\u81ea\u4e3b\u6027\u4f4e\u3001\u7f3a\u4e4f\u89e6\u89c9\u53cd\u9988\u7b49\u9650\u5236\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u3001\u81ea\u9002\u5e94\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5f00\u53d1\u9ad8\u4fdd\u771f\u5b9e\u65f6\u6a21\u62df\u5e73\u53f0CathSim\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff1b2. \u63d0\u51fa\u878d\u5408\u89c6\u89c9\u3001\u8fd0\u52a8\u5b66\u548c\u529b\u53cd\u9988\u7684\u4e13\u5bb6\u5bfc\u822a\u7f51\u7edc\uff1b3. \u521b\u5efa\u5305\u542b8,700\u591a\u5f20\u6807\u6ce8\u56fe\u50cf\u7684\u5f00\u6e90\u53cc\u5e73\u9762\u900f\u89c6\u6570\u636e\u96c6Guide3D\uff1b4. \u8bbe\u8ba1\u57fa\u4e8eTransformer\u7684SplineFormer\u6a21\u578b\uff0c\u76f4\u63a5\u9884\u6d4bB\u6837\u6761\u53c2\u6570\u6765\u8868\u793a\u5bfc\u4e1d\u51e0\u4f55\u5f62\u72b6\u3002", "result": "\u7ed3\u5408\u9ad8\u4fdd\u771f\u6a21\u62df\u3001\u591a\u6a21\u6001\u611f\u77e5\u878d\u5408\u548c\u51e0\u4f55\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\u7684\u6027\u80fd\uff0c\u652f\u6301\u66f4\u5b89\u5168\u3001\u66f4\u7cbe\u786e\u7684\u5fae\u521b\u624b\u672f\u3002", "conclusion": "\u8be5\u96c6\u6210AI\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u5e73\u53f0\u3001\u6570\u636e\u96c6\u548c\u51e0\u4f55\u5efa\u6a21\u65b9\u6cd5\u7684\u521b\u65b0\uff0c\u89e3\u51b3\u4e86\u8840\u7ba1\u5185\u5bfc\u822a\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u6a21\u62df\u4fdd\u771f\u5ea6\u548c\u5bfc\u822a\u7cbe\u5ea6\u7b49\u5173\u952e\u6311\u6218\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u66f4\u7cbe\u786e\u7684\u5fae\u521b\u5fc3\u8840\u7ba1\u4ecb\u5165\u6cbb\u7597\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2512.17917", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17917", "abs": "https://arxiv.org/abs/2512.17917", "authors": ["Aomufei Yuan", "Zhiming Wang", "Ruijie Miao", "Dayu Wang", "Yuxuan Tian", "Zihan Wang", "Yebo Peng", "Yuhan Wu", "Bairen Yi", "Xin Liu", "Tong Yang"], "title": "KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction", "comment": "12 pages, 6 figures", "summary": "As the context length of current large language models (LLMs) rapidly increases, the memory demand for the Key-Value (KV) cache is becoming a bottleneck for LLM deployment and batch processing. Traditional KV cache compression methods typically involve permanently evicting or irreversibly merging \"less important\" tokens with low attention scores. This approach results in the unrecoverable loss of token information, which we call Contextual Amnesia, significantly degrading the model's information retrieval capability. To address this issue, we propose KVReviver, a reversible KV cache compression method based on the sketch algorithm. This method allows reconstructing compressed tokens from an additional data structure, thus enabling full-scale computation within limited memory. Experiments showed that in 2k-length contexts, it requires only 10% of KV Cache budget while maintaining identical end-to-end inference accuracy. For 32k-length contexts, it achieves equivalent or comparable accuracy ~2% accuracy loss) using merely 25% of KV Cache budget.", "AI": {"tldr": "KVReviver\uff1a\u57fa\u4e8e\u8349\u56fe\u7b97\u6cd5\u7684\u53ef\u9006KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\uff0c\u89e3\u51b3LLM\u957f\u4e0a\u4e0b\u6587\u90e8\u7f72\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898", "motivation": "\u968f\u7740LLM\u4e0a\u4e0b\u6587\u957f\u5ea6\u5feb\u901f\u589e\u957f\uff0cKV\u7f13\u5b58\u7684\u5185\u5b58\u9700\u6c42\u6210\u4e3a\u90e8\u7f72\u548c\u6279\u5904\u7406\u7684\u74f6\u9888\u3002\u4f20\u7edfKV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u4f1a\u6c38\u4e45\u9a71\u9010\u6216\u5408\u5e76\"\u4e0d\u91cd\u8981\"\u7684token\uff0c\u5bfc\u81f4\u4e0d\u53ef\u6062\u590d\u7684\u4fe1\u606f\u4e22\u5931\uff08\u4e0a\u4e0b\u6587\u9057\u5fd8\uff09\uff0c\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51faKVReviver\uff0c\u4e00\u79cd\u57fa\u4e8e\u8349\u56fe\u7b97\u6cd5\u7684\u53ef\u9006KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u4ece\u989d\u5916\u7684\u6570\u636e\u7ed3\u6784\u4e2d\u91cd\u5efa\u538b\u7f29\u7684token\uff0c\u4ece\u800c\u5728\u6709\u9650\u5185\u5b58\u5185\u5b9e\u73b0\u5168\u89c4\u6a21\u8ba1\u7b97\u3002", "result": "\u57282k\u957f\u5ea6\u4e0a\u4e0b\u6587\u4e2d\uff0c\u4ec5\u970010%\u7684KV\u7f13\u5b58\u9884\u7b97\u5373\u53ef\u4fdd\u6301\u76f8\u540c\u7684\u7aef\u5230\u7aef\u63a8\u7406\u7cbe\u5ea6\u3002\u572832k\u957f\u5ea6\u4e0a\u4e0b\u6587\u4e2d\uff0c\u4ec5\u4f7f\u752825%\u7684KV\u7f13\u5b58\u9884\u7b97\u5c31\u80fd\u8fbe\u5230\u76f8\u5f53\u6216\u53ef\u6bd4\u7684\u7cbe\u5ea6\uff08\u7ea62%\u7cbe\u5ea6\u635f\u5931\uff09\u3002", "conclusion": "KVReviver\u901a\u8fc7\u53ef\u9006\u538b\u7f29\u89e3\u51b3\u4e86\u4f20\u7edfKV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u4e2d\u7684\u4e0a\u4e0b\u6587\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18146", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18146", "abs": "https://arxiv.org/abs/2512.18146", "authors": ["Stergios E. Bachoumas", "Panagiotis Artemiadis"], "title": "On Swarm Leader Identification using Probing Policies", "comment": "13 pages, journal", "summary": "Identifying the leader within a robotic swarm is crucial, especially in adversarial contexts where leader concealment is necessary for mission success. This work introduces the interactive Swarm Leader Identification (iSLI) problem, a novel approach where an adversarial probing agent identifies a swarm's leader by physically interacting with its members. We formulate the iSLI problem as a Partially Observable Markov Decision Process (POMDP) and employ Deep Reinforcement Learning, specifically Proximal Policy Optimization (PPO), to train the prober's policy. The proposed approach utilizes a novel neural network architecture featuring a Timed Graph Relationformer (TGR) layer combined with a Simplified Structured State Space Sequence (S5) model. The TGR layer effectively processes graph-based observations of the swarm, capturing temporal dependencies and fusing relational information using a learned gating mechanism to generate informative representations for policy learning. Extensive simulations demonstrate that our TGR-based model outperforms baseline graph neural network architectures and exhibits significant zero-shot generalization capabilities across varying swarm sizes and speeds different from those used during training. The trained prober achieves high accuracy in identifying the leader, maintaining performance even in out-of-training distribution scenarios, and showing appropriate confidence levels in its predictions. Real-world experiments with physical robots further validate the approach, confirming successful sim-to-real transfer and robustness to dynamic changes, such as unexpected agent disconnections.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faiSLI\u95ee\u9898\uff0c\u901a\u8fc7\u7269\u7406\u4ea4\u4e92\u8bc6\u522b\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u7684\u9886\u5bfc\u8005\uff0c\u4f7f\u7528POMDP\u548cPPO\u8bad\u7ec3\u63a2\u6d4b\u7b56\u7565\uff0c\u91c7\u7528TGR+S5\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u8bc6\u522b\u673a\u5668\u4eba\u7fa4\u4f53\u9886\u5bfc\u8005\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9886\u5bfc\u8005\u901a\u5e38\u4f1a\u9690\u85cf\u8eab\u4efd\u4ee5\u786e\u4fdd\u4efb\u52a1\u6210\u529f\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u9690\u853d\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u7269\u7406\u4ea4\u4e92\u6765\u8bc6\u522b\u9886\u5bfc\u8005\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5c06iSLI\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(POMDP)\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u8bad\u7ec3\u63a2\u6d4b\u7b56\u7565\u3002\u63d0\u51fa\u65b0\u9896\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff1a\u65f6\u95f4\u56fe\u5173\u7cfb\u53d8\u6362\u5668(TGR)\u5c42\u7ed3\u5408\u7b80\u5316\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5e8f\u5217(S5)\u6a21\u578b\uff0c\u6709\u6548\u5904\u7406\u7fa4\u4f53\u56fe\u89c2\u6d4b\u6570\u636e\uff0c\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5173\u7cfb\u4fe1\u606f\u3002", "result": "TGR\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u663e\u8457\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7fa4\u4f53\u89c4\u6a21\u548c\u901f\u5ea6\u3002\u63a2\u6d4b\u4ee3\u7406\u5728\u8bc6\u522b\u9886\u5bfc\u8005\u65b9\u9762\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u5206\u5e03\u5916\u573a\u666f\u4e2d\u4e5f\u80fd\u4fdd\u6301\u6027\u80fd\uff0c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u9002\u5f53\u3002\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5b9e\u73b0\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\uff0c\u5bf9\u52a8\u6001\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684iSLI\u65b9\u6cd5\u548cTGR+S5\u67b6\u6784\u80fd\u591f\u6709\u6548\u8bc6\u522b\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u7684\u9690\u853d\u9886\u5bfc\u8005\uff0c\u5177\u6709\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u7fa4\u4f53\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.17920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17920", "abs": "https://arxiv.org/abs/2512.17920", "authors": ["Rahul Baxi"], "title": "Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression", "comment": "19 pages, 9 figures; currently under peer review at TMLR", "summary": "Large language models (LLMs) exhibit degraded performance under prompt compression, but the mechanisms remain poorly understood. We introduce the Compression-Decay Comprehension Test (CDCT), a benchmark that independently measures constraint compliance (CC) and semantic accuracy (SA) across compression levels. We evaluate 9 frontier LLMs across 8 concepts using 5 compression levels from extreme (c=0.0, ~2 words) to none (c=1.0, ~135 words). A three-judge LLM jury achieves almost perfect inter-rater agreement on CC (Fleiss' \\k{appa}=0.90).\n  We observe a universal U-curve pattern in constraint compliance (97.2% prevalence), with violations peaking at medium compression (c=0.5, ~27 words). Counterintuitively, models perform better at extreme compression than medium lengths. The dimensions are statistically orthogonal (r=0.193, p=0.084), with constraint effects 2.9x larger than semantic effects.\n  Experimental validation via RLHF ablation confirms our constraint salience hypothesis: removing \"helpfulness\" signals improves CC by 598% on average (71/72 trials, p<0.001), with 79% achieving perfect compliance. This demonstrates that RLHF-trained helpfulness behaviors are the dominant cause of constraint violations at medium compression. Reasoning models outperform efficient models by 27.5% (Cohen's d=0.96).\n  Our findings reveal a fundamental tension between RLHF alignment and instruction-following, providing actionable guidelines for improving deployed systems.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0LLM\u5728\u63d0\u793a\u538b\u7f29\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u673a\u5236\uff1aRLHF\u8bad\u7ec3\u7684\"\u4e50\u4e8e\u52a9\u4eba\"\u884c\u4e3a\u4e0e\u6307\u4ee4\u9075\u5faa\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u51b2\u7a81\uff0c\u5bfc\u81f4\u4e2d\u7b49\u538b\u7f29\u65f6\u7ea6\u675f\u5408\u89c4\u6027\u6700\u5dee\uff0c\u5f62\u6210U\u578b\u66f2\u7ebf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u793a\u538b\u7f29\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u4f46\u5176\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u9700\u8981\u7406\u89e3\u538b\u7f29\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u7ea6\u675f\u5408\u89c4\u6027\u548c\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u4ee5\u6539\u8fdb\u5b9e\u9645\u90e8\u7f72\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u538b\u7f29\u8870\u51cf\u7406\u89e3\u6d4b\u8bd5(CDCT)\uff0c\u72ec\u7acb\u6d4b\u91cf\u7ea6\u675f\u5408\u89c4\u6027(CC)\u548c\u8bed\u4e49\u51c6\u786e\u6027(SA)\u3002\u8bc4\u4f309\u4e2a\u524d\u6cbfLLM\u57288\u4e2a\u6982\u5ff5\u4e0a\u76845\u4e2a\u538b\u7f29\u7ea7\u522b\uff08\u4ece\u6781\u7aef\u538b\u7f29\u5230\u65e0\u538b\u7f29\uff09\u3002\u4f7f\u7528\u4e09\u6cd5\u5b98LLM\u966a\u5ba1\u56e2\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7RLHF\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5047\u8bbe\u3002", "result": "\u53d1\u73b0\u666e\u904d\u7684U\u578b\u66f2\u7ebf\u6a21\u5f0f\uff0897.2%\u51fa\u73b0\u7387\uff09\uff0c\u7ea6\u675f\u8fdd\u89c4\u5728\u4e2d\u5ea6\u538b\u7f29\u65f6\u8fbe\u5230\u5cf0\u503c\u3002\u53cd\u76f4\u89c9\u5730\uff0c\u6781\u7aef\u538b\u7f29\u6bd4\u4e2d\u7b49\u957f\u5ea6\u8868\u73b0\u66f4\u597d\u3002\u7ea6\u675f\u6548\u5e94\u6bd4\u8bed\u4e49\u6548\u5e94\u59272.9\u500d\u3002RLHF\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u7ea6\u675f\u663e\u8457\u6027\u5047\u8bbe\uff1a\u79fb\u9664\"\u4e50\u4e8e\u52a9\u4eba\"\u4fe1\u53f7\u4f7fCC\u5e73\u5747\u63d0\u9ad8598%\u3002\u63a8\u7406\u6a21\u578b\u6bd4\u9ad8\u6548\u6a21\u578b\u8868\u73b0\u597d27.5%\u3002", "conclusion": "\u63ed\u793a\u4e86RLHF\u5bf9\u9f50\u4e0e\u6307\u4ee4\u9075\u5faa\u4e4b\u95f4\u7684\u6839\u672c\u5f20\u529b\uff0cRLHF\u8bad\u7ec3\u7684\"\u4e50\u4e8e\u52a9\u4eba\"\u884c\u4e3a\u662f\u4e2d\u7b49\u538b\u7f29\u65f6\u7ea6\u675f\u8fdd\u89c4\u7684\u4e3b\u8981\u539f\u56e0\u3002\u8fd9\u4e3a\u6539\u8fdb\u90e8\u7f72\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2512.18206", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.18206", "abs": "https://arxiv.org/abs/2512.18206", "authors": ["Trevor Stepp", "Parthan Olikkal", "Ramana Vinjamuri", "Rajasekhar Anguluri"], "title": "Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination", "comment": "7 pages, 5 figures", "summary": "Identifying motor synergies -- coordinated hand joint patterns activated at task-dependent time shifts -- from kinematic data is central to motor control and robotics. Existing two-stage methods first extract candidate waveforms (via SVD) and then select shifted templates using sparse optimization, requiring at least two datasets and complicating data collection. We introduce an optimization-based framework that jointly learns a small set of synergies and their sparse activation coefficients. The formulation enforces group sparsity for synergy selection and element-wise sparsity for activation timing. We develop an alternating minimization method in which coefficient updates decouple across tasks and synergy updates reduce to regularized least-squares problems. Our approach requires only a single data set, and simulations show accurate velocity reconstruction with compact, interpretable synergies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u5355\u4e2a\u6570\u636e\u96c6\u8054\u5408\u5b66\u4e60\u5c11\u91cf\u8fd0\u52a8\u534f\u540c\u6a21\u5f0f\u53ca\u5176\u7a00\u758f\u6fc0\u6d3b\u7cfb\u6570\uff0c\u65e0\u9700\u4e24\u9636\u6bb5\u65b9\u6cd5\u548c\u591a\u4e2a\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u65b9\u6cd5\u9700\u8981\u81f3\u5c11\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u5148\u63d0\u53d6\u5019\u9009\u6ce2\u5f62\u518d\u9009\u62e9\u65f6\u79fb\u6a21\u677f\uff0c\u6570\u636e\u6536\u96c6\u590d\u6742\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5355\u6570\u636e\u96c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4f18\u5316\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u5c11\u91cf\u534f\u540c\u6a21\u5f0f\u53ca\u5176\u7a00\u758f\u6fc0\u6d3b\u7cfb\u6570\uff0c\u901a\u8fc7\u7ec4\u7a00\u758f\u6027\u9009\u62e9\u534f\u540c\uff0c\u5143\u7d20\u7ea7\u7a00\u758f\u6027\u63a7\u5236\u6fc0\u6d3b\u65f6\u5e8f\u3002\u91c7\u7528\u4ea4\u66ff\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u7cfb\u6570\u66f4\u65b0\u5728\u4efb\u52a1\u95f4\u89e3\u8026\uff0c\u534f\u540c\u66f4\u65b0\u7b80\u5316\u4e3a\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u901f\u5ea6\uff0c\u83b7\u5f97\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u534f\u540c\u6a21\u5f0f\u3002\u65b9\u6cd5\u4ec5\u9700\u5355\u4e2a\u6570\u636e\u96c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u7b80\u5316\u4e86\u8fd0\u52a8\u534f\u540c\u8bc6\u522b\u6d41\u7a0b\uff0c\u4ec5\u9700\u5355\u4e2a\u6570\u636e\u96c6\u5373\u53ef\u83b7\u5f97\u51c6\u786e\u3001\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u534f\u540c\u6a21\u5f0f\uff0c\u4f18\u4e8e\u73b0\u6709\u4e24\u9636\u6bb5\u65b9\u6cd5\u3002"}}
{"id": "2512.18014", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18014", "abs": "https://arxiv.org/abs/2512.18014", "authors": ["Shubham Kumar Nigam", "Tanuj Tyagi", "Siddharth Shukla", "Aditya Kumar Guru", "Balaramamahanthi Deepak Patnaik", "Danush Khanna", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India", "comment": "Accepted in AILaw @ AAAI 2026 conference", "summary": "This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6cd5\u5f8bAI\u6846\u67b6ReGal\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u6307\u4ee4\u5fae\u8c03\u548cAI\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u5370\u5ea6\u6cd5\u5f8b\u80cc\u666f\u4e0b\u63a2\u7d22\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\uff0c\u5c3d\u7ba1\u6027\u80fd\u4e0d\u53ca\u76d1\u7763\u6a21\u578b\uff0c\u4f46\u4e3aRL\u5728\u6cd5\u5f8b\u6587\u672c\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5728\u5370\u5ea6\u6cd5\u5f8bAI\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u6cd5\u5f8b\u6587\u672c\u5904\u7406\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7684\u6cd5\u5f8bAI\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u63d0\u51faReGal\u6846\u67b6\uff0c\u6574\u5408\u591a\u4efb\u52a1\u6307\u4ee4\u5fae\u8c03\u548c\u57fa\u4e8eAI\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLAIF\uff09\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\uff0c\u5728\u6cd5\u9662\u5224\u51b3\u9884\u6d4b\u4e0e\u89e3\u91ca\uff08CJPE\uff09\u548c\u6cd5\u5f8b\u6587\u6863\u6458\u8981\u4e24\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6846\u67b6\u5728\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u53ca\u76d1\u7763\u548c\u4e13\u6709\u6a21\u578b\uff0c\u4f46\u901a\u8fc7\u5b9e\u8bc1\u548c\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86RL\u5e94\u7528\u4e8e\u6cd5\u5f8b\u6587\u672c\u7684\u6311\u6218\uff0c\u5305\u62ec\u5956\u52b1\u6a21\u578b\u5bf9\u9f50\u3001\u6cd5\u5f8b\u8bed\u8a00\u590d\u6742\u6027\u548c\u9886\u57df\u7279\u5b9a\u9002\u5e94\u7b49\u95ee\u9898\u3002", "conclusion": "\u5c3d\u7ba1\u6027\u80fd\u6709\u9650\uff0c\u4f46\u7814\u7a76\u4e3a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6cd5\u5f8b\u63a8\u7406\u6d41\u7a0b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5bf9\u6784\u5efa\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7684\u6cd5\u5f8bAI\u7cfb\u7edf\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u610f\u4e49\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.18211", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18211", "abs": "https://arxiv.org/abs/2512.18211", "authors": ["Yudong Liu", "Spencer Hallyburton", "Jiwoo Kim", "Yueqian Lin", "Yiming Li", "Qinsi Wang", "Hui Ye", "Jingwei Sun", "Miroslav Pajic", "Yiran Chen", "Hai Li"], "title": "LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning", "comment": null, "summary": "Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful \"VLM Trajectory Planner for Autonomous Driving.\" On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub.", "AI": {"tldr": "LLaViDA\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03+\u8f68\u8ff9\u504f\u597d\u4f18\u5316\uff09\u63d0\u5347\u573a\u666f\u7406\u89e3\u548c\u8f68\u8ff9\u89c4\u5212\u80fd\u529b\uff0c\u5728NuScenes\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u89c4\u5212\u5668\u5728\u6076\u52a3\u5929\u6c14\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u4eba\u7c7b\u884c\u4e3a\u6216\u590d\u6742\u9053\u8def\u5e03\u5c40\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u7684\u5f3a\u6cdb\u5316\u6216\u5c0f\u6837\u672c\u80fd\u529b\u3002", "method": "\u63d0\u51faLLaViDA\u7cfb\u7edf\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7269\u4f53\u8fd0\u52a8\u9884\u6d4b\u3001\u8bed\u4e49\u63a5\u5730\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u540e\u63a5\u8f68\u8ff9\u504f\u597d\u4f18\u5316\uff0c\u6ce8\u5165\u57fa\u4e8e\u56de\u5f52\u7684\u76d1\u7763\u3002", "result": "\u5728NuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLaViDA\u5728\u5f00\u73af\u8f68\u8ff9\u89c4\u5212\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u548c\u5176\u4ed6\u57fa\u4e8eVLM/LLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747L2\u8f68\u8ff9\u8bef\u5dee\u4e3a0.31\u7c73\uff0c\u78b0\u649e\u7387\u4e3a0.10%\u3002", "conclusion": "LLaViDA\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u8bad\u7ec3\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2512.18027", "categories": ["cs.CL", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.18027", "abs": "https://arxiv.org/abs/2512.18027", "authors": ["Samidh Chakrabarti", "David Willner", "Kevin Klyman", "Tiffany Saade", "Emily Capstick", "Sabina Nong"], "title": "CoPE: A Small Language Model for Steerable and Scalable Content Labeling", "comment": "21 pages, 2 figures, 7 tables", "summary": "This paper details the methodology behind CoPE, a policy-steerable small language model capable of fast and accurate content labeling. We present a novel training curricula called Contradictory Example Training that enables the model to learn policy interpretation rather than mere policy memorization. We also present a novel method for generating content policies, called Binocular Labeling, which enables rapid construction of unambiguous training datasets. When evaluated across seven different harm areas, CoPE exhibits equal or superior accuracy to frontier models at only 1% of their size. We openly release a 9 billion parameter version of the model that can be run on a single consumer-grade GPU. Models like CoPE represent a paradigm shift for classifier systems. By turning an ML task into a policy writing task, CoPE opens up new design possibilities for the governance of online platforms.", "AI": {"tldr": "CoPE\u662f\u4e00\u4e2a\u53ef\u7b56\u7565\u5f15\u5bfc\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u77db\u76fe\u793a\u4f8b\u8bad\u7ec3\u548c\u53cc\u76ee\u6807\u6ce8\u65b9\u6cd5\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u5185\u5bb9\u6807\u6ce8\uff0c\u57287\u4e2a\u5371\u5bb3\u9886\u57df\u8fbe\u5230\u4e0e\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u7684\u51c6\u786e\u7387\uff0c\u4f46\u4f53\u79ef\u4ec5\u4e3a\u51761%\u3002", "motivation": "\u5f53\u524d\u5185\u5bb9\u6807\u6ce8\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u5927\u578b\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u65b0\u653f\u7b56\u3002\u9700\u8981\u5f00\u53d1\u5c0f\u578b\u3001\u9ad8\u6548\u3001\u53ef\u7b56\u7565\u5f15\u5bfc\u7684\u6a21\u578b\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u8f6c\u53d8\u4e3a\u653f\u7b56\u7f16\u5199\u4efb\u52a1\uff0c\u4e3a\u5728\u7ebf\u5e73\u53f0\u6cbb\u7406\u63d0\u4f9b\u65b0\u8bbe\u8ba1\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u77db\u76fe\u793a\u4f8b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u5b66\u4e60\u653f\u7b56\u89e3\u91ca\u800c\u975e\u7b80\u5355\u8bb0\u5fc6\uff1b\u5f00\u53d1\u53cc\u76ee\u6807\u6ce8\u65b9\u6cd5\uff0c\u5feb\u901f\u6784\u5efa\u65e0\u6b67\u4e49\u8bad\u7ec3\u6570\u636e\u96c6\uff1b\u8bad\u7ec39\u4ebf\u53c2\u6570\u7248\u672c\uff0c\u53ef\u5728\u5355\u5f20\u6d88\u8d39\u7ea7GPU\u4e0a\u8fd0\u884c\u3002", "result": "\u57287\u4e2a\u4e0d\u540c\u5371\u5bb3\u9886\u57df\u7684\u8bc4\u4f30\u4e2d\uff0cCoPE\u8868\u73b0\u51fa\u4e0e\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u51c6\u786e\u7387\uff0c\u4f46\u6a21\u578b\u5927\u5c0f\u4ec5\u4e3a\u524d\u6cbf\u6a21\u578b\u76841%\uff1b\u516c\u5f00\u53d1\u5e039\u4ebf\u53c2\u6570\u7248\u672c\uff0c\u53ef\u5728\u5355\u5f20\u6d88\u8d39\u7ea7GPU\u4e0a\u8fd0\u884c\u3002", "conclusion": "CoPE\u4ee3\u8868\u4e86\u5206\u7c7b\u5668\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c06ML\u4efb\u52a1\u8f6c\u53d8\u4e3a\u653f\u7b56\u7f16\u5199\u4efb\u52a1\uff0c\u4e3a\u5728\u7ebf\u5e73\u53f0\u6cbb\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u8bbe\u8ba1\u53ef\u80fd\u6027\uff0c\u5c55\u793a\u4e86\u5c0f\u578b\u9ad8\u6548\u6a21\u578b\u5728\u5185\u5bb9\u6807\u6ce8\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.18213", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18213", "abs": "https://arxiv.org/abs/2512.18213", "authors": ["Wu-Te Yang", "Masayoshi Tomizuka"], "title": "Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization", "comment": "6 pages, 4 figures", "summary": "Modeling soft pneumatic actuators with high precision remains a fundamental challenge due to their highly nonlinear and compliant characteristics. This paper proposes an innovative modeling framework based on fractional-order differential equations (FODEs) to accurately capture the dynamic behavior of soft materials. The unknown parameters within the fractional-order model are identified using particle swarm optimization (PSO), enabling parameter estimation directly from experimental data without reliance on pre-established material databases or empirical constitutive laws. The proposed approach effectively represents the complex deformation phenomena inherent in soft actuators. Experimental results validate the accuracy and robustness of the developed model, demonstrating improvement in predictive performance compared to conventional modeling techniques. The presented framework provides a data-efficient and database-independent solution for soft actuator modeling, advancing the precision and adaptability of soft robotic system design.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u6570\u9636\u5fae\u5206\u65b9\u7a0b\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\u7684\u8f6f\u4f53\u6c14\u52a8\u6267\u884c\u5668\u5efa\u6a21\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u52a8\u6001\u884c\u4e3a\u9884\u6d4b", "motivation": "\u8f6f\u4f53\u6c14\u52a8\u6267\u884c\u5668\u5177\u6709\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u67d4\u987a\u7279\u6027\uff0c\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5efa\u6a21\u6846\u67b6", "method": "\u4f7f\u7528\u5206\u6570\u9636\u5fae\u5206\u65b9\u7a0b\u5efa\u7acb\u6a21\u578b\uff0c\u901a\u8fc7\u7c92\u5b50\u7fa4\u4f18\u5316\u4ece\u5b9e\u9a8c\u6570\u636e\u4e2d\u76f4\u63a5\u8bc6\u522b\u672a\u77e5\u53c2\u6570\uff0c\u4e0d\u4f9d\u8d56\u6750\u6599\u6570\u636e\u5e93\u6216\u7ecf\u9a8c\u672c\u6784\u5b9a\u5f8b", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u5efa\u6a21\u6280\u672f\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8f6f\u4f53\u6267\u884c\u5668\u5efa\u6a21\u63d0\u4f9b\u4e86\u6570\u636e\u9ad8\u6548\u4e14\u4e0d\u4f9d\u8d56\u6570\u636e\u5e93\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u8bbe\u8ba1\u7684\u7cbe\u5ea6\u548c\u9002\u5e94\u6027"}}
{"id": "2512.18041", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18041", "abs": "https://arxiv.org/abs/2512.18041", "authors": ["Roger A. Finger", "Eduardo G. Cortes", "Sandro J. Rigo", "Gabriel de O. Ramos"], "title": "Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts", "comment": null, "summary": "Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"\u53d9\u4e8b\u6574\u5408\"\u65b0\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u5c06\u91cd\u53e0\u53d9\u4e8b\u6587\u6863\u6574\u5408\u4e3a\u7edf\u4e00\u3001\u8fde\u8d2f\u3001\u65f6\u5e8f\u6b63\u786e\u7684\u6587\u672c\uff0c\u800c\u975e\u4f20\u7edf\u6458\u8981\u7684\u538b\u7f29\u3002\u901a\u8fc7\u5f15\u5165\u65f6\u5e8f\u5bf9\u9f50\u4e8b\u4ef6\u56fe(TAEG)\u5e76\u5e94\u7528\u4e2d\u5fc3\u6027\u7b97\u6cd5\uff0c\u5728\u5723\u7ecf\u56db\u798f\u97f3\u4e66\u7814\u7a76\u4e2d\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u65f6\u5e8f\u6392\u5e8f\u548c\u5185\u5bb9\u6307\u6807\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5904\u7406\u91cd\u53e0\u53d9\u4e8b\u6587\u6863\uff08\u5982\u6cd5\u5f8b\u8bc1\u8bcd\u3001\u5386\u53f2\u8bb0\u8f7d\uff09\u65f6\uff0c\u4f20\u7edf\u591a\u6587\u6863\u6458\u8981\u65b9\u6cd5\u6ce8\u91cd\u538b\u7f29\u800c\u7834\u574f\u4e86\u53d9\u4e8b\u6d41\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u65f6\u5e8f\u5b8c\u6574\u6027\u3001\u5b8c\u6574\u6027\u548c\u7ec6\u8282\u878d\u5408\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u538b\u7f29\u3002", "method": "\u63d0\u51fa\"\u53d9\u4e8b\u6574\u5408\"\u65b0\u4efb\u52a1\uff0c\u5f15\u5165\u65f6\u5e8f\u5bf9\u9f50\u4e8b\u4ef6\u56fe(TAEG)\u6765\u663e\u5f0f\u5efa\u6a21\u65f6\u5e8f\u548c\u4e8b\u4ef6\u5bf9\u9f50\u3002\u901a\u8fc7\u5e94\u7528\u6807\u51c6\u4e2d\u5fc3\u6027\u7b97\u6cd5\u5230TAEG\uff0c\u4f5c\u4e3a\u7248\u672c\u9009\u62e9\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u4e8b\u4ef6\u9009\u62e9\u6700\u4e2d\u5fc3\u7684\u8868\u793a\u5e76\u7f6e\u4e8e\u6b63\u786e\u65f6\u5e8f\u4f4d\u7f6e\u3002", "result": "\u5728\u5723\u7ecf\u56db\u798f\u97f3\u4e66\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4fdd\u8bc1\u4e86\u5b8c\u7f8e\u7684\u65f6\u5e8f\u6392\u5e8f\uff08Kendall's Tau\u4e3a1.000\uff09\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5185\u5bb9\u6307\u6807\uff08\u5982ROUGE-L F1\u63d0\u5347357.2%\uff09\u3002\u9a8c\u8bc1\u4e86\u53d9\u4e8b\u6574\u5408\u4f5c\u4e3a\u76f8\u5173\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65f6\u5e8f\u5bf9\u9f50\u4e8b\u4ef6\u56fe(TAEG)\u7684\u6210\u529f\u9a8c\u8bc1\u4e86\u53d9\u4e8b\u6574\u5408\u4f5c\u4e3a\u76f8\u5173\u4efb\u52a1\u7684\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u663e\u5f0f\u65f6\u5e8f\u9aa8\u67b6\u662f\u89e3\u51b3\u8be5\u4efb\u52a1\u7684\u57fa\u672c\u7ec4\u4ef6\u3002\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u91cd\u53e0\u53d9\u4e8b\u6587\u6863\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.18268", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.18268", "abs": "https://arxiv.org/abs/2512.18268", "authors": ["Si Wei Feng"], "title": "On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region", "comment": null, "summary": "With the popularity of drone technologies, aerial photography have become prevalent in many daily scenarios such as environment monitoring, structure inspection, law enforcement etc. A central challenge in this domain is the efficient coverage of a target area with photographs that can entirely capture the region, while respecting constraints such as the image resolution, and limited number of pictures that can be taken. This work investigates the computational complexity of several fundamental problems arised from this challenge. By abstracting the aerial photography problem into the coverage problems in computational geometry, we demonstrate that most of these problems are in fact computationally intractable, with the implication that traditional algorithms cannot solve them efficiently. The intuitions of this work can extend beyond aerial photography to broader applications such as pesticide spraying, and strategic sensor placement.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u65e0\u4eba\u673a\u822a\u62cd\u8986\u76d6\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u8bc1\u660e\u591a\u6570\u6b64\u7c7b\u95ee\u9898\u5728\u8ba1\u7b97\u4e0a\u662f\u96be\u89e3\u7684\uff0c\u4f20\u7edf\u7b97\u6cd5\u65e0\u6cd5\u9ad8\u6548\u6c42\u89e3", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u6280\u672f\u666e\u53ca\uff0c\u822a\u62cd\u5728\u73af\u5883\u76d1\u6d4b\u3001\u7ed3\u6784\u68c0\u67e5\u3001\u6267\u6cd5\u7b49\u573a\u666f\u5e7f\u6cdb\u5e94\u7528\u3002\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u5728\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u62cd\u6444\u6570\u91cf\u7b49\u7ea6\u675f\u4e0b\u9ad8\u6548\u8986\u76d6\u76ee\u6807\u533a\u57df", "method": "\u5c06\u822a\u62cd\u95ee\u9898\u62bd\u8c61\u4e3a\u8ba1\u7b97\u51e0\u4f55\u4e2d\u7684\u8986\u76d6\u95ee\u9898\uff0c\u5206\u6790\u591a\u4e2a\u57fa\u7840\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027", "result": "\u8bc1\u660e\u5927\u591a\u6570\u822a\u62cd\u8986\u76d6\u95ee\u9898\u5728\u8ba1\u7b97\u4e0a\u662f\u96be\u89e3\u7684\uff08computationally intractable\uff09\uff0c\u4f20\u7edf\u7b97\u6cd5\u65e0\u6cd5\u9ad8\u6548\u6c42\u89e3", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e0d\u4ec5\u9002\u7528\u4e8e\u822a\u62cd\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u519c\u836f\u55b7\u6d12\u3001\u6218\u7565\u4f20\u611f\u5668\u90e8\u7f72\u7b49\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u9886\u57df"}}
{"id": "2512.18072", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.18072", "abs": "https://arxiv.org/abs/2512.18072", "authors": ["Ashley M. A. Fehr", "Calla G. Beauregard", "Julia Witte Zimmerman", "Katie Ekstr\u00f6m", "Pablo Rosillo-Rodes", "Christopher M. Danforth", "Peter Sheridan Dodds"], "title": "Statistical laws and linguistics inform meaning in naturalistic and fictional conversation", "comment": null, "summary": "Conversation is a cornerstone of social connection and is linked to well-being outcomes. Conversations vary widely in type with some portion generating complex, dynamic stories. One approach to studying how conversations unfold in time is through statistical patterns such as Heaps' law, which holds that vocabulary size scales with document length. Little work on Heaps's law has looked at conversation and considered how language features impact scaling. We measure Heaps' law for conversations recorded in two distinct mediums: 1. Strangers brought together on video chat and 2. Fictional characters in movies. We find that scaling of vocabulary size differs by parts of speech. We discuss these findings through behavioral and linguistic frameworks.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u8bdd\u4e2d\u8bcd\u6c47\u589e\u957f\u89c4\u5f8b\uff0c\u53d1\u73b0\u4e0d\u540c\u8bcd\u6027\u7684\u8bcd\u6c47\u589e\u957f\u6a21\u5f0f\u5b58\u5728\u5dee\u5f02", "motivation": "\u5bf9\u8bdd\u662f\u793e\u4f1a\u8fde\u63a5\u7684\u6838\u5fc3\uff0c\u4e0e\u5e78\u798f\u611f\u76f8\u5173\u3002\u5bf9\u8bdd\u7c7b\u578b\u591a\u6837\uff0c\u90e8\u5206\u5bf9\u8bdd\u4f1a\u4ea7\u751f\u590d\u6742\u7684\u52a8\u6001\u6545\u4e8b\u3002\u76ee\u524d\u5bf9Heaps\u5b9a\u5f8b\uff08\u8bcd\u6c47\u91cf\u968f\u6587\u6863\u957f\u5ea6\u589e\u957f\uff09\u5728\u5bf9\u8bdd\u4e2d\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u7279\u522b\u662f\u8bed\u8a00\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u589e\u957f\u89c4\u5f8b\u3002", "method": "\u6d4b\u91cf\u4e24\u79cd\u4e0d\u540c\u5a92\u4ecb\u5bf9\u8bdd\u4e2d\u7684Heaps\u5b9a\u5f8b\uff1a1\uff09\u964c\u751f\u4eba\u89c6\u9891\u804a\u5929\uff1b2\uff09\u7535\u5f71\u4e2d\u865a\u6784\u4eba\u7269\u7684\u5bf9\u8bdd\u3002\u5206\u6790\u4e0d\u540c\u8bcd\u6027\uff08\u540d\u8bcd\u3001\u52a8\u8bcd\u7b49\uff09\u7684\u8bcd\u6c47\u589e\u957f\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u8bcd\u6c47\u91cf\u7684\u589e\u957f\u89c4\u5f8b\u56e0\u8bcd\u6027\u4e0d\u540c\u800c\u5b58\u5728\u5dee\u5f02\u3002\u4e0d\u540c\u8bcd\u7c7b\u7684\u8bcd\u6c47\u589e\u957f\u6a21\u5f0f\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u7edf\u8ba1\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7\u884c\u4e3a\u548c\u8bed\u8a00\u5b66\u6846\u67b6\u8ba8\u8bba\u8fd9\u4e9b\u53d1\u73b0\uff0c\u8868\u660e\u5bf9\u8bdd\u4e2d\u7684\u8bcd\u6c47\u589e\u957f\u6a21\u5f0f\u5177\u6709\u8bcd\u6027\u7279\u5f02\u6027\uff0c\u8fd9\u4e3a\u7406\u89e3\u5bf9\u8bdd\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2512.18333", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18333", "abs": "https://arxiv.org/abs/2512.18333", "authors": ["Youssef Mahran", "Zeyad Gamal", "Ayman El-Badawy"], "title": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)", "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper proposes a new Reinforcement Learning (RL) based control architecture for quadrotors. With the literature focusing on controlling the four rotors' RPMs directly, this paper aims to control the quadrotor's thrust vector. The RL agent computes the percentage of overall thrust along the quadrotor's z-axis along with the desired Roll ($\u03c6$) and Pitch ($\u03b8$) angles. The agent then sends the calculated control signals along with the current quadrotor's Yaw angle ($\u03c8$) to an attitude PID controller. The PID controller then maps the control signals to motor RPMs. The Soft Actor-Critic algorithm, a model-free off-policy stochastic RL algorithm, was used to train the RL agents. Training results show the faster training time of the proposed thrust vector controller in comparison to the conventional RPM controllers. Simulation results show smoother and more accurate path-following for the proposed thrust vector controller.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56db\u65cb\u7ffc\u63a8\u529b\u77e2\u91cf\u63a7\u5236\u67b6\u6784\uff0c\u76f8\u6bd4\u4f20\u7edfRPM\u63a7\u5236\u8bad\u7ec3\u66f4\u5feb\u3001\u8def\u5f84\u8ddf\u8e2a\u66f4\u5e73\u6ed1\u51c6\u786e", "motivation": "\u73b0\u6709\u6587\u732e\u4e3b\u8981\u5173\u6ce8\u76f4\u63a5\u63a7\u5236\u56db\u4e2a\u65cb\u7ffc\u7684RPM\uff0c\u672c\u6587\u65e8\u5728\u63a7\u5236\u56db\u65cb\u7ffc\u7684\u63a8\u529b\u77e2\u91cf\uff0c\u4ee5\u6539\u8fdb\u63a7\u5236\u6027\u80fd", "method": "\u4f7f\u7528Soft Actor-Critic\u7b97\u6cd5\u8bad\u7ec3RL\u667a\u80fd\u4f53\uff0c\u667a\u80fd\u4f53\u8ba1\u7b97\u6cbfz\u8f74\u7684\u603b\u63a8\u529b\u767e\u5206\u6bd4\u4ee5\u53ca\u671f\u671b\u7684\u6eda\u8f6c\u548c\u4fef\u4ef0\u89d2\uff0c\u7ed3\u5408\u5f53\u524d\u504f\u822a\u89d2\u53d1\u9001\u7ed9\u59ff\u6001PID\u63a7\u5236\u5668\uff0cPID\u63a7\u5236\u5668\u5c06\u63a7\u5236\u4fe1\u53f7\u6620\u5c04\u5230\u7535\u673aRPM", "result": "\u8bad\u7ec3\u7ed3\u679c\u663e\u793a\u63a8\u529b\u77e2\u91cf\u63a7\u5236\u5668\u6bd4\u4f20\u7edfRPM\u63a7\u5236\u5668\u8bad\u7ec3\u65f6\u95f4\u66f4\u5feb\uff0c\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8def\u5f84\u8ddf\u8e2a\u66f4\u5e73\u6ed1\u51c6\u786e", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u529b\u77e2\u91cf\u63a7\u5236\u67b6\u6784\u5728\u56db\u65cb\u7ffc\u63a7\u5236\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\u4e14\u63a7\u5236\u6027\u80fd\u66f4\u597d"}}
{"id": "2512.18196", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18196", "abs": "https://arxiv.org/abs/2512.18196", "authors": ["Jundong Xu", "Hao Fei", "Huichi Zhou", "Xin Quan", "Qijun Huang", "Shengqiong Wu", "William Yang Wang", "Mong-Li Lee", "Wynne Hsu"], "title": "Training LLMs with LogicReward for Faithful and Rigorous Reasoning", "comment": "Preprint", "summary": "Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\\% and 2\\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at https://llm-symbol.github.io/LogicReward.", "AI": {"tldr": "LogicReward\uff1a\u901a\u8fc7\u5b9a\u7406\u8bc1\u660e\u5668\u5f3a\u5236\u6267\u884c\u6b65\u9aa4\u7ea7\u903b\u8f91\u6b63\u786e\u6027\u7684\u5956\u52b1\u7cfb\u7edf\uff0c\u63d0\u5347LLM\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u7ed3\u679c\u53cd\u9988\uff0c\u53ef\u80fd\u4ea7\u751f\u6b63\u786e\u4f46\u63a8\u7406\u8fc7\u7a0b\u6709\u7f3a\u9677\u7684\u7b54\u6848\u3002\u5728\u903b\u8f91\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\uff0c\u9700\u8981\u4fdd\u8bc1\u63a8\u7406\u8fc7\u7a0b\u7684\u903b\u8f91\u6b63\u786e\u6027\u3002", "method": "\u63d0\u51faLogicReward\u5956\u52b1\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b9a\u7406\u8bc1\u660e\u5668\u5f3a\u5236\u6267\u884c\u6b65\u9aa4\u7ea7\u903b\u8f91\u6b63\u786e\u6027\uff1b\u5f15\u5165\u8f6f\u7edf\u4e00\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u51cf\u5c11\u81ea\u7136\u8bed\u8a00\u6b67\u4e49\uff0c\u63d0\u9ad8\u5f62\u5f0f\u5316\u8d28\u91cf\u3002", "result": "\u4f7f\u7528LogicReward\u8bad\u7ec3\u76848B\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u8d85\u8d8aGPT-4o\u548co4-mini 11.6%\u548c2%\uff1b\u63d0\u5347\u63a8\u7406\u5fe0\u5b9e\u6027\uff0c\u589e\u5f3a\u5bf9\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u7b49\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LogicReward\u901a\u8fc7\u5b9a\u7406\u8bc1\u660e\u5668\u63d0\u4f9b\u53ef\u9760\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u63d0\u5347LLM\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u7684\u53ef\u9760\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18336", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18336", "abs": "https://arxiv.org/abs/2512.18336", "authors": ["Youssef Mahran", "Zeyad Gamal", "Ayman El-Badawy"], "title": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism", "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy. Its performance is compared against algorithms that train a deterministic one. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy with both static entropy and dynamic entropy and then executing deterministic actions to control the quadcopter is explored. It is then compared against training a deterministic policy and executing deterministic actions. For the purpose of this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. The training and simulation results show the positive effect the dynamic entropy tuning has on controlling the quadcopter by preventing catastrophic forgetting and improving exploration efficiency.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u52a8\u6001\u71b5\u8c03\u8282\u5bf9\u8bad\u7ec3\u968f\u673a\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u786e\u5b9a\u6027\u7b56\u7565\u8bad\u7ec3\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u63a7\u5236\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u52a8\u6001\u71b5\u8c03\u8282\u80fd\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6bd4\u8f83\u968f\u673a\u7b56\u7565\u548c\u786e\u5b9a\u6027\u7b56\u7565\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u7279\u522b\u5173\u6ce8\u52a8\u6001\u71b5\u8c03\u8282\u5bf9\u968f\u673a\u7b56\u7565\u8bad\u7ec3\u6548\u679c\u7684\u5f71\u54cd\uff0c\u4ee5\u89e3\u51b3\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u63a7\u5236\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528Soft Actor-Critic (SAC)\u7b97\u6cd5\u8bad\u7ec3\u968f\u673a\u7b56\u7565\uff0c\u4f7f\u7528Twin Delayed Deep Deterministic Policy Gradient (TD3)\u7b97\u6cd5\u8bad\u7ec3\u786e\u5b9a\u6027\u7b56\u7565\u3002\u5728\u968f\u673a\u7b56\u7565\u8bad\u7ec3\u4e2d\uff0c\u6bd4\u8f83\u4e86\u9759\u6001\u71b5\u548c\u52a8\u6001\u71b5\u8c03\u8282\u7684\u6548\u679c\uff0c\u5e76\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u63a7\u5236\u4efb\u52a1\u4e2d\u8fdb\u884c\u4eff\u771f\u5b9e\u9a8c\u3002", "result": "\u8bad\u7ec3\u548c\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u52a8\u6001\u71b5\u8c03\u8282\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u63a7\u5236\u4e2d\u5177\u6709\u79ef\u6781\u6548\u679c\uff0c\u80fd\u591f\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002\u968f\u673a\u7b56\u7565\u4e0e\u786e\u5b9a\u6027\u7b56\u7565\u5728\u63a7\u5236\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u5dee\u5f02\uff0c\u52a8\u6001\u71b5\u8c03\u8282\u4f18\u5316\u4e86\u968f\u673a\u7b56\u7565\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "conclusion": "\u52a8\u6001\u71b5\u8c03\u8282\u5728\u5f3a\u5316\u5b66\u4e60\u968f\u673a\u7b56\u7565\u8bad\u7ec3\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u5982\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u63a7\u5236\u4e2d\uff0c\u80fd\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u9ad8\u7b97\u6cd5\u6027\u80fd\u3002"}}
{"id": "2512.18225", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.18225", "abs": "https://arxiv.org/abs/2512.18225", "authors": ["Deepit Sapru"], "title": "GeoSense-AI: Fast Location Inference from Crisis Microblogs", "comment": null, "summary": "This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.", "AI": {"tldr": "GeoSense-AI\uff1a\u4e00\u4e2a\u5b9e\u65f6\u4ece\u5608\u6742\u5fae\u535a\u6d41\u4e2d\u63d0\u53d6\u5730\u7406\u4f4d\u7f6e\u7684AI\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cdNLP\u6280\u672f\u548c\u5730\u7406\u77e5\u8bc6\u5e93\uff0c\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u4f4d\u7f6e\u63a8\u65ad\uff0c\u76f8\u6bd4\u4f20\u7edfNER\u5de5\u5177\u5177\u6709\u66f4\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edf\u4f9d\u8d56\u7a00\u758f\u5730\u7406\u6807\u7b7e\u7684\u65b9\u6cd5\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\uff0c\u9700\u8981\u76f4\u63a5\u4ece\u6587\u672c\u4e2d\u9ad8\u6548\u63d0\u53d6\u5730\u7406\u4f4d\u7f6e\u4fe1\u606f\u4ee5\u652f\u6301\u5e94\u6025\u54cd\u5e94\u548c\u6001\u52bf\u611f\u77e5\u3002", "method": "\u7edf\u4e00\u7edf\u8ba1\u54c8\u5e0c\u6807\u7b7e\u5206\u5272\u3001\u8bcd\u6027\u9a71\u52a8\u7684\u4e13\u6709\u540d\u8bcd\u68c0\u6d4b\u3001\u56f4\u7ed5\u707e\u5bb3\u8bcd\u5178\u7684\u4f9d\u5b58\u89e3\u6790\u3001\u8f7b\u91cf\u7ea7\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5730\u540d\u5f55\u57fa\u7840\u6d88\u6b67\uff0c\u6784\u5efa\u4f4e\u5ef6\u8fdfNLP\u7ec4\u4ef6\u5e76\u9ad8\u6548\u9a8c\u8bc1\u5730\u7406\u77e5\u8bc6\u5e93\u3002", "result": "\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684NER\u5de5\u5177\u5305\u76f8\u6bd4\uff0c\u7cfb\u7edf\u5728\u4fdd\u6301\u9ad8F1\u5206\u6570\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u66f4\u5feb\u7684\u541e\u5410\u91cf\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u5b9e\u65f6\u5371\u673a\u4fe1\u606f\u5b66\u573a\u666f\u4e2d\uff0c\u5e76\u5728\u6d2a\u6c34\u3001\u75ab\u60c5\u7b49\u5feb\u901f\u4e8b\u4ef6\u4e2d\u5c55\u793a\u7aef\u5230\u7aef\u529f\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5148\u8003\u8651\u975e\u6b63\u5f0f\u6587\u672c\u7684\u9c81\u68d2\u6027\u548c\u6d41\u5904\u7406\u6548\u7387\uff0cGeoSense-AI\u5c55\u793a\u4e86\u9886\u57df\u8c03\u4f18\u7684NLP\u548c\u77e5\u8bc6\u57fa\u7840\u5982\u4f55\u8d85\u8d8a\u4f20\u7edf\u5730\u7406\u6807\u7b7e\u4f9d\u8d56\uff0c\u63d0\u5347\u5e94\u6025\u54cd\u5e94\u80fd\u529b\u3002"}}
{"id": "2512.18368", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18368", "abs": "https://arxiv.org/abs/2512.18368", "authors": ["Yihang Zhu", "Weiqing Wang", "Shijie Wu", "Ye Shi", "Jingya Wang"], "title": "Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation", "comment": null, "summary": "While imitation learning has shown impressive results in single-task robot manipulation, scaling it to multi-task settings remains a fundamental challenge due to issues such as suboptimal demonstrations, trajectory noise, and behavioral multi-modality. Existing skill-based methods attempt to address this by decomposing actions into reusable abstractions, but they often rely on fixed-length segmentation or environmental priors that limit semantic consistency and cross-task generalization. In this work, we propose AtomSkill, a novel multi-task imitation learning framework that learns and leverages a structured Atomic Skill Space for composable robot manipulation. Our approach is built on two key technical contributions. First, we construct a Semantically Grounded Atomic Skill Library by partitioning demonstrations into variable-length skills using gripper-state keyframe detection and vision-language model annotation. A contrastive learning objective ensures the resulting skill embeddings are both semantically consistent and temporally coherent. Second, we propose an Action Generation module with Keypose Imagination, which jointly predicts a skill's long-horizon terminal keypose and its immediate action sequence. This enables the policy to reason about overarching motion goals and fine-grained control simultaneously, facilitating robust skill chaining. Extensive experiments in simulated and real-world environments show that AtomSkill consistently outperforms state-of-the-art methods across diverse manipulation tasks.", "AI": {"tldr": "AtomSkill\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4efb\u52a1\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7ed3\u6784\u5316\u539f\u5b50\u6280\u80fd\u7a7a\u95f4\u5b9e\u73b0\u53ef\u7ec4\u5408\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u80fd\u65b9\u6cd5\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u5355\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u5b58\u5728\u6311\u6218\uff0c\u5305\u62ec\u6b21\u4f18\u6f14\u793a\u3001\u8f68\u8ff9\u566a\u58f0\u548c\u884c\u4e3a\u591a\u6a21\u6001\u95ee\u9898\u3002\u73b0\u6709\u6280\u80fd\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u957f\u5ea6\u5206\u5272\u6216\u73af\u5883\u5148\u9a8c\uff0c\u9650\u5236\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u6784\u5efa\u8bed\u4e49\u57fa\u7840\u7684\u539f\u5b50\u6280\u80fd\u5e93\uff1a\u4f7f\u7528\u5939\u6301\u5668\u72b6\u6001\u5173\u952e\u5e27\u68c0\u6d4b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6807\u6ce8\uff0c\u5c06\u6f14\u793a\u5206\u5272\u4e3a\u53d8\u957f\u6280\u80fd\uff1b\u5bf9\u6bd4\u5b66\u4e60\u786e\u4fdd\u6280\u80fd\u5d4c\u5165\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u30022. \u52a8\u4f5c\u751f\u6210\u6a21\u5757\uff1a\u901a\u8fc7\u5173\u952e\u59ff\u6001\u60f3\u8c61\u8054\u5408\u9884\u6d4b\u6280\u80fd\u7684\u957f\u671f\u7ec8\u7aef\u5173\u952e\u59ff\u6001\u548c\u5373\u65f6\u52a8\u4f5c\u5e8f\u5217\uff0c\u4f7f\u7b56\u7565\u80fd\u591f\u540c\u65f6\u63a8\u7406\u6574\u4f53\u8fd0\u52a8\u76ee\u6807\u548c\u7cbe\u7ec6\u63a7\u5236\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAtomSkill\u5728\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "AtomSkill\u901a\u8fc7\u5b66\u4e60\u7ed3\u6784\u5316\u539f\u5b50\u6280\u80fd\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u53ef\u7ec4\u5408\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.18301", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18301", "abs": "https://arxiv.org/abs/2512.18301", "authors": ["Tanjim Taharat Aurpa", "Md Shoaib Ahmed", "Md Mahbubur Rahman", "Md. Golam Moazzam"], "title": "InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning", "comment": null, "summary": "People use search engines for various topics and items, from daily essentials to more aspirational and specialized objects. Therefore, search engines have taken over as peoples preferred resource. The How To prefix has become familiar and widely used in various search styles to find solutions to particular problems. This search allows people to find sequential instructions by providing detailed guidelines to accomplish specific tasks. Categorizing instructional text is also essential for task-oriented learning and creating knowledge bases. This study uses the How To articles to determine the multi-label instruction category. We have brought this work with a dataset comprising 11,121 observations from wikiHow, where each record has multiple categories. To find out the multi-label category meticulously, we employ some transformer-based deep neural architectures, such as Generalized Autoregressive Pretraining for Language Understanding (XLNet), Bidirectional Encoder Representation from Transformers (BERT), etc. In our multi-label instruction classification process, we have reckoned our proposed architectures using accuracy and macro f1-score as the performance metrics. This thorough evaluation showed us much about our strategys strengths and drawbacks. Specifically, our implementation of the XLNet architecture has demonstrated unprecedented performance, achieving an accuracy of 97.30% and micro and macro average scores of 89.02% and 93%, a noteworthy accomplishment in multi-label classification. This high level of accuracy and macro average score is a testament to the effectiveness of the XLNet architecture in our proposed InstructNet approach. By employing a multi-level strategy in our evaluation process, we have gained a more comprehensive knowledge of the effectiveness of our proposed architectures and identified areas for forthcoming improvement and refinement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528wikiHow\u7684How To\u6587\u7ae0\u6784\u5efa\u6570\u636e\u96c6\uff0c\u91c7\u7528XLNet\u3001BERT\u7b49Transformer\u67b6\u6784\u8fdb\u884c\u591a\u6807\u7b7e\u6307\u4ee4\u5206\u7c7b\uff0c\u5176\u4e2dXLNet\u67b6\u6784\u5728InstructNet\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u523097.30%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u641c\u7d22\u5f15\u64ce\u5df2\u6210\u4e3a\u4eba\u4eec\u83b7\u53d6\u4fe1\u606f\u7684\u4e3b\u8981\u9014\u5f84\uff0c\"How To\"\u524d\u7f00\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5bfb\u627e\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u3002\u5bf9\u6559\u5b66\u6587\u672c\u8fdb\u884c\u5206\u7c7b\u5bf9\u4e8e\u4efb\u52a1\u5bfc\u5411\u5b66\u4e60\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u591a\u6807\u7b7e\u6307\u4ee4\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528wikiHow\u6784\u5efa\u5305\u542b11,121\u6761\u591a\u6807\u7b7e\u89c2\u5bdf\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08\u5305\u62ecXLNet\u3001BERT\u7b49\uff09\u8fdb\u884c\u591a\u6807\u7b7e\u6307\u4ee4\u5206\u7c7b\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u548c\u5b8fF1\u5206\u6570\u4f5c\u4e3a\u6027\u80fd\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "XLNet\u67b6\u6784\u5728InstructNet\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u523097.30%\u7684\u51c6\u786e\u7387\uff0c\u5fae\u5e73\u5747\u548c\u5b8f\u5e73\u5747\u5206\u6570\u5206\u522b\u4e3a89.02%\u548c93%\uff0c\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "conclusion": "XLNet\u67b6\u6784\u5728\u591a\u6807\u7b7e\u6307\u4ee4\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u8bc4\u4f30\u7b56\u7565\u5168\u9762\u4e86\u89e3\u4e86\u6240\u63d0\u67b6\u6784\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\uff0c\u4e3a\u4efb\u52a1\u5bfc\u5411\u5b66\u4e60\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2512.18396", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18396", "abs": "https://arxiv.org/abs/2512.18396", "authors": ["Yulu Wu", "Jiujun Cheng", "Haowen Wang", "Dengyang Suo", "Pei Ren", "Qichao Mao", "Shangce Gao", "Yakun Huang"], "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) and world-model methods have improved generalization in tasks such as robotic manipulation and object interaction. However, Successful execution of such tasks depends on large, costly collections of real demonstrations, especially for fine-grained manipulation of articulated objects. To address this, we present AOMGen, a scalable data generation framework for articulated manipulation which is instantiated from a single real scan, demonstration and a library of readily available digital assets, yielding photoreal training data with verified physical states. The framework synthesizes synchronized multi-view RGB temporally aligned with action commands and state annotations for joints and contacts, and systematically varies camera viewpoints, object styles, and object poses to expand a single execution into a diverse corpus. Experimental results demonstrate that fine-tuning VLA policies on AOMGen data increases the success rate from 0% to 88.7%, and the policies are tested on unseen objects and layouts.", "AI": {"tldr": "AOMGen\u6846\u67b6\u901a\u8fc7\u5355\u6b21\u771f\u5b9e\u626b\u63cf\u548c\u6f14\u793a\uff0c\u7ed3\u5408\u6570\u5b57\u8d44\u4ea7\u5e93\uff0c\u751f\u6210\u7528\u4e8e\u5173\u8282\u7269\u4f53\u7cbe\u7ec6\u64cd\u4f5c\u7684\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347VLA\u7b56\u7565\u6027\u80fd", "motivation": "\u5f53\u524dVLA\u548c\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u771f\u5b9e\u6f14\u793a\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u5173\u8282\u7269\u4f53\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8fd9\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027", "method": "\u63d0\u51faAOMGen\u6846\u67b6\uff1a\u4ece\u5355\u6b21\u771f\u5b9e\u626b\u63cf\u548c\u6f14\u793a\u51fa\u53d1\uff0c\u7ed3\u5408\u6570\u5b57\u8d44\u4ea7\u5e93\uff0c\u751f\u6210\u5177\u6709\u7269\u7406\u72b6\u6001\u9a8c\u8bc1\u7684\u903c\u771f\u8bad\u7ec3\u6570\u636e\uff0c\u5305\u542b\u591a\u89c6\u89d2RGB\u3001\u52a8\u4f5c\u6307\u4ee4\u3001\u5173\u8282\u72b6\u6001\u548c\u63a5\u89e6\u6807\u6ce8\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u53d8\u5316\u76f8\u673a\u89c6\u89d2\u3001\u7269\u4f53\u98ce\u683c\u548c\u59ff\u6001", "result": "\u5728AOMGen\u6570\u636e\u4e0a\u5fae\u8c03\u7684VLA\u7b56\u7565\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u523088.7%\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u5e03\u5c40\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "AOMGen\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u5bf9\u5927\u91cf\u771f\u5b9e\u6f14\u793a\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u6709\u6548\u63d0\u5347\u4e86VLA\u7b56\u7565\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2512.18321", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18321", "abs": "https://arxiv.org/abs/2512.18321", "authors": ["Tianlun Liu", "Zhiliang Tian", "Zhen Huang", "Xingzhi Zhou", "Wanlong Yu", "Tianle Liu", "Feng Liu", "Dongsheng Li"], "title": "CTTA-T: Continual Test-Time Adaptation for Text Understanding via Teacher-Student with a Domain-aware and Generalized Teacher", "comment": null, "summary": "Text understanding often suffers from domain shifts. To handle testing domains, domain adaptation (DA) is trained to adapt to a fixed and observed testing domain; a more challenging paradigm, test-time adaptation (TTA), cannot access the testing domain during training and online adapts to the testing samples during testing, where the samples are from a fixed domain. We aim to explore a more practical and underexplored scenario, continual test-time adaptation (CTTA) for text understanding, which involves a sequence of testing (unobserved) domains in testing. Current CTTA methods struggle in reducing error accumulation over domains and enhancing generalization to handle unobserved domains: 1) Noise-filtering reduces accumulated errors but discards useful information, and 2) accumulating historical domains enhances generalization, but it is hard to achieve adaptive accumulation. In this paper, we propose a CTTA-T (continual test-time adaptation for text understanding) framework adaptable to evolving target domains: it adopts a teacher-student framework, where the teacher is domain-aware and generalized for evolving domains. To improve teacher predictions, we propose a refine-then-filter based on dropout-driven consistency, which calibrates predictions and removes unreliable guidance. For the adaptation-generalization trade-off, we construct a domain-aware teacher by dynamically accumulating cross-domain semantics via incremental PCA, which continuously tracks domain shifts. Experiments show CTTA-T excels baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCTTA-T\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u672c\u7406\u89e3\u7684\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u548c\u52a8\u6001\u9886\u57df\u611f\u77e5\u673a\u5236\u5904\u7406\u5e8f\u5217\u5316\u672a\u89c2\u5bdf\u6d4b\u8bd5\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u5047\u8bbe\u6d4b\u8bd5\u9886\u57df\u56fa\u5b9a\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u6d4b\u8bd5\u9886\u57df\u4f1a\u968f\u65f6\u95f4\u53d8\u5316\u3002\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u9762\u4e34\u9519\u8bef\u7d2f\u79ef\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\uff1a\u566a\u58f0\u8fc7\u6ee4\u4f1a\u4e22\u5f03\u6709\u7528\u4fe1\u606f\uff0c\u5386\u53f2\u9886\u57df\u79ef\u7d2f\u96be\u4ee5\u81ea\u9002\u5e94\u5b9e\u73b0\u3002", "method": "\u63d0\u51faCTTA-T\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u67b6\u6784\uff0c\u6559\u5e08\u6a21\u578b\u5177\u6709\u9886\u57df\u611f\u77e5\u80fd\u529b\uff1b2\uff09\u57fa\u4e8edropout\u9a71\u52a8\u4e00\u81f4\u6027\u7684\u7cbe\u70bc-\u8fc7\u6ee4\u673a\u5236\uff0c\u6821\u51c6\u9884\u6d4b\u5e76\u79fb\u9664\u4e0d\u53ef\u9760\u6307\u5bfc\uff1b3\uff09\u901a\u8fc7\u589e\u91cfPCA\u52a8\u6001\u79ef\u7d2f\u8de8\u9886\u57df\u8bed\u4e49\uff0c\u6784\u5efa\u9886\u57df\u611f\u77e5\u6559\u5e08\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCTTA-T\u5728\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5e8f\u5217\u5316\u672a\u89c2\u5bdf\u6d4b\u8bd5\u9886\u57df\u3002", "conclusion": "CTTA-T\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6587\u672c\u7406\u89e3\u4e2d\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u6311\u6218\uff0c\u901a\u8fc7\u9886\u57df\u611f\u77e5\u6559\u5e08\u6a21\u578b\u548c\u52a8\u6001\u8bed\u4e49\u79ef\u7d2f\u673a\u5236\uff0c\u5e73\u8861\u4e86\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9886\u57df\u6f02\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18474", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18474", "abs": "https://arxiv.org/abs/2512.18474", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "When Robots Say No: The Empathic Ethical Disobedience Benchmark", "comment": "Accepted at the ACM/IEEE International Conference on Human-Robot Interaction (HRI 2026). This is a preprint of the author-accepted manuscript", "summary": "Robots must balance compliance with safety and social expectations as blind obedience can cause harm, while over-refusal erodes trust. Existing safe reinforcement learning (RL) benchmarks emphasize physical hazards, while human-robot interaction trust studies are small-scale and hard to reproduce. We present the Empathic Ethical Disobedience (EED) Gym, a standardized testbed that jointly evaluates refusal safety and social acceptability. Agents weigh risk, affect, and trust when choosing to comply, refuse (with or without explanation), clarify, or propose safer alternatives. EED Gym provides different scenarios, multiple persona profiles, and metrics for safety, calibration, and refusals, with trust and blame models grounded in a vignette study. Using EED Gym, we find that action masking eliminates unsafe compliance, while explanatory refusals help sustain trust. Constructive styles are rated most trustworthy, empathic styles -- most empathic, and safe RL methods improve robustness but also make agents more prone to overly cautious behavior. We release code, configurations, and reference policies to enable reproducible evaluation and systematic human-robot interaction research on refusal and trust. At submission time, we include an anonymized reproducibility package with code and configs, and we commit to open-sourcing the full repository after the paper is accepted.", "AI": {"tldr": "EED Gym\u662f\u4e00\u4e2a\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u4eba\u62d2\u7edd\u884c\u4e3a\u7684\u5b89\u5168\u6027\u548c\u793e\u4f1a\u53ef\u63a5\u53d7\u6027\uff0c\u901a\u8fc7\u6743\u8861\u98ce\u9669\u3001\u60c5\u611f\u548c\u4fe1\u4efb\u6765\u51b3\u5b9a\u670d\u4ece\u3001\u62d2\u7edd\u3001\u6f84\u6e05\u6216\u63d0\u51fa\u66f4\u5b89\u5168\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7269\u7406\u5371\u9669\uff0c\u800c\u4eba\u673a\u4ea4\u4e92\u4fe1\u4efb\u7814\u7a76\u89c4\u6a21\u5c0f\u4e14\u96be\u4ee5\u590d\u73b0\u3002\u673a\u5668\u4eba\u9700\u8981\u5728\u670d\u4ece\u4e0e\u5b89\u5168/\u793e\u4f1a\u671f\u671b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u76f2\u76ee\u670d\u4ece\u53ef\u80fd\u9020\u6210\u4f24\u5bb3\uff0c\u8fc7\u5ea6\u62d2\u7edd\u5219\u4f1a\u7834\u574f\u4fe1\u4efb\u3002", "method": "\u63d0\u51faEmpathic Ethical Disobedience (EED) Gym\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b\u4e0d\u540c\u573a\u666f\u3001\u591a\u4e2a\u4eba\u683c\u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5c0f\u63d2\u66f2\u7814\u7a76\u7684\u5b89\u5168\u3001\u6821\u51c6\u548c\u62d2\u7edd\u6307\u6807\uff0c\u5e76\u5efa\u7acb\u4fe1\u4efb\u548c\u8d23\u5907\u6a21\u578b\u3002", "result": "\u52a8\u4f5c\u5c4f\u853d\u80fd\u6d88\u9664\u4e0d\u5b89\u5168\u670d\u4ece\uff0c\u89e3\u91ca\u6027\u62d2\u7edd\u6709\u52a9\u4e8e\u7ef4\u6301\u4fe1\u4efb\uff1b\u5efa\u8bbe\u6027\u98ce\u683c\u6700\u503c\u5f97\u4fe1\u8d56\uff0c\u5171\u60c5\u98ce\u683c\u6700\u5177\u5171\u60c5\u529b\uff1b\u5b89\u5168RL\u65b9\u6cd5\u63d0\u9ad8\u9c81\u68d2\u6027\u4f46\u4e5f\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u8c28\u614e\u884c\u4e3a\u3002", "conclusion": "EED Gym\u4e3a\u62d2\u7edd\u548c\u4fe1\u4efb\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4fc3\u8fdb\u7cfb\u7edf\u6027\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\uff0c\u5e76\u627f\u8bfa\u5f00\u6e90\u4ee3\u7801\u3001\u914d\u7f6e\u548c\u53c2\u8003\u7b56\u7565\u3002"}}
{"id": "2512.18329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18329", "abs": "https://arxiv.org/abs/2512.18329", "authors": ["Guo Chen", "Junjie Huang", "Huaijin Xie", "Fei Sun", "Tao Jia"], "title": "LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation", "comment": "AAAI2026", "summary": "Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLiR\u00b3AG\u6846\u67b6\uff0c\u8ba9\u975e\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u91cd\u6784\u68c0\u7d22\u8bc1\u636e\u4e3a\u8fde\u8d2f\u63a8\u7406\u94fe\u6765\u8f6c\u79fb\u63a8\u7406\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63a8\u7406\u6a21\u578b\u80fd\u63d0\u5347\u591a\u8df3QA\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5e26\u6765\u5de8\u5927\u8ba1\u7b97\u6210\u672c\uff08token\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\uff09\u3002\u9700\u8981\u7406\u89e3\u5e76\u7f13\u89e3\u8fd9\u79cd\u6743\u8861\u3002", "method": "\u63d0\u51faLiR\u00b3AG\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u68c0\u7d22\u8bc1\u636e\u91cd\u6784\u4e3a\u8fde\u8d2f\u63a8\u7406\u94fe\uff0c\u4f7f\u975e\u63a8\u7406\u6a21\u578b\u80fd\u591f\u8f6c\u79fb\u63a8\u7406\u7b56\u7565\u3002\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u91c7\u7528\u4e24\u79cd\u4e3b\u8981\u6a21\u5f0f\uff1a\u4e0a\u4e0b\u6587\u57fa\u7840\u63a8\u7406\u548c\u77e5\u8bc6\u534f\u8c03\u63a8\u7406\u3002", "result": "LiR\u00b3AG\u663e\u8457\u51cf\u5c11\u5e73\u574798%\u8f93\u51fatoken\u5f00\u9500\u548c58.6%\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u5c068B\u975e\u63a8\u7406\u6a21\u578b\u7684F1\u6027\u80fd\u63d0\u53476.2%\u523022.5%\uff0c\u8d85\u8d8a32B\u63a8\u7406\u6a21\u578b\u5728RAG\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "LiR\u00b3AG\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u91cd\u6392\u63a8\u7406\u7b56\u7565\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2512.18477", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18477", "abs": "https://arxiv.org/abs/2512.18477", "authors": ["Wenjun Lin", "Jensen Zhang", "Kaitong Cai", "Keze Wang"], "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "comment": "Under submission", "summary": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.", "AI": {"tldr": "STORM\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u65f6\u7a7a\u63a8\u7406\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u52a8\u4f5c\u751f\u6210\u3001\u6761\u4ef6\u89c6\u9891\u9884\u6d4b\u548c\u641c\u7d22\u89c4\u5212\uff0c\u5728SimperEnv\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523051.0%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f9d\u8d56\u62bd\u8c61\u6f5c\u5728\u52a8\u6001\u6216\u5c06\u63a8\u7406\u59d4\u6258\u7ed9\u8bed\u8a00\u7ec4\u4ef6\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u89c6\u89c9\u63a8\u6f14\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u53ef\u89e3\u91ca\u3001\u6709\u8fdc\u89c1\u7684\u51b3\u7b56\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "STORM\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u6269\u6563\u5f0fVLA\u7b56\u7565\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u52a8\u4f5c\uff1b2) \u751f\u6210\u5f0f\u89c6\u9891\u4e16\u754c\u6a21\u578b\u6a21\u62df\u89c6\u89c9\u548c\u5956\u52b1\u7ed3\u679c\uff1b3) \u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u901a\u8fc7\u524d\u77bb\u8bc4\u4f30\u9009\u62e9\u6027\u4f18\u5316\u89c4\u5212\u3002", "result": "\u5728SimperEnv\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523051.0%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u4f18\u4e8eCogACT\u7b49\u5f3a\u57fa\u7ebf\u3002\u5956\u52b1\u589e\u5f3a\u7684\u89c6\u9891\u9884\u6d4b\u5c06Frechet\u89c6\u9891\u8ddd\u79bb\u964d\u4f4e\u8d85\u8fc775%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65f6\u7a7a\u4fdd\u771f\u5ea6\u548c\u4efb\u52a1\u76f8\u5173\u6027\u3002", "conclusion": "STORM\u5c55\u793a\u4e86\u641c\u7d22\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u4f18\u52bf\uff0c\u5177\u6709\u5f3a\u5927\u7684\u91cd\u65b0\u89c4\u5212\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u65f6\u7a7a\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.18337", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18337", "abs": "https://arxiv.org/abs/2512.18337", "authors": ["Weizhe Lin", "Hui-Ling Zhen", "Shuai Yang", "Xian Wang", "Renxi Liu", "Hanting Chen", "Wangze Zhang", "Chuansai Zhou", "Yiming Li", "Chen Chen", "Xing Li", "Zhiyuan Yang", "Xiaosong Li", "Xianzhi Yu", "Zhenhua Dong", "Mingxuan Yuan", "Yunhe Wang"], "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System", "comment": null, "summary": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.", "AI": {"tldr": "AgentInfer\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u667a\u80fd\u4f53\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u4f18\u5316\u7684\u56db\u4e2a\u7ec4\u4ef6\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c1150%\u4ee5\u4e0a\u65e0\u6548token\u6d88\u8017\uff0c\u5b9e\u73b01.8-2.5\u500d\u6574\u4f53\u52a0\u901f\u3002", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u4e25\u91cd\u6548\u7387\u95ee\u9898\uff0c\u8fd9\u4e9b\u6548\u7387\u74f6\u9888\u4e0d\u4ec5\u6765\u81ea\u5355\u4e00\u6a21\u578b\u63a8\u7406\uff0c\u66f4\u6e90\u4e8e\u63a8\u7406\u5faa\u73af\u3001\u4e0a\u4e0b\u6587\u589e\u957f\u548c\u5f02\u6784\u5de5\u5177\u4ea4\u4e92\u4e2d\u79ef\u7d2f\u7684\u7cfb\u7edf\u6027\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faAgentInfer\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1aAgentCollab\uff08\u5206\u5c42\u53cc\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff09\u3001AgentSched\uff08\u7f13\u5b58\u611f\u77e5\u6df7\u5408\u8c03\u5ea6\u5668\uff09\u3001AgentSAM\uff08\u57fa\u4e8e\u540e\u7f00\u81ea\u52a8\u673a\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff09\u3001AgentCompress\uff08\u8bed\u4e49\u538b\u7f29\u673a\u5236\uff09\uff0c\u5171\u540c\u5f62\u6210\u81ea\u8fdb\u5316\u5f15\u64ce\u3002", "result": "\u5728BrowseComp-zh\u548cDeepDiver\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u65b9\u6cd5\u534f\u540c\u5408\u4f5c\uff0cAgentInfer\u51cf\u5c11\u8d85\u8fc750%\u7684\u65e0\u6548token\u6d88\u8017\uff0c\u5b9e\u73b0\u6574\u4f531.8-2.5\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u4f18\u5316\u667a\u80fd\u4f53\u4efb\u52a1\u5b8c\u6210\uff08\u800c\u975e\u5355\u7eaf\u6bcftoken\u541e\u5410\u91cf\uff09\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u3001\u81ea\u6539\u8fdb\u667a\u80fd\u7cfb\u7edf\u7684\u5173\u952e\uff0cAgentInfer\u5c55\u793a\u4e86\u7cfb\u7edf\u6027\u4f18\u5316\u667a\u80fd\u4f53\u6548\u7387\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2512.18537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18537", "abs": "https://arxiv.org/abs/2512.18537", "authors": ["Erdao Liang"], "title": "Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators", "comment": "Source code is available at https://github.com/LuminousLamp/SUMO-Benchmark", "summary": "This paper presents a systematic benchmarking of the model-based microscopic traffic simulator SUMO against state-of-the-art data-driven traffic simulators using large-scale real-world datasets. Using the Waymo Open Motion Dataset (WOMD) and the Waymo Open Sim Agents Challenge (WOSAC), we evaluate SUMO under both short-horizon (8s) and long-horizon (60s) closed-loop simulation settings. To enable scalable evaluation, we develop Waymo2SUMO, an automated pipeline that converts WOMD scenarios into SUMO simulations. On the WOSAC benchmark, SUMO achieves a realism meta metric of 0.653 while requiring fewer than 100 tunable parameters. Extended rollouts show that SUMO maintains low collision and offroad rates and exhibits stronger long-horizon stability than representative data-driven simulators. These results highlight complementary strengths of model-based and data-driven approaches for autonomous driving simulation and benchmarking.", "AI": {"tldr": "SUMO\u4ea4\u901a\u4eff\u771f\u5668\u5728Waymo\u6570\u636e\u96c6\u4e0a\u7684\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5176\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u540c\u65f6\u53c2\u6570\u66f4\u5c11", "motivation": "\u5bf9\u6a21\u578b\u9a71\u52a8\u7684\u5fae\u89c2\u4ea4\u901a\u4eff\u771f\u5668SUMO\u4e0e\u6700\u5148\u8fdb\u7684\u6570\u636e\u9a71\u52a8\u4ea4\u901a\u4eff\u771f\u5668\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e24\u8005\u5728\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u4e2d\u7684\u4e92\u8865\u4f18\u52bf", "method": "\u5f00\u53d1Waymo2SUMO\u81ea\u52a8\u5316\u7ba1\u9053\u5c06WOMD\u573a\u666f\u8f6c\u6362\u4e3aSUMO\u4eff\u771f\uff0c\u5728WOSAC\u57fa\u51c6\u4e0a\u8bc4\u4f30\u77ed\u65f6\u7a0b\uff088\u79d2\uff09\u548c\u957f\u65f6\u7a0b\uff0860\u79d2\uff09\u95ed\u73af\u4eff\u771f\u6027\u80fd", "result": "SUMO\u5728WOSAC\u57fa\u51c6\u4e0a\u83b7\u5f970.653\u7684\u771f\u5b9e\u6027\u5143\u6307\u6807\uff0c\u4ec5\u9700\u5c11\u4e8e100\u4e2a\u53ef\u8c03\u53c2\u6570\uff0c\u5728\u957f\u65f6\u7a0b\u4eff\u771f\u4e2d\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u78b0\u649e\u548c\u504f\u79bb\u9053\u8def\u7387\uff0c\u7a33\u5b9a\u6027\u4f18\u4e8e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5", "conclusion": "\u6a21\u578b\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u4e92\u8865\u4f18\u52bf\uff0cSUMO\u5c55\u793a\u4e86\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u7a33\u5b9a\u6027\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf"}}
{"id": "2512.18352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18352", "abs": "https://arxiv.org/abs/2512.18352", "authors": ["Fengzhu Zeng", "Qian Shao", "Ling Cheng", "Wei Gao", "Shih-Fen Cheng", "Jing Ma", "Cheng Niu"], "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent", "comment": null, "summary": "Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u81ea\u4e3b\u4ee3\u7406\u548cLLM\u7684\u65b0\u578b\u65e9\u671f\u8c23\u8a00\u68c0\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u65f6\u95f4\u5e8f\u5217\u68c0\u6d4b\u95ee\u9898\uff0c\u4ec5\u9700\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4ee3\u7406\uff0cLLM\u4fdd\u6301\u8bad\u7ec3\u81ea\u7531\u3002", "motivation": "\u65e9\u671f\u8c23\u8a00\u68c0\u6d4b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u5177\u6709\u6311\u6218\u6027\u3002LLM\u5728\u5c11\u6837\u672cNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4e0d\u9002\u5408\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u4e14\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684EARD\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u4e3b\u4ee3\u7406\u548c\u57fa\u4e8eLLM\u7684\u68c0\u6d4b\u6a21\u578b\uff1a\u4ee3\u7406\u4f5c\u4e3a\u65e9\u671f\u65f6\u95f4\u70b9\u786e\u5b9a\u7684\u53ef\u9760\u51b3\u7b56\u8005\uff0cLLM\u4f5c\u4e3a\u5f3a\u5927\u7684\u8c23\u8a00\u68c0\u6d4b\u5668\u3002\u4ec5\u9700\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4ee3\u7406\uff0cLLM\u4fdd\u6301\u8bad\u7ec3\u81ea\u7531\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u548c\u65e9\u671f\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684EARD\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5c11\u6837\u672c\u65e9\u671f\u8c23\u8a00\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u4e3b\u4ee3\u7406\u548cLLM\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u65e9\u671f\u68c0\u6d4b\u3002"}}
{"id": "2512.18662", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18662", "abs": "https://arxiv.org/abs/2512.18662", "authors": ["Chihiro Noguchi", "Takaki Yamamoto"], "title": "Offline Reinforcement Learning for End-to-End Autonomous Driving", "comment": "15 pages", "summary": "End-to-end (E2E) autonomous driving models that take only camera images as input and directly predict a future trajectory are appealing for their computational efficiency and potential for improved generalization via unified optimization; however, persistent failure modes remain due to reliance on imitation learning (IL). While online reinforcement learning (RL) could mitigate IL-induced issues, the computational burden of neural rendering-based simulation and large E2E networks renders iterative reward and hyperparameter tuning costly. We introduce a camera-only E2E offline RL framework that performs no additional exploration and trains solely on a fixed simulator dataset. Offline RL offers strong data efficiency and rapid experimental iteration, yet is susceptible to instability from overestimation on out-of-distribution (OOD) actions. To address this, we construct pseudo ground-truth trajectories from expert driving logs and use them as a behavior regularization signal, suppressing imitation of unsafe or suboptimal behavior while stabilizing value learning. Training and closed-loop evaluation are conducted in a neural rendering environment learned from the public nuScenes dataset. Empirically, the proposed method achieves substantial improvements in collision rate and route completion compared with IL baselines. Our code will be available at [URL].", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u7684\u7aef\u5230\u7aef\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u6b63\u5219\u5316\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\uff0c\u76f8\u6bd4\u6a21\u4eff\u5b66\u4e60\u57fa\u7ebf\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u5e76\u63d0\u9ad8\u8def\u7ebf\u5b8c\u6210\u7387\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u5177\u6709\u7edf\u4e00\u4f18\u5316\u7684\u6cdb\u5316\u6f5c\u529b\uff0c\u4f46\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\u5b58\u5728\u6301\u7eed\u5931\u8d25\u6a21\u5f0f\u3002\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u57fa\u4e8e\u795e\u7ecf\u6e32\u67d3\u7684\u4eff\u771f\u548c\u5927\u89c4\u6a21\u7aef\u5230\u7aef\u7f51\u7edc\u5bfc\u81f4\u8fed\u4ee3\u8c03\u4f18\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u7684\u7aef\u5230\u7aef\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e0d\u8fdb\u884c\u989d\u5916\u63a2\u7d22\uff0c\u4ec5\u4f7f\u7528\u56fa\u5b9a\u4eff\u771f\u6570\u636e\u96c6\u8bad\u7ec3\u3002\u901a\u8fc7\u4ece\u4e13\u5bb6\u9a7e\u9a76\u65e5\u5fd7\u6784\u5efa\u4f2a\u771f\u5b9e\u8f68\u8ff9\u4f5c\u4e3a\u884c\u4e3a\u6b63\u5219\u5316\u4fe1\u53f7\uff0c\u6291\u5236\u4e0d\u5b89\u5168\u6216\u6b21\u4f18\u884c\u4e3a\u6a21\u4eff\uff0c\u540c\u65f6\u7a33\u5b9a\u4ef7\u503c\u5b66\u4e60\u3002", "result": "\u5728\u4ecenuScenes\u6570\u636e\u96c6\u5b66\u4e60\u7684\u795e\u7ecf\u6e32\u67d3\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\u548c\u95ed\u73af\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6a21\u4eff\u5b66\u4e60\u57fa\u7ebf\u5728\u78b0\u649e\u7387\u548c\u8def\u7ebf\u5b8c\u6210\u7387\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u884c\u4e3a\u6b63\u5219\u5316\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6a21\u4eff\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u4f9b\u6570\u636e\u9ad8\u6548\u4e14\u5b9e\u9a8c\u8fed\u4ee3\u5feb\u901f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u9a7e\u9a76\u5b89\u5168\u6027\u3002"}}
{"id": "2512.18357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18357", "abs": "https://arxiv.org/abs/2512.18357", "authors": ["El Mokhtar Hribach", "Oussama Mechhour", "Mohammed Elmonstaser", "Yassine El Boudouri", "Othmane Kabal"], "title": "DACE For Railway Acronym Disambiguation", "comment": null, "summary": "Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.", "AI": {"tldr": "DACE\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u9009\u62e9\u548c\u96c6\u6210\u805a\u5408\uff0c\u5728\u6cd5\u8bed\u94c1\u8def\u6587\u6863\u7684\u9996\u5b57\u6bcd\u7f29\u5199\u6d88\u6b67\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0cF1\u5206\u6570\u8fbe0.9069\u3002", "motivation": "\u6280\u672f\u6587\u672c\u5904\u7406\u4e2d\u7684\u9996\u5b57\u6bcd\u7f29\u5199\u6d88\u6b67\uff08AD\uff09\u662f\u91cd\u8981\u6311\u6218\uff0c\u5c24\u5176\u5728\u4e13\u4e1a\u9886\u57df\u5982\u94c1\u8def\u6587\u6863\u4e2d\uff0c\u9ad8\u6b67\u4e49\u6027\u4f7f\u81ea\u52a8\u5316\u5206\u6790\u53d8\u5f97\u590d\u6742\u3002TextMine'26\u7ade\u8d5b\u7684\u6cd5\u8bed\u94c1\u8def\u6587\u6863\u4efb\u52a1\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u3002", "method": "\u63d0\u51faDACE\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u63d0\u793a\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u9009\u62e9\u548c\u96c6\u6210\u805a\u5408\u56db\u4e2a\u7ec4\u4ef6\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u9488\u5bf9\u7f29\u5199\u6b67\u4e49\u52a8\u6001\u5b9a\u5236\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u9884\u6d4b\u805a\u5408\u6765\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5728TextMine'26\u7ade\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0cF1\u5206\u6570\u8fbe\u52300.9069\uff0c\u6709\u6548\u5904\u7406\u4e86\u4f4e\u8d44\u6e90\u573a\u666f\u5e76\u51cf\u5c11\u4e86\u6a21\u578b\u5e7b\u89c9\u3002", "conclusion": "DACE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u63d0\u793a\u3001\u77e5\u8bc6\u6ce8\u5165\u548c\u96c6\u6210\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e13\u4e1a\u9886\u57df\u4e2d\u7684\u9996\u5b57\u6bcd\u7f29\u5199\u6d88\u6b67\u95ee\u9898\uff0c\u5728\u6cd5\u8bed\u94c1\u8def\u6587\u6863\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6280\u672f\u6587\u672c\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18703", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18703", "abs": "https://arxiv.org/abs/2512.18703", "authors": ["Cailin Lei", "Haiyang Wu", "Yuxiong Ji", "Xiaoyu Cai", "Yuchuan Du"], "title": "CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles", "comment": null, "summary": "Enhancing the performance of trajectory planners for lane - changing vehicles is one of the key challenges in autonomous driving within human - machine mixed traffic. Most existing studies have not incorporated human drivers' prior knowledge when designing trajectory planning models. To address this issue, this study proposes a novel trajectory planning framework that integrates causal prior knowledge into the control process. Both longitudinal and lateral microscopic behaviors of vehicles are modeled to quantify interaction risk, and a staged causal graph is constructed to capture causal dependencies in lane-changing scenarios. Causal effects between the lane-changing vehicle and surrounding vehicles are then estimated using causal inference, including average causal effects (ATE) and conditional average treatment effects (CATE). These causal priors are embedded into a model predictive control (MPC) framework to enhance trajectory planning. The proposed approach is validated on naturalistic vehicle trajectory datasets. Experimental results show that: (1) causal inference provides interpretable and stable quantification of vehicle interactions; (2) individual causal effects reveal driver heterogeneity; and (3) compared with the baseline MPC, the proposed method achieves a closer alignment with human driving behaviors, reducing maximum trajectory deviation from 1.2 m to 0.2 m, lateral velocity fluctuation by 60%, and yaw angle variability by 50%. These findings provide methodological support for human-like trajectory planning and practical value for improving safety, stability, and realism in autonomous vehicle testing and traffic simulation platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u56e0\u679c\u5148\u9a8c\u77e5\u8bc6\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u91cf\u5316\u8f66\u8f86\u4ea4\u4e92\u98ce\u9669\uff0c\u5e76\u5d4c\u5165MPC\u63a7\u5236\u5668\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u66f4\u8d34\u8fd1\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u89c4\u5212\u6a21\u578b\u5927\u591a\u672a\u878d\u5165\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7c7b\u4eba\u9a7e\u9a76\u884c\u4e3a\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u8f66\u8f86\u5fae\u89c2\u884c\u4e3a\u6a21\u578b\u91cf\u5316\u4ea4\u4e92\u98ce\u9669\uff0c\u6784\u5efa\u5206\u9636\u6bb5\u56e0\u679c\u56fe\u6355\u6349\u6362\u9053\u573a\u666f\u7684\u56e0\u679c\u4f9d\u8d56\uff0c\u4f7f\u7528\u56e0\u679c\u63a8\u65ad\u4f30\u8ba1\u5e73\u5747\u56e0\u679c\u6548\u5e94\u548c\u6761\u4ef6\u5e73\u5747\u5904\u7406\u6548\u5e94\uff0c\u5c06\u56e0\u679c\u5148\u9a8c\u5d4c\u5165MPC\u6846\u67b6\u3002", "result": "\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4ea4\u4e92\u91cf\u5316\uff0c\u63ed\u793a\u9a7e\u9a76\u5458\u5f02\u8d28\u6027\uff1b\u76f8\u6bd4\u57fa\u7ebfMPC\uff0c\u6700\u5927\u8f68\u8ff9\u504f\u5dee\u4ece1.2m\u964d\u81f30.2m\uff0c\u6a2a\u5411\u901f\u5ea6\u6ce2\u52a8\u51cf\u5c1160%\uff0c\u6a2a\u6446\u89d2\u53d8\u5f02\u6027\u964d\u4f4e50%\u3002", "conclusion": "\u878d\u5408\u56e0\u679c\u5148\u9a8c\u7684\u8f68\u8ff9\u89c4\u5212\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u7c7b\u4eba\u5316\u7a0b\u5ea6\uff0c\u4e3a\u5b89\u5168\u3001\u7a33\u5b9a\u3001\u771f\u5b9e\u7684\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u548c\u4ea4\u901a\u4eff\u771f\u63d0\u4f9b\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2512.18360", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18360", "abs": "https://arxiv.org/abs/2512.18360", "authors": ["Mateusz Lango", "Ond\u0159ej Du\u0161ek"], "title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators", "comment": "EMNLP 2025", "summary": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2aLLM\u4ee3\u7406\u534f\u4f5c\u751f\u6210RDF\u5230\u6587\u672c\u7684\u89c4\u5219\u4ee3\u7801\uff0c\u65e0\u9700\u76d1\u7763\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u73b0\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u6587\u672c\u751f\u6210", "motivation": "\u4f20\u7edfRDF\u5230\u6587\u672c\u751f\u6210\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u76d1\u7763\u8bad\u7ec3\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u65e0\u9700\u9886\u57df\u7279\u5b9a\u53c2\u8003\u6587\u672c\u3001\u5b8c\u5168\u53ef\u89e3\u91ca\u4e14\u51cf\u5c11\u5e7b\u89c9\u7684\u751f\u6210\u65b9\u6cd5", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2aLLM\u4ee3\u7406\u534f\u4f5c\u4ea4\u4e92\u800c\u975e\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u3002\u4ee3\u7406\u57fa\u4e8eRDF\u4e09\u5143\u7ec4\u751f\u6210\u57fa\u4e8e\u89c4\u5219\u7684Python\u4ee3\u7801\u4f5c\u4e3a\u751f\u6210\u5668\uff0c\u65e0\u9700\u9886\u57df\u5185\u4eba\u5de5\u53c2\u8003\u6587\u672c", "result": "\u5728WebNLG\u548cOpenDialKG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0c\u4ec5\u5e26\u6765\u8f7b\u5fae\u6d41\u7545\u6027\u635f\u5931\uff08\u76f8\u6bd4\u5fae\u8c03\u6216\u63d0\u793a\u8bed\u8a00\u6a21\u578b\uff09\u3002\u7cfb\u7edf\u5b8c\u5168\u53ef\u89e3\u91ca\uff0c\u65e0\u9700\u76d1\u7763\u8bad\u7ec3\u6570\u636e\uff0c\u4ec5\u9700\u5355\u4e2aCPU\u5373\u53ef\u8fd1\u4e4e\u5373\u65f6\u751f\u6210\u6587\u672c", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u4e3aRDF\u5230\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u534f\u4f5c\u751f\u6210\u89c4\u5219\u4ee3\u7801\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3001\u53ef\u89e3\u91ca\u4e14\u51cf\u5c11\u5e7b\u89c9\u7684\u6587\u672c\u751f\u6210\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2512.18712", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18712", "abs": "https://arxiv.org/abs/2512.18712", "authors": ["Maozeng Zhang", "Ke Shi", "Huijun Li", "Tongshu Chen", "Jiejun Yan", "Aiguo Song"], "title": "DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics", "comment": null, "summary": "Stroke-induced motor impairment often results in substantial loss of upper-limb function, creating a strong demand for rehabilitation robots that enable safe and transparent physical human-robot interaction (pHRI). Variable stiffness actuators are well suited for such applications. However, in most existing designs, stiffness is coupled with the deflection angle, complicating both modeling and control. To address this limitation, this paper presents a variable stiffness actuator featuring decoupled stiffness and output behavior for rehabilitation robotics. The system integrates a variable stiffness mechanism that combines a variable-length lever with a hypocycloidal straight-line mechanism to achieve a linear torque-deflection relationship and continuous stiffness modulation from near zero to theoretically infinite. It also incorporates a differential transmission mechanism based on a planetary gear system that enables dual-motor load sharing. A cascade PI controller is further developed on the basis of the differential configuration, in which the position-loop term jointly regulates stiffness and deflection angle, effectively suppressing stiffness fluctuations and output disturbances. The performance of prototype was experimentally validated through stiffness calibration, stiffness regulation, torque control, decoupled characteristics, and dual-motor load sharing, indicating the potential for rehabilitation exoskeletons and other pHRI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5eb7\u590d\u673a\u5668\u4eba\u7684\u89e3\u8026\u521a\u5ea6\u4e0e\u8f93\u51fa\u7684\u53d8\u521a\u5ea6\u9a71\u52a8\u5668\uff0c\u91c7\u7528\u53ef\u53d8\u957f\u5ea6\u6760\u6746\u4e0e\u5185\u6446\u7ebf\u76f4\u7ebf\u673a\u6784\u5b9e\u73b0\u7ebf\u6027\u626d\u77e9-\u504f\u8f6c\u5173\u7cfb\uff0c\u7ed3\u5408\u5dee\u52a8\u4f20\u52a8\u5b9e\u73b0\u53cc\u7535\u673a\u8d1f\u8f7d\u5171\u4eab\u3002", "motivation": "\u4e2d\u98ce\u5bfc\u81f4\u7684\u8fd0\u52a8\u969c\u788d\u5e38\u9020\u6210\u4e0a\u80a2\u529f\u80fd\u4e25\u91cd\u4e27\u5931\uff0c\u9700\u8981\u5b89\u5168\u900f\u660e\u7684\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u5eb7\u590d\u673a\u5668\u4eba\u3002\u73b0\u6709\u53d8\u521a\u5ea6\u9a71\u52a8\u5668\u901a\u5e38\u521a\u5ea6\u4e0e\u504f\u8f6c\u89d2\u8026\u5408\uff0c\u4f7f\u5efa\u6a21\u548c\u63a7\u5236\u590d\u6742\u5316\u3002", "method": "\u8bbe\u8ba1\u89e3\u8026\u521a\u5ea6\u4e0e\u8f93\u51fa\u884c\u4e3a\u7684\u53d8\u521a\u5ea6\u9a71\u52a8\u5668\uff1a1) \u53ef\u53d8\u957f\u5ea6\u6760\u6746\u4e0e\u5185\u6446\u7ebf\u76f4\u7ebf\u673a\u6784\u7ec4\u5408\u5b9e\u73b0\u7ebf\u6027\u626d\u77e9-\u504f\u8f6c\u5173\u7cfb\u548c\u8fde\u7eed\u521a\u5ea6\u8c03\u8282\uff1b2) \u57fa\u4e8e\u884c\u661f\u9f7f\u8f6e\u7cfb\u7edf\u7684\u5dee\u52a8\u4f20\u52a8\u673a\u5236\u5b9e\u73b0\u53cc\u7535\u673a\u8d1f\u8f7d\u5171\u4eab\uff1b3) \u5f00\u53d1\u57fa\u4e8e\u5dee\u52a8\u914d\u7f6e\u7684\u7ea7\u8054PI\u63a7\u5236\u5668\u3002", "result": "\u901a\u8fc7\u521a\u5ea6\u6821\u51c6\u3001\u521a\u5ea6\u8c03\u8282\u3001\u626d\u77e9\u63a7\u5236\u3001\u89e3\u8026\u7279\u6027\u548c\u53cc\u7535\u673a\u8d1f\u8f7d\u5171\u4eab\u7b49\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u539f\u578b\u6027\u80fd\uff0c\u8868\u660e\u8be5\u9a71\u52a8\u5668\u9002\u7528\u4e8e\u5eb7\u590d\u5916\u9aa8\u9abc\u548c\u5176\u4ed6\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u53d8\u521a\u5ea6\u9a71\u52a8\u5668\u6210\u529f\u5b9e\u73b0\u4e86\u521a\u5ea6\u4e0e\u8f93\u51fa\u7684\u89e3\u8026\uff0c\u5177\u6709\u7ebf\u6027\u626d\u77e9-\u504f\u8f6c\u5173\u7cfb\u548c\u8fde\u7eed\u521a\u5ea6\u8c03\u8282\u80fd\u529b\uff0c\u4e3a\u5eb7\u590d\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u900f\u660e\u7684\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18362", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18362", "abs": "https://arxiv.org/abs/2512.18362", "authors": ["Wiktor Kamzela", "Mateusz Lango", "Ondrej Dusek"], "title": "SRS-Stories: Vocabulary-constrained multilingual story generation for language learning", "comment": "EMNLP 2025", "summary": "In this paper, we use large language models to generate personalized stories for language learners, using only the vocabulary they know. The generated texts are specifically written to teach the user new vocabulary by simply reading stories where it appears in context, while at the same time seamlessly reviewing recently learned vocabulary. The generated stories are enjoyable to read and the vocabulary reviewing/learning is optimized by a Spaced Repetition System. The experiments are conducted in three languages: English, Chinese and Polish, evaluating three story generation methods and three strategies for enforcing lexical constraints. The results show that the generated stories are more grammatical, coherent, and provide better examples of word usage than texts generated by the standard constrained beam search approach", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u8bed\u8a00\u5b66\u4e60\u8005\u751f\u6210\u4e2a\u6027\u5316\u6545\u4e8b\uff0c\u4ec5\u4f7f\u7528\u4ed6\u4eec\u5df2\u77e5\u7684\u8bcd\u6c47\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u9605\u8bfb\u6559\u6388\u65b0\u8bcd\u6c47\u5e76\u590d\u4e60\u5df2\u5b66\u8bcd\u6c47\uff0c\u7ed3\u5408\u95f4\u9694\u91cd\u590d\u7cfb\u7edf\u4f18\u5316\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u4e3a\u8bed\u8a00\u5b66\u4e60\u8005\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u9605\u8bfb\u6750\u6599\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u81ea\u7136\u5b66\u4e60\u65b0\u8bcd\u6c47\uff0c\u540c\u65f6\u590d\u4e60\u5df2\u5b66\u8bcd\u6c47\uff0c\u89e3\u51b3\u4f20\u7edf\u8bed\u8a00\u5b66\u4e60\u6750\u6599\u7f3a\u4e4f\u4e2a\u6027\u5316\u548c\u8da3\u5473\u6027\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6545\u4e8b\uff0c\u91c7\u7528\u4e09\u79cd\u6545\u4e8b\u751f\u6210\u65b9\u6cd5\u548c\u4e09\u79cd\u8bcd\u6c47\u7ea6\u675f\u7b56\u7565\uff0c\u7ed3\u5408\u95f4\u9694\u91cd\u590d\u7cfb\u7edf\u4f18\u5316\u8bcd\u6c47\u5b66\u4e60\uff0c\u5728\u82f1\u8bed\u3001\u4e2d\u6587\u548c\u6ce2\u5170\u8bed\u4e09\u79cd\u8bed\u8a00\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u751f\u6210\u7684\u6545\u4e8b\u60c5\u8282\u66f4\u7b26\u5408\u8bed\u6cd5\u3001\u66f4\u8fde\u8d2f\uff0c\u6bd4\u6807\u51c6\u7ea6\u675f\u675f\u641c\u7d22\u65b9\u6cd5\u751f\u6210\u7684\u6587\u672c\u63d0\u4f9b\u66f4\u597d\u7684\u8bcd\u6c47\u4f7f\u7528\u793a\u4f8b\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u751f\u6210\u4e2a\u6027\u5316\u7684\u8bed\u8a00\u5b66\u4e60\u6750\u6599\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u9605\u8bfb\u81ea\u7136\u6559\u6388\u65b0\u8bcd\u6c47\u5e76\u590d\u4e60\u5df2\u5b66\u8bcd\u6c47\uff0c\u7ed3\u5408\u95f4\u9694\u91cd\u590d\u7cfb\u7edf\u53ef\u4ee5\u4f18\u5316\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2512.18836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18836", "abs": "https://arxiv.org/abs/2512.18836", "authors": ["Jingjia Teng", "Yang Li", "Jianqiang Wang", "Yingbai Hu", "Songyuan Tang", "Manjiang Hu"], "title": "Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes", "comment": null, "summary": "Four-wheel Independent Steering (4WIS) vehicles have attracted increasing attention for their superior maneuverability. Human drivers typically choose to cross or drive over the low-profile obstacles (e.g., plastic bags) to efficiently navigate through narrow spaces, while existing planners neglect obstacle attributes, causing inefficiency or path-finding failures. To address this, we propose a trajectory planning framework integrating the 4WIS hybrid A* and Optimal Control Problem (OCP), in which the hybrid A* provides an initial path to enhance the OCP solution. Specifically, a multimodal classification network is introduced to assess scene complexity (hard/easy task) by fusing image and vehicle state data. For hard tasks, guided points are set to decompose complex tasks into local subtasks, improving the search efficiency of 4WIS hybrid A*. The multiple steering modes of 4WIS vehicles (Ackermann, diagonal, and zero-turn) are also incorporated into node expansion and heuristic designs. Moreover, a hierarchical obstacle handling strategy is designed to guide the node expansion considering obstacle attributes, i.e., 'non-traversable', 'crossable', and 'drive-over' obstacles. It allows crossing or driving over obstacles instead of the 'avoid-only' strategy, greatly enhancing success rates of pathfinding. We also design a logical constraint for the 'drive-over' obstacle by limiting its velocity to ensure safety. Furthermore, to address dynamic obstacles with motion uncertainty, we introduce a probabilistic risk field model, constructing risk-aware driving corridors that serve as linear collision constraints in OCP. Experimental results demonstrate the proposed framework's effectiveness in generating safe, efficient, and smooth trajectories for 4WIS vehicles, especially in constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u54084WIS\u6df7\u5408A*\u548c\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5206\u7c7b\u8bc4\u4f30\u573a\u666f\u590d\u6742\u5ea6\uff0c\u8bbe\u8ba1\u5206\u5c42\u969c\u788d\u7269\u5904\u7406\u7b56\u7565\uff08\u53ef\u7a7f\u8d8a/\u53ef\u538b\u8fc7\u969c\u788d\uff09\uff0c\u5e76\u5f15\u5165\u6982\u7387\u98ce\u9669\u573a\u5904\u7406\u52a8\u6001\u969c\u788d\uff0c\u63d0\u5347\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u8f66\u8f86\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u89c4\u5212\u5668\u5ffd\u7565\u969c\u788d\u7269\u5c5e\u6027\uff08\u5982\u4f4e\u77ee\u969c\u788d\u7269\u53ef\u7a7f\u8d8a\u6216\u538b\u8fc7\uff09\uff0c\u5bfc\u81f4\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u8f66\u8f86\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u89c4\u5212\u6548\u7387\u4f4e\u4e0b\u6216\u5931\u8d25\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u969c\u788d\u7269\u5904\u7406\u7b56\u7565\u6765\u63d0\u5347\u5bfc\u822a\u80fd\u529b\u3002", "method": "1) \u591a\u6a21\u6001\u5206\u7c7b\u7f51\u7edc\u878d\u5408\u56fe\u50cf\u548c\u8f66\u8f86\u72b6\u6001\u8bc4\u4f30\u573a\u666f\u590d\u6742\u5ea6\uff1b2) 4WIS\u6df7\u5408A*\u7b97\u6cd5\u7ed3\u5408\u591a\u79cd\u8f6c\u5411\u6a21\u5f0f\uff08\u963f\u514b\u66fc\u3001\u5bf9\u89d2\u3001\u96f6\u8f6c\u5411\uff09\uff1b3) \u5206\u5c42\u969c\u788d\u7269\u5904\u7406\u7b56\u7565\uff08\u4e0d\u53ef\u7a7f\u8d8a\u3001\u53ef\u7a7f\u8d8a\u3001\u53ef\u538b\u8fc7\uff09\uff1b4) \u6982\u7387\u98ce\u9669\u573a\u6a21\u578b\u5904\u7406\u52a8\u6001\u969c\u788d\u4e0d\u786e\u5b9a\u6027\uff1b5) \u6700\u4f18\u63a7\u5236\u95ee\u9898\u751f\u6210\u5e73\u6ed1\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u751f\u6210\u5b89\u5168\u3001\u9ad8\u6548\u3001\u5e73\u6ed1\u7684\u8f68\u8ff9\uff0c\u7279\u522b\u662f\u5728\u53d7\u9650\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u5f84\u89c4\u5212\u7684\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\"\u4ec5\u907f\u8ba9\"\u7b56\u7565\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u969c\u788d\u7269\u5c5e\u6027\u5904\u7406\u548c\u52a8\u6001\u969c\u788d\u98ce\u9669\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e864WIS\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18399", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18399", "abs": "https://arxiv.org/abs/2512.18399", "authors": ["Mark Kashirskiy", "Artiom Lipinski", "Ilya Makarov"], "title": "AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3", "comment": "8 pages, 8 figures, 5 tables", "summary": "Tokenization is a critical preprocessing step for large language models (LLMs), directly impacting training efficiency and downstream performance. General-purpose tokenizers trained predominantly on English and Latin-script languages exhibit suboptimal performance on morphologically rich languages such as Arabic, resulting in inflated token sequences and reduced compression efficiency. In this work, we present AraToken, an Arabic-optimized tokenizer built on SentencePiece Unigram algorithm with a comprehensive normalization pipeline addressing Arabic-specific orthographic variations including Alif variants, diacritics, and Arabic-Indic numerals. We systematically compare BPE, WordPiece, and SentencePiece algorithms across multiple configurations, demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines. Furthermore, we introduce the Language Extension Pipeline (LEP), a method for integrating the optimized tokenizer into Qwen3-0.6B through vocabulary extension with mean subtoken initialization and selective transformer layer unfreezing. Our experiments show that LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples. We release our tokenizer, training scripts, and model checkpoints to facilitate Arabic NLP research.", "AI": {"tldr": "AraToken\uff1a\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u4f18\u5316\u7684\u5206\u8bcd\u5668\uff0c\u57fa\u4e8eSentencePiece Unigram\u7b97\u6cd5\uff0c\u901a\u8fc7\u89c4\u8303\u5316\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u7279\u5b9a\u53d8\u4f53\uff0c\u663e\u8457\u964d\u4f4e\u5206\u8bcd\u6570\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u8bed\u8a00\u6269\u5c55\u7ba1\u9053\uff08LEP\uff09\u5c06\u4f18\u5316\u5206\u8bcd\u5668\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\u3002", "motivation": "\u901a\u7528\u5206\u8bcd\u5668\u5728\u82f1\u8bed\u548c\u62c9\u4e01\u8bed\u7cfb\u4e0a\u8bad\u7ec3\uff0c\u5bf9\u963f\u62c9\u4f2f\u8bed\u7b49\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u5206\u8bcd\u5e8f\u5217\u8fc7\u957f\u3001\u538b\u7f29\u6548\u7387\u964d\u4f4e\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u4f18\u5316\u7684\u5206\u8bcd\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u57fa\u4e8eSentencePiece Unigram\u7b97\u6cd5\u6784\u5efaAraToken\u5206\u8bcd\u5668\uff1b2. \u8bbe\u8ba1\u5168\u9762\u7684\u89c4\u8303\u5316\u6d41\u7a0b\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u7279\u5b9a\u53d8\u4f53\uff08Alif\u53d8\u4f53\u3001\u53d8\u97f3\u7b26\u53f7\u3001\u963f\u62c9\u4f2f-\u5370\u5ea6\u6570\u5b57\uff09\uff1b3. \u7cfb\u7edf\u6bd4\u8f83BPE\u3001WordPiece\u548cSentencePiece\u7b97\u6cd5\uff1b4. \u63d0\u51fa\u8bed\u8a00\u6269\u5c55\u7ba1\u9053\uff08LEP\uff09\uff0c\u901a\u8fc7\u8bcd\u6c47\u6269\u5c55\u548c\u9009\u62e9\u6027\u5c42\u89e3\u51bb\u5c06\u4f18\u5316\u5206\u8bcd\u5668\u96c6\u6210\u5230Qwen3-0.6B\u6a21\u578b\u4e2d\u3002", "result": "1. \u89c4\u8303\u5316\u540e\u7684SentencePiece\u76f8\u6bd4\u672a\u89c4\u8303\u5316\u57fa\u7ebf\u964d\u4f4e18%\u7684\u5206\u8bcd\u6570\u91cf\uff081.199 vs 1.35 tokens/word\uff09\uff1b2. LEP\u5728100K\u963f\u62c9\u4f2f\u8bed\u6837\u672c\u4e0a\uff0c\u4ec5\u7528800\u8bad\u7ec3\u6b65\u5c31\u5c06\u8bc4\u4f30\u635f\u5931\u4ece8.28\u964d\u81f32.43\uff1b3. \u53d1\u5e03\u4e86\u5206\u8bcd\u5668\u3001\u8bad\u7ec3\u811a\u672c\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u3002", "conclusion": "AraToken\u663e\u8457\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u8bed\u7684\u5206\u8bcd\u6548\u7387\uff0cLEP\u65b9\u6cd5\u6709\u6548\u5c06\u4f18\u5316\u5206\u8bcd\u5668\u96c6\u6210\u5230\u73b0\u6709LLMs\u4e2d\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u6846\u67b6\u3002"}}
{"id": "2512.18850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18850", "abs": "https://arxiv.org/abs/2512.18850", "authors": ["Feeza Khan Khanzada", "Jaerock Kwon"], "title": "InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement", "comment": null, "summary": "Model-based reinforcement learning (MBRL) can reduce interaction cost for autonomous driving by learning a predictive world model, but it typically still depends on task-specific rewards that are difficult to design and often brittle under distribution shift. This paper presents InDRiVE, a DreamerV3-style MBRL agent that performs reward-free pretraining in CARLA using only intrinsic motivation derived from latent ensemble disagreement. Disagreement acts as a proxy for epistemic uncertainty and drives the agent toward under-explored driving situations, while an imagination-based actor-critic learns a planner-free exploration policy directly from the learned world model. After intrinsic pretraining, we evaluate zero-shot transfer by freezing all parameters and deploying the pretrained exploration policy in unseen towns and routes. We then study few-shot adaptation by training a task policy with limited extrinsic feedback for downstream objectives (lane following and collision avoidance). Experiments in CARLA across towns, routes, and traffic densities show that disagreement-based pretraining yields stronger zero-shot robustness and robust few-shot collision avoidance under town shift and matched interaction budgets, supporting the use of intrinsic disagreement as a practical reward-free pretraining signal for reusable driving world models.", "AI": {"tldr": "InDRiVE\u4f7f\u7528\u57fa\u4e8e\u6f5c\u5728\u96c6\u6210\u5206\u6b67\u7684\u5185\u5728\u52a8\u673a\u8fdb\u884c\u65e0\u5956\u52b1\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7DreamerV3\u98ce\u683c\u7684\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u63a2\u7d22\u7b56\u7565\uff0c\u5728CARLA\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u548c\u5c11\u6837\u672c\u9002\u5e94\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u4f9d\u8d56\u96be\u4ee5\u8bbe\u8ba1\u4e14\u5bf9\u5206\u5e03\u504f\u79fb\u654f\u611f\u7684\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5185\u5728\u52a8\u673a\u8fdb\u884c\u65e0\u5956\u52b1\u9884\u8bad\u7ec3\uff0c\u6784\u5efa\u53ef\u91cd\u7528\u7684\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\uff0c\u51cf\u5c11\u5bf9\u5916\u90e8\u5956\u52b1\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faInDRiVE\uff0c\u91c7\u7528DreamerV3\u98ce\u683c\u7684MBRL\u4ee3\u7406\uff0c\u4f7f\u7528\u6f5c\u5728\u96c6\u6210\u5206\u6b67\u4f5c\u4e3a\u5185\u5728\u52a8\u673a\u8fdb\u884c\u65e0\u5956\u52b1\u9884\u8bad\u7ec3\u3002\u5206\u6b67\u4f5c\u4e3a\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u4ee3\u7406\uff0c\u9a71\u52a8\u667a\u80fd\u4f53\u63a2\u7d22\u672a\u5145\u5206\u63a2\u7d22\u7684\u9a7e\u9a76\u60c5\u5883\u3002\u901a\u8fc7\u57fa\u4e8e\u60f3\u8c61\u7684actor-critic\u76f4\u63a5\u4ece\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u5b66\u4e60\u65e0\u89c4\u5212\u5668\u7684\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5728CARLA\u7684\u4e0d\u540c\u57ce\u9547\u3001\u8def\u7ebf\u548c\u4ea4\u901a\u5bc6\u5ea6\u4e0b\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5206\u6b67\u7684\u9884\u8bad\u7ec3\u5728\u96f6\u6837\u672c\u9c81\u68d2\u6027\u548c\u5c11\u6837\u672c\u78b0\u649e\u907f\u514d\u65b9\u9762\u8868\u73b0\u66f4\u5f3a\u3002\u5728\u57ce\u9547\u8f6c\u79fb\u548c\u76f8\u540c\u4ea4\u4e92\u9884\u7b97\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u5185\u5728\u5206\u6b67\u53ef\u4f5c\u4e3a\u5b9e\u7528\u7684\u65e0\u5956\u52b1\u9884\u8bad\u7ec3\u4fe1\u53f7\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u91cd\u7528\u7684\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\u548c\u5c11\u6837\u672c\u9002\u5e94\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18440", "abs": "https://arxiv.org/abs/2512.18440", "authors": ["Victor De Marez", "Jens Van Nooten", "Luna De Bruyne", "Walter Daelemans"], "title": "An Agentic AI Framework for Training General Practitioner Student Skills", "comment": null, "summary": "Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u5168\u79d1\u533b\u5b66\u751f\u6280\u80fd\uff0c\u901a\u8fc7\u53ef\u914d\u7f6e\u7684\u5faa\u8bc1\u6848\u4f8b\u751f\u6210\u3001\u53ef\u63a7\u7684\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u548c\u57fa\u4e8e\u6807\u51c6\u7684\u8bc4\u4f30\u53cd\u9988\uff0c\u63d0\u5347\u865a\u62df\u6a21\u62df\u60a3\u8005\u7684\u533b\u5b66\u51c6\u786e\u6027\u548c\u6559\u80b2\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u865a\u62df\u6a21\u62df\u60a3\u8005\u5728\u533b\u5b66\u6559\u80b2\u4e2d\u5b58\u5728\u533b\u5b66\u51c6\u786e\u6027\u4e0d\u8db3\u3001\u89d2\u8272\u626e\u6f14\u4e0d\u4e00\u81f4\u3001\u6848\u4f8b\u751f\u6210\u56f0\u96be\u4ee5\u53ca\u7f3a\u4e4f\u7ed3\u6784\u5316\u53cd\u9988\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u53ef\u914d\u7f6e\u7684\u5faa\u8bc1\u6848\u4f8b\u751f\u6210\uff0c(2) \u53ef\u63a7\u7684\u89d2\u8272\u9a71\u52a8\u60a3\u8005\u5bf9\u8bdd\uff08\u53ef\u9009\u68c0\u7d22\u589e\u5f3a\uff09\uff0c(3) \u57fa\u4e8e\u6807\u51c6\u7684\u8bc4\u4f30\u53cd\u9988\u7cfb\u7edf\uff08\u6db5\u76d6\u6c9f\u901a\u548c\u4e34\u5e8a\u63a8\u7406\uff09\u3002\u5728\u4ea4\u4e92\u5f0f\u53e3\u8bed\u54a8\u8be2\u573a\u666f\u4e2d\u5b9e\u73b0\u8be5\u6846\u67b6\u3002", "result": "\u5bf914\u540d\u533b\u5b66\u751f\u8fdb\u884c\u8bc4\u4f30\uff0c\u53c2\u4e0e\u8005\u62a5\u544a\uff1a\u771f\u5b9e\u4e14\u5fe0\u4e8e\u6848\u4f8b\u7684\u5bf9\u8bdd\u3001\u9002\u5f53\u7684\u96be\u5ea6\u6821\u51c6\u3001\u7a33\u5b9a\u7684\u4eba\u683c\u4fe1\u53f7\u3001\u4e30\u5bcc\u6709\u7528\u7684\u53cd\u9988\u793a\u4f8b\uff0c\u4ee5\u53ca\u4f18\u79c0\u7684\u6574\u4f53\u53ef\u7528\u6027\u3002", "conclusion": "\u5c06\u573a\u666f\u63a7\u5236\u3001\u4ea4\u4e92\u63a7\u5236\u548c\u57fa\u4e8e\u6807\u51c6\u7684\u8bc4\u4f30\u5206\u79bb\u7684\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u662f\u6784\u5efa\u53ef\u9760\u4e14\u5177\u6709\u6559\u5b66\u4ef7\u503c\u7684\u865a\u62df\u6a21\u62df\u60a3\u8005\u8bad\u7ec3\u5de5\u5177\u7684\u6709\u6548\u6a21\u5f0f\u3002"}}
{"id": "2512.18869", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.18869", "abs": "https://arxiv.org/abs/2512.18869", "authors": ["Georg Nawratil"], "title": "Construction and deformation of P-hedra using control polylines", "comment": "8 pages, 4 figures", "summary": "In the 19th International Symposium on Advances in Robot Kinematics the author introduced a novel class of continuous flexible discrete surfaces and mentioned that these so-called P-hedra (or P-nets) allow direct access to their spatial shapes by three control polylines. In this follow-up paper we study this intuitive method, which makes these flexible planar quad surfaces suitable for transformable design tasks by means of interactive tools. The construction of P-hedra from the control polylines can also be used for an efficient algorithmic computation of their isometric deformations. In addition we discuss flexion limits, bifurcation configurations, developable/flat-foldable pattern and tubular P-hedra.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76P-hedra\uff08P-nets\uff09\u7684\u4ea4\u4e92\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u6761\u63a7\u5236\u6298\u7ebf\u76f4\u63a5\u63a7\u5236\u7a7a\u95f4\u5f62\u72b6\uff0c\u9002\u7528\u4e8e\u53ef\u53d8\u5f62\u8bbe\u8ba1\u4efb\u52a1", "motivation": "\u5728\u4e4b\u524d\u7684\u7814\u7a76\u4e2d\u5f15\u5165\u4e86P-hedra\u8fd9\u7c7b\u8fde\u7eed\u67d4\u6027\u79bb\u6563\u66f2\u9762\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u76f4\u89c2\u7684\u4ea4\u4e92\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u53ef\u53d8\u5f62\u8bbe\u8ba1\u4efb\u52a1", "method": "\u4f7f\u7528\u4e09\u6761\u63a7\u5236\u6298\u7ebf\u76f4\u63a5\u63a7\u5236P-hedra\u7684\u7a7a\u95f4\u5f62\u72b6\uff0c\u5f00\u53d1\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u7814\u7a76\u7b49\u8ddd\u53d8\u5f62\u7684\u9ad8\u6548\u7b97\u6cd5\u8ba1\u7b97\uff0c\u5206\u6790\u5f2f\u66f2\u6781\u9650\u3001\u5206\u53c9\u914d\u7f6e\u3001\u53ef\u5c55/\u53ef\u5e73\u6298\u6a21\u5f0f\u53ca\u7ba1\u72b6P-hedra", "result": "\u5efa\u7acb\u4e86P-hedra\u7684\u4ea4\u4e92\u8bbe\u8ba1\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u901a\u8fc7\u63a7\u5236\u6298\u7ebf\u76f4\u89c2\u63a7\u5236\u66f2\u9762\u5f62\u72b6\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u9ad8\u6548\u8ba1\u7b97\u7b49\u8ddd\u53d8\u5f62\u7684\u7b97\u6cd5\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u5404\u79cd\u7279\u6b8a\u914d\u7f6e", "conclusion": "P-hedra\u901a\u8fc7\u4e09\u6761\u63a7\u5236\u6298\u7ebf\u7684\u4ea4\u4e92\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u53ef\u53d8\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u5176\u6784\u9020\u65b9\u6cd5\u4e5f\u53ef\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97\u7b49\u8ddd\u53d8\u5f62\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f"}}
{"id": "2512.18462", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18462", "abs": "https://arxiv.org/abs/2512.18462", "authors": ["Christopher Rom\u00e1n Jaimes"], "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling", "comment": null, "summary": "Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u53ef\u6269\u5c55\u7684pipeline\u89e3\u51b3NLI\u6a21\u578b\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7LF-LMI\u68c0\u6d4b\u8bed\u4e49\u4f2a\u5f71\u3001LLM\u5408\u6210\u9ad8\u8d28\u91cf\u5bf9\u6bd4\u96c6\u3001\u52a8\u6001\u5e73\u8861\u91c7\u6837\u9632\u6b62\u9057\u5fd8\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u4e00\u81f4\u6027\u3002", "motivation": "NLI\u6a21\u578b\u7ecf\u5e38\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u8bed\u4e49\u63a8\u7406\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u901a\u5e38\u6807\u6ce8\u6210\u672c\u9ad8\u6216\u5728\u5fae\u8c03\u65f6\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u63d0\u51faLF-LMI\u51c6\u786e\u68c0\u6d4b\u8bed\u4e49\u4f2a\u5f71\uff1b2) \u901a\u8fc7LLM\u5408\u6210pipeline\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u5bf9\u6bd4\u96c6\uff0c\u5e76\u8fdb\u884c\u591a\u6cd5\u5b98\u9a8c\u8bc1\uff1b3) \u5f15\u5165\u52a8\u6001\u5e73\u8861\u91c7\u6837\u8bad\u7ec3\u7b56\u7565\uff0c\u65cb\u8f6c\u539f\u59cb\u6570\u636e\u5206\u5e03\u9632\u6b62\u9057\u5fd8\u3002", "result": "\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e00\u81f4\u6027\u4ece63.5%\u63d0\u5347\u81f381.0%\uff0c\u540c\u65f6\u4fdd\u630188.4%\u7684\u57df\u5185\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u6734\u7d20\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316pipeline\u6709\u6548\u89e3\u51b3\u4e86NLI\u6a21\u578b\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u6d4b\u4f2a\u5f71\u3001\u751f\u6210\u5bf9\u6bd4\u96c6\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u5728\u63d0\u5347\u6a21\u578b\u4e00\u81f4\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002"}}
{"id": "2512.18922", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18922", "abs": "https://arxiv.org/abs/2512.18922", "authors": ["Tianyuan Liu", "Richard Dazeley", "Benjamin Champion", "Akan Cosgun"], "title": "Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction", "comment": "Accepted in ACRA 2025", "summary": "In this paper, we study whether inexpensive, physics-free supervision can reliably prioritize grasp-place candidates for budget-aware pick-and-place. From an object's initial pose, target pose, and a candidate grasp, we generate two path-aware geometric labels: path-wise inverse kinematics (IK) feasibility across a fixed approach-grasp-lift waypoint template, and a transit collision flag from mesh sweeps along the same template. A compact dual-output MLP learns these signals from pose encodings, and at test time its scores rank precomputed candidates for a rank-then-plan policy under the same IK gate and planner as the baseline. Although learned from cheap labels only, the scores transfer to physics-enabled executed trajectories: at a fixed planning budget the policy finds successful paths sooner with fewer planner calls while keeping final success on par or better. This work targets a single rigid cuboid with side-face grasps and a fixed waypoint template, and we outline extensions to varied objects and richer waypoint schemes.", "AI": {"tldr": "\u4f7f\u7528\u5ec9\u4ef7\u7684\u65e0\u7269\u7406\u76d1\u7763\u5b66\u4e60\u6765\u4f18\u5148\u9009\u62e9\u6293\u53d6-\u653e\u7f6e\u5019\u9009\u65b9\u6848\uff0c\u901a\u8fc7\u51e0\u4f55\u6807\u7b7e\u8bad\u7ec3MLP\u6a21\u578b\uff0c\u5728\u56fa\u5b9a\u89c4\u5212\u9884\u7b97\u4e0b\u66f4\u5feb\u627e\u5230\u6210\u529f\u8def\u5f84", "motivation": "\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5ec9\u4ef7\u3001\u65e0\u7269\u7406\u4eff\u771f\u7684\u76d1\u7763\u6765\u53ef\u9760\u5730\u4f18\u5148\u9009\u62e9\u6293\u53d6-\u653e\u7f6e\u5019\u9009\u65b9\u6848\uff0c\u4ee5\u964d\u4f4e\u9884\u7b97\u611f\u77e5\u7684\u62fe\u53d6-\u653e\u7f6e\u4efb\u52a1\u6210\u672c", "method": "\u4ece\u7269\u4f53\u521d\u59cb\u4f4d\u59ff\u3001\u76ee\u6807\u4f4d\u59ff\u548c\u5019\u9009\u6293\u53d6\u751f\u6210\u4e24\u79cd\u8def\u5f84\u611f\u77e5\u51e0\u4f55\u6807\u7b7e\uff1a\u57fa\u4e8e\u56fa\u5b9a\u8def\u5f84\u6a21\u677f\u7684\u9006\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\uff0c\u4ee5\u53ca\u6cbf\u76f8\u540c\u6a21\u677f\u7684\u7f51\u683c\u626b\u63cf\u78b0\u649e\u68c0\u6d4b\u3002\u4f7f\u7528\u7d27\u51d1\u7684\u53cc\u8f93\u51faMLP\u5b66\u4e60\u8fd9\u4e9b\u4fe1\u53f7\uff0c\u5728\u6d4b\u8bd5\u65f6\u7528\u5176\u5206\u6570\u5bf9\u9884\u8ba1\u7b97\u5019\u9009\u65b9\u6848\u8fdb\u884c\u6392\u5e8f\uff0c\u91c7\u7528\"\u5148\u6392\u5e8f\u518d\u89c4\u5212\"\u7b56\u7565", "result": "\u5c3d\u7ba1\u4ec5\u4ece\u5ec9\u4ef7\u6807\u7b7e\u5b66\u4e60\uff0c\u4f46\u5206\u6570\u80fd\u591f\u8fc1\u79fb\u5230\u7269\u7406\u542f\u7528\u7684\u6267\u884c\u8f68\u8ff9\uff1a\u5728\u56fa\u5b9a\u89c4\u5212\u9884\u7b97\u4e0b\uff0c\u7b56\u7565\u80fd\u66f4\u5feb\u627e\u5230\u6210\u529f\u8def\u5f84\uff0c\u51cf\u5c11\u89c4\u5212\u5668\u8c03\u7528\u6b21\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6700\u7ec8\u6210\u529f\u7387", "conclusion": "\u5ec9\u4ef7\u7684\u65e0\u7269\u7406\u76d1\u7763\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u4f18\u5148\u9009\u62e9\u6293\u53d6-\u653e\u7f6e\u5019\u9009\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u9488\u5bf9\u5355\u4e2a\u521a\u6027\u957f\u65b9\u4f53\u8bbe\u8ba1\uff0c\u4f46\u53ef\u6269\u5c55\u5230\u591a\u6837\u7269\u4f53\u548c\u66f4\u4e30\u5bcc\u7684\u8def\u5f84\u65b9\u6848"}}
{"id": "2512.18475", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18475", "abs": "https://arxiv.org/abs/2512.18475", "authors": ["Mykola Kuz", "Ihor Lazarovych", "Mykola Kozlenko", "Mykola Pikuliak", "Andrii Kvasniuk"], "title": "Research on a hybrid LSTM-CNN-Attention model for text-based web content classification", "comment": "10 pages, 5 figures, 2 tables. Accepted by Radio Electronics Computer Science Control 2025", "summary": "This study presents a hybrid deep learning architecture that integrates LSTM, CNN, and an Attention mechanism to enhance the classification of web content based on text. Pretrained GloVe embeddings are used to represent words as dense vectors that preserve semantic similarity. The CNN layer extracts local n-gram patterns and lexical features, while the LSTM layer models long-range dependencies and sequential structure. The integrated Attention mechanism enables the model to focus selectively on the most informative parts of the input sequence. A 5-fold cross-validation setup was used to assess the robustness and generalizability of the proposed solution. Experimental results show that the hybrid LSTM-CNN-Attention model achieved outstanding performance, with an accuracy of 0.98, precision of 0.94, recall of 0.92, and F1-score of 0.93. These results surpass the performance of baseline models based solely on CNNs, LSTMs, or transformer-based classifiers such as BERT. The combination of neural network components enabled the model to effectively capture both fine-grained text structures and broader semantic context. Furthermore, the use of GloVe embeddings provided an efficient and effective representation of textual data, making the model suitable for integration into systems with real-time or near-real-time requirements. The proposed hybrid architecture demonstrates high effectiveness in text-based web content classification, particularly in tasks requiring both syntactic feature extraction and semantic interpretation. By combining presented mechanisms, the model addresses the limitations of individual architectures and achieves improved generalization. These findings support the broader use of hybrid deep learning approaches in NLP applications, especially where complex, unstructured textual data must be processed and classified with high reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408LSTM\u3001CNN\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u7f51\u9875\u6587\u672c\u5206\u7c7b\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b", "motivation": "\u89e3\u51b3\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u540c\u65f6\u6355\u6349\u5c40\u90e8\u8bed\u6cd5\u7279\u5f81\u548c\u957f\u8ddd\u79bb\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3GloVe\u8bcd\u5d4c\u5165\uff0c\u7ed3\u5408CNN\u63d0\u53d6\u5c40\u90e8n-gram\u7279\u5f81\uff0cLSTM\u5efa\u6a21\u5e8f\u5217\u4f9d\u8d56\uff0c\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u5173\u952e\u4fe1\u606f\uff0c\u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30", "result": "\u6a21\u578b\u5728\u51c6\u786e\u7387(0.98)\u3001\u7cbe\u786e\u7387(0.94)\u3001\u53ec\u56de\u7387(0.92)\u548cF1\u5206\u6570(0.93)\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8aCNN\u3001LSTM\u548cBERT\u7b49\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u6df7\u5408\u67b6\u6784\u80fd\u6709\u6548\u7ed3\u5408\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u7ec4\u4ef6\u7684\u4f18\u52bf\uff0c\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u573a\u666f"}}
{"id": "2512.18938", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18938", "abs": "https://arxiv.org/abs/2512.18938", "authors": ["Yadong Liu", "Jianwei Liu", "He Liang", "Dimitrios Kanoulas"], "title": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation", "comment": null, "summary": "Quadruped mobile manipulators offer strong potential for agile loco-manipulation but remain difficult to control and transfer reliably from simulation to reality. Reinforcement learning (RL) shows promise for whole-body control, yet most frameworks are proprietary and hard to reproduce on real hardware. We present an open pipeline for training, benchmarking, and deploying RL-based controllers on the Unitree B1 quadruped with a Z1 arm. The framework unifies sim-to-sim and sim-to-real transfer through ROS, re-implementing a policy trained in Isaac Gym, extending it to MuJoCo via a hardware abstraction layer, and deploying the same controller on physical hardware. Sim-to-sim experiments expose discrepancies between Isaac Gym and MuJoCo contact models that influence policy behavior, while real-world teleoperated object-picking trials show that coordinated whole-body control extends reach and improves manipulation over floating-base baselines. The pipeline provides a transparent, reproducible foundation for developing and analyzing RL-based loco-manipulation controllers and will be released open source to support future research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u56db\u8db3\u79fb\u52a8\u673a\u68b0\u81c2RL\u63a7\u5236\u7684\u5f00\u6e90\u8bad\u7ec3\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u90e8\u7f72\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u4ece\u4eff\u771f\u5230\u4eff\u771f\u518d\u5230\u771f\u5b9e\u786c\u4ef6\u7684\u7edf\u4e00\u8fc1\u79fb\u3002", "motivation": "\u56db\u8db3\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u654f\u6377\u7684\u79fb\u52a8\u64cd\u4f5c\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u63a7\u5236\u56f0\u96be\u4e14\u96be\u4ee5\u4ece\u4eff\u771f\u53ef\u9760\u8fc1\u79fb\u5230\u73b0\u5b9e\u3002\u73b0\u6709RL\u6846\u67b6\u591a\u4e3a\u4e13\u6709\u4e14\u96be\u4ee5\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u590d\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7ROS\u7edf\u4e00\u4eff\u771f\u5230\u4eff\u771f\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002\u5728Isaac Gym\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u786c\u4ef6\u62bd\u8c61\u5c42\u6269\u5c55\u5230MuJoCo\uff0c\u5e76\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u90e8\u7f72\u76f8\u540c\u63a7\u5236\u5668\u3002", "result": "\u4eff\u771f\u5230\u4eff\u771f\u5b9e\u9a8c\u63ed\u793a\u4e86Isaac Gym\u548cMuJoCo\u63a5\u89e6\u6a21\u578b\u7684\u5dee\u5f02\u5bf9\u7b56\u7565\u884c\u4e3a\u7684\u5f71\u54cd\uff1b\u771f\u5b9e\u4e16\u754c\u9065\u63a7\u7269\u4f53\u62fe\u53d6\u8bd5\u9a8c\u663e\u793a\u534f\u8c03\u5168\u8eab\u63a7\u5236\u76f8\u6bd4\u6d6e\u52a8\u57fa\u5ea7\u57fa\u7ebf\u6269\u5c55\u4e86\u53ef\u8fbe\u8303\u56f4\u5e76\u6539\u8fdb\u4e86\u64cd\u4f5c\u6027\u80fd\u3002", "conclusion": "\u8be5\u6d41\u6c34\u7ebf\u4e3a\u5f00\u53d1\u548c\u5206\u6790\u57fa\u4e8eRL\u7684\u79fb\u52a8\u64cd\u4f5c\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0c\u5c06\u5f00\u6e90\u53d1\u5e03\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2512.18505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18505", "abs": "https://arxiv.org/abs/2512.18505", "authors": ["Vagrant Gautam"], "title": "Teaching and Critiquing Conceptualization and Operationalization in NLP", "comment": null, "summary": "NLP researchers regularly invoke abstract concepts like \"interpretability,\" \"bias,\" \"reasoning,\" and \"stereotypes,\" without defining them. Each subfield has a shared understanding or conceptualization of what these terms mean and how we should treat them, and this shared understanding is the basis on which operational decisions are made: Datasets are built to evaluate these concepts, metrics are proposed to quantify them, and claims are made about systems. But what do they mean, what should they mean, and how should we measure them? I outline a seminar I created for students to explore these questions of conceptualization and operationalization, with an interdisciplinary reading list and an emphasis on discussion and critique.", "AI": {"tldr": "\u4f5c\u8005\u521b\u5efa\u4e86\u4e00\u95e8\u7814\u8ba8\u8bfe\uff0c\u63a2\u8ba8NLP\u4e2d\"\u53ef\u89e3\u91ca\u6027\"\u3001\"\u504f\u89c1\"\u3001\"\u63a8\u7406\"\u7b49\u62bd\u8c61\u6982\u5ff5\u7684\u5b9a\u4e49\u3001\u610f\u4e49\u548c\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5f3a\u8c03\u6982\u5ff5\u5316\u548c\u64cd\u4f5c\u5316\u7684\u91cd\u8981\u6027\u3002", "motivation": "NLP\u7814\u7a76\u8005\u7ecf\u5e38\u4f7f\u7528\"\u53ef\u89e3\u91ca\u6027\"\u3001\"\u504f\u89c1\"\u3001\"\u63a8\u7406\"\u3001\"\u523b\u677f\u5370\u8c61\"\u7b49\u62bd\u8c61\u6982\u5ff5\u800c\u4e0d\u660e\u786e\u5b9a\u4e49\uff0c\u5404\u5b50\u9886\u57df\u5bf9\u8fd9\u4e9b\u6982\u5ff5\u6709\u4e0d\u540c\u7684\u7406\u89e3\u548c\u64cd\u4f5c\u65b9\u5f0f\uff0c\u8fd9\u5f71\u54cd\u4e86\u6570\u636e\u96c6\u6784\u5efa\u3001\u6307\u6807\u8bbe\u8ba1\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u95e8\u7814\u8ba8\u8bfe\uff0c\u91c7\u7528\u8de8\u5b66\u79d1\u9605\u8bfb\u6750\u6599\uff0c\u5f3a\u8c03\u8ba8\u8bba\u548c\u6279\u5224\u6027\u5206\u6790\uff0c\u5f15\u5bfc\u5b66\u751f\u63a2\u7d22\u8fd9\u4e9b\u6982\u5ff5\u7684\u6982\u5ff5\u5316\u548c\u64cd\u4f5c\u5316\u95ee\u9898\u3002", "result": "\u8bba\u6587\u63cf\u8ff0\u4e86\u4e00\u95e8\u5df2\u521b\u5efa\u7684\u7814\u8ba8\u8bfe\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u5b66\u751f\u6df1\u5165\u7406\u89e3NLP\u4e2d\u6838\u5fc3\u6982\u5ff5\u7684\u672c\u8d28\u3001\u610f\u4e49\u548c\u6d4b\u91cf\u65b9\u6cd5\u3002", "conclusion": "\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5ba1\u89c6NLP\u4e2d\u5e38\u7528\u6982\u5ff5\u7684\u5b9a\u4e49\u548c\u64cd\u4f5c\u5316\u65b9\u5f0f\uff0c\u8de8\u5b66\u79d1\u7814\u8ba8\u8bfe\u662f\u57f9\u517b\u8fd9\u79cd\u6279\u5224\u6027\u601d\u7ef4\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2512.18987", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18987", "abs": "https://arxiv.org/abs/2512.18987", "authors": ["Ryosuke Korekata", "Quanting Xie", "Yonatan Bisk", "Komei Sugiura"], "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation", "comment": "Accepted to IEEE RA-L, with presentation at ICRA 2026", "summary": "In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.", "AI": {"tldr": "\u63d0\u51faAffordance RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efaAffordance-Aware Embodied Memory\u5b9e\u73b0\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fbe\u523085%\u4efb\u52a1\u6210\u529f\u7387", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u95ee\u9898\uff0c\u673a\u5668\u4eba\u9700\u8981\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5c06\u5404\u79cd\u7269\u4f53\u8fd0\u9001\u5230\u5bb9\u5668\u4e2d\uff0c\u8fd9\u9700\u8981\u7406\u89e3\u89c6\u89c9\u8bed\u4e49\u548c\u64cd\u4f5c\u52a8\u4f5c\u7684\u53ef\u7528\u6027", "method": "\u63d0\u51faAffordance RAG\u6846\u67b6\uff0c\u4ece\u9884\u63a2\u7d22\u56fe\u50cf\u6784\u5efaAffordance-Aware Embodied Memory\uff0c\u901a\u8fc7\u533a\u57df\u548c\u89c6\u89c9\u8bed\u4e49\u68c0\u7d22\u5019\u9009\u76ee\u6807\uff0c\u7136\u540e\u7528\u53ef\u7528\u6027\u5206\u6570\u91cd\u65b0\u6392\u5e8f", "result": "\u5728\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u4e2d\u79fb\u52a8\u64cd\u4f5c\u6307\u4ee4\u68c0\u7d22\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u57fa\u4e8e\u81ea\u7531\u5f62\u5f0f\u6307\u4ee4\u7684\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523085%", "conclusion": "Affordance RAG\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u7406\u89e3\u548c\u53ef\u7528\u6027\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u6027\u80fd"}}
{"id": "2512.18533", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18533", "abs": "https://arxiv.org/abs/2512.18533", "authors": ["S Mahmudul Hasan", "Shaily Roy", "Akib Jawad Nafis"], "title": "Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset", "comment": null, "summary": "The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard \"Performance Ceiling\", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive \"Generalization Gap\" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.", "AI": {"tldr": "\u653f\u6cbb\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u9762\u4e34\u8bed\u4e49\u74f6\u9888\uff0c\u5355\u7eaf\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u652f\u6301", "motivation": "\u653f\u6cbb\u865a\u5047\u4fe1\u606f\u5177\u6709\u8bed\u8a00\u5fae\u5999\u6027\uff0c\u73b0\u6709\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u9762\u4e34\u6311\u6218\u3002\u5c3d\u7ba1\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u65e5\u76ca\u590d\u6742\uff0c\u4f46\u7eaf\u6587\u672c\u8bed\u8a00\u5efa\u6a21\u7684\u5b9e\u8bc1\u9650\u5236\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728LIAR\u57fa\u51c6\u4e0a\u5bf99\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u7cfb\u7edf\u8bca\u65ad\u8bc4\u4f30\uff0c\u9694\u79bb\u8bcd\u6c47\u7279\u5f81\uff08\u8bcd\u888b\u3001TF-IDF\uff09\u548c\u8bed\u4e49\u5d4c\u5165\uff08GloVe\uff09\uff0c\u5206\u6790\u6027\u80fd\u4e0a\u9650\u548c\u6cdb\u5316\u5dee\u8ddd\u3002", "result": "\u53d1\u73b0\u6027\u80fd\u4e0a\u9650\u4e3a\u52a0\u6743F1\u5206\u65700.32\uff1b\u7ebf\u6027SVM\uff08\u51c6\u786e\u73870.624\uff09\u4e0e\u9884\u8bad\u7ec3Transformer\uff08RoBERTa\u51c6\u786e\u73870.620\uff09\u6027\u80fd\u76f8\u5f53\uff1b\u57fa\u4e8e\u6811\u7684\u96c6\u6210\u65b9\u6cd5\u5b58\u5728\u5de8\u5927\u6cdb\u5316\u5dee\u8ddd\uff08\u8bad\u7ec3\u51c6\u786e\u7387>99%\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u7ea625%\uff09\uff1bSMOTE\u6570\u636e\u589e\u5f3a\u65e0\u5b9e\u8d28\u6027\u6539\u5584\u3002", "conclusion": "\u5bf9\u4e8e\u653f\u6cbb\u4e8b\u5b9e\u6838\u67e5\uff0c\u5355\u7eaf\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u800c\u4e0d\u878d\u5165\u5916\u90e8\u77e5\u8bc6\u4f1a\u4ea7\u751f\u6536\u76ca\u9012\u51cf\u6548\u5e94\u3002\u9650\u5236\u4e3b\u8981\u6765\u81ea\u8bed\u4e49\u7279\u5f81\u6a21\u7cca\u6027\u800c\u975e\u5206\u5e03\u95ee\u9898\uff0c\u9700\u8981\u8d85\u8d8a\u7eaf\u6587\u672c\u5efa\u6a21\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.18988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18988", "abs": "https://arxiv.org/abs/2512.18988", "authors": ["Yanding Yang", "Weitao Zhou", "Jinhai Wang", "Xiaomin Guo", "Junze Wen", "Xiaolong Liu", "Lang Ding", "Zheng Fu", "Jinyu Miao", "Kun Jiang", "Diange Yang"], "title": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners", "comment": null, "summary": "Autonomous buses run on fixed routes but must operate in open, dynamic urban environments. Disengagement events on these routes are often geographically concentrated and typically arise from planner failures in highly interactive regions. Such policy-level failures are difficult to correct using conventional imitation learning, which easily overfits to sparse disengagement data. To address this issue, this paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework that enables autonomous buses to improve planning policies through real-world operation. Each disengagement triggers cloud-based data augmentation that generates positive and negative samples by perturbing surrounding agents while preserving route context. Contrastive learning refines policy representations to better distinguish safe and unsafe behaviors, and continual updates are applied in a cloud-edge loop without human supervision. Experiments on urban bus routes demonstrate that DTCCL improves overall planning performance by 48.6 percent compared with direct retraining, validating its effectiveness for scalable, closed-loop policy improvement in autonomous public transport.", "AI": {"tldr": "DTCCL\u6846\u67b6\u901a\u8fc7\u89e6\u53d1\u5f0f\u5bf9\u6bd4\u6301\u7eed\u5b66\u4e60\uff0c\u5229\u7528\u81ea\u52a8\u9a7e\u9a76\u5df4\u58eb\u8131\u94a9\u4e8b\u4ef6\u6570\u636e\u6539\u8fdb\u89c4\u5212\u7b56\u7565\uff0c\u76f8\u6bd4\u76f4\u63a5\u91cd\u8bad\u7ec3\u63d0\u534748.6%\u6027\u80fd", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5df4\u58eb\u5728\u56fa\u5b9a\u8def\u7ebf\u4e0a\u8fd0\u884c\uff0c\u4f46\u5728\u5f00\u653e\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u5e38\u51fa\u73b0\u8131\u94a9\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u901a\u5e38\u96c6\u4e2d\u5728\u7279\u5b9a\u5730\u7406\u533a\u57df\uff0c\u6e90\u4e8e\u9ad8\u5ea6\u4ea4\u4e92\u533a\u57df\u7684\u89c4\u5212\u5668\u5931\u8d25\u3002\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u96be\u4ee5\u7ea0\u6b63\u6b64\u7c7b\u7b56\u7565\u7ea7\u5931\u8d25\uff0c\u5bb9\u6613\u5bf9\u7a00\u758f\u7684\u8131\u94a9\u6570\u636e\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51fa\u8131\u94a9\u89e6\u53d1\u5bf9\u6bd4\u6301\u7eed\u5b66\u4e60(DTCCL)\u6846\u67b6\uff1a1) \u6bcf\u6b21\u8131\u94a9\u89e6\u53d1\u4e91\u7aef\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u6270\u52a8\u5468\u56f4\u667a\u80fd\u4f53\u751f\u6210\u6b63\u8d1f\u6837\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u8def\u7ebf\u4e0a\u4e0b\u6587\uff1b2) \u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u8868\u793a\uff0c\u66f4\u597d\u533a\u5206\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u884c\u4e3a\uff1b3) \u5728\u4e91-\u8fb9\u5faa\u73af\u4e2d\u6301\u7eed\u66f4\u65b0\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u3002", "result": "\u5728\u57ce\u5e02\u5df4\u58eb\u8def\u7ebf\u5b9e\u9a8c\u4e2d\uff0cDTCCL\u76f8\u6bd4\u76f4\u63a5\u91cd\u8bad\u7ec3\u5c06\u6574\u4f53\u89c4\u5212\u6027\u80fd\u63d0\u534748.6%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u516c\u5171\u4ea4\u901a\u4e2d\u53ef\u6269\u5c55\u3001\u95ed\u73af\u7b56\u7565\u6539\u8fdb\u7684\u6709\u6548\u6027\u3002", "conclusion": "DTCCL\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u81ea\u52a8\u9a7e\u9a76\u5df4\u58eb\u5728\u771f\u5b9e\u8fd0\u8425\u4e2d\u7684\u8131\u94a9\u4e8b\u4ef6\u6570\u636e\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6301\u7eed\u5b66\u4e60\u6539\u8fdb\u89c4\u5212\u7b56\u7565\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u516c\u5171\u4ea4\u901a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u95ed\u73af\u7b56\u7565\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2512.18546", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18546", "abs": "https://arxiv.org/abs/2512.18546", "authors": ["Alexander Doudkin"], "title": "LLMs on Drugs: Language Models Are Few-Shot Consumers", "comment": "8 pages, 2 figures, 2 tables", "summary": "Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level \"drug\" interventions have never been benchmarked rigorously. We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge. Four single-sentence prompts -- LSD, cocaine, alcohol, and cannabis -- are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests. Control accuracy is 0.45; alcohol collapses to 0.10 (p = 3.2e-8), cocaine to 0.21 (p = 4.9e-4), LSD to 0.19 (p = 1.3e-4), and cannabis to 0.30 (p = 0.041), largely because persona prompts disrupt the mandated \"Answer: <LETTER>\" template. Persona text therefore behaves like a \"few-shot consumable\" that can destroy reliability without touching model weights. All experimental code, raw results, and analysis scripts are available at https://github.com/lexdoudkin/llms-on-drugs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u63a8\u7406\u65f6\u5bf9GPT-5-mini\u65bd\u52a0\u4e0d\u540c\"\u836f\u7269\"\u89d2\u8272\u63d0\u793a\u4f1a\u663e\u8457\u964d\u4f4e\u5176\u5728ARC-Challenge\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5176\u4e2d\u9152\u7cbe\u63d0\u793a\u6548\u679c\u6700\u5dee\uff0c\u51c6\u786e\u7387\u4ece0.45\u964d\u81f30.10\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5bf9\u65bd\u52a0\u7684\u89d2\u8272\u5f88\u654f\u611f\uff0c\u4f46\u4ece\u672a\u5bf9\u63d0\u793a\u7ea7\u522b\u7684\"\u836f\u7269\"\u5e72\u9884\u8fdb\u884c\u8fc7\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u7cbe\u795e\u6d3b\u6027\u6846\u67b6\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528GPT-5-mini\u5728ARC-Challenge\u4e0a\u8fdb\u884c\u5bf9\u7167\u7814\u7a76\uff0c\u6bd4\u8f83\u56db\u79cd\u5355\u53e5\u836f\u7269\u63d0\u793a\uff08LSD\u3001\u53ef\u5361\u56e0\u3001\u9152\u7cbe\u3001\u5927\u9ebb\uff09\u4e0e\u6e05\u9192\u5bf9\u7167\u6761\u4ef6\u3002\u6bcf\u4e2a\u6761\u4ef6\u6d4b\u8bd5100\u4e2a\u9a8c\u8bc1\u9879\u76ee\uff0c\u91c7\u7528\u786e\u5b9a\u6027\u89e3\u7801\u3001\u5b8c\u6574\u65e5\u5fd7\u8bb0\u5f55\u3001\u5a01\u5c14\u900a\u7f6e\u4fe1\u533a\u95f4\u548c\u8d39\u5e0c\u5c14\u7cbe\u786e\u68c0\u9a8c\u3002", "result": "\u5bf9\u7167\u51c6\u786e\u7387\u4e3a0.45\uff1b\u9152\u7cbe\u964d\u81f30.10\uff08p=3.2e-8\uff09\uff0c\u53ef\u5361\u56e00.21\uff08p=4.9e-4\uff09\uff0cLSD 0.19\uff08p=1.3e-4\uff09\uff0c\u5927\u9ebb0.30\uff08p=0.041\uff09\u3002\u4e3b\u8981\u539f\u56e0\u662f\u89d2\u8272\u63d0\u793a\u7834\u574f\u4e86\"Answer: <LETTER>\"\u6a21\u677f\u683c\u5f0f\u3002", "conclusion": "\u89d2\u8272\u6587\u672c\u5c31\u50cf\"\u5c11\u91cf\u53ef\u6d88\u8017\u63d0\u793a\"\uff0c\u53ef\u4ee5\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u7834\u574f\u53ef\u9760\u6027\u3002\u8fd9\u63ed\u793a\u4e86\u63d0\u793a\u5de5\u7a0b\u5bf9LLM\u6027\u80fd\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u5728\u90e8\u7f72\u4e2d\u4ed4\u7ec6\u8bbe\u8ba1\u63d0\u793a\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.19024", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19024", "abs": "https://arxiv.org/abs/2512.19024", "authors": ["Xu Liu", "Yu Liu", "Hanshuo Qiu", "Yang Qirong", "Zhouhui Lian"], "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments", "comment": null, "summary": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86IndoorUAV\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u5ba4\u5185\u65e0\u4eba\u673a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u5305\u542b\u957f\u89c6\u91ceVLN\u548c\u77ed\u89c6\u91ceVLA\u4e24\u4e2a\u5b50\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7684\u5bfc\u822a\u4ee3\u7406\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5730\u9762\u673a\u5668\u4eba\u6216\u5ba4\u5916\u65e0\u4eba\u673a\uff0c\u800c\u5ba4\u5185\u65e0\u4eba\u673aVLN\u7814\u7a76\u4e0d\u8db3\uff0c\u5c3d\u7ba1\u5728\u68c0\u67e5\u3001\u9012\u9001\u3001\u641c\u6551\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "1) \u4eceHabitat\u6a21\u62df\u5668\u6536\u96c61000\u591a\u4e2a\u591a\u6837\u5316\u76843D\u5ba4\u5185\u573a\u666f\uff1b2) \u6a21\u62df\u771f\u5b9e\u65e0\u4eba\u673a\u98de\u884c\u52a8\u6001\u6536\u96c6\u8f68\u8ff9\uff1b3) \u8bbe\u8ba1\u81ea\u52a8\u6807\u6ce8\u6d41\u6c34\u7ebf\u751f\u6210\u591a\u7c92\u5ea6\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff1b4) \u5c06\u957f\u8f68\u8ff9\u5206\u5272\u4e3a\u8bed\u4e49\u5173\u952e\u5e27\u5f62\u6210\u5b50\u8f68\u8ff9\uff1b5) \u63d0\u51faIndoorUAV-Agent\u5bfc\u822a\u6a21\u578b\uff0c\u5229\u7528\u4efb\u52a1\u5206\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b16000\u591a\u4e2a\u9ad8\u8d28\u91cf\u8f68\u8ff9\u7684IndoorUAV-VLN\u5b50\u96c6\uff08\u957f\u89c6\u91ce\u5bfc\u822a\uff09\u548cIndoorUAV-VLA\u5b50\u96c6\uff08\u77ed\u89c6\u91ce\u89c4\u5212\uff09\uff0c\u4e3a\u5ba4\u5185\u65e0\u4eba\u673a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e13\u95e8\u57fa\u51c6\u3002", "conclusion": "IndoorUAV\u586b\u8865\u4e86\u5ba4\u5185\u65e0\u4eba\u673a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u5177\u8eabAI\u5728\u5ba4\u5185\u7a7a\u4e2d\u5bfc\u822a\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2512.18551", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18551", "abs": "https://arxiv.org/abs/2512.18551", "authors": ["Sungjoon Park", "Varun Ramamurthi", "Owen Terry"], "title": "Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering", "comment": null, "summary": "In language modeling, neologisms are new tokens trained to represent a concept not already included in a given model's vocabulary. Neologisms can be used to encourage specific behavior in models, for example by appending prompts with \"Give me a neologism answer.\" Behavioral steering can also be achieved through fine-tuning, albeit with more compute and less flexibility: learning a neologism only trains d parameters and allows the user to still access the model's default behavior. We compare the performance of neologism learning against low-rank adaptation (LoRA) fine-tuning, finding that neologisms outperform fine-tuned models under a matched training setup (same data and hyperparameters). We also investigate self-verbalizations of neologisms, and observe that the model will occasionally make up its own new words when asked about a neologism.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u65b0\u8bcd\u5b66\u4e60\u4e0eLoRA\u5fae\u8c03\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u65b0\u8bcd\u5b66\u4e60\u5728\u76f8\u540c\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u5fae\u8c03\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u4e86\u6a21\u578b\u5bf9\u65b0\u8bcd\u7684\u81ea\u6211\u8868\u8fbe\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u884c\u4e3a\u5f15\u5bfc\u65b9\u6cd5\u3002\u4f20\u7edf\u5fae\u8c03\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u800c\u65b0\u8bcd\u5b66\u4e60\u53ea\u9700\u8bad\u7ec3\u5c11\u91cf\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u7684\u9ed8\u8ba4\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u65b0\u8bcd\u5b66\u4e60\u4e0eLoRA\u5fae\u8c03\u7684\u6027\u80fd\uff0c\u5728\u76f8\u540c\u6570\u636e\u548c\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5339\u914d\u8bad\u7ec3\u5b9e\u9a8c\u3002\u540c\u65f6\u7814\u7a76\u6a21\u578b\u5bf9\u65b0\u8bcd\u7684\u81ea\u6211\u8868\u8fbe\u884c\u4e3a\uff0c\u89c2\u5bdf\u6a21\u578b\u5728\u88ab\u95ee\u53ca\u65b0\u8bcd\u65f6\u662f\u5426\u4f1a\u521b\u9020\u81ea\u5df1\u7684\u65b0\u8bcd\u3002", "result": "\u65b0\u8bcd\u5b66\u4e60\u5728\u5339\u914d\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\uff08\u76f8\u540c\u6570\u636e\u548c\u8d85\u53c2\u6570\uff09\u4f18\u4e8eLoRA\u5fae\u8c03\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u88ab\u95ee\u53ca\u65b0\u8bcd\u65f6\u5076\u5c14\u4f1a\u521b\u9020\u81ea\u5df1\u7684\u65b0\u8bcd\u3002", "conclusion": "\u65b0\u8bcd\u5b66\u4e60\u662f\u4e00\u79cd\u6bd4\u4f20\u7edf\u5fae\u8c03\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u884c\u4e3a\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u9ed8\u8ba4\u884c\u4e3a\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u8bad\u7ec3\u5c11\u91cf\u53c2\u6570\u5c31\u80fd\u5b9e\u73b0\u7279\u5b9a\u884c\u4e3a\u5f15\u5bfc\u3002"}}
{"id": "2512.19043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19043", "abs": "https://arxiv.org/abs/2512.19043", "authors": ["Chao Yang", "Yingkai Sun", "Peng Ye", "Xin Chen", "Chong Yu", "Tao Chen"], "title": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control", "comment": null, "summary": "Learning a general motion tracking policy from human motions shows great potential for versatile humanoid whole-body control. Conventional approaches are not only inefficient in data utilization and training processes but also exhibit limited performance when tracking highly dynamic motions. To address these challenges, we propose EGM, a framework that enables efficient learning of a general motion tracking policy. EGM integrates four core designs. Firstly, we introduce a Bin-based Cross-motion Curriculum Adaptive Sampling strategy to dynamically orchestrate the sampling probabilities based on tracking error of each motion bin, eficiently balancing the training process across motions with varying dificulty and durations. The sampled data is then processed by our proposed Composite Decoupled Mixture-of-Experts (CDMoE) architecture, which efficiently enhances the ability to track motions from different distributions by grouping experts separately for upper and lower body and decoupling orthogonal experts from shared experts to separately handle dedicated features and general features. Central to our approach is a key insight we identified: for training a general motion tracking policy, data quality and diversity are paramount. Building on these designs, we develop a three-stage curriculum training flow to progressively enhance the policy's robustness against disturbances. Despite training on only 4.08 hours of data, EGM generalized robustly across 49.25 hours of test motions, outperforming baselines on both routine and highly dynamic tasks.", "AI": {"tldr": "EGM\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8bfe\u7a0b\u91c7\u6837\u3001\u590d\u5408\u89e3\u8026\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u4e09\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\uff0c\u4ec5\u75284.08\u5c0f\u65f6\u6570\u636e\u5c31\u80fd\u9ad8\u6548\u5b66\u4e60\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\uff0c\u572849.25\u5c0f\u65f6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u57fa\u7ebf", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u5229\u7528\u548c\u8bad\u7ec3\u6548\u7387\u4e0a\u4e0d\u8db3\uff0c\u4e14\u5728\u8ddf\u8e2a\u9ad8\u52a8\u6001\u8fd0\u52a8\u65f6\u6027\u80fd\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u9ad8\u6548\u5b66\u4e60\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\u7684\u6846\u67b6", "method": "\u63d0\u51faEGM\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u8bbe\u8ba1\uff1a1) \u57fa\u4e8e\u7bb1\u7684\u8de8\u8fd0\u52a8\u8bfe\u7a0b\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u6839\u636e\u8fd0\u52a8\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u91c7\u6837\u6982\u7387\uff1b2) \u590d\u5408\u89e3\u8026\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u5206\u522b\u5904\u7406\u4e0a\u4e0b\u534a\u8eab\u8fd0\u52a8\uff0c\u89e3\u8026\u4e13\u7528\u548c\u901a\u7528\u7279\u5f81\uff1b3) \u5f3a\u8c03\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\uff1b4) \u4e09\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\u6d41\u7a0b\uff0c\u9010\u6b65\u589e\u5f3a\u7b56\u7565\u5bf9\u6270\u52a8\u7684\u9c81\u68d2\u6027", "result": "\u4ec5\u75284.08\u5c0f\u65f6\u6570\u636e\u8bad\u7ec3\uff0c\u572849.25\u5c0f\u65f6\u6d4b\u8bd5\u8fd0\u52a8\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5e38\u89c4\u548c\u9ad8\u52a8\u6001\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "EGM\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u91c7\u6837\u7b56\u7565\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b66\u4e60\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u5229\u7528\u6548\u7387\u548c\u52a8\u6001\u8fd0\u52a8\u8ddf\u8e2a\u65b9\u9762\u7684\u5c40\u9650\u6027"}}
{"id": "2512.18593", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18593", "abs": "https://arxiv.org/abs/2512.18593", "authors": ["Amit Barman", "Atanu Mandal", "Sudip Kumar Naskar"], "title": "From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation", "comment": null, "summary": "In multilingual nations like India, access to legal information is often hindered by language barriers, as much of the legal and judicial documentation remains in English. Legal Machine Translation (L-MT) offers a scalable solution to this challenge by enabling accurate and accessible translations of legal documents. This paper presents our work for the JUST-NLP 2025 Legal MT shared task, focusing on English-Hindi translation using Transformer-based approaches. We experiment with 2 complementary strategies, fine-tuning a pre-trained OPUS-MT model for domain-specific adaptation and training a Transformer model from scratch using the provided legal corpus. Performance is evaluated using standard MT metrics, including SacreBLEU, chrF++, TER, ROUGE, BERTScore, METEOR, and COMET. Our fine-tuned OPUS-MT model achieves a SacreBLEU score of 46.03, significantly outperforming both baseline and from-scratch models. The results highlight the effectiveness of domain adaptation in enhancing translation quality and demonstrate the potential of L-MT systems to improve access to justice and legal transparency in multilingual contexts.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5370\u5ea6\u7b49\u591a\u8bed\u8a00\u56fd\u5bb6\u7684\u6cd5\u5f8b\u4fe1\u606f\u83b7\u53d6\u969c\u788d\uff0c\u63d0\u51fa\u4f7f\u7528\u6cd5\u5f8b\u673a\u5668\u7ffb\u8bd1(L-MT)\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3OPUS-MT\u6a21\u578b\u548c\u4ece\u5934\u8bad\u7ec3Transformer\u6a21\u578b\u4e24\u79cd\u7b56\u7565\uff0c\u5728\u82f1\u8bed-\u5370\u5730\u8bed\u6cd5\u5f8b\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5728\u591a\u8bed\u8a00\u56fd\u5bb6\u5982\u5370\u5ea6\uff0c\u6cd5\u5f8b\u548c\u53f8\u6cd5\u6587\u4ef6\u4e3b\u8981\u4f7f\u7528\u82f1\u8bed\uff0c\u5bfc\u81f4\u975e\u82f1\u8bed\u4f7f\u7528\u8005\u96be\u4ee5\u83b7\u53d6\u6cd5\u5f8b\u4fe1\u606f\uff0c\u5f62\u6210\u8bed\u8a00\u969c\u788d\u3002\u6cd5\u5f8b\u673a\u5668\u7ffb\u8bd1(L-MT)\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u51c6\u786e\u7ffb\u8bd1\u6cd5\u5f8b\u6587\u4ef6\uff0c\u63d0\u9ad8\u6cd5\u5f8b\u900f\u660e\u5ea6\u548c\u53f8\u6cd5\u53ef\u53ca\u6027\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a1) \u5bf9\u9884\u8bad\u7ec3\u7684OPUS-MT\u6a21\u578b\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff1b2) \u4f7f\u7528\u63d0\u4f9b\u7684\u6cd5\u5f8b\u8bed\u6599\u5e93\u4ece\u5934\u8bad\u7ec3Transformer\u6a21\u578b\u3002\u4f7f\u7528\u591a\u79cd\u6807\u51c6\u673a\u5668\u7ffb\u8bd1\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ecSacreBLEU\u3001chrF++\u3001TER\u3001ROUGE\u3001BERTScore\u3001METEOR\u548cCOMET\u3002", "result": "\u5fae\u8c03\u7684OPUS-MT\u6a21\u578b\u53d6\u5f97\u4e86SacreBLEU\u5206\u657046.03\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\u9886\u57df\u9002\u5e94\u5728\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9886\u57df\u9002\u5e94\u7684\u6cd5\u5f8b\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6cd5\u5f8b\u6587\u4ef6\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5177\u6709\u6539\u5584\u53f8\u6cd5\u53ef\u53ca\u6027\u548c\u6cd5\u5f8b\u900f\u660e\u5ea6\u7684\u6f5c\u529b\u3002\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2512.19083", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19083", "abs": "https://arxiv.org/abs/2512.19083", "authors": ["Pengyu Chen", "Tao Ouyang", "Ke Luo", "Weijie Hong", "Xu Chen"], "title": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models", "comment": "This paper is accepted by the IEEE Internet of Things Journal (IoT-J) for publication in the Special Issue on \"Augmented Edge Sensing Intelligence for Low-Altitude IoT Systems\"", "summary": "Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.", "AI": {"tldr": "CoDrone\uff1a\u9996\u4e2a\u5c06\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u65e0\u4eba\u673a\u81ea\u4e3b\u5de1\u822a\u573a\u666f\u7684\u4e91-\u8fb9-\u7aef\u534f\u540c\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u7070\u5ea6\u56fe\u50cf\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u5f3a\u5316\u5b66\u4e60\u8c03\u5ea6\u5668\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u63d0\u5347\u5bfc\u822a\u6027\u80fd", "motivation": "\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u673a\u8f7d\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u5173\u952e\u6311\u6218\uff0c\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u5904\u7406\u590d\u6742\u73af\u5883\uff0c\u800c\u5c06\u4efb\u52a1\u5378\u8f7d\u5230\u8fdc\u7a0b\u8fb9\u7f18\u670d\u52a1\u5668\u53c8\u4f1a\u5f15\u5165\u9ad8\u5ef6\u8fdf\uff0c\u9700\u8981\u5728\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u6743\u8861", "method": "1) \u4f7f\u7528\u7070\u5ea6\u56fe\u50cf\u51cf\u5c11\u673a\u8f7d\u8ba1\u7b97\u548c\u6570\u636e\u4f20\u8f93\u5f00\u9500\uff1b2) \u5229\u7528\u8fb9\u7f18\u8f85\u52a9\u57fa\u7840\u6a21\u578bDepth Anything V2\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\uff1b3) \u63d0\u51fa\u65b0\u9896\u7684\u4e00\u7ef4\u5360\u636e\u6805\u683c\u5bfc\u822a\u65b9\u6cd5\uff1b4) \u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u795e\u7ecf\u8c03\u5ea6\u5668\u6574\u5408\u6df1\u5ea6\u4f30\u8ba1\u4e0e\u5bfc\u822a\u51b3\u7b56\uff1b5) \u5f15\u5165\u65e0\u4eba\u673a\u4e13\u7528\u89c6\u89c9\u8bed\u8a00\u4ea4\u4e92\u6a21\u5757\uff0c\u5305\u542b\u9886\u57df\u5b9a\u5236\u7684\u4f4e\u7ea7\u98de\u884c\u539f\u8bed", "result": "\u5728\u4e0d\u540c\u98de\u884c\u901f\u5ea6\u548c\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0cCoDrone\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u98de\u884c\u8ddd\u79bb\u589e\u52a040%\uff0c\u5e73\u5747\u5bfc\u822a\u8d28\u91cf\u63d0\u53475%", "conclusion": "CoDrone\u901a\u8fc7\u4e91-\u8fb9-\u7aef\u534f\u540c\u8ba1\u7b97\u6846\u67b6\uff0c\u6709\u6548\u5229\u7528\u57fa\u7840\u6a21\u578b\u589e\u5f3a\u8d44\u6e90\u53d7\u9650\u65e0\u4eba\u673a\u5e73\u53f0\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u573a\u666f\u7406\u89e3\u4e0e\u81ea\u4e3b\u5bfc\u822a\u6548\u7387\u7684\u5e73\u8861"}}
{"id": "2512.18601", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18601", "abs": "https://arxiv.org/abs/2512.18601", "authors": ["Charles J. Lovering", "Seth Ebner", "Brandon Smock", "Michael Krumdick", "Saad Rabbani", "Ahmed Muhammad", "Varshini Reddy", "Chris Tanner"], "title": "On Finding Inconsistencies in Documents", "comment": null, "summary": "Professionals in academia, law, and finance audit their documents because inconsistencies can result in monetary, reputational, and scientific costs. Language models (LMs) have the potential to dramatically speed up this auditing process. To understand their abilities, we introduce a benchmark, FIND (Finding INconsistencies in Documents), where each example is a document with an inconsistency inserted manually by a domain expert. Despite the documents being long, technical, and complex, the best-performing model (gpt-5) recovered 64% of the inserted inconsistencies. Surprisingly, gpt-5 also found undiscovered inconsistencies present in the original documents. For example, on 50 arXiv papers, we judged 136 out of 196 of the model's suggestions to be legitimate inconsistencies missed by the original authors. However, despite these findings, even the best models miss almost half of the inconsistencies in FIND, demonstrating that inconsistency detection is still a challenging task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FIND\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u6587\u6863\u4e0d\u4e00\u81f4\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0GPT-5\u80fd\u68c0\u6d4b\u523064%\u7684\u4eba\u5de5\u63d2\u5165\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd8\u80fd\u53d1\u73b0\u539f\u59cb\u6587\u6863\u4e2d\u4f5c\u8005\u672a\u53d1\u73b0\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4f46\u4ecd\u6709\u8fd1\u4e00\u534a\u7684\u4e0d\u4e00\u81f4\u6027\u65e0\u6cd5\u68c0\u6d4b\u3002", "motivation": "\u5b66\u672f\u3001\u6cd5\u5f8b\u548c\u91d1\u878d\u9886\u57df\u7684\u4e13\u4e1a\u4eba\u58eb\u9700\u8981\u5ba1\u6838\u6587\u6863\uff0c\u56e0\u4e3a\u6587\u6863\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u53ef\u80fd\u5bfc\u81f4\u91d1\u94b1\u3001\u58f0\u8a89\u548c\u79d1\u5b66\u6210\u672c\u3002\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\u5927\u5e45\u52a0\u901f\u8fd9\u4e00\u5ba1\u6838\u8fc7\u7a0b\uff0c\u4f46\u9700\u8981\u8bc4\u4f30\u5176\u5b9e\u9645\u80fd\u529b\u3002", "method": "\u5f15\u5165FIND\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bcf\u4e2a\u793a\u4f8b\u90fd\u662f\u7531\u9886\u57df\u4e13\u5bb6\u624b\u52a8\u63d2\u5165\u4e0d\u4e00\u81f4\u6027\u7684\u6587\u6863\u3002\u572850\u7bc7arXiv\u8bba\u6587\u4e0a\u6d4b\u8bd5\u6a21\u578b\uff0c\u8bc4\u4f30\u6a21\u578b\u68c0\u6d4b\u4e0d\u4e00\u81f4\u6027\u7684\u80fd\u529b\u3002", "result": "\u6700\u4f73\u6a21\u578bGPT-5\u6062\u590d\u4e8664%\u7684\u4eba\u5de5\u63d2\u5165\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u539f\u59cb\u6587\u6863\u4e2d\u53d1\u73b0\u4e86\u4f5c\u8005\u672a\u53d1\u73b0\u7684\u4e0d\u4e00\u81f4\u6027\uff08\u5728arXiv\u8bba\u6587\u4e2d\uff0c\u6a21\u578b\u63d0\u51fa\u7684196\u4e2a\u5efa\u8bae\u4e2d\u6709136\u4e2a\u88ab\u5224\u5b9a\u4e3a\u5408\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\uff09\u3002\u4f46\u5373\u4f7f\u6700\u4f73\u6a21\u578b\u4e5f\u9057\u6f0f\u4e86FIND\u4e2d\u8fd1\u4e00\u534a\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u80fd\u591f\u53d1\u73b0\u4eba\u7c7b\u4f5c\u8005\u9057\u6f0f\u7684\u95ee\u9898\uff0c\u4f46\u8be5\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2512.19133", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19133", "abs": "https://arxiv.org/abs/2512.19133", "authors": ["Pengxuan Yang", "Ben Lu", "Zhongpu Xia", "Chao Han", "Yinfeng Gao", "Teng Zhang", "Kun Zhan", "XianPeng Lang", "Yupeng Zheng", "Qichao Zhang"], "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving", "comment": "AAAI 2026, first version", "summary": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).", "AI": {"tldr": "WorldRFT\u662f\u4e00\u4e2a\u9762\u5411\u89c4\u5212\u7684\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u5206\u89e3\u548c\u5c40\u90e8\u611f\u77e5\u4ea4\u4e92\u4f18\u5316\u673a\u5236\uff0c\u5c06\u573a\u666f\u8868\u793a\u5b66\u4e60\u4e0e\u89c4\u5212\u5bf9\u9f50\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u5347\u5b89\u5168\u5173\u952e\u7b56\u7565\u6027\u80fd\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u65f6\u95f4\u81ea\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u573a\u666f\u8868\u793a\uff0c\u4f46\u91cd\u5efa\u5bfc\u5411\u7684\u8868\u793a\u5b66\u4e60\u5c06\u611f\u77e5\u4e0e\u89c4\u5212\u4efb\u52a1\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u5bfc\u81f4\u89c4\u5212\u4f18\u5316\u4e0d\u7406\u60f3\u3002\u9700\u8981\u89e3\u51b3\u611f\u77e5\u4e0e\u89c4\u5212\u4efb\u52a1\u89e3\u8026\u7684\u95ee\u9898\uff0c\u4f7f\u573a\u666f\u8868\u793a\u5b66\u4e60\u66f4\u597d\u5730\u670d\u52a1\u4e8e\u89c4\u5212\u4efb\u52a1\u3002", "method": "1. \u96c6\u6210\u89c6\u89c9\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u63d0\u53473D\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff1b2. \u91c7\u7528\u5206\u5c42\u89c4\u5212\u4efb\u52a1\u5206\u89e3\u6307\u5bfc\u8868\u793a\u4f18\u5316\uff1b3. \u4f7f\u7528\u5c40\u90e8\u611f\u77e5\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u63a8\u5bfc\u9762\u5411\u89c4\u5212\u7684\u9a7e\u9a76\u7b56\u7565\uff1b4. \u5f15\u5165Group Relative Policy Optimization\uff08GRPO\uff09\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u5305\u542b\u8f68\u8ff9\u9ad8\u65af\u5316\u548c\u78b0\u649e\u611f\u77e5\u5956\u52b1\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u78b0\u649e\u7387\u964d\u4f4e83%\uff080.30%\u21920.05%\uff09\uff1b\u5728NavSim\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u4f20\u611f\u5668\u8f93\u5165\u5c31\u8fbe\u5230\u4e86\u4e0e\u57fa\u4e8eLiDAR\u7684SOTA\u65b9\u6cd5DiffusionDrive\u76f8\u5f53\u7684\u6027\u80fd\uff0887.8 vs. 88.1 PDMS\uff09\u3002", "conclusion": "WorldRFT\u901a\u8fc7\u89c4\u5212\u5bfc\u5411\u7684\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u6210\u529f\u5c06\u573a\u666f\u8868\u793a\u5b66\u4e60\u4e0e\u89c4\u5212\u4efb\u52a1\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2512.18608", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18608", "abs": "https://arxiv.org/abs/2512.18608", "authors": ["Prabigya Acharya", "Liza Shrestha"], "title": "A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts", "comment": null, "summary": "Automated masking of Personally Identifiable Information (PII) is critical for privacy-preserving conversational systems. While current frontier large language models demonstrate strong PII masking capabilities, concerns about data handling and computational costs motivate exploration of whether lightweight models can achieve comparable performance. We compare encoder-decoder and decoder-only architectures by fine-tuning T5-small and Mistral-Instruct-v0.3 on English datasets constructed from the AI4Privacy benchmark. We create different dataset variants to study label standardization and PII representation, covering 24 standardized PII categories and higher-granularity settings. Evaluation using entity-level and character-level metrics, type accuracy, and exact match shows that both lightweight models achieve performance comparable to frontier LLMs for PII masking tasks. Label normalization consistently improves performance across architectures. Mistral achieves higher F1 and recall with greater robustness across PII types but incurs significantly higher generation latency. T5, while less robust in conversational text, offers more controllable structured outputs and lower inference cost, motivating its use in a real-time Discord bot for real-world PII redaction. Evaluation on live messages reveals performance degradation under informal inputs. These results clarify trade-offs between accuracy, robustness, and computational efficiency, demonstrating that lightweight models can provide effective PII masking while addressing data handling concerns associated with frontier LLMs.", "AI": {"tldr": "\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08T5-small\u548cMistral\uff09\u5728PII\u63a9\u7801\u4efb\u52a1\u4e0a\u80fd\u8fbe\u5230\u4e0e\u524d\u6cbf\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u6807\u7b7e\u6807\u51c6\u5316\u80fd\u63d0\u5347\u6548\u679c\uff0c\u4f46\u5b58\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728PII\u63a9\u7801\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u6570\u636e\u5904\u7406\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u56e0\u6b64\u63a2\u7d22\u8f7b\u91cf\u7ea7\u6a21\u578b\u662f\u5426\u80fd\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002", "method": "\u4f7f\u7528AI4Privacy\u57fa\u51c6\u6784\u5efa\u82f1\u8bed\u6570\u636e\u96c6\uff0c\u5fae\u8c03T5-small\u548cMistral-Instruct-v0.3\u6a21\u578b\uff0c\u521b\u5efa\u4e0d\u540c\u6570\u636e\u96c6\u53d8\u4f53\u7814\u7a76\u6807\u7b7e\u6807\u51c6\u5316\u548cPII\u8868\u793a\uff0c\u6db5\u76d624\u4e2a\u6807\u51c6\u5316PII\u7c7b\u522b\u548c\u9ad8\u7c92\u5ea6\u8bbe\u7f6e\u3002", "result": "\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728PII\u63a9\u7801\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u4e86\u4e0e\u524d\u6cbfLLM\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6807\u7b7e\u6807\u51c6\u5316\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002Mistral\u5728F1\u548c\u53ec\u56de\u7387\u4e0a\u66f4\u9ad8\u4e14\u5bf9PII\u7c7b\u578b\u66f4\u9c81\u68d2\uff0c\u4f46\u751f\u6210\u5ef6\u8fdf\u663e\u8457\u66f4\u9ad8\u3002T5\u5728\u5bf9\u8bdd\u6587\u672c\u4e2d\u9c81\u68d2\u6027\u8f83\u5dee\uff0c\u4f46\u63d0\u4f9b\u66f4\u53ef\u63a7\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u66f4\u4f4e\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u80fd\u63d0\u4f9b\u6709\u6548\u7684PII\u63a9\u7801\uff0c\u540c\u65f6\u89e3\u51b3\u4e0e\u524d\u6cbfLLM\u76f8\u5173\u7684\u6570\u636e\u5904\u7406\u95ee\u9898\uff0c\u4f46\u9700\u8981\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u6743\u8861\u3002T5\u66f4\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u5982Discord\u673a\u5668\u4eba\uff0c\u4f46\u5728\u975e\u6b63\u5f0f\u8f93\u5165\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\u3002"}}
{"id": "2512.19148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19148", "abs": "https://arxiv.org/abs/2512.19148", "authors": ["Jose Gustavo Buenaventura Carreon", "Floris Erich", "Roman Mykhailyshyn", "Tomohiro Motoda", "Ryo Hanai", "Yukiyasu Domae"], "title": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors", "comment": "6 pages, 7 figures, conference: SII 2026. Cancun, Mexico", "summary": "We present a cross robot visuomotor learning framework that integrates diffusion policy based control with 3D semantic scene representations from D3Fields to enable category level generalization in manipulation. Its modular design supports diverse robot camera configurations including UR5 arms with Microsoft Azure Kinect arrays and bimanual manipulators with Intel RealSense sensors through a low latency control stack and intuitive teleoperation. A unified configuration layer enables seamless switching between setups for flexible data collection training and evaluation. In a grasp and lift block task the framework achieved an 80 percent success rate after only 100 demonstration episodes demonstrating robust skill transfer between platforms and sensing modalities. This design paves the way for scalable real world studies in cross robotic generalization.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u673a\u5668\u4eba\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u63a7\u5236\u4e0e3D\u8bed\u4e49\u573a\u666f\u8868\u793a\uff0c\u5b9e\u73b0\u7c7b\u522b\u7ea7\u6cdb\u5316\u64cd\u4f5c\uff0c\u652f\u6301\u591a\u79cd\u673a\u5668\u4eba\u914d\u7f6e\uff0c\u5728\u6293\u53d6\u4efb\u52a1\u4e2d\u4ec5\u9700100\u6b21\u6f14\u793a\u5373\u53ef\u8fbe\u523080%\u6210\u529f\u7387", "motivation": "\u89e3\u51b3\u8de8\u673a\u5668\u4eba\u5e73\u53f0\u548c\u611f\u77e5\u6a21\u6001\u7684\u6280\u80fd\u8fc1\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u771f\u5b9e\u4e16\u754c\u8de8\u673a\u5668\u4eba\u6cdb\u5316\u7814\u7a76", "method": "\u6a21\u5757\u5316\u8bbe\u8ba1\u6574\u5408\u6269\u6563\u7b56\u7565\u63a7\u5236\u4e0eD3Fields\u76843D\u8bed\u4e49\u573a\u666f\u8868\u793a\uff0c\u652f\u6301\u591a\u79cd\u673a\u5668\u4eba\u76f8\u673a\u914d\u7f6e\uff0c\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u63a7\u5236\u5806\u6808\u548c\u76f4\u89c2\u9065\u64cd\u4f5c\u5b9e\u73b0\u7075\u6d3b\u6570\u636e\u6536\u96c6", "result": "\u5728\u6293\u53d6\u548c\u4e3e\u8d77\u79ef\u6728\u4efb\u52a1\u4e2d\uff0c\u4ec5\u9700100\u6b21\u6f14\u793a\u5373\u53ef\u8fbe\u523080%\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u5e73\u53f0\u95f4\u548c\u611f\u77e5\u6a21\u6001\u95f4\u7684\u7a33\u5065\u6280\u80fd\u8fc1\u79fb", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u6269\u5c55\u7684\u771f\u5b9e\u4e16\u754c\u8de8\u673a\u5668\u4eba\u6cdb\u5316\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u7075\u6d3b\u914d\u7f6e\u5207\u6362"}}
{"id": "2512.18623", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18623", "abs": "https://arxiv.org/abs/2512.18623", "authors": ["Jensen Zhang", "Ningyuan Liu", "Yijia Fan", "Zihao Huang", "Qinglin Zeng", "Kaitong Cai", "Jian Wang", "Keze Wang"], "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction", "comment": "Accepted at AAAI 2026", "summary": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.", "AI": {"tldr": "LLM-CAS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u667a\u80fd\u4f53\u5728\u63a8\u7406\u65f6\u52a8\u6001\u9009\u62e9\u4e34\u65f6\u795e\u7ecf\u5143\u6270\u52a8\u6765\u5b9e\u65f6\u7ea0\u6b63\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u65e0\u9700\u6c38\u4e45\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u751f\u6210\u7f3a\u4e4f\u4e8b\u5b9e\u6216\u4e0a\u4e0b\u6587\u4f9d\u636e\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002\u73b0\u6709\u7684\u76d1\u7763\u5fae\u8c03\u548c\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6570\u636e\u5bc6\u96c6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u9759\u6001\u53c2\u6570\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0a\u4e0b\u6587\u4f9d\u8d56\u9519\u8bef\u5e76\u5bb9\u6613\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "LLM-CAS\u5c06\u5b9e\u65f6\u5e7b\u89c9\u7ea0\u6b63\u6784\u5efa\u4e3a\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u8bad\u7ec3\u4e00\u4e2a\u667a\u80fd\u4f53\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u63a8\u7406\u65f6\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u52a8\u6001\u9009\u62e9\u4e34\u65f6\u795e\u7ecf\u5143\u6270\u52a8\u3002\u8fd9\u79cd\u7b56\u7565\u9a71\u52a8\u673a\u5236\u4e0d\u540c\u4e8e\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u9884\u5b9a\u4e49\u8c03\u6574\u7684\u5148\u524d\u52a8\u6001\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u7ec6\u7c92\u5ea6\u7684\u7ea0\u6b63\uff0c\u800c\u65e0\u9700\u6c38\u4e45\u4fee\u6539\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLM-CAS\u6301\u7eed\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\uff1a\u5728StoryCloze\u4e0a\u63d0\u5347\u4e8610.98\u4e2a\u767e\u5206\u70b9\uff0c\u5728TriviaQA\u4e0a\u63d0\u5347\u4e862.71\u4e2a\u767e\u5206\u70b9\uff0c\u5728TruthfulQA\u7684MC1\u5206\u6570\u4e0a\u63d0\u5347\u4e862.06\u4e2a\u767e\u5206\u70b9\u3002\u8fd9\u4e9b\u7ed3\u679c\u4f18\u4e8e\u9759\u6001\u7f16\u8f91\u65b9\u6cd5\uff08\u5982ITI\u548cCAA\uff09\u548c\u52a8\u6001SADI\u6846\u67b6\u3002", "conclusion": "LLM-CAS\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u672a\u6765\u6269\u5c55\u5230\u591a\u6a21\u6001\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.19178", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19178", "abs": "https://arxiv.org/abs/2512.19178", "authors": ["Jin Wang", "Kim Tien Ly", "Jacques Cloete", "Nikos Tsagarakis", "Ioannis Havoutis"], "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning", "comment": "Manuscript under review", "summary": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u52a8\u6001\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u6846\u67b6VLP\uff0c\u80fd\u591f\u7406\u89e3\u8bed\u4e49\u6307\u4ee4\u3001\u63a8\u7406\u4efb\u52a1\u573a\u666f\u5e76\u751f\u6210\u884c\u4e3a\u7b56\u7565\uff0c\u652f\u6301\u52a8\u6001\u8c03\u6574\u4ee5\u9002\u5e94\u4efb\u52a1\u53d8\u5316\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u5f25\u5408\u4f4e\u7ea7\u6267\u884c\u4e0e\u9ad8\u7ea7\u4efb\u52a1\u63a8\u7406\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e14\u65e0\u6cd5\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u52a8\u6001\u66f4\u65b0\u4efb\u52a1\u7b56\u7565\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "method": "\u63d0\u51faVision-Language-Policy (VLP)\u6a21\u578b\uff0c\u57fa\u4e8e\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u89e3\u91ca\u8bed\u4e49\u6307\u4ee4\u5e76\u6574\u5408\u5f53\u524d\u4efb\u52a1\u573a\u666f\u7684\u63a8\u7406\uff0c\u751f\u6210\u63a7\u5236\u673a\u5668\u4eba\u5b8c\u6210\u4efb\u52a1\u7684\u884c\u4e3a\u7b56\u7565\u3002", "result": "\u5728\u4e0d\u540c\u673a\u5668\u4eba\u548c\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u80fd\u6709\u6548\u9002\u5e94\u65b0\u573a\u666f\u5e76\u52a8\u6001\u66f4\u65b0\u7b56\u7565\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u89c4\u5212\u81ea\u4e3b\u6027\u548c\u8de8\u5177\u8eab\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VLP\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u4efb\u52a1\u89c4\u5212\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.18658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18658", "abs": "https://arxiv.org/abs/2512.18658", "authors": ["Pierre Colombo", "Malik Boudiaf", "Allyn Sweet", "Michael Desa", "Hongxi Wang", "Kevin Candra", "Sym\u00e9on del Marmol"], "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital", "comment": null, "summary": "Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u8d44\u672c\u5316\u6838\u5bf9\u4f5c\u4e3a\u6cd5\u5f8bAI\u7684\u73b0\u5b9e\u57fa\u51c6\u4efb\u52a1\uff0c\u5206\u6790\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u9762\u5411\u81ea\u52a8\u5316\u6838\u5bf9\u7684\u4e16\u754c\u6a21\u578b\u67b6\u6784", "motivation": "\u98ce\u9669\u6295\u8d44\u878d\u8d44\u524d\u9700\u8981\u5f8b\u5e08\u8fdb\u884c\u8d44\u672c\u5316\u6838\u5bf9\uff0c\u9a8c\u8bc1\u6240\u6709\u8bc1\u5238\u548c\u53d1\u884c\u6761\u6b3e\u662f\u5426\u5f97\u5230\u5927\u91cf\u6cd5\u5f8b\u6587\u4ef6\u652f\u6301\u3002\u867d\u7136LLM\u5728\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6301\u7eed\u6539\u8fdb\uff0c\u4f46\u8d44\u672c\u5316\u6838\u5bf9\u8fd9\u7c7b\u4e13\u4e1a\u6cd5\u5f8b\u5de5\u4f5c\u6d41\u7a0b\u4ecd\u8d85\u51fa\u5f53\u524d\u4ee3\u7406\u7cfb\u7edf\u7684\u80fd\u529b\u8303\u56f4", "method": "\u5c06\u8d44\u672c\u5316\u6838\u5bf9\u4f5c\u4e3a\u6cd5\u5f8bAI\u7684\u73b0\u5b9e\u57fa\u51c6\u8fdb\u884c\u7279\u5f81\u5316\uff0c\u5206\u6790\u6bd4\u8f83\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e16\u754c\u6a21\u578b\u67b6\u6784\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u6838\u5bf9", "result": "\u5f53\u524d\u65b9\u6cd5\u5728\u591a\u6587\u6863\u63a8\u7406\u3001\u4e25\u683c\u8bc1\u636e\u53ef\u8ffd\u6eaf\u6027\u548c\u786e\u5b9a\u6027\u8f93\u51fa\u65b9\u9762\u65e0\u6cd5\u53ef\u9760\u5b8c\u6210\u4efb\u52a1\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218", "conclusion": "\u63d0\u51fa\u7684\u4e16\u754c\u6a21\u578b\u67b6\u6784\u4e0d\u4ec5\u53ef\u7528\u4e8e\u8d44\u672c\u5316\u6838\u5bf9\u81ea\u52a8\u5316\uff0c\u8fd8\u53ef\u4f5c\u4e3a\u5e94\u7528\u6cd5\u5f8b\u667a\u80fd\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e13\u4e1a\u6cd5\u5f8b\u5de5\u4f5c\u6d41\u7a0b\u7684AI\u81ea\u52a8\u5316"}}
{"id": "2512.19269", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19269", "abs": "https://arxiv.org/abs/2512.19269", "authors": ["Yitian Zheng", "Zhangchen Ye", "Weijun Dong", "Shengjie Wang", "Yuyang Liu", "Chongjie Zhang", "Chuan Wen", "Yang Gao"], "title": "Translating Flow to Policy via Hindsight Online Imitation", "comment": null, "summary": "Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.", "AI": {"tldr": "HinFlow\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u6539\u8fdb\u4f4e\u7ea7\u7b56\u7565\uff0c\u4f7f\u7528\u540e\u89c1\u4e4b\u660e\u91cd\u6807\u6ce8\u6536\u96c6\u7684\u7ecf\u9a8c\u6765\u66f4\u65b0\u76ee\u6807\u6761\u4ef6\u6a21\u4eff\u7b56\u7565\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b02\u500d\u4ee5\u4e0a\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5206\u5c42\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u9ad8\u7ea7\u89c4\u5212\u5668\u53ef\u4ee5\u5728\u65e0\u52a8\u4f5c\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4f46\u5c06\u9ad8\u7ea7\u8ba1\u5212\u843d\u5730\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u4ecd\u7136\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faHinFlow\u65b9\u6cd5\uff1a\u6536\u96c6\u5728\u7ebf\u8f68\u8ff9\uff0c\u540e\u89c1\u4e4b\u660e\u5730\u4ece\u5df2\u5b9e\u73b0\u7ed3\u679c\u4e2d\u6807\u6ce8\u76f8\u5e94\u9ad8\u7ea7\u76ee\u6807\uff0c\u805a\u5408\u8fd9\u4e9b\u91cd\u6807\u6ce8\u7ecf\u9a8c\u6765\u66f4\u65b0\u76ee\u6807\u6761\u4ef6\u6a21\u4eff\u7b56\u7565\uff0c\u4f7f\u75282D\u70b9\u6d41\u4f5c\u4e3a\u9ad8\u7ea7\u89c4\u5212\u5668\u3002", "result": "\u5728\u4eff\u771f\u548c\u7269\u7406\u4e16\u754c\u7684\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7840\u7b56\u7565\u5b9e\u73b02\u500d\u4ee5\u4e0a\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u4ece\u8de8\u5177\u8eab\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u7684\u89c4\u5212\u5668\u4e2d\u83b7\u53d6\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u6539\u8fdb\u4f4e\u7ea7\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u8fc1\u79fb\u7684\u673a\u5668\u4eba\u5b66\u4e60\u6f5c\u529b\uff0c\u80fd\u591f\u5229\u7528\u8de8\u5177\u8eab\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u7684\u89c4\u5212\u5668\u3002"}}
{"id": "2512.18682", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.18682", "abs": "https://arxiv.org/abs/2512.18682", "authors": ["Yuchen Li", "Handing Wang", "Bing Xue", "Mengjie Zhang", "Yaochu Jin"], "title": "Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design", "comment": null, "summary": "In the high-cost simulation-driven design domain, translating ambiguous design requirements into a mathematical optimization formulation is a bottleneck for optimizing product performance. This process is time-consuming and heavily reliant on expert knowledge. While large language models (LLMs) offer potential for automating this task, existing approaches either suffer from poor formalization that fails to accurately align with the design intent or rely on solver feedback for data filtering, which is unavailable due to the high simulation costs. To address this challenge, we propose APF, a framework for solver-independent, automated problem formulation via LLMs designed to automatically convert engineers' natural language requirements into executable optimization models. The core of this framework is an innovative pipeline for automatically generating high-quality data, which overcomes the difficulty of constructing suitable fine-tuning datasets in the absence of high-cost solver feedback with the help of data generation and test instance annotation. The generated high-quality dataset is used to perform supervised fine-tuning on LLMs, significantly enhancing their ability to generate accurate and executable optimization problem formulations. Experimental results on antenna design demonstrate that APF significantly outperforms the existing methods in both the accuracy of requirement formalization and the quality of resulting radiation efficiency curves in meeting the design goals.", "AI": {"tldr": "APF\u6846\u67b6\u5229\u7528LLM\u81ea\u52a8\u5c06\u81ea\u7136\u8bed\u8a00\u8bbe\u8ba1\u9700\u6c42\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u65e0\u9ad8\u6210\u672c\u6c42\u89e3\u5668\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u51c6\u786e\u7684\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\u5316\u3002", "motivation": "\u5728\u9ad8\u6210\u672c\u4eff\u771f\u9a71\u52a8\u8bbe\u8ba1\u9886\u57df\uff0c\u5c06\u6a21\u7cca\u7684\u8bbe\u8ba1\u9700\u6c42\u8f6c\u5316\u4e3a\u6570\u5b66\u4f18\u5316\u516c\u5f0f\u662f\u4f18\u5316\u4ea7\u54c1\u6027\u80fd\u7684\u74f6\u9888\u3002\u8fd9\u4e00\u8fc7\u7a0b\u8017\u65f6\u4e14\u4e25\u91cd\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u3002\u73b0\u6709LLM\u65b9\u6cd5\u8981\u4e48\u5f62\u5f0f\u5316\u6548\u679c\u5dee\uff0c\u8981\u4e48\u4f9d\u8d56\u6c42\u89e3\u5668\u53cd\u9988\u8fdb\u884c\u6570\u636e\u8fc7\u6ee4\uff0c\u800c\u9ad8\u4eff\u771f\u6210\u672c\u4f7f\u5f97\u8fd9\u79cd\u53cd\u9988\u4e0d\u53ef\u7528\u3002", "method": "\u63d0\u51faAPF\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u521b\u65b0\u6d41\u7a0b\uff0c\u5728\u65e0\u9ad8\u6210\u672c\u6c42\u89e3\u5668\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u6570\u636e\u751f\u6210\u548c\u6d4b\u8bd5\u5b9e\u4f8b\u6807\u6ce8\u6784\u5efa\u5408\u9002\u7684\u5fae\u8c03\u6570\u636e\u96c6\u3002\u4f7f\u7528\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5bf9LLM\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u5176\u751f\u6210\u51c6\u786e\u4e14\u53ef\u6267\u884c\u7684\u4f18\u5316\u95ee\u9898\u516c\u5f0f\u7684\u80fd\u529b\u3002", "result": "\u5728\u5929\u7ebf\u8bbe\u8ba1\u5b9e\u9a8c\u4e2d\uff0cAPF\u5728\u9700\u6c42\u5f62\u5f0f\u5316\u7684\u51c6\u786e\u6027\u548c\u6ee1\u8db3\u8bbe\u8ba1\u76ee\u6807\u7684\u8f90\u5c04\u6548\u7387\u66f2\u7ebf\u8d28\u91cf\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "APF\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u6210\u672c\u4eff\u771f\u9a71\u52a8\u8bbe\u8ba1\u4e2d\u81ea\u52a8\u5316\u95ee\u9898\u5f62\u5f0f\u5316\u7684\u6311\u6218\uff0c\u901a\u8fc7LLM\u5b9e\u73b0\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u5230\u53ef\u6267\u884c\u4f18\u5316\u6a21\u578b\u7684\u51c6\u786e\u8f6c\u6362\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u6c42\u89e3\u5668\u53cd\u9988\u3002"}}
{"id": "2512.19270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19270", "abs": "https://arxiv.org/abs/2512.19270", "authors": ["Zhaoyang Liu", "Weitao Zhou", "Junze Wen", "Cheng Jing", "Qian Cheng", "Kun Jiang", "Diange Yang"], "title": "Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization", "comment": "7 pages, 4 figures", "summary": "Collecting large-scale naturalistic driving data is essential for training robust autonomous driving planners. However, real-world datasets often contain a substantial amount of repetitive and low-value samples, which lead to excessive storage costs and bring limited benefits to policy learning. To address this issue, we propose an information-theoretic data pruning method that effectively reduces the training data volume without compromising model performance. Our approach evaluates the trajectory distribution information entropy of driving data and iteratively selects high-value samples that preserve the statistical characteristics of the original dataset in a model-agnostic manner. From a theoretical perspective, we show that maximizing trajectory entropy effectively constrains the Kullback-Leibler divergence between the pruned subset and the original data distribution, thereby maintaining generalization ability. Comprehensive experiments on the NuPlan benchmark with a large-scale imitation learning framework demonstrate that the proposed method can reduce the dataset size by up to 40% while maintaining closed-loop performance. This work provides a lightweight and theoretically grounded approach for scalable data management and efficient policy learning in autonomous driving systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u8f68\u8ff9\u71b5\u9009\u62e9\u9ad8\u4ef7\u503c\u6837\u672c\uff0c\u51cf\u5c1140%\u6570\u636e\u91cf\u800c\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u5927\u91cf\u91cd\u590d\u548c\u4f4e\u4ef7\u503c\u6837\u672c\uff0c\u5bfc\u81f4\u5b58\u50a8\u6210\u672c\u8fc7\u9ad8\u4e14\u5bf9\u7b56\u7565\u5b66\u4e60\u5e2e\u52a9\u6709\u9650\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u7ba1\u7406\u65b9\u6cd5", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6570\u636e\u526a\u679d\u65b9\u6cd5\uff0c\u8bc4\u4f30\u9a7e\u9a76\u6570\u636e\u7684\u8f68\u8ff9\u5206\u5e03\u4fe1\u606f\u71b5\uff0c\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u8fed\u4ee3\u9009\u62e9\u9ad8\u4ef7\u503c\u6837\u672c\uff0c\u4fdd\u6301\u539f\u59cb\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u6027", "result": "\u5728NuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u51cf\u5c1140%\u6570\u636e\u96c6\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u95ed\u73af\u6027\u80fd\u4e0d\u53d8", "conclusion": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u7406\u8bba\u57fa\u7840\u7684\u6269\u5c55\u6570\u636e\u7ba1\u7406\u548c\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5"}}
{"id": "2512.18746", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18746", "abs": "https://arxiv.org/abs/2512.18746", "authors": ["Guibin Zhang", "Haotian Ren", "Chong Zhan", "Zhenhong Zhou", "Junhao Wang", "He Zhu", "Wangchunshu Zhou", "Shuicheng Yan"], "title": "MemEvolve: Meta-Evolution of Agent Memory Systems", "comment": null, "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.", "AI": {"tldr": "MemEvolve\uff1a\u4e00\u4e2a\u5143\u8fdb\u5316\u6846\u67b6\uff0c\u8054\u5408\u8fdb\u5316\u4ee3\u7406\u7684\u7ecf\u9a8c\u77e5\u8bc6\u548c\u8bb0\u5fc6\u67b6\u6784\uff0c\u4f7f\u4ee3\u7406\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u79ef\u7d2f\u7ecf\u9a8c\uff0c\u8fd8\u80fd\u9010\u6b65\u6539\u8fdb\u5b66\u4e60\u65b9\u5f0f\u3002EvolveLab\u4f5c\u4e3a\u7edf\u4e00\u7684\u81ea\u8fdb\u5316\u8bb0\u5fc6\u4ee3\u7801\u5e93\uff0c\u5c0612\u4e2a\u4ee3\u8868\u6027\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u70bc\u4e3a\u6a21\u5757\u5316\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u9759\u6001\u8bb0\u5fc6\u67b6\u6784\u6765\u5b58\u50a8\u8f68\u8ff9\u3001\u63d0\u70bc\u7ecf\u9a8c\u548c\u5408\u6210\u53ef\u91cd\u7528\u5de5\u5177\uff0c\u8fd9\u79cd\u8303\u5f0f\u53d7\u9650\u4e8e\u8bb0\u5fc6\u7cfb\u7edf\u672c\u8eab\u7684\u9759\u6001\u6027\uff1a\u8bb0\u5fc6\u67b6\u6784\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u4e0b\u6587\u8fdb\u884c\u5143\u9002\u5e94\u3002", "method": "\u63d0\u51faMemEvolve\u6846\u67b6\uff0c\u8054\u5408\u8fdb\u5316\u4ee3\u7406\u7684\u7ecf\u9a8c\u77e5\u8bc6\u548c\u8bb0\u5fc6\u67b6\u6784\uff1b\u5f15\u5165EvolveLab\u4ee3\u7801\u5e93\uff0c\u5c0612\u4e2a\u4ee3\u8868\u6027\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u70bc\u4e3a\u6a21\u5757\u5316\u8bbe\u8ba1\u7a7a\u95f4\uff08\u7f16\u7801\u3001\u5b58\u50a8\u3001\u68c0\u7d22\u3001\u7ba1\u7406\uff09\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u5b9e\u73b0\u57fa\u5e95\u548c\u516c\u5e73\u5b9e\u9a8c\u5e73\u53f0\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemEvolve\u5b9e\u73b0\u4e86\uff1a(1) \u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c06SmolAgent\u548cFlash-Searcher\u7b49\u6846\u67b6\u63d0\u5347\u9ad8\u8fbe17.06%\uff1b(2) \u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u548c\u8de8LLM\u6cdb\u5316\u80fd\u529b\uff0c\u8bbe\u8ba1\u7684\u8bb0\u5fc6\u67b6\u6784\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u3002", "conclusion": "MemEvolve\u901a\u8fc7\u5143\u8fdb\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\u7684\u9759\u6001\u6027\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u8bb0\u5fc6\u67b6\u6784\u4e0e\u7ecf\u9a8c\u77e5\u8bc6\u7684\u8054\u5408\u8fdb\u5316\uff0c\u4e3a\u672a\u6765\u81ea\u8fdb\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f00\u653e\u7684\u7814\u7a76\u57fa\u7840\u548c\u6807\u51c6\u5316\u5b9e\u73b0\u6846\u67b6\u3002"}}
{"id": "2512.19289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19289", "abs": "https://arxiv.org/abs/2512.19289", "authors": ["Longxiang Shao", "Ulrich Dahmen", "Juergen Rossmann"], "title": "Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems", "comment": "Accepted at the 10th MHI-Fachkolloquium", "summary": "Rigid body dynamics simulators are important tools for the design, analysis and optimization of mechanical systems in a variety of technical and scientific applications. This study examines four different simulation environments (Adams, Simscape, OpenModelica, and VEROSIM), focusing in particular on the comparison of the modeling methods, the numerical solvers, and the treatment of numerical problems that arise especially in closed-loop kinematics (esp. redundant boundary conditions and static equilibrium problem). A novel and complex crane boom of a real forestry machine serves as a practical benchmark application example. The direct comparison of the different solution approaches in the examined simulation tools supports the user in selecting the most suitable tool for his application.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u591a\u4f53\u52a8\u529b\u5b66\u4eff\u771f\u73af\u5883\uff08Adams\u3001Simscape\u3001OpenModelica\u3001VEROSIM\uff09\uff0c\u91cd\u70b9\u5173\u6ce8\u5efa\u6a21\u65b9\u6cd5\u3001\u6570\u503c\u6c42\u89e3\u5668\u4ee5\u53ca\u5bf9\u95ed\u73af\u8fd0\u52a8\u5b66\u4e2d\u6570\u503c\u95ee\u9898\u7684\u5904\u7406\uff0c\u4ee5\u771f\u5b9e\u6797\u4e1a\u673a\u68b0\u7684\u8d77\u91cd\u673a\u81c2\u4f5c\u4e3a\u57fa\u51c6\u6848\u4f8b\u3002", "motivation": "\u591a\u4f53\u52a8\u529b\u5b66\u4eff\u771f\u5668\u5728\u673a\u68b0\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u5206\u6790\u548c\u4f18\u5316\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0d\u540c\u4eff\u771f\u5de5\u5177\u5728\u5efa\u6a21\u65b9\u6cd5\u3001\u6570\u503c\u6c42\u89e3\u548c\u95ee\u9898\u5904\u7406\u65b9\u9762\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u4ee5\u5e2e\u52a9\u7528\u6237\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5bf9\u6bd4\u5206\u6790\u65b9\u6cd5\uff0c\u4ee5\u771f\u5b9e\u6797\u4e1a\u673a\u68b0\u7684\u590d\u6742\u8d77\u91cd\u673a\u81c2\u4f5c\u4e3a\u57fa\u51c6\u5e94\u7528\u6848\u4f8b\uff0c\u7cfb\u7edf\u6bd4\u8f83\u56db\u79cd\u4eff\u771f\u73af\u5883\uff08Adams\u3001Simscape\u3001OpenModelica\u3001VEROSIM\uff09\u5728\u5efa\u6a21\u65b9\u6cd5\u3001\u6570\u503c\u6c42\u89e3\u5668\u4ee5\u53ca\u5bf9\u95ed\u73af\u8fd0\u52a8\u5b66\u4e2d\u5197\u4f59\u8fb9\u754c\u6761\u4ef6\u548c\u9759\u529b\u5e73\u8861\u95ee\u9898\u7b49\u6570\u503c\u95ee\u9898\u7684\u5904\u7406\u65b9\u5f0f\u3002", "result": "\u901a\u8fc7\u76f4\u63a5\u6bd4\u8f83\u4e0d\u540c\u4eff\u771f\u5de5\u5177\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\u65b9\u6cd5\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u5404\u5de5\u5177\u5728\u5904\u7406\u590d\u6742\u95ed\u73af\u8fd0\u52a8\u5b66\u95ee\u9898\u65f6\u7684\u7279\u70b9\u548c\u5dee\u5f02\uff0c\u4e3a\u7528\u6237\u9009\u62e9\u6700\u9002\u5408\u5176\u5e94\u7528\u7684\u4eff\u771f\u5de5\u5177\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u4e0d\u540c\u591a\u4f53\u52a8\u529b\u5b66\u4eff\u771f\u5de5\u5177\u5728\u5904\u7406\u590d\u6742\u673a\u68b0\u7cfb\u7edf\u65f6\u5404\u6709\u7279\u70b9\uff0c\u7279\u522b\u662f\u5bf9\u95ed\u73af\u8fd0\u52a8\u5b66\u4e2d\u6570\u503c\u95ee\u9898\u7684\u5904\u7406\u65b9\u5f0f\u4e0d\u540c\uff0c\u7528\u6237\u5e94\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u9009\u62e9\u6700\u5408\u9002\u7684\u4eff\u771f\u73af\u5883\u3002"}}
{"id": "2512.18779", "categories": ["cs.CL", "physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2512.18779", "abs": "https://arxiv.org/abs/2512.18779", "authors": ["Thorsten Hellert", "Nikolay Agladze", "Alex Giovannone", "Jan Jug", "Frank Mayet", "Mark Sherwin", "Antonin Sulc", "Chris Tennant"], "title": "From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure", "comment": null, "summary": "Modern experimental platforms such as particle accelerators, fusion devices, telescopes, and industrial process control systems expose tens to hundreds of thousands of control and diagnostic channels accumulated over decades of evolution. Operators and AI systems rely on informal expert knowledge, inconsistent naming conventions, and fragmented documentation to locate signals for monitoring, troubleshooting, and automated control, creating a persistent bottleneck for reliability, scalability, and language-model-driven interfaces. We formalize semantic channel finding-mapping natural-language intent to concrete control-system signals-as a general problem in complex experimental infrastructure, and introduce a four-paradigm framework to guide architecture selection across facility-specific data regimes. The paradigms span (i) direct in-context lookup over curated channel dictionaries, (ii) constrained hierarchical navigation through structured trees, (iii) interactive agent exploration using iterative reasoning and tool-based database queries, and (iv) ontology-grounded semantic search that decouples channel meaning from facility-specific naming conventions. We demonstrate each paradigm through proof-of-concept implementations at four operational facilities spanning two orders of magnitude in scale-from compact free-electron lasers to large synchrotron light sources-and diverse control-system architectures, from clean hierarchies to legacy environments. These implementations achieve 90-97% accuracy on expert-curated operational queries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bed\u4e49\u901a\u9053\u67e5\u627e\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u6620\u5c04\u5230\u590d\u6742\u5b9e\u9a8c\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u63a7\u5236\u4fe1\u53f7\uff0c\u5305\u542b\u56db\u79cd\u8303\u5f0f\uff0c\u5728\u56db\u4e2a\u5b9e\u9645\u8bbe\u65bd\u4e2d\u5b9e\u73b090-97%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u4ee3\u5b9e\u9a8c\u5e73\u53f0\uff08\u7c92\u5b50\u52a0\u901f\u5668\u3001\u805a\u53d8\u88c5\u7f6e\u7b49\uff09\u6709\u6570\u4e07\u5230\u6570\u5341\u4e07\u63a7\u5236\u901a\u9053\uff0c\u64cd\u4f5c\u5458\u548cAI\u7cfb\u7edf\u4f9d\u8d56\u975e\u6b63\u5f0f\u4e13\u5bb6\u77e5\u8bc6\u3001\u4e0d\u4e00\u81f4\u547d\u540d\u89c4\u8303\u548c\u788e\u7247\u5316\u6587\u6863\u6765\u5b9a\u4f4d\u4fe1\u53f7\uff0c\u8fd9\u6210\u4e3a\u53ef\u9760\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u63a5\u53e3\u7684\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u56db\u8303\u5f0f\u6846\u67b6\uff1a1) \u57fa\u4e8e\u7b56\u5212\u901a\u9053\u8bcd\u5178\u7684\u76f4\u63a5\u4e0a\u4e0b\u6587\u67e5\u627e\uff1b2) \u7ed3\u6784\u5316\u6811\u7684\u7ea6\u675f\u5c42\u6b21\u5bfc\u822a\uff1b3) \u4f7f\u7528\u8fed\u4ee3\u63a8\u7406\u548c\u5de5\u5177\u67e5\u8be2\u7684\u4ea4\u4e92\u5f0f\u4ee3\u7406\u63a2\u7d22\uff1b4) \u57fa\u4e8e\u672c\u4f53\u7684\u8bed\u4e49\u641c\u7d22\uff0c\u5c06\u901a\u9053\u542b\u4e49\u4e0e\u8bbe\u65bd\u7279\u5b9a\u547d\u540d\u89c4\u8303\u89e3\u8026\u3002\u5728\u56db\u4e2a\u5b9e\u9645\u8bbe\u65bd\u4e2d\u5b9e\u73b0\u6982\u5ff5\u9a8c\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u8fd0\u884c\u8bbe\u65bd\uff08\u4ece\u7d27\u51d1\u578b\u81ea\u7531\u7535\u5b50\u6fc0\u5149\u5668\u5230\u5927\u578b\u540c\u6b65\u8f90\u5c04\u5149\u6e90\uff09\u4e2d\u5b9e\u73b0\uff0c\u8986\u76d6\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u89c4\u6a21\uff0c\u5728\u4e13\u5bb6\u7b56\u5212\u7684\u64cd\u4f5c\u67e5\u8be2\u4e0a\u8fbe\u523090-97%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8bed\u4e49\u901a\u9053\u67e5\u627e\u662f\u590d\u6742\u5b9e\u9a8c\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u56db\u8303\u5f0f\u6846\u67b6\u4e3a\u4e0d\u540c\u6570\u636e\u4f53\u5236\u63d0\u4f9b\u4e86\u67b6\u6784\u9009\u62e9\u6307\u5bfc\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4fe1\u53f7\u5b9a\u4f4d\u74f6\u9888\uff0c\u652f\u6301\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u64cd\u4f5c\u548cAI\u63a5\u53e3\u3002"}}
{"id": "2512.19347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19347", "abs": "https://arxiv.org/abs/2512.19347", "authors": ["Han Fang", "Yize Huang", "Yuheng Zhao", "Paul Weng", "Xiao Li", "Yutong Ban"], "title": "OMP: One-step Meanflow Policy with Directional Alignment", "comment": null, "summary": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6539\u8fdb\u7684MeanFlow\u7b56\u7565\uff0c\u901a\u8fc7\u4f59\u5f26\u635f\u5931\u5bf9\u9f50\u901f\u5ea6\u65b9\u5411\u548cDDE\u4f18\u5316JVP\u7b97\u5b50\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u8f68\u8ff9\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u4e3b\u6d41\u751f\u6210\u7b56\u7565\u6846\u67b6\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u6d41\u5f0f\u65b9\u6cd5\u67b6\u6784\u590d\u6742\u7684\u95ee\u9898\u3002MeanFlow\u867d\u7136\u5b9e\u73b0\u5355\u6b65\u63a8\u7406\u4e14\u4f18\u4e8eFlowPolicy\uff0c\u4f46\u7f3a\u4e4f\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u539f\u56e0\u662f\u5176\u5206\u6563\u635f\u5931\u4e2d\u7684\u56fa\u5b9a\u6e29\u5ea6\u8d85\u53c2\u6570\u548c\u9884\u6d4b-\u771f\u5b9e\u5e73\u5747\u901f\u5ea6\u4e0d\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684MeanFlow\u7b56\u7565\uff1a1\uff09\u5f15\u5165\u8f7b\u91cf\u7ea7\u4f59\u5f26\u635f\u5931\u6765\u5bf9\u9f50\u901f\u5ea6\u65b9\u5411\uff1b2\uff09\u4f7f\u7528\u5fae\u5206\u63a8\u5bfc\u65b9\u7a0b\uff08DDE\uff09\u4f18\u5316\u96c5\u53ef\u6bd4-\u5411\u91cf\u79ef\uff08JVP\uff09\u7b97\u5b50\u3002", "result": "\u5728Adroit\u548cMeta-World\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u6210\u529f\u7387\u4e0a\u4f18\u4e8eMP1\u548cFlowPolicy\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684Meta-World\u4efb\u52a1\u4e2d\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u8f68\u8ff9\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u7cbe\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6539\u8fdbMeanFlow\u7b56\u7565\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u6cdb\u5316\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2512.18832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18832", "abs": "https://arxiv.org/abs/2512.18832", "authors": ["Yixia Li", "Hongru Wang", "Jiahao Qiu", "Zhenfei Yin", "Dongdong Zhang", "Cheng Qian", "Zeping Li", "Pony Ma", "Guanhua Chen", "Heng Ji", "Mengdi Wang"], "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?", "comment": null, "summary": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6587\u672c\u73af\u5883\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u6765\u63d0\u5347\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u7684\u6709\u6548\u6027\u548c\u6761\u4ef6\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u7ecf\u9a8c\u7684\u5f3a\u5316\u5b66\u4e60\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u4f46\u73b0\u5b9e\u73af\u5883\u4ecd\u7136\u662f\u975e\u81ea\u9002\u5e94\u7684\u3001\u8986\u76d6\u6709\u9650\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u3002\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u6a21\u62df\u7ecf\u9a8c\u53ef\u80fd\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695aLLM\u80fd\u5426\u53ef\u9760\u5730\u626e\u6f14\u8fd9\u4e00\u89d2\u8272\u4ee5\u53ca\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u80fd\u771f\u6b63\u6709\u76ca\u4e8e\u667a\u80fd\u4f53\u3002", "method": "\u5728\u6587\u672c\u73af\u5883\u4e2d\u5c06\u8bed\u8a00\u5efa\u6a21\u91cd\u65b0\u89e3\u91ca\u4e3a\u4ea4\u4e92\u4e0b\u7684\u4e0b\u4e00\u72b6\u6001\u9884\u6d4b\uff0c\u5f15\u5165\u4e09\u7ea7\u8bc4\u4f30\u6846\u67b6\uff1a\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3001\u667a\u80fd\u4f53\u6548\u7528\u3002\u5728\u4e94\u4e2a\u4ee3\u8868\u6027\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u5145\u5206\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\u80fd\u4fdd\u6301\u4e00\u81f4\u7684\u6f5c\u5728\u72b6\u6001\uff0c\u968f\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u53ef\u9884\u6d4b\u5730\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u52a8\u4f5c\u9a8c\u8bc1\u3001\u5408\u6210\u8f68\u8ff9\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u9884\u70ed\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u6536\u76ca\u5173\u952e\u53d6\u51b3\u4e8e\u884c\u4e3a\u8986\u76d6\u548c\u73af\u5883\u590d\u6742\u6027\uff0c\u660e\u786e\u4e86\u4e16\u754c\u6a21\u578b\u6709\u6548\u652f\u6301\u667a\u80fd\u4f53\u5b66\u4e60\u7684\u8fb9\u754c\u6761\u4ef6\u3002"}}
{"id": "2512.19390", "categories": ["cs.RO", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.19390", "abs": "https://arxiv.org/abs/2512.19390", "authors": ["Hongwei Fan", "Hang Dai", "Jiyao Zhang", "Jinzhou Li", "Qiyang Yan", "Yujie Zhao", "Mingju Gao", "Jinghang Wu", "Hao Tang", "Hao Dong"], "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation", "comment": null, "summary": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io", "AI": {"tldr": "TwinAligner\u662f\u4e00\u4e2aReal2Sim2Real\u7cfb\u7edf\uff0c\u901a\u8fc7\u89c6\u89c9\u5bf9\u9f50\u548c\u52a8\u6001\u5bf9\u9f50\u6a21\u5757\u89e3\u51b3\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u4eff\u771f\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u7b56\u7565\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u6b63\u671d\u7740\u6570\u636e\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u53d1\u5c55\uff0c\u4f46\u4f9d\u8d56\u6602\u8d35\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u9650\u5236\u4e86\u8fdb\u5c55\u3002\u4eff\u771f\u5668\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u963b\u788d\u4e86\u6709\u6548\u7684\u7b56\u7565\u8fc1\u79fb\u3002", "method": "\u63d0\u51faTwinAligner\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u89c6\u89c9\u5bf9\u9f50\u6a21\u5757\u901a\u8fc7SDF\u91cd\u5efa\u548c\u53ef\u7f16\u8f913DGS\u6e32\u67d3\u5b9e\u73b0\u50cf\u7d20\u7ea7\u5bf9\u9f50\uff1b2\uff09\u52a8\u6001\u5bf9\u9f50\u6a21\u5757\u901a\u8fc7\u8bc6\u522b\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u4e2d\u7684\u521a\u6027\u7269\u7406\u7279\u6027\u786e\u4fdd\u52a8\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793aTwinAligner\u5728\u89c6\u89c9\u548c\u52a8\u6001\u771f\u5b9e\u5230\u4eff\u771f\u5bf9\u9f50\u65b9\u9762\u5177\u6709\u5f3a\u5927\u80fd\u529b\u3002\u4eff\u771f\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u7b56\u7565\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u771f\u5b9e\u4e16\u754c\u4e0e\u4eff\u771f\u7b56\u7565\u6027\u80fd\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "TwinAligner\u901a\u8fc7\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6570\u636e\u6536\u96c6\u548c\u5efa\u7acb\u53ef\u4fe1\u7684\u8fed\u4ee3\u5faa\u73af\uff0c\u52a0\u901f\u4e86\u673a\u5668\u4eba\u7b97\u6cd5\u5f00\u53d1\uff0c\u6709\u671b\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u53d1\u5c55\u3002"}}
{"id": "2512.18834", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18834", "abs": "https://arxiv.org/abs/2512.18834", "authors": ["Sultan Alrashed", "Francesco Orabona"], "title": "AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus", "comment": "Initial version, without pretraining experiments", "summary": "We present AraMix, a deduplicated Arabic pretraining corpus containing approximately 178 billion tokens across 179 million documents. Rather than scraping the web again, AraMix demonstrates that substantial value lies in systematically reusing and curating existing pretraining datasets: we combine seven publicly available Arabic web datasets, apply quality filtering designed specifically for Arabic text to re-filter some datasets, and perform cross-dataset deduplication, both MinHash and sentence-level. This approach reveals that nearly 60% of tokens across these independently collected corpora are duplicates, redundancy that any new scraping efforts will reproduce. Our work suggests that for lower resource languages, investment in curation pipelines for existing data yields greater returns than additional web crawls, an approach that allowed us to curate the largest heavily filtered publicly available Arabic pretraining corpus.", "AI": {"tldr": "AraMix\u662f\u4e00\u4e2a\u53bb\u91cd\u540e\u7684\u963f\u62c9\u4f2f\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5305\u542b\u7ea61780\u4ebftoken\u548c1.79\u4ebf\u6587\u6863\uff0c\u901a\u8fc7\u6574\u5408\u548c\u4f18\u5316\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u6784\u5efa\u800c\u6210", "motivation": "\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4e0e\u5176\u91cd\u65b0\u722c\u53d6\u7f51\u7edc\u6570\u636e\uff0c\u4e0d\u5982\u7cfb\u7edf\u6027\u5730\u91cd\u7528\u548c\u4f18\u5316\u73b0\u6709\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u56e0\u4e3a\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59", "method": "\u6574\u54087\u4e2a\u516c\u5f00\u963f\u62c9\u4f2f\u8bed\u7f51\u7edc\u6570\u636e\u96c6\uff0c\u5e94\u7528\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u8bbe\u8ba1\u7684\u8d28\u91cf\u8fc7\u6ee4\uff0c\u5e76\u8fdb\u884c\u8de8\u6570\u636e\u96c6\u53bb\u91cd\uff08\u5305\u62ecMinHash\u548c\u53e5\u5b50\u7ea7\u53bb\u91cd\uff09", "result": "\u53d1\u73b0\u8fd9\u4e9b\u72ec\u7acb\u6536\u96c6\u7684\u8bed\u6599\u5e93\u4e2d\u8fd160%\u7684token\u662f\u91cd\u590d\u7684\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u6700\u5927\u7684\u7ecf\u8fc7\u4e25\u683c\u8fc7\u6ee4\u7684\u516c\u5f00\u963f\u62c9\u4f2f\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u5e93", "conclusion": "\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u6295\u8d44\u4e8e\u73b0\u6709\u6570\u636e\u7684\u4f18\u5316\u6d41\u7a0b\u6bd4\u989d\u5916\u8fdb\u884c\u7f51\u7edc\u722c\u53d6\u66f4\u6709\u4ef7\u503c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u73b0\u6709\u8d44\u6e90\u5e76\u907f\u514d\u5197\u4f59"}}
{"id": "2512.19402", "categories": ["cs.RO", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.19402", "abs": "https://arxiv.org/abs/2512.19402", "authors": ["Yujie Zhao", "Hongwei Fan", "Di Chen", "Shengcong Chen", "Liliang Chen", "Xiaoqi Li", "Guanghui Ren", "Hao Dong"], "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface", "comment": null, "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.", "AI": {"tldr": "Real2Edit2Real\uff1a\u901a\u8fc73D\u7f16\u8f91\u548c\u89c6\u9891\u751f\u6210\uff0c\u4ece\u5c11\u91cf\u771f\u5b9e\u6f14\u793a\u4e2d\u5408\u6210\u5927\u91cf\u7a7a\u95f4\u589e\u5f3a\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\uff0c\u5b9e\u73b010-50\u500d\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9700\u8981\u5927\u89c4\u6a21\u591a\u6837\u5316\u6f14\u793a\u6570\u636e\uff0c\u4f46\u6536\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u6cdb\u5316\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u3002\u9700\u8981\u51cf\u5c11\u91cd\u590d\u6570\u636e\u6536\u96c6\uff0c\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "method": "1) \u4ece\u591a\u89c6\u89d2RGB\u89c2\u6d4b\u91cd\u5efa\u573a\u666f\u51e0\u4f55\uff1b2) \u5728\u70b9\u4e91\u4e0a\u8fdb\u884c\u6df1\u5ea6\u53ef\u9760\u76843D\u7f16\u8f91\uff0c\u751f\u6210\u65b0\u64cd\u4f5c\u8f68\u8ff9\u5e76\u51e0\u4f55\u6821\u6b63\u673a\u5668\u4eba\u59ff\u6001\uff1b3) \u63d0\u51fa\u591a\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u6df1\u5ea6\u4e3a\u4e3b\u8981\u63a7\u5236\u4fe1\u53f7\uff0c\u7ed3\u5408\u52a8\u4f5c\u3001\u8fb9\u7f18\u548c\u5c04\u7ebf\u56fe\uff0c\u5408\u6210\u7a7a\u95f4\u589e\u5f3a\u7684\u591a\u89c6\u89d2\u64cd\u4f5c\u89c6\u9891\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u4ec5\u75281-5\u4e2a\u6e90\u6f14\u793a\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u6027\u80fd\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u752850\u4e2a\u771f\u5b9e\u6f14\u793a\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u6570\u636e\u6548\u7387\u63d0\u534710-50\u500d\u3002\u9ad8\u5ea6\u548c\u7eb9\u7406\u7f16\u8f91\u5b9e\u9a8c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Real2Edit2Real\u6846\u67b6\u901a\u8fc73D\u7f16\u8f91\u548c\u89c6\u9891\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6570\u636e\u6548\u7387\uff0c\u6709\u671b\u6210\u4e3a\u7edf\u4e00\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u7a7a\u95f4\u6cdb\u5316\u548c\u4efb\u52a1\u6269\u5c55\u3002"}}
{"id": "2512.18841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18841", "abs": "https://arxiv.org/abs/2512.18841", "authors": ["Tung Duong Ta", "Tim Oates"], "title": "MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models", "comment": null, "summary": "Despite advances in mathematical reasoning capabilities, Large Language Models (LLMs) still struggle with calculation verification when using established prompting techniques. We present MDToC (Metacognitive Dynamic Tree of Concepts), a three-phase approach that constructs a concept tree, develops accuracy-verified calculations for each concept, and employs majority voting to evaluate competing solutions. Evaluations across CHAMP, MATH, and Game-of-24 benchmarks demonstrate our MDToC's effectiveness, with GPT-4-Turbo achieving 58.1\\% on CHAMP, 86.6\\% on MATH, and 85\\% on Game-of-24 - outperforming GoT by 5\\%, 5.4\\%, and 4\\% on all these tasks, respectively, without hand-engineered hints. MDToC consistently surpasses existing prompting methods across all backbone models, yielding improvements of up to 7.6\\% over ToT and 6.2\\% over GoT, establishing metacognitive calculation verification as a promising direction for enhanced mathematical reasoning.", "AI": {"tldr": "MDToC\uff08\u5143\u8ba4\u77e5\u52a8\u6001\u6982\u5ff5\u6811\uff09\u901a\u8fc7\u6784\u5efa\u6982\u5ff5\u6811\u3001\u9a8c\u8bc1\u8ba1\u7b97\u548c\u591a\u6570\u6295\u7968\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u9a8c\u8bc1\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u5728\u4f7f\u7528\u73b0\u6709\u63d0\u793a\u6280\u672f\u65f6\u4ecd\u7136\u96be\u4ee5\u6709\u6548\u9a8c\u8bc1\u8ba1\u7b97\u51c6\u786e\u6027\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u9a8c\u8bc1\u673a\u5236\u3002", "method": "MDToC\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u6784\u5efa\u6982\u5ff5\u6811\uff1b2) \u4e3a\u6bcf\u4e2a\u6982\u5ff5\u5f00\u53d1\u7ecf\u8fc7\u51c6\u786e\u6027\u9a8c\u8bc1\u7684\u8ba1\u7b97\uff1b3) \u4f7f\u7528\u591a\u6570\u6295\u7968\u673a\u5236\u8bc4\u4f30\u7ade\u4e89\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728CHAMP\u3001MATH\u548cGame-of-24\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGPT-4-Turbo\u5206\u522b\u8fbe\u523058.1%\u300186.6%\u548c85%\u7684\u51c6\u786e\u7387\uff0c\u6bd4GoT\u5206\u522b\u63d0\u53475%\u30015.4%\u548c4%\uff0c\u5728\u6240\u6709\u9aa8\u5e72\u6a21\u578b\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u3002", "conclusion": "\u5143\u8ba4\u77e5\u8ba1\u7b97\u9a8c\u8bc1\u662f\u589e\u5f3a\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\uff0cMDToC\u65b9\u6cd5\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u63d0\u793a\u5373\u53ef\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u9a8c\u8bc1\u6027\u80fd\u3002"}}
{"id": "2512.19453", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19453", "abs": "https://arxiv.org/abs/2512.19453", "authors": ["Zhenglong Guo", "Yiming Zhao", "Feng Jiang", "Heng Jin", "Zongbao Feng", "Jianbin Zhou", "Siyuan Xu"], "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation", "comment": "8 pages, 10 figures, This work was completed in December 2024", "summary": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.", "AI": {"tldr": "\u63d0\u51faMaP-AVR\u89c4\u5212\u5668\uff0c\u7528\u5143\u52a8\u4f5c\u62bd\u8c61\u66ff\u4ee3\u4eba\u7c7b\u4e2d\u5fc3\u6982\u5ff5\uff0c\u7ed3\u5408RAG\u6280\u672f\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u80fd\u529b", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u901a\u8fc7\u5fae\u8c03\u6216\u601d\u7ef4\u94fe\u63d0\u793a\u589e\u5f3aLLM/VLM\u7684\u4efb\u52a1\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5ffd\u7565\u4e86\u89c4\u5212\u6280\u80fd\u96c6\u5b9a\u4e49\u7684\u91cd\u8981\u6027\u3002\u4e3a\u5904\u7406\u65e5\u5e38\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u6280\u80fd\u96c6\u9700\u8981\u5177\u5907\u9ad8\u5ea6\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5143\u52a8\u4f5c\u62bd\u8c61\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u5143\u52a8\u4f5c\u5305\u542b{\u79fb\u52a8/\u65cb\u8f6c\u3001\u672b\u7aef\u6267\u884c\u5668\u72b6\u6001\u53d8\u5316\u3001\u4e0e\u73af\u5883\u5173\u7cfb}\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u66ff\u4ee3\u4eba\u7c7b\u4e2d\u5fc3\u6982\u5ff5\u3002\u7ed3\u5408RAG\u6280\u672f\uff0c\u5229\u7528\u4eba\u7c7b\u6807\u6ce8\u7684\u89c4\u5212\u6f14\u793a\u6570\u636e\u5e93\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u7cfb\u7edf\u5b8c\u6210\u4efb\u52a1\u540e\u6570\u636e\u5e93\u81ea\u589e\u5f3a\u3002", "result": "\u5728GPT-4o\u4f5c\u4e3a\u9884\u8bad\u7ec3LLM/VLM\u6a21\u578b\u3001OmniGibson\u4f5c\u4e3a\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u6027\u80fd\u3002", "conclusion": "\u5143\u52a8\u4f5c\u96c6\u53ca\u5176\u4e0eRAG\u7684\u96c6\u6210\u662fMaP-AVR\u89c4\u5212\u5668\u7684\u4e24\u5927\u521b\u65b0\u8d21\u732e\uff0c\u4f7f\u89c4\u5212\u7ed3\u679c\u4e0e\u673a\u5668\u4eba\u56fa\u6709\u529f\u80fd\u65e0\u7f1d\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2512.18859", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18859", "abs": "https://arxiv.org/abs/2512.18859", "authors": ["Antonio San Martin"], "title": "Toward Human-Centered AI-Assisted Terminology Work", "comment": null, "summary": "The rapid diffusion of generative artificial intelligence is transforming terminology work. While this technology promises gains in efficiency, its unstructured adoption risks weakening professional autonomy, amplifying bias, and eroding linguistic and conceptual diversity. This paper argues that a human-centered approach to artificial intelligence has become a necessity for terminology work. Building on research in artificial intelligence and translation studies, it proposes a human-centered framework that conceptualizes artificial intelligence as a means of amplifying the terminologist's capabilities, rather than replacing them. The framework is organized around three interrelated dimensions: the augmented terminologist, ethical AI, and human-centered design. Together, these dimensions emphasize the compatibility of high automation with strong human control, the central role of terminologists in bias mitigation, and the importance of designing AI tools and workflows around the needs, values, and well-being of the terminologist. The paper concludes by stressing that current choices in AI adoption will shape not only terminological practice, but also the preservation of accuracy, adequacy, and diversity in terminology and specialized knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u5c06AI\u89c6\u4e3a\u589e\u5f3a\u672f\u8bed\u5b66\u5bb6\u80fd\u529b\u7684\u5de5\u5177\u800c\u975e\u66ff\u4ee3\u54c1\uff0c\u5f3a\u8c03\u5728\u4fdd\u6301\u9ad8\u81ea\u52a8\u5316\u7684\u540c\u65f6\u786e\u4fdd\u4eba\u7c7b\u63a7\u5236\uff0c\u4ee5\u7ef4\u62a4\u672f\u8bed\u5de5\u4f5c\u7684\u4e13\u4e1a\u81ea\u4e3b\u6027\u3001\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u6269\u6563\u6b63\u5728\u6539\u53d8\u672f\u8bed\u5de5\u4f5c\u3002\u867d\u7136\u8fd9\u9879\u6280\u672f\u6709\u671b\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5176\u65e0\u7ed3\u6784\u5316\u7684\u91c7\u7528\u53ef\u80fd\u4f1a\u524a\u5f31\u4e13\u4e1a\u81ea\u4e3b\u6027\u3001\u653e\u5927\u504f\u89c1\u5e76\u4fb5\u8680\u8bed\u8a00\u548c\u6982\u5ff5\u7684\u591a\u6837\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5efa\u7acb\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u6765\u4fdd\u62a4\u672f\u8bed\u5de5\u4f5c\u7684\u8d28\u91cf\u3002", "method": "\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u548c\u7ffb\u8bd1\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u56f4\u7ed5\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u7ef4\u5ea6\u7ec4\u7ec7\uff1a\u589e\u5f3a\u578b\u672f\u8bed\u5b66\u5bb6\u3001\u4f26\u7406\u4eba\u5de5\u667a\u80fd\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u3002\u8be5\u6846\u67b6\u5c06AI\u6982\u5ff5\u5316\u4e3a\u589e\u5f3a\u672f\u8bed\u5b66\u5bb6\u80fd\u529b\u7684\u624b\u6bb5\uff0c\u800c\u975e\u66ff\u4ee3\u54c1\u3002", "result": "\u8be5\u6846\u67b6\u5f3a\u8c03\u9ad8\u81ea\u52a8\u5316\u4e0e\u5f3a\u4eba\u7c7b\u63a7\u5236\u7684\u517c\u5bb9\u6027\uff0c\u672f\u8bed\u5b66\u5bb6\u5728\u504f\u89c1\u7f13\u89e3\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u4ee5\u53ca\u56f4\u7ed5\u672f\u8bed\u5b66\u5bb6\u7684\u9700\u6c42\u3001\u4ef7\u503c\u89c2\u548c\u798f\u7949\u8bbe\u8ba1AI\u5de5\u5177\u548c\u5de5\u4f5c\u6d41\u7a0b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u91c7\u7528\u7684\u9009\u62e9\u4e0d\u4ec5\u5c06\u5851\u9020\u672f\u8bed\u5b9e\u8df5\uff0c\u8fd8\u5c06\u5f71\u54cd\u672f\u8bed\u548c\u4e13\u4e1a\u77e5\u8bc6\u4e2d\u51c6\u786e\u6027\u3001\u9002\u5f53\u6027\u548c\u591a\u6837\u6027\u7684\u4fdd\u62a4\u3002\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u5bf9\u4e8e\u7ef4\u62a4\u672f\u8bed\u5de5\u4f5c\u7684\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2512.19562", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19562", "abs": "https://arxiv.org/abs/2512.19562", "authors": ["Martin Sedlacek", "Pavlo Yefanov", "Georgy Ponimatkin", "Jai Bardhan", "Simon Pilc", "Mederic Fourmy", "Evangelos Kazakos", "Cees G. M. Snoek", "Josef Sivic", "Vladimir Petrik"], "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation", "comment": "9 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the \u03c0_{0}, \u03c0_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm", "AI": {"tldr": "REALM\u662f\u4e00\u4e2a\u65b0\u7684\u4eff\u771f\u73af\u5883\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u89c6\u89c9\u548c\u5bf9\u9f50\u7684\u673a\u5668\u4eba\u63a7\u5236\u6765\u5efa\u7acb\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e16\u754c\u6027\u80fd\u4e4b\u95f4\u7684\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u8bad\u7ec3\u73af\u5883\u4e4b\u5916\u6cdb\u5316\u80fd\u529b\u96be\u4ee5\u8bc4\u4f30\uff0c\u73b0\u5b9e\u4e16\u754c\u8bc4\u4f30\u65e2\u56f0\u96be\u53c8\u6602\u8d35\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u4eff\u771f\u73af\u5883\u6765\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5f00\u53d1\u4e86REALM\u4eff\u771f\u73af\u5883\uff0c\u5305\u542b15\u79cd\u6270\u52a8\u56e0\u7d20\u30017\u79cd\u64cd\u4f5c\u6280\u80fd\u548c\u8d85\u8fc73,500\u4e2a\u7269\u4f53\uff0c\u5efa\u7acb\u4e86\u4e24\u4e2a\u4efb\u52a1\u96c6\u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u03c0\u2080\u3001\u03c0\u2080-FAST\u548cGR00T N1.5\u7b49VLA\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793aVLA\u6a21\u578b\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u4eff\u771f\u73af\u5883\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u6709\u6548\u4ee3\u7406\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u63a2\u6d4b\u548c\u91cf\u5316VLA\u6a21\u578b\u7684\u5f31\u70b9\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "REALM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30VLA\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u5f31\u70b9\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u5e76\u8bc1\u5b9e\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e16\u754c\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002"}}
{"id": "2512.18880", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.18880", "abs": "https://arxiv.org/abs/2512.18880", "authors": ["Ming Li", "Han Chen", "Yunze Xiao", "Jian Chen", "Hong Jiao", "Tianyi Zhou"], "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction", "comment": null, "summary": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.", "AI": {"tldr": "LLMs\u5728\u8bc4\u4f30\u9898\u76ee\u96be\u5ea6\u65f6\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u5347\u53cd\u800c\u963b\u788d\u51c6\u786e\u9884\u6d4b\uff0c\u65e0\u6cd5\u6a21\u62df\u5b66\u751f\u80fd\u529b\u9650\u5236", "motivation": "\u9898\u76ee\u96be\u5ea6\u8bc4\u4f30\u5bf9\u6559\u80b2\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b58\u5728\u51b7\u542f\u52a8\u95ee\u9898\u3002\u867d\u7136LLMs\u5c55\u73b0\u51fa\u8d85\u5f3a\u89e3\u9898\u80fd\u529b\uff0c\u4f46\u80fd\u5426\u611f\u77e5\u4eba\u7c7b\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u56f0\u96be\u4ecd\u662f\u672a\u89e3\u95ee\u9898", "method": "\u5bf920\u591a\u4e2a\u6a21\u578b\u5728\u533b\u5b66\u77e5\u8bc6\u548c\u6570\u5b66\u63a8\u7406\u7b49\u591a\u6837\u5316\u9886\u57df\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u4eba\u7c7b-AI\u96be\u5ea6\u5bf9\u9f50\u95ee\u9898", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u504f\u5dee\uff1a\u6a21\u578b\u89c4\u6a21\u6269\u5927\u65e0\u52a9\u4e8e\u5bf9\u9f50\uff0c\u53cd\u800c\u8d8b\u5411\u673a\u5668\u5171\u8bc6\uff1b\u9ad8\u6027\u80fd\u53cd\u800c\u963b\u788d\u51c6\u786e\u96be\u5ea6\u4f30\u8ba1\uff1b\u6a21\u578b\u65e0\u6cd5\u6a21\u62df\u5b66\u751f\u80fd\u529b\u9650\u5236\uff1b\u7f3a\u4e4f\u5185\u7701\u80fd\u529b\uff0c\u65e0\u6cd5\u9884\u6d4b\u81ea\u8eab\u5c40\u9650", "conclusion": "\u901a\u7528\u89e3\u9898\u80fd\u529b\u4e0d\u610f\u5473\u7740\u7406\u89e3\u4eba\u7c7b\u8ba4\u77e5\u56f0\u96be\uff0c\u5f53\u524d\u6a21\u578b\u7528\u4e8e\u81ea\u52a8\u96be\u5ea6\u9884\u6d4b\u9762\u4e34\u6311\u6218"}}
{"id": "2512.19564", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19564", "abs": "https://arxiv.org/abs/2512.19564", "authors": ["Yanliang Huang", "Xia Yan", "Peiran Yin", "Zhenduo Zhang", "Zeyan Shao", "Youran Wang", "Haoliang Huang", "Matthias Althoff"], "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles", "comment": null, "summary": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.", "AI": {"tldr": "\u7b2c\u56db\u5c4aCommonRoad\u8fd0\u52a8\u89c4\u5212\u7ade\u8d5b\uff082024\uff09\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u7b97\u6cd5\u5728\u9ad8\u901f\u516c\u8def\u548c\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u8fc7\u53bb\u5341\u5e74\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u96be\u4ee5\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u7684\u76f8\u5bf9\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u6bd4\u8f83\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528CommonRoad\u57fa\u51c6\u5957\u4ef6\u5efa\u7acb\u5f00\u6e90\u53ef\u590d\u73b0\u7684\u7ade\u8d5b\u6846\u67b6\uff0c\u6db5\u76d6\u9ad8\u901f\u516c\u8def\u548c\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u4ea4\u901a\u53c2\u4e0e\u8005\uff08\u8f7f\u8f66\u3001\u516c\u4ea4\u8f66\u3001\u81ea\u884c\u8f66\uff09\uff0c\u4ece\u6548\u7387\u3001\u5b89\u5168\u6027\u3001\u8212\u9002\u5ea6\u548c\u4ea4\u901a\u89c4\u5219\u9075\u5b88\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u89c4\u5212\u5668\u6027\u80fd\u3002", "result": "\u62a5\u544a\u4ecb\u7ecd\u4e86\u7ade\u8d5b\u5f62\u5f0f\uff0c\u5e76\u5bf92023\u5e74\u548c2024\u5e74\u7248\u672c\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u4ee3\u8868\u6027\u89c4\u5212\u5668\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u7b97\u6cd5\u5728\u6807\u51c6\u5316\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "CommonRoad\u8fd0\u52a8\u89c4\u5212\u7ade\u8d5b\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\uff0c\u4fc3\u8fdb\u4e86\u7b97\u6cd5\u6bd4\u8f83\u548c\u6027\u80fd\u63d0\u5347\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2512.18906", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18906", "abs": "https://arxiv.org/abs/2512.18906", "authors": ["Shaomu Tan", "Ryosuke Mitani", "Ritvik Choudhary", "Qiyu Wu", "Toshiyuki Sekiya", "Christof Monz"], "title": "Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations", "comment": null, "summary": "Over the years, automatic MT metrics have hillclimbed benchmarks and presented strong and sometimes human-level agreement with human ratings. Yet they remain black-box, offering little insight into their decision-making and often failing under real-world out-of-distribution (OOD) inputs. We introduce Remedy-R, a reasoning-driven generative MT metric trained with reinforcement learning from pairwise translation preferences, without requiring error-span annotations or distillation from closed LLMs. Remedy-R produces step-by-step analyses of accuracy, fluency, and completeness, followed by a final score, enabling more interpretable assessments. With only 60K training pairs across two language pairs, Remedy-R remains competitive with top scalar metrics and GPT-4-based judges on WMT22-24 meta-evaluation, generalizes to other languages, and exhibits strong robustness on OOD stress tests. Moreover, Remedy-R models generate self-reflective feedback that can be reused for translation improvement. Building on this finding, we introduce Remedy-R Agent, a simple evaluate-revise pipeline that leverages Remedy-R's evaluation analysis to refine translations. This agent consistently improves translation quality across diverse models, including Qwen2.5, ALMA-R, GPT-4o-mini, and Gemini-2.0-Flash, suggesting that Remedy-R's reasoning captures translation-relevant information and is practically useful.", "AI": {"tldr": "Remedy-R\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684\u751f\u6210\u5f0f\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ece\u7ffb\u8bd1\u504f\u597d\u4e2d\u8bad\u7ec3\uff0c\u65e0\u9700\u9519\u8bef\u6807\u6ce8\u6216\u5927\u6a21\u578b\u84b8\u998f\uff0c\u80fd\u751f\u6210\u9010\u6b65\u5206\u6790\u5e76\u63d0\u4f9b\u6700\u7ec8\u8bc4\u5206\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8MT\u6307\u6807\u867d\u7136\u5728\u67d0\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4ecd\u662f\u9ed1\u76d2\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u51b3\u7b56\u900f\u660e\u5ea6\uff0c\u4e14\u5728\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u5916\u8f93\u5165\u4e0b\u5bb9\u6613\u5931\u6548\u3002\u9700\u8981\u66f4\u53ef\u89e3\u91ca\u3001\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ece\u6210\u5bf9\u7ffb\u8bd1\u504f\u597d\u4e2d\u8bad\u7ec3\u751f\u6210\u5f0fMT\u8bc4\u4f30\u6307\u6807\uff0c\u65e0\u9700\u9519\u8bef\u6807\u6ce8\u6216\u5927\u6a21\u578b\u84b8\u998f\u3002\u6a21\u578b\u9010\u6b65\u5206\u6790\u51c6\u786e\u6027\u3001\u6d41\u7545\u6027\u548c\u5b8c\u6574\u6027\uff0c\u7136\u540e\u7ed9\u51fa\u6700\u7ec8\u8bc4\u5206\u3002\u4ec5\u97006\u4e07\u8bad\u7ec3\u5bf9\u5373\u53ef\u8de8\u8bed\u8a00\u6cdb\u5316\u3002", "result": "\u5728WMT22-24\u5143\u8bc4\u4f30\u4e2d\u4e0e\u9876\u7ea7\u6807\u91cf\u6307\u6807\u548cGPT-4\u8bc4\u4f30\u8005\u7ade\u4e89\uff0c\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u8bed\u8a00\uff0c\u5728OOD\u538b\u529b\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9c81\u68d2\u3002\u57fa\u4e8e\u5176\u5206\u6790\u6784\u5efa\u7684Remedy-R Agent\u80fd\u6301\u7eed\u6539\u8fdb\u591a\u79cd\u6a21\u578b\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "Remedy-R\u901a\u8fc7\u63a8\u7406\u9a71\u52a8\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684MT\u8bc4\u4f30\uff0c\u5176\u5206\u6790\u5305\u542b\u7ffb\u8bd1\u76f8\u5173\u4fe1\u606f\uff0c\u53ef\u5b9e\u9645\u7528\u4e8e\u7ffb\u8bd1\u6539\u8fdb\uff0c\u4e3a\u9ed1\u76d2MT\u6307\u6807\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.19567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19567", "abs": "https://arxiv.org/abs/2512.19567", "authors": ["Carlos P\u00e9rez-Ruiz", "Joan Sol\u00e0"], "title": "LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry", "comment": null, "summary": "This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello.", "AI": {"tldr": "LIMOncello\u662f\u4e00\u4e2a\u7d27\u8026\u5408\u7684LiDAR-IMU\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u5728SGal(3)\u6d41\u5f62\u4e0a\u5efa\u6a216\u81ea\u7531\u5ea6\u8fd0\u52a8\uff0c\u4f7f\u7528\u589e\u91cfi-Octree\u5730\u56fe\u540e\u7aef\uff0c\u5728\u51e0\u4f55\u7a00\u758f\u73af\u5883\u4e2d\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684LiDAR-IMU\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u5728\u4f4e\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5bb9\u6613\u6f02\u79fb\uff0c\u4f20\u7edf\u5730\u56fe\u7ed3\u6784\uff08\u5982\u589e\u91cfkd-tree\uff09\u66f4\u65b0\u6162\u4e14\u5185\u5b58\u5360\u7528\u9ad8\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u8fd0\u52a8\u8868\u793a\u548c\u5730\u56fe\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "1. \u5728SGal(3)\u6d41\u5f62\u4e0a\u5efa\u6a216-DoF\u8fd0\u52a8\uff0c\u76f8\u6bd4SO(3)\u00d7R^6\u63d0\u4f9b\u66f4\u4e00\u81f4\u548c\u6570\u503c\u7a33\u5b9a\u7684\u79bb\u6563\u65f6\u95f4\u4f20\u64ad\u6a21\u578b\uff1b2. \u4f7f\u7528\u8fed\u4ee3\u8bef\u5dee\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u540e\u7aef\uff1b3. \u91c7\u7528\u8f7b\u91cf\u7ea7\u589e\u91cfi-Octree\u5730\u56fe\u540e\u7aef\uff0c\u5b9e\u73b0\u66f4\u5feb\u66f4\u65b0\u548c\u66f4\u4f4e\u5185\u5b58\u5360\u7528\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLIMOncello\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u7cbe\u5ea6\uff0c\u5728\u51e0\u4f55\u7a00\u758f\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u4e14\u5185\u5b58\u589e\u957f\u7a33\u5b9a\u3002", "conclusion": "LIMOncello\u901a\u8fc7SGal(3)\u6d41\u5f62\u8868\u793a\u548ci-Octree\u5730\u56fe\u7ed3\u6784\uff0c\u4e3aLiDAR-IMU\u91cc\u7a0b\u8ba1\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u3001\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u5f00\u6e90\u5b9e\u73b0\u53d1\u5e03\u3002"}}
{"id": "2512.18940", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.18940", "abs": "https://arxiv.org/abs/2512.18940", "authors": ["Wen-Long Jin"], "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions", "comment": "13 pages, 3 figures. Supplementary materials at https://doi.org/10.17605/OSF.IO/PV6R3", "summary": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.", "AI": {"tldr": "FASTRIC\u662f\u4e00\u79cd\u63d0\u793a\u89c4\u8303\u8bed\u8a00\uff0c\u901a\u8fc7\u5c06\u9690\u5f0f\u6709\u9650\u72b6\u6001\u673a\u663e\u5f0f\u5316\u5230\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e2d\uff0c\u5b9e\u73b0LLM\u591a\u8f6e\u4ea4\u4e92\u534f\u8bae\u7684\u53ef\u9a8c\u8bc1\u6267\u884c\u3002", "motivation": "LLM\u6267\u884c\u590d\u6742\u7684\u591a\u8f6e\u4ea4\u4e92\u534f\u8bae\uff0c\u4f46\u7f3a\u4e4f\u5f62\u5f0f\u5316\u89c4\u8303\u6765\u9a8c\u8bc1\u6267\u884c\u662f\u5426\u7b26\u5408\u8bbe\u8ba1\u8005\u610f\u56fe\uff0c\u5f53\u524d\u8bbe\u8ba1\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u800c\u975e\u7cfb\u7edf\u5316\u5de5\u7a0b\u3002", "method": "\u63d0\u51faFASTRIC\u8bed\u8a00\uff0c\u6307\u5bfc\u8bbe\u8ba1\u8005\u660e\u786e\u8868\u8fbeFSM\u4e03\u8981\u7d20\uff08\u6700\u7ec8\u72b6\u6001\u3001\u4ee3\u7406\u3001\u72b6\u6001\u3001\u89e6\u53d1\u5668\u3001\u89d2\u8272\u3001\u521d\u59cb\u72b6\u6001\u3001\u7ea6\u675f\uff09\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\uff08\u89e3\u6790\u5668\u3001\u89e3\u91ca\u5668\u3001\u8fd0\u884c\u65f6\u73af\u5883\u3001\u5f00\u53d1\u52a9\u624b\uff09\uff0c\u901a\u8fc7\u6267\u884c\u8f68\u8ff9\u5206\u6790\u8fdb\u884c\u4e00\u81f4\u6027\u9a8c\u8bc1\u3002", "result": "\u6d4b\u8bd53\u72b6\u6001\u5e7c\u513f\u56ed\u8f85\u5bfcFSM\uff0c\u53d1\u73b0\u6700\u4f18\u89c4\u8303\u5f62\u5f0f\u5316\u7a0b\u5ea6\u662f\u6a21\u578b\u5bb9\u91cf\u7684\u51fd\u6570\uff1aDeepSeek-V3.2\u5728L2-L4\u8fbe\u5230\u5b8c\u7f8e\u4e00\u81f4\u6027\uff0cChatGPT-5\u5728L3\u8fbe\u5230\u5cf0\u503c\uff0cPhi4\u65e0\u7a33\u5b9a\u6700\u4f18\u4e14\u65b9\u5dee\u9ad8\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7279\u5b9a\u7684\"Goldilocks\u533a\u95f4\"\u3002", "conclusion": "FASTRIC\u5efa\u7acb\u4e86\u63d0\u793a\u89c4\u8303\u5de5\u7a0b\uff0c\u5c06\u591a\u8f6e\u4ea4\u4e92\u8bbe\u8ba1\u4ece\u542f\u53d1\u5f0f\u827a\u672f\u8f6c\u53d8\u4e3a\u5177\u6709\u53ef\u6d4b\u91cf\u7a0b\u5e8f\u4fdd\u8bc1\u7684\u7cfb\u7edf\u5de5\u7a0b\uff0c\u4e3a\u521b\u5efa\u53ef\u9a8c\u8bc1\u4ea4\u4e92\u534f\u8bae\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2512.19576", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.19576", "abs": "https://arxiv.org/abs/2512.19576", "authors": ["Kirill Djebko", "Tom Baumann", "Erik Dilger", "Frank Puppe", "Sergio Montenegro"], "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller", "comment": "55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository https://github.com/kdjebko/lelar-in-orbit-data", "summary": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universit\u00e4t W\u00fcrzburg in cooperation with the Technische Universit\u00e4t Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.", "AI": {"tldr": "\u9996\u6b21\u5728\u8f68\u6f14\u793a\u57fa\u4e8eAI\u7684\u536b\u661f\u59ff\u6001\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5e76\u5728InnoCube\u7eb3\u7c73\u536b\u661f\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfPD\u63a7\u5236\u5668", "motivation": "\u4f20\u7edf\u59ff\u6001\u63a7\u5236\u5668\u8bbe\u8ba1\u8017\u65f6\u4e14\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u64cd\u4f5c\u6761\u4ef6\u53d8\u5316\u654f\u611f\uff0c\u9700\u8981\u66f4\u81ea\u9002\u5e94\u3001\u9c81\u68d2\u7684\u63a7\u5236\u65b9\u6848\u3002\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46Sim2Real\u5dee\u8ddd\uff08\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u536b\u661f\u90e8\u7f72\uff09\u4ecd\u662f\u91cd\u5927\u6311\u6218", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3AI\u59ff\u6001\u63a7\u5236\u5668\uff0c\u7136\u540e\u5c06\u8bad\u7ec3\u597d\u7684\u667a\u80fd\u4f53\u90e8\u7f72\u5230InnoCube 3U\u7eb3\u7c73\u536b\u661f\u4e0a\u3002\u8bbe\u8ba1\u4e86AI\u667a\u80fd\u4f53\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u5904\u7406\u4e86\u4eff\u771f\u4e0e\u771f\u5b9e\u536b\u661f\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u5f02", "result": "\u6210\u529f\u5b8c\u6210\u4e86\u9996\u6b21\u57fa\u4e8eAI\u7684\u59ff\u6001\u63a7\u5236\u5668\u5728\u8f68\u6f14\u793a\uff0c\u5728\u60ef\u6027\u6307\u5411\u673a\u52a8\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\u3002\u7a33\u6001\u6307\u6807\u8bc1\u5b9e\u4e86AI\u63a7\u5236\u5668\u5728\u91cd\u590d\u5728\u8f68\u673a\u52a8\u4e2d\u7684\u4f18\u8d8a\u8868\u73b0\uff0c\u6027\u80fd\u4f18\u4e8eInnoCube\u7684\u4f20\u7edfPD\u63a7\u5236\u5668", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684AI\u59ff\u6001\u63a7\u5236\u5668\u80fd\u591f\u6210\u529f\u4ece\u4eff\u771f\u8f6c\u79fb\u5230\u771f\u5b9e\u5728\u8f68\u536b\u661f\uff0c\u514b\u670d\u4e86Sim2Real\u5dee\u8ddd\uff0c\u4e3a\u536b\u661f\u59ff\u6001\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u81ea\u9002\u5e94\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.18999", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18999", "abs": "https://arxiv.org/abs/2512.18999", "authors": ["Jinyan Liu", "Zikang Chen", "Qinchuan Wang", "Tan Xie", "Heming Zheng", "Xudong Lv"], "title": "Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework", "comment": "10 pages,3 figures,conference ICCBB2025", "summary": "When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.", "AI": {"tldr": "\u5bf9\u6bd4\u4e24\u79cd\u533b\u7597\u968f\u8bbf\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\uff1a\u7aef\u5230\u7aefLLM\u7cfb\u7edfvs\u6a21\u5757\u5316\u6d41\u7a0b\u63a7\u5236\u7cfb\u7edf\uff0c\u540e\u8005\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7a33\u5b9a\u6027\u548c\u4fe1\u606f\u63d0\u53d6\u51c6\u786e\u6027", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u5e94\u7528\u4e8e\u533b\u7597\u968f\u8bbf\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u968f\u8bbf\u8868\u683c\u7684\u590d\u6742\u6027\uff0c\u7ecf\u5e38\u51fa\u73b0\u5bf9\u8bdd\u6d41\u7a0b\u5931\u63a7\u548c\u4fe1\u606f\u63d0\u53d6\u4e0d\u51c6\u786e\u7684\u95ee\u9898", "method": "\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e24\u79cd\u968f\u8bbf\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\uff1a1) \u7aef\u5230\u7aefLLM\u7cfb\u7edf\uff08\u5bf9\u7167\u7ec4\uff09\uff1b2) \u57fa\u4e8e\u4efb\u52a1\u5206\u89e3\u3001\u8bed\u4e49\u805a\u7c7b\u548c\u6d41\u7a0b\u7ba1\u7406\u7684\u6a21\u5757\u5316\u7ba1\u9053\u7cfb\u7edf\uff08\u5b9e\u9a8c\u7ec4\uff09", "result": "\u6a21\u5757\u5316\u65b9\u6cd5\u663e\u8457\u6539\u5584\u5bf9\u8bdd\u7a33\u5b9a\u6027\u548c\u63d0\u53d6\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u5bf9\u8bdd\u8f6e\u657046.73%\uff0c\u964d\u4f4etoken\u6d88\u801780%-87.5%", "conclusion": "\u5728\u9ad8\u98ce\u9669\u533b\u7597\u968f\u8bbf\u573a\u666f\u4e2d\u90e8\u7f72LLM\u65f6\uff0c\u5fc5\u987b\u96c6\u6210\u5916\u90e8\u63a7\u5236\u673a\u5236\u4ee5\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u6548\u7387"}}
{"id": "2512.19583", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.19583", "abs": "https://arxiv.org/abs/2512.19583", "authors": ["Yinhuai Wang", "Runyi Yu", "Hok Wai Tsui", "Xiaoyi Lin", "Hui Zhang", "Qihan Zhao", "Ke Fan", "Miao Li", "Jie Song", "Jingbo Wang", "Qifeng Chen", "Ping Tan"], "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations", "comment": null, "summary": "We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4ece\u5408\u6210\u6570\u636e\u5b66\u4e60\u901a\u7528\u624b-\u7269\u4f53\u8ddf\u8e2a\u63a7\u5236\u5668\u7684\u7cfb\u7edf\uff0c\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\uff0c\u5305\u542bHOP\u8f68\u8ff9\u89c4\u5212\u5668\u548cHOT\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u5408\u6210\u5230\u7269\u7406\u7684\u8fc1\u79fb", "motivation": "\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u4e2d\u957f\u671f\u5b58\u5728\u7684\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u7c7b\u6f14\u793a\u6570\u636e\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u76ee\u6807\u662f\u5f00\u53d1\u5b8c\u5168\u4ece\u5408\u6210\u6570\u636e\u5b66\u4e60\u7684\u57fa\u7840\u63a7\u5236\u5668", "method": "\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) HOP\uff08\u624b-\u7269\u4f53\u89c4\u5212\u5668\uff09- \u5408\u6210\u591a\u6837\u7684\u624b-\u7269\u4f53\u8f68\u8ff9\uff1b2) HOT\uff08\u624b-\u7269\u4f53\u8ddf\u8e2a\u5668\uff09- \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4ea4\u4e92\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u5408\u6210\u5230\u7269\u7406\u7684\u8fc1\u79fb\uff0c\u751f\u6210\u4ee5\u76ee\u6807\u624b-\u7269\u4f53\u72b6\u6001\u4e3a\u6761\u4ef6\u7684\u901a\u7528\u63a7\u5236\u5668", "result": "\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u591a\u6837\u7269\u4f53\u5f62\u72b6\u548c\u624b\u90e8\u5f62\u6001\uff0c\u4f7f\u7075\u5de7\u624b\u80fd\u591f\u8ddf\u8e2a\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u65f6\u7a0b\u5e8f\u5217\uff0c\u5305\u62ec\u7269\u4f53\u91cd\u65b0\u6392\u5217\u548c\u654f\u6377\u7684\u624b\u5185\u91cd\u5b9a\u5411", "conclusion": "\u8be5\u65b9\u6cd5\u4ee3\u8868\u4e86\u5411\u53ef\u6269\u5c55\u7684\u64cd\u7eb5\u57fa\u7840\u63a7\u5236\u5668\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u80fd\u591f\u5b8c\u5168\u4ece\u5408\u6210\u6570\u636e\u5b66\u4e60\uff0c\u6253\u7834\u4e86\u957f\u671f\u5236\u7ea6\u7075\u5de7\u64cd\u4f5c\u8fdb\u5c55\u7684\u6570\u636e\u74f6\u9888"}}
{"id": "2512.19004", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19004", "abs": "https://arxiv.org/abs/2512.19004", "authors": ["Tongyuan Miao", "Gary Huang", "Kai Jun Han", "Annie Jiang"], "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models", "comment": null, "summary": "Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.", "AI": {"tldr": "DLLMs\u89e3\u7801\u6162\uff0c\u672c\u6587\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u7684\u65b9\u6cd5\uff1a\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u521d\u59cb\u5316\u7f29\u77ed\u751f\u6210\u8f68\u8ff9\uff0c\u7528\u8f7b\u91cf\u8f85\u52a9\u6a21\u578b\u6ce8\u5165\u63d0\u793a\u6761\u4ef6\u5148\u9a8c\uff0c\u51cf\u5c11\u7ea635%\u53bb\u566a\u8fed\u4ee3\uff0c\u4f46\u9762\u4e34\u51c6\u786e\u7387\u4e0b\u964d\u6311\u6218\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u867d\u7136\u652f\u6301\u5b8c\u5168\u5e76\u884c\u4ee4\u724c\u89e3\u7801\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u591a\u6b21\u53bb\u566a\u8fed\u4ee3\u5c06\u4fe1\u606f\u5168\u65e0\u7684\u5b8c\u5168\u63a9\u7801\u521d\u59cb\u5316\u8f6c\u5316\u4e3a\u8fde\u8d2f\u6587\u672c\uff0c\u63a8\u7406\u65f6\u4ecd\u4e0d\u5b9e\u7528\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u6539\u8fdb\u6c42\u89e3\u5668\u6216\u91c7\u6837\u7b56\u7565\u66f4\u9ad8\u6548\u5730\u904d\u5386\u751f\u6210\u8f68\u8ff9\uff0c\u672c\u6587\u63d0\u51fa\u8865\u5145\u89c6\u89d2\uff1a\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u521d\u59cb\u5316\u4ece\u66f4\u63a5\u8fd1\u76ee\u6807\u5206\u5e03\u7684\u4f4d\u7f6e\u5f00\u59cb\uff0c\u7f29\u77ed\u8f68\u8ff9\u672c\u8eab\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u7684\u63a5\u53e3\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6a21\u578b\u5c06\u63d0\u793a\u6761\u4ef6\u5148\u9a8c\u6ce8\u5165\u6269\u6563\u521d\u59cb\u5316\u3002\u5177\u4f53\u5b9e\u73b0\u4e24\u79cd\u673a\u5236\uff1a\u79bb\u6563\u4ee4\u724c\u6ce8\u5165\u548c\u8868\u793a\u7ea7\u5d4c\u5165\u63d2\u503c\u3002\u7531\u4e8e\u6ce8\u5165\u7684\u5148\u9a8c\u53ef\u80fd\u4e0d\u5b8c\u7f8e\u4e14\u4ec5\u89e3\u63a9\u7801\u89e3\u7801\u53ef\u80fd\u8fc7\u65e9\u627f\u8bfa\uff0c\u8fd8\u5f15\u5165\u4e86\u7b80\u5355\u7684\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u91cd\u65b0\u63a9\u7801\u673a\u5236\u4f5c\u4e3a\u5148\u9a8c\u6000\u7591\u5f62\u5f0f\u3002", "result": "\u5728GSM8K\u4e0a\u7684\u521d\u6b65\u8bc1\u636e\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u611f\u77e5\u521d\u59cb\u5316\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u53bb\u566a\u8fed\u4ee3\uff08\u5728\u6211\u4eec\u7684\u8bbe\u7f6e\u4e2d\u51cf\u5c11\u7ea635%\u7684\u51fd\u6570\u8bc4\u4f30\uff09\uff0c\u4f46\u4e5f\u66b4\u9732\u4e86\u4e00\u4e2a\u5173\u952e\u5f00\u653e\u6311\u6218\uff1a\u7b80\u5355\u7684\u9884\u70ed\u542f\u52a8\u53ef\u80fd\u76f8\u5bf9\u4e8e\u5f3a\u6269\u6563\u57fa\u7ebf\u964d\u4f4e\u6700\u7ec8\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4fc3\u4f7f\u56f4\u7ed5\u6821\u51c6\u3001\u4fee\u6b63\u673a\u5236\u548c\u8868\u793a\u5bf9\u9f50\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u9884\u70ed\u542f\u52a8\u6269\u6563\u89e3\u7801\u3002\u9700\u8981\u89e3\u51b3\u51c6\u786e\u7387\u4e0b\u964d\u95ee\u9898\uff0c\u786e\u4fdd\u52a0\u901f\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2512.19629", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19629", "abs": "https://arxiv.org/abs/2512.19629", "authors": ["Jiaqi Peng", "Wenzhe Cai", "Yuqiang Yang", "Tai Wang", "Yuan Shen", "Jiangmiao Pang"], "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry", "comment": "Project page:https://steinate.github.io/logoplanner.github.io/", "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}.", "AI": {"tldr": "LoGoPlanner\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u51e0\u4f55\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u9690\u5f0f\u72b6\u6001\u4f30\u8ba1\u548c\u573a\u666f\u51e0\u4f55\u91cd\u5efa\uff0c\u65e0\u9700\u5355\u72ec\u5b9a\u4f4d\u6a21\u5757\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u6a21\u5757\u5316\u8f68\u8ff9\u89c4\u5212\u5b58\u5728\u5ef6\u8fdf\u548c\u7ea7\u8054\u9519\u8bef\u95ee\u9898\uff0c\u73b0\u6709\u7aef\u5230\u7aef\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u9700\u8981\u7cbe\u786e\u4f20\u611f\u5668\u6807\u5b9a\u7684\u5355\u72ec\u5b9a\u4f4d\u6a21\u5757\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u673a\u5668\u4eba\u548c\u73af\u5883\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1) \u5fae\u8c03\u957f\u65f6\u7a0b\u89c6\u89c9\u51e0\u4f55\u9aa8\u5e72\u7f51\u7edc\u4ee5\u83b7\u5f97\u7edd\u5bf9\u5ea6\u91cf\u5c3a\u5ea6\u7684\u9884\u6d4b\uff0c\u63d0\u4f9b\u9690\u5f0f\u72b6\u6001\u4f30\u8ba1\uff1b2) \u4ece\u5386\u53f2\u89c2\u6d4b\u91cd\u5efa\u573a\u666f\u51e0\u4f55\uff0c\u63d0\u4f9b\u5bc6\u96c6\u73af\u5883\u611f\u77e5\uff1b3) \u57fa\u4e8e\u8f85\u52a9\u4efb\u52a1\u5f15\u5bfc\u7684\u9690\u5f0f\u51e0\u4f55\u6761\u4ef6\u5316\u7b56\u7565\uff0c\u51cf\u5c11\u8bef\u5dee\u4f20\u64ad\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cLoGoPlanner\u7684\u7aef\u5230\u7aef\u8bbe\u8ba1\u51cf\u5c11\u4e86\u7d2f\u79ef\u8bef\u5dee\uff0c\u5ea6\u91cf\u611f\u77e5\u7684\u51e0\u4f55\u8bb0\u5fc6\u589e\u5f3a\u4e86\u89c4\u5212\u4e00\u81f4\u6027\u548c\u907f\u969c\u80fd\u529b\uff0c\u76f8\u6bd4\u57fa\u4e8e\u7cbe\u786e\u5b9a\u4f4d\u7684\u57fa\u7ebf\u63d0\u5347\u8d85\u8fc727.3%\uff0c\u5728\u4e0d\u540c\u673a\u5668\u4eba\u548c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LoGoPlanner\u901a\u8fc7\u9690\u5f0f\u72b6\u6001\u4f30\u8ba1\u548c\u573a\u666f\u51e0\u4f55\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5355\u72ec\u5b9a\u4f4d\u6a21\u5757\u7684\u7aef\u5230\u7aef\u5bfc\u822a\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.19012", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19012", "abs": "https://arxiv.org/abs/2512.19012", "authors": ["Shijian Ma", "Yunqi Huang", "Yan Lin"], "title": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation", "comment": null, "summary": "Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.", "AI": {"tldr": "DramaBench\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u620f\u5267\u5267\u672c\u7eed\u5199\u8bc4\u4f30\u57fa\u51c6\uff0c\u4ece\u516d\u4e2a\u72ec\u7acb\u7ef4\u5ea6\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u3001\u63a8\u8fdb\u60c5\u8282\u8fde\u8d2f\u6027\u548c\u4fdd\u7559\u620f\u5267\u7ed3\u6784\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u620f\u5267\u5267\u672c\u7eed\u5199\u6240\u9700\u7684\u5173\u952e\u80fd\u529b\uff0c\u5305\u62ec\u89d2\u8272\u4e00\u81f4\u6027\u7ef4\u62a4\u3001\u60c5\u8282\u8fde\u8d2f\u63a8\u8fdb\u548c\u620f\u5267\u7ed3\u6784\u4fdd\u7559\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u5ba2\u89c2\u3001\u53ef\u91cd\u590d\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u6790\u3001LLM\u6807\u6ce8\u548c\u7edf\u8ba1\u6307\u6807\uff0c\u521b\u5efa\u5305\u542b\u516d\u4e2a\u72ec\u7acb\u7ef4\u5ea6\uff08\u683c\u5f0f\u6807\u51c6\u3001\u53d9\u4e8b\u6548\u7387\u3001\u89d2\u8272\u4e00\u81f4\u6027\u3001\u60c5\u611f\u6df1\u5ea6\u3001\u903b\u8f91\u4e00\u81f4\u6027\u548c\u51b2\u7a81\u5904\u7406\uff09\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u57281,103\u4e2a\u5267\u672c\u4e0a\u8fdb\u884c8,824\u6b21\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u548c\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "\u5bf98\u4e2a\u6700\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u663e\u793a65.9%\u7684\u6210\u5bf9\u6bd4\u8f83\u5177\u6709\u663e\u8457\u6027\u3002\u4eba\u5de5\u9a8c\u8bc1\u57285\u4e2a\u7ef4\u5ea6\u4e2d\u76843\u4e2a\u7ef4\u5ea6\u4e0a\u8fbe\u5230\u5b9e\u8d28\u6027\u4e00\u81f4\u3002\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u516d\u4e2a\u7ef4\u5ea6\u6355\u6349\u4e86\u72ec\u7acb\u7684\u8d28\u7d20\u65b9\u9762\uff08\u5e73\u5747|r|=0.020\uff09\u3002", "conclusion": "DramaBench\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u7ef4\u5ea6\u7279\u5b9a\u53cd\u9988\uff0c\u5e76\u4e3a\u521b\u610f\u5199\u4f5c\u8bc4\u4f30\u5efa\u7acb\u4e86\u4e25\u8c28\u6807\u51c6\uff0c\u586b\u8865\u4e86\u620f\u5267\u5267\u672c\u7eed\u5199\u8bc4\u4f30\u7684\u7a7a\u767d\u3002"}}
{"id": "2512.19092", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19092", "abs": "https://arxiv.org/abs/2512.19092", "authors": ["Ziyan Zhang", "Chao Wang", "Zhuo Chen", "Lei Chen", "Chiyi Li", "Kai Song"], "title": "A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs", "comment": null, "summary": "Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.", "AI": {"tldr": "ROG\u6846\u67b6\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u903b\u8f91\u67e5\u8be2\u3001\u68c0\u7d22\u76f8\u5173\u5b50\u56fe\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u5728\u590d\u6742KG\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4f20\u7edf\u5d4c\u5165\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5d4c\u5165\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u903b\u8f91\u67e5\u8be2\uff08\u6d89\u53ca\u591a\u4e2a\u64cd\u4f5c\u7b26\u3001\u6df1\u5ea6\u63a8\u7406\u94fe\u6216\u5f02\u6784KG\u6a21\u5f0f\uff09\u65f6\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51faROG\u6846\u67b6\uff1a1) \u5c06\u590d\u6742\u4e00\u9636\u903b\u8f91\u67e5\u8be2\u5206\u89e3\u4e3a\u7b80\u5355\u5b50\u67e5\u8be2\u5e8f\u5217\uff1b2) \u68c0\u7d22\u7d27\u51d1\u7684\u67e5\u8be2\u76f8\u5173\u5b50\u56fe\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8bc1\u636e\uff1b3) \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9010\u6b65\u94fe\u5f0f\u63a8\u7406\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u5d4c\u5165\u4f18\u5316\u3002", "result": "\u5728\u6807\u51c6KG\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cROG\u5728\u5e73\u5747\u5012\u6570\u6392\u540d(MRR)\u4e0a\u6301\u7eed\u4f18\u4e8e\u5f3a\u5d4c\u5165\u57fa\u7ebf\uff0c\u5728\u9ad8\u5ea6\u590d\u6742\u67e5\u8be2\u7c7b\u578b\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u5c06\u7ed3\u6784\u5316KG\u68c0\u7d22\u4e0eLLM\u9a71\u52a8\u7684\u903b\u8f91\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u4e3a\u590d\u6742KG\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.19117", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19117", "abs": "https://arxiv.org/abs/2512.19117", "authors": ["Amar Lakel"], "title": "Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?", "comment": "in French language", "summary": "This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ece\"\u5927\u8bed\u8a00\u6a21\u578b\"\u8f6c\u5411\"\u5927\u8bdd\u8bed\u6a21\u578b\"\u518d\u5230\"\u4eba\u5de5\u8bdd\u8bed\u4ee3\u7406\"\u7684\u8ba4\u8bc6\u8bba\u8f6c\u53d8\uff0c\u5efa\u7acb\u57fa\u4e8e\u73b0\u8c61\u89c4\u5f8b\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u8bed\u8a00\u6c89\u6dc0\u7684\u4e09\u5143\u672c\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u516c\u5171\u8bd5\u9a8c\u548c\u6cbb\u7406\u673a\u5236\u53d6\u4ee3\u5bf9AI\u7684\u8ff7\u604b/\u6050\u60e7\u4e8c\u5206\u6cd5\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u5206\u6790\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u8d85\u8d8a\u5355\u7eaf\u7684\u8bed\u8a00\u6a21\u578b\u8303\u7574\uff0c\u5efa\u7acb\u66f4\u5168\u9762\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u8fd9\u4e9b\u7cfb\u7edf\u5982\u4f55\u5efa\u6a21\u4eba\u7c7b\u7ecf\u9a8c\uff0c\u5e76\u89e3\u51b3\u793e\u4f1a\u5bf9AI\u7684\u8ff7\u604b\u4e0e\u6050\u60e7\u7684\u4e8c\u5143\u5bf9\u7acb\u3002", "method": "\u63d0\u51fa\u8ba4\u8bc6\u8bba\u8f6c\u53d8\uff1a\u4eceLLM\u5230LDM\u518d\u5230ADA\uff1b\u5efa\u7acb\u4e09\u5143\u672c\u4f53\u6846\u67b6\uff1a1) \u73b0\u8c61\u4e16\u754c\u89c4\u5f8b\u628a\u63e1\uff0c2) \u5177\u8eab\u8ba4\u77e5\u7ed3\u6784\u5316\uff0c3) \u793e\u4f1a\u5386\u53f2\u8bed\u5883\u4e2d\u7684\u8bed\u8a00\u6c89\u6dc0\uff1b\u5c06LDM\u89c6\u4e3a\u5bf9\u8fd9\u4e9b\u5b9e\u4f8b\u4ea7\u7269\uff08\u6587\u6863\uff09\u7684\u5efa\u6a21\u3002", "result": "\u6784\u5efa\u4e86\u4eba\u5de5\u8bdd\u8bed\u4ee3\u7406\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06AI\u7cfb\u7edf\u7406\u89e3\u4e3a\u5bf9\u4eba\u7c7b\u7ecf\u9a8c\u8bdd\u8bed\u6295\u5c04\u7684\u5efa\u6a21\uff0c\u4e3a\u7406\u89e3AI\u5728\u793e\u4f1a\u4e2d\u7684\u4f4d\u7f6e\u3001\u7528\u9014\u548c\u9650\u5236\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u3002", "conclusion": "\u9700\u8981\u901a\u8fc7\u516c\u5171\u8bd5\u9a8c\u7a0b\u5e8f\u548c\u591a\u65b9\u5171\u6cbb\u673a\u5236\uff08\u56fd\u5bb6\u3001\u4ea7\u4e1a\u3001\u516c\u6c11\u793e\u4f1a\u3001\u5b66\u672f\u754c\uff09\u6765\u660e\u786e\u4eba\u5de5\u8bdd\u8bed\u4ee3\u7406\u5728\u5f53\u4ee3\u793e\u4f1a\u7a7a\u95f4\u4e2d\u7684\u4f4d\u7f6e\uff0c\u53d6\u4ee3\u7b80\u5355\u7684\u8ff7\u604b/\u6050\u60e7\u4e8c\u5206\u6cd5\uff0c\u5b9e\u73b0\u8d1f\u8d23\u4efb\u7684AI\u6cbb\u7406\u3002"}}
{"id": "2512.19125", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19125", "abs": "https://arxiv.org/abs/2512.19125", "authors": ["Tzu-Yun Lee", "Ding-Yong Hong", "Jan-Jan Wu"], "title": "SAP: Syntactic Attention Pruning for Transformer-based Language Models", "comment": null, "summary": "This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.", "AI": {"tldr": "SAP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6ce8\u610f\u529b\u5934\u526a\u679d\u65b9\u6cd5\uff0c\u7ed3\u5408\u53e5\u6cd5\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u6307\u5bfc\u526a\u679d\uff0c\u5728\u514d\u91cd\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edfTransformer\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6a21\u578b\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u6570\u5b66\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u8003\u8651\u3002SAP\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u53e5\u6cd5\u4fe1\u606f\u6765\u66f4\u6709\u6548\u5730\u526a\u679d\u6ce8\u610f\u529b\u5934\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u5e76\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faSyntactic Attention Pruning (SAP)\u65b9\u6cd5\uff0c\u5229\u7528\u53e5\u6cd5\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u6307\u5bfc\u526a\u679d\u8fc7\u7a0b\u3002\u8fd8\u5f15\u5165Candidate Filtering (CF)\u673a\u5236\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u5934\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8d21\u732e\u8fdb\u884c\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u51cf\u5c11\u526a\u679d\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u3002", "result": "SAP\u5728\u514d\u91cd\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u7684\u6ce8\u610f\u529b\u5934\u526a\u679d\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u4fdd\u7559\u5177\u6709\u9ad8\u5bc6\u5ea6\u5f3a\u6ce8\u610f\u529b\u503c\u7684\u5173\u952e\u5934\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "SAP\u4e3a\u6a21\u578b\u538b\u7f29\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u4e3a\u6240\u6709\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u7075\u6d3b\u6027\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.19126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19126", "abs": "https://arxiv.org/abs/2512.19126", "authors": ["Zihan Lin", "Xiaohan Wang", "Hexiong Yang", "Jiajun Chai", "Jie Cao", "Guojun Yin", "Wei Lin", "Ran He"], "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards", "comment": null, "summary": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.", "AI": {"tldr": "AWPO\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6574\u5408\u663e\u5f0f\u63a8\u7406\u5956\u52b1\u6765\u589e\u5f3aLLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53ef\u9a8c\u8bc1\u7ed3\u679c\u5956\u52b1\uff0c\u5ffd\u89c6\u4e86\u663e\u5f0f\u63a8\u7406\u5956\u52b1\u5bf9\u63d0\u5347\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u6f5c\u529b\u3002\u540c\u65f6\uff0c\u7b80\u5355\u7ed3\u5408\u63a8\u7406\u548c\u7ed3\u679c\u5956\u52b1\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u6216\u4e0e\u4e3b\u8981\u4f18\u5316\u76ee\u6807\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4f18\u52bf\u52a0\u6743\u7b56\u7565\u4f18\u5316(AWPO)\u6846\u67b6\uff0c\u5305\u542b\u65b9\u5dee\u611f\u77e5\u95e8\u63a7\u548c\u96be\u5ea6\u611f\u77e5\u52a0\u6743\u673a\u5236\uff0c\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7edf\u8ba1\u81ea\u9002\u5e94\u8c03\u8282\u63a8\u7406\u4fe1\u53f7\u7684\u4f18\u52bf\uff0c\u5e76\u91c7\u7528\u5b9a\u5236\u88c1\u526a\u673a\u5236\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316\u3002", "result": "AWPO\u5728\u6807\u51c6\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u548c\u9886\u5148\u95ed\u6e90\u6a21\u578b\u30024B\u53c2\u6570\u6a21\u578b\u5728\u591a\u8f6e\u51c6\u786e\u7387\u4e0a\u8d85\u8d8aGrok-4\u8fbe16.0%\uff0c\u540c\u65f6\u5728MMLU-Pro\u5206\u5e03\u5916\u57fa\u51c6\u4e0a\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AWPO\u901a\u8fc7\u6709\u6548\u6574\u5408\u663e\u5f0f\u63a8\u7406\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5728\u53c2\u6570\u6548\u7387\u3001\u591a\u8f6e\u573a\u666f\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.19134", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.19134", "abs": "https://arxiv.org/abs/2512.19134", "authors": ["Dehai Min", "Kailin Zhang", "Tongtong Wu", "Lu Cheng"], "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "comment": null, "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "AI": {"tldr": "QuCo-RAG\u63d0\u51fa\u57fa\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\u7edf\u8ba1\u7684\u5ba2\u89c2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\u7684\u4e3b\u89c2\u7f6e\u4fe1\u5ea6\uff0c\u901a\u8fc7\u4f4e\u9891\u5b9e\u4f53\u8bc6\u522b\u548c\u5b9e\u4f53\u5171\u73b0\u9a8c\u8bc1\u6765\u52a8\u6001\u89e6\u53d1\u68c0\u7d22\uff0c\u663e\u8457\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\uff08\u5982logits\u3001\u71b5\uff09\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u53f7\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u6821\u51c6\u4e0d\u4f73\uff0c\u7ecf\u5e38\u5bf9\u9519\u8bef\u8f93\u51fa\u8868\u73b0\u51fa\u9ad8\u7f6e\u4fe1\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5ba2\u89c2\u3001\u57fa\u4e8e\u6570\u636e\u7edf\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faQuCo-RAG\u65b9\u6cd5\uff1a1\uff09\u751f\u6210\u524d\u8bc6\u522b\u4f4e\u9891\u5b9e\u4f53\uff0c\u6307\u793a\u957f\u5c3e\u77e5\u8bc6\u7f3a\u53e3\uff1b2\uff09\u751f\u6210\u65f6\u9a8c\u8bc1\u5b9e\u4f53\u5728\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u5171\u73b0\u60c5\u51b5\uff0c\u96f6\u5171\u73b0\u901a\u5e38\u8868\u793a\u5e7b\u89c9\u98ce\u9669\u3002\u4e24\u4e2a\u9636\u6bb5\u90fd\u5229\u7528Infini-gram\u57284\u4e07\u4ebftoken\u8bed\u6599\u4e0a\u8fdb\u884c\u6beb\u79d2\u7ea7\u5ef6\u8fdf\u67e5\u8be2\uff0c\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u65f6\u89e6\u53d1\u68c0\u7d22\u3002", "result": "\u5728\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQuCo-RAG\u5728OLMo-2\u6a21\u578b\u4e0a\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u53475-12\u4e2aEM\u70b9\uff0c\u5728\u672a\u516c\u5f00\u9884\u8bad\u7ec3\u6570\u636e\u7684\u6a21\u578b\uff08Llama\u3001Qwen\u3001GPT\uff09\u4e0a\u4e5f\u80fd\u6709\u6548\u8fc1\u79fb\uff0c\u63d0\u5347\u6700\u591a14\u4e2aEM\u70b9\u3002\u5728\u751f\u7269\u533b\u5b66QA\u9886\u57df\u7684\u6cdb\u5316\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u8303\u5f0f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u9a8c\u8bc1\u4e3a\u52a8\u6001RAG\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u5b9e\u9645\u6a21\u578b\u65e0\u5173\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5ba2\u89c2\u7edf\u8ba1\u66ff\u4ee3\u4e3b\u89c2\u7f6e\u4fe1\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2512.19161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19161", "abs": "https://arxiv.org/abs/2512.19161", "authors": ["Alessandro Lucca", "Francesco Pierri"], "title": "From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs", "comment": null, "summary": "Subtitles are essential for video accessibility and audience engagement. Modern Automatic Speech Recognition (ASR) systems, built upon Encoder-Decoder neural network architectures and trained on massive amounts of data, have progressively reduced transcription errors on standard benchmark datasets. However, their performance in real-world production environments, particularly for non-English content like long-form Italian videos, remains largely unexplored. This paper presents a case study on developing a professional subtitling system for an Italian media company. To inform our system design, we evaluated four state-of-the-art ASR models (Whisper Large v2, AssemblyAI Universal, Parakeet TDT v3 0.6b, and WhisperX) on a 50-hour dataset of Italian television programs. The study highlights their strengths and limitations, benchmarking their performance against the work of professional human subtitlers. The findings indicate that, while current models cannot meet the media industry's accuracy needs for full autonomy, they can serve as highly effective tools for enhancing human productivity. We conclude that a human-in-the-loop (HITL) approach is crucial and present the production-grade, cloud-based infrastructure we designed to support this workflow.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cd\u6700\u5148\u8fdbASR\u6a21\u578b\u5728\u610f\u5927\u5229\u957f\u89c6\u9891\u5b57\u5e55\u5236\u4f5c\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u867d\u7136\u65e0\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\uff0c\u4f46\u53ef\u4f5c\u4e3a\u63d0\u5347\u4eba\u7c7b\u751f\u4ea7\u529b\u7684\u6709\u6548\u5de5\u5177\uff0c\u63d0\u51fa\u4eba\u673a\u534f\u540c\u7684\u4e91\u57fa\u7840\u8bbe\u65bd\u65b9\u6848\u3002", "motivation": "\u5b57\u5e55\u5bf9\u89c6\u9891\u53ef\u8bbf\u95ee\u6027\u548c\u89c2\u4f17\u53c2\u4e0e\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u73b0\u4ee3ASR\u7cfb\u7edf\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u610f\u5927\u5229\u8bed\u957f\u89c6\u9891\u7b49\u975e\u82f1\u8bed\u5185\u5bb9\u7684\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6027\u80fd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e3a\u610f\u5927\u5229\u5a92\u4f53\u516c\u53f8\u5f00\u53d1\u4e13\u4e1a\u5b57\u5e55\u7cfb\u7edf\u3002", "method": "\u5bf9\u56db\u79cd\u6700\u5148\u8fdbASR\u6a21\u578b\uff08Whisper Large v2\u3001AssemblyAI Universal\u3001Parakeet TDT v3 0.6b\u548cWhisperX\uff09\u572850\u5c0f\u65f6\u610f\u5927\u5229\u7535\u89c6\u8282\u76ee\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4e13\u4e1a\u4eba\u5de5\u5b57\u5e55\u5458\u7684\u5de5\u4f5c\u8fdb\u884c\u57fa\u51c6\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u65e0\u6cd5\u6ee1\u8db3\u5a92\u4f53\u884c\u4e1a\u5bf9\u5b8c\u5168\u81ea\u4e3b\u5316\u7684\u51c6\u786e\u5ea6\u9700\u6c42\uff0c\u4f46\u53ef\u4ee5\u4f5c\u4e3a\u9ad8\u5ea6\u6709\u6548\u7684\u5de5\u5177\u6765\u63d0\u5347\u4eba\u7c7b\u751f\u4ea7\u529b\u3002\u6a21\u578b\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u4eba\u673a\u534f\u540c\u5de5\u4f5c\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\uff08HITL\uff09\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u6b64\u8bbe\u8ba1\u4e86\u652f\u6301\u8be5\u5de5\u4f5c\u6d41\u7a0b\u7684\u751f\u4ea7\u7ea7\u4e91\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u610f\u5927\u5229\u5a92\u4f53\u516c\u53f8\u63d0\u4f9b\u5b9e\u7528\u7684\u4e13\u4e1a\u5b57\u5e55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.19171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19171", "abs": "https://arxiv.org/abs/2512.19171", "authors": ["Bingyang Kelvin Liu", "Ziyu Patrick Chen"], "title": "JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation", "comment": null, "summary": "While Joint-Embedding Predictive Architecture (JEPA) has emerged as a powerful architecture for learning rich latent representations, it fundamentally lacks generative abilities. Meanwhile, latent space reasoning attempts for Transformer models like COCONUT do improve performance, but they ultimately rely on token-by-token generation, which still accumulates compounding error and relies on context information to gain reasoning insights. To address these limitations, we propose JEPA-Reasoner, a novel JEPA model enhanced with generative ability that reasons in latent space. We augment it with a separate action-taker model, Talker, to produce human-readable sentences. Our approach demonstrates that decoupling latent space reasoning and token generation enables JEPA-Reasoner to produce mixed latent vectors that might lay the foundation for multi-threaded reasoning, while performing autoregressive generation with superior robustness to compounding error.", "AI": {"tldr": "JEPA-Reasoner\uff1a\u4e00\u79cd\u589e\u5f3a\u751f\u6210\u80fd\u529b\u7684JEPA\u6a21\u578b\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\uff0c\u901a\u8fc7\u5206\u79bb\u7684Talker\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u53e5\u5b50\uff0c\u89e3\u51b3\u4e86JEPA\u7f3a\u4e4f\u751f\u6210\u80fd\u529b\u548c\u4f20\u7edfTransformer\u7d2f\u79ef\u8bef\u5dee\u7684\u95ee\u9898\u3002", "motivation": "JEPA\u67b6\u6784\u867d\u7136\u80fd\u5b66\u4e60\u4e30\u5bcc\u7684\u6f5c\u5728\u8868\u793a\uff0c\u4f46\u7f3a\u4e4f\u751f\u6210\u80fd\u529b\uff1b\u800c\u50cfCOCONUT\u8fd9\u6837\u7684Transformer\u6a21\u578b\u867d\u7136\u5c1d\u8bd5\u5728\u6f5c\u5728\u7a7a\u95f4\u63a8\u7406\uff0c\u4f46\u4ecd\u4f9d\u8d56\u9010token\u751f\u6210\uff0c\u4f1a\u7d2f\u79ef\u590d\u5408\u8bef\u5dee\u5e76\u4f9d\u8d56\u4e0a\u4e0b\u6587\u4fe1\u606f\u83b7\u53d6\u63a8\u7406\u6d1e\u5bdf\u3002", "method": "\u63d0\u51faJEPA-Reasoner\uff0c\u4e00\u79cd\u589e\u5f3a\u751f\u6210\u80fd\u529b\u7684JEPA\u6a21\u578b\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\u3002\u540c\u65f6\u589e\u52a0\u4e00\u4e2a\u72ec\u7acb\u7684\u52a8\u4f5c\u6267\u884c\u6a21\u578bTalker\u6765\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u53e5\u5b50\uff0c\u5c06\u6f5c\u5728\u7a7a\u95f4\u63a8\u7406\u4e0etoken\u751f\u6210\u89e3\u8026\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u6df7\u5408\u6f5c\u5728\u5411\u91cf\uff0c\u53ef\u80fd\u4e3a\u591a\u7ebf\u7a0b\u63a8\u7406\u5960\u5b9a\u57fa\u7840\uff0c\u540c\u65f6\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u5c55\u73b0\u51fa\u5bf9\u590d\u5408\u8bef\u5dee\u7684\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u89e3\u8026\u6f5c\u5728\u7a7a\u95f4\u63a8\u7406\u548ctoken\u751f\u6210\u4f7fJEPA-Reasoner\u80fd\u591f\u4ea7\u751f\u6df7\u5408\u6f5c\u5728\u5411\u91cf\uff0c\u652f\u6301\u591a\u7ebf\u7a0b\u63a8\u7406\uff0c\u5e76\u5728\u81ea\u56de\u5f52\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u5bf9\u590d\u5408\u8bef\u5dee\u7684\u4f18\u8d8a\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.19173", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19173", "abs": "https://arxiv.org/abs/2512.19173", "authors": ["Dazhen Deng", "Sen Yang", "Yuchen He", "Yuan Tian", "Yingcai Wu"], "title": "CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation", "comment": null, "summary": "Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.", "AI": {"tldr": "CycleChart\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u53cc\u5411\u56fe\u8868\u7406\u89e3\u4e0e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5f0f\u4e2d\u5fc3\u5316\u516c\u5f0f\u7edf\u4e00\u4e0d\u540c\u4efb\u52a1\uff0c\u5229\u7528\u751f\u6210-\u89e3\u6790\u4e00\u81f4\u6027\u76ee\u6807\u5b66\u4e60\u8de8\u65b9\u5411\u56fe\u8868\u8bed\u4e49\u3002", "motivation": "\u5f53\u524d\u56fe\u8868\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u56fe\u8868\u95ee\u7b54\u3001\u56fe\u8868\u89e3\u6790\u548c\u56fe\u8868\u751f\u6210\uff09\u901a\u5e38\u5b64\u7acb\u7814\u7a76\uff0c\u963b\u788d\u6a21\u578b\u5b66\u4e60\u8fde\u63a5\u56fe\u8868\u751f\u6210\u4e0e\u89e3\u91ca\u7684\u5171\u4eab\u8bed\u4e49\u3002\u9700\u8981\u7edf\u4e00\u6846\u67b6\u4fc3\u8fdb\u8de8\u4efb\u52a1\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u6a21\u5f0f\u4e2d\u5fc3\u5316\u516c\u5f0f\u4f5c\u4e3a\u8de8\u4efb\u52a1\u901a\u7528\u63a5\u53e3\uff1b\u6784\u5efa\u4e00\u81f4\u7684\u591a\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u56fe\u8868\u6837\u672c\u5305\u542b\u5bf9\u9f50\u7684\u6a21\u5f0f\u9884\u6d4b\u3001\u6570\u636e\u89e3\u6790\u548c\u95ee\u7b54\u6807\u6ce8\uff1b\u5f15\u5165\u751f\u6210-\u89e3\u6790\u4e00\u81f4\u6027\u76ee\u6807\uff1a\u6a21\u578b\u4ece\u8868\u683c\u548c\u6587\u672c\u67e5\u8be2\u751f\u6210\u56fe\u8868\u6a21\u5f0f\uff0c\u7136\u540e\u5b66\u4e60\u4ece\u751f\u6210\u7684\u56fe\u8868\u4e2d\u6062\u590d\u6a21\u5f0f\u548c\u6570\u636e\uff0c\u5f3a\u5236\u8de8\u65b9\u5411\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "CycleChart\u5728\u56fe\u8868\u751f\u6210\u3001\u56fe\u8868\u89e3\u6790\u548c\u56fe\u8868\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u5f3a\u52b2\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u6807\u5fd7\u7740\u5411\u66f4\u901a\u7528\u56fe\u8868\u7406\u89e3\u6a21\u578b\u8fc8\u8fdb\u4e00\u6b65\u3002", "conclusion": "CycleChart\u901a\u8fc7\u4e00\u81f4\u6027\u5b66\u4e60\u6846\u67b6\u7edf\u4e00\u56fe\u8868\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u53cc\u5411\u8bed\u4e49\u5bf9\u9f50\u5bf9\u63d0\u5347\u8de8\u4efb\u52a1\u6027\u80fd\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u7684\u56fe\u8868\u7406\u89e3\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.19238", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19238", "abs": "https://arxiv.org/abs/2512.19238", "authors": ["Anna-Maria Gueorguieva", "Aylin Caliskan"], "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation", "comment": null, "summary": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u975e\u53d7\u4fdd\u62a4\u6c61\u540d\u5316\u8eab\u4efd\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u5371\u9669\u5ea6\u9ad8\u7684\u6c61\u540d\uff08\u5982\u5e2e\u6d3e\u6210\u5458\u3001HIV\u60a3\u8005\uff09\u504f\u89c1\u6700\u4e25\u91cd\uff0c\u800c\u793e\u4f1a\u4eba\u53e3\u5b66\u6c61\u540d\u504f\u89c1\u6700\u5c11\u3002\u62a4\u680f\u6a21\u578b\u80fd\u51cf\u5c11\u504f\u89c1\u4f46\u6548\u679c\u6709\u9650\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u663e\u793a\u51fa\u793e\u4f1a\u504f\u89c1\uff0c\u4f46\u5bf9\u975e\u53d7\u4fdd\u62a4\u6c61\u540d\u5316\u8eab\u4efd\u7684\u504f\u89c1\u7814\u7a76\u4e0d\u8db3\u3002\u5fc3\u7406\u5b66\u7814\u7a76\u8868\u660e\u6c61\u540d\u5305\u542b\u516d\u4e2a\u793e\u4f1a\u7279\u5f81\uff1a\u5ba1\u7f8e\u6027\u3001\u53ef\u9690\u85cf\u6027\u3001\u75c5\u7a0b\u3001\u7834\u574f\u6027\u3001\u8d77\u6e90\u548c\u5371\u9669\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8fd9\u4e9b\u7279\u5f81\u5982\u4f55\u5f71\u54cdLLM\u5bf9\u6c61\u540d\u5316\u7fa4\u4f53\u7684\u504f\u89c1\u3002", "method": "\u4f7f\u7528SocialStigmaQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b37\u4e2a\u5173\u4e8e\u6c61\u540d\u5316\u8eab\u4efd\u7684\u793e\u4f1a\u573a\u666f\u3002\u8bc4\u4f30\u4e09\u4e2a\u4e3b\u6d41LLM\uff08Granite 3.0-8B\u3001Llama-3.1-8B\u3001Mistral-7B\uff09\u5bf993\u4e2a\u6c61\u540d\u5316\u7fa4\u4f53\u7684\u504f\u89c1\u3002\u5206\u6790\u4eba\u7c7b\u548cLLM\u5bf9\u6c61\u540d\u7279\u5f81\u7684\u8bc4\u5206\u3001\u63d0\u793a\u98ce\u683c\u548c\u6c61\u540d\u7c7b\u578b\u5bf9\u504f\u89c1\u7684\u5f71\u54cd\u3002\u6d4b\u8bd5\u5404\u6a21\u578b\u7684\u62a4\u680f\u7cfb\u7edf\uff08Granite Guardian 3.0\u3001Llama Guard 3.0\u3001Mistral Moderation API\uff09\u7684\u504f\u89c1\u7f13\u89e3\u6548\u679c\u3002", "result": "\u4eba\u7c7b\u8bc4\u4e3a\u9ad8\u5ea6\u5371\u9669\u7684\u6c61\u540d\uff08\u5982\u5e2e\u6d3e\u6210\u5458\u3001HIV\u60a3\u8005\uff09\u5728SocialStigmaQA\u63d0\u793a\u4e2d\u504f\u89c1\u6700\u4e25\u91cd\uff08\u6240\u6709\u6a21\u578b60%\u8f93\u51fa\u6709\u504f\u89c1\uff09\uff0c\u800c\u793e\u4f1a\u4eba\u53e3\u5b66\u6c61\u540d\uff08\u5982\u4e9a\u88d4\u7f8e\u56fd\u4eba\u3001\u8001\u5e74\u4eba\uff09\u504f\u89c1\u6700\u5c11\uff0811%\uff09\u3002\u62a4\u680f\u6a21\u578b\u80fd\u51cf\u5c11\u504f\u89c1\uff08\u5206\u522b\u51cf\u5c1110.4%\u30011.4%\u30017.8%\uff09\uff0c\u4f46\u504f\u89c1\u7279\u5f81\u7684\u5f71\u54cd\u6a21\u5f0f\u672a\u6539\u53d8\uff0c\u4e14\u62a4\u680f\u6a21\u578b\u5e38\u65e0\u6cd5\u8bc6\u522b\u63d0\u793a\u4e2d\u7684\u504f\u89c1\u610f\u56fe\u3002", "conclusion": "LLM\u5bf9\u6c61\u540d\u5316\u7fa4\u4f53\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u7279\u522b\u662f\u5bf9\u5371\u9669\u5ea6\u9ad8\u7684\u6c61\u540d\u3002\u62a4\u680f\u6a21\u578b\u80fd\u90e8\u5206\u7f13\u89e3\u504f\u89c1\u4f46\u6548\u679c\u6709\u9650\uff0c\u65e0\u6cd5\u6839\u672c\u6539\u53d8\u504f\u89c1\u6a21\u5f0f\u3002\u8fd9\u5bf9\u6d89\u53ca\u6c61\u540d\u5316\u7fa4\u4f53\u7684LLM\u5e94\u7528\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u6539\u8fdb\u62a4\u680f\u6a21\u578b\u7684\u504f\u89c1\u7f13\u89e3\u80fd\u529b\u3002"}}
{"id": "2512.19240", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19240", "abs": "https://arxiv.org/abs/2512.19240", "authors": ["Mingxu Zhang", "Dazhong Shen", "Qi Zhang", "Ying Sun"], "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.", "AI": {"tldr": "ChemATP\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u539f\u5b50\u7ea7\u6587\u672c\u77e5\u8bc6\u5e93\uff0c\u8ba9\u51bb\u7ed3\u7684LLM\u80fd\u591f\u52a8\u6001\u68c0\u7d22\u548c\u63a8\u7406\u5316\u5b66\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5206\u5b50\u79d1\u5b66\u4e2d\u7f3a\u4e4f\u5316\u5b66\u5148\u9a8c\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5b50\u79d1\u5b66\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6807\u51c6\u5b57\u7b26\u4e32\u8868\u793a\u7f3a\u4e4f\u660e\u786e\u7684\u5316\u5b66\u5148\u9a8c\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u5c06\u5148\u9a8c\u6ce8\u5165\u53c2\u6570\uff0c\u4f46\u9759\u6001\u8026\u5408\u963b\u788d\u77e5\u8bc6\u5feb\u901f\u66f4\u65b0\u5e76\u635f\u5bb3\u901a\u7528\u63a8\u7406\u80fd\u529b\uff1b\u800c\u514d\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u8868\u9762\u63d0\u793a\uff0c\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u5316\u5b66\u63a8\u7406\u6240\u9700\u7684\u539f\u5b50\u7ea7\u5148\u9a8c\u3002", "method": "\u63d0\u51faChemATP\u6846\u67b6\uff0c\u5c06\u5316\u5b66\u77e5\u8bc6\u4e0e\u63a8\u7406\u5f15\u64ce\u89e3\u8026\u3002\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u539f\u5b50\u7ea7\u6587\u672c\u77e5\u8bc6\u5e93\uff0c\u4f7f\u51bb\u7ed3\u7684LLM\u80fd\u591f\u52a8\u6001\u68c0\u7d22\u548c\u63a8\u7406\u8fd9\u4e9b\u4fe1\u606f\u3002\u8fd9\u79cd\u67b6\u6784\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u4fdd\u7559LLM\u7684\u5185\u5728\u901a\u7528\u667a\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cChemATP\u663e\u8457\u4f18\u4e8e\u514d\u8bad\u7ec3\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u8bc1\u660e\u663e\u5f0f\u5148\u9a8c\u6ce8\u5165\u662f\u9690\u5f0f\u53c2\u6570\u66f4\u65b0\u7684\u7ade\u4e89\u6027\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "ChemATP\u901a\u8fc7\u89e3\u8026\u5316\u5b66\u77e5\u8bc6\u4e0e\u63a8\u7406\u5f15\u64ce\uff0c\u4e3aLLM\u5728\u5206\u5b50\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u9002\u5e94\u4e14\u4fdd\u6301\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u5f0f\u5148\u9a8c\u6ce8\u5165\u662f\u6709\u6548\u7684\u66ff\u4ee3\u53c2\u6570\u66f4\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.19247", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19247", "abs": "https://arxiv.org/abs/2512.19247", "authors": ["Do Minh Duc", "Quan Xuan Truong", "Nguyen Tat Dat", "Nguyen Van Vinh"], "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics", "comment": null, "summary": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408RAG\u3001few-shot\u3001CoT\u548cAuto-CoT\u7684\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u7269\u6d41\u6587\u672c\u4e2d\u7684\u6846\u67b6\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\uff0c\u5728\u771f\u5b9e\u7269\u6d41\u6587\u672c\u6807\u6ce8\u4efb\u52a1\u4e2d\u63d0\u5347\u63a8\u7406\u51c6\u786e\u738715%", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u548c\u6807\u6ce8\u4efb\u52a1\u4e2d\u9700\u8981\u63d0\u793a\u5de5\u7a0b\u6765\u907f\u514d\u5927\u91cf\u5fae\u8c03\uff0c\u7279\u522b\u662f\u5728\u7269\u6d41\u6587\u672c\u5904\u7406\u8fd9\u7c7b\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u6846\u67b6\u68c0\u6d4b\u65b9\u6cd5", "method": "\u63d0\u51fa\u4e00\u4e2a\u63d0\u793a\u4f18\u5316\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u3001\u5c11\u6837\u672c\u63d0\u793a\u3001\u601d\u7ef4\u94fe\u63a8\u7406(CoT)\u548c\u81ea\u52a8CoT\u5408\u6210(Auto-CoT)\uff0c\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u63d0\u793a\u4f18\u5316\u4ee3\u7406\u901a\u8fc7\u68c0\u7d22\u793a\u4f8b\u3001\u6027\u80fd\u53cd\u9988\u548c\u81ea\u8bc4\u4f30\u8fed\u4ee3\u4f18\u5316\u63d0\u793a", "result": "\u5728\u771f\u5b9e\u7269\u6d41\u6587\u672c\u6807\u6ce8\u4efb\u52a1\u4e2d\uff0c\u4f18\u5316\u540e\u7684\u63d0\u793a\uff08\u7279\u522b\u662f\u901a\u8fc7Auto-CoT\u548cRAG\u589e\u5f3a\u7684\uff09\u76f8\u6bd4\u57fa\u7ebf\u96f6\u6837\u672c\u6216\u9759\u6001\u63d0\u793a\uff0c\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe15%\uff0c\u5728GPT-4o\u3001Qwen 2.5\u548cLLaMA 3.1\u7b49\u591a\u4e2aLLM\u4e0a\u8868\u73b0\u4e00\u81f4", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u4f18\u5316\u662f\u5b8c\u6574\u5fae\u8c03\u7684\u53ef\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u5728\u7269\u6d41\u7b49\u7279\u5b9a\u9886\u57dfNLP\u5e94\u7528\u4e2d\u90e8\u7f72LLM\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.19305", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19305", "abs": "https://arxiv.org/abs/2512.19305", "authors": ["Javier Vela-Tambo", "Jorge Gracia", "Fernando Dominguez-Castro"], "title": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs", "comment": null, "summary": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.", "AI": {"tldr": "CienaLLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u5f0f\u5f15\u5bfc\u751f\u6210\u5f0f\u4fe1\u606f\u63d0\u53d6\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u65b0\u95fb\u6587\u7ae0\u4e2d\u96f6\u6837\u672c\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u652f\u6301\u53ef\u914d\u7f6e\u63d0\u793a\u548c\u8f93\u51fa\u6a21\u5f0f\uff0c\u5728\u5e72\u65f1\u5f71\u54cd\u63d0\u53d6\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u76d1\u7763\u57fa\u7ebf\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5927\u89c4\u6a21\u76d1\u6d4b\u6c14\u5019\u707e\u5bb3\u7684\u793e\u4f1a\u7ecf\u6d4e\u5f71\u54cd\uff0c\u9700\u8981\u4ece\u5f02\u6784\u65b0\u95fb\u6587\u7ae0\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "method": "\u5f00\u53d1CienaLLM\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5f0f\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u63d0\u53d6\uff0c\u652f\u6301\u53ef\u914d\u7f6e\u63d0\u793a\u3001\u8f93\u51fa\u6a21\u5f0f\u548c\u591a\u6b65\u9aa4\u7ba1\u9053\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u56e0\u5b50\u5b9e\u9a8c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u3001\u5927\u5c0f\u3001\u7cbe\u5ea6\u548c\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "\u54cd\u5e94\u89e3\u6790\u6b65\u9aa4\u51e0\u4e4e\u6d88\u9664\u4e86\u683c\u5f0f\u9519\u8bef\uff1b\u66f4\u5927\u6a21\u578b\u63d0\u4f9b\u6700\u5f3a\u6700\u7a33\u5b9a\u7684\u6027\u80fd\uff1b\u91cf\u5316\u63d0\u4f9b\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u4f46\u7cbe\u5ea6\u7565\u6709\u4e0b\u964d\uff1b\u63d0\u793a\u7b56\u7565\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02\uff1b\u5728\u897f\u73ed\u7259\u65b0\u95fb\u5e72\u65f1\u5f71\u54cd\u63d0\u53d6\u4efb\u52a1\u4e0a\uff0cCienaLLM\u8fbe\u5230\u6216\u8d85\u8fc7\u76d1\u7763\u57fa\u7ebf\u7cbe\u5ea6\uff0c\u4f46\u63a8\u7406\u6210\u672c\u66f4\u9ad8\u3002", "conclusion": "CienaLLM\u7684\u6a21\u5f0f\u9a71\u52a8\u548c\u6a21\u578b\u65e0\u5173\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u901a\u8fc7\u7f16\u8f91\u63d0\u793a\u548c\u6a21\u5f0f\uff08\u800c\u975e\u91cd\u65b0\u8bad\u7ec3\uff09\u9002\u5e94\u76f8\u5173\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\uff0c\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u6c14\u5019\u707e\u5bb3\u5f71\u54cd\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2512.19378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19378", "abs": "https://arxiv.org/abs/2512.19378", "authors": ["Zhiqing Hu", "Chenxu Zhao", "Jiazhong Lu", "Xiaolei Liu"], "title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models", "comment": "Camera-ready version of the paper accepted for oral presentation at the 11th International Conference on Computer and Communications (ICCC 2025)", "summary": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e09\u8272\u5206\u533a\uff08\u7eff/\u9ec4/\u7ea2\uff09\u7684LLM\u6c34\u5370\u65b9\u6848\uff0c\u901a\u8fc7\u9650\u5236\u91c7\u6837\u8303\u56f4\u5d4c\u5165\u4fe1\u53f7\uff0c\u4f7f\u7528Fisher\u65b9\u6cd5\u805a\u5408p\u503c\u8fdb\u884c\u68c0\u6d4b\uff0c\u5728Llama 2 7B\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u6587\u672c\u8d28\u91cf\u4fdd\u6301\u3002", "motivation": "\u4e3a\u4e86\u904f\u5236LLM\u751f\u6210\u6587\u672c\u7684\u6ee5\u7528\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5d4c\u5165\u9690\u5f0f\u4fe1\u53f7\u7684\u6c34\u5370\u6280\u672f\uff0c\u4f7f\u751f\u6210\u7684\u6587\u672c\u80fd\u591f\u88ab\u6709\u6548\u68c0\u6d4b\u548c\u8ffd\u8e2a\u3002", "method": "\u5728\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u5c06\u8bcd\u6c47\u8868\u5212\u5206\u4e3a\u7eff/\u9ec4/\u7ea2\u4e09\u4e2a\u56fa\u5b9a\u6bd4\u4f8b\u7684\u5206\u533a\uff0c\u9650\u5236\u91c7\u6837\u5230\u7eff\u533a\u548c\u9ec4\u533a\u3002\u68c0\u6d4b\u65f6\u91cd\u653e\u76f8\u540c\u5206\u533a\uff0c\u8ba1\u7b97\u7eff\u533a\u5bcc\u96c6\u548c\u7ea2\u533a\u8d2b\u4e4f\u7edf\u8ba1\u91cf\uff0c\u8f6c\u6362\u4e3a\u5355\u4fa7z\u5206\u6570\uff0c\u901a\u8fc7Fisher\u65b9\u6cd5\u805a\u5408p\u503c\u5224\u65ad\u6587\u672c\u662f\u5426\u542b\u6c34\u5370\u3002", "result": "\u5728Llama 2 7B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e09\u8272\u5206\u533a\u65b9\u6848\u5728\u56fa\u5b9a\u8bef\u62a5\u7387\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6587\u672c\u7684\u53ef\u8bfb\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u8272\u5206\u533a\u6c34\u5370\u65b9\u6848\u80fd\u6709\u6548\u68c0\u6d4bLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u5728\u4fdd\u6301\u6587\u672c\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u9760\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3aLLM\u6587\u672c\u6ee5\u7528\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.19400", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19400", "abs": "https://arxiv.org/abs/2512.19400", "authors": ["Yacouba Diarra", "Panga Azazia Kamate", "Nouhoum Souleymane Coulibaly", "Michael Leventhal"], "title": "Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara", "comment": "7 pages, 2 figures", "summary": "We present Kunkado, a 160-hour Bambara ASR dataset compiled from Malian radio archives to capture present-day spontaneous speech across a wide range of topics. It includes code-switching, disfluencies, background noise, and overlapping speakers that practical ASR systems encounter in real-world use. We finetuned Parakeet-based models on a 33.47-hour human-reviewed subset and apply pragmatic transcript normalization to reduce variability in number formatting, tags, and code-switching annotations. Evaluated on two real-world test sets, finetuning with Kunkado reduces WER from 44.47\\% to 37.12\\% on one and from 36.07\\% to 32.33\\% on the other. In human evaluation, the resulting model also outperforms a comparable system with the same architecture trained on 98 hours of cleaner, less realistic speech. We release the data and models to support robust ASR for predominantly oral languages.", "AI": {"tldr": "Kunkado\u662f\u4e00\u4e2a160\u5c0f\u65f6\u7684\u73ed\u5df4\u62c9\u8bedASR\u6570\u636e\u96c6\uff0c\u4ece\u9a6c\u91cc\u5e7f\u64ad\u6863\u6848\u4e2d\u6536\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u81ea\u53d1\u8bed\u97f3\u7279\u5f81\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u7684\u6a21\u578b\u5728\u4e24\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u4e86WER\uff0c\u5e76\u5728\u4eba\u5de5\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u4f7f\u7528\u66f4\u5e72\u51c0\u4f46\u8f83\u5c11\u771f\u5b9e\u8bed\u97f3\u8bad\u7ec3\u7684\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u652f\u6301\u4e3b\u8981\u53e3\u8bed\u8bed\u8a00\u7684\u9c81\u68d2ASR\u7cfb\u7edf\uff0c\u9700\u8981\u5305\u542b\u771f\u5b9e\u4e16\u754c\u8bed\u97f3\u7279\u5f81\uff08\u5982\u8bed\u7801\u8f6c\u6362\u3001\u4e0d\u6d41\u7545\u3001\u80cc\u666f\u566a\u58f0\u3001\u8bf4\u8bdd\u4eba\u91cd\u53e0\uff09\u7684\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u8fc7\u4e8e\u5e72\u51c0\uff0c\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "method": "\u4ece\u9a6c\u91cc\u5e7f\u64ad\u6863\u6848\u4e2d\u7f16\u8bd1160\u5c0f\u65f6\u7684\u73ed\u5df4\u62c9\u8bedASR\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u8bed\u97f3\u7279\u5f81\u3002\u4f7f\u752833.47\u5c0f\u65f6\u4eba\u5de5\u5ba1\u6838\u7684\u5b50\u96c6\u5fae\u8c03Parakeet\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u5b9e\u7528\u7684\u8f6c\u5f55\u89c4\u8303\u5316\u6765\u51cf\u5c11\u6570\u5b57\u683c\u5f0f\u3001\u6807\u7b7e\u548c\u8bed\u7801\u8f6c\u6362\u6807\u6ce8\u7684\u53d8\u5f02\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4f7f\u7528Kunkado\u5fae\u8c03\u7684\u6a21\u578b\u5c06WER\u4ece44.47%\u964d\u4f4e\u523037.12%\uff08\u6d4b\u8bd5\u96c61\uff09\uff0c\u4ece36.07%\u964d\u4f4e\u523032.33%\uff08\u6d4b\u8bd5\u96c62\uff09\u3002\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u4f7f\u752898\u5c0f\u65f6\u66f4\u5e72\u51c0\u4f46\u8f83\u5c11\u771f\u5b9e\u8bed\u97f3\u8bad\u7ec3\u7684\u53ef\u6bd4\u7cfb\u7edf\u3002", "conclusion": "Kunkado\u6570\u636e\u96c6\u6709\u6548\u652f\u6301\u4e86\u73ed\u5df4\u62c9\u8bed\u7b49\u4e3b\u8981\u53e3\u8bed\u8bed\u8a00\u7684\u9c81\u68d2ASR\u7cfb\u7edf\u5f00\u53d1\u3002\u5305\u542b\u771f\u5b9e\u4e16\u754c\u8bed\u97f3\u7279\u5f81\u7684\u6570\u636e\u96c6\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f5c\u8005\u53d1\u5e03\u4e86\u6570\u636e\u548c\u6a21\u578b\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2512.19424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19424", "abs": "https://arxiv.org/abs/2512.19424", "authors": ["Jian Yang", "Wei Zhang", "Yizhi Li", "Shawn Guo", "Haowen Wang", "Aishan Liu", "Ge Zhang", "Zili Wang", "Zhoujun Li", "Xianglong Liu", "Weifeng Lv"], "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models", "comment": null, "summary": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CodeSimpleQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7a0b\u77e5\u8bc6\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u5f00\u53d1\u4e86\u5305\u542b6600\u4e07\u6837\u672c\u7684\u6307\u4ee4\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff1a\u786e\u4fdd\u6a21\u578b\u751f\u6210\u7684\u7f16\u7a0b\u6982\u5ff5\u3001\u6280\u672f\u5b9e\u73b0\u7b49\u54cd\u5e94\u5728\u4e8b\u5b9e\u4e0a\u51c6\u786e\u3002\u5927\u591a\u6570\u73b0\u6709\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u53ea\u5173\u6ce8\u4ee3\u7801\u6267\u884c\u6b63\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u7f16\u7a0b\u77e5\u8bc6\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "1) \u521b\u5efaCodeSimpleQA\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7cbe\u5fc3\u7b56\u5212\u7684\u82f1\u6587\u548c\u4e2d\u6587\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\uff1b2) \u6784\u5efaCodeSimpleQA-Instruct\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5305\u542b6600\u4e07\u4e2a\u6837\u672c\uff1b3) \u5f00\u53d1\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u5bf9\u591a\u79cdLLM\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u524d\u6cbf\u7684LLM\u5728\u4ee3\u7801\u4e8b\u5b9e\u6027\u65b9\u9762\u4e5f\u5b58\u5728\u56f0\u96be\u3002\u63d0\u51fa\u7684\u8bad\u7ec3\u6846\u67b6\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6709\u663e\u8457\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u4e8b\u5b9e\u6027\u5bf9\u9f50\u5728\u5f00\u53d1\u53ef\u9760\u4ee3\u7801LLM\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u4ee3\u7801LLM\u5728\u7f16\u7a0b\u77e5\u8bc6\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u4e8b\u5b9e\u6027\u5bf9\u9f50\u8bad\u7ec3\u3002CodeSimpleQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u63d0\u51fa\u7684\u8bad\u7ec3\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u4ee3\u7801LLM\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.19432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19432", "abs": "https://arxiv.org/abs/2512.19432", "authors": ["Quyu Kong", "Xu Zhang", "Zhenyu Yang", "Nolan Gao", "Chen Liu", "Panrong Tong", "Chenglin Cai", "Hanzhang Zhou", "Jianan Zhang", "Liangyu Chen", "Zhidan Liu", "Steven Hoi", "Yue Wang"], "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments", "comment": null, "summary": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.", "AI": {"tldr": "MobileWorld\uff1a\u4e00\u4e2a\u6bd4AndroidWorld\u66f4\u5177\u6311\u6218\u6027\u7684\u79fb\u52a8\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8de8\u5e94\u7528\u4ea4\u4e92\u3001\u7528\u6237\u4e92\u52a8\u548cMCP\u589e\u5f3a\u4efb\u52a1\uff0c\u6210\u529f\u7387\u5927\u5e45\u4e0b\u964d\u81f351.7%", "motivation": "\u73b0\u6709AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u5df2\u9971\u548c\uff08\u6210\u529f\u7387\u8d8590%\uff09\uff0c\u7f3a\u4e4f\u7535\u5546\u3001\u4f01\u4e1a\u901a\u4fe1\u7b49\u5173\u952e\u5e94\u7528\u7c7b\u522b\uff0c\u4e14\u672a\u53cd\u6620\u771f\u5b9e\u79fb\u52a8\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u6a21\u7cca\u6307\u4ee4\u548c\u6df7\u5408\u5de5\u5177\u4f7f\u7528", "method": "\u6784\u5efa\u5305\u542b201\u4e2a\u4efb\u52a1\u300120\u4e2a\u5e94\u7528\u7684MobileWorld\u57fa\u51c6\uff0c\u5f3a\u8c03\u957f\u65f6\u7a0b\u8de8\u5e94\u7528\u4ea4\u4e92\uff08\u5e73\u574727.8\u6b65\uff09\uff0c\u5f15\u5165\u7528\u6237\u4e92\u52a8\u548cMCP\u589e\u5f3a\u4efb\u52a1\uff0c\u63d0\u4f9b\u5feb\u7167\u5bb9\u5668\u73af\u5883\u548c\u7cbe\u786e\u529f\u80fd\u9a8c\u8bc1", "result": "\u6700\u4f73\u667a\u80fd\u4f53\u6846\u67b6\u548c\u7aef\u5230\u7aef\u6a21\u578b\u6210\u529f\u7387\u5206\u522b\u4e3a51.7%\u548c20.9%\uff0c\u8fdc\u4f4e\u4e8eAndroidWorld\uff0c\u6a21\u578b\u5728\u7528\u6237\u4e92\u52a8\u548cMCP\u8c03\u7528\u65b9\u9762\u8868\u73b0\u663e\u8457\u4e0d\u8db3", "conclusion": "MobileWorld\u4e3a\u4e0b\u4e00\u4ee3\u79fb\u52a8\u667a\u80fd\u63d0\u4f9b\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6307\u660e\u4e86\u7528\u6237\u4e92\u52a8\u548cMCP\u80fd\u529b\u7684\u53d1\u5c55\u65b9\u5411"}}
{"id": "2512.19455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19455", "abs": "https://arxiv.org/abs/2512.19455", "authors": ["Thittipat Pairatsuppawat", "Abhibhu Tachaapornchai", "Paweekorn Kusolsomboon", "Chutikan Chaiwong", "Thodsaporn Chay-intr", "Kobkrit Viriyayudhakorn", "Nongnuch Ketui", "Aslan B. Wong"], "title": "SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation", "comment": null, "summary": "Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.", "AI": {"tldr": "SiamGPT-32B\u662f\u57fa\u4e8eQwen3-32B\u5fae\u8c03\u7684\u5f00\u6e90\u6cf0\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u8d28\u91cf\u4f18\u5148\u7b56\u7565\uff0c\u901a\u8fc7\u7ffb\u8bd1\u9ad8\u8d28\u91cf\u82f1\u6587\u6307\u4ee4\u6570\u636e\u548c\u6cf0\u8bed\u9002\u914d\u7684AutoIF\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cf0\u8bed\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u751f\u6210\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cf0\u8bed\u90e8\u7f72\u4e0a\u9762\u4e34\u6311\u6218\uff1a\u5c3d\u7ba1\u82f1\u6587\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u6cf0\u8bed\u751f\u6210\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u89e3\u51b3\u6cf0\u8bed\u6307\u4ee4\u9075\u5faa\u3001\u591a\u8f6e\u5bf9\u8bdd\u9c81\u68d2\u6027\u548c\u8bed\u8a00\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u57fa\u4e8eQwen3-32B\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u91c7\u7528\u8d28\u91cf\u4f18\u5148\u7b56\u7565\uff0c\u5f3a\u8c03\u6570\u636e\u8d28\u91cf\u800c\u975e\u6570\u91cf\u3002\u65b9\u6cd5\u5305\u62ec\uff1a1) \u7ffb\u8bd1\u9ad8\u8d28\u91cf\u590d\u6742\u82f1\u6587\u6307\u4ee4\u6570\u636e\uff1b2) \u6cf0\u8bed\u9002\u914d\u7684AutoIF\u6846\u67b6\u7528\u4e8e\u6307\u4ee4\u548c\u8bed\u8a00\u7ea6\u675f\uff1b3) \u4ec5\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff0c\u65e0\u9700\u6301\u7eed\u9884\u8bad\u7ec3\u6216\u8bed\u6599\u6269\u5c55\u3002", "result": "\u5728SEA-HELM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSiamGPT-32B\u5728\u76f8\u4f3c\u89c4\u6a21\u7684\u5f00\u6e90\u6cf0\u8bed\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6307\u4ee4\u9075\u5faa\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u8d28\u91cf\u4f18\u5148\u7684\u5fae\u8c03\u7b56\u7565\u548c\u6cf0\u8bed\u9002\u914d\u7684AutoIF\u6846\u67b6\uff0cSiamGPT-32B\u6709\u6548\u89e3\u51b3\u4e86\u6cf0\u8bedLLM\u90e8\u7f72\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2512.19456", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19456", "abs": "https://arxiv.org/abs/2512.19456", "authors": ["Jinwei Chi", "Ke Wang", "Yu Chen", "Xuanye Lin", "Qiang Xu"], "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations", "comment": null, "summary": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5728\u8de8\u63d0\u793a\u4f5c\u6587\u8bc4\u5206\u4e2d\u7684\u5224\u522b\u80fd\u529b\uff0c\u53d1\u73b0\u6fc0\u6d3b\u5177\u6709\u5f3a\u5224\u522b\u529b\u4e14LLMs\u80fd\u9002\u5e94\u4e0d\u540c\u8bc4\u5206\u6807\u51c6\u548c\u4f5c\u6587\u7c7b\u578b\u3002", "motivation": "\u8de8\u63d0\u793a\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u9762\u4e34\u8bc4\u5206\u6807\u51c6\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLMs\u7684\u8f93\u51fa\uff0c\u4f46\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u53ef\u80fd\u5305\u542b\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002", "method": "\u4f7f\u7528LLMs\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u62df\u5408\u63a2\u9488\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u548c\u8f93\u5165\u5185\u5bb9\u5bf9\u5224\u522b\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u8ba1\u7b97\u4e0d\u540c\u63d0\u793a\u4e0b\u5404\u7279\u8d28\u7ef4\u5ea6\u7684\u65b9\u5411\u5411\u91cf\u3002", "result": "\u6fc0\u6d3b\u5728\u8bc4\u4f30\u4f5c\u6587\u8d28\u91cf\u65b9\u9762\u5177\u6709\u5f3a\u5224\u522b\u80fd\u529b\uff0cLLMs\u80fd\u6839\u636e\u4e0d\u540c\u7279\u8d28\u548c\u4f5c\u6587\u7c7b\u578b\u8c03\u6574\u8bc4\u4f30\u89c6\u89d2\uff0c\u6709\u6548\u5904\u7406\u8de8\u63d0\u793a\u8bbe\u7f6e\u4e2d\u7684\u8bc4\u5206\u6807\u51c6\u591a\u6837\u6027\u3002", "conclusion": "LLMs\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u4e3a\u8de8\u63d0\u793a\u4f5c\u6587\u8bc4\u5206\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5224\u522b\u4fe1\u606f\uff0c\u6a21\u578b\u80fd\u81ea\u9002\u5e94\u8c03\u6574\u8bc4\u4f30\u89c6\u89d2\uff0c\u4e3a\u89e3\u51b3\u8bc4\u5206\u6807\u51c6\u591a\u6837\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.19475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19475", "abs": "https://arxiv.org/abs/2512.19475", "authors": ["Ivan Decostanzi", "Yelena Mejova", "Kyriaki Kalimeri"], "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting", "comment": "18 pages, 3 figures", "summary": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u5f02\u6784\u4eba\u9053\u4e3b\u4e49\u6587\u6863\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u6709\u8bc1\u636e\u652f\u6491\u7684\u62a5\u544a\u6846\u67b6\uff0c\u572813\u4e2a\u4eba\u9053\u4e3b\u4e49\u4e8b\u4ef6\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u4eba\u9053\u4e3b\u4e49\u51b3\u7b56\u4f9d\u8d56\u7684\u624b\u5de5\u62a5\u544a\u5236\u4f5c\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u3001\u8d44\u6e90\u5bc6\u96c6\u4e14\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u65f6\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u6574\u5408\u8bed\u4e49\u6587\u672c\u805a\u7c7b\u3001\u81ea\u52a8\u95ee\u9898\u751f\u6210\u3001\u68c0\u7d22\u589e\u5f3a\u7684\u5f15\u7528\u5f0f\u7b54\u6848\u63d0\u53d6\u3001\u591a\u7ea7\u6458\u8981\u548c\u884c\u653f\u6458\u8981\u751f\u6210\uff0c\u5e76\u91c7\u7528\u6a21\u62df\u4e13\u5bb6\u63a8\u7406\u7684\u5185\u90e8\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u572813\u4e2a\u4eba\u9053\u4e3b\u4e49\u4e8b\u4ef6\uff08\u81ea\u7136\u707e\u5bb3\u548c\u51b2\u7a81\uff09\u76841100\u591a\u4efd\u6587\u6863\u4e0a\u8bc4\u4f30\uff0c\u751f\u6210\u95ee\u9898\u76f8\u5173\u5ea684.7%\u3001\u91cd\u8981\u602784.0%\u3001\u7d27\u6025\u602776.4%\uff1b\u63d0\u53d6\u7b54\u6848\u76f8\u5173\u5ea686.3%\uff0c\u5f15\u7528\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u5747\u8d85\u8fc776%\uff1b\u4eba\u5de5\u4e0eLLM\u8bc4\u4f30\u4e00\u81f4\u6027F1\u5206\u6570\u8d85\u8fc70.80\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408LLM\u63a8\u7406\u3001\u900f\u660e\u5f15\u7528\u94fe\u63a5\u548c\u591a\u7ea7\u8bc4\u4f30\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u51c6\u786e\u3001\u53ef\u9a8c\u8bc1\u4e14\u5177\u6709\u64cd\u4f5c\u4ef7\u503c\u7684\u4eba\u9053\u4e3b\u4e49\u5f62\u52bf\u62a5\u544a\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2512.19537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19537", "abs": "https://arxiv.org/abs/2512.19537", "authors": ["Bobo Li", "Xudong Han", "Jiang Liu", "Yuzhe Ding", "Liqiang Jing", "Zhaoqi Zhang", "Jinheng Li", "Xinya Du", "Fei Li", "Meishan Zhang", "Min Zhang", "Aixin Sun", "Philip S. Yu", "Hao Fei"], "title": "Event Extraction in Large Language Model", "comment": "38 pages, 9 Figures, 5 Tables", "summary": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u4e8b\u4ef6\u62bd\u53d6\u5728LLM\u65f6\u4ee3\u7684\u53d1\u5c55\uff0c\u4e3b\u5f20\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4e3aLLM\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u8ba4\u77e5\u652f\u67b6\u7684\u7cfb\u7edf\u7ec4\u4ef6\uff0c\u6db5\u76d6\u6587\u672c\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1LLM\u548c\u591a\u6a21\u6001LLM\u6b63\u5728\u6539\u53d8\u4e8b\u4ef6\u62bd\u53d6\u9886\u57df\uff0c\u4f46\u57fa\u4e8eLLM\u7684\u7ba1\u9053\u4ecd\u9762\u4e34\u90e8\u7f72\u5dee\u8ddd\uff1a\u5f31\u7ea6\u675f\u4e0b\u7684\u5e7b\u89c9\u3001\u957f\u4e0a\u4e0b\u6587\u548c\u8de8\u6587\u6863\u7684\u8106\u5f31\u65f6\u95f4\u56e0\u679c\u94fe\u63a5\u3001\u6709\u9650\u7684\u957f\u89c6\u91ce\u77e5\u8bc6\u7ba1\u7406\u3002\u9700\u8981\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4e3aLLM\u4e2d\u5fc3\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u8ba4\u77e5\u652f\u67b6\u7684\u7cfb\u7edf\u7ec4\u4ef6\u3002", "method": "\u901a\u8fc7\u4e8b\u4ef6\u6a21\u5f0f\u548c\u69fd\u7ea6\u675f\u521b\u5efa\u63a5\u53e3\u8fdb\u884c\u63a5\u5730\u548c\u9a8c\u8bc1\uff1b\u4e8b\u4ef6\u4e2d\u5fc3\u7ed3\u6784\u4f5c\u4e3a\u9010\u6b65\u63a8\u7406\u7684\u53d7\u63a7\u4e2d\u95f4\u8868\u793a\uff1b\u4e8b\u4ef6\u94fe\u63a5\u652f\u6301\u57fa\u4e8e\u56fe\u7684RAG\u8fdb\u884c\u5173\u7cfb\u611f\u77e5\u68c0\u7d22\uff1b\u4e8b\u4ef6\u5b58\u50a8\u63d0\u4f9b\u53ef\u66f4\u65b0\u7684\u60c5\u666f\u548c\u4ee3\u7406\u8bb0\u5fc6\u3002\u7efc\u8ff0\u6db5\u76d6\u6587\u672c\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\uff0c\u7ec4\u7ec7\u4efb\u52a1\u548c\u5206\u7c7b\uff0c\u8ffd\u8e2a\u4ece\u57fa\u4e8e\u89c4\u5219\u548c\u795e\u7ecf\u6a21\u578b\u5230\u6307\u4ee4\u9a71\u52a8\u548c\u751f\u6210\u6846\u67b6\u7684\u65b9\u6cd5\u6f14\u8fdb\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u4e8b\u4ef6\u62bd\u53d6\u7684\u8868\u8ff0\u3001\u89e3\u7801\u7b56\u7565\u3001\u67b6\u6784\u3001\u8868\u793a\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u56de\u987e\u4e86\u8de8\u8bed\u8a00\u3001\u4f4e\u8d44\u6e90\u548c\u9886\u57df\u7279\u5b9a\u8bbe\u7f6e\uff0c\u5e76\u7a81\u51fa\u4e86\u53ef\u9760\u4e8b\u4ef6\u4e2d\u5fc3\u7cfb\u7edf\u7684\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u4e8b\u4ef6\u62bd\u53d6\u5e94\u4ece\u9759\u6001\u63d0\u53d6\u6f14\u53d8\u4e3a\u7ed3\u6784\u53ef\u9760\u3001\u4ee3\u7406\u5c31\u7eea\u7684\u611f\u77e5\u548c\u8bb0\u5fc6\u5c42\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u7cfb\u7edf\u670d\u52a1\u3002\u9700\u8981\u89e3\u51b3LLM\u65f6\u4ee3\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5c06\u4e8b\u4ef6\u62bd\u53d6\u53d1\u5c55\u4e3a\u53ef\u9760\u7684\u4e8b\u4ef6\u4e2d\u5fc3\u7cfb\u7edf\u7ec4\u4ef6\u3002"}}
{"id": "2512.19543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19543", "abs": "https://arxiv.org/abs/2512.19543", "authors": ["Zakaria Benmounah", "Abdennour Boulesnane"], "title": "Algerian Dialect", "comment": null, "summary": "We present Algerian Dialect, a large-scale sentiment-annotated dataset consisting of 45,000 YouTube comments written in Algerian Arabic dialect. The comments were collected from more than 30 Algerian press and media channels using the YouTube Data API. Each comment is manually annotated into one of five sentiment categories: very negative, negative, neutral, positive, and very positive. In addition to sentiment labels, the dataset includes rich metadata such as collection timestamps, like counts, video URLs, and annotation dates. This dataset addresses the scarcity of publicly available resources for Algerian dialect and aims to support research in sentiment analysis, dialectal Arabic NLP, and social media analytics. The dataset is publicly available on Mendeley Data under a CC BY 4.0 license at https://doi.org/10.17632/zzwg3nnhsz.2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b45,000\u6761\u963f\u5c14\u53ca\u5229\u4e9a\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00YouTube\u8bc4\u8bba\u7684\u5927\u89c4\u6a21\u60c5\u611f\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u8be5\u65b9\u8a00\u8d44\u6e90\u7a00\u7f3a\u7684\u7a7a\u767d\u3002", "motivation": "\u963f\u5c14\u53ca\u5229\u4e9a\u65b9\u8a00\u7684\u516c\u5f00\u53ef\u7528\u8d44\u6e90\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u8be5\u65b9\u8a00\u7684\u60c5\u611f\u5206\u6790\u3001\u963f\u62c9\u4f2f\u8bedNLP\u548c\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u7814\u7a76\u3002", "method": "\u4f7f\u7528YouTube Data API\u4ece30\u591a\u4e2a\u963f\u5c14\u53ca\u5229\u4e9a\u65b0\u95fb\u548c\u5a92\u4f53\u9891\u9053\u6536\u96c6\u8bc4\u8bba\uff0c\u7136\u540e\u624b\u52a8\u5c06\u6bcf\u6761\u8bc4\u8bba\u6807\u6ce8\u4e3a\u4e94\u79cd\u60c5\u611f\u7c7b\u522b\u4e4b\u4e00\uff08\u975e\u5e38\u8d1f\u9762\u3001\u8d1f\u9762\u3001\u4e2d\u6027\u3001\u6b63\u9762\u3001\u975e\u5e38\u6b63\u9762\uff09\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b45,000\u6761\u6807\u6ce8\u8bc4\u8bba\u7684\u6570\u636e\u96c6\uff0c\u9644\u5e26\u4e30\u5bcc\u5143\u6570\u636e\uff08\u65f6\u95f4\u6233\u3001\u70b9\u8d5e\u6570\u3001\u89c6\u9891URL\u3001\u6807\u6ce8\u65e5\u671f\uff09\uff0c\u5df2\u5728Mendeley Data\u516c\u5f00\u63d0\u4f9b\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u963f\u5c14\u53ca\u5229\u4e9a\u65b9\u8a00\u7684\u60c5\u611f\u5206\u6790\u548cNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u652f\u6301\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2512.19585", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19585", "abs": "https://arxiv.org/abs/2512.19585", "authors": ["Ignacio Iacobacci", "Zhaozhi Qian", "Faroq AL-Tam", "Muhammad AL-Qurishi", "Riad Souissi"], "title": "Increasing the Thinking Budget is Not All You Need", "comment": "4 pages, 4 figures, 3 tables", "summary": "Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5355\u7eaf\u589e\u52a0\u5927\u8bed\u8a00\u6a21\u578b\u7684\"\u601d\u8003\u9884\u7b97\"\uff08\u63a8\u7406\u8fc7\u7a0b\u957f\u5ea6\uff09\u5e76\u975e\u6700\u6709\u6548\u7684\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u65b9\u5f0f\uff0c\u901a\u8fc7\u81ea\u6d3d\u6027\u548c\u81ea\u6211\u53cd\u601d\u7b49\u914d\u7f6e\u53ef\u4ee5\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u7ed3\u679c\u3002", "motivation": "\u968f\u7740\u5177\u5907\u601d\u8003\u80fd\u529b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u51fa\u73b0\uff0c\u65e9\u671f\u7814\u7a76\u5f00\u59cb\u63a2\u7d22\u63a8\u7406\u8fc7\u7a0b\u957f\u5ea6\uff08\u601d\u8003\u9884\u7b97\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u601d\u8003\u9884\u7b97\u4f5c\u4e3a\u5173\u952e\u53c2\u6570\uff0c\u53ca\u5176\u4e0e\u81ea\u6d3d\u6027\u3001\u53cd\u601d\u7b49\u914d\u7f6e\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u8c03\u67e5\u6846\u67b6\uff0c\u5c06\u601d\u8003\u9884\u7b97\u4f5c\u4e3a\u5173\u952e\u53c2\u6570\uff0c\u7814\u7a76\u5176\u4e0e\u5404\u79cd\u914d\u7f6e\uff08\u5982\u81ea\u6d3d\u6027\u3001\u53cd\u601d\u7b49\uff09\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u65e2\u8003\u8651\u6027\u80fd\u7ed3\u679c\u53c8\u8003\u8651\u8ba1\u7b97\u6210\u672c\u7684\u5e73\u8861\u6bd4\u8f83\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5355\u7eaf\u589e\u52a0\u601d\u8003\u9884\u7b97\u5e76\u4e0d\u662f\u6700\u6709\u6548\u7684\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u65b9\u5f0f\u3002\u76f8\u53cd\uff0c\u901a\u8fc7\u81ea\u6d3d\u6027\u548c\u81ea\u6211\u53cd\u601d\u7b49\u66ff\u4ee3\u914d\u7f6e\u53ef\u4ee5\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u54cd\u5e94\u3002", "conclusion": "\u601d\u8003\u9884\u7b97\u7684\u4f18\u5316\u4e0d\u5e94\u4ec5\u4ec5\u5173\u6ce8\u589e\u52a0\u63a8\u7406\u957f\u5ea6\uff0c\u800c\u5e94\u8003\u8651\u4e0e\u81ea\u6d3d\u6027\u3001\u53cd\u601d\u7b49\u914d\u7f6e\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u5e73\u8861\u3002"}}
{"id": "2512.19612", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.19612", "abs": "https://arxiv.org/abs/2512.19612", "authors": ["Angelo Ortiz Tandazo", "Manel Khentout", "Youssef Benchekroun", "Thomas Hueber", "Emmanuel Dupoux"], "title": "MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery", "comment": null, "summary": "This paper introduces MauBERT, a multilingual extension of HuBERT that leverages articulatory features for robust cross-lingual phonetic representation learning. We continue HuBERT pre-training with supervision based on a phonetic-to-articulatory feature mapping in 55 languages. Our models learn from multilingual data to predict articulatory features or phones, resulting in language-independent representations that capture multilingual phonetic properties. Through comprehensive ABX discriminability testing, we show MauBERT models produce more context-invariant representations than state-of-the-art multilingual self-supervised learning models. Additionally, the models effectively adapt to unseen languages and casual speech with minimal self-supervised fine-tuning (10 hours of speech). This establishes an effective approach for instilling linguistic inductive biases in self-supervised speech models.", "AI": {"tldr": "MauBERT\uff1a\u57fa\u4e8eHuBERT\u7684\u591a\u8bed\u8a00\u6269\u5c55\uff0c\u901a\u8fc7\u53d1\u97f3\u7279\u5f81\u76d1\u7763\u5b66\u4e60\u8de8\u8bed\u8a00\u8bed\u97f3\u8868\u793a\uff0c\u572855\u79cd\u8bed\u8a00\u4e0a\u8bad\u7ec3\uff0c\u4ea7\u751f\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u97f3\u8868\u793a\uff0c\u5728ABX\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u7f3a\u4e4f\u8bed\u8a00\u5b66\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u96be\u4ee5\u5b66\u4e60\u8de8\u8bed\u8a00\u7684\u9c81\u68d2\u8bed\u97f3\u8868\u793a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53d1\u97f3\u7279\u5f81\u76d1\u7763\uff0c\u4e3a\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u6ce8\u5165\u8bed\u8a00\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u8bed\u8a00\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u3002", "method": "\u5728HuBERT\u57fa\u7840\u4e0a\u7ee7\u7eed\u9884\u8bad\u7ec3\uff0c\u4f7f\u752855\u79cd\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u6570\u636e\uff0c\u901a\u8fc7\u8bed\u97f3\u5230\u53d1\u97f3\u7279\u5f81\u7684\u6620\u5c04\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\uff0c\u9884\u6d4b\u53d1\u97f3\u7279\u5f81\u6216\u97f3\u7d20\uff0c\u5b66\u4e60\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u97f3\u8868\u793a\u3002", "result": "MauBERT\u6a21\u578b\u5728ABX\u533a\u5206\u6027\u6d4b\u8bd5\u4e2d\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u4ea7\u751f\u66f4\u4e0a\u4e0b\u6587\u4e0d\u53d8\u7684\u8868\u793a\u3002\u6a21\u578b\u80fd\u6709\u6548\u9002\u5e94\u672a\u89c1\u8bed\u8a00\u548c\u65e5\u5e38\u8bed\u97f3\uff0c\u4ec5\u9700\u5c11\u91cf\u81ea\u76d1\u7763\u5fae\u8c03\uff0810\u5c0f\u65f6\u8bed\u97f3\uff09\u3002", "conclusion": "MauBERT\u4e3a\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u6ce8\u5165\u8bed\u8a00\u5b66\u5f52\u7eb3\u504f\u7f6e\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u8de8\u8bed\u8a00\u8bed\u97f3\u8868\u793a\u5b66\u4e60\uff0c\u5728\u591a\u79cd\u8bed\u8a00\u548c\u8bed\u97f3\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.19620", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19620", "abs": "https://arxiv.org/abs/2512.19620", "authors": ["Zahra Sadeghi", "Evangelos Milios", "Frank Rudzicz"], "title": "Exploring the features used for summary evaluation by Human and GPT", "comment": null, "summary": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22LLMs\u8bc4\u4f30\u6458\u8981\u65f6\u4f7f\u7528\u7684\u7279\u5f81\uff0c\u53d1\u73b0\u901a\u8fc7\u6307\u5bfcGPTs\u4f7f\u7528\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u53ef\u4ee5\u6539\u5584\u5176\u5224\u65ad\u5e76\u4e0e\u4eba\u7c7b\u54cd\u5e94\u66f4\u4e00\u81f4", "motivation": "\u867d\u7136LLMs\u5df2\u88ab\u7528\u4f5c\u81ea\u52a8\u6458\u8981\u8bc4\u4f30\u7684\"\u88c1\u5224\"\uff0c\u4f46\u4eba\u4eec\u5e76\u4e0d\u6e05\u695a\u5b83\u4eec\u57fa\u4e8e\u7279\u5b9a\u8d28\u91cf\u7ef4\u5ea6\u8bc4\u4f30\u65f6\u5229\u7528\u4e86\u54ea\u4e9b\u7279\u5f81\u6216\u5c5e\u6027\uff0c\u800c\u4e14\u8bc4\u4f30\u5206\u6570\u4e0e\u6307\u6807\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u4e5f\u7f3a\u4e4f\u7814\u7a76", "method": "\u901a\u8fc7\u7814\u7a76\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u6307\u6807\uff0c\u53d1\u73b0\u4e0e\u4eba\u7c7b\u548cGPTs\u54cd\u5e94\u5bf9\u9f50\u7684\u7279\u5f81\uff0c\u5e76\u6307\u5bfcGPTs\u4f7f\u7528\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u6765\u6539\u8fdb\u5224\u65ad", "result": "\u53d1\u73b0\u53ef\u4ee5\u901a\u8fc7\u6307\u5bfcGPTs\u4f7f\u7528\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u6765\u6539\u5584\u5176\u5224\u65ad\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u54cd\u5e94", "conclusion": "\u7406\u89e3LLMs\u5728\u6458\u8981\u8bc4\u4f30\u4e2d\u4f7f\u7528\u7684\u7279\u5f81\u6709\u52a9\u4e8e\u6539\u8fdb\u5176\u8bc4\u4f30\u80fd\u529b\uff0c\u6307\u5bfc\u5b83\u4eec\u4f7f\u7528\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u53ef\u4ee5\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027"}}
{"id": "2512.19630", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19630", "abs": "https://arxiv.org/abs/2512.19630", "authors": ["Rolando Coto-Solano", "Daisy Li", "Manoela Teleginski Ferraz", "Olivia Sasse", "Cha Krupka", "Sharid Lo\u00e1iciga", "Sally Akevai Tenamu Nicholas"], "title": "Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands M\u0101ori", "comment": null, "summary": "We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands M\u0101ori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e24\u79cd\u6781\u5ea6\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\uff08Bribri\u8bed\u548c\u5e93\u514b\u7fa4\u5c9b\u6bdb\u5229\u8bed\uff09\u7684\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u6027\u80fd\uff0c\u53d1\u73b0\u5b57\u7b26\u7ea7\u5fae\u8c03LLM\u6548\u679c\u6700\u4f73\uff0c\u9700\u8981\u7ea610,000\u8bcd\u6570\u636e\u624d\u80fd\u83b7\u5f97\u53ef\u9760\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6765\u81ea\u4e24\u65b9\u9762\uff1a\u4e00\u662f\u8bed\u8a00\u793e\u533a\u5bf9\u6587\u672c\u89c4\u8303\u5316\u5904\u7406\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u4e8c\u662fNLP\u7814\u7a76\u4e2d\u5173\u4e8e\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u7840\u7814\u7a76\u95ee\u9898\u3002\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u5bf9NLP\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6781\u5ea6\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u7684\u76f8\u5173\u7814\u7a76\u5f88\u5c11\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u7b97\u6cd5\uff0c\u5305\u62ec\u5b57\u7b26\u7ea7\u5fae\u8c03LLM\u3001\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6a21\u578b\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u3002\u5b9e\u9a8c\u5728\u4e24\u79cd\u6781\u5ea6\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\uff08Bribri\u8bed\u548c\u5e93\u514b\u7fa4\u5c9b\u6bdb\u5229\u8bed\uff09\u4e0a\u8fdb\u884c\uff0c\u8003\u5bdf\u4e86\u4e0d\u540c\u6570\u636e\u91cf\uff08\u4ece\u5c11\u91cf\u523010,000\u8bcd\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e86\u53d8\u97f3\u7b26\u53f7\u6821\u6b63\u4efb\u52a1\u3002", "result": "\u5b57\u7b26\u7ea7\u5fae\u8c03LLM\u8868\u73b0\u6700\u4f73\uff0c\u53ef\u80fd\u56e0\u5176\u80fd\u5c06\u590d\u6742\u5b57\u7b26\u5206\u89e3\u4e3aUTF-8\u5b57\u8282\u8868\u793a\u3002\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\u6548\u679c\u8f83\u5dee\u3002\u6240\u6709\u6a21\u578b\u9700\u8981\u7ea610,000\u8bcd\u6570\u636e\u624d\u80fd\u83b7\u5f97\u53ef\u9760\u6027\u80fd\u3002\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u8868\u73b0\u90fd\u5f88\u5dee\u3002", "conclusion": "\u5bf9\u4e8e\u6781\u5ea6\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u7684\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u4efb\u52a1\uff0c\u5b57\u7b26\u7ea7\u5fae\u8c03LLM\u662f\u6700\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u7ea610,000\u8bcd\u7684\u6570\u636e\u91cf\u624d\u80fd\u8fbe\u5230\u53ef\u9760\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u5bf9\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u7684NLP\u5904\u7406\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2512.19651", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19651", "abs": "https://arxiv.org/abs/2512.19651", "authors": ["Filippos Ventirozos", "Peter Appleby", "Matthew Shardlow"], "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting", "comment": "9 pages, 3 figures, 3 tables", "summary": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u4e00\u610f\u4e49\u8868\u793a\uff08UMR\uff09\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u6280\u672f\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u65b9\u9762\u7c7b\u522b\u60c5\u611f\u5206\u6790\uff0c\u4ee5\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u65b9\u9762\u7c7b\u522b\u60c5\u611f\u5206\u6790\uff08ACSA\uff09\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u65b0\u9886\u57df\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u6210\u672c\u9ad8\u6602\u3002\u5728\u6807\u6ce8\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u5b66\u4e60\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u6280\u672f\uff0c\u901a\u8fc7\u4e2d\u95f4\u7684\u7edf\u4e00\u610f\u4e49\u8868\u793a\uff08UMR\uff09\u6765\u7ed3\u6784\u5316ACSA\u4efb\u52a1\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u5728\u4e09\u4e2a\u6a21\u578b\uff08Qwen3-4B\u3001Qwen3-8B\u548cGemini-2.5-Pro\uff09\u548c\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8fd9\u79cd\u57fa\u4e8eUMR\u7684\u65b9\u6cd5\u3002", "result": "UMR\u7684\u6709\u6548\u6027\u53ef\u80fd\u4f9d\u8d56\u4e8e\u5177\u4f53\u6a21\u578b\u3002\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u4e2d\u7b49\u89c4\u6a21\u7684\u6a21\u578b\u5982Qwen3-8B\uff0c\u6027\u80fd\u4e0e\u6807\u51c6CoT\u57fa\u7ebf\u76f8\u5f53\uff0c\u4f46\u8fd9\u4e9b\u89c2\u5bdf\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u66f4\u5c0f\u6a21\u578b\u67b6\u6784\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u53d1\u73b0\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u7684\u666e\u9002\u6027\u3002UMR\u65b9\u6cd5\u5728\u96f6\u6837\u672cACSA\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u6548\u679c\u53ef\u80fd\u56e0\u6a21\u578b\u800c\u5f02\u3002"}}
{"id": "2512.19682", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19682", "abs": "https://arxiv.org/abs/2512.19682", "authors": ["Jiacheng Guo", "Ling Yang", "Peter Chen", "Qixin Xiao", "Yinjie Wang", "Xinzhe Juan", "Jiahao Qiu", "Ke Shen", "Mengdi Wang"], "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators", "comment": "Our codes are available at https://github.com/Gen-Verse/GenEnv", "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $\u03b1$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.", "AI": {"tldr": "GenEnv\u662f\u4e00\u4e2a\u901a\u8fc7\u751f\u6210\u5f0f\u73af\u5883\u6a21\u62df\u5668\u4e0e\u667a\u80fd\u4f53\u8fdb\u884c\u96be\u5ea6\u5bf9\u9f50\u534f\u540c\u8fdb\u5316\u7684\u6846\u67b6\uff0c\u80fd\u591f\u52a8\u6001\u751f\u6210\u9002\u5408\u667a\u80fd\u4f53\u5f53\u524d\u80fd\u529b\u6c34\u5e73\u7684\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u6027\u80fd\u5e76\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u9762\u4e34\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6570\u636e\u6210\u672c\u9ad8\u3001\u9759\u6001\u4e0d\u53d8\u7684\u74f6\u9888\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u667a\u80fd\u4f53\u4e0e\u53ef\u6269\u5c55\u751f\u6210\u5f0f\u73af\u5883\u6a21\u62df\u5668\u4e4b\u95f4\u7684\u534f\u540c\u8fdb\u5316\u6e38\u620f\uff0c\u4f7f\u7528\u03b1-\u8bfe\u7a0b\u5956\u52b1\u6307\u5bfc\u6a21\u62df\u5668\u52a8\u6001\u751f\u6210\u9002\u5408\u667a\u80fd\u4f53\"\u6700\u8fd1\u53d1\u5c55\u533a\"\u7684\u4efb\u52a1\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGenEnv\u5c067B\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe40.3%\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u7684\u5e73\u5747\u6027\u80fd\uff0c\u76f8\u6bd4Gemini 2.5 Pro\u7684\u79bb\u7ebf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f7f\u7528\u6570\u636e\u91cf\u51cf\u5c113.3\u500d\u4e14\u6027\u80fd\u66f4\u597d\u3002", "conclusion": "\u901a\u8fc7\u4ece\u9759\u6001\u76d1\u7763\u8f6c\u5411\u81ea\u9002\u5e94\u6a21\u62df\uff0cGenEnv\u4e3a\u6269\u5c55\u667a\u80fd\u4f53\u80fd\u529b\u63d0\u4f9b\u4e86\u6570\u636e\u9ad8\u6548\u7684\u9014\u5f84\u3002"}}
