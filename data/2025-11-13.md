<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Practical and Performant Enhancements for Maximization of Algebraic Connectivity](https://arxiv.org/abs/2511.08694)
*Leonard Jung,Alan Papalia,Kevin Doherty,Michael Everett*

Main category: cs.RO

TL;DR: 本文改进了基于代数连通性最大化的图稀疏化算法(MAC)，通过开发专用求解器、优化步长策略和自动连通性保证方案，使其更适合实时估计应用。


<details>
  <summary>Details</summary>
Motivation: 当前图估计算法在大规模长期图上扩展性差，MAC算法虽然能保持估计性能但计算成本高且需要手动指定连通边集，限制了在线应用。

Method: 开发了专用代数连通性求解器（平均2倍加速）、优化MAC的步长策略、提出自动连通性保证方案。

Result: 使MAC算法更可扩展、可靠且适合实时估计应用。

Conclusion: 这些改进使MAC算法在保持估计性能的同时，显著提升了计算效率和实用性，为实时图估计应用提供了可行解决方案。

Abstract: Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.

</details>


### [2] [Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration](https://arxiv.org/abs/2511.08732)
*Marta Lagomarsino,Elena Merlo,Andrea Pupa,Timo Birr,Franziska Krebs,Cristian Secchi,Tamim Asfour,Arash Ajoudani*

Main category: cs.RO

TL;DR: 这篇综述论文探讨了实现人机协同合作(HRC)的关键组件，重点关注如何建立双向信息流，使人类能够直观地传达指令和需求，同时机器人能够清晰传达内部状态和即将执行的动作。


<details>
  <summary>Details</summary>
Motivation: 当前机器人和AI虽然能完成复杂任务，但人类往往只是被动观察者，而机器人在人类环境中无法充分发挥潜力。需要建立有效的人机信息交换机制来实现协同合作。

Method: 通过分析完整的人机交互流程：从多模态输入到机器人可理解表示的转换、自适应规划和角色分配、控制层和反馈机制，来构建闭环的人机协作系统。

Result: 识别并连接了实现人机直观信息交换和技能转移的关键组件，建立了从人类到机器人通信桥梁的完整交互管道。

Conclusion: 论文指出了实现更自适应、更易访问的人机协同合作的趋势和有前景的研究方向。

Abstract: Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.

</details>


### [3] [ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements](https://arxiv.org/abs/2511.08741)
*Kai S. Yun,Navid Azizan*

Main category: cs.RO

TL;DR: ATOM-CBF是一个新型安全控制框架，通过显式计算和适应分布外测量的认知不确定性来确保系统安全，无需真实标签或分布偏移信息。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统的安全性面临挑战，特别是当它们依赖学习感知模块从高维传感器数据推断系统状态时。这些感知模块容易受到认知不确定性的影响，在遇到训练时未见过的分布外测量时经常失效。

Method: ATOM-CBF包含两个关键组件：(1) 分布外感知的自适应感知误差边界；(2) 集成该自适应误差边界的安全过滤器，使其能够实时调整保守程度。

Result: 在仿真中进行了实证验证，证明了ATOM-CBF能够为配备LiDAR扫描的F1Tenth车辆和配备RGB图像的四足机器人保持安全性。

Conclusion: ATOM-CBF框架通过显式处理分布外测量的认知不确定性，有效解决了学习感知模块在现实世界应用中的安全性问题。

Abstract: Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.

</details>


### [4] [CENIC: Convex Error-controlled Numerical Integration for Contact](https://arxiv.org/abs/2511.08771)
*Vince Kurtz,Alejandro Castro*

Main category: cs.RO

TL;DR: CENIC是一个新的连续时间积分器，结合了凸时间步进和误差控制积分的最新进展，在保持实时速度的同时提供精度和收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 现有离散时间模拟器需要选择时间步长，大步长会产生非物理伪影，小步长则运行缓慢；而现有的误差控制积分器难以处理接触的刚性动力学，无法满足现代机器人工作流的速度和可扩展性要求。

Method: 将凸时间步进和误差控制积分相结合，继承连续积分和离散时间步进的优点，开发CENIC连续时间积分器。

Result: CENIC能够以与MuJoCo、Drake和Isaac Sim等离散时间机器人模拟器相当的快速实时速率运行。

Conclusion: CENIC在保持实时速度的同时，提供了精度和收敛性的保证，解决了现有模拟器在时间步长选择和接触处理方面的挑战。

Abstract: State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.

</details>


### [5] [Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains](https://arxiv.org/abs/2511.08778)
*Richard Cheng,Peter Werner,Carolyn Matl*

Main category: cs.RO

TL;DR: 提出一种针对双机械臂机器人的高效运动规划方法，通过利用共享关节结构构建动态路线图，显著降低高维度规划的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决高自由度双机械臂机器人在未知动态环境中实时运动规划的挑战，克服配置空间高维度和复杂避障约束带来的计算困难。

Method: 为每个运动链（左臂+躯干、右臂+躯干）构建动态路线图，利用共享关节的结构特性，通过高效搜索两个路线图的组合来规避维度灾难。

Result: 在真实超市环境中进行杂货拣选任务测试，19自由度移动操作机器人实现了0.4秒平均规划时间，超过2000次运动规划的成功率达到99.9%。

Conclusion: 该方法通过利用双机械臂系统的结构特性，有效解决了高维度运动规划问题，实现了实时可靠的规划性能。

Abstract: High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.

</details>


### [6] [Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research](https://arxiv.org/abs/2511.08822)
*Nelson Durrant,Braden Meyers,Matthew McMurray,Clayton Smith,Brighton Anderson,Tristan Hodgins,Kalliyan Velasco,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 开发低成本、可配置的水下自主机器人平台CoUGARs，用于多智能体自主性研究，成本低于3000美元，支持声学协作定位研究。


<details>
  <summary>Details</summary>
Motivation: 真实世界的水下多智能体自主性测试面临高昂的成本和工程挑战，需要开发低成本、可配置的解决方案。

Method: 基于商用和3D打印部件设计低成本AUV平台，配备DVL和USBL声学阵列，开发容器化软件栈并与HoloOcean模拟器集成。

Result: 成功开发出成本低于3000美元的CoUGARs平台，在模拟和犹他州湖泊实地测试中验证了系统性能。

Conclusion: CoUGARs为水下多智能体自主性研究提供了经济可行的测试平台，支持声学协作定位等高级研究。

Abstract: Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.

</details>


### [7] [XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping](https://arxiv.org/abs/2511.08863)
*Hyesu Jang,Wooseong Yang,Ayoung Kim,Dongje Lee,Hanguen Kim*

Main category: cs.RO

TL;DR: 提出一种专门针对X波段雷达的场所识别算法，通过基于目标密度的候选选择规则和故意降低雷达检测精度来实现稳健的检索性能。


<details>
  <summary>Details</summary>
Motivation: X波段雷达作为船舶主要传感器，在自主导航中应用受限，原因是传感器分辨率低和信息内容不足。

Method: 采用基于目标密度的规则进行高效候选选择，并故意降低雷达检测精度以实现稳健检索性能。

Result: 在公共海事雷达数据集和自收集数据集上评估，与最先进的雷达场所识别方法进行比较。

Conclusion: 通过消融研究评估算法对关键参数的敏感性，证明了所提方法的有效性。

Abstract: X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.

</details>


### [8] [MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror](https://arxiv.org/abs/2511.08865)
*Cong Tai,Hansheng Wu,Haixu Long,Zhengbin Long,Zhaoyu Zheng,Haodong Xiang,Tao Shen*

Main category: cs.RO

TL;DR: 提出基于PICO的机器人远程操作框架，实现低成本实时手部运动捕捉，兼容RealMirror生态系统，支持机器人轨迹记录和实时遥操作。


<details>
  <summary>Details</summary>
Motivation: 降低上肢机器人操作研究的技术门槛，加速VLA相关研究进展，提供比主流视觉跟踪和动作捕捉方案更具成本效益的解决方案。

Method: 开发PICO-based机器人远程操作框架，实现低成本实时手部运动和姿态数据采集，兼容RealMirror生态系统，在Isaac仿真环境中进行稳定的机器人轨迹记录。

Result: 框架在成本效益上优于主流视觉跟踪和运动捕捉方案，支持多种末端执行器机器人的实时遥操作，包括灵巧手和机器人夹爪。

Conclusion: 该工作为VLA数据集构建提供了即用型功能，有效促进了上肢机器人操作研究的发展，为VLA相关研究提供了实用的技术支撑。

Abstract: In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.

</details>


### [9] [A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction](https://arxiv.org/abs/2511.08912)
*Jinyu Zhang,Lijun Han,Feng Jian,Lingxi Zhang,Hesheng Wang*

Main category: cs.RO

TL;DR: 提出了一种具有规划级意图预测的移动机器人共享控制框架，通过意图域表示人类运动意图，结合深度强化学习进行路径重规划，显著降低了操作员工作负荷并提高了安全性。


<details>
  <summary>Details</summary>
Motivation: 在移动机器人共享控制中，有效理解人类运动意图对于实现无缝的人机协作至关重要，现有方法在这方面存在不足。

Method: 引入意图域概念表示未来运动意图，将意图预测和路径重规划问题建模为马尔可夫决策过程，通过深度强化学习求解，并开发基于Voronoi图的人类轨迹生成算法进行无监督训练。

Result: 大量仿真和真实用户研究表明，该方法相比现有辅助遥操作方法显著降低了操作员工作负荷，提高了安全性，同时保持了任务效率。

Conclusion: 所提出的共享控制框架通过规划级意图预测有效提升了人机协作性能，为移动机器人共享控制提供了新的解决方案。

Abstract: In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.

</details>


### [10] [Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation](https://arxiv.org/abs/2511.08935)
*Ningnan Wang,Weihuang Chen,Liming Chen,Haoxuan Ji,Zhongyu Guo,Xuchong Zhang,Hongbin Sun*

Main category: cs.RO

TL;DR: SCOPE是一个零样本视觉导航框架，通过显式利用边界信息驱动基于潜力的探索，结合时空潜力图和自我重新考虑机制，在未知环境中实现更智能的目标导向导航。


<details>
  <summary>Details</summary>
Motivation: 现有零样本导航方法忽视了视觉边界对轨迹和观测的根本影响，且难以推断部分视觉观察与导航目标之间的关系，导致长时程规划性能受限。

Method: 提出SCOPE框架：1) 使用视觉语言模型估计探索潜力；2) 构建时空潜力图捕捉边界动态；3) 引入自我重新考虑机制重新评估和优化先前决策。

Result: 在两个多样化具身导航任务上的实验表明，SCOPE在准确率上比最先进基线方法提升4.6%，并显示出更好的校准性、更强的泛化能力和更高的决策质量。

Conclusion: SCOPE通过显式利用边界信息和自我重新考虑机制，显著提升了零样本视觉导航的性能，证明了边界感知和决策反思在长时程规划中的重要性。

Abstract: Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.

</details>


### [11] [Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning](https://arxiv.org/abs/2511.08942)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 该论文提出了一个框架，将视觉语言模型从被动观察者转变为主动策略制定者，通过结构化思维链提示、动态动作历史集成和障碍物地图理解来指导基于前沿的探索代理，在多个导航基准测试中显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用视觉语言模型的推理能力，需要将其角色从被动观察者转变为导航过程中的主动策略制定者，以释放其在机器人导航中的全部潜力。

Method: 将高级规划外包给视觉语言模型，利用其上下文理解能力指导基于前沿的探索代理。采用三种技术：结构化思维链提示、动态集成代理最近动作历史、以及解释自上而下障碍物地图与第一人称视图的新能力。

Result: 在HM3D、Gibson和MP3D等具有挑战性的基准测试中，该方法产生了极其直接和逻辑的轨迹，在导航效率方面相比现有方法有显著提升。

Conclusion: 该方法为开发更强大的具身代理开辟了道路，展示了视觉语言模型作为主动策略制定者在机器人导航中的巨大潜力。

Abstract: While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.

</details>


### [12] [UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving](https://arxiv.org/abs/2511.09013)
*Ziyi Song,Chen Xia,Chenbing Wang,Haibao Yu,Sheng Zhou,Zhisheng Niu*

Main category: cs.RO

TL;DR: UniMM-V2X是一个端到端多智能体框架，通过多层次融合策略和混合专家架构实现感知、预测和规划的层次化协作，在DAIR-V2X数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统存在感知能力有限和孤立决策的问题，现有多智能体方法主要关注感知层面，未能与下游规划和控制系统有效对齐，也无法充分利用端到端自动驾驶的全部潜力。

Method: 提出多层次融合策略统一感知和预测协作，让智能体共享查询并协同推理；采用混合专家架构动态增强BEV表示，并将MoE扩展到解码器以更好地捕捉多样化运动模式。

Result: 在DAIR-V2X数据集上，相比UniV2X，感知精度提升39.7%，预测误差降低7.2%，规划性能提升33.2%。

Conclusion: MoE增强的多层次协作范式展示了强大的性能，为端到端多智能体自动驾驶提供了有效解决方案。

Abstract: Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.

</details>


### [13] [A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem](https://arxiv.org/abs/2511.09020)
*Mingyang Yu,Haorui Yang,Kangning An,Xinjian Wei,Xiaoxuan Xu,Jing Xu*

Main category: cs.RO

TL;DR: 提出增强型多策略矮獴优化算法(EDMO)，用于三维无人机动态路径规划，通过三种新策略解决传统元启发式算法的早熟收敛和多样性不足问题。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的广泛应用，路径规划变得日益重要。传统元启发式算法虽然高效，但在复杂场景中仍面临早熟收敛和解决方案多样性不足的挑战。

Method: EDMO算法整合三种新策略：动态量子隧穿优化策略(DQTOS)使粒子概率性逃离局部最优；生物趋光动态聚焦搜索策略(BDFSS)基于微生物趋光性进行自适应局部优化；正交透镜对立学习策略(OLOBL)通过结构化维度重组增强全局探索。

Result: 在CEC2017和CEC2020的39个标准测试函数上，EDMO在收敛速度、鲁棒性和优化精度方面优于14种先进算法。在无人机三维路径规划和三个工程设计任务中的实际验证证实了其有效性。

Conclusion: EDMO算法在动态障碍物丰富的环境中具有实际适用性，能够为需要智能、自适应和高效规划的现场机器人任务提供有效解决方案。

Abstract: With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.

</details>


### [14] [SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields](https://arxiv.org/abs/2511.09072)
*Sangheon Yang,Yeongin Yoon,Hong Mo Jung,Jongwoo Lim*

Main category: cs.RO

TL;DR: SMF-VO是一种轻量级视觉里程计方法，通过直接估计瞬时线速度和角速度来替代传统的位姿估计，实现了在资源受限设备上的高效运行。


<details>
  <summary>Details</summary>
Motivation: 传统VO/VIO方法依赖位姿中心范式，需要大规模地标维护和持续地图优化，计算成本高，限制了在资源受限设备上的实时性能。

Method: 提出稀疏运动场视觉里程计(SMF-VO)，直接从稀疏光流估计瞬时线速度和角速度，绕过显式位姿估计和昂贵的地标跟踪，采用基于3D射线的通用运动场公式，适用于各种相机模型。

Result: 在基准数据集上表现出优越的效率和竞争性精度，在树莓派5上仅使用CPU即可实现超过100 FPS的性能。

Conclusion: 为传统方法提供了一个可扩展且高效的替代方案，非常适合移动机器人和可穿戴设备应用。

Abstract: Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.

</details>


### [15] [D-AWSIM: Distributed Autonomous Driving Simulator for Dynamic Map Generation Framework](https://arxiv.org/abs/2511.09080)
*Shunsuke Ito,Chaoran Zhao,Ryo Okamura,Takuya Azumi*

Main category: cs.RO

TL;DR: D-AWSIM是一个分布式模拟器，通过多机负载分配支持大规模传感器部署和密集交通环境模拟，为自动驾驶研究提供信息共享策略探索平台。


<details>
  <summary>Details</summary>
Motivation: 现实世界基础设施传感器实验成本高且面临监管挑战，传统单机模拟器无法处理大规模城市交通场景，需要分布式解决方案来支持自动驾驶系统的安全保证研究。

Method: 提出D-AWSIM分布式模拟器，将工作负载分配到多台机器上，并构建动态地图生成框架，支持大规模传感器部署和密集交通环境模拟。

Result: 评估显示D-AWSIM在车辆数量和LiDAR传感器处理方面的吞吐量相比单机设置显著提高，并与Autoware集成验证了其在自动驾驶研究中的适用性。

Conclusion: D-AWSIM为研究人员提供了无需依赖物理测试平台的信息共享策略探索能力，是支持自动驾驶系统大规模测试的有效工具。

Abstract: Autonomous driving systems have achieved significant advances, and full autonomy within defined operational design domains near practical deployment. Expanding these domains requires addressing safety assurance under diverse conditions. Information sharing through vehicle-to-vehicle and vehicle-to-infrastructure communication, enabled by a Dynamic Map platform built from vehicle and roadside sensor data, offers a promising solution. Real-world experiments with numerous infrastructure sensors incur high costs and regulatory challenges. Conventional single-host simulators lack the capacity for large-scale urban traffic scenarios. This paper proposes D-AWSIM, a distributed simulator that partitions its workload across multiple machines to support the simulation of extensive sensor deployment and dense traffic environments. A Dynamic Map generation framework on D-AWSIM enables researchers to explore information-sharing strategies without relying on physical testbeds. The evaluation shows that D-AWSIM increases throughput for vehicle count and LiDAR sensor processing substantially compared to a single-machine setup. Integration with Autoware demonstrates applicability for autonomous driving research.

</details>


### [16] [APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots](https://arxiv.org/abs/2511.09091)
*Shivam Sood,Laukik Nakhwa,Sun Ge,Yuhong Cao,Jin Cheng,Fatemah Zargarbashi,Taerim Yoon,Sungjoon Choi,Stelian Coros,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: APEX是一种即插即用的强化学习方法，通过衰减动作先验将专家演示整合到RL中，消除了部署时对参考数据的依赖，提高了样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的运动跟踪方法需要大量调参且在部署时依赖参考数据，限制了适应性。需要一种能够利用演示数据但不过度依赖它们的方法。

Method: 结合衰减动作先验和多评论家框架，初始时偏向专家演示但逐渐允许独立探索，平衡任务性能和运动风格。

Result: 在仿真和Unitree Go2机器人上的实验表明，APEX能够以更高的稳定性、效率和泛化能力学习自然运动，并能跨不同地形和速度转移运动风格。

Conclusion: APEX为从运动到操作等各种机器人任务的自然技能获取提供了指导驱动的强化学习新途径。

Abstract: Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.

</details>


### [17] [Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles](https://arxiv.org/abs/2511.09104)
*Amirhossein Kazemipour,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出了一个统一框架，用于实时独立控制软执行器的扭矩和刚度，通过级联控制器和分析逆动力学在动态接触瞬态中保持解耦控制。


<details>
  <summary>Details</summary>
Motivation: 现有软执行器控制器难以在动态接触瞬态中维持独立控制，需要实现类似生物阻抗策略的扭矩-刚度解耦控制。

Method: 使用统一力律捕捉多种软执行器物理特性，采用级联控制器和分析逆动力学，通过共收缩/偏置坐标独立调制扭矩和刚度。

Result: 在软表面上实现200倍更快的稳定时间，在刚性表面上减少81%的力，与固定策略相比稳定性从22-54%提升到稳定交互。

Conclusion: 该框架为肌肉骨骼拮抗系统执行自适应阻抗控制提供了基础，可实现安全的人机交互。

Abstract: Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.

</details>


### [18] [Data Assessment for Embodied Intelligence](https://arxiv.org/abs/2511.09119)
*Jiahao Xiao,Bowen Yan,Jianbo Zhang,Jia Wang,Chunyi Li,Zhengxue Cheng,Guangtao Zhai*

Main category: cs.RO

TL;DR: 本文提出了两种数据驱动工具来评估具身智能数据集：多样性熵用于衡量数据集信息量，以及无需训练的算法来量化数据集可学习性，帮助设计更高质量的具身智能数据集。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能数据集评估主要关注多样性（通过统计任务和场景）但缺乏全面性，而可学习性评估通常需要昂贵的模型训练且缺乏可解释性，无法为数据集改进提供指导。

Method: 1) 构建统一的多模态数据样本表示，提出多样性熵作为连续度量；2) 引入首个可解释的数据驱动算法，无需训练即可量化数据集可学习性。

Result: 在模拟和真实世界具身数据集上验证了算法的有效性，能够提供可信且可操作的见解，帮助同时改进多样性和可学习性。

Conclusion: 这项工作为设计更高质量的具身智能数据集奠定了基础，有助于推动具身智能的发展。

Abstract: In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.

</details>


### [19] [RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation](https://arxiv.org/abs/2511.09141)
*Xuetao Li,Wenke Huang,Nengyuan Pan,Kaiyan Zhao,Songhua Yang,Yiming Wang,Mengde Li,Mang Ye,Jifeng Xuan,Miao Li*

Main category: cs.RO

TL;DR: RGMP是一个端到端框架，通过几何语义技能推理与数据高效的视觉运动控制相结合，解决了人形机器人多模态决策和泛化控制中的数据效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法需要大量训练数据，忽视了几何推理在新场景中的作用，且低效建模机器人-目标关系，造成训练资源浪费。

Method: 提出几何先验技能选择器，将几何归纳偏置注入视觉语言模型，生成自适应技能序列；引入自适应递归高斯网络，将机器人-物体交互参数化为高斯过程层次结构，实现数据高效的运动合成。

Result: 在人形机器人和桌面双臂机器人上评估，RGMP在泛化测试中达到87%任务成功率，比最先进模型数据效率提高5倍。

Conclusion: RGMP通过几何语义推理和递归高斯自适应，实现了优越的跨域泛化能力，为数据高效的多模态机器人控制提供了有效解决方案。

Abstract: Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.

</details>


### [20] [LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation](https://arxiv.org/abs/2511.09142)
*Eungchang Mason Lee,Kevin Christiansen Marsim,Hyun Myung*

Main category: cs.RO

TL;DR: LODESTAR是一种新颖的激光雷达惯性里程计方法，通过退化感知自适应Schmidt-Kalman滤波器和退化感知数据利用模块，解决在退化环境中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 传统激光雷达惯性里程计在退化环境（如长走廊、高空飞行）中性能下降，因为激光雷达测量不平衡或稀疏，导致状态估计不适定。

Method: 采用两个关键模块：DA-ASKF（退化感知自适应Schmidt-Kalman滤波器）使用滑动窗口和Schmidt-Kalman更新，根据退化程度自适应分类状态；DA-DE（退化感知数据利用）基于局部化贡献和雅可比矩阵条件数修剪和选择测量数据。

Result: 实验结果表明，LODESTAR在各种退化条件下，在准确性和鲁棒性方面优于现有的基于激光雷达的里程计方法和退化感知模块。

Conclusion: LODESTAR通过退化感知约束优化和测量稀疏性缓解，有效解决了激光雷达惯性里程计在退化环境中的性能问题。

Abstract: LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.

</details>


### [21] [Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots](https://arxiv.org/abs/2511.09241)
*Yuxi Wei,Zirui Wang,Kangning Yin,Yue Hu,Jingbo Wang,Siheng Chen*

Main category: cs.RO

TL;DR: Humanoid-Union是一个通过自主管道生成的大规模人形机器人运动数据集，包含260+小时多样化高质量运动数据，并基于此提出了SCHUR可扩展学习框架，显著提升了运动生成质量和文本-运动对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中数据扩展的瓶颈问题，利用丰富的人类视频和运动数据作为免费的大规模数据源，探索如何有效挖掘原始视频、提取机器人可学习表示并用于可扩展学习。

Method: 提出Humanoid-Union数据集生成管道和SCHUR学习框架，通过从人类运动视频中提取语义标注来生成机器人运动数据，并研究大规模数据对人形机器人高级控制的影响。

Result: SCHUR在数据和模型扩展下实现了高质量机器人运动生成和强文本-运动对齐，相比先前方法在MPJPE指标上提升37%，在FID指标上提升25%，并在真实人形机器人上验证了有效性。

Conclusion: Humanoid-Union数据集和SCHUR框架为解决机器人学习中的数据扩展问题提供了有效方案，证明了大规模数据对提升人形机器人高级控制性能的重要性。

Abstract: Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\% reconstruction improvement under MPJPE and 25\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.

</details>


### [22] [UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning](https://arxiv.org/abs/2511.09302)
*Yan Huang,Shoujie Li,Xingting Li,Wenbo Ding*

Main category: cs.RO

TL;DR: UMIGen是一个统一框架，通过手持设备Cloud-UMI收集点云观测-动作对，并采用可见性感知优化机制，实现高效的数据生成和跨机器人具身的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动机器人学习面临的数据收集难题：需要大规模高质量演示数据，但收集成本高、依赖专用硬件，且现有方法空间泛化能力有限。

Method: 包含两个关键组件：1) Cloud-UMI手持数据收集设备，无需视觉SLAM，同时记录点云观测-动作对；2) 可见性感知优化机制，扩展DemoGen流程到自我中心3D观测，只生成相机视野内的点。

Result: 在模拟和真实环境中的实验表明，UMIGen支持强大的跨具身泛化能力，并加速了多种操作任务的数据收集。

Conclusion: UMIGen框架能够实现高效的数据生成，与真实自我中心观测对齐，并可直接在不同机器人具身间迁移，无需后处理。

Abstract: Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.

</details>


### [23] [CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance](https://arxiv.org/abs/2511.09331)
*Stepan Dergachev,Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: CoRL-MPPI结合协作强化学习和MPPI框架，通过训练深度神经网络策略来指导MPPI的采样分布，显著提升多机器人导航的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统MPPI控制器在分散式碰撞避免中可能产生次优轨迹，因为其性能严重依赖于无信息的随机采样。

Method: 在仿真中训练深度神经网络策略学习局部协作碰撞避免行为，然后将该学习策略嵌入MPPI框架中以指导其采样分布。

Result: 在密集动态仿真环境中，CoRL-MPPI相比ORCA、BVC和多智能体MPPI等基线方法，显著提高了导航效率（成功率和时间跨度）和安全性。

Conclusion: CoRL-MPPI在保持MPPI所有理论保证的同时，实现了更敏捷和鲁棒的多机器人导航。

Abstract: Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.

</details>


### [24] [SPIDER: Scalable Physics-Informed Dexterous Retargeting](https://arxiv.org/abs/2511.09484)
*Chaoyi Pan,Changhao Wang,Haozhi Qi,Zixi Liu,Homanga Bharadhwaj,Akash Sharma,Tingfan Wu,Guanya Shi,Jitendra Malik,Francois Hogan*

Main category: cs.RO

TL;DR: SPIDER是一个基于物理的重新定位框架，可将人类运动演示转化为机器人可执行的动态可行轨迹，解决数据稀缺问题，提高成功率18%，比强化学习快10倍。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人和灵巧手控制中数据稀缺问题，利用丰富的人类运动数据来弥补机器人特定数据收集的高成本。

Method: 提出基于物理的重新定位框架，结合人类演示的全局任务结构和物理模拟采样，通过课程式虚拟接触指导确保动态可行性和正确的接触序列。

Result: 在9种人形/灵巧手机器人和6个数据集上验证，成功率提高18%，比标准采样快10倍，生成了240万帧动态可行机器人数据集。

Conclusion: SPIDER作为通用物理重定位方法，能够处理不同质量的数据并生成高质量数据，有效支持强化学习等策略学习方法。

Abstract: Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.

</details>


### [25] [WMPO: World Model-based Policy Optimization for Vision-Language-Action Models](https://arxiv.org/abs/2511.09515)
*Fangqi Zhu,Zhengyang Yan,Zicong Hong,Quanxin Shou,Xiao Ma,Song Guo*

Main category: cs.RO

TL;DR: 提出了WMPO框架，通过基于像素预测的世界模型实现无需真实环境交互的VLA强化学习，显著提升样本效率并实现自我纠正等涌现行为。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖专家演示无法从失败中学习，而强化学习样本效率低，需要一种能在不接触真实环境的情况下进行策略优化的方法。

Method: WMPO框架使用基于像素预测的世界模型，将"想象"轨迹与预训练的VLA特征对齐，支持在线策略GRPO优化。

Result: 在仿真和真实机器人实验中，WMPO显著提升样本效率、获得更强性能，展现出自我纠正、泛化能力和终身学习等涌现行为。

Conclusion: WMPO为VLA强化学习提供了无需真实环境交互的有效解决方案，实现了高效的策略优化和智能行为涌现。

Abstract: Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

</details>


### [26] [MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation](https://arxiv.org/abs/2511.09516)
*Runhao Li,Wenkai Guo,Zhenyu Wu,Changyuan Wang,Haoyuan Deng,Zhenyu Weng,Yap-Peng Tan,Ziwei Wang*

Main category: cs.RO

TL;DR: MAP-VLA是一种增强预训练视觉-语言-动作模型的方法，通过从演示中构建记忆库并动态检索相关记忆提示来提升长时程机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练视觉-语言-动作模型在处理长时程任务时表现不佳，因为它们缺乏记忆机制且仅依赖即时感官输入。

Method: 构建基于历史演示的记忆库，每个记忆单元捕获任务的特定阶段信息；通过轨迹相似性匹配检索相关记忆，并将其作为可学习的软提示动态集成到冻结的VLA模型中。

Result: 在仿真基准测试中实现7.0%的绝对性能提升，在真实机器人评估中实现25.0%的性能提升，超越了当前最先进的方法。

Conclusion: MAP-VLA提供了一种轻量级、灵活的即插即用解决方案，能够显著提升预训练VLA模型在长时程机器人操作任务中的表现。

Abstract: Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

</details>


### [27] [SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation](https://arxiv.org/abs/2511.09555)
*Hao Shi,Bin Xie,Yingfei Liu,Yang Yue,Tiancai Wang,Haoqiang Fan,Xiangyu Zhang,Gao Huang*

Main category: cs.RO

TL;DR: SpatialActor是一个用于机器人操作的解耦框架，通过分离语义和几何信息，解决了现有方法在噪声深度数据和空间理解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的点云方法存在稀疏采样问题，而基于图像的方法将RGB和深度数据输入到2D骨干网络中，其纠缠的语义和几何对真实世界中的深度噪声敏感，且忽视了精确交互所需的低层空间线索。

Method: 提出SpatialActor框架，包含语义引导的几何模块，自适应融合来自噪声深度和语义引导专家先验的两种互补几何信息；以及空间变换器，利用低层空间线索进行准确的2D-3D映射并实现空间特征间的交互。

Result: 在多个仿真和真实世界场景的50+任务中评估，SpatialActor在RLBench上达到87.4%的最优性能，在变化噪声条件下提升13.9%到19.4%，表现出强鲁棒性。同时显著增强了新任务的少样本泛化能力，并在各种空间扰动下保持鲁棒性。

Conclusion: SpatialActor通过解耦语义和几何表示，为机器人操作提供了鲁棒的解决方案，在噪声环境、少样本学习和空间扰动下均表现出优越性能。

Abstract: Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: https://shihao1895.github.io/SpatialActor

</details>


### [28] [IFG: Internet-Scale Guidance for Functional Grasping Generation](https://arxiv.org/abs/2511.09558)
*Ray Muxin Liu,Mingxuan Li,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: 该论文提出了一种结合互联网规模视觉模型的语义理解与基于仿真的局部几何感知力闭合的语义抓取方法，无需手动收集训练数据即可实现高性能的3D抓取。


<details>
  <summary>Details</summary>
Motivation: 大型视觉模型虽然能在杂乱场景中分割和理解物体部件，但缺乏精确控制灵巧机器人手进行3D抓取所需的几何理解能力。

Method: 利用仿真和力闭合抓取生成管道理解手和物体的局部几何，然后将生成的数据蒸馏到可在相机点云上实时运行的扩散模型中。

Result: 该方法实现了高性能的语义抓取，无需手动收集训练数据。

Conclusion: 通过结合互联网规模模型的全局语义理解与基于仿真的局部几何感知力闭合，可以克服现有模型在精确3D抓取方面的局限性。

Abstract: Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at https://ifgrasping.github.io/

</details>
