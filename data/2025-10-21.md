<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 48]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: VAR-SLAM是一个基于ORB-SLAM3的视觉SLAM系统，结合轻量级语义关键点滤波器和自适应鲁棒损失函数，在动态环境中实现更好的轨迹精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有动态环境下的视觉SLAM方法要么依赖只能处理已知物体类别的语义滤波，要么使用无法适应未知移动物体的固定鲁棒核函数，导致在未知移动物体出现时精度下降。

Method: 结合轻量级语义关键点滤波器处理已知移动物体，使用Barron自适应鲁棒损失函数处理未知移动物体，通过在线估计鲁棒核的形状参数来自动调整高斯和重尾行为。

Result: 在TUM RGB-D、Bonn RGB-D Dynamic和OpenLORIS数据集上的评估显示，相比最先进基线方法，轨迹精度和鲁棒性均有提升，在挑战性序列上比NGD-SLAM的ATE RMSE降低达25%，同时平均性能保持在27 FPS。

Conclusion: VAR-SLAM在动态环境中表现出优越性能，能够有效处理已知和未知移动物体，实现高精度和实时性能。

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [2] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: DeGrip是一款专为废旧电脑台式机拆卸设计的定制化夹爪，具有3个自由度，采用线缆驱动机制，能在受限空间中操作，并在Isaac Sim环境中验证了其拆卸能力。


<details>
  <summary>Details</summary>
Motivation: 智能机器人拆卸报废产品一直是机器人领域的长期挑战，现有机器学习技术因缺乏专用硬件而难以在实际场景中应用。

Method: 开发DeGrip定制夹爪，提供3个自由度，采用线缆驱动传输机制减小尺寸，设计腕部解耦腕关节和夹爪关节的驱动，并在Isaac Sim中建立拆卸环境进行评估。

Result: 评估结果证实了DeGrip在废旧电脑台式机拆卸中的能力，特别是在受限空间操作和任意配置下拆卸组件方面表现出色。

Conclusion: DeGrip夹爪成功解决了机器人拆卸中的硬件限制问题，为实际应用中的智能拆卸提供了有效解决方案。

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [3] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: Cosmos-Surg-dVRK是一个基于世界基础模型的手术仿真平台，通过视频分类器实现手术策略的自动化在线评估和基准测试，在缝合任务和胆囊切除术中显示出与真实机器人平台的良好相关性。


<details>
  <summary>Details</summary>
Motivation: 解决在物理机器人平台（如dVRK）上直接评估手术策略面临的高成本、耗时、可重复性差和执行变异性等问题。

Method: 开发Cosmos-Surg-dVRK手术微调模型，结合训练的视频分类器，构建自动化评估管道，在缝合垫任务和离体猪胆囊切除术上进行验证。

Result: 在缝合任务中，仿真平台与真实dVRK平台结果强相关，视频分类器与人工标注者一致性良好；在胆囊切除术中，仿真结果与真实评估显示有前景的对齐性。

Conclusion: Cosmos-Surg-dVRK为复杂手术程序提供了有潜力的自动化评估平台，能够有效替代昂贵的物理机器人测试。

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [4] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA是一个用于单臂操作任务的统一评估生态系统，通过细粒度能力测试和系统性压力测试来解决现有VLA评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA评估方法使用粗糙的最终任务成功率指标，无法提供精确的技能诊断或测量对现实世界扰动的鲁棒性，且数据碎片化阻碍了可重复研究和通用模型的发展。

Method: 引入双轴评估协议：细粒度能力测试用于精确技能诊断，系统性压力测试用于测量鲁棒性；提供标准化API和大规模聚合数据集以减少碎片化。

Result: 使用NEBULA发现，表现最佳的VLA在空间推理和动态适应等关键能力上存在困难，这些缺陷被传统最终任务成功率指标所掩盖。

Conclusion: NEBULA通过同时测量代理能做什么以及何时可靠地执行，为构建鲁棒、通用的具身智能体提供了实用基础。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [5] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 提出了一种无需训练的策略引导方法，通过模拟候选动作序列并使用视觉语言模型选择与文本计划最一致的动作，提升推理视觉语言动作模型在分布外场景下的忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有推理VLA模型即使生成正确的文本计划，在分布外场景下仍可能产生与计划意图不符的动作，存在推理-动作对齐问题。

Method: 基于推理VLA的中间文本计划，采样多个候选动作序列，通过模拟预测结果，使用预训练VLM选择与文本计划最一致的动作序列执行。

Result: 在行为组合任务上比先前工作提升15%性能，对语义和视觉分布外扰动具有鲁棒性，且性能随计算和数据多样性提升。

Conclusion: 将VLA的自然动作多样性从错误来源转化为优势，无需重新训练即可实现新颖行为组合，提升推理-动作对齐的忠实度。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [6] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: SPOT是一个用于无人机动态避障的统一规划框架，通过将感知目标显式整合到运动优化中，解决了单深度相机视野有限和盲区问题。


<details>
  <summary>Details</summary>
Motivation: 配备单深度相机的无人机在动态避障中面临视野有限和不可避免的盲区挑战，现有方法将运动规划与感知考虑分离，导致避障效果不佳和响应延迟。

Method: 提出基于高斯过程的障碍物信念地图，建立已识别和潜在障碍物的统一概率表示，并通过碰撞感知推理机制将空间不确定性和轨迹接近度转化为时变观测紧急度地图，在视野内整合紧急度值定义可微分目标。

Result: 在动态、杂乱和遮挡环境中的仿真和真实世界实验表明，该方法比基线方法提前2.8秒检测到潜在动态障碍物，动态障碍物可见性提高超过500%，能够在杂乱、遮挡环境中安全导航。

Conclusion: SPOT框架通过将感知目标整合到运动规划中，实现了实时、感知感知的轨迹规划，计算时间低于10毫秒，显著提升了无人机在复杂环境中的动态避障能力。

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [7] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: Manual2Skill++是一个从装配手册中自动提取结构化连接信息的视觉语言框架，将装配任务编码为层次化图结构，其中节点代表零件和子装配体，边显式建模组件间的连接关系。


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配方法通常将连接器视为次要考虑因素，而连接实际上是决定装配成败的关键环节。本文旨在将连接作为装配表示中的首要原语。

Method: 利用大规模视觉语言模型解析装配手册中的符号图表和注释，构建层次化装配图，其中明确建模连接类型、规格、数量和位置等连接信息。

Result: 在包含20多个装配任务的数据集上验证了表示提取方法，并在仿真环境中评估了四个复杂装配场景的完整任务理解到执行流程。

Conclusion: 将连接作为首要原语的装配表示方法能够有效利用人类设计的装配手册中蕴含的丰富连接知识，提高机器人装配的可靠性。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [8] [Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach](https://arxiv.org/abs/2510.16424)
*Dan Guo,Xibin Jin,Shuai Wang,Zhigang Wen,Miaowen Wen,Chengzhong Xu*

Main category: cs.RO

TL;DR: 提出集成感知、运动和通信(IPMC)的边缘机器人系统，通过动态调整通信策略减少传感器数据上传需求，并采用模仿学习神经网络降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略机器人功能与通信条件之间的相互依赖性，导致通信开销过大。

Method: 利用机器人感知和运动动态知识动态调整通信策略(压缩比、传输频率、发射功率)，并设计模仿学习神经网络实现学习优化(LTO)范式。

Result: 计算复杂度比最先进优化求解器降低10倍以上，实验证明IPMC的优越性和LTO的实时执行能力。

Conclusion: IPMC系统通过集成感知、运动和通信，显著减少了边缘机器人系统的通信开销，同时LTO方法提供了高效的实时优化解决方案。

Abstract: Edge robotics involves frequent exchanges of large-volume multi-modal data.
Existing methods ignore the interdependency between robotic functionalities and
communication conditions, leading to excessive communication overhead. This
paper revolutionizes edge robotics systems through integrated perception,
motion, and communication (IPMC). As such, robots can dynamically adapt their
communication strategies (i.e., compression ratio, transmission frequency,
transmit power) by leveraging the knowledge of robotic perception and motion
dynamics, thus reducing the need for excessive sensor data uploads.
Furthermore, by leveraging the learning to optimize (LTO) paradigm, an
imitation learning neural network is designed and implemented, which reduces
the computational complexity by over 10x compared to state-of-the art
optimization solvers. Experiments demonstrate the superiority of the proposed
IPMC and the real-time execution capability of LTO.

</details>


### [9] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 该研究收集了1,893个用户对家用机器人的问题，涵盖12个类别和70个子类别，揭示了用户最关心的问题类型及其重要性排名，为机器人问答系统设计提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和对话界面在人机交互中的广泛应用，机器人回答用户问题的能力变得至关重要。现有可解释机器人研究主要关注"为什么"问题，而缺乏对用户实际提问多样性的理解。

Method: 通过创建15个视频刺激和7个文本刺激，展示机器人执行各种家务任务的情景，在Prolific平台上向100名参与者收集他们在每个情景中想向机器人提出的问题。

Result: 最常见的类别是任务执行细节问题(22.5%)、机器人能力问题(12.7%)和性能评估问题(11.3%)。虽然关于机器人处理困难场景和确保正确行为的问题较少，但用户认为这些是最重要的。新手用户倾向于询问简单事实问题。

Conclusion: 该数据集为识别机器人需要记录和暴露的信息、基准测试问答模块以及设计与用户期望一致的解释策略提供了宝贵基础。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [10] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: ORAD-3D是目前最大的越野自动驾驶数据集，涵盖多种地形和环境条件，并建立了包含5个核心任务的基准评测体系。


<details>
  <summary>Details</summary>
Motivation: 越野自动驾驶研究面临大规模高质量数据集稀缺的瓶颈问题，需要填补这一空白。

Method: 构建ORAD-3D数据集，覆盖林地、农田、草地、河岸、碎石路、水泥路和乡村地区等多种地形，以及不同天气和光照条件。

Result: 建立了包含2D自由空间检测、3D占据预测、粗略GPS路径规划、视觉语言模型驱动自动驾驶和越野环境世界模型等5个任务的基准评测体系。

Conclusion: 该数据集和基准为推进挑战性越野场景下的感知和规划研究提供了统一且强大的资源。

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [11] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 提出了一种新型SPD机械夹爪，具有线性平行夹持功能，无需调整整体高度即可抓取桌面上不同尺寸的物体，解决了传统平行夹爪需要调整机械臂高度的问题。


<details>
  <summary>Details</summary>
Motivation: 传统工业平行夹爪的指尖呈弧形运动轨迹，需要调整整个机械臂的高度以避免与桌面碰撞。SPD夹爪旨在通过线性平行夹持机制解决这一问题。

Method: 设计了具有手掌和两个机械相同、对称排列手指的夹爪结构，指尖遵循线性运动轨迹。提出了设计理念、基本组成原理和优化分析理论，并制作了原型进行测试。

Result: 实验结果表明，该机械夹爪成功实现了线性平行夹持功能，并展现出良好的适应性。

Conclusion: SPD夹爪在具身智能技术发展中，能够帮助各种机器人实现有效抓取，为收集数据以增强深度学习训练奠定基础。

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [12] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: DIV-Nav是一个实时导航系统，能够处理包含空间关系的复杂自由文本查询，通过分解语言指令、计算语义置信度图的交集以及使用LVLM验证空间约束来实现高效的对象导航。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本对象导航通常只能处理简单查询（如"电视"或"蓝色地毯"），无法应对包含空间关系的复杂自由文本查询（如"在桌子上找遥控器"）。

Method: 通过三个步骤的松弛处理：1）将复杂空间约束的自然语言指令分解为语义图上的简单对象查询；2）计算个体语义置信度图的交集以识别所有对象共存的区域；3）使用LVLM验证发现的对象是否符合原始复杂空间约束。

Result: 在MultiON基准测试和波士顿动力Spot机器人的真实世界部署中进行了广泛实验验证。

Conclusion: DIV-Nav系统能够有效处理包含空间关系的复杂自由文本查询，并改进了在线语义映射的前沿探索目标以更有效地指导搜索过程。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [13] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SP-Diff平行夹爪系统采用创新的差动连杆机构和模块化对称双指配置，实现线性平行抓取，通过行星齿轮传动实现同步线性运动和独立手指姿态调整，减少30%的Z轴重新校准需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统末端执行器在智能工业自动化中适应性有限的问题，为柔性制造提供更智能的机器人末端执行器解决方案。

Method: 采用差动连杆机构、模块化对称双指配置、行星齿轮传动、运动学优化的平行四边形连杆和差动机构，集成未来就绪的力/视觉传感器接口。

Result: 系统实现了自适应抓取能力，适用于各种工业工件和可变形物体（如柑橘类水果），在数字孪生框架中支持多模态数据采集。

Conclusion: SP-Diff通过其自适应架构推进了机器人末端执行器的智能化，在协作机器人、物流自动化和特殊操作场景中具有广阔应用前景。

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [14] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: MoS-VLA是一个视觉-语言-动作模型框架，通过将机器人操作策略表示为有限基函数的线性组合，实现快速适应新任务。仅需单次专家演示，通过轻量级凸优化即可适应新环境。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在新环境、新机器人或新任务中往往无法直接使用，需要更高效的适应方法。

Method: 在预训练阶段联合学习基函数，创建结构化技能空间。测试时通过单次演示和L1动作误差最小化的凸优化实现梯度自由适应。

Result: 在五个未见数据集上实现更低的动作预测误差，在仿真和真实机器人任务中成功，而预训练VLA模型完全失败。

Conclusion: MoS-VLA框架通过技能混合表示和轻量级适应方法，显著提升了VLA模型的泛化能力和部署效率。

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [15] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本研究调查了急救人员在紧急行动中对机器人系统使用语义信息和态势感知的态度。通过对8个国家22名急救人员的问卷调查发现，他们普遍对机器人持积极态度，认为语义信息对态势感知很有用，并愿意使用准确率约75%的不完美AI支持工具。


<details>
  <summary>Details</summary>
Motivation: 了解急救人员对语义增强态势感知在机器人系统中的态度和需求，填补该领域跨国调查研究的空白，促进开发更符合用户需求的应急响应机器人系统。

Method: 采用结构化问卷调查法，对8个国家的22名急救人员进行调查，收集人口统计信息、对机器人的一般态度以及语义增强态势感知的体验。

Result: 大多数急救人员对机器人持积极态度，语义信息对态势感知的有用性评分为3.6/5，对预测突发事件的有用性评分为3.9/5。参与者要求语义输出准确率达到74.6%才信任，67.8%才认为有用。最受重视的语义信息包括物体身份、空间关系和风险背景。

Conclusion: 研究发现实验室机器人能力与现场部署现实之间存在关键差距，需要加强急救人员与机器人研究人员之间的合作。这些见解有助于开发更符合用户需求且具有情境感知能力的应急响应机器人系统。

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [16] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 该论文分析了激励设计对纳米和微卫星惯性参数识别的影响，比较了最小二乘法和扩展卡尔曼滤波器在不同激励条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究激励设计如何影响刚性纳米和微卫星惯性参数的识别精度，为在轨自适应惯性识别提供实用指导。

Method: 模拟非线性姿态动力学，考虑反作用轮耦合、执行器限制和外部干扰，使用8种不同频谱丰富度的扭矩剖面激励系统，比较批处理最小二乘法和扩展卡尔曼滤波器。

Result: 结果表明，激励频率内容和估计器假设共同决定了估计精度和鲁棒性，明确了每种方法表现最佳的条件。

Conclusion: 研究为在轨自适应惯性识别提供了实用指导，通过确定每种方法的最佳工作条件来优化估计性能。

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [17] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: 提出自适应不变扩展卡尔曼滤波方法，通过在线协方差估计自适应调整接触足模型噪声水平，改进腿式机器人的本体状态估计


<details>
  <summary>Details</summary>
Motivation: 状态估计对腿式机器人的控制性能和运动稳定性至关重要，传统方法难以处理微小滑动且过于敏感的滑动拒绝设置可能导致滤波器发散

Method: 采用自适应不变扩展卡尔曼滤波，基于在线协方差估计调整接触足模型噪声水平，使用接触检测算法替代接触传感器

Result: 在四足机器人LeoQuad上的真实实验验证了该方法在动态运动场景中具有增强的状态估计性能

Conclusion: 该方法能有效处理传统滑动拒绝方法无法解决的小滑动问题，减少对额外硬件的依赖，提高腿式机器人在变化接触条件下的状态估计精度

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [18] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: T3 Planner是一个基于大语言模型的机器人运动规划框架，通过三个级联模块分解时空任务约束，使用信号时序逻辑验证器来确保生成可行的运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域专业知识定制规划器，难以处理时空耦合问题，而大语言模型虽然擅长语义推理但会产生不可行的运动规划。需要一种能够自我修正输出的框架来解决这些问题。

Method: 使用三个级联模块分解时空任务约束，每个模块刺激LLM生成候选轨迹序列，并通过信号时序逻辑验证器检查可行性，直到找到满足复杂约束的解决方案。

Result: 在不同场景下的实验表明，T3 Planner显著优于基线方法，所需的推理能力可以蒸馏到轻量级的Qwen3-4B模型中实现高效部署。

Conclusion: T3 Planner成功解决了自然语言指令到可执行运动规划的转换问题，通过形式化方法确保运动规划的可行性，为机器人运动规划提供了有效的解决方案。

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [19] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: 该论文提出了从"工具导向"转向"认知驱动"的定位、导航与授时(PNT)新范式，通过融合机器PNT的高精度与脑启发空间认知导航，构建四层融合框架来推进通用PNT发展。


<details>
  <summary>Details</summary>
Motivation: 当前复杂环境需要更具韧性、节能和认知能力的PNT系统，目标是赋予无人系统脑启发的空间认知导航能力，同时利用机器PNT的高精度优势。

Method: 提出四层(观测-能力-决策-硬件)融合框架，将数值精度与脑启发智能相结合；对传统PNT、生物脑PNT和脑启发PNT进行多层次对比分析。

Result: 建立了认知驱动的PNT新视角和发展路线图，为脑启发PNT的未来发展提供了前瞻性建议。

Conclusion: 通过融合机器PNT的精度优势和脑启发的认知能力，可以实现从工具导向到认知驱动的PNT范式转变，推动通用PNT系统的进一步发展。

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [20] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: 提出了一种新的轨迹采样方法C-Free-Uniform，该方法通过考虑局部地图信息来均匀采样自由配置空间，并将其集成到MPPI控制器中，在复杂环境中以更少的采样预算实现更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹采样机制中控制输入分布p(u|x)独立于环境，这限制了在复杂环境中的导航性能。需要一种能够考虑环境信息并均匀采样自由配置空间的方法。

Method: 提出了C-Free-Uniform采样器，它显式地基于当前局部地图生成控制输入分布，以实现自由配置空间的均匀采样。然后将该采样器集成到新的MPPI控制器CFU-MPPI中。

Result: 实验表明，CFU-MPPI在杂乱多边形环境的挑战性导航任务中，相比现有方法具有更高的成功率，同时所需的采样预算显著减少。

Conclusion: C-Free-Uniform通过环境感知的采样机制有效提高了导航性能，证明了在轨迹采样中考虑环境信息的重要性。

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [21] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: RAPID Hand是一个低成本、20自由度的灵巧手原型，采用创新的仿人驱动和传动方案，专为灵巧遥操作设计，通过3D打印部件和定制齿轮实现经济性。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧遥操作中缺乏经济实惠的五指灵巧手的问题，这对于在'从演示中学习'范式中收集大规模真实机器人数据至关重要。

Method: 采用新型仿人驱动和传动方案，包括非拇指手指的通用指骨传动方案和全向拇指驱动机制，使用3D打印部件和定制齿轮实现低成本设计。

Result: 在灵巧遥操作系统中通过定量指标和定性测试评估，在多项挑战性任务中表现良好，包括多指抓取、勺子操作和类人钢琴演奏。

Conclusion: RAPID Hand的完全驱动20自由度设计在灵巧遥操作方面具有显著潜力，为大规模真实机器人数据收集提供了经济可行的解决方案。

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [22] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: DINO-CVA是一个多模态目标条件行为克隆框架，用于实现自主导管导航，融合视觉观察和操纵杆运动学，减少对操作者的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前心脏导管介入主要依赖手动操作，现有机器人系统缺乏智能自主性，导致操作者疲劳、辐射暴露增加和结果变异性。

Method: 提出DINO-CVA框架，将视觉观察和操纵杆运动学融合到联合嵌入空间，通过自回归方式从专家演示中预测动作，并使用目标条件引导导航。

Result: DINO-CVA在预测动作方面达到高准确率，与仅使用运动学的基线性能相当，同时将预测基于解剖环境。

Conclusion: 多模态目标条件架构在导管导航中具有可行性，是减少操作者依赖、提高导管治疗可靠性的重要一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [23] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 提出CEM-RM框架，通过基于遥操作控制策略优化肌腱驱动软体机器人手，相比纯优化方法减少超过一半的设计评估次数，同时从预收集的遥操作数据中学习优化手设计分布。


<details>
  <summary>Details</summary>
Motivation: 软体机器人手虽然能提供柔顺安全的交互，但设计同时满足柔顺性和功能多样性具有挑战性。硬件与控制协同设计虽然能更好耦合形态与行为，但搜索空间高维且仿真评估计算昂贵。

Method: 提出交叉熵方法与奖励模型(CEM-RM)框架，推导由柔性手指组成的软体机器人手设计空间，在仿真中实现并行化训练，优化后的手通过3D打印制造并在真实世界中部署。

Result: 在仿真和硬件实验中，优化设计在多种挑战性物体的抓取成功率上显著优于基线手。

Conclusion: CEM-RM框架能有效优化软体机器人手设计，大幅减少评估次数，提升抓取性能。

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [24] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 这篇论文系统综述了提升视觉-语言-动作（VLA）模型效率的方法，重点关注减少延迟、内存占用和训练/推理成本，将现有解决方案分为模型架构、感知特征、动作生成和训练/推理策略四个维度。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人控制中面临巨大的计算和内存需求，这与需要实时性能的边缘平台（如移动机械臂）的约束相冲突，因此需要提高VLA系统的效率和可扩展性。

Method: 采用系统综述方法，将现有效率提升方案分类为四个维度：模型架构、感知特征、动作生成和训练/推理策略，并在每个类别中总结代表性技术。

Result: 论文提供了VLA效率提升技术的全面分类框架，识别了各维度的关键方法，为开发更高效的具身智能系统奠定了基础。

Conclusion: 论文讨论了未来趋势和开放挑战，强调了推进高效具身智能的发展方向，为VLA系统的优化提供了系统性的指导框架。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [25] [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](https://arxiv.org/abs/2510.17143)
*Shantnav Agarwal,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 提出了一种基于机器学习的去中心化动力学规划方法，用于多无人机协同运输缆绳悬挂载荷，该方法在部分可观测和无通信条件下有效工作。


<details>
  <summary>Details</summary>
Motivation: 现有的多无人机运输方法通常依赖集中式控制架构或可靠的通信，这在实际应用中存在局限性。

Method: 利用模仿学习训练去中心化学生策略，通过模仿具有全局观测的集中式动力学运动规划器，使用物理信息神经网络生成平滑轨迹。

Result: 在仿真和真实环境中验证了该方法能够跟踪敏捷参考轨迹，性能与集中式方法相当，且每个策略训练时间少于2小时。

Conclusion: 该方法提供了一种高效的去中心化解决方案，在部分可观测和无通信条件下实现了与集中式方法相当的性能。

Abstract: Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

</details>


### [26] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++是一个增强型自动驾驶框架，通过度量引导的对齐机制桥接认知推理和端到端规划，结合VLA模型的世界知识和E2E模型的物理可行性。


<details>
  <summary>Details</summary>
Motivation: 传统E2E驾驶模型缺乏世界知识难以处理长尾场景，而VLA模型3D推理能力有限导致物理不可行动作，需要结合两者优势。

Method: 构建VLA模块生成语义驱动的轨迹，设计E2E模块确保物理可行性，引入度量引导的轨迹评分器对齐两个模块输出。

Result: 在ICCV 2025自动驾驶挑战赛上取得EPDMS 49.12的成绩。

Conclusion: DiffVLA++成功整合了认知推理和物理规划，在保持物理可行性的同时提升了复杂场景的处理能力。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [27] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: OmniVIC是一个基于视觉语言模型的通用可变阻抗控制器，通过自改进的检索增强生成和上下文学习技术，在接触丰富的机器人操作任务中提升安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统可变阻抗控制器在物理交互中虽有优势，但在未见过的、复杂的、非结构化的安全交互场景中缺乏泛化能力，无法适应涉及接触或不确定性的通用任务场景。

Method: 使用自改进的检索增强生成(RAG)从结构化记忆库中检索相关先验经验，结合上下文学习(ICL)利用检索到的示例和当前任务提示查询VLM，生成上下文感知的自适应阻抗参数，并通过实时力/力矩反馈确保交互力在安全阈值内。

Result: 在复杂接触丰富任务套件上优于基线方法，仿真和真实机器人任务中均表现出改进的成功率和减少的力违规。平均成功率从27%(基线)提升到61.4%(OmniVIC)。

Conclusion: OmniVIC在连接高层语义推理和底层合规控制方面迈出了一步，实现了更安全、更可泛化的机器人操作。

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [28] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: SimpleVSF是一个端到端自动驾驶框架，通过结合视觉语言模型(VLM)的认知能力和先进轨迹融合技术，提升复杂场景下的决策质量。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法在复杂场景中决策能力不足，需要更智能的规划系统来平衡安全性、舒适性和效率。

Method: 使用传统评分器和VLM增强评分器，结合鲁棒的权重融合器进行定量聚合，以及基于VLM的融合器进行定性、上下文感知的决策。

Result: 在ICCV 2025 NAVSIM v2端到端驾驶挑战赛中取得领先地位，展现出最先进的性能表现。

Conclusion: SimpleVSF框架通过融合VLM认知能力和轨迹融合技术，实现了安全性、舒适性和效率的卓越平衡。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [29] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 提出了一种基于事件相机的新型自定位系统，结合可见光通信(VLC)和可见光定位(VLP)，能够在GPS受限环境（如隧道）中实现车辆定位。


<details>
  <summary>Details</summary>
Motivation: 利用事件相机的高时间分辨率和高动态范围特性，解决传统传感器在快速移动物体和极端光照条件下（如隧道出口）的感知限制，为GPS受限环境提供可靠的定位方案。

Method: 使用Walsh-Hadamard码为多个LED分配唯一导频序列，通过事件相机接收信号并与码字进行相关来识别单个LED，同时利用相位相关(POC)进行距离估计，实现VLC和VLP的同步功能。

Result: 在车辆以30 km/h速度行驶的现场实验中，距离估计的均方根误差(RMSE)在100米范围内小于0.75米，误码率(BER)低于0.01。

Conclusion: 这是首个使用单个事件相机同时实现VLC和VLP功能的车辆载系统，在真实环境中表现出鲁棒的性能，为GPS受限环境下的定位提供了有效解决方案。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [30] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 提出Pole-Image表示方法，利用杆状地标作为锚点生成周围3D结构的签名，通过对比学习实现鲁棒的自定位和地图维护。


<details>
  <summary>Details</summary>
Motivation: 解决传统地标方法在可检测性和独特性之间的权衡问题，利用杆状地标的高检测性来生成具有高区分度的局部点云签名。

Method: 将杆状地标及其周围环境表示为以杆为原点的2D极坐标图像，通过对比学习训练视角不变且高区分度的描述符。

Result: 描述符能够克服感知混淆，实现鲁棒自定位；高精度编码支持高灵敏度变化检测，有助于地图维护。

Conclusion: Pole-Image方法成功结合了杆状地标的高检测性和对比学习的优势，为移动机器人长期自主性提供了有效的解决方案。

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [31] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了一个模块化控制框架，结合模型动态控制、在线模型自适应和自适应脚步规划，用于解决四足机器人在未知环境中的导航问题。


<details>
  <summary>Details</summary>
Motivation: 行星探索任务需要能在极端未知环境中导航的机器人。轮式机器人的移动性受限，而四足机器人能克服这些限制，但在未知条件下部署面临环境特定控制的挑战。

Method: 模块化控制框架结合模型动态控制、在线模型自适应和自适应脚步规划，包括状态估计、支持运行时重配置，并集成到ROS 2中开源可用。

Result: 在两个四足平台上验证性能，支持多种硬件架构，在火山实地测试中机器人行走超过700米。

Conclusion: 该框架成功解决了机器人和地形参数不确定性问题，为四足机器人在未知环境中的可靠部署提供了有效解决方案。

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [32] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: 提出基于Nets-within-Nets框架和Transformer的多机器人系统异常检测方法，用于识别LTL规范下的异常执行行为，在实验中获得高准确率。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统中异构智能体的高可靠性任务执行需要检测异常行为，包括错误任务序列、空间约束违反、时间不一致和任务语义偏离等问题。

Method: 采用Nets-within-Nets范式构建结构化数据生成框架，协调机器人动作与LTL全局任务规范，并提出基于Transformer的异常检测管道对机器人轨迹进行分类。

Result: 实验显示该方法在执行效率异常检测上达到91.3%准确率，核心任务违反检测88.3%，约束自适应异常检测66.8%。消融实验验证了所提方法的优越性。

Conclusion: 该方法能有效识别多机器人系统中的异常执行行为，为可靠的任务执行提供了有力保障。

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [33] [Floating-Base Deep Lagrangian Networks](https://arxiv.org/abs/2510.17270)
*Lucas Schulze,Juliano Decico Negri,Victor Barasuol,Vivian Suzano Medeiros,Marcelo Becker,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 提出了Floating-Base Deep Lagrangian Networks (FeLaN)，一种满足浮动基系统物理约束的灰盒建模方法，在保持物理可解释性的同时提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有灰盒模型忽略了浮动基系统（如人形机器人和四足机器人）的特定物理约束，如惯性矩阵的正定性、分支诱导稀疏性和输入独立性等特性。

Method: 引入满足所有物理约束的惯性矩阵参数化方法，基于Deep Lagrangian Networks (DeLaN)训练神经网络预测物理合理的惯性矩阵，在拉格朗日力学下最小化逆动力学误差。

Result: 在多个人形机器人和四足机器人数据集上的实验表明，FeLaN在仿真和真实机器人上都达到了高度竞争力的性能。

Conclusion: FeLaN方法不仅提供了更好的物理可解释性，还在浮动基系统建模中实现了优越的性能。

Abstract: Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

</details>


### [34] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出了一种集成交互时数据的新型视频规划框架，通过在线更新模型参数和过滤失败计划，实现隐式状态估计和动态适应能力


<details>
  <summary>Details</summary>
Motivation: 现有视频规划框架难以适应交互时的失败情况，无法在部分观测环境中处理不确定性

Method: 集成交互时数据到规划过程，在线更新模型参数，过滤先前失败的计划，实现隐式状态估计

Result: 在新模拟操作基准测试中展示出改进的重规划性能

Conclusion: 该框架能够提高重规划性能，推动基于视频的决策领域发展

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [35] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: 提出DDBot框架用于自动化操作颗粒材料，通过可微分物理模拟器实现高效系统识别和挖掘技能优化，在真实环境中实现零样本高精度部署。


<details>
  <summary>Details</summary>
Motivation: 解决颗粒材料操作中的复杂接触动力学、不可预测材料特性和复杂系统状态等挑战，填补现有方法在效率和精度上的不足。

Method: 使用GPU加速并行计算和自动微分的可微分物理模拟器，结合可微分技能到动作映射、任务导向演示方法、梯度裁剪和线搜索梯度下降。

Result: DDBot能在5-20分钟内收敛，高效识别未知颗粒材料动力学并优化挖掘技能，在零样本真实世界部署中实现高精度结果。

Conclusion: DDBot在挖掘任务中展现出鲁棒性和高效性，基准测试结果证实其优于现有最先进方法。

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [36] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 提出了统一的交互力-阻抗控制(IFIC)框架，通过适应交互功率流确保在接触丰富环境中的轻松安全交互。


<details>
  <summary>Details</summary>
Motivation: 在混合或统一的力-阻抗控制下，机器人系统与主动人类或非被动环境物理交互时可能失去被动性，从而危及安全。

Method: 在端口哈密顿框架内制定控制架构，包含交互和任务控制端口，通过该框架保证系统被动性。

Result: IFIC框架能够适应交互功率流，确保在接触丰富环境中的轻松安全交互。

Conclusion: 所提出的控制架构通过端口哈密顿框架保证了系统被动性，解决了在接触丰富环境中机器人系统可能失去被动性的安全问题。

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [37] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: 将视觉-语言-动作模型部署到软连续机械臂上，通过微调实现安全的人机交互


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型仅限于刚性机械臂，缺乏与环境安全交互的能力，而软机器人具有安全性和适应性优势

Method: 提出结构化微调和部署流程，评估两种先进VLA模型在代表性操作任务中的表现

Result: 未经微调的策略因体现形式不匹配而失败，但通过针对性微调后软机器人性能与刚性机器人相当

Conclusion: 微调对于弥合体现形式差距至关重要，VLA模型与软机器人结合可在人机共享环境中实现安全灵活的具身AI

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [38] [Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting](https://arxiv.org/abs/2510.17408)
*Halima I. Kure,Jishna Retnakumari,Augustine O. Nwajana,Umar M. Ismail,Bilyaminu A. Romo,Ehigiator Egho-Promise*

Main category: cs.RO

TL;DR: 提出了一种结合可信AI与节能机械臂的智能垃圾分类方法，使用基于MobileNetV2的CNN模型实现六类垃圾的准确分类，并通过机械臂模拟器进行虚拟分拣和能耗优化。


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中智能废物管理系统的需求，通过可信AI技术提高垃圾分类的准确性、效率和可靠性。

Method: 使用MobileNetV2进行迁移学习增强的卷积神经网络进行垃圾分类，结合机械臂模拟器进行虚拟分拣，通过欧几里得距离计算能耗以实现最优运动。

Result: 模型训练准确率达到99.8%，验证准确率为80.5%，展示了强大的学习和泛化能力。

Conclusion: 该框架整合了可信AI的关键要素（透明度、鲁棒性、公平性和安全性），为城市智能废物管理系统提供了可靠且可扩展的解决方案。

Abstract: This paper presents a novel methodology that integrates trustworthy
artificial intelligence (AI) with an energy-efficient robotic arm for
intelligent waste classification and sorting. By utilizing a convolutional
neural network (CNN) enhanced through transfer learning with MobileNetV2, the
system accurately classifies waste into six categories: plastic, glass, metal,
paper, cardboard, and trash. The model achieved a high training accuracy of
99.8% and a validation accuracy of 80.5%, demonstrating strong learning and
generalization. A robotic arm simulator is implemented to perform virtual
sorting, calculating the energy cost for each action using Euclidean distance
to ensure optimal and efficient movement. The framework incorporates key
elements of trustworthy AI, such as transparency, robustness, fairness, and
safety, making it a reliable and scalable solution for smart waste management
systems in urban settings.

</details>


### [39] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON通过将丰富的3D空间token注入动作头，解决了现有VLA模型在3D空间推理方面的局限性，无需依赖专用传感器即可实现跨模态的强几何先验。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D编码器的视觉-语言-动作模型在3D真实世界中存在空间推理差距，限制了泛化能力和适应性。现有的3D集成技术要么需要专用传感器且跨模态迁移能力差，要么注入的几何线索较弱且会损害视觉-语言对齐。

Method: 提出FALCON范式，利用空间基础模型从RGB图像中提取强几何先验，通过空间增强动作头消费空间token而非将其连接到视觉-语言骨干网络，可选融合深度或姿态信息而无需重新训练或架构更改。

Result: 在三个仿真基准和十一个真实世界任务中，FALCON实现了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件以及物体尺度和高度变化下保持鲁棒性。

Conclusion: FALCON通过创新的空间token注入方法有效解决了空间表示、模态可迁移性和对齐方面的限制，为3D真实世界中的视觉-语言-动作模型提供了强大的空间推理能力。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [40] [A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions](https://arxiv.org/abs/2510.17448)
*Mirko Mizzoni,Pieter van Goor,Barbara Bazzana,Antonio Franchi*

Main category: cs.RO

TL;DR: 提出了一个在非线性系统控制中切换不同输出集的系统框架，通过反馈线性化实现输出切换，并证明了在适当条件下系统状态的均匀有界性。


<details>
  <summary>Details</summary>
Motivation: 为了在非线性系统控制中实现不同输出集之间的切换，同时保证系统的稳定性和性能。

Method: 引入meld概念来定义可从更大输出集合中选择的有效、可反馈线性化的输出子集，并建立切换条件。

Result: 证明了在适当的驻留时间和兼容性条件下，可以在不同meld之间切换，同时保证系统状态的均匀有界性，且误差动态在每个切换区间内保持指数稳定。

Conclusion: 该理论适用于任何可反馈线性化的非线性系统，如机器人、空中和地面车辆等，并通过机器人机械臂的数值模拟进行了验证。

Abstract: This letter presents a systematic framework for switching between different
sets of outputs for the control of nonlinear systems via feedback
linearization. We introduce the concept of a meld to formally define a valid,
feedback-linearizable subset of outputs that can be selected from a larger deck
of possible outputs. The main contribution is a formal proof establishing that
under suitable dwell-time and compatibility conditions, it is possible to
switch between different melds while guaranteeing the uniform boundedness of
the system state. We further show that the error dynamics of the active outputs
remain exponentially stable within each switching interval and that outputs
common to consecutive melds are tracked seamlessly through transitions. The
proposed theory is valid for any feedback linearizable nonlinear system, such
as, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a
simple numerical simulation of a robotic manipulator.

</details>


### [41] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: HumanMPC是一个用于3D微型飞行器在人群中导航的模型预测控制框架，结合了理论安全保证和数据驱动的人类运动预测模型，能够在保证安全的同时实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注简化的2D人群导航，未能充分考虑人体动态的复杂性，需要开发能够处理完整3D人体动态的安全导航方法。

Method: 提出基于可达性的安全公式，仅约束初始控制输入以确保安全，同时在整个规划时域内建模其效果；结合数据驱动的人类运动预测模型。

Result: 在模拟实验和真实世界测试中验证了方法的有效性，在目标导向导航和视觉伺服跟踪等任务中表现优异，确保安全且不过度保守。

Conclusion: HumanMPC框架在保证安全的同时实现了高效导航，优于基线方法，且具有通用性可适配其他平台。

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [42] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一种基于ADMM和DDP的分布式时空轨迹优化框架D-PDDP，用于解决大规模无人机群轨迹优化问题，通过参数化DDP进行局部规划，ADMM实现时空参数共识，并采用自适应惩罚参数调整减少迭代次数。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预先设定无人机最终时间且迭代次数过多，限制了在大规模无人机群中的实际应用，需要开发更高效的分布式优化方法。

Method: 采用双层架构：参数化DDP作为单个无人机的轨迹优化器，ADMM用于满足局部约束并实现所有无人机间的时空参数共识，形成分布式参数化DDP算法，并提出基于谱梯度方法的自适应惩罚参数调整准则。

Result: 仿真实验验证了所提算法的有效性，能够高效解决大规模无人机群的轨迹优化问题。

Conclusion: D-PDDP框架成功解决了大规模无人机群轨迹优化的挑战，通过分布式架构和自适应参数调整实现了高效优化。

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [43] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出了一个意图驱动的规划管道，用于多机器人协作完成复杂操作任务，通过集成感知到文本的场景编码、LLM集合生成候选动作序列、LLM验证器和确定性一致性过滤器，实现从人类简单语言指令到可执行多机器人计划的可靠映射。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人协作完成复杂操作任务的规划问题，这些机器人具有不同的末端执行器和能力，需要在非结构化场景中规划并执行串联的动作序列，同时能够灵活处理不同程度的人类监督输入。

Method: 提出意图驱动的规划管道，包括：(i)感知到文本的场景编码，(ii)基于LLM集合生成候选移除序列，(iii)LLM验证器强制执行格式和优先级约束，(iv)确定性一致性过滤器拒绝幻觉对象。

Result: 在200个真实场景和600个操作员提示的评估中，使用完整序列正确性和下一任务正确性指标比较了五种基于LLM的规划器。结果表明集成验证方法能够可靠地将操作员意图映射到安全、可执行的多机器人计划，同时保持低用户工作量。

Conclusion: 该管道能够可靠地将操作员意图映射到安全、可执行的多机器人计划，同时保持低用户工作量，为复杂多机器人协作任务提供了有效的解决方案。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [44] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 将TLIO扩展到自行车定位，提出改进的MoE模型，在保持精度的同时大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 自行车共享和多样化骑行应用快速发展，需要精确的自行车定位。传统GNSS方法受多路径效应影响，惯性导航方法依赖精确建模且鲁棒性有限。TLIO虽然漂移小但计算成本高，难以在移动设备上部署

Method: 扩展TLIO到自行车定位，引入改进的混合专家(MoE)模型，结合原始IMU数据和神经网络预测的位移

Result: 与最先进的LLIO框架相比，在保持相当精度的同时，参数减少64.7%，计算成本降低81.8%

Conclusion: 提出的改进MoE模型有效解决了TLIO计算成本高的问题，为移动设备上的自行车定位提供了可行方案

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [45] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了RESample框架，通过探索性采样自动生成OOD数据，增强VLA模型从分布外状态恢复的能力，提高机器人操作的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习数据集只包含成功轨迹，缺乏失败和恢复数据，导致VLA模型在处理分布外状态时表现不佳。

Method: 使用离线强化学习获取动作价值网络识别次优动作，通过rollout采样潜在OOD状态，设计探索性采样机制将动作代理纳入训练数据集。

Result: 在LIBERO基准测试和真实机器人操作任务中，RESample显著提升了VLA模型的稳定性和泛化能力。

Conclusion: RESample框架有效解决了VLA模型在分布外状态下的性能问题，为机器人操作提供了更鲁棒的解决方案。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [46] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Botany-Bot是一个植物表型分析系统，使用立体相机、数字转台、工业机械臂和3D高斯溅射模型构建植物的详细数字孪生，通过机械臂操作叶片拍摄被遮挡的植物细节图像。


<details>
  <summary>Details</summary>
Motivation: 商业植物表型分析系统使用固定相机无法感知被叶片遮挡的植物细节，需要开发能够主动操作叶片来获取遮挡区域高分辨率图像的系统。

Method: 使用两个立体相机、数字转台、工业机械臂和3D分割高斯溅射模型构建系统，开发了机械臂算法来抬起和推动叶片，拍摄被遮挡的茎芽和叶片正反面的高分辨率图像。

Result: 实验结果显示：叶片分割准确率90.8%，叶片检测准确率86.2%，叶片抬起/推动准确率77.9%，叶片正反面详细图像拍摄准确率77.3%。

Conclusion: Botany-Bot系统能够成功构建植物的详细数字孪生，通过机械臂操作有效获取被遮挡植物细节的高分辨率图像，代码、视频和数据集已公开。

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [47] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: SoftMimic是一个从示例动作学习人形机器人柔顺全身控制策略的框架，通过奖励策略匹配柔顺响应而非刚性跟踪参考动作，使机器人能够顺应外力同时保持平衡和姿态。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法鼓励刚性控制，当机器人遇到意外接触时会导致脆弱和不安全的行为。需要一种能够顺应外力同时保持平衡的方法。

Method: 利用逆运动学求解器生成可行的柔顺动作增强数据集，用于训练强化学习策略，奖励策略匹配柔顺响应而非刚性跟踪参考动作。

Result: 在仿真和真实世界实验中验证了该方法，展示了与环境的安���有效交互，能够吸收干扰并从单个动作片段泛化到各种任务。

Conclusion: SoftMimic框架能够学习柔顺的全身控制策略，使机器人能够安全有效地与环境交互，同时保持平衡和姿态。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [48] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了RoboBench基准，用于系统评估多模态大语言模型作为具身大脑在机器人操作任务中的认知能力，涵盖5个维度、14种能力、25个任务和6092个问答对。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注执行成功率，针对高级推理的评估存在维度不完整和任务真实性问题，无法全面评估认知能力。

Method: 构建了RoboBench基准，定义了指令理解、感知推理、泛化规划、功能预测和失败分析五个维度，并引入了MLLM-as-world-simulator评估框架来模拟计划可行性。

Result: 对14个MLLM的实验揭示了基本局限性：在隐式指令理解、时空推理、跨场景规划、细粒度功能理解和执行失败诊断方面存在困难。

Conclusion: RoboBench为量化高级认知能力提供了全面框架，可指导下一代具身MLLM的开发。

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>
