<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning](https://arxiv.org/abs/2511.15886)
*Jeremias Ferrao,Ezgi Basar,Khondoker Ittehadul Islam,Mahrokh Hassani*

Main category: cs.CL

TL;DR: 本研究调查多语言LLM中思维链推理的归因模式，发现归因分数过度强调最终推理步骤，结构化CoT提示主要对高资源拉丁语系语言有效，扰动会降低模型准确性和归因一致性。


<details>
  <summary>Details</summary>
Motivation: 评估多语言LLM中思维链推理的忠实性和可解释性，特别是在不同语言环境下的表现。

Method: 使用ContextCite进行步骤级归因和Inseq进行词级归因，在Qwen2.5 1.5B-Instruct模型上应用MGSM基准测试，并通过否定和干扰句进行受控扰动。

Result: 归因分数过度强调最终推理步骤（尤其在错误生成中）；结构化CoT提示显著提高高资源拉丁语系语言的准确性；扰动会降低模型准确性和归因一致性。

Conclusion: 思维链提示在多语言鲁棒性和解释透明度方面存在局限性。

Abstract: This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.

</details>


### [2] [Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language](https://arxiv.org/abs/2511.15887)
*Seungbeen Lee,Jinhong Jeong,Donghyun Kim,Yejin Son,Youngjae Yu*

Main category: cs.CL

TL;DR: Motion2Mind是一个评估机器通过非语言线索理解他人心理状态能力的框架，包含精心策划的视频数据集，涵盖222种非语言线索和397种心理状态。


<details>
  <summary>Details</summary>
Motivation: 现有心理理论基准主要关注错误信念任务和不对称信息推理，忽视了信念之外的其他心理状态和丰富的非语言交流。

Method: 利用专家策划的身体语言参考作为知识库，构建Motion2Mind数据集，包含细粒度非语言线索标注和手动验证的心理解释。

Result: 当前AI系统在非语言线索解释方面表现显著不足，在检测方面存在巨大性能差距，在解释方面相比人类标注者表现出过度解释模式。

Conclusion: 机器在理解非语言线索方面仍面临重大挑战，需要更全面的心理理论评估框架。

Abstract: Our ability to interpret others' mental states through nonverbal cues (NVCs) is fundamental to our survival and social cohesion. While existing Theory of Mind (ToM) benchmarks have primarily focused on false-belief tasks and reasoning with asymmetric information, they overlook other mental states beyond belief and the rich tapestry of human nonverbal communication. We present Motion2Mind, a framework for evaluating the ToM capabilities of machines in interpreting NVCs. Leveraging an expert-curated body-language reference as a proxy knowledge base, we build Motion2Mind, a carefully curated video dataset with fine-grained nonverbal cue annotations paired with manually verified psychological interpretations. It encompasses 222 types of nonverbal cues and 397 mind states. Our evaluation reveals that current AI systems struggle significantly with NVC interpretation, exhibiting not only a substantial performance gap in Detection, as well as patterns of over-interpretation in Explanation compared to human annotators.

</details>


### [3] [TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues](https://arxiv.org/abs/2511.15976)
*Sarik Ghazarian,Abhinav Gullapalli,Swair Shah,Anurag Beniwal,Nanyun Peng,Narayanan Sadagopan,Zhou Yu*

Main category: cs.CL

TL;DR: 提出了TOD-ProcBench基准，用于评估LLM在多轮任务导向对话中遵循复杂流程指令的能力，包含三个核心任务：相关语句检索与动作预测、违规响应识别和条件生成指令遵循响应。


<details>
  <summary>Details</summary>
Motivation: 现有TOD基准过度简化复杂指令为简单意图-槽位-API配置，无法反映现实世界中代理需要严格遵循包含复杂约束的自然语言流程指令的需求。

Method: 基于高质量ABCD数据集构建包含复杂过程指令的对话数据，将细粒度约束和动作流程表述为多级条件-动作指令语句，设计三个任务全面评估LLM的指令遵循能力。

Result: 构建了包含复杂流程指令的基准数据集，设计了三个评估任务来系统测试LLM在多轮TOD中的指令理解与遵循能力，并研究了多语言设置和指令格式对性能的影响。

Conclusion: TOD-ProcBench填补了现有基准在复杂指令遵循评估方面的空白，为系统评估LLM在多轮任务导向对话中的指令遵循能力提供了标准化工具。

Abstract: In real-world task-oriented dialogue (TOD) settings, agents are required to strictly adhere to complex instructions while conducting multi-turn conversations with customers. These instructions are typically presented in natural language format and include general guidelines and step-by-step procedures with complex constraints. Existing TOD benchmarks often oversimplify the complex nature of these instructions by reducing them to simple schemas composed of intents, slots, and API call configurations. To address this gap and systematically benchmark LLMs' instruction-following capabilities, we propose TOD-ProcBench, a challenging benchmark featuring complex process instructions with intricate, fine-grained constraints that evaluates various LLMs' abilities to understand and follow instructions in multi-turn TODs. Our benchmark dataset comprises instruction documents derived from the high-quality ABCD dataset with corresponding conversations under human quality control. We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements. We design three tasks to comprehensively benchmark LLMs' complex instruction-following capabilities in multi-turn TODs. Task 1 evaluates how LLMs retrieve the most relevant statement from a complex instruction and predict the corresponding next action. In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions, and then we analyze how effectively LLMs can identify instruction-violating responses. Task 3 investigates LLMs' abilities in conditional generation of instruction-following responses based on the original complex instructions. Additionally, we conduct studies on the impact of multilingual settings and different instruction text formats on compliance performance. We release our benchmark under the Llama 3.3 Community License Agreement.

</details>


### [4] [Liars' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/abs/2511.16035)
*Kieron Kretschmar,Walter Laurito,Sharan Maiya,Samuel Marks*

Main category: cs.CL

TL;DR: LIARS' BENCH是一个包含72,863个谎言和诚实回答的测试平台，涵盖四种开源模型和七个数据集，用于评估现有谎言检测技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型谎言检测技术通常在狭窄环境中验证，无法捕捉模型可能生成的各种谎言类型。

Method: 构建LIARS' BENCH测试平台，包含72,863个谎言和诚实回答示例，涵盖四种开源模型和七个数据集，从两个维度分析谎言：模型说谎的原因和谎言针对的信念对象。

Result: 评估三种黑白盒谎言检测技术发现，现有技术系统性无法识别某些类型的谎言，特别是在仅从文本无法判断模型是否说谎的场景中。

Conclusion: LIARS' BENCH揭示了现有技术的局限性，为谎言检测技术的进步提供了实用的测试平台。

Abstract: Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.

</details>


### [5] [Learning Tractable Distributions Of Language Model Continuations](https://arxiv.org/abs/2511.16054)
*Gwen Yidou-Weng,Ian Li,Anji Liu,Oliver Broadrick,Guy Van den Broeck,Benjie Wang*

Main category: cs.CL

TL;DR: LTLA是一种混合方法，通过结合基础语言模型和固定可处理的代理模型，在保持推理效率的同时提高受控文本生成的质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有受控文本生成方法中代理模型上下文感知能力弱的问题，这些方法使用可处理的替代模型来近似继续概率，但往往缺乏足够的上下文信息。

Method: LTLA将基础语言模型用于丰富前缀编码，与固定可处理的代理模型配对计算精确的继续概率。通过单次批处理HMM更新处理所有下一个token候选，并仅让代理模型的潜在状态先验依赖于LM的隐藏表示。

Result: LTLA获得了比无条件HMM更高的条件似然，能够为视觉语言模型近似继续分布，在受控生成任务中提高了约束满足度且保持相当的流畅性，推理开销最小。

Conclusion: LTLA方法有效解决了受控文本生成中的效率和上下文感知问题，在保持推理效率的同时显著提升了生成质量。

Abstract: Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.

</details>


### [6] [Early science acceleration experiments with GPT-5](https://arxiv.org/abs/2511.16072)
*Sébastien Bubeck,Christian Coester,Ronen Eldan,Timothy Gowers,Yin Tat Lee,Alexandru Lupsasca,Mehtaab Sawhney,Robert Scherrer,Mark Sellke,Brian K. Spears,Derya Unutmaz,Kevin Weil,Steven Yin,Nikita Zhivotovskiy*

Main category: cs.CL

TL;DR: GPT-5在数学、物理、天文学、计算机科学、生物学和材料科学等多个科学领域的研究中提供了新的具体研究步骤，加速了科研工作，并在数学领域产生了四个经人类作者仔细验证的新结果。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型如GPT-5对科学家来说是越来越有价值的工具，但许多科学家仍不了解其能力。本文旨在通过案例研究展示GPT-5如何加速科学研究，并说明人机协作的有效模式。

Method: 收集了多个科学领域的短案例研究，记录了人类作者与GPT-5的互动过程，展示了富有成效的人机协作范例。

Result: GPT-5在多个科学领域提供了新的研究步骤，特别是在数学领域产生了四个经仔细验证的新结果，帮助人类数学家解决了之前未解决的问题。

Conclusion: 虽然GPT-5的贡献在范围上相对有限，但考虑到前沿AI的快速发展速度，其影响是深远的。AI在某些方面节省了专家时间，但在某些关键环节仍需要人类输入。

Abstract: AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.

</details>


### [7] [ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models](https://arxiv.org/abs/2511.16122)
*Qing Zhang,Bing Xu,Xudong Zhang,Yifan Shi,Yang Li,Chen Zhang,Yik Chung Wu,Ngai Wong,Yijie Chen,Hong Dai,Xiansen Chen,Mian Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于集成学习的自动提示优化框架ELPO，通过投票机制和多样化搜索方法提升提示优化的准确性和鲁棒性，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法主要依赖单一模型或算法，在处理复杂任务时性能受限，需要更有效的优化框架。

Method: ELPO框架结合集成学习思想，采用投票机制，引入共享生成策略和不同搜索方法，并提出了更高效的提示生成和搜索算法。

Result: 实验结果表明ELPO在多个任务上优于现有最优方法，如在ArSarcasm数据集上F1分数提高了7.6。

Conclusion: ELPO框架通过集成学习方法有效提升了提示优化的性能，为复杂任务提供了更准确和鲁棒的解决方案。

Abstract: The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.

</details>


### [8] [TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating](https://arxiv.org/abs/2511.16147)
*Dabiao Ma,Ziming Dai,Zhimin Xin,Shu Wang,Ye Wang,Haojun Fei*

Main category: cs.CL

TL;DR: 本文提出了一种新的参数高效微调范式TS-PEFT，通过选择性地对部分位置索引应用PEFT修改，挑战了传统PEFT对所有位置都进行修改的做法。


<details>
  <summary>Details</summary>
Motivation: 质疑传统PEFT方法对所有位置索引都进行修改的必要性，认为这种不加区分的修改可能不仅多余甚至有害。

Method: 引入Token-Selective PEFT (TS-PEFT)，使用选择函数S仅对部分位置索引应用PEFT修改，而不是对所有位置。

Result: 实验结果表明，对所有索引不加区分地应用PEFT不仅多余，而且可能适得其反。

Conclusion: 为PEFT提供了新视角，倡导更有针对性的修改方法，并为未来优化大模型微调过程提供了框架。

Abstract: In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.

</details>


### [9] [SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning](https://arxiv.org/abs/2511.16198)
*Sebastian Haan*

Main category: cs.CL

TL;DR: SemanticCite是一个AI驱动的系统，通过全文分析验证引文准确性，提供详细推理和相关文本片段，支持大规模引文验证。


<details>
  <summary>Details</summary>
Motivation: 学术文献面临语义引文错误、AI生成幻觉引用和传统引文格式无法指明具体支持章节的问题，需要准确验证引文以维护研究完整性。

Method: 结合多种检索方法和四类分类系统（支持、部分支持、不支持、不确定），使用微调的轻量级语言模型进行全文源分析。

Result: 微调的轻量级语言模型性能与大型商业系统相当，计算需求显著降低，使大规模引文验证实际可行。

Conclusion: SemanticCite通过可扩展的引文验证、简化同行评审和AI生成内容质量控制，为维护大规模引文准确性提供了开源基础。

Abstract: Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.

</details>


### [10] [SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs](https://arxiv.org/abs/2511.16275)
*Xingtao Zhao,Hao Peng,Dingli Su,Xianghua Zeng,Chunyang Liu,Jinzhi Liao,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了语义结构熵（SeSE），一个从结构信息角度量化大语言模型语义不确定性的框架，用于幻觉检测。该方法通过自适应稀疏化有向语义图建模语义空间，利用层次抽象提取潜在语义结构信息，在29个模型-数据集组合上显著优于现有不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法主要依赖语义概率分布或成对距离，忽略了潜在的语义结构信息，而这些信息可能实现更精确的不确定性估计。在安全关键场景中，可靠的不确定性量化对于避免模型产生幻觉至关重要。

Method: 首先开发自适应稀疏化有向语义图构建算法，捕捉方向性语义依赖同时自动修剪引入负面干扰的不必要连接。然后通过层次抽象利用潜在语义结构信息：SeSE定义为最优语义编码树的结构熵，形式化语义空间在最优压缩后的内在不确定性。对于长文本生成，扩展SeSE通过建模随机语义交互来量化单个声明的不确定性。

Result: 在29个模型-数据集组合上的广泛实验表明，SeSE显著优于先进的不确定性量化基线，包括强监督方法和最近提出的KLE方法。

Conclusion: SeSE提供了一个原则性的不确定性量化框架，能够从结构信息角度有效检测大语言模型的幻觉，在多个基准测试中表现出优越性能。

Abstract: Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.

</details>


### [11] [SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning](https://arxiv.org/abs/2511.16324)
*Wei Xia,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: 提出了SDA（Steering-Driven Distribution Alignment）框架，一种无需训练、模型无关的对齐方法，通过动态调整模型输出概率来增强LLMs与人类意图的对齐，在3H维度上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的广泛部署，确保模型响应与人类意图对齐成为关键挑战。需要在不进行昂贵重训练或大量监督的情况下，在推理阶段有效且高效地对齐模型行为。

Method: SDA框架基于用户定义的对齐指令动态重新分布模型输出概率，无需微调即可增强模型行为与人类意图的对齐。该方法轻量级、资源高效，兼容多种开源LLMs，支持独立推理或与训练策略集成。

Result: 在8个不同规模和来源的开源LLMs上评估，SDA在3H维度上取得显著提升：帮助性平均提升64.4%，诚实性提升30%，无害性提升11.5%，证明了其有效性和泛化能力。

Conclusion: SDA是一种高效、灵活的对齐框架，能够在推理阶段显著提升LLMs与人类意图的对齐性能，支持个性化偏好对齐，适用于多种模型和应用场景。

Abstract: With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.

</details>


### [12] [Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement](https://arxiv.org/abs/2511.16331)
*Jiashu Yao,Heyan Huang,Shuang Zeng,Chuwei Luo,WangJie You,Jie Tang,Qingsong Liu,Yuhang Guo,Yangyang Kang*

Main category: cs.CL

TL;DR: 提出自重写框架，通过模型重写自身推理文本来改进内部推理过程质量，解决仅关注最终正确性的单边奖励导致的推理质量问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习仅关注最终正确性，无法对内部推理过程提供详细监督，导致推理质量不佳，出现过思考、欠思考、冗余思考和混乱思考等问题。

Method: 采用选择性重写方法，仅对模型一致正确的"简单"样本进行重写，保持GRPO的原始奖励信号，并在单个批次中编译重写和原始生成，仅增加约10%开销。

Result: 在准确率-长度权衡方面，自重写方法在显著缩短推理长度(-46%)的同时提高了准确率(+0.6)；在内部推理质量方面，LLM作为评判者的评分显著提高(+7.2)，成功缓解了内部推理缺陷。

Conclusion: 自重写框架有效改进了大型推理模型的内部推理过程质量，在保持强化学习可扩展性的同时显著提升了推理效率和准确性。

Abstract: Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.

</details>


### [13] [NLP Datasets for Idiom and Figurative Language Tasks](https://arxiv.org/abs/2511.16345)
*Blake Matheny,Phuong Minh Nguyen,Minh Le Nguyen,Stephanie Reynolds*

Main category: cs.CL

TL;DR: 本文创建了大规模习语和比喻语言数据集，用于评估预训练语言模型在习语识别任务中的表现，并通过训练和评估展示了这些数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 习语和比喻语言在口语和书面语中占很大比例，但大型语言模型在处理这类语言时仍存在困难。虽然微调方法被证明是最优的，但更好更大的数据集可以进一步缩小这一差距。

Method: 使用现有习语和比喻语言数据集获取合并的习语列表，从大型语料库中检索上下文序列，创建了一个大规模潜在习语数据集和两个人工标注的确切习语数据集，并进行了后处理以适应模型无关的训练。

Result: 创建的数据集被用于训练，并在槽位标注和序列标注任务上进行了评估，展示了这些数据集在评估预训练语言模型处理比喻意义能力方面的有效性。

Conclusion: 本文提供的数据集为解决习语和比喻语言处理问题提供了一个答案，同时为构建新模型和开发新方法提供了多样化的类别基础。

Abstract: Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.

</details>


### [14] [Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies](https://arxiv.org/abs/2511.16353)
*Jonathan Kamp,Lisa Beinborn,Antske Fokkens*

Main category: cs.CL

TL;DR: 本文分析了自然语言解释中rationales的充分性指标，发现其与token分类和注意力正则化两种建模范式的关系，揭示了rationales信息的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有的充分性指标对rationales信息对模型性能的影响提供有限洞察，需要更深入地理解rationales在模型学习中的作用。

Method: 通过将充分性与token分类能力和注意力正则化方法相关联，分析rationales信息对模型性能的影响。

Result: 研究发现高度信息化的rationales不一定有助于正确分类；充分性反映了非rationalized上下文对分类的干扰；rationales信息可以提升跨域分类，但效果因任务和模型类型而异；充分性与token分类能力无关。

Conclusion: rationales信息具有复杂性，需要开发能够系统捕捉这类信息的指标进行进一步研究。

Abstract: Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.

</details>


### [15] [AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser](https://arxiv.org/abs/2511.16397)
*Ren Ma,Jiantao Qiu,Chao Xu,Pei Chu,Kaiwen Liu,Pengli Ren,Yuan Qu,Jiahui Peng,Linfeng Hou,Mengjie Liu,Lindong Lu,Wenchang Ning,Jia Yu,Rui Min,Jin Shi,Haojiong Chen,Peng Zhang,Wenjian Zhang,Qian Jiang,Zengjie Hu,Guoqiang Yang,Zhenxiang Li,Fukai Shang,Zhongying Tu,Wentao Zhang,Dahua Lin,Conghui He*

Main category: cs.CL

TL;DR: MinerU-HTML是一个基于语言模型的HTML到文本提取管道，通过序列标注方法显著提升了网页内容提取质量，特别是在保留结构化元素方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的网页语料库主要依赖启发式提取器（如Trafilatura），这些方法难以保留文档结构和结构化元素（如公式、代码、表格），而提高提取质量可能对下游性能产生与过滤策略同等重要的影响。

Method: 将内容提取重新定义为序列标注问题，使用0.6B参数的语言模型解决，采用两阶段格式化管道，先对语义元素进行显式分类，再转换为Markdown格式。

Result: 在MainWebBench基准测试中，MinerU-HTML的ROUGE-N F1达到81.8%，远高于Trafilatura的63.6%，在结构化元素保留方面表现卓越（代码块90.9%，公式94.0%）。使用MinerU-HTML构建的AICC语料库在13个基准测试中平均准确率达到50.8%，比Trafilatura提取的语料库高出1.08个百分点。

Conclusion: HTML提取是网页语料库构建中一个关键但常被低估的组件，基于模型的方法在提取质量和下游模型性能方面都显著优于传统的启发式方法。

Abstract: While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\% ROUGE-N F1 compared to Trafilatura's 63.6\%, with exceptional structured element preservation (90.9\% for code blocks, 94.0\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.

</details>


### [16] [Classification of worldwide news articles by perceived quality, 2018-2024](https://arxiv.org/abs/2511.16416)
*Connor McElroy,Thiago E. A. de Oliveira,Chris Brogly*

Main category: cs.CL

TL;DR: 本研究评估了机器学习和深度学习模型在区分低质量与高质量新闻文章方面的效果，使用141万篇新闻文章数据集，结果显示深度学习模型（特别是ModernBERT-large）表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习和深度学习模型是否能有效区分感知上的低质量与高质量新闻文章，以应对新闻质量评估的需求。

Method: 使用3种机器学习分类器和3种深度学习模型，基于141万篇英语新闻文章的数据集，每篇文章有194个语言特征，按专家共识评分将新闻源分为低质量和高质量两类。

Result: 传统机器学习分类器（如随机森林）表现良好（准确率0.7355，ROC AUC 0.8131），而深度学习模型表现更优，ModernBERT-large达到最佳性能（准确率0.8744，ROC-AUC 0.9593，F1 0.8739）。

Conclusion: 传统CPU机器学习和深度学习分类器都能有效区分全球新闻文章的感知质量，其中深度学习模型表现更佳。

Abstract: This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.

</details>


### [17] [ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports](https://arxiv.org/abs/2511.16438)
*Sherine George,Nithish Saji*

Main category: cs.CL

TL;DR: ESGBench是一个用于评估可解释ESG问答系统的基准数据集和评估框架，包含基于企业可持续发展报告的问题、人工标注的答案和证据支持。


<details>
  <summary>Details</summary>
Motivation: 需要评估ESG问答系统的可解释性和准确性，促进透明和可问责的ESG人工智能系统研究。

Method: 构建包含多个ESG主题领域问题的基准数据集，配备人工标注的答案和证据支持，用于细粒度评估模型推理能力。

Result: 分析了最先进大语言模型在ESGBench上的表现，揭示了在事实一致性、可追溯性和领域对齐方面的关键挑战。

Conclusion: ESGBench旨在加速透明和可问责的ESG人工智能系统的研究发展。

Abstract: We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.

</details>


### [18] [Anatomy of an Idiom: Tracing Non-Compositionality in Language Models](https://arxiv.org/abs/2511.16467)
*Andrew Gomes*

Main category: cs.CL

TL;DR: 该论文研究了基于Transformer的语言模型中习语表达的处理机制，通过电路发现和分析技术识别了习语处理的独特计算模式，包括"习语头"和"增强接收"现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解Transformer模型如何处理非组合性语言（如习语），探索模型在计算效率和鲁棒性之间的平衡机制。

Method: 采用改进的路径修补算法进行电路发现，识别和分析"习语头"（在多种习语中频繁激活的注意力头）以及"增强接收"（习语token间因早期处理而增强的注意力）。

Result: 发现习语处理具有独特的计算模式，识别出特定的习语头和增强接收现象，这些机制帮助Transformer平衡计算效率和鲁棒性。

Conclusion: 这些发现为理解Transformer处理非组合性语言提供了见解，并为研究更复杂语法结构的处理机制指明了方向。

Abstract: We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.

</details>


### [19] [Arctic-Extract Technical Report](https://arxiv.org/abs/2511.16470)
*Mateusz Chiliński,Julita Ołtusek,Wojciech Jaśkowski*

Main category: cs.CL

TL;DR: Arctic-Extract是一个轻量级但性能先进的模型，专门用于从扫描或数字商业文档中提取结构化数据，能在资源受限的硬件上部署。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限环境中高效处理长文档结构化数据提取的需求，特别是针对商业文档的问答、实体和表格提取任务。

Method: 开发了轻量级模型架构，通过优化的训练协议实现高性能，模型大小仅6.6 GiB，可在A10 GPU等资源受限设备上运行。

Result: 模型在A10 GPU（24GB内存）上可处理多达125页A4文档，在文档理解任务中表现出强大的性能。

Conclusion: Arctic-Extract证明了在保持最先进性能的同时，能够在资源受限硬件上高效处理长文档的结构化数据提取任务。

Abstract: Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.

</details>


### [20] [TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval](https://arxiv.org/abs/2511.16528)
*Özay Ezerceli,Mahmoud El Hussieni,Selva Taş,Reyhan Bayraktar,Fatma Betül Terzioğlu,Yusuf Çelebi,Yağız Asker*

Main category: cs.CL

TL;DR: TurkColBERT是首个针对土耳其语的密集编码器和延迟交互模型的综合基准测试，结果显示延迟交互模型在参数效率上显著优于密集编码器，且MUVERA+Rerank索引算法比PLAID快3.33倍。


<details>
  <summary>Details</summary>
Motivation: 神经信息检索系统在高资源语言中表现出色，但对土耳其语等形态丰富、资源较少的语言研究不足。目前土耳其语IR主要使用密集双编码器，而延迟交互模型尚未得到系统评估。

Method: 采用两阶段适应流程：先在土耳其语NLI/STS任务上微调英语和多语言编码器，然后使用PyLate在MS MARCO-TR上将其转换为ColBERT风格的检索器。评估了10个模型在5个土耳其BEIR数据集上的表现。

Result: 延迟交互模型在参数效率上表现突出：1.0M参数的colbert-hash-nano-tr比600M参数的turkish-e5-large小600倍，但保留了其71%以上的平均mAP。3-5倍小于密集编码器的延迟交互模型显著优于后者，ColmmBERT-base-TR在特定领域任务上mAP提升达+13.8%。

Conclusion: TurkColBERT为土耳其语检索提供了有效的基准，延迟交互模型在参数效率和性能上均优于密集编码器，MUVERA+Rerank实现了低延迟检索。但存在对中等规模数据集和翻译基准的依赖，需要更大规模的MUVERA评估。

Abstract: Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5$\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\times$ faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.

</details>


### [21] [Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks](https://arxiv.org/abs/2511.16540)
*Éloïse Benito-Rodriguez,Einar Urdshals,Jasmina Nasufi,Nicky Pochinkov*

Main category: cs.CL

TL;DR: 本文提出了一个预测框架，通过LLM的激活状态来预测输入文本的体裁，使用Mistral-7B模型在两个数据集上实现了最高98%和71%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型对于确保其安全有益部署至关重要，但由于LLM结构难以解释且无法人工评估所有输出，这一任务变得复杂。

Method: 使用Mistral-7B模型和两个数据集，通过scikit-learn分类器基于LLM的激活状态来预测文本体裁。

Result: 在两个数据集上分别实现了98%和71%的F1分数，结果始终优于控制任务。

Conclusion: 这提供了一个概念验证，表明可以使用浅层学习模型从LLM中推断文本体裁。

Abstract: Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.

</details>


### [22] [WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue](https://arxiv.org/abs/2511.16544)
*Zachary Ellis,Jared Joselowitz,Yash Deo,Yajie He,Anna Kalygina,Aisling Higham,Mana Rahimzadeh,Yan Jia,Ibrahim Habli,Ernest Lim*

Main category: cs.CL

TL;DR: 该论文挑战了ASR评估中过度依赖WER的传统，发现WER等现有指标与临床影响相关性差，提出了基于LLM的自动化评估框架来替代人工专家评估。


<details>
  <summary>Details</summary>
Motivation: 当前ASR在临床对话中的评估过度依赖词错误率(WER)，但WER无法反映转录错误对临床实践的实际影响，需要建立与临床风险相关的评估标准。

Method: 通过专家临床医生标注两个医患对话数据集的转录错误临床影响，然后使用GEPA优化LLM-as-a-Judge框架，让Gemini-2.5-Pro模型模拟专家评估。

Result: WER和现有指标与临床风险标签相关性差，优化后的LLM评估器达到90%准确率和0.816的Cohen's κ系数，性能与人类专家相当。

Conclusion: 提出了一个经过验证的自动化框架，将ASR评估从简单的文本保真度转向必要的、可扩展的临床对话安全性评估。

Abstract: As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $κ$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.

</details>


### [23] [Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation](https://arxiv.org/abs/2511.16577)
*Kexin Zhao,Ken Forbus*

Main category: cs.CL

TL;DR: 提出了一种使用统计语言模型作为消歧神谕的方法，无需人工标注训练数据，通过将符号NLU系统生成的候选含义转换为可区分的自然语言替代项，让LLM根据上下文选择合适解释。


<details>
  <summary>Details</summary>
Motivation: 当前词义消歧方法主要针对粗粒度表示（如WordNet同义词集或FrameNet框架），需要人工标注训练数据，难以自动消歧更丰富的表示（如基于OpenCyc的表示），而这些表示对于复杂推理是必需的。

Method: 将符号NLU系统生成的多个候选含义转换为可区分的自然语言替代项，使用LLM作为神谕查询，根据语言上下文选择适当的解释，并将选定的含义传播回符号NLU系统。

Result: 通过与人工标注的金标准答案进行比较评估，证明了该方法的有效性。

Conclusion: 该方法提供了一种无需人工标注训练数据的词义消歧解决方案，能够处理更丰富的语义表示，为复杂推理任务提供了支持。

Abstract: Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.

</details>


### [24] [Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems](https://arxiv.org/abs/2511.16654)
*Elias Lumer,Alex Cardenas,Matt Melich,Myles Mason,Sara Dieter,Vamse Kumar Subbiah,Pradeep Honaganahalli Basavaraju,Roberto Hernandez*

Main category: cs.CL

TL;DR: 本文比较了多模态RAG系统中两种检索方法：基于文本分块检索（图像先被LLM总结成文本）和直接多模态嵌入检索（图像以原生形式存储在向量空间中）。实验证明直接多模态嵌入检索显著优于基于LLM摘要的方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统依赖LLM将图像总结成文本进行预处理，导致关键视觉信息和上下文细节丢失，影响下游检索和问答性能。

Method: 对6个LLM模型和2个多模态嵌入模型进行综合比较分析，使用包含40个问答对的新建财务财报电话会议基准数据集，每个问答对配有1张图像和1个文本块。

Result: 直接多模态嵌入检索显著优于基于LLM摘要的方法，在平均精度均值上绝对提升13%，在归一化折损累计增益上绝对提升11%，相对提升分别为32%和20%。

Conclusion: LLM摘要会在预处理阶段引入信息损失，而直接多模态嵌入能保留视觉上下文用于检索和推理，产生更准确和事实一致的答案。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.

</details>


### [25] [Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs](https://arxiv.org/abs/2511.16664)
*Ali Taghibakhshi,Sharath Turuvekere Sreenivas,Saurav Muralidharan,Ruisi Cai,Marcin Chochowski,Ameya Sunil Mahabaleshwarkar,Yoshi Suhara,Oluwatobi Olabiyi,Daniel Korzekwa,Mostofa Patwary,Mohammad Shoeybi,Jan Kautz,Bryan Catanzaro,Ashwath Aithal,Nima Tajbakhsh,Pavlo Molchanov*

Main category: cs.CL

TL;DR: Nemotron Elastic是一个用于构建推理导向LLM的框架，通过单一父模型嵌入多个嵌套子模型，实现多预算部署，显著降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统训练不同规模模型家族成本过高，现有压缩方法仍需大量训练成本。需要一种更高效的方法来构建多尺度推理模型。

Method: 采用混合Mamba-Attention架构，通过端到端训练的路由器和两阶段训练课程，实现权重共享的嵌套子模型零-shot提取。

Result: 在Nemotron Nano V2 12B模型上应用，仅用110B训练token同时生成9B和6B模型，成本降低360倍，各嵌套模型精度达到或超过SOTA。

Conclusion: 该框架实现了多合一推理模型，部署内存恒定，为构建高效多尺度LLM提供了新范式。

Abstract: Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [26] [Gimballed Rotor Mechanism for Omnidirectional Quadrotors](https://arxiv.org/abs/2511.15909)
*J. Cristobal,A. Z. Zain Aldeen,M. Izadi,R. Faieghi*

Main category: cs.RO

TL;DR: 提出了一种采用万向节转子机制的模块化全向四旋翼设计，通过独立倾斜每个转子实现六自由度全驱动控制，无需对四旋翼中心结构进行重大改动。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼是欠驱动的，无法实现六自由度的独立控制。现有全向四旋翼设计通常需要显著的结构修改，本设计旨在提供轻量级且易于集成的解决方案。

Method: 在转子平台内集成舵机电机，使每个转子能够独立倾斜，同时开发了PX4自动驾驶仪中的新控制分配方案。

Result: 成功进行了飞行测试，验证了所提出方法的有效性。

Conclusion: 万向节转子机制为构建全向四旋翼提供了一种模块化且高效的解决方案，实现了全驱动控制而无需重大结构改动。

Abstract: This paper presents the design of a gimballed rotor mechanism as a modular and efficient solution for constructing omnidirectional quadrotors. Unlike conventional quadrotors, which are underactuated, this class of quadrotors achieves full actuation, enabling independent motion in all six degrees of freedom. While existing omnidirectional quadrotor designs often require significant structural modifications, the proposed gimballed rotor system maintains a lightweight and easy-to-integrate design by incorporating servo motors within the rotor platforms, allowing independent tilting of each rotor without major alterations to the central structure of a quadrotor. To accommodate this unconventional design, we develop a new control allocation scheme in PX4 Autopilot and present successful flight tests, validating the effectiveness of the proposed approach.

</details>


### [27] [I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration](https://arxiv.org/abs/2511.15914)
*Debasmita Ghose,Oz Gitelson,Ryan Jin,Grace Abawe,Marynel Vazquez,Brian Scassellati*

Main category: cs.RO

TL;DR: 提出了一种检测人类目标变化的方法，通过跟踪多个候选动作序列并与策略库验证其合理性。检测到变化后，机器人重新评估相关历史动作并构建RHP树来主动选择协助人类的动作，同时鼓励区分性动作以揭示更新后的目标。


<details>
  <summary>Details</summary>
Motivation: 在真实世界场景中，人类经常改变目标，这使得机器人在没有明确沟通的情况下难以适应。现有方法通常假设固定目标，将目标预测简化为一次性推理，无法有效处理目标变化。

Method: 跟踪多个候选动作序列并验证其与策略库的合理性；检测到目标变化后，重新评估相关历史动作；构建Receding Horizon Planning树来主动选择协助动作，同时鼓励区分性动作以揭示更新目标。

Result: 在包含30个独特食谱的协作烹饪环境中评估，该方法优于三个可比的人类目标预测算法，在目标切换后快速收敛到正确目标，减少任务完成时间，提高协作效率。

Conclusion: 该方法能够有效检测人类目标变化并快速适应，显著改善人机协作效率。

Abstract: For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.

</details>


### [28] [The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces](https://arxiv.org/abs/2511.15956)
*Aliyah Smith,Monroe Kennedy*

Main category: cs.RO

TL;DR: 研究机器人声音对人类感知的影响，发现操作声音不影响感知，侧向空间定位准确但正面定位下降，空间声音能同时传递任务信息并提升温暖感。


<details>
  <summary>Details</summary>
Motivation: 随着机器人融入日常生活，理解它们如何与人类沟通至关重要。声音提供了强大的交互渠道，包括操作噪音和有意设计的听觉提示。

Method: 通过定位和交接任务，研究结果性和功能性声音对人类感知和行为的影响，特别探索了空间声音的作用。

Result: Kinova Gen3机械臂的结果性声音未对感知产生负面影响，侧向空间定位高度准确但正面定位下降，空间声音能同时传达任务相关信息并促进温暖感、减少不适。

Conclusion: 功能性和变革性听觉设计有潜力增强人机协作，为未来基于声音的交互策略提供信息。

Abstract: As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.

</details>


### [29] [PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization](https://arxiv.org/abs/2511.15995)
*Zili Tang,Ying Zhang,Meng Guo*

Main category: cs.RO

TL;DR: 该论文提出了一种多机器人协作推动任意形状物体到目标位置的方法，结合了动态任务分配、混合优化搜索和混合控制，解决了复杂环境下的非抓取操作问题。


<details>
  <summary>Details</summary>
Motivation: 许多机器人没有配备机械手，许多物体不适合抓取操作（如大箱子和圆柱体）。在这种情况下，推动是一种简单有效的非抓取技能，但现有工作通常假设预定义的推动模式和固定形状物体。

Method: 方法基于组合混合优化，包括三个主要组件：(I) 推动子任务的分解、排序和滚动分配给机器人子组；(II) 关键帧引导的混合搜索优化每个子任务的参数化推动模式序列；(III) 混合控制执行这些模式并在它们之间转换。还采用了基于扩散的加速器来预测关键帧和推动模式。

Result: 该框架在温和假设下是完备的，在不同机器人数量和一般形状物体下的效率和有效性在仿真和硬件实验中得到了广泛验证，并可推广到异构机器人、平面装配和6D推动。

Conclusion: 提出的方法能够有效解决多机器人系统在复杂环境中协作推动任意形状物体的挑战，具有实际应用价值。

Abstract: Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.

</details>


### [30] [Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud](https://arxiv.org/abs/2511.16048)
*Qing Zhang,Jing Huang,Mingyang Xu,Jun Rekimoto*

Main category: cs.RO

TL;DR: 本文提出了一种"低保真"的软体飞行机器人艺术装置"语义故障"，使用多模态大语言模型进行语义导航，创造了具有生物启发个性的机器人，其成功标准是角色而非效率。


<details>
  <summary>Details</summary>
Motivation: 探索故意"低保真"方法的创造潜力，拒绝传统传感器，依赖语义理解来导航，创造具有历史负载身体的"叙事思维"机器人。

Method: 开发了新颖的自主管道，仅使用多模态大语言模型的定性语义理解进行导航，通过自然语言提示为机器人编写生物启发个性。

Result: 13分钟自主飞行日志和后续研究统计验证了框架的鲁棒性，能够创建可量化的不同个性，并观察到从地标导航到"计划到执行"差距等涌现行为。

Conclusion: 展示了一个低保真框架，用于创建不完美的伴侣机器人，其成功标准是角色特征而非效率，不可预测但合理的行为源于缺乏精确的本体感知。

Abstract: While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.

</details>


### [31] [Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers](https://arxiv.org/abs/2511.16050)
*Takeru Tsunoori,Masato Kobayashi,Yuki Uranishi*

Main category: cs.RO

TL;DR: Bi-AQUA是首个基于双边控制的水下模仿学习框架，通过三层光照适应机制解决水下机器人操作中的极端光照变化问题。


<details>
  <summary>Details</summary>
Motivation: 水下机器人操作面临极端光照变化、颜色失真和能见度降低等挑战，需要开发能够适应不同光照条件的鲁棒系统。

Method: 采用分层三层光照适应机制：1) 无需手动标注的照明编码器；2) 通过FiLM调制视觉主干特征进行自适应特征提取；3) 在transformer编码器输入中添加显式照明标记进行任务感知条件化。

Result: 在真实水下拾放任务中，Bi-AQUA在多样静态和动态光照条件下表现出鲁棒性能，显著优于无光照建模的双边基线。消融研究证实所有三个光照感知组件都至关重要。

Conclusion: 该工作将陆地双边控制模仿学习与水下操作连接起来，实现了在挑战性海洋环境中的力敏感自主操作。

Abstract: Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua

</details>


### [32] [MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics](https://arxiv.org/abs/2511.16158)
*Lara Bergmann,Cedric Grothues,Klaus Neumann*

Main category: cs.RO

TL;DR: 本文介绍了MagBotSim，一个用于磁悬浮系统的物理仿真平台，将磁悬浮系统视为机器人集群，为下一代制造业系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 磁悬浮系统在工业自动化中具有革命性潜力，不仅能实现动态运输，还具有未被开发的操纵能力。通过将运输和操纵能力结合到协调的磁机器人集群中，可以显著提高制造系统的效率、适应性和紧凑性。

Method: 开发了MagBotSim（磁机器人仿真），这是一个基于物理的磁悬浮系统仿真平台，将磁悬浮系统框架化为机器人集群。

Result: 提供了MagBotSim的完整平台，包括文档、视频、实验和代码，支持磁悬浮系统智能算法的开发。

Conclusion: 通过将磁悬浮系统视为机器人集群并提供专用仿真，这项工作为下一代由磁机器人驱动的制造系统奠定了基础。

Abstract: Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/

</details>


### [33] [PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks](https://arxiv.org/abs/2511.16200)
*Kewei Chen,Yayu Long,Mingsheng Shang*

Main category: cs.RO

TL;DR: PIPHEN框架通过语义蒸馏将高维感知数据压缩为紧凑的物理表示，解决了多机器人系统中的"共享大脑困境"，显著降低带宽需求和决策延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人系统在复杂物理协作中面临的高维多媒体数据传输带来的带宽瓶颈和决策延迟问题，即"共享大脑困境"。

Method: 提出PIPHEN分布式物理认知控制框架：1）物理交互预测网络（PIPN）进行语义蒸馏，生成紧凑的物理表示；2）哈密顿能量网络（HEN）控制器基于能量守恒原理将表示转化为协调动作。

Result: 信息表示压缩至原始数据量的5%以下，协作决策延迟从315ms降至76ms，同时显著提高任务成功率。

Conclusion: 为资源受限的多机器人系统解决"共享大脑困境"提供了根本性的高效范式。

Abstract: Multi-robot systems in complex physical collaborations face a "shared brain dilemma": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace "raw data communication" with "semantic communication" by performing "semantic distillation" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the "shared brain dilemma" in resource-constrained multi-robot systems.

</details>


### [34] [DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks](https://arxiv.org/abs/2511.16223)
*Vincenzo Pomponi,Paolo Franceschi,Stefano Baraldo,Loris Roveda,Oliver Avram,Luca Maria Gambardella,Anna Valente*

Main category: cs.RO

TL;DR: DynaMimicGen (D-MG) 是一个可扩展的数据集生成框架，仅需少量人类演示就能训练机器人策略，特别支持动态任务设置。它通过分割演示为子任务，利用动态运动基元(DMPs)来适应和泛化演示行为到新环境，生成平滑、真实的笛卡尔轨迹，实时适应物体位姿、机器人状态或场景几何的变化。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量多样化数据集，收集耗时且不适用于动态环境。D-MG旨在通过最小化人类监督来训练鲁棒的操作策略，同时支持动态任务设置。

Method: 1. 将少量人类演示分割为有意义的子任务；2. 利用动态运动基元(DMPs)来适应和泛化演示行为；3. 生成平滑、真实且任务一致的笛卡尔轨迹；4. 实时适应环境变化（物体位姿、机器人状态、场景几何）。

Result: 在模仿学习中使用D-MG生成的数据训练的机器人代理，在长视野和接触丰富的基准测试中表现出色，包括积木堆叠和将杯子放入抽屉等任务，即使在不可预测的环境变化下也能保持良好性能。

Conclusion: D-MG通过消除对大量人类演示的需求并支持动态环境中的泛化，为手动数据收集提供了强大而高效的替代方案，为实现可扩展的自主机器人学习铺平了道路。

Abstract: Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.

</details>


### [35] [FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models](https://arxiv.org/abs/2511.16233)
*Kewei Chen,Yayu Long,Shuai Li,Mingsheng Shang*

Main category: cs.RO

TL;DR: FT-NCFM是一个数据中心的生成式数据蒸馏框架，通过事实追踪和对抗性合成方法，从大规模VLA数据集中提取信息密集的核心数据集，仅用5%的数据就能达到85-90%的全数据集性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型对大规模冗余数据集的依赖问题，现有模型中心优化方法（如模型压缩、策略蒸馏）无法从根本上解决数据层面的挑战。

Method: 使用事实追踪（FT）引擎结合因果归因和程序化对比验证来评估样本内在价值，通过对抗性NCFM过程合成模型无关、信息密集且可重用的数据资产。

Result: 在多个主流VLA基准测试中，仅使用5%蒸馏核心集训练的模型达到85-90%的全数据集成功率，同时减少80%以上的训练时间。

Conclusion: 智能数据蒸馏是构建高效高性能VLA模型的极具前景的新路径。

Abstract: The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.

</details>


### [36] [How Robot Dogs See the Unseeable](https://arxiv.org/abs/2511.16262)
*Oliver Bimber,Karl Dietrich von Ellenrieder,Michael Haller,Rakesh John Amala Arokia Nathan,Gianni Lunardi,Marco Camurri,Mohamed Youssef,Santos Miguel Orozco Soto,Jeremy E. Niven*

Main category: cs.RO

TL;DR: 该研究提出了一种基于动物摆动行为的合成孔径感知方法，通过机器人执行侧向摆动运动来创建宽合成孔径，从而在遮挡环境中实现清晰的背景感知。


<details>
  <summary>Details</summary>
Motivation: 解决机器人视觉中因小孔径和大景深导致的遮挡问题，传统相机使前景障碍物和背景物体都清晰成像，导致关键场景信息被遮挡。

Method: 让机器人执行摆动运动，相机形成宽合成孔径，通过计算整合捕获的图像合成具有极浅景深的图像，有效模糊遮挡元素同时使背景清晰。

Result: 该方法能够实时、高分辨率地恢复被遮挡场景，支持大型多模态模型进行高级视觉推理，且对遮挡具有鲁棒性。

Conclusion: 通过摆动运动实现合成孔径感知是将动物行为与机器人技术结合的关键，可在复杂杂乱环境中实现高级场景理解。

Abstract: Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.

</details>


### [37] [Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist](https://arxiv.org/abs/2511.16265)
*Haru Fukatsu,Ryoji Yasuda,Yuki Funabora,Shinji Doki*

Main category: cs.RO

TL;DR: 开发了Funabot-Upper可穿戴触觉套装，通过刺激关节和肌肉独立诱导14种上半身运动的动觉感知，相比前代设计显著减少了感知混合问题，识别准确率从68.8%提升至94.6%。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴触觉设备多局限于单一身体部位的验证，缺乏在多个身体部位应用相同方法的研究。前代设计在肩部和肘部之间存在感知混合问题。

Method: 建立了新的简化设计策略，通过独立刺激关节和肌肉来诱导躯干、肩部、肘部和腕部的动觉感知。利用人工肌肉收缩使服装三维变形的技术。

Result: 实验证实Funabot-Upper成功诱导了多个关节的动觉感知，同时减少了前代设计中观察到的感知混合。识别准确率从68.8%提升至94.6%。

Conclusion: 新设计的触觉套装在减少感知混合方面表现出优越性，为未来触觉应用展示了潜力。

Abstract: This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications.

</details>


### [38] [InEKFormer: A Hybrid State Estimator for Humanoid Robots](https://arxiv.org/abs/2511.16306)
*Lasse Hohmeyer,Mihaela Popescu,Ivan Bergonzani,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了一种结合不变扩展卡尔曼滤波器和Transformer网络的混合状态估计方法InEKFormer，用于人形机器人的状态估计。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在不同环境中的双足运动仍然具有挑战性，需要快速准确的状态估计来支持运动控制。传统方法需要专家知识调参，而深度学习方法为此提供了新可能。

Method: 结合不变扩展卡尔曼滤波器(InEKF)和Transformer网络，提出InEKFormer混合方法。

Result: 在RH5人形机器人数据集上的实验表明，Transformer在人形状态估计中具有潜力，但高维问题需要鲁棒的自回归训练。

Conclusion: Transformer网络在人形机器人状态估计中展现出潜力，但需要改进训练方法以应对高维挑战。

Abstract: Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot's floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. Due to recent advances in the field of machine learning, deep learning methods are increasingly used for state estimation tasks. In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network. We compare our method with the InEKF and the KalmanNet approaches on datasets obtained from the humanoid robot RH5. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.

</details>


### [39] [Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning](https://arxiv.org/abs/2511.16330)
*Shreyas Kumar,Ravi Prakash*

Main category: cs.RO

TL;DR: 提出了C-GMS框架，通过从数学定义的稳定增益调度流形中采样，在强化学习中同时学习DMP运动基元和VIC可变阻抗控制策略，保证李雅普诺夫稳定性和执行器可行性。


<details>
  <summary>Details</summary>
Motivation: 传统无模型强化学习在机器人技能学习中存在不稳定和不安全探索的风险，特别是由于阻抗增益的时变特性。

Method: 引入认证高斯流形采样(C-GMS)框架，将策略探索重新定义为从数学定义的稳定增益调度流形中采样，确保每个策略rollout都是稳定且物理可实现的。

Result: 在仿真和真实机器人上验证了C-GMS的有效性，即使在有界模型误差和部署时不确定性的情况下也能保证有界跟踪误差。

Conclusion: C-GMS为复杂环境中可靠的自主交互铺平了道路，消除了对奖励惩罚或事后验证的需求。

Abstract: Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.

</details>


### [40] [Flow-Aided Flight Through Dynamic Clutters From Point To Motion](https://arxiv.org/abs/2511.16372)
*Bowen Xu,Zexuan Yan,Minghao Lu,Xiyu Fan,Yi Luo,Youshen Lin,Zhiqiang Chen,Yeke Chen,Qiyuan Qiao,Peng Lu*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的自主飞行系统，通过单激光雷达感知实现从点到运动的直接控制，无需依赖目标检测、跟踪和预测，在动态环境中实现安全避障。


<details>
  <summary>Details</summary>
Motivation: 传统方法在动态环境中依赖显式建模障碍物运动进行避障，但在高度动态和遮挡场景中耗时且不可靠。本文旨在开发一种不依赖物体检测、跟踪和预测的自主飞行系统。

Method: 使用深度感知距离图和环境变化感知点流作为运动特征，构建轻量级环境表示；通过强化学习训练策略，利用相对运动调制的距离场进行策略优化；采用部署友好的感知仿真和无动力学模型的加速度控制。

Result: 所提系统在成功率和适应性方面优于替代方案，从模拟器导出的策略能够在真实四旋翼飞行器上实现安全机动。

Conclusion: 该方法通过变化感知的感知表示和强化学习，实现了在动态环境中的高效避障，展示了从模拟到真实世界的良好迁移能力。

Abstract: Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.

</details>


### [41] [Robot Metacognition: Decision Making with Confidence for Tool Invention](https://arxiv.org/abs/2511.16390)
*Ajith Anil Meera,Poppy Collis,Polina Arbuzova,Abián Torres,Paul F Kinghorn,Ricardo Sanz,Pablo Lanillos*

Main category: cs.RO

TL;DR: 提出基于置信度的机器人元认知架构，用于自主工具发明的用例，通过置信度评估决策可靠性，提高机器人在现实世界部署中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人缺乏对人类智能行为至关重要的元认知能力，即反思自身认知过程和决策的能力。人类通过元认知评估任务执行信心，从而调节行为并分配适当资源。

Method: 受神经科学启发，提出以置信度为中心的机器人元认知架构，将置信度作为机器人决策方案中的元认知度量，强调通过具身动作监控实现更明智的决策。

Result: 置信度知情的机器人能够评估其决策的可靠性，在现实世界物理部署中提高鲁棒性。

Conclusion: 机器人元认知架构通过置信度评估提升决策质量，强调了具身动作监控的重要性，并指出了机器人元认知的潜在应用和研究方向。

Abstract: Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition.

</details>


### [42] [Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators](https://arxiv.org/abs/2511.16406)
*Luis Luna,Isaac Chairez,Andrey Polyakov*

Main category: cs.RO

TL;DR: 提出了一种新颖的齐次PID控制策略，用于移动机器人操作器的鲁棒协调运动控制，通过齐次控制理论提升系统稳定性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 移动机器人操作器由于非线性动力学、欠驱动特性以及基座与机械臂子系统间的耦合，存在显著的控制挑战，需要开发更鲁棒的控制方法。

Method: 设计齐次PID控制结构，将传统PID增益推广为非线性状态相关函数，采用分级齐次性方法改善跟踪误差收敛性，基于Lyapunov方法进行稳定性分析。

Result: 实验验证表明hPID控制器在移动基座和机械臂的高精度轨迹跟踪方面优于传统线性PID控制器，在响应时间、稳态误差和对模型不确定性的鲁棒性方面表现更佳。

Conclusion: 该研究为增强下一代移动操作系统在结构化和非结构化环境中的自主性和可靠性，提供了一个可扩展且基于分析的控制框架。

Abstract: Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.

</details>


### [43] [LAOF: Robust Latent Action Learning with Optical Flow Constraints](https://arxiv.org/abs/2511.16407)
*Xizhou Bu,Jiexi Lyu,Fulei Sun,Ruichen Yang,Zhiqiang Ma,Wei Li*

Main category: cs.RO

TL;DR: LAOF是一个利用光流作为动作驱动信号的伪监督框架，通过学习对干扰物具有鲁棒性的潜在动作表示，在标签稀缺条件下显著提升下游模仿学习和强化学习任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模视频中的潜在动作学习时，容易受到动作无关干扰物的影响。虽然引入动作监督可以缓解这一问题，但可用动作标签的稀缺性限制了其有效性。光流能够自然抑制背景元素并强调运动物体，因此被用作动作驱动信号。

Method: 提出LAOF框架，利用智能体的光流作为动作驱动信号，通过光流约束学习对干扰物具有鲁棒性的潜在动作表示。这是一个伪监督框架，在标签稀缺条件下稳定训练过程。

Result: 实验结果表明，LAOF学习的潜在表示在下游模仿学习和强化学习任务中优于现有方法。在标签极其稀缺的条件下，光流约束显著稳定了训练过程并提高了潜在表示的质量，即使在动作标签比例增加到10%时仍然有效。

Conclusion: LAOF框架在没有动作监督的情况下，能够达到或超越使用1%动作标签训练的动作监督方法的性能，证明了光流约束在潜在动作学习中的有效性。

Abstract: Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.

</details>


### [44] [From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization](https://arxiv.org/abs/2511.16434)
*Chenming Wu,Xiaofan Li,Chengkai Dai*

Main category: cs.RO

TL;DR: SEG是一个新颖的3D生成框架，通过将带偏移的直接偏好优化(ODPO)集成到3D生成流程中，直接在模型生成阶段优化以减少支撑材料使用。


<details>
  <summary>Details</summary>
Motivation: 当前3D打印从数字模型到物理对象的转换需要支撑结构来防止悬垂特征在制造过程中坍塌。现有切片技术主要关注后处理优化，而非在模型生成阶段解决支撑效率问题。

Method: SEG框架将支撑结构模拟集成到训练过程中，使用带偏移的直接偏好优化(ODPO)来鼓励生成需要较少支撑的几何形状。

Result: 在Thingi10k-Val和GPT-3DP-Val两个基准数据集上的实验表明，SEG在支撑体积减少和可打印性方面显著优于TRELLIS、DPO和DRO等基线模型。定性结果显示SEG在保持输入提示高保真度的同时最小化支撑结构需求。

Conclusion: SEG通过在生成过程中直接优化模型，有潜力改变3D打印实践，为更可持续和高效的数字制造铺平道路。

Abstract: The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\textit{\underline{S}upport-\underline{E}ffective \underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.

</details>


### [45] [MiMo-Embodied: X-Embodied Foundation Model Technical Report](https://arxiv.org/abs/2511.16518)
*Xiaoshuai Hao,Lei Zhou,Zhijian Huang,Zhiwen Hou,Yingbo Tang,Lingfeng Zhang,Guang Li,Zheng Lu,Shuhuai Ren,Xianhui Meng,Yuchen Zhang,Jing Wu,Jinghui Lu,Chenxu Dang,Jiayi Guan,Jianhua Wu,Zhiyi Hou,Hanbing Li,Shumeng Xia,Mingliang Zhou,Yinan Zheng,Zihao Yue,Shuhao Gu,Hao Tian,Yuannan Shen,Jianwei Cui,Wen Zhang,Shaoqing Xu,Bing Wang,Haiyang Sun,Zeyu Zhu,Yuncheng Jiang,Zibin Guo,Chuhong Gong,Chaofan Zhang,Wenbo Ding,Kun Ma,Guang Chen,Rui Cai,Diyun Xiang,Heng Qu,Fuli Luo,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: MiMo-Embodied是首个在自动驾驶和具身AI领域都取得SOTA性能的跨具身基础模型，在29个基准测试中创下新记录。


<details>
  <summary>Details</summary>
Motivation: 探索自动驾驶和具身AI两个领域之间的正向迁移和相互增强效应，开发统一的跨领域基础模型。

Method: 采用多阶段学习、精心构建的数据集以及思维链/强化学习微调方法，整合两个领域的知识。

Result: 在17个具身AI基准测试（任务规划、功能预测、空间理解）和12个自动驾驶基准测试（环境感知、状态预测、驾驶规划）中均显著超越现有开源、闭源和专业基线模型。

Conclusion: 通过多阶段学习、数据构建和微调策略，自动驾驶和具身AI领域确实存在强烈的正向迁移效应，可以相互促进性能提升。

Abstract: We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.

</details>


### [46] [InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy](https://arxiv.org/abs/2511.16651)
*Yang Tian,Yuyin Yang,Yiman Xie,Zetao Cai,Xu Shi,Ning Gao,Hangxu Liu,Xuekun Jiang,Zherui Qiu,Feng Yuan,Yaping Li,Ping Wang,Junhao Cai,Jia Zeng,Hao Dong,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 该论文首次证明仅使用合成数据就能达到最强真实机器人数据集在VLA模型预训练中的性能，展示了大规模仿真的重要价值。


<details>
  <summary>Details</summary>
Motivation: 探索合成数据在视觉-语言-动作模型泛化中的作用，验证大规模仿真数据是否能替代真实机器人数据进行预训练。

Method: 构建包含63万条轨迹、7433小时的合成数据集InternData-A1，涵盖4种机器人形态、18种技能、70项任务和227个场景，使用高度自主、完全解耦的组合式仿真流水线生成数据。

Result: 仅使用合成数据预训练的模型在49个仿真任务、5个真实世界任务和4个长时程灵巧任务上与使用真实数据的π_0模型表现相当，并展现出零样本仿真到现实的迁移能力。

Conclusion: 合成数据能够替代真实机器人数据进行大规模VLA模型预训练，为具身AI研究提供了可扩展的数据创建方法，降低了获取大规模机器人数据的门槛。

Abstract: Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.

</details>


### [47] [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](https://arxiv.org/abs/2511.16661)
*Irmak Guzey,Haozhi Qi,Julen Urain,Changhao Wang,Jessica Yin,Krishna Bodduluri,Mike Lambeta,Lerrel Pinto,Akshara Rai,Jitendra Malik,Tingfan Wu,Akash Sharma,Homanga Bharadhwaj*

Main category: cs.RO

TL;DR: 提出了AINA框架，能够从任何人、任何地方、任何环境中使用Aria Gen 2眼镜收集的人类视频数据学习多指机器人策略，无需机器人数据即可直接部署。


<details>
  <summary>Details</summary>
Motivation: 解决从自然环境中的人类日常任务视频学习多指机器人策略的瓶颈问题，包括人类与机器人之间的具身差距以及从野外人类视频中提取相关上下文和运动线索的困难。

Method: 使用轻便便携的Aria Gen 2眼镜收集数据，该眼镜配备高分辨率RGB摄像头、准确的3D头部和手部姿态估计以及宽立体视图用于场景深度估计，学习基于3D点的多指手策略。

Result: 在九个日常操作任务中展示了结果，相比之前的人类到机器人策略学习方法表现出优势，策略对背景变化具有鲁棒性。

Conclusion: 通过简单的硬件设备和AINA框架，在实现从人类视频学习多指机器人策略的目标上迈出了重要一步，为实现通用机器人操作提供了可行路径。

Abstract: Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.

</details>
