{"id": "2512.21375", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.21375", "abs": "https://arxiv.org/abs/2512.21375", "authors": ["Yuanshuang Fu", "Qianyao Wang", "Qihao Wang", "Bonan Zhang", "Jiaxin Zhao", "Yiming Cao", "Zhijun Li"], "title": "Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) spectral remote sensing technology is widely used in water quality monitoring. However, in dynamic environments, varying illumination conditions, such as shadows and specular reflection (sun glint), can cause severe spectral distortion, thereby reducing data availability. To maximize the acquisition of high-quality data while ensuring flight safety, this paper proposes an active path planning method for dynamic light and shadow disturbance avoidance. First, a dynamic prediction model is constructed to transform the time-varying light and shadow disturbance areas into three-dimensional virtual obstacles. Second, an improved Interfered Fluid Dynamical System (IFDS) algorithm is introduced, which generates a smooth initial obstacle avoidance path by building a repulsive force field. Subsequently, a Model Predictive Control (MPC) framework is employed for rolling-horizon path optimization to handle flight dynamics constraints and achieve real-time trajectory tracking. Furthermore, a Dynamic Flight Altitude Adjustment (DFAA) mechanism is designed to actively reduce the flight altitude when the observable area is narrow, thereby enhancing spatial resolution. Simulation results show that, compared with traditional PID and single obstacle avoidance algorithms, the proposed method achieves an obstacle avoidance success rate of 98% in densely disturbed scenarios, significantly improves path smoothness, and increases the volume of effective observation data by approximately 27%. This research provides an effective engineering solution for precise UAV water quality monitoring in complex illumination environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u6c34\u8d28\u76d1\u6d4b\u7684\u52a8\u6001\u5149\u7167\u9634\u5f71\u5e72\u6270\u4e3b\u52a8\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u52a8\u6001\u9884\u6d4b\u6a21\u578b\u3001\u6539\u8fdbIFDS\u7b97\u6cd5\u3001MPC\u6846\u67b6\u548c\u52a8\u6001\u98de\u884c\u9ad8\u5ea6\u8c03\u6574\u673a\u5236\uff0c\u5728\u590d\u6742\u5149\u7167\u73af\u5883\u4e0b\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u548c\u98de\u884c\u5b89\u5168\u3002", "motivation": "\u65e0\u4eba\u673a\u5149\u8c31\u9065\u611f\u6280\u672f\u5728\u6c34\u8d28\u76d1\u6d4b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u53d8\u5316\u7684\u7167\u660e\u6761\u4ef6\uff08\u5982\u9634\u5f71\u548c\u955c\u9762\u53cd\u5c04\uff09\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u5149\u8c31\u5931\u771f\uff0c\u964d\u4f4e\u6570\u636e\u53ef\u7528\u6027\u3002\u4e3a\u4e86\u5728\u786e\u4fdd\u98de\u884c\u5b89\u5168\u7684\u540c\u65f6\u6700\u5927\u5316\u83b7\u53d6\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u9700\u8981\u89e3\u51b3\u5149\u7167\u9634\u5f71\u5e72\u6270\u95ee\u9898\u3002", "method": "1. \u6784\u5efa\u52a8\u6001\u9884\u6d4b\u6a21\u578b\uff0c\u5c06\u65f6\u53d8\u7684\u5149\u7167\u9634\u5f71\u5e72\u6270\u533a\u57df\u8f6c\u5316\u4e3a\u4e09\u7ef4\u865a\u62df\u969c\u788d\u7269\uff1b2. \u5f15\u5165\u6539\u8fdb\u7684\u5e72\u6270\u6d41\u4f53\u52a8\u529b\u5b66\u7cfb\u7edf\uff08IFDS\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u6392\u65a5\u529b\u573a\u751f\u6210\u5e73\u6ed1\u7684\u521d\u59cb\u907f\u969c\u8def\u5f84\uff1b3. \u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\u8fdb\u884c\u6eda\u52a8\u65f6\u57df\u8def\u5f84\u4f18\u5316\uff0c\u5904\u7406\u98de\u884c\u52a8\u529b\u5b66\u7ea6\u675f\u5e76\u5b9e\u73b0\u5b9e\u65f6\u8f68\u8ff9\u8ddf\u8e2a\uff1b4. \u8bbe\u8ba1\u52a8\u6001\u98de\u884c\u9ad8\u5ea6\u8c03\u6574\uff08DFAA\uff09\u673a\u5236\uff0c\u5728\u53ef\u89c2\u6d4b\u533a\u57df\u72ed\u7a84\u65f6\u4e3b\u52a8\u964d\u4f4e\u98de\u884c\u9ad8\u5ea6\u4ee5\u63d0\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edfPID\u548c\u5355\u4e00\u907f\u969c\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5bc6\u96c6\u5e72\u6270\u573a\u666f\u4e0b\u5b9e\u73b0\u4e8698%\u7684\u907f\u969c\u6210\u529f\u7387\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u5f84\u5e73\u6ed1\u5ea6\uff0c\u5e76\u5c06\u6709\u6548\u89c2\u6d4b\u6570\u636e\u91cf\u589e\u52a0\u4e86\u7ea627%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u5149\u7167\u73af\u5883\u4e0b\u7684\u7cbe\u786e\u65e0\u4eba\u673a\u6c34\u8d28\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e3b\u52a8\u8def\u5f84\u89c4\u5212\u548c\u52a8\u6001\u9ad8\u5ea6\u8c03\u6574\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u8d28\u91cf\u548c\u98de\u884c\u5b89\u5168\u6027\u3002"}}
{"id": "2512.21398", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21398", "abs": "https://arxiv.org/abs/2512.21398", "authors": ["Rahul Moorthy Mahesh", "Oguzhan Goktug Poyrazoglu", "Yukang Cao", "Volkan Isler"], "title": "Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction", "comment": null, "summary": "In cluttered environments, motion planners often face a trade-off between safety and speed due to uncertainty caused by occlusions and limited sensor range. In this work, we investigate whether co-pilot instructions can help robots plan more decisively while remaining safe. We introduce PaceForecaster, as an approach that incorporates such co-pilot instructions into local planners. PaceForecaster takes the robot's local sensor footprint (Level-1) and the provided co-pilot instructions as input and predicts (i) a forecasted map with all regions visible from Level-1 (Level-2) and (ii) an instruction-conditioned subgoal within Level-2. The subgoal provides the planner with explicit guidance to exploit the forecasted environment in a goal-directed manner. We integrate PaceForecaster with a Log-MPPI controller and demonstrate that using language-conditioned forecasts and goals improves navigation performance by 36% over a local-map-only baseline while in polygonal environments.", "AI": {"tldr": "PaceForecaster\u5229\u7528\u4eba\u7c7b\u6307\u4ee4\u9884\u6d4b\u73af\u5883\u5730\u56fe\u548c\u5b50\u76ee\u6807\uff0c\u5e2e\u52a9\u673a\u5668\u4eba\u5728\u906e\u6321\u73af\u5883\u4e2d\u66f4\u5b89\u5168\u5feb\u901f\u5730\u5bfc\u822a", "motivation": "\u5728\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u8fd0\u52a8\u89c4\u5212\u5668\u9762\u4e34\u5b89\u5168\u4e0e\u901f\u5ea6\u7684\u6743\u8861\uff0c\u56e0\u4e3a\u906e\u6321\u548c\u6709\u9650\u4f20\u611f\u5668\u8303\u56f4\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u3002\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u4eba\u7c7b\u6307\u4ee4\u5e2e\u52a9\u673a\u5668\u4eba\u66f4\u679c\u65ad\u5730\u89c4\u5212\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u3002", "method": "\u63d0\u51faPaceForecaster\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u6307\u4ee4\u6574\u5408\u5230\u5c40\u90e8\u89c4\u5212\u5668\u4e2d\u3002\u8f93\u5165\u5305\u62ec\u673a\u5668\u4eba\u5c40\u90e8\u4f20\u611f\u5668\u8303\u56f4(Level-1)\u548c\u4eba\u7c7b\u6307\u4ee4\uff0c\u8f93\u51fa\u9884\u6d4b\u5730\u56fe(Level-2)\u548c\u6307\u4ee4\u6761\u4ef6\u5316\u7684\u5b50\u76ee\u6807\u3002\u5b50\u76ee\u6807\u4e3a\u89c4\u5212\u5668\u63d0\u4f9b\u660e\u786e\u6307\u5bfc\uff0c\u4ee5\u76ee\u6807\u5bfc\u5411\u65b9\u5f0f\u5229\u7528\u9884\u6d4b\u73af\u5883\u3002", "result": "\u5c06PaceForecaster\u4e0eLog-MPPI\u63a7\u5236\u5668\u96c6\u6210\uff0c\u5728\u591a\u8fb9\u5f62\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u8bed\u8a00\u6761\u4ef6\u5316\u9884\u6d4b\u548c\u76ee\u6807\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5c40\u90e8\u5730\u56fe\u7684\u57fa\u7ebf\u5bfc\u822a\u6027\u80fd\u63d0\u534736%\u3002", "conclusion": "\u4eba\u7c7b\u6307\u4ee4\u53ef\u4ee5\u5e2e\u52a9\u673a\u5668\u4eba\u5728\u906e\u6321\u73af\u5883\u4e2d\u66f4\u6709\u6548\u5730\u89c4\u5212\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u53ef\u89c1\u533a\u57df\u548c\u8bbe\u7f6e\u6307\u4ee4\u6761\u4ef6\u5316\u5b50\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2512.21425", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21425", "abs": "https://arxiv.org/abs/2512.21425", "authors": ["Hang Zhou", "Yuhui Zhai", "Shiyu Shen", "Yanfeng Ouyang", "Xiaowei Shi", "Xiaopeng"], "title": "Developing a Fundamental Diagram for Urban Air Mobility Based on Physical Experiments", "comment": null, "summary": "Urban Air Mobility (UAM) is an emerging application of unmanned aerial vehicles (UAVs) that promises to reduce travel time and alleviate congestion in urban transportation systems. As drone density increases, UAM operations are expected to experience congestion similar to that in ground traffic. However, the fundamental characteristics of UAM traffic flow, particularly under real-world operating conditions, remain poorly understood. This study proposes a general framework for constructing the fundamental diagram (FD) of UAM traffic by integrating theoretical analysis with physical experiments. To the best of our knowledge, this is the first study to derive a UAM FD using real-world physical test data. On the theoretical side, we design two drone control laws for collision avoidance and develop simulation-based traffic generation methods to produce diverse UAM traffic scenarios. Based on Edie's definition, traffic flow theory is then applied to construct the FD and characterize the macroscopic properties of UAM traffic. To account for real-world disturbances and modeling uncertainties, we further conduct physical experiments on a reduced-scale testbed using Bitcraze Crazyflie drones. Both simulation and physical test trajectory data are collected and organized into the UAMTra2Flow dataset, which is analyzed using the proposed framework. Preliminary results indicate that classical FD structures for ground transportation are also applicable to UAM systems. Notably, FD curves obtained from physical experiments exhibit deviations from simulation-based results, highlighting the importance of experimental validation. Finally, results from the reduced-scale testbed are scaled to realistic operating conditions to provide practical insights for future UAM traffic systems. The dataset and code for this paper are publicly available at https://github.com/CATS-Lab/UAM-FD.", "AI": {"tldr": "\u9996\u4e2a\u901a\u8fc7\u7269\u7406\u5b9e\u9a8c\u6784\u5efa\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u57fa\u672c\u56fe\u7684\u7814\u7a76\uff0c\u7ed3\u5408\u7406\u8bba\u5206\u6790\u4e0e\u7269\u7406\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5730\u9762\u4ea4\u901aFD\u7ed3\u6784\u9002\u7528\u4e8eUAM\u7cfb\u7edf\uff0c\u5e76\u53d1\u73b0\u5b9e\u9a8c\u4e0e\u4eff\u771f\u7ed3\u679c\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5bc6\u5ea6\u589e\u52a0\uff0c\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u9884\u8ba1\u4f1a\u51fa\u73b0\u7c7b\u4f3c\u5730\u9762\u4ea4\u901a\u7684\u62e5\u5835\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u5bf9UAM\u4ea4\u901a\u6d41\u7684\u57fa\u672c\u7279\u6027\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u7279\u6027\u4e86\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6784\u5efaUAM\u4ea4\u901a\u57fa\u672c\u56fe\u7684\u901a\u7528\u6846\u67b6\uff0c\u7ed3\u5408\u7406\u8bba\u5206\u6790\u548c\u7269\u7406\u5b9e\u9a8c\u3002\u7406\u8bba\u65b9\u9762\u8bbe\u8ba1\u4e24\u79cd\u65e0\u4eba\u673a\u907f\u649e\u63a7\u5236\u5f8b\u548c\u57fa\u4e8e\u4eff\u771f\u7684\u4ea4\u901a\u751f\u6210\u65b9\u6cd5\uff1b\u5b9e\u9a8c\u65b9\u9762\u5728\u7f29\u6bd4\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u4f7f\u7528Crazyflie\u65e0\u4eba\u673a\u8fdb\u884c\u7269\u7406\u5b9e\u9a8c\uff0c\u6536\u96c6\u8f68\u8ff9\u6570\u636e\u6784\u5efaUAMTra2Flow\u6570\u636e\u96c6\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u5730\u9762\u4ea4\u901a\u7684\u7ecf\u5178\u57fa\u672c\u56fe\u7ed3\u6784\u540c\u6837\u9002\u7528\u4e8eUAM\u7cfb\u7edf\u3002\u7269\u7406\u5b9e\u9a8c\u83b7\u5f97\u7684\u57fa\u672c\u56fe\u66f2\u7ebf\u4e0e\u4eff\u771f\u7ed3\u679c\u5b58\u5728\u504f\u5dee\uff0c\u7a81\u663e\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002\u7f29\u6bd4\u6d4b\u8bd5\u7ed3\u679c\u88ab\u7f29\u653e\u5230\u5b9e\u9645\u8fd0\u884c\u6761\u4ef6\uff0c\u4e3a\u672a\u6765UAM\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4f7f\u7528\u771f\u5b9e\u7269\u7406\u6d4b\u8bd5\u6570\u636e\u63a8\u5bfcUAM\u57fa\u672c\u56fe\u7684\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u7406\u89e3UAM\u4ea4\u901a\u6d41\u7279\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2512.21430", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21430", "abs": "https://arxiv.org/abs/2512.21430", "authors": ["Yusuf Ali", "Gryphon Patlin", "Karthik Kothuri", "Muhammad Zubair Irshad", "Wuwei Liang", "Zsolt Kira"], "title": "EVE: A Generator-Verifier System for Generative Policies", "comment": null, "summary": "Visuomotor policies based on generative architectures such as diffusion and flow-based matching have shown strong performance but degrade under distribution shifts, demonstrating limited recovery capabilities without costly finetuning. In the language modeling domain, test-time compute scaling has revolutionized reasoning capabilities of modern LLMs by leveraging additional inference-time compute for candidate solution refinement. These methods typically leverage foundation models as verification modules in a zero-shot manner to synthesize improved candidate solutions. In this work, we hypothesize that generative policies can similarly benefit from additional inference-time compute that employs zero-shot VLM-based verifiers. A systematic analysis of improving policy performance through the generation-verification framework remains relatively underexplored in the current literature. To this end, we introduce EVE - a modular, generator-verifier interaction framework - that boosts the performance of pretrained generative policies at test time, with no additional training. EVE wraps a frozen base policy with multiple zero-shot, VLM-based verifier agents. Each verifier proposes action refinements to the base policy candidate actions, while an action incorporator fuses the aggregated verifier output into the base policy action prediction to produce the final executed action. We study design choices for generator-verifier information interfacing across a system of verifiers with distinct capabilities. Across a diverse suite of manipulation tasks, EVE consistently improves task success rates without any additional policy training. Through extensive ablations, we isolate the contribution of verifier capabilities and action incorporator strategies, offering practical guidelines to build scalable, modular generator-verifier systems for embodied control.", "AI": {"tldr": "EVE\u6846\u67b6\u901a\u8fc7\u96f6\u6837\u672cVLM\u9a8c\u8bc1\u5668\u5728\u63a8\u7406\u65f6\u63d0\u5347\u751f\u6210\u5f0f\u7b56\u7565\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3", "motivation": "\u751f\u6210\u5f0f\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff08\u5982\u6269\u6563\u548c\u6d41\u5339\u914d\uff09\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u8bed\u8a00\u6a21\u578b\u9886\u57df\u901a\u8fc7\u63a8\u7406\u65f6\u8ba1\u7b97\u6269\u5c55\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002\u672c\u7814\u7a76\u63a2\u7d22\u751f\u6210\u5f0f\u7b56\u7565\u662f\u5426\u4e5f\u80fd\u901a\u8fc7\u96f6\u6837\u672cVLM\u9a8c\u8bc1\u5668\u5728\u63a8\u7406\u65f6\u83b7\u5f97\u7c7b\u4f3c\u63d0\u5347\u3002", "method": "\u63d0\u51faEVE\u6846\u67b6\uff1a\u5c06\u9884\u8bad\u7ec3\u7684\u751f\u6210\u5f0f\u7b56\u7565\u4e0e\u591a\u4e2a\u96f6\u6837\u672cVLM\u9a8c\u8bc1\u5668\u7ed3\u5408\u3002\u9a8c\u8bc1\u5668\u63d0\u51fa\u52a8\u4f5c\u6539\u8fdb\u5efa\u8bae\uff0c\u52a8\u4f5c\u6574\u5408\u5668\u878d\u5408\u9a8c\u8bc1\u5668\u8f93\u51fa\u4e0e\u57fa\u7840\u7b56\u7565\u9884\u6d4b\uff0c\u751f\u6210\u6700\u7ec8\u6267\u884c\u52a8\u4f5c\u3002\u7814\u7a76\u4e86\u4e0d\u540c\u80fd\u529b\u9a8c\u8bc1\u5668\u95f4\u7684\u4fe1\u606f\u4ea4\u4e92\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cEVE\u4e00\u81f4\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u65e0\u9700\u989d\u5916\u7b56\u7565\u8bad\u7ec3\u3002\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u4e86\u9a8c\u8bc1\u5668\u80fd\u529b\u548c\u52a8\u4f5c\u6574\u5408\u7b56\u7565\u7684\u8d21\u732e\u3002", "conclusion": "EVE\u6846\u67b6\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u7b56\u7565\u901a\u8fc7\u63a8\u7406\u65f6\u8ba1\u7b97\u6269\u5c55\u63d0\u5347\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u4e3a\u96f6\u6837\u672cVLM\u9a8c\u8bc1\u5668\u5728\u5177\u8eab\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2512.21422", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21422", "abs": "https://arxiv.org/abs/2512.21422", "authors": ["Nathan Stringham", "Fateme Hashemi Chaleshtori", "Xinyuan Yan", "Zhichao Xu", "Bei Wang", "Ana Marasovi\u0107"], "title": "Teaching People LLM's Errors and Getting it Right", "comment": null, "summary": "People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.\n  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e3a\u4f55\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u51cf\u5c11\u7528\u6237\u5bf9LLM\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u53d1\u73b0\u5931\u8d25\u6a21\u5f0f\u786e\u5b9e\u5b58\u5728\uff0c\u4f46\u81ea\u52a8\u53d1\u73b0\u65b9\u6cd5\u6548\u679c\u4e0d\u4e00\uff0c\u4e14\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u7528\u6237\u7ecf\u5e38\u5728\u4e0d\u8be5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u65f6\u8fc7\u5ea6\u4f9d\u8d56\u5b83\u4eec\uff0c\u56e0\u4e3a\u770b\u5230LLM\u80fd\u4f5c\u8bd7\u56de\u7b54\u590d\u6742\u95ee\u9898\uff0c\u5c31\u9519\u8bef\u5730\u8ba4\u4e3a\u5b83\u4eec\u4e0d\u4f1a\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u51fa\u9519\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u5b9e\u4f8b\u5d4c\u5165\u8bc6\u522b\u5931\u8d25\u6a21\u5f0f\u5e76\u6559\u7ed9\u7528\u6237\uff0c\u4f46\u6548\u679c\u4e0d\u4f73\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u539f\u56e0\u3002", "method": "1) \u68c0\u67e5\u662f\u5426\u5b58\u5728\u5931\u8d25\u6a21\u5f0f\uff1a\u6309\u5143\u6807\u7b7e\u5206\u7ec4\u5b9e\u4f8b\uff0c\u8bc4\u4f30LLM\u5728\u5404\u7ec4\u7684\u9884\u6d4b\uff0c\u8bc6\u522b\u89c4\u6a21\u5927\u4e14\u9519\u8bef\u7387\u9ad8\u7684\u7ec4\uff1b2) \u6d4b\u8bd5\u63d0\u793a\u548c\u5d4c\u5165\u65b9\u6cd5\u80fd\u5426\u53d1\u73b0\u5df2\u77e5\u5931\u8d25\uff1b3) \u63d0\u51fa\u65b0\u8bc4\u4f30\u6307\u6807\u8861\u91cf\u7528\u6237\u5229\u7528\u5931\u8d25\u6a21\u5f0f\u9884\u6d4bLLM\u9519\u8bef\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "1) \u786e\u5b9e\u5b58\u5728\u53ef\u6559\u7684\u5931\u8d25\u6a21\u5f0f\uff1b2) \u81ea\u52a8\u53d1\u73b0\u65b9\u6cd5\u6548\u679c\u4e0d\u4e00\uff0c\u53ef\u80fd\u89e3\u91ca\u73b0\u6709\u65b9\u6cd5\u7684\u5931\u8d25\uff1b3) \u65b0\u8bc4\u4f30\u6307\u6807\u663e\u793a\u6559\u5b66\u6709\u79ef\u6781\u6548\u679c\uff0c\u800c\u4f20\u7edf\u7684\u4eba\u673a\u534f\u4f5c\u51c6\u786e\u7387\u6307\u6807\u672a\u80fd\u4f53\u73b0\u3002", "conclusion": "\u6559\u6388\u5931\u8d25\u6a21\u5f0f\u53ef\u80fd\u662f\u51cf\u5c11\u8fc7\u5ea6\u4f9d\u8d56\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u6210\u529f\u53d6\u51b3\u4e8e\u66f4\u597d\u7684\u81ea\u52a8\u5931\u8d25\u53d1\u73b0\u65b9\u6cd5\u548c\u4f7f\u7528\u672c\u6587\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2512.21438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21438", "abs": "https://arxiv.org/abs/2512.21438", "authors": ["Marvin Chanc\u00e1n", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Planetary Terrain Datasets and Benchmarks for Rover Path Planning", "comment": null, "summary": "Planetary rover exploration is attracting renewed interest with several upcoming space missions to the Moon and Mars. However, a substantial amount of data from prior missions remain underutilized for path planning and autonomous navigation research. As a result, there is a lack of space mission-based planetary datasets, standardized benchmarks, and evaluation protocols. In this paper, we take a step towards coordinating these three research directions in the context of planetary rover path planning. We propose the first two large planar benchmark datasets, MarsPlanBench and MoonPlanBench, derived from high-resolution digital terrain images of Mars and the Moon. In addition, we set up classical and learned path planning algorithms, in a unified framework, and evaluate them on our proposed datasets and on a popular planning benchmark. Through comprehensive experiments, we report new insights on the performance of representative path planning algorithms on planetary terrains, for the first time to the best of our knowledge. Our results show that classical algorithms can achieve up to 100% global path planning success rates on average across challenging terrains such as Moon's north and south poles. This suggests, for instance, why these algorithms are used in practice by NASA. Conversely, learning-based models, although showing promising results in less complex environments, still struggle to generalize to planetary domains. To serve as a starting point for fundamental path planning research, our code and datasets will be released at: https://github.com/mchancan/PlanetaryPathBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u578b\u884c\u661f\u8def\u5f84\u89c4\u5212\u57fa\u51c6\u6570\u636e\u96c6MarsPlanBench\u548cMoonPlanBench\uff0c\u8bc4\u4f30\u4e86\u7ecf\u5178\u4e0e\u5b66\u4e60\u578b\u7b97\u6cd5\u5728\u884c\u661f\u5730\u5f62\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u7ecf\u5178\u7b97\u6cd5\u5728\u6311\u6218\u6027\u5730\u5f62\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u5b66\u4e60\u578b\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5148\u524d\u4efb\u52a1\u7684\u5927\u91cf\u6570\u636e\u672a\u88ab\u5145\u5206\u5229\u7528\u4e8e\u8def\u5f84\u89c4\u5212\u7814\u7a76\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u7a7a\u95f4\u4efb\u52a1\u7684\u884c\u661f\u6570\u636e\u96c6\u3001\u6807\u51c6\u5316\u57fa\u51c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "method": "\u4ece\u706b\u661f\u548c\u6708\u7403\u7684\u9ad8\u5206\u8fa8\u7387\u6570\u5b57\u5730\u5f62\u56fe\u50cf\u4e2d\u521b\u5efa\u4e86\u4e24\u4e2a\u5927\u578b\u5e73\u9762\u57fa\u51c6\u6570\u636e\u96c6MarsPlanBench\u548cMoonPlanBench\uff0c\u5e76\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u8bbe\u7f6e\u4e86\u7ecf\u5178\u548c\u5b66\u4e60\u578b\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ecf\u5178\u7b97\u6cd5\u5728\u6708\u7403\u5357\u5317\u6781\u7b49\u6311\u6218\u6027\u5730\u5f62\u4e0a\u5e73\u5747\u80fd\u8fbe\u5230100%\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u6210\u529f\u7387\uff0c\u89e3\u91ca\u4e86NASA\u5728\u5b9e\u8df5\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u7b97\u6cd5\u7684\u539f\u56e0\uff1b\u5b66\u4e60\u578b\u6a21\u578b\u5728\u8f83\u7b80\u5355\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u884c\u661f\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u901a\u8fc7\u521b\u5efa\u9996\u4e2a\u884c\u661f\u8def\u5f84\u89c4\u5212\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u884c\u661f\u63a2\u6d4b\u8def\u5f84\u89c4\u5212\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u7ecf\u5178\u7b97\u6cd5\u5728\u884c\u661f\u5730\u5f62\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5b66\u4e60\u578b\u7b97\u6cd5\u5728\u6cdb\u5316\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2512.21439", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21439", "abs": "https://arxiv.org/abs/2512.21439", "authors": ["Geoffroy Morlat", "Marceau Nahon", "Augustin Chartouny", "Raja Chatila", "Ismael T. Freire", "Mehdi Khamassi"], "title": "Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models", "comment": "11 pages, 5 figures, +24 pages of Appendix", "summary": "Moral actions are judged not only by their outcomes but by the context in which they occur. We present COMETH (Contextual Organization of Moral Evaluation from Textual Human inputs), a framework that integrates a probabilistic context learner with LLM-based semantic abstraction and human moral evaluations to model how context shapes the acceptability of ambiguous actions. We curate an empirically grounded dataset of 300 scenarios across six core actions (violating Do not kill, Do not deceive, and Do not break the law) and collect ternary judgments (Blame/Neutral/Support) from N=101 participants. A preprocessing pipeline standardizes actions via an LLM filter and MiniLM embeddings with K-means, producing robust, reproducible core-action clusters. COMETH then learns action-specific moral contexts by clustering scenarios online from human judgment distributions using principled divergence criteria. To generalize and explain predictions, a Generalization module extracts concise, non-evaluative binary contextual features and learns feature weights in a transparent likelihood-based model. Empirically, COMETH roughly doubles alignment with majority human judgments relative to end-to-end LLM prompting (approx. 60% vs. approx. 30% on average), while revealing which contextual features drive its predictions. The contributions are: (i) an empirically grounded moral-context dataset, (ii) a reproducible pipeline combining human judgments with model-based context learning and LLM semantics, and (iii) an interpretable alternative to end-to-end LLMs for context-sensitive moral prediction and explanation.", "AI": {"tldr": "COMETH\u6846\u67b6\u7ed3\u5408\u6982\u7387\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001LLM\u8bed\u4e49\u62bd\u8c61\u548c\u4eba\u7c7b\u9053\u5fb7\u8bc4\u4f30\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u9884\u6d4b\u60c5\u5883\u5bf9\u6a21\u7cca\u884c\u4e3a\u9053\u5fb7\u5224\u65ad\u7684\u5f71\u54cd\uff0c\u76f8\u6bd4\u7aef\u5230\u7aefLLM\u63d0\u793a\u5c06\u4eba\u7c7b\u5224\u65ad\u5bf9\u9f50\u5ea6\u63d0\u9ad8\u7ea6\u4e00\u500d\u3002", "motivation": "\u9053\u5fb7\u5224\u65ad\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u884c\u4e3a\u7ed3\u679c\uff0c\u8fd8\u53d7\u60c5\u5883\u5f71\u54cd\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u7aef\u5230\u7aefLLM\uff09\u5728\u9884\u6d4b\u60c5\u5883\u654f\u611f\u7684\u9053\u5fb7\u5224\u65ad\u65b9\u9762\u8868\u73b0\u6709\u9650\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5b66\u4e60\u60c5\u5883\u5f71\u54cd\u5e76\u89e3\u91ca\u9884\u6d4b\u7684\u6a21\u578b\u3002", "method": "1. \u6784\u5efa\u5305\u542b300\u4e2a\u60c5\u5883\u30016\u7c7b\u6838\u5fc3\u884c\u4e3a\uff08\u4e0d\u6740\u3001\u4e0d\u6b3a\u9a97\u3001\u4e0d\u8fdd\u6cd5\uff09\u7684\u6570\u636e\u96c6\uff0c\u6536\u96c6101\u540d\u53c2\u4e0e\u8005\u7684\u4e09\u5143\u5224\u65ad\uff08\u8d23\u5907/\u4e2d\u7acb/\u652f\u6301\uff09\uff1b2. \u4f7f\u7528LLM\u8fc7\u6ee4\u548cMiniLM\u5d4c\u5165+K-means\u9884\u5904\u7406\u6807\u51c6\u5316\u884c\u4e3a\uff0c\u5f62\u6210\u6838\u5fc3\u884c\u4e3a\u805a\u7c7b\uff1b3. COMETH\u6846\u67b6\uff1a\u5728\u7ebf\u805a\u7c7b\u5b66\u4e60\u884c\u4e3a\u7279\u5b9a\u7684\u9053\u5fb7\u60c5\u5883\uff0c\u4f7f\u7528\u539f\u5219\u6027\u5dee\u5f02\u6807\u51c6\uff1b4. \u6cdb\u5316\u6a21\u5757\u63d0\u53d6\u7b80\u6d01\u7684\u4e8c\u5143\u60c5\u5883\u7279\u5f81\uff0c\u5728\u57fa\u4e8e\u4f3c\u7136\u7684\u900f\u660e\u6a21\u578b\u4e2d\u5b66\u4e60\u7279\u5f81\u6743\u91cd\u3002", "result": "COMETH\u5c06\u4eba\u7c7b\u591a\u6570\u5224\u65ad\u5bf9\u9f50\u5ea6\u4ece\u7aef\u5230\u7aefLLM\u63d0\u793a\u7684\u7ea630%\u63d0\u9ad8\u5230\u7ea660%\uff08\u7ea6\u7ffb\u500d\uff09\uff0c\u540c\u65f6\u80fd\u63ed\u793a\u9a71\u52a8\u9884\u6d4b\u7684\u60c5\u5883\u7279\u5f81\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3002", "conclusion": "COMETH\u63d0\u4f9b\u4e86\uff1a1\uff09\u57fa\u4e8e\u5b9e\u8bc1\u7684\u9053\u5fb7\u60c5\u5883\u6570\u636e\u96c6\uff1b2\uff09\u7ed3\u5408\u4eba\u7c7b\u5224\u65ad\u3001\u6a21\u578b\u4e0a\u4e0b\u6587\u5b66\u4e60\u548cLLM\u8bed\u4e49\u7684\u53ef\u590d\u73b0\u6d41\u7a0b\uff1b3\uff09\u7528\u4e8e\u60c5\u5883\u654f\u611f\u9053\u5fb7\u9884\u6d4b\u548c\u89e3\u91ca\u7684\u53ef\u89e3\u91ca\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u7aef\u5230\u7aefLLM\u65b9\u6cd5\u3002"}}
{"id": "2512.21497", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.21497", "abs": "https://arxiv.org/abs/2512.21497", "authors": ["Siddhartha Upadhyay", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task in Uncertain Dynamic Environment", "comment": null, "summary": "In this work, we extend the Spatiotemporal Tube (STT) framework to address Probabilistic Temporal Reach-Avoid-Stay (PrT-RAS) tasks in dynamic environments with uncertain obstacles. We develop a real-time tube synthesis procedure that explicitly accounts for time-varying uncertain obstacles and provides formal probabilistic safety guarantees. The STT is formulated as a time-varying ball in the state space whose center and radius evolve online based on uncertain sensory information. We derive a closed-form, approximation-free control law that confines the system trajectory within the tube, ensuring both probabilistic safety and task satisfaction. Our method offers a formal guarantee for probabilistic avoidance and finite-time task completion. The resulting controller is model-free, approximation-free, and optimization-free, enabling efficient real-time execution while guaranteeing convergence to the target. The effectiveness and scalability of the framework are demonstrated through simulation studies and hardware experiments on mobile robots, a UAV, and a 7-DOF manipulator navigating in cluttered and uncertain environments.", "AI": {"tldr": "\u5c06\u65f6\u7a7a\u7ba1\u6846\u67b6\u6269\u5c55\u81f3\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u6982\u7387\u65f6\u5e8f\u53ef\u8fbe-\u907f\u969c-\u505c\u7559\u4efb\u52a1\uff0c\u63d0\u51fa\u5b9e\u65f6\u7ba1\u5408\u6210\u65b9\u6cd5\uff0c\u63d0\u4f9b\u5f62\u5f0f\u5316\u6982\u7387\u5b89\u5168\u4fdd\u8bc1", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5904\u7406\u5177\u6709\u4e0d\u786e\u5b9a\u969c\u788d\u7269\u7684\u6982\u7387\u65f6\u5e8f\u53ef\u8fbe-\u907f\u969c-\u505c\u7559\u4efb\u52a1\u9700\u8981\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u6027\u3001\u5b89\u5168\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u8981\u6c42", "method": "\u6269\u5c55\u65f6\u7a7a\u7ba1\u6846\u67b6\uff0c\u5c06\u7ba1\u5b9a\u4e49\u4e3a\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u65f6\u53d8\u7403\u4f53\uff0c\u5176\u4e2d\u5fc3\u548c\u534a\u5f84\u57fa\u4e8e\u4e0d\u786e\u5b9a\u4f20\u611f\u4fe1\u606f\u5728\u7ebf\u6f14\u5316\uff0c\u63a8\u5bfc\u51fa\u65e0\u8fd1\u4f3c\u3001\u65e0\u4f18\u5316\u7684\u95ed\u5f0f\u63a7\u5236\u5f8b\uff0c\u786e\u4fdd\u7cfb\u7edf\u8f68\u8ff9\u4fdd\u6301\u5728\u7ba1\u5185", "result": "\u65b9\u6cd5\u63d0\u4f9b\u6982\u7387\u907f\u969c\u548c\u6709\u9650\u65f6\u95f4\u4efb\u52a1\u5b8c\u6210\u7684\u6b63\u5f0f\u4fdd\u8bc1\uff0c\u63a7\u5236\u5668\u65e0\u6a21\u578b\u3001\u65e0\u8fd1\u4f3c\u3001\u65e0\u4f18\u5316\uff0c\u80fd\u5b9e\u65f6\u9ad8\u6548\u6267\u884c\u5e76\u4fdd\u8bc1\u6536\u655b\u5230\u76ee\u6807\uff0c\u5728\u79fb\u52a8\u673a\u5668\u4eba\u3001\u65e0\u4eba\u673a\u548c7\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7684\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027", "conclusion": "\u63d0\u51fa\u7684\u65f6\u7a7a\u7ba1\u6269\u5c55\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u6982\u7387\u65f6\u5e8f\u53ef\u8fbe-\u907f\u969c-\u505c\u7559\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6267\u884c\u3001\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\u548c\u4efb\u52a1\u5b8c\u6210\u7684\u7edf\u4e00"}}
{"id": "2512.21494", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21494", "abs": "https://arxiv.org/abs/2512.21494", "authors": ["Soichiro Murakami", "Hidetaka Kamigaito", "Hiroya Takamura", "Manabu Okumura"], "title": "Oogiri-Master: Benchmarking Humor Understanding via Oogiri", "comment": null, "summary": "Humor is a salient testbed for human-like creative thinking in large language models (LLMs). We study humor using the Japanese creative response game Oogiri, in which participants produce witty responses to a given prompt, and ask the following research question: What makes such responses funny to humans? Previous work has offered only limited reliable means to answer this question. Existing datasets contain few candidate responses per prompt, expose popularity signals during ratings, and lack objective and comparable metrics for funniness. Thus, we introduce Oogiri-Master and Oogiri-Corpus, which are a benchmark and dataset designed to enable rigorous evaluation of humor understanding in LLMs. Each prompt is paired with approximately 100 diverse candidate responses, and funniness is rated independently by approximately 100 human judges without access to others' ratings, reducing popularity bias and enabling robust aggregation. Using Oogiri-Corpus, we conduct a quantitative analysis of the linguistic factors associated with funniness, such as text length, ambiguity, and incongruity resolution, and derive objective metrics for predicting human judgments. Subsequently, we benchmark a range of LLMs and human baselines in Oogiri-Master, demonstrating that state-of-the-art models approach human performance and that insight-augmented prompting improves the model performance. Our results provide a principled basis for evaluating and advancing humor understanding in LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u65e5\u672c\u521b\u610f\u56de\u5e94\u6e38\u620fOogiri\uff0c\u6784\u5efa\u4e86Oogiri-Master\u57fa\u51c6\u548cOogiri-Corpus\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7d\u9ed8\u7406\u89e3\u80fd\u529b\uff0c\u5206\u6790\u4e86\u4e0e\u5e7d\u9ed8\u76f8\u5173\u7684\u8bed\u8a00\u56e0\u7d20\uff0c\u5e76\u5c55\u793a\u4e86\u5148\u8fdb\u6a21\u578b\u63a5\u8fd1\u4eba\u7c7b\u8868\u73b0\u7684\u7ed3\u679c\u3002", "motivation": "\u5e7d\u9ed8\u662f\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u7c7b\u4eba\u521b\u9020\u6027\u601d\u7ef4\u7684\u91cd\u8981\u9886\u57df\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5b58\u5728\u5c40\u9650\uff1a\u6570\u636e\u96c6\u5019\u9009\u56de\u5e94\u5c11\u3001\u8bc4\u5206\u65f6\u66b4\u9732\u6d41\u884c\u5ea6\u4fe1\u53f7\u3001\u7f3a\u4e4f\u5ba2\u89c2\u53ef\u6bd4\u7684\u5e7d\u9ed8\u5ea6\u91cf\u6807\u51c6\u3002\u9700\u8981\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u4ec0\u4e48\u56e0\u7d20\u8ba9\u4eba\u7c7b\u89c9\u5f97\u56de\u5e94\u6709\u8da3\u3002", "method": "\u5f15\u5165Oogiri-Master\u57fa\u51c6\u548cOogiri-Corpus\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u63d0\u793a\u5bf9\u5e94\u7ea6100\u4e2a\u591a\u6837\u5316\u5019\u9009\u56de\u5e94\uff0c\u7531\u7ea6100\u540d\u72ec\u7acb\u4eba\u7c7b\u8bc4\u59d4\u8bc4\u5206\uff08\u907f\u514d\u770b\u5230\u4ed6\u4eba\u8bc4\u5206\u4ee5\u51cf\u5c11\u6d41\u884c\u5ea6\u504f\u5dee\uff09\u3002\u5206\u6790\u6587\u672c\u957f\u5ea6\u3001\u6b67\u4e49\u6027\u3001\u4e0d\u4e00\u81f4\u6027\u89e3\u51b3\u7b49\u8bed\u8a00\u56e0\u7d20\u4e0e\u5e7d\u9ed8\u6027\u7684\u5173\u8054\uff0c\u63a8\u5bfc\u9884\u6d4b\u4eba\u7c7b\u5224\u65ad\u7684\u5ba2\u89c2\u6307\u6807\u3002", "result": "\u5b9a\u91cf\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u5e7d\u9ed8\u76f8\u5173\u7684\u5177\u4f53\u8bed\u8a00\u56e0\u7d20\uff0c\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728Oogiri-Master\u57fa\u51c6\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u8868\u73b0\uff0c\u901a\u8fc7\u6d1e\u5bdf\u589e\u5f3a\u63d0\u793a\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7d\u9ed8\u7406\u89e3\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6765\u7cfb\u7edf\u7814\u7a76\u5e7d\u9ed8\u8fd9\u4e00\u590d\u6742\u7684\u521b\u9020\u6027\u8ba4\u77e5\u80fd\u529b\u3002"}}
{"id": "2512.21534", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21534", "abs": "https://arxiv.org/abs/2512.21534", "authors": ["Congrui Bai", "Zhenting Du", "Weibang Bai"], "title": "A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming", "comment": null, "summary": "This paper introduces a novel variable stiffness mechanism termed Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ) and systematically investigates its potential applications in variable stiffness robotic finger design. The proposed method utilizes electrostatic attraction to enhance interlayer friction, thereby suppressing relative sliding and enabling tunable stiffness. Compared with conventional planar ELJ, the helical configuration of HWS-ELJ provides exponentially increasing stiffness adjustment with winding angle, achieving significantly greater stiffness enhancement for the same electrode contact area while reducing the required footprint under equivalent stiffness conditions. Considering the practical advantage of voltage-based control, a series of experimental tests under different initial force conditions were conducted to evaluate the stiffness modulation characteristics of HWS-ELJ. The results demonstrated its rational design and efficacy, with outcomes following the deduced theoretical trends. Furthermore, a robotic finger prototype integrating HWS-ELJ was developed, demonstrating voltage-driven stiffness modulation and confirming the feasibility of the proposed robotic variable stiffness mechanism.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u87ba\u65cb\u7f20\u7ed5\u7ed3\u6784\u9759\u7535\u5c42\u5361\u6ede\uff08HWS-ELJ\uff09\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u624b\u6307\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9759\u7535\u5438\u9644\u589e\u5f3a\u5c42\u95f4\u6469\u64e6\u5b9e\u73b0\u521a\u5ea6\u8c03\u8282\uff0c\u76f8\u6bd4\u4f20\u7edf\u5e73\u9762ELJ\u5177\u6709\u6307\u6570\u7ea7\u521a\u5ea6\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5e73\u9762\u9759\u7535\u5c42\u5361\u6ede\uff08ELJ\uff09\u5728\u521a\u5ea6\u8c03\u8282\u8303\u56f4\u548c\u7a7a\u95f4\u6548\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u4ee5\u9002\u5e94\u673a\u5668\u4eba\u624b\u6307\u7b49\u7d27\u51d1\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u87ba\u65cb\u7f20\u7ed5\u7ed3\u6784\u8bbe\u8ba1\u9759\u7535\u5c42\u5361\u6ede\u673a\u5236\uff0c\u5229\u7528\u9759\u7535\u5438\u9644\u589e\u5f3a\u5c42\u95f4\u6469\u64e6\u6291\u5236\u76f8\u5bf9\u6ed1\u52a8\uff0c\u901a\u8fc7\u7535\u538b\u63a7\u5236\u5b9e\u73b0\u521a\u5ea6\u8c03\u8282\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u63a8\u5bfc\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "HWS-ELJ\u76f8\u6bd4\u4f20\u7edf\u5e73\u9762ELJ\u5728\u76f8\u540c\u7535\u6781\u63a5\u89e6\u9762\u79ef\u4e0b\u5b9e\u73b0\u6307\u6570\u7ea7\u521a\u5ea6\u589e\u5f3a\uff0c\u5728\u540c\u7b49\u521a\u5ea6\u6761\u4ef6\u4e0b\u51cf\u5c11\u5360\u7528\u7a7a\u95f4\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e0e\u7406\u8bba\u8d8b\u52bf\u4e00\u81f4\uff0c\u673a\u5668\u4eba\u624b\u6307\u539f\u578b\u9a8c\u8bc1\u4e86\u7535\u538b\u9a71\u52a8\u521a\u5ea6\u8c03\u8282\u7684\u53ef\u884c\u6027\u3002", "conclusion": "HWS-ELJ\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u7279\u522b\u9002\u5408\u673a\u5668\u4eba\u624b\u6307\u7b49\u7d27\u51d1\u578b\u5e94\u7528\uff0c\u901a\u8fc7\u7535\u538b\u63a7\u5236\u5b9e\u73b0\u7cbe\u786e\u521a\u5ea6\u8c03\u8282\uff0c\u4e3a\u53ef\u53d8\u521a\u5ea6\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2512.21567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21567", "abs": "https://arxiv.org/abs/2512.21567", "authors": ["Changzhi Sun", "Xiangyu Chen", "Jixiang Luo", "Dell Zhang", "Xuelong Li"], "title": "Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management", "comment": null, "summary": "External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.", "AI": {"tldr": "\u63d0\u51faDAM\u6846\u67b6\uff0c\u5c06LLM\u5916\u90e8\u5185\u5b58\u7ba1\u7406\u91cd\u6784\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u7528\u4ef7\u503c\u51fd\u6570\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8bc4\u4f30\u64cd\u4f5c\uff0c\u66ff\u4ee3\u542f\u53d1\u5f0f\u65b9\u6cd5", "motivation": "\u5f53\u524dLLM\u7cfb\u7edf\u7684\u5916\u90e8\u5185\u5b58\u7ba1\u7406\u4e3b\u8981\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u96be\u4ee5\u9884\u6d4b\u5185\u5b58\u51b3\u7b56\u7684\u957f\u671f\u548c\u4e0d\u786e\u5b9a\u6027\u540e\u679c\uff0c\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u6846\u67b6", "method": "\u63d0\u51faDAM\u6846\u67b6\uff0c\u5c06\u5185\u5b58\u7ba1\u7406\u5206\u89e3\u4e3a\u5373\u65f6\u4fe1\u606f\u8bbf\u95ee\u548c\u5206\u5c42\u5b58\u50a8\u7ef4\u62a4\uff0c\u901a\u8fc7\u4ef7\u503c\u51fd\u6570\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8bc4\u4f30\u5019\u9009\u64cd\u4f5c\uff0c\u57fa\u4e8e\u957f\u671f\u6548\u7528\u548c\u98ce\u9669\u8fdb\u884c\u51b3\u7b56", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u91cd\u6784\u6846\u67b6\uff0c\u9610\u660e\u4e86\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5185\u5b58\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840", "conclusion": "\u5185\u5b58\u7ba1\u7406\u5e94\u89c6\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0cDAM\u6846\u67b6\u63d0\u4f9b\u4e86\u51b3\u7b56\u7406\u8bba\u89c6\u89d2\uff0c\u652f\u6301\u57fa\u4e8e\u957f\u671f\u6548\u7528\u548c\u98ce\u9669\u7684\u5185\u5b58\u7ba1\u7406"}}
{"id": "2512.21573", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21573", "abs": "https://arxiv.org/abs/2512.21573", "authors": ["Zhangzheng Tu", "Kailun Su", "Shaolong Zhu", "Yukun Zheng"], "title": "World-Coordinate Human Motion Retargeting via SAM 3D Body", "comment": null, "summary": "Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics. To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate. Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization. Finally, we retarget the reconstructed motion to the Unitree G1 humanoid using a kinematics-aware two-stage inverse kinematics pipeline. Results on real monocular videos show that our method has stable world trajectories and reliable robot retargeting, indicating that structured human representations with lightweight physical constraints can yield robot-ready motion from monocular input.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u4ece\u5355\u76ee\u89c6\u9891\u6062\u590d\u4e16\u754c\u5750\u6807\u7cfb\u4eba\u4f53\u8fd0\u52a8\u5e76\u91cd\u5b9a\u5411\u5230\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u4f7f\u7528SAM 3D Body\u4f5c\u4e3a\u611f\u77e5\u9aa8\u5e72\u548cMomentum HumanRig\u4f5c\u4e3a\u673a\u5668\u4eba\u53cb\u597d\u4e2d\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u4f18\u5316\u548c\u7269\u7406\u7ea6\u675f\u5b9e\u73b0\u7a33\u5b9a\u8fd0\u52a8\u91cd\u5efa\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u6062\u590d\u4e16\u754c\u5750\u6807\u7cfb\u4eba\u4f53\u8fd0\u52a8\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u548c\u673a\u5668\u4eba\u5b66\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684SLAM\u6d41\u7a0b\u6216\u91cd\u578b\u65f6\u5e8f\u6a21\u578b\uff0c\u9700\u8981\u66f4\u8f7b\u91cf\u3001\u5de5\u7a0b\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684SAM 3D Body\u4f5c\u4e3a\u611f\u77e5\u9aa8\u5e72\uff0cMomentum HumanRig\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff1b\u9501\u5b9a\u8eab\u4efd\u548c\u9aa8\u67b6\u5c3a\u5ea6\u53c2\u6570\u4fdd\u8bc1\u65f6\u5e8f\u4e00\u81f4\u6027\uff1b\u5728\u4f4e\u7ef4MHR\u6f5c\u7a7a\u95f4\u8fdb\u884c\u6ed1\u52a8\u7a97\u53e3\u4f18\u5316\u5e73\u6ed1\u9884\u6d4b\uff1b\u4f7f\u7528\u53ef\u5fae\u5206\u8f6f\u8db3\u5730\u63a5\u89e6\u6a21\u578b\u548c\u63a5\u89e6\u611f\u77e5\u5168\u5c40\u4f18\u5316\u6062\u590d\u7269\u7406\u5408\u7406\u7684\u6839\u8f68\u8ff9\uff1b\u6700\u540e\u901a\u8fc7\u8fd0\u52a8\u5b66\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u9006\u8fd0\u52a8\u5b66\u7ba1\u9053\u91cd\u5b9a\u5411\u5230Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u3002", "result": "\u5728\u771f\u5b9e\u5355\u76ee\u89c6\u9891\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7a33\u5b9a\u7684\u4e16\u754c\u8f68\u8ff9\u548c\u53ef\u9760\u7684\u673a\u5668\u4eba\u91cd\u5b9a\u5411\u6548\u679c\uff0c\u8868\u660e\u7ed3\u6784\u5316\u4eba\u4f53\u8868\u793a\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7269\u7406\u7ea6\u675f\u53ef\u4ee5\u4ece\u5355\u76ee\u8f93\u5165\u4ea7\u751f\u673a\u5668\u4eba\u5c31\u7eea\u7684\u8fd0\u52a8\u3002", "conclusion": "\u7ed3\u6784\u5316\u4eba\u4f53\u8868\u793a\u914d\u5408\u8f7b\u91cf\u7ea7\u7269\u7406\u7ea6\u675f\u80fd\u591f\u6709\u6548\u4ece\u5355\u76ee\u89c6\u9891\u6062\u590d\u673a\u5668\u4eba\u53ef\u7528\u7684\u8fd0\u52a8\uff0c\u907f\u514d\u4e86\u590d\u6742SLAM\u6d41\u7a0b\u6216\u91cd\u578b\u65f6\u5e8f\u6a21\u578b\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u91cd\u5b9a\u5411\u63d0\u4f9b\u4e86\u5de5\u7a0b\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21577", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.21577", "abs": "https://arxiv.org/abs/2512.21577", "authors": ["Emmy Liu", "Varun Gangal", "Chelsea Zou", "Xiaoqi Huang", "Michael Yu", "Alex Chang", "Zhuofu Tao", "Sachin Kumar", "Steven Y. Feng"], "title": "A Unified Definition of Hallucination, Or: It's the World Model, Stupid", "comment": null, "summary": "Despite numerous attempts to solve the issue of hallucination since the inception of neural language models, it remains a problem in even frontier large language models today. Why is this the case? We walk through definitions of hallucination used in the literature from a historical perspective up to the current day, and fold them into a single definition of hallucination, wherein different prior definitions focus on different aspects of our definition. At its core, we argue that hallucination is simply inaccurate (internal) world modeling, in a form where it is observable to the user (e.g., stating a fact which contradicts a knowledge base, or producing a summary which contradicts a known source). By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), we arrive at the different existing definitions of hallucination present in the literature.\n  We argue that this unified view is useful because it forces evaluations to make clear their assumed \"world\" or source of truth, clarifies what should and should not be called hallucination (as opposed to planning or reward/incentive-related errors), and provides a common language to compare benchmarks and mitigation techniques. Building on this definition, we outline plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments, and sketch out how these benchmarks can use such settings to stress-test and improve the world modeling components of language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5e7b\u89c9\u5b9a\u4e49\uff1a\u5e7b\u89c9\u662f\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u4e16\u754c\u5efa\u6a21\u7684\u4e0d\u51c6\u786e\u6027\uff0c\u8868\u73b0\u4e3a\u4e0e\u53c2\u8003\u4e16\u754c\u6a21\u578b\uff08\u5982\u77e5\u8bc6\u5e93\u3001\u4e0a\u4e0b\u6587\u7b49\uff09\u7684\u51b2\u7a81\u3002\u901a\u8fc7\u8fd9\u4e00\u7edf\u4e00\u5b9a\u4e49\uff0c\u4f5c\u8005\u65e8\u5728\u6f84\u6e05\u5e7b\u89c9\u6982\u5ff5\u3001\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u89c4\u5212\u57fa\u4e8e\u5408\u6210\u4e16\u754c\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7f51\u7edc\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u5df2\u4e45\uff0c\u5e7b\u89c9\u95ee\u9898\u81f3\u4eca\u4ecd\u662f\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u6587\u732e\u5bf9\u5e7b\u89c9\u7684\u5b9a\u4e49\u591a\u6837\u4e14\u5206\u6563\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u5bfc\u81f4\u8bc4\u4f30\u6807\u51c6\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u6bd4\u8f83\u4e0d\u540c\u57fa\u51c6\u548c\u7f13\u89e3\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5386\u53f2\u68b3\u7406\uff0c\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u5e7b\u89c9\u5b9a\u4e49\uff0c\u4e3a\u7814\u7a76\u548c\u8bc4\u4f30\u63d0\u4f9b\u5171\u540c\u8bed\u8a00\u3002", "method": "\u4f5c\u8005\u4ece\u5386\u53f2\u89d2\u5ea6\u68b3\u7406\u4e86\u6587\u732e\u4e2d\u5404\u79cd\u5e7b\u89c9\u5b9a\u4e49\uff0c\u5c06\u5176\u6574\u5408\u4e3a\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff1a\u5e7b\u89c9\u662f\u5185\u90e8\u4e16\u754c\u5efa\u6a21\u7684\u4e0d\u51c6\u786e\u6027\uff0c\u8868\u73b0\u4e3a\u4e0e\u53c2\u8003\u4e16\u754c\u6a21\u578b\u7684\u51b2\u7a81\u3002\u901a\u8fc7\u53d8\u5316\u53c2\u8003\u4e16\u754c\u6a21\u578b\uff08\u5982\u77e5\u8bc6\u5e93\u3001\u4e0a\u4e0b\u6587\u7b49\uff09\u548c\u77e5\u8bc6\u51b2\u7a81\u7b56\u7565\uff0c\u53ef\u4ee5\u6db5\u76d6\u73b0\u6709\u5404\u79cd\u5b9a\u4e49\u3002\u57fa\u4e8e\u6b64\u5b9a\u4e49\uff0c\u4f5c\u8005\u89c4\u5212\u4e86\u57fa\u4e8e\u5408\u6210\u4e16\u754c\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5bb6\u65cf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5e7b\u89c9\u5b9a\u4e49\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u6838\u5fc3\u754c\u5b9a\u4e3a\u4e0d\u51c6\u786e\u7684\u4e16\u754c\u5efa\u6a21\u3002\u8be5\u6846\u67b6\u8981\u6c42\u8bc4\u4f30\u660e\u786e\u5176\u5047\u8bbe\u7684\"\u4e16\u754c\"\u6216\u771f\u76f8\u6765\u6e90\uff0c\u6f84\u6e05\u4e86\u5e7b\u89c9\u4e0e\u89c4\u5212\u9519\u8bef\u3001\u5956\u52b1/\u6fc0\u52b1\u76f8\u5173\u9519\u8bef\u7684\u533a\u522b\uff0c\u4e3a\u6bd4\u8f83\u57fa\u51c6\u548c\u7f13\u89e3\u6280\u672f\u63d0\u4f9b\u4e86\u5171\u540c\u8bed\u8a00\u3002\u5e76\u57fa\u4e8e\u6b64\u89c4\u5212\u4e86\u5408\u6210\u4e16\u754c\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u7edf\u4e00\u7684\u5e7b\u89c9\u5b9a\u4e49\u6846\u67b6\u6709\u52a9\u4e8e\u6f84\u6e05\u6982\u5ff5\u6df7\u6dc6\uff0c\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u4e0d\u540c\u7814\u7a76\u4e4b\u95f4\u7684\u6bd4\u8f83\u3002\u901a\u8fc7\u57fa\u4e8e\u5408\u6210\u4e16\u754c\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u538b\u529b\u6d4b\u8bd5\u548c\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002"}}
{"id": "2512.21627", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21627", "abs": "https://arxiv.org/abs/2512.21627", "authors": ["Botao Ren", "Junjun Hu", "Xinda Xue", "Minghua Luo", "Jintao Chen", "Haochen Bai", "Liangliang You", "Mu Xu"], "title": "AstraNav-Memory: Contexts Compression for Long Memory", "comment": null, "summary": "Lifelong embodied navigation requires agents to accumulate, retain, and exploit spatial-semantic experience across tasks, enabling efficient exploration in novel environments and rapid goal reaching in familiar ones. While object-centric memory is interpretable, it depends on detection and reconstruction pipelines that limit robustness and scalability. We propose an image-centric memory framework that achieves long-term implicit memory via an efficient visual context compression module end-to-end coupled with a Qwen2.5-VL-based navigation policy. Built atop a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, our visual tokenizer supports configurable compression rates; for example, under a representative 16$\\times$ compression setting, each image is encoded with about 30 tokens, expanding the effective context capacity from tens to hundreds of images. Experimental results on GOAT-Bench and HM3D-OVON show that our method achieves state-of-the-art navigation performance, improving exploration in unfamiliar environments and shortening paths in familiar ones. Ablation studies further reveal that moderate compression provides the best balance between efficiency and accuracy. These findings position compressed image-centric memory as a practical and scalable interface for lifelong embodied agents, enabling them to reason over long visual histories and navigate with human-like efficiency.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u4e2d\u5fc3\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u4e0a\u4e0b\u6587\u538b\u7f29\u6a21\u5757\u5b9e\u73b0\u957f\u671f\u9690\u5f0f\u8bb0\u5fc6\uff0c\u7ed3\u5408Qwen2.5-VL\u5bfc\u822a\u7b56\u7565\uff0c\u5728GOAT-Bench\u548cHM3D-OVON\u4e0a\u8fbe\u5230SOTA\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u7ec8\u8eab\u5177\u8eab\u5bfc\u822a\u9700\u8981\u667a\u80fd\u4f53\u8de8\u4efb\u52a1\u79ef\u7d2f\u3001\u4fdd\u7559\u548c\u5229\u7528\u7a7a\u95f4\u8bed\u4e49\u7ecf\u9a8c\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u7269\u4f53\u4e2d\u5fc3\u7684\u8bb0\u5fc6\u65b9\u6cd5\u4f9d\u8d56\u68c0\u6d4b\u548c\u91cd\u5efa\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u56fe\u50cf\u4e2d\u5fc3\u8bb0\u5fc6\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u6548\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u538b\u7f29\u6a21\u5757\uff0c\u57fa\u4e8eViT\u9aa8\u5e72\u7f51\u7edc\uff08\u51bb\u7ed3DINOv3\u7279\u5f81\uff09\u548c\u8f7b\u91cf\u7ea7PixelUnshuffle+Conv\u5757\uff0c\u652f\u6301\u53ef\u914d\u7f6e\u538b\u7f29\u7387\uff08\u598216\u500d\u538b\u7f29\u4e0b\u6bcf\u5f20\u56fe\u50cf\u7ea630\u4e2atoken\uff09\u3002", "result": "\u5728GOAT-Bench\u548cHM3D-OVON\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u63d0\u5347\u964c\u751f\u73af\u5883\u63a2\u7d22\u80fd\u529b\u5e76\u7f29\u77ed\u719f\u6089\u73af\u5883\u8def\u5f84\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u9002\u5ea6\u538b\u7f29\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u538b\u7f29\u7684\u56fe\u50cf\u4e2d\u5fc3\u8bb0\u5fc6\u4e3a\u7ec8\u8eab\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u63a5\u53e3\uff0c\u4f7f\u5176\u80fd\u591f\u57fa\u4e8e\u957f\u89c6\u89c9\u5386\u53f2\u8fdb\u884c\u63a8\u7406\uff0c\u5b9e\u73b0\u7c7b\u4eba\u6548\u7387\u7684\u5bfc\u822a\u3002"}}
{"id": "2512.21580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21580", "abs": "https://arxiv.org/abs/2512.21580", "authors": ["Alexander Podolskiy", "Semen Molokov", "Timofey Gerasin", "Maksim Titov", "Alexey Rukhovich", "Artem Khrapov", "Kirill Morozov", "Evgeny Tetin", "Constantine Korikov", "Pavel Efimov", "Polina Lazukova", "Yuliya Skripkar", "Nikita Okhotnikov", "Irina Piontkovskaya", "Meng Xiaojun", "Zou Xueyi", "Zhang Zhenhe"], "title": "Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM", "comment": null, "summary": "We present Gamayun, a 1.5B-parameter multilingual language model trained entirely from scratch on 2.5T tokens. Designed for efficiency and deployment in resource-constrained environments, Gamayun addresses the lack of research on small non-English-centric LLMs by adopting a novel two-stage pre-training strategy: balanced multilingual training for cross-lingual alignment, followed by high-quality English enrichment to transfer performance gains across languages. Our model supports 12 languages, with special focus on Russian. Despite a significantly smaller training budget than comparable models, Gamayun outperforms LLaMA3.2-1B (9T tokens) on all considered benchmarks, and surpasses Qwen2.5-1.5B (18T tokens) on a wide range of English and multilingual tasks. It matches or exceeds Qwen3 (36T tokens) on most tasks outside advanced STEM, achieving state-of-the-art results in Russian, including the MERA benchmark, among the models of comparable size (1-2B parameters).", "AI": {"tldr": "Gamayun\u662f\u4e00\u4e2a15\u4ebf\u53c2\u6570\u7684\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u66f4\u5927\u8bad\u7ec3\u9884\u7b97\u7684\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u975e\u82f1\u8bed\u4e2d\u5fc3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u9ad8\u6548\u90e8\u7f72\u65b9\u6848\uff0c\u7279\u522b\u5173\u6ce8\u4fc4\u8bed\u7b49\u975e\u82f1\u8bed\u8bed\u8a00\u652f\u6301\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u5e73\u8861\u591a\u8bed\u8a00\u8bad\u7ec3\u5b9e\u73b0\u8de8\u8bed\u8a00\u5bf9\u9f50\uff1b2\uff09\u9ad8\u8d28\u91cf\u82f1\u8bed\u4e30\u5bcc\u5316\u4ee5\u5c06\u6027\u80fd\u589e\u76ca\u8f6c\u79fb\u5230\u5176\u4ed6\u8bed\u8a00\u3002\u6a21\u578b\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\uff0c\u4f7f\u752825\u4e07\u4ebftoken\u6570\u636e\u3002", "result": "\u572815\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u8d85\u8d8a\u4e86LLaMA3.2-1B\uff089T tokens\uff09\u7684\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u5e7f\u6cdb\u82f1\u8bed\u548c\u591a\u8bed\u8a00\u4efb\u52a1\u4e0a\u8d85\u8d8aQwen2.5-1.5B\uff0818T tokens\uff09\uff0c\u5728\u975e\u9ad8\u7ea7STEM\u4efb\u52a1\u4e0a\u4e0eQwen3\uff0836T tokens\uff09\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5728\u4fc4\u8bed\u4efb\u52a1\u4e0a\u8fbe\u5230\u540c\u7c7b\u89c4\u6a21\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Gamayun\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5c0f\u578b\u591a\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u591a\u8bed\u8a00AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21654", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21654", "abs": "https://arxiv.org/abs/2512.21654", "authors": ["Zikun Guo", "Adeyinka P. Adedigba", "Rammohan Mallipeddi", "Heoncheol Lee"], "title": "Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning", "comment": "20pages, 6Figues", "summary": "Multi-robot path planning is a fundamental yet challenging problem due to its combinatorial complexity and the need to balance global efficiency with fair task allocation among robots. Traditional swarm intelligence methods, although effective on small instances, often converge prematurely and struggle to scale to complex environments. In this work, we present a structure-induced exploration framework that integrates structural priors into the search process of the ant colony optimization (ACO). The approach leverages the spatial distribution of the task to induce a structural prior at initialization, thereby constraining the search space. The pheromone update rule is then designed to emphasize structurally meaningful connections and incorporates a load-aware objective to reconcile the total travel distance with individual robot workload. An explicit overlap suppression strategy further ensures that tasks remain distinct and balanced across the team. The proposed framework was validated on diverse benchmark scenarios covering a wide range of instance sizes and robot team configurations. The results demonstrate consistent improvements in route compactness, stability, and workload distribution compared to representative metaheuristic baselines. Beyond performance gains, the method also provides a scalable and interpretable framework that can be readily applied to logistics, surveillance, and search-and-rescue applications where reliable large-scale coordination is essential.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7ed3\u6784\u5148\u9a8c\u7684\u8681\u7fa4\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff0c\u901a\u8fc7\u7a7a\u95f4\u5206\u5e03\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\uff0c\u5e73\u8861\u5168\u5c40\u6548\u7387\u548c\u4e2a\u4f53\u5de5\u4f5c\u91cf\u5206\u914d\u3002", "motivation": "\u4f20\u7edf\u7fa4\u4f53\u667a\u80fd\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u6709\u6548\uff0c\u4f46\u5bb9\u6613\u8fc7\u65e9\u6536\u655b\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u590d\u6742\u73af\u5883\u3002\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u5177\u6709\u7ec4\u5408\u590d\u6742\u6027\uff0c\u9700\u8981\u5728\u5168\u5c40\u6548\u7387\u548c\u516c\u5e73\u4efb\u52a1\u5206\u914d\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u8bf1\u5bfc\u63a2\u7d22\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5148\u9a8c\u96c6\u6210\u5230\u8681\u7fa4\u4f18\u5316\u4e2d\uff1a1) \u5229\u7528\u4efb\u52a1\u7a7a\u95f4\u5206\u5e03\u521d\u59cb\u5316\u7ed3\u6784\u5148\u9a8c\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\uff1b2) \u8bbe\u8ba1\u4fe1\u606f\u7d20\u66f4\u65b0\u89c4\u5219\u5f3a\u8c03\u7ed3\u6784\u4e0a\u6709\u610f\u4e49\u7684\u8fde\u63a5\uff1b3) \u5f15\u5165\u8d1f\u8f7d\u611f\u77e5\u76ee\u6807\u5e73\u8861\u603b\u884c\u7a0b\u548c\u4e2a\u4f53\u5de5\u4f5c\u91cf\uff1b4) \u91c7\u7528\u663e\u5f0f\u91cd\u53e0\u6291\u5236\u7b56\u7565\u786e\u4fdd\u4efb\u52a1\u533a\u5206\u548c\u5e73\u8861\u5206\u914d\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u573a\u666f\uff08\u4e0d\u540c\u5b9e\u4f8b\u89c4\u6a21\u548c\u673a\u5668\u4eba\u56e2\u961f\u914d\u7f6e\uff09\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4ee3\u8868\u6027\u5143\u542f\u53d1\u5f0f\u57fa\u7ebf\uff0c\u5728\u8def\u5f84\u7d27\u51d1\u6027\u3001\u7a33\u5b9a\u6027\u548c\u5de5\u4f5c\u91cf\u5206\u5e03\u65b9\u9762\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u6027\u80fd\u63d0\u5347\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u53ef\u5e94\u7528\u4e8e\u7269\u6d41\u3001\u76d1\u89c6\u548c\u641c\u6551\u7b49\u9700\u8981\u53ef\u9760\u5927\u89c4\u6a21\u534f\u8c03\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.21625", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21625", "abs": "https://arxiv.org/abs/2512.21625", "authors": ["Xinyu Tang", "Yuliang Zhan", "Zhixun Li", "Wayne Xin Zhao", "Zhenduo Zhang", "Zujie Wen", "Zhiqiang Zhang", "Jun Zhou"], "title": "Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faA3PO\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u975e\u5bf9\u79f0\u7684token\u7ea7\u4f18\u52bf\u4fe1\u53f7\u5206\u914d\u6765\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3", "motivation": "\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8bad\u7ec3\u4e2d\u6b63\u8d1f\u6837\u672c\u6781\u6027\u5bf9\u8bad\u7ec3\u52a8\u6001\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u52bf\u4fe1\u53f7\u5206\u914d\u4e0a\u4e0d\u591f\u7cbe\u786e", "method": "\u63d0\u51faA3PO\u65b9\u6cd5\uff1a\u81ea\u9002\u5e94\u975e\u5bf9\u79f0\u7684token\u7ea7\u4f18\u52bf\u4fe1\u53f7\u5206\u914d\u7b56\u7565\uff0c\u66f4\u7cbe\u786e\u5730\u4e3a\u4e0d\u540c\u6781\u6027\u7684\u5173\u952etoken\u5206\u914d\u4f18\u52bf\u4fe1\u53f7", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86A3PO\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u6b63\u8d1f\u6837\u672c\u5728RLVR\u8bad\u7ec3\u4e2d\u53d1\u6325\u4e0d\u540c\u4f5c\u7528\uff0c\u901a\u8fc7\u7cbe\u786e\u7684token\u7ea7\u4f18\u52bf\u4fe1\u53f7\u5206\u914d\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c"}}
{"id": "2512.21722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21722", "abs": "https://arxiv.org/abs/2512.21722", "authors": ["Zishuo Wang", "Xinyu Zhang", "Zhuonan Liu", "Tomohito Kawabata", "Daeun Song", "Xuesu Xiao", "Ling Xiao"], "title": "MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning", "comment": null, "summary": "Socially compliant navigation requires robots to move safely and appropriately in human-centered environments by respecting social norms. However, social norms are often ambiguous, and in a single scenario, multiple actions may be equally acceptable. Most existing methods simplify this problem by assuming a single correct action, which limits their ability to handle real-world social uncertainty. In this work, we propose MAction-SocialNav, an efficient vision language model for socially compliant navigation that explicitly addresses action ambiguity, enabling generating multiple plausible actions within one scenario. To enhance the model's reasoning capability, we introduce a novel meta-cognitive prompt (MCP) method. Furthermore, to evaluate the proposed method, we curate a multi-action socially compliant navigation dataset that accounts for diverse conditions, including crowd density, indoor and outdoor environments, and dual human annotations. The dataset contains 789 samples, each with three-turn conversation, split into 710 training samples and 79 test samples through random selection. We also design five evaluation metrics to assess high-level decision precision, safety, and diversity. Extensive experiments demonstrate that the proposed MAction-SocialNav achieves strong social reasoning performance while maintaining high efficiency, highlighting its potential for real-world human robot navigation. Compared with zero-shot GPT-4o and Claude, our model achieves substantially higher decision quality (APG: 0.595 vs. 0.000/0.025) and safety alignment (ER: 0.264 vs. 0.642/0.668), while maintaining real-time efficiency (1.524 FPS, over 3x faster).", "AI": {"tldr": "MAction-SocialNav\u662f\u4e00\u4e2a\u7528\u4e8e\u793e\u4ea4\u5408\u89c4\u5bfc\u822a\u7684\u9ad8\u6548\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u63d0\u793a\u65b9\u6cd5\u5904\u7406\u52a8\u4f5c\u6a21\u7cca\u6027\uff0c\u80fd\u591f\u751f\u6210\u591a\u79cd\u5408\u7406\u52a8\u4f5c\uff0c\u5728\u51b3\u7b56\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u6548\u7387\u65b9\u9762\u4f18\u4e8eGPT-4o\u548cClaude\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5355\u4e00\u6b63\u786e\u52a8\u4f5c\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u793e\u4ea4\u89c4\u8303\u56fa\u6709\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002", "method": "\u63d0\u51faMAction-SocialNav\u6a21\u578b\uff0c\u91c7\u7528\u5143\u8ba4\u77e5\u63d0\u793a\u65b9\u6cd5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u5305\u542b789\u4e2a\u6837\u672c\u7684\u591a\u52a8\u4f5c\u793e\u4ea4\u5bfc\u822a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u4eba\u7fa4\u5bc6\u5ea6\u548c\u5ba4\u5185\u5916\u73af\u5883\u3002", "result": "\u6a21\u578b\u5728\u51b3\u7b56\u8d28\u91cf(APG: 0.595 vs. 0.000/0.025)\u548c\u5b89\u5168\u6027(ER: 0.264 vs. 0.642/0.668)\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\u548cClaude\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6548\u7387(1.524 FPS\uff0c\u5feb3\u500d\u4ee5\u4e0a)\u3002", "conclusion": "MAction-SocialNav\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u52a8\u4f5c\u6a21\u7cca\u6027\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u793e\u4ea4\u63a8\u7406\u80fd\u529b\u548c\u9ad8\u6548\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u4eba\u673a\u5bfc\u822a\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.21635", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21635", "abs": "https://arxiv.org/abs/2512.21635", "authors": ["Chengxu Yang", "Jingling Yuan", "Siqi Cai", "Jiawei Jiang", "Chuang Hu"], "title": "Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations", "comment": "Published as a conference paper at KDD 2026", "summary": "Hallucinations in large language models (LLMs) are commonly regarded as errors to be minimized. However, recent perspectives suggest that some hallucinations may encode creative or epistemically valuable content, a dimension that remains underquantified in current literature. Existing hallucination detection methods primarily focus on factual consistency, struggling to handle heterogeneous scientific tasks and balance creativity with accuracy. To address these challenges, we propose HIC-Bench, a novel evaluation framework that categorizes hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH), enabling systematic investigation of their interplay in LLM creativity. HIC-Bench features three core characteristics: (1) Structured IH/DH Assessment. using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation); (2) Cross-Domain Applicability. spanning ten scientific domains with open-ended innovation tasks; and (3) Dynamic Prompt Optimization. leveraging the Dynamic Hallucination Prompt (DHP) to guide models toward creative and reliable outputs. The evaluation process employs multiple LLM judges, averaging scores to mitigate bias, with human annotators verifying IH/DH classifications. Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized. These insights position IH as a catalyst for creativity and reveal the ability of LLM hallucinations to drive scientific innovation.Additionally, the HIC-Bench offers a valuable platform for advancing research into the creative intelligence of LLM hallucinations.", "AI": {"tldr": "HIC-Bench\uff1a\u5c06LLM\u5e7b\u89c9\u5206\u4e3a\u667a\u80fd\u5e7b\u89c9(IH)\u4e0e\u7f3a\u9677\u5e7b\u89c9(DH)\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u7814\u7a76LLM\u521b\u9020\u529b\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u7cfb", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5c06\u5e7b\u89c9\u89c6\u4e3a\u9700\u8981\u6700\u5c0f\u5316\u7684\u9519\u8bef\uff0c\u4f46\u5ffd\u89c6\u4e86\u67d0\u4e9b\u5e7b\u89c9\u53ef\u80fd\u5305\u542b\u521b\u9020\u6027\u6216\u6709\u8ba4\u77e5\u4ef7\u503c\u7684\u5185\u5bb9\u3002\u5f53\u524d\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u5904\u7406\u5f02\u8d28\u79d1\u5b66\u4efb\u52a1\uff0c\u4e5f\u65e0\u6cd5\u5e73\u8861\u521b\u9020\u529b\u4e0e\u51c6\u786e\u6027", "method": "\u63d0\u51faHIC-Bench\u6846\u67b6\uff1a1) \u7ed3\u6784\u5316IH/DH\u8bc4\u4f30\uff0c\u6574\u5408\u6258\u5170\u65af\u521b\u9020\u6027\u601d\u7ef4\u6d4b\u8bd5\u6307\u6807\u4e0e\u5e7b\u89c9\u7279\u5b9a\u7ef4\u5ea6\uff1b2) \u8de8\u9886\u57df\u9002\u7528\u6027\uff0c\u6db5\u76d610\u4e2a\u79d1\u5b66\u9886\u57df\uff1b3) \u52a8\u6001\u63d0\u793a\u4f18\u5316\uff0c\u4f7f\u7528\u52a8\u6001\u5e7b\u89c9\u63d0\u793a\u5f15\u5bfc\u6a21\u578b\uff1b\u91c7\u7528\u591aLLM\u8bc4\u59d4\u5e73\u5747\u8bc4\u5206\u51cf\u5c11\u504f\u5dee\uff0c\u4eba\u5de5\u6807\u6ce8\u9a8c\u8bc1\u5206\u7c7b", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86IH\u4e0eDH\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u8868\u660e\u521b\u9020\u529b\u4e0e\u6b63\u786e\u6027\u53ef\u4ee5\u5171\u540c\u4f18\u5316\u3002\u667a\u80fd\u5e7b\u89c9\u53ef\u4f5c\u4e3a\u521b\u9020\u529b\u7684\u50ac\u5316\u5242\uff0cLLM\u5e7b\u89c9\u5177\u6709\u63a8\u52a8\u79d1\u5b66\u521b\u65b0\u7684\u6f5c\u529b", "conclusion": "HIC-Bench\u4e3a\u7814\u7a76LLM\u5e7b\u89c9\u7684\u521b\u9020\u6027\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5e73\u53f0\uff0c\u91cd\u65b0\u5b9a\u4f4d\u4e86\u667a\u80fd\u5e7b\u89c9\u4f5c\u4e3a\u521b\u9020\u529b\u50ac\u5316\u5242\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u4e86LLM\u5e7b\u89c9\u9a71\u52a8\u79d1\u5b66\u521b\u65b0\u7684\u80fd\u529b"}}
{"id": "2512.21723", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21723", "abs": "https://arxiv.org/abs/2512.21723", "authors": ["Alexandr V. Korchemnyi", "Anatoly O. Onishchenko", "Eva A. Bakaeva", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "HELP: Hierarchical Embodied Language Planner for Household Tasks", "comment": null, "summary": "Embodied agents tasked with complex scenarios, whether in real or simulated environments, rely heavily on robust planning capabilities. When instructions are formulated in natural language, large language models (LLMs) equipped with extensive linguistic knowledge can play this role. However, to effectively exploit the ability of such models to handle linguistic ambiguity, to retrieve information from the environment, and to be based on the available skills of an agent, an appropriate architecture must be designed. We propose a Hierarchical Embodied Language Planner, called HELP, consisting of a set of LLM-based agents, each dedicated to solving a different subtask. We evaluate the proposed approach on a household task and perform real-world experiments with an embodied agent. We also focus on the use of open source LLMs with a relatively small number of parameters, to enable autonomous deployment.", "AI": {"tldr": "HELP\uff1a\u57fa\u4e8e\u5f00\u6e90LLM\u7684\u5206\u5c42\u5177\u8eab\u8bed\u8a00\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u89e3\u51b3\u5bb6\u5ead\u4efb\u52a1\u89c4\u5212\u95ee\u9898", "motivation": "\u5177\u8eab\u667a\u80fd\u4f53\u5728\u590d\u6742\u573a\u666f\u4e2d\u9700\u8981\u5f3a\u5927\u7684\u89c4\u5212\u80fd\u529b\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u8a00\u77e5\u8bc6\u53ef\u4ee5\u62c5\u4efb\u8fd9\u4e00\u89d2\u8272\u3002\u4f46\u9700\u8981\u8bbe\u8ba1\u5408\u9002\u7684\u67b6\u6784\u6765\u5145\u5206\u5229\u7528LLMs\u5904\u7406\u8bed\u8a00\u6b67\u4e49\u3001\u4ece\u73af\u5883\u4e2d\u68c0\u7d22\u4fe1\u606f\u3001\u57fa\u4e8e\u667a\u80fd\u4f53\u53ef\u7528\u6280\u80fd\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u5177\u8eab\u8bed\u8a00\u89c4\u5212\u5668HELP\uff0c\u7531\u4e00\u7ec4\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7ec4\u6210\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e13\u95e8\u89e3\u51b3\u4e0d\u540c\u7684\u5b50\u4efb\u52a1\u3002\u4f7f\u7528\u53c2\u6570\u76f8\u5bf9\u8f83\u5c11\u7684\u5f00\u6e90LLM\uff0c\u4ee5\u5b9e\u73b0\u81ea\u4e3b\u90e8\u7f72\u3002", "result": "\u5728\u5bb6\u5ead\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u3002", "conclusion": "HELP\u67b6\u6784\u80fd\u591f\u6709\u6548\u5229\u7528LLMs\u7684\u89c4\u5212\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u5c42\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u89e3\u51b3\u5177\u8eab\u4efb\u52a1\uff0c\u4e14\u4f7f\u7528\u5f00\u6e90\u5c0f\u6a21\u578b\u53ef\u5b9e\u73b0\u81ea\u4e3b\u90e8\u7f72\u3002"}}
{"id": "2512.21706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21706", "abs": "https://arxiv.org/abs/2512.21706", "authors": ["Shuchang Pan", "Siddharth Banerjee", "Dhruv Hebbar", "Siddhant Patel", "Akshaj Gupta", "Kan Jen Cheng", "Hanjo Kim", "Zeyi Austin Li", "Martin Q. Ma", "Tingle Li", "Gopala Anumanchipalli", "Jiachen Lian"], "title": "Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech", "comment": null, "summary": "Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this causal pathway is key to building natural full-duplex interactive systems. We introduce a framework that enables reasoning over conversational behaviors by modeling this process as causal inference within a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a hybrid corpus that pairs controllable, event-rich simulations with human-annotated rationales and real conversational speech. The GoT framework structures streaming predictions as an evolving graph, enabling a multimodal transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u56fe\u601d\u7ef4\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u5bf9\u8bdd\u4e2d\u7684\u610f\u56fe-\u884c\u4e3a\u8def\u5f84\uff0c\u5b9e\u73b0\u5168\u53cc\u5de5\u4ea4\u4e92\u7cfb\u7edf\u7684\u81ea\u7136\u5bf9\u8bdd\u63a8\u7406", "motivation": "\u4eba\u7c7b\u5bf9\u8bdd\u7531\u9690\u542b\u7684\u601d\u7ef4\u94fe\u7ec4\u7ec7\uff0c\u8868\u73b0\u4e3a\u5b9a\u65f6\u7684\u8a00\u8bed\u884c\u4e3a\u3002\u6355\u6349\u8fd9\u79cd\u56e0\u679c\u8def\u5f84\u662f\u6784\u5efa\u81ea\u7136\u5168\u53cc\u5de5\u4ea4\u4e92\u7cfb\u7edf\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8fd9\u79cd\u56e0\u679c\u5173\u7cfb\u7684\u5efa\u6a21\u80fd\u529b", "method": "\u5f15\u5165\u56fe\u601d\u7ef4\u6846\u67b6\uff0c\u5c06\u5bf9\u8bdd\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u56e0\u679c\u63a8\u7406\u3002\u91c7\u7528\u5206\u5c42\u6807\u6ce8\u65b9\u6848\u9884\u6d4b\u9ad8\u5c42\u4ea4\u9645\u610f\u56fe\u548c\u4f4e\u5c42\u8a00\u8bed\u884c\u4e3a\uff0c\u5b66\u4e60\u5176\u56e0\u679c\u548c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u4f7f\u7528\u6df7\u5408\u8bed\u6599\u5e93\uff08\u53ef\u63a7\u6a21\u62df\u5bf9\u8bdd+\u4eba\u5de5\u6807\u6ce8\u63a8\u7406+\u771f\u5b9e\u5bf9\u8bdd\uff09\u8bad\u7ec3\u7cfb\u7edf\uff0c\u5c06\u6d41\u5f0f\u9884\u6d4b\u6784\u5efa\u4e3a\u6f14\u5316\u56fe\uff0c\u4f7f\u591a\u6a21\u6001Transformer\u80fd\u591f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8a00\u8bed\u884c\u4e3a\u3001\u751f\u6210\u51b3\u7b56\u7406\u7531\u5e76\u52a8\u6001\u4f18\u5316\u63a8\u7406", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5168\u53cc\u5de5\u5bf9\u8bdd\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u884c\u4e3a\u68c0\u6d4b\uff0c\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0c\u5e76\u4e3a\u5168\u53cc\u5de5\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5bf9\u8bdd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840", "conclusion": "\u56fe\u601d\u7ef4\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u5bf9\u8bdd\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u8def\u5f84\uff0c\u4e3a\u6784\u5efa\u81ea\u7136\u5168\u53cc\u5de5\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u5b9e\u73b0\uff0c\u5728\u884c\u4e3a\u68c0\u6d4b\u548c\u63a8\u7406\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272"}}
{"id": "2512.21853", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21853", "abs": "https://arxiv.org/abs/2512.21853", "authors": ["Kentaro Uno", "Elian Neppel", "Gustavo H. Diaz", "Ashutosh Mishra", "Shamistan Karimov", "A. Sejal Jain", "Ayesha Habib", "Pascal Pama", "Hazal Gozbasi", "Shreya Santra", "Kazuya Yoshida"], "title": "MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction", "comment": "This is the authors' version of a paper accepted for publication in IEEE Transactions on Field Robotics, (c) IEEE. The final published version is available at https://doi.org/10.1109/TFR.2025.3624346", "summary": "The allure of lunar surface exploration and development has recently captured widespread global attention. Robots have proved to be indispensable for exploring uncharted terrains, uncovering and leveraging local resources, and facilitating the construction of future human habitats. In this article, we introduce the modular and on-demand reconfigurable robot (MoonBot), a modular and reconfigurable robotic system engineered to maximize functionality while operating within the stringent mass constraints of lunar payloads and adapting to varying environmental conditions and task requirements. This article details the design and development of MoonBot and presents a preliminary field demonstration that validates the proof of concept through the execution of milestone tasks simulating the establishment of lunar infrastructure. These tasks include essential civil engineering operations, infrastructural component transportation and deployment, and assistive operations with inflatable modules. Furthermore, we systematically summarize the lessons learned during testing, focusing on the connector design and providing valuable insights for the advancement of modular robotic systems in future lunar missions.", "AI": {"tldr": "MoonBot\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u53ef\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4e13\u4e3a\u6708\u7403\u63a2\u7d22\u8bbe\u8ba1\uff0c\u80fd\u5728\u4e25\u683c\u8d28\u91cf\u9650\u5236\u4e0b\u9002\u5e94\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u548c\u4efb\u52a1\u9700\u6c42\uff0c\u901a\u8fc7\u73b0\u573a\u6f14\u793a\u9a8c\u8bc1\u4e86\u5176\u6267\u884c\u6708\u7403\u57fa\u7840\u8bbe\u65bd\u5efa\u7acb\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u6708\u7403\u8868\u9762\u63a2\u7d22\u548c\u5f00\u53d1\u65e5\u76ca\u53d7\u5230\u5168\u7403\u5173\u6ce8\uff0c\u673a\u5668\u4eba\u5bf9\u4e8e\u63a2\u7d22\u672a\u77e5\u5730\u5f62\u3001\u5229\u7528\u672c\u5730\u8d44\u6e90\u548c\u5efa\u8bbe\u672a\u6765\u4eba\u7c7b\u6816\u606f\u5730\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5f00\u53d1\u80fd\u5728\u4e25\u683c\u8d28\u91cf\u9650\u5236\u4e0b\u9002\u5e94\u591a\u53d8\u73af\u5883\u6761\u4ef6\u548c\u4efb\u52a1\u9700\u6c42\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u6309\u9700\u53ef\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\uff08MoonBot\uff09\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u80fd\u591f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u8fdb\u884c\u91cd\u6784\u3002\u8fdb\u884c\u4e86\u521d\u6b65\u73b0\u573a\u6f14\u793a\uff0c\u6a21\u62df\u5efa\u7acb\u6708\u7403\u57fa\u7840\u8bbe\u65bd\u7684\u91cc\u7a0b\u7891\u4efb\u52a1\uff0c\u5305\u62ec\u571f\u6728\u5de5\u7a0b\u64cd\u4f5c\u3001\u57fa\u7840\u8bbe\u65bd\u7ec4\u4ef6\u8fd0\u8f93\u90e8\u7f72\u4ee5\u53ca\u4e0e\u5145\u6c14\u6a21\u5757\u7684\u8f85\u52a9\u64cd\u4f5c\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u6982\u5ff5\u8bc1\u660e\uff0c\u5c55\u793a\u4e86MoonBot\u6267\u884c\u6708\u7403\u57fa\u7840\u8bbe\u65bd\u5efa\u7acb\u4efb\u52a1\u7684\u80fd\u529b\u3002\u7cfb\u7edf\u603b\u7ed3\u4e86\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u7279\u522b\u5173\u6ce8\u8fde\u63a5\u5668\u8bbe\u8ba1\uff0c\u4e3a\u672a\u6765\u6708\u7403\u4efb\u52a1\u4e2d\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002", "conclusion": "MoonBot\u4f5c\u4e3a\u6a21\u5757\u5316\u53ef\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4e3a\u6708\u7403\u63a2\u7d22\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u7279\u522b\u662f\u8fde\u63a5\u5668\u8bbe\u8ba1\u7684\u6539\u8fdb\uff0c\u5c06\u63a8\u52a8\u672a\u6765\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6708\u7403\u4efb\u52a1\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.21708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21708", "abs": "https://arxiv.org/abs/2512.21708", "authors": ["Jing Han", "Binwei Yan", "Tianyu Guo", "Zheyuan Bai", "Mengyu Zheng", "Hanting Chen", "Ying Nie"], "title": "MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles", "comment": "Accepted by ICML 2025", "summary": "Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant Reason+Action paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification. We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io.", "AI": {"tldr": "\u63d0\u51faMixture-of-Roles (MoR)\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u95e8\u5316\u7684LoRA\u7ec4\uff08\u63a8\u7406\u8005\u3001\u6267\u884c\u8005\u3001\u603b\u7ed3\u8005\uff09\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u7528\u4e8e\u63d0\u5347LLM\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u5fae\u8c03\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5728\u8be5\u9886\u57df\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5bf9\u6574\u4e2a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7f3a\u4e4f\u9488\u5bf9\u667a\u80fd\u4f53\u4efb\u52a1\u7279\u5b9a\u89d2\u8272\u7684\u4e13\u95e8\u5316\u8bbe\u8ba1\u3002", "method": "1) \u5c06\u667a\u80fd\u4f53\u80fd\u529b\u5206\u89e3\u4e3a\u4e09\u4e2a\u89d2\u8272\uff1a\u63a8\u7406\u8005\uff08\u7406\u89e3\u67e5\u8be2\u3001\u51b3\u5b9a\u4e0b\u4e00\u6b65\u89d2\u8272\uff09\u3001\u6267\u884c\u8005\uff08\u8bc6\u522b\u8c03\u7528\u51fd\u6570\u548c\u53c2\u6570\uff09\u3001\u603b\u7ed3\u8005\uff08\u63d0\u70bc\u5bf9\u8bdd\u4fe1\u606f\uff09\uff1b2) \u63d0\u51faMoR\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u95e8\u7684LoRA\u7ec4\uff0c\u6bcf\u4e2a\u7ec4\u5bf9\u5e94\u4e00\u4e2a\u89d2\u8272\uff1b3) \u5f00\u53d1\u57fa\u4e8e\u516c\u5f00\u6570\u636e\u96c6\u7684\u591a\u89d2\u8272\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5305\u542b\u89d2\u8272\u7279\u5b9a\u5185\u5bb9\u8865\u5168\u548c\u53ef\u9760\u6027\u9a8c\u8bc1\u3002", "result": "\u5728\u591a\u79cdLLM\u548c\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u9879\u76ee\u5df2\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "MoR\u6846\u67b6\u901a\u8fc7\u89d2\u8272\u5206\u89e3\u548c\u4e13\u95e8\u5316LoRA\u7ec4\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u5fae\u8c03\uff0c\u4e3aLLM\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.21882", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.21882", "abs": "https://arxiv.org/abs/2512.21882", "authors": ["Kenta Iizuka", "Akiyoshi Uchida", "Kentaro Uno", "Kazuya Yoshida"], "title": "Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking", "comment": "Author's version of a manuscript accepted at the International Conference on Space Robotics 2025 (iSpaRo 2025). (c) IEEE", "summary": "Approaching a tumbling target safely is a critical challenge in space debris removal missions utilizing robotic manipulators onboard servicing satellites. In this work, we propose a trajectory planning method based on nonlinear optimization for a close-range rendezvous to bring a free-floating, rotating debris object in a two-dimensional plane into the manipulator's workspace, as a preliminary step for its capture. The proposed method introduces a dynamic keep-out sphere that adapts depending on the approach conditions, allowing for closer and safer access to the target. Furthermore, a control strategy is developed to reproduce the optimized trajectory using discrete ON/OFF thrusters, considering practical implementation constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u4f18\u5316\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a7a\u95f4\u788e\u7247\u6e05\u9664\u4efb\u52a1\u4e2d\u63a5\u8fd1\u7ffb\u6eda\u76ee\u6807\u7684\u8fd1\u8ddd\u79bb\u4ea4\u4f1a\uff0c\u5f15\u5165\u52a8\u6001\u7981\u5165\u7403\u4f53\u786e\u4fdd\u5b89\u5168\u63a5\u8fd1\uff0c\u5e76\u5f00\u53d1\u4e86\u8003\u8651\u5b9e\u9645\u7ea6\u675f\u7684ON/OFF\u63a8\u529b\u5668\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u7a7a\u95f4\u788e\u7247\u6e05\u9664\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u673a\u68b0\u81c2\u7684\u670d\u52a1\u536b\u661f\u9700\u8981\u5b89\u5168\u63a5\u8fd1\u7ffb\u6eda\u7684\u788e\u7247\u76ee\u6807\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u9700\u8981\u89e3\u51b3\u5728\u76ee\u6807\u65cb\u8f6c\u72b6\u6001\u4e0b\u5c06\u5176\u5e26\u5165\u673a\u68b0\u81c2\u5de5\u4f5c\u7a7a\u95f4\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u975e\u7ebf\u6027\u4f18\u5316\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u5f15\u5165\u52a8\u6001\u7981\u5165\u7403\u4f53\uff08\u6839\u636e\u63a5\u8fd1\u6761\u4ef6\u81ea\u9002\u5e94\u8c03\u6574\uff09\uff0c\u5141\u8bb8\u66f4\u8fd1\u66f4\u5b89\u5168\u5730\u63a5\u8fd1\u76ee\u6807\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4f7f\u7528\u79bb\u6563ON/OFF\u63a8\u529b\u5668\u7684\u63a7\u5236\u7b56\u7565\u6765\u590d\u73b0\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u81ea\u7531\u6f02\u6d6e\u3001\u65cb\u8f6c\u7684\u788e\u7247\u76ee\u6807\u5728\u4e8c\u7ef4\u5e73\u9762\u5185\u89c4\u5212\u51fa\u5b89\u5168\u7684\u63a5\u8fd1\u8f68\u8ff9\uff0c\u4f5c\u4e3a\u6355\u83b7\u524d\u7684\u9884\u5907\u6b65\u9aa4\u3002\u52a8\u6001\u7981\u5165\u7403\u4f53\u673a\u5236\u63d0\u9ad8\u4e86\u63a5\u8fd1\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f68\u8ff9\u89c4\u5212\u548c\u63a7\u5236\u7b56\u7565\u4e3a\u89e3\u51b3\u7a7a\u95f4\u788e\u7247\u6e05\u9664\u4e2d\u7684\u5b89\u5168\u63a5\u8fd1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u52a8\u6001\u7981\u5165\u7403\u4f53\u6982\u5ff5\u548cON/OFF\u63a8\u529b\u5668\u63a7\u5236\u65b9\u6848\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.21709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21709", "abs": "https://arxiv.org/abs/2512.21709", "authors": ["Md. Rakibul Islam", "Most. Sharmin Sultana Samu", "Md. Zahid Hossain", "Farhad Uz Zaman", "Md. Kamrozzaman Bhuiyan"], "title": "Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers", "comment": "Accepted for publication in 2025 28th International Conference on Computer and Information Technology (ICCIT)", "summary": "Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5b5f\u52a0\u62c9\u8bed\u4e2dAI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u96f6\u6837\u672c\u8bc4\u4f30\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u7ecf\u8fc7\u5fae\u8c03\u540eXLM-RoBERTa\u7b49\u6a21\u578b\u80fd\u8fbe\u5230\u7ea691%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6587\u672c\uff0c\u5f15\u53d1\u4e86\u5bf9\u865a\u5047\u4fe1\u606f\u548c\u5185\u5bb9\u6ee5\u7528\u7684\u62c5\u5fe7\u3002\u867d\u7136\u5df2\u6709\u591a\u79cd\u8bed\u8a00\u7684\u68c0\u6d4b\u7814\u7a76\uff0c\u4f46\u5b5f\u52a0\u62c9\u8bed\u56e0\u5176\u4e30\u5bcc\u7684\u8bcd\u6c47\u548c\u590d\u6742\u7ed3\u6784\u800c\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7814\u7a76\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u7814\u7a76\u8c03\u67e5\u4e86\u4e94\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff1aXLMRoBERTa-Large\u3001mDeBERTaV3-Base\u3001BanglaBERT-Base\u3001IndicBERT-Base\u548cMultilingualBERT-Base\u3002\u9996\u5148\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u7136\u540e\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u3002", "result": "\u96f6\u6837\u672c\u8bc4\u4f30\u663e\u793a\u6240\u6709\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff08\u7ea650%\u51c6\u786e\u7387\uff09\u3002\u5fae\u8c03\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cXLM-RoBERTa\u3001mDeBERTa\u548cMultilingualBERT\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u8fbe\u5230\u7ea691%\u3002IndicBERT\u8868\u73b0\u76f8\u5bf9\u8f83\u5f31\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u5b5f\u52a0\u62c9\u8bedAI\u751f\u6210\u6587\u672c\u68c0\u6d4b\uff0c\u4e3a\u6784\u5efa\u5f3a\u5927\u7684\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5fae\u8c03\u5bf9\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u5b5f\u52a0\u62c9\u8bed\u8fd9\u79cd\u590d\u6742\u8bed\u8a00\u4e2d\u3002"}}
{"id": "2512.21886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21886", "abs": "https://arxiv.org/abs/2512.21886", "authors": ["Akiyoshi Uchida", "Antonine Richard", "Kentaro Uno", "Miguel Olivares-Mendez", "Kazuya Yoshida"], "title": "Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications", "comment": "Author's version of a manuscript accepted at the International Conference on Space Robotics 2025 (iSpaRo 2025). (c) IEEE", "summary": "Knowing the inertia parameters of a grasped object is crucial for dynamics-aware manipulation, especially in space robotics with free-floating bases. This work addresses the problem of estimating the inertia parameters of an unknown target object during manipulation. We apply and extend an existing online identification method by incorporating momentum conservation, enabling its use for the floating-base robots. The proposed method is validated through numerical simulations, and the estimated parameters are compared with ground-truth values. Results demonstrate accurate identification in the scenarios, highlighting the method's applicability to on-orbit servicing and other space missions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u81ea\u7531\u6f02\u6d6e\u57fa\u5ea7\u673a\u5668\u4eba\u7684\u5728\u7ebf\u60ef\u6027\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u91cf\u5b88\u6052\u6269\u5c55\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u673a\u5668\u4eba\u64cd\u4f5c\u672a\u77e5\u76ee\u6807\u7269\u4f53", "motivation": "\u5728\u7a7a\u95f4\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\uff0c\u638c\u63e1\u88ab\u6293\u53d6\u7269\u4f53\u7684\u60ef\u6027\u53c2\u6570\u5bf9\u4e8e\u52a8\u529b\u5b66\u611f\u77e5\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u81ea\u7531\u6f02\u6d6e\u57fa\u5ea7\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u51c6\u786e\u4f30\u8ba1\u672a\u77e5\u76ee\u6807\u7269\u4f53\u7684\u60ef\u6027\u53c2\u6570", "method": "\u6269\u5c55\u73b0\u6709\u5728\u7ebf\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u52a8\u91cf\u5b88\u6052\u539f\u7406\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u81ea\u7531\u6f02\u6d6e\u57fa\u5ea7\u673a\u5668\u4eba\uff0c\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u4f30\u8ba1\u76ee\u6807\u7269\u4f53\u7684\u60ef\u6027\u53c2\u6570", "result": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4f30\u8ba1\u53c2\u6570\u4e0e\u771f\u5b9e\u503c\u5bf9\u6bd4\u663e\u793a\u51c6\u786e\u8bc6\u522b\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5728\u8f68\u670d\u52a1\u548c\u5176\u4ed6\u7a7a\u95f4\u4efb\u52a1\uff0c\u4e3a\u81ea\u7531\u6f02\u6d6e\u57fa\u5ea7\u673a\u5668\u4eba\u64cd\u4f5c\u672a\u77e5\u7269\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u60ef\u6027\u53c2\u6570\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.21711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21711", "abs": "https://arxiv.org/abs/2512.21711", "authors": ["Yuyi Zhang", "Boyu Tang", "Tianjie Ju", "Sufeng Duan", "Gongshen Liu"], "title": "Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought", "comment": "13 pages, 5 figures", "summary": "Latent tokens are gaining attention for enhancing reasoning in large language models (LLMs), yet their internal mechanisms remain unclear. This paper examines the problem from a reliability perspective, uncovering fundamental weaknesses: latent tokens function as uninterpretable placeholders rather than encoding faithful reasoning. While resistant to perturbation, they promote shortcut usage over genuine reasoning. We focus on Chain-of-Continuous-Thought (COCONUT), which claims better efficiency and stability than explicit Chain-of-Thought (CoT) while maintaining performance. We investigate this through two complementary approaches. First, steering experiments perturb specific token subsets, namely COCONUT and explicit CoT. Unlike CoT tokens, COCONUT tokens show minimal sensitivity to steering and lack reasoning-critical information. Second, shortcut experiments evaluate models under biased and out-of-distribution settings. Results on MMLU and HotpotQA demonstrate that COCONUT consistently exploits dataset artifacts, inflating benchmark performance without true reasoning. These findings reposition COCONUT as a pseudo-reasoning mechanism: it generates plausible traces that conceal shortcut dependence rather than faithfully representing reasoning processes.", "AI": {"tldr": "COCONUT\uff08Chain-of-Continuous-Thought\uff09\u4f5c\u4e3a\u6f5c\u5728token\u63a8\u7406\u65b9\u6cd5\uff0c\u5b9e\u9645\u4e0a\u662f\u4e00\u79cd\u4f2a\u63a8\u7406\u673a\u5236\uff0c\u5b83\u4f9d\u8d56\u6570\u636e\u96c6\u4f2a\u5f71\u800c\u975e\u771f\u5b9e\u63a8\u7406\uff0c\u901a\u8fc7\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u63a8\u7406\u8f68\u8ff9\u6765\u63a9\u76d6\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\u3002", "motivation": "\u5c3d\u7ba1\u6f5c\u5728token\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b9\u9762\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u672c\u6587\u4ece\u53ef\u9760\u6027\u89d2\u5ea6\u7814\u7a76\u8be5\u95ee\u9898\uff0c\u65e8\u5728\u63ed\u793a\u6f5c\u5728token\u63a8\u7406\u65b9\u6cd5\uff08\u7279\u522b\u662fCOCONUT\uff09\u662f\u5426\u771f\u6b63\u7f16\u7801\u4e86\u5fe0\u5b9e\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u6377\u5f84\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a1\uff09\u5f15\u5bfc\u5b9e\u9a8c\uff1a\u6270\u52a8COCONUT\u548c\u663e\u5f0fCoT\u7684\u7279\u5b9atoken\u5b50\u96c6\uff0c\u6bd4\u8f83\u654f\u611f\u6027\uff1b2\uff09\u6377\u5f84\u5b9e\u9a8c\uff1a\u5728\u504f\u89c1\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u6a21\u578b\uff0c\u68c0\u9a8c\u662f\u5426\u4f9d\u8d56\u6570\u636e\u96c6\u4f2a\u5f71\u3002", "result": "\u5728MMLU\u548cHotpotQA\u4e0a\u7684\u7ed3\u679c\u663e\u793a\uff1aCOCONUT token\u5bf9\u5f15\u5bfc\u6270\u52a8\u4e0d\u654f\u611f\uff0c\u7f3a\u4e4f\u63a8\u7406\u5173\u952e\u4fe1\u606f\uff1b\u5728\u6377\u5f84\u5b9e\u9a8c\u4e2d\uff0cCOCONUT\u6301\u7eed\u5229\u7528\u6570\u636e\u96c6\u4f2a\u5f71\u6765\u63d0\u5347\u57fa\u51c6\u6027\u80fd\uff0c\u800c\u975e\u8fdb\u884c\u771f\u5b9e\u63a8\u7406\u3002", "conclusion": "COCONUT\u662f\u4e00\u79cd\u4f2a\u63a8\u7406\u673a\u5236\uff0c\u5b83\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u63a8\u7406\u8f68\u8ff9\u6765\u63a9\u76d6\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\uff0c\u800c\u975e\u5fe0\u5b9e\u8868\u793a\u63a8\u7406\u8fc7\u7a0b\u3002\u6f5c\u5728token\u4f5c\u4e3a\u4e0d\u53ef\u89e3\u91ca\u7684\u5360\u4f4d\u7b26\uff0c\u4fc3\u8fdb\u6377\u5f84\u4f7f\u7528\u800c\u975e\u771f\u6b63\u63a8\u7406\u3002"}}
{"id": "2512.21887", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21887", "abs": "https://arxiv.org/abs/2512.21887", "authors": ["Weichen Zhang", "Peizhi Tang", "Xin Zeng", "Fanhang Man", "Shiquan Yu", "Zichao Dai", "Baining Zhao", "Hongjin Chen", "Yu Shang", "Wei Wu", "Chen Gao", "Xinlei Chen", "Xin Wang", "Yong Li", "Wenwu Zhu"], "title": "Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) have emerged as powerful embodied agents. One of the core abilities is autonomous navigation in large-scale three-dimensional environments. Existing navigation policies, however, are typically optimized for low-level objectives such as obstacle avoidance and trajectory smoothness, lacking the ability to incorporate high-level semantics into planning. To bridge this gap, we propose ANWM, an aerial navigation world model that predicts future visual observations conditioned on past frames and actions, thereby enabling agents to rank candidate trajectories by their semantic plausibility and navigational utility. ANWM is trained on 4-DoF UAV trajectories and introduces a physics-inspired module: Future Frame Projection (FFP), which projects past frames into future viewpoints to provide coarse geometric priors. This module mitigates representational uncertainty in long-distance visual generation and captures the mapping between 3D trajectories and egocentric observations. Empirical results demonstrate that ANWM significantly outperforms existing world models in long-distance visual forecasting and improves UAV navigation success rates in large-scale environments.", "AI": {"tldr": "ANWM\u662f\u4e00\u4e2a\u65e0\u4eba\u673a\u5bfc\u822a\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u89c6\u89c9\u89c2\u6d4b\u6765\u8bc4\u4f30\u8f68\u8ff9\u7684\u8bed\u4e49\u5408\u7406\u6027\u548c\u5bfc\u822a\u6548\u7528\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u5728\u5927\u578b3D\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u5bfc\u822a\u7b56\u7565\u4e3b\u8981\u4f18\u5316\u4f4e\u5c42\u76ee\u6807\uff08\u5982\u907f\u969c\u548c\u8f68\u8ff9\u5e73\u6ed1\uff09\uff0c\u7f3a\u4e4f\u5c06\u9ad8\u5c42\u8bed\u4e49\u878d\u5165\u89c4\u5212\u7684\u80fd\u529b\uff0c\u9700\u8981\u6865\u63a5\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faANWM\u6a21\u578b\uff0c\u57fa\u4e8e4-DoF\u65e0\u4eba\u673a\u8f68\u8ff9\u8bad\u7ec3\uff0c\u5f15\u5165\u7269\u7406\u542f\u53d1\u7684\u672a\u6765\u5e27\u6295\u5f71\u6a21\u5757\uff08FFP\uff09\uff0c\u5c06\u8fc7\u53bb\u5e27\u6295\u5f71\u5230\u672a\u6765\u89c6\u70b9\u63d0\u4f9b\u51e0\u4f55\u5148\u9a8c\uff0c\u51cf\u5c11\u957f\u8ddd\u79bb\u89c6\u89c9\u751f\u6210\u7684\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u3002", "result": "ANWM\u5728\u957f\u8ddd\u79bb\u89c6\u89c9\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u5728\u5927\u578b\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002", "conclusion": "ANWM\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u7406\u89e3\u548c\u7269\u7406\u51e0\u4f55\u5148\u9a8c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2512.21715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21715", "abs": "https://arxiv.org/abs/2512.21715", "authors": ["Rui Ke", "Jiahui Xu", "Shenghao Yang", "Kuang Wang", "Feng Jiang", "Haizhou Li"], "title": "CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation", "comment": null, "summary": "Theme detection is a fundamental task in user-centric dialogue systems, aiming to identify the latent topic of each utterance without relying on predefined schemas. Unlike intent induction, which operates within fixed label spaces, theme detection requires cross-dialogue consistency and alignment with personalized user preferences, posing significant challenges. Existing methods often struggle with sparse, short utterances for accurate topic representation and fail to capture user-level thematic preferences across dialogues. To address these challenges, we propose CATCH (Controllable Theme Detection with Contextualized Clustering and Hierarchical Generation), a unified framework that integrates three core components: (1) context-aware topic representation, which enriches utterance-level semantics using surrounding topic segments; (2) preference-guided topic clustering, which jointly models semantic proximity and personalized feedback to align themes across dialogue; and (3) a hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels. Experiments on a multi-domain customer dialogue benchmark (DSTC-12) demonstrate the effectiveness of CATCH with 8B LLM in both theme clustering and topic generation quality.", "AI": {"tldr": "CATCH\u662f\u4e00\u4e2a\u7528\u4e8e\u4e3b\u9898\u68c0\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u4e3b\u9898\u8868\u793a\u3001\u504f\u597d\u5f15\u5bfc\u4e3b\u9898\u805a\u7c7b\u548c\u5206\u5c42\u4e3b\u9898\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u4e3b\u9898\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7a00\u758f\u3001\u7b80\u77ed\u7684\u8bdd\u8bed\u65f6\u96be\u4ee5\u51c6\u786e\u8868\u793a\u4e3b\u9898\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u8de8\u5bf9\u8bdd\u7684\u7528\u6237\u7ea7\u4e3b\u9898\u504f\u597d\u3002\u4e3b\u9898\u68c0\u6d4b\u9700\u8981\u8de8\u5bf9\u8bdd\u4e00\u81f4\u6027\u548c\u4e2a\u6027\u5316\u7528\u6237\u504f\u597d\u5bf9\u9f50\uff0c\u8fd9\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "CATCH\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u4e0a\u4e0b\u6587\u611f\u77e5\u4e3b\u9898\u8868\u793a\uff0c\u5229\u7528\u5468\u56f4\u4e3b\u9898\u7247\u6bb5\u4e30\u5bcc\u8bdd\u8bed\u7ea7\u8bed\u4e49\uff1b2) \u504f\u597d\u5f15\u5bfc\u4e3b\u9898\u805a\u7c7b\uff0c\u8054\u5408\u5efa\u6a21\u8bed\u4e49\u90bb\u8fd1\u6027\u548c\u4e2a\u6027\u5316\u53cd\u9988\u4ee5\u5bf9\u9f50\u8de8\u5bf9\u8bdd\u4e3b\u9898\uff1b3) \u5206\u5c42\u4e3b\u9898\u751f\u6210\u673a\u5236\uff0c\u6291\u5236\u566a\u58f0\u5e76\u4ea7\u751f\u9c81\u68d2\u3001\u8fde\u8d2f\u7684\u4e3b\u9898\u6807\u7b7e\u3002", "result": "\u5728\u591a\u9886\u57df\u5ba2\u6237\u5bf9\u8bdd\u57fa\u51c6\uff08DSTC-12\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCATCH\u5728\u4e3b\u9898\u805a\u7c7b\u548c\u4e3b\u9898\u751f\u6210\u8d28\u91cf\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4f7f\u7528\u4e868B LLM\u3002", "conclusion": "CATCH\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u8868\u793a\u3001\u4e2a\u6027\u5316\u805a\u7c7b\u548c\u5206\u5c42\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e3b\u9898\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u4e3b\u9898\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2512.21898", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21898", "abs": "https://arxiv.org/abs/2512.21898", "authors": ["Chaoqi Liu", "Haonan Chen", "Sigmund H. H\u00f8eg", "Shaoxiong Yao", "Yunzhu Li", "Kris Hauser", "Yilun Du"], "title": "Flexible Multitask Learning with Factorized Diffusion Policy", "comment": null, "summary": "Multitask learning poses significant challenges due to the highly multimodal and diverse nature of robot action distributions. However, effectively fitting policies to these complex task distributions is often difficult, and existing monolithic models often underfit the action distribution and lack the flexibility required for efficient adaptation. We introduce a novel modular diffusion policy framework that factorizes complex action distributions into a composition of specialized diffusion models, each capturing a distinct sub-mode of the behavior space for a more effective overall policy. In addition, this modular structure enables flexible policy adaptation to new tasks by adding or fine-tuning components, which inherently mitigates catastrophic forgetting. Empirically, across both simulation and real-world robotic manipulation settings, we illustrate how our method consistently outperforms strong modular and monolithic baselines.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u5757\u5316\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u5c06\u590d\u6742\u52a8\u4f5c\u5206\u5e03\u5206\u89e3\u4e3a\u591a\u4e2a\u4e13\u95e8\u6269\u6563\u6a21\u578b\u7684\u7ec4\u5408\uff0c\u6bcf\u4e2a\u6a21\u578b\u6355\u6349\u884c\u4e3a\u7a7a\u95f4\u7684\u4e0d\u540c\u5b50\u6a21\u5f0f\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5b66\u4e60\u591a\u4efb\u52a1\u673a\u5668\u4eba\u7b56\u7565\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u9762\u4e34\u673a\u5668\u4eba\u52a8\u4f5c\u5206\u5e03\u9ad8\u5ea6\u591a\u6a21\u6001\u548c\u591a\u6837\u6027\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u5355\u4e00\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u6709\u6548\u62df\u5408\u8fd9\u4e9b\u590d\u6742\u5206\u5e03\uff0c\u5bb9\u6613\u6b20\u62df\u5408\u4e14\u7f3a\u4e4f\u7075\u6d3b\u9002\u5e94\u80fd\u529b\u3002", "method": "\u5f15\u5165\u6a21\u5757\u5316\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u5c06\u590d\u6742\u52a8\u4f5c\u5206\u5e03\u5206\u89e3\u4e3a\u591a\u4e2a\u4e13\u95e8\u6269\u6563\u6a21\u578b\u7684\u7ec4\u5408\u3002\u6bcf\u4e2a\u6269\u6563\u6a21\u578b\u6355\u6349\u884c\u4e3a\u7a7a\u95f4\u7684\u4e0d\u540c\u5b50\u6a21\u5f0f\uff0c\u5f62\u6210\u6574\u4f53\u7b56\u7565\u3002\u6a21\u5757\u5316\u7ed3\u6784\u652f\u6301\u901a\u8fc7\u6dfb\u52a0\u6216\u5fae\u8c03\u7ec4\u4ef6\u6765\u7075\u6d3b\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u6a21\u5757\u5316\u548c\u5355\u4e00\u6a21\u578b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u5757\u5316\u6269\u6563\u7b56\u7565\u6846\u67b6\u80fd\u6709\u6548\u5206\u89e3\u590d\u6742\u52a8\u4f5c\u5206\u5e03\uff0c\u63d0\u9ad8\u591a\u4efb\u52a1\u5b66\u4e60\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u6a21\u5757\u5316\u7ed3\u6784\u5b9e\u73b0\u7075\u6d3b\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2512.21787", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21787", "abs": "https://arxiv.org/abs/2512.21787", "authors": ["Abdullah Alabdullah", "Lifeng Han", "Chenghua Lin"], "title": "Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation", "comment": null, "summary": "Dialectal Arabic to Modern Standard Arabic (DA-MSA) translation is a challenging task in Machine Translation (MT) due to significant lexical, syntactic, and semantic divergences between Arabic dialects and MSA. Existing automatic evaluation metrics and general-purpose human evaluation frameworks struggle to capture dialect-specific MT errors, hindering progress in translation assessment. This paper introduces Ara-HOPE, a human-centric post-editing evaluation framework designed to systematically address these challenges. The framework includes a five-category error taxonomy and a decision-tree annotation protocol. Through comparative evaluation of three MT systems (Arabic-centric Jais, general-purpose GPT-3.5, and baseline NLLB-200), Ara-HOPE effectively highlights systematic performance differences between these systems. The results show that dialect-specific terminology and semantic preservation remain the most persistent challenges in DA-MSA translation. Ara-HOPE establishes a new framework for evaluating Dialectal Arabic MT quality and provides actionable guidance for improving dialect-aware MT systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ara-HOPE\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u963f\u62c9\u4f2f\u65b9\u8a00\u5230\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u7684\u4eba\u7c7b\u4e2d\u5fc3\u5316\u540e\u7f16\u8f91\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e94\u7c7b\u9519\u8bef\u5206\u7c7b\u6cd5\u548c\u51b3\u7b56\u6811\u6807\u6ce8\u534f\u8bae\uff0c\u80fd\u6709\u6548\u8bc4\u4f30\u7ffb\u8bd1\u7cfb\u7edf\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u963f\u62c9\u4f2f\u65b9\u8a00\u5230\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u548c\u901a\u7528\u4eba\u5de5\u8bc4\u4f30\u6846\u67b6\u96be\u4ee5\u6355\u6349\u65b9\u8a00\u7279\u5b9a\u7684\u7ffb\u8bd1\u9519\u8bef\uff0c\u963b\u788d\u4e86\u7ffb\u8bd1\u8bc4\u4f30\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faAra-HOPE\u6846\u67b6\uff0c\u5305\u542b\u4e94\u7c7b\u9519\u8bef\u5206\u7c7b\u6cd5\u548c\u51b3\u7b56\u6811\u6807\u6ce8\u534f\u8bae\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u4e09\u4e2a\u7ffb\u8bd1\u7cfb\u7edf\uff1a\u963f\u62c9\u4f2f\u4e2d\u5fc3\u5316\u7684Jais\u3001\u901a\u7528GPT-3.5\u548c\u57fa\u7ebfNLLB-200\u3002", "result": "Ara-HOPE\u80fd\u6709\u6548\u7a81\u51fa\u4e0d\u540c\u7cfb\u7edf\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u7ed3\u679c\u663e\u793a\u65b9\u8a00\u7279\u5b9a\u672f\u8bed\u548c\u8bed\u4e49\u4fdd\u7559\u662f\u963f\u62c9\u4f2f\u65b9\u8a00\u7ffb\u8bd1\u4e2d\u6700\u6301\u4e45\u7684\u6311\u6218\u3002", "conclusion": "Ara-HOPE\u4e3a\u8bc4\u4f30\u963f\u62c9\u4f2f\u65b9\u8a00\u7ffb\u8bd1\u8d28\u91cf\u5efa\u7acb\u4e86\u65b0\u6846\u67b6\uff0c\u5e76\u4e3a\u6539\u8fdb\u65b9\u8a00\u611f\u77e5\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002"}}
{"id": "2512.21970", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21970", "abs": "https://arxiv.org/abs/2512.21970", "authors": ["Shengliang Deng", "Mi Yan", "Yixin Zheng", "Jiayi Su", "Wenhao Zhang", "Xiaoguang Zhao", "Heming Cui", "Zhizheng Zhang", "He Wang"], "title": "StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision", "comment": null, "summary": "Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.", "AI": {"tldr": "StereoVLA\uff1a\u5229\u7528\u7acb\u4f53\u89c6\u89c9\u51e0\u4f55\u7ebf\u7d22\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u51e0\u4f55-\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u4ea4\u4e92\u533a\u57df\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7acb\u4f53\u89c6\u89c9\u80fd\u63d0\u4f9b\u4e30\u5bcc\u7684\u7a7a\u95f4\u7ebf\u7d22\uff0c\u5bf9\u673a\u5668\u4eba\u7cbe\u786e\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faStereoVLA\u6a21\u578b\uff0c\u5305\u542b\u51e0\u4f55-\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff08\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u5e76\u878d\u5408\u7acb\u4f53\u89c6\u56fe\u7684\u51e0\u4f55\u7279\u5f81\u548c\u5355\u76ee\u89c6\u56fe\u7684\u8bed\u4e49\u7279\u5f81\uff09\uff0c\u4ee5\u53ca\u8f85\u52a9\u7684\u4ea4\u4e92\u533a\u57df\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u6765\u589e\u5f3a\u7a7a\u95f4\u611f\u77e5\u3002", "result": "\u5728\u7acb\u4f53\u89c6\u89c9\u8bbe\u7f6e\u4e0b\u7684\u591a\u6837\u5316\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5927\u5e45\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5bf9\u76f8\u673a\u59ff\u6001\u53d8\u5316\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "StereoVLA\u6210\u529f\u5c06\u7acb\u4f53\u89c6\u89c9\u7684\u51e0\u4f55\u7ebf\u7d22\u6574\u5408\u5230VLA\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u7cbe\u786e\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21789", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21789", "abs": "https://arxiv.org/abs/2512.21789", "authors": ["Ting-Hao K. Huang", "Ryan A. Rossi", "Sungchul Kim", "Tong Yu", "Ting-Yao E. Hsu", "Ho Yin", "Ng", "C. Lee Giles"], "title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning", "comment": "Accepted to the 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE 2026)", "summary": "Between 2021 and 2025, the SciCap project grew from a small seed-funded idea at The Pennsylvania State University (Penn State) into one of the central efforts shaping the scientific figure-captioning landscape. Supported by a Penn State seed grant, Adobe, and the Alfred P. Sloan Foundation, what began as our attempt to test whether domain-specific training, which was successful in text models like SciBERT, could also work for figure captions expanded into a multi-institution collaboration. Over these five years, we curated, released, and continually updated a large collection of figure-caption pairs from arXiv papers, conducted extensive automatic and human evaluations on both generated and author-written captions, navigated the rapid rise of large language models (LLMs), launched annual challenges, and built interactive systems that help scientists write better captions. In this piece, we look back at the first five years of SciCap and summarize the key technical and methodological lessons we learned. We then outline five major unsolved challenges and propose directions for the next phase of research in scientific figure captioning.", "AI": {"tldr": "SciCap\u9879\u76ee\u56de\u987e\uff1a\u4ece2011-2025\u5e74\u79d1\u5b66\u56fe\u8868\u6807\u9898\u751f\u6210\u7814\u7a76\u7684\u4e94\u5e74\u53d1\u5c55\u5386\u7a0b\uff0c\u603b\u7ed3\u4e86\u6280\u672f\u65b9\u6cd5\u6559\u8bad\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u4e94\u5927\u6311\u6218\u65b9\u5411", "motivation": "\u6d4b\u8bd5\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\uff08\u5728\u6587\u672c\u6a21\u578b\u5982SciBERT\u4e2d\u6210\u529f\u7684\u65b9\u6cd5\uff09\u662f\u5426\u540c\u6837\u9002\u7528\u4e8e\u56fe\u8868\u6807\u9898\u751f\u6210\uff0c\u63a2\u7d22\u79d1\u5b66\u56fe\u8868\u6807\u9898\u81ea\u52a8\u751f\u6210\u7684\u53ef\u80fd\u6027", "method": "1. \u4ecearXiv\u8bba\u6587\u4e2d\u6536\u96c6\u548c\u53d1\u5e03\u5927\u91cf\u56fe\u8868-\u6807\u9898\u5bf9\u6570\u636e\u96c6\uff1b2. \u8fdb\u884c\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\uff1b3. \u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5174\u8d77\uff1b4. \u4e3e\u529e\u5e74\u5ea6\u6311\u6218\u8d5b\uff1b5. \u6784\u5efa\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u5e2e\u52a9\u79d1\u5b66\u5bb6\u64b0\u5199\u66f4\u597d\u7684\u6807\u9898", "result": "\u9879\u76ee\u4ece\u5bbe\u5dde\u5927\u5b66\u79cd\u5b50\u57fa\u91d1\u9879\u76ee\u53d1\u5c55\u6210\u4e3a\u5851\u9020\u79d1\u5b66\u56fe\u8868\u6807\u9898\u751f\u6210\u9886\u57df\u7684\u6838\u5fc3\u52aa\u529b\uff0c\u5efa\u7acb\u4e86\u591a\u673a\u6784\u5408\u4f5c\uff0c\u521b\u5efa\u4e86\u6301\u7eed\u66f4\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u7528\u5de5\u5177", "conclusion": "\u603b\u7ed3\u4e86\u4e94\u5e74\u6765\u7684\u6280\u672f\u548c\u65b9\u6cd5\u6559\u8bad\uff0c\u63d0\u51fa\u4e86\u79d1\u5b66\u56fe\u8868\u6807\u9898\u751f\u6210\u9886\u57df\u672a\u6765\u7814\u7a76\u7684\u4e94\u5927\u672a\u89e3\u51b3\u6311\u6218\u548c\u53d1\u5c55\u65b9\u5411"}}
{"id": "2512.21983", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21983", "abs": "https://arxiv.org/abs/2512.21983", "authors": ["Saksham Gupta", "Sarthak Mishra", "Arshad Ayub", "Kamran Farooque", "Spandan Roy", "Babita Gupta"], "title": "Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation", "comment": null, "summary": "Endotracheal intubation is a critical yet technically demanding procedure, with failure or improper tube placement leading to severe complications. Existing robotic and teleoperated intubation systems primarily focus on airway navigation and do not provide integrated control of endotracheal tube advancement or objective verification of tube depth relative to the carina. This paper presents the Robotic Intubation System (BRIS), a compact, human-in-the-loop platform designed to assist fiberoptic-guided intubation while enabling real-time, objective depth awareness. BRIS integrates a four-way steerable fiberoptic bronchoscope, an independent endotracheal tube advancement mechanism, and a camera-augmented mouthpiece compatible with standard clinical workflows. A learning-enabled closed-loop control framework leverages real-time shape sensing to map joystick inputs to distal bronchoscope tip motion in Cartesian space, providing stable and intuitive teleoperation under tendon nonlinearities and airway contact. Monocular endoscopic depth estimation is used to classify airway regions and provide interpretable, anatomy-aware guidance for safe tube positioning relative to the carina. The system is validated on high-fidelity airway mannequins under standard and difficult airway configurations, demonstrating reliable navigation and controlled tube placement. These results highlight BRIS as a step toward safer, more consistent, and clinically compatible robotic airway management.", "AI": {"tldr": "BRIS\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u4eba\u673a\u534f\u540c\u673a\u5668\u4eba\u63d2\u7ba1\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u56db\u5411\u53ef\u64cd\u7eb5\u7ea4\u7ef4\u652f\u6c14\u7ba1\u955c\u3001\u72ec\u7acb\u6c14\u7ba1\u5bfc\u7ba1\u63a8\u8fdb\u673a\u5236\u548c\u6444\u50cf\u5934\u589e\u5f3a\u53e3\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u578b\u95ed\u73af\u63a7\u5236\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5b9e\u73b0\u5b89\u5168\u3001\u76f4\u89c2\u7684\u6c14\u7ba1\u63d2\u7ba1\u8f85\u52a9\u3002", "motivation": "\u6c14\u7ba1\u63d2\u7ba1\u662f\u5173\u952e\u4f46\u6280\u672f\u8981\u6c42\u9ad8\u7684\u64cd\u4f5c\uff0c\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u6c14\u9053\u5bfc\u822a\uff0c\u7f3a\u4e4f\u5bf9\u6c14\u7ba1\u5bfc\u7ba1\u63a8\u8fdb\u7684\u96c6\u6210\u63a7\u5236\u548c\u76f8\u5bf9\u4e8e\u9686\u7a81\u7684\u5ba2\u89c2\u6df1\u5ea6\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u5e76\u53d1\u75c7\u3002", "method": "BRIS\u6574\u5408\u4e86\u56db\u5411\u53ef\u64cd\u7eb5\u7ea4\u7ef4\u652f\u6c14\u7ba1\u955c\u3001\u72ec\u7acb\u6c14\u7ba1\u5bfc\u7ba1\u63a8\u8fdb\u673a\u5236\u548c\u6444\u50cf\u5934\u589e\u5f3a\u53e3\u5668\u3002\u91c7\u7528\u5b66\u4e60\u578b\u95ed\u73af\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528\u5b9e\u65f6\u5f62\u72b6\u4f20\u611f\u5c06\u64cd\u7eb5\u6746\u8f93\u5165\u6620\u5c04\u5230\u652f\u6c14\u7ba1\u955c\u5c16\u7aef\u5728\u7b1b\u5361\u5c14\u7a7a\u95f4\u7684\u8fd0\u52a8\u3002\u4f7f\u7528\u5355\u76ee\u5185\u7aa5\u955c\u6df1\u5ea6\u4f30\u8ba1\u5bf9\u6c14\u9053\u533a\u57df\u8fdb\u884c\u5206\u7c7b\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89e3\u5256\u611f\u77e5\u5f15\u5bfc\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u6c14\u9053\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5305\u62ec\u6807\u51c6\u548c\u56f0\u96be\u6c14\u9053\u914d\u7f6e\uff0c\u7cfb\u7edf\u5c55\u793a\u4e86\u53ef\u9760\u7684\u5bfc\u822a\u548c\u53ef\u63a7\u7684\u5bfc\u7ba1\u653e\u7f6e\u6027\u80fd\u3002", "conclusion": "BRIS\u662f\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u4e00\u81f4\u4e14\u4e34\u5e8a\u517c\u5bb9\u7684\u673a\u5668\u4eba\u6c14\u9053\u7ba1\u7406\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u6c14\u7ba1\u63d2\u7ba1\u63d0\u4f9b\u4e86\u96c6\u6210\u63a7\u5236\u548c\u5ba2\u89c2\u6df1\u5ea6\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21809", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.21809", "abs": "https://arxiv.org/abs/2512.21809", "authors": ["Vitthal Bhandari"], "title": "On The Conceptualization and Societal Impact of Cross-Cultural Bias", "comment": "Term paper for LING 575 (Societal Impacts of Language Technologies)", "summary": "Research has shown that while large language models (LLMs) can generate their responses based on cultural context, they are not perfect and tend to generalize across cultures. However, when evaluating the cultural bias of a language technology on any dataset, researchers may choose not to engage with stakeholders actually using that technology in real life, which evades the very fundamental problem they set out to address.\n  Inspired by the work done by arXiv:2005.14050v2, I set out to analyse recent literature about identifying and evaluating cultural bias in Natural Language Processing (NLP). I picked out 20 papers published in 2025 about cultural bias and came up with a set of observations to allow NLP researchers in the future to conceptualize bias concretely and evaluate its harms effectively. My aim is to advocate for a robust assessment of the societal impact of language technologies exhibiting cross-cultural bias.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e862025\u5e74\u53d1\u8868\u768420\u7bc7\u5173\u4e8eNLP\u6587\u5316\u504f\u89c1\u7684\u6587\u732e\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u89c2\u5bdf\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u5177\u4f53\u5730\u6982\u5ff5\u5316\u504f\u89c1\u5e76\u6709\u6548\u8bc4\u4f30\u5176\u5371\u5bb3\uff0c\u5021\u5bfc\u5bf9\u8bed\u8a00\u6280\u672f\u7684\u793e\u4f1a\u5f71\u54cd\u8fdb\u884c\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u57fa\u4e8e\u6587\u5316\u80cc\u666f\u751f\u6210\u56de\u5e94\uff0c\u4f46\u5b58\u5728\u8de8\u6587\u5316\u6cdb\u5316\u95ee\u9898\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u7814\u7a76\u8005\u5728\u8bc4\u4f30\u8bed\u8a00\u6280\u672f\u6587\u5316\u504f\u89c1\u65f6\uff0c\u5f80\u5f80\u4e0d\u63a5\u89e6\u5b9e\u9645\u4f7f\u7528\u8be5\u6280\u672f\u7684\u5229\u76ca\u76f8\u5173\u8005\uff0c\u8fd9\u56de\u907f\u4e86\u4ed6\u4eec\u672c\u5e94\u89e3\u51b3\u7684\u6838\u5fc3\u95ee\u9898\u3002\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u66f4\u6709\u6548\u7684\u6587\u5316\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u53d7arXiv:2005.14050v2\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u7cfb\u7edf\u5206\u6790\u4e862025\u5e74\u53d1\u8868\u768420\u7bc7\u5173\u4e8eNLP\u6587\u5316\u504f\u89c1\u8bc6\u522b\u4e0e\u8bc4\u4f30\u7684\u6587\u732e\uff0c\u4ece\u4e2d\u63d0\u70bc\u51fa\u4e00\u5957\u89c2\u5bdf\u6846\u67b6\u548c\u6307\u5bfc\u539f\u5219\u3002", "result": "\u901a\u8fc7\u5bf920\u7bc7\u8bba\u6587\u7684\u5206\u6790\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u5957\u5177\u4f53\u7684\u89c2\u5bdf\u6846\u67b6\uff0c\u5e2e\u52a9NLP\u7814\u7a76\u8005\u66f4\u5177\u4f53\u5730\u6982\u5ff5\u5316\u6587\u5316\u504f\u89c1\uff0c\u5e76\u6709\u6548\u8bc4\u4f30\u5176\u5371\u5bb3\u3002\u8fd9\u4e9b\u89c2\u5bdf\u65e8\u5728\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u5b9e\u8df5\u3002", "conclusion": "\u8bba\u6587\u5021\u5bfc\u5bf9\u8868\u73b0\u51fa\u8de8\u6587\u5316\u504f\u89c1\u7684\u8bed\u8a00\u6280\u672f\u8fdb\u884c\u66f4\u7a33\u5065\u7684\u793e\u4f1a\u5f71\u54cd\u8bc4\u4f30\uff0c\u5f3a\u8c03\u9700\u8981\u5efa\u7acb\u66f4\u5177\u4f53\u3001\u6709\u6548\u7684\u6587\u5316\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u786e\u4fdd\u7814\u7a76\u771f\u6b63\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u800c\u975e\u56de\u907f\u6838\u5fc3\u95ee\u9898\u3002"}}
{"id": "2512.21817", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21817", "abs": "https://arxiv.org/abs/2512.21817", "authors": ["Hong Su"], "title": "Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments", "comment": null, "summary": "Intelligent IoT systems increasingly rely on large language models (LLMs) to generate task-execution methods for dynamic environments. However, existing approaches lack the ability to systematically produce new methods when facing previously unseen situations, and they often depend on fixed, device-specific logic that cannot adapt to changing environmental conditions.In this paper, we propose Method Decoration (DeMe), a general framework that modifies the method-generation path of an LLM using explicit decorations derived from hidden goals, accumulated learned methods, and environmental feedback. Unlike traditional rule augmentation, decorations in DeMe are not hardcoded; instead, they are extracted from universal behavioral principles, experience, and observed environmental differences. DeMe enables the agent to reshuffle the structure of its method path-through pre-decoration, post-decoration, intermediate-step modification, and step insertion-thereby producing context-aware, safety-aligned, and environment-adaptive methods. Experimental results show that method decoration allows IoT devices to derive ore appropriate methods when confronting unknown or faulty operating conditions.", "AI": {"tldr": "\u63d0\u51faMethod Decoration (DeMe)\u6846\u67b6\uff0c\u901a\u8fc7\u88c5\u9970LLM\u7684\u65b9\u6cd5\u751f\u6210\u8def\u5f84\u6765\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7cfb\u7edf\u5e94\u5bf9\u65b0\u573a\u666f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u7269\u8054\u7f51\u7cfb\u7edf\u4f9d\u8d56LLM\u751f\u6210\u4efb\u52a1\u6267\u884c\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5e94\u5bf9\u672a\u77e5\u573a\u666f\u7684\u80fd\u529b\uff0c\u4e14\u4f9d\u8d56\u56fa\u5b9a\u7684\u8bbe\u5907\u7279\u5b9a\u903b\u8f91\uff0c\u65e0\u6cd5\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "method": "DeMe\u6846\u67b6\u901a\u8fc7\u4ece\u9690\u85cf\u76ee\u6807\u3001\u79ef\u7d2f\u7684\u5b66\u4e60\u65b9\u6cd5\u548c\u73af\u5883\u53cd\u9988\u4e2d\u63d0\u53d6\u663e\u5f0f\u88c5\u9970\u6765\u4fee\u6539LLM\u7684\u65b9\u6cd5\u751f\u6210\u8def\u5f84\u3002\u88c5\u9970\u4e0d\u662f\u786c\u7f16\u7801\u7684\uff0c\u800c\u662f\u4ece\u901a\u7528\u884c\u4e3a\u539f\u5219\u3001\u7ecf\u9a8c\u548c\u89c2\u5bdf\u5230\u7684\u73af\u5883\u5dee\u5f02\u4e2d\u63d0\u53d6\u3002\u652f\u6301\u9884\u88c5\u9970\u3001\u540e\u88c5\u9970\u3001\u4e2d\u95f4\u6b65\u9aa4\u4fee\u6539\u548c\u6b65\u9aa4\u63d2\u5165\u56db\u79cd\u88c5\u9970\u65b9\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b9\u6cd5\u88c5\u9970\u4f7f\u7269\u8054\u7f51\u8bbe\u5907\u5728\u9762\u5bf9\u672a\u77e5\u6216\u6545\u969c\u64cd\u4f5c\u6761\u4ef6\u65f6\u80fd\u591f\u63a8\u5bfc\u51fa\u66f4\u5408\u9002\u7684\u65b9\u6cd5\u3002", "conclusion": "DeMe\u6846\u67b6\u901a\u8fc7\u88c5\u9970LLM\u7684\u65b9\u6cd5\u751f\u6210\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u5b89\u5168\u5bf9\u9f50\u548c\u73af\u5883\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u751f\u6210\uff0c\u63d0\u5347\u4e86\u667a\u80fd\u7269\u8054\u7f51\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2512.21837", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21837", "abs": "https://arxiv.org/abs/2512.21837", "authors": ["Siyu Li", "Chenwei Song", "Wan Zhou", "Xinyi Liu"], "title": "Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco", "comment": null, "summary": "This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u56fe\u7ed3\u6784\u4fe1\u606f\u7684LLM\u65b9\u6cd5\uff0c\u7528\u4e8e\u70df\u8349\u75c5\u866b\u5bb3\u9632\u6cbb\u7684\u77e5\u8bc6\u63a8\u7406\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u68c0\u7d22\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u70df\u8349\u75c5\u866b\u5bb3\u9632\u6cbb\u9700\u8981\u51c6\u786e\u7684\u77e5\u8bc6\u63a8\u7406\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u591a\u8df3\u548c\u6bd4\u8f83\u63a8\u7406\u573a\u666f\uff0c\u9700\u8981\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8eGraphRAG\u6846\u67b6\uff0c\u9996\u5148\u7528LLM\u8f85\u52a9\u6784\u5efa\u70df\u8349\u75c5\u866b\u5bb3\u77e5\u8bc6\u56fe\u8c31\uff0c\u7136\u540e\u7ed3\u5408GNN\u5b66\u4e60\u8282\u70b9\u8868\u793a\uff0c\u4f7f\u7528ChatGLM\u4f5c\u4e3a\u9aa8\u5e72\u6a21\u578b\u5e76\u901a\u8fc7LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u6df1\u5ea6\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u591a\u8df3\u548c\u6bd4\u8f83\u63a8\u7406\u573a\u666f\u4e2d\u3002", "conclusion": "\u7ed3\u5408\u56fe\u7ed3\u6784\u4fe1\u606f\u7684LLM\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u70df\u8349\u75c5\u866b\u5bb3\u9632\u6cbb\u9886\u57df\u7684\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u9886\u57df\u7279\u5b9a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21842", "abs": "https://arxiv.org/abs/2512.21842", "authors": ["Baorong Huang", "Ali Asiri"], "title": "AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts", "comment": null, "summary": "High-quality parallel corpora are essential for Machine Translation (MT) research and translation teaching. However, Arabic-English resources remain scarce and existing datasets mainly consist of simple one-to-one mappings. In this paper, we present AlignAR, a generative sentence alignment method, and a new Arabic-English dataset comprising complex legal and literary texts. Our evaluation demonstrates that \"Easy\" datasets lack the discriminatory power to fully assess alignment methods. By reducing one-to-one mappings in our \"Hard\" subset, we exposed the limitations of traditional alignment methods. In contrast, LLM-based approaches demonstrated superior robustness, achieving an overall F1-score of 85.5%, a 9% improvement over previous methods. Our datasets and codes are open-sourced at https://github.com/XXX.", "AI": {"tldr": "\u63d0\u51faAlignAR\u751f\u6210\u5f0f\u53e5\u5b50\u5bf9\u9f50\u65b9\u6cd5\u548c\u65b0\u7684\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u6570\u636e\u96c6\uff0c\u5305\u542b\u590d\u6742\u6cd5\u5f8b\u548c\u6587\u5b66\u6587\u672c\uff0c\u8bc4\u4f30\u663e\u793a\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5728\u56f0\u96be\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u66f4\u7a33\u5065\uff0cF1\u5206\u6570\u8fbe85.5%", "motivation": "\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u5e73\u884c\u8bed\u6599\u5e93\u7a00\u7f3a\uff0c\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u662f\u7b80\u5355\u7684\u4e00\u5bf9\u4e00\u6620\u5c04\uff0c\u7f3a\u4e4f\u590d\u6742\u6587\u672c\u7684\u5bf9\u9f50\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u548c\u7ffb\u8bd1\u6559\u5b66\u7684\u53d1\u5c55", "method": "\u63d0\u51faAlignAR\u751f\u6210\u5f0f\u53e5\u5b50\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6784\u5efa\u5305\u542b\u590d\u6742\u6cd5\u5f8b\u548c\u6587\u5b66\u6587\u672c\u7684\u65b0\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u51cf\u5c11\u4e00\u5bf9\u4e00\u6620\u5c04\u521b\u5efa\"\u56f0\u96be\"\u5b50\u96c6\u6765\u8bc4\u4f30\u4e0d\u540c\u5bf9\u9f50\u65b9\u6cd5", "result": "\"\u7b80\u5355\"\u6570\u636e\u96c6\u7f3a\u4e4f\u533a\u5206\u5bf9\u9f50\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\"\u56f0\u96be\"\u5b50\u96c6\u66b4\u9732\u4e86\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u6574\u4f53F1\u5206\u6570\u8fbe\u523085.5%\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u53479%", "conclusion": "\u9700\u8981\u66f4\u590d\u6742\u7684\u6570\u636e\u96c6\u6765\u5145\u5206\u8bc4\u4f30\u53e5\u5b50\u5bf9\u9f50\u65b9\u6cd5\uff0c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5bf9\u9f50\u4efb\u52a1\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u5f00\u6e90\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u53d1\u5c55"}}
{"id": "2512.21849", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21849", "abs": "https://arxiv.org/abs/2512.21849", "authors": ["Jiaxin Liu", "Peiyi Tu", "Wenyu Chen", "Yihong Zhuang", "Xinxia Ling", "Anji Zhou", "Chenxi Wang", "Zhuo Han", "Zhengkai Yang", "Junbo Zhao", "Zenan Huang", "Yuanyuan Wang"], "title": "HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs", "comment": "10 pages", "summary": "While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.", "AI": {"tldr": "HeartBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u4e2d\u6587\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u3001\u6587\u5316\u548c\u4f26\u7406\u7ef4\u5ea6\u4e0a\u7c7b\u4eba\u667a\u80fd\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u771f\u5b9e\u5fc3\u7406\u54a8\u8be2\u573a\u666f\u5f00\u53d1\uff0c\u5305\u542b5\u4e2a\u4e3b\u8981\u7ef4\u5ea6\u548c15\u9879\u6b21\u7ea7\u80fd\u529b\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u4ec5\u80fd\u8fbe\u5230\u4e13\u5bb6\u7406\u60f3\u5206\u6570\u768460%\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7c7b\u4eba\u667a\u80fd\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4e2d\u6587\u8bed\u8a00\u6587\u5316\u80cc\u666f\u4e0b\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u9ad8\u8d28\u91cf\u7684\u793e\u4f1a\u60c5\u611f\u6570\u636e\u963b\u788d\u4e86\u8fdb\u5c55\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u5fc3\u7406\u54a8\u8be2\u573a\u666f\uff0c\u4e0e\u4e34\u5e8a\u4e13\u5bb6\u5408\u4f5c\u5f00\u53d1\u7406\u8bba\u9a71\u52a8\u7684\u5206\u7c7b\u6cd5\uff085\u4e2a\u4e3b\u8981\u7ef4\u5ea6\uff0c15\u4e2a\u6b21\u7ea7\u80fd\u529b\uff09\uff0c\u91c7\u7528\u6848\u4f8b\u7279\u5b9a\u3001\u57fa\u4e8e\u91cf\u89c4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\"\u5148\u63a8\u7406\u540e\u8bc4\u5206\"\u7684\u8bc4\u4f30\u534f\u8bae\u5c06\u62bd\u8c61\u7684\u4eba\u7c7b\u7279\u8d28\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u7684\u6807\u51c6\u3002", "result": "\u8bc4\u4f3013\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u663e\u793a\u5b58\u5728\u663e\u8457\u6027\u80fd\u4e0a\u9650\uff1a\u5373\u4f7f\u9886\u5148\u6a21\u578b\u4e5f\u53ea\u80fd\u8fbe\u5230\u4e13\u5bb6\u5b9a\u4e49\u7406\u60f3\u5206\u6570\u768460%\u3002\u5728\u6d89\u53ca\u5fae\u5999\u60c5\u611f\u6f5c\u53f0\u8bcd\u548c\u590d\u6742\u4f26\u7406\u6743\u8861\u7684\"\u56f0\u96be\u96c6\"\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "HeartBench\u4e3a\u7c7b\u4ebaAI\u8bc4\u4f30\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u6307\u6807\uff0c\u5e76\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u84dd\u56fe\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u60c5\u611f\u3001\u6587\u5316\u548c\u4f26\u7406\u667a\u80fd\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.21859", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21859", "abs": "https://arxiv.org/abs/2512.21859", "authors": ["Qi Fan", "An Zou", "Yehan Ma"], "title": "TimeBill: Time-Budgeted Inference for Large Language Models", "comment": "Accepted to AAAI 2026", "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.", "AI": {"tldr": "TimeBill\u662f\u4e00\u4e2a\u9762\u5411\u65f6\u95f4\u9884\u7b97\u7684LLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u54cd\u5e94\u957f\u5ea6\u548c\u6267\u884c\u65f6\u95f4\uff0c\u81ea\u9002\u5e94\u8c03\u6574KV\u7f13\u5b58\u6dd8\u6c70\u6bd4\u4f8b\uff0c\u5728\u7ed9\u5b9a\u65f6\u95f4\u9884\u7b97\u5185\u5e73\u8861\u63a8\u7406\u6548\u7387\u548c\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u65f6\u95f4\u5173\u952e\u7cfb\u7edf\u4e2d\uff08\u5982\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\uff09\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u9700\u8981\u5728\u7ed9\u5b9a\u65f6\u95f4\u9884\u7b97\u5185\u751f\u6210\u51c6\u786e\u54cd\u5e94\u3002\u7136\u800c\uff0c\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4f7f\u5f97\u7aef\u5230\u7aef\u6267\u884c\u65f6\u95f4\u96be\u4ee5\u5efa\u6a21\u548c\u4f30\u8ba1\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u56fa\u5b9aKV\u7f13\u5b58\u6dd8\u6c70\u6bd4\u4f8b\u7684\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u5177\u6709\u4e0d\u540c\u65f6\u95f4\u9884\u7b97\u7684\u591a\u6837\u5316\u4efb\u52a1\u3002", "method": "\u63d0\u51faTimeBill\u6846\u67b6\uff1a1\uff09\u7ec6\u7c92\u5ea6\u54cd\u5e94\u957f\u5ea6\u9884\u6d4b\u5668\uff08RLP\uff09\u9884\u6d4bLLM\u8f93\u51fa\u957f\u5ea6\uff1b2\uff09\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u5668\uff08ETE\uff09\u51c6\u786e\u9884\u6d4b\u7aef\u5230\u7aef\u6267\u884c\u65f6\u95f4\uff1b3\uff09\u57fa\u4e8e\u65f6\u95f4\u9884\u7b97\u7684\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\uff0c\u6839\u636e\u6267\u884c\u65f6\u95f4\u9884\u6d4b\u548c\u7ed9\u5b9a\u65f6\u95f4\u9884\u7b97\u81ea\u9002\u5e94\u8c03\u6574KV\u7f13\u5b58\u6dd8\u6c70\u6bd4\u4f8b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eTimeBill\u5728\u5404\u79cd\u8d85\u65f6\u7b56\u7565\u4e0b\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\u5e76\u4fdd\u6301\u54cd\u5e94\u6027\u80fd\u7684\u4f18\u52bf\u3002", "conclusion": "TimeBill\u662f\u4e00\u4e2a\u6709\u6548\u7684\u65f6\u95f4\u9884\u7b97\u63a8\u7406\u6846\u67b6\uff0c\u80fd\u591f\u5728\u65f6\u95f4\u5173\u952e\u7cfb\u7edf\u4e2d\u5e73\u8861LLM\u63a8\u7406\u6548\u7387\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7684\u65f6\u95f4\u9884\u7b97\u8981\u6c42\u3002"}}
{"id": "2512.21871", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.21871", "abs": "https://arxiv.org/abs/2512.21871", "authors": ["Naen Xu", "Jinghuai Zhang", "Changjiang Li", "Hengyu An", "Chunyi Zhou", "Jun Wang", "Boyu Xu", "Yuyuan Li", "Tianyu Du", "Shouling Ji"], "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?", "comment": "AAAI 2026 (Oral)", "summary": "Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u7248\u6743\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u5de5\u5177\u589e\u5f3a\u7684\u9632\u5fa1\u6846\u67b6\u6765\u964d\u4f4e\u4fb5\u6743\u98ce\u9669\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5e7f\u6cdb\u53ef\u8bbf\u95ee\u6027\u5f15\u53d1\u4e86\u7248\u6743\u4fb5\u6743\u7684\u5173\u952e\u62c5\u5fe7\u3002\u6a21\u578b\u5728\u9047\u5230\u7248\u6743\u5185\u5bb9\u65f6\u80fd\u5426\u51c6\u786e\u8bc6\u522b\u5e76\u9075\u5b88\u7248\u6743\u6cd5\u89c4\uff0c\u907f\u514d\u4ea7\u751f\u6cd5\u5f8b\u548c\u4f26\u7406\u540e\u679c\uff0c\u662f\u672c\u7814\u7a76\u7684\u4e3b\u8981\u52a8\u673a\u3002", "method": "1. \u521b\u5efa\u5305\u542b50,000\u4e2a\u591a\u6a21\u6001\u67e5\u8be2-\u5185\u5bb9\u5bf9\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e66\u7c4d\u6458\u5f55\u3001\u65b0\u95fb\u6587\u7ae0\u3001\u97f3\u4e50\u6b4c\u8bcd\u548c\u4ee3\u7801\u6587\u6863\u7b49\u7248\u6743\u5185\u5bb9\uff1b2. \u6570\u636e\u96c6\u5305\u542b\u6709\u7248\u6743\u58f0\u660e\u548c\u65e0\u7248\u6743\u58f0\u660e\u4e24\u79cd\u573a\u666f\uff1b3. \u5f15\u5165\u5de5\u5177\u589e\u5f3a\u7684\u9632\u5fa1\u6846\u67b6\u6765\u786e\u4fdd\u7248\u6743\u5408\u89c4\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u95ed\u6e90\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u548c\u5c0a\u91cd\u7248\u6743\u5185\u5bb9\u65b9\u9762\u4e5f\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u5373\u4f7f\u63d0\u4f9b\u4e86\u7248\u6743\u58f0\u660e\u3002\u63d0\u51fa\u7684\u5de5\u5177\u589e\u5f3a\u9632\u5fa1\u6846\u67b6\u5728\u6240\u6709\u573a\u666f\u4e2d\u90fd\u80fd\u6709\u6548\u964d\u4f4e\u4fb5\u6743\u98ce\u9669\u3002", "conclusion": "\u5f00\u53d1\u7248\u6743\u611f\u77e5\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u4e8e\u786e\u4fdd\u8d1f\u8d23\u4efb\u548c\u5408\u6cd5\u4f7f\u7528\u7248\u6743\u5185\u5bb9\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u6a21\u578b\u5728\u7248\u6743\u5408\u89c4\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u9632\u5fa1\u673a\u5236\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2512.21877", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21877", "abs": "https://arxiv.org/abs/2512.21877", "authors": ["Vaibhav Devraj", "Dhruv Kumar", "Jagat Sesh Challa"], "title": "CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics", "comment": "Under Review", "summary": "Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a \"Gold Standard\" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.", "AI": {"tldr": "CricBench\uff1a\u9996\u4e2a\u9488\u5bf9\u677f\u7403\u6570\u636e\u5206\u6790\u7684\u6587\u672c\u5230SQL\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u82f1\u8bed\u548c\u5370\u5730\u8bed\u67e5\u8be2\uff0c\u8bc4\u4f30\u53d1\u73b0\u4e13\u4e1a\u9886\u57df\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u5370\u5730\u8bed\u67e5\u8be2\u8868\u73b0\u4e0e\u82f1\u8bed\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002", "motivation": "\u677f\u7403\u4f5c\u4e3a\u5168\u7403\u7b2c\u4e8c\u5927\u8fd0\u52a8\uff0c\u62e5\u6709\u8d85\u8fc725\u4ebf\u7c89\u4e1d\uff0c\u4f46\u73b0\u6709LLM\u5728\u5904\u7406\u677f\u7403\u9886\u57df\u7279\u5b9a\u590d\u6742\u67e5\u8be2\u65f6\u5b58\u5728\u80fd\u529b\u5dee\u8ddd\u3002\u9700\u8981\u8bc4\u4f30LLM\u5728\u4e13\u4e1a\u4f53\u80b2\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5904\u7406\u9886\u57df\u7279\u5b9a\u6a21\u5f0f\u3001\u590d\u6742\u6a21\u5f0f\u53d8\u5316\u548c\u591a\u8bed\u8a00\u9700\u6c42\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efaCricBench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff1a1\uff09\u4e0e\u677f\u7403\u548cSQL\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u624b\u52a8\u7f16\u5199\u590d\u6742\u67e5\u8be2\u4f5c\u4e3a\"\u9ec4\u91d1\u6807\u51c6\"\u6570\u636e\u96c6\uff1b2\uff09\u6784\u5efa\u82f1\u8bed\u548c\u5370\u5730\u8bed\u53cc\u8bed\u57fa\u51c6\uff1b3\uff09\u8bc4\u4f306\u4e2aSOTA\u6a21\u578b\uff08\u5305\u62ecGPT-4o\u3001Claude 3.7 Sonnet\u548c\u5f00\u6e90\u6a21\u578b\uff09\uff1b4\uff09\u91c7\u7528\u4e25\u683c\u8bc4\u4f30\u534f\u8bae\u3002", "result": "1\uff09\u901a\u7528\u57fa\u51c6\u7684\u9ad8\u6027\u80fd\u4e0d\u80fd\u4fdd\u8bc1\u4e13\u4e1a\u9886\u57df\u6210\u529f\uff1b2\uff09\u5f00\u6e90\u63a8\u7406\u6a21\u578bDeepSeek R1\u8868\u73b0\u6700\u4f73\uff0850.6%\uff09\uff0c\u8d85\u8fc7Claude 3.7 Sonnet\uff0847.7%\uff09\u548cGPT-4o\uff0833.7%\uff09\uff1b3\uff09\u4ece\u901a\u7528\u57fa\u51c6\uff08BIRD\uff09\u5230CricBench\u65f6\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\uff1b4\uff09\u5370\u5730\u8bed\u6df7\u5408\u67e5\u8be2\u8868\u73b0\u4e0e\u82f1\u8bed\u76f8\u5f53\u751a\u81f3\u66f4\u597d\uff0c\u6311\u6218\u4e86\u82f1\u8bed\u662f\u6700\u4f73\u63d0\u793a\u8bed\u8a00\u7684\u5047\u8bbe\u3002", "conclusion": "LLM\u5728\u4e13\u4e1a\u4f53\u80b2\u5206\u6790\u9886\u57df\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\uff0c\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u591a\u8bed\u8a00\u652f\u6301\u5728\u4e13\u4e1aSQL\u4efb\u52a1\u4e2d\u5f88\u91cd\u8981\uff0c\u82f1\u8bed\u4e0d\u4e00\u5b9a\u662f\u6700\u4f73\u63d0\u793a\u8bed\u8a00\u3002\u5f00\u6e90\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u53ef\u4ee5\u8d85\u8d8a\u4e13\u6709\u6a21\u578b\u3002"}}
{"id": "2512.21902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21902", "abs": "https://arxiv.org/abs/2512.21902", "authors": ["Sachin Pawar", "Girish Keshav Palshikar", "Anindita Sinha Banerjee", "Nitin Ramrakhiyani", "Basit Ali"], "title": "Explainable Statute Prediction via Attention-based Model and LLM Prompting", "comment": null, "summary": "In this paper, we explore the problem of automatic statute prediction where for a given case description, a subset of relevant statutes are to be predicted. Here, the term \"statute\" refers to a section, a sub-section, or an article of any specific Act. Addressing this problem would be useful in several applications such as AI-assistant for lawyers and legal question answering system. For better user acceptance of such Legal AI systems, we believe the predictions should also be accompanied by human understandable explanations. We propose two techniques for addressing this problem of statute prediction with explanations -- (i) AoS (Attention-over-Sentences) which uses attention over sentences in a case description to predict statutes relevant for it and (ii) LLMPrompt which prompts an LLM to predict as well as explain relevance of a certain statute. AoS uses smaller language models, specifically sentence transformers and is trained in a supervised manner whereas LLMPrompt uses larger language models in a zero-shot manner and explores both standard as well as Chain-of-Thought (CoT) prompting techniques. Both these models produce explanations for their predictions in human understandable forms. We compare statute prediction performance of both the proposed techniques with each other as well as with a set of competent baselines, across two popular datasets. Also, we evaluate the quality of the generated explanations through an automated counter-factual manner as well as through human evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u81ea\u52a8\u6cd5\u89c4\u9884\u6d4b\u4e0e\u89e3\u91ca\u65b9\u6cd5\uff1aAoS\uff08\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u53e5\u5b50\u7ea7\u6a21\u578b\uff09\u548cLLMPrompt\uff08\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff09\uff0c\u7528\u4e8e\u6839\u636e\u6848\u4ef6\u63cf\u8ff0\u9884\u6d4b\u76f8\u5173\u6cd5\u89c4\u6761\u6b3e\u5e76\u63d0\u4f9b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002", "motivation": "\u81ea\u52a8\u6cd5\u89c4\u9884\u6d4b\u5bf9\u4e8e\u6cd5\u5f8bAI\u52a9\u624b\u548c\u95ee\u7b54\u7cfb\u7edf\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u7528\u6237\u63a5\u53d7\u5ea6\uff0c\u9884\u6d4b\u7ed3\u679c\u9700\u8981\u9644\u5e26\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) AoS\uff1a\u4f7f\u7528\u53e5\u5b50\u8f6c\u6362\u5668\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5728\u6848\u4ef6\u63cf\u8ff0\u7684\u53e5\u5b50\u4e0a\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\uff1b2) LLMPrompt\uff1a\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u63a2\u7d22\u6807\u51c6\u63d0\u793a\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u6280\u672f\uff0c\u540c\u65f6\u751f\u6210\u89e3\u91ca\u3002", "result": "\u5728\u4e24\u4e2a\u6d41\u884c\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e86\u4e24\u79cd\u65b9\u6cd5\u4e0e\u57fa\u51c6\u6a21\u578b\u7684\u6cd5\u89c4\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u548c\u4eba\u5de5\u8bc4\u4f30\u8bc4\u4f30\u4e86\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u5728\u9884\u6d4b\u6cd5\u89c4\u7684\u540c\u65f6\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u4e3a\u6cd5\u5f8bAI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21911", "abs": "https://arxiv.org/abs/2512.21911", "authors": ["Jikai Wang", "Jianchao Tan", "Yuxuan Hu", "Jiayu Qin", "Yerui Sun", "Yuchen Xie", "Xunliang Cai", "Juntao Li", "Min Zhang"], "title": "Accelerate Speculative Decoding with Sparse Computation in Verification", "comment": "Pre-print", "summary": "Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsification methods are designed primarily for standard token-by-token autoregressive decoding to remove substantial computational redundancy in LLMs. This work systematically adopts different sparse methods on the verification stage of the speculative decoding and identifies structured redundancy across multiple dimensions. Based on these observations, we propose a sparse verification framework that jointly sparsifies attention, FFN, and MoE components during the verification stage to reduce the dominant computation cost. The framework further incorporates an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without introducing additional training. Extensive experiments across summarization, question answering, and mathematical reasoning datasets demonstrate that the proposed methods achieve favorable efficiency-accuracy trade-offs, while maintaining stable acceptance length.", "AI": {"tldr": "\u63d0\u51fa\u7a00\u758f\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u7a00\u758f\u5316\u6ce8\u610f\u529b\u3001FFN\u548cMoE\u7ec4\u4ef6\u6765\u52a0\u901f\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u9a8c\u8bc1\u9636\u6bb5\uff0c\u51cf\u5c11\u8ba1\u7b97\u74f6\u9888", "motivation": "\u63a8\u6d4b\u89e3\u7801\u7684\u9a8c\u8bc1\u9636\u6bb5\u5df2\u6210\u4e3a\u4e3b\u8981\u8ba1\u7b97\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u548cMoE\u6a21\u578b\u4e2d\uff0c\u73b0\u6709\u7a00\u758f\u5316\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6807\u51c6\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u4e0d\u9002\u7528\u4e8e\u9a8c\u8bc1\u9636\u6bb5", "method": "\u7cfb\u7edf\u5206\u6790\u9a8c\u8bc1\u9636\u6bb5\u7684\u591a\u7ef4\u7ed3\u6784\u5316\u5197\u4f59\uff0c\u63d0\u51fa\u8054\u5408\u7a00\u758f\u5316\u6ce8\u610f\u529b\u3001FFN\u548cMoE\u7ec4\u4ef6\u7684\u7a00\u758f\u9a8c\u8bc1\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u8de8\u8349\u7a3f\u4ee4\u724c\u548c\u8de8\u5c42\u68c0\u7d22\u91cd\u7528\u7b56\u7565", "result": "\u5728\u6458\u8981\u3001\u95ee\u7b54\u548c\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u63a5\u53d7\u957f\u5ea6", "conclusion": "\u63d0\u51fa\u7684\u7a00\u758f\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u63a8\u6d4b\u89e3\u7801\u9a8c\u8bc1\u9636\u6bb5\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u548cMoE\u6a21\u578b\u7684\u52a0\u901f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.21919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21919", "abs": "https://arxiv.org/abs/2512.21919", "authors": ["KaShun Shum", "Binyuan Hui", "Jiawei Chen", "Lei Zhang", "X. W.", "Jiaxi Yang", "Yuzhen Huang", "Junyang Lin", "Junxian He"], "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents", "comment": "21 pages", "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSWE-RM\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5728\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6267\u884c\u53cd\u9988\u7a00\u758f\u6027\u548c\u6d4b\u8bd5\u7528\u4f8b\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u7684\u6267\u884c\u53cd\u9988\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u9700\u8981\u5927\u91cf\u53ef\u9760\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u53cd\u9988\u7a00\u758f\u4e14\u96be\u4ee5\u533a\u5206\u76f8\u4f3c\u8f68\u8ff9\uff1b2\uff09\u6267\u884c\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\u53cd\u9988\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u6027\u80fd\u76f8\u4f3c\u7684\u9a8c\u8bc1\u5668\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u5dee\u5f02\u5f88\u5927\uff0c\u8868\u660e\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u63d0\u51faSWE-RM\u5956\u52b1\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff08\u603b\u53c2\u6570\u91cf30B\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b3B\uff09\u3002\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u5206\u6790\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3001\u7b56\u7565\u6df7\u5408\u3001\u6570\u636e\u6e90\u7ec4\u6210\u7b49\u56e0\u7d20\u5bf9\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u8bad\u7ec3\u51fa\u5728\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u90fd\u80fd\u7a33\u5065\u8868\u73b0\u7684\u5956\u52b1\u6a21\u578b\u3002", "result": "SWE-RM\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u6027\u80fd\uff1a\u5728SWE-Bench Verified\u4e0a\uff0cQwen3-Coder-Flash\u51c6\u786e\u7387\u4ece51.6%\u63d0\u5347\u523062.0%\uff0cQwen3-Coder-Max\u4ece67.0%\u63d0\u5347\u523074.6%\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u6267\u884c\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\u53cd\u9988\u80fd\u591f\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u4fe1\u53f7\uff0c\u4e14\u4e0d\u4f9d\u8d56\u5355\u5143\u6d4b\u8bd5\u3002\u901a\u8fc7\u5173\u6ce8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u7b49\u5173\u952e\u6307\u6807\uff0c\u53ef\u4ee5\u8bad\u7ec3\u51fa\u5728\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u90fd\u80fd\u6709\u6548\u5de5\u4f5c\u7684\u7a33\u5065\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u6027\u80fd\u3002"}}
{"id": "2512.21933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21933", "abs": "https://arxiv.org/abs/2512.21933", "authors": ["Sachin Pawar", "Manoj Apte", "Kshitij Jadhav", "Girish Keshav Palshikar", "Nitin Ramrakhiyani"], "title": "Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs", "comment": "International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2025)", "summary": "Tokenization is the first step in training any Large Language Model (LLM), where the text is split into a sequence of tokens as per the model's fixed vocabulary. This tokenization in LLMs is different from the traditional tokenization in NLP where the text is split into a sequence of \"natural\" words. In LLMs, a natural word may also be broken into multiple tokens due to limited vocabulary size of the LLMs (e.g., Mistral's tokenizer splits \"martial\" into \"mart\" and \"ial\"). In this paper, we hypothesize that such breaking of natural words negatively impacts LLM performance on various NLP tasks. To quantify this effect, we propose a set of penalty functions that compute a tokenization penalty for a given text for a specific LLM, indicating how \"bad\" the tokenization is. We establish statistical significance of our hypothesis on multiple NLP tasks for a set of different LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faLLM\u5206\u8bcd\u8fc7\u7a0b\u4e2d\u81ea\u7136\u8bcd\u88ab\u62c6\u5206\u4f1a\u635f\u5bb3\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u5957\u60e9\u7f5a\u51fd\u6570\u6765\u91cf\u5316\u8fd9\u79cd\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "LLM\u5206\u8bcd\u4e0e\u4f20\u7edfNLP\u5206\u8bcd\u4e0d\u540c\uff0c\u7531\u4e8e\u8bcd\u6c47\u8868\u6709\u9650\uff0c\u81ea\u7136\u8bcd\u5e38\u88ab\u62c6\u5206\u4e3a\u591a\u4e2atoken\u3002\u4f5c\u8005\u5047\u8bbe\u8fd9\u79cd\u62c6\u5206\u4f1a\u5bf9LLM\u5728\u5404\u79cdNLP\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957\u60e9\u7f5a\u51fd\u6570\u6765\u8ba1\u7b97\u7ed9\u5b9a\u6587\u672c\u5728\u7279\u5b9aLLM\u4e0a\u7684\u5206\u8bcd\u60e9\u7f5a\uff0c\u91cf\u5316\u5206\u8bcd\"\u7cdf\u7cd5\"\u7a0b\u5ea6\u3002\u5728\u591a\u4e2aNLP\u4efb\u52a1\u548c\u4e0d\u540cLLM\u4e0a\u9a8c\u8bc1\u5047\u8bbe\u7684\u7edf\u8ba1\u663e\u8457\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u5206\u8bcd\u60e9\u7f5a\u4e0eLLM\u6027\u80fd\u8d1f\u76f8\u5173\u7684\u7edf\u8ba1\u663e\u8457\u6027\u8bc1\u636e\uff0c\u8868\u660e\u81ea\u7136\u8bcd\u62c6\u5206\u786e\u5b9e\u635f\u5bb3\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "LLM\u5206\u8bcd\u8fc7\u7a0b\u4e2d\u81ea\u7136\u8bcd\u88ab\u62c6\u5206\u4f1a\u8d1f\u9762\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u60e9\u7f5a\u51fd\u6570\u80fd\u6709\u6548\u91cf\u5316\u8fd9\u79cd\u5f71\u54cd\uff0c\u4e3a\u6539\u8fdb\u5206\u8bcd\u7b56\u7565\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2512.21956", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21956", "abs": "https://arxiv.org/abs/2512.21956", "authors": ["Tal Halevi", "Yarden Tzach", "Ronit D. Gross", "Shalom Rosner", "Ido Kanter"], "title": "Self-attention vector output similarities reveal how machines pay attention", "comment": "22 pages, 13 figures", "summary": "The self-attention mechanism has significantly advanced the field of natural language processing, facilitating the development of advanced language-learning machines. Although its utility is widely acknowledged, the precise mechanisms of self-attention underlying its advanced learning and the quantitative characterization of this learning process remains an open research question. This study introduces a new approach for quantifying information processing within the self-attention mechanism. The analysis conducted on the BERT-12 architecture reveals that, in the final layers, the attention map focuses on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features. Based on the vector space emerging from the self-attention heads, a context similarity matrix, measuring the scalar product between two token vectors was derived, revealing distinct similarities between different token vector pairs within each head and layer. The findings demonstrated that different attention heads within an attention block focused on different linguistic characteristics, such as identifying token repetitions in a given text or recognizing a token of common appearance in the text and its surrounding context. This specialization is also reflected in the distribution of distances between token vectors with high similarity as the architecture progresses. The initial attention layers exhibit substantially long-range similarities; however, as the layers progress, a more short-range similarity develops, culminating in a preference for attention heads to create strong similarities within the same sentence. Finally, the behavior of individual heads was analyzed by examining the uniqueness of their most common tokens in their high similarity elements. Each head tends to focus on a unique token from the text and builds similarity pairs centered around it.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4fe1\u606f\u5904\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790BERT-12\u67b6\u6784\u53d1\u73b0\uff1a\u6df1\u5c42\u6ce8\u610f\u529b\u805a\u7126\u4e8e\u53e5\u5b50\u5206\u9694\u7b26\uff0c\u53ef\u7528\u4e8e\u57fa\u4e8e\u8bed\u4e49\u7279\u5f81\u7684\u6587\u672c\u5206\u5272\uff1b\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u4e13\u6ce8\u4e8e\u4e0d\u540c\u8bed\u8a00\u7279\u5f81\uff1b\u968f\u7740\u7f51\u7edc\u5c42\u6570\u52a0\u6df1\uff0c\u76f8\u4f3c\u6027\u4ece\u957f\u8ddd\u79bb\u8f6c\u5411\u77ed\u8ddd\u79bb\uff0c\u6700\u7ec8\u504f\u597d\u540c\u4e00\u53e5\u5b50\u5185\u7684\u5f3a\u76f8\u4f3c\u6027\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u5177\u4f53\u7684\u5b66\u4e60\u673a\u5236\u548c\u91cf\u5316\u8868\u5f81\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b9\u6cd5\u6765\u91cf\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5185\u90e8\u7684\u4fe1\u606f\u5904\u7406\u8fc7\u7a0b\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4fe1\u606f\u5904\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u5934\u4ea7\u751f\u7684\u5411\u91cf\u7a7a\u95f4\u6784\u5efa\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u77e9\u9635\uff08\u6d4b\u91cf\u4e24\u4e2a\u6807\u8bb0\u5411\u91cf\u4e4b\u95f4\u7684\u6807\u91cf\u79ef\uff09\u3002\u5728BERT-12\u67b6\u6784\u4e0a\u8fdb\u884c\u5206\u6790\uff0c\u7814\u7a76\u6ce8\u610f\u529b\u56fe\u3001\u76f8\u4f3c\u6027\u5206\u5e03\u548c\u4e2a\u4f53\u5934\u7684\u7279\u6027\u3002", "result": "1. \u6df1\u5c42\u6ce8\u610f\u529b\u805a\u7126\u4e8e\u53e5\u5b50\u5206\u9694\u7b26\u6807\u8bb0\uff0c\u53ef\u7528\u4e8e\u57fa\u4e8e\u8bed\u4e49\u7279\u5f81\u7684\u6587\u672c\u5206\u5272\n2. \u4e0d\u540c\u6ce8\u610f\u529b\u5934\u4e13\u6ce8\u4e8e\u4e0d\u540c\u8bed\u8a00\u7279\u5f81\uff08\u5982\u8bc6\u522b\u6807\u8bb0\u91cd\u590d\u3001\u8bc6\u522b\u5e38\u89c1\u6807\u8bb0\u53ca\u5176\u4e0a\u4e0b\u6587\uff09\n3. \u76f8\u4f3c\u6027\u5206\u5e03\u968f\u7f51\u7edc\u5c42\u6570\u53d8\u5316\uff1a\u521d\u59cb\u5c42\u663e\u793a\u957f\u8ddd\u79bb\u76f8\u4f3c\u6027\uff0c\u6df1\u5c42\u53d1\u5c55\u51fa\u77ed\u8ddd\u79bb\u76f8\u4f3c\u6027\uff0c\u6700\u7ec8\u504f\u597d\u540c\u4e00\u53e5\u5b50\u5185\u7684\u5f3a\u76f8\u4f3c\u6027\n4. \u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u503e\u5411\u4e8e\u805a\u7126\u4e8e\u6587\u672c\u4e2d\u7684\u72ec\u7279\u6807\u8bb0\uff0c\u5e76\u56f4\u7ed5\u8be5\u6807\u8bb0\u6784\u5efa\u76f8\u4f3c\u6027\u5bf9", "conclusion": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u4e0d\u540c\u5934\u7684\u4e13\u4e1a\u5316\u5206\u5de5\u5904\u7406\u4e0d\u540c\u8bed\u8a00\u7279\u5f81\uff0c\u968f\u7740\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\uff0c\u4ece\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u9010\u6e10\u8f6c\u5411\u5904\u7406\u5c40\u90e8\u8bed\u4e49\u5173\u7cfb\uff0c\u6700\u7ec8\u5b9e\u73b0\u53e5\u5b50\u7ea7\u522b\u7684\u8bed\u4e49\u6574\u5408\u3002\u8fd9\u79cd\u91cf\u5316\u5206\u6790\u65b9\u6cd5\u4e3a\u7406\u89e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u5de5\u4f5c\u539f\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2512.22087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22087", "abs": "https://arxiv.org/abs/2512.22087", "authors": ["Shukai Liu", "Jian Yang", "Bo Jiang", "Yizhi Li", "Jinyang Guo", "Xianglong Liu", "Bryan Dai"], "title": "Context as a Tool: Context Management for Long-Horizon SWE-Agents", "comment": null, "summary": "Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.", "AI": {"tldr": "CAT\u662f\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u8303\u5f0f\uff0c\u5c06\u4e0a\u4e0b\u6587\u7ef4\u62a4\u63d0\u5347\u4e3a\u53ef\u8c03\u7528\u7684\u5de5\u5177\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u7684\u957f\u65f6\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5728\u5904\u7406\u4ed3\u5e93\u7ea7\u4ee3\u7801\u5e93\u7684\u957f\u65f6\u4ea4\u4e92\u4efb\u52a1\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u8ffd\u52a0\u5f0f\u4e0a\u4e0b\u6587\u7ef4\u62a4\u6216\u88ab\u52a8\u89e6\u53d1\u7684\u538b\u7f29\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4e0a\u4e0b\u6587\u7206\u70b8\u3001\u8bed\u4e49\u6f02\u79fb\u548c\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u63d0\u51faCAT\u8303\u5f0f\uff0c\u5c06\u4e0a\u4e0b\u6587\u7ef4\u62a4\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\u96c6\u6210\u5230\u4ee3\u7406\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff1b\u8bbe\u8ba1\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u4f5c\u7a7a\u95f4\uff08\u7a33\u5b9a\u4efb\u52a1\u8bed\u4e49\u3001\u538b\u7f29\u957f\u671f\u8bb0\u5fc6\u3001\u9ad8\u4fdd\u771f\u77ed\u671f\u4ea4\u4e92\uff09\uff1b\u5f00\u53d1CAT-GENERATOR\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u6570\u636e\u6784\u5efa\u7ba1\u9053\u6ce8\u5165\u4e0a\u4e0b\u6587\u7ba1\u7406\u52a8\u4f5c\uff1b\u8bad\u7ec3\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578bSWE-Compressor\u3002", "result": "\u5728SWE-Bench-Verified\u4e0a\uff0cSWE-Compressor\u8fbe\u523057.6%\u7684\u89e3\u51b3\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eReAct\u7684\u4ee3\u7406\u548c\u9759\u6001\u538b\u7f29\u57fa\u7ebf\uff0c\u540c\u65f6\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\u4fdd\u6301\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\u7684\u957f\u65f6\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CAT\u8303\u5f0f\u901a\u8fc7\u4e3b\u52a8\u3001\u7ed3\u6784\u5316\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u957f\u65f6\u4ea4\u4e92\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.22100", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22100", "abs": "https://arxiv.org/abs/2512.22100", "authors": ["Duygu Altinok"], "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis", "comment": "under review by Springer", "summary": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TrGLUE\u2014\u2014\u9996\u4e2a\u571f\u8033\u5176\u8bed\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7efc\u5408\u57fa\u51c6\uff0c\u4ee5\u53caSentiTurca\u60c5\u611f\u5206\u6790\u4e13\u7528\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u571f\u8033\u5176\u8bedNLU\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u867d\u7136\u82f1\u8bed\u6709GLUE\u57fa\u51c6\uff0c\u5176\u4ed6\u8bed\u8a00\u5982\u4e2d\u6587\u3001\u6cd5\u8bed\u3001\u65e5\u8bed\u4e5f\u6709\u76f8\u5e94\u57fa\u51c6\uff0c\u4f46\u571f\u8033\u5176\u8bed\u7f3a\u4e4f\u7c7b\u4f3c\u7684\u5168\u9762NLU\u8bc4\u4f30\u57fa\u51c6\u3002\u9700\u8981\u5efa\u7acb\u80fd\u591f\u4ece\u591a\u7ef4\u5ea6\u8bc4\u4f30\u571f\u8033\u5176\u8bed\u6a21\u578b\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u521b\u5efaTrGLUE\u57fa\u51c6\uff0c\u5305\u542b\u571f\u8033\u5176\u672c\u571f\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff1a\u7ed3\u5408\u5f3aLLM\u6807\u6ce8\u3001\u8de8\u6a21\u578b\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u4eba\u5de5\u9a8c\u8bc1\u3002\u540c\u65f6\u63d0\u4f9b\u57fa\u4e8eTransformer\u6a21\u578b\u7684\u5fae\u8c03\u548c\u8bc4\u4f30\u4ee3\u7801\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86TrGLUE\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cdNLU\u4efb\u52a1\uff0c\u4ee5\u53caSentiTurca\u60c5\u611f\u5206\u6790\u57fa\u51c6\u3002\u6807\u6ce8\u6d41\u7a0b\u4f18\u5148\u8003\u8651\u8bed\u8a00\u81ea\u7136\u6027\uff0c\u51cf\u5c11\u7ffb\u8bd1\u75d5\u8ff9\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "TrGLUE\u4e3a\u571f\u8033\u5176\u8bedNLU\u5efa\u7acb\u4e86\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u5e76\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u534a\u81ea\u52a8\u5316\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u586b\u8865\u4e86\u571f\u8033\u5176\u8bed\u8bc4\u4f30\u57fa\u51c6\u7684\u7a7a\u767d\u3002"}}
