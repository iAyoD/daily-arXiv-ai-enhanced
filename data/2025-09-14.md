<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: 提出了一种针对通信受限环境下分布式多智能体系统的任务分配方法，特别处理主动非对称障碍物和部分可观测环境


<details>
  <summary>Details</summary>
Motivation: 解决在通信带宽和载荷极其有限的情况下，多智能体系统在高度部分可观测环境中协调任务的挑战，特别是处理现实世界中常见的非对称障碍物问题

Method: 基于市场机制的任务分配方法，开发了新型分布式协调算法，专门考虑非对称障碍物特性，适用于低通信场景

Result: 在仿真和真实NAO机器人RoboCup比赛中验证，在有限通信环境下显著减少任务重叠，最频繁重新分配任务减少了52%

Conclusion: 该方法有效解决了通信受限、环境部分可观测且存在主动非对称障碍物的复杂多智能体协调问题，具有实际应用价值

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [2] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: 本文提出了一种基于3D纤维缠绕技术(3DFiT)制造轻量化面心立方晶格结构无人机框架的方法，使用连续单丝纤维确保精确纤维排列，消除了传统复合材料组装的弱点。


<details>
  <summary>Details</summary>
Motivation: 航空航天和机器人领域对轻量化高强度复合材料结构的需求日益增长，但传统复合材料制造方法难以实现复杂的3D架构，且组装接头存在弱点，连续纤维增强也面临挑战。

Method: 采用3D纤维缠绕技术(3DFiT)制造面心立方晶格结构的无人机框架，使用连续单丝纤维确保精确纤维排列，避免传统组装方法的弱点。

Result: 制造的无人机框架重量仅260克，比商用DJI F450框架轻10%，比强度和金属高4-8倍，飞行时间延长3分钟，飞行测试证实了其稳定性和耐久性。

Conclusion: 单丝晶格桁架无人机框架具有巨大潜力，3DFiT技术作为一种可扩展的高效制造方法，为轻量化复合材料结构制造提供了新途径。

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [3] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出基于流场的运动规划方法KoopMotion，使用Koopman算子参数化动力学系统，确保机器人从任意初始状态收敛到期望轨迹并跟踪至终点


<details>
  <summary>Details</summary>
Motivation: 现有Koopman算子理论在建模动力学系统方面有效，但无法保证收敛到期望轨迹或指定目标，这在从演示学习中是一个重要需求

Method: KoopMotion将运动流场表示为动力学系统，用Koopman算子参数化来模仿期望轨迹，利用学习流场的发散特性获得平滑运动场，使机器人能收敛到参考轨迹

Result: 在LASA手写数据集和3D机械臂末端轨迹数据集上评估，包括频谱分析；在物理机器人上验证，在非静态流体环境中运行；仅需3% LASA数据即可生成密集运动规划，在时空动力学建模指标上显著优于基线

Conclusion: KoopMotion方法具有高度样本效率，能有效生成收敛到期望轨迹的运动规划，在空间和时间动态建模方面表现优异

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [4] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: 提出了一种欠驱动变形加载机械臂(UMLM)，结合变形臂和被动自适应抓手，通过几何约束实现拓扑重构和灵活运动轨迹，无需额外执行器。


<details>
  <summary>Details</summary>
Motivation: 传统固定自由度加载机构存在执行器过多、控制复杂、对动态任务适应性有限等问题，需要开发更高效、适应性强的加载解决方案。

Method: 集成变形臂和被动自适应抓手，利用几何约束实现拓扑重构；建立结构模型并进行运动静力学分析；使用粒子群优化(PSO)优化抓手尺寸参数。

Result: 仿真验证了UMLM控制策略易于实现、操作灵活，在动态环境中能有效抓取各种物体，展现了优异的适应性和操作性能。

Conclusion: 欠驱动变形机构在需要高效、适应性加载解决方案的应用中具有实际潜力，所提出的通用建模和优化框架可扩展到更广泛的机械臂类别，为开发高效、灵活、性能稳健的机器人系统提供了可扩展方法。

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [5] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: 提出基于线性倒立摆模型(LIPM)的新型奖励设计，用于实现双足机器人在非结构化户外环境中的感知和稳定运动，通过双评论家架构和奖励融合模块提升地形适应性和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在复杂非结构化户外环境中因地形几何复杂和外部干扰而难以实现稳定感知运动的关键挑战。

Method: 基于LIPM理论设计奖励函数，调节质心高度和躯干姿态以保持动态平衡；采用奖励融合模块(RFM)自适应平衡速度跟踪和稳定性；使用双评论家架构分别评估稳定性和运动目标。

Result: 在仿真和真实户外环境中进行了广泛实验，结果显示该方法具有优异的地形适应性、干扰抑制能力，并在各种速度和感知条件下表现一致。

Conclusion: 提出的LIPM启发式奖励设计和双评论家架构有效提升了双足机器人在野外环境中的感知运动稳定性和鲁棒性。

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [6] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: AEOS是一个受猫头鹰主动感知启发的自适应LiDAR控制框架，结合模型预测控制和强化学习，在无人机LiDAR惯性里程计中显著提高了定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决无人机LiDAR感知的局限性：紧凑LiDAR传感器视野狭窄、负载限制无法使用多传感器配置，以及传统电机扫描系统缺乏场景感知和任务适应性。

Method: 采用混合架构：模型预测控制(MPC)预测未来位姿可观测性进行利用，轻量级神经网络从全景深度表示学习隐式成本图进行探索。开发了点云仿真环境支持可扩展训练和泛化。

Result: 在仿真和真实环境中的大量实验表明，AEOS相比固定速率、仅优化和完全学习基线方法显著提高了里程计精度，同时保持实时性能。

Conclusion: AEOS框架成功解决了无人机LiDAR感知的挑战，通过生物启发的自适应扫描策略实现了更好的环境感知和任务适应性，为复杂遮挡环境中的定位和建图提供了有效解决方案。

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [7] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: 提出了一种预测未来停车位占用情况的自动驾驶代客泊车框架，通过区分初始空置和占用车位、预测动态代理运动，结合概率估计和自适应规划策略，显著提升泊车效率和安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖瞬时观测或静态假设，无法在动态不确定环境中准确预测停车位可用性并进行集成规划，影响了自动驾驶代客泊车的安全性和效率。

Method: 开发概率车位占用估计器处理有限视野内的部分噪声观测，考虑未观测区域的不确定性演化；设计自适应策略规划器平衡目标导向泊车与信息增益探索，智能整合等待行为。

Result: 通过大型停车场随机模拟实验证明，该框架相比现有方法显著提高了泊车效率、安全裕度和轨迹平滑度。

Conclusion: 该研究为动态不确定环境中的自动驾驶代客泊车提供了有效的未来停车位预测和集成规划解决方案，具有重要的实际应用价值。

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [8] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: 提出RENet框架，通过双估计器架构解决四足机器人在户外环境中视觉定位的挑战，特别是在视觉感知退化时保持稳定运动性能


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在户外环境中基于视觉的运动控制面临的部署挑战，包括环境预测不准确和深度传感器噪声处理困难等问题

Method: 采用冗余估计器网络（RENet）框架，包含双估计器架构和在线估计器自适应机制，能够在视觉感知不确定性时实现估计模块间的无缝切换

Result: 在真实机器人上的实验验证表明，该框架在复杂户外环境中有效，特别是在视觉感知退化场景中表现出优势

Conclusion: 该框架为解决具有挑战性的野外条件下可靠机器人部署提供了实用解决方案，展示了在实际应用中的潜力

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [9] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: OmniEVA是一个多模态大语言模型驱动的具身智能规划器，通过任务自适应的3D接地机制和具身感知推理框架，解决了现有系统的几何适应性差距和具身约束差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的具身系统存在两个关键限制：几何适应性差距（2D输入或硬编码3D几何注入导致空间信息不足或泛化能力受限）和具身约束差距（忽视真实机器人的物理约束，导致计划理论上有效但实际不可行）。

Method: 提出两个关键创新：(1)任务自适应的3D接地机制，使用门控路由器根据上下文需求进行选择性3D融合调节；(2)具身感知推理框架，将任务目标和具身约束联合纳入推理循环。

Result: 实验结果表明OmniEVA在通用具身推理性能上达到最先进水平，在广泛的下游场景中表现出强大能力，在包括原始和复合任务的基准测试中证实了其稳健和通用的规划能力。

Conclusion: OmniEVA通过创新的3D接地和具身感知推理方法，有效解决了现有MLLM具身系统的局限性，为具身智能提供了更强大和实用的规划解决方案。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [10] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: AGILOped是一个开源人形机器人，旨在填补高性能与可访问性之间的差距，使用现成的驱动器和标准电子组件，便于单人操作，并通过多种实验验证了其研究可行性。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人系统多为闭源或成本高昂，限制了研究和应用的可及性。本文旨在开发一个开源、高性能且易于获取的人形机器人平台。

Method: 采用现成的可反向驱动驱动器（具有高功率密度）和标准电子组件，设计了一个高110厘米、重14.5公斤的机器人，无需龙门架即可由单人操作。

Result: 通过行走、跳跃、冲击缓解和起身等实验，证明了AGILOped在研究中具有可行性和实用性。

Conclusion: AGILOped成功实现了高性能与可访问性的平衡，为研究社区提供了一个开源、低成本且易于操作的人形机器人平台。

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [11] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: VLA-Adapter是一种新颖的视觉-语言-动作模型范式，通过轻量级策略模块和桥接注意力机制，无需大规模预训练即可高效连接视觉语言表示与动作空间。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型依赖大规模视觉语言模型和机器人数据预训练，成本高昂。本文旨在降低这种依赖，探索如何有效桥接视觉语言表示与动作空间。

Method: 提出VLA-Adapter框架，包括系统分析各种视觉语言条件的有效性，设计轻量级策略模块和桥接注意力机制，自动将最优条件注入动作空间。

Result: 仅使用0.5B参数主干网络，无需机器人数据预训练，在仿真和真实机器人基准测试中达到最先进性能，推理速度最快，单消费级GPU仅需8小时训练。

Conclusion: VLA-Adapter显著降低了VLA模型的部署门槛，提供高效、低成本且高性能的视觉-语言-动作建模解决方案。

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [12] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: 提出了一种疲劳感知的连续体机器人设计，通过混合铰链-梁结构、被动限位器和实时疲劳感知方法，显著减少疲劳积累并实现无额外传感器的在线疲劳估计


<details>
  <summary>Details</summary>
Motivation: 电缆驱动的连续体机器人在长期使用中会产生机械疲劳和材料退化，影响性能并可能导致结构失效，而现有技术对连续体机器人的疲劳估计研究不足

Method: 采用三种创新设计：(1)混合铰链-梁结构分离扭转和弯曲，(2)被动限位器通过机械约束安全限制运动并利用电机扭矩检测极限扭矩，(3)基于极限位姿电机扭矩估计刚度的实时疲劳感知方法

Result: 实验显示该设计比传统设计减少约49%的疲劳积累，被动机械限制与电机侧传感结合能够准确估计结构疲劳和损伤

Conclusion: 该架构能够实现安全可靠的长时期操作，证实了所提方法的有效性

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [13] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于自适应感兴趣结构(SOI)的双机械臂自动装袋系统，通过实时视觉反馈动态调整动作，无需预先了解袋子属性。


<details>
  <summary>Details</summary>
Motivation: 工业装袋任务面临可变形袋子的复杂性和不可预测性挑战，需要开发能够自适应处理各种袋子的自动化系统。

Method: 采用高斯混合模型(GMM)估计SOI状态，优化技术生成SOI，通过约束双向快速探索随机树(CBiRRT)进行运动规划，使用模型预测控制(MPC)实现双机械臂协调。

Result: 大量实验验证了系统在各种物体上执行精确和鲁棒装袋的能力，展示了其适应性。

Conclusion: 这项工作为机器人可变形物体操作(DOM)，特别是在自动装袋任务中提供了新的解决方案。

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [14] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: SMapper是一个开源的硬件平台，集成了LiDAR、多摄像头和惯性传感器，提供精确的时空同步校准，并发布了包含室内外序列的SLAM数据集SMapper-light，用于推动SLAM算法的开发和评估。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM和自主导航数据集在传感模态、环境多样性和硬件设置可复现性方面存在局限，需要可靠且可复现的多模态数据集来推动研究进展。

Method: 设计并开发了SMapper开源硬件平台，集成了同步的LiDAR、多摄像头和惯性传感，建立了稳健的校准和同步流程，确保跨模态的精确时空对齐，并发布了SMapper-light数据集。

Result: 发布了包含室内外序列的SMapper-light数据集，提供紧密同步的多模态数据和亚厘米精度的地面真实轨迹，并在最先进的LiDAR和视觉SLAM框架上进行了基准测试。

Conclusion: 通过结合开源硬件设计、可复现的数据收集和全面的基准测试，SMapper为推进SLAM算法开发、评估和可复现性建立了坚实基础。

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [15] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 基于NeuroTac传感器和脉冲卷积神经网络的神经形态触觉传感系统，用于早期滑移检测，在能量受限的边缘平台上实现高效滑移状态分类


<details>
  <summary>Details</summary>
Motivation: 在边缘平台上部署滑移检测系统面临能量约束挑战，需要开发低功耗的早期滑移检测方案来增强机器人操作安全性

Method: 使用NeuroTac传感器（具有突出乳头状皮肤的触觉传感器）和脉冲卷积神经网络（SCNN）进行三类滑移状态分类（无滑移、初始滑移、完全滑移）

Result: SCNN模型在传感器运动诱导的滑移条件下达到94.33%的分类准确率；在动态重力诱导滑移验证条件下，系统能在完全滑移发生前至少360毫秒检测到初始滑移

Conclusion: 该神经形态系统具有稳定且响应迅速的初始滑移检测能力，适用于能量受限的边缘计算平台

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [16] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 提出了一种基于对象相对控制的视觉导航新范式，使用相对3D场景图作为拓扑地图表示，训练ObjectReact局部控制器，在跨传感器高度变化和反向导航等任务中优于图像相对方法。


<details>
  <summary>Details</summary>
Motivation: 传统的图像相对视觉导航方法受限于图像与智能体位姿的严格绑定，而对象作为地图属性提供了与位姿和轨迹无关的世界表示，能够实现更好的跨部署泛化能力。

Method: 提出相对3D场景图作为拓扑地图表示，用于获取对象级别的全局路径规划成本。训练ObjectReact局部控制器，直接基于高级WayObject Costmap表示，无需显式RGB输入。

Result: 在传感器高度变化和多种导航任务中，对象相对控制方法优于图像相对方法，特别是在反向导航等需要空间理解能力的任务中表现优异。仅使用模拟训练的策略能够很好地泛化到真实室内环境。

Conclusion: 对象相对控制范式具有多个优势：无需严格模仿先验经验即可遍历新路线，控制预测问题可与图像匹配问题解耦，在跨部署设置中实现高度不变性，为视觉导航提供了更鲁棒和泛化的解决方案。

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [17] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: 开发了能够全身膨胀收缩的毛绒机器人MOFU，通过实验发现膨胀收缩运动能显著增强机器人被感知的生命感，是未来机器人设计中的重要元素


<details>
  <summary>Details</summary>
Motivation: 现有治疗和社交机器人主要模仿外观和关节运动，但忽视了生物体中观察到的全身膨胀收缩这种体积变化运动对生命感感知的影响

Method: 开发了采用Jitterbug结构的MOFU机器人，使用单个电机实现直径210-280mm的平滑体积变化。通过在线调查视频，使用Godspeed问卷评估膨胀收缩运动对生命感感知的影响

Result: 1) 静止状态下膨胀收缩显著增加感知生命感；2) 双机器人未比单机器人显著增加生命感；3) 膨胀收缩与运动结合比单独运动获得更高生命感评分

Conclusion: 体积变化运动如膨胀收缩能增强机器人的感知生命感，应作为未来旨在塑造人类印象的机器人开发中的重要设计元素

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [18] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: Dexplore是一个统一单循环优化框架，直接从大规模运动捕捉数据学习机器人控制策略，通过联合重定向和跟踪，将不完美的人类演示转化为有效的灵巧操作训练信号。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用三阶段工作流（重定向、跟踪、残差校正）导致演示数据利用不足和误差累积，且人类演示的不准确性和人-机器人手之间的本体差异限制了数据的直接使用。

Method: 提出统一单循环优化，联合执行重定向和跟踪；将演示作为软指导而非绝对真值；从原始轨迹推导自适应空间范围；使用强化学习训练策略保持在范围内同时最小化控制努力和完成任务。

Result: 统一框架保留了演示意图，使机器人特定策略涌现，提高对噪声的鲁棒性，并能扩展到大规模演示语料库；还将跟踪策略蒸馏为基于视觉的技能条件生成控制器。

Conclusion: Dexplore作为原则性桥梁，将不完美的演示转化为灵巧操作的有效训练信号，支持跨物体泛化和真实世界部署。

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [19] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: SimpleVLA-RL是一个针对视觉-语言-动作模型的强化学习框架，通过改进采样、并行化和损失计算，在减少对大规模数据依赖的同时实现了更好的泛化性能，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型面临的两个核心挑战：大规模人类操作轨迹数据稀缺且成本高昂，以及在分布偏移任务上的泛化能力有限。受大型推理模型通过强化学习提升推理能力的启发，探索RL是否能类似地提升VLA模型的长期动作规划能力。

Method: 基于veRL框架，引入了VLA特定的轨迹采样方法、可扩展并行化、多环境渲染和优化的损失计算。应用于OpenVLA-OFT模型，并引入了探索增强策略。

Result: 在LIBERO基准测试中达到最先进性能，在RoboTwin 1.0和2.0上甚至超越了π_0。不仅减少了对大规模数据的依赖，实现了鲁棒泛化，而且在真实世界任务中显著超越了监督微调方法。

Conclusion: SimpleVLA-RL框架成功证明了强化学习可以有效提升VLA模型的长期动作规划能力，发现了训练过程中的"pushcut"现象，表明策略能够发现之前未见的新模式，为VLA模型的进一步发展提供了新方向。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>
