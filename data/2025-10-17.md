<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 37]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking](https://arxiv.org/abs/2510.14000)
*Mingyang Jiang,Yueyuan Li,Jiaru Zhang,Songan Zhang,Ming Yang*

Main category: cs.RO

TL;DR: 提出DRIP方法，结合强化学习和扩散模型，用于受限空间自动停车规划，提高规划成功率和精度


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式动作建模，在受限复杂环境中难以准确建模最优动作分布，影响规划成功率

Method: 使用RL预训练策略提供先验动作分布来正则化扩散训练过程，在推理阶段通过去噪过程将粗略先验细化为精确动作分布

Result: 在具有不同空间约束的停车场景中评估，显著提高了受限空间停车环境中的规划性能，同时保持常见场景中的强泛化能力

Conclusion: DRIP方法通过RL先验分布引导扩散模型训练，实现了更准确的动作建模、更高的规划成功率和更少的推理步骤

Abstract: The growing demand for parking has increased the need for automated parking
planning methods that can operate reliably in confined spaces. In restricted
and complex environments, high-precision maneuvers are required to achieve a
high success rate in planning, yet existing approaches often rely on explicit
action modeling, which faces challenges when accurately modeling the optimal
action distribution. In this paper, we propose DRIP, a diffusion-refined
planner anchored in reinforcement learning (RL) prior action distribution, in
which an RL-pretrained policy provides prior action distributions to regularize
the diffusion training process. During the inference phase the denoising
process refines these coarse priors into more precise action distributions. By
steering the denoising trajectory through the reinforcement learning prior
distribution during training, the diffusion model inherits a well-informed
initialization, resulting in more accurate action modeling, a higher planning
success rate, and reduced inference steps. We evaluate our approach across
parking scenarios with varying degrees of spatial constraints. Experimental
results demonstrate that our method significantly improves planning performance
in confined-space parking environments while maintaining strong generalization
in common scenarios.

</details>


### [2] [Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms](https://arxiv.org/abs/2510.14018)
*Adam Morris,Timothy Pelham,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 本文提出了一种设计空间智能机器人群体行为的方法，用于定位隐藏的无线电发射器。通过差分进化生成几何巡逻路线，能够独立于发射器参数定位未知信号。研究表明巡逻形状和天线类型影响信息增益，进而决定有效的三角测量覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 电磁监视中的一个关键挑战是在不知道发射器参数的情况下定位未知信号。本文旨在开发一种能够独立于发射器参数定位隐藏无线电发射器的机器人群体方法。

Method: 使用差分进化生成几何巡逻路线，模拟四机器人群体在八种配置下的行为。基于指定的巡逻形状和感知能力（全向或定向天线）分配预生成的巡逻路线。在每个试验中随机放置发射器位置、传输功率和频率。

Result: 全向天线定位成功率主要受源位置而非信号特性影响，当源放置在区域边缘时失败率最高。定向天线由于更高的增益和方向性，平均检测成功率达到98.75%，而全向天线为80.25%。定向感知的平均定位误差为1.01-1.30米，全向感知为1.67-1.90米。

Conclusion: 群体预测电磁现象的能力直接依赖于其与环境的物理交互。通过优化的巡逻路线和天线选择实现的空间智能是有效机器人监视的关键设计考虑因素。

Abstract: This paper introduces a method for designing spatially intelligent robot
swarm behaviors to localize concealed radio emitters. We use differential
evolution to generate geometric patrol routes that localize unknown signals
independently of emitter parameters, a key challenge in electromagnetic
surveillance. Patrol shape and antenna type are shown to influence information
gain, which in turn determines the effective triangulation coverage. We
simulate a four-robot swarm across eight configurations, assigning
pre-generated patrol routes based on a specified patrol shape and sensing
capability (antenna type: omnidirectional or directional). An emitter is placed
within the map for each trial, with randomized position, transmission power and
frequency. Results show that omnidirectional localization success rates are
driven primarily by source location rather than signal properties, with
failures occurring most often when sources are placed in peripheral areas of
the map. Directional antennas are able to overcome this limitation due to their
higher gain and directivity, with an average detection success rate of 98.75%
compared to 80.25% for omnidirectional. Average localization errors range from
1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional
sensing; while directional sensing also benefits from shorter patrol edges.
These results demonstrate that a swarm's ability to predict electromagnetic
phenomena is directly dependent on its physical interaction with the
environment. Consequently, spatial intelligence, realized here through
optimized patrol routes and antenna selection, is a critical design
consideration for effective robotic surveillance.

</details>


### [3] [Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming](https://arxiv.org/abs/2510.14063)
*Nan Li,Jiming Ren,Haris Miller,Samuel Coogan,Karen M. Feigh,Ye Zhao*

Main category: cs.RO

TL;DR: 提出OATH框架，通过自适应Halton序列地图和集群-拍卖-选择机制，解决多智能体任务分配中的可扩展性、空间推理和障碍物环境适应性挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体任务分配和规划在可扩展性、空间推理和障碍物丰富环境适应性方面仍面临挑战，需要新的解决方案。

Method: 开发自适应Halton序列地图（首次在MATP中应用Halton采样），提出集群-拍卖-选择框架，集成障碍感知聚类、加权拍卖和集群内任务选择。

Result: 在NVIDIA Isaac Sim中验证，相比现有MATP基线方法，在任务分配质量、可扩展性、动态变化适应性和整体执行性能方面有显著提升。

Conclusion: OATH框架通过障碍感知策略有效协调异构机器人团队，保持可扩展性和接近最优的分配性能。

Abstract: Multi-Agent Task Assignment and Planning (MATP) has attracted growing
attention but remains challenging in terms of scalability, spatial reasoning,
and adaptability in obstacle-rich environments. To address these challenges, we
propose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for
Heterogeneous Robot Teaming, which advances MATP by introducing a novel
obstacle-aware strategy for task assignment. First, we develop an adaptive
Halton sequence map, the first known application of Halton sampling with
obstacle-aware adaptation in MATP, which adjusts sampling density based on
obstacle distribution. Second, we propose a cluster-auction-selection framework
that integrates obstacle-aware clustering with weighted auctions and
intra-cluster task selection. These mechanisms jointly enable effective
coordination among heterogeneous robots while maintaining scalability and
near-optimal allocation performance. In addition, our framework leverages an
LLM to interpret human instructions and directly guide the planner in real
time. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in
task assignment quality, scalability, adaptability to dynamic changes, and
overall execution performance compared to state-of-the-art MATP baselines. A
project website is available at https://llm-oath.github.io/.

</details>


### [4] [Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning](https://arxiv.org/abs/2510.14065)
*Gaoyuan Liu,Joris de Winter,Yuri Durodie,Denis Steckelmacher,Ann Nowe,Bram Vanderborght*

Main category: cs.RO

TL;DR: 该论文提出了一种将强化学习技能集成到任务与运动规划(TAMP)中的方法，通过数据驱动的逻辑组件和计划精炼子程序来处理概率性技能的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统TAMP方法在处理具有不确定性的概率性动作时面临挑战，而强化学习虽然擅长获取鲁棒的短视界操作技能，但缺乏长视界推理能力。需要结合两者的优势。

Method: 设计了一种将RL技能集成到TAMP管道的方法，为RL技能定义数据驱动的逻辑组件，使其能够被符号规划部署，并设计了计划精炼子程序来处理效果不确定性。

Result: 实验表明，通过嵌入RL技能，扩展了TAMP在概率性技能领域的能力，相比之前的方法提高了规划效率。

Conclusion: 该方法成功地将RL技能与TAMP结合，解决了概率性动作规划问题，提高了规划效率和能力范围。

Abstract: Task and motion planning (TAMP) for robotics manipulation necessitates
long-horizon reasoning involving versatile actions and skills. While
deterministic actions can be crafted by sampling or optimizing with certain
constraints, planning actions with uncertainty, i.e., probabilistic actions,
remains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)
excels in acquiring versatile, yet short-horizon, manipulation skills that are
robust with uncertainties. In this letter, we design a method that integrates
RL skills into TAMP pipelines. Besides the policy, a RL skill is defined with
data-driven logical components that enable the skill to be deployed by symbolic
planning. A plan refinement sub-routine is designed to further tackle the
inevitable effect uncertainties. In the experiments, we compare our method with
baseline hierarchical planning from both TAMP and RL fields and illustrate the
strength of the method. The results show that by embedding RL skills, we extend
the capability of TAMP to domains with probabilistic skills, and improve the
planning efficiency compared to the previous methods.

</details>


### [5] [Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load](https://arxiv.org/abs/2510.14072)
*Hemjyoti Das,Christian Ott*

Main category: cs.RO

TL;DR: 提出了一种基于部分反馈线性化的控制方法，用于稳定带有负载的悬挂空中平台，适用于建筑工地的起重机应用。


<details>
  <summary>Details</summary>
Motivation: 开发适用于建筑工地起重机应用的控制方法，能够处理重物搬运和运输任务，同时考虑系统的欠驱动特性和耦合动力学。

Method: 采用部分反馈线性化控制方法，利用系统的耦合动力学进行稳定，仅依赖机载传感器，适用于室外建筑工地环境。

Result: 通过数值稳定性分析证明了耦合项对系统稳定的重要性，进行了鲁棒性分析（风扰动、传感器噪声、系统不确定性），并通过仿真和实验验证了控制方法的有效性。

Conclusion: 所提出的控制方法能够有效稳定带有负载的悬挂空中平台，在存在外部干扰和不确定性的情况下仍能保持良好性能，适用于实际建筑工地应用。

Abstract: In this work, we present a novel control approach based on partial feedback
linearization (PFL) for the stabilization of a suspended aerial platform with
an attached load. Such systems are envisioned for various applications in
construction sites involving cranes, such as the holding and transportation of
heavy objects. Our proposed control approach considers the underactuation of
the whole system while utilizing its coupled dynamics for stabilization. We
demonstrate using numerical stability analysis that these coupled terms are
crucial for the stabilization of the complete system. We also carried out
robustness analysis of the proposed approach in the presence of external wind
disturbances, sensor noise, and uncertainties in system dynamics. As our
envisioned target application involves cranes in outdoor construction sites,
our control approaches rely on only onboard sensors, thus making it suitable
for such applications. We carried out extensive simulation studies and
experimental tests to validate our proposed control approach.

</details>


### [6] [ViTacGen: Robotic Pushing with Vision-to-Touch Generation](https://arxiv.org/abs/2510.14117)
*Zhiyuan Wu,Yijiong Lin,Yongqiang Zhao,Xuyang Zhang,Zhuo Chen,Nathan Lepora,Shan Luo*

Main category: cs.RO

TL;DR: ViTacGen是一个机器人操作框架，通过视觉到触觉生成技术，在强化学习中为视觉机器人推动任务生成触觉反馈，无需依赖真实高分辨率触觉传感器。


<details>
  <summary>Details</summary>
Motivation: 真实触觉传感器存在硬件限制（高成本、易碎）和部署挑战（校准困难、传感器差异），而纯视觉策略性能不足。受人类从视觉推断触觉状态的启发，开发视觉到触觉生成方法。

Method: 包含编码器-解码器的视觉到触觉生成网络，从视觉图像序列生成接触深度图像（标准化触觉表示），然后通过强化学习策略融合视觉-触觉数据，基于视觉和生成触觉观察进行对比学习。

Result: 在仿真和真实世界实验中验证有效性，展示了优越性能，成功率高达86%。

Conclusion: ViTacGen框架能够有效消除对高分辨率真实触觉传感器的依赖，实现视觉机器人系统的零样本部署。

Abstract: Robotic pushing is a fundamental manipulation task that requires tactile
feedback to capture subtle contact forces and dynamics between the end-effector
and the object. However, real tactile sensors often face hardware limitations
such as high costs and fragility, and deployment challenges involving
calibration and variations between different sensors, while vision-only
policies struggle with satisfactory performance. Inspired by humans' ability to
infer tactile states from vision, we propose ViTacGen, a novel robot
manipulation framework designed for visual robotic pushing with vision-to-touch
generation in reinforcement learning to eliminate the reliance on
high-resolution real tactile sensors, enabling effective zero-shot deployment
on visual-only robotic systems. Specifically, ViTacGen consists of an
encoder-decoder vision-to-touch generation network that generates contact depth
images, a standardized tactile representation, directly from visual image
sequence, followed by a reinforcement learning policy that fuses visual-tactile
data with contrastive learning based on visual and generated tactile
observations. We validate the effectiveness of our approach in both simulation
and real world experiments, demonstrating its superior performance and
achieving a success rate of up to 86\%.

</details>


### [7] [Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space](https://arxiv.org/abs/2510.14234)
*Ning Han,Gu Gong,Bin Zhang,Yuexuan Xu,Bohan Yang,Yunhui Liu,David Navarro-Alarcon*

Main category: cs.RO

TL;DR: 提出了一种基于关键点的无模型形变物体形状控制方法，通过深度学习提取关键点坐标作为特征向量，结合变形雅可比矩阵和约束性能控制实现精确的形状控制。


<details>
  <summary>Details</summary>
Motivation: 三维可变形物体的操控面临无限维状态空间和复杂变形动力学的挑战，现有方法依赖特征降维，需要更有效的控制策略。

Method: 使用深度学习从点云提取关键点坐标作为特征向量，建立变形雅可比矩阵描述形状动力学，结合障碍李雅普诺夫函数开发约束性能控制方法。

Result: 实验结果表明该方法具有有效性和鲁棒性，能够实现精确的形状控制。

Conclusion: 提出的无模型方法简化了可变形物体的操控问题，通过关键点提取和视觉伺服控制实现了稳定和精确的形状控制。

Abstract: Manipulating three-dimensional (3D) deformable objects presents significant
challenges for robotic systems due to their infinite-dimensional state space
and complex deformable dynamics. This paper proposes a novel model-free
approach for shape control with constraints imposed on key points. Unlike
existing methods that rely on feature dimensionality reduction, the proposed
controller leverages the coordinates of key points as the feature vector, which
are extracted from the deformable object's point cloud using deep learning
methods. This approach not only reduces the dimensionality of the feature space
but also retains the spatial information of the object. By extracting key
points, the manipulation of deformable objects is simplified into a visual
servoing problem, where the shape dynamics are described using a deformation
Jacobian matrix. To enhance control accuracy, a prescribed performance control
method is developed by integrating barrier Lyapunov functions (BLF) to enforce
constraints on the key points. The stability of the closed-loop system is
rigorously analyzed and verified using the Lyapunov method. Experimental
results further demonstrate the effectiveness and robustness of the proposed
method.

</details>


### [8] [Learning Human-Humanoid Coordination for Collaborative Object Carrying](https://arxiv.org/abs/2510.14293)
*Yushi Du,Yixuan Li,Baoxiong Jia,Yutang Lin,Pei Zhou,Wei Liang,Yanchao Yang,Siyuan Huang*

Main category: cs.RO

TL;DR: 提出了一种仅使用本体感觉的强化学习方法COLA，通过单一策略结合领导者和跟随者行为，实现人形机器人与人类的顺从协作搬运，无需外部传感器或复杂交互模型。


<details>
  <summary>Details</summary>
Motivation: 虽然机械臂的顺从人机协作已广泛发展，但由于人形机器人复杂的全身动力学，实现顺从的人-人形机器人协作仍未被充分探索。

Method: 使用仅本体感觉的强化学习方法，在闭环环境中训练单一策略，结合领导者和跟随者行为，通过协调轨迹规划预测物体运动模式和人类意图，保持负载平衡。

Result: 模拟实验显示模型减少人类24.7%的努力，同时保持物体稳定性。真实世界实验验证了在不同物体类型和运动模式下的鲁棒协作搬运。23名参与者的人体研究确认相比基线模型平均提升27.4%。

Conclusion: 该方法无需外部传感器或复杂交互模型即可实现顺从的人-人形协作搬运，为实际部署提供了实用解决方案。

Abstract: Human-humanoid collaboration shows significant promise for applications in
healthcare, domestic assistance, and manufacturing. While compliant robot-human
collaboration has been extensively developed for robotic arms, enabling
compliant human-humanoid collaboration remains largely unexplored due to
humanoids' complex whole-body dynamics. In this paper, we propose a
proprioception-only reinforcement learning approach, COLA, that combines leader
and follower behaviors within a single policy. The model is trained in a
closed-loop environment with dynamic object interactions to predict object
motion patterns and human intentions implicitly, enabling compliant
collaboration to maintain load balance through coordinated trajectory planning.
We evaluate our approach through comprehensive simulator and real-world
experiments on collaborative carrying tasks, demonstrating the effectiveness,
generalization, and robustness of our model across various terrains and
objects. Simulation experiments demonstrate that our model reduces human effort
by 24.7%. compared to baseline approaches while maintaining object stability.
Real-world experiments validate robust collaborative carrying across different
object types (boxes, desks, stretchers, etc.) and movement patterns
(straight-line, turning, slope climbing). Human user studies with 23
participants confirm an average improvement of 27.4% compared to baseline
models. Our method enables compliant human-humanoid collaborative carrying
without requiring external sensors or complex interaction models, offering a
practical solution for real-world deployment.

</details>


### [9] [Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning](https://arxiv.org/abs/2510.14300)
*Weijie Shen,Yitian Liu,Yuhao Wu,Zhixuan Liang,Sijia Gu,Dehui Wang,Tian Nian,Lei Xu,Yusen Qin,Jiangmiao Pang,Xinping Guan,Xiaokang Yang,Yao Mu*

Main category: cs.RO

TL;DR: AdaMoE是一种基于混合专家(MoE)的视觉-语言-动作模型架构，通过继承预训练的密集VLA模型权重，将前馈层替换为稀疏激活的MoE层来扩展动作专家，实现了性能提升和计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型扩展的两个关键挑战：(1)从头训练需要大量计算资源和数据集，而机器人数据稀缺，需要充分利用预训练权重；(2)实时控制需要在模型容量和计算效率之间取得平衡。

Method: 提出AdaMoE架构，采用解耦技术将专家选择与专家权重分配分离，通过独立的尺度适配器与传统路由器协同工作，实现基于任务相关性的专家选择和独立控制的权重贡献。

Result: 在关键基准测试中持续优于基线模型：LIBERO上提升1.8%，RoboTwin上提升9.3%，真实世界实验中显著提升21.5%。

Conclusion: 通过协作式专家利用而非赢家通吃的动态，可以在保持计算效率的同时实现优越性能，证明专业知识无需垄断。

Abstract: Vision-Language-Action (VLA) models are experiencing rapid development and
demonstrating promising capabilities in robotic manipulation tasks. However,
scaling up VLA models presents several critical challenges: (1) Training new
VLA models from scratch demands substantial computational resources and
extensive datasets. Given the current scarcity of robot data, it becomes
particularly valuable to fully leverage well-pretrained VLA model weights
during the scaling process. (2) Real-time control requires carefully balancing
model capacity with computational efficiency. To address these challenges, We
propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits
pretrained weights from dense VLA models, and scales up the action expert by
substituting the feedforward layers into sparsely activated MoE layers. AdaMoE
employs a decoupling technique that decouples expert selection from expert
weighting through an independent scale adapter working alongside the
traditional router. This enables experts to be selected based on task relevance
while contributing with independently controlled weights, allowing
collaborative expert utilization rather than winner-takes-all dynamics. Our
approach demonstrates that expertise need not monopolize. Instead, through
collaborative expert utilization, we can achieve superior performance while
maintaining computational efficiency. AdaMoE consistently outperforms the
baseline model across key benchmarks, delivering performance gains of 1.8% on
LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement
in real-world experiments validates its practical effectiveness for robotic
manipulation tasks.

</details>


### [10] [Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion](https://arxiv.org/abs/2510.14338)
*Yuanhong Zeng,Anushri Dixit*

Main category: cs.RO

TL;DR: 提出了一种用于四足机器人运动的风险感知强化学习方法，使用CVaR约束策略优化训练风险条件策略族，并通过多臂赌博机框架在线自适应选择最佳策略。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在未知环境中运动时的鲁棒性问题，确保在不同动态条件、接触、感知噪声和地形下的稳定性能。

Method: 使用条件风险价值(CVaR)约束策略优化技术训练风险条件策略族，部署时通过多臂赌博机框架仅基于观察到的回合回报自适应选择最佳策略。

Result: 在8个未见设置中，风险感知策略的平均性能和尾部性能比其他基线方法提高近一倍；赌博机自适应方法在未知地形中2分钟内即可选择出最佳风险感知策略。

Conclusion: 该方法能够有效提升四足机器人在未知环境中的运动鲁棒性和性能，通过在线自适应选择实现动态环境下的最优表现。

Abstract: In this work, we study risk-aware reinforcement learning for quadrupedal
locomotion. Our approach trains a family of risk-conditioned policies using a
Conditional Value-at-Risk (CVaR) constrained policy optimization technique that
provides improved stability and sample efficiency. At deployment, we adaptively
select the best performing policy from the family of policies using a
multi-armed bandit framework that uses only observed episodic returns, without
any privileged environment information, and adapts to unknown conditions on the
fly. Hence, we train quadrupedal locomotion policies at various levels of
robustness using CVaR and adaptively select the desired level of robustness
online to ensure performance in unknown environments. We evaluate our method in
simulation across eight unseen settings (by changing dynamics, contacts,
sensing noise, and terrain) and on a Unitree Go2 robot in previously unseen
terrains. Our risk-aware policy attains nearly twice the mean and tail
performance in unseen environments compared to other baselines and our
bandit-based adaptation selects the best-performing risk-aware policy in
unknown terrain within two minutes of operation.

</details>


### [11] [SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation](https://arxiv.org/abs/2510.14357)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 提出了SUM-AgriVLN方法，通过空间理解记忆模块改进农业视觉语言导航，在A2A基准测试中将成功率从0.47提升到0.54。


<details>
  <summary>Details</summary>
Motivation: 农业机器人导航指令经常重复出现，但现有方法将每个指令视为独立片段，忽略了利用过往经验提供空间上下文信息的潜力。

Method: SUM-AgriVLN方法通过空间理解记忆模块，利用3D重建和表示来保存空间记忆，为后续导航提供空间上下文。

Result: 在A2A基准测试中，成功率从0.47提升到0.54，导航误差从2.91米略微增加到2.93米，实现了农业领域的最先进性能。

Conclusion: 空间理解记忆模块能有效利用过往经验改进农业视觉语言导航性能，展示了在重复导航场景中利用空间记忆的价值。

Abstract: Agricultural robots are emerging as powerful assistants across a wide range
of agricultural tasks, nevertheless, still heavily rely on manual operation or
fixed rail systems for movement. The AgriVLN method and the A2A benchmark
pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural
domain, enabling robots to navigate to the target positions following the
natural language instructions. In practical agricultural scenarios, navigation
instructions often repeatedly occur, yet AgriVLN treat each instruction as an
independent episode, overlooking the potential of past experiences to provide
spatial context for subsequent ones. To bridge this gap, we propose the method
of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation
(SUM-AgriVLN), in which the SUM module employs spatial understanding and save
spatial memory through 3D reconstruction and representation. When evaluated on
the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47
to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,
demonstrating the state-of-the-art performance in the agricultural domain.
Code: https://github.com/AlexTraveling/SUM-AgriVLN.

</details>


### [12] [RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit](https://arxiv.org/abs/2510.14414)
*Baris Baysal,Omid Arfaie,Ramazan Unal*

Main category: cs.RO

TL;DR: 该研究开发了一款名为RoboANKLE的主动式胫骨假肢，采用ESER和EES机制提供完整的推进辅助，能够实现自然踝关节运动，重量1.92kg，扭矩生成能力比自然行走需求高57%。


<details>
  <summary>Details</summary>
Motivation: 设计主动式胫骨假肢面临能量自主性和重量最小化的挑战，研究旨在通过模仿人类踝关节提供广泛的推进辅助，实现自然的扭矩分布。

Method: 采用能量存储和扩展释放机制(ESER)及新型额外能量存储(EES)机制，进行运动学和动力学分析确定设计参数，通过CAD建模、动态和结构分析优化设计，最终制造原型进行实验评估。

Result: 原型重量1.92kg，尺寸261x107x420mm，能够以95%的准确度实现自然最大背屈角度，扭矩生成能力比自然行走需求高57%，功率生成能力比步态周期自然功率高10%。

Conclusion: RoboANKLE成功实现了自然踝关节运动，在扭矩和功率生成方面优于自然需求，验证了所采用机制的有效性和设计的可行性。

Abstract: This study presents a powered transtibial prosthesis with complete push-off
assistance, RoboANKLE. The design aims to fulfill specific requirements, such
as a sufficient range of motion (RoM) while providing the necessary torque for
achieving natural ankle motion in daily activities. Addressing the challenges
faced in designing active transtibial prostheses, such as maintaining energetic
autonomy and minimizing weight, is vital for the study. With this aim, we try
to imitate the human ankle by providing extensive push-off assistance to
achieve a natural-like torque profile. Thus, Energy Store and Extended Release
mechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.
Kinematic and kinetic analyses are carried out to determine the design
parameters and assess the design performance. Subsequently, a Computer-Aided
Design (CAD) model is built and used in comprehensive dynamic and structural
analyses. These analyses are used for the design performance evaluation and
determine the forces and torques applied to the prosthesis, which aids in
optimizing the design for minimal weight via structural analysis and topology
optimization. The design of the prototype is then finalized and manufactured
for experimental evaluation to validate the design and functionality. The
prototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.
The Functional evaluations of the RoboANKLE revealed that it is capable of
achieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,
Thanks to the implemented mechanisms, the results show that RoboANKLE can
generate 57% higher than the required torque for natural walking. The result of
the power generation capacity of the RoboANKLE is 10% more than the natural
power during the gait cycle.

</details>


### [13] [Towards Adaptable Humanoid Control via Adaptive Motion Tracking](https://arxiv.org/abs/2510.14454)
*Tao Huang,Huayi Wang,Junli Ren,Kangning Yin,Zirui Wang,Xiao Chen,Feiyu Jia,Wentao Zhang,Junfeng Long,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: AdaMimic是一种新颖的运动跟踪算法，能够从单一参考运动实现可适应的人形机器人控制，通过关键帧稀疏化和轻量编辑创建增强数据集，结合时间扭曲技术提高模仿精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有运动先验方法虽然适应性好但模仿精度不足，而运动跟踪方法精度高但需要大量训练数据和目标运动。需要结合两者优势，实现从单一参考运动既能保持高模仿精度又具备良好适应性。

Method: 首先将单一参考运动稀疏化为关键帧并进行轻量编辑创建增强数据集；然后训练策略跟踪这些稀疏关键帧生成密集中间运动；最后训练适配器调整跟踪速度和细化低级动作，实现灵活的时间扭曲。

Result: 在仿真和真实世界Unitree G1人形机器人上的多个任务验证中，该方法在广泛的适应条件下都表现出显著的改进。

Conclusion: AdaMimic成功结合了运动先验和运动跟踪方法的优势，实现了从单一参考运动的高精度模仿和良好适应性，为人形机器人的运动控制提供了有效解决方案。

Abstract: Humanoid robots are envisioned to adapt demonstrated motions to diverse
real-world conditions while accurately preserving motion patterns. Existing
motion prior approaches enable well adaptability with a few motions but often
sacrifice imitation accuracy, whereas motion-tracking methods achieve accurate
imitation yet require many training motions and a test-time target motion to
adapt. To combine their strengths, we introduce AdaMimic, a novel motion
tracking algorithm that enables adaptable humanoid control from a single
reference motion. To reduce data dependence while ensuring adaptability, our
method first creates an augmented dataset by sparsifying the single reference
motion into keyframes and applying light editing with minimal physical
assumptions. A policy is then initialized by tracking these sparse keyframes to
generate dense intermediate motions, and adapters are subsequently trained to
adjust tracking speed and refine low-level actions based on the adjustment,
enabling flexible time warping that further improves imitation accuracy and
adaptability. We validate these significant improvements in our approach in
both simulation and the real-world Unitree G1 humanoid robot in multiple tasks
across a wide range of adaptation conditions. Videos and code are available at
https://taohuang13.github.io/adamimic.github.io/.

</details>


### [14] [Restoring Noisy Demonstration for Imitation Learning With Diffusion Models](https://arxiv.org/abs/2510.14467)
*Shang-Fu Chen,Co Yong,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出了一种过滤-恢复框架来处理包含噪声的专家演示数据，通过先过滤干净样本再学习条件扩散模型来恢复噪声样本，在多个机器人任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法假设专家演示是完美的，但实际中专家演示常包含人为错误或传感器/控制系统不准确造成的噪声，需要处理不完美演示数据。

Method: 采用两阶段框架：首先从演示数据中过滤出干净样本，然后学习条件扩散模型来恢复噪声样本。

Result: 在机器人手臂操作、灵巧操作和运动等任务中，该方法始终优于现有方法，消融研究验证了各组件有效性及对不同噪声类型和水平的鲁棒性。

Conclusion: 该框架对噪声离线演示数据具有实际适用性，能够有效处理不完美的专家演示。

Abstract: Imitation learning (IL) aims to learn a policy from expert demonstrations and
has been applied to various applications. By learning from the expert policy,
IL methods do not require environmental interactions or reward signals.
However, most existing imitation learning algorithms assume perfect expert
demonstrations, but expert demonstrations often contain imperfections caused by
errors from human experts or sensor/control system inaccuracies. To address the
above problems, this work proposes a filter-and-restore framework to best
leverage expert demonstrations with inherent noise. Our proposed method first
filters clean samples from the demonstrations and then learns conditional
diffusion models to recover the noisy ones. We evaluate our proposed framework
and existing methods in various domains, including robot arm manipulation,
dexterous manipulation, and locomotion. The experiment results show that our
proposed framework consistently outperforms existing methods across all the
tasks. Ablation studies further validate the effectiveness of each component
and demonstrate the framework's robustness to different noise types and levels.
These results confirm the practical applicability of our framework to noisy
offline demonstration data.

</details>


### [15] [Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots](https://arxiv.org/abs/2510.14511)
*Mingtian Du,Suhas Raghavendra Kulkarni,Simone Kager,Domenico Campolo*

Main category: cs.RO

TL;DR: 该论文建立了机器人介导的人-人交互系统的分析稳定性标准，重点关注网络时延下的触觉通信，提出了时延无关和时延相关的稳定性判据。


<details>
  <summary>Details</summary>
Motivation: 研究机器人介导的人-人交互系统在网络时延下的稳定性问题，为设计稳定的远程交互系统提供理论基础。

Method: 通过频域分析和数值模拟，识别时延无关和时延相关的稳定性判据，并通过机器人执行类人运动的实验验证。

Result: 发现刚度增加会以非线性方式降低最大可容忍时延，从而增加系统脆弱性；实验显示了稳定性与运动性能之间的相关性。

Conclusion: 提出的稳定性标准可推广到广泛的机器人介导交互场景，为有效的时延补偿策略提供了先决条件，并为稳定远程交互系统设计提供了指导方针。

Abstract: This paper establishes analytical stability criteria for robot-mediated
human-human (dyadic) interaction systems, focusing on haptic communication
under network-induced time delays. Through frequency-domain analysis supported
by numerical simulations, we identify both delay-independent and
delay-dependent stability criteria. The delay-independent criterion guarantees
stability irrespective of the delay, whereas the delay-dependent criterion is
characterised by a maximum tolerable delay before instability occurs. The
criteria demonstrate dependence on controller and robot dynamic parameters,
where increasing stiffness reduces the maximum tolerable delay in a non-linear
manner, thereby heightening system vulnerability. The proposed criteria can be
generalised to a wide range of robot-mediated interactions and serve as design
guidelines for stable remote dyadic systems. Experiments with robots performing
human-like movements further illustrate the correlation between stability and
motor performance. The findings of this paper suggest the prerequisites for
effective delay-compensation strategies.

</details>


### [16] [QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps](https://arxiv.org/abs/2510.14546)
*Matti Pekkanen,Francesco Verdoja,Ville Kyrki*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言模型嵌入的机器人地图查询方法，利用同义词和反义词在嵌入空间中扩展查询语义，训练分类器来划分环境中的相关区域。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在地图中执行查询任务时确定环境相关部分的关键挑战，传统方法受限于有限标签，而嵌入方法需要更精确的语义理解。

Method: 利用查询相关的自然语言同义词和反义词，在嵌入空间中应用启发式方法估计相关语言空间，训练分类器将环境划分为匹配和非匹配区域。

Result: 通过大量实验验证，在查询地图和标准图像基准测试中表现出更高的查询能力，方法对表示和编码器具有通用性且训练需求有限。

Conclusion: 该方法有效提升了机器人地图的查询能力，通过语义扩展和分类器训练实现了更精确的环境语义划分。

Abstract: Embeddings from Visual-Language Models are increasingly utilized to represent
semantics in robotic maps, offering an open-vocabulary scene understanding that
surpasses traditional, limited labels. Embeddings enable on-demand querying by
comparing embedded user text prompts to map embeddings via a similarity metric.
The key challenge in performing the task indicated in a query is that the robot
must determine the parts of the environment relevant to the query.
  This paper proposes a solution to this challenge. We leverage
natural-language synonyms and antonyms associated with the query within the
embedding space, applying heuristics to estimate the language space relevant to
the query, and use that to train a classifier to partition the environment into
matches and non-matches. We evaluate our method through extensive experiments,
querying both maps and standard image benchmarks. The results demonstrate
increased queryability of maps and images. Our querying technique is agnostic
to the representation and encoder used, and requires limited training.

</details>


### [17] [A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning](https://arxiv.org/abs/2510.14584)
*Benno Wingender,Nils Dengler,Rohit Menon,Sicong Pan,Maren Bennewitz*

Main category: cs.RO

TL;DR: 提出了一种广义的可放置性度量方法，直接从噪声点云评估放置姿态，无需形状先验，实现了无模型统一的抓取-放置推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖强物体先验（如CAD模型）或平面支撑假设，限制了泛化能力和抓取-放置的统一推理。需要一种能够处理未知物体和真实世界感知噪声的可靠方法。

Method: 从原始几何中提取物体的支撑表面，生成多方向放置候选，并采样满足碰撞和稳定性约束的接触点。通过将抓取分数条件化到每个候选放置上，实现统一的抓取-放置推理。

Result: 在未见过的真实物体和非平面物体支撑上，该度量方法在预测稳定性损失方面达到与CAD相当的精度，比基于学习的预测器产生更物理合理的放置。

Conclusion: 该方法能够直接从噪声点云进行可靠的抓取-放置推理，无需形状先验，在真实场景中表现出良好的泛化能力和物理合理性。

Abstract: To reliably pick and place unknown objects under real-world sensing noise
remains a challenging task, as existing methods rely on strong object priors
(e.g., CAD models), or planar-support assumptions, limiting generalization and
unified reasoning between grasping and placing. In this work, we introduce a
generalized placeability metric that evaluates placement poses directly from
noisy point clouds, without any shape priors. The metric jointly scores
stability, graspability, and clearance. From raw geometry, we extract the
support surfaces of the object to generate diverse candidates for
multi-orientation placement and sample contacts that satisfy collision and
stability constraints. By conditioning grasp scores on each candidate
placement, our proposed method enables model-free unified pick-and-place
reasoning and selects grasp-place pairs that lead to stable, collision-free
placements. On unseen real objects and non-planar object supports, our metric
delivers CAD-comparable accuracy in predicting stability loss and generally
produces more physically plausible placements than learning-based predictors.

</details>


### [18] [Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning](https://arxiv.org/abs/2510.14612)
*Gabriel Fischer Abati,João Carlos Virgolino Soares,Giulio Turrisi,Victor Barasuol,Claudio Semini*

Main category: cs.RO

TL;DR: 将四足机器人的本体感知时间序列数据编码为结构化二维图像，利用CNN学习运动相关任务，在接触估计问题上显著提升准确率


<details>
  <summary>Details</summary>
Motivation: 传统时间序列处理方法难以有效捕捉机器人本体感知信号间的相关性和步态依赖模式，需要一种能保留机器人形态结构并增强特征表达的表示方法

Method: 将多通道本体感知信号（关节位置、IMU读数、足端速度等）按机器人形态结构空间排列编码为二维图像，利用CNN处理这种跨模态表示

Result: 在真实世界和仿真环境中，图像表示方法相比传统序列模型显著提升了预测准确率和泛化能力，接触状态准确率从87.7%提升至94.5%，且使用15倍更短的窗口大小

Conclusion: 跨模态编码策略在机器人状态学习中具有巨大潜力，图像表示能有效捕捉信号间相关性和步态模式，为运动相关任务提供更丰富的特征空间

Abstract: This paper presents a novel approach for representing proprioceptive
time-series data from quadruped robots as structured two-dimensional images,
enabling the use of convolutional neural networks for learning
locomotion-related tasks. The proposed method encodes temporal dynamics from
multiple proprioceptive signals, such as joint positions, IMU readings, and
foot velocities, while preserving the robot's morphological structure in the
spatial arrangement of the image. This transformation captures inter-signal
correlations and gait-dependent patterns, providing a richer feature space than
direct time-series processing. We apply this concept in the problem of contact
estimation, a key capability for stable and adaptive locomotion on diverse
terrains. Experimental evaluations on both real-world datasets and simulated
environments show that our image-based representation consistently enhances
prediction accuracy and generalization over conventional sequence-based models,
underscoring the potential of cross-modal encoding strategies for robotic state
learning. Our method achieves superior performance on the contact dataset,
improving contact state accuracy from 87.7% to 94.5% over the recently proposed
MI-HGNN method, using a 15 times shorter window size.

</details>


### [19] [Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models](https://arxiv.org/abs/2510.14615)
*Edward Sandra,Lander Vanroye,Dries Dirckx,Ruben Cartuyvels,Jan Swevers,Wilm Decré*

Main category: cs.RO

TL;DR: 提出了CAMPD方法，使用条件扩散模型进行机器人运动规划，能够泛化到未见过的环境，无需重新训练


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法在高维状态空间和复杂环境中扩展性差，现有扩散模型方法大多针对单一环境训练，缺乏泛化能力

Method: 使用无分类器去噪概率扩散模型，通过注意力机制整合传感器无关的上下文信息，基于U-Net架构

Result: 在7自由度机械臂上验证，相比最先进方法，能在更短时间内生成高质量多模态轨迹，并能泛化到未见环境

Conclusion: CAMPD方法有效解决了运动规划的泛化问题，无需特定传感器，在真实世界任务中表现出色

Abstract: Classical methods in robot motion planning, such as sampling-based and
optimization-based methods, often struggle with scalability towards
higher-dimensional state spaces and complex environments. Diffusion models,
known for their capability to learn complex, high-dimensional and multi-modal
data distributions, provide a promising alternative when applied to motion
planning problems and have already shown interesting results. However, most of
the current approaches train their model for a single environment, limiting
their generalization to environments not seen during training. The techniques
that do train a model for multiple environments rely on a specific camera to
provide the model with the necessary environmental information and therefore
always require that sensor. To effectively adapt to diverse scenarios without
the need for retraining, this research proposes Context-Aware Motion Planning
Diffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic
diffusion model, conditioned on sensor-agnostic contextual information. An
attention mechanism, integrated in the well-known U-Net architecture,
conditions the model on an arbitrary number of contextual parameters. CAMPD is
evaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art
approaches on real-world tasks, showing its ability to generalize to unseen
environments and generate high-quality, multi-modal trajectories, at a fraction
of the time required by existing methods.

</details>


### [20] [GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement](https://arxiv.org/abs/2510.14627)
*Yao Zhong,Hanzhi Chen,Simon Schaefer,Anran Zhang,Stefan Leutenegger*

Main category: cs.RO

TL;DR: GOPLA是一个分层框架，通过增强的人类演示学习可泛化的物体放置，结合语义推理和几何可行性，在真实世界机器人放置场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决机器人作为智能助手在家庭环境中进行物体放置的挑战，需要同时考虑语义偏好（常识性物体关系）和几何可行性（碰撞避免）。

Method: 使用多模态大语言模型将人类指令和视觉输入转换为结构化计划，空间映射器生成具有几何常识的3D可操作地图，基于扩散的规划器在测试时成本指导下生成放置姿态。

Result: 在定位准确性和物理合理性评估中，比第二名方法提升了30.04个百分点的放置成功率，在广泛的真实世界机器人放置场景中表现出强泛化能力。

Conclusion: GOPLA框架通过结合语义推理和几何约束，有效解决了机器人物体放置问题，并通过可扩展的数据增强管道克服了数据稀缺问题。

Abstract: Robots are expected to serve as intelligent assistants, helping humans with
everyday household organization. A central challenge in this setting is the
task of object placement, which requires reasoning about both semantic
preferences (e.g., common-sense object relations) and geometric feasibility
(e.g., collision avoidance). We present GOPLA, a hierarchical framework that
learns generalizable object placement from augmented human demonstrations. A
multi-modal large language model translates human instructions and visual
inputs into structured plans that specify pairwise object relationships. These
plans are then converted into 3D affordance maps with geometric common sense by
a spatial mapper, while a diffusion-based planner generates placement poses
guided by test-time costs, considering multi-plan distributions and collision
avoidance. To overcome data scarcity, we introduce a scalable pipeline that
expands human placement demonstrations into diverse synthetic training data.
Extensive experiments show that our approach improves placement success rates
by 30.04 percentage points over the runner-up, evaluated on positioning
accuracy and physical plausibility, demonstrating strong generalization across
a wide range of real-world robotic placement scenarios.

</details>


### [21] [Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation](https://arxiv.org/abs/2510.14643)
*Lara Brudermüller,Brandon Hung,Xinghao Zhu,Jiuguang Wang,Nick Hawes,Preston Culbertson,Simon Le Cleac'h*

Main category: cs.RO

TL;DR: 提出了一种生成式预测控制(GPC)框架，通过使用在仿真中收集的基于采样的模型预测控制(SPC)序列训练条件流匹配模型，来摊销SPC的计算成本。该方法能够直接从噪声SPC数据中学习有意义的提议分布，实现更高效的在线规划采样，并首次应用于四足机器人的真实世界接触丰富操作任务。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的模型预测控制(SPC)计算成本高昂，限制了其在实时机器人控制中的应用。现有方法依赖迭代优化或基于梯度的求解器，难以处理复杂的接触丰富操作任务。

Method: 使用在仿真中收集的SPC控制序列训练条件流匹配模型，学习有意义的提议分布。通过摊销SPC的计算成本，在在线规划时实现更高效的采样，无需依赖迭代优化或梯度求解器。

Result: 在仿真和硬件实验中，该方法提高了采样效率，减少了规划时域要求，并在任务变化中展现出鲁棒的泛化能力。首次成功应用于四足机器人的真实世界接触丰富操作任务。

Conclusion: GPC框架能够有效摊销SPC的计算成本，通过直接从噪声数据学习提议分布实现高效采样，为复杂机器人操作任务提供了实用的实时控制解决方案。

Abstract: We present a generative predictive control (GPC) framework that amortizes
sampling-based Model Predictive Control (SPC) by bootstrapping it with
conditional flow-matching models trained on SPC control sequences collected in
simulation. Unlike prior work relying on iterative refinement or gradient-based
solvers, we show that meaningful proposal distributions can be learned directly
from noisy SPC data, enabling more efficient and informed sampling during
online planning. We further demonstrate, for the first time, the application of
this approach to real-world contact-rich loco-manipulation with a quadruped
robot. Extensive experiments in simulation and on hardware show that our method
improves sample efficiency, reduces planning horizon requirements, and
generalizes robustly across task variations.

</details>


### [22] [Spatially anchored Tactile Awareness for Robust Dexterous Manipulation](https://arxiv.org/abs/2510.14647)
*Jialei Huang,Yang Ye,Yuanqing Gong,Xuezhou Zhu,Yang Gao,Kaifeng Zhang*

Main category: cs.RO

TL;DR: SaTA框架通过将触觉特征锚定到手的运动学框架中，实现了亚毫米级的灵巧操作精度，在USB-C对接、灯泡安装和卡片滑动等任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉触觉学习方法在亚毫米级精度任务上表现不佳，无法充分利用触觉信号的感知丰富性和空间关系。理想的触觉表示应该将接触测量显式地锚定在稳定参考系中，同时保留详细的感官信息。

Method: 提出SaTA端到端策略框架，通过前向运动学将触觉特征显式锚定到手的运动学框架中，无需物体模型或显式姿态估计即可实现精确的几何推理。

Result: 在多个基准测试中，SaTA显著优于强视觉触觉基线方法，成功率提高达30个百分点，任务完成时间减少27个百分点。

Conclusion: 空间锚定的触觉表示使策略不仅能检测接触发生，还能精确推断手坐标系中的物体几何形状，为高精度灵巧操作提供了有效解决方案。

Abstract: Dexterous manipulation requires precise geometric reasoning, yet existing
visuo-tactile learning methods struggle with sub-millimeter precision tasks
that are routine for traditional model-based approaches. We identify a key
limitation: while tactile sensors provide rich contact information, current
learning frameworks fail to effectively leverage both the perceptual richness
of tactile signals and their spatial relationship with hand kinematics. We
believe an ideal tactile representation should explicitly ground contact
measurements in a stable reference frame while preserving detailed sensory
information, enabling policies to not only detect contact occurrence but also
precisely infer object geometry in the hand's coordinate system. We introduce
SaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an
end-to-end policy framework that explicitly anchors tactile features to the
hand's kinematic frame through forward kinematics, enabling accurate geometric
reasoning without requiring object models or explicit pose estimation. Our key
insight is that spatially grounded tactile representations allow policies to
not only detect contact occurrence but also precisely infer object geometry in
the hand's coordinate system. We validate SaTA on challenging dexterous
manipulation tasks, including bimanual USB-C mating in free space, a task
demanding sub-millimeter alignment precision, as well as light bulb
installation requiring precise thread engagement and rotational control, and
card sliding that demands delicate force modulation and angular precision.
These tasks represent significant challenges for learning-based methods due to
their stringent precision requirements. Across multiple benchmarks, SaTA
significantly outperforms strong visuo-tactile baselines, improving success
rates by up to 30 percentage while reducing task completion times by 27
percentage.

</details>


### [23] [When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks](https://arxiv.org/abs/2510.14677)
*Steffen Hagedorn,Luka Donkov,Aron Distelzweig,Alexandru P. Condurache*

Main category: cs.RO

TL;DR: 该论文将先进的智能交通代理模型SMART集成到nuPlan中，首次在更真实条件下评估规划器，并量化了缩小仿真与现实差距时结论的变化。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的交通代理（如IDM）行为简单被动，会隐藏规划器缺陷并导致评估偏差，无法测试复杂的交互能力。

Method: 将学习型交通代理模型SMART集成到nuPlan仿真平台中，在14个最新规划器和基准方法上进行闭环仿真评估。

Result: 基于IDM的仿真高估了规划性能：几乎所有分数都下降。但在多车道交互场景中，许多规划器表现优于预期。学习型规划器在闭环训练中表现最佳，但在边缘场景中会突然退化。

Conclusion: 建议将SMART反应式仿真作为nuPlan的新标准闭环基准，并发布了SMART代理作为IDM的替代方案。

Abstract: Planner evaluation in closed-loop simulation often uses rule-based traffic
agents, whose simplistic and passive behavior can hide planner deficiencies and
bias rankings. Widely used IDM agents simply follow a lead vehicle and cannot
react to vehicles in adjacent lanes, hindering tests of complex interaction
capabilities. We address this issue by integrating the state-of-the-art learned
traffic agent model SMART into nuPlan. Thus, we are the first to evaluate
planners under more realistic conditions and quantify how conclusions shift
when narrowing the sim-to-real gap. Our analysis covers 14 recent planners and
established baselines and shows that IDM-based simulation overestimates
planning performance: nearly all scores deteriorate. In contrast, many planners
interact better than previously assumed and even improve in multi-lane,
interaction-heavy scenarios like lane changes or turns. Methods trained in
closed-loop demonstrate the best and most stable driving performance. However,
when reaching their limits in augmented edge-case scenarios, all learned
planners degrade abruptly, whereas rule-based planners maintain reasonable
basic behavior. Based on our results, we suggest SMART-reactive simulation as a
new standard closed-loop benchmark in nuPlan and release the SMART agents as a
drop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.

</details>


### [24] [Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery](https://arxiv.org/abs/2510.14768)
*Fan Yang,Zixuan Huang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了CADRE框架，通过神经描述符场提取接触特征，用于在灵巧操作中捕捉掉落物体并重置系统，提高训练效率和恢复成功率。


<details>
  <summary>Details</summary>
Motivation: 现实世界的灵巧操作经常遇到意外错误和干扰，可能导致灾难性失败（如物体掉落）。需要解决在物体仍处于抓取范围内时捕捉掉落物体，并将系统重置到有利于恢复主要操作任务的配置的问题。

Method: 提出Contact-Aware Dynamic Recovery (CADRE)强化学习框架，包含神经描述符场(NDF)模块来提取隐式接触特征，能够直接推理手指-物体对应关系并适应不同物体几何形状。

Result: 实验表明，加入接触特征提高了训练效率，增强了强化学习训练的收敛性能，最终导致更成功的恢复。CADRE能够零样本泛化到具有不同几何形状的未见物体。

Conclusion: CADRE框架通过神经描述符场提取接触特征，有效解决了灵巧操作中的动态恢复问题，提高了恢复成功率和泛化能力。

Abstract: Real-world dexterous manipulation often encounters unexpected errors and
disturbances, which can lead to catastrophic failures, such as dropping the
manipulated object. To address this challenge, we focus on the problem of
catching a falling object while it remains within grasping range and,
importantly, resetting the system to a configuration favorable for resuming the
primary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a
reinforcement learning framework that incorporates a Neural Descriptor Field
(NDF)-inspired module to extract implicit contact features. Compared to methods
that rely solely on object pose or point cloud input, NDFs can directly reason
about finger-object correspondence and adapt to different object geometries.
Our experiments show that incorporating contact features improves training
efficiency, enhances convergence performance for RL training, and ultimately
leads to more successful recoveries. Additionally, we demonstrate that CADRE
can generalize zero-shot to unseen objects with different geometries.

</details>


### [25] [Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation](https://arxiv.org/abs/2510.14771)
*Xu Chi,Chao Zhang,Yang Su,Lingfeng Dou,Fujia Yang,Jiakuo Zhao,Haoyu Zhou,Xiaoyou Jia,Yong Zhou,Shan An*

Main category: cs.RO

TL;DR: 开发了Open TeleDex统一遥操作框架，解决机器人模仿学习中高质量演示数据获取的瓶颈问题，支持任何机械臂、灵巧手和输入设备。


<details>
  <summary>Details</summary>
Motivation: 异构机器人平台的高精度演示数据获取是机器人模仿学习系统部署的关键瓶颈，现有遥操作系统难以保证跨不同设备的精确数据收集。

Method: 开发统一遥操作框架Open TeleDex，提出新的手部姿态重定向算法，增强系统与异构主从设备的兼容性。

Result: Open TeleDex实现了TripleAny挑战，无缝支持任何机械臂、灵巧手和外部输入设备，建立了高质量公开平台。

Conclusion: Open TeleDex为复杂机器人操作和模仿学习提供了基础性、高质量、公开可用的平台，加速学术研究和工业发展。

Abstract: Accurate and high-fidelity demonstration data acquisition is a critical
bottleneck for deploying robot Imitation Learning (IL) systems, particularly
when dealing with heterogeneous robotic platforms. Existing teleoperation
systems often fail to guarantee high-precision data collection across diverse
types of teleoperation devices. To address this, we developed Open TeleDex, a
unified teleoperation framework engineered for demonstration data collection.
Open TeleDex specifically tackles the TripleAny challenge, seamlessly
supporting any robotic arm, any dexterous hand, and any external input device.
Furthermore, we propose a novel hand pose retargeting algorithm that
significantly boosts the interoperability of Open TeleDex, enabling robust and
accurate compatibility with an even wider spectrum of heterogeneous master and
slave equipment. Open TeleDex establishes a foundational, high-quality, and
publicly available platform for accelerating both academic research and
industry development in complex robotic manipulation and IL.

</details>


### [26] [SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning](https://arxiv.org/abs/2510.14783)
*Aderik Verraest,Stavrow Bahnam,Robin Ferede,Guido de Croon,Christophe De Wagter*

Main category: cs.RO

TL;DR: SkyDreamer是首个端到端基于视觉的自主无人机竞速策略，直接将像素表示映射到电机指令，实现了完整的模拟到现实迁移、机载执行和冠军级性能。


<details>
  <summary>Details</summary>
Motivation: 现有自主无人机竞速系统高度专业化，缺乏通用性。端到端视觉方法虽有广泛适用性潜力，但尚无系统能同时实现完整模拟到现实迁移、机载执行和冠军级性能。

Method: 基于informed Dreamer模型强化学习方法，世界模型解码为仅在训练时可用的特权信息。世界模型作为隐式状态和参数估计器，无需外部相机标定，通过世界模型隐藏状态解码的状态跟踪进度来解决视觉模糊问题。

Result: 在真实环境中实现稳健高速飞行，执行倒飞环、分S和梯子等复杂机动，速度达21m/s，加速度达6g。在低质量分割掩码上展示非平凡视觉模拟到现实迁移，通过实时估计最大可达电机RPM并调整飞行路径来应对电池耗尽。

Conclusion: SkyDreamer能够适应现实差距的重要方面，在实现极高速敏捷飞行的同时保持稳健性，展示了端到端视觉方法的强大潜力。

Abstract: Autonomous drone racing (ADR) systems have recently achieved champion-level
performance, yet remain highly specific to drone racing. While end-to-end
vision-based methods promise broader applicability, no system to date
simultaneously achieves full sim-to-real transfer, onboard execution, and
champion-level performance. In this work, we present SkyDreamer, to the best of
our knowledge, the first end-to-end vision-based ADR policy that maps directly
from pixel-level representations to motor commands. SkyDreamer builds on
informed Dreamer, a model-based reinforcement learning approach where the world
model decodes to privileged information only available during training. By
extending this concept to end-to-end vision-based ADR, the world model
effectively functions as an implicit state and parameter estimator, greatly
improving interpretability. SkyDreamer runs fully onboard without external aid,
resolves visual ambiguities by tracking progress using the state decoded from
the world model's hidden state, and requires no extrinsic camera calibration,
enabling rapid deployment across different drones without retraining.
Real-world experiments show that SkyDreamer achieves robust, high-speed flight,
executing tight maneuvers such as an inverted loop, a split-S and a ladder,
reaching speeds of up to 21 m/s and accelerations of up to 6 g. It further
demonstrates a non-trivial visual sim-to-real transfer by operating on
poor-quality segmentation masks, and exhibits robustness to battery depletion
by accurately estimating the maximum attainable motor RPM and adjusting its
flight path in real-time. These results highlight SkyDreamer's adaptability to
important aspects of the reality gap, bringing robustness while still achieving
extremely high-speed, agile flight.

</details>


### [27] [Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping](https://arxiv.org/abs/2510.14827)
*Yufei Zhu,Shih-Min Yang,Andrey Rudenko,Tomasz P. Kucner,Achim J. Lilienthal,Martin Magnusson*

Main category: cs.RO

TL;DR: 提出了一种基于隐式神经函数的连续时空动态地图表示方法，使用半包裹高斯混合模型直接从坐标映射到运动模式参数，无需离散化处理。


<details>
  <summary>Details</summary>
Motivation: 在复杂人类环境中实现安全高效的机器人操作需要良好的特定场景运动模式模型。现有动态地图方法使用离散空间采样且需要昂贵的离线构建过程。

Method: 基于隐式神经函数的连续时空表示，直接映射坐标到半包裹高斯混合模型参数，消除离散化和不均匀采样区域的插补需求。

Result: 在大型公共数据集上的评估显示，相比现有基线方法，该方法在运动表示精度和稀疏区域速度分布平滑性方面表现更好，同时保持计算效率。

Conclusion: 该方法为建模复杂人类运动模式提供了一种强大而高效的途径。

Abstract: Safe and efficient robot operation in complex human environments can benefit
from good models of site-specific motion patterns. Maps of Dynamics (MoDs)
provide such models by encoding statistical motion patterns in a map, but
existing representations use discrete spatial sampling and typically require
costly offline construction. We propose a continuous spatio-temporal MoD
representation based on implicit neural functions that directly map coordinates
to the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the
need for discretization and imputation for unevenly sampled regions, enabling
smooth generalization across both space and time. Evaluated on a large public
dataset with long-term real-world people tracking data, our method achieves
better accuracy of motion representation and smoother velocity distributions in
sparse regions while still being computationally efficient, compared to
available baselines. The proposed approach demonstrates a powerful and
efficient way of modeling complex human motion patterns.

</details>


### [28] [RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning](https://arxiv.org/abs/2510.14830)
*Kun Lei,Huanyu Li,Dongjie Yu,Zhenyu Wei,Lingxiao Guo,Zhennan Jiang,Ziyu Wang,Shiyu Liang,Huazhe Xu*

Main category: cs.RO

TL;DR: RL-100是一个基于扩散视觉运动策略的三阶段强化学习框架，通过模仿学习、离线强化学习和在线强化学习实现机器人操作的100%成功率，支持多种输入和机器人平台。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人操作需要接近或超越熟练人类操作员的可靠性、效率和鲁棒性。

Method: 三阶段训练流程：1) 模仿学习利用人类先验；2) 迭代离线强化学习使用离线策略评估(OPE)来门控PPO风格更新；3) 在线强化学习消除剩余故障模式。还包含轻量级一致性蒸馏头将多步采样压缩为单步策略。

Result: 在7个真实机器人任务中实现了900/900次试验的100%成功率，包括一个任务上连续250/250次试验。达到接近或优于人类遥操作的时间效率，并展示了长达2小时不间断运行的多小时鲁棒性。

Conclusion: RL-100框架在任务、体现形式和表示方式上具有通用性，能够实现真实机器人操作的高可靠性和高效率。

Abstract: Real-world robotic manipulation in homes and factories demands reliability,
efficiency, and robustness that approach or surpass skilled human operators. We
present RL-100, a real-world reinforcement learning training framework built on
diffusion visuomotor policies trained bu supervised learning. RL-100 introduces
a three-stage pipeline. First, imitation learning leverages human priors.
Second, iterative offline reinforcement learning uses an Offline Policy
Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are
applied in the denoising process for conservative and reliable improvement.
Third, online reinforcement learning eliminates residual failure modes. An
additional lightweight consistency distillation head compresses the multi-step
sampling process in diffusion into a single-step policy, enabling
high-frequency control with an order-of-magnitude reduction in latency while
preserving task performance. The framework is task-, embodiment-, and
representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a
variety of robot platforms, and both single-step and action-chunk policies. We
evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,
such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth
folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100
attains 100\% success across evaluated trials for a total of 900 out of 900
episodes, including up to 250 out of 250 consecutive trials on one task. The
method achieves near-human teleoperation or better time efficiency and
demonstrates multi-hour robustness with uninterrupted operation lasting up to
two hours.

</details>


### [29] [Multi Agent Switching Mode Controller for Sound Source localization](https://arxiv.org/abs/2510.14849)
*Marcello Sorge,Nicola Cigarini,Riccardo Lorigiola,Giulia Michieletto,Andrea Masiero,Angelo Cenedese,Alberto Guarnieri*

Main category: cs.RO

TL;DR: 多智能体切换模式控制策略用于声学目标定位，包括单源定位的刚性编队移动和多源场景的独立搜索。


<details>
  <summary>Details</summary>
Motivation: 声学传感器能在无法建立直接视线的情况下定位目标，这在机器人研究中具有重要意义。

Method: 设计了多智能体切换模式控制策略，在单源场景中保持刚性编队移动，在多源场景中各智能体独立搜索目标。

Result: 提出了适用于不同场景的声学目标定位控制方法。

Conclusion: 该控制策略能够有效处理单源和多源声学目标定位问题。

Abstract: Source seeking is an important topic in robotic research, especially
considering sound-based sensors since they allow the agents to locate a target
even in critical conditions where it is not possible to establish a direct line
of sight. In this work, we design a multi- agent switching mode control
strategy for acoustic-based target localization. Two scenarios are considered:
single source localization, in which the agents are driven maintaining a rigid
formation towards the target, and multi-source scenario, in which each agent
searches for the targets independently from the others.

</details>


### [30] [SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time](https://arxiv.org/abs/2510.14851)
*Jakob Bichler,Andreu Matoses Gimenez,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: Sadcher是一个用于异构多机器人团队的实时任务分配框架，结合了动态联盟形成和任务优先级约束，通过模仿学习训练，能够扩展到更大的任务集和团队规模。


<details>
  <summary>Details</summary>
Motivation: 解决异构多机器人团队在实时环境中的任务分配问题，需要处理动态联盟形成、任务优先级约束以及时空推理等复杂因素。

Method: 使用模仿学习训练，结合图注意力和transformer预测机器人与任务之间的分配奖励，通过松弛二分匹配生成高质量调度方案，并显式建模机器人和任务位置、任务持续时间等时空因素。

Result: 在随机未见问题上，Sadcher优于其他基于学习和启发式的基线方法，计算时间适合实时操作，并能扩展到更大的团队规模。

Conclusion: Sadcher框架在异构多机器人任务分配中表现出色，具有实时性和良好的可扩展性，同时发布了包含25万个最优调度的数据集。

Abstract: We present Sadcher, a real-time task assignment framework for heterogeneous
multi-robot teams that incorporates dynamic coalition formation and task
precedence constraints. Sadcher is trained through Imitation Learning and
combines graph attention and transformers to predict assignment rewards between
robots and tasks. Based on the predicted rewards, a relaxed bipartite matching
step generates high-quality schedules with feasibility guarantees. We
explicitly model robot and task positions, task durations, and robots'
remaining processing times, enabling advanced temporal and spatial reasoning
and generalization to environments with different spatiotemporal distributions
compared to training. Trained on optimally solved small-scale instances, our
method can scale to larger task sets and team sizes. Sadcher outperforms other
learning-based and heuristic baselines on randomized, unseen problems for small
and medium-sized teams with computation times suitable for real-time operation.
We also explore sampling-based variants and evaluate scalability across robot
and task counts. In addition, we release our dataset of 250,000 optimal
schedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/

</details>


### [31] [STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search](https://arxiv.org/abs/2510.14893)
*Helene J. Levy,Brett T. Lopez*

Main category: cs.RO

TL;DR: STITCHER是一种无需优化的规划框架，通过将短轨迹段拼接并结合图搜索，实时计算长距离、表达性强且接近最优的轨迹。


<details>
  <summary>Details</summary>
Motivation: 高速自主导航需要实时生成动态可行、无碰撞且满足状态或执行器约束的敏捷轨迹。基于数值优化的规划方法虽然能系统计算高质量轨迹，但在计算时间和数值稳定性方面存在限制，影响在安全关键场景中的应用。

Method: 提出STITCHER框架，通过创新的规划架构和算法开发，将短轨迹段拼接并使用图搜索来生成长距离轨迹，避免了传统优化方法的计算负担。

Result: 仿真测试显示，STITCHER能在几毫秒内为跨越50m×50m环境的安全轨迹。硬件测试验证了其能实时生成可跟踪路径，并处理非凸约束（如倾斜角和电机力限制）。

Conclusion: STITCHER在实时性能、轨迹质量和约束处理方面优于现代基于优化的规划器，为高速自主导航提供了更可靠的解决方案。

Abstract: Autonomous high-speed navigation through large, complex environments requires
real-time generation of agile trajectories that are dynamically feasible,
collision-free, and satisfy state or actuator constraints. Modern trajectory
planning techniques primarily use numerical optimization, as they enable the
systematic computation of high-quality, expressive trajectories that satisfy
various constraints. However, stringent requirements on computation time and
the risk of numerical instability can limit the use of optimization-based
planners in safety-critical scenarios. This work presents an optimization-free
planning framework called STITCHER that stitches short trajectory segments
together with graph search to compute long-range, expressive, and near-optimal
trajectories in real-time. STITCHER outperforms modern optimization-based
planners through our innovative planning architecture and several algorithmic
developments that make real-time planning possible. Extensive simulation
testing is performed to analyze the algorithmic components that make up
STITCHER, along with a thorough comparison with two state-of-the-art
optimization planners. Simulation tests show that safe trajectories can be
created within a few milliseconds for paths that span the entirety of two 50 m
x 50 m environments. Hardware tests with a custom quadrotor verify that
STITCHER can produce trackable paths in real-time while respecting nonconvex
constraints, such as limits on tilt angle and motor forces, which are otherwise
hard to include in optimization-based planners.

</details>


### [32] [VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation](https://arxiv.org/abs/2510.14902)
*Han Zhao,Jiaxuan Zhang,Wenxuan Song,Pengxiang Ding,Donglin Wang*

Main category: cs.RO

TL;DR: 提出了VLA^2框架，通过集成外部模块（网络检索和物体检测）来增强视觉-语言-动作模型处理分布外对象的能力，在困难级别基准测试中相比基线模型提升了44.2%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在处理训练数据中未见过的对象概念（如新物体描述和纹理）时成功率显著下降，需要解决这种分布外泛化问题。

Method: 基于OpenVLA作为执行骨干，集成外部模块（网络检索和物体检测）为目标对象提供视觉和文本知识，构建了包含三个难度级别的新评估基准。

Result: 在设计的困难级别泛化基准测试中优于当前最先进模型，相比OpenVLA基线在困难级别基准上成功率提升44.2%，在所有定制环境中平均提升20.2%，且不影响域内任务性能。

Conclusion: VLA^2框架通过有效利用外部知识模块，显著提升了VLA模型处理分布外对象的能力，实现了更好的泛化性能。

Abstract: Current vision-language-action (VLA) models, pre-trained on large-scale
robotic data, exhibit strong multi-task capabilities and generalize well to
variations in visual and language instructions for manipulation. However, their
success rate drops significantly when faced with object concepts outside the
training data, such as unseen object descriptions and textures in the dataset.
To address this, we propose a novel agentic framework, VLA^2, which leverages
OpenVLA as the execution backbone and effectively leverages external modules
such as web retrieval and object detection to provide visual and textual
knowledge about target objects to the VLA. This approach mitigates
generalization failure when handling out-of-distribution objects. Based on the
LIBERO simulation environment, we introduced novel objects and object
descriptions to construct a new evaluation benchmark with three difficulty
levels to test the effectiveness of our method. Our framework successfully
outperformed the current state-of-the-art models on our designed hard-level
generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2
achieves a 44.2% improvement in the success rate in the hard-level benchmark
and an average improvement of 20.2% in all customized environments without any
performance degradation on in-domain tasks. Project website:
https://vla-2.github.io.

</details>


### [33] [VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin](https://arxiv.org/abs/2510.14930)
*Binghao Huang,Jie Xu,Iretiayo Akinola,Wei Yang,Balakumar Sundaralingam,Rowland O'Flaherty,Dieter Fox,Xiaolong Wang,Arsalan Mousavian,Yu-Wei Chao,Yunzhu Li*

Main category: cs.RO

TL;DR: VT-Refine是一个视觉触觉策略学习框架，结合真实世界演示、高保真触觉模拟和强化学习来解决精确的双手机器人装配任务。


<details>
  <summary>Details</summary>
Motivation: 人类通过丰富的触觉反馈适应双手机器人装配任务，但仅通过行为克隆难以在机器人中复制这种能力，因为人类演示存在次优性和有限多样性问题。

Method: 首先使用同步视觉和触觉输入在少量演示上训练扩散策略，然后将该策略转移到配备模拟触觉传感器的数字孪生中，通过大规模强化学习进行细化以增强鲁棒性和泛化能力。

Result: 实验结果表明，VT-Refine通过增加数据多样性和实现更有效的策略微调，提高了仿真和真实世界中的装配性能。

Conclusion: 该框架成功解决了接触丰富的双手机器人装配任务，通过结合真实演示、触觉模拟和强化学习实现了有效的sim-to-real迁移。

Abstract: Humans excel at bimanual assembly tasks by adapting to rich tactile feedback
-- a capability that remains difficult to replicate in robots through
behavioral cloning alone, due to the suboptimality and limited diversity of
human demonstrations. In this work, we present VT-Refine, a visuo-tactile
policy learning framework that combines real-world demonstrations,
high-fidelity tactile simulation, and reinforcement learning to tackle precise,
contact-rich bimanual assembly. We begin by training a diffusion policy on a
small set of demonstrations using synchronized visual and tactile inputs. This
policy is then transferred to a simulated digital twin equipped with simulated
tactile sensors and further refined via large-scale reinforcement learning to
enhance robustness and generalization. To enable accurate sim-to-real transfer,
we leverage high-resolution piezoresistive tactile sensors that provide normal
force signals and can be realistically modeled in parallel using
GPU-accelerated simulation. Experimental results show that VT-Refine improves
assembly performance in both simulation and the real world by increasing data
diversity and enabling more effective policy fine-tuning. Our project page is
available at https://binghao-huang.github.io/vt_refine/.

</details>


### [34] [Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion](https://arxiv.org/abs/2510.14947)
*Blake Werner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出分层控制架构(LCA)，将高频本体感觉稳定器与低频感知策略结合，通过两阶段训练方法，在复杂地形上实现比端到端设计更鲁棒的人形机器人运动。


<details>
  <summary>Details</summary>
Motivation: 非结构化环境中的人形机器人运动需要平衡快速低级稳定和慢速感知决策，传统端到端设计在鲁棒性方面存在不足。

Method: 采用分层控制架构：高频本体感觉稳定器+低频紧凑感知策略，通过两阶段训练（盲稳定器预训练+感知微调）。

Result: 在Unitree G1人形机器人上，该方法在楼梯和边缘任务中成功，而单阶段感知策略失败，仿真和硬件实验均表现更优。

Conclusion: 时间尺度的架构分离（而非网络规模或复杂度）是实现鲁棒感知条件运动的关键因素。

Abstract: Robust humanoid locomotion in unstructured environments requires
architectures that balance fast low-level stabilization with slower perceptual
decision-making. We show that a simple layered control architecture (LCA), a
proprioceptive stabilizer running at high rate, coupled with a compact low-rate
perceptual policy, enables substantially more robust performance than
monolithic end-to-end designs, even when using minimal perception encoders.
Through a two-stage training curriculum (blind stabilizer pretraining followed
by perceptual fine-tuning), we demonstrate that layered policies consistently
outperform one-stage alternatives in both simulation and hardware. On a Unitree
G1 humanoid, our approach succeeds across stair and ledge tasks where one-stage
perceptual policies fail. These results highlight that architectural separation
of timescales, rather than network scale or complexity, is the key enabler for
robust perception-conditioned locomotion.

</details>


### [35] [From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance](https://arxiv.org/abs/2510.14952)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Yibo Peng,Tao Huang,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang,Chang Xu*

Main category: cs.RO

TL;DR: RoboGhost是一个无需重定向的框架，通过将人形机器人策略直接基于语言驱动的运动潜变量，实现从语言到动作的直接映射，显著降低延迟并提高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导的人形机器人运动流程存在多阶段处理、累积误差、高延迟和语义与控制耦合弱的问题，需要更直接的从语言到动作的路径。

Method: 采用基于扩散的策略，直接从噪声中解噪可执行动作，使用混合因果变换器-扩散运动生成器确保长期一致性，同时保持稳定性和多样性。

Result: 实验表明RoboGhost显著降低部署延迟，提高成功率和跟踪精度，在真实人形机器人上产生平滑且语义对齐的运动。

Conclusion: 该框架为视觉-语言-动作人形系统提供了通用基础，可自然扩展到图像、音频和音乐等其他模态。

Abstract: Natural language offers a natural interface for humanoid robots, but existing
language-guided humanoid locomotion pipelines remain cumbersome and unreliable.
They typically decode human motion, retarget it to robot morphology, and then
track it with a physics-based controller. However, this multi-stage process is
prone to cumulative errors, introduces high latency, and yields weak coupling
between semantics and control. These limitations call for a more direct pathway
from language to action, one that eliminates fragile intermediate stages.
Therefore, we present RoboGhost, a retargeting-free framework that directly
conditions humanoid policies on language-grounded motion latents. By bypassing
explicit motion decoding and retargeting, RoboGhost enables a diffusion-based
policy to denoise executable actions directly from noise, preserving semantic
intent and supporting fast, reactive control. A hybrid causal
transformer-diffusion motion generator further ensures long-horizon consistency
while maintaining stability and diversity, yielding rich latent representations
for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost
substantially reduces deployment latency, improves success rates and tracking
accuracy, and produces smooth, semantically aligned locomotion on real
humanoids. Beyond text, the framework naturally extends to other modalities
such as images, audio, and music, providing a general foundation for
vision-language-action humanoid systems.

</details>


### [36] [CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions](https://arxiv.org/abs/2510.14959)
*Lizhi Yang,Blake Werner,Massimiliano de Sa Aaron D. Ames*

Main category: cs.RO

TL;DR: CBF-RL框架通过在训练中强制执行控制屏障函数来生成安全的强化学习行为，使学习到的策略内化安全约束，无需在线安全过滤器即可安全部署。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习往往以牺牲安全性为代价来优先考虑性能，而安全违规在现实世界部署中可能导致灾难性后果。现有的安全过滤器方法虽然能确保安全行为，但会导致保守行为，因为RL策略不了解CBF。

Method: 提出CBF-RL框架，通过两个关键属性：(1) 通过CBF项最小化修改名义RL策略以编码安全约束；(2) 在训练中对策略展开进行安全过滤。理论上证明连续时间安全过滤器可以通过闭式表达式在离散时间展开上部署。

Result: CBF-RL使学习到的策略内化安全约束，既强制执行更安全的动作，又偏向更安全的奖励。在导航任务和Unitree G1人形机器人上的消融研究表明，CBF-RL实现了更安全的探索、更快的收敛和鲁棒性能，使人形机器人能够在没有运行时安全过滤器的情况下在真实环境中安全避障和爬楼梯。

Conclusion: CBF-RL框架成功地将安全约束内化到强化学习策略中，实现了无需在线安全过滤器的安全部署，在真实机器人应用中表现出优越的安全性能和鲁棒性。

Abstract: Reinforcement learning (RL), while powerful and expressive, can often
prioritize performance at the expense of safety. Yet safety violations can lead
to catastrophic outcomes in real-world deployments. Control Barrier Functions
(CBFs) offer a principled method to enforce dynamic safety -- traditionally
deployed \emph{online} via safety filters. While the result is safe behavior,
the fact that the RL policy does not have knowledge of the CBF can lead to
conservative behaviors. This paper proposes CBF-RL, a framework for generating
safe behaviors with RL by enforcing CBFs \emph{in training}. CBF-RL has two key
attributes: (1) minimally modifying a nominal RL policy to encode safety
constraints via a CBF term, (2) and safety filtering of the policy rollouts in
training. Theoretically, we prove that continuous-time safety filters can be
deployed via closed-form expressions on discrete-time roll-outs. Practically,
we demonstrate that CBF-RL internalizes the safety constraints in the learned
policy -- both enforcing safer actions and biasing towards safer rewards --
enabling safe deployment without the need for an online safety filter. We
validate our framework through ablation studies on navigation tasks and on the
Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster
convergence, and robust performance under uncertainty, enabling the humanoid
robot to avoid obstacles and climb stairs safely in real-world settings without
a runtime safety filter.

</details>


### [37] [RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks](https://arxiv.org/abs/2510.14968)
*Mingxuan Yan,Yuping Wang,Zechun Liu,Jiachen Li*

Main category: cs.RO

TL;DR: 提出了一种基于检索的演示分解器(RDD)，通过将分解后的子任务视觉特征与低层视觉运动策略训练数据对齐，自动将演示分解为子任务，解决了传统方法中启发式子任务与策略训练数据不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统分层视觉-语言-动作框架中，VLM规划器需要人工标注或启发式规则来分解任务，但启发式子任务可能与视觉运动策略的训练数据存在显著偏差，导致任务性能下降。

Method: RDD通过检索视觉特征相似性，自动将演示分解为与低层视觉运动策略训练数据对齐的子任务，无需人工标注或启发式规则。

Result: 在仿真和真实世界任务中均优于最先进的子任务分解器，在不同设置下表现出鲁棒性。

Conclusion: RDD提供了一种自动、有效的演示分解方法，解决了传统方法中任务分解与策略训练数据不匹配的问题，提升了分层VLA框架的性能。

Abstract: To tackle long-horizon tasks, recent hierarchical vision-language-action
(VLAs) frameworks employ vision-language model (VLM)-based planners to
decompose complex manipulation tasks into simpler sub-tasks that low-level
visuomotor policies can easily handle. Typically, the VLM planner is finetuned
to learn to decompose a target task. This finetuning requires target task
demonstrations segmented into sub-tasks by either human annotation or heuristic
rules. However, the heuristic subtasks can deviate significantly from the
training data of the visuomotor policy, which degrades task performance. To
address these issues, we propose a Retrieval-based Demonstration Decomposer
(RDD) that automatically decomposes demonstrations into sub-tasks by aligning
the visual features of the decomposed sub-task intervals with those from the
training data of the low-level visuomotor policies. Our method outperforms the
state-of-the-art sub-task decomposer on both simulation and real-world tasks,
demonstrating robustness across diverse settings. Code and more results are
available at rdd-neurips.github.io.

</details>
