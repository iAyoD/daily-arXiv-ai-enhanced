<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 30]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 本文探讨了当前基于token补全的大语言模型(LLMs)的推理机制，认为它们本质上是随机的模式匹配而非真正的溯因推理，只是通过训练数据模仿了人类推理的表面形式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是澄清LLMs的推理本质，揭示其表面看似溯因推理的能力实际上只是对训练数据中人类推理模式的模仿，而非真正的逻辑推理过程。

Method: 通过分析LLMs的随机本质和token补全机制，与人类溯因推理进行对比，使用具体示例展示LLMs如何产生看似合理的想法、模仿常识推理和给出解释性答案。

Result: LLMs能够生成看似合理的想法、模仿常识推理并提供解释性答案，但这些输出缺乏真理基础、语义理解、验证机制和真实理解，只是对训练数据中推理结构的模式匹配。

Conclusion: LLMs具有随机基础但表面呈现溯因推理特征的双重性质，这对评估和应用LLMs有重要影响。它们可以辅助创意生成和人类思考，但输出需要批判性评估，因为它们无法识别真理或验证解释。文章最后回应了五个反对观点并指出了分析的限制。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [2] [Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models](https://arxiv.org/abs/2512.10110)
*Yumou Wei,John Stamper,Paulo F. Carvalho*

Main category: cs.CL

TL;DR: 使用小型语言模型(SLMs)进行自动问题生成，通过"生成-验证"策略和概率推理生成高质量问题，评估显示生成的问题质量良好。


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型在自动问题生成中的应用，作为大型语言模型在学习分析研究中的补充，旨在利用SLMs的优势生成高质量问题。

Method: 采用"生成-验证"策略：首先进行扩展生成创建大量候选问题，然后通过基于概率推理的选择性验证进行精炼，利用SLMs的文本生成和概率推理能力。

Result: 通过人类专家和大型语言模型评估，大多数评判者(人类或LLM)认为生成的问题具有清晰答案，并且与预期学习目标基本一致。

Conclusion: 当通过精心设计的管道引导并充分利用其优势时，小型语言模型能够有效生成高质量问题，为学习分析提供了新的技术途径。

Abstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.

</details>


### [3] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: DeepNews框架通过模拟专业记者的认知过程，解决了长文本生成中的"不可能三角"问题，在金融报道中显著降低了幻觉率并提高了逻辑连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在垂直领域长文本生成中存在"不可能三角"瓶颈：难以同时实现低幻觉率、深度逻辑连贯性和个性化表达。研究发现这是由于现有生成范式陷入了"统计平滑陷阱"，忽略了专家写作所需的高熵信息获取和结构化认知过程。

Method: 提出DeepNews框架，模拟资深金融记者的隐式认知过程，包含三个核心模块：1)基于信息觅食理论的双粒度检索机制，强制10:1饱和信息输入比；2)基于领域专家知识库（叙事模式）和原子块的模式引导战略规划；3)对抗约束提示技术，使用节奏中断和逻辑迷雾等策略打破模型生成文本的概率平滑性。

Result: 实验发现深度金融报道中存在明显的"知识悬崖"：当检索上下文低于15,000字符时，内容真实性崩溃；而高冗余输入超过30,000字符时，无幻觉率稳定在85%以上。在生态效度盲测中，基于上一代模型的DeepNews系统获得了25%的提交接受率，显著优于SOTA模型的零样本生成（0%接受率）。

Conclusion: 通过显式建模专家认知过程，DeepNews框架成功解决了长文本生成中的"不可能三角"问题，证明了结构化认知建模在提升专业领域文本生成质量方面的有效性。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [4] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 提出两阶段提示框架，从短评文本推断显式和隐式用户画像，用于生成个性化回复，无需微调模型


<details>
  <summary>Details</summary>
Motivation: 在用户信息有限（如外卖平台）的场景下，个性化回复生成面临挑战。大语言模型缺乏上下文用户数据时会产生通用回复，降低参与度和效果

Method: 两阶段提示框架：1）从短评文本推断显式（用户陈述偏好）和隐式（人口统计或风格线索）用户画像；2）将推断的用户画像属性融入回复生成提示。通过调整解码温度鼓励多样且忠实的生成

Result: 在韩国外卖应用的真实数据集上评估，在精确度、多样性和语义一致性方面表现良好。用户画像增强提示能有效提升自动回复的相关性和个性化

Conclusion: 用户画像增强提示框架能显著提升个性化回复生成的相关性和个性化程度，且无需模型微调，在用户信息有限的场景下具有实用价值

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [5] [Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150)
*Lama Alssum,Hani Itani,Hasan Abed Al Kader Hammoud,Philip Torr,Adel Bibi,Bernard Ghanem*

Main category: cs.CL

TL;DR: 该论文研究大语言模型在适应新任务时的安全性退化问题，将安全保护视为持续学习问题，并提出多种CL方法在微调服务场景中有效缓解安全性下降。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，其安全性对齐变得越来越重要。研究发现模型在适应新任务时会出现安全性退化，这归因于灾难性遗忘。在微调即服务的场景下，用户上传数据给服务提供商以获得定制化模型，需要解决如何在保持任务性能的同时维护模型安全性。

Method: 将微调中的安全保护问题框架化为持续学习问题，采用了三种CL方法：基于正则化的方法、基于记忆的方法和模型融合方法。研究考虑了两种场景：良性用户数据和中毒用户数据。在三个下游任务（GSM8K、SST2、Code）和三个模型家族（LLaMA2-7B、Mistral-7B、Gemma-2B）上进行系统评估。

Result: 持续学习方法相比标准微调始终获得更低的攻击成功率。其中DER方法在CL方法和现有安全保护基线中表现最佳，同时保持了任务效用。这些发现在不同任务和模型家族中具有普遍性。

Conclusion: 持续学习是保护大语言模型在微调过程中安全性的实用解决方案，特别是在微调即服务的场景下，能够有效缓解安全性退化问题。

Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.

</details>


### [6] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: AutoMedic：一个多智能体仿真框架，用于自动化评估大语言模型作为临床对话代理的性能，通过将静态医疗问答数据集转化为虚拟患者档案，实现多轮临床对话评估。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域LLM评估存在局限性：静态问答基准无法评估动态交互临床对话能力，缺乏多维度评估策略，且动态临床场景因患者状态和交互轨迹组合空间巨大而难以标准化评估。

Method: 开发AutoMedic多智能体仿真框架，将现成的静态医疗QA数据集转化为虚拟患者档案，实现LLM代理之间的真实临床多轮对话，使用CARE指标进行多维度评估（临床准确性、效率/策略、共情、鲁棒性）。

Result: AutoMedic被证明是临床对话代理的有效自动化评估框架，经人类专家验证有效，为对话医疗应用中LLM的有效开发提供实用指南。

Conclusion: AutoMedic解决了医疗LLM动态交互评估的挑战，提供了一个标准化、自动化的多维度评估框架，有助于推动LLM在临床对话应用中的安全可信发展。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [7] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 本文探讨了将英语训练的视觉语言模型适配到其他语言的挑战，比较了翻译管道、LoRA微调和两阶段微调等方法的性能与计算成本，发现数据集翻译是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型主要在英语上发展，限制了非英语用户的可访问性，需要将这些能力扩展到更广泛的语言范围。

Method: 比较了三种方法：1）基于翻译的管道方法；2）LoRA微调；3）分离视觉适配和语言适配的两阶段微调策略。使用翻译后的多模态基准测试和母语专家手动评估进行验证。

Result: 数据集翻译是多语言VLM性能的主要瓶颈，数据质量限制了训练和评估的有效性。

Conclusion: 未来工作应聚焦于原生语言数据集的收集和改进的翻译策略。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [8] [Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale](https://arxiv.org/abs/2512.10398)
*Zhaodong Wang,Zhenting Qi,Sherman Wong,Nathan Hu,Samuel Lin,Jun Ge,Erwin Gao,Yining Yang,Ben Maurer,Wenlin Chen,David Recordon,Yilun Du,Minlan Yu,Ying Zhang*

Main category: cs.CL

TL;DR: Confucius Code Agent (CCA) 是一个开源AI软件工程师，在工业规模任务上表现优异，在SWE-Bench-Pro上达到54.3%的Resolve@1性能，显著优于现有编码代理。


<details>
  <summary>Details</summary>
Motivation: 现实AI软件工程需要能够处理大规模代码库、保持长期记忆、协调复杂工具链的编码代理。现有开源代理在工业规模任务上表现不足，而专有代理缺乏可扩展性、可解释性和可控性。

Method: 基于Confucius SDK构建，该平台包含：统一编排器与分层工作记忆用于长上下文推理；持久笔记系统用于跨会话持续学习；模块化扩展模块用于工具使用；元代理通过构建-测试-改进循环自动化配置合成、评估和优化。

Result: 在SWE-Bench-Pro上达到54.3%的Resolve@1性能，创下新纪录，显著优于之前的编码代理。CCA在真实世界软件工程任务上表现出色。

Conclusion: Confucius SDK和CCA为AI代理提供了透明、可扩展、可复现的基础，弥合了研究原型与生产级系统之间的差距，支持工业规模的代理开发和部署。

Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.

</details>


### [9] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: FA预训练的LLMs可通过SWAA方法有效适配滑动窗口注意力，无需重新预训练，在保持长上下文性能的同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: Transformer LLMs的自注意力机制在处理长上下文时存在二次方复杂度问题，滑动窗口注意力(SWA)可降低为线性复杂度，但直接在FA预训练模型上使用SWA会导致性能严重下降，需要探索适配方法。

Method: 提出滑动窗口注意力适配(SWAA)方法，包含五种技术组合：(1)仅在预填充阶段使用SWA；(2)保留"sink"令牌；(3)交错FA/SWA层；(4)思维链(CoT)；(5)微调。

Result: 实验表明SWA适配是可行但非平凡的：单一方法不足，但特定协同组合能有效恢复原始长上下文性能，分析了不同配置的性能-效率权衡。

Conclusion: FA预训练的LLMs可通过SWAA方法有效适配SWA，无需重新预训练，为不同场景提供了推荐的配置方案，实现了性能与效率的良好平衡。

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [10] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: CoopRAG：一种新颖的检索增强生成框架，通过检索器和LLM的协同工作，以及检索器模型不同层之间的协同，提高问答任务的准确性和减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在简单和多跳问答中仍然存在检索错误和幻觉问题，需要更有效的框架来缓解LLM生成事实不准确输出的倾向。

Method: 1) 将问题分解为子问题和带有不确定位置掩码的推理链；2) 使用子问题和推理链增强检索相关文档；3) 通过对比检索器不同层来重排文档；4) 利用LLM填充掩码位置重建推理链。

Result: CoopRAG在三个多跳QA数据集和一个简单QA数据集上，在检索和QA性能方面均优于最先进的QA方法。

Conclusion: CoopRAG通过检索器和LLM的协同工作，以及检索器模型内部的协同，有效提高了问答任务的准确性和可靠性。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [11] [T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/abs/2512.10430)
*Dmitrii Stoianov,Danil Taranets,Olga Tsymboi,Ramil Latypov,Almaz Dautov,Vladislav Kruglikov,Nikita Surkov,German Abramov,Pavel Gein,Dmitry Abulkhanov,Mikhail Gashkov,Viktor Zelenkovskiy,Artem Batalov,Aleksandr Medvedev,Anatolii Potapov*

Main category: cs.CL

TL;DR: T-pro 2.0是一个开源俄语大语言模型，支持混合推理和高效推理，包含模型权重、指令数据集、数学推理基准和推理优化工具


<details>
  <summary>Details</summary>
Motivation: 为俄语社区提供可复现、可扩展的开源LLM系统，支持俄语推理研究和高效推理应用开发

Method: 使用西里尔字母密集分词器，采用EAGLE推测解码流水线降低延迟，支持直接回答和推理轨迹生成

Result: 发布了完整的开源资源包：模型权重、T-Wix 50万指令语料、T-Math推理基准、EAGLE权重，并提供公开Web演示展示推理加速效果

Conclusion: T-pro 2.0为构建和评估高效实用的俄语LLM应用提供了可访问的开源系统

Abstract: We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.

</details>


### [12] [Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature](https://arxiv.org/abs/2512.10435)
*Agniva Maiti,Prajwal Panth,Suresh Chandra Satapathy*

Main category: cs.CL

TL;DR: SRAP框架通过两阶段方法检测并恢复对抗性剽窃文本中的"扭曲短语"，使用统计异常检测和语义重建技术，在科学文本中实现23.67%的恢复准确率。


<details>
  <summary>Details</summary>
Motivation: 科学文献的完整性正受到对抗性文本生成技术的威胁，特别是使用自动改写工具掩盖剽窃行为。这些工具生成"扭曲短语"（如用"counterfeit consciousness"替代"artificial intelligence"），保留局部语法但模糊原始来源。现有检测方法依赖静态黑名单或通用领域语言模型，对新出现的混淆方式漏报率高，且无法确定剽窃内容来源。

Method: 提出SRAP框架，采用两阶段架构：1) 使用领域特定掩码语言模型（SciBERT）进行基于伪困惑度的统计异常检测；2) 使用密集向量检索（FAISS）和句子级对齐（SBERT）进行基于来源的语义重建。该方法不仅能检测异常，还能数学上恢复原始术语。

Result: 在对抗性科学文本平行语料库上的实验显示，零基线方法完全失败（0.00%恢复准确率），而检索增强方法达到23.67%恢复准确率，显著优于基线方法。研究还表明，在术语密集的科学文本中，静态决策边界对于稳健检测是必要的，因为动态阈值在高方差下会失效。

Conclusion: SRAP框架能够检测对抗性剽窃文本中的扭曲短语，并通过语义重建技术将混淆表达链接回最可能的源文档，为科学文献的完整性提供法医分析能力。该方法在恢复准确率上显著优于现有方法，为解决科学剽窃检测中的新挑战提供了有效方案。

Abstract: The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.

</details>


### [13] [Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT](https://arxiv.org/abs/2512.10440)
*Nour El Houda Ben Chaabene,Hamza Hammami*

Main category: cs.CL

TL;DR: 通过知识图谱增强大语言模型，解决事实不一致问题，提升知识密集型任务表现


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然擅长自然语言处理，但缺乏结构化知识，导致事实不一致问题。需要增强模型的事实可靠性和推理能力。

Method: 通过KG-BERT将知识图谱集成到大语言模型中，实现知识增强和基础推理

Result: 在问答和实体链接等知识密集型任务上取得显著提升，提高了事实可靠性

Conclusion: 知识图谱集成能有效增强大语言模型的事实基础和推理能力，为下一代上下文感知的LLMs提供支持

Abstract: Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.

</details>


### [14] [Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis](https://arxiv.org/abs/2512.10441)
*Nour El Houda Ben Chaabene,Hamza Hammami,Laid Kahloul*

Main category: cs.CL

TL;DR: 开发了一个结合LLM、知识图谱增强BERT和双向LSTM的心理学感知对话代理，通过多模态数据实时分类学生认知和情感状态，在教育环境中提升学习表现和情感健康


<details>
  <summary>Details</summary>
Motivation: 现有教育聊天机器人通常只专注于辅导或情感支持，缺乏能够同时处理认知和情感状态的综合系统。需要开发一个能够实时理解学生认知参与度、压力和概念理解的多模态系统，以提供更全面的教育支持。

Method: 结合大型语言模型(LLMs)、知识图谱增强BERT(KG-BERT)和带注意力的双向LSTM网络，利用多模态数据（文本语义、语音韵律特征、时间行为趋势）实时分类学生的认知和情感状态。

Result: 针对大学生的试点研究表明，相比基线方法，该系统提高了学习动机、降低了压力水平，并带来了适度的学业提升。证明了语义推理、多模态融合和时间建模整合的有效性。

Conclusion: 通过整合语义推理、多模态融合和时间建模，可以开发出支持自适应、以学生为中心的教育干预系统，在提升学习表现的同时改善情感健康，展示了心理学感知对话代理在教育领域的应用前景。

Abstract: This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.

</details>


### [15] [Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs](https://arxiv.org/abs/2512.10453)
*Lars G. B. Johnsen*

Main category: cs.CL

TL;DR: 大型语言模型仅通过表层形式训练就能区分语法和不合语法的句子，表现出对句法结构的敏感性，而非仅仅是线性顺序。


<details>
  <summary>Details</summary>
Motivation: 传统生成语法中，语法性对比（如主语-助动词倒装和寄生空位许可）被视为内部层级语法的证据。本研究旨在测试仅通过表层形式训练的大型语言模型是否也能复现这些对比，从而暗示其具有底层结构表征。

Method: 聚焦两个经典结构：主语-助动词倒装（测试主语边界识别）和寄生空位许可（测试抽象依赖结构）。使用GPT-4和LLaMA-3等模型，通过提示引出可接受性评分进行评估。

Result: LLMs在两种结构中都能可靠地区分语法和不合语法的变体，表明它们对结构敏感，而不仅仅是线性顺序。结构概括从表层形式的预测训练中涌现，显示出对句法的功能性敏感性，无需显式编码。

Conclusion: 仅通过表层形式训练的大型语言模型能够表现出对句法结构的敏感性，支持结构概括可以从预测训练中涌现，无需显式语法编码。

Abstract: What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.
  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.

</details>


### [16] [XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs](https://arxiv.org/abs/2512.10545)
*Iñaki Lacunza,José Javier Saiz,Alexander Shvets,Aitor Gonzalez-Agirre,Marta Villegas*

Main category: cs.CL

TL;DR: 该论文提出XDoGE方法优化多语言LLM训练中的语言分布，通过代理模型确定最佳语言权重，并训练了专注于伊比利亚语言的新模型IberianLLM-7B-Instruct。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型过度依赖英语等高资源语言，导致在中低资源语言上表现不佳。需要解决语言分布不平衡问题，提升多语言能力。

Method: 提出XDoGE算法扩展DoGE到多语言设置，使用代理模型优化语言分布权重；通过数据重缩放训练全尺寸模型，支持从头训练和持续预训练；针对6种不同资源水平的伊比利亚语言进行实验。

Result: 研究了重复低资源语言数据和对高资源语言欠采样的效果，使用IberoBench框架评估；发布了新的IberianLLM-7B-Instruct模型，该模型专注于伊比利亚语言和英语。

Conclusion: XDoGE方法能有效优化多语言LLM的语言分布，提升中低资源语言性能；新发布的IberianLLM-7B-Instruct模型在多语言任务中表现良好。

Abstract: Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.

</details>


### [17] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: ICL在因果推理中不可靠，编码器和编码器-解码器架构通过微调在因果推理上优于仅解码器架构，特别是在小规模模型和分布变化下。


<details>
  <summary>Details</summary>
Motivation: 研究ICL在因果推理中的作用和性能，因果推理需要多跳组合和严格合取控制，但ICL可能依赖输入中的虚假词汇关系导致误导结果。

Method: 比较微调后的编码器、编码器-解码器和仅解码器架构在零样本和少样本ICL下的表现，测试场景包括自然语言和非自然语言。

Result: ICL单独不足以进行可靠的因果推理，常过度关注不相关的输入特征。仅解码器模型对分布变化脆弱，而微调的编码器和编码器-解码器模型能更稳健地泛化，包括非自然语言场景。仅解码器架构需要大规模才能匹配或超越前两者。

Conclusion: 对于成本效益高、短期稳健的因果推理，建议使用编码器或编码器-解码器架构并进行针对性微调。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [18] [RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems](https://arxiv.org/abs/2512.10575)
*Hang Ding,Qiming Feng,Dongqi Liu,Qi Zhao,Tao Yao,Shuo Wang,Dongsheng Chen,Jian Li,Zhenye Gan,Jiangning Zhang,Chengjie Wang,Yabiao Wang*

Main category: cs.CL

TL;DR: RoleRMBench是首个角色扮演对话奖励建模基准，揭示了现有奖励模型在主观开放领域表现不佳，提出RoleRM模型通过连续隐式偏好方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在主观开放领域（如角色扮演）表现严重退化，难以捕捉基于角色的细微人类判断，需要专门针对角色扮演对话的奖励建模基准和方法

Method: 提出RoleRMBench基准，覆盖7个细粒度能力；提出RoleRM模型，采用连续隐式偏好（CIP）方法，将主观评估重构为多结构策略下的连续一致成对监督

Result: RoleRMBench评估显示通用奖励模型与人类判断存在巨大差距；RoleRM在平均性能上超越强开源和闭源奖励模型超过24%，在叙事连贯性和风格保真度方面获得显著提升

Conclusion: 连续偏好表示和标注一致性对主观对齐至关重要，为以人为中心的对话系统主观对齐奠定了基础，RoleRMBench和RoleRM填补了角色扮演奖励建模的空白

Abstract: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.

</details>


### [19] [AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: AgriGPT-Omni是一个农业全模态框架，整合了语音、视觉和文本，通过三阶段训练范式构建了首个农业全模态模型，并提出了首个农业三模态基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在农业应用中面临三大限制：缺乏多语言语音数据、统一的多模态架构以及全面的评估基准。这些限制阻碍了农业智能系统的发展。

Method: 1. 构建可扩展的数据合成与收集管道，将农业文本和图像转换为训练数据，创建了最大的农业语音数据集（49.2万合成+1400真实样本，覆盖6种语言）。2. 采用三阶段训练范式：文本知识注入、渐进式多模态对齐、基于GRPO的强化学习。3. 提出AgriBench-Omni-2K基准测试，涵盖多样化的语音-视觉-文本任务和多语言切片。

Result: AgriGPT-Omni在多语言多模态推理以及真实世界语音理解方面显著优于通用基线模型。实验证明了该框架在农业领域的有效性。

Conclusion: AgriGPT-Omni通过统一的多模态框架解决了农业AI的关键瓶颈，促进了可重复研究、包容性农业智能以及低资源地区的可持续AI发展。所有模型、数据、基准和代码都将开源。

Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.

</details>


### [20] [From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages](https://arxiv.org/abs/2512.10630)
*Smiljana Antonijevic Ubois*

Main category: cs.CL

TL;DR: 该研究以塞尔维亚语为例，探讨低资源语言在AI时代面临的结构性挑战，提出基于CARE原则的Data Care框架，旨在从数据设计源头解决文化偏见问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要基于英语等优势语言训练，对低资源语言（如塞尔维亚语）的表征往往反映了源语言材料中的文化和语言偏见。研究旨在探索影响低资源语言技术发展的结构、历史和社会技术因素。

Method: 采用质性研究方法，对10位学者和从业者（包括语言学家、数字人文研究者和AI开发者）进行半结构化访谈，分析塞尔维亚语面临的挑战，并提出Data Care框架。

Result: 研究发现塞尔维亚语面临历史文本遗产破坏、浅层音译、依赖英语训练模型、数据偏见、缺乏文化特异性数据集等挑战。这些因素导致工程优先方法忽视语言细微差别。

Conclusion: 提出基于CARE原则（集体利益、控制权、责任、伦理）的Data Care框架，将偏见缓解从技术修复转变为语料库设计、标注和治理的核心组成部分，为构建包容、可持续、文化基础的语言技术提供可复制模型。

Abstract: Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.

</details>


### [21] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出一个全面的数据偏见检测与缓解管道，包含四个组件，用于处理表示偏见和刻板印象两种数据偏见类型，并通过人类验证和模型微调进行双重评估。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型的文本数据存在多方面偏见表现，包括有害语言和倾斜的人口分布。欧盟AI法案等法规要求识别和减轻针对受保护群体的数据偏见，但缺乏实用的操作指导。

Method: 提出四组件管道：1) 基于质量标准的LLM生成相关群体标签词表；2) 使用人口统计表示分数量化表示偏见；3) 社会语言学过滤检测和缓解刻板印象；4) 语法和上下文感知的反事实数据增强补偿表示偏见。

Result: 成功减少了文本数据集中的表示偏见和显性刻板印象。然而，在偏见基准测试中，使用去偏见数据微调的LLM（0.6B-8B参数）并未一致表现出改进性能，暴露了当前评估方法的差距。

Conclusion: 虽然提出的方法能有效减少数据偏见，但数据去偏见不一定能转化为模型偏见的减少，突显了需要更有针对性的数据操作来解决显现的模型偏见，以及改进评估方法的必要性。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [22] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: OPV（基于结果的流程验证器）通过验证长推理链中总结结果的过程，实现准确高效的验证，解决了现有验证器在长推理链中检测错误的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的验证器无法检查长推理链中的不可靠中间步骤，而基于过程的验证器由于人工标注成本过高导致高质量标注稀缺，难以可靠检测复杂长推理链中的错误。

Method: 提出OPV验证器，通过验证长推理链中总结结果的过程来实现准确高效验证。采用迭代主动学习框架，通过专家标注逐步提升验证能力，使用拒绝微调和RLVR训练新OPV。

Result: OPV在held-out基准上达到SOTA结果（F1 83.1 vs Qwen3-Max-Preview的76.3），有效检测合成数据集中的假阳性，与专家评估高度一致。与策略模型协作时显著提升性能（如DeepSeek-R1-Distill-Qwen-32B在AIME2025上从55.2%提升到73.3%）。

Conclusion: OPV通过验证总结结果的过程，实现了对长推理链的准确高效验证，在减少标注成本的同时显著提升了验证性能，具有广泛的适用性。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [23] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: TRIDENT是一个三层调度员支持架构，通过加勒比口音优化的ASR、本地实体提取和生物声学痛苦检测，在ASR失败时仍能结构化紧急呼叫输入，确保加勒比口音人群公平获得国家分诊协议服务。


<details>
  <summary>Details</summary>
Motivation: 现有紧急语音识别系统对非标准英语变体（特别是加勒比口音）存在系统性性能下降，导致加勒比人群在紧急服务中存在关键差距。需要开发能够处理口音变化的系统，确保公平获得紧急分诊服务。

Method: 三层架构设计：1) 加勒比口音优化的自动语音识别；2) 通过大语言模型进行本地实体提取；3) 生物声学痛苦检测。关键洞察：低ASR置信度可作为队列优先级信号，特别是与升高的声音痛苦标记结合时。实体提取层捕获临床指标，弥补副语言特征遗漏。

Result: 建立了口音弹性紧急AI框架，确保加勒比口音能够公平获得国家分诊协议（ESI常规操作和START大规模伤亡事件）。系统在灾难场景下支持离线操作。加勒比紧急呼叫的实证验证是未来工作。

Conclusion: TRIDENT通过将低ASR置信度重新定义为有价值的优先级信号，结合声音痛苦检测和语义分析，创建了能够处理口音变化的紧急响应系统。这为口音弹性紧急AI建立了框架，确保边缘化语言群体获得公平的紧急服务。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>


### [24] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 提出OPV验证器，通过总结长思维链的结果来验证推理过程，结合主动学习和专家标注实现高效准确验证，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的验证器无法检查长思维链中的不可靠中间步骤，而基于过程的验证器由于人工标注成本高昂导致高质量标注稀缺，难以可靠检测复杂长思维链中的错误。

Method: 提出基于结果的流程验证器OPV，通过总结长思维链的结果来验证推理过程。采用迭代主动学习框架，通过专家标注不确定案例，使用拒绝微调和RLVR训练新OPV。

Result: 在OPV-Bench上取得83.1的F1分数，优于Qwen3-Max-Preview的76.3。能有效检测合成数据集中的假阳性，与专家评估高度一致。与策略模型协作时显著提升性能，如将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升到73.3%。

Conclusion: OPV验证器通过总结结果验证推理过程，实现了准确高效的验证和大规模标注，在多个任务上表现出优越性能和广泛适用性。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [25] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 通过模型缩放而非持续预训练来高效适应新语言，大模型在数据效率上表现更好，能减少灾难性遗忘，但语言模型合并仍不如联合多语言训练有效。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多语言模型在中等和低资源语言上表现不佳，且语言特定适应模型在较小规模时表现更好。需要探索更高效的预训练模型适应新目标语言的方法。

Method: 通过FLOP匹配的模型进行全面的缩放消融实验，比较放大英语基础模型与标准持续预训练的效果，并探索缩放后的语言特定模型合并构建模块化多语言系统。

Result: 当接触足够目标语言数据后，放大的模型能匹配或超越在更多数据上持续预训练的小模型，显示缩放对数据效率的益处。缩放还有助于保留基础模型的英语能力，减少灾难性遗忘。模型合并效果仍不如联合多语言训练，但放大的合并模型表现更好。

Conclusion: 缩放是适应新语言的高效策略，能提高数据效率并减少遗忘。虽然模型合并目前效果有限，但缩放能改善合并性能，且不同合并方法存在显著差异，表明通过专门的语言级集成方法有改进潜力。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [26] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: LLMs在印度临床应用中面临罗马化文本的性能下降问题，可能导致数百万次分类错误


<details>
  <summary>Details</summary>
Motivation: 印度语言使用者常使用罗马化文本而非原生文字，但现有研究很少用真实数据评估这种书写变体对LLMs可靠性的影响，特别是在母婴健康分诊等高风险临床应用中

Method: 在真实世界用户生成查询数据集上对主流LLMs进行基准测试，涵盖五种印度语言和尼泊尔语，比较罗马化文本与原生文字的性能差异

Result: 罗马化文本导致性能持续下降，F1分数比原生文字低5-12个百分点；在合作母婴健康组织中，这种差距可能导致近200万次额外分诊错误；性能差距并非临床推理失败，LLMs通常能正确推断罗马化查询的语义意图，但在存在罗马化输入的正字法噪声时，最终分类输出仍然脆弱

Conclusion: 研究揭示了基于LLM的健康系统中一个关键的安全盲点：看似理解罗马化输入的模型可能仍无法可靠地对其采取行动，这对高风险临床应用的可靠性构成威胁

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


### [27] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: FACTS Leaderboard是一个综合评估语言模型事实准确性的在线评测套件，包含四个子评测板：多模态、参数知识、搜索和文档引用，通过自动化评分模型评估模型在各种场景下生成事实准确文本的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估语言模型事实准确性的标准评测体系。不同模型在不同场景下的表现差异很大，需要一套综合的评测方法来全面衡量模型在各种实际应用中的事实准确性。

Method: 创建包含四个子评测板的综合评测套件：1) FACTS Multimodal评估图像问答的事实性；2) FACTS Parametric评估模型参数中的世界知识；3) FACTS Search评估信息检索场景中的事实性；4) FACTS Grounding (v2)评估长文本回答是否基于提供的文档。使用自动化评分模型进行评分，最终得分为四个部分的平均值。

Result: 开发了一个全面的在线评测平台，包含公开和私有数据集，支持外部参与同时保护评测完整性。平台已在Kaggle上发布，并将持续维护更新。

Conclusion: FACTS Leaderboard提供了一个全面、平衡且稳健的语言模型事实性评估框架，能够综合评估模型在各种实际场景下生成事实准确文本的能力，为模型开发和比较提供了重要基准。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [28] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion是一个融合集成方法，通过结合传统Transformer分类器和LLMs的优势，实现准确且成本感知的文本分类，支持多类和多标签任务。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer分类器在文本分类中表现良好，但大型语言模型(LLMs)具有更强的推理能力。然而，LLMs成本高、延迟大。需要一种方法能结合两者的优势，同时考虑实际部署中的成本、延迟等因素。

Method: 1. 将传统Transformer分类器(如RoBERTa)的嵌入向量与LLMs(如GPT、Gemini、DeepSeek)通过结构化提示工程获得的每类分数进行拼接；2. 将拼接后的联合表示输入到紧凑的多层感知机(FusionMLP)中生成最终预测；3. 提供简单的高级接口(AutoFusionClassifier)和灵活API。

Result: 在AG News上达到92.4%准确率，在10类Reuters 21578主题分类上达到92.3%准确率。能够实现准确性、延迟和成本之间的实用权衡。

Conclusion: LabelFusion通过融合传统Transformer分类器和LLMs的优势，实现了鲁棒的文本分类性能，同时提供了实际部署中的成本-准确性权衡能力，是一个实用且高效的融合集成解决方案。

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


### [29] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 使用计算文本分析研究《霍比特人》对话的情感基调，发现整体积极、平静，随着故事发展角色能动性增强，情感在紧张与舒适间循环。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过计算文本分析方法揭示文学作品中的微妙情感结构，特别是分析托尔金《霍比特人》对话的情感节奏和情感调节模式。

Method: 使用正则表达式提取对话文本，进行预处理，然后使用NRC-VAD词典对情感维度（效价、唤醒度、支配度）进行量化评分。

Result: 对话整体保持积极（高效价）和冷静（低唤醒度）的基调，随着故事进展角色能动性（支配度）逐渐增强，情感在危险兴奋与幽默慰藉间平衡循环。

Conclusion: 计算工具与文学阐释结合能有效揭示文学作品中的情感结构，展示《霍比特人》中塑造叙事的稳定节奏和情感调节模式。

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [30] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 评估多模态大语言模型在政治视频情感分析中的表现，发现在理想条件下表现良好，但在真实议会辩论中效果不佳


<details>
  <summary>Details</summary>
Motivation: 虽然多模态生成AI在情感分析方面前景广阔，但缺乏关于其在政治沟通中情感分析有效性的证据，需要填补这一研究空白

Method: 使用两个互补的人工标注视频数据集，评估当前多模态大语言模型在视频情感唤起分析中的表现，包括理想条件和真实议会辩论场景

Result: 在理想条件下，mLLMs的情感唤起评分高度可靠且几乎没有人口统计偏差；但在真实议会辩论录音中，mLLMs的情感唤起评分表现不佳，可能对下游统计推断产生负面影响

Conclusion: 需要持续、彻底评估新兴生成AI方法在政治分析中的应用，并提供了一个可复制的评估框架

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [31] [Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.10071)
*Junjie Bai,Yu-Wei Chao,Qizhi Chen,Jinwei Gu,Moo Jin Kim,Zhaoshuo Li,Xuan Li,Tsung-Yi Lin,Ming-Yu Liu,Nic Ma,Kaichun Mo,Delin Qu,Shangkun Sun,Hongchi Xia,Fangyin Wei,Xiaohui Zeng*

Main category: cs.RO

TL;DR: 该论文介绍了2025 BEHAVIOR挑战赛的解决方案，基于π₀.₅模型，通过系统研究训练技术和数据效果，在预训练和后训练阶段展示扩展能力，获得第二名并显著超越其他参赛方案。


<details>
  <summary>Details</summary>
Motivation: BEHAVIOR挑战赛旨在追踪物理智能体在模拟环境中解决长期任务方面的进展，BEHAVIOR-1K专注于日常家庭任务，这些任务引入了现实环境中的长期移动操作挑战，弥合当前研究与真实世界、以人为中心应用之间的差距。

Method: 基于π₀.₅模型，通过系统构建解决方案，研究训练技术和数据的效果，进行仔细的消融实验，展示预训练和后训练阶段的扩展能力。

Result: 在2025 BEHAVIOR挑战赛中获得非常接近的第二名，并显著超越其他提交方案，展示了预训练和后训练阶段的扩展能力对竞争性能的重要性。

Conclusion: 总结了实用经验和设计建议，希望为更广泛的具身AI社区在将强大的基础模型适应复杂具身场景时提供可操作的见解。

Abstract: The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on $π_{0.5}$, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.

</details>


### [32] [Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation](https://arxiv.org/abs/2512.10099)
*Steven Caro,Stephen L. Smith*

Main category: cs.RO

TL;DR: HeRD：分层强化学习-扩散策略，通过高层RL选择中间空间目标，底层扩散模型生成轨迹，用于非抓取操作中的推动任务


<details>
  <summary>Details</summary>
Motivation: 非抓取操作（如推动物体穿越杂乱环境）面临复杂接触动力学和长时程规划的挑战，需要有效的控制策略

Method: 分层强化学习-扩散策略：高层RL代理选择中间空间目标，底层目标条件扩散模型生成可行高效轨迹

Result: 在2D仿真环境中，该方法在成功率、路径效率和跨环境配置的泛化能力上优于现有基线

Conclusion: 分层控制结合生成式底层规划是面向可扩展、目标导向非抓取操作的有前景方向

Abstract: Nonprehensile manipulation, such as pushing objects across cluttered environments, presents a challenging control problem due to complex contact dynamics and long-horizon planning requirements. In this work, we propose HeRD, a hierarchical reinforcement learning-diffusion policy that decomposes pushing tasks into two levels: high-level goal selection and low-level trajectory generation. We employ a high-level reinforcement learning (RL) agent to select intermediate spatial goals, and a low-level goal-conditioned diffusion model to generate feasible, efficient trajectories to reach them.
  This architecture combines the long-term reward maximizing behaviour of RL with the generative capabilities of diffusion models. We evaluate our method in a 2D simulation environment and show that it outperforms the state-of-the-art baseline in success rate, path efficiency, and generalization across multiple environment configurations. Our results suggest that hierarchical control with generative low-level planning is a promising direction for scalable, goal-directed nonprehensile manipulation. Code, documentation, and trained models are available: https://github.com/carosteven/HeRD.

</details>


### [33] [Fast Functionally Redundant Inverse Kinematics for Robotic Toolpath Optimisation in Manufacturing Tasks](https://arxiv.org/abs/2512.10116)
*Andrew Razjigaev,Hans Lohr,Alejandro Vargas-Uscategui,Peter King,Tirthankar Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出一种新颖的功能冗余逆运动学算法，利用任务空间分解、阻尼最小二乘法和Halley方法，实现快速鲁棒的机器人运动规划，减少关节运动


<details>
  <summary>Details</summary>
Motivation: 六轴工业机器人在焊接和增材制造等应用中存在功能冗余（对称工具轴使其成为五轴任务），现有逆运动学算法在快速反应框架中能解决此问题，但相比计算昂贵的离线规划方法使用不足，需要开发快速鲁棒的算法来利用冗余度优化工具路径规划

Method: 采用任务空间分解方法、阻尼最小二乘法（DLS）和Halley方法，提出一种新颖的功能冗余逆运动学算法，实现快速鲁棒的解决方案并减少关节运动

Result: 该算法能快速求解最小化关节运动的运动规划，扩展复杂工具路径的可行操作空间，在ABB工业机器人和冷喷涂枪上验证了计算工具路径的有效性

Conclusion: 提出的功能冗余逆运动学算法能够有效利用机器人冗余度，实现快速运动规划，优化工具路径，特别适用于冷喷涂等工业应用中的非平面表面加工

Abstract: Industrial automation with six-axis robotic arms is critical for many manufacturing tasks, including welding and additive manufacturing applications; however, many of these operations are functionally redundant due to the symmetrical tool axis, which effectively makes the operation a five-axis task. Exploiting this redundancy is crucial for achieving the desired workspace and dexterity required for the feasibility and optimisation of toolpath planning. Inverse kinematics algorithms can solve this in a fast, reactive framework, but these techniques are underutilised over the more computationally expensive offline planning methods. We propose a novel algorithm to solve functionally redundant inverse kinematics for robotic manipulation utilising a task space decomposition approach, the damped least-squares method and Halley's method to achieve fast and robust solutions with reduced joint motion. We evaluate our methodology in the case of toolpath optimisation in a cold spray coating application on a non-planar surface. The functionally redundant inverse kinematics algorithm can quickly solve motion plans that minimise joint motion, expanding the feasible operating space of the complex toolpath. We validate our approach on an industrial ABB manipulator and cold-spray gun executing the computed toolpath.

</details>


### [34] [Inertial Magnetic SLAM Systems Using Low-Cost Sensors](https://arxiv.org/abs/2512.10128)
*Chuan Huang,Gustaf Hendeby,Isaac Skog*

Main category: cs.RO

TL;DR: 提出两种惯性磁SLAM系统（松散耦合与紧密耦合），使用低成本IMU、磁力计阵列和气压计，实现无需视觉或轮式里程计的三维定位与建图，在低能见度环境下具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有磁SLAM系统通常需要视觉里程计或轮式编码器提供低漂移里程数据，限制了在无地图区域的定位精度。需要开发不依赖视觉、仅使用低成本传感器的SLAM系统，以适应低能见度应急场景。

Method: 提出松散耦合和紧密耦合两种IM-SLAM系统：1）松散耦合系统将局部和全局磁场模型分别用于两个状态空间模型；2）紧密耦合系统将两种磁场模型集成到单一状态空间模型中。系统使用IMU、磁力计阵列和气压计，在不同尺度上建立磁场模型。

Result: 紧密耦合IM-SLAM系统在大多数场景下定位误差低于松散耦合系统，典型误差约为每100米行程数米级别。证明了使用低成本传感器开发完整3D IM-SLAM系统的可行性。

Conclusion: 基于低成本传感器的IM-SLAM系统具有实际应用潜力，特别是在矿井/火灾救援等应急响应场景中，能够在低能见度条件下提供可靠的定位和建图能力。

Abstract: Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning. Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly. Moreover, they have bounded error within mapped regions. However, state-of-the-art methods typically require low-drift odometry data provided by visual odometry or a wheel encoder, etc. This is because these systems need to minimize/reduce positioning errors while exploring, which happens when they are in unmapped regions. To address these limitations, this work proposes a loosely coupled and a tightly coupled inertial magnetic SLAM (IM-SLAM) system. The proposed systems use commonly available low-cost sensors: an inertial measurement unit (IMU), a magnetometer array, and a barometer. The use of non-visual data provides a significant advantage over visual-based systems, making it robust to low-visibility conditions. Both systems employ state-space representations, and magnetic field models on different scales. The difference lies in how they use a local and global magnetic field model. The loosely coupled system uses these models separately in two state-space models, while the tightly coupled system integrates them into one state-space model. Experiment results show that the tightly coupled IM-SLAM system achieves lower positioning errors than the loosely coupled system in most scenarios, with typical errors on the order of meters per 100 meters traveled. These results demonstrate the feasiblity of developing a full 3D IM-SLAM systems using low-cost sensors and the potential of applying these systems in emergency response scenarios such as mine/fire rescue.

</details>


### [35] [Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine](https://arxiv.org/abs/2512.10235)
*Hui Li,Akhlak Uz Zaman,Fujian Yan,Hongsheng He*

Main category: cs.RO

TL;DR: 提出结合上下文奖励机的强化学习框架，用于任务导向抓取，通过任务分解和阶段特定上下文提高学习效率，在仿真和真实机器人上均取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 任务导向抓取具有高复杂性，传统方法学习效率低。需要一种能分解复杂任务、提供阶段指导、加速收敛的框架。

Method: 提出上下文奖励机框架，将抓取任务分解为子任务，每个阶段包含特定上下文（奖励函数、动作空间、状态抽象函数）。引入转移奖励引导阶段间转换，结合PPO算法实现。

Result: 在1000个仿真抓取任务中达到95%成功率，学习速度和成功率均优于SOTA方法。在真实机器人60个任务中达到83.3%成功率，涵盖6种功能。

Conclusion: 上下文奖励机框架显著提高了任务导向抓取的准确性、数据效率和收敛速度，在仿真和真实场景中均表现出优越性能，具有实际应用潜力。

Abstract: This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.

</details>


### [36] [Lies We Can Trust: Quantifying Action Uncertainty with Inaccurate Stochastic Dynamics through Conformalized Nonholonomic Lie Groups](https://arxiv.org/abs/2512.10294)
*Luís Marques,Maani Ghaffari,Dmitry Berenson*

Main category: cs.RO

TL;DR: CLAPS是一种基于保形预测的对称感知算法，为机器人动作构建包含系统配置的预测集，提供非渐近概率保证，适用于非欧几里得配置空间。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法通常需要强假设或依赖未校准的估计，而现有保形预测方法将机器人视为欧几里得点，无法处理具有非欧几里得配置空间（如SE(2)）的机器人系统。

Method: 提出CLAPS算法，使用李群严格分析配置误差，将欧几里得空间的保形预测理论保证扩展到SE(2)等非欧几里得空间，利用对称感知的非符合性评分构建预测集。

Result: 在模拟JetBot和真实MBot上的实验表明，通过考虑配置空间结构，对称感知方法能产生更体积高效的预测区域，比现有方法更好地表示底层不确定性。

Conclusion: CLAPS为机器人控制提供了具有严格概率保证的对称感知不确定性量化方法，适用于非欧几里得配置空间，且不需要强假设或校准的误差估计。

Abstract: We propose Conformal Lie-group Action Prediction Sets (CLAPS), a symmetry-aware conformal prediction-based algorithm that constructs, for a given action, a set guaranteed to contain the resulting system configuration at a user-defined probability. Our assurance holds under both aleatoric and epistemic uncertainty, non-asymptotically, and does not require strong assumptions about the true system dynamics, the uncertainty sources, or the quality of the approximate dynamics model. Typically, uncertainty quantification is tackled by making strong assumptions about the error distribution or magnitude, or by relying on uncalibrated uncertainty estimates - i.e., with no link to frequentist probabilities - which are insufficient for safe control. Recently, conformal prediction has emerged as a statistical framework capable of providing distribution-free probabilistic guarantees on test-time prediction accuracy. While current conformal methods treat robots as Euclidean points, many systems have non-Euclidean configurations, e.g., some mobile robots have SE(2). In this work, we rigorously analyze configuration errors using Lie groups, extending previous Euclidean Space theoretical guarantees to SE(2). Our experiments on a simulated JetBot, and on a real MBot, suggest that by considering the configuration space's structure, our symmetry-informed nonconformity score leads to more volume-efficient prediction regions which represent the underlying uncertainty better than existing approaches.

</details>


### [37] [Design of a six wheel suspension and a three-axis linear actuation mechanism for a laser weeding robot](https://arxiv.org/abs/2512.10319)
*Muhammad Usama,Muhammad Ibrahim Khan,Ahmad Hasan,Muhammad Shaaf Nadeem,Khawaja Fahad Iqbal,Jawad Aslam,Mian Ashfaq Ali,Asad Nisar Awan*

Main category: cs.RO

TL;DR: 本文提出了一种使用低能量激光束进行杂草清除的自主除草机器人，采用六轮设计和新型双四连杆悬挂系统提高稳定性，通过三维线性驱动机制引导激光瞄准检测到的杂草。


<details>
  <summary>Details</summary>
Motivation: 传统机械除草在大面积农田中效率低下，而除草剂会破坏土壤生态系统。激光除草作为一种可持续的精准农业替代方案，需要开发能够有效导航农田地形并精确瞄准杂草的自主机器人系统。

Method: 设计了一个六轮自主除草机器人，配备新型双四连杆悬挂系统以提高稳定性。采用三维线性驱动机制引导低能量激光束瞄准检测到的杂草。系统包括杂草检测模块和精确的激光定位控制。

Result: 田间测试显示机器人能有效导航农业地形，克服高达15厘米的障碍物。在42.5厘米/秒的最优速度下，杂草检测率达到86.2%，每米操作时间为87秒。激光驱动机制保持1.54毫米的最小平均位置误差，命中率高达97%。

Conclusion: 该自主激光除草机器人结合了速度、精度和效率，展示了在精准农业实践中显著提升杂草管理能力的潜力，为可持续农业提供了一种有前景的解决方案。

Abstract: Mobile robots are increasingly utilized in agriculture to automate labor-intensive tasks such as weeding, sowing, harvesting and soil analysis. Recently, agricultural robots have been developed to detect and remove weeds using mechanical tools or precise herbicide sprays. Mechanical weeding is inefficient over large fields, and herbicides harm the soil ecosystem. Laser weeding with mobile robots has emerged as a sustainable alternative in precision farming. In this paper, we present an autonomous weeding robot that uses controlled exposure to a low energy laser beam for weed removal. The proposed robot is six-wheeled with a novel double four-bar suspension for higher stability. The laser is guided towards the detected weeds by a three-dimensional linear actuation mechanism. Field tests have demonstrated the robot's capability to navigate agricultural terrains effectively by overcoming obstacles up to 15 cm in height. At an optimal speed of 42.5 cm/s, the robot achieves a weed detection rate of 86.2\% and operating time of 87 seconds per meter. The laser actuation mechanism maintains a minimal mean positional error of 1.54 mm, combined with a high hit rate of 97\%, ensuring effective and accurate weed removal. This combination of speed, accuracy, and efficiency highlights the robot's potential for significantly enhancing precision farming practices.

</details>


### [38] [Design and Validation of an Under-actuated Robotic Finger with Synchronous Tendon Routing](https://arxiv.org/abs/2512.10349)
*Quan Yuan,Zhenting Du,Daqian Cao,Weibang Bai*

Main category: cs.RO

TL;DR: 提出一种欠驱动腱驱动机器人手指，通过同步腱绳布线实现所有关节机械耦合，单驱动器即可驱动整个手指，在保持刚度和顺应性的同时减少多指手所需驱动器数量。


<details>
  <summary>Details</summary>
Motivation: 腱驱动欠驱动机器人手指在灵巧操作方面具有优势，但如何在紧凑形式下同时实现高负载能力和自适应顺应性仍然具有挑战性。

Method: 设计同步腱绳布线方案，使所有关节以固定角速度比机械耦合；建立包含腱弹性的运动学和静力学模型以预测结构刚度；制作单指原型并进行静态负载测试。

Result: 单指原型测试显示平均偏转预测误差为1.0毫米（总指长的0.322%），在3公斤指尖负载下测得刚度为1.2×10³ N/m；集成到五指机械手中展示了有效的物体操作能力。

Conclusion: 所提出的布线方案实现了可预测的刚度和可靠的抓取性能，同时最小化驱动器数量，为紧凑型多指机械手设计提供了有效解决方案。

Abstract: Tendon-driven under-actuated robotic fingers provide advantages for dexterous manipulation through reduced actuator requirements and simplified mechanical design. However, achieving both high load capacity and adaptive compliance in a compact form remains challenging. This paper presents an under-actuated tendon-driven robotic finger (UTRF) featuring a synchronous tendon routing that mechanically couples all joints with fixed angular velocity ratios, enabling the entire finger to be actuated by a single actuator. This approach significantly reduces the number of actuators required in multi-finger hands, resulting in a lighter and more compact structure without sacrificing stiffness or compliance. The kinematic and static models of the finger are derived, incorporating tendon elasticity to predict structural stiffness. A single-finger prototype was fabricated and tested under static loading, showing an average deflection prediction error of 1.0 mm (0.322% of total finger length) and a measured stiffness of 1.2x10^3 N/m under a 3 kg tip load. Integration into a five-finger robotic hand (UTRF-RoboHand) demonstrates effective object manipulation across diverse scenarios, confirming that the proposed routing achieves predictable stiffness and reliable grasping performance with a minimal actuator count.

</details>


### [39] [CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation](https://arxiv.org/abs/2512.10360)
*Liuyi Wang,Zongtao He,Jinlong Li,Xiaoyan Qi,Mengxian Hu,Chenpeng Yao,Chengju Liu,Qijun Chen*

Main category: cs.RO

TL;DR: CLASH框架通过结合反应式小模型规划器和反思式大模型推理器，在视觉语言导航任务中实现SOTA性能，并在真实世界部署中验证了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言大模型具有强大的推理能力，但在视觉语言导航任务中往往不如专门的全景小模型。为了解决这一问题，需要结合两者的优势。

Method: 提出CLASH框架：1）反应式小模型规划器采用因果学习的双分支架构增强泛化；2）反思式大模型推理器利用全景视觉提示和思维链推理；3）不确定性感知协作机制自适应融合两个模型的决策；4）在仿真中用可学习的点目标策略替代规则控制器，在真实世界部署中设计LiDAR聚类模块和SLAM本地控制器。

Result: 在VLN-CE排行榜上获得第一名，在测试未见集上显著提高了SR和SPL指标，超越了之前的SOTA方法。真实世界实验验证了框架的强鲁棒性。

Conclusion: CLASH框架成功整合了大模型和小模型的优势，通过协作层次结构在视觉语言导航任务中实现了卓越性能，并在仿真和真实世界部署中都表现出色。

Abstract: Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios.

</details>


### [40] [RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI](https://arxiv.org/abs/2512.10394)
*Weifan Guan,Huasen Xi,Chenxiao Zhang,Aosheng Li,Qinghao Hu,Jian Cheng*

Main category: cs.RO

TL;DR: RoboNeuron：首个深度融合LLM/VLA认知能力与ROS实时执行框架的通用部署框架，通过MCP协议实现语义桥接，提升跨场景适应性和模块解耦


<details>
  <summary>Details</summary>
Motivation: 当前具身AI系统面临跨场景适应性差、模块耦合刚性、推理加速碎片化等工程障碍，需要统一的部署框架来克服这些限制

Method: 1) 利用MCP协议作为语义桥接，让LLM动态编排底层机器人工具；2) 建立高度模块化架构，通过ROS统一通信接口严格解耦感知、推理和控制；3) 引入自动化工具将ROS消息转换为可调用的MCP函数

Result: 显著增强跨场景适应性和组件灵活性，建立系统化的水平性能基准测试平台，为可扩展的真实世界具身应用奠定坚实基础

Conclusion: RoboNeuron通过深度融合LLM/VLA认知能力与ROS实时执行框架，解决了当前具身AI系统的关键工程障碍，为可扩展的具身智能部署提供了统一框架

Abstract: Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.

</details>


### [41] [Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots](https://arxiv.org/abs/2512.10477)
*Timur Ishuov,Michele Folgheraiter,Madi Nurmanov,Goncalo Gordo,Richárd Farkas,József Dombi*

Main category: cs.RO

TL;DR: 提出Symphony算法，通过"襁褓"正则化限制动作强度、使用确定性策略和衰减回放缓冲区，实现人形机器人从零开始的安全高效训练。


<details>
  <summary>Details</summary>
Motivation: 人类学习需要时间，但机器人训练不能等待数百万步。现有随机算法中的高斯噪声会损害电机和齿轮箱，需要更安全高效的训练方法。

Method: 提出Symphony算法：1) "襁褓"正则化限制动作强度；2) 确定性策略与有限参数噪声；3) 衰减回放缓冲区；4) 时间优势更新，将Actor和Critic结合在单一对象中。

Result: 训练过程对环境和机器人机制都更安全，实现了样本效率、样本邻近性和动作安全性的平衡。

Conclusion: Symphony算法为人形机器人从零开始训练提供了一种安全、高效的新方法，解决了传统随机算法中的硬件损害问题。

Abstract: In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. "Swaddling" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.

</details>


### [42] [Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS/UWB/IMU Fusion: A Comparison of EKF, FGO, and PF](https://arxiv.org/abs/2512.10480)
*Jiaqiang Zhang,Xianjia Yu,Sier Ha,Paola Torrico Moron,Sahar Salimpour,Farhad Kerama,Haizhou Zhang,Tomi Westerlund*

Main category: cs.RO

TL;DR: 提出统一的GNSS/UWB/IMU融合框架，比较三种概率后端（ESKF、因子图优化、粒子滤波），实现无缝室内外行人定位


<details>
  <summary>Details</summary>
Motivation: 解决室外-室内环境中行人连续精确定位的挑战，因为GNSS、UWB和惯性PDR各自存在信号遮挡、多径效应和漂移等问题，需要互补融合

Method: 使用胸戴式IMU的PDR作为运动主干，室外集成GNSS绝对更新，室内集成UWB绝对更新。引入基于OpenStreetMap建筑轮廓的轻量级地图可行性约束，增强过渡鲁棒性。实现三种概率后端比较：误差状态扩展卡尔曼滤波、滑动窗口因子图优化和粒子滤波

Result: 在三种场景（室内UWB+PDR、室外GNSS+PDR、无缝室外-室内GNSS+UWB+PDR）中评估，结果显示ESKF在实现中提供了最一致的整体性能

Conclusion: 提出的统一融合框架能够实现无缝行人定位，其中误差状态扩展卡尔曼滤波在比较的三种概率后端中表现最为稳定可靠

Abstract: Accurate and continuous pedestrian positioning across outdoor-indoor environments remains challenging because GNSS, UWB, and inertial PDR are complementary yet individually fragile under signal blockage, multipath, and drift. This paper presents a unified GNSS/UWB/IMU fusion framework for seamless pedestrian localization and provides a controlled comparison of three probabilistic back-ends: an error-state extended Kalman filter, sliding-window factor graph optimization, and a particle filter. The system uses chest-mounted IMU-based PDR as the motion backbone and integrates absolute updates from GNSS outdoors and UWB indoors. To enhance transition robustness and mitigate urban GNSS degradation, we introduce a lightweight map-based feasibility constraint derived from OpenStreetMap building footprints, treating most building interiors as non-navigable while allowing motion inside a designated UWB-instrumented building. The framework is implemented in ROS 2 and runs in real time on a wearable platform, with visualization in Foxglove. We evaluate three scenarios: indoor (UWB+PDR), outdoor (GNSS+PDR), and seamless outdoor-indoor (GNSS+UWB+PDR). Results show that the ESKF provides the most consistent overall performance in our implementation.

</details>


### [43] [Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks](https://arxiv.org/abs/2512.10481)
*Gaozhao Wang,Xing Liu,Zhenduo Ye,Zhengxiong Liu,Panfeng Huang*

Main category: cs.RO

TL;DR: 提出了一种名为"Contact SLAM"的物理驱动接触认知方法，仅使用触觉传感和场景先验知识来估计环境状态并实现操作，特别适用于视觉被遮挡的"盲操作"场景。


<details>
  <summary>Details</summary>
Motivation: 在接触丰富的操作任务中，机器人需要准确感知环境。但在某些场景中视觉会被遮挡，机器人无法通过视觉反馈获取实时场景状态信息，这被称为"盲操作"。需要开发仅依赖触觉传感的方法来解决这一问题。

Method: 提出了"Contact SLAM"方法，这是一种物理驱动的接触认知方法。它仅使用触觉传感和场景先验知识来估计环境状态。同时设计了主动探索策略，通过逐步减少操作场景中的不确定性来最大化探索效率。

Result: 实验结果表明，该方法在多个接触丰富的任务中表现出有效性和准确性，包括困难的插座装配任务和方块推动任务。

Conclusion: Contact SLAM方法能够在视觉被遮挡的情况下，仅通过触觉传感成功完成接触丰富的操作任务，为解决盲操作问题提供了有效的解决方案。

Abstract: Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation". In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM", is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.

</details>


### [44] [Neural Ranging Inertial Odometry](https://arxiv.org/abs/2512.10531)
*Si Wang,Bingqi Shen,Fei Wang,Yanjun Cao,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 提出IR-ULSG神经融合框架，结合图注意力UWB网络和循环神经惯性网络，解决UWB在GPS拒止环境中的定位问题，特别针对多径干扰和传感器布置敏感性问题。


<details>
  <summary>Details</summary>
Motivation: UWB在GPS拒止定位中具有轻量级和无漂移特性，但在实际场景中精度受限，因为其对传感器布置敏感且存在多径或多信号干扰导致的非高斯模式，这在长隧道等典型应用中常见。

Method: 提出神经融合框架，包含图注意力UWB网络（学习场景相关测距模式，适应任意数量锚点或标签，无需校准）和循环神经惯性网络。结合最小二乘法和名义帧增强整体性能和可扩展性。

Result: 在公共和自收集数据集上进行广泛实验，涵盖室内、室外和隧道环境。结果显示IR-ULSG在挑战性条件下具有优越性，包括凸包外部场景和仅单个锚点可用的情况。

Conclusion: IR-ULSG框架有效解决了UWB定位中的多径干扰和传感器布置敏感性问题，在GPS拒止环境中实现了鲁棒且准确的定位，特别适用于隧道等复杂场景。

Abstract: Ultra-wideband (UWB) has shown promising potential in GPS-denied localization thanks to its lightweight and drift-free characteristics, while the accuracy is limited in real scenarios due to its sensitivity to sensor arrangement and non-Gaussian pattern induced by multi-path or multi-signal interference, which commonly occurs in many typical applications like long tunnels. We introduce a novel neural fusion framework for ranging inertial odometry which involves a graph attention UWB network and a recurrent neural inertial network. Our graph net learns scene-relevant ranging patterns and adapts to any number of anchors or tags, realizing accurate positioning without calibration. Additionally, the integration of least squares and the incorporation of nominal frame enhance overall performance and scalability. The effectiveness and robustness of our methods are validated through extensive experiments on both public and self-collected datasets, spanning indoor, outdoor, and tunnel environments. The results demonstrate the superiority of our proposed IR-ULSG in handling challenging conditions, including scenarios outside the convex envelope and cases where only a single anchor is available.

</details>


### [45] [Mr. Virgil: Learning Multi-robot Visual-range Relative Localization](https://arxiv.org/abs/2512.10540)
*Si Wang,Zhehan Li,Jiadong Lu,Rong Xiong,Yanjun Cao,Yue Wang*

Main category: cs.RO

TL;DR: Mr. Virgil是一个端到端学习的多机器人视觉-测距相对定位框架，通过图神经网络进行UWB测距与视觉检测的数据关联，结合可微位姿图优化后端，实现鲁棒的匹配和精确的位姿估计。


<details>
  <summary>Details</summary>
Motivation: 现有的UWB-视觉融合定位方法在多机器人相对定位中面临机器人身份与视觉检测匹配的挑战，高度依赖身份编码硬件或精细调参算法，错误匹配可能对定位系统造成不可逆损害。

Method: 提出端到端学习框架Mr. Virgil：1) 图神经网络前端处理UWB测距和视觉检测的数据关联，提供鲁棒匹配结果、准确初始位置预测和可信不确定性估计；2) 可微位姿图优化后端整合前端输出提升位姿估计精度；3) 实现去中心化系统用于实际应用。

Result: 在不同机器人数量、仿真与真实环境、遮挡与非遮挡条件下进行实验，相比传统方法在各种场景下展现出稳定性和精确性。

Conclusion: Mr. Virgil框架有效解决了多机器人UWB-视觉融合定位中的匹配问题，通过端到端学习实现了鲁棒的相对定位，在实际应用中表现出色。

Abstract: Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.

</details>


### [46] [Motion Planning for Safe Landing of a Human-Piloted Parafoil](https://arxiv.org/abs/2512.10595)
*Maximillian Fainkich,Kiril Solovey,Anna Clarke*

Main category: cs.RO

TL;DR: 该研究开发了用于滑翔伞飞行训练的计算机生成轨迹规划算法，相比人类飞行员表现提升20%-80%，可集成到未来模拟器中提高飞行安全性


<details>
  <summary>Details</summary>
Motivation: 大多数跳伞事故发生在滑翔伞操控和着陆阶段，源于飞行员判断失误。新手飞行员训练周期长，缺乏功能完善且易于访问的训练模拟器，且适合辅助人类训练的滑翔伞轨迹规划研究有限

Method: 采用基于采样的运动规划器Stable Sparse RRT (SST)，针对问题约束进行适配，以最小化倾斜角（控制努力）作为安全性的代理指标

Result: 算法生成的解决方案相比人类飞行数据，在成本上实现了20%-80%的相对改进。人类飞行员倾向于先水平接近着陆区，然后通过盘旋下降调整高度；而算法能进行更平滑、渐进的下降，在精确高度到达着陆区并保持安全约束

Conclusion: 研究表明计算机生成的指导方案（而非传统经验法则）具有潜力，可集成到未来模拟器中，训练飞行员实现更安全、更经济高效的飞行

Abstract: Most skydiving accidents occur during the parafoil-piloting and landing stages and result from human lapses in judgment while piloting the parafoil. Training of novice pilots is protracted due to the lack of functional and easily accessible training simulators. Moreover, work on parafoil trajectory planning suitable for aiding human training remains limited. To bridge this gap, we study the problem of computing safe trajectories for human-piloted parafoil flight and examine how such trajectories fare against human-generated solutions. For the algorithmic part, we adapt the sampling-based motion planner Stable Sparse RRT (SST) by Li et al., to cope with the problem constraints while minimizing the bank angle (control effort) as a proxy for safety. We then compare the computer-generated solutions with data from human-generated parafoil flight, where the algorithm offers a relative cost improvement of 20\%-80\% over the performance of the human pilot. We observe that human pilots tend to, first, close the horizontal distance to the landing area, and then address the vertical gap by spiraling down to the suitable altitude for starting a landing maneuver. The algorithm considered here makes smoother and more gradual descents, arriving at the landing area at the precise altitude necessary for the final approach while maintaining safety constraints. Overall, the study demonstrates the potential of computer-generated guidelines, rather than traditional rules of thumb, which can be integrated into future simulators to train pilots for safer and more cost-effective flights.

</details>


### [47] [LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator](https://arxiv.org/abs/2512.10605)
*Lihuang Chen,Xiangyu Luo,Jun Meng*

Main category: cs.RO

TL;DR: LEO-RobotAgent是一个通用的语言驱动智能体框架，让大语言模型能够操作不同类型机器人完成跨场景的复杂任务，具有模块化工具集和人机交互机制。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务规划研究大多专注于单一任务场景和单一机器人类型，算法结构复杂且缺乏通用性。需要一种通用框架让大模型能够独立思考、规划和执行任务。

Method: 设计了简洁的框架结构，提供模块化、易注册的工具集，让大模型能灵活调用各种工具。同时集成了人机交互机制，使算法能像伙伴一样与人类协作。

Result: 实验验证该框架能轻松适配主流机器人平台（无人机、机械臂、轮式机器人），并高效执行不同复杂度的任务。

Conclusion: LEO-RobotAgent框架具有强大的泛化性、鲁棒性和效率，能增强双向人机意图理解，降低人机交互门槛，是一个通用的语言驱动智能体解决方案。

Abstract: We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.

</details>


### [48] [Evaluating Gemini Robotics Policies in a Veo World Simulator](https://arxiv.org/abs/2512.10675)
*Gemini Robotics Team,Coline Devin,Yilun Du,Debidatta Dwibedi,Ruiqi Gao,Abhishek Jindal,Thomas Kipf,Sean Kirmani,Fangchen Liu,Anirudha Majumdar,Andrew Marmon,Carolina Parada,Yulia Rubanova,Dhruv Shah,Vikas Sindhwani,Jie Tan,Fei Xia,Ted Xiao,Sherry Yang,Wenhao Yu,Allan Zhou*

Main category: cs.RO

TL;DR: 研究人员开发了一个基于前沿视频模型Veo的生成式评估系统，用于全面评估机器人策略性能，包括正常情况、分布外泛化、物理和语义安全性测试。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型在机器人领域的应用主要局限于分布内评估，无法全面评估策略在真实世界中的表现。需要一种能够评估策略在多种场景下性能的系统，包括分布外泛化、安全性测试等。

Method: 构建基于Veo视频基础模型的生成式评估系统，优化支持机器人动作条件和多视角一致性，集成生成式图像编辑和多视角补全技术，合成真实世界场景的多种变体。

Result: 系统能够准确模拟包含新交互物体、新视觉背景和新干扰物体的场景，能够预测不同策略在正常和分布外条件下的相对性能，识别不同泛化维度对策略性能的影响，并进行红队测试暴露违反安全约束的行为。

Conclusion: 视频模型可以用于机器人策略评估的完整谱系，从正常性能评估到分布外泛化，再到物理和语义安全性探测，为机器人策略开发提供了强大的评估工具。

Abstract: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.

</details>


### [49] [How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning](https://arxiv.org/abs/2512.10698)
*Jianbo Wang,Galina Sidorenko,Johan Thunberg*

Main category: cs.RO

TL;DR: 本文提出了一种结合深度强化学习（DRL）与解析方法的混合方法，用于多车跟随场景中的紧急制动决策，旨在实现整体伤害最小化和碰撞避免，而非仅关注单车安全。


<details>
  <summary>Details</summary>
Motivation: 联网自动驾驶车辆（CAVs）能提升驾驶安全，但传统保守的最坏情况控制策略会降低灵活性并影响整体性能。需要研究如何在多车跟随紧急制动场景中，通过DRL实现整体（集体）伤害最小化，而非仅关注单车安全。

Method: 提出混合方法：结合深度强化学习（DRL）与基于解析表达式选择最优恒定减速度的先前方法。DRL利用车对车通信来道德选择紧急制动策略，而混合方法提高了可靠性并实现更好的整体性能。

Result: 混合方法相比独立DRL提高了可靠性，同时在整体伤害减少和碰撞避免方面实现了更优的性能。

Conclusion: 通过结合DRL与解析方法，可以在多车跟随紧急制动场景中实现更可靠、更有效的整体安全决策，平衡安全要求与性能表现。

Abstract: Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.

</details>


### [50] [On the Stabilization of Rigid Formations on Regular Curves](https://arxiv.org/abs/2512.10700)
*Mohamed Elobaid,Shinkyu Park,Eric Feron*

Main category: cs.RO

TL;DR: 提出了一种在多智能体系统中将等边多边形编队稳定在平面曲线上的方法，包括曲线扫描和编队收敛


<details>
  <summary>Details</summary>
Motivation: 解决多智能体刚性编队在一般平面曲线上稳定化的问题，特别是在曲线扫描后将等边多边形编队稳定在封闭可微曲线上

Method: 使用随机多起点牛顿类算法寻找曲线上内接正多边形的顶点，然后设计连续反馈控制律保证曲线扫描和编队收敛，同时确保智能体间避碰

Result: 方法在不同类型曲线和不同刚性编队配置下通过数值模拟验证有效，代码已开源

Conclusion: 提出的方法能够有效解决多智能体在平面曲线上形成等边多边形编队的问题，实现了曲线扫描、编队稳定和避碰的综合目标

Abstract: This work deals with the problem of stabilizing a multi-agent rigid formation on a general class of planar curves. Namely, we seek to stabilize an equilateral polygonal formation on closed planar differentiable curves after a path sweep. The task of finding an inscribed regular polygon centered at the point of interest is solved via a randomized multi-start Newton-Like algorithm for which one is able to ascertain the existence of a minimizer. Then we design a continuous feedback law that guarantees convergence to, and sufficient sweeping of the curve, followed by convergence to the desired formation vertices while ensuring inter-agent avoidance. The proposed approach is validated through numerical simulations for different classes of curves and different rigid formations. Code: https://github.com/mebbaid/paper-elobaid-ifacwc-2026

</details>


### [51] [AERMANI-Diffusion: Regime-Conditioned Diffusion for Dynamics Learning in Aerial Manipulators](https://arxiv.org/abs/2512.10773)
*Samaksh Ujjawal,Shivansh Pratap Singh,Naveen Sudheer Nair,Rishabh Dev Yadav,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: 提出一种基于条件扩散过程的框架，用于建模空中机械臂的残余力分布，通过轻量级时间编码器提取运动状态特征，结合自适应控制器显著提升跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 空中机械臂在快速运动中经历配置相关的惯性耦合力和空气动力变化，传统解析模型在这些非线性非平稳效应下精度不足，而标准数据驱动方法（如深度神经网络和高斯过程）无法有效表示不同操作条件下多样的残余行为。

Method: 提出一种基于条件扩散过程的框架，使用条件扩散过程建模残余力的完整分布，配合轻量级时间编码器提取最近运动和配置的紧凑摘要，确保在突变或未知负载情况下仍能保持一致的残余预测。

Result: 该框架与自适应控制器结合后，能够补偿动力学不确定性，在实际测试中显著提高了跟踪精度。

Conclusion: 提出的基于条件扩散过程的框架能够有效建模空中机械臂的复杂残余力分布，通过轻量级编码器实现状态感知，结合自适应控制显著提升系统性能，为解决非线性非平稳系统的动力学建模问题提供了有效方案。

Abstract: Aerial manipulators undergo rapid, configuration-dependent changes in inertial coupling forces and aerodynamic forces, making accurate dynamics modeling a core challenge for reliable control. Analytical models lose fidelity under these nonlinear and nonstationary effects, while standard data-driven methods such as deep neural networks and Gaussian processes cannot represent the diverse residual behaviors that arise across different operating conditions. We propose a regime-conditioned diffusion framework that models the full distribution of residual forces using a conditional diffusion process and a lightweight temporal encoder. The encoder extracts a compact summary of recent motion and configuration, enabling consistent residual predictions even through abrupt transitions or unseen payloads. When combined with an adaptive controller, the framework enables dynamics uncertainty compensation and yields markedly improved tracking accuracy in real-world tests.

</details>


### [52] [Iterative Compositional Data Generation for Robot Control](https://arxiv.org/abs/2512.10891)
*Anh-Quan Pham,Marcel Hussing,Shubhankar P. Patankar,Dani S. Bassett,Jorge Mendez-Mendez,Eric Eaton*

Main category: cs.RO

TL;DR: 提出语义组合扩散变换器，通过分解机器人、物体、障碍物和目标组件来生成未见任务组合的合成数据，并通过迭代自改进提升性能


<details>
  <summary>Details</summary>
Motivation: 机器人操作数据收集成本高昂，难以覆盖多物体、多机器人、多环境场景中的组合任务空间。现有生成模型无法利用机器人领域的组合结构，难以泛化到未见任务组合。

Method: 提出语义组合扩散变换器，将状态转移分解为机器人、物体、障碍物和目标特定组件，通过注意力机制学习组件间交互。训练后能零样本生成高质量转移数据，并引入迭代自改进流程：通过离线强化学习验证合成数据并纳入后续训练轮次。

Result: 方法在零样本性能上显著优于单一模型和硬编码组合基线，最终解决了几乎所有保留任务，并在学习表示中展现出有意义的组合结构。

Conclusion: 语义组合扩散变换器通过分解学习和迭代自改进，能够有效生成未见任务组合的合成数据，为组合性机器人任务的数据高效学习提供了有前景的解决方案。

Abstract: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.

</details>


### [53] [Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit](https://arxiv.org/abs/2512.10934)
*Zamirddine Mari,Jérôme Pasquet,Julien Seinturier*

Main category: cs.RO

TL;DR: 使用强化学习让无人机在未知三维管道中自主导航，仅依赖局部LiDAR观测和条件视觉检测，无需管道几何先验知识，在部分可观测条件下优于传统Pure Pursuit算法。


<details>
  <summary>Details</summary>
Motivation: 解决在受限管道环境中无人机自主导航的挑战，包括管道几何约束、墙壁接近以及感知限制，为工业、地下或医疗应用提供解决方案。

Method: 采用强化学习（PPO）方法，结合渐进式课程学习策略，逐步暴露给更弯曲的几何形状；使用基于直接可见性、方向记忆和LiDAR对称线索的转向协商机制处理部分可观测性。

Result: PPO策略获得鲁棒且可泛化的行为，在缺乏几何模型信息的情况下持续优于确定性控制器（Pure Pursuit算法），并在高保真3D环境中验证了学习行为的可迁移性。

Conclusion: 该方法为未知管道环境中的自主导航提供了完整框架，为工业、地下或医疗应用中在狭窄且感知受限的管道中行进开辟了前景。

Abstract: Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.
  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.

</details>


### [54] [ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning](https://arxiv.org/abs/2512.10946)
*Wendi Chen,Han Xue,Yi Wang,Fangyuan Zhou,Jun Lv,Yang Jin,Shirun Tang,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: ImplicitRDP是一个统一的视觉-力觉扩散策略，通过结构化的慢-快学习机制和虚拟目标表示正则化，在接触丰富的操作任务中实现了优于纯视觉和分层基线的性能。


<details>
  <summary>Details</summary>
Motivation: 人类水平的接触丰富操作依赖于视觉和力觉两种模态的协同作用：视觉提供空间丰富但时间缓慢的全局上下文，而力觉捕捉快速、高频的局部接触动态。然而，由于它们在频率和信息内容上的根本差异，整合这些信号具有挑战性。

Method: 提出了ImplicitRDP，一个统一的端到端视觉-力觉扩散策略，将视觉规划和反应式力控制集成在单一网络中。采用结构化慢-快学习机制，利用因果注意力同时处理异步的视觉和力觉标记，使策略能够在力觉频率上进行闭环调整，同时保持动作块的时间一致性。此外，提出了基于虚拟目标的表示正则化，将力反馈映射到与动作相同的空间，提供比原始力预测更强的物理基础学习信号。

Result: 在接触丰富的任务上进行广泛实验表明，ImplicitRDP显著优于纯视觉和分层基线，实现了更高的反应性和成功率，同时具有简化的训练流程。

Conclusion: ImplicitRDP通过结构化慢-快学习和虚拟目标表示正则化，成功整合了视觉和力觉模态，为接触丰富的操作任务提供了一个高效、统一的端到端解决方案，在反应性和成功率方面都取得了显著提升。

Abstract: Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.

</details>
