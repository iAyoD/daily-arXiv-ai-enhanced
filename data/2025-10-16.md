<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Learning to Grasp Anything by Playing with Random Toys](https://arxiv.org/abs/2510.12866)
*Dantong Niu,Yuvan Sharma,Baifeng Shi,Rachel Ding,Matteo Gioia,Haoru Xue,Henry Tsai,Konstantinos Kallidromitis,Anirudh Pai,Shankar Shastry,Trevor Darrell,Jitendra Malik,Roei Herzig*

Main category: cs.RO

TL;DR: 该论文提出通过训练机器人使用仅由四种基本形状（球体、长方体、圆柱体、圆环）随机组合的"玩具"来学习通用抓取技能，实现了对真实世界物体的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 受儿童通过掌握简单玩具发展灵巧操作技能的认知科学启发，研究机器人是否也能通过类似方式实现通用化操作能力。

Method: 使用四种基本形状随机组合的物体进行训练，提出物体为中心的视觉表示和检测池化机制，在模拟和物理机器人上进行评估。

Result: 在YCB数据集上达到67%的真实世界抓取成功率，优于依赖更多领域内数据的现有方法，并研究了训练玩具数量和多样性的影响。

Conclusion: 这项工作为机器人操作的可扩展和通用化学习提供了一条有前景的路径。

Abstract: Robotic manipulation policies often struggle to generalize to novel objects,
limiting their real-world utility. In contrast, cognitive science suggests that
children develop generalizable dexterous manipulation skills by mastering a
small set of simple toys and then applying that knowledge to more complex
items. Inspired by this, we study if similar generalization capabilities can
also be achieved by robots. Our results indicate robots can learn generalizable
grasping using randomly assembled objects that are composed from just four
shape primitives: spheres, cuboids, cylinders, and rings. We show that training
on these "toys" enables robust generalization to real-world objects, yielding
strong zero-shot performance. Crucially, we find the key to this generalization
is an object-centric visual representation induced by our proposed detection
pooling mechanism. Evaluated in both simulation and on physical robots, our
model achieves a 67% real-world grasping success rate on the YCB dataset,
outperforming state-of-the-art approaches that rely on substantially more
in-domain data. We further study how zero-shot generalization performance
scales by varying the number and diversity of training toys and the
demonstrations per toy. We believe this work offers a promising path to
scalable and generalizable learning in robotic manipulation. Demonstration
videos, code, checkpoints and our dataset are available on our project page:
https://lego-grasp.github.io/ .

</details>


### [2] [Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation](https://arxiv.org/abs/2510.12919)
*Mouhyemen Khan,Tatsuya Ibuki,Abhijit Chatterjee*

Main category: cs.RO

TL;DR: 提出了一种统一框架，将高斯过程隐式表面（GPIS）作为控制屏障函数（CBF），用于安全边界表示和碰撞避免。


<details>
  <summary>Details</summary>
Motivation: 结合水平集方法和控制屏障函数的安全技术，利用隐式表面作为安全边界，通过高斯过程的不确定性估计提供鲁棒的安全保证。

Method: 使用高斯过程隐式表面表示安全边界，利用传感器测量的安全样本训练GP，并通过稀疏化解决方案解决GP计算复杂度问题。

Result: 在7自由度机械臂和四旋翼无人机上的实验验证了该方法能够在原本会与物体相交的轨迹上实现安全交互和无碰撞执行。

Conclusion: 高斯CBF（包括稀疏版本）能够有效实现安全边界表示和碰撞避免，为安全控制提供了新的统一框架。

Abstract: Level set methods underpin modern safety techniques such as control barrier
functions (CBFs), while also serving as implicit surface representations for
geometric shapes via distance fields. Inspired by these two paradigms, we
propose a unified framework where the implicit surface itself acts as a CBF. We
leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety
boundaries, using safety samples which are derived from sensor measurements to
condition the GP. The GP posterior mean defines the implicit safety surface
(safety belief), while the posterior variance provides a robust safety margin.
Although GPs have favorable properties such as uncertainty estimation and
analytical tractability, they scale cubically with data. To alleviate this
issue, we develop a sparse solution called sparse Gaussian CBFs. To the best of
our knowledge, GPIS have not been explicitly used to synthesize CBFs. We
validate the approach on collision avoidance tasks in two settings: a simulated
7-DOF manipulator operating around the Stanford bunny, and a quadrotor
navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with
and without sparsity) enable safe interaction and collision-free execution of
trajectories that would otherwise intersect the objects.

</details>


### [3] [Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance](https://arxiv.org/abs/2510.12924)
*Pavel Pochobradský,Ondřej Procházka,Robert Pěnička,Vojtěch Vonásek,Martin Saska*

Main category: cs.RO

TL;DR: GMPPI是一种基于采样的控制器，结合几何SE(3)控制和改进的MPPI方法，能够在跟踪敏捷轨迹的同时避障，在仿真和真实环境中都表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有MPPI控制器在跟踪平滑低速轨迹时性能不足，且缺乏有效的在线避障能力，限制了无人机在复杂环境中的自主飞行。

Method: 提出几何模型预测路径积分控制，使用SE(3)控制生成部分轨迹，引入变化的仿真时间步长和动态成本噪声参数，并与立体深度相机集成实现在线避障。

Result: 仿真中位置误差与几何SE(3)控制器相当，在森林环境中能以13m/s速度避障，超越现有避障规划器；真实实验中能以10m/s速度跟踪轨迹并避障。

Conclusion: GMPPI成功结合了精确轨迹跟踪和高速避障能力，是实现无人机在复杂环境中自主飞行的关键进展。

Abstract: In this letter, we introduce Geometric Model Predictive Path Integral
(GMPPI), a sampling-based controller capable of tracking agile trajectories
while avoiding obstacles. In each iteration, GMPPI generates a large number of
candidate rollout trajectories and then averages them to create a nominal
control to be followed by the Unmanned Aerial Vehicle (UAV). We propose using
geometric SE(3) control to generate part of the rollout trajectories,
significantly increasing precision in agile flight. Furthermore, we introduce
varying rollout simulation time step length and dynamic cost and noise
parameters, vastly improving tracking performance of smooth and low-speed
trajectories over an existing Model Predictive Path Integral (MPPI)
implementation. Finally, we propose an integration of GMPPI with a stereo depth
camera, enabling online obstacle avoidance at high speeds, a crucial step
towards autonomous UAV flights in complex environments. The proposed controller
can track simulated agile reference trajectories with position error similar to
the geometric SE(3) controller. However, the same configuration of the proposed
controller can avoid obstacles in a simulated forest environment at speeds of
up to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware
planner. In real-world experiments, GMPPI retains the capability to track agile
trajectories and avoids obstacles at speeds of up to 10m/s.

</details>


### [4] [Enhancing Sampling-based Planning with a Library of Paths](https://arxiv.org/abs/2510.12962)
*Michal Minařík,Vojtěch Vonásek,Robert Pěnička*

Main category: cs.RO

TL;DR: 提出一种基于经验库的3D物体路径规划方法，通过重用历史路径信息显著提高在狭窄通道场景中的规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统采样规划器在狭窄通道中采样概率低、规划时间长，且每次规划都从头开始，浪费了历史经验。

Method: 构建历史路径库，为新物体寻找最相似的历史物体，将其路径作为近似解并进行变换调整，然后沿近似路径采样。

Result: 在狭窄通道场景中相比OMPL库的先进方法，规划时间减少高达85%，且在传统规划器失败的情况下仍能找到解。

Conclusion: 提出的经验重用方法能显著提升3D物体路径规划效率，特别是在狭窄通道场景中表现优异。

Abstract: Path planning for 3D solid objects is a challenging problem, requiring a
search in a six-dimensional configuration space, which is, nevertheless,
essential in many robotic applications such as bin-picking and assembly. The
commonly used sampling-based planners, such as Rapidly-exploring Random Trees,
struggle with narrow passages where the sampling probability is low, increasing
the time needed to find a solution. In scenarios like robotic bin-picking,
various objects must be transported through the same environment. However,
traditional planners start from scratch each time, losing valuable information
gained during the planning process. We address this by using a library of past
solutions, allowing the reuse of previous experiences even when planning for a
new, previously unseen object. Paths for a set of objects are stored, and when
planning for a new object, we find the most similar one in the library and use
its paths as approximate solutions, adjusting for possible mutual
transformations. The configuration space is then sampled along the approximate
paths. Our method is tested in various narrow passage scenarios and compared
with state-of-the-art methods from the OMPL library. Results show significant
speed improvements (up to 85% decrease in the required time) of our method,
often finding a solution in cases where the other planners fail. Our
implementation of the proposed method is released as an open-source package.

</details>


### [5] [The Omega Turn: A General Turning Template for Elongate Robots](https://arxiv.org/abs/2510.12970)
*Baxi Chong,Tianyu Wang,Kelimar Diaz,Christopher J. Pierce,Eva Erickson,Julian Whitman,Yuelin Deng,Esteban Flores,Ruijie Fu,Juntao He,Jianfeng Lin,Hang Lu,Guillaume Sartoretti,Howie Choset,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 该论文提出了一种基于线虫Ω转向的仿生控制方法，通过叠加两个行波来为无肢和多足细长机器人实现鲁棒有效的转向性能。


<details>
  <summary>Details</summary>
Motivation: 细长无肢机器人在紧密空间中的转向能力有限，而线虫C. elegans在复杂环境中的Ω转向能力具有启发意义，但目前缺乏有效的数学描述和机器人实现方法。

Method: 采用理论-生物学比较方法，将Ω转向描述为两个行波的叠加，并基于此设计机器人控制器。

Result: 成功在实验室和杂乱野外环境中实现了鲁棒的转向行为，且该控制器可推广到多足细长机器人。

Conclusion: Ω转向控制器为有肢和无肢细长机器人提供了一种有效的身体驱动转向策略。

Abstract: Elongate limbless robots have the potential to locomote through tightly
packed spaces for applications such as search-and-rescue and industrial
inspections. The capability to effectively and robustly maneuver elongate
limbless robots is crucial to realize such potential. However, there has been
limited research on turning strategies for such systems. To achieve effective
and robust turning performance in cluttered spaces, we take inspiration from a
microscopic nematode, C. elegans, which exhibits remarkable maneuverability in
rheologically complex environments partially because of its ability to perform
omega turns. Despite recent efforts to analyze omega turn kinematics, it
remains unknown if there exists a wave equation sufficient to prescribe an
omega turn, let alone its reconstruction on robot platforms. Here, using a
comparative theory-biology approach, we prescribe the omega turn as a
superposition of two traveling waves. With wave equations as a guideline, we
design a controller for limbless robots enabling robust and effective turning
behaviors in lab and cluttered field environments. Finally, we show that such
omega turn controllers can also generalize to elongate multi-legged robots,
demonstrating an alternative effective body-driven turning strategy for
elongate robots, with and without limbs.

</details>


### [6] [Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation](https://arxiv.org/abs/2510.12971)
*Anran Zhang,Hanzhi Chen,Yannick Burkhardt,Yao Zhong,Johannes Betz,Helen Oleynikova,Stefan Leutenegger*

Main category: cs.RO

TL;DR: Actron3D是一个从少量单目RGB人类视频中学习可迁移6-DoF操作技能的框架，通过神经功能表示实现跨任务的技能转移。


<details>
  <summary>Details</summary>
Motivation: 解决从少量未标定的人类演示视频中学习精确6-DoF操作技能的问题，实现技能的跨任务迁移。

Method: 使用神经功能表示法，将几何、视觉外观和功能线索压缩到轻量神经网络中，形成操作技能记忆库，通过粗到细优化实现策略转移。

Result: 在模拟和真实环境中显著优于现有方法，在13个任务上平均成功率提高14.9个百分点，每个任务仅需2-3个演示视频。

Conclusion: Actron3D证明了从少量未标定人类视频中学习可迁移6-DoF操作技能的可行性，为机器人技能学习提供了有效解决方案。

Abstract: We present Actron3D, a framework that enables robots to acquire transferable
6-DoF manipulation skills from just a few monocular, uncalibrated, RGB-only
human videos. At its core lies the Neural Affordance Function, a compact
object-centric representation that distills actionable cues from diverse
uncalibrated videos-geometry, visual appearance, and affordance-into a
lightweight neural network, forming a memory bank of manipulation skills.
During deployment, we adopt a pipeline that retrieves relevant affordance
functions and transfers precise 6-DoF manipulation policies via coarse-to-fine
optimization, enabled by continuous queries to the multimodal features encoded
in the neural functions. Experiments in both simulation and the real world
demonstrate that Actron3D significantly outperforms prior methods, achieving a
14.9 percentage point improvement in average success rate across 13 tasks while
requiring only 2-3 demonstration videos per task.

</details>


### [7] [UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles](https://arxiv.org/abs/2510.12992)
*Neel P. Bhatt,Po-han Li,Kushagra Gupta,Rohan Siva,Daniel Milan,Alexander T. Hogue,Sandeep P. Chinchali,David Fridovich-Keil,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: UNCAP提出了一种基于视觉语言模型的规划方法，通过轻量级自然语言消息进行CAV间通信，并显式考虑感知不确定性，显著降低了通信带宽并提高了驾驶安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖传输高带宽原始传感器数据，要么忽略共享数据中的感知和规划不确定性，导致系统既不可扩展也不安全。

Method: 采用两阶段通信协议：首先识别最相关的车辆进行信息交换，然后选定的CAV传输量化表达感知不确定性的消息，通过选择性融合最大化互信息的消息来整合最相关信号。

Result: 实验显示通信带宽减少63%，驾驶安全评分提高31%，决策不确定性降低61%，在接近碰撞事件中碰撞距离裕度增加四倍。

Conclusion: UNCAP通过自然语言通信和不确定性引导的规划，实现了CAV协同规划的可扩展性和可靠性提升。

Abstract: Safe large-scale coordination of multiple cooperative connected autonomous
vehicles (CAVs) hinges on communication that is both efficient and
interpretable. Existing approaches either rely on transmitting high-bandwidth
raw sensor data streams or neglect perception and planning uncertainties
inherent in shared data, resulting in systems that are neither scalable nor
safe. To address these limitations, we propose Uncertainty-Guided Natural
Language Cooperative Autonomous Planning (UNCAP), a vision-language model-based
planning approach that enables CAVs to communicate via lightweight natural
language messages while explicitly accounting for perception uncertainty in
decision-making. UNCAP features a two-stage communication protocol: (i) an ego
CAV first identifies the subset of vehicles most relevant for information
exchange, and (ii) the selected CAVs then transmit messages that quantitatively
express their perception uncertainty. By selectively fusing messages that
maximize mutual information, this strategy allows the ego vehicle to integrate
only the most relevant signals into its decision-making, improving both the
scalability and reliability of cooperative planning. Experiments across diverse
driving scenarios show a 63% reduction in communication bandwidth with a 31%
increase in driving safety score, a 61% reduction in decision uncertainty, and
a four-fold increase in collision distance margin during near-miss events.
Project website: https://uncap-project.github.io/

</details>


### [8] [Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations](https://arxiv.org/abs/2510.13005)
*Robert Muldrow,Channing Ludden,Christopher Petersen*

Main category: cs.RO

TL;DR: 设计开发了一个用于模拟在轨服务、组装和制造(ISAM)的硬件在环实验测试平台，通过6自由度机器人臂和1自由度导轨系统来验证空间运动、机器人操作和接触力学模型。


<details>
  <summary>Details</summary>
Motivation: 在轨服务、组装和制造(ISAM)操作中，机器人臂在自由飞行卫星上的运动会产生复杂的扰动力和运动，这构成了一个复杂的控制问题，需要实验验证现有的动力学模型。

Method: 开发了一个硬件在环(HIL)实验测试平台，包括安装在卫星总线上的6自由度UR3e机器人臂，该卫星总线安装在1自由度导轨系统上，使卫星总线和机器人臂能够在一个线性方向上自由移动。

Result: 成功设计并开发了能够模拟ISAM操作的实验系统，为验证空间运动、串行机器人操作和接触力学模型提供了实验平台。

Conclusion: 该实验测试平台为解决ISAM操作中的复杂控制问题提供了有效的实验验证手段，能够帮助验证和改进现有的动力学模型。

Abstract: In-Space Servicing, Assembly, and Manufacturing (ISAM) is a set of emerging
operations that provides several benefits to improve the longevity, capacity,
mo- bility, and expandability of existing and future space assets. Serial
robotic ma- nipulators are particularly vital in accomplishing ISAM operations,
however, the complex perturbation forces and motions associated with movement
of a robotic arm on a free-flying satellite presents a complex controls problem
requiring addi- tional study. While many dynamical models are developed,
experimentally test- ing and validating these models is challenging given that
the models operate in space, where satellites have six-degrees-of-freedom
(6-DOF). This paper attempts to resolve those challenges by presenting the
design and development of a new hardware-in-the-loop (HIL) experimental testbed
utilized to emulate ISAM. This emulation will be accomplished by means of a
6-DOF UR3e robotic arm attached to a satellite bus. This satellite bus is
mounted to a 1-DOF guide-rail system, en- abling the satellite bus and robotic
arm to move freely in one linear direction. This experimental ISAM emulation
system will explore and validate models for space motion, serial robot
manipulation, and contact mechanics.

</details>


### [9] [Kinematic Kitbashing for Modeling Functional Articulated Objects](https://arxiv.org/abs/2510.13048)
*Minghao Guo,Victor Zordan,Sheldon Andrews,Wojciech Matusik,Maneesh Agrawala,Hsueh-Ti Derek Liu*

Main category: cs.RO

TL;DR: Kinematic Kitbashing是一个自动框架，通过重用现有模型的零件来合成功能感知的铰接对象。该框架通过优化算法解决零件的空间布局问题，确保几何连接的合理性和用户指定的功能目标。


<details>
  <summary>Details</summary>
Motivation: 现有的零件建模方法通常只关注几何形状，而忽略了铰接对象的运动功能和连接合理性。该研究旨在将基于零件的形状建模与功能装配设计相结合，快速创建交互式铰接资产。

Method: 提出了一种运动学感知的连接能量函数，该函数在多个铰接快照中采样向量距离函数特征。将此连接项嵌入退火黎曼朗之万动力学采样器中，将功能目标作为额外能量，支持非可微功能目标和约束的全局探索。

Result: 该框架生成了广泛的铰接形状，从垃圾桶轮子移植到汽车车身到多段灯具、齿轮驱动的划桨器和可重构家具。在几何、运动学和功能指标上相比现有基线方法有显著改进。

Conclusion: 通过紧密耦合运动学感知的几何匹配与功能驱动的优化，Kinematic Kitbashing桥接了基于零件的形状建模和功能装配设计，赋能快速创建交互式铰接资产。

Abstract: We introduce Kinematic Kitbashing, an automatic framework that synthesizes
functionality-aware articulated objects by reusing parts from existing models.
Given a kinematic graph with a small collection of articulated parts, our
optimizer jointly solves for the spatial placement of every part so that (i)
attachments remain geometrically sound over the entire range of motion and (ii)
the assembled object satisfies user-specified functional goals such as
collision-free actuation, reachability, or trajectory following. At its core is
a kinematics-aware attachment energy that aligns vector distance function
features sampled across multiple articulation snapshots. We embed this
attachment term within an annealed Riemannian Langevin dynamics sampler that
treats functionality objectives as additional energies, enabling robust global
exploration while accommodating non-differentiable functionality objectives and
constraints. Our framework produces a wide spectrum of assembled articulated
shapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,
gear-driven paddlers, and reconfigurable furniture, and delivers strong
quantitative improvements over state-of-the-art baselines across geometric,
kinematic, and functional metrics. By tightly coupling articulation-aware
geometry matching with functionality-driven optimization, Kinematic Kitbashing
bridges part-based shape modeling and functional assembly design, empowering
rapid creation of interactive articulated assets.

</details>


### [10] [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://arxiv.org/abs/2510.13054)
*Ankit Goyal,Hugo Hadfield,Xuning Yang,Valts Blukis,Fabio Ramos*

Main category: cs.RO

TL;DR: VLA-0是一个简单的视觉-语言-动作模型，通过将动作直接表示为文本，在机器人操作任务中表现出色，超越了更复杂的模型设计。


<details>
  <summary>Details</summary>
Motivation: 探索最简单的视觉-语言-动作模型构建方法，即直接将动作表示为文本，而不是采用复杂的词汇表修改或特殊动作头设计。

Method: 使用文本直接表示动作的简单策略，通过特定的设计技巧来解锁这种简单设计的性能潜力。

Result: 在LIBERO基准测试中，VLA-0超越了所有使用相同机器人数据训练的方法，包括π0.5-KI、OpenVLA-OFT和SmolVLA。即使没有大规模机器人特定训练，也超越了π0.5-KI、π0、GR00T-N1和MolmoAct等模型。在真实世界测试中，VLA-0也优于经过大规模真实数据预训练的SmolVLA。

Conclusion: 直接将动作表示为文本的简单VLA设计不仅有效，而且出人意料地强大，在适当的技巧支持下，能够超越更复杂的模型设计。

Abstract: Vision-Language-Action models (VLAs) hold immense promise for enabling
generalist robot manipulation. However, the best way to build them remains an
open question. Current approaches often add complexity, such as modifying the
existing vocabulary of a Vision-Language Model (VLM) with action tokens or
introducing special action heads. Curiously, the simplest strategy of
representing actions directly as text has remained largely unexplored. This
work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only
effective; it is surprisingly powerful. With the right design, VLA-0
outperforms more involved models. On LIBERO, a popular benchmark for evaluating
VLAs, VLA-0 outperforms all existing methods trained on the same robotic data,
including $\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without
large-scale robotics-specific training, it outperforms methods trained on
large-scale robotic data, like $\pi_0.5$-KI, $\pi_0$, GR00T-N1 and MolmoAct.
These findings also translate to the real world, where VLA-0 outperforms
SmolVLA, a VLA model pre-trained on large-scale real data. This paper
summarizes our unexpected findings and spells out the specific techniques
required to unlock the high performance of this simple yet potent VLA design.
Visual results, code, and trained models are provided here:
https://vla0.github.io/.

</details>


### [11] [RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation](https://arxiv.org/abs/2510.13149)
*Yangtao Chen,Zixuan Chen,Nga Teng Chan,Junting Chen,Junhui Yin,Jieqi Shi,Yang Gao,Yong-Lu Li,Jing Huo*

Main category: cs.RO

TL;DR: 提出了RoboHiMan层次化评估范式，用于系统评估长时程操作中的组合泛化能力，包括基准测试、数据集和三种评估模式。


<details>
  <summary>Details</summary>
Motivation: 现有机器人技能组合方法在复杂扰动下表现有限，且缺乏对组合泛化、鲁棒性以及规划与执行交互的系统评估基准。

Method: 提出RoboHiMan框架，包含HiMan-Bench基准（原子和组合任务）、多级训练数据集和三种评估范式（vanilla、decoupled、coupled）。

Result: 实验揭示了代表性模型和架构在能力上的明显差距，为改进真实世界长时程操作任务模型指明了方向。

Conclusion: RoboHiMan为系统研究组合泛化提供了有效工具，有助于开发更适合真实世界长时程操作任务的模型。

Abstract: Enabling robots to flexibly schedule and compose learned skills for novel
long-horizon manipulation under diverse perturbations remains a core challenge.
Early explorations with end-to-end VLA models show limited success, as these
models struggle to generalize beyond the training distribution. Hierarchical
approaches, where high-level planners generate subgoals for low-level policies,
bring certain improvements but still suffer under complex perturbations,
revealing limited capability in skill composition. However, existing benchmarks
primarily emphasize task completion in long-horizon settings, offering little
insight into compositional generalization, robustness, and the interplay
between planning and execution. To systematically investigate these gaps, we
propose RoboHiMan, a hierarchical evaluation paradigm for compositional
generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,
a benchmark of atomic and compositional tasks under diverse perturbations,
supported by a multi-level training dataset for analyzing progressive data
scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)
that probe the necessity of skill composition and reveal bottlenecks in
hierarchical architectures. Experiments highlight clear capability gaps across
representative models and architectures, pointing to directions for advancing
models better suited to real-world long-horizon manipulation tasks. Videos and
open-source code can be found on our project website:
https://chenyt31.github.io/robo-himan.github.io/.

</details>


### [12] [ALOHA2 Robot Kitchen Application Scenario Reproduction Report](https://arxiv.org/abs/2510.13284)
*Haoyang Wu,Siheng Wu,William X. Liu,Fangui Zeng*

Main category: cs.RO

TL;DR: ALOHA2是ALOHA双臂遥操作机器人的增强版本，具有更高性能、更强鲁棒性和更好的人体工程学设计，包含两个夹爪、两个ViperX 6自由度机械臂和两个较小的WidowX机械臂，通过反向驱动实现遥操作控制。


<details>
  <summary>Details</summary>
Motivation: 开发一个比原始ALOHA设计性能更高、鲁棒性更强且更符合人体工程学的双臂遥操作机器人系统。

Method: 使用两个夹爪、两个ViperX 6自由度机械臂和两个WidowX机械臂构建系统，通过反向驱动实现遥操作控制，配备多视角摄像头收集RGB数据，安装在带铝框架的桌面上。

Result: 成功开发了ALOHA2系统，这是一个增强版的遥操作机器人平台，具有改进的性能和人体工程学特性。

Conclusion: ALOHA2代表了双臂遥操作机器人技术的显著进步，为机器人遥操作研究提供了更强大和用户友好的平台。

Abstract: ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,
featuring higher performance and robustness compared to the original design,
while also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers
and two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control
the follower mechanical arms by operating the leader mechanical arms through
back-driving. The device also includes cameras that generate images from
multiple viewpoints, allowing for RGB data collection during teleoperation. The
robot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame
that provides additional mounting points for cameras and gravity compensation
systems.

</details>


### [13] [DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping](https://arxiv.org/abs/2510.13287)
*Nishant Chandna,Akshat Kaushal*

Main category: cs.RO

TL;DR: 提出DAMM-LOAM系统，通过点云分类和退化感知的ICP算法，解决LiDAR SLAM在稀疏特征和重复结构环境中的位姿估计退化问题。


<details>
  <summary>Details</summary>
Motivation: 当前点对面ICP算法在结构化环境中表现良好，但在稀疏特征、重复几何结构和高频运动场景中会出现6自由度位姿估计退化问题，现有LiDAR-only解决方案仍面临限制。

Method: 1. 基于表面法线和邻域分析的点云分类（地面、墙壁、屋顶、边缘、非平面点）；2. 退化感知的加权最小二乘ICP算法；3. Scan Context后端支持鲁棒闭环检测。

Result: DAMM-LOAM在里程计精度方面有显著提升，特别是在长走廊等室内环境中表现优异。

Conclusion: 该论文提出的多度量LiDAR里程计和建图系统有效解决了退化环境下的位姿估计问题，为LiDAR-only SLAM提供了更鲁棒的解决方案。

Abstract: LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for
enabling precise navigation and environmental reconstruction across various
applications. Although current point-to-plane ICP algorithms perform effec-
tively in structured, feature-rich environments, they struggle in scenarios
with sparse features, repetitive geometric structures, and high-frequency
motion. This leads to degeneracy in 6- DOF pose estimation. Most
state-of-the-art algorithms address these challenges by incorporating
additional sensing modalities, but LiDAR-only solutions continue to face
limitations under such conditions. To address these issues, we propose a novel
Degeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.
Our system improves mapping accuracy through point cloud classification based
on surface normals and neighborhood analysis. Points are classified into
ground, walls, roof, edges, and non-planar points, enabling accurate
correspondences. A Degeneracy-based weighted least squares-based ICP algorithm
is then applied for accurate odom- etry estimation. Additionally, a Scan
Context based back-end is implemented to support robust loop closures.
DAMM-LOAM demonstrates significant improvements in odometry accuracy,
especially in indoor environments such as long corridors

</details>


### [14] [Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation](https://arxiv.org/abs/2510.13324)
*Erik Helmut,Niklas Funk,Tim Schneider,Cristiana de Farias,Jan Peters*

Main category: cs.RO

TL;DR: 提出了FARM框架，通过整合高维触觉数据来推断触觉条件化的力信号，并基于此定义力控制动作空间，在多种力需求任务中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法通常只将视觉触觉反馈作为额外观察，而将施加的力视为夹爪命令的不可控结果，无法精确控制接触力。

Method: 使用集成了GelSight Mini视觉触觉传感器的改进版手持UMI夹爪收集人类演示，开发了匹配几何形状的驱动版UMI夹爪用于策略部署，通过扩散策略联合预测机器人位姿、夹爪宽度和夹持力。

Result: FARM在三种具有不同力需求的任务（高力、低力和动态力适应）中均优于多个基线方法，证明了其两个关键组件的优势。

Conclusion: 通过利用基于力的高维触觉观测和力控制空间，FARM框架在接触丰富的操作任务中表现出色，代码和设计文件已开源。

Abstract: Contact-rich manipulation depends on applying the correct grasp forces
throughout the manipulation task, especially when handling fragile or
deformable objects. Most existing imitation learning approaches often treat
visuotactile feedback only as an additional observation, leaving applied forces
as an uncontrolled consequence of gripper commands. In this work, we present
Force-Aware Robotic Manipulation (FARM), an imitation learning framework that
integrates high-dimensional tactile data to infer tactile-conditioned force
signals, which in turn define a matching force-based action space. We collect
human demonstrations using a modified version of the handheld Universal
Manipulation Interface (UMI) gripper that integrates a GelSight Mini visual
tactile sensor. For deploying the learned policies, we developed an actuated
variant of the UMI gripper with geometry matching our handheld version. During
policy rollouts, the proposed FARM diffusion policy jointly predicts robot
pose, grip width, and grip force. FARM outperforms several baselines across
three tasks with distinct force requirements -- high-force, low-force, and
dynamic force adaptation -- demonstrating the advantages of its two key
components: leveraging force-grounded, high-dimensional tactile observations
and a force-based control space. The codebase and design files are open-sourced
and available at https://tactile-farm.github.io .

</details>


### [15] [MODUR: A Modular Dual-reconfigurable Robot](https://arxiv.org/abs/2510.13356)
*Jie Gu,Tin Lun Lam,Chunxu Tian,Zhihao Xia,Yongheng Xing,Dan Zhang*

Main category: cs.RO

TL;DR: 提出了一种名为MODUR的新型模块化自重构机器人，具有双重重构能力，既能进行模块间的高层重构，又能实现模块内部的形状变化。


<details>
  <summary>Details</summary>
Motivation: 将可重构机制集成到模块化自重构机器人系统中，以增强机器人在各种环境中的适应性和鲁棒性。

Method: 设计紧凑的连接器和剪刀连杆组，形成平行机构，实现连接器运动解耦和相邻位置迁移能力，并对工作空间进行全面分析。

Result: 通过一系列实验验证了MODUR的运动能力，证明了其双重重构机制的有效性。

Conclusion: MODUR成功实现了模块化自重构机器人的双重重构能力，为未来机器人系统的设计提供了理论基础和实践验证。

Abstract: Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots
capable of forming higher-level robotic systems by altering the topological
relationships between modules, offering enhanced adaptability and robustness in
various environments. This paper presents a novel MSRR called MODUR, featuring
dual-level reconfiguration capabilities designed to integrate reconfigurable
mechanisms into MSRR. Specifically, MODUR can perform high-level
self-reconfiguration among modules to create different configurations, while
each module is also able to change its shape to execute basic motions. The
design of MODUR primarily includes a compact connector and scissor linkage
groups that provide actuation, forming a parallel mechanism capable of
achieving both connector motion decoupling and adjacent position migration
capabilities. Furthermore, the workspace, considering the interdependent
connectors, is comprehensively analyzed, laying a theoretical foundation for
the design of the module's basic motion. Finally, the motion of MODUR is
validated through a series of experiments.

</details>


### [16] [Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control](https://arxiv.org/abs/2510.13358)
*Shingo Ayabe,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.RO

TL;DR: 提出一种离线到在线框架，通过在离线训练的策略上注入动作空间扰动进行对抗性微调，结合性能感知课程学习策略，提升策略在不确定环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习虽然样本效率高，但在面对执行器故障等动作空间扰动时策略表现脆弱。需要一种方法既能保持离线学习的效率，又能提升在线适应性。

Method: 先离线训练策略，然后在线微调时注入动作空间扰动，使用基于指数移动平均的性能感知课程自适应调整扰动概率，平衡鲁棒性和稳定性。

Result: 在连续控制运动任务中，该方法相比纯离线基线显著提升鲁棒性，比从头训练收敛更快。匹配微调与评估条件时鲁棒性最强，自适应课程策略缓解了线性课程策略的性能下降问题。

Conclusion: 对抗性微调能够在不确定环境中实现自适应和鲁棒控制，弥合了离线效率与在线适应性之间的差距。

Abstract: Offline reinforcement learning enables sample-efficient policy acquisition
without risky online interaction, yet policies trained on static datasets
remain brittle under action-space perturbations such as actuator faults. This
study introduces an offline-to-online framework that trains policies on clean
data and then performs adversarial fine-tuning, where perturbations are
injected into executed actions to induce compensatory behavior and improve
resilience. A performance-aware curriculum further adjusts the perturbation
probability during training via an exponential-moving-average signal, balancing
robustness and stability throughout the learning process. Experiments on
continuous-control locomotion tasks demonstrate that the proposed method
consistently improves robustness over offline-only baselines and converges
faster than training from scratch. Matching the fine-tuning and evaluation
conditions yields the strongest robustness to action-space perturbations, while
the adaptive curriculum strategy mitigates the degradation of nominal
performance observed with the linear curriculum strategy. Overall, the results
show that adversarial fine-tuning enables adaptive and robust control under
uncertain environments, bridging the gap between offline efficiency and online
adaptability.

</details>


### [17] [Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets](https://arxiv.org/abs/2510.13443)
*Mojtaba Mollahossein,Gholamreza Vossoughi,Mohammad Hossein Rohban*

Main category: cs.RO

TL;DR: 提出了一种基于迁移学习的膝关节角度预测框架，仅需少量步态周期数据，使用轻量级注意力CNN-LSTM模型，在多个数据集上实现了优异的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有EMG信号预测关节角度的方法存在实时性差、测试条件不具代表性、需要大量数据等问题，需要开发更高效、泛化能力强的解决方案。

Method: 开发轻量级注意力CNN-LSTM模型，在Georgia Tech数据集上预训练，然后迁移到UCI和SMLE数据集，结合EMG信号、历史角度和交互力等多模态输入。

Result: 仅使用EMG输入时，异常受试者的单步和50步预测NMAE分别为6.8%和13.7%；结合历史角度后，正常和异常受试者的误差分别降至3.1%/3.5%和2.8%/7.5%；在SMLE外骨骼上结合多模态输入，误差进一步降至1.09%和3.1%。

Conclusion: 该框架在短期和长期康复场景中均表现出鲁棒性能和强泛化能力，为基于EMG的关节角度预测提供了高效解决方案。

Abstract: Electromyography (EMG) signals are widely used for predicting body joint
angles through machine learning (ML) and deep learning (DL) methods. However,
these approaches often face challenges such as limited real-time applicability,
non-representative test conditions, and the need for large datasets to achieve
optimal performance. This paper presents a transfer-learning framework for knee
joint angle prediction that requires only a few gait cycles from new subjects.
Three datasets - Georgia Tech, the University of California Irvine (UCI), and
the Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels
relevant to knee motion were utilized. A lightweight attention-based CNN-LSTM
model was developed and pre-trained on the Georgia Tech dataset, then
transferred to the UCI and SMLE datasets. The proposed model achieved
Normalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for
one-step and 50-step predictions on abnormal subjects using EMG inputs alone.
Incorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5
percent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal
subjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and
interaction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE
for one- and 50-step predictions, respectively. These results demonstrate
robust performance and strong generalization for both short- and long-term
rehabilitation scenarios.

</details>


### [18] [Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations](https://arxiv.org/abs/2510.13488)
*Maximilian Stasica,Arne Bick,Nico Bohlinger,Omid Mohseni,Max Johannes Alois Fritzsche,Clemens Hübler,Jan Peters,André Seyfarth*

Main category: cs.RO

TL;DR: 本文提出了一种在振荡桥面上训练四足机器人增强运动鲁棒性的方法，使用强化学习在模拟环境中训练多种步态策略，并实现零样本迁移到真实世界。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在崎岖地形上表现出色，但在垂直地面扰动（如振荡表面）下的性能研究不足，需要开发更鲁棒的运动控制方法。

Method: 使用PPO强化学习算法在MuJoCo模拟器中训练Unitree Go2机器人，结合5种步态和3种训练条件（刚性桥面和两种振荡桥面设置），通过领域随机化实现零样本迁移。

Result: 在振荡桥面上训练的策略表现出比刚性表面训练策略更优越的稳定性和适应性，能够产生鲁棒的步态模式而无需事先接触桥面。

Conclusion: 基于模拟的强化学习能够显著提高四足机器人在动态地面扰动下的运动能力，为设计能够穿越振动环境的机器人提供了重要见解。

Abstract: Legged robots, particularly quadrupeds, excel at navigating rough terrains,
yet their performance under vertical ground perturbations, such as those from
oscillating surfaces, remains underexplored. This study introduces a novel
approach to enhance quadruped locomotion robustness by training the Unitree Go2
robot on an oscillating bridge - a 13.24-meter steel-and-concrete structure
with a 2.0 Hz eigenfrequency designed to perturb locomotion. Using
Reinforcement Learning (RL) with the Proximal Policy Optimization (PPO)
algorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,
combining five gaits (trot, pace, bound, free, default) with three training
conditions: rigid bridge and two oscillating bridge setups with differing
height regulation strategies (relative to bridge surface or ground). Domain
randomization ensured zero-shot transfer to the real-world bridge. Our results
demonstrate that policies trained on the oscillating bridge exhibit superior
stability and adaptability compared to those trained on rigid surfaces. Our
framework enables robust gait patterns even without prior bridge exposure.
These findings highlight the potential of simulation-based RL to improve
quadruped locomotion during dynamic ground perturbations, offering insights for
designing robots capable of traversing vibrating environments.

</details>


### [19] [A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints](https://arxiv.org/abs/2510.13535)
*Wentao Guo,Yizhou Wang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 提出了一种新型欠驱动自适应机器人手Hockens-A Hand，通过集成Hoeckens机构、双平行四边形连杆和专用四杆机构，仅需单个线性执行器即可实现三种自适应抓取模式：平行夹持、非对称铲取和包络抓取。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在非结构化环境中实现自适应和顺应性抓取的机器人手，通过被动机械智能减少执行器数量，提高抓取适应性。

Method: 结合Hoeckens机构提供顺应性，双平行四边形连杆确保指尖线接触，四杆放大系统实现抓取模式间的自然过渡，并通过运动学分析优化推角和连杆长度设计。

Result: 仿真验证了指尖运动和抓取模式间的平滑过渡，3D打印原型在环境约束下展示了三种抓取模式的稳定性，证明了其广泛的适用性。

Conclusion: Hockens-A Hand成功实现了仅用单个执行器的多模式自适应抓取，在非结构化环境中表现出良好的顺应性和抓取稳定性。

Abstract: This paper presents a novel underactuated adaptive robotic hand, Hockens-A
Hand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,
and a specialized four-bar linkage to achieve three adaptive grasping modes:
parallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand
requires only a single linear actuator, leveraging passive mechanical
intelligence to ensure adaptability and compliance in unstructured
environments. Specifically, the vertical motion of the Hoeckens mechanism
introduces compliance, the double-parallelogram linkage ensures line contact at
the fingertip, and the four-bar amplification system enables natural
transitions between different grasping modes. Additionally, the inclusion of a
mesh-textured silicone phalanx further enhances the ability to envelop objects
of various shapes and sizes. This study employs detailed kinematic analysis to
optimize the push angle and design the linkage lengths for optimal performance.
Simulations validated the design by analyzing the fingertip motion and ensuring
smooth transitions between grasping modes. Furthermore, the grasping force was
analyzed using power equations to enhance the understanding of the system's
performance.Experimental validation using a 3D-printed prototype demonstrates
the three grasping modes of the hand in various scenarios under environmental
constraints, verifying its grasping stability and broad applicability.

</details>


### [20] [Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.13553)
*Wentao Guo,Wenzeng Zhang*

Main category: cs.RO

TL;DR: Hoecken-D Hand是一种欠驱动机器人抓手，结合改进的Hoecken连杆机构和差动弹簧机制，能够在直线平行夹持和中程自适应包络之间切换，使用单个线性驱动器实现复杂抓取功能。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在结构化夹持和自适应包络抓取之间无缝切换的机器人抓手，以应对非结构化环境中各种几何形状物体的抓取需求，同时保持结构简单和成本效益。

Method: 重新配置原始Hoecken连杆机构，用差动连杆替换一个构件，保持直线引导同时实现接触触发重构；采用双平行四边形布置保持指尖平行性，差动机制允许手指在遇到障碍时向内包裹。

Result: 原型实现了约200mm的直线夹持范围，初步测试显示在多种物体几何形状下两种抓取模式都能可靠工作，证明了该设计在非结构化环境中的适用性。

Conclusion: Hoecken-D Hand提供了一种紧凑、适应性强且成本效益高的解决方案，通过单一机制实现了多种抓取模式，特别适合非结构化环境中的操作任务。

Abstract: This paper presents the Hoecken-D Hand, an underactuated robotic gripper that
combines a modified Hoecken linkage with a differential spring mechanism to
achieve both linear parallel pinching and a mid-stroke transition to adaptive
envelope. The original Hoecken linkage is reconfigured by replacing one member
with differential links, preserving straight-line guidance while enabling
contact-triggered reconfiguration without additional actuators. A
double-parallelogram arrangement maintains fingertip parallelism during
conventional pinching, whereas the differential mechanism allows one finger to
wrap inward upon encountering an obstacle, improving stability on irregular or
thin objects. The mechanism can be driven by a single linear actuator,
minimizing complexity and cost; in our prototype, each finger is driven by its
own linear actuator for simplicity. We perform kinematic modeling and force
analysis to characterize grasp performance, including simulated grasping forces
and spring-opening behavior under varying geometric parameters. The design was
prototyped using PLA-based 3D printing, achieving a linear pinching span of
approximately 200 mm. Preliminary tests demonstrate reliable grasping in both
modes across a wide range of object geometries, highlighting the Hoecken-D Hand
as a compact, adaptable, and cost-effective solution for manipulation in
unstructured environments.

</details>


### [21] [Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots](https://arxiv.org/abs/2510.13594)
*Austin Barret,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 开发面向非专家用户的简单直观人形机器人图形用户界面，用于控制机器人通过FIRA障碍赛道


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人系统缺乏专门为非专家操作者设计的图形用户界面，限制了系统的易用性和可操作性

Method: 结合用户界面开发实践和人机交互概念，开发新的非专家远程操作系统界面

Result: 开发出可扩展的、简单直观的GUI界面

Conclusion: 成功创建了适合非专家操作者使用的机器人控制界面，提高了系统的可访问性

Abstract: The operation of humanoid robotics is an essential field of research with
many practical and competitive applications. Many of these systems, however, do
not invest heavily in developing a non-expert-centered graphical user interface
(GUI) for operation. The focus of this research is to develop a scalable GUI
that is tailored to be simple and intuitive so non-expert operators can control
the robot through a FIRA-regulated obstacle course. Using common practices from
user interface development (UI) and understanding concepts described in
human-robot interaction (HRI) and other related concepts, we will develop a new
interface with the goal of a non-expert teleoperation system.

</details>


### [22] [Active Tactile Exploration for Rigid Body Pose and Shape Estimation](https://arxiv.org/abs/2510.13595)
*Ethan K. Gordon,Bruke Baraki,Hien Bui,Michael Posa*

Main category: cs.RO

TL;DR: 提出一种仅使用触觉数据同时确定刚性物体形状和位置的学习与探索框架，通过物理约束违反惩罚和期望信息增益最大化实现高效学习


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作需要处理未见过的物体，学习物理精确模型在测试时能提供数据效率、可预测性和任务间重用的优势，触觉感知能补充视觉对遮挡的鲁棒性

Method: 基于接触丰富系统识别的最新进展，制定惩罚物理约束违反的损失函数，避免刚体接触的数值刚度；优化该损失学习立方体和凸多面体几何；探索方案寻求最大化期望信息增益

Result: 在首次接触后仅用不到10秒的随机收集数据就能学习几何形状；探索方案在仿真和真实机器人实验中显著加快学习速度

Conclusion: 该框架能够仅使用触觉数据高效学习物体的形状和位置，为机器人操作提供了数据高效的方法

Abstract: General robot manipulation requires the handling of previously unseen
objects. Learning a physically accurate model at test time can provide
significant benefits in data efficiency, predictability, and reuse between
tasks. Tactile sensing can compliment vision with its robustness to occlusion,
but its temporal sparsity necessitates careful online exploration to maintain
data efficiency. Direct contact can also cause an unrestrained object to move,
requiring both shape and location estimation. In this work, we propose a
learning and exploration framework that uses only tactile data to
simultaneously determine the shape and location of rigid objects with minimal
robot motion. We build on recent advances in contact-rich system identification
to formulate a loss function that penalizes physical constraint violation
without introducing the numerical stiffness inherent in rigid-body contact.
Optimizing this loss, we can learn cuboid and convex polyhedral geometries with
less than 10s of randomly collected data after first contact. Our exploration
scheme seeks to maximize Expected Information Gain and results in significantly
faster learning in both simulated and real-robot experiments. More information
can be found at https://dairlab.github.io/activetactile

</details>


### [23] [PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction](https://arxiv.org/abs/2510.13599)
*Jiahao Wang,Nived Chebrolu,Yifu Tao,Lintong Zhang,Ayoung Kim,Maurice Fallon*

Main category: cs.RO

TL;DR: PlanarMesh是一种新颖的增量式、基于网格的LiDAR重建系统，通过自适应调整网格分辨率实现紧凑、详细的实时重建，结合平面建模和网格化来捕捉大表面和精细几何结构。


<details>
  <summary>Details</summary>
Motivation: 构建一个在线3D LiDAR建图系统，既能产生详细的表面重建，又能保持计算效率，这是一个具有挑战性的任务。

Method: 引入平面网格表示法，结合平面建模和网格化；采用多线程架构和边界体积层次结构(BVH)进行高效数据存储和快速搜索；根据局部表面曲率和传感器测量的自由空间信息进行增量更新。

Result: 在重建精度上与或超过最先进技术（包括截断符号距离函数、占用映射和基于体素的网格化），同时产生更小的输出文件大小（比原始输入小10倍，比基于网格的方法小5倍以上），并保持实时性能（64束传感器约2Hz）。

Conclusion: PlanarMesh系统在保持实时性能的同时，实现了高精度的LiDAR重建，并显著减少了存储需求。

Abstract: Building an online 3D LiDAR mapping system that produces a detailed surface
reconstruction while remaining computationally efficient is a challenging task.
In this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR
reconstruction system that adaptively adjusts mesh resolution to achieve
compact, detailed reconstructions in real-time. It introduces a new
representation, planar-mesh, which combines plane modeling and meshing to
capture both large surfaces and detailed geometry. The planar-mesh can be
incrementally updated considering both local surface curvature and free-space
information from sensor measurements. We employ a multi-threaded architecture
with a Bounding Volume Hierarchy (BVH) for efficient data storage and fast
search operations, enabling real-time performance. Experimental results show
that our method achieves reconstruction accuracy on par with, or exceeding,
state-of-the-art techniques-including truncated signed distance functions,
occupancy mapping, and voxel-based meshing-while producing smaller output file
sizes (10 times smaller than raw input and more than 5 times smaller than
mesh-based methods) and maintaining real-time performance (around 2 Hz for a
64-beam sensor).

</details>


### [24] [Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor](https://arxiv.org/abs/2510.13616)
*Preston Fairchild,Claudia Chen,Xiaobo Tan*

Main category: cs.RO

TL;DR: 开发了一种低成本柔性压力传感器，集成到机器人夹爪中，用于处理不同形状、大小和硬度的农产品，实现精确抓握和产品质量检测。


<details>
  <summary>Details</summary>
Motivation: 农业自动化收获和加工中，机器人需要正确处理易损农产品，掌握合适的抓握力度以避免损坏产品。

Method: 将柔性压力传感器集成到刚性机器人夹爪和气动软指中，提出基于瞬态响应的稳态值快速估计算法，实现实时应用。

Result: 传感器能有效提供反馈以正确抓握未知大小和硬度的物体，同时估计这些参数用于识别成熟度和损伤等品质特征。

Conclusion: 该技术不仅可用于农产品识别，还能用于质量控制和基于成熟度的选择性分拣等任务。

Abstract: Properly handling delicate produce with robotic manipulators is a major part
of the future role of automation in agricultural harvesting and processing.
Grasping with the correct amount of force is crucial in not only ensuring
proper grip on the object, but also to avoid damaging or bruising the product.
In this work, a flexible pressure sensor that is both low cost and easy to
fabricate is integrated with robotic grippers for working with produce of
varying shapes, sizes, and stiffnesses. The sensor is successfully integrated
with both a rigid robotic gripper, as well as a pneumatically actuated soft
finger. Furthermore, an algorithm is proposed for accelerated estimation of the
steady-state value of the sensor output based on the transient response data,
to enable real-time applications. The sensor is shown to be effective in
incorporating feedback to correctly grasp objects of unknown sizes and
stiffnesses. At the same time, the sensor provides estimates for these values
which can be utilized for identification of qualities such as ripeness levels
and bruising. It is also shown to be able to provide force feedback for objects
of variable stiffnesses. This enables future use not only for produce
identification, but also for tasks such as quality control and selective
distribution based on ripeness levels.

</details>


### [25] [Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization](https://arxiv.org/abs/2510.13619)
*Daniel Choate,Jason Rife*

Main category: cs.RO

TL;DR: 提出一种可视化方法，帮助分析人员分类影响激光雷达扫描匹配的逆境模式，通过向量场图显示配准点云间的局部差异。


<details>
  <summary>Details</summary>
Motivation: 帮助人类分析人员识别和分类影响激光雷达扫描匹配的逆境模式，这些模式很难从原始点云数据中直接提取。

Method: 生成向量场图来表征配准点云对之间的局部差异，该方法适用于离线分析而非实时分析。

Result: 在仿真研究和现场实验两个概念验证示例中，分析人员能够推理一系列逆境机制，并迭代地从原始数据中移除这些机制。

Conclusion: 该可视化方法能有效揭示点云配准中的局部差异模式，帮助分析人员逐步关注更小的差异。

Abstract: In this paper we introduce a visualization methodology to aid a human analyst
in classifying adversity modes that impact lidar scan matching. Our methodology
is intended for offline rather than real-time analysis. The method generates a
vector-field plot that characterizes local discrepancies between a pair of
registered point clouds. The vector field plot reveals patterns that would be
difficult for the analyst to extract from raw point-cloud data. After
introducing our methodology, we apply the process to two proof-of-concept
examples: one a simulation study and the other a field experiment. For both
data sets, a human analyst was able to reason about a series of adversity
mechanisms and iteratively remove those mechanisms from the raw data, to help
focus attention on progressively smaller discrepancies.

</details>


### [26] [A Modular Object Detection System for Humanoid Robots Using YOLO](https://arxiv.org/abs/2510.13625)
*Nicolas Pottier,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 提出基于YOLOv9的通用视觉模块，用于机器人环境，在FIRA Hurocup数据集上训练，与现有几何方法相比获得可比精度但计算成本更高，同时提供更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人领域中的计算机视觉仍是重要障碍，许多任务因低效的视觉系统而受阻，需要优化计算受限环境下的视觉解决方案。

Method: 使用YOLOv9框架构建通用视觉模块，在FIRA Hurocup数据集上训练，通过ROS1虚拟环境实现YOLO兼容性，使用FPS和mAP指标评估性能。

Result: YOLO模型在静态和动态场景下与几何模型相比达到可比精度，但计算成本更高，同时提供了改进的鲁棒性。

Conclusion: YOLOv9视觉模块在机器人应用中提供了有前景的精度和鲁棒性，但需要权衡计算效率与性能。

Abstract: Within the field of robotics, computer vision remains a significant barrier
to progress, with many tasks hindered by inefficient vision systems. This
research proposes a generalized vision module leveraging YOLOv9, a
state-of-the-art framework optimized for computationally constrained
environments like robots. The model is trained on a dataset tailored to the
FIRA robotics Hurocup. A new vision module is implemented in ROS1 using a
virtual environment to enable YOLO compatibility. Performance is evaluated
using metrics such as frames per second (FPS) and Mean Average Precision (mAP).
Performance is then compared to the existing geometric framework in static and
dynamic contexts. The YOLO model achieved comparable precision at a higher
computational cost then the geometric model, while providing improved
robustness.

</details>


### [27] [LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models](https://arxiv.org/abs/2510.13626)
*Senyu Fei,Siyin Wang,Junhao Shi,Zihao Dai,Jikun Cai,Pengfang Qian,Li Ji,Xinzhe He,Shiduo Zhang,Zhaoye Fei,Jinlan Fu,Jingjing Gong,Xipeng Qiu*

Main category: cs.RO

TL;DR: 对VLA模型进行系统性脆弱性分析，发现在看似高成功率下存在严重鲁棒性问题，性能在轻微扰动下从95%骤降至30%以下，且模型倾向于完全忽略语言指令。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型在机器人操作基准测试中报告了令人印象深刻的成功率，但这些结果可能掩盖了鲁棒性的根本弱点，需要进行系统性脆弱性分析。

Method: 在七个维度引入受控扰动：物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声，全面分析多个最先进模型。

Result: 模型表现出对扰动因素的极端敏感性，特别是相机视角和机器人初始状态，性能大幅下降。令人惊讶的是，模型对语言变化不敏感，倾向于完全忽略语言指令。

Conclusion: 研究结果挑战了高基准分数等同于真正能力的假设，强调了需要在真实变化下评估可靠性的评估实践。

Abstract: Visual-Language-Action (VLA) models report impressive success rates on
robotic manipulation benchmarks, yet these results may mask fundamental
weaknesses in robustness. We perform a systematic vulnerability analysis by
introducing controlled perturbations across seven dimensions: objects layout,
camera viewpoints, robot initial states, language instructions, light
conditions, background textures and sensor noise. We comprehensively analyzed
multiple state-of-the-art models and revealed consistent brittleness beneath
apparent competence. Our analysis exposes critical weaknesses: models exhibit
extreme sensitivity to perturbation factors, including camera viewpoints and
robot initial states, with performance dropping from 95% to below 30% under
modest perturbations. Surprisingly, models are largely insensitive to language
variations, with further experiments revealing that models tend to ignore
language instructions completely. Our findings challenge the assumption that
high benchmark scores equate to true competency and highlight the need for
evaluation practices that assess reliability under realistic variation.

</details>


### [28] [On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas](https://arxiv.org/abs/2510.13644)
*Michael Bosello,Flavio Pinzarrone,Sara Kiade,Davide Aguiari,Yvo Keuter,Aaesha AlShehhi,Gyordan Caminati,Kei Long Wong,Ka Seng Chou,Junaid Halepota,Fares Alneyadi,Jacopo Panerati,Giovanni Pau*

Main category: cs.RO

TL;DR: 该论文提出了一种基于视觉的无人机自主系统，在受控和无仪器环境中均能匹配专业人类飞行员的性能，并公开了相关飞行数据。


<details>
  <summary>Details</summary>
Motivation: 当前自主无人机系统主要在高度受控环境中训练和评估，限制了其在商业和实地应用中的直接适用性。需要验证系统在无外部跟踪的真实环境中的性能。

Method: 在受控环境（有外部跟踪提供地面真值）和无仪器环境（无地面真值测量）中测试自主无人机系统，并与专业人类飞行员进行性能比较。

Result: 所提出的方法在两种场景下都能匹配专业人类飞行员的性能表现。

Conclusion: 该研究证明了视觉自主无人机系统在实际应用中的可行性，为商业和实地部署提供了重要参考，并公开了宝贵的数据集供进一步研究。

Abstract: Drone technology is proliferating in many industries, including agriculture,
logistics, defense, infrastructure, and environmental monitoring. Vision-based
autonomy is one of its key enablers, particularly for real-world applications.
This is essential for operating in novel, unstructured environments where
traditional navigation methods may be unavailable. Autonomous drone racing has
become the de facto benchmark for such systems. State-of-the-art research has
shown that autonomous systems can surpass human-level performance in racing
arenas. However, direct applicability to commercial and field operations is
still limited as current systems are often trained and evaluated in highly
controlled environments. In our contribution, the system's capabilities are
analyzed within a controlled environment -- where external tracking is
available for ground-truth comparison -- but also demonstrated in a
challenging, uninstrumented environment -- where ground-truth measurements were
never available. We show that our approach can match the performance of
professional human pilots in both scenarios. We also publicly release the data
from the flights carried out by our approach and a world-class human pilot.

</details>


### [29] [Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures](https://arxiv.org/abs/2510.13686)
*Miana Smith,Paul Arthur Richard,Alexander Htet Kyaw,Neil Gershenfeld*

Main category: cs.RO

TL;DR: 提出了一种使用简单机器人和互锁晶格构建块来制造可扩展宏观结构的方法，通过体素化、分层分组和移动机器人组装实现米级结构的自动化建造。


<details>
  <summary>Details</summary>
Motivation: 桌面级数字制造技术已成熟，但大型结构制造系统通常复杂、昂贵且不可靠，需要开发更简单可靠的大规模制造方法。

Method: 将目标结构体素化并填充架构晶格，将体素分组为更大互连块，使用移动机器人遍历结构并放置新块，引入数字孪生模拟工具进行控制和协调。

Result: 验证了系统在米级物体上的体素化、分层分组、路径规划和机器人制造能力，展示了可扩展宏观结构的自动化建造。

Conclusion: 该方法通过简单机器人和互锁晶格构建块实现了可靠的大型结构制造，为大规模数字制造提供了新途径。

Abstract: Although digital fabrication processes at the desktop scale have become
proficient and prolific, systems aimed at producing larger-scale structures are
still typically complex, expensive, and unreliable. In this work, we present an
approach for the fabrication of scalable macroscale structures using simple
robots and interlocking lattice building blocks. A target structure is first
voxelized so that it can be populated with an architected lattice. These voxels
are then grouped into larger interconnected blocks, which are produced using
standard digital fabrication processes, leveraging their capability to produce
highly complex geometries at a small scale. These blocks, on the size scale of
tens of centimeters, are then fed to mobile relative robots that are able to
traverse over the structure and place new blocks to form structures on the
meter scale. To facilitate the assembly of large structures, we introduce a
live digital twin simulation tool for controlling and coordinating assembly
robots that enables both global planning for a target structure and live user
design, interaction, or intervention. To improve assembly throughput, we
introduce a new modular assembly robot, designed for hierarchical voxel
handling. We validate this system by demonstrating the voxelization,
hierarchical blocking, path planning, and robotic fabrication of a set of
meter-scale objects.

</details>


### [30] [InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy](https://arxiv.org/abs/2510.13778)
*Xinyi Chen,Yilun Chen,Yanwei Fu,Ning Gao,Jiaya Jia,Weiyang Jin,Hao Li,Yao Mu,Jiangmiao Pang,Yu Qiao,Yang Tian,Bin Wang,Bolun Wang,Fangjing Wang,Hanqing Wang,Tai Wang,Ziqin Wang,Xueyuan Wei,Chao Wu,Shuai Yang,Jinhui Ye,Junqiu Yu,Jia Zeng,Jingjing Zhang,Jinyu Zhang,Shi Zhang,Feng Zheng,Bowen Zhou,Yangkun Zhu*

Main category: cs.RO

TL;DR: InternVLA-M1是一个统一的机器人控制框架，通过空间引导的视觉-语言-动作训练，将空间定位作为指令和机器人动作之间的关键链接，显著提升了机器人的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 推动指令跟随机器人向可扩展、通用智能方向发展，解决如何将自然语言指令有效转化为机器人动作的问题。

Method: 采用两阶段训练流程：第一阶段进行空间定位预训练，在230万空间推理数据上学习"在哪里行动"；第二阶段进行空间引导的动作后训练，通过即插即用的空间提示来决定"如何行动"。

Result: 在多个基准测试中表现优异：在SimplerEnv Google Robot上提升14.6%，WidowX上提升17%，LIBERO Franka上提升4.3%。在真实世界聚类拾放任务中提升7.3%，在未见物体和新配置上提升20.6%。

Conclusion: 空间引导训练是构建可扩展和鲁棒通用机器人的统一原则，为机器人的通用智能发展提供了有效路径。

Abstract: We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence. Its core idea is spatially guided
vision-language-action training, where spatial grounding serves as the critical
link between instructions and robot actions. InternVLA-M1 employs a two-stage
pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
data to determine ``where to act'' by aligning instructions with visual,
embodiment-agnostic positions, and (ii) spatially guided action post-training
to decide ``how to act'' by generating embodiment-aware actions through
plug-and-play spatial prompting. This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction. To further scale instruction following, we built a
simulation engine to collect 244K generalizable pick-and-place episodes,
enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
surpassed existing works by over 10%. These results highlight spatially guided
training as a unifying principle for scalable and resilient generalist robots.
Code and models are available at
https://github.com/InternRobotics/InternVLA-M1.

</details>
